<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250723.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting", "author": "Jianchuan Chen and Jingchuan Hu and Gaige Wang and Zhonghua Jiang and Tiansong Zhou and Zhiwen Chen and Chengfei Lv", "abstract": "  Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.\n", "link": "http://arxiv.org/abs/2503.17032v2", "date": "2025-07-23", "relevancy": 3.3715, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6952}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6952}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaoAvatar%3A%20Real-Time%20Lifelike%20Full-Body%20Talking%20Avatars%20for%20Augmented%0A%20%20Reality%20via%203D%20Gaussian%20Splatting&body=Title%3A%20TaoAvatar%3A%20Real-Time%20Lifelike%20Full-Body%20Talking%20Avatars%20for%20Augmented%0A%20%20Reality%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jianchuan%20Chen%20and%20Jingchuan%20Hu%20and%20Gaige%20Wang%20and%20Zhonghua%20Jiang%20and%20Tiansong%20Zhou%20and%20Zhiwen%20Chen%20and%20Chengfei%20Lv%0AAbstract%3A%20%20%20Realistic%203D%20full-body%20talking%20avatars%20hold%20great%20potential%20in%20AR%2C%20with%0Aapplications%20ranging%20from%20e-commerce%20live%20streaming%20to%20holographic%0Acommunication.%20Despite%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20lifelike%0Aavatar%20creation%2C%20existing%20methods%20struggle%20with%20fine-grained%20control%20of%20facial%0Aexpressions%20and%20body%20movements%20in%20full-body%20talking%20tasks.%20Additionally%2C%20they%0Aoften%20lack%20sufficient%20details%20and%20cannot%20run%20in%20real-time%20on%20mobile%20devices.%20We%0Apresent%20TaoAvatar%2C%20a%20high-fidelity%2C%20lightweight%2C%203DGS-based%20full-body%20talking%0Aavatar%20driven%20by%20various%20signals.%20Our%20approach%20starts%20by%20creating%20a%0Apersonalized%20clothed%20human%20parametric%20template%20that%20binds%20Gaussians%20to%0Arepresent%20appearances.%20We%20then%20pre-train%20a%20StyleUnet-based%20network%20to%20handle%0Acomplex%20pose-dependent%20non-rigid%20deformation%2C%20which%20can%20capture%20high-frequency%0Aappearance%20details%20but%20is%20too%20resource-intensive%20for%20mobile%20devices.%20To%0Aovercome%20this%2C%20we%20%22bake%22%20the%20non-rigid%20deformations%20into%20a%20lightweight%0AMLP-based%20network%20using%20a%20distillation%20technique%20and%20develop%20blend%20shapes%20to%0Acompensate%20for%20details.%20Extensive%20experiments%20show%20that%20TaoAvatar%20achieves%0Astate-of-the-art%20rendering%20quality%20while%20running%20in%20real-time%20across%20various%0Adevices%2C%20maintaining%2090%20FPS%20on%20high-definition%20stereo%20devices%20such%20as%20the%20Apple%0AVision%20Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaoAvatar%253A%2520Real-Time%2520Lifelike%2520Full-Body%2520Talking%2520Avatars%2520for%2520Augmented%250A%2520%2520Reality%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJianchuan%2520Chen%2520and%2520Jingchuan%2520Hu%2520and%2520Gaige%2520Wang%2520and%2520Zhonghua%2520Jiang%2520and%2520Tiansong%2520Zhou%2520and%2520Zhiwen%2520Chen%2520and%2520Chengfei%2520Lv%26entry.1292438233%3D%2520%2520Realistic%25203D%2520full-body%2520talking%2520avatars%2520hold%2520great%2520potential%2520in%2520AR%252C%2520with%250Aapplications%2520ranging%2520from%2520e-commerce%2520live%2520streaming%2520to%2520holographic%250Acommunication.%2520Despite%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520for%2520lifelike%250Aavatar%2520creation%252C%2520existing%2520methods%2520struggle%2520with%2520fine-grained%2520control%2520of%2520facial%250Aexpressions%2520and%2520body%2520movements%2520in%2520full-body%2520talking%2520tasks.%2520Additionally%252C%2520they%250Aoften%2520lack%2520sufficient%2520details%2520and%2520cannot%2520run%2520in%2520real-time%2520on%2520mobile%2520devices.%2520We%250Apresent%2520TaoAvatar%252C%2520a%2520high-fidelity%252C%2520lightweight%252C%25203DGS-based%2520full-body%2520talking%250Aavatar%2520driven%2520by%2520various%2520signals.%2520Our%2520approach%2520starts%2520by%2520creating%2520a%250Apersonalized%2520clothed%2520human%2520parametric%2520template%2520that%2520binds%2520Gaussians%2520to%250Arepresent%2520appearances.%2520We%2520then%2520pre-train%2520a%2520StyleUnet-based%2520network%2520to%2520handle%250Acomplex%2520pose-dependent%2520non-rigid%2520deformation%252C%2520which%2520can%2520capture%2520high-frequency%250Aappearance%2520details%2520but%2520is%2520too%2520resource-intensive%2520for%2520mobile%2520devices.%2520To%250Aovercome%2520this%252C%2520we%2520%2522bake%2522%2520the%2520non-rigid%2520deformations%2520into%2520a%2520lightweight%250AMLP-based%2520network%2520using%2520a%2520distillation%2520technique%2520and%2520develop%2520blend%2520shapes%2520to%250Acompensate%2520for%2520details.%2520Extensive%2520experiments%2520show%2520that%2520TaoAvatar%2520achieves%250Astate-of-the-art%2520rendering%2520quality%2520while%2520running%2520in%2520real-time%2520across%2520various%250Adevices%252C%2520maintaining%252090%2520FPS%2520on%2520high-definition%2520stereo%2520devices%2520such%2520as%2520the%2520Apple%250AVision%2520Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaoAvatar%3A%20Real-Time%20Lifelike%20Full-Body%20Talking%20Avatars%20for%20Augmented%0A%20%20Reality%20via%203D%20Gaussian%20Splatting&entry.906535625=Jianchuan%20Chen%20and%20Jingchuan%20Hu%20and%20Gaige%20Wang%20and%20Zhonghua%20Jiang%20and%20Tiansong%20Zhou%20and%20Zhiwen%20Chen%20and%20Chengfei%20Lv&entry.1292438233=%20%20Realistic%203D%20full-body%20talking%20avatars%20hold%20great%20potential%20in%20AR%2C%20with%0Aapplications%20ranging%20from%20e-commerce%20live%20streaming%20to%20holographic%0Acommunication.%20Despite%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20lifelike%0Aavatar%20creation%2C%20existing%20methods%20struggle%20with%20fine-grained%20control%20of%20facial%0Aexpressions%20and%20body%20movements%20in%20full-body%20talking%20tasks.%20Additionally%2C%20they%0Aoften%20lack%20sufficient%20details%20and%20cannot%20run%20in%20real-time%20on%20mobile%20devices.%20We%0Apresent%20TaoAvatar%2C%20a%20high-fidelity%2C%20lightweight%2C%203DGS-based%20full-body%20talking%0Aavatar%20driven%20by%20various%20signals.%20Our%20approach%20starts%20by%20creating%20a%0Apersonalized%20clothed%20human%20parametric%20template%20that%20binds%20Gaussians%20to%0Arepresent%20appearances.%20We%20then%20pre-train%20a%20StyleUnet-based%20network%20to%20handle%0Acomplex%20pose-dependent%20non-rigid%20deformation%2C%20which%20can%20capture%20high-frequency%0Aappearance%20details%20but%20is%20too%20resource-intensive%20for%20mobile%20devices.%20To%0Aovercome%20this%2C%20we%20%22bake%22%20the%20non-rigid%20deformations%20into%20a%20lightweight%0AMLP-based%20network%20using%20a%20distillation%20technique%20and%20develop%20blend%20shapes%20to%0Acompensate%20for%20details.%20Extensive%20experiments%20show%20that%20TaoAvatar%20achieves%0Astate-of-the-art%20rendering%20quality%20while%20running%20in%20real-time%20across%20various%0Adevices%2C%20maintaining%2090%20FPS%20on%20high-definition%20stereo%20devices%20such%20as%20the%20Apple%0AVision%20Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17032v2&entry.124074799=Read"},
{"title": "RemixFusion: Residual-based Mixed Representation for Large-scale Online\n  RGB-D Reconstruction", "author": "Yuqing Lan and Chenyang Zhu and Shuaifeng Zhi and Jiazhao Zhang and Zhoufeng Wang and Renjiao Yi and Yijie Wang and Kai Xu", "abstract": "  The introduction of the neural implicit representation has notably propelled\nthe advancement of online dense reconstruction techniques. Compared to\ntraditional explicit representations, such as TSDF, it improves the mapping\ncompleteness and memory efficiency. However, the lack of reconstruction details\nand the time-consuming learning of neural representations hinder the widespread\napplication of neural-based methods to large-scale online reconstruction. We\nintroduce RemixFusion, a novel residual-based mixed representation for scene\nreconstruction and camera pose estimation dedicated to high-quality and\nlarge-scale online RGB-D reconstruction. In particular, we propose a\nresidual-based map representation comprised of an explicit coarse TSDF grid and\nan implicit neural module that produces residuals representing fine-grained\ndetails to be added to the coarse grid. Such mixed representation allows for\ndetail-rich reconstruction with bounded time and memory budget, contrasting\nwith the overly-smoothed results by the purely implicit representations, thus\npaving the way for high-quality camera tracking. Furthermore, we extend the\nresidual-based representation to handle multi-frame joint pose optimization via\nbundle adjustment (BA). In contrast to the existing methods, which optimize\nposes directly, we opt to optimize pose changes. Combined with a novel\ntechnique for adaptive gradient amplification, our method attains better\noptimization convergence and global optimality. Furthermore, we adopt a local\nmoving volume to factorize the mixed scene representation with a\ndivide-and-conquer design to facilitate efficient online learning in our\nresidual-based framework. Extensive experiments demonstrate that our method\nsurpasses all state-of-the-art ones, including those based either on explicit\nor implicit representations, in terms of the accuracy of both mapping and\ntracking on large-scale scenes.\n", "link": "http://arxiv.org/abs/2507.17594v1", "date": "2025-07-23", "relevancy": 3.1559, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6981}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5979}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RemixFusion%3A%20Residual-based%20Mixed%20Representation%20for%20Large-scale%20Online%0A%20%20RGB-D%20Reconstruction&body=Title%3A%20RemixFusion%3A%20Residual-based%20Mixed%20Representation%20for%20Large-scale%20Online%0A%20%20RGB-D%20Reconstruction%0AAuthor%3A%20Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Shuaifeng%20Zhi%20and%20Jiazhao%20Zhang%20and%20Zhoufeng%20Wang%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu%0AAbstract%3A%20%20%20The%20introduction%20of%20the%20neural%20implicit%20representation%20has%20notably%20propelled%0Athe%20advancement%20of%20online%20dense%20reconstruction%20techniques.%20Compared%20to%0Atraditional%20explicit%20representations%2C%20such%20as%20TSDF%2C%20it%20improves%20the%20mapping%0Acompleteness%20and%20memory%20efficiency.%20However%2C%20the%20lack%20of%20reconstruction%20details%0Aand%20the%20time-consuming%20learning%20of%20neural%20representations%20hinder%20the%20widespread%0Aapplication%20of%20neural-based%20methods%20to%20large-scale%20online%20reconstruction.%20We%0Aintroduce%20RemixFusion%2C%20a%20novel%20residual-based%20mixed%20representation%20for%20scene%0Areconstruction%20and%20camera%20pose%20estimation%20dedicated%20to%20high-quality%20and%0Alarge-scale%20online%20RGB-D%20reconstruction.%20In%20particular%2C%20we%20propose%20a%0Aresidual-based%20map%20representation%20comprised%20of%20an%20explicit%20coarse%20TSDF%20grid%20and%0Aan%20implicit%20neural%20module%20that%20produces%20residuals%20representing%20fine-grained%0Adetails%20to%20be%20added%20to%20the%20coarse%20grid.%20Such%20mixed%20representation%20allows%20for%0Adetail-rich%20reconstruction%20with%20bounded%20time%20and%20memory%20budget%2C%20contrasting%0Awith%20the%20overly-smoothed%20results%20by%20the%20purely%20implicit%20representations%2C%20thus%0Apaving%20the%20way%20for%20high-quality%20camera%20tracking.%20Furthermore%2C%20we%20extend%20the%0Aresidual-based%20representation%20to%20handle%20multi-frame%20joint%20pose%20optimization%20via%0Abundle%20adjustment%20%28BA%29.%20In%20contrast%20to%20the%20existing%20methods%2C%20which%20optimize%0Aposes%20directly%2C%20we%20opt%20to%20optimize%20pose%20changes.%20Combined%20with%20a%20novel%0Atechnique%20for%20adaptive%20gradient%20amplification%2C%20our%20method%20attains%20better%0Aoptimization%20convergence%20and%20global%20optimality.%20Furthermore%2C%20we%20adopt%20a%20local%0Amoving%20volume%20to%20factorize%20the%20mixed%20scene%20representation%20with%20a%0Adivide-and-conquer%20design%20to%20facilitate%20efficient%20online%20learning%20in%20our%0Aresidual-based%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asurpasses%20all%20state-of-the-art%20ones%2C%20including%20those%20based%20either%20on%20explicit%0Aor%20implicit%20representations%2C%20in%20terms%20of%20the%20accuracy%20of%20both%20mapping%20and%0Atracking%20on%20large-scale%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemixFusion%253A%2520Residual-based%2520Mixed%2520Representation%2520for%2520Large-scale%2520Online%250A%2520%2520RGB-D%2520Reconstruction%26entry.906535625%3DYuqing%2520Lan%2520and%2520Chenyang%2520Zhu%2520and%2520Shuaifeng%2520Zhi%2520and%2520Jiazhao%2520Zhang%2520and%2520Zhoufeng%2520Wang%2520and%2520Renjiao%2520Yi%2520and%2520Yijie%2520Wang%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%2520the%2520neural%2520implicit%2520representation%2520has%2520notably%2520propelled%250Athe%2520advancement%2520of%2520online%2520dense%2520reconstruction%2520techniques.%2520Compared%2520to%250Atraditional%2520explicit%2520representations%252C%2520such%2520as%2520TSDF%252C%2520it%2520improves%2520the%2520mapping%250Acompleteness%2520and%2520memory%2520efficiency.%2520However%252C%2520the%2520lack%2520of%2520reconstruction%2520details%250Aand%2520the%2520time-consuming%2520learning%2520of%2520neural%2520representations%2520hinder%2520the%2520widespread%250Aapplication%2520of%2520neural-based%2520methods%2520to%2520large-scale%2520online%2520reconstruction.%2520We%250Aintroduce%2520RemixFusion%252C%2520a%2520novel%2520residual-based%2520mixed%2520representation%2520for%2520scene%250Areconstruction%2520and%2520camera%2520pose%2520estimation%2520dedicated%2520to%2520high-quality%2520and%250Alarge-scale%2520online%2520RGB-D%2520reconstruction.%2520In%2520particular%252C%2520we%2520propose%2520a%250Aresidual-based%2520map%2520representation%2520comprised%2520of%2520an%2520explicit%2520coarse%2520TSDF%2520grid%2520and%250Aan%2520implicit%2520neural%2520module%2520that%2520produces%2520residuals%2520representing%2520fine-grained%250Adetails%2520to%2520be%2520added%2520to%2520the%2520coarse%2520grid.%2520Such%2520mixed%2520representation%2520allows%2520for%250Adetail-rich%2520reconstruction%2520with%2520bounded%2520time%2520and%2520memory%2520budget%252C%2520contrasting%250Awith%2520the%2520overly-smoothed%2520results%2520by%2520the%2520purely%2520implicit%2520representations%252C%2520thus%250Apaving%2520the%2520way%2520for%2520high-quality%2520camera%2520tracking.%2520Furthermore%252C%2520we%2520extend%2520the%250Aresidual-based%2520representation%2520to%2520handle%2520multi-frame%2520joint%2520pose%2520optimization%2520via%250Abundle%2520adjustment%2520%2528BA%2529.%2520In%2520contrast%2520to%2520the%2520existing%2520methods%252C%2520which%2520optimize%250Aposes%2520directly%252C%2520we%2520opt%2520to%2520optimize%2520pose%2520changes.%2520Combined%2520with%2520a%2520novel%250Atechnique%2520for%2520adaptive%2520gradient%2520amplification%252C%2520our%2520method%2520attains%2520better%250Aoptimization%2520convergence%2520and%2520global%2520optimality.%2520Furthermore%252C%2520we%2520adopt%2520a%2520local%250Amoving%2520volume%2520to%2520factorize%2520the%2520mixed%2520scene%2520representation%2520with%2520a%250Adivide-and-conquer%2520design%2520to%2520facilitate%2520efficient%2520online%2520learning%2520in%2520our%250Aresidual-based%2520framework.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Asurpasses%2520all%2520state-of-the-art%2520ones%252C%2520including%2520those%2520based%2520either%2520on%2520explicit%250Aor%2520implicit%2520representations%252C%2520in%2520terms%2520of%2520the%2520accuracy%2520of%2520both%2520mapping%2520and%250Atracking%2520on%2520large-scale%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RemixFusion%3A%20Residual-based%20Mixed%20Representation%20for%20Large-scale%20Online%0A%20%20RGB-D%20Reconstruction&entry.906535625=Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Shuaifeng%20Zhi%20and%20Jiazhao%20Zhang%20and%20Zhoufeng%20Wang%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu&entry.1292438233=%20%20The%20introduction%20of%20the%20neural%20implicit%20representation%20has%20notably%20propelled%0Athe%20advancement%20of%20online%20dense%20reconstruction%20techniques.%20Compared%20to%0Atraditional%20explicit%20representations%2C%20such%20as%20TSDF%2C%20it%20improves%20the%20mapping%0Acompleteness%20and%20memory%20efficiency.%20However%2C%20the%20lack%20of%20reconstruction%20details%0Aand%20the%20time-consuming%20learning%20of%20neural%20representations%20hinder%20the%20widespread%0Aapplication%20of%20neural-based%20methods%20to%20large-scale%20online%20reconstruction.%20We%0Aintroduce%20RemixFusion%2C%20a%20novel%20residual-based%20mixed%20representation%20for%20scene%0Areconstruction%20and%20camera%20pose%20estimation%20dedicated%20to%20high-quality%20and%0Alarge-scale%20online%20RGB-D%20reconstruction.%20In%20particular%2C%20we%20propose%20a%0Aresidual-based%20map%20representation%20comprised%20of%20an%20explicit%20coarse%20TSDF%20grid%20and%0Aan%20implicit%20neural%20module%20that%20produces%20residuals%20representing%20fine-grained%0Adetails%20to%20be%20added%20to%20the%20coarse%20grid.%20Such%20mixed%20representation%20allows%20for%0Adetail-rich%20reconstruction%20with%20bounded%20time%20and%20memory%20budget%2C%20contrasting%0Awith%20the%20overly-smoothed%20results%20by%20the%20purely%20implicit%20representations%2C%20thus%0Apaving%20the%20way%20for%20high-quality%20camera%20tracking.%20Furthermore%2C%20we%20extend%20the%0Aresidual-based%20representation%20to%20handle%20multi-frame%20joint%20pose%20optimization%20via%0Abundle%20adjustment%20%28BA%29.%20In%20contrast%20to%20the%20existing%20methods%2C%20which%20optimize%0Aposes%20directly%2C%20we%20opt%20to%20optimize%20pose%20changes.%20Combined%20with%20a%20novel%0Atechnique%20for%20adaptive%20gradient%20amplification%2C%20our%20method%20attains%20better%0Aoptimization%20convergence%20and%20global%20optimality.%20Furthermore%2C%20we%20adopt%20a%20local%0Amoving%20volume%20to%20factorize%20the%20mixed%20scene%20representation%20with%20a%0Adivide-and-conquer%20design%20to%20facilitate%20efficient%20online%20learning%20in%20our%0Aresidual-based%20framework.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Asurpasses%20all%20state-of-the-art%20ones%2C%20including%20those%20based%20either%20on%20explicit%0Aor%20implicit%20representations%2C%20in%20terms%20of%20the%20accuracy%20of%20both%20mapping%20and%0Atracking%20on%20large-scale%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17594v1&entry.124074799=Read"},
{"title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "author": "Yiwen Chen and Zhihao Li and Yikai Wang and Hu Zhang and Qin Li and Chi Zhang and Guosheng Lin", "abstract": "  Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.\n", "link": "http://arxiv.org/abs/2507.17745v1", "date": "2025-07-23", "relevancy": 3.1257, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6322}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&body=Title%3A%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention%0AAuthor%3A%20Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra3D%253A%2520Efficient%2520and%2520High-Fidelity%25203D%2520Generation%2520with%2520Part%2520Attention%26entry.906535625%3DYiwen%2520Chen%2520and%2520Zhihao%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Hu%2520Zhang%2520and%2520Qin%2520Li%2520and%2520Chi%2520Zhang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520sparse%2520voxel%2520representations%2520have%2520significantly%2520improved%250Athe%2520quality%2520of%25203D%2520content%2520generation%252C%2520enabling%2520high-resolution%2520modeling%2520with%250Afine-grained%2520geometry.%2520However%252C%2520existing%2520frameworks%2520suffer%2520from%2520severe%250Acomputational%2520inefficiencies%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520attention%250Amechanisms%2520in%2520their%2520two-stage%2520diffusion%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520propose%250AUltra3D%252C%2520an%2520efficient%25203D%2520generation%2520framework%2520that%2520significantly%2520accelerates%250Asparse%2520voxel%2520modeling%2520without%2520compromising%2520quality.%2520Our%2520method%2520leverages%2520the%250Acompact%2520VecSet%2520representation%2520to%2520efficiently%2520generate%2520a%2520coarse%2520object%2520layout%2520in%250Athe%2520first%2520stage%252C%2520reducing%2520token%2520count%2520and%2520accelerating%2520voxel%2520coordinate%250Aprediction.%2520To%2520refine%2520per-voxel%2520latent%2520features%2520in%2520the%2520second%2520stage%252C%2520we%250Aintroduce%2520Part%2520Attention%252C%2520a%2520geometry-aware%2520localized%2520attention%2520mechanism%2520that%250Arestricts%2520attention%2520computation%2520within%2520semantically%2520consistent%2520part%2520regions.%250AThis%2520design%2520preserves%2520structural%2520continuity%2520while%2520avoiding%2520unnecessary%2520global%250Aattention%252C%2520achieving%2520up%2520to%25206.7x%2520speed-up%2520in%2520latent%2520generation.%2520To%2520support%2520this%250Amechanism%252C%2520we%2520construct%2520a%2520scalable%2520part%2520annotation%2520pipeline%2520that%2520converts%2520raw%250Ameshes%2520into%2520part-labeled%2520sparse%2520voxels.%2520Extensive%2520experiments%2520demonstrate%2520that%250AUltra3D%2520supports%2520high-resolution%25203D%2520generation%2520at%25201024%2520resolution%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520visual%2520fidelity%2520and%2520user%2520preference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&entry.906535625=Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin&entry.1292438233=%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17745v1&entry.124074799=Read"},
{"title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks", "author": "Rahul Ramachandran and Ali Garjani and Roman Bachmann and Andrei Atanov and O\u011fuzhan Fatih Kar and Amir Zamir", "abstract": "  Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.\n", "link": "http://arxiv.org/abs/2507.01955v2", "date": "2025-07-23", "relevancy": 3.0833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.651}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Well%20Does%20GPT-4o%20Understand%20Vision%3F%20Evaluating%20Multimodal%20Foundation%0A%20%20Models%20on%20Standard%20Computer%20Vision%20Tasks&body=Title%3A%20How%20Well%20Does%20GPT-4o%20Understand%20Vision%3F%20Evaluating%20Multimodal%20Foundation%0A%20%20Models%20on%20Standard%20Computer%20Vision%20Tasks%0AAuthor%3A%20Rahul%20Ramachandran%20and%20Ali%20Garjani%20and%20Roman%20Bachmann%20and%20Andrei%20Atanov%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Amir%20Zamir%0AAbstract%3A%20%20%20Multimodal%20foundation%20models%2C%20such%20as%20GPT-4o%2C%20have%20recently%20made%20remarkable%0Aprogress%2C%20but%20it%20is%20not%20clear%20where%20exactly%20these%20models%20stand%20in%20terms%20of%0Aunderstanding%20vision.%20In%20this%20paper%2C%20we%20benchmark%20the%20performance%20of%20popular%0Amultimodal%20foundation%20models%20%28GPT-4o%2C%20o4-mini%2C%20Gemini%201.5%20Pro%20and%20Gemini%202.0%0AFlash%2C%20Claude%203.5%20Sonnet%2C%20Qwen2-VL%2C%20Llama%203.2%29%20on%20standard%20computer%20vision%0Atasks%20%28semantic%20segmentation%2C%20object%20detection%2C%20image%20classification%2C%20depth%20and%0Asurface%20normal%20prediction%29%20using%20established%20datasets%20%28e.g.%2C%20COCO%2C%20ImageNet%20and%0Aits%20variants%2C%20etc%29.%0A%20%20The%20main%20challenges%20to%20performing%20this%20are%3A%201%29%20most%20models%20are%20trained%20to%0Aoutput%20text%20and%20cannot%20natively%20express%20versatile%20domains%2C%20such%20as%20segments%20or%0A3D%20geometry%2C%20and%202%29%20many%20leading%20models%20are%20proprietary%20and%20accessible%20only%20at%0Aan%20API%20level%2C%20i.e.%2C%20there%20is%20no%20weight%20access%20to%20adapt%20them.%20We%20address%20these%0Achallenges%20by%20translating%20standard%20vision%20tasks%20into%20equivalent%20text-promptable%0Aand%20API-compatible%20tasks%20via%20prompt%20chaining%20to%20create%20a%20standardized%0Abenchmarking%20framework.%0A%20%20We%20observe%20that%201%29%20the%20models%20are%20not%20close%20to%20the%20state-of-the-art%0Aspecialist%20models%20at%20any%20task.%20However%2C%202%29%20they%20are%20respectable%20generalists%3B%0Athis%20is%20remarkable%20as%20they%20are%20presumably%20trained%20on%20primarily%20image-text-based%0Atasks.%203%29%20They%20perform%20semantic%20tasks%20notably%20better%20than%20geometric%20ones.%204%29%0AWhile%20the%20prompt-chaining%20techniques%20affect%20performance%2C%20better%20models%20exhibit%0Aless%20sensitivity%20to%20prompt%20variations.%205%29%20GPT-4o%20performs%20the%20best%20among%0Anon-reasoning%20models%2C%20securing%20the%20top%20position%20in%204%20out%20of%206%20tasks%2C%206%29%0Areasoning%20models%2C%20e.g.%20o3%2C%20show%20improvements%20in%20geometric%20tasks%2C%20and%207%29%20a%0Apreliminary%20analysis%20of%20models%20with%20native%20image%20generation%2C%20like%20the%20latest%0AGPT-4o%2C%20shows%20they%20exhibit%20quirks%20like%20hallucinations%20and%20spatial%0Amisalignments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Well%2520Does%2520GPT-4o%2520Understand%2520Vision%253F%2520Evaluating%2520Multimodal%2520Foundation%250A%2520%2520Models%2520on%2520Standard%2520Computer%2520Vision%2520Tasks%26entry.906535625%3DRahul%2520Ramachandran%2520and%2520Ali%2520Garjani%2520and%2520Roman%2520Bachmann%2520and%2520Andrei%2520Atanov%2520and%2520O%25C4%259Fuzhan%2520Fatih%2520Kar%2520and%2520Amir%2520Zamir%26entry.1292438233%3D%2520%2520Multimodal%2520foundation%2520models%252C%2520such%2520as%2520GPT-4o%252C%2520have%2520recently%2520made%2520remarkable%250Aprogress%252C%2520but%2520it%2520is%2520not%2520clear%2520where%2520exactly%2520these%2520models%2520stand%2520in%2520terms%2520of%250Aunderstanding%2520vision.%2520In%2520this%2520paper%252C%2520we%2520benchmark%2520the%2520performance%2520of%2520popular%250Amultimodal%2520foundation%2520models%2520%2528GPT-4o%252C%2520o4-mini%252C%2520Gemini%25201.5%2520Pro%2520and%2520Gemini%25202.0%250AFlash%252C%2520Claude%25203.5%2520Sonnet%252C%2520Qwen2-VL%252C%2520Llama%25203.2%2529%2520on%2520standard%2520computer%2520vision%250Atasks%2520%2528semantic%2520segmentation%252C%2520object%2520detection%252C%2520image%2520classification%252C%2520depth%2520and%250Asurface%2520normal%2520prediction%2529%2520using%2520established%2520datasets%2520%2528e.g.%252C%2520COCO%252C%2520ImageNet%2520and%250Aits%2520variants%252C%2520etc%2529.%250A%2520%2520The%2520main%2520challenges%2520to%2520performing%2520this%2520are%253A%25201%2529%2520most%2520models%2520are%2520trained%2520to%250Aoutput%2520text%2520and%2520cannot%2520natively%2520express%2520versatile%2520domains%252C%2520such%2520as%2520segments%2520or%250A3D%2520geometry%252C%2520and%25202%2529%2520many%2520leading%2520models%2520are%2520proprietary%2520and%2520accessible%2520only%2520at%250Aan%2520API%2520level%252C%2520i.e.%252C%2520there%2520is%2520no%2520weight%2520access%2520to%2520adapt%2520them.%2520We%2520address%2520these%250Achallenges%2520by%2520translating%2520standard%2520vision%2520tasks%2520into%2520equivalent%2520text-promptable%250Aand%2520API-compatible%2520tasks%2520via%2520prompt%2520chaining%2520to%2520create%2520a%2520standardized%250Abenchmarking%2520framework.%250A%2520%2520We%2520observe%2520that%25201%2529%2520the%2520models%2520are%2520not%2520close%2520to%2520the%2520state-of-the-art%250Aspecialist%2520models%2520at%2520any%2520task.%2520However%252C%25202%2529%2520they%2520are%2520respectable%2520generalists%253B%250Athis%2520is%2520remarkable%2520as%2520they%2520are%2520presumably%2520trained%2520on%2520primarily%2520image-text-based%250Atasks.%25203%2529%2520They%2520perform%2520semantic%2520tasks%2520notably%2520better%2520than%2520geometric%2520ones.%25204%2529%250AWhile%2520the%2520prompt-chaining%2520techniques%2520affect%2520performance%252C%2520better%2520models%2520exhibit%250Aless%2520sensitivity%2520to%2520prompt%2520variations.%25205%2529%2520GPT-4o%2520performs%2520the%2520best%2520among%250Anon-reasoning%2520models%252C%2520securing%2520the%2520top%2520position%2520in%25204%2520out%2520of%25206%2520tasks%252C%25206%2529%250Areasoning%2520models%252C%2520e.g.%2520o3%252C%2520show%2520improvements%2520in%2520geometric%2520tasks%252C%2520and%25207%2529%2520a%250Apreliminary%2520analysis%2520of%2520models%2520with%2520native%2520image%2520generation%252C%2520like%2520the%2520latest%250AGPT-4o%252C%2520shows%2520they%2520exhibit%2520quirks%2520like%2520hallucinations%2520and%2520spatial%250Amisalignments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Well%20Does%20GPT-4o%20Understand%20Vision%3F%20Evaluating%20Multimodal%20Foundation%0A%20%20Models%20on%20Standard%20Computer%20Vision%20Tasks&entry.906535625=Rahul%20Ramachandran%20and%20Ali%20Garjani%20and%20Roman%20Bachmann%20and%20Andrei%20Atanov%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Amir%20Zamir&entry.1292438233=%20%20Multimodal%20foundation%20models%2C%20such%20as%20GPT-4o%2C%20have%20recently%20made%20remarkable%0Aprogress%2C%20but%20it%20is%20not%20clear%20where%20exactly%20these%20models%20stand%20in%20terms%20of%0Aunderstanding%20vision.%20In%20this%20paper%2C%20we%20benchmark%20the%20performance%20of%20popular%0Amultimodal%20foundation%20models%20%28GPT-4o%2C%20o4-mini%2C%20Gemini%201.5%20Pro%20and%20Gemini%202.0%0AFlash%2C%20Claude%203.5%20Sonnet%2C%20Qwen2-VL%2C%20Llama%203.2%29%20on%20standard%20computer%20vision%0Atasks%20%28semantic%20segmentation%2C%20object%20detection%2C%20image%20classification%2C%20depth%20and%0Asurface%20normal%20prediction%29%20using%20established%20datasets%20%28e.g.%2C%20COCO%2C%20ImageNet%20and%0Aits%20variants%2C%20etc%29.%0A%20%20The%20main%20challenges%20to%20performing%20this%20are%3A%201%29%20most%20models%20are%20trained%20to%0Aoutput%20text%20and%20cannot%20natively%20express%20versatile%20domains%2C%20such%20as%20segments%20or%0A3D%20geometry%2C%20and%202%29%20many%20leading%20models%20are%20proprietary%20and%20accessible%20only%20at%0Aan%20API%20level%2C%20i.e.%2C%20there%20is%20no%20weight%20access%20to%20adapt%20them.%20We%20address%20these%0Achallenges%20by%20translating%20standard%20vision%20tasks%20into%20equivalent%20text-promptable%0Aand%20API-compatible%20tasks%20via%20prompt%20chaining%20to%20create%20a%20standardized%0Abenchmarking%20framework.%0A%20%20We%20observe%20that%201%29%20the%20models%20are%20not%20close%20to%20the%20state-of-the-art%0Aspecialist%20models%20at%20any%20task.%20However%2C%202%29%20they%20are%20respectable%20generalists%3B%0Athis%20is%20remarkable%20as%20they%20are%20presumably%20trained%20on%20primarily%20image-text-based%0Atasks.%203%29%20They%20perform%20semantic%20tasks%20notably%20better%20than%20geometric%20ones.%204%29%0AWhile%20the%20prompt-chaining%20techniques%20affect%20performance%2C%20better%20models%20exhibit%0Aless%20sensitivity%20to%20prompt%20variations.%205%29%20GPT-4o%20performs%20the%20best%20among%0Anon-reasoning%20models%2C%20securing%20the%20top%20position%20in%204%20out%20of%206%20tasks%2C%206%29%0Areasoning%20models%2C%20e.g.%20o3%2C%20show%20improvements%20in%20geometric%20tasks%2C%20and%207%29%20a%0Apreliminary%20analysis%20of%20models%20with%20native%20image%20generation%2C%20like%20the%20latest%0AGPT-4o%2C%20shows%20they%20exhibit%20quirks%20like%20hallucinations%20and%20spatial%0Amisalignments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01955v2&entry.124074799=Read"},
{"title": "Infinite Video Understanding", "author": "Dell Zhang and Xiangyu Chen and Jixiang Luo and Mengxi Jia and Changzhi Sun and Ruilong Ren and Jingren Liu and Hao Sun and Xuelong Li", "abstract": "  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n", "link": "http://arxiv.org/abs/2507.09068v2", "date": "2025-07-23", "relevancy": 3.0422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6339}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6339}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%20Video%20Understanding&body=Title%3A%20Infinite%20Video%20Understanding%0AAuthor%3A%20Dell%20Zhang%20and%20Xiangyu%20Chen%20and%20Jixiang%20Luo%20and%20Mengxi%20Jia%20and%20Changzhi%20Sun%20and%20Ruilong%20Ren%20and%20Jingren%20Liu%20and%20Hao%20Sun%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20their%20multimodal%0Aextensions%20%28MLLMs%29%20have%20ushered%20in%20remarkable%20progress%20in%20video%20understanding.%0AHowever%2C%20a%20fundamental%20challenge%20persists%3A%20effectively%20processing%20and%0Acomprehending%20video%20content%20that%20extends%20beyond%20minutes%20or%20hours.%20While%20recent%0Aefforts%20like%20Video-XL-2%20have%20demonstrated%20novel%20architectural%20solutions%20for%0Aextreme%20efficiency%2C%20and%20advancements%20in%20positional%20encoding%20such%20as%20HoPE%20and%0AVideoRoPE%2B%2B%20aim%20to%20improve%20spatio-temporal%20understanding%20over%20extensive%0Acontexts%2C%20current%20state-of-the-art%20models%20still%20encounter%20significant%0Acomputational%20and%20memory%20constraints%20when%20faced%20with%20the%20sheer%20volume%20of%20visual%0Atokens%20from%20lengthy%20sequences.%20Furthermore%2C%20maintaining%20temporal%20coherence%2C%0Atracking%20complex%20events%2C%20and%20preserving%20fine-grained%20details%20over%20extended%0Aperiods%20remain%20formidable%20hurdles%2C%20despite%20progress%20in%20agentic%20reasoning%0Asystems%20like%20Deep%20Video%20Discovery.%20This%20position%20paper%20posits%20that%20a%20logical%2C%0Aalbeit%20ambitious%2C%20next%20frontier%20for%20multimedia%20research%20is%20Infinite%20Video%0AUnderstanding%20--%20the%20capability%20for%20models%20to%20continuously%20process%2C%20understand%2C%0Aand%20reason%20about%20video%20data%20of%20arbitrary%2C%20potentially%20never-ending%20duration.%20We%0Aargue%20that%20framing%20Infinite%20Video%20Understanding%20as%20a%20blue-sky%20research%0Aobjective%20provides%20a%20vital%20north%20star%20for%20the%20multimedia%2C%20and%20the%20wider%20AI%2C%0Aresearch%20communities%2C%20driving%20innovation%20in%20areas%20such%20as%20streaming%0Aarchitectures%2C%20persistent%20memory%20mechanisms%2C%20hierarchical%20and%20adaptive%0Arepresentations%2C%20event-centric%20reasoning%2C%20and%20novel%20evaluation%20paradigms.%0ADrawing%20inspiration%20from%20recent%20work%20on%20long/ultra-long%20video%20understanding%20and%0Aseveral%20closely%20related%20fields%2C%20we%20outline%20the%20core%20challenges%20and%20key%20research%0Adirections%20towards%20achieving%20this%20transformative%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%2520Video%2520Understanding%26entry.906535625%3DDell%2520Zhang%2520and%2520Xiangyu%2520Chen%2520and%2520Jixiang%2520Luo%2520and%2520Mengxi%2520Jia%2520and%2520Changzhi%2520Sun%2520and%2520Ruilong%2520Ren%2520and%2520Jingren%2520Liu%2520and%2520Hao%2520Sun%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520their%2520multimodal%250Aextensions%2520%2528MLLMs%2529%2520have%2520ushered%2520in%2520remarkable%2520progress%2520in%2520video%2520understanding.%250AHowever%252C%2520a%2520fundamental%2520challenge%2520persists%253A%2520effectively%2520processing%2520and%250Acomprehending%2520video%2520content%2520that%2520extends%2520beyond%2520minutes%2520or%2520hours.%2520While%2520recent%250Aefforts%2520like%2520Video-XL-2%2520have%2520demonstrated%2520novel%2520architectural%2520solutions%2520for%250Aextreme%2520efficiency%252C%2520and%2520advancements%2520in%2520positional%2520encoding%2520such%2520as%2520HoPE%2520and%250AVideoRoPE%252B%252B%2520aim%2520to%2520improve%2520spatio-temporal%2520understanding%2520over%2520extensive%250Acontexts%252C%2520current%2520state-of-the-art%2520models%2520still%2520encounter%2520significant%250Acomputational%2520and%2520memory%2520constraints%2520when%2520faced%2520with%2520the%2520sheer%2520volume%2520of%2520visual%250Atokens%2520from%2520lengthy%2520sequences.%2520Furthermore%252C%2520maintaining%2520temporal%2520coherence%252C%250Atracking%2520complex%2520events%252C%2520and%2520preserving%2520fine-grained%2520details%2520over%2520extended%250Aperiods%2520remain%2520formidable%2520hurdles%252C%2520despite%2520progress%2520in%2520agentic%2520reasoning%250Asystems%2520like%2520Deep%2520Video%2520Discovery.%2520This%2520position%2520paper%2520posits%2520that%2520a%2520logical%252C%250Aalbeit%2520ambitious%252C%2520next%2520frontier%2520for%2520multimedia%2520research%2520is%2520Infinite%2520Video%250AUnderstanding%2520--%2520the%2520capability%2520for%2520models%2520to%2520continuously%2520process%252C%2520understand%252C%250Aand%2520reason%2520about%2520video%2520data%2520of%2520arbitrary%252C%2520potentially%2520never-ending%2520duration.%2520We%250Aargue%2520that%2520framing%2520Infinite%2520Video%2520Understanding%2520as%2520a%2520blue-sky%2520research%250Aobjective%2520provides%2520a%2520vital%2520north%2520star%2520for%2520the%2520multimedia%252C%2520and%2520the%2520wider%2520AI%252C%250Aresearch%2520communities%252C%2520driving%2520innovation%2520in%2520areas%2520such%2520as%2520streaming%250Aarchitectures%252C%2520persistent%2520memory%2520mechanisms%252C%2520hierarchical%2520and%2520adaptive%250Arepresentations%252C%2520event-centric%2520reasoning%252C%2520and%2520novel%2520evaluation%2520paradigms.%250ADrawing%2520inspiration%2520from%2520recent%2520work%2520on%2520long/ultra-long%2520video%2520understanding%2520and%250Aseveral%2520closely%2520related%2520fields%252C%2520we%2520outline%2520the%2520core%2520challenges%2520and%2520key%2520research%250Adirections%2520towards%2520achieving%2520this%2520transformative%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%20Video%20Understanding&entry.906535625=Dell%20Zhang%20and%20Xiangyu%20Chen%20and%20Jixiang%20Luo%20and%20Mengxi%20Jia%20and%20Changzhi%20Sun%20and%20Ruilong%20Ren%20and%20Jingren%20Liu%20and%20Hao%20Sun%20and%20Xuelong%20Li&entry.1292438233=%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20their%20multimodal%0Aextensions%20%28MLLMs%29%20have%20ushered%20in%20remarkable%20progress%20in%20video%20understanding.%0AHowever%2C%20a%20fundamental%20challenge%20persists%3A%20effectively%20processing%20and%0Acomprehending%20video%20content%20that%20extends%20beyond%20minutes%20or%20hours.%20While%20recent%0Aefforts%20like%20Video-XL-2%20have%20demonstrated%20novel%20architectural%20solutions%20for%0Aextreme%20efficiency%2C%20and%20advancements%20in%20positional%20encoding%20such%20as%20HoPE%20and%0AVideoRoPE%2B%2B%20aim%20to%20improve%20spatio-temporal%20understanding%20over%20extensive%0Acontexts%2C%20current%20state-of-the-art%20models%20still%20encounter%20significant%0Acomputational%20and%20memory%20constraints%20when%20faced%20with%20the%20sheer%20volume%20of%20visual%0Atokens%20from%20lengthy%20sequences.%20Furthermore%2C%20maintaining%20temporal%20coherence%2C%0Atracking%20complex%20events%2C%20and%20preserving%20fine-grained%20details%20over%20extended%0Aperiods%20remain%20formidable%20hurdles%2C%20despite%20progress%20in%20agentic%20reasoning%0Asystems%20like%20Deep%20Video%20Discovery.%20This%20position%20paper%20posits%20that%20a%20logical%2C%0Aalbeit%20ambitious%2C%20next%20frontier%20for%20multimedia%20research%20is%20Infinite%20Video%0AUnderstanding%20--%20the%20capability%20for%20models%20to%20continuously%20process%2C%20understand%2C%0Aand%20reason%20about%20video%20data%20of%20arbitrary%2C%20potentially%20never-ending%20duration.%20We%0Aargue%20that%20framing%20Infinite%20Video%20Understanding%20as%20a%20blue-sky%20research%0Aobjective%20provides%20a%20vital%20north%20star%20for%20the%20multimedia%2C%20and%20the%20wider%20AI%2C%0Aresearch%20communities%2C%20driving%20innovation%20in%20areas%20such%20as%20streaming%0Aarchitectures%2C%20persistent%20memory%20mechanisms%2C%20hierarchical%20and%20adaptive%0Arepresentations%2C%20event-centric%20reasoning%2C%20and%20novel%20evaluation%20paradigms.%0ADrawing%20inspiration%20from%20recent%20work%20on%20long/ultra-long%20video%20understanding%20and%0Aseveral%20closely%20related%20fields%2C%20we%20outline%20the%20core%20challenges%20and%20key%20research%0Adirections%20towards%20achieving%20this%20transformative%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09068v2&entry.124074799=Read"},
{"title": "Multi-modal Multi-task Pre-training for Improved Point Cloud\n  Understanding", "author": "Liwen Liu and Weidong Yang and Lipeng Ma and Ben Fei", "abstract": "  Recent advances in multi-modal pre-training methods have shown promising\neffectiveness in learning 3D representations by aligning multi-modal features\nbetween 3D shapes and their corresponding 2D counterparts. However, existing\nmulti-modal pre-training frameworks primarily rely on a single pre-training\ntask to gather multi-modal data in 3D applications. This limitation prevents\nthe models from obtaining the abundant information provided by other relevant\ntasks, which can hinder their performance in downstream tasks, particularly in\ncomplex and diverse domains. In order to tackle this issue, we propose MMPT, a\nMulti-modal Multi-task Pre-training framework designed to enhance point cloud\nunderstanding. Specifically, three pre-training tasks are devised: (i)\nToken-level reconstruction (TLR) aims to recover masked point tokens, endowing\nthe model with representative learning abilities. (ii) Point-level\nreconstruction (PLR) is integrated to predict the masked point positions\ndirectly, and the reconstructed point cloud can be considered as a transformed\npoint cloud used in the subsequent task. (iii) Multi-modal contrastive learning\n(MCL) combines feature correspondences within and across modalities, thus\nassembling a rich learning signal from both 3D point cloud and 2D image\nmodalities in a self-supervised manner. Moreover, this framework operates\nwithout requiring any 3D annotations, making it scalable for use with large\ndatasets. The trained encoder can be effectively transferred to various\ndownstream tasks. To demonstrate its effectiveness, we evaluated its\nperformance compared to state-of-the-art methods in various discriminant and\ngenerative applications under widely-used benchmarks.\n", "link": "http://arxiv.org/abs/2507.17533v1", "date": "2025-07-23", "relevancy": 3.0049, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6476}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Multi-task%20Pre-training%20for%20Improved%20Point%20Cloud%0A%20%20Understanding&body=Title%3A%20Multi-modal%20Multi-task%20Pre-training%20for%20Improved%20Point%20Cloud%0A%20%20Understanding%0AAuthor%3A%20Liwen%20Liu%20and%20Weidong%20Yang%20and%20Lipeng%20Ma%20and%20Ben%20Fei%0AAbstract%3A%20%20%20Recent%20advances%20in%20multi-modal%20pre-training%20methods%20have%20shown%20promising%0Aeffectiveness%20in%20learning%203D%20representations%20by%20aligning%20multi-modal%20features%0Abetween%203D%20shapes%20and%20their%20corresponding%202D%20counterparts.%20However%2C%20existing%0Amulti-modal%20pre-training%20frameworks%20primarily%20rely%20on%20a%20single%20pre-training%0Atask%20to%20gather%20multi-modal%20data%20in%203D%20applications.%20This%20limitation%20prevents%0Athe%20models%20from%20obtaining%20the%20abundant%20information%20provided%20by%20other%20relevant%0Atasks%2C%20which%20can%20hinder%20their%20performance%20in%20downstream%20tasks%2C%20particularly%20in%0Acomplex%20and%20diverse%20domains.%20In%20order%20to%20tackle%20this%20issue%2C%20we%20propose%20MMPT%2C%20a%0AMulti-modal%20Multi-task%20Pre-training%20framework%20designed%20to%20enhance%20point%20cloud%0Aunderstanding.%20Specifically%2C%20three%20pre-training%20tasks%20are%20devised%3A%20%28i%29%0AToken-level%20reconstruction%20%28TLR%29%20aims%20to%20recover%20masked%20point%20tokens%2C%20endowing%0Athe%20model%20with%20representative%20learning%20abilities.%20%28ii%29%20Point-level%0Areconstruction%20%28PLR%29%20is%20integrated%20to%20predict%20the%20masked%20point%20positions%0Adirectly%2C%20and%20the%20reconstructed%20point%20cloud%20can%20be%20considered%20as%20a%20transformed%0Apoint%20cloud%20used%20in%20the%20subsequent%20task.%20%28iii%29%20Multi-modal%20contrastive%20learning%0A%28MCL%29%20combines%20feature%20correspondences%20within%20and%20across%20modalities%2C%20thus%0Aassembling%20a%20rich%20learning%20signal%20from%20both%203D%20point%20cloud%20and%202D%20image%0Amodalities%20in%20a%20self-supervised%20manner.%20Moreover%2C%20this%20framework%20operates%0Awithout%20requiring%20any%203D%20annotations%2C%20making%20it%20scalable%20for%20use%20with%20large%0Adatasets.%20The%20trained%20encoder%20can%20be%20effectively%20transferred%20to%20various%0Adownstream%20tasks.%20To%20demonstrate%20its%20effectiveness%2C%20we%20evaluated%20its%0Aperformance%20compared%20to%20state-of-the-art%20methods%20in%20various%20discriminant%20and%0Agenerative%20applications%20under%20widely-used%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Multi-task%2520Pre-training%2520for%2520Improved%2520Point%2520Cloud%250A%2520%2520Understanding%26entry.906535625%3DLiwen%2520Liu%2520and%2520Weidong%2520Yang%2520and%2520Lipeng%2520Ma%2520and%2520Ben%2520Fei%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multi-modal%2520pre-training%2520methods%2520have%2520shown%2520promising%250Aeffectiveness%2520in%2520learning%25203D%2520representations%2520by%2520aligning%2520multi-modal%2520features%250Abetween%25203D%2520shapes%2520and%2520their%2520corresponding%25202D%2520counterparts.%2520However%252C%2520existing%250Amulti-modal%2520pre-training%2520frameworks%2520primarily%2520rely%2520on%2520a%2520single%2520pre-training%250Atask%2520to%2520gather%2520multi-modal%2520data%2520in%25203D%2520applications.%2520This%2520limitation%2520prevents%250Athe%2520models%2520from%2520obtaining%2520the%2520abundant%2520information%2520provided%2520by%2520other%2520relevant%250Atasks%252C%2520which%2520can%2520hinder%2520their%2520performance%2520in%2520downstream%2520tasks%252C%2520particularly%2520in%250Acomplex%2520and%2520diverse%2520domains.%2520In%2520order%2520to%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520MMPT%252C%2520a%250AMulti-modal%2520Multi-task%2520Pre-training%2520framework%2520designed%2520to%2520enhance%2520point%2520cloud%250Aunderstanding.%2520Specifically%252C%2520three%2520pre-training%2520tasks%2520are%2520devised%253A%2520%2528i%2529%250AToken-level%2520reconstruction%2520%2528TLR%2529%2520aims%2520to%2520recover%2520masked%2520point%2520tokens%252C%2520endowing%250Athe%2520model%2520with%2520representative%2520learning%2520abilities.%2520%2528ii%2529%2520Point-level%250Areconstruction%2520%2528PLR%2529%2520is%2520integrated%2520to%2520predict%2520the%2520masked%2520point%2520positions%250Adirectly%252C%2520and%2520the%2520reconstructed%2520point%2520cloud%2520can%2520be%2520considered%2520as%2520a%2520transformed%250Apoint%2520cloud%2520used%2520in%2520the%2520subsequent%2520task.%2520%2528iii%2529%2520Multi-modal%2520contrastive%2520learning%250A%2528MCL%2529%2520combines%2520feature%2520correspondences%2520within%2520and%2520across%2520modalities%252C%2520thus%250Aassembling%2520a%2520rich%2520learning%2520signal%2520from%2520both%25203D%2520point%2520cloud%2520and%25202D%2520image%250Amodalities%2520in%2520a%2520self-supervised%2520manner.%2520Moreover%252C%2520this%2520framework%2520operates%250Awithout%2520requiring%2520any%25203D%2520annotations%252C%2520making%2520it%2520scalable%2520for%2520use%2520with%2520large%250Adatasets.%2520The%2520trained%2520encoder%2520can%2520be%2520effectively%2520transferred%2520to%2520various%250Adownstream%2520tasks.%2520To%2520demonstrate%2520its%2520effectiveness%252C%2520we%2520evaluated%2520its%250Aperformance%2520compared%2520to%2520state-of-the-art%2520methods%2520in%2520various%2520discriminant%2520and%250Agenerative%2520applications%2520under%2520widely-used%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Multi-task%20Pre-training%20for%20Improved%20Point%20Cloud%0A%20%20Understanding&entry.906535625=Liwen%20Liu%20and%20Weidong%20Yang%20and%20Lipeng%20Ma%20and%20Ben%20Fei&entry.1292438233=%20%20Recent%20advances%20in%20multi-modal%20pre-training%20methods%20have%20shown%20promising%0Aeffectiveness%20in%20learning%203D%20representations%20by%20aligning%20multi-modal%20features%0Abetween%203D%20shapes%20and%20their%20corresponding%202D%20counterparts.%20However%2C%20existing%0Amulti-modal%20pre-training%20frameworks%20primarily%20rely%20on%20a%20single%20pre-training%0Atask%20to%20gather%20multi-modal%20data%20in%203D%20applications.%20This%20limitation%20prevents%0Athe%20models%20from%20obtaining%20the%20abundant%20information%20provided%20by%20other%20relevant%0Atasks%2C%20which%20can%20hinder%20their%20performance%20in%20downstream%20tasks%2C%20particularly%20in%0Acomplex%20and%20diverse%20domains.%20In%20order%20to%20tackle%20this%20issue%2C%20we%20propose%20MMPT%2C%20a%0AMulti-modal%20Multi-task%20Pre-training%20framework%20designed%20to%20enhance%20point%20cloud%0Aunderstanding.%20Specifically%2C%20three%20pre-training%20tasks%20are%20devised%3A%20%28i%29%0AToken-level%20reconstruction%20%28TLR%29%20aims%20to%20recover%20masked%20point%20tokens%2C%20endowing%0Athe%20model%20with%20representative%20learning%20abilities.%20%28ii%29%20Point-level%0Areconstruction%20%28PLR%29%20is%20integrated%20to%20predict%20the%20masked%20point%20positions%0Adirectly%2C%20and%20the%20reconstructed%20point%20cloud%20can%20be%20considered%20as%20a%20transformed%0Apoint%20cloud%20used%20in%20the%20subsequent%20task.%20%28iii%29%20Multi-modal%20contrastive%20learning%0A%28MCL%29%20combines%20feature%20correspondences%20within%20and%20across%20modalities%2C%20thus%0Aassembling%20a%20rich%20learning%20signal%20from%20both%203D%20point%20cloud%20and%202D%20image%0Amodalities%20in%20a%20self-supervised%20manner.%20Moreover%2C%20this%20framework%20operates%0Awithout%20requiring%20any%203D%20annotations%2C%20making%20it%20scalable%20for%20use%20with%20large%0Adatasets.%20The%20trained%20encoder%20can%20be%20effectively%20transferred%20to%20various%0Adownstream%20tasks.%20To%20demonstrate%20its%20effectiveness%2C%20we%20evaluated%20its%0Aperformance%20compared%20to%20state-of-the-art%20methods%20in%20various%20discriminant%20and%0Agenerative%20applications%20under%20widely-used%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17533v1&entry.124074799=Read"},
{"title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time\n  Open-Vocabulary Object Detection", "author": "Yehao Lu and Minghe Weng and Zekang Xiao and Rui Jiang and Wei Su and Guangcong Zheng and Ping Lu and Xi Li", "abstract": "  The Mixture of Experts (MoE) architecture has excelled in Large\nVision-Language Models (LVLMs), yet its potential in real-time open-vocabulary\nobject detectors, which also leverage large-scale vision-language datasets but\nsmaller models, remains unexplored. This work investigates this domain,\nrevealing intriguing insights. In the shallow layers, experts tend to cooperate\nwith diverse peers to expand the search space. While in the deeper layers,\nfixed collaborative structures emerge, where each expert maintains 2-3 fixed\npartners and distinct expert combinations are specialized in processing\nspecific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding\nDINO 1.5 Edge from a dense model to a dynamic inference framework via an\nefficient MoE-Tuning strategy. Additionally, we design a granularity\ndecomposition mechanism to decompose the Feed-Forward Network (FFN) of base\nmodel into multiple smaller expert networks, expanding the subnet search space.\nTo prevent performance degradation at the start of fine-tuning, we further\npropose a pre-trained weight allocation strategy for the experts, coupled with\na specific router initialization. During inference, only the input-relevant\nexperts are activated to form a compact subnet. Experiments show that,\npretrained with merely 1.56M open-source data, Dynamic-DINO outperforms\nGrounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.\n", "link": "http://arxiv.org/abs/2507.17436v1", "date": "2025-07-23", "relevancy": 2.9879, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-DINO%3A%20Fine-Grained%20Mixture%20of%20Experts%20Tuning%20for%20Real-time%0A%20%20Open-Vocabulary%20Object%20Detection&body=Title%3A%20Dynamic-DINO%3A%20Fine-Grained%20Mixture%20of%20Experts%20Tuning%20for%20Real-time%0A%20%20Open-Vocabulary%20Object%20Detection%0AAuthor%3A%20Yehao%20Lu%20and%20Minghe%20Weng%20and%20Zekang%20Xiao%20and%20Rui%20Jiang%20and%20Wei%20Su%20and%20Guangcong%20Zheng%20and%20Ping%20Lu%20and%20Xi%20Li%0AAbstract%3A%20%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20has%20excelled%20in%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20yet%20its%20potential%20in%20real-time%20open-vocabulary%0Aobject%20detectors%2C%20which%20also%20leverage%20large-scale%20vision-language%20datasets%20but%0Asmaller%20models%2C%20remains%20unexplored.%20This%20work%20investigates%20this%20domain%2C%0Arevealing%20intriguing%20insights.%20In%20the%20shallow%20layers%2C%20experts%20tend%20to%20cooperate%0Awith%20diverse%20peers%20to%20expand%20the%20search%20space.%20While%20in%20the%20deeper%20layers%2C%0Afixed%20collaborative%20structures%20emerge%2C%20where%20each%20expert%20maintains%202-3%20fixed%0Apartners%20and%20distinct%20expert%20combinations%20are%20specialized%20in%20processing%0Aspecific%20patterns.%20Concretely%2C%20we%20propose%20Dynamic-DINO%2C%20which%20extends%20Grounding%0ADINO%201.5%20Edge%20from%20a%20dense%20model%20to%20a%20dynamic%20inference%20framework%20via%20an%0Aefficient%20MoE-Tuning%20strategy.%20Additionally%2C%20we%20design%20a%20granularity%0Adecomposition%20mechanism%20to%20decompose%20the%20Feed-Forward%20Network%20%28FFN%29%20of%20base%0Amodel%20into%20multiple%20smaller%20expert%20networks%2C%20expanding%20the%20subnet%20search%20space.%0ATo%20prevent%20performance%20degradation%20at%20the%20start%20of%20fine-tuning%2C%20we%20further%0Apropose%20a%20pre-trained%20weight%20allocation%20strategy%20for%20the%20experts%2C%20coupled%20with%0Aa%20specific%20router%20initialization.%20During%20inference%2C%20only%20the%20input-relevant%0Aexperts%20are%20activated%20to%20form%20a%20compact%20subnet.%20Experiments%20show%20that%2C%0Apretrained%20with%20merely%201.56M%20open-source%20data%2C%20Dynamic-DINO%20outperforms%0AGrounding%20DINO%201.5%20Edge%2C%20pretrained%20on%20the%20private%20Grounding20M%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-DINO%253A%2520Fine-Grained%2520Mixture%2520of%2520Experts%2520Tuning%2520for%2520Real-time%250A%2520%2520Open-Vocabulary%2520Object%2520Detection%26entry.906535625%3DYehao%2520Lu%2520and%2520Minghe%2520Weng%2520and%2520Zekang%2520Xiao%2520and%2520Rui%2520Jiang%2520and%2520Wei%2520Su%2520and%2520Guangcong%2520Zheng%2520and%2520Ping%2520Lu%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520The%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architecture%2520has%2520excelled%2520in%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%252C%2520yet%2520its%2520potential%2520in%2520real-time%2520open-vocabulary%250Aobject%2520detectors%252C%2520which%2520also%2520leverage%2520large-scale%2520vision-language%2520datasets%2520but%250Asmaller%2520models%252C%2520remains%2520unexplored.%2520This%2520work%2520investigates%2520this%2520domain%252C%250Arevealing%2520intriguing%2520insights.%2520In%2520the%2520shallow%2520layers%252C%2520experts%2520tend%2520to%2520cooperate%250Awith%2520diverse%2520peers%2520to%2520expand%2520the%2520search%2520space.%2520While%2520in%2520the%2520deeper%2520layers%252C%250Afixed%2520collaborative%2520structures%2520emerge%252C%2520where%2520each%2520expert%2520maintains%25202-3%2520fixed%250Apartners%2520and%2520distinct%2520expert%2520combinations%2520are%2520specialized%2520in%2520processing%250Aspecific%2520patterns.%2520Concretely%252C%2520we%2520propose%2520Dynamic-DINO%252C%2520which%2520extends%2520Grounding%250ADINO%25201.5%2520Edge%2520from%2520a%2520dense%2520model%2520to%2520a%2520dynamic%2520inference%2520framework%2520via%2520an%250Aefficient%2520MoE-Tuning%2520strategy.%2520Additionally%252C%2520we%2520design%2520a%2520granularity%250Adecomposition%2520mechanism%2520to%2520decompose%2520the%2520Feed-Forward%2520Network%2520%2528FFN%2529%2520of%2520base%250Amodel%2520into%2520multiple%2520smaller%2520expert%2520networks%252C%2520expanding%2520the%2520subnet%2520search%2520space.%250ATo%2520prevent%2520performance%2520degradation%2520at%2520the%2520start%2520of%2520fine-tuning%252C%2520we%2520further%250Apropose%2520a%2520pre-trained%2520weight%2520allocation%2520strategy%2520for%2520the%2520experts%252C%2520coupled%2520with%250Aa%2520specific%2520router%2520initialization.%2520During%2520inference%252C%2520only%2520the%2520input-relevant%250Aexperts%2520are%2520activated%2520to%2520form%2520a%2520compact%2520subnet.%2520Experiments%2520show%2520that%252C%250Apretrained%2520with%2520merely%25201.56M%2520open-source%2520data%252C%2520Dynamic-DINO%2520outperforms%250AGrounding%2520DINO%25201.5%2520Edge%252C%2520pretrained%2520on%2520the%2520private%2520Grounding20M%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-DINO%3A%20Fine-Grained%20Mixture%20of%20Experts%20Tuning%20for%20Real-time%0A%20%20Open-Vocabulary%20Object%20Detection&entry.906535625=Yehao%20Lu%20and%20Minghe%20Weng%20and%20Zekang%20Xiao%20and%20Rui%20Jiang%20and%20Wei%20Su%20and%20Guangcong%20Zheng%20and%20Ping%20Lu%20and%20Xi%20Li&entry.1292438233=%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20has%20excelled%20in%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20yet%20its%20potential%20in%20real-time%20open-vocabulary%0Aobject%20detectors%2C%20which%20also%20leverage%20large-scale%20vision-language%20datasets%20but%0Asmaller%20models%2C%20remains%20unexplored.%20This%20work%20investigates%20this%20domain%2C%0Arevealing%20intriguing%20insights.%20In%20the%20shallow%20layers%2C%20experts%20tend%20to%20cooperate%0Awith%20diverse%20peers%20to%20expand%20the%20search%20space.%20While%20in%20the%20deeper%20layers%2C%0Afixed%20collaborative%20structures%20emerge%2C%20where%20each%20expert%20maintains%202-3%20fixed%0Apartners%20and%20distinct%20expert%20combinations%20are%20specialized%20in%20processing%0Aspecific%20patterns.%20Concretely%2C%20we%20propose%20Dynamic-DINO%2C%20which%20extends%20Grounding%0ADINO%201.5%20Edge%20from%20a%20dense%20model%20to%20a%20dynamic%20inference%20framework%20via%20an%0Aefficient%20MoE-Tuning%20strategy.%20Additionally%2C%20we%20design%20a%20granularity%0Adecomposition%20mechanism%20to%20decompose%20the%20Feed-Forward%20Network%20%28FFN%29%20of%20base%0Amodel%20into%20multiple%20smaller%20expert%20networks%2C%20expanding%20the%20subnet%20search%20space.%0ATo%20prevent%20performance%20degradation%20at%20the%20start%20of%20fine-tuning%2C%20we%20further%0Apropose%20a%20pre-trained%20weight%20allocation%20strategy%20for%20the%20experts%2C%20coupled%20with%0Aa%20specific%20router%20initialization.%20During%20inference%2C%20only%20the%20input-relevant%0Aexperts%20are%20activated%20to%20form%20a%20compact%20subnet.%20Experiments%20show%20that%2C%0Apretrained%20with%20merely%201.56M%20open-source%20data%2C%20Dynamic-DINO%20outperforms%0AGrounding%20DINO%201.5%20Edge%2C%20pretrained%20on%20the%20private%20Grounding20M%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17436v1&entry.124074799=Read"},
{"title": "Yume: An Interactive World Generation Model", "author": "Xiaofeng Mao and Shaoheng Lin and Zhen Li and Chuanhao Li and Wenshuo Peng and Tong He and Jiangmiao Pang and Mingmin Chi and Yu Qiao and Kaipeng Zhang", "abstract": "  Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.\n", "link": "http://arxiv.org/abs/2507.17744v1", "date": "2025-07-23", "relevancy": 2.9616, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.599}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5959}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yume%3A%20An%20Interactive%20World%20Generation%20Model&body=Title%3A%20Yume%3A%20An%20Interactive%20World%20Generation%20Model%0AAuthor%3A%20Xiaofeng%20Mao%20and%20Shaoheng%20Lin%20and%20Zhen%20Li%20and%20Chuanhao%20Li%20and%20Wenshuo%20Peng%20and%20Tong%20He%20and%20Jiangmiao%20Pang%20and%20Mingmin%20Chi%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Yume%20aims%20to%20use%20images%2C%20text%2C%20or%20videos%20to%20create%20an%20interactive%2C%20realistic%2C%0Aand%20dynamic%20world%2C%20which%20allows%20exploration%20and%20control%20using%20peripheral%0Adevices%20or%20neural%20signals.%20In%20this%20report%2C%20we%20present%20a%20preview%20version%20of%0A%5Cmethod%2C%20which%20creates%20a%20dynamic%20world%20from%20an%20input%20image%20and%20allows%0Aexploration%20of%20the%20world%20using%20keyboard%20actions.%20To%20achieve%20this%20high-fidelity%0Aand%20interactive%20video%20world%20generation%2C%20we%20introduce%20a%20well-designed%20framework%2C%0Awhich%20consists%20of%20four%20main%20components%2C%20including%20camera%20motion%20quantization%2C%0Avideo%20generation%20architecture%2C%20advanced%20sampler%2C%20and%20model%20acceleration.%20First%2C%0Awe%20quantize%20camera%20motions%20for%20stable%20training%20and%20user-friendly%20interaction%0Ausing%20keyboard%20inputs.%20Then%2C%20we%20introduce%20the%20Masked%20Video%20Diffusion%0ATransformer~%28MVDT%29%20with%20a%20memory%20module%20for%20infinite%20video%20generation%20in%20an%0Aautoregressive%20manner.%20After%20that%2C%20training-free%20Anti-Artifact%20Mechanism%20%28AAM%29%0Aand%20Time%20Travel%20Sampling%20based%20on%20Stochastic%20Differential%20Equations%20%28TTS-SDE%29%0Aare%20introduced%20to%20the%20sampler%20for%20better%20visual%20quality%20and%20more%20precise%0Acontrol.%20Moreover%2C%20we%20investigate%20model%20acceleration%20by%20synergistic%0Aoptimization%20of%20adversarial%20distillation%20and%20caching%20mechanisms.%20We%20use%20the%0Ahigh-quality%20world%20exploration%20dataset%20%5Csekai%20to%20train%20%5Cmethod%2C%20and%20it%20achieves%0Aremarkable%20results%20in%20diverse%20scenes%20and%20applications.%20All%20data%2C%20codebase%2C%20and%0Amodel%20weights%20are%20available%20on%20https%3A//github.com/stdstu12/YUME.%20Yume%20will%0Aupdate%20monthly%20to%20achieve%20its%20original%20goal.%20Project%20page%3A%0Ahttps%3A//stdstu12.github.io/YUME-Project/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYume%253A%2520An%2520Interactive%2520World%2520Generation%2520Model%26entry.906535625%3DXiaofeng%2520Mao%2520and%2520Shaoheng%2520Lin%2520and%2520Zhen%2520Li%2520and%2520Chuanhao%2520Li%2520and%2520Wenshuo%2520Peng%2520and%2520Tong%2520He%2520and%2520Jiangmiao%2520Pang%2520and%2520Mingmin%2520Chi%2520and%2520Yu%2520Qiao%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Yume%2520aims%2520to%2520use%2520images%252C%2520text%252C%2520or%2520videos%2520to%2520create%2520an%2520interactive%252C%2520realistic%252C%250Aand%2520dynamic%2520world%252C%2520which%2520allows%2520exploration%2520and%2520control%2520using%2520peripheral%250Adevices%2520or%2520neural%2520signals.%2520In%2520this%2520report%252C%2520we%2520present%2520a%2520preview%2520version%2520of%250A%255Cmethod%252C%2520which%2520creates%2520a%2520dynamic%2520world%2520from%2520an%2520input%2520image%2520and%2520allows%250Aexploration%2520of%2520the%2520world%2520using%2520keyboard%2520actions.%2520To%2520achieve%2520this%2520high-fidelity%250Aand%2520interactive%2520video%2520world%2520generation%252C%2520we%2520introduce%2520a%2520well-designed%2520framework%252C%250Awhich%2520consists%2520of%2520four%2520main%2520components%252C%2520including%2520camera%2520motion%2520quantization%252C%250Avideo%2520generation%2520architecture%252C%2520advanced%2520sampler%252C%2520and%2520model%2520acceleration.%2520First%252C%250Awe%2520quantize%2520camera%2520motions%2520for%2520stable%2520training%2520and%2520user-friendly%2520interaction%250Ausing%2520keyboard%2520inputs.%2520Then%252C%2520we%2520introduce%2520the%2520Masked%2520Video%2520Diffusion%250ATransformer~%2528MVDT%2529%2520with%2520a%2520memory%2520module%2520for%2520infinite%2520video%2520generation%2520in%2520an%250Aautoregressive%2520manner.%2520After%2520that%252C%2520training-free%2520Anti-Artifact%2520Mechanism%2520%2528AAM%2529%250Aand%2520Time%2520Travel%2520Sampling%2520based%2520on%2520Stochastic%2520Differential%2520Equations%2520%2528TTS-SDE%2529%250Aare%2520introduced%2520to%2520the%2520sampler%2520for%2520better%2520visual%2520quality%2520and%2520more%2520precise%250Acontrol.%2520Moreover%252C%2520we%2520investigate%2520model%2520acceleration%2520by%2520synergistic%250Aoptimization%2520of%2520adversarial%2520distillation%2520and%2520caching%2520mechanisms.%2520We%2520use%2520the%250Ahigh-quality%2520world%2520exploration%2520dataset%2520%255Csekai%2520to%2520train%2520%255Cmethod%252C%2520and%2520it%2520achieves%250Aremarkable%2520results%2520in%2520diverse%2520scenes%2520and%2520applications.%2520All%2520data%252C%2520codebase%252C%2520and%250Amodel%2520weights%2520are%2520available%2520on%2520https%253A//github.com/stdstu12/YUME.%2520Yume%2520will%250Aupdate%2520monthly%2520to%2520achieve%2520its%2520original%2520goal.%2520Project%2520page%253A%250Ahttps%253A//stdstu12.github.io/YUME-Project/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yume%3A%20An%20Interactive%20World%20Generation%20Model&entry.906535625=Xiaofeng%20Mao%20and%20Shaoheng%20Lin%20and%20Zhen%20Li%20and%20Chuanhao%20Li%20and%20Wenshuo%20Peng%20and%20Tong%20He%20and%20Jiangmiao%20Pang%20and%20Mingmin%20Chi%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Yume%20aims%20to%20use%20images%2C%20text%2C%20or%20videos%20to%20create%20an%20interactive%2C%20realistic%2C%0Aand%20dynamic%20world%2C%20which%20allows%20exploration%20and%20control%20using%20peripheral%0Adevices%20or%20neural%20signals.%20In%20this%20report%2C%20we%20present%20a%20preview%20version%20of%0A%5Cmethod%2C%20which%20creates%20a%20dynamic%20world%20from%20an%20input%20image%20and%20allows%0Aexploration%20of%20the%20world%20using%20keyboard%20actions.%20To%20achieve%20this%20high-fidelity%0Aand%20interactive%20video%20world%20generation%2C%20we%20introduce%20a%20well-designed%20framework%2C%0Awhich%20consists%20of%20four%20main%20components%2C%20including%20camera%20motion%20quantization%2C%0Avideo%20generation%20architecture%2C%20advanced%20sampler%2C%20and%20model%20acceleration.%20First%2C%0Awe%20quantize%20camera%20motions%20for%20stable%20training%20and%20user-friendly%20interaction%0Ausing%20keyboard%20inputs.%20Then%2C%20we%20introduce%20the%20Masked%20Video%20Diffusion%0ATransformer~%28MVDT%29%20with%20a%20memory%20module%20for%20infinite%20video%20generation%20in%20an%0Aautoregressive%20manner.%20After%20that%2C%20training-free%20Anti-Artifact%20Mechanism%20%28AAM%29%0Aand%20Time%20Travel%20Sampling%20based%20on%20Stochastic%20Differential%20Equations%20%28TTS-SDE%29%0Aare%20introduced%20to%20the%20sampler%20for%20better%20visual%20quality%20and%20more%20precise%0Acontrol.%20Moreover%2C%20we%20investigate%20model%20acceleration%20by%20synergistic%0Aoptimization%20of%20adversarial%20distillation%20and%20caching%20mechanisms.%20We%20use%20the%0Ahigh-quality%20world%20exploration%20dataset%20%5Csekai%20to%20train%20%5Cmethod%2C%20and%20it%20achieves%0Aremarkable%20results%20in%20diverse%20scenes%20and%20applications.%20All%20data%2C%20codebase%2C%20and%0Amodel%20weights%20are%20available%20on%20https%3A//github.com/stdstu12/YUME.%20Yume%20will%0Aupdate%20monthly%20to%20achieve%20its%20original%20goal.%20Project%20page%3A%0Ahttps%3A//stdstu12.github.io/YUME-Project/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17744v1&entry.124074799=Read"},
{"title": "SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in\n  Autonomous Driving", "author": "Chuang Chen and Xiaolin Qin and Jing Hu and Wenyi Ge", "abstract": "  Upsampling LiDAR point clouds in autonomous driving scenarios remains a\nsignificant challenge due to the inherent sparsity and complex 3D structures of\nthe data. Recent studies have attempted to address this problem by converting\nthe complex 3D spatial scenes into 2D image super-resolution tasks. However,\ndue to the sparse and blurry feature representation of range images, accurately\nreconstructing detailed and complex spatial topologies remains a major\ndifficulty. To tackle this, we propose a novel sparse point cloud upsampling\nmethod named SRMambaV2, which enhances the upsampling accuracy in long-range\nsparse regions while preserving the overall geometric reconstruction quality.\nSpecifically, inspired by human driver visual perception, we design a\nbiomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the\nfeature distribution in distant sparse areas. Meanwhile, we introduce a\ndual-branch network architecture to enhance the representation of sparse\nfeatures. In addition, we introduce a progressive adaptive loss (PAL) function\nto further refine the reconstruction of fine-grained details during the\nupsampling process. Experimental results demonstrate that SRMambaV2 achieves\nsuperior performance in both qualitative and quantitative evaluations,\nhighlighting its effectiveness and practical value in automotive sparse point\ncloud upsampling tasks.\n", "link": "http://arxiv.org/abs/2507.17479v1", "date": "2025-07-23", "relevancy": 2.9308, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5844}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRMambaV2%3A%20Biomimetic%20Attention%20for%20Sparse%20Point%20Cloud%20Upsampling%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20SRMambaV2%3A%20Biomimetic%20Attention%20for%20Sparse%20Point%20Cloud%20Upsampling%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Chuang%20Chen%20and%20Xiaolin%20Qin%20and%20Jing%20Hu%20and%20Wenyi%20Ge%0AAbstract%3A%20%20%20Upsampling%20LiDAR%20point%20clouds%20in%20autonomous%20driving%20scenarios%20remains%20a%0Asignificant%20challenge%20due%20to%20the%20inherent%20sparsity%20and%20complex%203D%20structures%20of%0Athe%20data.%20Recent%20studies%20have%20attempted%20to%20address%20this%20problem%20by%20converting%0Athe%20complex%203D%20spatial%20scenes%20into%202D%20image%20super-resolution%20tasks.%20However%2C%0Adue%20to%20the%20sparse%20and%20blurry%20feature%20representation%20of%20range%20images%2C%20accurately%0Areconstructing%20detailed%20and%20complex%20spatial%20topologies%20remains%20a%20major%0Adifficulty.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20sparse%20point%20cloud%20upsampling%0Amethod%20named%20SRMambaV2%2C%20which%20enhances%20the%20upsampling%20accuracy%20in%20long-range%0Asparse%20regions%20while%20preserving%20the%20overall%20geometric%20reconstruction%20quality.%0ASpecifically%2C%20inspired%20by%20human%20driver%20visual%20perception%2C%20we%20design%20a%0Abiomimetic%202D%20selective%20scanning%20self-attention%20%282DSSA%29%20mechanism%20to%20model%20the%0Afeature%20distribution%20in%20distant%20sparse%20areas.%20Meanwhile%2C%20we%20introduce%20a%0Adual-branch%20network%20architecture%20to%20enhance%20the%20representation%20of%20sparse%0Afeatures.%20In%20addition%2C%20we%20introduce%20a%20progressive%20adaptive%20loss%20%28PAL%29%20function%0Ato%20further%20refine%20the%20reconstruction%20of%20fine-grained%20details%20during%20the%0Aupsampling%20process.%20Experimental%20results%20demonstrate%20that%20SRMambaV2%20achieves%0Asuperior%20performance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%0Ahighlighting%20its%20effectiveness%20and%20practical%20value%20in%20automotive%20sparse%20point%0Acloud%20upsampling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRMambaV2%253A%2520Biomimetic%2520Attention%2520for%2520Sparse%2520Point%2520Cloud%2520Upsampling%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DChuang%2520Chen%2520and%2520Xiaolin%2520Qin%2520and%2520Jing%2520Hu%2520and%2520Wenyi%2520Ge%26entry.1292438233%3D%2520%2520Upsampling%2520LiDAR%2520point%2520clouds%2520in%2520autonomous%2520driving%2520scenarios%2520remains%2520a%250Asignificant%2520challenge%2520due%2520to%2520the%2520inherent%2520sparsity%2520and%2520complex%25203D%2520structures%2520of%250Athe%2520data.%2520Recent%2520studies%2520have%2520attempted%2520to%2520address%2520this%2520problem%2520by%2520converting%250Athe%2520complex%25203D%2520spatial%2520scenes%2520into%25202D%2520image%2520super-resolution%2520tasks.%2520However%252C%250Adue%2520to%2520the%2520sparse%2520and%2520blurry%2520feature%2520representation%2520of%2520range%2520images%252C%2520accurately%250Areconstructing%2520detailed%2520and%2520complex%2520spatial%2520topologies%2520remains%2520a%2520major%250Adifficulty.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520novel%2520sparse%2520point%2520cloud%2520upsampling%250Amethod%2520named%2520SRMambaV2%252C%2520which%2520enhances%2520the%2520upsampling%2520accuracy%2520in%2520long-range%250Asparse%2520regions%2520while%2520preserving%2520the%2520overall%2520geometric%2520reconstruction%2520quality.%250ASpecifically%252C%2520inspired%2520by%2520human%2520driver%2520visual%2520perception%252C%2520we%2520design%2520a%250Abiomimetic%25202D%2520selective%2520scanning%2520self-attention%2520%25282DSSA%2529%2520mechanism%2520to%2520model%2520the%250Afeature%2520distribution%2520in%2520distant%2520sparse%2520areas.%2520Meanwhile%252C%2520we%2520introduce%2520a%250Adual-branch%2520network%2520architecture%2520to%2520enhance%2520the%2520representation%2520of%2520sparse%250Afeatures.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520progressive%2520adaptive%2520loss%2520%2528PAL%2529%2520function%250Ato%2520further%2520refine%2520the%2520reconstruction%2520of%2520fine-grained%2520details%2520during%2520the%250Aupsampling%2520process.%2520Experimental%2520results%2520demonstrate%2520that%2520SRMambaV2%2520achieves%250Asuperior%2520performance%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations%252C%250Ahighlighting%2520its%2520effectiveness%2520and%2520practical%2520value%2520in%2520automotive%2520sparse%2520point%250Acloud%2520upsampling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRMambaV2%3A%20Biomimetic%20Attention%20for%20Sparse%20Point%20Cloud%20Upsampling%20in%0A%20%20Autonomous%20Driving&entry.906535625=Chuang%20Chen%20and%20Xiaolin%20Qin%20and%20Jing%20Hu%20and%20Wenyi%20Ge&entry.1292438233=%20%20Upsampling%20LiDAR%20point%20clouds%20in%20autonomous%20driving%20scenarios%20remains%20a%0Asignificant%20challenge%20due%20to%20the%20inherent%20sparsity%20and%20complex%203D%20structures%20of%0Athe%20data.%20Recent%20studies%20have%20attempted%20to%20address%20this%20problem%20by%20converting%0Athe%20complex%203D%20spatial%20scenes%20into%202D%20image%20super-resolution%20tasks.%20However%2C%0Adue%20to%20the%20sparse%20and%20blurry%20feature%20representation%20of%20range%20images%2C%20accurately%0Areconstructing%20detailed%20and%20complex%20spatial%20topologies%20remains%20a%20major%0Adifficulty.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20sparse%20point%20cloud%20upsampling%0Amethod%20named%20SRMambaV2%2C%20which%20enhances%20the%20upsampling%20accuracy%20in%20long-range%0Asparse%20regions%20while%20preserving%20the%20overall%20geometric%20reconstruction%20quality.%0ASpecifically%2C%20inspired%20by%20human%20driver%20visual%20perception%2C%20we%20design%20a%0Abiomimetic%202D%20selective%20scanning%20self-attention%20%282DSSA%29%20mechanism%20to%20model%20the%0Afeature%20distribution%20in%20distant%20sparse%20areas.%20Meanwhile%2C%20we%20introduce%20a%0Adual-branch%20network%20architecture%20to%20enhance%20the%20representation%20of%20sparse%0Afeatures.%20In%20addition%2C%20we%20introduce%20a%20progressive%20adaptive%20loss%20%28PAL%29%20function%0Ato%20further%20refine%20the%20reconstruction%20of%20fine-grained%20details%20during%20the%0Aupsampling%20process.%20Experimental%20results%20demonstrate%20that%20SRMambaV2%20achieves%0Asuperior%20performance%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%0Ahighlighting%20its%20effectiveness%20and%20practical%20value%20in%20automotive%20sparse%20point%0Acloud%20upsampling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17479v1&entry.124074799=Read"},
{"title": "VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization", "author": "Sania Waheed and Na Min An and Michael Milford and Sarvapali D. Ramchurn and Shoaib Ehsan", "abstract": "  Geo-localization from a single image at planet scale (essentially an advanced\nor extreme version of the kidnapped robot problem) is a fundamental and\nchallenging task in applications such as navigation, autonomous driving and\ndisaster response due to the vast diversity of locations, environmental\nconditions, and scene variations. Traditional retrieval-based methods for\ngeo-localization struggle with scalability and perceptual aliasing, while\nclassification-based approaches lack generalization and require extensive\ntraining data. Recent advances in vision-language models (VLMs) offer a\npromising alternative by leveraging contextual understanding and reasoning.\nHowever, while VLMs achieve high accuracy, they are often prone to\nhallucinations and lack interpretability, making them unreliable as standalone\nsolutions. In this work, we propose a novel hybrid geo-localization framework\nthat combines the strengths of VLMs with retrieval-based visual place\nrecognition (VPR) methods. Our approach first leverages a VLM to generate a\nprior, effectively guiding and constraining the retrieval search space. We then\nemploy a retrieval step, followed by a re-ranking mechanism that selects the\nmost geographically plausible matches based on feature similarity and proximity\nto the initially estimated coordinates. We evaluate our approach on multiple\ngeo-localization benchmarks and show that it consistently outperforms prior\nstate-of-the-art methods, particularly at street (up to 4.51%) and city level\n(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in\ncombination with VPR lead to scalable, robust, and accurate geo-localization\nsystems.\n", "link": "http://arxiv.org/abs/2507.17455v1", "date": "2025-07-23", "relevancy": 2.9282, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6247}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-Guided%20Visual%20Place%20Recognition%20for%20Planet-Scale%20Geo-Localization&body=Title%3A%20VLM-Guided%20Visual%20Place%20Recognition%20for%20Planet-Scale%20Geo-Localization%0AAuthor%3A%20Sania%20Waheed%20and%20Na%20Min%20An%20and%20Michael%20Milford%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan%0AAbstract%3A%20%20%20Geo-localization%20from%20a%20single%20image%20at%20planet%20scale%20%28essentially%20an%20advanced%0Aor%20extreme%20version%20of%20the%20kidnapped%20robot%20problem%29%20is%20a%20fundamental%20and%0Achallenging%20task%20in%20applications%20such%20as%20navigation%2C%20autonomous%20driving%20and%0Adisaster%20response%20due%20to%20the%20vast%20diversity%20of%20locations%2C%20environmental%0Aconditions%2C%20and%20scene%20variations.%20Traditional%20retrieval-based%20methods%20for%0Ageo-localization%20struggle%20with%20scalability%20and%20perceptual%20aliasing%2C%20while%0Aclassification-based%20approaches%20lack%20generalization%20and%20require%20extensive%0Atraining%20data.%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20offer%20a%0Apromising%20alternative%20by%20leveraging%20contextual%20understanding%20and%20reasoning.%0AHowever%2C%20while%20VLMs%20achieve%20high%20accuracy%2C%20they%20are%20often%20prone%20to%0Ahallucinations%20and%20lack%20interpretability%2C%20making%20them%20unreliable%20as%20standalone%0Asolutions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hybrid%20geo-localization%20framework%0Athat%20combines%20the%20strengths%20of%20VLMs%20with%20retrieval-based%20visual%20place%0Arecognition%20%28VPR%29%20methods.%20Our%20approach%20first%20leverages%20a%20VLM%20to%20generate%20a%0Aprior%2C%20effectively%20guiding%20and%20constraining%20the%20retrieval%20search%20space.%20We%20then%0Aemploy%20a%20retrieval%20step%2C%20followed%20by%20a%20re-ranking%20mechanism%20that%20selects%20the%0Amost%20geographically%20plausible%20matches%20based%20on%20feature%20similarity%20and%20proximity%0Ato%20the%20initially%20estimated%20coordinates.%20We%20evaluate%20our%20approach%20on%20multiple%0Ageo-localization%20benchmarks%20and%20show%20that%20it%20consistently%20outperforms%20prior%0Astate-of-the-art%20methods%2C%20particularly%20at%20street%20%28up%20to%204.51%25%29%20and%20city%20level%0A%28up%20to%2013.52%25%29.%20Our%20results%20demonstrate%20that%20VLM-generated%20geographic%20priors%20in%0Acombination%20with%20VPR%20lead%20to%20scalable%2C%20robust%2C%20and%20accurate%20geo-localization%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-Guided%2520Visual%2520Place%2520Recognition%2520for%2520Planet-Scale%2520Geo-Localization%26entry.906535625%3DSania%2520Waheed%2520and%2520Na%2520Min%2520An%2520and%2520Michael%2520Milford%2520and%2520Sarvapali%2520D.%2520Ramchurn%2520and%2520Shoaib%2520Ehsan%26entry.1292438233%3D%2520%2520Geo-localization%2520from%2520a%2520single%2520image%2520at%2520planet%2520scale%2520%2528essentially%2520an%2520advanced%250Aor%2520extreme%2520version%2520of%2520the%2520kidnapped%2520robot%2520problem%2529%2520is%2520a%2520fundamental%2520and%250Achallenging%2520task%2520in%2520applications%2520such%2520as%2520navigation%252C%2520autonomous%2520driving%2520and%250Adisaster%2520response%2520due%2520to%2520the%2520vast%2520diversity%2520of%2520locations%252C%2520environmental%250Aconditions%252C%2520and%2520scene%2520variations.%2520Traditional%2520retrieval-based%2520methods%2520for%250Ageo-localization%2520struggle%2520with%2520scalability%2520and%2520perceptual%2520aliasing%252C%2520while%250Aclassification-based%2520approaches%2520lack%2520generalization%2520and%2520require%2520extensive%250Atraining%2520data.%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520offer%2520a%250Apromising%2520alternative%2520by%2520leveraging%2520contextual%2520understanding%2520and%2520reasoning.%250AHowever%252C%2520while%2520VLMs%2520achieve%2520high%2520accuracy%252C%2520they%2520are%2520often%2520prone%2520to%250Ahallucinations%2520and%2520lack%2520interpretability%252C%2520making%2520them%2520unreliable%2520as%2520standalone%250Asolutions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520geo-localization%2520framework%250Athat%2520combines%2520the%2520strengths%2520of%2520VLMs%2520with%2520retrieval-based%2520visual%2520place%250Arecognition%2520%2528VPR%2529%2520methods.%2520Our%2520approach%2520first%2520leverages%2520a%2520VLM%2520to%2520generate%2520a%250Aprior%252C%2520effectively%2520guiding%2520and%2520constraining%2520the%2520retrieval%2520search%2520space.%2520We%2520then%250Aemploy%2520a%2520retrieval%2520step%252C%2520followed%2520by%2520a%2520re-ranking%2520mechanism%2520that%2520selects%2520the%250Amost%2520geographically%2520plausible%2520matches%2520based%2520on%2520feature%2520similarity%2520and%2520proximity%250Ato%2520the%2520initially%2520estimated%2520coordinates.%2520We%2520evaluate%2520our%2520approach%2520on%2520multiple%250Ageo-localization%2520benchmarks%2520and%2520show%2520that%2520it%2520consistently%2520outperforms%2520prior%250Astate-of-the-art%2520methods%252C%2520particularly%2520at%2520street%2520%2528up%2520to%25204.51%2525%2529%2520and%2520city%2520level%250A%2528up%2520to%252013.52%2525%2529.%2520Our%2520results%2520demonstrate%2520that%2520VLM-generated%2520geographic%2520priors%2520in%250Acombination%2520with%2520VPR%2520lead%2520to%2520scalable%252C%2520robust%252C%2520and%2520accurate%2520geo-localization%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-Guided%20Visual%20Place%20Recognition%20for%20Planet-Scale%20Geo-Localization&entry.906535625=Sania%20Waheed%20and%20Na%20Min%20An%20and%20Michael%20Milford%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan&entry.1292438233=%20%20Geo-localization%20from%20a%20single%20image%20at%20planet%20scale%20%28essentially%20an%20advanced%0Aor%20extreme%20version%20of%20the%20kidnapped%20robot%20problem%29%20is%20a%20fundamental%20and%0Achallenging%20task%20in%20applications%20such%20as%20navigation%2C%20autonomous%20driving%20and%0Adisaster%20response%20due%20to%20the%20vast%20diversity%20of%20locations%2C%20environmental%0Aconditions%2C%20and%20scene%20variations.%20Traditional%20retrieval-based%20methods%20for%0Ageo-localization%20struggle%20with%20scalability%20and%20perceptual%20aliasing%2C%20while%0Aclassification-based%20approaches%20lack%20generalization%20and%20require%20extensive%0Atraining%20data.%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20offer%20a%0Apromising%20alternative%20by%20leveraging%20contextual%20understanding%20and%20reasoning.%0AHowever%2C%20while%20VLMs%20achieve%20high%20accuracy%2C%20they%20are%20often%20prone%20to%0Ahallucinations%20and%20lack%20interpretability%2C%20making%20them%20unreliable%20as%20standalone%0Asolutions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hybrid%20geo-localization%20framework%0Athat%20combines%20the%20strengths%20of%20VLMs%20with%20retrieval-based%20visual%20place%0Arecognition%20%28VPR%29%20methods.%20Our%20approach%20first%20leverages%20a%20VLM%20to%20generate%20a%0Aprior%2C%20effectively%20guiding%20and%20constraining%20the%20retrieval%20search%20space.%20We%20then%0Aemploy%20a%20retrieval%20step%2C%20followed%20by%20a%20re-ranking%20mechanism%20that%20selects%20the%0Amost%20geographically%20plausible%20matches%20based%20on%20feature%20similarity%20and%20proximity%0Ato%20the%20initially%20estimated%20coordinates.%20We%20evaluate%20our%20approach%20on%20multiple%0Ageo-localization%20benchmarks%20and%20show%20that%20it%20consistently%20outperforms%20prior%0Astate-of-the-art%20methods%2C%20particularly%20at%20street%20%28up%20to%204.51%25%29%20and%20city%20level%0A%28up%20to%2013.52%25%29.%20Our%20results%20demonstrate%20that%20VLM-generated%20geographic%20priors%20in%0Acombination%20with%20VPR%20lead%20to%20scalable%2C%20robust%2C%20and%20accurate%20geo-localization%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17455v1&entry.124074799=Read"},
{"title": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering", "author": "Junjie Wang and Yunhan Tang and Yijie Wang and Zhihao Yuan and Huan Wang and Yangfan He and Bin Li", "abstract": "  Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.\n", "link": "http://arxiv.org/abs/2507.17659v1", "date": "2025-07-23", "relevancy": 2.894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering&body=Title%3A%20See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering%0AAuthor%3A%20Junjie%20Wang%20and%20Yunhan%20Tang%20and%20Yijie%20Wang%20and%20Zhihao%20Yuan%20and%20Huan%20Wang%20and%20Yangfan%20He%20and%20Bin%20Li%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20pushed%20the%20frontiers%20of%0AKnowledge-Based%20Visual%20Question%20Answering%20%28KBVQA%29%2C%20yet%20their%20reasoning%20is%0Afundamentally%20bottlenecked%20by%20a%20reliance%20on%20uni-dimensional%20evidence.%20This%0A%22seeing%20only%20the%20trees%2C%20but%20not%20the%20forest%22%20approach%20prevents%20robust%2C%0Amulti-faceted%20understanding.%20Inspired%20by%20the%20principle%20of%20seeing%20both%20the%0Aforest%20and%20trees%2C%20we%20propose%20Synergos-VQA%2C%20a%20novel%20synergistic%20reasoning%0Aframework.%20At%20its%20core%2C%20Synergos-VQA%20concurrently%20generates%20and%20fuses%20three%0Acomplementary%20evidence%20streams%20at%20inference%20time%3A%20%281%29%20Holistic%20Evidence%20to%0Aperceive%20the%20entire%20scene%20%28the%20%22forest%22%29%2C%20%282%29%20Structural%20Evidence%20from%20a%0Aprototype-driven%20module%20to%20identify%20key%20objects%20%28the%20%22trees%22%29%2C%20and%20%283%29%20Causal%0AEvidence%20from%20a%20counterfactual%20probe%20to%20ensure%20the%20reasoning%20is%20robustly%0Agrounded.%20By%20synergistically%20fusing%20this%20multi-faceted%20evidence%2C%20our%20framework%0Aachieves%20a%20more%20comprehensive%20and%20reliable%20reasoning%20process.%20Extensive%0Aexperiments%20show%20that%20Synergos-VQA%20decisively%20establishes%20a%20new%0Astate-of-the-art%20on%20three%20challenging%20benchmarks%2C%20including%20OK-VQA%20and%20A-OKVQA.%0AFurthermore%2C%20our%20approach%20demonstrates%20strong%20plug-and-play%20capabilities%2C%0Asignificantly%20boosting%20various%20open-source%20MLLMs%20and%20proving%20that%20superior%0Amethodological%20design%20can%20outperform%20sheer%20model%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520the%2520Forest%2520and%2520the%2520Trees%253A%2520A%2520Synergistic%2520Reasoning%2520Framework%2520for%250A%2520%2520Knowledge-Based%2520Visual%2520Question%2520Answering%26entry.906535625%3DJunjie%2520Wang%2520and%2520Yunhan%2520Tang%2520and%2520Yijie%2520Wang%2520and%2520Zhihao%2520Yuan%2520and%2520Huan%2520Wang%2520and%2520Yangfan%2520He%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520pushed%2520the%2520frontiers%2520of%250AKnowledge-Based%2520Visual%2520Question%2520Answering%2520%2528KBVQA%2529%252C%2520yet%2520their%2520reasoning%2520is%250Afundamentally%2520bottlenecked%2520by%2520a%2520reliance%2520on%2520uni-dimensional%2520evidence.%2520This%250A%2522seeing%2520only%2520the%2520trees%252C%2520but%2520not%2520the%2520forest%2522%2520approach%2520prevents%2520robust%252C%250Amulti-faceted%2520understanding.%2520Inspired%2520by%2520the%2520principle%2520of%2520seeing%2520both%2520the%250Aforest%2520and%2520trees%252C%2520we%2520propose%2520Synergos-VQA%252C%2520a%2520novel%2520synergistic%2520reasoning%250Aframework.%2520At%2520its%2520core%252C%2520Synergos-VQA%2520concurrently%2520generates%2520and%2520fuses%2520three%250Acomplementary%2520evidence%2520streams%2520at%2520inference%2520time%253A%2520%25281%2529%2520Holistic%2520Evidence%2520to%250Aperceive%2520the%2520entire%2520scene%2520%2528the%2520%2522forest%2522%2529%252C%2520%25282%2529%2520Structural%2520Evidence%2520from%2520a%250Aprototype-driven%2520module%2520to%2520identify%2520key%2520objects%2520%2528the%2520%2522trees%2522%2529%252C%2520and%2520%25283%2529%2520Causal%250AEvidence%2520from%2520a%2520counterfactual%2520probe%2520to%2520ensure%2520the%2520reasoning%2520is%2520robustly%250Agrounded.%2520By%2520synergistically%2520fusing%2520this%2520multi-faceted%2520evidence%252C%2520our%2520framework%250Aachieves%2520a%2520more%2520comprehensive%2520and%2520reliable%2520reasoning%2520process.%2520Extensive%250Aexperiments%2520show%2520that%2520Synergos-VQA%2520decisively%2520establishes%2520a%2520new%250Astate-of-the-art%2520on%2520three%2520challenging%2520benchmarks%252C%2520including%2520OK-VQA%2520and%2520A-OKVQA.%250AFurthermore%252C%2520our%2520approach%2520demonstrates%2520strong%2520plug-and-play%2520capabilities%252C%250Asignificantly%2520boosting%2520various%2520open-source%2520MLLMs%2520and%2520proving%2520that%2520superior%250Amethodological%2520design%2520can%2520outperform%2520sheer%2520model%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20the%20Forest%20and%20the%20Trees%3A%20A%20Synergistic%20Reasoning%20Framework%20for%0A%20%20Knowledge-Based%20Visual%20Question%20Answering&entry.906535625=Junjie%20Wang%20and%20Yunhan%20Tang%20and%20Yijie%20Wang%20and%20Zhihao%20Yuan%20and%20Huan%20Wang%20and%20Yangfan%20He%20and%20Bin%20Li&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20pushed%20the%20frontiers%20of%0AKnowledge-Based%20Visual%20Question%20Answering%20%28KBVQA%29%2C%20yet%20their%20reasoning%20is%0Afundamentally%20bottlenecked%20by%20a%20reliance%20on%20uni-dimensional%20evidence.%20This%0A%22seeing%20only%20the%20trees%2C%20but%20not%20the%20forest%22%20approach%20prevents%20robust%2C%0Amulti-faceted%20understanding.%20Inspired%20by%20the%20principle%20of%20seeing%20both%20the%0Aforest%20and%20trees%2C%20we%20propose%20Synergos-VQA%2C%20a%20novel%20synergistic%20reasoning%0Aframework.%20At%20its%20core%2C%20Synergos-VQA%20concurrently%20generates%20and%20fuses%20three%0Acomplementary%20evidence%20streams%20at%20inference%20time%3A%20%281%29%20Holistic%20Evidence%20to%0Aperceive%20the%20entire%20scene%20%28the%20%22forest%22%29%2C%20%282%29%20Structural%20Evidence%20from%20a%0Aprototype-driven%20module%20to%20identify%20key%20objects%20%28the%20%22trees%22%29%2C%20and%20%283%29%20Causal%0AEvidence%20from%20a%20counterfactual%20probe%20to%20ensure%20the%20reasoning%20is%20robustly%0Agrounded.%20By%20synergistically%20fusing%20this%20multi-faceted%20evidence%2C%20our%20framework%0Aachieves%20a%20more%20comprehensive%20and%20reliable%20reasoning%20process.%20Extensive%0Aexperiments%20show%20that%20Synergos-VQA%20decisively%20establishes%20a%20new%0Astate-of-the-art%20on%20three%20challenging%20benchmarks%2C%20including%20OK-VQA%20and%20A-OKVQA.%0AFurthermore%2C%20our%20approach%20demonstrates%20strong%20plug-and-play%20capabilities%2C%0Asignificantly%20boosting%20various%20open-source%20MLLMs%20and%20proving%20that%20superior%0Amethodological%20design%20can%20outperform%20sheer%20model%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17659v1&entry.124074799=Read"},
{"title": "HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in\n  Tuning-Free Multimodal LLMs", "author": "Zhaolin Cai and Fan Li and Ziwei Zheng and Yanjun Qin", "abstract": "  Video Anomaly Detection (VAD) aims to identify and locate deviations from\nnormal patterns in video sequences. Traditional methods often struggle with\nsubstantial computational demands and a reliance on extensive labeled datasets,\nthereby restricting their practical applicability. To address these\nconstraints, we propose HiProbe-VAD, a novel framework that leverages\npre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring\nfine-tuning. In this paper, we discover that the intermediate hidden states of\nMLLMs contain information-rich representations, exhibiting higher sensitivity\nand linear separability for anomalies compared to the output layer. To\ncapitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)\nmechanism that intelligently identifies and extracts the most informative\nhidden states from the optimal intermediate layer during the MLLMs reasoning.\nThen a lightweight anomaly scorer and temporal localization module efficiently\ndetects anomalies using these extracted hidden states and finally generate\nexplanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate\nthat HiProbe-VAD outperforms existing training-free and most traditional\napproaches. Furthermore, our framework exhibits remarkable cross-model\ngeneralization capabilities in different MLLMs without any tuning, unlocking\nthe potential of pre-trained MLLMs for video anomaly detection and paving the\nway for more practical and scalable solutions.\n", "link": "http://arxiv.org/abs/2507.17394v1", "date": "2025-07-23", "relevancy": 2.8881, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiProbe-VAD%3A%20Video%20Anomaly%20Detection%20via%20Hidden%20States%20Probing%20in%0A%20%20Tuning-Free%20Multimodal%20LLMs&body=Title%3A%20HiProbe-VAD%3A%20Video%20Anomaly%20Detection%20via%20Hidden%20States%20Probing%20in%0A%20%20Tuning-Free%20Multimodal%20LLMs%0AAuthor%3A%20Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Yanjun%20Qin%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20and%20locate%20deviations%20from%0Anormal%20patterns%20in%20video%20sequences.%20Traditional%20methods%20often%20struggle%20with%0Asubstantial%20computational%20demands%20and%20a%20reliance%20on%20extensive%20labeled%20datasets%2C%0Athereby%20restricting%20their%20practical%20applicability.%20To%20address%20these%0Aconstraints%2C%20we%20propose%20HiProbe-VAD%2C%20a%20novel%20framework%20that%20leverages%0Apre-trained%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20for%20VAD%20without%20requiring%0Afine-tuning.%20In%20this%20paper%2C%20we%20discover%20that%20the%20intermediate%20hidden%20states%20of%0AMLLMs%20contain%20information-rich%20representations%2C%20exhibiting%20higher%20sensitivity%0Aand%20linear%20separability%20for%20anomalies%20compared%20to%20the%20output%20layer.%20To%0Acapitalize%20on%20this%2C%20we%20propose%20a%20Dynamic%20Layer%20Saliency%20Probing%20%28DLSP%29%0Amechanism%20that%20intelligently%20identifies%20and%20extracts%20the%20most%20informative%0Ahidden%20states%20from%20the%20optimal%20intermediate%20layer%20during%20the%20MLLMs%20reasoning.%0AThen%20a%20lightweight%20anomaly%20scorer%20and%20temporal%20localization%20module%20efficiently%0Adetects%20anomalies%20using%20these%20extracted%20hidden%20states%20and%20finally%20generate%0Aexplanations.%20Experiments%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets%20demonstrate%0Athat%20HiProbe-VAD%20outperforms%20existing%20training-free%20and%20most%20traditional%0Aapproaches.%20Furthermore%2C%20our%20framework%20exhibits%20remarkable%20cross-model%0Ageneralization%20capabilities%20in%20different%20MLLMs%20without%20any%20tuning%2C%20unlocking%0Athe%20potential%20of%20pre-trained%20MLLMs%20for%20video%20anomaly%20detection%20and%20paving%20the%0Away%20for%20more%20practical%20and%20scalable%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiProbe-VAD%253A%2520Video%2520Anomaly%2520Detection%2520via%2520Hidden%2520States%2520Probing%2520in%250A%2520%2520Tuning-Free%2520Multimodal%2520LLMs%26entry.906535625%3DZhaolin%2520Cai%2520and%2520Fan%2520Li%2520and%2520Ziwei%2520Zheng%2520and%2520Yanjun%2520Qin%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520aims%2520to%2520identify%2520and%2520locate%2520deviations%2520from%250Anormal%2520patterns%2520in%2520video%2520sequences.%2520Traditional%2520methods%2520often%2520struggle%2520with%250Asubstantial%2520computational%2520demands%2520and%2520a%2520reliance%2520on%2520extensive%2520labeled%2520datasets%252C%250Athereby%2520restricting%2520their%2520practical%2520applicability.%2520To%2520address%2520these%250Aconstraints%252C%2520we%2520propose%2520HiProbe-VAD%252C%2520a%2520novel%2520framework%2520that%2520leverages%250Apre-trained%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520for%2520VAD%2520without%2520requiring%250Afine-tuning.%2520In%2520this%2520paper%252C%2520we%2520discover%2520that%2520the%2520intermediate%2520hidden%2520states%2520of%250AMLLMs%2520contain%2520information-rich%2520representations%252C%2520exhibiting%2520higher%2520sensitivity%250Aand%2520linear%2520separability%2520for%2520anomalies%2520compared%2520to%2520the%2520output%2520layer.%2520To%250Acapitalize%2520on%2520this%252C%2520we%2520propose%2520a%2520Dynamic%2520Layer%2520Saliency%2520Probing%2520%2528DLSP%2529%250Amechanism%2520that%2520intelligently%2520identifies%2520and%2520extracts%2520the%2520most%2520informative%250Ahidden%2520states%2520from%2520the%2520optimal%2520intermediate%2520layer%2520during%2520the%2520MLLMs%2520reasoning.%250AThen%2520a%2520lightweight%2520anomaly%2520scorer%2520and%2520temporal%2520localization%2520module%2520efficiently%250Adetects%2520anomalies%2520using%2520these%2520extracted%2520hidden%2520states%2520and%2520finally%2520generate%250Aexplanations.%2520Experiments%2520on%2520the%2520UCF-Crime%2520and%2520XD-Violence%2520datasets%2520demonstrate%250Athat%2520HiProbe-VAD%2520outperforms%2520existing%2520training-free%2520and%2520most%2520traditional%250Aapproaches.%2520Furthermore%252C%2520our%2520framework%2520exhibits%2520remarkable%2520cross-model%250Ageneralization%2520capabilities%2520in%2520different%2520MLLMs%2520without%2520any%2520tuning%252C%2520unlocking%250Athe%2520potential%2520of%2520pre-trained%2520MLLMs%2520for%2520video%2520anomaly%2520detection%2520and%2520paving%2520the%250Away%2520for%2520more%2520practical%2520and%2520scalable%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiProbe-VAD%3A%20Video%20Anomaly%20Detection%20via%20Hidden%20States%20Probing%20in%0A%20%20Tuning-Free%20Multimodal%20LLMs&entry.906535625=Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Yanjun%20Qin&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20and%20locate%20deviations%20from%0Anormal%20patterns%20in%20video%20sequences.%20Traditional%20methods%20often%20struggle%20with%0Asubstantial%20computational%20demands%20and%20a%20reliance%20on%20extensive%20labeled%20datasets%2C%0Athereby%20restricting%20their%20practical%20applicability.%20To%20address%20these%0Aconstraints%2C%20we%20propose%20HiProbe-VAD%2C%20a%20novel%20framework%20that%20leverages%0Apre-trained%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20for%20VAD%20without%20requiring%0Afine-tuning.%20In%20this%20paper%2C%20we%20discover%20that%20the%20intermediate%20hidden%20states%20of%0AMLLMs%20contain%20information-rich%20representations%2C%20exhibiting%20higher%20sensitivity%0Aand%20linear%20separability%20for%20anomalies%20compared%20to%20the%20output%20layer.%20To%0Acapitalize%20on%20this%2C%20we%20propose%20a%20Dynamic%20Layer%20Saliency%20Probing%20%28DLSP%29%0Amechanism%20that%20intelligently%20identifies%20and%20extracts%20the%20most%20informative%0Ahidden%20states%20from%20the%20optimal%20intermediate%20layer%20during%20the%20MLLMs%20reasoning.%0AThen%20a%20lightweight%20anomaly%20scorer%20and%20temporal%20localization%20module%20efficiently%0Adetects%20anomalies%20using%20these%20extracted%20hidden%20states%20and%20finally%20generate%0Aexplanations.%20Experiments%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets%20demonstrate%0Athat%20HiProbe-VAD%20outperforms%20existing%20training-free%20and%20most%20traditional%0Aapproaches.%20Furthermore%2C%20our%20framework%20exhibits%20remarkable%20cross-model%0Ageneralization%20capabilities%20in%20different%20MLLMs%20without%20any%20tuning%2C%20unlocking%0Athe%20potential%20of%20pre-trained%20MLLMs%20for%20video%20anomaly%20detection%20and%20paving%20the%0Away%20for%20more%20practical%20and%20scalable%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17394v1&entry.124074799=Read"},
{"title": "Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object\n  Interaction Detection", "author": "Francesco Tonini and Lorenzo Vaquero and Alessandro Conti and Cigdem Beyan and Elisa Ricci", "abstract": "  Human-Object Interaction (HOI) detection aims to identify humans and objects\nwithin images and interpret their interactions. Existing HOI methods rely\nheavily on large datasets with manual annotations to learn interactions from\nvisual cues. These annotations are labor-intensive to create, prone to\ninconsistency, and limit scalability to new domains and rare interactions. We\nargue that recent advances in Vision-Language Models (VLMs) offer untapped\npotential, particularly in enhancing interaction representation. While prior\nwork has injected such potential and even proposed training-free methods, there\nremain key gaps. Consequently, we propose a novel training-free HOI detection\nframework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively\nutilizes textual and visual interaction representations within a multimodal\nregistry, enabling robust and nuanced interaction understanding. This registry\nincorporates a small set of visual cues and uses innovative interaction\nsignatures to improve the semantic alignment of verbs, facilitating effective\ngeneralization to rare interactions. Additionally, we propose a unique\nmulti-head attention mechanism that adaptively weights the contributions of the\nvisual and textual features. Experimental results demonstrate that our DYSCO\nsurpasses training-free state-of-the-art models and is competitive with\ntraining-based approaches, particularly excelling in rare interactions. Code is\navailable at https://github.com/francescotonini/dysco.\n", "link": "http://arxiv.org/abs/2507.17456v1", "date": "2025-07-23", "relevancy": 2.8734, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Scoring%20with%20Enhanced%20Semantics%20for%20Training-Free%20Human-Object%0A%20%20Interaction%20Detection&body=Title%3A%20Dynamic%20Scoring%20with%20Enhanced%20Semantics%20for%20Training-Free%20Human-Object%0A%20%20Interaction%20Detection%0AAuthor%3A%20Francesco%20Tonini%20and%20Lorenzo%20Vaquero%20and%20Alessandro%20Conti%20and%20Cigdem%20Beyan%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20identify%20humans%20and%20objects%0Awithin%20images%20and%20interpret%20their%20interactions.%20Existing%20HOI%20methods%20rely%0Aheavily%20on%20large%20datasets%20with%20manual%20annotations%20to%20learn%20interactions%20from%0Avisual%20cues.%20These%20annotations%20are%20labor-intensive%20to%20create%2C%20prone%20to%0Ainconsistency%2C%20and%20limit%20scalability%20to%20new%20domains%20and%20rare%20interactions.%20We%0Aargue%20that%20recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20offer%20untapped%0Apotential%2C%20particularly%20in%20enhancing%20interaction%20representation.%20While%20prior%0Awork%20has%20injected%20such%20potential%20and%20even%20proposed%20training-free%20methods%2C%20there%0Aremain%20key%20gaps.%20Consequently%2C%20we%20propose%20a%20novel%20training-free%20HOI%20detection%0Aframework%20for%20Dynamic%20Scoring%20with%20enhanced%20semantics%20%28DYSCO%29%20that%20effectively%0Autilizes%20textual%20and%20visual%20interaction%20representations%20within%20a%20multimodal%0Aregistry%2C%20enabling%20robust%20and%20nuanced%20interaction%20understanding.%20This%20registry%0Aincorporates%20a%20small%20set%20of%20visual%20cues%20and%20uses%20innovative%20interaction%0Asignatures%20to%20improve%20the%20semantic%20alignment%20of%20verbs%2C%20facilitating%20effective%0Ageneralization%20to%20rare%20interactions.%20Additionally%2C%20we%20propose%20a%20unique%0Amulti-head%20attention%20mechanism%20that%20adaptively%20weights%20the%20contributions%20of%20the%0Avisual%20and%20textual%20features.%20Experimental%20results%20demonstrate%20that%20our%20DYSCO%0Asurpasses%20training-free%20state-of-the-art%20models%20and%20is%20competitive%20with%0Atraining-based%20approaches%2C%20particularly%20excelling%20in%20rare%20interactions.%20Code%20is%0Aavailable%20at%20https%3A//github.com/francescotonini/dysco.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Scoring%2520with%2520Enhanced%2520Semantics%2520for%2520Training-Free%2520Human-Object%250A%2520%2520Interaction%2520Detection%26entry.906535625%3DFrancesco%2520Tonini%2520and%2520Lorenzo%2520Vaquero%2520and%2520Alessandro%2520Conti%2520and%2520Cigdem%2520Beyan%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520aims%2520to%2520identify%2520humans%2520and%2520objects%250Awithin%2520images%2520and%2520interpret%2520their%2520interactions.%2520Existing%2520HOI%2520methods%2520rely%250Aheavily%2520on%2520large%2520datasets%2520with%2520manual%2520annotations%2520to%2520learn%2520interactions%2520from%250Avisual%2520cues.%2520These%2520annotations%2520are%2520labor-intensive%2520to%2520create%252C%2520prone%2520to%250Ainconsistency%252C%2520and%2520limit%2520scalability%2520to%2520new%2520domains%2520and%2520rare%2520interactions.%2520We%250Aargue%2520that%2520recent%2520advances%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520offer%2520untapped%250Apotential%252C%2520particularly%2520in%2520enhancing%2520interaction%2520representation.%2520While%2520prior%250Awork%2520has%2520injected%2520such%2520potential%2520and%2520even%2520proposed%2520training-free%2520methods%252C%2520there%250Aremain%2520key%2520gaps.%2520Consequently%252C%2520we%2520propose%2520a%2520novel%2520training-free%2520HOI%2520detection%250Aframework%2520for%2520Dynamic%2520Scoring%2520with%2520enhanced%2520semantics%2520%2528DYSCO%2529%2520that%2520effectively%250Autilizes%2520textual%2520and%2520visual%2520interaction%2520representations%2520within%2520a%2520multimodal%250Aregistry%252C%2520enabling%2520robust%2520and%2520nuanced%2520interaction%2520understanding.%2520This%2520registry%250Aincorporates%2520a%2520small%2520set%2520of%2520visual%2520cues%2520and%2520uses%2520innovative%2520interaction%250Asignatures%2520to%2520improve%2520the%2520semantic%2520alignment%2520of%2520verbs%252C%2520facilitating%2520effective%250Ageneralization%2520to%2520rare%2520interactions.%2520Additionally%252C%2520we%2520propose%2520a%2520unique%250Amulti-head%2520attention%2520mechanism%2520that%2520adaptively%2520weights%2520the%2520contributions%2520of%2520the%250Avisual%2520and%2520textual%2520features.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520DYSCO%250Asurpasses%2520training-free%2520state-of-the-art%2520models%2520and%2520is%2520competitive%2520with%250Atraining-based%2520approaches%252C%2520particularly%2520excelling%2520in%2520rare%2520interactions.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/francescotonini/dysco.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Scoring%20with%20Enhanced%20Semantics%20for%20Training-Free%20Human-Object%0A%20%20Interaction%20Detection&entry.906535625=Francesco%20Tonini%20and%20Lorenzo%20Vaquero%20and%20Alessandro%20Conti%20and%20Cigdem%20Beyan%20and%20Elisa%20Ricci&entry.1292438233=%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20identify%20humans%20and%20objects%0Awithin%20images%20and%20interpret%20their%20interactions.%20Existing%20HOI%20methods%20rely%0Aheavily%20on%20large%20datasets%20with%20manual%20annotations%20to%20learn%20interactions%20from%0Avisual%20cues.%20These%20annotations%20are%20labor-intensive%20to%20create%2C%20prone%20to%0Ainconsistency%2C%20and%20limit%20scalability%20to%20new%20domains%20and%20rare%20interactions.%20We%0Aargue%20that%20recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20offer%20untapped%0Apotential%2C%20particularly%20in%20enhancing%20interaction%20representation.%20While%20prior%0Awork%20has%20injected%20such%20potential%20and%20even%20proposed%20training-free%20methods%2C%20there%0Aremain%20key%20gaps.%20Consequently%2C%20we%20propose%20a%20novel%20training-free%20HOI%20detection%0Aframework%20for%20Dynamic%20Scoring%20with%20enhanced%20semantics%20%28DYSCO%29%20that%20effectively%0Autilizes%20textual%20and%20visual%20interaction%20representations%20within%20a%20multimodal%0Aregistry%2C%20enabling%20robust%20and%20nuanced%20interaction%20understanding.%20This%20registry%0Aincorporates%20a%20small%20set%20of%20visual%20cues%20and%20uses%20innovative%20interaction%0Asignatures%20to%20improve%20the%20semantic%20alignment%20of%20verbs%2C%20facilitating%20effective%0Ageneralization%20to%20rare%20interactions.%20Additionally%2C%20we%20propose%20a%20unique%0Amulti-head%20attention%20mechanism%20that%20adaptively%20weights%20the%20contributions%20of%20the%0Avisual%20and%20textual%20features.%20Experimental%20results%20demonstrate%20that%20our%20DYSCO%0Asurpasses%20training-free%20state-of-the-art%20models%20and%20is%20competitive%20with%0Atraining-based%20approaches%2C%20particularly%20excelling%20in%20rare%20interactions.%20Code%20is%0Aavailable%20at%20https%3A//github.com/francescotonini/dysco.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17456v1&entry.124074799=Read"},
{"title": "InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and\n  LiDAR Reflectance Modeling", "author": "Xiaoxue Chen and Bhargav Chandaka and Chih-Hao Lin and Ya-Qin Zhang and David Forsyth and Hao Zhao and Shenlong Wang", "abstract": "  We present InvRGB+L, a novel inverse rendering model that reconstructs large,\nrelightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional\ninverse graphics methods rely primarily on RGB observations and use LiDAR\nmainly for geometric information, often resulting in suboptimal material\nestimates due to visible light interference. We find that LiDAR's intensity\nvalues-captured with active illumination in a different spectral range-offer\ncomplementary cues for robust material estimation under variable lighting.\nInspired by this, InvRGB+L leverages LiDAR intensity cues to overcome\nchallenges inherent in RGB-centric inverse graphics through two key\ninnovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR\nmaterial consistency losses. The model produces novel-view RGB and LiDAR\nrenderings of urban and indoor scenes and supports relighting, night\nsimulations, and dynamic object insertions, achieving results that surpass\ncurrent state-of-the-art methods in both scene-level urban inverse rendering\nand LiDAR simulation.\n", "link": "http://arxiv.org/abs/2507.17613v1", "date": "2025-07-23", "relevancy": 2.8236, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InvRGB%2BL%3A%20Inverse%20Rendering%20of%20Complex%20Scenes%20with%20Unified%20Color%20and%0A%20%20LiDAR%20Reflectance%20Modeling&body=Title%3A%20InvRGB%2BL%3A%20Inverse%20Rendering%20of%20Complex%20Scenes%20with%20Unified%20Color%20and%0A%20%20LiDAR%20Reflectance%20Modeling%0AAuthor%3A%20Xiaoxue%20Chen%20and%20Bhargav%20Chandaka%20and%20Chih-Hao%20Lin%20and%20Ya-Qin%20Zhang%20and%20David%20Forsyth%20and%20Hao%20Zhao%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20We%20present%20InvRGB%2BL%2C%20a%20novel%20inverse%20rendering%20model%20that%20reconstructs%20large%2C%0Arelightable%2C%20and%20dynamic%20scenes%20from%20a%20single%20RGB%2BLiDAR%20sequence.%20Conventional%0Ainverse%20graphics%20methods%20rely%20primarily%20on%20RGB%20observations%20and%20use%20LiDAR%0Amainly%20for%20geometric%20information%2C%20often%20resulting%20in%20suboptimal%20material%0Aestimates%20due%20to%20visible%20light%20interference.%20We%20find%20that%20LiDAR%27s%20intensity%0Avalues-captured%20with%20active%20illumination%20in%20a%20different%20spectral%20range-offer%0Acomplementary%20cues%20for%20robust%20material%20estimation%20under%20variable%20lighting.%0AInspired%20by%20this%2C%20InvRGB%2BL%20leverages%20LiDAR%20intensity%20cues%20to%20overcome%0Achallenges%20inherent%20in%20RGB-centric%20inverse%20graphics%20through%20two%20key%0Ainnovations%3A%20%281%29%20a%20novel%20physics-based%20LiDAR%20shading%20model%20and%20%282%29%20RGB-LiDAR%0Amaterial%20consistency%20losses.%20The%20model%20produces%20novel-view%20RGB%20and%20LiDAR%0Arenderings%20of%20urban%20and%20indoor%20scenes%20and%20supports%20relighting%2C%20night%0Asimulations%2C%20and%20dynamic%20object%20insertions%2C%20achieving%20results%20that%20surpass%0Acurrent%20state-of-the-art%20methods%20in%20both%20scene-level%20urban%20inverse%20rendering%0Aand%20LiDAR%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvRGB%252BL%253A%2520Inverse%2520Rendering%2520of%2520Complex%2520Scenes%2520with%2520Unified%2520Color%2520and%250A%2520%2520LiDAR%2520Reflectance%2520Modeling%26entry.906535625%3DXiaoxue%2520Chen%2520and%2520Bhargav%2520Chandaka%2520and%2520Chih-Hao%2520Lin%2520and%2520Ya-Qin%2520Zhang%2520and%2520David%2520Forsyth%2520and%2520Hao%2520Zhao%2520and%2520Shenlong%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520InvRGB%252BL%252C%2520a%2520novel%2520inverse%2520rendering%2520model%2520that%2520reconstructs%2520large%252C%250Arelightable%252C%2520and%2520dynamic%2520scenes%2520from%2520a%2520single%2520RGB%252BLiDAR%2520sequence.%2520Conventional%250Ainverse%2520graphics%2520methods%2520rely%2520primarily%2520on%2520RGB%2520observations%2520and%2520use%2520LiDAR%250Amainly%2520for%2520geometric%2520information%252C%2520often%2520resulting%2520in%2520suboptimal%2520material%250Aestimates%2520due%2520to%2520visible%2520light%2520interference.%2520We%2520find%2520that%2520LiDAR%2527s%2520intensity%250Avalues-captured%2520with%2520active%2520illumination%2520in%2520a%2520different%2520spectral%2520range-offer%250Acomplementary%2520cues%2520for%2520robust%2520material%2520estimation%2520under%2520variable%2520lighting.%250AInspired%2520by%2520this%252C%2520InvRGB%252BL%2520leverages%2520LiDAR%2520intensity%2520cues%2520to%2520overcome%250Achallenges%2520inherent%2520in%2520RGB-centric%2520inverse%2520graphics%2520through%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520a%2520novel%2520physics-based%2520LiDAR%2520shading%2520model%2520and%2520%25282%2529%2520RGB-LiDAR%250Amaterial%2520consistency%2520losses.%2520The%2520model%2520produces%2520novel-view%2520RGB%2520and%2520LiDAR%250Arenderings%2520of%2520urban%2520and%2520indoor%2520scenes%2520and%2520supports%2520relighting%252C%2520night%250Asimulations%252C%2520and%2520dynamic%2520object%2520insertions%252C%2520achieving%2520results%2520that%2520surpass%250Acurrent%2520state-of-the-art%2520methods%2520in%2520both%2520scene-level%2520urban%2520inverse%2520rendering%250Aand%2520LiDAR%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InvRGB%2BL%3A%20Inverse%20Rendering%20of%20Complex%20Scenes%20with%20Unified%20Color%20and%0A%20%20LiDAR%20Reflectance%20Modeling&entry.906535625=Xiaoxue%20Chen%20and%20Bhargav%20Chandaka%20and%20Chih-Hao%20Lin%20and%20Ya-Qin%20Zhang%20and%20David%20Forsyth%20and%20Hao%20Zhao%20and%20Shenlong%20Wang&entry.1292438233=%20%20We%20present%20InvRGB%2BL%2C%20a%20novel%20inverse%20rendering%20model%20that%20reconstructs%20large%2C%0Arelightable%2C%20and%20dynamic%20scenes%20from%20a%20single%20RGB%2BLiDAR%20sequence.%20Conventional%0Ainverse%20graphics%20methods%20rely%20primarily%20on%20RGB%20observations%20and%20use%20LiDAR%0Amainly%20for%20geometric%20information%2C%20often%20resulting%20in%20suboptimal%20material%0Aestimates%20due%20to%20visible%20light%20interference.%20We%20find%20that%20LiDAR%27s%20intensity%0Avalues-captured%20with%20active%20illumination%20in%20a%20different%20spectral%20range-offer%0Acomplementary%20cues%20for%20robust%20material%20estimation%20under%20variable%20lighting.%0AInspired%20by%20this%2C%20InvRGB%2BL%20leverages%20LiDAR%20intensity%20cues%20to%20overcome%0Achallenges%20inherent%20in%20RGB-centric%20inverse%20graphics%20through%20two%20key%0Ainnovations%3A%20%281%29%20a%20novel%20physics-based%20LiDAR%20shading%20model%20and%20%282%29%20RGB-LiDAR%0Amaterial%20consistency%20losses.%20The%20model%20produces%20novel-view%20RGB%20and%20LiDAR%0Arenderings%20of%20urban%20and%20indoor%20scenes%20and%20supports%20relighting%2C%20night%0Asimulations%2C%20and%20dynamic%20object%20insertions%2C%20achieving%20results%20that%20surpass%0Acurrent%20state-of-the-art%20methods%20in%20both%20scene-level%20urban%20inverse%20rendering%0Aand%20LiDAR%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17613v1&entry.124074799=Read"},
{"title": "Monocular Semantic Scene Completion via Masked Recurrent Networks", "author": "Xuzhi Wang and Xinran Wu and Song Wang and Lingdong Kong and Ziping Zhao", "abstract": "  Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise\noccupancy and semantic category from a single-view RGB image. Existing methods\nadopt a single-stage framework that aims to simultaneously achieve visible\nregion segmentation and occluded region hallucination, while also being\naffected by inaccurate depth estimation. Such methods often achieve suboptimal\nperformance, especially in complex scenes. We propose a novel two-stage\nframework that decomposes MSSC into coarse MSSC followed by the Masked\nRecurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent\nUnit (MS-GRU) which concentrates on the occupied regions by the proposed mask\nupdating mechanism, and a sparse GRU design is proposed to reduce the\ncomputation cost. Additionally, we propose the distance attention projection to\nreduce projection errors by assigning different attention scores according to\nthe distance to the observed surface. Experimental results demonstrate that our\nproposed unified framework, MonoMRN, effectively supports both indoor and\noutdoor scenes and achieves state-of-the-art performance on the NYUv2 and\nSemanticKITTI datasets. Furthermore, we conduct robustness analysis under\nvarious disturbances, highlighting the role of the Masked Recurrent Network in\nenhancing the model's resilience to such challenges. The source code is\npublicly available.\n", "link": "http://arxiv.org/abs/2507.17661v1", "date": "2025-07-23", "relevancy": 2.7824, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Semantic%20Scene%20Completion%20via%20Masked%20Recurrent%20Networks&body=Title%3A%20Monocular%20Semantic%20Scene%20Completion%20via%20Masked%20Recurrent%20Networks%0AAuthor%3A%20Xuzhi%20Wang%20and%20Xinran%20Wu%20and%20Song%20Wang%20and%20Lingdong%20Kong%20and%20Ziping%20Zhao%0AAbstract%3A%20%20%20Monocular%20Semantic%20Scene%20Completion%20%28MSSC%29%20aims%20to%20predict%20the%20voxel-wise%0Aoccupancy%20and%20semantic%20category%20from%20a%20single-view%20RGB%20image.%20Existing%20methods%0Aadopt%20a%20single-stage%20framework%20that%20aims%20to%20simultaneously%20achieve%20visible%0Aregion%20segmentation%20and%20occluded%20region%20hallucination%2C%20while%20also%20being%0Aaffected%20by%20inaccurate%20depth%20estimation.%20Such%20methods%20often%20achieve%20suboptimal%0Aperformance%2C%20especially%20in%20complex%20scenes.%20We%20propose%20a%20novel%20two-stage%0Aframework%20that%20decomposes%20MSSC%20into%20coarse%20MSSC%20followed%20by%20the%20Masked%0ARecurrent%20Network.%20Specifically%2C%20we%20propose%20the%20Masked%20Sparse%20Gated%20Recurrent%0AUnit%20%28MS-GRU%29%20which%20concentrates%20on%20the%20occupied%20regions%20by%20the%20proposed%20mask%0Aupdating%20mechanism%2C%20and%20a%20sparse%20GRU%20design%20is%20proposed%20to%20reduce%20the%0Acomputation%20cost.%20Additionally%2C%20we%20propose%20the%20distance%20attention%20projection%20to%0Areduce%20projection%20errors%20by%20assigning%20different%20attention%20scores%20according%20to%0Athe%20distance%20to%20the%20observed%20surface.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20unified%20framework%2C%20MonoMRN%2C%20effectively%20supports%20both%20indoor%20and%0Aoutdoor%20scenes%20and%20achieves%20state-of-the-art%20performance%20on%20the%20NYUv2%20and%0ASemanticKITTI%20datasets.%20Furthermore%2C%20we%20conduct%20robustness%20analysis%20under%0Avarious%20disturbances%2C%20highlighting%20the%20role%20of%20the%20Masked%20Recurrent%20Network%20in%0Aenhancing%20the%20model%27s%20resilience%20to%20such%20challenges.%20The%20source%20code%20is%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Semantic%2520Scene%2520Completion%2520via%2520Masked%2520Recurrent%2520Networks%26entry.906535625%3DXuzhi%2520Wang%2520and%2520Xinran%2520Wu%2520and%2520Song%2520Wang%2520and%2520Lingdong%2520Kong%2520and%2520Ziping%2520Zhao%26entry.1292438233%3D%2520%2520Monocular%2520Semantic%2520Scene%2520Completion%2520%2528MSSC%2529%2520aims%2520to%2520predict%2520the%2520voxel-wise%250Aoccupancy%2520and%2520semantic%2520category%2520from%2520a%2520single-view%2520RGB%2520image.%2520Existing%2520methods%250Aadopt%2520a%2520single-stage%2520framework%2520that%2520aims%2520to%2520simultaneously%2520achieve%2520visible%250Aregion%2520segmentation%2520and%2520occluded%2520region%2520hallucination%252C%2520while%2520also%2520being%250Aaffected%2520by%2520inaccurate%2520depth%2520estimation.%2520Such%2520methods%2520often%2520achieve%2520suboptimal%250Aperformance%252C%2520especially%2520in%2520complex%2520scenes.%2520We%2520propose%2520a%2520novel%2520two-stage%250Aframework%2520that%2520decomposes%2520MSSC%2520into%2520coarse%2520MSSC%2520followed%2520by%2520the%2520Masked%250ARecurrent%2520Network.%2520Specifically%252C%2520we%2520propose%2520the%2520Masked%2520Sparse%2520Gated%2520Recurrent%250AUnit%2520%2528MS-GRU%2529%2520which%2520concentrates%2520on%2520the%2520occupied%2520regions%2520by%2520the%2520proposed%2520mask%250Aupdating%2520mechanism%252C%2520and%2520a%2520sparse%2520GRU%2520design%2520is%2520proposed%2520to%2520reduce%2520the%250Acomputation%2520cost.%2520Additionally%252C%2520we%2520propose%2520the%2520distance%2520attention%2520projection%2520to%250Areduce%2520projection%2520errors%2520by%2520assigning%2520different%2520attention%2520scores%2520according%2520to%250Athe%2520distance%2520to%2520the%2520observed%2520surface.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aproposed%2520unified%2520framework%252C%2520MonoMRN%252C%2520effectively%2520supports%2520both%2520indoor%2520and%250Aoutdoor%2520scenes%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520NYUv2%2520and%250ASemanticKITTI%2520datasets.%2520Furthermore%252C%2520we%2520conduct%2520robustness%2520analysis%2520under%250Avarious%2520disturbances%252C%2520highlighting%2520the%2520role%2520of%2520the%2520Masked%2520Recurrent%2520Network%2520in%250Aenhancing%2520the%2520model%2527s%2520resilience%2520to%2520such%2520challenges.%2520The%2520source%2520code%2520is%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Semantic%20Scene%20Completion%20via%20Masked%20Recurrent%20Networks&entry.906535625=Xuzhi%20Wang%20and%20Xinran%20Wu%20and%20Song%20Wang%20and%20Lingdong%20Kong%20and%20Ziping%20Zhao&entry.1292438233=%20%20Monocular%20Semantic%20Scene%20Completion%20%28MSSC%29%20aims%20to%20predict%20the%20voxel-wise%0Aoccupancy%20and%20semantic%20category%20from%20a%20single-view%20RGB%20image.%20Existing%20methods%0Aadopt%20a%20single-stage%20framework%20that%20aims%20to%20simultaneously%20achieve%20visible%0Aregion%20segmentation%20and%20occluded%20region%20hallucination%2C%20while%20also%20being%0Aaffected%20by%20inaccurate%20depth%20estimation.%20Such%20methods%20often%20achieve%20suboptimal%0Aperformance%2C%20especially%20in%20complex%20scenes.%20We%20propose%20a%20novel%20two-stage%0Aframework%20that%20decomposes%20MSSC%20into%20coarse%20MSSC%20followed%20by%20the%20Masked%0ARecurrent%20Network.%20Specifically%2C%20we%20propose%20the%20Masked%20Sparse%20Gated%20Recurrent%0AUnit%20%28MS-GRU%29%20which%20concentrates%20on%20the%20occupied%20regions%20by%20the%20proposed%20mask%0Aupdating%20mechanism%2C%20and%20a%20sparse%20GRU%20design%20is%20proposed%20to%20reduce%20the%0Acomputation%20cost.%20Additionally%2C%20we%20propose%20the%20distance%20attention%20projection%20to%0Areduce%20projection%20errors%20by%20assigning%20different%20attention%20scores%20according%20to%0Athe%20distance%20to%20the%20observed%20surface.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20unified%20framework%2C%20MonoMRN%2C%20effectively%20supports%20both%20indoor%20and%0Aoutdoor%20scenes%20and%20achieves%20state-of-the-art%20performance%20on%20the%20NYUv2%20and%0ASemanticKITTI%20datasets.%20Furthermore%2C%20we%20conduct%20robustness%20analysis%20under%0Avarious%20disturbances%2C%20highlighting%20the%20role%20of%20the%20Masked%20Recurrent%20Network%20in%0Aenhancing%20the%20model%27s%20resilience%20to%20such%20challenges.%20The%20source%20code%20is%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17661v1&entry.124074799=Read"},
{"title": "MolX: Enhancing Large Language Models for Molecular Understanding With A\n  Multi-Modal Extension", "author": "Khiem Le and Zhichun Guo and Kaiwen Dong and Xiaobao Huang and Bozhao Nan and Roshni Iyer and Xiangliang Zhang and Olaf Wiest and Wei Wang and Ting Hua and Nitesh V. Chawla", "abstract": "  Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving molecule-related tasks. This\nchallenge is attributed to their inherent limitations in comprehending\nmolecules using only common textual representations, i.e. SMILES strings. In\nthis study, we seek to enhance the ability of LLMs to comprehend molecules by\nequipping them with a multi-modal external module, termed MolX. Instead of\ndirectly using SMILES strings to represent a molecule, we utilize specific\nencoders to extract fine-grained features from both SMILES string and 2D\nmolecular graph representations for feeding into an LLM. A hand-crafted\nmolecular fingerprint is incorporated to leverage its embedded domain\nknowledge. To establish an alignment between MolX and the LLM's textual input\nspace, the model in which the LLM is frozen, is pre-trained with a strategy\nincluding a diverse set of tasks. Experimental evaluations show that our\nproposed method outperforms baselines across 4 downstream molecule-related\ntasks ranging from molecule-to-text translation to retrosynthesis, with and\nwithout fine-tuning the LLM, while only introducing a small number of trainable\nparameters--0.53\\% and 0.82\\%, respectively.\n", "link": "http://arxiv.org/abs/2406.06777v8", "date": "2025-07-23", "relevancy": 2.7705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolX%3A%20Enhancing%20Large%20Language%20Models%20for%20Molecular%20Understanding%20With%20A%0A%20%20Multi-Modal%20Extension&body=Title%3A%20MolX%3A%20Enhancing%20Large%20Language%20Models%20for%20Molecular%20Understanding%20With%20A%0A%20%20Multi-Modal%20Extension%0AAuthor%3A%20Khiem%20Le%20and%20Zhichun%20Guo%20and%20Kaiwen%20Dong%20and%20Xiaobao%20Huang%20and%20Bozhao%20Nan%20and%20Roshni%20Iyer%20and%20Xiangliang%20Zhang%20and%20Olaf%20Wiest%20and%20Wei%20Wang%20and%20Ting%20Hua%20and%20Nitesh%20V.%20Chawla%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20their%20strong%20task-handling%20capabilities%0Ahave%20shown%20remarkable%20advancements%20across%20a%20spectrum%20of%20fields%2C%20moving%20beyond%0Anatural%20language%20understanding.%20However%2C%20their%20proficiency%20within%20the%20chemistry%0Adomain%20remains%20restricted%2C%20especially%20in%20solving%20molecule-related%20tasks.%20This%0Achallenge%20is%20attributed%20to%20their%20inherent%20limitations%20in%20comprehending%0Amolecules%20using%20only%20common%20textual%20representations%2C%20i.e.%20SMILES%20strings.%20In%0Athis%20study%2C%20we%20seek%20to%20enhance%20the%20ability%20of%20LLMs%20to%20comprehend%20molecules%20by%0Aequipping%20them%20with%20a%20multi-modal%20external%20module%2C%20termed%20MolX.%20Instead%20of%0Adirectly%20using%20SMILES%20strings%20to%20represent%20a%20molecule%2C%20we%20utilize%20specific%0Aencoders%20to%20extract%20fine-grained%20features%20from%20both%20SMILES%20string%20and%202D%0Amolecular%20graph%20representations%20for%20feeding%20into%20an%20LLM.%20A%20hand-crafted%0Amolecular%20fingerprint%20is%20incorporated%20to%20leverage%20its%20embedded%20domain%0Aknowledge.%20To%20establish%20an%20alignment%20between%20MolX%20and%20the%20LLM%27s%20textual%20input%0Aspace%2C%20the%20model%20in%20which%20the%20LLM%20is%20frozen%2C%20is%20pre-trained%20with%20a%20strategy%0Aincluding%20a%20diverse%20set%20of%20tasks.%20Experimental%20evaluations%20show%20that%20our%0Aproposed%20method%20outperforms%20baselines%20across%204%20downstream%20molecule-related%0Atasks%20ranging%20from%20molecule-to-text%20translation%20to%20retrosynthesis%2C%20with%20and%0Awithout%20fine-tuning%20the%20LLM%2C%20while%20only%20introducing%20a%20small%20number%20of%20trainable%0Aparameters--0.53%5C%25%20and%200.82%5C%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06777v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolX%253A%2520Enhancing%2520Large%2520Language%2520Models%2520for%2520Molecular%2520Understanding%2520With%2520A%250A%2520%2520Multi-Modal%2520Extension%26entry.906535625%3DKhiem%2520Le%2520and%2520Zhichun%2520Guo%2520and%2520Kaiwen%2520Dong%2520and%2520Xiaobao%2520Huang%2520and%2520Bozhao%2520Nan%2520and%2520Roshni%2520Iyer%2520and%2520Xiangliang%2520Zhang%2520and%2520Olaf%2520Wiest%2520and%2520Wei%2520Wang%2520and%2520Ting%2520Hua%2520and%2520Nitesh%2520V.%2520Chawla%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520their%2520strong%2520task-handling%2520capabilities%250Ahave%2520shown%2520remarkable%2520advancements%2520across%2520a%2520spectrum%2520of%2520fields%252C%2520moving%2520beyond%250Anatural%2520language%2520understanding.%2520However%252C%2520their%2520proficiency%2520within%2520the%2520chemistry%250Adomain%2520remains%2520restricted%252C%2520especially%2520in%2520solving%2520molecule-related%2520tasks.%2520This%250Achallenge%2520is%2520attributed%2520to%2520their%2520inherent%2520limitations%2520in%2520comprehending%250Amolecules%2520using%2520only%2520common%2520textual%2520representations%252C%2520i.e.%2520SMILES%2520strings.%2520In%250Athis%2520study%252C%2520we%2520seek%2520to%2520enhance%2520the%2520ability%2520of%2520LLMs%2520to%2520comprehend%2520molecules%2520by%250Aequipping%2520them%2520with%2520a%2520multi-modal%2520external%2520module%252C%2520termed%2520MolX.%2520Instead%2520of%250Adirectly%2520using%2520SMILES%2520strings%2520to%2520represent%2520a%2520molecule%252C%2520we%2520utilize%2520specific%250Aencoders%2520to%2520extract%2520fine-grained%2520features%2520from%2520both%2520SMILES%2520string%2520and%25202D%250Amolecular%2520graph%2520representations%2520for%2520feeding%2520into%2520an%2520LLM.%2520A%2520hand-crafted%250Amolecular%2520fingerprint%2520is%2520incorporated%2520to%2520leverage%2520its%2520embedded%2520domain%250Aknowledge.%2520To%2520establish%2520an%2520alignment%2520between%2520MolX%2520and%2520the%2520LLM%2527s%2520textual%2520input%250Aspace%252C%2520the%2520model%2520in%2520which%2520the%2520LLM%2520is%2520frozen%252C%2520is%2520pre-trained%2520with%2520a%2520strategy%250Aincluding%2520a%2520diverse%2520set%2520of%2520tasks.%2520Experimental%2520evaluations%2520show%2520that%2520our%250Aproposed%2520method%2520outperforms%2520baselines%2520across%25204%2520downstream%2520molecule-related%250Atasks%2520ranging%2520from%2520molecule-to-text%2520translation%2520to%2520retrosynthesis%252C%2520with%2520and%250Awithout%2520fine-tuning%2520the%2520LLM%252C%2520while%2520only%2520introducing%2520a%2520small%2520number%2520of%2520trainable%250Aparameters--0.53%255C%2525%2520and%25200.82%255C%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06777v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolX%3A%20Enhancing%20Large%20Language%20Models%20for%20Molecular%20Understanding%20With%20A%0A%20%20Multi-Modal%20Extension&entry.906535625=Khiem%20Le%20and%20Zhichun%20Guo%20and%20Kaiwen%20Dong%20and%20Xiaobao%20Huang%20and%20Bozhao%20Nan%20and%20Roshni%20Iyer%20and%20Xiangliang%20Zhang%20and%20Olaf%20Wiest%20and%20Wei%20Wang%20and%20Ting%20Hua%20and%20Nitesh%20V.%20Chawla&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20their%20strong%20task-handling%20capabilities%0Ahave%20shown%20remarkable%20advancements%20across%20a%20spectrum%20of%20fields%2C%20moving%20beyond%0Anatural%20language%20understanding.%20However%2C%20their%20proficiency%20within%20the%20chemistry%0Adomain%20remains%20restricted%2C%20especially%20in%20solving%20molecule-related%20tasks.%20This%0Achallenge%20is%20attributed%20to%20their%20inherent%20limitations%20in%20comprehending%0Amolecules%20using%20only%20common%20textual%20representations%2C%20i.e.%20SMILES%20strings.%20In%0Athis%20study%2C%20we%20seek%20to%20enhance%20the%20ability%20of%20LLMs%20to%20comprehend%20molecules%20by%0Aequipping%20them%20with%20a%20multi-modal%20external%20module%2C%20termed%20MolX.%20Instead%20of%0Adirectly%20using%20SMILES%20strings%20to%20represent%20a%20molecule%2C%20we%20utilize%20specific%0Aencoders%20to%20extract%20fine-grained%20features%20from%20both%20SMILES%20string%20and%202D%0Amolecular%20graph%20representations%20for%20feeding%20into%20an%20LLM.%20A%20hand-crafted%0Amolecular%20fingerprint%20is%20incorporated%20to%20leverage%20its%20embedded%20domain%0Aknowledge.%20To%20establish%20an%20alignment%20between%20MolX%20and%20the%20LLM%27s%20textual%20input%0Aspace%2C%20the%20model%20in%20which%20the%20LLM%20is%20frozen%2C%20is%20pre-trained%20with%20a%20strategy%0Aincluding%20a%20diverse%20set%20of%20tasks.%20Experimental%20evaluations%20show%20that%20our%0Aproposed%20method%20outperforms%20baselines%20across%204%20downstream%20molecule-related%0Atasks%20ranging%20from%20molecule-to-text%20translation%20to%20retrosynthesis%2C%20with%20and%0Awithout%20fine-tuning%20the%20LLM%2C%20while%20only%20introducing%20a%20small%20number%20of%20trainable%0Aparameters--0.53%5C%25%20and%200.82%5C%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06777v8&entry.124074799=Read"},
{"title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems", "author": "Malsha Ashani Mahawatta Dona and Beatriz Cabrero-Daniel and Yinan Yu and Christian Berger", "abstract": "  Large language models (LLMs) are growingly extended to process multimodal\ndata such as text and video simultaneously. Their remarkable performance in\nunderstanding what is shown in images is surpassing specialized neural networks\n(NNs) such as Yolo that is supporting only a well-formed but very limited\nvocabulary, ie., objects that they are able to detect. When being\nnon-restricted, LLMs and in particular state-of-the-art vision language models\n(VLMs) show impressive performance to describe even complex traffic situations.\nThis is making them potentially suitable components for automotive perception\nsystems to support the understanding of complex traffic situations or edge case\nsituation. However, LLMs and VLMs are prone to hallucination, which mean to\neither potentially not seeing traffic agents such as vulnerable road users who\nare present in a situation, or to seeing traffic agents who are not there in\nreality. While the latter is unwanted making an ADAS or autonomous driving\nsystems (ADS) to unnecessarily slow down, the former could lead to disastrous\ndecisions from an ADS. In our work, we are systematically assessing the\nperformance of 3 state-of-the-art VLMs on a diverse subset of traffic\nsituations sampled from the Waymo Open Dataset to support safety guardrails for\ncapturing such hallucinations in VLM-supported perception systems. We observe\nthat both, proprietary and open VLMs exhibit remarkable image understanding\ncapabilities even paying thorough attention to fine details sometimes difficult\nto spot for us humans. However, they are also still prone to making up elements\nin their descriptions to date requiring hallucination detection strategies such\nas BetterCheck that we propose in our work.\n", "link": "http://arxiv.org/abs/2507.17722v1", "date": "2025-07-23", "relevancy": 2.7505, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BetterCheck%3A%20Towards%20Safeguarding%20VLMs%20for%20Automotive%20Perception%20Systems&body=Title%3A%20BetterCheck%3A%20Towards%20Safeguarding%20VLMs%20for%20Automotive%20Perception%20Systems%0AAuthor%3A%20Malsha%20Ashani%20Mahawatta%20Dona%20and%20Beatriz%20Cabrero-Daniel%20and%20Yinan%20Yu%20and%20Christian%20Berger%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20growingly%20extended%20to%20process%20multimodal%0Adata%20such%20as%20text%20and%20video%20simultaneously.%20Their%20remarkable%20performance%20in%0Aunderstanding%20what%20is%20shown%20in%20images%20is%20surpassing%20specialized%20neural%20networks%0A%28NNs%29%20such%20as%20Yolo%20that%20is%20supporting%20only%20a%20well-formed%20but%20very%20limited%0Avocabulary%2C%20ie.%2C%20objects%20that%20they%20are%20able%20to%20detect.%20When%20being%0Anon-restricted%2C%20LLMs%20and%20in%20particular%20state-of-the-art%20vision%20language%20models%0A%28VLMs%29%20show%20impressive%20performance%20to%20describe%20even%20complex%20traffic%20situations.%0AThis%20is%20making%20them%20potentially%20suitable%20components%20for%20automotive%20perception%0Asystems%20to%20support%20the%20understanding%20of%20complex%20traffic%20situations%20or%20edge%20case%0Asituation.%20However%2C%20LLMs%20and%20VLMs%20are%20prone%20to%20hallucination%2C%20which%20mean%20to%0Aeither%20potentially%20not%20seeing%20traffic%20agents%20such%20as%20vulnerable%20road%20users%20who%0Aare%20present%20in%20a%20situation%2C%20or%20to%20seeing%20traffic%20agents%20who%20are%20not%20there%20in%0Areality.%20While%20the%20latter%20is%20unwanted%20making%20an%20ADAS%20or%20autonomous%20driving%0Asystems%20%28ADS%29%20to%20unnecessarily%20slow%20down%2C%20the%20former%20could%20lead%20to%20disastrous%0Adecisions%20from%20an%20ADS.%20In%20our%20work%2C%20we%20are%20systematically%20assessing%20the%0Aperformance%20of%203%20state-of-the-art%20VLMs%20on%20a%20diverse%20subset%20of%20traffic%0Asituations%20sampled%20from%20the%20Waymo%20Open%20Dataset%20to%20support%20safety%20guardrails%20for%0Acapturing%20such%20hallucinations%20in%20VLM-supported%20perception%20systems.%20We%20observe%0Athat%20both%2C%20proprietary%20and%20open%20VLMs%20exhibit%20remarkable%20image%20understanding%0Acapabilities%20even%20paying%20thorough%20attention%20to%20fine%20details%20sometimes%20difficult%0Ato%20spot%20for%20us%20humans.%20However%2C%20they%20are%20also%20still%20prone%20to%20making%20up%20elements%0Ain%20their%20descriptions%20to%20date%20requiring%20hallucination%20detection%20strategies%20such%0Aas%20BetterCheck%20that%20we%20propose%20in%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetterCheck%253A%2520Towards%2520Safeguarding%2520VLMs%2520for%2520Automotive%2520Perception%2520Systems%26entry.906535625%3DMalsha%2520Ashani%2520Mahawatta%2520Dona%2520and%2520Beatriz%2520Cabrero-Daniel%2520and%2520Yinan%2520Yu%2520and%2520Christian%2520Berger%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520growingly%2520extended%2520to%2520process%2520multimodal%250Adata%2520such%2520as%2520text%2520and%2520video%2520simultaneously.%2520Their%2520remarkable%2520performance%2520in%250Aunderstanding%2520what%2520is%2520shown%2520in%2520images%2520is%2520surpassing%2520specialized%2520neural%2520networks%250A%2528NNs%2529%2520such%2520as%2520Yolo%2520that%2520is%2520supporting%2520only%2520a%2520well-formed%2520but%2520very%2520limited%250Avocabulary%252C%2520ie.%252C%2520objects%2520that%2520they%2520are%2520able%2520to%2520detect.%2520When%2520being%250Anon-restricted%252C%2520LLMs%2520and%2520in%2520particular%2520state-of-the-art%2520vision%2520language%2520models%250A%2528VLMs%2529%2520show%2520impressive%2520performance%2520to%2520describe%2520even%2520complex%2520traffic%2520situations.%250AThis%2520is%2520making%2520them%2520potentially%2520suitable%2520components%2520for%2520automotive%2520perception%250Asystems%2520to%2520support%2520the%2520understanding%2520of%2520complex%2520traffic%2520situations%2520or%2520edge%2520case%250Asituation.%2520However%252C%2520LLMs%2520and%2520VLMs%2520are%2520prone%2520to%2520hallucination%252C%2520which%2520mean%2520to%250Aeither%2520potentially%2520not%2520seeing%2520traffic%2520agents%2520such%2520as%2520vulnerable%2520road%2520users%2520who%250Aare%2520present%2520in%2520a%2520situation%252C%2520or%2520to%2520seeing%2520traffic%2520agents%2520who%2520are%2520not%2520there%2520in%250Areality.%2520While%2520the%2520latter%2520is%2520unwanted%2520making%2520an%2520ADAS%2520or%2520autonomous%2520driving%250Asystems%2520%2528ADS%2529%2520to%2520unnecessarily%2520slow%2520down%252C%2520the%2520former%2520could%2520lead%2520to%2520disastrous%250Adecisions%2520from%2520an%2520ADS.%2520In%2520our%2520work%252C%2520we%2520are%2520systematically%2520assessing%2520the%250Aperformance%2520of%25203%2520state-of-the-art%2520VLMs%2520on%2520a%2520diverse%2520subset%2520of%2520traffic%250Asituations%2520sampled%2520from%2520the%2520Waymo%2520Open%2520Dataset%2520to%2520support%2520safety%2520guardrails%2520for%250Acapturing%2520such%2520hallucinations%2520in%2520VLM-supported%2520perception%2520systems.%2520We%2520observe%250Athat%2520both%252C%2520proprietary%2520and%2520open%2520VLMs%2520exhibit%2520remarkable%2520image%2520understanding%250Acapabilities%2520even%2520paying%2520thorough%2520attention%2520to%2520fine%2520details%2520sometimes%2520difficult%250Ato%2520spot%2520for%2520us%2520humans.%2520However%252C%2520they%2520are%2520also%2520still%2520prone%2520to%2520making%2520up%2520elements%250Ain%2520their%2520descriptions%2520to%2520date%2520requiring%2520hallucination%2520detection%2520strategies%2520such%250Aas%2520BetterCheck%2520that%2520we%2520propose%2520in%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BetterCheck%3A%20Towards%20Safeguarding%20VLMs%20for%20Automotive%20Perception%20Systems&entry.906535625=Malsha%20Ashani%20Mahawatta%20Dona%20and%20Beatriz%20Cabrero-Daniel%20and%20Yinan%20Yu%20and%20Christian%20Berger&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20growingly%20extended%20to%20process%20multimodal%0Adata%20such%20as%20text%20and%20video%20simultaneously.%20Their%20remarkable%20performance%20in%0Aunderstanding%20what%20is%20shown%20in%20images%20is%20surpassing%20specialized%20neural%20networks%0A%28NNs%29%20such%20as%20Yolo%20that%20is%20supporting%20only%20a%20well-formed%20but%20very%20limited%0Avocabulary%2C%20ie.%2C%20objects%20that%20they%20are%20able%20to%20detect.%20When%20being%0Anon-restricted%2C%20LLMs%20and%20in%20particular%20state-of-the-art%20vision%20language%20models%0A%28VLMs%29%20show%20impressive%20performance%20to%20describe%20even%20complex%20traffic%20situations.%0AThis%20is%20making%20them%20potentially%20suitable%20components%20for%20automotive%20perception%0Asystems%20to%20support%20the%20understanding%20of%20complex%20traffic%20situations%20or%20edge%20case%0Asituation.%20However%2C%20LLMs%20and%20VLMs%20are%20prone%20to%20hallucination%2C%20which%20mean%20to%0Aeither%20potentially%20not%20seeing%20traffic%20agents%20such%20as%20vulnerable%20road%20users%20who%0Aare%20present%20in%20a%20situation%2C%20or%20to%20seeing%20traffic%20agents%20who%20are%20not%20there%20in%0Areality.%20While%20the%20latter%20is%20unwanted%20making%20an%20ADAS%20or%20autonomous%20driving%0Asystems%20%28ADS%29%20to%20unnecessarily%20slow%20down%2C%20the%20former%20could%20lead%20to%20disastrous%0Adecisions%20from%20an%20ADS.%20In%20our%20work%2C%20we%20are%20systematically%20assessing%20the%0Aperformance%20of%203%20state-of-the-art%20VLMs%20on%20a%20diverse%20subset%20of%20traffic%0Asituations%20sampled%20from%20the%20Waymo%20Open%20Dataset%20to%20support%20safety%20guardrails%20for%0Acapturing%20such%20hallucinations%20in%20VLM-supported%20perception%20systems.%20We%20observe%0Athat%20both%2C%20proprietary%20and%20open%20VLMs%20exhibit%20remarkable%20image%20understanding%0Acapabilities%20even%20paying%20thorough%20attention%20to%20fine%20details%20sometimes%20difficult%0Ato%20spot%20for%20us%20humans.%20However%2C%20they%20are%20also%20still%20prone%20to%20making%20up%20elements%0Ain%20their%20descriptions%20to%20date%20requiring%20hallucination%20detection%20strategies%20such%0Aas%20BetterCheck%20that%20we%20propose%20in%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17722v1&entry.124074799=Read"},
{"title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars", "author": "Xiaosheng Zhao and Yang Huang and Guirong Xue and Xiao Kong and Jifeng Liu and Xiaoyu Tang and Timothy C. Beers and Yuan-Sen Ting and A-Li Luo", "abstract": "  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n", "link": "http://arxiv.org/abs/2507.01939v2", "date": "2025-07-23", "relevancy": 2.7354, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&body=Title%3A%20SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars%0AAuthor%3A%20Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%0Alanguage%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%0AInspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%0Athat%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%0Aspectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%0Ainformation%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%0Adatasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%0Adiverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%0Apre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%0Aby%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%0APre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%0Ainstruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%0Aspectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%0Aspectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%0Abetween%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%0Aenabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%0Ademonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%0Aimproves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%0Achemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%0Aprecision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%0AAdditionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%0Aoffer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%0Atrained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%0Aprecision%20stellar%20spectroscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecCLIP%253A%2520Aligning%2520and%2520Translating%2520Spectroscopic%2520Measurements%2520for%2520Stars%26entry.906535625%3DXiaosheng%2520Zhao%2520and%2520Yang%2520Huang%2520and%2520Guirong%2520Xue%2520and%2520Xiao%2520Kong%2520and%2520Jifeng%2520Liu%2520and%2520Xiaoyu%2520Tang%2520and%2520Timothy%2520C.%2520Beers%2520and%2520Yuan-Sen%2520Ting%2520and%2520A-Li%2520Luo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520natural%250Alanguage%2520understanding%2520through%2520vast%2520datasets%2520and%2520large-scale%2520parameterization.%250AInspired%2520by%2520this%2520success%252C%2520we%2520present%2520SpecCLIP%252C%2520a%2520foundation%2520model%2520framework%250Athat%2520extends%2520LLM-inspired%2520methodologies%2520to%2520stellar%2520spectral%2520analysis.%2520Stellar%250Aspectra%252C%2520akin%2520to%2520structured%2520language%252C%2520encode%2520rich%2520physical%2520and%2520chemical%250Ainformation%2520about%2520stars.%2520By%2520training%2520foundation%2520models%2520on%2520large-scale%2520spectral%250Adatasets%252C%2520our%2520goal%2520is%2520to%2520learn%2520robust%2520and%2520informative%2520embeddings%2520that%2520support%250Adiverse%2520downstream%2520applications.%2520As%2520a%2520proof%2520of%2520concept%252C%2520SpecCLIP%2520involves%250Apre-training%2520on%2520two%2520spectral%2520types--LAMOST%2520low-resolution%2520and%2520Gaia%2520XP--followed%250Aby%2520contrastive%2520alignment%2520using%2520the%2520CLIP%2520%2528Contrastive%2520Language-Image%250APre-training%2529%2520framework%252C%2520adapted%2520to%2520associate%2520spectra%2520from%2520different%250Ainstruments.%2520This%2520alignment%2520is%2520complemented%2520by%2520auxiliary%2520decoders%2520that%2520preserve%250Aspectrum-specific%2520information%2520and%2520enable%2520translation%2520%2528prediction%2529%2520between%250Aspectral%2520types%252C%2520with%2520the%2520former%2520achieved%2520by%2520maximizing%2520mutual%2520information%250Abetween%2520embeddings%2520and%2520input%2520spectra.%2520The%2520result%2520is%2520a%2520cross-spectrum%2520framework%250Aenabling%2520intrinsic%2520calibration%2520and%2520flexible%2520applications%2520across%2520instruments.%2520We%250Ademonstrate%2520that%2520fine-tuning%2520these%2520models%2520on%2520moderate-sized%2520labeled%2520datasets%250Aimproves%2520adaptability%2520to%2520tasks%2520such%2520as%2520stellar-parameter%2520estimation%2520and%250Achemical-abundance%2520determination.%2520SpecCLIP%2520also%2520enhances%2520the%2520accuracy%2520and%250Aprecision%2520of%2520parameter%2520estimates%2520benchmarked%2520against%2520external%2520survey%2520data.%250AAdditionally%252C%2520its%2520similarity%2520search%2520and%2520cross-spectrum%2520prediction%2520capabilities%250Aoffer%2520potential%2520for%2520anomaly%2520detection.%2520Our%2520results%2520suggest%2520that%2520contrastively%250Atrained%2520foundation%2520models%2520enriched%2520with%2520spectrum-aware%2520decoders%2520can%2520advance%250Aprecision%2520stellar%2520spectroscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecCLIP%3A%20Aligning%20and%20Translating%20Spectroscopic%20Measurements%20for%20Stars&entry.906535625=Xiaosheng%20Zhao%20and%20Yang%20Huang%20and%20Guirong%20Xue%20and%20Xiao%20Kong%20and%20Jifeng%20Liu%20and%20Xiaoyu%20Tang%20and%20Timothy%20C.%20Beers%20and%20Yuan-Sen%20Ting%20and%20A-Li%20Luo&entry.1292438233=%20%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20transformed%20natural%0Alanguage%20understanding%20through%20vast%20datasets%20and%20large-scale%20parameterization.%0AInspired%20by%20this%20success%2C%20we%20present%20SpecCLIP%2C%20a%20foundation%20model%20framework%0Athat%20extends%20LLM-inspired%20methodologies%20to%20stellar%20spectral%20analysis.%20Stellar%0Aspectra%2C%20akin%20to%20structured%20language%2C%20encode%20rich%20physical%20and%20chemical%0Ainformation%20about%20stars.%20By%20training%20foundation%20models%20on%20large-scale%20spectral%0Adatasets%2C%20our%20goal%20is%20to%20learn%20robust%20and%20informative%20embeddings%20that%20support%0Adiverse%20downstream%20applications.%20As%20a%20proof%20of%20concept%2C%20SpecCLIP%20involves%0Apre-training%20on%20two%20spectral%20types--LAMOST%20low-resolution%20and%20Gaia%20XP--followed%0Aby%20contrastive%20alignment%20using%20the%20CLIP%20%28Contrastive%20Language-Image%0APre-training%29%20framework%2C%20adapted%20to%20associate%20spectra%20from%20different%0Ainstruments.%20This%20alignment%20is%20complemented%20by%20auxiliary%20decoders%20that%20preserve%0Aspectrum-specific%20information%20and%20enable%20translation%20%28prediction%29%20between%0Aspectral%20types%2C%20with%20the%20former%20achieved%20by%20maximizing%20mutual%20information%0Abetween%20embeddings%20and%20input%20spectra.%20The%20result%20is%20a%20cross-spectrum%20framework%0Aenabling%20intrinsic%20calibration%20and%20flexible%20applications%20across%20instruments.%20We%0Ademonstrate%20that%20fine-tuning%20these%20models%20on%20moderate-sized%20labeled%20datasets%0Aimproves%20adaptability%20to%20tasks%20such%20as%20stellar-parameter%20estimation%20and%0Achemical-abundance%20determination.%20SpecCLIP%20also%20enhances%20the%20accuracy%20and%0Aprecision%20of%20parameter%20estimates%20benchmarked%20against%20external%20survey%20data.%0AAdditionally%2C%20its%20similarity%20search%20and%20cross-spectrum%20prediction%20capabilities%0Aoffer%20potential%20for%20anomaly%20detection.%20Our%20results%20suggest%20that%20contrastively%0Atrained%20foundation%20models%20enriched%20with%20spectrum-aware%20decoders%20can%20advance%0Aprecision%20stellar%20spectroscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01939v2&entry.124074799=Read"},
{"title": "ViRN: Variational Inference and Distribution Trilateration for\n  Long-Tailed Continual Representation Learning", "author": "Hao Dai and Chong Tang and Jagmohan Chauhan", "abstract": "  Continual learning (CL) with long-tailed data distributions remains a\ncritical challenge for real-world AI systems, where models must sequentially\nadapt to new classes while retaining knowledge of old ones, despite severe\nclass imbalance. Existing methods struggle to balance stability and plasticity,\noften collapsing under extreme sample scarcity. To address this, we propose\nViRN, a novel CL framework that integrates variational inference (VI) with\ndistributional trilateration for robust long-tailed learning. First, we model\nclass-conditional distributions via a Variational Autoencoder to mitigate bias\ntoward head classes. Second, we reconstruct tail-class distributions via\nWasserstein distance-based neighborhood retrieval and geometric fusion,\nenabling sample-efficient alignment of tail-class representations. Evaluated on\nsix long-tailed classification benchmarks, including speech (e.g., rare\nacoustic events, accents) and image tasks, ViRN achieves a 10.24% average\naccuracy gain over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2507.17368v1", "date": "2025-07-23", "relevancy": 2.7086, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5568}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViRN%3A%20Variational%20Inference%20and%20Distribution%20Trilateration%20for%0A%20%20Long-Tailed%20Continual%20Representation%20Learning&body=Title%3A%20ViRN%3A%20Variational%20Inference%20and%20Distribution%20Trilateration%20for%0A%20%20Long-Tailed%20Continual%20Representation%20Learning%0AAuthor%3A%20Hao%20Dai%20and%20Chong%20Tang%20and%20Jagmohan%20Chauhan%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20with%20long-tailed%20data%20distributions%20remains%20a%0Acritical%20challenge%20for%20real-world%20AI%20systems%2C%20where%20models%20must%20sequentially%0Aadapt%20to%20new%20classes%20while%20retaining%20knowledge%20of%20old%20ones%2C%20despite%20severe%0Aclass%20imbalance.%20Existing%20methods%20struggle%20to%20balance%20stability%20and%20plasticity%2C%0Aoften%20collapsing%20under%20extreme%20sample%20scarcity.%20To%20address%20this%2C%20we%20propose%0AViRN%2C%20a%20novel%20CL%20framework%20that%20integrates%20variational%20inference%20%28VI%29%20with%0Adistributional%20trilateration%20for%20robust%20long-tailed%20learning.%20First%2C%20we%20model%0Aclass-conditional%20distributions%20via%20a%20Variational%20Autoencoder%20to%20mitigate%20bias%0Atoward%20head%20classes.%20Second%2C%20we%20reconstruct%20tail-class%20distributions%20via%0AWasserstein%20distance-based%20neighborhood%20retrieval%20and%20geometric%20fusion%2C%0Aenabling%20sample-efficient%20alignment%20of%20tail-class%20representations.%20Evaluated%20on%0Asix%20long-tailed%20classification%20benchmarks%2C%20including%20speech%20%28e.g.%2C%20rare%0Aacoustic%20events%2C%20accents%29%20and%20image%20tasks%2C%20ViRN%20achieves%20a%2010.24%25%20average%0Aaccuracy%20gain%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViRN%253A%2520Variational%2520Inference%2520and%2520Distribution%2520Trilateration%2520for%250A%2520%2520Long-Tailed%2520Continual%2520Representation%2520Learning%26entry.906535625%3DHao%2520Dai%2520and%2520Chong%2520Tang%2520and%2520Jagmohan%2520Chauhan%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520with%2520long-tailed%2520data%2520distributions%2520remains%2520a%250Acritical%2520challenge%2520for%2520real-world%2520AI%2520systems%252C%2520where%2520models%2520must%2520sequentially%250Aadapt%2520to%2520new%2520classes%2520while%2520retaining%2520knowledge%2520of%2520old%2520ones%252C%2520despite%2520severe%250Aclass%2520imbalance.%2520Existing%2520methods%2520struggle%2520to%2520balance%2520stability%2520and%2520plasticity%252C%250Aoften%2520collapsing%2520under%2520extreme%2520sample%2520scarcity.%2520To%2520address%2520this%252C%2520we%2520propose%250AViRN%252C%2520a%2520novel%2520CL%2520framework%2520that%2520integrates%2520variational%2520inference%2520%2528VI%2529%2520with%250Adistributional%2520trilateration%2520for%2520robust%2520long-tailed%2520learning.%2520First%252C%2520we%2520model%250Aclass-conditional%2520distributions%2520via%2520a%2520Variational%2520Autoencoder%2520to%2520mitigate%2520bias%250Atoward%2520head%2520classes.%2520Second%252C%2520we%2520reconstruct%2520tail-class%2520distributions%2520via%250AWasserstein%2520distance-based%2520neighborhood%2520retrieval%2520and%2520geometric%2520fusion%252C%250Aenabling%2520sample-efficient%2520alignment%2520of%2520tail-class%2520representations.%2520Evaluated%2520on%250Asix%2520long-tailed%2520classification%2520benchmarks%252C%2520including%2520speech%2520%2528e.g.%252C%2520rare%250Aacoustic%2520events%252C%2520accents%2529%2520and%2520image%2520tasks%252C%2520ViRN%2520achieves%2520a%252010.24%2525%2520average%250Aaccuracy%2520gain%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViRN%3A%20Variational%20Inference%20and%20Distribution%20Trilateration%20for%0A%20%20Long-Tailed%20Continual%20Representation%20Learning&entry.906535625=Hao%20Dai%20and%20Chong%20Tang%20and%20Jagmohan%20Chauhan&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20with%20long-tailed%20data%20distributions%20remains%20a%0Acritical%20challenge%20for%20real-world%20AI%20systems%2C%20where%20models%20must%20sequentially%0Aadapt%20to%20new%20classes%20while%20retaining%20knowledge%20of%20old%20ones%2C%20despite%20severe%0Aclass%20imbalance.%20Existing%20methods%20struggle%20to%20balance%20stability%20and%20plasticity%2C%0Aoften%20collapsing%20under%20extreme%20sample%20scarcity.%20To%20address%20this%2C%20we%20propose%0AViRN%2C%20a%20novel%20CL%20framework%20that%20integrates%20variational%20inference%20%28VI%29%20with%0Adistributional%20trilateration%20for%20robust%20long-tailed%20learning.%20First%2C%20we%20model%0Aclass-conditional%20distributions%20via%20a%20Variational%20Autoencoder%20to%20mitigate%20bias%0Atoward%20head%20classes.%20Second%2C%20we%20reconstruct%20tail-class%20distributions%20via%0AWasserstein%20distance-based%20neighborhood%20retrieval%20and%20geometric%20fusion%2C%0Aenabling%20sample-efficient%20alignment%20of%20tail-class%20representations.%20Evaluated%20on%0Asix%20long-tailed%20classification%20benchmarks%2C%20including%20speech%20%28e.g.%2C%20rare%0Aacoustic%20events%2C%20accents%29%20and%20image%20tasks%2C%20ViRN%20achieves%20a%2010.24%25%20average%0Aaccuracy%20gain%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17368v1&entry.124074799=Read"},
{"title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning", "author": "Yu Li and Zhuoshi Pan and Honglin Lin and Mengyuan Sun and Conghui He and Lijun Wu", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.\n", "link": "http://arxiv.org/abs/2507.17512v1", "date": "2025-07-23", "relevancy": 2.704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20One%20Domain%20Help%20Others%3F%20A%20Data-Centric%20Study%20on%20Multi-Domain%0A%20%20Reasoning%20via%20Reinforcement%20Learning&body=Title%3A%20Can%20One%20Domain%20Help%20Others%3F%20A%20Data-Centric%20Study%20on%20Multi-Domain%0A%20%20Reasoning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Yu%20Li%20and%20Zhuoshi%20Pan%20and%20Honglin%20Lin%20and%20Mengyuan%20Sun%20and%20Conghui%20He%20and%20Lijun%20Wu%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20enhancing%20the%20reasoning%20capabilities%20of%20LLMs.%20Existing%0Aresearch%20has%20predominantly%20concentrated%20on%20isolated%20reasoning%20domains%20such%20as%0Amathematical%20problem-solving%2C%20coding%20tasks%2C%20or%20logical%20reasoning.%20However%2C%20real%0Aworld%20reasoning%20scenarios%20inherently%20demand%20an%20integrated%20application%20of%0Amultiple%20cognitive%20skills.%20Despite%20this%2C%20the%20interplay%20among%20these%20reasoning%0Askills%20under%20reinforcement%20learning%20remains%20poorly%20understood.%20To%20bridge%20this%0Agap%2C%20we%20present%20a%20systematic%20investigation%20of%20multi-domain%20reasoning%20within%20the%0ARLVR%20framework%2C%20explicitly%20focusing%20on%20three%20primary%20domains%3A%20mathematical%0Areasoning%2C%20code%20generation%2C%20and%20logical%20puzzle%20solving.%20We%20conduct%20a%0Acomprehensive%20study%20comprising%20four%20key%20components%3A%20%281%29%20Leveraging%20the%20GRPO%0Aalgorithm%20and%20the%20Qwen-2.5-7B%20model%20family%2C%20our%20study%20thoroughly%20evaluates%20the%0Amodels%27%20in-domain%20improvements%20and%20cross-domain%20generalization%20capabilities%0Awhen%20trained%20on%20single-domain%20datasets.%20%282%29%20Additionally%2C%20we%20examine%20the%0Aintricate%20interactions%20including%20mutual%20enhancements%20and%20conflicts%20that%20emerge%0Aduring%20combined%20cross-domain%20training.%20%283%29%20To%20further%20understand%20the%20influence%0Aof%20SFT%20on%20RL%2C%20we%20also%20analyze%20and%20compare%20performance%20differences%20between%20base%0Aand%20instruct%20models%20under%20identical%20RL%20configurations.%20%284%29%20Furthermore%2C%20we%0Adelve%20into%20critical%20RL%20training%20details%2C%20systematically%20exploring%20the%20impacts%0Aof%20curriculum%20learning%20strategies%2C%20variations%20in%20reward%20design%2C%20and%0Alanguage-specific%20factors.%20Through%20extensive%20experiments%2C%20our%20results%20offer%0Asignificant%20insights%20into%20the%20dynamics%20governing%20domain%20interactions%2C%20revealing%0Akey%20factors%20influencing%20both%20specialized%20and%20generalizable%20reasoning%0Aperformance.%20These%20findings%20provide%20valuable%20guidance%20for%20optimizing%20RL%0Amethodologies%20to%20foster%20comprehensive%2C%20multi-domain%20reasoning%20capabilities%20in%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520One%2520Domain%2520Help%2520Others%253F%2520A%2520Data-Centric%2520Study%2520on%2520Multi-Domain%250A%2520%2520Reasoning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DYu%2520Li%2520and%2520Zhuoshi%2520Pan%2520and%2520Honglin%2520Lin%2520and%2520Mengyuan%2520Sun%2520and%2520Conghui%2520He%2520and%2520Lijun%2520Wu%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520LLMs.%2520Existing%250Aresearch%2520has%2520predominantly%2520concentrated%2520on%2520isolated%2520reasoning%2520domains%2520such%2520as%250Amathematical%2520problem-solving%252C%2520coding%2520tasks%252C%2520or%2520logical%2520reasoning.%2520However%252C%2520real%250Aworld%2520reasoning%2520scenarios%2520inherently%2520demand%2520an%2520integrated%2520application%2520of%250Amultiple%2520cognitive%2520skills.%2520Despite%2520this%252C%2520the%2520interplay%2520among%2520these%2520reasoning%250Askills%2520under%2520reinforcement%2520learning%2520remains%2520poorly%2520understood.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520present%2520a%2520systematic%2520investigation%2520of%2520multi-domain%2520reasoning%2520within%2520the%250ARLVR%2520framework%252C%2520explicitly%2520focusing%2520on%2520three%2520primary%2520domains%253A%2520mathematical%250Areasoning%252C%2520code%2520generation%252C%2520and%2520logical%2520puzzle%2520solving.%2520We%2520conduct%2520a%250Acomprehensive%2520study%2520comprising%2520four%2520key%2520components%253A%2520%25281%2529%2520Leveraging%2520the%2520GRPO%250Aalgorithm%2520and%2520the%2520Qwen-2.5-7B%2520model%2520family%252C%2520our%2520study%2520thoroughly%2520evaluates%2520the%250Amodels%2527%2520in-domain%2520improvements%2520and%2520cross-domain%2520generalization%2520capabilities%250Awhen%2520trained%2520on%2520single-domain%2520datasets.%2520%25282%2529%2520Additionally%252C%2520we%2520examine%2520the%250Aintricate%2520interactions%2520including%2520mutual%2520enhancements%2520and%2520conflicts%2520that%2520emerge%250Aduring%2520combined%2520cross-domain%2520training.%2520%25283%2529%2520To%2520further%2520understand%2520the%2520influence%250Aof%2520SFT%2520on%2520RL%252C%2520we%2520also%2520analyze%2520and%2520compare%2520performance%2520differences%2520between%2520base%250Aand%2520instruct%2520models%2520under%2520identical%2520RL%2520configurations.%2520%25284%2529%2520Furthermore%252C%2520we%250Adelve%2520into%2520critical%2520RL%2520training%2520details%252C%2520systematically%2520exploring%2520the%2520impacts%250Aof%2520curriculum%2520learning%2520strategies%252C%2520variations%2520in%2520reward%2520design%252C%2520and%250Alanguage-specific%2520factors.%2520Through%2520extensive%2520experiments%252C%2520our%2520results%2520offer%250Asignificant%2520insights%2520into%2520the%2520dynamics%2520governing%2520domain%2520interactions%252C%2520revealing%250Akey%2520factors%2520influencing%2520both%2520specialized%2520and%2520generalizable%2520reasoning%250Aperformance.%2520These%2520findings%2520provide%2520valuable%2520guidance%2520for%2520optimizing%2520RL%250Amethodologies%2520to%2520foster%2520comprehensive%252C%2520multi-domain%2520reasoning%2520capabilities%2520in%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20One%20Domain%20Help%20Others%3F%20A%20Data-Centric%20Study%20on%20Multi-Domain%0A%20%20Reasoning%20via%20Reinforcement%20Learning&entry.906535625=Yu%20Li%20and%20Zhuoshi%20Pan%20and%20Honglin%20Lin%20and%20Mengyuan%20Sun%20and%20Conghui%20He%20and%20Lijun%20Wu&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20enhancing%20the%20reasoning%20capabilities%20of%20LLMs.%20Existing%0Aresearch%20has%20predominantly%20concentrated%20on%20isolated%20reasoning%20domains%20such%20as%0Amathematical%20problem-solving%2C%20coding%20tasks%2C%20or%20logical%20reasoning.%20However%2C%20real%0Aworld%20reasoning%20scenarios%20inherently%20demand%20an%20integrated%20application%20of%0Amultiple%20cognitive%20skills.%20Despite%20this%2C%20the%20interplay%20among%20these%20reasoning%0Askills%20under%20reinforcement%20learning%20remains%20poorly%20understood.%20To%20bridge%20this%0Agap%2C%20we%20present%20a%20systematic%20investigation%20of%20multi-domain%20reasoning%20within%20the%0ARLVR%20framework%2C%20explicitly%20focusing%20on%20three%20primary%20domains%3A%20mathematical%0Areasoning%2C%20code%20generation%2C%20and%20logical%20puzzle%20solving.%20We%20conduct%20a%0Acomprehensive%20study%20comprising%20four%20key%20components%3A%20%281%29%20Leveraging%20the%20GRPO%0Aalgorithm%20and%20the%20Qwen-2.5-7B%20model%20family%2C%20our%20study%20thoroughly%20evaluates%20the%0Amodels%27%20in-domain%20improvements%20and%20cross-domain%20generalization%20capabilities%0Awhen%20trained%20on%20single-domain%20datasets.%20%282%29%20Additionally%2C%20we%20examine%20the%0Aintricate%20interactions%20including%20mutual%20enhancements%20and%20conflicts%20that%20emerge%0Aduring%20combined%20cross-domain%20training.%20%283%29%20To%20further%20understand%20the%20influence%0Aof%20SFT%20on%20RL%2C%20we%20also%20analyze%20and%20compare%20performance%20differences%20between%20base%0Aand%20instruct%20models%20under%20identical%20RL%20configurations.%20%284%29%20Furthermore%2C%20we%0Adelve%20into%20critical%20RL%20training%20details%2C%20systematically%20exploring%20the%20impacts%0Aof%20curriculum%20learning%20strategies%2C%20variations%20in%20reward%20design%2C%20and%0Alanguage-specific%20factors.%20Through%20extensive%20experiments%2C%20our%20results%20offer%0Asignificant%20insights%20into%20the%20dynamics%20governing%20domain%20interactions%2C%20revealing%0Akey%20factors%20influencing%20both%20specialized%20and%20generalizable%20reasoning%0Aperformance.%20These%20findings%20provide%20valuable%20guidance%20for%20optimizing%20RL%0Amethodologies%20to%20foster%20comprehensive%2C%20multi-domain%20reasoning%20capabilities%20in%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17512v1&entry.124074799=Read"},
{"title": "Reusing Attention for One-stage Lane Topology Understanding", "author": "Yang Li and Zongzheng Zhang and Xuchong Qiu and Xinrun Li and Ziming Liu and Leichen Wang and Ruikai Li and Zhenxin Zhu and Huan-ang Gao and Xiaojian Lin and Zhiyong Cui and Hang Zhao and Hao Zhao", "abstract": "  Understanding lane toplogy relationships accurately is critical for safe\nautonomous driving. However, existing two-stage methods suffer from\ninefficiencies due to error propagations and increased computational overheads.\nTo address these challenges, we propose a one-stage architecture that\nsimultaneously predicts traffic elements, lane centerlines and topology\nrelationship, improving both the accuracy and inference speed of lane topology\nunderstanding for autonomous driving. Our key innovation lies in reusing\nintermediate attention resources within distinct transformer decoders. This\napproach effectively leverages the inherent relational knowledge within the\nelement detection module to enable the modeling of topology relationships among\ntraffic elements and lanes without requiring additional computationally\nexpensive graph networks. Furthermore, we are the first to demonstrate that\nknowledge can be distilled from models that utilize standard definition (SD)\nmaps to those operates without using SD maps, enabling superior performance\neven in the absence of SD maps. Extensive experiments on the OpenLane-V2\ndataset show that our approach outperforms baseline methods in both accuracy\nand efficiency, achieving superior results in lane detection, traffic element\nidentification, and topology reasoning. Our code is available at\nhttps://github.com/Yang-Li-2000/one-stage.git.\n", "link": "http://arxiv.org/abs/2507.17617v1", "date": "2025-07-23", "relevancy": 2.678, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5598}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5243}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reusing%20Attention%20for%20One-stage%20Lane%20Topology%20Understanding&body=Title%3A%20Reusing%20Attention%20for%20One-stage%20Lane%20Topology%20Understanding%0AAuthor%3A%20Yang%20Li%20and%20Zongzheng%20Zhang%20and%20Xuchong%20Qiu%20and%20Xinrun%20Li%20and%20Ziming%20Liu%20and%20Leichen%20Wang%20and%20Ruikai%20Li%20and%20Zhenxin%20Zhu%20and%20Huan-ang%20Gao%20and%20Xiaojian%20Lin%20and%20Zhiyong%20Cui%20and%20Hang%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Understanding%20lane%20toplogy%20relationships%20accurately%20is%20critical%20for%20safe%0Aautonomous%20driving.%20However%2C%20existing%20two-stage%20methods%20suffer%20from%0Ainefficiencies%20due%20to%20error%20propagations%20and%20increased%20computational%20overheads.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20one-stage%20architecture%20that%0Asimultaneously%20predicts%20traffic%20elements%2C%20lane%20centerlines%20and%20topology%0Arelationship%2C%20improving%20both%20the%20accuracy%20and%20inference%20speed%20of%20lane%20topology%0Aunderstanding%20for%20autonomous%20driving.%20Our%20key%20innovation%20lies%20in%20reusing%0Aintermediate%20attention%20resources%20within%20distinct%20transformer%20decoders.%20This%0Aapproach%20effectively%20leverages%20the%20inherent%20relational%20knowledge%20within%20the%0Aelement%20detection%20module%20to%20enable%20the%20modeling%20of%20topology%20relationships%20among%0Atraffic%20elements%20and%20lanes%20without%20requiring%20additional%20computationally%0Aexpensive%20graph%20networks.%20Furthermore%2C%20we%20are%20the%20first%20to%20demonstrate%20that%0Aknowledge%20can%20be%20distilled%20from%20models%20that%20utilize%20standard%20definition%20%28SD%29%0Amaps%20to%20those%20operates%20without%20using%20SD%20maps%2C%20enabling%20superior%20performance%0Aeven%20in%20the%20absence%20of%20SD%20maps.%20Extensive%20experiments%20on%20the%20OpenLane-V2%0Adataset%20show%20that%20our%20approach%20outperforms%20baseline%20methods%20in%20both%20accuracy%0Aand%20efficiency%2C%20achieving%20superior%20results%20in%20lane%20detection%2C%20traffic%20element%0Aidentification%2C%20and%20topology%20reasoning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Yang-Li-2000/one-stage.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReusing%2520Attention%2520for%2520One-stage%2520Lane%2520Topology%2520Understanding%26entry.906535625%3DYang%2520Li%2520and%2520Zongzheng%2520Zhang%2520and%2520Xuchong%2520Qiu%2520and%2520Xinrun%2520Li%2520and%2520Ziming%2520Liu%2520and%2520Leichen%2520Wang%2520and%2520Ruikai%2520Li%2520and%2520Zhenxin%2520Zhu%2520and%2520Huan-ang%2520Gao%2520and%2520Xiaojian%2520Lin%2520and%2520Zhiyong%2520Cui%2520and%2520Hang%2520Zhao%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Understanding%2520lane%2520toplogy%2520relationships%2520accurately%2520is%2520critical%2520for%2520safe%250Aautonomous%2520driving.%2520However%252C%2520existing%2520two-stage%2520methods%2520suffer%2520from%250Ainefficiencies%2520due%2520to%2520error%2520propagations%2520and%2520increased%2520computational%2520overheads.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520one-stage%2520architecture%2520that%250Asimultaneously%2520predicts%2520traffic%2520elements%252C%2520lane%2520centerlines%2520and%2520topology%250Arelationship%252C%2520improving%2520both%2520the%2520accuracy%2520and%2520inference%2520speed%2520of%2520lane%2520topology%250Aunderstanding%2520for%2520autonomous%2520driving.%2520Our%2520key%2520innovation%2520lies%2520in%2520reusing%250Aintermediate%2520attention%2520resources%2520within%2520distinct%2520transformer%2520decoders.%2520This%250Aapproach%2520effectively%2520leverages%2520the%2520inherent%2520relational%2520knowledge%2520within%2520the%250Aelement%2520detection%2520module%2520to%2520enable%2520the%2520modeling%2520of%2520topology%2520relationships%2520among%250Atraffic%2520elements%2520and%2520lanes%2520without%2520requiring%2520additional%2520computationally%250Aexpensive%2520graph%2520networks.%2520Furthermore%252C%2520we%2520are%2520the%2520first%2520to%2520demonstrate%2520that%250Aknowledge%2520can%2520be%2520distilled%2520from%2520models%2520that%2520utilize%2520standard%2520definition%2520%2528SD%2529%250Amaps%2520to%2520those%2520operates%2520without%2520using%2520SD%2520maps%252C%2520enabling%2520superior%2520performance%250Aeven%2520in%2520the%2520absence%2520of%2520SD%2520maps.%2520Extensive%2520experiments%2520on%2520the%2520OpenLane-V2%250Adataset%2520show%2520that%2520our%2520approach%2520outperforms%2520baseline%2520methods%2520in%2520both%2520accuracy%250Aand%2520efficiency%252C%2520achieving%2520superior%2520results%2520in%2520lane%2520detection%252C%2520traffic%2520element%250Aidentification%252C%2520and%2520topology%2520reasoning.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Yang-Li-2000/one-stage.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reusing%20Attention%20for%20One-stage%20Lane%20Topology%20Understanding&entry.906535625=Yang%20Li%20and%20Zongzheng%20Zhang%20and%20Xuchong%20Qiu%20and%20Xinrun%20Li%20and%20Ziming%20Liu%20and%20Leichen%20Wang%20and%20Ruikai%20Li%20and%20Zhenxin%20Zhu%20and%20Huan-ang%20Gao%20and%20Xiaojian%20Lin%20and%20Zhiyong%20Cui%20and%20Hang%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Understanding%20lane%20toplogy%20relationships%20accurately%20is%20critical%20for%20safe%0Aautonomous%20driving.%20However%2C%20existing%20two-stage%20methods%20suffer%20from%0Ainefficiencies%20due%20to%20error%20propagations%20and%20increased%20computational%20overheads.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20one-stage%20architecture%20that%0Asimultaneously%20predicts%20traffic%20elements%2C%20lane%20centerlines%20and%20topology%0Arelationship%2C%20improving%20both%20the%20accuracy%20and%20inference%20speed%20of%20lane%20topology%0Aunderstanding%20for%20autonomous%20driving.%20Our%20key%20innovation%20lies%20in%20reusing%0Aintermediate%20attention%20resources%20within%20distinct%20transformer%20decoders.%20This%0Aapproach%20effectively%20leverages%20the%20inherent%20relational%20knowledge%20within%20the%0Aelement%20detection%20module%20to%20enable%20the%20modeling%20of%20topology%20relationships%20among%0Atraffic%20elements%20and%20lanes%20without%20requiring%20additional%20computationally%0Aexpensive%20graph%20networks.%20Furthermore%2C%20we%20are%20the%20first%20to%20demonstrate%20that%0Aknowledge%20can%20be%20distilled%20from%20models%20that%20utilize%20standard%20definition%20%28SD%29%0Amaps%20to%20those%20operates%20without%20using%20SD%20maps%2C%20enabling%20superior%20performance%0Aeven%20in%20the%20absence%20of%20SD%20maps.%20Extensive%20experiments%20on%20the%20OpenLane-V2%0Adataset%20show%20that%20our%20approach%20outperforms%20baseline%20methods%20in%20both%20accuracy%0Aand%20efficiency%2C%20achieving%20superior%20results%20in%20lane%20detection%2C%20traffic%20element%0Aidentification%2C%20and%20topology%20reasoning.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Yang-Li-2000/one-stage.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17617v1&entry.124074799=Read"},
{"title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging", "author": "Farnaz Khun Jush and Steffen Vogler and Matthias Lenga", "abstract": "  The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.\n", "link": "http://arxiv.org/abs/2507.17412v1", "date": "2025-07-23", "relevancy": 2.6653, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5355}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging&body=Title%3A%20Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging%0AAuthor%3A%20Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20The%20increasing%20volume%20of%20medical%20images%20poses%20challenges%20for%20radiologists%20in%0Aretrieving%20relevant%20cases.%20Content-based%20image%20retrieval%20%28CBIR%29%20systems%20offer%0Apotential%20for%20efficient%20access%20to%20similar%20cases%2C%20yet%20lack%20standardized%0Aevaluation%20and%20comprehensive%20studies.%20Building%20on%20prior%20studies%20for%20tumor%0Acharacterization%20via%20CBIR%2C%20this%20study%20advances%20CBIR%20research%20for%20volumetric%0Amedical%20images%20through%20three%20key%20contributions%3A%20%281%29%20a%20framework%20eliminating%0Areliance%20on%20pre-segmented%20data%20and%20organ-specific%20datasets%2C%20aligning%20with%20large%0Aand%20unstructured%20image%20archiving%20systems%2C%20i.e.%20PACS%20in%20clinical%20practice%3B%20%282%29%0Aintroduction%20of%20C-MIR%2C%20a%20novel%20volumetric%20re-ranking%20method%20adapting%20ColBERT%27s%0Acontextualized%20late%20interaction%20mechanism%20for%203D%20medical%20imaging%3B%20%283%29%0Acomprehensive%20evaluation%20across%20four%20tumor%20sites%20using%20three%20feature%20extractors%0Aand%20three%20database%20configurations.%20Our%20evaluations%20highlight%20the%20significant%0Aadvantages%20of%20C-MIR.%20We%20demonstrate%20the%20successful%20adaptation%20of%20the%20late%0Ainteraction%20principle%20to%20volumetric%20medical%20images%2C%20enabling%20effective%0Acontext-aware%20re-ranking.%20A%20key%20finding%20is%20C-MIR%27s%20ability%20to%20effectively%0Alocalize%20the%20region%20of%20interest%2C%20eliminating%20the%20need%20for%20pre-segmentation%20of%0Adatasets%20and%20offering%20a%20computationally%20efficient%20alternative%20to%20systems%0Arelying%20on%20expensive%20data%20enrichment%20steps.%20C-MIR%20demonstrates%20promising%0Aimprovements%20in%20tumor%20flagging%2C%20achieving%20improved%20performance%2C%20particularly%0Afor%20colon%20and%20lung%20tumors%20%28p%3C0.05%29.%20C-MIR%20also%20shows%20potential%20for%20improving%0Atumor%20staging%2C%20warranting%20further%20exploration%20of%20its%20capabilities.%20Ultimately%2C%0Aour%20work%20seeks%20to%20bridge%20the%20gap%20between%20advanced%20retrieval%20techniques%20and%0Atheir%20practical%20applications%20in%20healthcare%2C%20paving%20the%20way%20for%20improved%0Adiagnostic%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-based%25203D%2520Image%2520Retrieval%2520and%2520a%2520ColBERT-inspired%2520Re-ranking%2520for%250A%2520%2520Tumor%2520Flagging%2520and%2520Staging%26entry.906535625%3DFarnaz%2520Khun%2520Jush%2520and%2520Steffen%2520Vogler%2520and%2520Matthias%2520Lenga%26entry.1292438233%3D%2520%2520The%2520increasing%2520volume%2520of%2520medical%2520images%2520poses%2520challenges%2520for%2520radiologists%2520in%250Aretrieving%2520relevant%2520cases.%2520Content-based%2520image%2520retrieval%2520%2528CBIR%2529%2520systems%2520offer%250Apotential%2520for%2520efficient%2520access%2520to%2520similar%2520cases%252C%2520yet%2520lack%2520standardized%250Aevaluation%2520and%2520comprehensive%2520studies.%2520Building%2520on%2520prior%2520studies%2520for%2520tumor%250Acharacterization%2520via%2520CBIR%252C%2520this%2520study%2520advances%2520CBIR%2520research%2520for%2520volumetric%250Amedical%2520images%2520through%2520three%2520key%2520contributions%253A%2520%25281%2529%2520a%2520framework%2520eliminating%250Areliance%2520on%2520pre-segmented%2520data%2520and%2520organ-specific%2520datasets%252C%2520aligning%2520with%2520large%250Aand%2520unstructured%2520image%2520archiving%2520systems%252C%2520i.e.%2520PACS%2520in%2520clinical%2520practice%253B%2520%25282%2529%250Aintroduction%2520of%2520C-MIR%252C%2520a%2520novel%2520volumetric%2520re-ranking%2520method%2520adapting%2520ColBERT%2527s%250Acontextualized%2520late%2520interaction%2520mechanism%2520for%25203D%2520medical%2520imaging%253B%2520%25283%2529%250Acomprehensive%2520evaluation%2520across%2520four%2520tumor%2520sites%2520using%2520three%2520feature%2520extractors%250Aand%2520three%2520database%2520configurations.%2520Our%2520evaluations%2520highlight%2520the%2520significant%250Aadvantages%2520of%2520C-MIR.%2520We%2520demonstrate%2520the%2520successful%2520adaptation%2520of%2520the%2520late%250Ainteraction%2520principle%2520to%2520volumetric%2520medical%2520images%252C%2520enabling%2520effective%250Acontext-aware%2520re-ranking.%2520A%2520key%2520finding%2520is%2520C-MIR%2527s%2520ability%2520to%2520effectively%250Alocalize%2520the%2520region%2520of%2520interest%252C%2520eliminating%2520the%2520need%2520for%2520pre-segmentation%2520of%250Adatasets%2520and%2520offering%2520a%2520computationally%2520efficient%2520alternative%2520to%2520systems%250Arelying%2520on%2520expensive%2520data%2520enrichment%2520steps.%2520C-MIR%2520demonstrates%2520promising%250Aimprovements%2520in%2520tumor%2520flagging%252C%2520achieving%2520improved%2520performance%252C%2520particularly%250Afor%2520colon%2520and%2520lung%2520tumors%2520%2528p%253C0.05%2529.%2520C-MIR%2520also%2520shows%2520potential%2520for%2520improving%250Atumor%2520staging%252C%2520warranting%2520further%2520exploration%2520of%2520its%2520capabilities.%2520Ultimately%252C%250Aour%2520work%2520seeks%2520to%2520bridge%2520the%2520gap%2520between%2520advanced%2520retrieval%2520techniques%2520and%250Atheir%2520practical%2520applications%2520in%2520healthcare%252C%2520paving%2520the%2520way%2520for%2520improved%250Adiagnostic%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging&entry.906535625=Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Matthias%20Lenga&entry.1292438233=%20%20The%20increasing%20volume%20of%20medical%20images%20poses%20challenges%20for%20radiologists%20in%0Aretrieving%20relevant%20cases.%20Content-based%20image%20retrieval%20%28CBIR%29%20systems%20offer%0Apotential%20for%20efficient%20access%20to%20similar%20cases%2C%20yet%20lack%20standardized%0Aevaluation%20and%20comprehensive%20studies.%20Building%20on%20prior%20studies%20for%20tumor%0Acharacterization%20via%20CBIR%2C%20this%20study%20advances%20CBIR%20research%20for%20volumetric%0Amedical%20images%20through%20three%20key%20contributions%3A%20%281%29%20a%20framework%20eliminating%0Areliance%20on%20pre-segmented%20data%20and%20organ-specific%20datasets%2C%20aligning%20with%20large%0Aand%20unstructured%20image%20archiving%20systems%2C%20i.e.%20PACS%20in%20clinical%20practice%3B%20%282%29%0Aintroduction%20of%20C-MIR%2C%20a%20novel%20volumetric%20re-ranking%20method%20adapting%20ColBERT%27s%0Acontextualized%20late%20interaction%20mechanism%20for%203D%20medical%20imaging%3B%20%283%29%0Acomprehensive%20evaluation%20across%20four%20tumor%20sites%20using%20three%20feature%20extractors%0Aand%20three%20database%20configurations.%20Our%20evaluations%20highlight%20the%20significant%0Aadvantages%20of%20C-MIR.%20We%20demonstrate%20the%20successful%20adaptation%20of%20the%20late%0Ainteraction%20principle%20to%20volumetric%20medical%20images%2C%20enabling%20effective%0Acontext-aware%20re-ranking.%20A%20key%20finding%20is%20C-MIR%27s%20ability%20to%20effectively%0Alocalize%20the%20region%20of%20interest%2C%20eliminating%20the%20need%20for%20pre-segmentation%20of%0Adatasets%20and%20offering%20a%20computationally%20efficient%20alternative%20to%20systems%0Arelying%20on%20expensive%20data%20enrichment%20steps.%20C-MIR%20demonstrates%20promising%0Aimprovements%20in%20tumor%20flagging%2C%20achieving%20improved%20performance%2C%20particularly%0Afor%20colon%20and%20lung%20tumors%20%28p%3C0.05%29.%20C-MIR%20also%20shows%20potential%20for%20improving%0Atumor%20staging%2C%20warranting%20further%20exploration%20of%20its%20capabilities.%20Ultimately%2C%0Aour%20work%20seeks%20to%20bridge%20the%20gap%20between%20advanced%20retrieval%20techniques%20and%0Atheir%20practical%20applications%20in%20healthcare%2C%20paving%20the%20way%20for%20improved%0Adiagnostic%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17412v1&entry.124074799=Read"},
{"title": "Probing Vision-Language Understanding through the Visual Entailment\n  Task: promises and pitfalls", "author": "Elena Pitta and Tom Kouwenhoven and Tessa Verhoef", "abstract": "  This study investigates the extent to which the Visual Entailment (VE) task\nserves as a reliable probe of vision-language understanding in multimodal\nlanguage models, using the LLaMA 3.2 11B Vision model as a test case. Beyond\nreporting performance metrics, we aim to interpret what these results reveal\nabout the underlying possibilities and limitations of the VE task. We conduct a\nseries of experiments across zero-shot, few-shot, and fine-tuning settings,\nexploring how factors such as prompt design, the number and order of in-context\nexamples and access to visual information might affect VE performance. To\nfurther probe the reasoning processes of the model, we used explanation-based\nevaluations. Results indicate that three-shot inference outperforms the\nzero-shot baselines. However, additional examples introduce more noise than\nthey provide benefits. Additionally, the order of the labels in the prompt is a\ncritical factor that influences the predictions. In the absence of visual\ninformation, the model has a strong tendency to hallucinate and imagine\ncontent, raising questions about the model's over-reliance on linguistic\npriors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on\nthe e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.\nAdditionally, the explanation evaluation demonstrates that the fine-tuned model\nprovides semantically meaningful explanations similar to those of humans, with\na BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore\nresults in experiments with limited vision, questioning the visual grounding of\nthis task. Overall, our results highlight both the utility and limitations of\nVE as a diagnostic task for vision-language understanding and point to\ndirections for refining multimodal evaluation methods.\n", "link": "http://arxiv.org/abs/2507.17467v1", "date": "2025-07-23", "relevancy": 2.6594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Vision-Language%20Understanding%20through%20the%20Visual%20Entailment%0A%20%20Task%3A%20promises%20and%20pitfalls&body=Title%3A%20Probing%20Vision-Language%20Understanding%20through%20the%20Visual%20Entailment%0A%20%20Task%3A%20promises%20and%20pitfalls%0AAuthor%3A%20Elena%20Pitta%20and%20Tom%20Kouwenhoven%20and%20Tessa%20Verhoef%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20extent%20to%20which%20the%20Visual%20Entailment%20%28VE%29%20task%0Aserves%20as%20a%20reliable%20probe%20of%20vision-language%20understanding%20in%20multimodal%0Alanguage%20models%2C%20using%20the%20LLaMA%203.2%2011B%20Vision%20model%20as%20a%20test%20case.%20Beyond%0Areporting%20performance%20metrics%2C%20we%20aim%20to%20interpret%20what%20these%20results%20reveal%0Aabout%20the%20underlying%20possibilities%20and%20limitations%20of%20the%20VE%20task.%20We%20conduct%20a%0Aseries%20of%20experiments%20across%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%2C%0Aexploring%20how%20factors%20such%20as%20prompt%20design%2C%20the%20number%20and%20order%20of%20in-context%0Aexamples%20and%20access%20to%20visual%20information%20might%20affect%20VE%20performance.%20To%0Afurther%20probe%20the%20reasoning%20processes%20of%20the%20model%2C%20we%20used%20explanation-based%0Aevaluations.%20Results%20indicate%20that%20three-shot%20inference%20outperforms%20the%0Azero-shot%20baselines.%20However%2C%20additional%20examples%20introduce%20more%20noise%20than%0Athey%20provide%20benefits.%20Additionally%2C%20the%20order%20of%20the%20labels%20in%20the%20prompt%20is%20a%0Acritical%20factor%20that%20influences%20the%20predictions.%20In%20the%20absence%20of%20visual%0Ainformation%2C%20the%20model%20has%20a%20strong%20tendency%20to%20hallucinate%20and%20imagine%0Acontent%2C%20raising%20questions%20about%20the%20model%27s%20over-reliance%20on%20linguistic%0Apriors.%20Fine-tuning%20yields%20strong%20results%2C%20achieving%20an%20accuracy%20of%2083.3%25%20on%0Athe%20e-SNLI-VE%20dataset%20and%20outperforming%20the%20state-of-the-art%20OFA-X%20model.%0AAdditionally%2C%20the%20explanation%20evaluation%20demonstrates%20that%20the%20fine-tuned%20model%0Aprovides%20semantically%20meaningful%20explanations%20similar%20to%20those%20of%20humans%2C%20with%0Aa%20BERTScore%20F1-score%20of%2089.2%25.%20We%20do%2C%20however%2C%20find%20comparable%20BERTScore%0Aresults%20in%20experiments%20with%20limited%20vision%2C%20questioning%20the%20visual%20grounding%20of%0Athis%20task.%20Overall%2C%20our%20results%20highlight%20both%20the%20utility%20and%20limitations%20of%0AVE%20as%20a%20diagnostic%20task%20for%20vision-language%20understanding%20and%20point%20to%0Adirections%20for%20refining%20multimodal%20evaluation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Vision-Language%2520Understanding%2520through%2520the%2520Visual%2520Entailment%250A%2520%2520Task%253A%2520promises%2520and%2520pitfalls%26entry.906535625%3DElena%2520Pitta%2520and%2520Tom%2520Kouwenhoven%2520and%2520Tessa%2520Verhoef%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520extent%2520to%2520which%2520the%2520Visual%2520Entailment%2520%2528VE%2529%2520task%250Aserves%2520as%2520a%2520reliable%2520probe%2520of%2520vision-language%2520understanding%2520in%2520multimodal%250Alanguage%2520models%252C%2520using%2520the%2520LLaMA%25203.2%252011B%2520Vision%2520model%2520as%2520a%2520test%2520case.%2520Beyond%250Areporting%2520performance%2520metrics%252C%2520we%2520aim%2520to%2520interpret%2520what%2520these%2520results%2520reveal%250Aabout%2520the%2520underlying%2520possibilities%2520and%2520limitations%2520of%2520the%2520VE%2520task.%2520We%2520conduct%2520a%250Aseries%2520of%2520experiments%2520across%2520zero-shot%252C%2520few-shot%252C%2520and%2520fine-tuning%2520settings%252C%250Aexploring%2520how%2520factors%2520such%2520as%2520prompt%2520design%252C%2520the%2520number%2520and%2520order%2520of%2520in-context%250Aexamples%2520and%2520access%2520to%2520visual%2520information%2520might%2520affect%2520VE%2520performance.%2520To%250Afurther%2520probe%2520the%2520reasoning%2520processes%2520of%2520the%2520model%252C%2520we%2520used%2520explanation-based%250Aevaluations.%2520Results%2520indicate%2520that%2520three-shot%2520inference%2520outperforms%2520the%250Azero-shot%2520baselines.%2520However%252C%2520additional%2520examples%2520introduce%2520more%2520noise%2520than%250Athey%2520provide%2520benefits.%2520Additionally%252C%2520the%2520order%2520of%2520the%2520labels%2520in%2520the%2520prompt%2520is%2520a%250Acritical%2520factor%2520that%2520influences%2520the%2520predictions.%2520In%2520the%2520absence%2520of%2520visual%250Ainformation%252C%2520the%2520model%2520has%2520a%2520strong%2520tendency%2520to%2520hallucinate%2520and%2520imagine%250Acontent%252C%2520raising%2520questions%2520about%2520the%2520model%2527s%2520over-reliance%2520on%2520linguistic%250Apriors.%2520Fine-tuning%2520yields%2520strong%2520results%252C%2520achieving%2520an%2520accuracy%2520of%252083.3%2525%2520on%250Athe%2520e-SNLI-VE%2520dataset%2520and%2520outperforming%2520the%2520state-of-the-art%2520OFA-X%2520model.%250AAdditionally%252C%2520the%2520explanation%2520evaluation%2520demonstrates%2520that%2520the%2520fine-tuned%2520model%250Aprovides%2520semantically%2520meaningful%2520explanations%2520similar%2520to%2520those%2520of%2520humans%252C%2520with%250Aa%2520BERTScore%2520F1-score%2520of%252089.2%2525.%2520We%2520do%252C%2520however%252C%2520find%2520comparable%2520BERTScore%250Aresults%2520in%2520experiments%2520with%2520limited%2520vision%252C%2520questioning%2520the%2520visual%2520grounding%2520of%250Athis%2520task.%2520Overall%252C%2520our%2520results%2520highlight%2520both%2520the%2520utility%2520and%2520limitations%2520of%250AVE%2520as%2520a%2520diagnostic%2520task%2520for%2520vision-language%2520understanding%2520and%2520point%2520to%250Adirections%2520for%2520refining%2520multimodal%2520evaluation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Vision-Language%20Understanding%20through%20the%20Visual%20Entailment%0A%20%20Task%3A%20promises%20and%20pitfalls&entry.906535625=Elena%20Pitta%20and%20Tom%20Kouwenhoven%20and%20Tessa%20Verhoef&entry.1292438233=%20%20This%20study%20investigates%20the%20extent%20to%20which%20the%20Visual%20Entailment%20%28VE%29%20task%0Aserves%20as%20a%20reliable%20probe%20of%20vision-language%20understanding%20in%20multimodal%0Alanguage%20models%2C%20using%20the%20LLaMA%203.2%2011B%20Vision%20model%20as%20a%20test%20case.%20Beyond%0Areporting%20performance%20metrics%2C%20we%20aim%20to%20interpret%20what%20these%20results%20reveal%0Aabout%20the%20underlying%20possibilities%20and%20limitations%20of%20the%20VE%20task.%20We%20conduct%20a%0Aseries%20of%20experiments%20across%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%2C%0Aexploring%20how%20factors%20such%20as%20prompt%20design%2C%20the%20number%20and%20order%20of%20in-context%0Aexamples%20and%20access%20to%20visual%20information%20might%20affect%20VE%20performance.%20To%0Afurther%20probe%20the%20reasoning%20processes%20of%20the%20model%2C%20we%20used%20explanation-based%0Aevaluations.%20Results%20indicate%20that%20three-shot%20inference%20outperforms%20the%0Azero-shot%20baselines.%20However%2C%20additional%20examples%20introduce%20more%20noise%20than%0Athey%20provide%20benefits.%20Additionally%2C%20the%20order%20of%20the%20labels%20in%20the%20prompt%20is%20a%0Acritical%20factor%20that%20influences%20the%20predictions.%20In%20the%20absence%20of%20visual%0Ainformation%2C%20the%20model%20has%20a%20strong%20tendency%20to%20hallucinate%20and%20imagine%0Acontent%2C%20raising%20questions%20about%20the%20model%27s%20over-reliance%20on%20linguistic%0Apriors.%20Fine-tuning%20yields%20strong%20results%2C%20achieving%20an%20accuracy%20of%2083.3%25%20on%0Athe%20e-SNLI-VE%20dataset%20and%20outperforming%20the%20state-of-the-art%20OFA-X%20model.%0AAdditionally%2C%20the%20explanation%20evaluation%20demonstrates%20that%20the%20fine-tuned%20model%0Aprovides%20semantically%20meaningful%20explanations%20similar%20to%20those%20of%20humans%2C%20with%0Aa%20BERTScore%20F1-score%20of%2089.2%25.%20We%20do%2C%20however%2C%20find%20comparable%20BERTScore%0Aresults%20in%20experiments%20with%20limited%20vision%2C%20questioning%20the%20visual%20grounding%20of%0Athis%20task.%20Overall%2C%20our%20results%20highlight%20both%20the%20utility%20and%20limitations%20of%0AVE%20as%20a%20diagnostic%20task%20for%20vision-language%20understanding%20and%20point%20to%0Adirections%20for%20refining%20multimodal%20evaluation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17467v1&entry.124074799=Read"},
{"title": "The Early Bird Identifies the Worm: You Can't Beat a Head Start in\n  Long-Term Body Re-ID (ECHO-BID)", "author": "Thomas M. Metz and Matthew Q. Hill and Alice J. O'Toole", "abstract": "  Person identification in unconstrained viewing environments presents\nsignificant challenges due to variations in distance, viewpoint, imaging\nconditions, and clothing. We introduce $\\textbf{E}$va $\\textbf{C}$lothes-Change\nfrom $\\textbf{H}$idden $\\textbf{O}$bjects - $\\textbf{B}$ody\n$\\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built\non object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other\nmodels that vary systematically in backbone architecture, model size, scale of\nobject classification pretraining, and transfer learning protocol. Models were\nevaluated on benchmark datasets across constrained, unconstrained, and occluded\nsettings. ECHO-BID, with transfer learning on the most challenging\nclothes-change data, achieved state-of-the-art results on long-term re-id --\nsubstantially outperforming other methods. ECHO-BID also surpassed other\nmethods by a wide margin in occluded viewing scenarios. A combination of\nincreased model size and Masked Image Modeling during pretraining underlie\nECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more\nchallenging transfer learning dataset, generalized better across datasets than\na larger, less challenging one. However, the larger dataset with an additional\nfine-tuning step proved best on the most difficult data. Selecting the correct\npretrained backbone architecture and transfer learning protocols can drive\nsubstantial gains in long-term re-id performance.\n", "link": "http://arxiv.org/abs/2507.17640v1", "date": "2025-07-23", "relevancy": 2.6567, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Early%20Bird%20Identifies%20the%20Worm%3A%20You%20Can%27t%20Beat%20a%20Head%20Start%20in%0A%20%20Long-Term%20Body%20Re-ID%20%28ECHO-BID%29&body=Title%3A%20The%20Early%20Bird%20Identifies%20the%20Worm%3A%20You%20Can%27t%20Beat%20a%20Head%20Start%20in%0A%20%20Long-Term%20Body%20Re-ID%20%28ECHO-BID%29%0AAuthor%3A%20Thomas%20M.%20Metz%20and%20Matthew%20Q.%20Hill%20and%20Alice%20J.%20O%27Toole%0AAbstract%3A%20%20%20Person%20identification%20in%20unconstrained%20viewing%20environments%20presents%0Asignificant%20challenges%20due%20to%20variations%20in%20distance%2C%20viewpoint%2C%20imaging%0Aconditions%2C%20and%20clothing.%20We%20introduce%20%24%5Ctextbf%7BE%7D%24va%20%24%5Ctextbf%7BC%7D%24lothes-Change%0Afrom%20%24%5Ctextbf%7BH%7D%24idden%20%24%5Ctextbf%7BO%7D%24bjects%20-%20%24%5Ctextbf%7BB%7D%24ody%0A%24%5Ctextbf%7BID%7D%24entification%20%28ECHO-BID%29%2C%20a%20class%20of%20long-term%20re-id%20models%20built%0Aon%20object-pretrained%20EVA-02%20Large%20backbones.%20We%20compare%20ECHO-BID%20to%209%20other%0Amodels%20that%20vary%20systematically%20in%20backbone%20architecture%2C%20model%20size%2C%20scale%20of%0Aobject%20classification%20pretraining%2C%20and%20transfer%20learning%20protocol.%20Models%20were%0Aevaluated%20on%20benchmark%20datasets%20across%20constrained%2C%20unconstrained%2C%20and%20occluded%0Asettings.%20ECHO-BID%2C%20with%20transfer%20learning%20on%20the%20most%20challenging%0Aclothes-change%20data%2C%20achieved%20state-of-the-art%20results%20on%20long-term%20re-id%20--%0Asubstantially%20outperforming%20other%20methods.%20ECHO-BID%20also%20surpassed%20other%0Amethods%20by%20a%20wide%20margin%20in%20occluded%20viewing%20scenarios.%20A%20combination%20of%0Aincreased%20model%20size%20and%20Masked%20Image%20Modeling%20during%20pretraining%20underlie%0AECHO-BID%27s%20strong%20performance%20on%20long-term%20re-id.%20Notably%2C%20a%20smaller%2C%20but%20more%0Achallenging%20transfer%20learning%20dataset%2C%20generalized%20better%20across%20datasets%20than%0Aa%20larger%2C%20less%20challenging%20one.%20However%2C%20the%20larger%20dataset%20with%20an%20additional%0Afine-tuning%20step%20proved%20best%20on%20the%20most%20difficult%20data.%20Selecting%20the%20correct%0Apretrained%20backbone%20architecture%20and%20transfer%20learning%20protocols%20can%20drive%0Asubstantial%20gains%20in%20long-term%20re-id%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Early%2520Bird%2520Identifies%2520the%2520Worm%253A%2520You%2520Can%2527t%2520Beat%2520a%2520Head%2520Start%2520in%250A%2520%2520Long-Term%2520Body%2520Re-ID%2520%2528ECHO-BID%2529%26entry.906535625%3DThomas%2520M.%2520Metz%2520and%2520Matthew%2520Q.%2520Hill%2520and%2520Alice%2520J.%2520O%2527Toole%26entry.1292438233%3D%2520%2520Person%2520identification%2520in%2520unconstrained%2520viewing%2520environments%2520presents%250Asignificant%2520challenges%2520due%2520to%2520variations%2520in%2520distance%252C%2520viewpoint%252C%2520imaging%250Aconditions%252C%2520and%2520clothing.%2520We%2520introduce%2520%2524%255Ctextbf%257BE%257D%2524va%2520%2524%255Ctextbf%257BC%257D%2524lothes-Change%250Afrom%2520%2524%255Ctextbf%257BH%257D%2524idden%2520%2524%255Ctextbf%257BO%257D%2524bjects%2520-%2520%2524%255Ctextbf%257BB%257D%2524ody%250A%2524%255Ctextbf%257BID%257D%2524entification%2520%2528ECHO-BID%2529%252C%2520a%2520class%2520of%2520long-term%2520re-id%2520models%2520built%250Aon%2520object-pretrained%2520EVA-02%2520Large%2520backbones.%2520We%2520compare%2520ECHO-BID%2520to%25209%2520other%250Amodels%2520that%2520vary%2520systematically%2520in%2520backbone%2520architecture%252C%2520model%2520size%252C%2520scale%2520of%250Aobject%2520classification%2520pretraining%252C%2520and%2520transfer%2520learning%2520protocol.%2520Models%2520were%250Aevaluated%2520on%2520benchmark%2520datasets%2520across%2520constrained%252C%2520unconstrained%252C%2520and%2520occluded%250Asettings.%2520ECHO-BID%252C%2520with%2520transfer%2520learning%2520on%2520the%2520most%2520challenging%250Aclothes-change%2520data%252C%2520achieved%2520state-of-the-art%2520results%2520on%2520long-term%2520re-id%2520--%250Asubstantially%2520outperforming%2520other%2520methods.%2520ECHO-BID%2520also%2520surpassed%2520other%250Amethods%2520by%2520a%2520wide%2520margin%2520in%2520occluded%2520viewing%2520scenarios.%2520A%2520combination%2520of%250Aincreased%2520model%2520size%2520and%2520Masked%2520Image%2520Modeling%2520during%2520pretraining%2520underlie%250AECHO-BID%2527s%2520strong%2520performance%2520on%2520long-term%2520re-id.%2520Notably%252C%2520a%2520smaller%252C%2520but%2520more%250Achallenging%2520transfer%2520learning%2520dataset%252C%2520generalized%2520better%2520across%2520datasets%2520than%250Aa%2520larger%252C%2520less%2520challenging%2520one.%2520However%252C%2520the%2520larger%2520dataset%2520with%2520an%2520additional%250Afine-tuning%2520step%2520proved%2520best%2520on%2520the%2520most%2520difficult%2520data.%2520Selecting%2520the%2520correct%250Apretrained%2520backbone%2520architecture%2520and%2520transfer%2520learning%2520protocols%2520can%2520drive%250Asubstantial%2520gains%2520in%2520long-term%2520re-id%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Early%20Bird%20Identifies%20the%20Worm%3A%20You%20Can%27t%20Beat%20a%20Head%20Start%20in%0A%20%20Long-Term%20Body%20Re-ID%20%28ECHO-BID%29&entry.906535625=Thomas%20M.%20Metz%20and%20Matthew%20Q.%20Hill%20and%20Alice%20J.%20O%27Toole&entry.1292438233=%20%20Person%20identification%20in%20unconstrained%20viewing%20environments%20presents%0Asignificant%20challenges%20due%20to%20variations%20in%20distance%2C%20viewpoint%2C%20imaging%0Aconditions%2C%20and%20clothing.%20We%20introduce%20%24%5Ctextbf%7BE%7D%24va%20%24%5Ctextbf%7BC%7D%24lothes-Change%0Afrom%20%24%5Ctextbf%7BH%7D%24idden%20%24%5Ctextbf%7BO%7D%24bjects%20-%20%24%5Ctextbf%7BB%7D%24ody%0A%24%5Ctextbf%7BID%7D%24entification%20%28ECHO-BID%29%2C%20a%20class%20of%20long-term%20re-id%20models%20built%0Aon%20object-pretrained%20EVA-02%20Large%20backbones.%20We%20compare%20ECHO-BID%20to%209%20other%0Amodels%20that%20vary%20systematically%20in%20backbone%20architecture%2C%20model%20size%2C%20scale%20of%0Aobject%20classification%20pretraining%2C%20and%20transfer%20learning%20protocol.%20Models%20were%0Aevaluated%20on%20benchmark%20datasets%20across%20constrained%2C%20unconstrained%2C%20and%20occluded%0Asettings.%20ECHO-BID%2C%20with%20transfer%20learning%20on%20the%20most%20challenging%0Aclothes-change%20data%2C%20achieved%20state-of-the-art%20results%20on%20long-term%20re-id%20--%0Asubstantially%20outperforming%20other%20methods.%20ECHO-BID%20also%20surpassed%20other%0Amethods%20by%20a%20wide%20margin%20in%20occluded%20viewing%20scenarios.%20A%20combination%20of%0Aincreased%20model%20size%20and%20Masked%20Image%20Modeling%20during%20pretraining%20underlie%0AECHO-BID%27s%20strong%20performance%20on%20long-term%20re-id.%20Notably%2C%20a%20smaller%2C%20but%20more%0Achallenging%20transfer%20learning%20dataset%2C%20generalized%20better%20across%20datasets%20than%0Aa%20larger%2C%20less%20challenging%20one.%20However%2C%20the%20larger%20dataset%20with%20an%20additional%0Afine-tuning%20step%20proved%20best%20on%20the%20most%20difficult%20data.%20Selecting%20the%20correct%0Apretrained%20backbone%20architecture%20and%20transfer%20learning%20protocols%20can%20drive%0Asubstantial%20gains%20in%20long-term%20re-id%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17640v1&entry.124074799=Read"},
{"title": "Multi-Level Explanations for Generative Language Models", "author": "Lucas Monteiro Paes and Dennis Wei and Hyo Jin Do and Hendrik Strobelt and Ronny Luss and Amit Dhurandhar and Manish Nagireddy and Karthikeyan Natesan Ramamurthy and Prasanna Sattigeri and Werner Geyer and Soumya Ghosh", "abstract": "  Despite the increasing use of large language models (LLMs) for\ncontext-grounded tasks like summarization and question-answering, understanding\nwhat makes an LLM produce a certain response is challenging. We propose\nMulti-Level Explanations for Generative Language Models (MExGen), a technique\nto provide explanations for context-grounded text generation. MExGen assigns\nscores to parts of the context to quantify their influence on the model's\noutput. It extends attribution methods like LIME and SHAP to LLMs used in\ncontext-grounded tasks where (1) inference cost is high, (2) input text is\nlong, and (3) the output is text. We conduct a systematic evaluation, both\nautomated and human, of perturbation-based attribution methods for\nsummarization and question answering. The results show that our framework can\nprovide more faithful explanations of generated output than available\nalternatives, including LLM self-explanations. We open-source code for MExGen\nas part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.\n", "link": "http://arxiv.org/abs/2403.14459v2", "date": "2025-07-23", "relevancy": 2.6428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Explanations%20for%20Generative%20Language%20Models&body=Title%3A%20Multi-Level%20Explanations%20for%20Generative%20Language%20Models%0AAuthor%3A%20Lucas%20Monteiro%20Paes%20and%20Dennis%20Wei%20and%20Hyo%20Jin%20Do%20and%20Hendrik%20Strobelt%20and%20Ronny%20Luss%20and%20Amit%20Dhurandhar%20and%20Manish%20Nagireddy%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Prasanna%20Sattigeri%20and%20Werner%20Geyer%20and%20Soumya%20Ghosh%0AAbstract%3A%20%20%20Despite%20the%20increasing%20use%20of%20large%20language%20models%20%28LLMs%29%20for%0Acontext-grounded%20tasks%20like%20summarization%20and%20question-answering%2C%20understanding%0Awhat%20makes%20an%20LLM%20produce%20a%20certain%20response%20is%20challenging.%20We%20propose%0AMulti-Level%20Explanations%20for%20Generative%20Language%20Models%20%28MExGen%29%2C%20a%20technique%0Ato%20provide%20explanations%20for%20context-grounded%20text%20generation.%20MExGen%20assigns%0Ascores%20to%20parts%20of%20the%20context%20to%20quantify%20their%20influence%20on%20the%20model%27s%0Aoutput.%20It%20extends%20attribution%20methods%20like%20LIME%20and%20SHAP%20to%20LLMs%20used%20in%0Acontext-grounded%20tasks%20where%20%281%29%20inference%20cost%20is%20high%2C%20%282%29%20input%20text%20is%0Along%2C%20and%20%283%29%20the%20output%20is%20text.%20We%20conduct%20a%20systematic%20evaluation%2C%20both%0Aautomated%20and%20human%2C%20of%20perturbation-based%20attribution%20methods%20for%0Asummarization%20and%20question%20answering.%20The%20results%20show%20that%20our%20framework%20can%0Aprovide%20more%20faithful%20explanations%20of%20generated%20output%20than%20available%0Aalternatives%2C%20including%20LLM%20self-explanations.%20We%20open-source%20code%20for%20MExGen%0Aas%20part%20of%20the%20ICX360%20toolkit%3A%20https%3A//github%24.%24com/IBM/ICX360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Level%2520Explanations%2520for%2520Generative%2520Language%2520Models%26entry.906535625%3DLucas%2520Monteiro%2520Paes%2520and%2520Dennis%2520Wei%2520and%2520Hyo%2520Jin%2520Do%2520and%2520Hendrik%2520Strobelt%2520and%2520Ronny%2520Luss%2520and%2520Amit%2520Dhurandhar%2520and%2520Manish%2520Nagireddy%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%2520and%2520Prasanna%2520Sattigeri%2520and%2520Werner%2520Geyer%2520and%2520Soumya%2520Ghosh%26entry.1292438233%3D%2520%2520Despite%2520the%2520increasing%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%250Acontext-grounded%2520tasks%2520like%2520summarization%2520and%2520question-answering%252C%2520understanding%250Awhat%2520makes%2520an%2520LLM%2520produce%2520a%2520certain%2520response%2520is%2520challenging.%2520We%2520propose%250AMulti-Level%2520Explanations%2520for%2520Generative%2520Language%2520Models%2520%2528MExGen%2529%252C%2520a%2520technique%250Ato%2520provide%2520explanations%2520for%2520context-grounded%2520text%2520generation.%2520MExGen%2520assigns%250Ascores%2520to%2520parts%2520of%2520the%2520context%2520to%2520quantify%2520their%2520influence%2520on%2520the%2520model%2527s%250Aoutput.%2520It%2520extends%2520attribution%2520methods%2520like%2520LIME%2520and%2520SHAP%2520to%2520LLMs%2520used%2520in%250Acontext-grounded%2520tasks%2520where%2520%25281%2529%2520inference%2520cost%2520is%2520high%252C%2520%25282%2529%2520input%2520text%2520is%250Along%252C%2520and%2520%25283%2529%2520the%2520output%2520is%2520text.%2520We%2520conduct%2520a%2520systematic%2520evaluation%252C%2520both%250Aautomated%2520and%2520human%252C%2520of%2520perturbation-based%2520attribution%2520methods%2520for%250Asummarization%2520and%2520question%2520answering.%2520The%2520results%2520show%2520that%2520our%2520framework%2520can%250Aprovide%2520more%2520faithful%2520explanations%2520of%2520generated%2520output%2520than%2520available%250Aalternatives%252C%2520including%2520LLM%2520self-explanations.%2520We%2520open-source%2520code%2520for%2520MExGen%250Aas%2520part%2520of%2520the%2520ICX360%2520toolkit%253A%2520https%253A//github%2524.%2524com/IBM/ICX360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Explanations%20for%20Generative%20Language%20Models&entry.906535625=Lucas%20Monteiro%20Paes%20and%20Dennis%20Wei%20and%20Hyo%20Jin%20Do%20and%20Hendrik%20Strobelt%20and%20Ronny%20Luss%20and%20Amit%20Dhurandhar%20and%20Manish%20Nagireddy%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Prasanna%20Sattigeri%20and%20Werner%20Geyer%20and%20Soumya%20Ghosh&entry.1292438233=%20%20Despite%20the%20increasing%20use%20of%20large%20language%20models%20%28LLMs%29%20for%0Acontext-grounded%20tasks%20like%20summarization%20and%20question-answering%2C%20understanding%0Awhat%20makes%20an%20LLM%20produce%20a%20certain%20response%20is%20challenging.%20We%20propose%0AMulti-Level%20Explanations%20for%20Generative%20Language%20Models%20%28MExGen%29%2C%20a%20technique%0Ato%20provide%20explanations%20for%20context-grounded%20text%20generation.%20MExGen%20assigns%0Ascores%20to%20parts%20of%20the%20context%20to%20quantify%20their%20influence%20on%20the%20model%27s%0Aoutput.%20It%20extends%20attribution%20methods%20like%20LIME%20and%20SHAP%20to%20LLMs%20used%20in%0Acontext-grounded%20tasks%20where%20%281%29%20inference%20cost%20is%20high%2C%20%282%29%20input%20text%20is%0Along%2C%20and%20%283%29%20the%20output%20is%20text.%20We%20conduct%20a%20systematic%20evaluation%2C%20both%0Aautomated%20and%20human%2C%20of%20perturbation-based%20attribution%20methods%20for%0Asummarization%20and%20question%20answering.%20The%20results%20show%20that%20our%20framework%20can%0Aprovide%20more%20faithful%20explanations%20of%20generated%20output%20than%20available%0Aalternatives%2C%20including%20LLM%20self-explanations.%20We%20open-source%20code%20for%20MExGen%0Aas%20part%20of%20the%20ICX360%20toolkit%3A%20https%3A//github%24.%24com/IBM/ICX360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14459v2&entry.124074799=Read"},
{"title": "Audio-Vision Contrastive Learning for Phonological Class Recognition", "author": "Daiqi Liu and Tom\u00e1s Arias-Vergara and Jana Hutter and Andreas Maier and Paula Andrea P\u00e9rez-Toro", "abstract": "  Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.\n", "link": "http://arxiv.org/abs/2507.17682v1", "date": "2025-07-23", "relevancy": 2.595, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Vision%20Contrastive%20Learning%20for%20Phonological%20Class%20Recognition&body=Title%3A%20Audio-Vision%20Contrastive%20Learning%20for%20Phonological%20Class%20Recognition%0AAuthor%3A%20Daiqi%20Liu%20and%20Tom%C3%A1s%20Arias-Vergara%20and%20Jana%20Hutter%20and%20Andreas%20Maier%20and%20Paula%20Andrea%20P%C3%A9rez-Toro%0AAbstract%3A%20%20%20Accurate%20classification%20of%20articulatory-phonological%20features%20plays%20a%20vital%0Arole%20in%20understanding%20human%20speech%20production%20and%20developing%20robust%20speech%0Atechnologies%2C%20particularly%20in%20clinical%20contexts%20where%20targeted%20phonemic%0Aanalysis%20and%20therapy%20can%20improve%20disease%20diagnosis%20accuracy%20and%20personalized%0Arehabilitation.%20In%20this%20work%2C%20we%20propose%20a%20multimodal%20deep%20learning%20framework%0Athat%20combines%20real-time%20magnetic%20resonance%20imaging%20%28rtMRI%29%20and%20speech%20signals%0Ato%20classify%20three%20key%20articulatory%20dimensions%3A%20manner%20of%20articulation%2C%20place%20of%0Aarticulation%2C%20and%20voicing.%20We%20perform%20classification%20on%2015%20phonological%20classes%0Aderived%20from%20the%20aforementioned%20articulatory%20dimensions%20and%20evaluate%20the%20system%0Awith%20four%20audio/vision%20configurations%3A%20unimodal%20rtMRI%2C%20unimodal%20audio%20signals%2C%0Amultimodal%20middle%20fusion%2C%20and%20contrastive%20learning-based%20audio-vision%20fusion.%0AExperimental%20results%20on%20the%20USC-TIMIT%20dataset%20show%20that%20our%20contrastive%0Alearning-based%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20an%20average%0AF1-score%20of%200.81%2C%20representing%20an%20absolute%20increase%20of%200.23%20over%20the%20unimodal%0Abaseline.%20The%20results%20confirm%20the%20effectiveness%20of%20contrastive%20representation%0Alearning%20for%20multimodal%20articulatory%20analysis.%20Our%20code%20and%20processed%20dataset%0Awill%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/DaE-plz/AC_Contrastive_Phonology%20to%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Vision%2520Contrastive%2520Learning%2520for%2520Phonological%2520Class%2520Recognition%26entry.906535625%3DDaiqi%2520Liu%2520and%2520Tom%25C3%25A1s%2520Arias-Vergara%2520and%2520Jana%2520Hutter%2520and%2520Andreas%2520Maier%2520and%2520Paula%2520Andrea%2520P%25C3%25A9rez-Toro%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520articulatory-phonological%2520features%2520plays%2520a%2520vital%250Arole%2520in%2520understanding%2520human%2520speech%2520production%2520and%2520developing%2520robust%2520speech%250Atechnologies%252C%2520particularly%2520in%2520clinical%2520contexts%2520where%2520targeted%2520phonemic%250Aanalysis%2520and%2520therapy%2520can%2520improve%2520disease%2520diagnosis%2520accuracy%2520and%2520personalized%250Arehabilitation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520multimodal%2520deep%2520learning%2520framework%250Athat%2520combines%2520real-time%2520magnetic%2520resonance%2520imaging%2520%2528rtMRI%2529%2520and%2520speech%2520signals%250Ato%2520classify%2520three%2520key%2520articulatory%2520dimensions%253A%2520manner%2520of%2520articulation%252C%2520place%2520of%250Aarticulation%252C%2520and%2520voicing.%2520We%2520perform%2520classification%2520on%252015%2520phonological%2520classes%250Aderived%2520from%2520the%2520aforementioned%2520articulatory%2520dimensions%2520and%2520evaluate%2520the%2520system%250Awith%2520four%2520audio/vision%2520configurations%253A%2520unimodal%2520rtMRI%252C%2520unimodal%2520audio%2520signals%252C%250Amultimodal%2520middle%2520fusion%252C%2520and%2520contrastive%2520learning-based%2520audio-vision%2520fusion.%250AExperimental%2520results%2520on%2520the%2520USC-TIMIT%2520dataset%2520show%2520that%2520our%2520contrastive%250Alearning-based%2520approach%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520an%2520average%250AF1-score%2520of%25200.81%252C%2520representing%2520an%2520absolute%2520increase%2520of%25200.23%2520over%2520the%2520unimodal%250Abaseline.%2520The%2520results%2520confirm%2520the%2520effectiveness%2520of%2520contrastive%2520representation%250Alearning%2520for%2520multimodal%2520articulatory%2520analysis.%2520Our%2520code%2520and%2520processed%2520dataset%250Awill%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/DaE-plz/AC_Contrastive_Phonology%2520to%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Vision%20Contrastive%20Learning%20for%20Phonological%20Class%20Recognition&entry.906535625=Daiqi%20Liu%20and%20Tom%C3%A1s%20Arias-Vergara%20and%20Jana%20Hutter%20and%20Andreas%20Maier%20and%20Paula%20Andrea%20P%C3%A9rez-Toro&entry.1292438233=%20%20Accurate%20classification%20of%20articulatory-phonological%20features%20plays%20a%20vital%0Arole%20in%20understanding%20human%20speech%20production%20and%20developing%20robust%20speech%0Atechnologies%2C%20particularly%20in%20clinical%20contexts%20where%20targeted%20phonemic%0Aanalysis%20and%20therapy%20can%20improve%20disease%20diagnosis%20accuracy%20and%20personalized%0Arehabilitation.%20In%20this%20work%2C%20we%20propose%20a%20multimodal%20deep%20learning%20framework%0Athat%20combines%20real-time%20magnetic%20resonance%20imaging%20%28rtMRI%29%20and%20speech%20signals%0Ato%20classify%20three%20key%20articulatory%20dimensions%3A%20manner%20of%20articulation%2C%20place%20of%0Aarticulation%2C%20and%20voicing.%20We%20perform%20classification%20on%2015%20phonological%20classes%0Aderived%20from%20the%20aforementioned%20articulatory%20dimensions%20and%20evaluate%20the%20system%0Awith%20four%20audio/vision%20configurations%3A%20unimodal%20rtMRI%2C%20unimodal%20audio%20signals%2C%0Amultimodal%20middle%20fusion%2C%20and%20contrastive%20learning-based%20audio-vision%20fusion.%0AExperimental%20results%20on%20the%20USC-TIMIT%20dataset%20show%20that%20our%20contrastive%0Alearning-based%20approach%20achieves%20state-of-the-art%20performance%2C%20with%20an%20average%0AF1-score%20of%200.81%2C%20representing%20an%20absolute%20increase%20of%200.23%20over%20the%20unimodal%0Abaseline.%20The%20results%20confirm%20the%20effectiveness%20of%20contrastive%20representation%0Alearning%20for%20multimodal%20articulatory%20analysis.%20Our%20code%20and%20processed%20dataset%0Awill%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/DaE-plz/AC_Contrastive_Phonology%20to%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17682v1&entry.124074799=Read"},
{"title": "LTLZinc: a Benchmarking Framework for Continual Learning and\n  Neuro-Symbolic Temporal Reasoning", "author": "Luca Salvatore Lorello and Nikolaos Manginas and Marco Lippi and Stefano Melacci", "abstract": "  Neuro-symbolic artificial intelligence aims to combine neural architectures\nwith symbolic approaches that can represent knowledge in a human-interpretable\nformalism. Continual learning concerns with agents that expand their knowledge\nover time, improving their skills while avoiding to forget previously learned\nconcepts. Most of the existing approaches for neuro-symbolic artificial\nintelligence are applied to static scenarios only, and the challenging setting\nwhere reasoning along the temporal dimension is necessary has been seldom\nexplored. In this work we introduce LTLZinc, a benchmarking framework that can\nbe used to generate datasets covering a variety of different problems, against\nwhich neuro-symbolic and continual learning methods can be evaluated along the\ntemporal and constraint-driven dimensions. Our framework generates expressive\ntemporal reasoning and continual learning tasks from a linear temporal logic\nspecification over MiniZinc constraints, and arbitrary image classification\ndatasets. Fine-grained annotations allow multiple neural and neuro-symbolic\ntraining settings on the same generated datasets. Experiments on six\nneuro-symbolic sequence classification and four class-continual learning tasks\ngenerated by LTLZinc, demonstrate the challenging nature of temporal learning\nand reasoning, and highlight limitations of current state-of-the-art methods.\nWe release the LTLZinc generator and ten ready-to-use tasks to the\nneuro-symbolic and continual learning communities, in the hope of fostering\nresearch towards unified temporal learning and reasoning frameworks.\n", "link": "http://arxiv.org/abs/2507.17482v1", "date": "2025-07-23", "relevancy": 2.5727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTLZinc%3A%20a%20Benchmarking%20Framework%20for%20Continual%20Learning%20and%0A%20%20Neuro-Symbolic%20Temporal%20Reasoning&body=Title%3A%20LTLZinc%3A%20a%20Benchmarking%20Framework%20for%20Continual%20Learning%20and%0A%20%20Neuro-Symbolic%20Temporal%20Reasoning%0AAuthor%3A%20Luca%20Salvatore%20Lorello%20and%20Nikolaos%20Manginas%20and%20Marco%20Lippi%20and%20Stefano%20Melacci%0AAbstract%3A%20%20%20Neuro-symbolic%20artificial%20intelligence%20aims%20to%20combine%20neural%20architectures%0Awith%20symbolic%20approaches%20that%20can%20represent%20knowledge%20in%20a%20human-interpretable%0Aformalism.%20Continual%20learning%20concerns%20with%20agents%20that%20expand%20their%20knowledge%0Aover%20time%2C%20improving%20their%20skills%20while%20avoiding%20to%20forget%20previously%20learned%0Aconcepts.%20Most%20of%20the%20existing%20approaches%20for%20neuro-symbolic%20artificial%0Aintelligence%20are%20applied%20to%20static%20scenarios%20only%2C%20and%20the%20challenging%20setting%0Awhere%20reasoning%20along%20the%20temporal%20dimension%20is%20necessary%20has%20been%20seldom%0Aexplored.%20In%20this%20work%20we%20introduce%20LTLZinc%2C%20a%20benchmarking%20framework%20that%20can%0Abe%20used%20to%20generate%20datasets%20covering%20a%20variety%20of%20different%20problems%2C%20against%0Awhich%20neuro-symbolic%20and%20continual%20learning%20methods%20can%20be%20evaluated%20along%20the%0Atemporal%20and%20constraint-driven%20dimensions.%20Our%20framework%20generates%20expressive%0Atemporal%20reasoning%20and%20continual%20learning%20tasks%20from%20a%20linear%20temporal%20logic%0Aspecification%20over%20MiniZinc%20constraints%2C%20and%20arbitrary%20image%20classification%0Adatasets.%20Fine-grained%20annotations%20allow%20multiple%20neural%20and%20neuro-symbolic%0Atraining%20settings%20on%20the%20same%20generated%20datasets.%20Experiments%20on%20six%0Aneuro-symbolic%20sequence%20classification%20and%20four%20class-continual%20learning%20tasks%0Agenerated%20by%20LTLZinc%2C%20demonstrate%20the%20challenging%20nature%20of%20temporal%20learning%0Aand%20reasoning%2C%20and%20highlight%20limitations%20of%20current%20state-of-the-art%20methods.%0AWe%20release%20the%20LTLZinc%20generator%20and%20ten%20ready-to-use%20tasks%20to%20the%0Aneuro-symbolic%20and%20continual%20learning%20communities%2C%20in%20the%20hope%20of%20fostering%0Aresearch%20towards%20unified%20temporal%20learning%20and%20reasoning%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTLZinc%253A%2520a%2520Benchmarking%2520Framework%2520for%2520Continual%2520Learning%2520and%250A%2520%2520Neuro-Symbolic%2520Temporal%2520Reasoning%26entry.906535625%3DLuca%2520Salvatore%2520Lorello%2520and%2520Nikolaos%2520Manginas%2520and%2520Marco%2520Lippi%2520and%2520Stefano%2520Melacci%26entry.1292438233%3D%2520%2520Neuro-symbolic%2520artificial%2520intelligence%2520aims%2520to%2520combine%2520neural%2520architectures%250Awith%2520symbolic%2520approaches%2520that%2520can%2520represent%2520knowledge%2520in%2520a%2520human-interpretable%250Aformalism.%2520Continual%2520learning%2520concerns%2520with%2520agents%2520that%2520expand%2520their%2520knowledge%250Aover%2520time%252C%2520improving%2520their%2520skills%2520while%2520avoiding%2520to%2520forget%2520previously%2520learned%250Aconcepts.%2520Most%2520of%2520the%2520existing%2520approaches%2520for%2520neuro-symbolic%2520artificial%250Aintelligence%2520are%2520applied%2520to%2520static%2520scenarios%2520only%252C%2520and%2520the%2520challenging%2520setting%250Awhere%2520reasoning%2520along%2520the%2520temporal%2520dimension%2520is%2520necessary%2520has%2520been%2520seldom%250Aexplored.%2520In%2520this%2520work%2520we%2520introduce%2520LTLZinc%252C%2520a%2520benchmarking%2520framework%2520that%2520can%250Abe%2520used%2520to%2520generate%2520datasets%2520covering%2520a%2520variety%2520of%2520different%2520problems%252C%2520against%250Awhich%2520neuro-symbolic%2520and%2520continual%2520learning%2520methods%2520can%2520be%2520evaluated%2520along%2520the%250Atemporal%2520and%2520constraint-driven%2520dimensions.%2520Our%2520framework%2520generates%2520expressive%250Atemporal%2520reasoning%2520and%2520continual%2520learning%2520tasks%2520from%2520a%2520linear%2520temporal%2520logic%250Aspecification%2520over%2520MiniZinc%2520constraints%252C%2520and%2520arbitrary%2520image%2520classification%250Adatasets.%2520Fine-grained%2520annotations%2520allow%2520multiple%2520neural%2520and%2520neuro-symbolic%250Atraining%2520settings%2520on%2520the%2520same%2520generated%2520datasets.%2520Experiments%2520on%2520six%250Aneuro-symbolic%2520sequence%2520classification%2520and%2520four%2520class-continual%2520learning%2520tasks%250Agenerated%2520by%2520LTLZinc%252C%2520demonstrate%2520the%2520challenging%2520nature%2520of%2520temporal%2520learning%250Aand%2520reasoning%252C%2520and%2520highlight%2520limitations%2520of%2520current%2520state-of-the-art%2520methods.%250AWe%2520release%2520the%2520LTLZinc%2520generator%2520and%2520ten%2520ready-to-use%2520tasks%2520to%2520the%250Aneuro-symbolic%2520and%2520continual%2520learning%2520communities%252C%2520in%2520the%2520hope%2520of%2520fostering%250Aresearch%2520towards%2520unified%2520temporal%2520learning%2520and%2520reasoning%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTLZinc%3A%20a%20Benchmarking%20Framework%20for%20Continual%20Learning%20and%0A%20%20Neuro-Symbolic%20Temporal%20Reasoning&entry.906535625=Luca%20Salvatore%20Lorello%20and%20Nikolaos%20Manginas%20and%20Marco%20Lippi%20and%20Stefano%20Melacci&entry.1292438233=%20%20Neuro-symbolic%20artificial%20intelligence%20aims%20to%20combine%20neural%20architectures%0Awith%20symbolic%20approaches%20that%20can%20represent%20knowledge%20in%20a%20human-interpretable%0Aformalism.%20Continual%20learning%20concerns%20with%20agents%20that%20expand%20their%20knowledge%0Aover%20time%2C%20improving%20their%20skills%20while%20avoiding%20to%20forget%20previously%20learned%0Aconcepts.%20Most%20of%20the%20existing%20approaches%20for%20neuro-symbolic%20artificial%0Aintelligence%20are%20applied%20to%20static%20scenarios%20only%2C%20and%20the%20challenging%20setting%0Awhere%20reasoning%20along%20the%20temporal%20dimension%20is%20necessary%20has%20been%20seldom%0Aexplored.%20In%20this%20work%20we%20introduce%20LTLZinc%2C%20a%20benchmarking%20framework%20that%20can%0Abe%20used%20to%20generate%20datasets%20covering%20a%20variety%20of%20different%20problems%2C%20against%0Awhich%20neuro-symbolic%20and%20continual%20learning%20methods%20can%20be%20evaluated%20along%20the%0Atemporal%20and%20constraint-driven%20dimensions.%20Our%20framework%20generates%20expressive%0Atemporal%20reasoning%20and%20continual%20learning%20tasks%20from%20a%20linear%20temporal%20logic%0Aspecification%20over%20MiniZinc%20constraints%2C%20and%20arbitrary%20image%20classification%0Adatasets.%20Fine-grained%20annotations%20allow%20multiple%20neural%20and%20neuro-symbolic%0Atraining%20settings%20on%20the%20same%20generated%20datasets.%20Experiments%20on%20six%0Aneuro-symbolic%20sequence%20classification%20and%20four%20class-continual%20learning%20tasks%0Agenerated%20by%20LTLZinc%2C%20demonstrate%20the%20challenging%20nature%20of%20temporal%20learning%0Aand%20reasoning%2C%20and%20highlight%20limitations%20of%20current%20state-of-the-art%20methods.%0AWe%20release%20the%20LTLZinc%20generator%20and%20ten%20ready-to-use%20tasks%20to%20the%0Aneuro-symbolic%20and%20continual%20learning%20communities%2C%20in%20the%20hope%20of%20fostering%0Aresearch%20towards%20unified%20temporal%20learning%20and%20reasoning%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17482v1&entry.124074799=Read"},
{"title": "Continual Generalized Category Discovery: Learning and Forgetting from a\n  Bayesian Perspective", "author": "Hao Dai and Jagmohan Chauhan", "abstract": "  Continual Generalized Category Discovery (C-GCD) faces a critical challenge:\nincrementally learning new classes from unlabeled data streams while preserving\nknowledge of old classes. Existing methods struggle with catastrophic\nforgetting, especially when unlabeled data mixes known and novel categories. We\naddress this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,\nrevealing that covariance misalignment between old and new classes drives\nperformance degradation. Building on this insight, we propose Variational Bayes\nC-GCD (VB-CGCD), a novel framework that integrates variational inference with\ncovariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns\nclass distributions while suppressing pseudo-label noise via stochastic\nvariational updates. Experiments show VB-CGCD surpasses prior art by +15.21%\nwith the overall accuracy in the final session on standard benchmarks. We also\nintroduce a new challenging benchmark with only 10% labeled data and extended\nonline phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher\nthan state-of-the-art (38.55%), demonstrating its robust applicability across\ndiverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD\n", "link": "http://arxiv.org/abs/2507.17382v1", "date": "2025-07-23", "relevancy": 2.5403, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.505}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Generalized%20Category%20Discovery%3A%20Learning%20and%20Forgetting%20from%20a%0A%20%20Bayesian%20Perspective&body=Title%3A%20Continual%20Generalized%20Category%20Discovery%3A%20Learning%20and%20Forgetting%20from%20a%0A%20%20Bayesian%20Perspective%0AAuthor%3A%20Hao%20Dai%20and%20Jagmohan%20Chauhan%0AAbstract%3A%20%20%20Continual%20Generalized%20Category%20Discovery%20%28C-GCD%29%20faces%20a%20critical%20challenge%3A%0Aincrementally%20learning%20new%20classes%20from%20unlabeled%20data%20streams%20while%20preserving%0Aknowledge%20of%20old%20classes.%20Existing%20methods%20struggle%20with%20catastrophic%0Aforgetting%2C%20especially%20when%20unlabeled%20data%20mixes%20known%20and%20novel%20categories.%20We%0Aaddress%20this%20by%20analyzing%20C-GCD%27s%20forgetting%20dynamics%20through%20a%20Bayesian%20lens%2C%0Arevealing%20that%20covariance%20misalignment%20between%20old%20and%20new%20classes%20drives%0Aperformance%20degradation.%20Building%20on%20this%20insight%2C%20we%20propose%20Variational%20Bayes%0AC-GCD%20%28VB-CGCD%29%2C%20a%20novel%20framework%20that%20integrates%20variational%20inference%20with%0Acovariance-aware%20nearest-class-mean%20classification.%20VB-CGCD%20adaptively%20aligns%0Aclass%20distributions%20while%20suppressing%20pseudo-label%20noise%20via%20stochastic%0Avariational%20updates.%20Experiments%20show%20VB-CGCD%20surpasses%20prior%20art%20by%20%2B15.21%25%0Awith%20the%20overall%20accuracy%20in%20the%20final%20session%20on%20standard%20benchmarks.%20We%20also%0Aintroduce%20a%20new%20challenging%20benchmark%20with%20only%2010%25%20labeled%20data%20and%20extended%0Aonline%20phases%2C%20VB-CGCD%20achieves%20a%2067.86%25%20final%20accuracy%2C%20significantly%20higher%0Athan%20state-of-the-art%20%2838.55%25%29%2C%20demonstrating%20its%20robust%20applicability%20across%0Adiverse%20scenarios.%20Code%20is%20available%20at%3A%20https%3A//github.com/daihao42/VB-CGCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Generalized%2520Category%2520Discovery%253A%2520Learning%2520and%2520Forgetting%2520from%2520a%250A%2520%2520Bayesian%2520Perspective%26entry.906535625%3DHao%2520Dai%2520and%2520Jagmohan%2520Chauhan%26entry.1292438233%3D%2520%2520Continual%2520Generalized%2520Category%2520Discovery%2520%2528C-GCD%2529%2520faces%2520a%2520critical%2520challenge%253A%250Aincrementally%2520learning%2520new%2520classes%2520from%2520unlabeled%2520data%2520streams%2520while%2520preserving%250Aknowledge%2520of%2520old%2520classes.%2520Existing%2520methods%2520struggle%2520with%2520catastrophic%250Aforgetting%252C%2520especially%2520when%2520unlabeled%2520data%2520mixes%2520known%2520and%2520novel%2520categories.%2520We%250Aaddress%2520this%2520by%2520analyzing%2520C-GCD%2527s%2520forgetting%2520dynamics%2520through%2520a%2520Bayesian%2520lens%252C%250Arevealing%2520that%2520covariance%2520misalignment%2520between%2520old%2520and%2520new%2520classes%2520drives%250Aperformance%2520degradation.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Variational%2520Bayes%250AC-GCD%2520%2528VB-CGCD%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520variational%2520inference%2520with%250Acovariance-aware%2520nearest-class-mean%2520classification.%2520VB-CGCD%2520adaptively%2520aligns%250Aclass%2520distributions%2520while%2520suppressing%2520pseudo-label%2520noise%2520via%2520stochastic%250Avariational%2520updates.%2520Experiments%2520show%2520VB-CGCD%2520surpasses%2520prior%2520art%2520by%2520%252B15.21%2525%250Awith%2520the%2520overall%2520accuracy%2520in%2520the%2520final%2520session%2520on%2520standard%2520benchmarks.%2520We%2520also%250Aintroduce%2520a%2520new%2520challenging%2520benchmark%2520with%2520only%252010%2525%2520labeled%2520data%2520and%2520extended%250Aonline%2520phases%252C%2520VB-CGCD%2520achieves%2520a%252067.86%2525%2520final%2520accuracy%252C%2520significantly%2520higher%250Athan%2520state-of-the-art%2520%252838.55%2525%2529%252C%2520demonstrating%2520its%2520robust%2520applicability%2520across%250Adiverse%2520scenarios.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/daihao42/VB-CGCD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Generalized%20Category%20Discovery%3A%20Learning%20and%20Forgetting%20from%20a%0A%20%20Bayesian%20Perspective&entry.906535625=Hao%20Dai%20and%20Jagmohan%20Chauhan&entry.1292438233=%20%20Continual%20Generalized%20Category%20Discovery%20%28C-GCD%29%20faces%20a%20critical%20challenge%3A%0Aincrementally%20learning%20new%20classes%20from%20unlabeled%20data%20streams%20while%20preserving%0Aknowledge%20of%20old%20classes.%20Existing%20methods%20struggle%20with%20catastrophic%0Aforgetting%2C%20especially%20when%20unlabeled%20data%20mixes%20known%20and%20novel%20categories.%20We%0Aaddress%20this%20by%20analyzing%20C-GCD%27s%20forgetting%20dynamics%20through%20a%20Bayesian%20lens%2C%0Arevealing%20that%20covariance%20misalignment%20between%20old%20and%20new%20classes%20drives%0Aperformance%20degradation.%20Building%20on%20this%20insight%2C%20we%20propose%20Variational%20Bayes%0AC-GCD%20%28VB-CGCD%29%2C%20a%20novel%20framework%20that%20integrates%20variational%20inference%20with%0Acovariance-aware%20nearest-class-mean%20classification.%20VB-CGCD%20adaptively%20aligns%0Aclass%20distributions%20while%20suppressing%20pseudo-label%20noise%20via%20stochastic%0Avariational%20updates.%20Experiments%20show%20VB-CGCD%20surpasses%20prior%20art%20by%20%2B15.21%25%0Awith%20the%20overall%20accuracy%20in%20the%20final%20session%20on%20standard%20benchmarks.%20We%20also%0Aintroduce%20a%20new%20challenging%20benchmark%20with%20only%2010%25%20labeled%20data%20and%20extended%0Aonline%20phases%2C%20VB-CGCD%20achieves%20a%2067.86%25%20final%20accuracy%2C%20significantly%20higher%0Athan%20state-of-the-art%20%2838.55%25%29%2C%20demonstrating%20its%20robust%20applicability%20across%0Adiverse%20scenarios.%20Code%20is%20available%20at%3A%20https%3A//github.com/daihao42/VB-CGCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17382v1&entry.124074799=Read"},
{"title": "Physics-based Human Pose Estimation from a Single Moving RGB Camera", "author": "Ayce Idil Aytekin and Chuqiao Li and Diogo Luvizon and Rishabh Dabral and Martin Oswald and Marc Habermann and Christian Theobalt", "abstract": "  Most monocular and physics-based human pose tracking methods, while achieving\nstate-of-the-art results, suffer from artifacts when the scene does not have a\nstrictly flat ground plane or when the camera is moving. Moreover, these\nmethods are often evaluated on in-the-wild real world videos without\nground-truth data or on synthetic datasets, which fail to model the real world\nlight transport, camera motion, and pose-induced appearance and geometry\nchanges. To tackle these two problems, we introduce MoviCam, the first\nnon-synthetic dataset containing ground-truth camera trajectories of a\ndynamically moving monocular RGB camera, scene geometry, and 3D human motion\nwith human-scene contact labels. Additionally, we propose PhysDynPose, a\nphysics-based method that incorporates scene geometry and physical constraints\nfor more accurate human motion tracking in case of camera motion and non-flat\nscenes. More precisely, we use a state-of-the-art kinematics estimator to\nobtain the human pose and a robust SLAM method to capture the dynamic camera\ntrajectory, enabling the recovery of the human pose in the world frame. We then\nrefine the kinematic pose estimate using our scene-aware physics optimizer.\nFrom our new benchmark, we found that even state-of-the-art methods struggle\nwith this inherently challenging setting, i.e. a moving camera and non-planar\nenvironments, while our method robustly estimates both human and camera poses\nin world coordinates.\n", "link": "http://arxiv.org/abs/2507.17406v1", "date": "2025-07-23", "relevancy": 2.528, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6707}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6049}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-based%20Human%20Pose%20Estimation%20from%20a%20Single%20Moving%20RGB%20Camera&body=Title%3A%20Physics-based%20Human%20Pose%20Estimation%20from%20a%20Single%20Moving%20RGB%20Camera%0AAuthor%3A%20Ayce%20Idil%20Aytekin%20and%20Chuqiao%20Li%20and%20Diogo%20Luvizon%20and%20Rishabh%20Dabral%20and%20Martin%20Oswald%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Most%20monocular%20and%20physics-based%20human%20pose%20tracking%20methods%2C%20while%20achieving%0Astate-of-the-art%20results%2C%20suffer%20from%20artifacts%20when%20the%20scene%20does%20not%20have%20a%0Astrictly%20flat%20ground%20plane%20or%20when%20the%20camera%20is%20moving.%20Moreover%2C%20these%0Amethods%20are%20often%20evaluated%20on%20in-the-wild%20real%20world%20videos%20without%0Aground-truth%20data%20or%20on%20synthetic%20datasets%2C%20which%20fail%20to%20model%20the%20real%20world%0Alight%20transport%2C%20camera%20motion%2C%20and%20pose-induced%20appearance%20and%20geometry%0Achanges.%20To%20tackle%20these%20two%20problems%2C%20we%20introduce%20MoviCam%2C%20the%20first%0Anon-synthetic%20dataset%20containing%20ground-truth%20camera%20trajectories%20of%20a%0Adynamically%20moving%20monocular%20RGB%20camera%2C%20scene%20geometry%2C%20and%203D%20human%20motion%0Awith%20human-scene%20contact%20labels.%20Additionally%2C%20we%20propose%20PhysDynPose%2C%20a%0Aphysics-based%20method%20that%20incorporates%20scene%20geometry%20and%20physical%20constraints%0Afor%20more%20accurate%20human%20motion%20tracking%20in%20case%20of%20camera%20motion%20and%20non-flat%0Ascenes.%20More%20precisely%2C%20we%20use%20a%20state-of-the-art%20kinematics%20estimator%20to%0Aobtain%20the%20human%20pose%20and%20a%20robust%20SLAM%20method%20to%20capture%20the%20dynamic%20camera%0Atrajectory%2C%20enabling%20the%20recovery%20of%20the%20human%20pose%20in%20the%20world%20frame.%20We%20then%0Arefine%20the%20kinematic%20pose%20estimate%20using%20our%20scene-aware%20physics%20optimizer.%0AFrom%20our%20new%20benchmark%2C%20we%20found%20that%20even%20state-of-the-art%20methods%20struggle%0Awith%20this%20inherently%20challenging%20setting%2C%20i.e.%20a%20moving%20camera%20and%20non-planar%0Aenvironments%2C%20while%20our%20method%20robustly%20estimates%20both%20human%20and%20camera%20poses%0Ain%20world%20coordinates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-based%2520Human%2520Pose%2520Estimation%2520from%2520a%2520Single%2520Moving%2520RGB%2520Camera%26entry.906535625%3DAyce%2520Idil%2520Aytekin%2520and%2520Chuqiao%2520Li%2520and%2520Diogo%2520Luvizon%2520and%2520Rishabh%2520Dabral%2520and%2520Martin%2520Oswald%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Most%2520monocular%2520and%2520physics-based%2520human%2520pose%2520tracking%2520methods%252C%2520while%2520achieving%250Astate-of-the-art%2520results%252C%2520suffer%2520from%2520artifacts%2520when%2520the%2520scene%2520does%2520not%2520have%2520a%250Astrictly%2520flat%2520ground%2520plane%2520or%2520when%2520the%2520camera%2520is%2520moving.%2520Moreover%252C%2520these%250Amethods%2520are%2520often%2520evaluated%2520on%2520in-the-wild%2520real%2520world%2520videos%2520without%250Aground-truth%2520data%2520or%2520on%2520synthetic%2520datasets%252C%2520which%2520fail%2520to%2520model%2520the%2520real%2520world%250Alight%2520transport%252C%2520camera%2520motion%252C%2520and%2520pose-induced%2520appearance%2520and%2520geometry%250Achanges.%2520To%2520tackle%2520these%2520two%2520problems%252C%2520we%2520introduce%2520MoviCam%252C%2520the%2520first%250Anon-synthetic%2520dataset%2520containing%2520ground-truth%2520camera%2520trajectories%2520of%2520a%250Adynamically%2520moving%2520monocular%2520RGB%2520camera%252C%2520scene%2520geometry%252C%2520and%25203D%2520human%2520motion%250Awith%2520human-scene%2520contact%2520labels.%2520Additionally%252C%2520we%2520propose%2520PhysDynPose%252C%2520a%250Aphysics-based%2520method%2520that%2520incorporates%2520scene%2520geometry%2520and%2520physical%2520constraints%250Afor%2520more%2520accurate%2520human%2520motion%2520tracking%2520in%2520case%2520of%2520camera%2520motion%2520and%2520non-flat%250Ascenes.%2520More%2520precisely%252C%2520we%2520use%2520a%2520state-of-the-art%2520kinematics%2520estimator%2520to%250Aobtain%2520the%2520human%2520pose%2520and%2520a%2520robust%2520SLAM%2520method%2520to%2520capture%2520the%2520dynamic%2520camera%250Atrajectory%252C%2520enabling%2520the%2520recovery%2520of%2520the%2520human%2520pose%2520in%2520the%2520world%2520frame.%2520We%2520then%250Arefine%2520the%2520kinematic%2520pose%2520estimate%2520using%2520our%2520scene-aware%2520physics%2520optimizer.%250AFrom%2520our%2520new%2520benchmark%252C%2520we%2520found%2520that%2520even%2520state-of-the-art%2520methods%2520struggle%250Awith%2520this%2520inherently%2520challenging%2520setting%252C%2520i.e.%2520a%2520moving%2520camera%2520and%2520non-planar%250Aenvironments%252C%2520while%2520our%2520method%2520robustly%2520estimates%2520both%2520human%2520and%2520camera%2520poses%250Ain%2520world%2520coordinates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-based%20Human%20Pose%20Estimation%20from%20a%20Single%20Moving%20RGB%20Camera&entry.906535625=Ayce%20Idil%20Aytekin%20and%20Chuqiao%20Li%20and%20Diogo%20Luvizon%20and%20Rishabh%20Dabral%20and%20Martin%20Oswald%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20Most%20monocular%20and%20physics-based%20human%20pose%20tracking%20methods%2C%20while%20achieving%0Astate-of-the-art%20results%2C%20suffer%20from%20artifacts%20when%20the%20scene%20does%20not%20have%20a%0Astrictly%20flat%20ground%20plane%20or%20when%20the%20camera%20is%20moving.%20Moreover%2C%20these%0Amethods%20are%20often%20evaluated%20on%20in-the-wild%20real%20world%20videos%20without%0Aground-truth%20data%20or%20on%20synthetic%20datasets%2C%20which%20fail%20to%20model%20the%20real%20world%0Alight%20transport%2C%20camera%20motion%2C%20and%20pose-induced%20appearance%20and%20geometry%0Achanges.%20To%20tackle%20these%20two%20problems%2C%20we%20introduce%20MoviCam%2C%20the%20first%0Anon-synthetic%20dataset%20containing%20ground-truth%20camera%20trajectories%20of%20a%0Adynamically%20moving%20monocular%20RGB%20camera%2C%20scene%20geometry%2C%20and%203D%20human%20motion%0Awith%20human-scene%20contact%20labels.%20Additionally%2C%20we%20propose%20PhysDynPose%2C%20a%0Aphysics-based%20method%20that%20incorporates%20scene%20geometry%20and%20physical%20constraints%0Afor%20more%20accurate%20human%20motion%20tracking%20in%20case%20of%20camera%20motion%20and%20non-flat%0Ascenes.%20More%20precisely%2C%20we%20use%20a%20state-of-the-art%20kinematics%20estimator%20to%0Aobtain%20the%20human%20pose%20and%20a%20robust%20SLAM%20method%20to%20capture%20the%20dynamic%20camera%0Atrajectory%2C%20enabling%20the%20recovery%20of%20the%20human%20pose%20in%20the%20world%20frame.%20We%20then%0Arefine%20the%20kinematic%20pose%20estimate%20using%20our%20scene-aware%20physics%20optimizer.%0AFrom%20our%20new%20benchmark%2C%20we%20found%20that%20even%20state-of-the-art%20methods%20struggle%0Awith%20this%20inherently%20challenging%20setting%2C%20i.e.%20a%20moving%20camera%20and%20non-planar%0Aenvironments%2C%20while%20our%20method%20robustly%20estimates%20both%20human%20and%20camera%20poses%0Ain%20world%20coordinates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17406v1&entry.124074799=Read"},
{"title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via\n  Real-Time Multi-View Box Fusion", "author": "Yuqing Lan and Chenyang Zhu and Zhirui Gao and Jiazhao Zhang and Yihan Cao and Renjiao Yi and Yijie Wang and Kai Xu", "abstract": "  Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.\n", "link": "http://arxiv.org/abs/2506.15610v2", "date": "2025-07-23", "relevancy": 2.5128, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6333}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6333}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion&body=Title%3A%20BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion%0AAuthor%3A%20Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Zhirui%20Gao%20and%20Jiazhao%20Zhang%20and%20Yihan%20Cao%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Open-vocabulary%203D%20object%20detection%20has%20gained%20significant%20interest%20due%20to%0Aits%20critical%20applications%20in%20autonomous%20driving%20and%20embodied%20AI.%20Existing%0Adetection%20methods%2C%20whether%20offline%20or%20online%2C%20typically%20rely%20on%20dense%20point%0Acloud%20reconstruction%2C%20which%20imposes%20substantial%20computational%20overhead%20and%0Amemory%20constraints%2C%20hindering%20real-time%20deployment%20in%20downstream%20tasks.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reconstruction-free%20online%20framework%20tailored%0Afor%20memory-efficient%20and%20real-time%203D%20detection.%20Specifically%2C%20given%20streaming%0Aposed%20RGB-D%20video%20input%2C%20we%20leverage%20Cubify%20Anything%20as%20a%20pre-trained%20visual%0Afoundation%20model%20%28VFM%29%20for%20single-view%203D%20object%20detection%20by%20bounding%20boxes%2C%0Acoupled%20with%20CLIP%20to%20capture%20open-vocabulary%20semantics%20of%20detected%20objects.%20To%0Afuse%20all%20detected%20bounding%20boxes%20across%20different%20views%20into%20a%20unified%20one%2C%20we%0Aemploy%20an%20association%20module%20for%20correspondences%20of%20multi-views%20and%20an%0Aoptimization%20module%20to%20fuse%20the%203D%20bounding%20boxes%20of%20the%20same%20instance%0Apredicted%20in%20multi-views.%20The%20association%20module%20utilizes%203D%20Non-Maximum%0ASuppression%20%28NMS%29%20and%20a%20box%20correspondence%20matching%20module%2C%20while%20the%0Aoptimization%20module%20uses%20an%20IoU-guided%20efficient%20random%20optimization%20technique%0Abased%20on%20particle%20filtering%20to%20enforce%20multi-view%20consistency%20of%20the%203D%0Abounding%20boxes%20while%20minimizing%20computational%20complexity.%20Extensive%20experiments%0Aon%20ScanNetV2%20and%20CA-1M%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20among%20online%20methods.%20Benefiting%20from%20this%20novel%0Areconstruction-free%20paradigm%20for%203D%20object%20detection%2C%20our%20method%20exhibits%20great%0Ageneralization%20abilities%20in%20various%20scenarios%2C%20enabling%20real-time%20perception%0Aeven%20in%20environments%20exceeding%201000%20square%20meters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxFusion%253A%2520Reconstruction-Free%2520Open-Vocabulary%25203D%2520Object%2520Detection%2520via%250A%2520%2520Real-Time%2520Multi-View%2520Box%2520Fusion%26entry.906535625%3DYuqing%2520Lan%2520and%2520Chenyang%2520Zhu%2520and%2520Zhirui%2520Gao%2520and%2520Jiazhao%2520Zhang%2520and%2520Yihan%2520Cao%2520and%2520Renjiao%2520Yi%2520and%2520Yijie%2520Wang%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520object%2520detection%2520has%2520gained%2520significant%2520interest%2520due%2520to%250Aits%2520critical%2520applications%2520in%2520autonomous%2520driving%2520and%2520embodied%2520AI.%2520Existing%250Adetection%2520methods%252C%2520whether%2520offline%2520or%2520online%252C%2520typically%2520rely%2520on%2520dense%2520point%250Acloud%2520reconstruction%252C%2520which%2520imposes%2520substantial%2520computational%2520overhead%2520and%250Amemory%2520constraints%252C%2520hindering%2520real-time%2520deployment%2520in%2520downstream%2520tasks.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520reconstruction-free%2520online%2520framework%2520tailored%250Afor%2520memory-efficient%2520and%2520real-time%25203D%2520detection.%2520Specifically%252C%2520given%2520streaming%250Aposed%2520RGB-D%2520video%2520input%252C%2520we%2520leverage%2520Cubify%2520Anything%2520as%2520a%2520pre-trained%2520visual%250Afoundation%2520model%2520%2528VFM%2529%2520for%2520single-view%25203D%2520object%2520detection%2520by%2520bounding%2520boxes%252C%250Acoupled%2520with%2520CLIP%2520to%2520capture%2520open-vocabulary%2520semantics%2520of%2520detected%2520objects.%2520To%250Afuse%2520all%2520detected%2520bounding%2520boxes%2520across%2520different%2520views%2520into%2520a%2520unified%2520one%252C%2520we%250Aemploy%2520an%2520association%2520module%2520for%2520correspondences%2520of%2520multi-views%2520and%2520an%250Aoptimization%2520module%2520to%2520fuse%2520the%25203D%2520bounding%2520boxes%2520of%2520the%2520same%2520instance%250Apredicted%2520in%2520multi-views.%2520The%2520association%2520module%2520utilizes%25203D%2520Non-Maximum%250ASuppression%2520%2528NMS%2529%2520and%2520a%2520box%2520correspondence%2520matching%2520module%252C%2520while%2520the%250Aoptimization%2520module%2520uses%2520an%2520IoU-guided%2520efficient%2520random%2520optimization%2520technique%250Abased%2520on%2520particle%2520filtering%2520to%2520enforce%2520multi-view%2520consistency%2520of%2520the%25203D%250Abounding%2520boxes%2520while%2520minimizing%2520computational%2520complexity.%2520Extensive%2520experiments%250Aon%2520ScanNetV2%2520and%2520CA-1M%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520among%2520online%2520methods.%2520Benefiting%2520from%2520this%2520novel%250Areconstruction-free%2520paradigm%2520for%25203D%2520object%2520detection%252C%2520our%2520method%2520exhibits%2520great%250Ageneralization%2520abilities%2520in%2520various%2520scenarios%252C%2520enabling%2520real-time%2520perception%250Aeven%2520in%2520environments%2520exceeding%25201000%2520square%2520meters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion&entry.906535625=Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Zhirui%20Gao%20and%20Jiazhao%20Zhang%20and%20Yihan%20Cao%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu&entry.1292438233=%20%20Open-vocabulary%203D%20object%20detection%20has%20gained%20significant%20interest%20due%20to%0Aits%20critical%20applications%20in%20autonomous%20driving%20and%20embodied%20AI.%20Existing%0Adetection%20methods%2C%20whether%20offline%20or%20online%2C%20typically%20rely%20on%20dense%20point%0Acloud%20reconstruction%2C%20which%20imposes%20substantial%20computational%20overhead%20and%0Amemory%20constraints%2C%20hindering%20real-time%20deployment%20in%20downstream%20tasks.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reconstruction-free%20online%20framework%20tailored%0Afor%20memory-efficient%20and%20real-time%203D%20detection.%20Specifically%2C%20given%20streaming%0Aposed%20RGB-D%20video%20input%2C%20we%20leverage%20Cubify%20Anything%20as%20a%20pre-trained%20visual%0Afoundation%20model%20%28VFM%29%20for%20single-view%203D%20object%20detection%20by%20bounding%20boxes%2C%0Acoupled%20with%20CLIP%20to%20capture%20open-vocabulary%20semantics%20of%20detected%20objects.%20To%0Afuse%20all%20detected%20bounding%20boxes%20across%20different%20views%20into%20a%20unified%20one%2C%20we%0Aemploy%20an%20association%20module%20for%20correspondences%20of%20multi-views%20and%20an%0Aoptimization%20module%20to%20fuse%20the%203D%20bounding%20boxes%20of%20the%20same%20instance%0Apredicted%20in%20multi-views.%20The%20association%20module%20utilizes%203D%20Non-Maximum%0ASuppression%20%28NMS%29%20and%20a%20box%20correspondence%20matching%20module%2C%20while%20the%0Aoptimization%20module%20uses%20an%20IoU-guided%20efficient%20random%20optimization%20technique%0Abased%20on%20particle%20filtering%20to%20enforce%20multi-view%20consistency%20of%20the%203D%0Abounding%20boxes%20while%20minimizing%20computational%20complexity.%20Extensive%20experiments%0Aon%20ScanNetV2%20and%20CA-1M%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20among%20online%20methods.%20Benefiting%20from%20this%20novel%0Areconstruction-free%20paradigm%20for%203D%20object%20detection%2C%20our%20method%20exhibits%20great%0Ageneralization%20abilities%20in%20various%20scenarios%2C%20enabling%20real-time%20perception%0Aeven%20in%20environments%20exceeding%201000%20square%20meters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15610v2&entry.124074799=Read"},
{"title": "Towards Effective Open-set Graph Class-incremental Learning", "author": "Jiazhen Chen and Zheng Ma and Sichao Fu and Mingbin Feng and Tony S. Wirjanto and Weihua Ou", "abstract": "  Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)\nto adapt to evolving graph analytical tasks by incrementally learning new class\nknowledge while retaining knowledge of old classes. Existing GCIL methods\nprimarily focus on a closed-set assumption, where all test samples are presumed\nto belong to previously known classes. Such an assumption restricts their\napplicability in real-world scenarios, where unknown classes naturally emerge\nduring inference, and are absent during training. In this paper, we explore a\nmore challenging open-set graph class-incremental learning scenario with two\nintertwined challenges: catastrophic forgetting of old classes, which impairs\nthe detection of unknown classes, and inadequate open-set recognition, which\ndestabilizes the retention of learned knowledge. To address the above problems,\na novel OGCIL framework is proposed, which utilizes pseudo-sample embedding\ngeneration to effectively mitigate catastrophic forgetting and enable robust\ndetection of unknown classes. To be specific, a prototypical conditional\nvariational autoencoder is designed to synthesize node embeddings for old\nclasses, enabling knowledge replay without storing raw graph data. To handle\nunknown classes, we employ a mixing-based strategy to generate\nout-of-distribution (OOD) samples from pseudo in-distribution and current node\nembeddings. A novel prototypical hypersphere classification loss is further\nproposed, which anchors in-distribution embeddings to their respective class\nprototypes, while repelling OOD embeddings away. Instead of assigning all\nunknown samples into one cluster, our proposed objective function explicitly\nmodels them as outliers through prototype-aware rejection regions, ensuring a\nrobust open-set recognition. Extensive experiments on five benchmarks\ndemonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN\nmethods.\n", "link": "http://arxiv.org/abs/2507.17687v1", "date": "2025-07-23", "relevancy": 2.5107, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5144}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5015}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Effective%20Open-set%20Graph%20Class-incremental%20Learning&body=Title%3A%20Towards%20Effective%20Open-set%20Graph%20Class-incremental%20Learning%0AAuthor%3A%20Jiazhen%20Chen%20and%20Zheng%20Ma%20and%20Sichao%20Fu%20and%20Mingbin%20Feng%20and%20Tony%20S.%20Wirjanto%20and%20Weihua%20Ou%0AAbstract%3A%20%20%20Graph%20class-incremental%20learning%20%28GCIL%29%20allows%20graph%20neural%20networks%20%28GNNs%29%0Ato%20adapt%20to%20evolving%20graph%20analytical%20tasks%20by%20incrementally%20learning%20new%20class%0Aknowledge%20while%20retaining%20knowledge%20of%20old%20classes.%20Existing%20GCIL%20methods%0Aprimarily%20focus%20on%20a%20closed-set%20assumption%2C%20where%20all%20test%20samples%20are%20presumed%0Ato%20belong%20to%20previously%20known%20classes.%20Such%20an%20assumption%20restricts%20their%0Aapplicability%20in%20real-world%20scenarios%2C%20where%20unknown%20classes%20naturally%20emerge%0Aduring%20inference%2C%20and%20are%20absent%20during%20training.%20In%20this%20paper%2C%20we%20explore%20a%0Amore%20challenging%20open-set%20graph%20class-incremental%20learning%20scenario%20with%20two%0Aintertwined%20challenges%3A%20catastrophic%20forgetting%20of%20old%20classes%2C%20which%20impairs%0Athe%20detection%20of%20unknown%20classes%2C%20and%20inadequate%20open-set%20recognition%2C%20which%0Adestabilizes%20the%20retention%20of%20learned%20knowledge.%20To%20address%20the%20above%20problems%2C%0Aa%20novel%20OGCIL%20framework%20is%20proposed%2C%20which%20utilizes%20pseudo-sample%20embedding%0Ageneration%20to%20effectively%20mitigate%20catastrophic%20forgetting%20and%20enable%20robust%0Adetection%20of%20unknown%20classes.%20To%20be%20specific%2C%20a%20prototypical%20conditional%0Avariational%20autoencoder%20is%20designed%20to%20synthesize%20node%20embeddings%20for%20old%0Aclasses%2C%20enabling%20knowledge%20replay%20without%20storing%20raw%20graph%20data.%20To%20handle%0Aunknown%20classes%2C%20we%20employ%20a%20mixing-based%20strategy%20to%20generate%0Aout-of-distribution%20%28OOD%29%20samples%20from%20pseudo%20in-distribution%20and%20current%20node%0Aembeddings.%20A%20novel%20prototypical%20hypersphere%20classification%20loss%20is%20further%0Aproposed%2C%20which%20anchors%20in-distribution%20embeddings%20to%20their%20respective%20class%0Aprototypes%2C%20while%20repelling%20OOD%20embeddings%20away.%20Instead%20of%20assigning%20all%0Aunknown%20samples%20into%20one%20cluster%2C%20our%20proposed%20objective%20function%20explicitly%0Amodels%20them%20as%20outliers%20through%20prototype-aware%20rejection%20regions%2C%20ensuring%20a%0Arobust%20open-set%20recognition.%20Extensive%20experiments%20on%20five%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20OGCIL%20over%20existing%20GCIL%20and%20open-set%20GNN%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Effective%2520Open-set%2520Graph%2520Class-incremental%2520Learning%26entry.906535625%3DJiazhen%2520Chen%2520and%2520Zheng%2520Ma%2520and%2520Sichao%2520Fu%2520and%2520Mingbin%2520Feng%2520and%2520Tony%2520S.%2520Wirjanto%2520and%2520Weihua%2520Ou%26entry.1292438233%3D%2520%2520Graph%2520class-incremental%2520learning%2520%2528GCIL%2529%2520allows%2520graph%2520neural%2520networks%2520%2528GNNs%2529%250Ato%2520adapt%2520to%2520evolving%2520graph%2520analytical%2520tasks%2520by%2520incrementally%2520learning%2520new%2520class%250Aknowledge%2520while%2520retaining%2520knowledge%2520of%2520old%2520classes.%2520Existing%2520GCIL%2520methods%250Aprimarily%2520focus%2520on%2520a%2520closed-set%2520assumption%252C%2520where%2520all%2520test%2520samples%2520are%2520presumed%250Ato%2520belong%2520to%2520previously%2520known%2520classes.%2520Such%2520an%2520assumption%2520restricts%2520their%250Aapplicability%2520in%2520real-world%2520scenarios%252C%2520where%2520unknown%2520classes%2520naturally%2520emerge%250Aduring%2520inference%252C%2520and%2520are%2520absent%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%250Amore%2520challenging%2520open-set%2520graph%2520class-incremental%2520learning%2520scenario%2520with%2520two%250Aintertwined%2520challenges%253A%2520catastrophic%2520forgetting%2520of%2520old%2520classes%252C%2520which%2520impairs%250Athe%2520detection%2520of%2520unknown%2520classes%252C%2520and%2520inadequate%2520open-set%2520recognition%252C%2520which%250Adestabilizes%2520the%2520retention%2520of%2520learned%2520knowledge.%2520To%2520address%2520the%2520above%2520problems%252C%250Aa%2520novel%2520OGCIL%2520framework%2520is%2520proposed%252C%2520which%2520utilizes%2520pseudo-sample%2520embedding%250Ageneration%2520to%2520effectively%2520mitigate%2520catastrophic%2520forgetting%2520and%2520enable%2520robust%250Adetection%2520of%2520unknown%2520classes.%2520To%2520be%2520specific%252C%2520a%2520prototypical%2520conditional%250Avariational%2520autoencoder%2520is%2520designed%2520to%2520synthesize%2520node%2520embeddings%2520for%2520old%250Aclasses%252C%2520enabling%2520knowledge%2520replay%2520without%2520storing%2520raw%2520graph%2520data.%2520To%2520handle%250Aunknown%2520classes%252C%2520we%2520employ%2520a%2520mixing-based%2520strategy%2520to%2520generate%250Aout-of-distribution%2520%2528OOD%2529%2520samples%2520from%2520pseudo%2520in-distribution%2520and%2520current%2520node%250Aembeddings.%2520A%2520novel%2520prototypical%2520hypersphere%2520classification%2520loss%2520is%2520further%250Aproposed%252C%2520which%2520anchors%2520in-distribution%2520embeddings%2520to%2520their%2520respective%2520class%250Aprototypes%252C%2520while%2520repelling%2520OOD%2520embeddings%2520away.%2520Instead%2520of%2520assigning%2520all%250Aunknown%2520samples%2520into%2520one%2520cluster%252C%2520our%2520proposed%2520objective%2520function%2520explicitly%250Amodels%2520them%2520as%2520outliers%2520through%2520prototype-aware%2520rejection%2520regions%252C%2520ensuring%2520a%250Arobust%2520open-set%2520recognition.%2520Extensive%2520experiments%2520on%2520five%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520OGCIL%2520over%2520existing%2520GCIL%2520and%2520open-set%2520GNN%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Effective%20Open-set%20Graph%20Class-incremental%20Learning&entry.906535625=Jiazhen%20Chen%20and%20Zheng%20Ma%20and%20Sichao%20Fu%20and%20Mingbin%20Feng%20and%20Tony%20S.%20Wirjanto%20and%20Weihua%20Ou&entry.1292438233=%20%20Graph%20class-incremental%20learning%20%28GCIL%29%20allows%20graph%20neural%20networks%20%28GNNs%29%0Ato%20adapt%20to%20evolving%20graph%20analytical%20tasks%20by%20incrementally%20learning%20new%20class%0Aknowledge%20while%20retaining%20knowledge%20of%20old%20classes.%20Existing%20GCIL%20methods%0Aprimarily%20focus%20on%20a%20closed-set%20assumption%2C%20where%20all%20test%20samples%20are%20presumed%0Ato%20belong%20to%20previously%20known%20classes.%20Such%20an%20assumption%20restricts%20their%0Aapplicability%20in%20real-world%20scenarios%2C%20where%20unknown%20classes%20naturally%20emerge%0Aduring%20inference%2C%20and%20are%20absent%20during%20training.%20In%20this%20paper%2C%20we%20explore%20a%0Amore%20challenging%20open-set%20graph%20class-incremental%20learning%20scenario%20with%20two%0Aintertwined%20challenges%3A%20catastrophic%20forgetting%20of%20old%20classes%2C%20which%20impairs%0Athe%20detection%20of%20unknown%20classes%2C%20and%20inadequate%20open-set%20recognition%2C%20which%0Adestabilizes%20the%20retention%20of%20learned%20knowledge.%20To%20address%20the%20above%20problems%2C%0Aa%20novel%20OGCIL%20framework%20is%20proposed%2C%20which%20utilizes%20pseudo-sample%20embedding%0Ageneration%20to%20effectively%20mitigate%20catastrophic%20forgetting%20and%20enable%20robust%0Adetection%20of%20unknown%20classes.%20To%20be%20specific%2C%20a%20prototypical%20conditional%0Avariational%20autoencoder%20is%20designed%20to%20synthesize%20node%20embeddings%20for%20old%0Aclasses%2C%20enabling%20knowledge%20replay%20without%20storing%20raw%20graph%20data.%20To%20handle%0Aunknown%20classes%2C%20we%20employ%20a%20mixing-based%20strategy%20to%20generate%0Aout-of-distribution%20%28OOD%29%20samples%20from%20pseudo%20in-distribution%20and%20current%20node%0Aembeddings.%20A%20novel%20prototypical%20hypersphere%20classification%20loss%20is%20further%0Aproposed%2C%20which%20anchors%20in-distribution%20embeddings%20to%20their%20respective%20class%0Aprototypes%2C%20while%20repelling%20OOD%20embeddings%20away.%20Instead%20of%20assigning%20all%0Aunknown%20samples%20into%20one%20cluster%2C%20our%20proposed%20objective%20function%20explicitly%0Amodels%20them%20as%20outliers%20through%20prototype-aware%20rejection%20regions%2C%20ensuring%20a%0Arobust%20open-set%20recognition.%20Extensive%20experiments%20on%20five%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20OGCIL%20over%20existing%20GCIL%20and%20open-set%20GNN%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17687v1&entry.124074799=Read"},
{"title": "From Scan to Action: Leveraging Realistic Scans for Embodied Scene\n  Understanding", "author": "Anna-Maria Halacheva and Jan-Nico Zaech and Sombit Dey and Luc Van Gool and Danda Pani Paudel", "abstract": "  Real-world 3D scene-level scans offer realism and can enable better\nreal-world generalizability for downstream applications. However, challenges\nsuch as data volume, diverse annotation formats, and tool compatibility limit\ntheir use. This paper demonstrates a methodology to effectively leverage these\nscans and their annotations. We propose a unified annotation integration using\nUSD, with application-specific USD flavors. We identify challenges in utilizing\nholistic real-world scan datasets and present mitigation strategies. The\nefficacy of our approach is demonstrated through two downstream applications:\nLLM-based scene editing, enabling effective LLM understanding and adaptation of\nthe data (80% success), and robotic simulation, achieving an 87% success rate\nin policy learning.\n", "link": "http://arxiv.org/abs/2507.17585v1", "date": "2025-07-23", "relevancy": 2.4969, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6288}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Scan%20to%20Action%3A%20Leveraging%20Realistic%20Scans%20for%20Embodied%20Scene%0A%20%20Understanding&body=Title%3A%20From%20Scan%20to%20Action%3A%20Leveraging%20Realistic%20Scans%20for%20Embodied%20Scene%0A%20%20Understanding%0AAuthor%3A%20Anna-Maria%20Halacheva%20and%20Jan-Nico%20Zaech%20and%20Sombit%20Dey%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Real-world%203D%20scene-level%20scans%20offer%20realism%20and%20can%20enable%20better%0Areal-world%20generalizability%20for%20downstream%20applications.%20However%2C%20challenges%0Asuch%20as%20data%20volume%2C%20diverse%20annotation%20formats%2C%20and%20tool%20compatibility%20limit%0Atheir%20use.%20This%20paper%20demonstrates%20a%20methodology%20to%20effectively%20leverage%20these%0Ascans%20and%20their%20annotations.%20We%20propose%20a%20unified%20annotation%20integration%20using%0AUSD%2C%20with%20application-specific%20USD%20flavors.%20We%20identify%20challenges%20in%20utilizing%0Aholistic%20real-world%20scan%20datasets%20and%20present%20mitigation%20strategies.%20The%0Aefficacy%20of%20our%20approach%20is%20demonstrated%20through%20two%20downstream%20applications%3A%0ALLM-based%20scene%20editing%2C%20enabling%20effective%20LLM%20understanding%20and%20adaptation%20of%0Athe%20data%20%2880%25%20success%29%2C%20and%20robotic%20simulation%2C%20achieving%20an%2087%25%20success%20rate%0Ain%20policy%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Scan%2520to%2520Action%253A%2520Leveraging%2520Realistic%2520Scans%2520for%2520Embodied%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DAnna-Maria%2520Halacheva%2520and%2520Jan-Nico%2520Zaech%2520and%2520Sombit%2520Dey%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Real-world%25203D%2520scene-level%2520scans%2520offer%2520realism%2520and%2520can%2520enable%2520better%250Areal-world%2520generalizability%2520for%2520downstream%2520applications.%2520However%252C%2520challenges%250Asuch%2520as%2520data%2520volume%252C%2520diverse%2520annotation%2520formats%252C%2520and%2520tool%2520compatibility%2520limit%250Atheir%2520use.%2520This%2520paper%2520demonstrates%2520a%2520methodology%2520to%2520effectively%2520leverage%2520these%250Ascans%2520and%2520their%2520annotations.%2520We%2520propose%2520a%2520unified%2520annotation%2520integration%2520using%250AUSD%252C%2520with%2520application-specific%2520USD%2520flavors.%2520We%2520identify%2520challenges%2520in%2520utilizing%250Aholistic%2520real-world%2520scan%2520datasets%2520and%2520present%2520mitigation%2520strategies.%2520The%250Aefficacy%2520of%2520our%2520approach%2520is%2520demonstrated%2520through%2520two%2520downstream%2520applications%253A%250ALLM-based%2520scene%2520editing%252C%2520enabling%2520effective%2520LLM%2520understanding%2520and%2520adaptation%2520of%250Athe%2520data%2520%252880%2525%2520success%2529%252C%2520and%2520robotic%2520simulation%252C%2520achieving%2520an%252087%2525%2520success%2520rate%250Ain%2520policy%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Scan%20to%20Action%3A%20Leveraging%20Realistic%20Scans%20for%20Embodied%20Scene%0A%20%20Understanding&entry.906535625=Anna-Maria%20Halacheva%20and%20Jan-Nico%20Zaech%20and%20Sombit%20Dey%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Real-world%203D%20scene-level%20scans%20offer%20realism%20and%20can%20enable%20better%0Areal-world%20generalizability%20for%20downstream%20applications.%20However%2C%20challenges%0Asuch%20as%20data%20volume%2C%20diverse%20annotation%20formats%2C%20and%20tool%20compatibility%20limit%0Atheir%20use.%20This%20paper%20demonstrates%20a%20methodology%20to%20effectively%20leverage%20these%0Ascans%20and%20their%20annotations.%20We%20propose%20a%20unified%20annotation%20integration%20using%0AUSD%2C%20with%20application-specific%20USD%20flavors.%20We%20identify%20challenges%20in%20utilizing%0Aholistic%20real-world%20scan%20datasets%20and%20present%20mitigation%20strategies.%20The%0Aefficacy%20of%20our%20approach%20is%20demonstrated%20through%20two%20downstream%20applications%3A%0ALLM-based%20scene%20editing%2C%20enabling%20effective%20LLM%20understanding%20and%20adaptation%20of%0Athe%20data%20%2880%25%20success%29%2C%20and%20robotic%20simulation%2C%20achieving%20an%2087%25%20success%20rate%0Ain%20policy%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17585v1&entry.124074799=Read"},
{"title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation", "author": "Xinyu Liu and Hengyu Liu and Cheng Wang and Tianming Liu and Yixuan Yuan", "abstract": "  Endoscopic video generation is crucial for advancing medical imaging and\nenhancing diagnostic capabilities. However, prior efforts in this field have\neither focused on static images, lacking the dynamic context required for\npractical applications, or have relied on unconditional generation that fails\nto provide meaningful references for clinicians. Therefore, in this paper, we\npropose the first conditional endoscopic video generation framework, namely\nEndoGen. Specifically, we build an autoregressive model with a tailored\nSpatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the\nlearning of generating multiple frames as a grid-based image generation\npattern, which effectively capitalizes the inherent global dependency modeling\ncapabilities of autoregressive architectures. Furthermore, we propose a\nSemantic-Aware Token Masking (SAT) mechanism, which enhances the model's\nability to produce rich and diverse content by selectively focusing on\nsemantically meaningful regions during the generation process. Through\nextensive experiments, we demonstrate the effectiveness of our framework in\ngenerating high-quality, conditionally guided endoscopic content, and improves\nthe performance of downstream task of polyp segmentation. Code released at\nhttps://www.github.com/CUHK-AIM-Group/EndoGen.\n", "link": "http://arxiv.org/abs/2507.17388v1", "date": "2025-07-23", "relevancy": 2.4942, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6284}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6266}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoGen%3A%20Conditional%20Autoregressive%20Endoscopic%20Video%20Generation&body=Title%3A%20EndoGen%3A%20Conditional%20Autoregressive%20Endoscopic%20Video%20Generation%0AAuthor%3A%20Xinyu%20Liu%20and%20Hengyu%20Liu%20and%20Cheng%20Wang%20and%20Tianming%20Liu%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Endoscopic%20video%20generation%20is%20crucial%20for%20advancing%20medical%20imaging%20and%0Aenhancing%20diagnostic%20capabilities.%20However%2C%20prior%20efforts%20in%20this%20field%20have%0Aeither%20focused%20on%20static%20images%2C%20lacking%20the%20dynamic%20context%20required%20for%0Apractical%20applications%2C%20or%20have%20relied%20on%20unconditional%20generation%20that%20fails%0Ato%20provide%20meaningful%20references%20for%20clinicians.%20Therefore%2C%20in%20this%20paper%2C%20we%0Apropose%20the%20first%20conditional%20endoscopic%20video%20generation%20framework%2C%20namely%0AEndoGen.%20Specifically%2C%20we%20build%20an%20autoregressive%20model%20with%20a%20tailored%0ASpatiotemporal%20Grid-Frame%20Patterning%20%28SGP%29%20strategy.%20It%20reformulates%20the%0Alearning%20of%20generating%20multiple%20frames%20as%20a%20grid-based%20image%20generation%0Apattern%2C%20which%20effectively%20capitalizes%20the%20inherent%20global%20dependency%20modeling%0Acapabilities%20of%20autoregressive%20architectures.%20Furthermore%2C%20we%20propose%20a%0ASemantic-Aware%20Token%20Masking%20%28SAT%29%20mechanism%2C%20which%20enhances%20the%20model%27s%0Aability%20to%20produce%20rich%20and%20diverse%20content%20by%20selectively%20focusing%20on%0Asemantically%20meaningful%20regions%20during%20the%20generation%20process.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Agenerating%20high-quality%2C%20conditionally%20guided%20endoscopic%20content%2C%20and%20improves%0Athe%20performance%20of%20downstream%20task%20of%20polyp%20segmentation.%20Code%20released%20at%0Ahttps%3A//www.github.com/CUHK-AIM-Group/EndoGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoGen%253A%2520Conditional%2520Autoregressive%2520Endoscopic%2520Video%2520Generation%26entry.906535625%3DXinyu%2520Liu%2520and%2520Hengyu%2520Liu%2520and%2520Cheng%2520Wang%2520and%2520Tianming%2520Liu%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Endoscopic%2520video%2520generation%2520is%2520crucial%2520for%2520advancing%2520medical%2520imaging%2520and%250Aenhancing%2520diagnostic%2520capabilities.%2520However%252C%2520prior%2520efforts%2520in%2520this%2520field%2520have%250Aeither%2520focused%2520on%2520static%2520images%252C%2520lacking%2520the%2520dynamic%2520context%2520required%2520for%250Apractical%2520applications%252C%2520or%2520have%2520relied%2520on%2520unconditional%2520generation%2520that%2520fails%250Ato%2520provide%2520meaningful%2520references%2520for%2520clinicians.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%250Apropose%2520the%2520first%2520conditional%2520endoscopic%2520video%2520generation%2520framework%252C%2520namely%250AEndoGen.%2520Specifically%252C%2520we%2520build%2520an%2520autoregressive%2520model%2520with%2520a%2520tailored%250ASpatiotemporal%2520Grid-Frame%2520Patterning%2520%2528SGP%2529%2520strategy.%2520It%2520reformulates%2520the%250Alearning%2520of%2520generating%2520multiple%2520frames%2520as%2520a%2520grid-based%2520image%2520generation%250Apattern%252C%2520which%2520effectively%2520capitalizes%2520the%2520inherent%2520global%2520dependency%2520modeling%250Acapabilities%2520of%2520autoregressive%2520architectures.%2520Furthermore%252C%2520we%2520propose%2520a%250ASemantic-Aware%2520Token%2520Masking%2520%2528SAT%2529%2520mechanism%252C%2520which%2520enhances%2520the%2520model%2527s%250Aability%2520to%2520produce%2520rich%2520and%2520diverse%2520content%2520by%2520selectively%2520focusing%2520on%250Asemantically%2520meaningful%2520regions%2520during%2520the%2520generation%2520process.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%250Agenerating%2520high-quality%252C%2520conditionally%2520guided%2520endoscopic%2520content%252C%2520and%2520improves%250Athe%2520performance%2520of%2520downstream%2520task%2520of%2520polyp%2520segmentation.%2520Code%2520released%2520at%250Ahttps%253A//www.github.com/CUHK-AIM-Group/EndoGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoGen%3A%20Conditional%20Autoregressive%20Endoscopic%20Video%20Generation&entry.906535625=Xinyu%20Liu%20and%20Hengyu%20Liu%20and%20Cheng%20Wang%20and%20Tianming%20Liu%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Endoscopic%20video%20generation%20is%20crucial%20for%20advancing%20medical%20imaging%20and%0Aenhancing%20diagnostic%20capabilities.%20However%2C%20prior%20efforts%20in%20this%20field%20have%0Aeither%20focused%20on%20static%20images%2C%20lacking%20the%20dynamic%20context%20required%20for%0Apractical%20applications%2C%20or%20have%20relied%20on%20unconditional%20generation%20that%20fails%0Ato%20provide%20meaningful%20references%20for%20clinicians.%20Therefore%2C%20in%20this%20paper%2C%20we%0Apropose%20the%20first%20conditional%20endoscopic%20video%20generation%20framework%2C%20namely%0AEndoGen.%20Specifically%2C%20we%20build%20an%20autoregressive%20model%20with%20a%20tailored%0ASpatiotemporal%20Grid-Frame%20Patterning%20%28SGP%29%20strategy.%20It%20reformulates%20the%0Alearning%20of%20generating%20multiple%20frames%20as%20a%20grid-based%20image%20generation%0Apattern%2C%20which%20effectively%20capitalizes%20the%20inherent%20global%20dependency%20modeling%0Acapabilities%20of%20autoregressive%20architectures.%20Furthermore%2C%20we%20propose%20a%0ASemantic-Aware%20Token%20Masking%20%28SAT%29%20mechanism%2C%20which%20enhances%20the%20model%27s%0Aability%20to%20produce%20rich%20and%20diverse%20content%20by%20selectively%20focusing%20on%0Asemantically%20meaningful%20regions%20during%20the%20generation%20process.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20framework%20in%0Agenerating%20high-quality%2C%20conditionally%20guided%20endoscopic%20content%2C%20and%20improves%0Athe%20performance%20of%20downstream%20task%20of%20polyp%20segmentation.%20Code%20released%20at%0Ahttps%3A//www.github.com/CUHK-AIM-Group/EndoGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17388v1&entry.124074799=Read"},
{"title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics", "author": "Suyuan Zhao and Yizhen Luo and Ganbo Yang and Yan Zhong and Hao Zhou and Zaiqing Nie", "abstract": "  Spatial Transcriptomics (ST) technologies provide biologists with rich\ninsights into single-cell biology by preserving spatial context of cells.\nBuilding foundational models for ST can significantly enhance the analysis of\nvast and complex data sources, unlocking new perspectives on the intricacies of\nbiological tissues. However, modeling ST data is inherently challenging due to\nthe need to extract multi-scale information from tissue slices containing vast\nnumbers of cells. This process requires integrating macro-scale tissue\nmorphology, micro-scale cellular microenvironment, and gene-scale gene\nexpression profile. To address this challenge, we propose SToFM, a multi-scale\nSpatial Transcriptomics Foundation Model. SToFM first performs multi-scale\ninformation extraction on each ST slice, to construct a set of ST sub-slices\nthat aggregate macro-, micro- and gene-scale information. Then an SE(2)\nTransformer is used to obtain high-quality cell representations from the\nsub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest\nhigh-resolution spatial transcriptomics corpus for pretraining. SToFM achieves\noutstanding performance on a variety of downstream tasks, such as tissue region\nsemantic segmentation and cell type annotation, demonstrating its comprehensive\nunderstanding of ST data through capturing and integrating multi-scale\ninformation.\n", "link": "http://arxiv.org/abs/2507.11588v2", "date": "2025-07-23", "relevancy": 2.4887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SToFM%3A%20a%20Multi-scale%20Foundation%20Model%20for%20Spatial%20Transcriptomics&body=Title%3A%20SToFM%3A%20a%20Multi-scale%20Foundation%20Model%20for%20Spatial%20Transcriptomics%0AAuthor%3A%20Suyuan%20Zhao%20and%20Yizhen%20Luo%20and%20Ganbo%20Yang%20and%20Yan%20Zhong%20and%20Hao%20Zhou%20and%20Zaiqing%20Nie%0AAbstract%3A%20%20%20Spatial%20Transcriptomics%20%28ST%29%20technologies%20provide%20biologists%20with%20rich%0Ainsights%20into%20single-cell%20biology%20by%20preserving%20spatial%20context%20of%20cells.%0ABuilding%20foundational%20models%20for%20ST%20can%20significantly%20enhance%20the%20analysis%20of%0Avast%20and%20complex%20data%20sources%2C%20unlocking%20new%20perspectives%20on%20the%20intricacies%20of%0Abiological%20tissues.%20However%2C%20modeling%20ST%20data%20is%20inherently%20challenging%20due%20to%0Athe%20need%20to%20extract%20multi-scale%20information%20from%20tissue%20slices%20containing%20vast%0Anumbers%20of%20cells.%20This%20process%20requires%20integrating%20macro-scale%20tissue%0Amorphology%2C%20micro-scale%20cellular%20microenvironment%2C%20and%20gene-scale%20gene%0Aexpression%20profile.%20To%20address%20this%20challenge%2C%20we%20propose%20SToFM%2C%20a%20multi-scale%0ASpatial%20Transcriptomics%20Foundation%20Model.%20SToFM%20first%20performs%20multi-scale%0Ainformation%20extraction%20on%20each%20ST%20slice%2C%20to%20construct%20a%20set%20of%20ST%20sub-slices%0Athat%20aggregate%20macro-%2C%20micro-%20and%20gene-scale%20information.%20Then%20an%20SE%282%29%0ATransformer%20is%20used%20to%20obtain%20high-quality%20cell%20representations%20from%20the%0Asub-slices.%20Additionally%2C%20we%20construct%20%5Ctextbf%7BSToCorpus-88M%7D%2C%20the%20largest%0Ahigh-resolution%20spatial%20transcriptomics%20corpus%20for%20pretraining.%20SToFM%20achieves%0Aoutstanding%20performance%20on%20a%20variety%20of%20downstream%20tasks%2C%20such%20as%20tissue%20region%0Asemantic%20segmentation%20and%20cell%20type%20annotation%2C%20demonstrating%20its%20comprehensive%0Aunderstanding%20of%20ST%20data%20through%20capturing%20and%20integrating%20multi-scale%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11588v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSToFM%253A%2520a%2520Multi-scale%2520Foundation%2520Model%2520for%2520Spatial%2520Transcriptomics%26entry.906535625%3DSuyuan%2520Zhao%2520and%2520Yizhen%2520Luo%2520and%2520Ganbo%2520Yang%2520and%2520Yan%2520Zhong%2520and%2520Hao%2520Zhou%2520and%2520Zaiqing%2520Nie%26entry.1292438233%3D%2520%2520Spatial%2520Transcriptomics%2520%2528ST%2529%2520technologies%2520provide%2520biologists%2520with%2520rich%250Ainsights%2520into%2520single-cell%2520biology%2520by%2520preserving%2520spatial%2520context%2520of%2520cells.%250ABuilding%2520foundational%2520models%2520for%2520ST%2520can%2520significantly%2520enhance%2520the%2520analysis%2520of%250Avast%2520and%2520complex%2520data%2520sources%252C%2520unlocking%2520new%2520perspectives%2520on%2520the%2520intricacies%2520of%250Abiological%2520tissues.%2520However%252C%2520modeling%2520ST%2520data%2520is%2520inherently%2520challenging%2520due%2520to%250Athe%2520need%2520to%2520extract%2520multi-scale%2520information%2520from%2520tissue%2520slices%2520containing%2520vast%250Anumbers%2520of%2520cells.%2520This%2520process%2520requires%2520integrating%2520macro-scale%2520tissue%250Amorphology%252C%2520micro-scale%2520cellular%2520microenvironment%252C%2520and%2520gene-scale%2520gene%250Aexpression%2520profile.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520SToFM%252C%2520a%2520multi-scale%250ASpatial%2520Transcriptomics%2520Foundation%2520Model.%2520SToFM%2520first%2520performs%2520multi-scale%250Ainformation%2520extraction%2520on%2520each%2520ST%2520slice%252C%2520to%2520construct%2520a%2520set%2520of%2520ST%2520sub-slices%250Athat%2520aggregate%2520macro-%252C%2520micro-%2520and%2520gene-scale%2520information.%2520Then%2520an%2520SE%25282%2529%250ATransformer%2520is%2520used%2520to%2520obtain%2520high-quality%2520cell%2520representations%2520from%2520the%250Asub-slices.%2520Additionally%252C%2520we%2520construct%2520%255Ctextbf%257BSToCorpus-88M%257D%252C%2520the%2520largest%250Ahigh-resolution%2520spatial%2520transcriptomics%2520corpus%2520for%2520pretraining.%2520SToFM%2520achieves%250Aoutstanding%2520performance%2520on%2520a%2520variety%2520of%2520downstream%2520tasks%252C%2520such%2520as%2520tissue%2520region%250Asemantic%2520segmentation%2520and%2520cell%2520type%2520annotation%252C%2520demonstrating%2520its%2520comprehensive%250Aunderstanding%2520of%2520ST%2520data%2520through%2520capturing%2520and%2520integrating%2520multi-scale%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11588v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SToFM%3A%20a%20Multi-scale%20Foundation%20Model%20for%20Spatial%20Transcriptomics&entry.906535625=Suyuan%20Zhao%20and%20Yizhen%20Luo%20and%20Ganbo%20Yang%20and%20Yan%20Zhong%20and%20Hao%20Zhou%20and%20Zaiqing%20Nie&entry.1292438233=%20%20Spatial%20Transcriptomics%20%28ST%29%20technologies%20provide%20biologists%20with%20rich%0Ainsights%20into%20single-cell%20biology%20by%20preserving%20spatial%20context%20of%20cells.%0ABuilding%20foundational%20models%20for%20ST%20can%20significantly%20enhance%20the%20analysis%20of%0Avast%20and%20complex%20data%20sources%2C%20unlocking%20new%20perspectives%20on%20the%20intricacies%20of%0Abiological%20tissues.%20However%2C%20modeling%20ST%20data%20is%20inherently%20challenging%20due%20to%0Athe%20need%20to%20extract%20multi-scale%20information%20from%20tissue%20slices%20containing%20vast%0Anumbers%20of%20cells.%20This%20process%20requires%20integrating%20macro-scale%20tissue%0Amorphology%2C%20micro-scale%20cellular%20microenvironment%2C%20and%20gene-scale%20gene%0Aexpression%20profile.%20To%20address%20this%20challenge%2C%20we%20propose%20SToFM%2C%20a%20multi-scale%0ASpatial%20Transcriptomics%20Foundation%20Model.%20SToFM%20first%20performs%20multi-scale%0Ainformation%20extraction%20on%20each%20ST%20slice%2C%20to%20construct%20a%20set%20of%20ST%20sub-slices%0Athat%20aggregate%20macro-%2C%20micro-%20and%20gene-scale%20information.%20Then%20an%20SE%282%29%0ATransformer%20is%20used%20to%20obtain%20high-quality%20cell%20representations%20from%20the%0Asub-slices.%20Additionally%2C%20we%20construct%20%5Ctextbf%7BSToCorpus-88M%7D%2C%20the%20largest%0Ahigh-resolution%20spatial%20transcriptomics%20corpus%20for%20pretraining.%20SToFM%20achieves%0Aoutstanding%20performance%20on%20a%20variety%20of%20downstream%20tasks%2C%20such%20as%20tissue%20region%0Asemantic%20segmentation%20and%20cell%20type%20annotation%2C%20demonstrating%20its%20comprehensive%0Aunderstanding%20of%20ST%20data%20through%20capturing%20and%20integrating%20multi-scale%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11588v2&entry.124074799=Read"},
{"title": "Efficient Neural Network Verification via Order Leading Exploration of\n  Branch-and-Bound Trees", "author": "Guanqin Zhang and Kota Fukuda and Zhenya Zhang and H. M. N. Dilum Bandara and Shiping Chen and Jianjun Zhao and Yulei Sui", "abstract": "  The vulnerability of neural networks to adversarial perturbations has\nnecessitated formal verification techniques that can rigorously certify the\nquality of neural networks. As the state-of-the-art, branch and bound (BaB) is\na \"divide-and-conquer\" strategy that applies off-the-shelf verifiers to\nsub-problems for which they perform better. While BaB can identify the\nsub-problems that are necessary to be split, it explores the space of these\nsub-problems in a naive \"first-come-first-serve\" manner, thereby suffering from\nan issue of inefficiency to reach a verification conclusion. To bridge this\ngap, we introduce an order over different sub-problems produced by BaB,\nconcerning with their different likelihoods of containing counterexamples.\nBased on this order, we propose a novel verification framework Oliva that\nexplores the sub-problem space by prioritizing those sub-problems that are more\nlikely to find counterexamples, in order to efficiently reach the conclusion of\nthe verification. Even if no counterexample can be found in any sub-problem, it\nonly changes the order of visiting different sub-problem and so will not lead\nto a performance degradation. Specifically, Oliva has two variants, including\n$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that\nare more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy\ninspired by simulated annealing that gradually shifts from exploration to\nexploitation to locate the globally optimal sub-problems. We experimentally\nevaluate the performance of Oliva on 690 verification problems spanning over 5\nmodels with datasets MNIST and CIFAR10. Compared to the state-of-the-art\napproaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up\nto 80X in CIFAR10.\n", "link": "http://arxiv.org/abs/2507.17453v1", "date": "2025-07-23", "relevancy": 2.4738, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Neural%20Network%20Verification%20via%20Order%20Leading%20Exploration%20of%0A%20%20Branch-and-Bound%20Trees&body=Title%3A%20Efficient%20Neural%20Network%20Verification%20via%20Order%20Leading%20Exploration%20of%0A%20%20Branch-and-Bound%20Trees%0AAuthor%3A%20Guanqin%20Zhang%20and%20Kota%20Fukuda%20and%20Zhenya%20Zhang%20and%20H.%20M.%20N.%20Dilum%20Bandara%20and%20Shiping%20Chen%20and%20Jianjun%20Zhao%20and%20Yulei%20Sui%0AAbstract%3A%20%20%20The%20vulnerability%20of%20neural%20networks%20to%20adversarial%20perturbations%20has%0Anecessitated%20formal%20verification%20techniques%20that%20can%20rigorously%20certify%20the%0Aquality%20of%20neural%20networks.%20As%20the%20state-of-the-art%2C%20branch%20and%20bound%20%28BaB%29%20is%0Aa%20%22divide-and-conquer%22%20strategy%20that%20applies%20off-the-shelf%20verifiers%20to%0Asub-problems%20for%20which%20they%20perform%20better.%20While%20BaB%20can%20identify%20the%0Asub-problems%20that%20are%20necessary%20to%20be%20split%2C%20it%20explores%20the%20space%20of%20these%0Asub-problems%20in%20a%20naive%20%22first-come-first-serve%22%20manner%2C%20thereby%20suffering%20from%0Aan%20issue%20of%20inefficiency%20to%20reach%20a%20verification%20conclusion.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20an%20order%20over%20different%20sub-problems%20produced%20by%20BaB%2C%0Aconcerning%20with%20their%20different%20likelihoods%20of%20containing%20counterexamples.%0ABased%20on%20this%20order%2C%20we%20propose%20a%20novel%20verification%20framework%20Oliva%20that%0Aexplores%20the%20sub-problem%20space%20by%20prioritizing%20those%20sub-problems%20that%20are%20more%0Alikely%20to%20find%20counterexamples%2C%20in%20order%20to%20efficiently%20reach%20the%20conclusion%20of%0Athe%20verification.%20Even%20if%20no%20counterexample%20can%20be%20found%20in%20any%20sub-problem%2C%20it%0Aonly%20changes%20the%20order%20of%20visiting%20different%20sub-problem%20and%20so%20will%20not%20lead%0Ato%20a%20performance%20degradation.%20Specifically%2C%20Oliva%20has%20two%20variants%2C%20including%0A%24Oliva%5E%7BGR%7D%24%2C%20a%20greedy%20strategy%20that%20always%20prioritizes%20the%20sub-problems%20that%0Aare%20more%20likely%20to%20find%20counterexamples%2C%20and%20%24Oliva%5E%7BSA%7D%24%2C%20a%20balanced%20strategy%0Ainspired%20by%20simulated%20annealing%20that%20gradually%20shifts%20from%20exploration%20to%0Aexploitation%20to%20locate%20the%20globally%20optimal%20sub-problems.%20We%20experimentally%0Aevaluate%20the%20performance%20of%20Oliva%20on%20690%20verification%20problems%20spanning%20over%205%0Amodels%20with%20datasets%20MNIST%20and%20CIFAR10.%20Compared%20to%20the%20state-of-the-art%0Aapproaches%2C%20we%20demonstrate%20the%20speedup%20of%20Oliva%20for%20up%20to%2025X%20in%20MNIST%2C%20and%20up%0Ato%2080X%20in%20CIFAR10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Neural%2520Network%2520Verification%2520via%2520Order%2520Leading%2520Exploration%2520of%250A%2520%2520Branch-and-Bound%2520Trees%26entry.906535625%3DGuanqin%2520Zhang%2520and%2520Kota%2520Fukuda%2520and%2520Zhenya%2520Zhang%2520and%2520H.%2520M.%2520N.%2520Dilum%2520Bandara%2520and%2520Shiping%2520Chen%2520and%2520Jianjun%2520Zhao%2520and%2520Yulei%2520Sui%26entry.1292438233%3D%2520%2520The%2520vulnerability%2520of%2520neural%2520networks%2520to%2520adversarial%2520perturbations%2520has%250Anecessitated%2520formal%2520verification%2520techniques%2520that%2520can%2520rigorously%2520certify%2520the%250Aquality%2520of%2520neural%2520networks.%2520As%2520the%2520state-of-the-art%252C%2520branch%2520and%2520bound%2520%2528BaB%2529%2520is%250Aa%2520%2522divide-and-conquer%2522%2520strategy%2520that%2520applies%2520off-the-shelf%2520verifiers%2520to%250Asub-problems%2520for%2520which%2520they%2520perform%2520better.%2520While%2520BaB%2520can%2520identify%2520the%250Asub-problems%2520that%2520are%2520necessary%2520to%2520be%2520split%252C%2520it%2520explores%2520the%2520space%2520of%2520these%250Asub-problems%2520in%2520a%2520naive%2520%2522first-come-first-serve%2522%2520manner%252C%2520thereby%2520suffering%2520from%250Aan%2520issue%2520of%2520inefficiency%2520to%2520reach%2520a%2520verification%2520conclusion.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520introduce%2520an%2520order%2520over%2520different%2520sub-problems%2520produced%2520by%2520BaB%252C%250Aconcerning%2520with%2520their%2520different%2520likelihoods%2520of%2520containing%2520counterexamples.%250ABased%2520on%2520this%2520order%252C%2520we%2520propose%2520a%2520novel%2520verification%2520framework%2520Oliva%2520that%250Aexplores%2520the%2520sub-problem%2520space%2520by%2520prioritizing%2520those%2520sub-problems%2520that%2520are%2520more%250Alikely%2520to%2520find%2520counterexamples%252C%2520in%2520order%2520to%2520efficiently%2520reach%2520the%2520conclusion%2520of%250Athe%2520verification.%2520Even%2520if%2520no%2520counterexample%2520can%2520be%2520found%2520in%2520any%2520sub-problem%252C%2520it%250Aonly%2520changes%2520the%2520order%2520of%2520visiting%2520different%2520sub-problem%2520and%2520so%2520will%2520not%2520lead%250Ato%2520a%2520performance%2520degradation.%2520Specifically%252C%2520Oliva%2520has%2520two%2520variants%252C%2520including%250A%2524Oliva%255E%257BGR%257D%2524%252C%2520a%2520greedy%2520strategy%2520that%2520always%2520prioritizes%2520the%2520sub-problems%2520that%250Aare%2520more%2520likely%2520to%2520find%2520counterexamples%252C%2520and%2520%2524Oliva%255E%257BSA%257D%2524%252C%2520a%2520balanced%2520strategy%250Ainspired%2520by%2520simulated%2520annealing%2520that%2520gradually%2520shifts%2520from%2520exploration%2520to%250Aexploitation%2520to%2520locate%2520the%2520globally%2520optimal%2520sub-problems.%2520We%2520experimentally%250Aevaluate%2520the%2520performance%2520of%2520Oliva%2520on%2520690%2520verification%2520problems%2520spanning%2520over%25205%250Amodels%2520with%2520datasets%2520MNIST%2520and%2520CIFAR10.%2520Compared%2520to%2520the%2520state-of-the-art%250Aapproaches%252C%2520we%2520demonstrate%2520the%2520speedup%2520of%2520Oliva%2520for%2520up%2520to%252025X%2520in%2520MNIST%252C%2520and%2520up%250Ato%252080X%2520in%2520CIFAR10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Neural%20Network%20Verification%20via%20Order%20Leading%20Exploration%20of%0A%20%20Branch-and-Bound%20Trees&entry.906535625=Guanqin%20Zhang%20and%20Kota%20Fukuda%20and%20Zhenya%20Zhang%20and%20H.%20M.%20N.%20Dilum%20Bandara%20and%20Shiping%20Chen%20and%20Jianjun%20Zhao%20and%20Yulei%20Sui&entry.1292438233=%20%20The%20vulnerability%20of%20neural%20networks%20to%20adversarial%20perturbations%20has%0Anecessitated%20formal%20verification%20techniques%20that%20can%20rigorously%20certify%20the%0Aquality%20of%20neural%20networks.%20As%20the%20state-of-the-art%2C%20branch%20and%20bound%20%28BaB%29%20is%0Aa%20%22divide-and-conquer%22%20strategy%20that%20applies%20off-the-shelf%20verifiers%20to%0Asub-problems%20for%20which%20they%20perform%20better.%20While%20BaB%20can%20identify%20the%0Asub-problems%20that%20are%20necessary%20to%20be%20split%2C%20it%20explores%20the%20space%20of%20these%0Asub-problems%20in%20a%20naive%20%22first-come-first-serve%22%20manner%2C%20thereby%20suffering%20from%0Aan%20issue%20of%20inefficiency%20to%20reach%20a%20verification%20conclusion.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20an%20order%20over%20different%20sub-problems%20produced%20by%20BaB%2C%0Aconcerning%20with%20their%20different%20likelihoods%20of%20containing%20counterexamples.%0ABased%20on%20this%20order%2C%20we%20propose%20a%20novel%20verification%20framework%20Oliva%20that%0Aexplores%20the%20sub-problem%20space%20by%20prioritizing%20those%20sub-problems%20that%20are%20more%0Alikely%20to%20find%20counterexamples%2C%20in%20order%20to%20efficiently%20reach%20the%20conclusion%20of%0Athe%20verification.%20Even%20if%20no%20counterexample%20can%20be%20found%20in%20any%20sub-problem%2C%20it%0Aonly%20changes%20the%20order%20of%20visiting%20different%20sub-problem%20and%20so%20will%20not%20lead%0Ato%20a%20performance%20degradation.%20Specifically%2C%20Oliva%20has%20two%20variants%2C%20including%0A%24Oliva%5E%7BGR%7D%24%2C%20a%20greedy%20strategy%20that%20always%20prioritizes%20the%20sub-problems%20that%0Aare%20more%20likely%20to%20find%20counterexamples%2C%20and%20%24Oliva%5E%7BSA%7D%24%2C%20a%20balanced%20strategy%0Ainspired%20by%20simulated%20annealing%20that%20gradually%20shifts%20from%20exploration%20to%0Aexploitation%20to%20locate%20the%20globally%20optimal%20sub-problems.%20We%20experimentally%0Aevaluate%20the%20performance%20of%20Oliva%20on%20690%20verification%20problems%20spanning%20over%205%0Amodels%20with%20datasets%20MNIST%20and%20CIFAR10.%20Compared%20to%20the%20state-of-the-art%0Aapproaches%2C%20we%20demonstrate%20the%20speedup%20of%20Oliva%20for%20up%20to%2025X%20in%20MNIST%2C%20and%20up%0Ato%2080X%20in%20CIFAR10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17453v1&entry.124074799=Read"},
{"title": "When and Where Localization Fails: An Analysis of the Iterative Closest\n  Point in Evolving Environment", "author": "Abdel-Raouf Dannaoui and Johann Laconte and Christophe Debain and Francois Pomerleau and Paul Checchin", "abstract": "  Robust relocalization in dynamic outdoor environments remains a key challenge\nfor autonomous systems relying on 3D lidar. While long-term localization has\nbeen widely studied, short-term environmental changes, occurring over days or\nweeks, remain underexplored despite their practical significance. To address\nthis gap, we present a highresolution, short-term multi-temporal dataset\ncollected weekly from February to April 2025 across natural and semi-urban\nsettings. Each session includes high-density point cloud maps, 360 deg\npanoramic images, and trajectory data. Projected lidar scans, derived from the\npoint cloud maps and modeled with sensor-accurate occlusions, are used to\nevaluate alignment accuracy against the ground truth using two Iterative\nClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show\nthat Point-to-Plane offers significantly more stable and accurate registration,\nparticularly in areas with sparse features or dense vegetation. This study\nprovides a structured dataset for evaluating short-term localization\nrobustness, a reproducible framework for analyzing scan-to-map alignment under\nnoise, and a comparative evaluation of ICP performance in evolving outdoor\nenvironments. Our analysis underscores how local geometry and environmental\nvariability affect localization success, offering insights for designing more\nresilient robotic systems.\n", "link": "http://arxiv.org/abs/2507.17531v1", "date": "2025-07-23", "relevancy": 2.454, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6449}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20and%20Where%20Localization%20Fails%3A%20An%20Analysis%20of%20the%20Iterative%20Closest%0A%20%20Point%20in%20Evolving%20Environment&body=Title%3A%20When%20and%20Where%20Localization%20Fails%3A%20An%20Analysis%20of%20the%20Iterative%20Closest%0A%20%20Point%20in%20Evolving%20Environment%0AAuthor%3A%20Abdel-Raouf%20Dannaoui%20and%20Johann%20Laconte%20and%20Christophe%20Debain%20and%20Francois%20Pomerleau%20and%20Paul%20Checchin%0AAbstract%3A%20%20%20Robust%20relocalization%20in%20dynamic%20outdoor%20environments%20remains%20a%20key%20challenge%0Afor%20autonomous%20systems%20relying%20on%203D%20lidar.%20While%20long-term%20localization%20has%0Abeen%20widely%20studied%2C%20short-term%20environmental%20changes%2C%20occurring%20over%20days%20or%0Aweeks%2C%20remain%20underexplored%20despite%20their%20practical%20significance.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20highresolution%2C%20short-term%20multi-temporal%20dataset%0Acollected%20weekly%20from%20February%20to%20April%202025%20across%20natural%20and%20semi-urban%0Asettings.%20Each%20session%20includes%20high-density%20point%20cloud%20maps%2C%20360%20deg%0Apanoramic%20images%2C%20and%20trajectory%20data.%20Projected%20lidar%20scans%2C%20derived%20from%20the%0Apoint%20cloud%20maps%20and%20modeled%20with%20sensor-accurate%20occlusions%2C%20are%20used%20to%0Aevaluate%20alignment%20accuracy%20against%20the%20ground%20truth%20using%20two%20Iterative%0AClosest%20Point%20%28ICP%29%20variants%3A%20Point-to-Point%20and%20Point-to-Plane.%20Results%20show%0Athat%20Point-to-Plane%20offers%20significantly%20more%20stable%20and%20accurate%20registration%2C%0Aparticularly%20in%20areas%20with%20sparse%20features%20or%20dense%20vegetation.%20This%20study%0Aprovides%20a%20structured%20dataset%20for%20evaluating%20short-term%20localization%0Arobustness%2C%20a%20reproducible%20framework%20for%20analyzing%20scan-to-map%20alignment%20under%0Anoise%2C%20and%20a%20comparative%20evaluation%20of%20ICP%20performance%20in%20evolving%20outdoor%0Aenvironments.%20Our%20analysis%20underscores%20how%20local%20geometry%20and%20environmental%0Avariability%20affect%20localization%20success%2C%20offering%20insights%20for%20designing%20more%0Aresilient%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520and%2520Where%2520Localization%2520Fails%253A%2520An%2520Analysis%2520of%2520the%2520Iterative%2520Closest%250A%2520%2520Point%2520in%2520Evolving%2520Environment%26entry.906535625%3DAbdel-Raouf%2520Dannaoui%2520and%2520Johann%2520Laconte%2520and%2520Christophe%2520Debain%2520and%2520Francois%2520Pomerleau%2520and%2520Paul%2520Checchin%26entry.1292438233%3D%2520%2520Robust%2520relocalization%2520in%2520dynamic%2520outdoor%2520environments%2520remains%2520a%2520key%2520challenge%250Afor%2520autonomous%2520systems%2520relying%2520on%25203D%2520lidar.%2520While%2520long-term%2520localization%2520has%250Abeen%2520widely%2520studied%252C%2520short-term%2520environmental%2520changes%252C%2520occurring%2520over%2520days%2520or%250Aweeks%252C%2520remain%2520underexplored%2520despite%2520their%2520practical%2520significance.%2520To%2520address%250Athis%2520gap%252C%2520we%2520present%2520a%2520highresolution%252C%2520short-term%2520multi-temporal%2520dataset%250Acollected%2520weekly%2520from%2520February%2520to%2520April%25202025%2520across%2520natural%2520and%2520semi-urban%250Asettings.%2520Each%2520session%2520includes%2520high-density%2520point%2520cloud%2520maps%252C%2520360%2520deg%250Apanoramic%2520images%252C%2520and%2520trajectory%2520data.%2520Projected%2520lidar%2520scans%252C%2520derived%2520from%2520the%250Apoint%2520cloud%2520maps%2520and%2520modeled%2520with%2520sensor-accurate%2520occlusions%252C%2520are%2520used%2520to%250Aevaluate%2520alignment%2520accuracy%2520against%2520the%2520ground%2520truth%2520using%2520two%2520Iterative%250AClosest%2520Point%2520%2528ICP%2529%2520variants%253A%2520Point-to-Point%2520and%2520Point-to-Plane.%2520Results%2520show%250Athat%2520Point-to-Plane%2520offers%2520significantly%2520more%2520stable%2520and%2520accurate%2520registration%252C%250Aparticularly%2520in%2520areas%2520with%2520sparse%2520features%2520or%2520dense%2520vegetation.%2520This%2520study%250Aprovides%2520a%2520structured%2520dataset%2520for%2520evaluating%2520short-term%2520localization%250Arobustness%252C%2520a%2520reproducible%2520framework%2520for%2520analyzing%2520scan-to-map%2520alignment%2520under%250Anoise%252C%2520and%2520a%2520comparative%2520evaluation%2520of%2520ICP%2520performance%2520in%2520evolving%2520outdoor%250Aenvironments.%2520Our%2520analysis%2520underscores%2520how%2520local%2520geometry%2520and%2520environmental%250Avariability%2520affect%2520localization%2520success%252C%2520offering%2520insights%2520for%2520designing%2520more%250Aresilient%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20and%20Where%20Localization%20Fails%3A%20An%20Analysis%20of%20the%20Iterative%20Closest%0A%20%20Point%20in%20Evolving%20Environment&entry.906535625=Abdel-Raouf%20Dannaoui%20and%20Johann%20Laconte%20and%20Christophe%20Debain%20and%20Francois%20Pomerleau%20and%20Paul%20Checchin&entry.1292438233=%20%20Robust%20relocalization%20in%20dynamic%20outdoor%20environments%20remains%20a%20key%20challenge%0Afor%20autonomous%20systems%20relying%20on%203D%20lidar.%20While%20long-term%20localization%20has%0Abeen%20widely%20studied%2C%20short-term%20environmental%20changes%2C%20occurring%20over%20days%20or%0Aweeks%2C%20remain%20underexplored%20despite%20their%20practical%20significance.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20highresolution%2C%20short-term%20multi-temporal%20dataset%0Acollected%20weekly%20from%20February%20to%20April%202025%20across%20natural%20and%20semi-urban%0Asettings.%20Each%20session%20includes%20high-density%20point%20cloud%20maps%2C%20360%20deg%0Apanoramic%20images%2C%20and%20trajectory%20data.%20Projected%20lidar%20scans%2C%20derived%20from%20the%0Apoint%20cloud%20maps%20and%20modeled%20with%20sensor-accurate%20occlusions%2C%20are%20used%20to%0Aevaluate%20alignment%20accuracy%20against%20the%20ground%20truth%20using%20two%20Iterative%0AClosest%20Point%20%28ICP%29%20variants%3A%20Point-to-Point%20and%20Point-to-Plane.%20Results%20show%0Athat%20Point-to-Plane%20offers%20significantly%20more%20stable%20and%20accurate%20registration%2C%0Aparticularly%20in%20areas%20with%20sparse%20features%20or%20dense%20vegetation.%20This%20study%0Aprovides%20a%20structured%20dataset%20for%20evaluating%20short-term%20localization%0Arobustness%2C%20a%20reproducible%20framework%20for%20analyzing%20scan-to-map%20alignment%20under%0Anoise%2C%20and%20a%20comparative%20evaluation%20of%20ICP%20performance%20in%20evolving%20outdoor%0Aenvironments.%20Our%20analysis%20underscores%20how%20local%20geometry%20and%20environmental%0Avariability%20affect%20localization%20success%2C%20offering%20insights%20for%20designing%20more%0Aresilient%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17531v1&entry.124074799=Read"},
{"title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras", "author": "Lingdong Kong and Dongyue Lu and Ao Liang and Rong Li and Yuhao Dong and Tianshuai Hu and Lai Xing Ng and Wei Tsang Ooi and Benoit R. Cottereau", "abstract": "  Event cameras offer microsecond-level latency and robustness to motion blur,\nmaking them ideal for understanding dynamic environments. Yet, connecting these\nasynchronous streams to human language remains an open challenge. We introduce\nTalk2Event, the first large-scale benchmark for language-driven object\ngrounding in event-based perception. Built from real-world driving data, we\nprovide over 30,000 validated referring expressions, each enriched with four\ngrounding attributes -- appearance, status, relation to viewer, and relation to\nother objects -- bridging spatial, temporal, and relational reasoning. To fully\nexploit these cues, we propose EventRefer, an attribute-aware grounding\nframework that dynamically fuses multi-attribute representations through a\nMixture of Event-Attribute Experts (MoEE). Our method adapts to different\nmodalities and scene dynamics, achieving consistent gains over state-of-the-art\nbaselines in event-only, frame-only, and event-frame fusion settings. We hope\nour dataset and approach will establish a foundation for advancing multimodal,\ntemporally-aware, and language-driven perception in real-world robotics and\nautonomy.\n", "link": "http://arxiv.org/abs/2507.17664v1", "date": "2025-07-23", "relevancy": 2.4324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6093}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talk2Event%3A%20Grounded%20Understanding%20of%20Dynamic%20Scenes%20from%20Event%20Cameras&body=Title%3A%20Talk2Event%3A%20Grounded%20Understanding%20of%20Dynamic%20Scenes%20from%20Event%20Cameras%0AAuthor%3A%20Lingdong%20Kong%20and%20Dongyue%20Lu%20and%20Ao%20Liang%20and%20Rong%20Li%20and%20Yuhao%20Dong%20and%20Tianshuai%20Hu%20and%20Lai%20Xing%20Ng%20and%20Wei%20Tsang%20Ooi%20and%20Benoit%20R.%20Cottereau%0AAbstract%3A%20%20%20Event%20cameras%20offer%20microsecond-level%20latency%20and%20robustness%20to%20motion%20blur%2C%0Amaking%20them%20ideal%20for%20understanding%20dynamic%20environments.%20Yet%2C%20connecting%20these%0Aasynchronous%20streams%20to%20human%20language%20remains%20an%20open%20challenge.%20We%20introduce%0ATalk2Event%2C%20the%20first%20large-scale%20benchmark%20for%20language-driven%20object%0Agrounding%20in%20event-based%20perception.%20Built%20from%20real-world%20driving%20data%2C%20we%0Aprovide%20over%2030%2C000%20validated%20referring%20expressions%2C%20each%20enriched%20with%20four%0Agrounding%20attributes%20--%20appearance%2C%20status%2C%20relation%20to%20viewer%2C%20and%20relation%20to%0Aother%20objects%20--%20bridging%20spatial%2C%20temporal%2C%20and%20relational%20reasoning.%20To%20fully%0Aexploit%20these%20cues%2C%20we%20propose%20EventRefer%2C%20an%20attribute-aware%20grounding%0Aframework%20that%20dynamically%20fuses%20multi-attribute%20representations%20through%20a%0AMixture%20of%20Event-Attribute%20Experts%20%28MoEE%29.%20Our%20method%20adapts%20to%20different%0Amodalities%20and%20scene%20dynamics%2C%20achieving%20consistent%20gains%20over%20state-of-the-art%0Abaselines%20in%20event-only%2C%20frame-only%2C%20and%20event-frame%20fusion%20settings.%20We%20hope%0Aour%20dataset%20and%20approach%20will%20establish%20a%20foundation%20for%20advancing%20multimodal%2C%0Atemporally-aware%2C%20and%20language-driven%20perception%20in%20real-world%20robotics%20and%0Aautonomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalk2Event%253A%2520Grounded%2520Understanding%2520of%2520Dynamic%2520Scenes%2520from%2520Event%2520Cameras%26entry.906535625%3DLingdong%2520Kong%2520and%2520Dongyue%2520Lu%2520and%2520Ao%2520Liang%2520and%2520Rong%2520Li%2520and%2520Yuhao%2520Dong%2520and%2520Tianshuai%2520Hu%2520and%2520Lai%2520Xing%2520Ng%2520and%2520Wei%2520Tsang%2520Ooi%2520and%2520Benoit%2520R.%2520Cottereau%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520microsecond-level%2520latency%2520and%2520robustness%2520to%2520motion%2520blur%252C%250Amaking%2520them%2520ideal%2520for%2520understanding%2520dynamic%2520environments.%2520Yet%252C%2520connecting%2520these%250Aasynchronous%2520streams%2520to%2520human%2520language%2520remains%2520an%2520open%2520challenge.%2520We%2520introduce%250ATalk2Event%252C%2520the%2520first%2520large-scale%2520benchmark%2520for%2520language-driven%2520object%250Agrounding%2520in%2520event-based%2520perception.%2520Built%2520from%2520real-world%2520driving%2520data%252C%2520we%250Aprovide%2520over%252030%252C000%2520validated%2520referring%2520expressions%252C%2520each%2520enriched%2520with%2520four%250Agrounding%2520attributes%2520--%2520appearance%252C%2520status%252C%2520relation%2520to%2520viewer%252C%2520and%2520relation%2520to%250Aother%2520objects%2520--%2520bridging%2520spatial%252C%2520temporal%252C%2520and%2520relational%2520reasoning.%2520To%2520fully%250Aexploit%2520these%2520cues%252C%2520we%2520propose%2520EventRefer%252C%2520an%2520attribute-aware%2520grounding%250Aframework%2520that%2520dynamically%2520fuses%2520multi-attribute%2520representations%2520through%2520a%250AMixture%2520of%2520Event-Attribute%2520Experts%2520%2528MoEE%2529.%2520Our%2520method%2520adapts%2520to%2520different%250Amodalities%2520and%2520scene%2520dynamics%252C%2520achieving%2520consistent%2520gains%2520over%2520state-of-the-art%250Abaselines%2520in%2520event-only%252C%2520frame-only%252C%2520and%2520event-frame%2520fusion%2520settings.%2520We%2520hope%250Aour%2520dataset%2520and%2520approach%2520will%2520establish%2520a%2520foundation%2520for%2520advancing%2520multimodal%252C%250Atemporally-aware%252C%2520and%2520language-driven%2520perception%2520in%2520real-world%2520robotics%2520and%250Aautonomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk2Event%3A%20Grounded%20Understanding%20of%20Dynamic%20Scenes%20from%20Event%20Cameras&entry.906535625=Lingdong%20Kong%20and%20Dongyue%20Lu%20and%20Ao%20Liang%20and%20Rong%20Li%20and%20Yuhao%20Dong%20and%20Tianshuai%20Hu%20and%20Lai%20Xing%20Ng%20and%20Wei%20Tsang%20Ooi%20and%20Benoit%20R.%20Cottereau&entry.1292438233=%20%20Event%20cameras%20offer%20microsecond-level%20latency%20and%20robustness%20to%20motion%20blur%2C%0Amaking%20them%20ideal%20for%20understanding%20dynamic%20environments.%20Yet%2C%20connecting%20these%0Aasynchronous%20streams%20to%20human%20language%20remains%20an%20open%20challenge.%20We%20introduce%0ATalk2Event%2C%20the%20first%20large-scale%20benchmark%20for%20language-driven%20object%0Agrounding%20in%20event-based%20perception.%20Built%20from%20real-world%20driving%20data%2C%20we%0Aprovide%20over%2030%2C000%20validated%20referring%20expressions%2C%20each%20enriched%20with%20four%0Agrounding%20attributes%20--%20appearance%2C%20status%2C%20relation%20to%20viewer%2C%20and%20relation%20to%0Aother%20objects%20--%20bridging%20spatial%2C%20temporal%2C%20and%20relational%20reasoning.%20To%20fully%0Aexploit%20these%20cues%2C%20we%20propose%20EventRefer%2C%20an%20attribute-aware%20grounding%0Aframework%20that%20dynamically%20fuses%20multi-attribute%20representations%20through%20a%0AMixture%20of%20Event-Attribute%20Experts%20%28MoEE%29.%20Our%20method%20adapts%20to%20different%0Amodalities%20and%20scene%20dynamics%2C%20achieving%20consistent%20gains%20over%20state-of-the-art%0Abaselines%20in%20event-only%2C%20frame-only%2C%20and%20event-frame%20fusion%20settings.%20We%20hope%0Aour%20dataset%20and%20approach%20will%20establish%20a%20foundation%20for%20advancing%20multimodal%2C%0Atemporally-aware%2C%20and%20language-driven%20perception%20in%20real-world%20robotics%20and%0Aautonomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17664v1&entry.124074799=Read"},
{"title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based\n  Priors", "author": "Chen Ma and Xinjie Xu and Shuyu Cheng and Qi Xuan", "abstract": "  One of the most practical and challenging types of black-box adversarial\nattacks is the hard-label attack, where only the top-1 predicted label is\navailable. One effective approach is to search for the optimal ray direction\nfrom the benign image that minimizes the $\\ell_p$-norm distance to the\nadversarial region. The unique advantage of this approach is that it transforms\nthe hard-label attack into a continuous optimization problem. The objective\nfunction value is the ray's radius, which can be obtained via binary search at\na high query cost. Existing methods use a \"sign trick\" in gradient estimation\nto reduce the number of queries. In this paper, we theoretically analyze the\nquality of this gradient estimation and propose a novel prior-guided approach\nto improve ray search efficiency both theoretically and empirically.\nSpecifically, we utilize the transfer-based priors from surrogate models, and\nour gradient estimators appropriately integrate them by approximating the\nprojection of the true gradient onto the subspace spanned by these priors and\nrandom directions, in a query-efficient manner. We theoretically derive the\nexpected cosine similarities between the obtained gradient estimators and the\ntrue gradient, and demonstrate the improvement achieved by incorporating\npriors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that\nour approach significantly outperforms 11 state-of-the-art methods in terms of\nquery efficiency.\n", "link": "http://arxiv.org/abs/2507.17577v1", "date": "2025-07-23", "relevancy": 2.4207, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.491}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4826}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Ray%20Search%20Procedure%20of%20Hard-label%20Attacks%20with%20Transfer-based%0A%20%20Priors&body=Title%3A%20Boosting%20Ray%20Search%20Procedure%20of%20Hard-label%20Attacks%20with%20Transfer-based%0A%20%20Priors%0AAuthor%3A%20Chen%20Ma%20and%20Xinjie%20Xu%20and%20Shuyu%20Cheng%20and%20Qi%20Xuan%0AAbstract%3A%20%20%20One%20of%20the%20most%20practical%20and%20challenging%20types%20of%20black-box%20adversarial%0Aattacks%20is%20the%20hard-label%20attack%2C%20where%20only%20the%20top-1%20predicted%20label%20is%0Aavailable.%20One%20effective%20approach%20is%20to%20search%20for%20the%20optimal%20ray%20direction%0Afrom%20the%20benign%20image%20that%20minimizes%20the%20%24%5Cell_p%24-norm%20distance%20to%20the%0Aadversarial%20region.%20The%20unique%20advantage%20of%20this%20approach%20is%20that%20it%20transforms%0Athe%20hard-label%20attack%20into%20a%20continuous%20optimization%20problem.%20The%20objective%0Afunction%20value%20is%20the%20ray%27s%20radius%2C%20which%20can%20be%20obtained%20via%20binary%20search%20at%0Aa%20high%20query%20cost.%20Existing%20methods%20use%20a%20%22sign%20trick%22%20in%20gradient%20estimation%0Ato%20reduce%20the%20number%20of%20queries.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%0Aquality%20of%20this%20gradient%20estimation%20and%20propose%20a%20novel%20prior-guided%20approach%0Ato%20improve%20ray%20search%20efficiency%20both%20theoretically%20and%20empirically.%0ASpecifically%2C%20we%20utilize%20the%20transfer-based%20priors%20from%20surrogate%20models%2C%20and%0Aour%20gradient%20estimators%20appropriately%20integrate%20them%20by%20approximating%20the%0Aprojection%20of%20the%20true%20gradient%20onto%20the%20subspace%20spanned%20by%20these%20priors%20and%0Arandom%20directions%2C%20in%20a%20query-efficient%20manner.%20We%20theoretically%20derive%20the%0Aexpected%20cosine%20similarities%20between%20the%20obtained%20gradient%20estimators%20and%20the%0Atrue%20gradient%2C%20and%20demonstrate%20the%20improvement%20achieved%20by%20incorporating%0Apriors.%20Extensive%20experiments%20on%20the%20ImageNet%20and%20CIFAR-10%20datasets%20show%20that%0Aour%20approach%20significantly%20outperforms%2011%20state-of-the-art%20methods%20in%20terms%20of%0Aquery%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Ray%2520Search%2520Procedure%2520of%2520Hard-label%2520Attacks%2520with%2520Transfer-based%250A%2520%2520Priors%26entry.906535625%3DChen%2520Ma%2520and%2520Xinjie%2520Xu%2520and%2520Shuyu%2520Cheng%2520and%2520Qi%2520Xuan%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520practical%2520and%2520challenging%2520types%2520of%2520black-box%2520adversarial%250Aattacks%2520is%2520the%2520hard-label%2520attack%252C%2520where%2520only%2520the%2520top-1%2520predicted%2520label%2520is%250Aavailable.%2520One%2520effective%2520approach%2520is%2520to%2520search%2520for%2520the%2520optimal%2520ray%2520direction%250Afrom%2520the%2520benign%2520image%2520that%2520minimizes%2520the%2520%2524%255Cell_p%2524-norm%2520distance%2520to%2520the%250Aadversarial%2520region.%2520The%2520unique%2520advantage%2520of%2520this%2520approach%2520is%2520that%2520it%2520transforms%250Athe%2520hard-label%2520attack%2520into%2520a%2520continuous%2520optimization%2520problem.%2520The%2520objective%250Afunction%2520value%2520is%2520the%2520ray%2527s%2520radius%252C%2520which%2520can%2520be%2520obtained%2520via%2520binary%2520search%2520at%250Aa%2520high%2520query%2520cost.%2520Existing%2520methods%2520use%2520a%2520%2522sign%2520trick%2522%2520in%2520gradient%2520estimation%250Ato%2520reduce%2520the%2520number%2520of%2520queries.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520analyze%2520the%250Aquality%2520of%2520this%2520gradient%2520estimation%2520and%2520propose%2520a%2520novel%2520prior-guided%2520approach%250Ato%2520improve%2520ray%2520search%2520efficiency%2520both%2520theoretically%2520and%2520empirically.%250ASpecifically%252C%2520we%2520utilize%2520the%2520transfer-based%2520priors%2520from%2520surrogate%2520models%252C%2520and%250Aour%2520gradient%2520estimators%2520appropriately%2520integrate%2520them%2520by%2520approximating%2520the%250Aprojection%2520of%2520the%2520true%2520gradient%2520onto%2520the%2520subspace%2520spanned%2520by%2520these%2520priors%2520and%250Arandom%2520directions%252C%2520in%2520a%2520query-efficient%2520manner.%2520We%2520theoretically%2520derive%2520the%250Aexpected%2520cosine%2520similarities%2520between%2520the%2520obtained%2520gradient%2520estimators%2520and%2520the%250Atrue%2520gradient%252C%2520and%2520demonstrate%2520the%2520improvement%2520achieved%2520by%2520incorporating%250Apriors.%2520Extensive%2520experiments%2520on%2520the%2520ImageNet%2520and%2520CIFAR-10%2520datasets%2520show%2520that%250Aour%2520approach%2520significantly%2520outperforms%252011%2520state-of-the-art%2520methods%2520in%2520terms%2520of%250Aquery%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Ray%20Search%20Procedure%20of%20Hard-label%20Attacks%20with%20Transfer-based%0A%20%20Priors&entry.906535625=Chen%20Ma%20and%20Xinjie%20Xu%20and%20Shuyu%20Cheng%20and%20Qi%20Xuan&entry.1292438233=%20%20One%20of%20the%20most%20practical%20and%20challenging%20types%20of%20black-box%20adversarial%0Aattacks%20is%20the%20hard-label%20attack%2C%20where%20only%20the%20top-1%20predicted%20label%20is%0Aavailable.%20One%20effective%20approach%20is%20to%20search%20for%20the%20optimal%20ray%20direction%0Afrom%20the%20benign%20image%20that%20minimizes%20the%20%24%5Cell_p%24-norm%20distance%20to%20the%0Aadversarial%20region.%20The%20unique%20advantage%20of%20this%20approach%20is%20that%20it%20transforms%0Athe%20hard-label%20attack%20into%20a%20continuous%20optimization%20problem.%20The%20objective%0Afunction%20value%20is%20the%20ray%27s%20radius%2C%20which%20can%20be%20obtained%20via%20binary%20search%20at%0Aa%20high%20query%20cost.%20Existing%20methods%20use%20a%20%22sign%20trick%22%20in%20gradient%20estimation%0Ato%20reduce%20the%20number%20of%20queries.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%0Aquality%20of%20this%20gradient%20estimation%20and%20propose%20a%20novel%20prior-guided%20approach%0Ato%20improve%20ray%20search%20efficiency%20both%20theoretically%20and%20empirically.%0ASpecifically%2C%20we%20utilize%20the%20transfer-based%20priors%20from%20surrogate%20models%2C%20and%0Aour%20gradient%20estimators%20appropriately%20integrate%20them%20by%20approximating%20the%0Aprojection%20of%20the%20true%20gradient%20onto%20the%20subspace%20spanned%20by%20these%20priors%20and%0Arandom%20directions%2C%20in%20a%20query-efficient%20manner.%20We%20theoretically%20derive%20the%0Aexpected%20cosine%20similarities%20between%20the%20obtained%20gradient%20estimators%20and%20the%0Atrue%20gradient%2C%20and%20demonstrate%20the%20improvement%20achieved%20by%20incorporating%0Apriors.%20Extensive%20experiments%20on%20the%20ImageNet%20and%20CIFAR-10%20datasets%20show%20that%0Aour%20approach%20significantly%20outperforms%2011%20state-of-the-art%20methods%20in%20terms%20of%0Aquery%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17577v1&entry.124074799=Read"},
{"title": "Towards Efficient Generative Large Language Model Serving: A Survey from\n  Algorithms to Systems", "author": "Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Hongyi Jin and Tianqi Chen and Zhihao Jia", "abstract": "  In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.\n", "link": "http://arxiv.org/abs/2312.15234v2", "date": "2025-07-23", "relevancy": 2.4197, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Generative%20Large%20Language%20Model%20Serving%3A%20A%20Survey%20from%0A%20%20Algorithms%20to%20Systems&body=Title%3A%20Towards%20Efficient%20Generative%20Large%20Language%20Model%20Serving%3A%20A%20Survey%20from%0A%20%20Algorithms%20to%20Systems%0AAuthor%3A%20Xupeng%20Miao%20and%20Gabriele%20Oliaro%20and%20Zhihao%20Zhang%20and%20Xinhao%20Cheng%20and%20Hongyi%20Jin%20and%20Tianqi%20Chen%20and%20Zhihao%20Jia%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20artificial%20intelligence%20%28AI%29%2C%20generative%0Alarge%20language%20models%20%28LLMs%29%20stand%20at%20the%20forefront%2C%20revolutionizing%20how%20we%0Ainteract%20with%20our%20data.%20However%2C%20the%20computational%20intensity%20and%20memory%0Aconsumption%20of%20deploying%20these%20models%20present%20substantial%20challenges%20in%20terms%0Aof%20serving%20efficiency%2C%20particularly%20in%20scenarios%20demanding%20low%20latency%20and%20high%0Athroughput.%20This%20survey%20addresses%20the%20imperative%20need%20for%20efficient%20LLM%20serving%0Amethodologies%20from%20a%20machine%20learning%20system%20%28MLSys%29%20research%20perspective%2C%0Astanding%20at%20the%20crux%20of%20advanced%20AI%20innovations%20and%20practical%20system%0Aoptimizations.%20We%20provide%20in-depth%20analysis%2C%20covering%20a%20spectrum%20of%20solutions%2C%0Aranging%20from%20cutting-edge%20algorithmic%20modifications%20to%20groundbreaking%20changes%0Ain%20system%20designs.%20The%20survey%20aims%20to%20provide%20a%20comprehensive%20understanding%20of%0Athe%20current%20state%20and%20future%20directions%20in%20efficient%20LLM%20serving%2C%20offering%0Avaluable%20insights%20for%20researchers%20and%20practitioners%20in%20overcoming%20the%20barriers%0Aof%20effective%20LLM%20deployment%2C%20thereby%20reshaping%20the%20future%20of%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Generative%2520Large%2520Language%2520Model%2520Serving%253A%2520A%2520Survey%2520from%250A%2520%2520Algorithms%2520to%2520Systems%26entry.906535625%3DXupeng%2520Miao%2520and%2520Gabriele%2520Oliaro%2520and%2520Zhihao%2520Zhang%2520and%2520Xinhao%2520Cheng%2520and%2520Hongyi%2520Jin%2520and%2520Tianqi%2520Chen%2520and%2520Zhihao%2520Jia%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520generative%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520stand%2520at%2520the%2520forefront%252C%2520revolutionizing%2520how%2520we%250Ainteract%2520with%2520our%2520data.%2520However%252C%2520the%2520computational%2520intensity%2520and%2520memory%250Aconsumption%2520of%2520deploying%2520these%2520models%2520present%2520substantial%2520challenges%2520in%2520terms%250Aof%2520serving%2520efficiency%252C%2520particularly%2520in%2520scenarios%2520demanding%2520low%2520latency%2520and%2520high%250Athroughput.%2520This%2520survey%2520addresses%2520the%2520imperative%2520need%2520for%2520efficient%2520LLM%2520serving%250Amethodologies%2520from%2520a%2520machine%2520learning%2520system%2520%2528MLSys%2529%2520research%2520perspective%252C%250Astanding%2520at%2520the%2520crux%2520of%2520advanced%2520AI%2520innovations%2520and%2520practical%2520system%250Aoptimizations.%2520We%2520provide%2520in-depth%2520analysis%252C%2520covering%2520a%2520spectrum%2520of%2520solutions%252C%250Aranging%2520from%2520cutting-edge%2520algorithmic%2520modifications%2520to%2520groundbreaking%2520changes%250Ain%2520system%2520designs.%2520The%2520survey%2520aims%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%250Athe%2520current%2520state%2520and%2520future%2520directions%2520in%2520efficient%2520LLM%2520serving%252C%2520offering%250Avaluable%2520insights%2520for%2520researchers%2520and%2520practitioners%2520in%2520overcoming%2520the%2520barriers%250Aof%2520effective%2520LLM%2520deployment%252C%2520thereby%2520reshaping%2520the%2520future%2520of%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Generative%20Large%20Language%20Model%20Serving%3A%20A%20Survey%20from%0A%20%20Algorithms%20to%20Systems&entry.906535625=Xupeng%20Miao%20and%20Gabriele%20Oliaro%20and%20Zhihao%20Zhang%20and%20Xinhao%20Cheng%20and%20Hongyi%20Jin%20and%20Tianqi%20Chen%20and%20Zhihao%20Jia&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20artificial%20intelligence%20%28AI%29%2C%20generative%0Alarge%20language%20models%20%28LLMs%29%20stand%20at%20the%20forefront%2C%20revolutionizing%20how%20we%0Ainteract%20with%20our%20data.%20However%2C%20the%20computational%20intensity%20and%20memory%0Aconsumption%20of%20deploying%20these%20models%20present%20substantial%20challenges%20in%20terms%0Aof%20serving%20efficiency%2C%20particularly%20in%20scenarios%20demanding%20low%20latency%20and%20high%0Athroughput.%20This%20survey%20addresses%20the%20imperative%20need%20for%20efficient%20LLM%20serving%0Amethodologies%20from%20a%20machine%20learning%20system%20%28MLSys%29%20research%20perspective%2C%0Astanding%20at%20the%20crux%20of%20advanced%20AI%20innovations%20and%20practical%20system%0Aoptimizations.%20We%20provide%20in-depth%20analysis%2C%20covering%20a%20spectrum%20of%20solutions%2C%0Aranging%20from%20cutting-edge%20algorithmic%20modifications%20to%20groundbreaking%20changes%0Ain%20system%20designs.%20The%20survey%20aims%20to%20provide%20a%20comprehensive%20understanding%20of%0Athe%20current%20state%20and%20future%20directions%20in%20efficient%20LLM%20serving%2C%20offering%0Avaluable%20insights%20for%20researchers%20and%20practitioners%20in%20overcoming%20the%20barriers%0Aof%20effective%20LLM%20deployment%2C%20thereby%20reshaping%20the%20future%20of%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15234v2&entry.124074799=Read"},
{"title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image\n  Generation with Multi-Stage Prompt Refinement", "author": "Sofia Jamil and Bollampalli Areen Reddy and Raghvendra Kumar and Sriparna Saha and Koustava Goswami and K. J. Joseph", "abstract": "  Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.\n", "link": "http://arxiv.org/abs/2507.13708v2", "date": "2025-07-23", "relevancy": 2.3864, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6037}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5952}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoemTale%20Diffusion%3A%20Minimising%20Information%20Loss%20in%20Poem%20to%20Image%0A%20%20Generation%20with%20Multi-Stage%20Prompt%20Refinement&body=Title%3A%20PoemTale%20Diffusion%3A%20Minimising%20Information%20Loss%20in%20Poem%20to%20Image%0A%20%20Generation%20with%20Multi-Stage%20Prompt%20Refinement%0AAuthor%3A%20Sofia%20Jamil%20and%20Bollampalli%20Areen%20Reddy%20and%20Raghvendra%20Kumar%20and%20Sriparna%20Saha%20and%20Koustava%20Goswami%20and%20K.%20J.%20Joseph%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20diffusion%20models%20have%20achieved%0Aremarkable%20success%20in%20generating%20realistic%20and%20diverse%20visual%20content.%20A%0Acritical%20factor%20in%20this%20process%20is%20the%20model%27s%20ability%20to%20accurately%20interpret%0Atextual%20prompts.%20However%2C%20these%20models%20often%20struggle%20with%20creative%0Aexpressions%2C%20particularly%20those%20involving%20complex%2C%20abstract%2C%20or%20highly%0Adescriptive%20language.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20training-free%20approach%0Atailored%20to%20improve%20image%20generation%20for%20a%20unique%20form%20of%20creative%20language%3A%0Apoetic%20verse%2C%20which%20frequently%20features%20layered%2C%20abstract%2C%20and%20dual%20meanings.%0AOur%20proposed%20PoemTale%20Diffusion%20approach%20aims%20to%20minimise%20the%20information%20that%0Ais%20lost%20during%20poetic%20text-to-image%20conversion%20by%20integrating%20a%20multi%20stage%0Aprompt%20refinement%20loop%20into%20Language%20Models%20to%20enhance%20the%20interpretability%20of%0Apoetic%20texts.%20To%20support%20this%2C%20we%20adapt%20existing%20state-of-the-art%20diffusion%0Amodels%20by%20modifying%20their%20self-attention%20mechanisms%20with%20a%20consistent%0Aself-attention%20technique%20to%20generate%20multiple%20consistent%20images%2C%20which%20are%20then%0Acollectively%20used%20to%20convey%20the%20poem%27s%20meaning.%20Moreover%2C%20to%20encourage%20research%0Ain%20the%20field%20of%20poetry%2C%20we%20introduce%20the%20P4I%20%28PoemForImage%29%20dataset%2C%20consisting%0Aof%201111%20poems%20sourced%20from%20multiple%20online%20and%20offline%20resources.%20We%20engaged%20a%0Apanel%20of%20poetry%20experts%20for%20qualitative%20assessments.%20The%20results%20from%20both%0Ahuman%20and%20quantitative%20evaluations%20validate%20the%20efficacy%20of%20our%20method%20and%0Acontribute%20a%20novel%20perspective%20to%20poem-to-image%20generation%20with%20enhanced%0Ainformation%20capture%20in%20the%20generated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoemTale%2520Diffusion%253A%2520Minimising%2520Information%2520Loss%2520in%2520Poem%2520to%2520Image%250A%2520%2520Generation%2520with%2520Multi-Stage%2520Prompt%2520Refinement%26entry.906535625%3DSofia%2520Jamil%2520and%2520Bollampalli%2520Areen%2520Reddy%2520and%2520Raghvendra%2520Kumar%2520and%2520Sriparna%2520Saha%2520and%2520Koustava%2520Goswami%2520and%2520K.%2520J.%2520Joseph%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520diffusion%2520models%2520have%2520achieved%250Aremarkable%2520success%2520in%2520generating%2520realistic%2520and%2520diverse%2520visual%2520content.%2520A%250Acritical%2520factor%2520in%2520this%2520process%2520is%2520the%2520model%2527s%2520ability%2520to%2520accurately%2520interpret%250Atextual%2520prompts.%2520However%252C%2520these%2520models%2520often%2520struggle%2520with%2520creative%250Aexpressions%252C%2520particularly%2520those%2520involving%2520complex%252C%2520abstract%252C%2520or%2520highly%250Adescriptive%2520language.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520training-free%2520approach%250Atailored%2520to%2520improve%2520image%2520generation%2520for%2520a%2520unique%2520form%2520of%2520creative%2520language%253A%250Apoetic%2520verse%252C%2520which%2520frequently%2520features%2520layered%252C%2520abstract%252C%2520and%2520dual%2520meanings.%250AOur%2520proposed%2520PoemTale%2520Diffusion%2520approach%2520aims%2520to%2520minimise%2520the%2520information%2520that%250Ais%2520lost%2520during%2520poetic%2520text-to-image%2520conversion%2520by%2520integrating%2520a%2520multi%2520stage%250Aprompt%2520refinement%2520loop%2520into%2520Language%2520Models%2520to%2520enhance%2520the%2520interpretability%2520of%250Apoetic%2520texts.%2520To%2520support%2520this%252C%2520we%2520adapt%2520existing%2520state-of-the-art%2520diffusion%250Amodels%2520by%2520modifying%2520their%2520self-attention%2520mechanisms%2520with%2520a%2520consistent%250Aself-attention%2520technique%2520to%2520generate%2520multiple%2520consistent%2520images%252C%2520which%2520are%2520then%250Acollectively%2520used%2520to%2520convey%2520the%2520poem%2527s%2520meaning.%2520Moreover%252C%2520to%2520encourage%2520research%250Ain%2520the%2520field%2520of%2520poetry%252C%2520we%2520introduce%2520the%2520P4I%2520%2528PoemForImage%2529%2520dataset%252C%2520consisting%250Aof%25201111%2520poems%2520sourced%2520from%2520multiple%2520online%2520and%2520offline%2520resources.%2520We%2520engaged%2520a%250Apanel%2520of%2520poetry%2520experts%2520for%2520qualitative%2520assessments.%2520The%2520results%2520from%2520both%250Ahuman%2520and%2520quantitative%2520evaluations%2520validate%2520the%2520efficacy%2520of%2520our%2520method%2520and%250Acontribute%2520a%2520novel%2520perspective%2520to%2520poem-to-image%2520generation%2520with%2520enhanced%250Ainformation%2520capture%2520in%2520the%2520generated%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoemTale%20Diffusion%3A%20Minimising%20Information%20Loss%20in%20Poem%20to%20Image%0A%20%20Generation%20with%20Multi-Stage%20Prompt%20Refinement&entry.906535625=Sofia%20Jamil%20and%20Bollampalli%20Areen%20Reddy%20and%20Raghvendra%20Kumar%20and%20Sriparna%20Saha%20and%20Koustava%20Goswami%20and%20K.%20J.%20Joseph&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20diffusion%20models%20have%20achieved%0Aremarkable%20success%20in%20generating%20realistic%20and%20diverse%20visual%20content.%20A%0Acritical%20factor%20in%20this%20process%20is%20the%20model%27s%20ability%20to%20accurately%20interpret%0Atextual%20prompts.%20However%2C%20these%20models%20often%20struggle%20with%20creative%0Aexpressions%2C%20particularly%20those%20involving%20complex%2C%20abstract%2C%20or%20highly%0Adescriptive%20language.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20training-free%20approach%0Atailored%20to%20improve%20image%20generation%20for%20a%20unique%20form%20of%20creative%20language%3A%0Apoetic%20verse%2C%20which%20frequently%20features%20layered%2C%20abstract%2C%20and%20dual%20meanings.%0AOur%20proposed%20PoemTale%20Diffusion%20approach%20aims%20to%20minimise%20the%20information%20that%0Ais%20lost%20during%20poetic%20text-to-image%20conversion%20by%20integrating%20a%20multi%20stage%0Aprompt%20refinement%20loop%20into%20Language%20Models%20to%20enhance%20the%20interpretability%20of%0Apoetic%20texts.%20To%20support%20this%2C%20we%20adapt%20existing%20state-of-the-art%20diffusion%0Amodels%20by%20modifying%20their%20self-attention%20mechanisms%20with%20a%20consistent%0Aself-attention%20technique%20to%20generate%20multiple%20consistent%20images%2C%20which%20are%20then%0Acollectively%20used%20to%20convey%20the%20poem%27s%20meaning.%20Moreover%2C%20to%20encourage%20research%0Ain%20the%20field%20of%20poetry%2C%20we%20introduce%20the%20P4I%20%28PoemForImage%29%20dataset%2C%20consisting%0Aof%201111%20poems%20sourced%20from%20multiple%20online%20and%20offline%20resources.%20We%20engaged%20a%0Apanel%20of%20poetry%20experts%20for%20qualitative%20assessments.%20The%20results%20from%20both%0Ahuman%20and%20quantitative%20evaluations%20validate%20the%20efficacy%20of%20our%20method%20and%0Acontribute%20a%20novel%20perspective%20to%20poem-to-image%20generation%20with%20enhanced%0Ainformation%20capture%20in%20the%20generated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13708v2&entry.124074799=Read"},
{"title": "Deep Generative Learning of Magnetic Frustration in Artificial Spin Ice\n  from Magnetic Force Microscopy Images", "author": "Arnab Neogi and Suryakant Mishra and Prasad P Iyer and Tzu-Ming Lu and Ezra Bussmann and Sergei Tretiak and Andrew Crandall Jones and Jian-Xin Zhu", "abstract": "  Increasingly large datasets of microscopic images with atomic resolution\nfacilitate the development of machine learning methods to identify and analyze\nsubtle physical phenomena embedded within the images. In this work, microscopic\nimages of honeycomb lattice spin-ice samples serve as datasets from which we\nautomate the calculation of net magnetic moments and directional orientations\nof spin-ice configurations. In the first stage of our workflow, machine\nlearning models are trained to accurately predict magnetic moments and\ndirections within spin-ice structures. Variational Autoencoders (VAEs), an\nemergent unsupervised deep learning technique, are employed to generate\nhigh-quality synthetic magnetic force microscopy (MFM) images and extract\nlatent feature representations, thereby reducing experimental and segmentation\nerrors. The second stage of proposed methodology enables precise identification\nand prediction of frustrated vertices and nanomagnetic segments, effectively\ncorrelating structural and functional aspects of microscopic images. This\nfacilitates the design of optimized spin-ice configurations with controlled\nfrustration patterns, enabling potential on-demand synthesis.\n", "link": "http://arxiv.org/abs/2507.17726v1", "date": "2025-07-23", "relevancy": 2.3651, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4779}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4711}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Learning%20of%20Magnetic%20Frustration%20in%20Artificial%20Spin%20Ice%0A%20%20from%20Magnetic%20Force%20Microscopy%20Images&body=Title%3A%20Deep%20Generative%20Learning%20of%20Magnetic%20Frustration%20in%20Artificial%20Spin%20Ice%0A%20%20from%20Magnetic%20Force%20Microscopy%20Images%0AAuthor%3A%20Arnab%20Neogi%20and%20Suryakant%20Mishra%20and%20Prasad%20P%20Iyer%20and%20Tzu-Ming%20Lu%20and%20Ezra%20Bussmann%20and%20Sergei%20Tretiak%20and%20Andrew%20Crandall%20Jones%20and%20Jian-Xin%20Zhu%0AAbstract%3A%20%20%20Increasingly%20large%20datasets%20of%20microscopic%20images%20with%20atomic%20resolution%0Afacilitate%20the%20development%20of%20machine%20learning%20methods%20to%20identify%20and%20analyze%0Asubtle%20physical%20phenomena%20embedded%20within%20the%20images.%20In%20this%20work%2C%20microscopic%0Aimages%20of%20honeycomb%20lattice%20spin-ice%20samples%20serve%20as%20datasets%20from%20which%20we%0Aautomate%20the%20calculation%20of%20net%20magnetic%20moments%20and%20directional%20orientations%0Aof%20spin-ice%20configurations.%20In%20the%20first%20stage%20of%20our%20workflow%2C%20machine%0Alearning%20models%20are%20trained%20to%20accurately%20predict%20magnetic%20moments%20and%0Adirections%20within%20spin-ice%20structures.%20Variational%20Autoencoders%20%28VAEs%29%2C%20an%0Aemergent%20unsupervised%20deep%20learning%20technique%2C%20are%20employed%20to%20generate%0Ahigh-quality%20synthetic%20magnetic%20force%20microscopy%20%28MFM%29%20images%20and%20extract%0Alatent%20feature%20representations%2C%20thereby%20reducing%20experimental%20and%20segmentation%0Aerrors.%20The%20second%20stage%20of%20proposed%20methodology%20enables%20precise%20identification%0Aand%20prediction%20of%20frustrated%20vertices%20and%20nanomagnetic%20segments%2C%20effectively%0Acorrelating%20structural%20and%20functional%20aspects%20of%20microscopic%20images.%20This%0Afacilitates%20the%20design%20of%20optimized%20spin-ice%20configurations%20with%20controlled%0Afrustration%20patterns%2C%20enabling%20potential%20on-demand%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Learning%2520of%2520Magnetic%2520Frustration%2520in%2520Artificial%2520Spin%2520Ice%250A%2520%2520from%2520Magnetic%2520Force%2520Microscopy%2520Images%26entry.906535625%3DArnab%2520Neogi%2520and%2520Suryakant%2520Mishra%2520and%2520Prasad%2520P%2520Iyer%2520and%2520Tzu-Ming%2520Lu%2520and%2520Ezra%2520Bussmann%2520and%2520Sergei%2520Tretiak%2520and%2520Andrew%2520Crandall%2520Jones%2520and%2520Jian-Xin%2520Zhu%26entry.1292438233%3D%2520%2520Increasingly%2520large%2520datasets%2520of%2520microscopic%2520images%2520with%2520atomic%2520resolution%250Afacilitate%2520the%2520development%2520of%2520machine%2520learning%2520methods%2520to%2520identify%2520and%2520analyze%250Asubtle%2520physical%2520phenomena%2520embedded%2520within%2520the%2520images.%2520In%2520this%2520work%252C%2520microscopic%250Aimages%2520of%2520honeycomb%2520lattice%2520spin-ice%2520samples%2520serve%2520as%2520datasets%2520from%2520which%2520we%250Aautomate%2520the%2520calculation%2520of%2520net%2520magnetic%2520moments%2520and%2520directional%2520orientations%250Aof%2520spin-ice%2520configurations.%2520In%2520the%2520first%2520stage%2520of%2520our%2520workflow%252C%2520machine%250Alearning%2520models%2520are%2520trained%2520to%2520accurately%2520predict%2520magnetic%2520moments%2520and%250Adirections%2520within%2520spin-ice%2520structures.%2520Variational%2520Autoencoders%2520%2528VAEs%2529%252C%2520an%250Aemergent%2520unsupervised%2520deep%2520learning%2520technique%252C%2520are%2520employed%2520to%2520generate%250Ahigh-quality%2520synthetic%2520magnetic%2520force%2520microscopy%2520%2528MFM%2529%2520images%2520and%2520extract%250Alatent%2520feature%2520representations%252C%2520thereby%2520reducing%2520experimental%2520and%2520segmentation%250Aerrors.%2520The%2520second%2520stage%2520of%2520proposed%2520methodology%2520enables%2520precise%2520identification%250Aand%2520prediction%2520of%2520frustrated%2520vertices%2520and%2520nanomagnetic%2520segments%252C%2520effectively%250Acorrelating%2520structural%2520and%2520functional%2520aspects%2520of%2520microscopic%2520images.%2520This%250Afacilitates%2520the%2520design%2520of%2520optimized%2520spin-ice%2520configurations%2520with%2520controlled%250Afrustration%2520patterns%252C%2520enabling%2520potential%2520on-demand%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Learning%20of%20Magnetic%20Frustration%20in%20Artificial%20Spin%20Ice%0A%20%20from%20Magnetic%20Force%20Microscopy%20Images&entry.906535625=Arnab%20Neogi%20and%20Suryakant%20Mishra%20and%20Prasad%20P%20Iyer%20and%20Tzu-Ming%20Lu%20and%20Ezra%20Bussmann%20and%20Sergei%20Tretiak%20and%20Andrew%20Crandall%20Jones%20and%20Jian-Xin%20Zhu&entry.1292438233=%20%20Increasingly%20large%20datasets%20of%20microscopic%20images%20with%20atomic%20resolution%0Afacilitate%20the%20development%20of%20machine%20learning%20methods%20to%20identify%20and%20analyze%0Asubtle%20physical%20phenomena%20embedded%20within%20the%20images.%20In%20this%20work%2C%20microscopic%0Aimages%20of%20honeycomb%20lattice%20spin-ice%20samples%20serve%20as%20datasets%20from%20which%20we%0Aautomate%20the%20calculation%20of%20net%20magnetic%20moments%20and%20directional%20orientations%0Aof%20spin-ice%20configurations.%20In%20the%20first%20stage%20of%20our%20workflow%2C%20machine%0Alearning%20models%20are%20trained%20to%20accurately%20predict%20magnetic%20moments%20and%0Adirections%20within%20spin-ice%20structures.%20Variational%20Autoencoders%20%28VAEs%29%2C%20an%0Aemergent%20unsupervised%20deep%20learning%20technique%2C%20are%20employed%20to%20generate%0Ahigh-quality%20synthetic%20magnetic%20force%20microscopy%20%28MFM%29%20images%20and%20extract%0Alatent%20feature%20representations%2C%20thereby%20reducing%20experimental%20and%20segmentation%0Aerrors.%20The%20second%20stage%20of%20proposed%20methodology%20enables%20precise%20identification%0Aand%20prediction%20of%20frustrated%20vertices%20and%20nanomagnetic%20segments%2C%20effectively%0Acorrelating%20structural%20and%20functional%20aspects%20of%20microscopic%20images.%20This%0Afacilitates%20the%20design%20of%20optimized%20spin-ice%20configurations%20with%20controlled%0Afrustration%20patterns%2C%20enabling%20potential%20on-demand%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17726v1&entry.124074799=Read"},
{"title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding", "author": "Xiaoyi Zhang and Zhaoyang Jia and Zongyu Guo and Jiahao Li and Bin Li and Houqiang Li and Yan Lu", "abstract": "  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code has been released in\nhttps://github.com/microsoft/DeepVideoDiscovery.\n", "link": "http://arxiv.org/abs/2505.18079v3", "date": "2025-07-23", "relevancy": 2.3232, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding&body=Title%3A%20Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding%0AAuthor%3A%20Xiaoyi%20Zhang%20and%20Zhaoyang%20Jia%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Houqiang%20Li%20and%20Yan%20Lu%0AAbstract%3A%20%20%20Long-form%20video%20understanding%20presents%20significant%20challenges%20due%20to%0Aextensive%20temporal-spatial%20complexity%20and%20the%20difficulty%20of%20question%20answering%0Aunder%20such%20extended%20contexts.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20considerable%20advancements%20in%20video%20analysis%20capabilities%20and%20long%0Acontext%20handling%2C%20they%20continue%20to%20exhibit%20limitations%20when%20processing%0Ainformation-dense%20hour-long%20videos.%20To%20overcome%20such%20limitations%2C%20we%20propose%0Athe%20Deep%20Video%20Discovery%20agent%20to%20leverage%20an%20agentic%20search%20strategy%20over%0Asegmented%20video%20clips.%20Different%20from%20previous%20video%20agents%20manually%20designing%0Aa%20rigid%20workflow%2C%20our%20approach%20emphasizes%20the%20autonomous%20nature%20of%20agents.%20By%0Aproviding%20a%20set%20of%20search-centric%20tools%20on%20multi-granular%20video%20database%2C%20our%0ADVD%20agent%20leverages%20the%20advanced%20reasoning%20capability%20of%20LLM%20to%20plan%20on%20its%0Acurrent%20observation%20state%2C%20strategically%20selects%20tools%2C%20formulates%20appropriate%0Aparameters%20for%20actions%2C%20and%20iteratively%20refines%20its%20internal%20reasoning%20in%20light%0Aof%20the%20gathered%20information.%20We%20perform%20comprehensive%20evaluation%20on%20multiple%0Along%20video%20understanding%20benchmarks%20that%20demonstrates%20the%20advantage%20of%20the%0Aentire%20system%20design.%20Our%20DVD%20agent%20achieves%20SOTA%20performance%2C%20significantly%0Asurpassing%20prior%20works%20by%20a%20large%20margin%20on%20the%20challenging%20LVBench%20dataset.%0AComprehensive%20ablation%20studies%20and%20in-depth%20tool%20analyses%20are%20also%20provided%2C%0Ayielding%20insights%20to%20further%20advance%20intelligent%20agents%20tailored%20for%20long-form%0Avideo%20understanding%20tasks.%20The%20code%20has%20been%20released%20in%0Ahttps%3A//github.com/microsoft/DeepVideoDiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18079v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Video%2520Discovery%253A%2520Agentic%2520Search%2520with%2520Tool%2520Use%2520for%2520Long-form%2520Video%250A%2520%2520Understanding%26entry.906535625%3DXiaoyi%2520Zhang%2520and%2520Zhaoyang%2520Jia%2520and%2520Zongyu%2520Guo%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Houqiang%2520Li%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520Long-form%2520video%2520understanding%2520presents%2520significant%2520challenges%2520due%2520to%250Aextensive%2520temporal-spatial%2520complexity%2520and%2520the%2520difficulty%2520of%2520question%2520answering%250Aunder%2520such%2520extended%2520contexts.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Ademonstrated%2520considerable%2520advancements%2520in%2520video%2520analysis%2520capabilities%2520and%2520long%250Acontext%2520handling%252C%2520they%2520continue%2520to%2520exhibit%2520limitations%2520when%2520processing%250Ainformation-dense%2520hour-long%2520videos.%2520To%2520overcome%2520such%2520limitations%252C%2520we%2520propose%250Athe%2520Deep%2520Video%2520Discovery%2520agent%2520to%2520leverage%2520an%2520agentic%2520search%2520strategy%2520over%250Asegmented%2520video%2520clips.%2520Different%2520from%2520previous%2520video%2520agents%2520manually%2520designing%250Aa%2520rigid%2520workflow%252C%2520our%2520approach%2520emphasizes%2520the%2520autonomous%2520nature%2520of%2520agents.%2520By%250Aproviding%2520a%2520set%2520of%2520search-centric%2520tools%2520on%2520multi-granular%2520video%2520database%252C%2520our%250ADVD%2520agent%2520leverages%2520the%2520advanced%2520reasoning%2520capability%2520of%2520LLM%2520to%2520plan%2520on%2520its%250Acurrent%2520observation%2520state%252C%2520strategically%2520selects%2520tools%252C%2520formulates%2520appropriate%250Aparameters%2520for%2520actions%252C%2520and%2520iteratively%2520refines%2520its%2520internal%2520reasoning%2520in%2520light%250Aof%2520the%2520gathered%2520information.%2520We%2520perform%2520comprehensive%2520evaluation%2520on%2520multiple%250Along%2520video%2520understanding%2520benchmarks%2520that%2520demonstrates%2520the%2520advantage%2520of%2520the%250Aentire%2520system%2520design.%2520Our%2520DVD%2520agent%2520achieves%2520SOTA%2520performance%252C%2520significantly%250Asurpassing%2520prior%2520works%2520by%2520a%2520large%2520margin%2520on%2520the%2520challenging%2520LVBench%2520dataset.%250AComprehensive%2520ablation%2520studies%2520and%2520in-depth%2520tool%2520analyses%2520are%2520also%2520provided%252C%250Ayielding%2520insights%2520to%2520further%2520advance%2520intelligent%2520agents%2520tailored%2520for%2520long-form%250Avideo%2520understanding%2520tasks.%2520The%2520code%2520has%2520been%2520released%2520in%250Ahttps%253A//github.com/microsoft/DeepVideoDiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18079v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding&entry.906535625=Xiaoyi%20Zhang%20and%20Zhaoyang%20Jia%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Houqiang%20Li%20and%20Yan%20Lu&entry.1292438233=%20%20Long-form%20video%20understanding%20presents%20significant%20challenges%20due%20to%0Aextensive%20temporal-spatial%20complexity%20and%20the%20difficulty%20of%20question%20answering%0Aunder%20such%20extended%20contexts.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20considerable%20advancements%20in%20video%20analysis%20capabilities%20and%20long%0Acontext%20handling%2C%20they%20continue%20to%20exhibit%20limitations%20when%20processing%0Ainformation-dense%20hour-long%20videos.%20To%20overcome%20such%20limitations%2C%20we%20propose%0Athe%20Deep%20Video%20Discovery%20agent%20to%20leverage%20an%20agentic%20search%20strategy%20over%0Asegmented%20video%20clips.%20Different%20from%20previous%20video%20agents%20manually%20designing%0Aa%20rigid%20workflow%2C%20our%20approach%20emphasizes%20the%20autonomous%20nature%20of%20agents.%20By%0Aproviding%20a%20set%20of%20search-centric%20tools%20on%20multi-granular%20video%20database%2C%20our%0ADVD%20agent%20leverages%20the%20advanced%20reasoning%20capability%20of%20LLM%20to%20plan%20on%20its%0Acurrent%20observation%20state%2C%20strategically%20selects%20tools%2C%20formulates%20appropriate%0Aparameters%20for%20actions%2C%20and%20iteratively%20refines%20its%20internal%20reasoning%20in%20light%0Aof%20the%20gathered%20information.%20We%20perform%20comprehensive%20evaluation%20on%20multiple%0Along%20video%20understanding%20benchmarks%20that%20demonstrates%20the%20advantage%20of%20the%0Aentire%20system%20design.%20Our%20DVD%20agent%20achieves%20SOTA%20performance%2C%20significantly%0Asurpassing%20prior%20works%20by%20a%20large%20margin%20on%20the%20challenging%20LVBench%20dataset.%0AComprehensive%20ablation%20studies%20and%20in-depth%20tool%20analyses%20are%20also%20provided%2C%0Ayielding%20insights%20to%20further%20advance%20intelligent%20agents%20tailored%20for%20long-form%0Avideo%20understanding%20tasks.%20The%20code%20has%20been%20released%20in%0Ahttps%3A//github.com/microsoft/DeepVideoDiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18079v3&entry.124074799=Read"},
{"title": "Perspective-Invariant 3D Object Detection", "author": "Ao Liang and Lingdong Kong and Dongyue Lu and Youquan Liu and Jian Fang and Huaici Zhao and Wei Tsang Ooi", "abstract": "  With the rise of robotics, LiDAR-based 3D object detection has garnered\nsignificant attention in both academia and industry. However, existing datasets\nand methods predominantly focus on vehicle-mounted platforms, leaving other\nautonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,\nthe first benchmark featuring LiDAR data and 3D bounding box annotations\ncollected from multiple platforms: vehicle, quadruped, and drone, thereby\nfacilitating research in 3D object detection for non-vehicle platforms as well\nas cross-platform 3D detection. Based on Pi3DET, we propose a novel\ncross-platform adaptation framework that transfers knowledge from the\nwell-studied vehicle platform to other platforms. This framework achieves\nperspective-invariant 3D detection through robust alignment at both geometric\nand feature levels. Additionally, we establish a benchmark to evaluate the\nresilience and robustness of current 3D detectors in cross-platform scenarios,\nproviding valuable insights for developing adaptive 3D perception systems.\nExtensive experiments validate the effectiveness of our approach on challenging\ncross-platform tasks, demonstrating substantial gains over existing adaptation\nmethods. We hope this work paves the way for generalizable and unified 3D\nperception systems across diverse and complex environments. Our Pi3DET dataset,\ncross-platform benchmark suite, and annotation toolkit have been made publicly\navailable.\n", "link": "http://arxiv.org/abs/2507.17665v1", "date": "2025-07-23", "relevancy": 2.3176, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5997}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perspective-Invariant%203D%20Object%20Detection&body=Title%3A%20Perspective-Invariant%203D%20Object%20Detection%0AAuthor%3A%20Ao%20Liang%20and%20Lingdong%20Kong%20and%20Dongyue%20Lu%20and%20Youquan%20Liu%20and%20Jian%20Fang%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20With%20the%20rise%20of%20robotics%2C%20LiDAR-based%203D%20object%20detection%20has%20garnered%0Asignificant%20attention%20in%20both%20academia%20and%20industry.%20However%2C%20existing%20datasets%0Aand%20methods%20predominantly%20focus%20on%20vehicle-mounted%20platforms%2C%20leaving%20other%0Aautonomous%20platforms%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Pi3DET%2C%0Athe%20first%20benchmark%20featuring%20LiDAR%20data%20and%203D%20bounding%20box%20annotations%0Acollected%20from%20multiple%20platforms%3A%20vehicle%2C%20quadruped%2C%20and%20drone%2C%20thereby%0Afacilitating%20research%20in%203D%20object%20detection%20for%20non-vehicle%20platforms%20as%20well%0Aas%20cross-platform%203D%20detection.%20Based%20on%20Pi3DET%2C%20we%20propose%20a%20novel%0Across-platform%20adaptation%20framework%20that%20transfers%20knowledge%20from%20the%0Awell-studied%20vehicle%20platform%20to%20other%20platforms.%20This%20framework%20achieves%0Aperspective-invariant%203D%20detection%20through%20robust%20alignment%20at%20both%20geometric%0Aand%20feature%20levels.%20Additionally%2C%20we%20establish%20a%20benchmark%20to%20evaluate%20the%0Aresilience%20and%20robustness%20of%20current%203D%20detectors%20in%20cross-platform%20scenarios%2C%0Aproviding%20valuable%20insights%20for%20developing%20adaptive%203D%20perception%20systems.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20on%20challenging%0Across-platform%20tasks%2C%20demonstrating%20substantial%20gains%20over%20existing%20adaptation%0Amethods.%20We%20hope%20this%20work%20paves%20the%20way%20for%20generalizable%20and%20unified%203D%0Aperception%20systems%20across%20diverse%20and%20complex%20environments.%20Our%20Pi3DET%20dataset%2C%0Across-platform%20benchmark%20suite%2C%20and%20annotation%20toolkit%20have%20been%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerspective-Invariant%25203D%2520Object%2520Detection%26entry.906535625%3DAo%2520Liang%2520and%2520Lingdong%2520Kong%2520and%2520Dongyue%2520Lu%2520and%2520Youquan%2520Liu%2520and%2520Jian%2520Fang%2520and%2520Huaici%2520Zhao%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520robotics%252C%2520LiDAR-based%25203D%2520object%2520detection%2520has%2520garnered%250Asignificant%2520attention%2520in%2520both%2520academia%2520and%2520industry.%2520However%252C%2520existing%2520datasets%250Aand%2520methods%2520predominantly%2520focus%2520on%2520vehicle-mounted%2520platforms%252C%2520leaving%2520other%250Aautonomous%2520platforms%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Pi3DET%252C%250Athe%2520first%2520benchmark%2520featuring%2520LiDAR%2520data%2520and%25203D%2520bounding%2520box%2520annotations%250Acollected%2520from%2520multiple%2520platforms%253A%2520vehicle%252C%2520quadruped%252C%2520and%2520drone%252C%2520thereby%250Afacilitating%2520research%2520in%25203D%2520object%2520detection%2520for%2520non-vehicle%2520platforms%2520as%2520well%250Aas%2520cross-platform%25203D%2520detection.%2520Based%2520on%2520Pi3DET%252C%2520we%2520propose%2520a%2520novel%250Across-platform%2520adaptation%2520framework%2520that%2520transfers%2520knowledge%2520from%2520the%250Awell-studied%2520vehicle%2520platform%2520to%2520other%2520platforms.%2520This%2520framework%2520achieves%250Aperspective-invariant%25203D%2520detection%2520through%2520robust%2520alignment%2520at%2520both%2520geometric%250Aand%2520feature%2520levels.%2520Additionally%252C%2520we%2520establish%2520a%2520benchmark%2520to%2520evaluate%2520the%250Aresilience%2520and%2520robustness%2520of%2520current%25203D%2520detectors%2520in%2520cross-platform%2520scenarios%252C%250Aproviding%2520valuable%2520insights%2520for%2520developing%2520adaptive%25203D%2520perception%2520systems.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520challenging%250Across-platform%2520tasks%252C%2520demonstrating%2520substantial%2520gains%2520over%2520existing%2520adaptation%250Amethods.%2520We%2520hope%2520this%2520work%2520paves%2520the%2520way%2520for%2520generalizable%2520and%2520unified%25203D%250Aperception%2520systems%2520across%2520diverse%2520and%2520complex%2520environments.%2520Our%2520Pi3DET%2520dataset%252C%250Across-platform%2520benchmark%2520suite%252C%2520and%2520annotation%2520toolkit%2520have%2520been%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perspective-Invariant%203D%20Object%20Detection&entry.906535625=Ao%20Liang%20and%20Lingdong%20Kong%20and%20Dongyue%20Lu%20and%20Youquan%20Liu%20and%20Jian%20Fang%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20With%20the%20rise%20of%20robotics%2C%20LiDAR-based%203D%20object%20detection%20has%20garnered%0Asignificant%20attention%20in%20both%20academia%20and%20industry.%20However%2C%20existing%20datasets%0Aand%20methods%20predominantly%20focus%20on%20vehicle-mounted%20platforms%2C%20leaving%20other%0Aautonomous%20platforms%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Pi3DET%2C%0Athe%20first%20benchmark%20featuring%20LiDAR%20data%20and%203D%20bounding%20box%20annotations%0Acollected%20from%20multiple%20platforms%3A%20vehicle%2C%20quadruped%2C%20and%20drone%2C%20thereby%0Afacilitating%20research%20in%203D%20object%20detection%20for%20non-vehicle%20platforms%20as%20well%0Aas%20cross-platform%203D%20detection.%20Based%20on%20Pi3DET%2C%20we%20propose%20a%20novel%0Across-platform%20adaptation%20framework%20that%20transfers%20knowledge%20from%20the%0Awell-studied%20vehicle%20platform%20to%20other%20platforms.%20This%20framework%20achieves%0Aperspective-invariant%203D%20detection%20through%20robust%20alignment%20at%20both%20geometric%0Aand%20feature%20levels.%20Additionally%2C%20we%20establish%20a%20benchmark%20to%20evaluate%20the%0Aresilience%20and%20robustness%20of%20current%203D%20detectors%20in%20cross-platform%20scenarios%2C%0Aproviding%20valuable%20insights%20for%20developing%20adaptive%203D%20perception%20systems.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20on%20challenging%0Across-platform%20tasks%2C%20demonstrating%20substantial%20gains%20over%20existing%20adaptation%0Amethods.%20We%20hope%20this%20work%20paves%20the%20way%20for%20generalizable%20and%20unified%203D%0Aperception%20systems%20across%20diverse%20and%20complex%20environments.%20Our%20Pi3DET%20dataset%2C%0Across-platform%20benchmark%20suite%2C%20and%20annotation%20toolkit%20have%20been%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17665v1&entry.124074799=Read"},
{"title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction", "author": "Shijie Lyu", "abstract": "  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n", "link": "http://arxiv.org/abs/2505.10027v2", "date": "2025-07-23", "relevancy": 2.3114, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6493}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5689}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORL-LDM%3A%20Offline%20Reinforcement%20Learning%20Guided%20Latent%20Diffusion%20Model%0A%20%20Super-Resolution%20Reconstruction&body=Title%3A%20ORL-LDM%3A%20Offline%20Reinforcement%20Learning%20Guided%20Latent%20Diffusion%20Model%0A%20%20Super-Resolution%20Reconstruction%0AAuthor%3A%20Shijie%20Lyu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20remote%20sensing%20technology%2C%20super-resolution%0Aimage%20reconstruction%20is%20of%20great%20research%20and%20practical%20significance.%20Existing%0Adeep%20learning%20methods%20have%20made%20progress%20but%20still%20face%20limitations%20in%20handling%0Acomplex%20scenes%20and%20preserving%20image%20details.%20This%20paper%20proposes%20a%0Areinforcement%20learning-based%20latent%20diffusion%20model%20%28LDM%29%20fine-tuning%20method%0Afor%20remote%20sensing%20image%20super-resolution.%20The%20method%20constructs%20a%0Areinforcement%20learning%20environment%20with%20states%2C%20actions%2C%20and%20rewards%2C%0Aoptimizing%20decision%20objectives%20through%20proximal%20policy%20optimization%20%28PPO%29%0Aduring%20the%20reverse%20denoising%20process%20of%20the%20LDM%20model.%20Experiments%20on%20the%0ARESISC45%20dataset%20show%20significant%20improvements%20over%20the%20baseline%20model%20in%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%2C%20with%20PSNR%20increasing%20by%203-4dB%2C%20SSIM%20improving%20by%200.08-0.11%2C%0Aand%20LPIPS%20reducing%20by%200.06-0.10%2C%20particularly%20in%20structured%20and%20complex%20natural%0Ascenes.%20The%20results%20demonstrate%20the%20method%27s%20effectiveness%20in%20enhancing%0Asuper-resolution%20quality%20and%20adaptability%20across%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORL-LDM%253A%2520Offline%2520Reinforcement%2520Learning%2520Guided%2520Latent%2520Diffusion%2520Model%250A%2520%2520Super-Resolution%2520Reconstruction%26entry.906535625%3DShijie%2520Lyu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520remote%2520sensing%2520technology%252C%2520super-resolution%250Aimage%2520reconstruction%2520is%2520of%2520great%2520research%2520and%2520practical%2520significance.%2520Existing%250Adeep%2520learning%2520methods%2520have%2520made%2520progress%2520but%2520still%2520face%2520limitations%2520in%2520handling%250Acomplex%2520scenes%2520and%2520preserving%2520image%2520details.%2520This%2520paper%2520proposes%2520a%250Areinforcement%2520learning-based%2520latent%2520diffusion%2520model%2520%2528LDM%2529%2520fine-tuning%2520method%250Afor%2520remote%2520sensing%2520image%2520super-resolution.%2520The%2520method%2520constructs%2520a%250Areinforcement%2520learning%2520environment%2520with%2520states%252C%2520actions%252C%2520and%2520rewards%252C%250Aoptimizing%2520decision%2520objectives%2520through%2520proximal%2520policy%2520optimization%2520%2528PPO%2529%250Aduring%2520the%2520reverse%2520denoising%2520process%2520of%2520the%2520LDM%2520model.%2520Experiments%2520on%2520the%250ARESISC45%2520dataset%2520show%2520significant%2520improvements%2520over%2520the%2520baseline%2520model%2520in%2520PSNR%252C%250ASSIM%252C%2520and%2520LPIPS%252C%2520with%2520PSNR%2520increasing%2520by%25203-4dB%252C%2520SSIM%2520improving%2520by%25200.08-0.11%252C%250Aand%2520LPIPS%2520reducing%2520by%25200.06-0.10%252C%2520particularly%2520in%2520structured%2520and%2520complex%2520natural%250Ascenes.%2520The%2520results%2520demonstrate%2520the%2520method%2527s%2520effectiveness%2520in%2520enhancing%250Asuper-resolution%2520quality%2520and%2520adaptability%2520across%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORL-LDM%3A%20Offline%20Reinforcement%20Learning%20Guided%20Latent%20Diffusion%20Model%0A%20%20Super-Resolution%20Reconstruction&entry.906535625=Shijie%20Lyu&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20remote%20sensing%20technology%2C%20super-resolution%0Aimage%20reconstruction%20is%20of%20great%20research%20and%20practical%20significance.%20Existing%0Adeep%20learning%20methods%20have%20made%20progress%20but%20still%20face%20limitations%20in%20handling%0Acomplex%20scenes%20and%20preserving%20image%20details.%20This%20paper%20proposes%20a%0Areinforcement%20learning-based%20latent%20diffusion%20model%20%28LDM%29%20fine-tuning%20method%0Afor%20remote%20sensing%20image%20super-resolution.%20The%20method%20constructs%20a%0Areinforcement%20learning%20environment%20with%20states%2C%20actions%2C%20and%20rewards%2C%0Aoptimizing%20decision%20objectives%20through%20proximal%20policy%20optimization%20%28PPO%29%0Aduring%20the%20reverse%20denoising%20process%20of%20the%20LDM%20model.%20Experiments%20on%20the%0ARESISC45%20dataset%20show%20significant%20improvements%20over%20the%20baseline%20model%20in%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%2C%20with%20PSNR%20increasing%20by%203-4dB%2C%20SSIM%20improving%20by%200.08-0.11%2C%0Aand%20LPIPS%20reducing%20by%200.06-0.10%2C%20particularly%20in%20structured%20and%20complex%20natural%0Ascenes.%20The%20results%20demonstrate%20the%20method%27s%20effectiveness%20in%20enhancing%0Asuper-resolution%20quality%20and%20adaptability%20across%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10027v2&entry.124074799=Read"},
{"title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "author": "Maciej K. Wozniak and Lianhang Liu and Yixi Cai and Patric Jensfelt", "abstract": "  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n", "link": "http://arxiv.org/abs/2507.17596v1", "date": "2025-07-23", "relevancy": 2.301, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.574}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving&body=Title%3A%20PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Maciej%20K.%20Wozniak%20and%20Lianhang%20Liu%20and%20Yixi%20Cai%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20While%20end-to-end%20autonomous%20driving%20models%20show%20promising%20results%2C%20their%0Apractical%20deployment%20is%20often%20hindered%20by%20large%20model%20sizes%2C%20a%20reliance%20on%0Aexpensive%20LiDAR%20sensors%20and%20computationally%20intensive%20BEV%20feature%0Arepresentations.%20This%20limits%20their%20scalability%2C%20especially%20for%20mass-market%0Avehicles%20equipped%20only%20with%20cameras.%20To%20address%20these%20challenges%2C%20we%20propose%0APRIX%20%28Plan%20from%20Raw%20Pixels%29.%20Our%20novel%20and%20efficient%20end-to-end%20driving%0Aarchitecture%20operates%20using%20only%20camera%20data%2C%20without%20explicit%20BEV%0Arepresentation%20and%20forgoing%20the%20need%20for%20LiDAR.%20PRIX%20leverages%20a%20visual%20feature%0Aextractor%20coupled%20with%20a%20generative%20planning%20head%20to%20predict%20safe%20trajectories%0Afrom%20raw%20pixel%20inputs%20directly.%20A%20core%20component%20of%20our%20architecture%20is%20the%0AContext-aware%20Recalibration%20Transformer%20%28CaRT%29%2C%20a%20novel%20module%20designed%20to%0Aeffectively%20enhance%20multi-level%20visual%20features%20for%20more%20robust%20planning.%20We%0Ademonstrate%20through%20comprehensive%20experiments%20that%20PRIX%20achieves%0Astate-of-the-art%20performance%20on%20the%20NavSim%20and%20nuScenes%20benchmarks%2C%20matching%0Athe%20capabilities%20of%20larger%2C%20multimodal%20diffusion%20planners%20while%20being%0Asignificantly%20more%20efficient%20in%20terms%20of%20inference%20speed%20and%20model%20size%2C%20making%0Ait%20a%20practical%20solution%20for%20real-world%20deployment.%20Our%20work%20is%20open-source%20and%0Athe%20code%20will%20be%20at%20https%3A//maxiuw.github.io/prix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIX%253A%2520Learning%2520to%2520Plan%2520from%2520Raw%2520Pixels%2520for%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DMaciej%2520K.%2520Wozniak%2520and%2520Lianhang%2520Liu%2520and%2520Yixi%2520Cai%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520While%2520end-to-end%2520autonomous%2520driving%2520models%2520show%2520promising%2520results%252C%2520their%250Apractical%2520deployment%2520is%2520often%2520hindered%2520by%2520large%2520model%2520sizes%252C%2520a%2520reliance%2520on%250Aexpensive%2520LiDAR%2520sensors%2520and%2520computationally%2520intensive%2520BEV%2520feature%250Arepresentations.%2520This%2520limits%2520their%2520scalability%252C%2520especially%2520for%2520mass-market%250Avehicles%2520equipped%2520only%2520with%2520cameras.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250APRIX%2520%2528Plan%2520from%2520Raw%2520Pixels%2529.%2520Our%2520novel%2520and%2520efficient%2520end-to-end%2520driving%250Aarchitecture%2520operates%2520using%2520only%2520camera%2520data%252C%2520without%2520explicit%2520BEV%250Arepresentation%2520and%2520forgoing%2520the%2520need%2520for%2520LiDAR.%2520PRIX%2520leverages%2520a%2520visual%2520feature%250Aextractor%2520coupled%2520with%2520a%2520generative%2520planning%2520head%2520to%2520predict%2520safe%2520trajectories%250Afrom%2520raw%2520pixel%2520inputs%2520directly.%2520A%2520core%2520component%2520of%2520our%2520architecture%2520is%2520the%250AContext-aware%2520Recalibration%2520Transformer%2520%2528CaRT%2529%252C%2520a%2520novel%2520module%2520designed%2520to%250Aeffectively%2520enhance%2520multi-level%2520visual%2520features%2520for%2520more%2520robust%2520planning.%2520We%250Ademonstrate%2520through%2520comprehensive%2520experiments%2520that%2520PRIX%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520NavSim%2520and%2520nuScenes%2520benchmarks%252C%2520matching%250Athe%2520capabilities%2520of%2520larger%252C%2520multimodal%2520diffusion%2520planners%2520while%2520being%250Asignificantly%2520more%2520efficient%2520in%2520terms%2520of%2520inference%2520speed%2520and%2520model%2520size%252C%2520making%250Ait%2520a%2520practical%2520solution%2520for%2520real-world%2520deployment.%2520Our%2520work%2520is%2520open-source%2520and%250Athe%2520code%2520will%2520be%2520at%2520https%253A//maxiuw.github.io/prix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIX%3A%20Learning%20to%20Plan%20from%20Raw%20Pixels%20for%20End-to-End%20Autonomous%20Driving&entry.906535625=Maciej%20K.%20Wozniak%20and%20Lianhang%20Liu%20and%20Yixi%20Cai%20and%20Patric%20Jensfelt&entry.1292438233=%20%20While%20end-to-end%20autonomous%20driving%20models%20show%20promising%20results%2C%20their%0Apractical%20deployment%20is%20often%20hindered%20by%20large%20model%20sizes%2C%20a%20reliance%20on%0Aexpensive%20LiDAR%20sensors%20and%20computationally%20intensive%20BEV%20feature%0Arepresentations.%20This%20limits%20their%20scalability%2C%20especially%20for%20mass-market%0Avehicles%20equipped%20only%20with%20cameras.%20To%20address%20these%20challenges%2C%20we%20propose%0APRIX%20%28Plan%20from%20Raw%20Pixels%29.%20Our%20novel%20and%20efficient%20end-to-end%20driving%0Aarchitecture%20operates%20using%20only%20camera%20data%2C%20without%20explicit%20BEV%0Arepresentation%20and%20forgoing%20the%20need%20for%20LiDAR.%20PRIX%20leverages%20a%20visual%20feature%0Aextractor%20coupled%20with%20a%20generative%20planning%20head%20to%20predict%20safe%20trajectories%0Afrom%20raw%20pixel%20inputs%20directly.%20A%20core%20component%20of%20our%20architecture%20is%20the%0AContext-aware%20Recalibration%20Transformer%20%28CaRT%29%2C%20a%20novel%20module%20designed%20to%0Aeffectively%20enhance%20multi-level%20visual%20features%20for%20more%20robust%20planning.%20We%0Ademonstrate%20through%20comprehensive%20experiments%20that%20PRIX%20achieves%0Astate-of-the-art%20performance%20on%20the%20NavSim%20and%20nuScenes%20benchmarks%2C%20matching%0Athe%20capabilities%20of%20larger%2C%20multimodal%20diffusion%20planners%20while%20being%0Asignificantly%20more%20efficient%20in%20terms%20of%20inference%20speed%20and%20model%20size%2C%20making%0Ait%20a%20practical%20solution%20for%20real-world%20deployment.%20Our%20work%20is%20open-source%20and%0Athe%20code%20will%20be%20at%20https%3A//maxiuw.github.io/prix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17596v1&entry.124074799=Read"},
{"title": "RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with\n  Retrieval-Augmented Learning", "author": "Jiacheng Zuo and Haibo Hu and Zikang Zhou and Yufei Cui and Ziquan Liu and Jianping Wang and Nan Guan and Jin Wang and Chun Jason Xue", "abstract": "  In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.\n", "link": "http://arxiv.org/abs/2501.12296v3", "date": "2025-07-23", "relevancy": 2.2953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning&body=Title%3A%20RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning%0AAuthor%3A%20Jiacheng%20Zuo%20and%20Haibo%20Hu%20and%20Zikang%20Zhou%20and%20Yufei%20Cui%20and%20Ziquan%20Liu%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Jin%20Wang%20and%20Chun%20Jason%20Xue%0AAbstract%3A%20%20%20In%20the%20pursuit%20of%20robust%20autonomous%20driving%20systems%2C%20models%20trained%20on%0Areal-world%20datasets%20often%20struggle%20to%20adapt%20to%20new%20environments%2C%20particularly%0Awhen%20confronted%20with%20corner%20cases%20such%20as%20extreme%20weather%20conditions.%0ACollecting%20these%20corner%20cases%20in%20the%20real%20world%20is%20non-trivial%2C%20which%0Anecessitates%20the%20use%20of%20simulators%20for%20validation.%20However%2Cthe%20high%0Acomputational%20cost%20and%20the%20domain%20gap%20in%20data%20distribution%20have%20hindered%20the%0Aseamless%20transition%20between%20real%20and%20simulated%20driving%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20Retrieval-Augmented%20Learning%20for%20Autonomous%20Driving%0A%28RALAD%29%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20real-to-sim%20gap%20at%20a%20low%0Acost.%20RALAD%20features%20three%20primary%20designs%2C%20including%20%281%29%20domain%20adaptation%20via%0Aan%20enhanced%20Optimal%20Transport%20%28OT%29%20method%20that%20accounts%20for%20both%20individual%20and%0Agrouped%20image%20distances%2C%20%282%29%20a%20simple%20and%20unified%20framework%20that%20can%20be%20applied%0Ato%20various%20models%2C%20and%20%283%29%20efficient%20fine-tuning%20techniques%20that%20freeze%20the%0Acomputationally%20expensive%20layers%20while%20maintaining%20robustness.%20Experimental%0Aresults%20demonstrate%20that%20RALAD%20compensates%20for%20the%20performance%20degradation%20in%0Asimulated%20environments%20while%20maintaining%20accuracy%20in%20real-world%20scenarios%0Aacross%20three%20different%20models.%20Taking%20Cross%20View%20as%20an%20example%2C%20the%20mIOU%20and%0AmAP%20metrics%20in%20real-world%20scenarios%20remain%20stable%20before%20and%20after%20RALAD%0Afine-tuning%2C%20while%20in%20simulated%20environments%2Cthe%20mIOU%20and%20mAP%20metrics%20are%0Aimproved%20by%2010.30%25%20and%2012.29%25%2C%20respectively.%20Moreover%2C%20the%20re-training%20cost%20of%0Aour%20approach%20is%20reduced%20by%20approximately%2088.1%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JiachengZuo/RALAD.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12296v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRALAD%253A%2520Bridging%2520the%2520Real-to-Sim%2520Domain%2520Gap%2520in%2520Autonomous%2520Driving%2520with%250A%2520%2520Retrieval-Augmented%2520Learning%26entry.906535625%3DJiacheng%2520Zuo%2520and%2520Haibo%2520Hu%2520and%2520Zikang%2520Zhou%2520and%2520Yufei%2520Cui%2520and%2520Ziquan%2520Liu%2520and%2520Jianping%2520Wang%2520and%2520Nan%2520Guan%2520and%2520Jin%2520Wang%2520and%2520Chun%2520Jason%2520Xue%26entry.1292438233%3D%2520%2520In%2520the%2520pursuit%2520of%2520robust%2520autonomous%2520driving%2520systems%252C%2520models%2520trained%2520on%250Areal-world%2520datasets%2520often%2520struggle%2520to%2520adapt%2520to%2520new%2520environments%252C%2520particularly%250Awhen%2520confronted%2520with%2520corner%2520cases%2520such%2520as%2520extreme%2520weather%2520conditions.%250ACollecting%2520these%2520corner%2520cases%2520in%2520the%2520real%2520world%2520is%2520non-trivial%252C%2520which%250Anecessitates%2520the%2520use%2520of%2520simulators%2520for%2520validation.%2520However%252Cthe%2520high%250Acomputational%2520cost%2520and%2520the%2520domain%2520gap%2520in%2520data%2520distribution%2520have%2520hindered%2520the%250Aseamless%2520transition%2520between%2520real%2520and%2520simulated%2520driving%2520scenarios.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520propose%2520Retrieval-Augmented%2520Learning%2520for%2520Autonomous%2520Driving%250A%2528RALAD%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520bridge%2520the%2520real-to-sim%2520gap%2520at%2520a%2520low%250Acost.%2520RALAD%2520features%2520three%2520primary%2520designs%252C%2520including%2520%25281%2529%2520domain%2520adaptation%2520via%250Aan%2520enhanced%2520Optimal%2520Transport%2520%2528OT%2529%2520method%2520that%2520accounts%2520for%2520both%2520individual%2520and%250Agrouped%2520image%2520distances%252C%2520%25282%2529%2520a%2520simple%2520and%2520unified%2520framework%2520that%2520can%2520be%2520applied%250Ato%2520various%2520models%252C%2520and%2520%25283%2529%2520efficient%2520fine-tuning%2520techniques%2520that%2520freeze%2520the%250Acomputationally%2520expensive%2520layers%2520while%2520maintaining%2520robustness.%2520Experimental%250Aresults%2520demonstrate%2520that%2520RALAD%2520compensates%2520for%2520the%2520performance%2520degradation%2520in%250Asimulated%2520environments%2520while%2520maintaining%2520accuracy%2520in%2520real-world%2520scenarios%250Aacross%2520three%2520different%2520models.%2520Taking%2520Cross%2520View%2520as%2520an%2520example%252C%2520the%2520mIOU%2520and%250AmAP%2520metrics%2520in%2520real-world%2520scenarios%2520remain%2520stable%2520before%2520and%2520after%2520RALAD%250Afine-tuning%252C%2520while%2520in%2520simulated%2520environments%252Cthe%2520mIOU%2520and%2520mAP%2520metrics%2520are%250Aimproved%2520by%252010.30%2525%2520and%252012.29%2525%252C%2520respectively.%2520Moreover%252C%2520the%2520re-training%2520cost%2520of%250Aour%2520approach%2520is%2520reduced%2520by%2520approximately%252088.1%2525.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JiachengZuo/RALAD.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12296v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RALAD%3A%20Bridging%20the%20Real-to-Sim%20Domain%20Gap%20in%20Autonomous%20Driving%20with%0A%20%20Retrieval-Augmented%20Learning&entry.906535625=Jiacheng%20Zuo%20and%20Haibo%20Hu%20and%20Zikang%20Zhou%20and%20Yufei%20Cui%20and%20Ziquan%20Liu%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Jin%20Wang%20and%20Chun%20Jason%20Xue&entry.1292438233=%20%20In%20the%20pursuit%20of%20robust%20autonomous%20driving%20systems%2C%20models%20trained%20on%0Areal-world%20datasets%20often%20struggle%20to%20adapt%20to%20new%20environments%2C%20particularly%0Awhen%20confronted%20with%20corner%20cases%20such%20as%20extreme%20weather%20conditions.%0ACollecting%20these%20corner%20cases%20in%20the%20real%20world%20is%20non-trivial%2C%20which%0Anecessitates%20the%20use%20of%20simulators%20for%20validation.%20However%2Cthe%20high%0Acomputational%20cost%20and%20the%20domain%20gap%20in%20data%20distribution%20have%20hindered%20the%0Aseamless%20transition%20between%20real%20and%20simulated%20driving%20scenarios.%20To%20tackle%0Athis%20challenge%2C%20we%20propose%20Retrieval-Augmented%20Learning%20for%20Autonomous%20Driving%0A%28RALAD%29%2C%20a%20novel%20framework%20designed%20to%20bridge%20the%20real-to-sim%20gap%20at%20a%20low%0Acost.%20RALAD%20features%20three%20primary%20designs%2C%20including%20%281%29%20domain%20adaptation%20via%0Aan%20enhanced%20Optimal%20Transport%20%28OT%29%20method%20that%20accounts%20for%20both%20individual%20and%0Agrouped%20image%20distances%2C%20%282%29%20a%20simple%20and%20unified%20framework%20that%20can%20be%20applied%0Ato%20various%20models%2C%20and%20%283%29%20efficient%20fine-tuning%20techniques%20that%20freeze%20the%0Acomputationally%20expensive%20layers%20while%20maintaining%20robustness.%20Experimental%0Aresults%20demonstrate%20that%20RALAD%20compensates%20for%20the%20performance%20degradation%20in%0Asimulated%20environments%20while%20maintaining%20accuracy%20in%20real-world%20scenarios%0Aacross%20three%20different%20models.%20Taking%20Cross%20View%20as%20an%20example%2C%20the%20mIOU%20and%0AmAP%20metrics%20in%20real-world%20scenarios%20remain%20stable%20before%20and%20after%20RALAD%0Afine-tuning%2C%20while%20in%20simulated%20environments%2Cthe%20mIOU%20and%20mAP%20metrics%20are%0Aimproved%20by%2010.30%25%20and%2012.29%25%2C%20respectively.%20Moreover%2C%20the%20re-training%20cost%20of%0Aour%20approach%20is%20reduced%20by%20approximately%2088.1%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JiachengZuo/RALAD.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12296v3&entry.124074799=Read"},
{"title": "Turing Test 2.0: The General Intelligence Threshold", "author": "Georgios Mappouras", "abstract": "  With the rise of artificial intelligence (A.I.) and large language models\nlike ChatGPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a system\n(computer or any other) has reached or surpassed A.G.I. To achieve this, we\nmake two new contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing test 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing test 2.0 framework\non modern A.I. models.\n", "link": "http://arxiv.org/abs/2505.19550v4", "date": "2025-07-23", "relevancy": 2.2796, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4875}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4449}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turing%20Test%202.0%3A%20The%20General%20Intelligence%20Threshold&body=Title%3A%20Turing%20Test%202.0%3A%20The%20General%20Intelligence%20Threshold%0AAuthor%3A%20Georgios%20Mappouras%0AAbstract%3A%20%20%20With%20the%20rise%20of%20artificial%20intelligence%20%28A.I.%29%20and%20large%20language%20models%0Alike%20ChatGPT%2C%20a%20new%20race%20for%20achieving%20artificial%20general%20intelligence%20%28A.G.I%29%0Ahas%20started.%20While%20many%20speculate%20how%20and%20when%20A.I.%20will%20achieve%20A.G.I.%2C%20there%0Ais%20no%20clear%20agreement%20on%20how%20A.G.I.%20can%20be%20detected%20in%20A.I.%20models%2C%20even%20when%0Apopular%20tools%20like%20the%20Turing%20test%20%28and%20its%20modern%20variations%29%20are%20used%20to%0Ameasure%20their%20intelligence.%20In%20this%20work%2C%20we%20discuss%20why%20traditional%20methods%0Alike%20the%20Turing%20test%20do%20not%20suffice%20for%20measuring%20or%20detecting%20A.G.I.%20and%0Aprovide%20a%20new%2C%20practical%20method%20that%20can%20be%20used%20to%20decide%20if%20a%20system%0A%28computer%20or%20any%20other%29%20has%20reached%20or%20surpassed%20A.G.I.%20To%20achieve%20this%2C%20we%0Amake%20two%20new%20contributions.%20First%2C%20we%20present%20a%20clear%20definition%20for%20general%0Aintelligence%20%28G.I.%29%20and%20set%20a%20G.I.%20Threshold%20%28G.I.T.%29%20that%20can%20be%20used%20to%0Adistinguish%20between%20systems%20that%20achieve%20A.G.I.%20and%20systems%20that%20do%20not.%0ASecond%2C%20we%20present%20a%20new%20framework%20on%20how%20to%20construct%20tests%20that%20can%20detect%20if%0Aa%20system%20has%20achieved%20G.I.%20in%20a%20simple%2C%20comprehensive%2C%20and%20clear-cut%20fail/pass%0Away.%20We%20call%20this%20novel%20framework%20the%20Turing%20test%202.0.%20We%20then%20demonstrate%0Areal-life%20examples%20of%20applying%20tests%20that%20follow%20our%20Turing%20test%202.0%20framework%0Aon%20modern%20A.I.%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19550v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuring%2520Test%25202.0%253A%2520The%2520General%2520Intelligence%2520Threshold%26entry.906535625%3DGeorgios%2520Mappouras%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520artificial%2520intelligence%2520%2528A.I.%2529%2520and%2520large%2520language%2520models%250Alike%2520ChatGPT%252C%2520a%2520new%2520race%2520for%2520achieving%2520artificial%2520general%2520intelligence%2520%2528A.G.I%2529%250Ahas%2520started.%2520While%2520many%2520speculate%2520how%2520and%2520when%2520A.I.%2520will%2520achieve%2520A.G.I.%252C%2520there%250Ais%2520no%2520clear%2520agreement%2520on%2520how%2520A.G.I.%2520can%2520be%2520detected%2520in%2520A.I.%2520models%252C%2520even%2520when%250Apopular%2520tools%2520like%2520the%2520Turing%2520test%2520%2528and%2520its%2520modern%2520variations%2529%2520are%2520used%2520to%250Ameasure%2520their%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520discuss%2520why%2520traditional%2520methods%250Alike%2520the%2520Turing%2520test%2520do%2520not%2520suffice%2520for%2520measuring%2520or%2520detecting%2520A.G.I.%2520and%250Aprovide%2520a%2520new%252C%2520practical%2520method%2520that%2520can%2520be%2520used%2520to%2520decide%2520if%2520a%2520system%250A%2528computer%2520or%2520any%2520other%2529%2520has%2520reached%2520or%2520surpassed%2520A.G.I.%2520To%2520achieve%2520this%252C%2520we%250Amake%2520two%2520new%2520contributions.%2520First%252C%2520we%2520present%2520a%2520clear%2520definition%2520for%2520general%250Aintelligence%2520%2528G.I.%2529%2520and%2520set%2520a%2520G.I.%2520Threshold%2520%2528G.I.T.%2529%2520that%2520can%2520be%2520used%2520to%250Adistinguish%2520between%2520systems%2520that%2520achieve%2520A.G.I.%2520and%2520systems%2520that%2520do%2520not.%250ASecond%252C%2520we%2520present%2520a%2520new%2520framework%2520on%2520how%2520to%2520construct%2520tests%2520that%2520can%2520detect%2520if%250Aa%2520system%2520has%2520achieved%2520G.I.%2520in%2520a%2520simple%252C%2520comprehensive%252C%2520and%2520clear-cut%2520fail/pass%250Away.%2520We%2520call%2520this%2520novel%2520framework%2520the%2520Turing%2520test%25202.0.%2520We%2520then%2520demonstrate%250Areal-life%2520examples%2520of%2520applying%2520tests%2520that%2520follow%2520our%2520Turing%2520test%25202.0%2520framework%250Aon%2520modern%2520A.I.%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19550v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turing%20Test%202.0%3A%20The%20General%20Intelligence%20Threshold&entry.906535625=Georgios%20Mappouras&entry.1292438233=%20%20With%20the%20rise%20of%20artificial%20intelligence%20%28A.I.%29%20and%20large%20language%20models%0Alike%20ChatGPT%2C%20a%20new%20race%20for%20achieving%20artificial%20general%20intelligence%20%28A.G.I%29%0Ahas%20started.%20While%20many%20speculate%20how%20and%20when%20A.I.%20will%20achieve%20A.G.I.%2C%20there%0Ais%20no%20clear%20agreement%20on%20how%20A.G.I.%20can%20be%20detected%20in%20A.I.%20models%2C%20even%20when%0Apopular%20tools%20like%20the%20Turing%20test%20%28and%20its%20modern%20variations%29%20are%20used%20to%0Ameasure%20their%20intelligence.%20In%20this%20work%2C%20we%20discuss%20why%20traditional%20methods%0Alike%20the%20Turing%20test%20do%20not%20suffice%20for%20measuring%20or%20detecting%20A.G.I.%20and%0Aprovide%20a%20new%2C%20practical%20method%20that%20can%20be%20used%20to%20decide%20if%20a%20system%0A%28computer%20or%20any%20other%29%20has%20reached%20or%20surpassed%20A.G.I.%20To%20achieve%20this%2C%20we%0Amake%20two%20new%20contributions.%20First%2C%20we%20present%20a%20clear%20definition%20for%20general%0Aintelligence%20%28G.I.%29%20and%20set%20a%20G.I.%20Threshold%20%28G.I.T.%29%20that%20can%20be%20used%20to%0Adistinguish%20between%20systems%20that%20achieve%20A.G.I.%20and%20systems%20that%20do%20not.%0ASecond%2C%20we%20present%20a%20new%20framework%20on%20how%20to%20construct%20tests%20that%20can%20detect%20if%0Aa%20system%20has%20achieved%20G.I.%20in%20a%20simple%2C%20comprehensive%2C%20and%20clear-cut%20fail/pass%0Away.%20We%20call%20this%20novel%20framework%20the%20Turing%20test%202.0.%20We%20then%20demonstrate%0Areal-life%20examples%20of%20applying%20tests%20that%20follow%20our%20Turing%20test%202.0%20framework%0Aon%20modern%20A.I.%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19550v4&entry.124074799=Read"},
{"title": "Fairness Evaluation of Large Language Models in Academic Library\n  Reference Services", "author": "Haining Wang and Jason Clark and Yueru Yan and Star Bradley and Ruiyang Chen and Yiqiong Zhang and Hengyi Fu and Zuoyu Tian", "abstract": "  As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.\n", "link": "http://arxiv.org/abs/2507.04224v2", "date": "2025-07-23", "relevancy": 2.2796, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%0A%20%20Reference%20Services&body=Title%3A%20Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%0A%20%20Reference%20Services%0AAuthor%3A%20Haining%20Wang%20and%20Jason%20Clark%20and%20Yueru%20Yan%20and%20Star%20Bradley%20and%20Ruiyang%20Chen%20and%20Yiqiong%20Zhang%20and%20Hengyi%20Fu%20and%20Zuoyu%20Tian%0AAbstract%3A%20%20%20As%20libraries%20explore%20large%20language%20models%20%28LLMs%29%20for%20use%20in%20virtual%0Areference%20services%2C%20a%20key%20question%20arises%3A%20Can%20LLMs%20serve%20all%20users%20equitably%2C%0Aregardless%20of%20demographics%20or%20social%20status%3F%20While%20they%20offer%20great%20potential%0Afor%20scalable%20support%2C%20LLMs%20may%20also%20reproduce%20societal%20biases%20embedded%20in%20their%0Atraining%20data%2C%20risking%20the%20integrity%20of%20libraries%27%20commitment%20to%20equitable%0Aservice.%20To%20address%20this%20concern%2C%20we%20evaluate%20whether%20LLMs%20differentiate%0Aresponses%20across%20user%20identities%20by%20prompting%20six%20state-of-the-art%20LLMs%20to%0Aassist%20patrons%20differing%20in%20sex%2C%20race/ethnicity%2C%20and%20institutional%20role.%20We%0Afound%20no%20evidence%20of%20differentiation%20by%20race%20or%20ethnicity%2C%20and%20only%20minor%0Aevidence%20of%20stereotypical%20bias%20against%20women%20in%20one%20model.%20LLMs%20demonstrated%0Anuanced%20accommodation%20of%20institutional%20roles%20through%20the%20use%20of%20linguistic%0Achoices%20related%20to%20formality%2C%20politeness%2C%20and%20domain-specific%20vocabularies%2C%0Areflecting%20professional%20norms%20rather%20than%20discriminatory%20treatment.%20These%0Afindings%20suggest%20that%20current%20LLMs%20show%20a%20promising%20degree%20of%20readiness%20to%0Asupport%20equitable%20and%20contextually%20appropriate%20communication%20in%20academic%0Alibrary%20reference%20services.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520in%2520Academic%2520Library%250A%2520%2520Reference%2520Services%26entry.906535625%3DHaining%2520Wang%2520and%2520Jason%2520Clark%2520and%2520Yueru%2520Yan%2520and%2520Star%2520Bradley%2520and%2520Ruiyang%2520Chen%2520and%2520Yiqiong%2520Zhang%2520and%2520Hengyi%2520Fu%2520and%2520Zuoyu%2520Tian%26entry.1292438233%3D%2520%2520As%2520libraries%2520explore%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520use%2520in%2520virtual%250Areference%2520services%252C%2520a%2520key%2520question%2520arises%253A%2520Can%2520LLMs%2520serve%2520all%2520users%2520equitably%252C%250Aregardless%2520of%2520demographics%2520or%2520social%2520status%253F%2520While%2520they%2520offer%2520great%2520potential%250Afor%2520scalable%2520support%252C%2520LLMs%2520may%2520also%2520reproduce%2520societal%2520biases%2520embedded%2520in%2520their%250Atraining%2520data%252C%2520risking%2520the%2520integrity%2520of%2520libraries%2527%2520commitment%2520to%2520equitable%250Aservice.%2520To%2520address%2520this%2520concern%252C%2520we%2520evaluate%2520whether%2520LLMs%2520differentiate%250Aresponses%2520across%2520user%2520identities%2520by%2520prompting%2520six%2520state-of-the-art%2520LLMs%2520to%250Aassist%2520patrons%2520differing%2520in%2520sex%252C%2520race/ethnicity%252C%2520and%2520institutional%2520role.%2520We%250Afound%2520no%2520evidence%2520of%2520differentiation%2520by%2520race%2520or%2520ethnicity%252C%2520and%2520only%2520minor%250Aevidence%2520of%2520stereotypical%2520bias%2520against%2520women%2520in%2520one%2520model.%2520LLMs%2520demonstrated%250Anuanced%2520accommodation%2520of%2520institutional%2520roles%2520through%2520the%2520use%2520of%2520linguistic%250Achoices%2520related%2520to%2520formality%252C%2520politeness%252C%2520and%2520domain-specific%2520vocabularies%252C%250Areflecting%2520professional%2520norms%2520rather%2520than%2520discriminatory%2520treatment.%2520These%250Afindings%2520suggest%2520that%2520current%2520LLMs%2520show%2520a%2520promising%2520degree%2520of%2520readiness%2520to%250Asupport%2520equitable%2520and%2520contextually%2520appropriate%2520communication%2520in%2520academic%250Alibrary%2520reference%2520services.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%0A%20%20Reference%20Services&entry.906535625=Haining%20Wang%20and%20Jason%20Clark%20and%20Yueru%20Yan%20and%20Star%20Bradley%20and%20Ruiyang%20Chen%20and%20Yiqiong%20Zhang%20and%20Hengyi%20Fu%20and%20Zuoyu%20Tian&entry.1292438233=%20%20As%20libraries%20explore%20large%20language%20models%20%28LLMs%29%20for%20use%20in%20virtual%0Areference%20services%2C%20a%20key%20question%20arises%3A%20Can%20LLMs%20serve%20all%20users%20equitably%2C%0Aregardless%20of%20demographics%20or%20social%20status%3F%20While%20they%20offer%20great%20potential%0Afor%20scalable%20support%2C%20LLMs%20may%20also%20reproduce%20societal%20biases%20embedded%20in%20their%0Atraining%20data%2C%20risking%20the%20integrity%20of%20libraries%27%20commitment%20to%20equitable%0Aservice.%20To%20address%20this%20concern%2C%20we%20evaluate%20whether%20LLMs%20differentiate%0Aresponses%20across%20user%20identities%20by%20prompting%20six%20state-of-the-art%20LLMs%20to%0Aassist%20patrons%20differing%20in%20sex%2C%20race/ethnicity%2C%20and%20institutional%20role.%20We%0Afound%20no%20evidence%20of%20differentiation%20by%20race%20or%20ethnicity%2C%20and%20only%20minor%0Aevidence%20of%20stereotypical%20bias%20against%20women%20in%20one%20model.%20LLMs%20demonstrated%0Anuanced%20accommodation%20of%20institutional%20roles%20through%20the%20use%20of%20linguistic%0Achoices%20related%20to%20formality%2C%20politeness%2C%20and%20domain-specific%20vocabularies%2C%0Areflecting%20professional%20norms%20rather%20than%20discriminatory%20treatment.%20These%0Afindings%20suggest%20that%20current%20LLMs%20show%20a%20promising%20degree%20of%20readiness%20to%0Asupport%20equitable%20and%20contextually%20appropriate%20communication%20in%20academic%0Alibrary%20reference%20services.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04224v2&entry.124074799=Read"},
{"title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via\n  Reinforcement Learning", "author": "Bob Zhang and Haoran Li and Tao Zhang and Cilin Yan and Jiayin Cai and Yanbin Hao", "abstract": "  Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications that involve complex multi-image\ncompositions and multi-modal instructions, revealing limitations in cross-image\nreasoning and generalization. To address these challenges, we adopt a\nReinforcement Learning (RL) based post-training strategy to improve the\nreasoning of MLLMs in multi-image grounding tasks. Our approach begins with\nsynthesizing high-quality chain-of-thought (CoT) data for cold-start\ninitialization, followed by supervised fine-tuning (SFT) using low-rank\nadaptation (LoRA). The cold-start training stage enables the model to identify\ncorrect solutions. Subsequently, we perform rejection sampling using the merged\nSFT model to curate high-quality RL data and leverage rule-based RL to guide\nthe model toward optimal reasoning paths. Extensive experimental results\ndemonstrate the effectiveness of our approach, yielding improvements of +9.04%\non MIG-Bench, +6.37% on MC-Bench, and +4.98% on several out-of-domain reasoning\ngrounding benchmarks compared to the SFT baseline. Furthermore, our method\nexhibits strong generalization in multi-image perception, with gains of +3.1%\nand +2.4% over the base model on BLINK and MMIU benchmarks, respectively.\n", "link": "http://arxiv.org/abs/2507.00748v2", "date": "2025-07-23", "relevancy": 2.2755, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5824}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Reasoning%20of%20Multi-Image%20Grounding%20in%20MLLMs%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20Improving%20the%20Reasoning%20of%20Multi-Image%20Grounding%20in%20MLLMs%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Bob%20Zhang%20and%20Haoran%20Li%20and%20Tao%20Zhang%20and%20Cilin%20Yan%20and%20Jiayin%20Cai%20and%20Yanbin%20Hao%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20visual%20grounding%0Ain%20single-image%20scenarios%20with%20textual%20references.%20However%2C%20their%20performance%0Adegrades%20when%20handling%20real-world%20applications%20that%20involve%20complex%20multi-image%0Acompositions%20and%20multi-modal%20instructions%2C%20revealing%20limitations%20in%20cross-image%0Areasoning%20and%20generalization.%20To%20address%20these%20challenges%2C%20we%20adopt%20a%0AReinforcement%20Learning%20%28RL%29%20based%20post-training%20strategy%20to%20improve%20the%0Areasoning%20of%20MLLMs%20in%20multi-image%20grounding%20tasks.%20Our%20approach%20begins%20with%0Asynthesizing%20high-quality%20chain-of-thought%20%28CoT%29%20data%20for%20cold-start%0Ainitialization%2C%20followed%20by%20supervised%20fine-tuning%20%28SFT%29%20using%20low-rank%0Aadaptation%20%28LoRA%29.%20The%20cold-start%20training%20stage%20enables%20the%20model%20to%20identify%0Acorrect%20solutions.%20Subsequently%2C%20we%20perform%20rejection%20sampling%20using%20the%20merged%0ASFT%20model%20to%20curate%20high-quality%20RL%20data%20and%20leverage%20rule-based%20RL%20to%20guide%0Athe%20model%20toward%20optimal%20reasoning%20paths.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%2C%20yielding%20improvements%20of%20%2B9.04%25%0Aon%20MIG-Bench%2C%20%2B6.37%25%20on%20MC-Bench%2C%20and%20%2B4.98%25%20on%20several%20out-of-domain%20reasoning%0Agrounding%20benchmarks%20compared%20to%20the%20SFT%20baseline.%20Furthermore%2C%20our%20method%0Aexhibits%20strong%20generalization%20in%20multi-image%20perception%2C%20with%20gains%20of%20%2B3.1%25%0Aand%20%2B2.4%25%20over%20the%20base%20model%20on%20BLINK%20and%20MMIU%20benchmarks%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Reasoning%2520of%2520Multi-Image%2520Grounding%2520in%2520MLLMs%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DBob%2520Zhang%2520and%2520Haoran%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Cilin%2520Yan%2520and%2520Jiayin%2520Cai%2520and%2520Yanbin%2520Hao%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520visual%2520grounding%250Ain%2520single-image%2520scenarios%2520with%2520textual%2520references.%2520However%252C%2520their%2520performance%250Adegrades%2520when%2520handling%2520real-world%2520applications%2520that%2520involve%2520complex%2520multi-image%250Acompositions%2520and%2520multi-modal%2520instructions%252C%2520revealing%2520limitations%2520in%2520cross-image%250Areasoning%2520and%2520generalization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520adopt%2520a%250AReinforcement%2520Learning%2520%2528RL%2529%2520based%2520post-training%2520strategy%2520to%2520improve%2520the%250Areasoning%2520of%2520MLLMs%2520in%2520multi-image%2520grounding%2520tasks.%2520Our%2520approach%2520begins%2520with%250Asynthesizing%2520high-quality%2520chain-of-thought%2520%2528CoT%2529%2520data%2520for%2520cold-start%250Ainitialization%252C%2520followed%2520by%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520using%2520low-rank%250Aadaptation%2520%2528LoRA%2529.%2520The%2520cold-start%2520training%2520stage%2520enables%2520the%2520model%2520to%2520identify%250Acorrect%2520solutions.%2520Subsequently%252C%2520we%2520perform%2520rejection%2520sampling%2520using%2520the%2520merged%250ASFT%2520model%2520to%2520curate%2520high-quality%2520RL%2520data%2520and%2520leverage%2520rule-based%2520RL%2520to%2520guide%250Athe%2520model%2520toward%2520optimal%2520reasoning%2520paths.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520yielding%2520improvements%2520of%2520%252B9.04%2525%250Aon%2520MIG-Bench%252C%2520%252B6.37%2525%2520on%2520MC-Bench%252C%2520and%2520%252B4.98%2525%2520on%2520several%2520out-of-domain%2520reasoning%250Agrounding%2520benchmarks%2520compared%2520to%2520the%2520SFT%2520baseline.%2520Furthermore%252C%2520our%2520method%250Aexhibits%2520strong%2520generalization%2520in%2520multi-image%2520perception%252C%2520with%2520gains%2520of%2520%252B3.1%2525%250Aand%2520%252B2.4%2525%2520over%2520the%2520base%2520model%2520on%2520BLINK%2520and%2520MMIU%2520benchmarks%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Reasoning%20of%20Multi-Image%20Grounding%20in%20MLLMs%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Bob%20Zhang%20and%20Haoran%20Li%20and%20Tao%20Zhang%20and%20Cilin%20Yan%20and%20Jiayin%20Cai%20and%20Yanbin%20Hao&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20visual%20grounding%0Ain%20single-image%20scenarios%20with%20textual%20references.%20However%2C%20their%20performance%0Adegrades%20when%20handling%20real-world%20applications%20that%20involve%20complex%20multi-image%0Acompositions%20and%20multi-modal%20instructions%2C%20revealing%20limitations%20in%20cross-image%0Areasoning%20and%20generalization.%20To%20address%20these%20challenges%2C%20we%20adopt%20a%0AReinforcement%20Learning%20%28RL%29%20based%20post-training%20strategy%20to%20improve%20the%0Areasoning%20of%20MLLMs%20in%20multi-image%20grounding%20tasks.%20Our%20approach%20begins%20with%0Asynthesizing%20high-quality%20chain-of-thought%20%28CoT%29%20data%20for%20cold-start%0Ainitialization%2C%20followed%20by%20supervised%20fine-tuning%20%28SFT%29%20using%20low-rank%0Aadaptation%20%28LoRA%29.%20The%20cold-start%20training%20stage%20enables%20the%20model%20to%20identify%0Acorrect%20solutions.%20Subsequently%2C%20we%20perform%20rejection%20sampling%20using%20the%20merged%0ASFT%20model%20to%20curate%20high-quality%20RL%20data%20and%20leverage%20rule-based%20RL%20to%20guide%0Athe%20model%20toward%20optimal%20reasoning%20paths.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%2C%20yielding%20improvements%20of%20%2B9.04%25%0Aon%20MIG-Bench%2C%20%2B6.37%25%20on%20MC-Bench%2C%20and%20%2B4.98%25%20on%20several%20out-of-domain%20reasoning%0Agrounding%20benchmarks%20compared%20to%20the%20SFT%20baseline.%20Furthermore%2C%20our%20method%0Aexhibits%20strong%20generalization%20in%20multi-image%20perception%2C%20with%20gains%20of%20%2B3.1%25%0Aand%20%2B2.4%25%20over%20the%20base%20model%20on%20BLINK%20and%20MMIU%20benchmarks%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00748v2&entry.124074799=Read"},
{"title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration\n  Through Clinical Cognitive Chain Reasoning", "author": "Xinyao Liu and Diping Song", "abstract": "  Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.\n", "link": "http://arxiv.org/abs/2507.17539v1", "date": "2025-07-23", "relevancy": 2.2747, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20Ophthalmic%20MLLM%20for%20Positioning-diagnosis%20Collaboration%0A%20%20Through%20Clinical%20Cognitive%20Chain%20Reasoning&body=Title%3A%20Constructing%20Ophthalmic%20MLLM%20for%20Positioning-diagnosis%20Collaboration%0A%20%20Through%20Clinical%20Cognitive%20Chain%20Reasoning%0AAuthor%3A%20Xinyao%20Liu%20and%20Diping%20Song%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20significant%20potential%20in%0Athe%20field%20of%20medical%20diagnosis.%20However%2C%20they%20face%20critical%20challenges%20in%0Aspecialized%20domains%20such%20as%20ophthalmology%2C%20particularly%20the%20fragmentation%20of%0Aannotation%20granularity%20and%20inconsistencies%20in%20clinical%20reasoning%20logic%2C%20which%0Ahinder%20precise%20cross-modal%20understanding.%20This%20paper%20introduces%20FundusExpert%2C%0Aan%20ophthalmology-specific%20MLLM%20with%20integrated%20positioning-diagnosis%20reasoning%0Acapabilities%2C%20along%20with%20FundusGen%2C%20a%20dataset%20constructed%20through%20the%0Aintelligent%20Fundus-Engine%20system.%20Fundus-Engine%20automates%20localization%20and%0Aleverages%20MLLM-based%20semantic%20expansion%20to%20integrate%20global%20disease%0Aclassification%2C%20local%20object%20detection%2C%20and%20fine-grained%20feature%20analysis%0Awithin%20a%20single%20fundus%20image.%20Additionally%2C%20by%20constructing%20a%20clinically%0Aaligned%20cognitive%20chain%2C%20it%20guides%20the%20model%20to%20generate%20interpretable%0Areasoning%20paths.%20FundusExpert%2C%20fine-tuned%20with%20instruction%20data%20from%20FundusGen%2C%0Aachieves%20the%20best%20performance%20in%20ophthalmic%20question-answering%20tasks%2C%0Asurpassing%20the%20average%20accuracy%20of%20the%2040B%20MedRegA%20by%2026.6%25.%20It%20also%20excels%20in%0Azero-shot%20report%20generation%20tasks%2C%20achieving%20a%20clinical%20consistency%20of%2077.0%25%2C%0Asignificantly%20outperforming%20GPT-4o%27s%2047.6%25.%20Furthermore%2C%20we%20reveal%20a%20scaling%0Alaw%20between%20data%20quality%20and%20model%20capability%20%28%24L%20%5Cpropto%20N%5E%7B0.068%7D%24%29%2C%0Ademonstrating%20that%20the%20cognitive%20alignment%20annotations%20in%20FundusGen%20enhance%0Adata%20utilization%20efficiency.%20By%20integrating%20region-level%20localization%20with%0Adiagnostic%20reasoning%20chains%2C%20our%20work%20develops%20a%20scalable%2C%20clinically-aligned%0AMLLM%20and%20explores%20a%20pathway%20toward%20bridging%20the%20visual-language%20gap%20in%20specific%0AMLLMs.%20Our%20project%20can%20be%20found%20at%20https%3A//github.com/MeteorElf/FundusExpert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520Ophthalmic%2520MLLM%2520for%2520Positioning-diagnosis%2520Collaboration%250A%2520%2520Through%2520Clinical%2520Cognitive%2520Chain%2520Reasoning%26entry.906535625%3DXinyao%2520Liu%2520and%2520Diping%2520Song%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520demonstrate%2520significant%2520potential%2520in%250Athe%2520field%2520of%2520medical%2520diagnosis.%2520However%252C%2520they%2520face%2520critical%2520challenges%2520in%250Aspecialized%2520domains%2520such%2520as%2520ophthalmology%252C%2520particularly%2520the%2520fragmentation%2520of%250Aannotation%2520granularity%2520and%2520inconsistencies%2520in%2520clinical%2520reasoning%2520logic%252C%2520which%250Ahinder%2520precise%2520cross-modal%2520understanding.%2520This%2520paper%2520introduces%2520FundusExpert%252C%250Aan%2520ophthalmology-specific%2520MLLM%2520with%2520integrated%2520positioning-diagnosis%2520reasoning%250Acapabilities%252C%2520along%2520with%2520FundusGen%252C%2520a%2520dataset%2520constructed%2520through%2520the%250Aintelligent%2520Fundus-Engine%2520system.%2520Fundus-Engine%2520automates%2520localization%2520and%250Aleverages%2520MLLM-based%2520semantic%2520expansion%2520to%2520integrate%2520global%2520disease%250Aclassification%252C%2520local%2520object%2520detection%252C%2520and%2520fine-grained%2520feature%2520analysis%250Awithin%2520a%2520single%2520fundus%2520image.%2520Additionally%252C%2520by%2520constructing%2520a%2520clinically%250Aaligned%2520cognitive%2520chain%252C%2520it%2520guides%2520the%2520model%2520to%2520generate%2520interpretable%250Areasoning%2520paths.%2520FundusExpert%252C%2520fine-tuned%2520with%2520instruction%2520data%2520from%2520FundusGen%252C%250Aachieves%2520the%2520best%2520performance%2520in%2520ophthalmic%2520question-answering%2520tasks%252C%250Asurpassing%2520the%2520average%2520accuracy%2520of%2520the%252040B%2520MedRegA%2520by%252026.6%2525.%2520It%2520also%2520excels%2520in%250Azero-shot%2520report%2520generation%2520tasks%252C%2520achieving%2520a%2520clinical%2520consistency%2520of%252077.0%2525%252C%250Asignificantly%2520outperforming%2520GPT-4o%2527s%252047.6%2525.%2520Furthermore%252C%2520we%2520reveal%2520a%2520scaling%250Alaw%2520between%2520data%2520quality%2520and%2520model%2520capability%2520%2528%2524L%2520%255Cpropto%2520N%255E%257B0.068%257D%2524%2529%252C%250Ademonstrating%2520that%2520the%2520cognitive%2520alignment%2520annotations%2520in%2520FundusGen%2520enhance%250Adata%2520utilization%2520efficiency.%2520By%2520integrating%2520region-level%2520localization%2520with%250Adiagnostic%2520reasoning%2520chains%252C%2520our%2520work%2520develops%2520a%2520scalable%252C%2520clinically-aligned%250AMLLM%2520and%2520explores%2520a%2520pathway%2520toward%2520bridging%2520the%2520visual-language%2520gap%2520in%2520specific%250AMLLMs.%2520Our%2520project%2520can%2520be%2520found%2520at%2520https%253A//github.com/MeteorElf/FundusExpert.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20Ophthalmic%20MLLM%20for%20Positioning-diagnosis%20Collaboration%0A%20%20Through%20Clinical%20Cognitive%20Chain%20Reasoning&entry.906535625=Xinyao%20Liu%20and%20Diping%20Song&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20significant%20potential%20in%0Athe%20field%20of%20medical%20diagnosis.%20However%2C%20they%20face%20critical%20challenges%20in%0Aspecialized%20domains%20such%20as%20ophthalmology%2C%20particularly%20the%20fragmentation%20of%0Aannotation%20granularity%20and%20inconsistencies%20in%20clinical%20reasoning%20logic%2C%20which%0Ahinder%20precise%20cross-modal%20understanding.%20This%20paper%20introduces%20FundusExpert%2C%0Aan%20ophthalmology-specific%20MLLM%20with%20integrated%20positioning-diagnosis%20reasoning%0Acapabilities%2C%20along%20with%20FundusGen%2C%20a%20dataset%20constructed%20through%20the%0Aintelligent%20Fundus-Engine%20system.%20Fundus-Engine%20automates%20localization%20and%0Aleverages%20MLLM-based%20semantic%20expansion%20to%20integrate%20global%20disease%0Aclassification%2C%20local%20object%20detection%2C%20and%20fine-grained%20feature%20analysis%0Awithin%20a%20single%20fundus%20image.%20Additionally%2C%20by%20constructing%20a%20clinically%0Aaligned%20cognitive%20chain%2C%20it%20guides%20the%20model%20to%20generate%20interpretable%0Areasoning%20paths.%20FundusExpert%2C%20fine-tuned%20with%20instruction%20data%20from%20FundusGen%2C%0Aachieves%20the%20best%20performance%20in%20ophthalmic%20question-answering%20tasks%2C%0Asurpassing%20the%20average%20accuracy%20of%20the%2040B%20MedRegA%20by%2026.6%25.%20It%20also%20excels%20in%0Azero-shot%20report%20generation%20tasks%2C%20achieving%20a%20clinical%20consistency%20of%2077.0%25%2C%0Asignificantly%20outperforming%20GPT-4o%27s%2047.6%25.%20Furthermore%2C%20we%20reveal%20a%20scaling%0Alaw%20between%20data%20quality%20and%20model%20capability%20%28%24L%20%5Cpropto%20N%5E%7B0.068%7D%24%29%2C%0Ademonstrating%20that%20the%20cognitive%20alignment%20annotations%20in%20FundusGen%20enhance%0Adata%20utilization%20efficiency.%20By%20integrating%20region-level%20localization%20with%0Adiagnostic%20reasoning%20chains%2C%20our%20work%20develops%20a%20scalable%2C%20clinically-aligned%0AMLLM%20and%20explores%20a%20pathway%20toward%20bridging%20the%20visual-language%20gap%20in%20specific%0AMLLMs.%20Our%20project%20can%20be%20found%20at%20https%3A//github.com/MeteorElf/FundusExpert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17539v1&entry.124074799=Read"},
{"title": "Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with\n  Consistency Rewards", "author": "Aakash Garg and Libing Zeng and Andrii Tsarov and Nima Khademi Kalantari", "abstract": "  In this paper, we propose a novel diffusion-based approach to generate stereo\nimages given a text prompt. Since stereo image datasets with large baselines\nare scarce, training a diffusion model from scratch is not feasible. Therefore,\nwe propose leveraging the strong priors learned by Stable Diffusion and\nfine-tuning it on stereo image datasets to adapt it to the task of stereo\ngeneration. To improve stereo consistency and text-to-image alignment, we\nfurther tune the model using prompt alignment and our proposed stereo\nconsistency reward functions. Comprehensive experiments demonstrate the\nsuperiority of our approach in generating high-quality stereo images across\ndiverse scenarios, outperforming existing methods.\n", "link": "http://arxiv.org/abs/2506.05367v2", "date": "2025-07-23", "relevancy": 2.2705, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5787}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5602}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Stereo%3A%20Repurposing%20Stable%20Diffusion%20for%20Stereo%20Generation%20with%0A%20%20Consistency%20Rewards&body=Title%3A%20Text2Stereo%3A%20Repurposing%20Stable%20Diffusion%20for%20Stereo%20Generation%20with%0A%20%20Consistency%20Rewards%0AAuthor%3A%20Aakash%20Garg%20and%20Libing%20Zeng%20and%20Andrii%20Tsarov%20and%20Nima%20Khademi%20Kalantari%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20diffusion-based%20approach%20to%20generate%20stereo%0Aimages%20given%20a%20text%20prompt.%20Since%20stereo%20image%20datasets%20with%20large%20baselines%0Aare%20scarce%2C%20training%20a%20diffusion%20model%20from%20scratch%20is%20not%20feasible.%20Therefore%2C%0Awe%20propose%20leveraging%20the%20strong%20priors%20learned%20by%20Stable%20Diffusion%20and%0Afine-tuning%20it%20on%20stereo%20image%20datasets%20to%20adapt%20it%20to%20the%20task%20of%20stereo%0Ageneration.%20To%20improve%20stereo%20consistency%20and%20text-to-image%20alignment%2C%20we%0Afurther%20tune%20the%20model%20using%20prompt%20alignment%20and%20our%20proposed%20stereo%0Aconsistency%20reward%20functions.%20Comprehensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20in%20generating%20high-quality%20stereo%20images%20across%0Adiverse%20scenarios%2C%20outperforming%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Stereo%253A%2520Repurposing%2520Stable%2520Diffusion%2520for%2520Stereo%2520Generation%2520with%250A%2520%2520Consistency%2520Rewards%26entry.906535625%3DAakash%2520Garg%2520and%2520Libing%2520Zeng%2520and%2520Andrii%2520Tsarov%2520and%2520Nima%2520Khademi%2520Kalantari%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520diffusion-based%2520approach%2520to%2520generate%2520stereo%250Aimages%2520given%2520a%2520text%2520prompt.%2520Since%2520stereo%2520image%2520datasets%2520with%2520large%2520baselines%250Aare%2520scarce%252C%2520training%2520a%2520diffusion%2520model%2520from%2520scratch%2520is%2520not%2520feasible.%2520Therefore%252C%250Awe%2520propose%2520leveraging%2520the%2520strong%2520priors%2520learned%2520by%2520Stable%2520Diffusion%2520and%250Afine-tuning%2520it%2520on%2520stereo%2520image%2520datasets%2520to%2520adapt%2520it%2520to%2520the%2520task%2520of%2520stereo%250Ageneration.%2520To%2520improve%2520stereo%2520consistency%2520and%2520text-to-image%2520alignment%252C%2520we%250Afurther%2520tune%2520the%2520model%2520using%2520prompt%2520alignment%2520and%2520our%2520proposed%2520stereo%250Aconsistency%2520reward%2520functions.%2520Comprehensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach%2520in%2520generating%2520high-quality%2520stereo%2520images%2520across%250Adiverse%2520scenarios%252C%2520outperforming%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Stereo%3A%20Repurposing%20Stable%20Diffusion%20for%20Stereo%20Generation%20with%0A%20%20Consistency%20Rewards&entry.906535625=Aakash%20Garg%20and%20Libing%20Zeng%20and%20Andrii%20Tsarov%20and%20Nima%20Khademi%20Kalantari&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20diffusion-based%20approach%20to%20generate%20stereo%0Aimages%20given%20a%20text%20prompt.%20Since%20stereo%20image%20datasets%20with%20large%20baselines%0Aare%20scarce%2C%20training%20a%20diffusion%20model%20from%20scratch%20is%20not%20feasible.%20Therefore%2C%0Awe%20propose%20leveraging%20the%20strong%20priors%20learned%20by%20Stable%20Diffusion%20and%0Afine-tuning%20it%20on%20stereo%20image%20datasets%20to%20adapt%20it%20to%20the%20task%20of%20stereo%0Ageneration.%20To%20improve%20stereo%20consistency%20and%20text-to-image%20alignment%2C%20we%0Afurther%20tune%20the%20model%20using%20prompt%20alignment%20and%20our%20proposed%20stereo%0Aconsistency%20reward%20functions.%20Comprehensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20in%20generating%20high-quality%20stereo%20images%20across%0Adiverse%20scenarios%2C%20outperforming%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05367v2&entry.124074799=Read"},
{"title": "Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained\n  Models", "author": "Shen Tan and Dong Zhou and Xiangyu Shao and Junqiao Wang and Guanghui Sun", "abstract": "  Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2507.17379v1", "date": "2025-07-23", "relevancy": 2.2668, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Conditioned%20Open-Vocabulary%20Mobile%20Manipulation%20with%20Pretrained%0A%20%20Models&body=Title%3A%20Language-Conditioned%20Open-Vocabulary%20Mobile%20Manipulation%20with%20Pretrained%0A%20%20Models%0AAuthor%3A%20Shen%20Tan%20and%20Dong%20Zhou%20and%20Xiangyu%20Shao%20and%20Junqiao%20Wang%20and%20Guanghui%20Sun%0AAbstract%3A%20%20%20Open-vocabulary%20mobile%20manipulation%20%28OVMM%29%20that%20involves%20the%20handling%20of%0Anovel%20and%20unseen%20objects%20across%20different%20workspaces%20remains%20a%20significant%0Achallenge%20for%20real-world%20robotic%20applications.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Language-conditioned%20Open-Vocabulary%20Mobile%20Manipulation%20framework%2C%20named%0ALOVMM%2C%20incorporating%20the%20large%20language%20model%20%28LLM%29%20and%20vision-language%20model%0A%28VLM%29%20to%20tackle%20various%20mobile%20manipulation%20tasks%20in%20household%20environments.%0AOur%20approach%20is%20capable%20of%20solving%20various%20OVMM%20tasks%20with%20free-form%20natural%0Alanguage%20instructions%20%28e.g.%20%22toss%20the%20food%20boxes%20on%20the%20office%20room%20desk%20to%20the%0Atrash%20bin%20in%20the%20corner%22%2C%20and%20%22pack%20the%20bottles%20from%20the%20bed%20to%20the%20box%20in%20the%0Aguestroom%22%29.%20Extensive%20experiments%20simulated%20in%20complex%20household%20environments%0Ashow%20strong%20zero-shot%20generalization%20and%20multi-task%20learning%20abilities%20of%0ALOVMM.%20Moreover%2C%20our%20approach%20can%20also%20generalize%20to%20multiple%20tabletop%0Amanipulation%20tasks%20and%20achieve%20better%20success%20rates%20compared%20to%20other%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Conditioned%2520Open-Vocabulary%2520Mobile%2520Manipulation%2520with%2520Pretrained%250A%2520%2520Models%26entry.906535625%3DShen%2520Tan%2520and%2520Dong%2520Zhou%2520and%2520Xiangyu%2520Shao%2520and%2520Junqiao%2520Wang%2520and%2520Guanghui%2520Sun%26entry.1292438233%3D%2520%2520Open-vocabulary%2520mobile%2520manipulation%2520%2528OVMM%2529%2520that%2520involves%2520the%2520handling%2520of%250Anovel%2520and%2520unseen%2520objects%2520across%2520different%2520workspaces%2520remains%2520a%2520significant%250Achallenge%2520for%2520real-world%2520robotic%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520Language-conditioned%2520Open-Vocabulary%2520Mobile%2520Manipulation%2520framework%252C%2520named%250ALOVMM%252C%2520incorporating%2520the%2520large%2520language%2520model%2520%2528LLM%2529%2520and%2520vision-language%2520model%250A%2528VLM%2529%2520to%2520tackle%2520various%2520mobile%2520manipulation%2520tasks%2520in%2520household%2520environments.%250AOur%2520approach%2520is%2520capable%2520of%2520solving%2520various%2520OVMM%2520tasks%2520with%2520free-form%2520natural%250Alanguage%2520instructions%2520%2528e.g.%2520%2522toss%2520the%2520food%2520boxes%2520on%2520the%2520office%2520room%2520desk%2520to%2520the%250Atrash%2520bin%2520in%2520the%2520corner%2522%252C%2520and%2520%2522pack%2520the%2520bottles%2520from%2520the%2520bed%2520to%2520the%2520box%2520in%2520the%250Aguestroom%2522%2529.%2520Extensive%2520experiments%2520simulated%2520in%2520complex%2520household%2520environments%250Ashow%2520strong%2520zero-shot%2520generalization%2520and%2520multi-task%2520learning%2520abilities%2520of%250ALOVMM.%2520Moreover%252C%2520our%2520approach%2520can%2520also%2520generalize%2520to%2520multiple%2520tabletop%250Amanipulation%2520tasks%2520and%2520achieve%2520better%2520success%2520rates%2520compared%2520to%2520other%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Conditioned%20Open-Vocabulary%20Mobile%20Manipulation%20with%20Pretrained%0A%20%20Models&entry.906535625=Shen%20Tan%20and%20Dong%20Zhou%20and%20Xiangyu%20Shao%20and%20Junqiao%20Wang%20and%20Guanghui%20Sun&entry.1292438233=%20%20Open-vocabulary%20mobile%20manipulation%20%28OVMM%29%20that%20involves%20the%20handling%20of%0Anovel%20and%20unseen%20objects%20across%20different%20workspaces%20remains%20a%20significant%0Achallenge%20for%20real-world%20robotic%20applications.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Language-conditioned%20Open-Vocabulary%20Mobile%20Manipulation%20framework%2C%20named%0ALOVMM%2C%20incorporating%20the%20large%20language%20model%20%28LLM%29%20and%20vision-language%20model%0A%28VLM%29%20to%20tackle%20various%20mobile%20manipulation%20tasks%20in%20household%20environments.%0AOur%20approach%20is%20capable%20of%20solving%20various%20OVMM%20tasks%20with%20free-form%20natural%0Alanguage%20instructions%20%28e.g.%20%22toss%20the%20food%20boxes%20on%20the%20office%20room%20desk%20to%20the%0Atrash%20bin%20in%20the%20corner%22%2C%20and%20%22pack%20the%20bottles%20from%20the%20bed%20to%20the%20box%20in%20the%0Aguestroom%22%29.%20Extensive%20experiments%20simulated%20in%20complex%20household%20environments%0Ashow%20strong%20zero-shot%20generalization%20and%20multi-task%20learning%20abilities%20of%0ALOVMM.%20Moreover%2C%20our%20approach%20can%20also%20generalize%20to%20multiple%20tabletop%0Amanipulation%20tasks%20and%20achieve%20better%20success%20rates%20compared%20to%20other%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17379v1&entry.124074799=Read"},
{"title": "Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using\n  Generative Adversarial Imitation Learning", "author": "Joobin Jin and Seokjun Hong and Gyeongseon Baek and Yeeun Kim and Byeongjoon Noh", "abstract": "  Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.\n", "link": "http://arxiv.org/abs/2507.17418v1", "date": "2025-07-23", "relevancy": 2.2567, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5794}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5546}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ctx2TrajGen%3A%20Traffic%20Context-Aware%20Microscale%20Vehicle%20Trajectories%20using%0A%20%20Generative%20Adversarial%20Imitation%20Learning&body=Title%3A%20Ctx2TrajGen%3A%20Traffic%20Context-Aware%20Microscale%20Vehicle%20Trajectories%20using%0A%20%20Generative%20Adversarial%20Imitation%20Learning%0AAuthor%3A%20Joobin%20Jin%20and%20Seokjun%20Hong%20and%20Gyeongseon%20Baek%20and%20Yeeun%20Kim%20and%20Byeongjoon%20Noh%0AAbstract%3A%20%20%20Precise%20modeling%20of%20microscopic%20vehicle%20trajectories%20is%20critical%20for%20traffic%0Abehavior%20analysis%20and%20autonomous%20driving%20systems.%20We%20propose%20Ctx2TrajGen%2C%20a%0Acontext-aware%20trajectory%20generation%20framework%20that%20synthesizes%20realistic%20urban%0Adriving%20behaviors%20using%20GAIL.%20Leveraging%20PPO%20and%20WGAN-GP%2C%20our%20model%20addresses%0Anonlinear%20interdependencies%20and%20training%20instability%20inherent%20in%20microscopic%0Asettings.%20By%20explicitly%20conditioning%20on%20surrounding%20vehicles%20and%20road%20geometry%2C%0ACtx2TrajGen%20generates%20interaction-aware%20trajectories%20aligned%20with%20real-world%0Acontext.%20Experiments%20on%20the%20drone-captured%20DRIFT%20dataset%20demonstrate%20superior%0Aperformance%20over%20existing%20methods%20in%20terms%20of%20realism%2C%20behavioral%20diversity%2C%0Aand%20contextual%20fidelity%2C%20offering%20a%20robust%20solution%20to%20data%20scarcity%20and%20domain%0Ashift%20without%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCtx2TrajGen%253A%2520Traffic%2520Context-Aware%2520Microscale%2520Vehicle%2520Trajectories%2520using%250A%2520%2520Generative%2520Adversarial%2520Imitation%2520Learning%26entry.906535625%3DJoobin%2520Jin%2520and%2520Seokjun%2520Hong%2520and%2520Gyeongseon%2520Baek%2520and%2520Yeeun%2520Kim%2520and%2520Byeongjoon%2520Noh%26entry.1292438233%3D%2520%2520Precise%2520modeling%2520of%2520microscopic%2520vehicle%2520trajectories%2520is%2520critical%2520for%2520traffic%250Abehavior%2520analysis%2520and%2520autonomous%2520driving%2520systems.%2520We%2520propose%2520Ctx2TrajGen%252C%2520a%250Acontext-aware%2520trajectory%2520generation%2520framework%2520that%2520synthesizes%2520realistic%2520urban%250Adriving%2520behaviors%2520using%2520GAIL.%2520Leveraging%2520PPO%2520and%2520WGAN-GP%252C%2520our%2520model%2520addresses%250Anonlinear%2520interdependencies%2520and%2520training%2520instability%2520inherent%2520in%2520microscopic%250Asettings.%2520By%2520explicitly%2520conditioning%2520on%2520surrounding%2520vehicles%2520and%2520road%2520geometry%252C%250ACtx2TrajGen%2520generates%2520interaction-aware%2520trajectories%2520aligned%2520with%2520real-world%250Acontext.%2520Experiments%2520on%2520the%2520drone-captured%2520DRIFT%2520dataset%2520demonstrate%2520superior%250Aperformance%2520over%2520existing%2520methods%2520in%2520terms%2520of%2520realism%252C%2520behavioral%2520diversity%252C%250Aand%2520contextual%2520fidelity%252C%2520offering%2520a%2520robust%2520solution%2520to%2520data%2520scarcity%2520and%2520domain%250Ashift%2520without%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ctx2TrajGen%3A%20Traffic%20Context-Aware%20Microscale%20Vehicle%20Trajectories%20using%0A%20%20Generative%20Adversarial%20Imitation%20Learning&entry.906535625=Joobin%20Jin%20and%20Seokjun%20Hong%20and%20Gyeongseon%20Baek%20and%20Yeeun%20Kim%20and%20Byeongjoon%20Noh&entry.1292438233=%20%20Precise%20modeling%20of%20microscopic%20vehicle%20trajectories%20is%20critical%20for%20traffic%0Abehavior%20analysis%20and%20autonomous%20driving%20systems.%20We%20propose%20Ctx2TrajGen%2C%20a%0Acontext-aware%20trajectory%20generation%20framework%20that%20synthesizes%20realistic%20urban%0Adriving%20behaviors%20using%20GAIL.%20Leveraging%20PPO%20and%20WGAN-GP%2C%20our%20model%20addresses%0Anonlinear%20interdependencies%20and%20training%20instability%20inherent%20in%20microscopic%0Asettings.%20By%20explicitly%20conditioning%20on%20surrounding%20vehicles%20and%20road%20geometry%2C%0ACtx2TrajGen%20generates%20interaction-aware%20trajectories%20aligned%20with%20real-world%0Acontext.%20Experiments%20on%20the%20drone-captured%20DRIFT%20dataset%20demonstrate%20superior%0Aperformance%20over%20existing%20methods%20in%20terms%20of%20realism%2C%20behavioral%20diversity%2C%0Aand%20contextual%20fidelity%2C%20offering%20a%20robust%20solution%20to%20data%20scarcity%20and%20domain%0Ashift%20without%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17418v1&entry.124074799=Read"},
{"title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference", "author": "Kai Huang and Hao Zou and Bochen Wang and Ye Xi and Zhen Xie and Hao Wang", "abstract": "  Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.\n", "link": "http://arxiv.org/abs/2503.23956v3", "date": "2025-07-23", "relevancy": 2.241, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirCache%3A%20Activating%20Inter-modal%20Relevancy%20KV%20Cache%20Compression%20for%0A%20%20Efficient%20Large%20Vision-Language%20Model%20Inference&body=Title%3A%20AirCache%3A%20Activating%20Inter-modal%20Relevancy%20KV%20Cache%20Compression%20for%0A%20%20Efficient%20Large%20Vision-Language%20Model%20Inference%0AAuthor%3A%20Kai%20Huang%20and%20Hao%20Zou%20and%20Bochen%20Wang%20and%20Ye%20Xi%20and%20Zhen%20Xie%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20gained%0Asignificant%20attention%20due%20to%20their%20remarkable%20reasoning%20capabilities%20and%0Aproficiency%20in%20generalization.%20However%2C%20processing%20a%20large%20number%20of%20visual%0Atokens%20and%20generating%20long-context%20outputs%20impose%20substantial%20computational%0Aoverhead%2C%20leading%20to%20excessive%20demands%20for%20key-value%20%28KV%29%20cache.%20To%20address%0Athis%20critical%20bottleneck%2C%20we%20propose%20AirCache%2C%20a%20novel%20KV%20cache%20compression%0Amethod%20aimed%20at%20accelerating%20LVLMs%20inference.%20This%20work%20systematically%0Ainvestigates%20the%20correlations%20between%20visual%20and%20textual%20tokens%20within%20the%0Aattention%20mechanisms%20of%20LVLMs.%20Our%20empirical%20analysis%20reveals%20considerable%0Aredundancy%20in%20cached%20visual%20tokens%2C%20wherein%20strategically%20eliminating%20these%0Atokens%20preserves%20model%20performance%20while%20significantly%20accelerating%20context%0Ageneration.%20Inspired%20by%20these%20findings%2C%20we%20introduce%20an%20elite%20observation%0Awindow%20for%20assessing%20the%20importance%20of%20visual%20components%20in%20the%20KV%20cache%2C%0Afocusing%20on%20stable%20inter-modal%20relevancy%20modeling%20with%20enhanced%0Amulti-perspective%20consistency.%20Additionally%2C%20we%20develop%20an%20adaptive%20layer-wise%0Abudget%20allocation%20strategy%20that%20capitalizes%20on%20the%20strength%20and%20skewness%20of%0Atoken%20importance%20distribution%2C%20showcasing%20superior%20efficiency%20compared%20to%0Auniform%20allocation.%20Comprehensive%20evaluations%20across%20multiple%20LVLMs%20and%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20comparable%20performance%20to%20the%0Afull%20cache%20while%20retaining%20only%2010%25%20of%20visual%20KV%20cache%2C%20thereby%20reducing%0Adecoding%20latency%20by%2029%25%20to%2066%25%20across%20various%20batch%20size%20and%20prompt%20length%20of%0Ainputs.%20Notably%2C%20as%20cache%20retention%20rates%20decrease%2C%20our%20method%20exhibits%0Aincreasing%20performance%20advantages%20over%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23956v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirCache%253A%2520Activating%2520Inter-modal%2520Relevancy%2520KV%2520Cache%2520Compression%2520for%250A%2520%2520Efficient%2520Large%2520Vision-Language%2520Model%2520Inference%26entry.906535625%3DKai%2520Huang%2520and%2520Hao%2520Zou%2520and%2520Bochen%2520Wang%2520and%2520Ye%2520Xi%2520and%2520Zhen%2520Xie%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520gained%250Asignificant%2520attention%2520due%2520to%2520their%2520remarkable%2520reasoning%2520capabilities%2520and%250Aproficiency%2520in%2520generalization.%2520However%252C%2520processing%2520a%2520large%2520number%2520of%2520visual%250Atokens%2520and%2520generating%2520long-context%2520outputs%2520impose%2520substantial%2520computational%250Aoverhead%252C%2520leading%2520to%2520excessive%2520demands%2520for%2520key-value%2520%2528KV%2529%2520cache.%2520To%2520address%250Athis%2520critical%2520bottleneck%252C%2520we%2520propose%2520AirCache%252C%2520a%2520novel%2520KV%2520cache%2520compression%250Amethod%2520aimed%2520at%2520accelerating%2520LVLMs%2520inference.%2520This%2520work%2520systematically%250Ainvestigates%2520the%2520correlations%2520between%2520visual%2520and%2520textual%2520tokens%2520within%2520the%250Aattention%2520mechanisms%2520of%2520LVLMs.%2520Our%2520empirical%2520analysis%2520reveals%2520considerable%250Aredundancy%2520in%2520cached%2520visual%2520tokens%252C%2520wherein%2520strategically%2520eliminating%2520these%250Atokens%2520preserves%2520model%2520performance%2520while%2520significantly%2520accelerating%2520context%250Ageneration.%2520Inspired%2520by%2520these%2520findings%252C%2520we%2520introduce%2520an%2520elite%2520observation%250Awindow%2520for%2520assessing%2520the%2520importance%2520of%2520visual%2520components%2520in%2520the%2520KV%2520cache%252C%250Afocusing%2520on%2520stable%2520inter-modal%2520relevancy%2520modeling%2520with%2520enhanced%250Amulti-perspective%2520consistency.%2520Additionally%252C%2520we%2520develop%2520an%2520adaptive%2520layer-wise%250Abudget%2520allocation%2520strategy%2520that%2520capitalizes%2520on%2520the%2520strength%2520and%2520skewness%2520of%250Atoken%2520importance%2520distribution%252C%2520showcasing%2520superior%2520efficiency%2520compared%2520to%250Auniform%2520allocation.%2520Comprehensive%2520evaluations%2520across%2520multiple%2520LVLMs%2520and%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520comparable%2520performance%2520to%2520the%250Afull%2520cache%2520while%2520retaining%2520only%252010%2525%2520of%2520visual%2520KV%2520cache%252C%2520thereby%2520reducing%250Adecoding%2520latency%2520by%252029%2525%2520to%252066%2525%2520across%2520various%2520batch%2520size%2520and%2520prompt%2520length%2520of%250Ainputs.%2520Notably%252C%2520as%2520cache%2520retention%2520rates%2520decrease%252C%2520our%2520method%2520exhibits%250Aincreasing%2520performance%2520advantages%2520over%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23956v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirCache%3A%20Activating%20Inter-modal%20Relevancy%20KV%20Cache%20Compression%20for%0A%20%20Efficient%20Large%20Vision-Language%20Model%20Inference&entry.906535625=Kai%20Huang%20and%20Hao%20Zou%20and%20Bochen%20Wang%20and%20Ye%20Xi%20and%20Zhen%20Xie%20and%20Hao%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Visual%20Language%20Models%20%28LVLMs%29%20have%20gained%0Asignificant%20attention%20due%20to%20their%20remarkable%20reasoning%20capabilities%20and%0Aproficiency%20in%20generalization.%20However%2C%20processing%20a%20large%20number%20of%20visual%0Atokens%20and%20generating%20long-context%20outputs%20impose%20substantial%20computational%0Aoverhead%2C%20leading%20to%20excessive%20demands%20for%20key-value%20%28KV%29%20cache.%20To%20address%0Athis%20critical%20bottleneck%2C%20we%20propose%20AirCache%2C%20a%20novel%20KV%20cache%20compression%0Amethod%20aimed%20at%20accelerating%20LVLMs%20inference.%20This%20work%20systematically%0Ainvestigates%20the%20correlations%20between%20visual%20and%20textual%20tokens%20within%20the%0Aattention%20mechanisms%20of%20LVLMs.%20Our%20empirical%20analysis%20reveals%20considerable%0Aredundancy%20in%20cached%20visual%20tokens%2C%20wherein%20strategically%20eliminating%20these%0Atokens%20preserves%20model%20performance%20while%20significantly%20accelerating%20context%0Ageneration.%20Inspired%20by%20these%20findings%2C%20we%20introduce%20an%20elite%20observation%0Awindow%20for%20assessing%20the%20importance%20of%20visual%20components%20in%20the%20KV%20cache%2C%0Afocusing%20on%20stable%20inter-modal%20relevancy%20modeling%20with%20enhanced%0Amulti-perspective%20consistency.%20Additionally%2C%20we%20develop%20an%20adaptive%20layer-wise%0Abudget%20allocation%20strategy%20that%20capitalizes%20on%20the%20strength%20and%20skewness%20of%0Atoken%20importance%20distribution%2C%20showcasing%20superior%20efficiency%20compared%20to%0Auniform%20allocation.%20Comprehensive%20evaluations%20across%20multiple%20LVLMs%20and%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20comparable%20performance%20to%20the%0Afull%20cache%20while%20retaining%20only%2010%25%20of%20visual%20KV%20cache%2C%20thereby%20reducing%0Adecoding%20latency%20by%2029%25%20to%2066%25%20across%20various%20batch%20size%20and%20prompt%20length%20of%0Ainputs.%20Notably%2C%20as%20cache%20retention%20rates%20decrease%2C%20our%20method%20exhibits%0Aincreasing%20performance%20advantages%20over%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23956v3&entry.124074799=Read"},
{"title": "Clustering-based hard negative sampling for supervised contrastive\n  speaker verification", "author": "Piotr Masztalski and Micha\u0142 Romaniuk and Jakub \u017bak and Mateusz Matuszewski and Konrad Kowalczyk", "abstract": "  In speaker verification, contrastive learning is gaining popularity as an\nalternative to the traditionally used classification-based approaches.\nContrastive methods can benefit from an effective use of hard negative pairs,\nwhich are different-class samples particularly challenging for a verification\nmodel due to their similarity. In this paper, we propose CHNS - a\nclustering-based hard negative sampling method, dedicated for supervised\ncontrastive speaker representation learning. Our approach clusters embeddings\nof similar speakers, and adjusts batch composition to obtain an optimal ratio\nof hard and easy negatives during contrastive loss calculation. Experimental\nevaluation shows that CHNS outperforms a baseline supervised contrastive\napproach with and without loss-based hard negative sampling, as well as a\nstate-of-the-art classification-based approach to speaker verification by as\nmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using two\nlightweight model architectures.\n", "link": "http://arxiv.org/abs/2507.17540v1", "date": "2025-07-23", "relevancy": 2.2403, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4789}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4411}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering-based%20hard%20negative%20sampling%20for%20supervised%20contrastive%0A%20%20speaker%20verification&body=Title%3A%20Clustering-based%20hard%20negative%20sampling%20for%20supervised%20contrastive%0A%20%20speaker%20verification%0AAuthor%3A%20Piotr%20Masztalski%20and%20Micha%C5%82%20Romaniuk%20and%20Jakub%20%C5%BBak%20and%20Mateusz%20Matuszewski%20and%20Konrad%20Kowalczyk%0AAbstract%3A%20%20%20In%20speaker%20verification%2C%20contrastive%20learning%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20the%20traditionally%20used%20classification-based%20approaches.%0AContrastive%20methods%20can%20benefit%20from%20an%20effective%20use%20of%20hard%20negative%20pairs%2C%0Awhich%20are%20different-class%20samples%20particularly%20challenging%20for%20a%20verification%0Amodel%20due%20to%20their%20similarity.%20In%20this%20paper%2C%20we%20propose%20CHNS%20-%20a%0Aclustering-based%20hard%20negative%20sampling%20method%2C%20dedicated%20for%20supervised%0Acontrastive%20speaker%20representation%20learning.%20Our%20approach%20clusters%20embeddings%0Aof%20similar%20speakers%2C%20and%20adjusts%20batch%20composition%20to%20obtain%20an%20optimal%20ratio%0Aof%20hard%20and%20easy%20negatives%20during%20contrastive%20loss%20calculation.%20Experimental%0Aevaluation%20shows%20that%20CHNS%20outperforms%20a%20baseline%20supervised%20contrastive%0Aapproach%20with%20and%20without%20loss-based%20hard%20negative%20sampling%2C%20as%20well%20as%20a%0Astate-of-the-art%20classification-based%20approach%20to%20speaker%20verification%20by%20as%0Amuch%20as%2018%20%25%20relative%20EER%20and%20minDCF%20on%20the%20VoxCeleb%20dataset%20using%20two%0Alightweight%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering-based%2520hard%2520negative%2520sampling%2520for%2520supervised%2520contrastive%250A%2520%2520speaker%2520verification%26entry.906535625%3DPiotr%2520Masztalski%2520and%2520Micha%25C5%2582%2520Romaniuk%2520and%2520Jakub%2520%25C5%25BBak%2520and%2520Mateusz%2520Matuszewski%2520and%2520Konrad%2520Kowalczyk%26entry.1292438233%3D%2520%2520In%2520speaker%2520verification%252C%2520contrastive%2520learning%2520is%2520gaining%2520popularity%2520as%2520an%250Aalternative%2520to%2520the%2520traditionally%2520used%2520classification-based%2520approaches.%250AContrastive%2520methods%2520can%2520benefit%2520from%2520an%2520effective%2520use%2520of%2520hard%2520negative%2520pairs%252C%250Awhich%2520are%2520different-class%2520samples%2520particularly%2520challenging%2520for%2520a%2520verification%250Amodel%2520due%2520to%2520their%2520similarity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CHNS%2520-%2520a%250Aclustering-based%2520hard%2520negative%2520sampling%2520method%252C%2520dedicated%2520for%2520supervised%250Acontrastive%2520speaker%2520representation%2520learning.%2520Our%2520approach%2520clusters%2520embeddings%250Aof%2520similar%2520speakers%252C%2520and%2520adjusts%2520batch%2520composition%2520to%2520obtain%2520an%2520optimal%2520ratio%250Aof%2520hard%2520and%2520easy%2520negatives%2520during%2520contrastive%2520loss%2520calculation.%2520Experimental%250Aevaluation%2520shows%2520that%2520CHNS%2520outperforms%2520a%2520baseline%2520supervised%2520contrastive%250Aapproach%2520with%2520and%2520without%2520loss-based%2520hard%2520negative%2520sampling%252C%2520as%2520well%2520as%2520a%250Astate-of-the-art%2520classification-based%2520approach%2520to%2520speaker%2520verification%2520by%2520as%250Amuch%2520as%252018%2520%2525%2520relative%2520EER%2520and%2520minDCF%2520on%2520the%2520VoxCeleb%2520dataset%2520using%2520two%250Alightweight%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering-based%20hard%20negative%20sampling%20for%20supervised%20contrastive%0A%20%20speaker%20verification&entry.906535625=Piotr%20Masztalski%20and%20Micha%C5%82%20Romaniuk%20and%20Jakub%20%C5%BBak%20and%20Mateusz%20Matuszewski%20and%20Konrad%20Kowalczyk&entry.1292438233=%20%20In%20speaker%20verification%2C%20contrastive%20learning%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20the%20traditionally%20used%20classification-based%20approaches.%0AContrastive%20methods%20can%20benefit%20from%20an%20effective%20use%20of%20hard%20negative%20pairs%2C%0Awhich%20are%20different-class%20samples%20particularly%20challenging%20for%20a%20verification%0Amodel%20due%20to%20their%20similarity.%20In%20this%20paper%2C%20we%20propose%20CHNS%20-%20a%0Aclustering-based%20hard%20negative%20sampling%20method%2C%20dedicated%20for%20supervised%0Acontrastive%20speaker%20representation%20learning.%20Our%20approach%20clusters%20embeddings%0Aof%20similar%20speakers%2C%20and%20adjusts%20batch%20composition%20to%20obtain%20an%20optimal%20ratio%0Aof%20hard%20and%20easy%20negatives%20during%20contrastive%20loss%20calculation.%20Experimental%0Aevaluation%20shows%20that%20CHNS%20outperforms%20a%20baseline%20supervised%20contrastive%0Aapproach%20with%20and%20without%20loss-based%20hard%20negative%20sampling%2C%20as%20well%20as%20a%0Astate-of-the-art%20classification-based%20approach%20to%20speaker%20verification%20by%20as%0Amuch%20as%2018%20%25%20relative%20EER%20and%20minDCF%20on%20the%20VoxCeleb%20dataset%20using%20two%0Alightweight%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17540v1&entry.124074799=Read"},
{"title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with\n  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "author": "An Wang and Rulin Zhou and Mengya Xu and Yiru Ye and Longfei Gou and Yiting Chang and Hao Chen and Chwee Ming Lim and Jiankun Wang and Hongliang Ren", "abstract": "  Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.\n", "link": "http://arxiv.org/abs/2507.15292v3", "date": "2025-07-23", "relevancy": 2.2261, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Contro&body=Title%3A%20EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Contro%0AAuthor%3A%20An%20Wang%20and%20Rulin%20Zhou%20and%20Mengya%20Xu%20and%20Yiru%20Ye%20and%20Longfei%20Gou%20and%20Yiting%20Chang%20and%20Hao%20Chen%20and%20Chwee%20Ming%20Lim%20and%20Jiankun%20Wang%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20Visualizing%20subtle%20vascular%20motions%20in%20endoscopic%20surgery%20is%20crucial%20for%0Asurgical%20precision%20and%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20dynamic%20nature%20of%20surgical%20scenes.%20To%20address%20this%2C%20we%20introduce%0AEndoControlMag%2C%20a%20training-free%2C%20Lagrangian-based%20framework%20with%0Amask-conditioned%20vascular%20motion%20magnification%20tailored%20to%20endoscopic%0Aenvironments.%20Our%20approach%20features%20two%20key%20modules%3A%20a%20Periodic%20Reference%0AResetting%20%28PRR%29%20scheme%20that%20divides%20videos%20into%20short%20overlapping%20clips%20with%0Adynamically%20updated%20reference%20frames%20to%20prevent%20error%20accumulation%20while%0Amaintaining%20temporal%20coherence%2C%20and%20a%20Hierarchical%20Tissue-aware%20Magnification%0A%28HTM%29%20framework%20with%20dual-mode%20mask%20dilation.%20HTM%20first%20tracks%20vessel%20cores%0Ausing%20a%20pretrained%20visual%20tracking%20model%20to%20maintain%20accurate%20localization%0Adespite%20occlusions%20and%20view%20changes.%20It%20then%20applies%20one%20of%20two%20adaptive%0Asoftening%20strategies%20to%20surrounding%20tissues%3A%20motion-based%20softening%20that%0Amodulates%20magnification%20strength%20proportional%20to%20observed%20tissue%20displacement%2C%0Aor%20distance-based%20exponential%20decay%20that%20simulates%20biomechanical%20force%0Aattenuation.%20This%20dual-mode%20approach%20accommodates%20diverse%20surgical%0Ascenarios-motion-based%20softening%20excels%20with%20complex%20tissue%20deformations%20while%0Adistance-based%20softening%20provides%20stability%20during%20unreliable%20optical%20flow%0Aconditions.%20We%20evaluate%20EndoControlMag%20on%20our%20EndoVMM24%20dataset%20spanning%20four%0Adifferent%20surgery%20types%20and%20various%20challenging%20scenarios%2C%20including%0Aocclusions%2C%20instrument%20disturbance%2C%20view%20changes%2C%20and%20vessel%20deformations.%0AQuantitative%20metrics%2C%20visual%20assessments%2C%20and%20expert%20surgeon%20evaluations%0Ademonstrate%20that%20EndoControlMag%20significantly%20outperforms%20existing%20methods%20in%0Aboth%20magnification%20accuracy%20and%20visual%20quality%20while%20maintaining%20robustness%0Aacross%20challenging%20surgical%20conditions.%20The%20code%2C%20dataset%2C%20and%20video%20results%0Aare%20available%20at%20https%3A//szupc.github.io/EndoControlMag/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15292v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoControlMag%253A%2520Robust%2520Endoscopic%2520Vascular%2520Motion%2520Magnification%2520with%250A%2520%2520Periodic%2520Reference%2520Resetting%2520and%2520Hierarchical%2520Tissue-aware%2520Dual-Mask%2520Contro%26entry.906535625%3DAn%2520Wang%2520and%2520Rulin%2520Zhou%2520and%2520Mengya%2520Xu%2520and%2520Yiru%2520Ye%2520and%2520Longfei%2520Gou%2520and%2520Yiting%2520Chang%2520and%2520Hao%2520Chen%2520and%2520Chwee%2520Ming%2520Lim%2520and%2520Jiankun%2520Wang%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520Visualizing%2520subtle%2520vascular%2520motions%2520in%2520endoscopic%2520surgery%2520is%2520crucial%2520for%250Asurgical%2520precision%2520and%2520decision-making%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520the%250Acomplex%2520and%2520dynamic%2520nature%2520of%2520surgical%2520scenes.%2520To%2520address%2520this%252C%2520we%2520introduce%250AEndoControlMag%252C%2520a%2520training-free%252C%2520Lagrangian-based%2520framework%2520with%250Amask-conditioned%2520vascular%2520motion%2520magnification%2520tailored%2520to%2520endoscopic%250Aenvironments.%2520Our%2520approach%2520features%2520two%2520key%2520modules%253A%2520a%2520Periodic%2520Reference%250AResetting%2520%2528PRR%2529%2520scheme%2520that%2520divides%2520videos%2520into%2520short%2520overlapping%2520clips%2520with%250Adynamically%2520updated%2520reference%2520frames%2520to%2520prevent%2520error%2520accumulation%2520while%250Amaintaining%2520temporal%2520coherence%252C%2520and%2520a%2520Hierarchical%2520Tissue-aware%2520Magnification%250A%2528HTM%2529%2520framework%2520with%2520dual-mode%2520mask%2520dilation.%2520HTM%2520first%2520tracks%2520vessel%2520cores%250Ausing%2520a%2520pretrained%2520visual%2520tracking%2520model%2520to%2520maintain%2520accurate%2520localization%250Adespite%2520occlusions%2520and%2520view%2520changes.%2520It%2520then%2520applies%2520one%2520of%2520two%2520adaptive%250Asoftening%2520strategies%2520to%2520surrounding%2520tissues%253A%2520motion-based%2520softening%2520that%250Amodulates%2520magnification%2520strength%2520proportional%2520to%2520observed%2520tissue%2520displacement%252C%250Aor%2520distance-based%2520exponential%2520decay%2520that%2520simulates%2520biomechanical%2520force%250Aattenuation.%2520This%2520dual-mode%2520approach%2520accommodates%2520diverse%2520surgical%250Ascenarios-motion-based%2520softening%2520excels%2520with%2520complex%2520tissue%2520deformations%2520while%250Adistance-based%2520softening%2520provides%2520stability%2520during%2520unreliable%2520optical%2520flow%250Aconditions.%2520We%2520evaluate%2520EndoControlMag%2520on%2520our%2520EndoVMM24%2520dataset%2520spanning%2520four%250Adifferent%2520surgery%2520types%2520and%2520various%2520challenging%2520scenarios%252C%2520including%250Aocclusions%252C%2520instrument%2520disturbance%252C%2520view%2520changes%252C%2520and%2520vessel%2520deformations.%250AQuantitative%2520metrics%252C%2520visual%2520assessments%252C%2520and%2520expert%2520surgeon%2520evaluations%250Ademonstrate%2520that%2520EndoControlMag%2520significantly%2520outperforms%2520existing%2520methods%2520in%250Aboth%2520magnification%2520accuracy%2520and%2520visual%2520quality%2520while%2520maintaining%2520robustness%250Aacross%2520challenging%2520surgical%2520conditions.%2520The%2520code%252C%2520dataset%252C%2520and%2520video%2520results%250Aare%2520available%2520at%2520https%253A//szupc.github.io/EndoControlMag/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15292v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoControlMag%3A%20Robust%20Endoscopic%20Vascular%20Motion%20Magnification%20with%0A%20%20Periodic%20Reference%20Resetting%20and%20Hierarchical%20Tissue-aware%20Dual-Mask%20Contro&entry.906535625=An%20Wang%20and%20Rulin%20Zhou%20and%20Mengya%20Xu%20and%20Yiru%20Ye%20and%20Longfei%20Gou%20and%20Yiting%20Chang%20and%20Hao%20Chen%20and%20Chwee%20Ming%20Lim%20and%20Jiankun%20Wang%20and%20Hongliang%20Ren&entry.1292438233=%20%20Visualizing%20subtle%20vascular%20motions%20in%20endoscopic%20surgery%20is%20crucial%20for%0Asurgical%20precision%20and%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20the%0Acomplex%20and%20dynamic%20nature%20of%20surgical%20scenes.%20To%20address%20this%2C%20we%20introduce%0AEndoControlMag%2C%20a%20training-free%2C%20Lagrangian-based%20framework%20with%0Amask-conditioned%20vascular%20motion%20magnification%20tailored%20to%20endoscopic%0Aenvironments.%20Our%20approach%20features%20two%20key%20modules%3A%20a%20Periodic%20Reference%0AResetting%20%28PRR%29%20scheme%20that%20divides%20videos%20into%20short%20overlapping%20clips%20with%0Adynamically%20updated%20reference%20frames%20to%20prevent%20error%20accumulation%20while%0Amaintaining%20temporal%20coherence%2C%20and%20a%20Hierarchical%20Tissue-aware%20Magnification%0A%28HTM%29%20framework%20with%20dual-mode%20mask%20dilation.%20HTM%20first%20tracks%20vessel%20cores%0Ausing%20a%20pretrained%20visual%20tracking%20model%20to%20maintain%20accurate%20localization%0Adespite%20occlusions%20and%20view%20changes.%20It%20then%20applies%20one%20of%20two%20adaptive%0Asoftening%20strategies%20to%20surrounding%20tissues%3A%20motion-based%20softening%20that%0Amodulates%20magnification%20strength%20proportional%20to%20observed%20tissue%20displacement%2C%0Aor%20distance-based%20exponential%20decay%20that%20simulates%20biomechanical%20force%0Aattenuation.%20This%20dual-mode%20approach%20accommodates%20diverse%20surgical%0Ascenarios-motion-based%20softening%20excels%20with%20complex%20tissue%20deformations%20while%0Adistance-based%20softening%20provides%20stability%20during%20unreliable%20optical%20flow%0Aconditions.%20We%20evaluate%20EndoControlMag%20on%20our%20EndoVMM24%20dataset%20spanning%20four%0Adifferent%20surgery%20types%20and%20various%20challenging%20scenarios%2C%20including%0Aocclusions%2C%20instrument%20disturbance%2C%20view%20changes%2C%20and%20vessel%20deformations.%0AQuantitative%20metrics%2C%20visual%20assessments%2C%20and%20expert%20surgeon%20evaluations%0Ademonstrate%20that%20EndoControlMag%20significantly%20outperforms%20existing%20methods%20in%0Aboth%20magnification%20accuracy%20and%20visual%20quality%20while%20maintaining%20robustness%0Aacross%20challenging%20surgical%20conditions.%20The%20code%2C%20dataset%2C%20and%20video%20results%0Aare%20available%20at%20https%3A//szupc.github.io/EndoControlMag/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15292v3&entry.124074799=Read"},
{"title": "MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI", "author": "Jiahui Yin and Xinxing Cheng and Jinming Duan and Yan Pang and Declan O'Regan and Hadrien Reynaud and Qingjie Meng", "abstract": "  Myocardial motion tracking is important for assessing cardiac function and\ndiagnosing cardiovascular diseases, for which cine cardiac magnetic resonance\n(CMR) has been established as the gold standard imaging modality. Many existing\nmethods learn motion from single image pairs consisting of a reference frame\nand a randomly selected target frame from the cardiac cycle. However, these\nmethods overlook the continuous nature of cardiac motion and often yield\ninconsistent and non-smooth motion estimations. In this work, we propose a\nnovel Mamba-based cardiac motion tracking network (MCM) that explicitly\nincorporates target image sequence from the cardiac cycle to achieve smooth and\ntemporally consistent motion tracking. By developing a bi-directional Mamba\nblock equipped with a bi-directional scanning mechanism, our method facilitates\nthe estimation of plausible deformation fields. With our proposed motion\ndecoder that integrates motion information from frames adjacent to the target\nframe, our method further enhances temporal coherence. Moreover, by taking\nadvantage of Mamba's structured state-space formulation, the proposed method\nlearns the continuous dynamics of the myocardium from sequential images without\nincreasing computational complexity. We evaluate the proposed method on two\npublic datasets. The experimental results demonstrate that the proposed method\nquantitatively and qualitatively outperforms both conventional and\nstate-of-the-art learning-based cardiac motion tracking methods. The code is\navailable at https://github.com/yjh-0104/MCM.\n", "link": "http://arxiv.org/abs/2507.17678v1", "date": "2025-07-23", "relevancy": 2.2193, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5711}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5685}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCM%3A%20Mamba-based%20Cardiac%20Motion%20Tracking%20using%20Sequential%20Images%20in%20MRI&body=Title%3A%20MCM%3A%20Mamba-based%20Cardiac%20Motion%20Tracking%20using%20Sequential%20Images%20in%20MRI%0AAuthor%3A%20Jiahui%20Yin%20and%20Xinxing%20Cheng%20and%20Jinming%20Duan%20and%20Yan%20Pang%20and%20Declan%20O%27Regan%20and%20Hadrien%20Reynaud%20and%20Qingjie%20Meng%0AAbstract%3A%20%20%20Myocardial%20motion%20tracking%20is%20important%20for%20assessing%20cardiac%20function%20and%0Adiagnosing%20cardiovascular%20diseases%2C%20for%20which%20cine%20cardiac%20magnetic%20resonance%0A%28CMR%29%20has%20been%20established%20as%20the%20gold%20standard%20imaging%20modality.%20Many%20existing%0Amethods%20learn%20motion%20from%20single%20image%20pairs%20consisting%20of%20a%20reference%20frame%0Aand%20a%20randomly%20selected%20target%20frame%20from%20the%20cardiac%20cycle.%20However%2C%20these%0Amethods%20overlook%20the%20continuous%20nature%20of%20cardiac%20motion%20and%20often%20yield%0Ainconsistent%20and%20non-smooth%20motion%20estimations.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20Mamba-based%20cardiac%20motion%20tracking%20network%20%28MCM%29%20that%20explicitly%0Aincorporates%20target%20image%20sequence%20from%20the%20cardiac%20cycle%20to%20achieve%20smooth%20and%0Atemporally%20consistent%20motion%20tracking.%20By%20developing%20a%20bi-directional%20Mamba%0Ablock%20equipped%20with%20a%20bi-directional%20scanning%20mechanism%2C%20our%20method%20facilitates%0Athe%20estimation%20of%20plausible%20deformation%20fields.%20With%20our%20proposed%20motion%0Adecoder%20that%20integrates%20motion%20information%20from%20frames%20adjacent%20to%20the%20target%0Aframe%2C%20our%20method%20further%20enhances%20temporal%20coherence.%20Moreover%2C%20by%20taking%0Aadvantage%20of%20Mamba%27s%20structured%20state-space%20formulation%2C%20the%20proposed%20method%0Alearns%20the%20continuous%20dynamics%20of%20the%20myocardium%20from%20sequential%20images%20without%0Aincreasing%20computational%20complexity.%20We%20evaluate%20the%20proposed%20method%20on%20two%0Apublic%20datasets.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%0Aquantitatively%20and%20qualitatively%20outperforms%20both%20conventional%20and%0Astate-of-the-art%20learning-based%20cardiac%20motion%20tracking%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yjh-0104/MCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCM%253A%2520Mamba-based%2520Cardiac%2520Motion%2520Tracking%2520using%2520Sequential%2520Images%2520in%2520MRI%26entry.906535625%3DJiahui%2520Yin%2520and%2520Xinxing%2520Cheng%2520and%2520Jinming%2520Duan%2520and%2520Yan%2520Pang%2520and%2520Declan%2520O%2527Regan%2520and%2520Hadrien%2520Reynaud%2520and%2520Qingjie%2520Meng%26entry.1292438233%3D%2520%2520Myocardial%2520motion%2520tracking%2520is%2520important%2520for%2520assessing%2520cardiac%2520function%2520and%250Adiagnosing%2520cardiovascular%2520diseases%252C%2520for%2520which%2520cine%2520cardiac%2520magnetic%2520resonance%250A%2528CMR%2529%2520has%2520been%2520established%2520as%2520the%2520gold%2520standard%2520imaging%2520modality.%2520Many%2520existing%250Amethods%2520learn%2520motion%2520from%2520single%2520image%2520pairs%2520consisting%2520of%2520a%2520reference%2520frame%250Aand%2520a%2520randomly%2520selected%2520target%2520frame%2520from%2520the%2520cardiac%2520cycle.%2520However%252C%2520these%250Amethods%2520overlook%2520the%2520continuous%2520nature%2520of%2520cardiac%2520motion%2520and%2520often%2520yield%250Ainconsistent%2520and%2520non-smooth%2520motion%2520estimations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520Mamba-based%2520cardiac%2520motion%2520tracking%2520network%2520%2528MCM%2529%2520that%2520explicitly%250Aincorporates%2520target%2520image%2520sequence%2520from%2520the%2520cardiac%2520cycle%2520to%2520achieve%2520smooth%2520and%250Atemporally%2520consistent%2520motion%2520tracking.%2520By%2520developing%2520a%2520bi-directional%2520Mamba%250Ablock%2520equipped%2520with%2520a%2520bi-directional%2520scanning%2520mechanism%252C%2520our%2520method%2520facilitates%250Athe%2520estimation%2520of%2520plausible%2520deformation%2520fields.%2520With%2520our%2520proposed%2520motion%250Adecoder%2520that%2520integrates%2520motion%2520information%2520from%2520frames%2520adjacent%2520to%2520the%2520target%250Aframe%252C%2520our%2520method%2520further%2520enhances%2520temporal%2520coherence.%2520Moreover%252C%2520by%2520taking%250Aadvantage%2520of%2520Mamba%2527s%2520structured%2520state-space%2520formulation%252C%2520the%2520proposed%2520method%250Alearns%2520the%2520continuous%2520dynamics%2520of%2520the%2520myocardium%2520from%2520sequential%2520images%2520without%250Aincreasing%2520computational%2520complexity.%2520We%2520evaluate%2520the%2520proposed%2520method%2520on%2520two%250Apublic%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aquantitatively%2520and%2520qualitatively%2520outperforms%2520both%2520conventional%2520and%250Astate-of-the-art%2520learning-based%2520cardiac%2520motion%2520tracking%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/yjh-0104/MCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCM%3A%20Mamba-based%20Cardiac%20Motion%20Tracking%20using%20Sequential%20Images%20in%20MRI&entry.906535625=Jiahui%20Yin%20and%20Xinxing%20Cheng%20and%20Jinming%20Duan%20and%20Yan%20Pang%20and%20Declan%20O%27Regan%20and%20Hadrien%20Reynaud%20and%20Qingjie%20Meng&entry.1292438233=%20%20Myocardial%20motion%20tracking%20is%20important%20for%20assessing%20cardiac%20function%20and%0Adiagnosing%20cardiovascular%20diseases%2C%20for%20which%20cine%20cardiac%20magnetic%20resonance%0A%28CMR%29%20has%20been%20established%20as%20the%20gold%20standard%20imaging%20modality.%20Many%20existing%0Amethods%20learn%20motion%20from%20single%20image%20pairs%20consisting%20of%20a%20reference%20frame%0Aand%20a%20randomly%20selected%20target%20frame%20from%20the%20cardiac%20cycle.%20However%2C%20these%0Amethods%20overlook%20the%20continuous%20nature%20of%20cardiac%20motion%20and%20often%20yield%0Ainconsistent%20and%20non-smooth%20motion%20estimations.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20Mamba-based%20cardiac%20motion%20tracking%20network%20%28MCM%29%20that%20explicitly%0Aincorporates%20target%20image%20sequence%20from%20the%20cardiac%20cycle%20to%20achieve%20smooth%20and%0Atemporally%20consistent%20motion%20tracking.%20By%20developing%20a%20bi-directional%20Mamba%0Ablock%20equipped%20with%20a%20bi-directional%20scanning%20mechanism%2C%20our%20method%20facilitates%0Athe%20estimation%20of%20plausible%20deformation%20fields.%20With%20our%20proposed%20motion%0Adecoder%20that%20integrates%20motion%20information%20from%20frames%20adjacent%20to%20the%20target%0Aframe%2C%20our%20method%20further%20enhances%20temporal%20coherence.%20Moreover%2C%20by%20taking%0Aadvantage%20of%20Mamba%27s%20structured%20state-space%20formulation%2C%20the%20proposed%20method%0Alearns%20the%20continuous%20dynamics%20of%20the%20myocardium%20from%20sequential%20images%20without%0Aincreasing%20computational%20complexity.%20We%20evaluate%20the%20proposed%20method%20on%20two%0Apublic%20datasets.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%0Aquantitatively%20and%20qualitatively%20outperforms%20both%20conventional%20and%0Astate-of-the-art%20learning-based%20cardiac%20motion%20tracking%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yjh-0104/MCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17678v1&entry.124074799=Read"},
{"title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large\n  Language Models", "author": "Haoran Sun and Zekun Zhang and Shaoning Zeng", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.\n", "link": "http://arxiv.org/abs/2507.17477v1", "date": "2025-07-23", "relevancy": 2.2056, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6347}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Uncertainty-Driven%20Adaptive%20Self-Alignment%20Framework%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20An%20Uncertainty-Driven%20Adaptive%20Self-Alignment%20Framework%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Haoran%20Sun%20and%20Zekun%20Zhang%20and%20Shaoning%20Zeng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20progress%20in%0Ainstruction%20following%20and%20general-purpose%20reasoning.%20However%2C%20achieving%0Ahigh-quality%20alignment%20with%20human%20intent%20and%20safety%20norms%20without%20human%0Aannotations%20remains%20a%20fundamental%20challenge.%20In%20this%20work%2C%20we%20propose%20an%0AUncertainty-Driven%20Adaptive%20Self-Alignment%20%28UDASA%29%20framework%20designed%20to%0Aimprove%20LLM%20alignment%20in%20a%20fully%20automated%20manner.%20UDASA%20first%20generates%0Amultiple%20responses%20for%20each%20input%20and%20quantifies%20output%20uncertainty%20across%0Athree%20dimensions%3A%20semantics%2C%20factuality%2C%20and%20value%20alignment.%20Based%20on%20these%0Auncertainty%20scores%2C%20the%20framework%20constructs%20preference%20pairs%20and%20categorizes%0Atraining%20samples%20into%20three%20stages%2C%20conservative%2C%20moderate%2C%20and%20exploratory%2C%0Aaccording%20to%20their%20uncertainty%20difference.%20The%20model%20is%20then%20optimized%0Aprogressively%20across%20these%20stages.%20In%20addition%2C%20we%20conduct%20a%20series%20of%0Apreliminary%20studies%20to%20validate%20the%20core%20design%20assumptions%20and%20provide%20strong%0Aempirical%20motivation%20for%20the%20proposed%20framework.%20Experimental%20results%20show%20that%0AUDASA%20outperforms%20existing%20alignment%20methods%20across%20multiple%20tasks%2C%20including%0Aharmlessness%2C%20helpfulness%2C%20truthfulness%2C%20and%20controlled%20sentiment%20generation%2C%0Asignificantly%20improving%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Uncertainty-Driven%2520Adaptive%2520Self-Alignment%2520Framework%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHaoran%2520Sun%2520and%2520Zekun%2520Zhang%2520and%2520Shaoning%2520Zeng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520progress%2520in%250Ainstruction%2520following%2520and%2520general-purpose%2520reasoning.%2520However%252C%2520achieving%250Ahigh-quality%2520alignment%2520with%2520human%2520intent%2520and%2520safety%2520norms%2520without%2520human%250Aannotations%2520remains%2520a%2520fundamental%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250AUncertainty-Driven%2520Adaptive%2520Self-Alignment%2520%2528UDASA%2529%2520framework%2520designed%2520to%250Aimprove%2520LLM%2520alignment%2520in%2520a%2520fully%2520automated%2520manner.%2520UDASA%2520first%2520generates%250Amultiple%2520responses%2520for%2520each%2520input%2520and%2520quantifies%2520output%2520uncertainty%2520across%250Athree%2520dimensions%253A%2520semantics%252C%2520factuality%252C%2520and%2520value%2520alignment.%2520Based%2520on%2520these%250Auncertainty%2520scores%252C%2520the%2520framework%2520constructs%2520preference%2520pairs%2520and%2520categorizes%250Atraining%2520samples%2520into%2520three%2520stages%252C%2520conservative%252C%2520moderate%252C%2520and%2520exploratory%252C%250Aaccording%2520to%2520their%2520uncertainty%2520difference.%2520The%2520model%2520is%2520then%2520optimized%250Aprogressively%2520across%2520these%2520stages.%2520In%2520addition%252C%2520we%2520conduct%2520a%2520series%2520of%250Apreliminary%2520studies%2520to%2520validate%2520the%2520core%2520design%2520assumptions%2520and%2520provide%2520strong%250Aempirical%2520motivation%2520for%2520the%2520proposed%2520framework.%2520Experimental%2520results%2520show%2520that%250AUDASA%2520outperforms%2520existing%2520alignment%2520methods%2520across%2520multiple%2520tasks%252C%2520including%250Aharmlessness%252C%2520helpfulness%252C%2520truthfulness%252C%2520and%2520controlled%2520sentiment%2520generation%252C%250Asignificantly%2520improving%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Uncertainty-Driven%20Adaptive%20Self-Alignment%20Framework%20for%20Large%0A%20%20Language%20Models&entry.906535625=Haoran%20Sun%20and%20Zekun%20Zhang%20and%20Shaoning%20Zeng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20progress%20in%0Ainstruction%20following%20and%20general-purpose%20reasoning.%20However%2C%20achieving%0Ahigh-quality%20alignment%20with%20human%20intent%20and%20safety%20norms%20without%20human%0Aannotations%20remains%20a%20fundamental%20challenge.%20In%20this%20work%2C%20we%20propose%20an%0AUncertainty-Driven%20Adaptive%20Self-Alignment%20%28UDASA%29%20framework%20designed%20to%0Aimprove%20LLM%20alignment%20in%20a%20fully%20automated%20manner.%20UDASA%20first%20generates%0Amultiple%20responses%20for%20each%20input%20and%20quantifies%20output%20uncertainty%20across%0Athree%20dimensions%3A%20semantics%2C%20factuality%2C%20and%20value%20alignment.%20Based%20on%20these%0Auncertainty%20scores%2C%20the%20framework%20constructs%20preference%20pairs%20and%20categorizes%0Atraining%20samples%20into%20three%20stages%2C%20conservative%2C%20moderate%2C%20and%20exploratory%2C%0Aaccording%20to%20their%20uncertainty%20difference.%20The%20model%20is%20then%20optimized%0Aprogressively%20across%20these%20stages.%20In%20addition%2C%20we%20conduct%20a%20series%20of%0Apreliminary%20studies%20to%20validate%20the%20core%20design%20assumptions%20and%20provide%20strong%0Aempirical%20motivation%20for%20the%20proposed%20framework.%20Experimental%20results%20show%20that%0AUDASA%20outperforms%20existing%20alignment%20methods%20across%20multiple%20tasks%2C%20including%0Aharmlessness%2C%20helpfulness%2C%20truthfulness%2C%20and%20controlled%20sentiment%20generation%2C%0Asignificantly%20improving%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17477v1&entry.124074799=Read"},
{"title": "Optimal differentially private kernel learning with random projection", "author": "Bonwoo Lee and Cheolwoo Park and Jeongyoun Ahn", "abstract": "  Differential privacy has become a cornerstone in the development of\nprivacy-preserving learning algorithms. This work addresses optimizing\ndifferentially private kernel learning within the empirical risk minimization\n(ERM) framework. We propose a novel differentially private kernel ERM algorithm\nbased on random projection in the reproducing kernel Hilbert space using\nGaussian processes. Our method achieves minimax-optimal excess risk for both\nthe squared loss and Lipschitz-smooth convex loss functions under a local\nstrong convexity condition. We further show that existing approaches based on\nalternative dimension reduction techniques, such as random Fourier feature\nmappings or $\\ell_2$ regularization, yield suboptimal generalization\nperformance. Our key theoretical contribution also includes the derivation of\ndimension-free generalization bounds for objective perturbation-based private\nlinear ERM -- marking the first such result that does not rely on noisy\ngradient-based mechanisms. Additionally, we obtain sharper generalization\nbounds for existing differentially private kernel ERM algorithms. Empirical\nevaluations support our theoretical claims, demonstrating that random\nprojection enables statistically efficient and optimally private kernel\nlearning. These findings provide new insights into the design of differentially\nprivate algorithms and highlight the central role of dimension reduction in\nbalancing privacy and utility.\n", "link": "http://arxiv.org/abs/2507.17544v1", "date": "2025-07-23", "relevancy": 2.2048, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4515}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4393}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20differentially%20private%20kernel%20learning%20with%20random%20projection&body=Title%3A%20Optimal%20differentially%20private%20kernel%20learning%20with%20random%20projection%0AAuthor%3A%20Bonwoo%20Lee%20and%20Cheolwoo%20Park%20and%20Jeongyoun%20Ahn%0AAbstract%3A%20%20%20Differential%20privacy%20has%20become%20a%20cornerstone%20in%20the%20development%20of%0Aprivacy-preserving%20learning%20algorithms.%20This%20work%20addresses%20optimizing%0Adifferentially%20private%20kernel%20learning%20within%20the%20empirical%20risk%20minimization%0A%28ERM%29%20framework.%20We%20propose%20a%20novel%20differentially%20private%20kernel%20ERM%20algorithm%0Abased%20on%20random%20projection%20in%20the%20reproducing%20kernel%20Hilbert%20space%20using%0AGaussian%20processes.%20Our%20method%20achieves%20minimax-optimal%20excess%20risk%20for%20both%0Athe%20squared%20loss%20and%20Lipschitz-smooth%20convex%20loss%20functions%20under%20a%20local%0Astrong%20convexity%20condition.%20We%20further%20show%20that%20existing%20approaches%20based%20on%0Aalternative%20dimension%20reduction%20techniques%2C%20such%20as%20random%20Fourier%20feature%0Amappings%20or%20%24%5Cell_2%24%20regularization%2C%20yield%20suboptimal%20generalization%0Aperformance.%20Our%20key%20theoretical%20contribution%20also%20includes%20the%20derivation%20of%0Adimension-free%20generalization%20bounds%20for%20objective%20perturbation-based%20private%0Alinear%20ERM%20--%20marking%20the%20first%20such%20result%20that%20does%20not%20rely%20on%20noisy%0Agradient-based%20mechanisms.%20Additionally%2C%20we%20obtain%20sharper%20generalization%0Abounds%20for%20existing%20differentially%20private%20kernel%20ERM%20algorithms.%20Empirical%0Aevaluations%20support%20our%20theoretical%20claims%2C%20demonstrating%20that%20random%0Aprojection%20enables%20statistically%20efficient%20and%20optimally%20private%20kernel%0Alearning.%20These%20findings%20provide%20new%20insights%20into%20the%20design%20of%20differentially%0Aprivate%20algorithms%20and%20highlight%20the%20central%20role%20of%20dimension%20reduction%20in%0Abalancing%20privacy%20and%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520differentially%2520private%2520kernel%2520learning%2520with%2520random%2520projection%26entry.906535625%3DBonwoo%2520Lee%2520and%2520Cheolwoo%2520Park%2520and%2520Jeongyoun%2520Ahn%26entry.1292438233%3D%2520%2520Differential%2520privacy%2520has%2520become%2520a%2520cornerstone%2520in%2520the%2520development%2520of%250Aprivacy-preserving%2520learning%2520algorithms.%2520This%2520work%2520addresses%2520optimizing%250Adifferentially%2520private%2520kernel%2520learning%2520within%2520the%2520empirical%2520risk%2520minimization%250A%2528ERM%2529%2520framework.%2520We%2520propose%2520a%2520novel%2520differentially%2520private%2520kernel%2520ERM%2520algorithm%250Abased%2520on%2520random%2520projection%2520in%2520the%2520reproducing%2520kernel%2520Hilbert%2520space%2520using%250AGaussian%2520processes.%2520Our%2520method%2520achieves%2520minimax-optimal%2520excess%2520risk%2520for%2520both%250Athe%2520squared%2520loss%2520and%2520Lipschitz-smooth%2520convex%2520loss%2520functions%2520under%2520a%2520local%250Astrong%2520convexity%2520condition.%2520We%2520further%2520show%2520that%2520existing%2520approaches%2520based%2520on%250Aalternative%2520dimension%2520reduction%2520techniques%252C%2520such%2520as%2520random%2520Fourier%2520feature%250Amappings%2520or%2520%2524%255Cell_2%2524%2520regularization%252C%2520yield%2520suboptimal%2520generalization%250Aperformance.%2520Our%2520key%2520theoretical%2520contribution%2520also%2520includes%2520the%2520derivation%2520of%250Adimension-free%2520generalization%2520bounds%2520for%2520objective%2520perturbation-based%2520private%250Alinear%2520ERM%2520--%2520marking%2520the%2520first%2520such%2520result%2520that%2520does%2520not%2520rely%2520on%2520noisy%250Agradient-based%2520mechanisms.%2520Additionally%252C%2520we%2520obtain%2520sharper%2520generalization%250Abounds%2520for%2520existing%2520differentially%2520private%2520kernel%2520ERM%2520algorithms.%2520Empirical%250Aevaluations%2520support%2520our%2520theoretical%2520claims%252C%2520demonstrating%2520that%2520random%250Aprojection%2520enables%2520statistically%2520efficient%2520and%2520optimally%2520private%2520kernel%250Alearning.%2520These%2520findings%2520provide%2520new%2520insights%2520into%2520the%2520design%2520of%2520differentially%250Aprivate%2520algorithms%2520and%2520highlight%2520the%2520central%2520role%2520of%2520dimension%2520reduction%2520in%250Abalancing%2520privacy%2520and%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20differentially%20private%20kernel%20learning%20with%20random%20projection&entry.906535625=Bonwoo%20Lee%20and%20Cheolwoo%20Park%20and%20Jeongyoun%20Ahn&entry.1292438233=%20%20Differential%20privacy%20has%20become%20a%20cornerstone%20in%20the%20development%20of%0Aprivacy-preserving%20learning%20algorithms.%20This%20work%20addresses%20optimizing%0Adifferentially%20private%20kernel%20learning%20within%20the%20empirical%20risk%20minimization%0A%28ERM%29%20framework.%20We%20propose%20a%20novel%20differentially%20private%20kernel%20ERM%20algorithm%0Abased%20on%20random%20projection%20in%20the%20reproducing%20kernel%20Hilbert%20space%20using%0AGaussian%20processes.%20Our%20method%20achieves%20minimax-optimal%20excess%20risk%20for%20both%0Athe%20squared%20loss%20and%20Lipschitz-smooth%20convex%20loss%20functions%20under%20a%20local%0Astrong%20convexity%20condition.%20We%20further%20show%20that%20existing%20approaches%20based%20on%0Aalternative%20dimension%20reduction%20techniques%2C%20such%20as%20random%20Fourier%20feature%0Amappings%20or%20%24%5Cell_2%24%20regularization%2C%20yield%20suboptimal%20generalization%0Aperformance.%20Our%20key%20theoretical%20contribution%20also%20includes%20the%20derivation%20of%0Adimension-free%20generalization%20bounds%20for%20objective%20perturbation-based%20private%0Alinear%20ERM%20--%20marking%20the%20first%20such%20result%20that%20does%20not%20rely%20on%20noisy%0Agradient-based%20mechanisms.%20Additionally%2C%20we%20obtain%20sharper%20generalization%0Abounds%20for%20existing%20differentially%20private%20kernel%20ERM%20algorithms.%20Empirical%0Aevaluations%20support%20our%20theoretical%20claims%2C%20demonstrating%20that%20random%0Aprojection%20enables%20statistically%20efficient%20and%20optimally%20private%20kernel%0Alearning.%20These%20findings%20provide%20new%20insights%20into%20the%20design%20of%20differentially%0Aprivate%20algorithms%20and%20highlight%20the%20central%20role%20of%20dimension%20reduction%20in%0Abalancing%20privacy%20and%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17544v1&entry.124074799=Read"},
{"title": "Towards Generalist Robot Learning from Internet Video: A Survey", "author": "Robert McCarthy and Daniel C. H. Tan and Dominik Schmidt and Fernando Acero and Nathan Herr and Yilun Du and Thomas G. Thuruthel and Zhibin Li", "abstract": "  Scaling deep learning to massive and diverse internet data has driven\nremarkable breakthroughs in domains such as video generation and natural\nlanguage processing. Robot learning, however, has thus far failed to replicate\nthis success and remains constrained by a scarcity of available data. Learning\nfrom videos (LfV) methods aim to address this data bottleneck by augmenting\ntraditional robot data with large-scale internet video. This video data\nprovides foundational information regarding physical dynamics, behaviours, and\ntasks, and can be highly informative for general-purpose robots.\n  This survey systematically examines the emerging field of LfV. We first\noutline essential concepts, including detailing fundamental LfV challenges such\nas distribution shift and missing action labels in video data. Next, we\ncomprehensively review current methods for extracting knowledge from\nlarge-scale internet video, overcoming LfV challenges, and improving robot\nlearning through video-informed training. The survey concludes with a critical\ndiscussion of future opportunities. Here, we emphasize the need for scalable\nfoundation model approaches that can leverage the full range of available\ninternet video and enhance the learning of robot policies and dynamics models.\nOverall, the survey aims to inform and catalyse future LfV research, driving\nprogress towards general-purpose robots.\n", "link": "http://arxiv.org/abs/2404.19664v5", "date": "2025-07-23", "relevancy": 2.1966, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5612}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&body=Title%3A%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey%0AAuthor%3A%20Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Scaling%20deep%20learning%20to%20massive%20and%20diverse%20internet%20data%20has%20driven%0Aremarkable%20breakthroughs%20in%20domains%20such%20as%20video%20generation%20and%20natural%0Alanguage%20processing.%20Robot%20learning%2C%20however%2C%20has%20thus%20far%20failed%20to%20replicate%0Athis%20success%20and%20remains%20constrained%20by%20a%20scarcity%20of%20available%20data.%20Learning%0Afrom%20videos%20%28LfV%29%20methods%20aim%20to%20address%20this%20data%20bottleneck%20by%20augmenting%0Atraditional%20robot%20data%20with%20large-scale%20internet%20video.%20This%20video%20data%0Aprovides%20foundational%20information%20regarding%20physical%20dynamics%2C%20behaviours%2C%20and%0Atasks%2C%20and%20can%20be%20highly%20informative%20for%20general-purpose%20robots.%0A%20%20This%20survey%20systematically%20examines%20the%20emerging%20field%20of%20LfV.%20We%20first%0Aoutline%20essential%20concepts%2C%20including%20detailing%20fundamental%20LfV%20challenges%20such%0Aas%20distribution%20shift%20and%20missing%20action%20labels%20in%20video%20data.%20Next%2C%20we%0Acomprehensively%20review%20current%20methods%20for%20extracting%20knowledge%20from%0Alarge-scale%20internet%20video%2C%20overcoming%20LfV%20challenges%2C%20and%20improving%20robot%0Alearning%20through%20video-informed%20training.%20The%20survey%20concludes%20with%20a%20critical%0Adiscussion%20of%20future%20opportunities.%20Here%2C%20we%20emphasize%20the%20need%20for%20scalable%0Afoundation%20model%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%0Ainternet%20video%20and%20enhance%20the%20learning%20of%20robot%20policies%20and%20dynamics%20models.%0AOverall%2C%20the%20survey%20aims%20to%20inform%20and%20catalyse%20future%20LfV%20research%2C%20driving%0Aprogress%20towards%20general-purpose%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19664v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Robot%2520Learning%2520from%2520Internet%2520Video%253A%2520A%2520Survey%26entry.906535625%3DRobert%2520McCarthy%2520and%2520Daniel%2520C.%2520H.%2520Tan%2520and%2520Dominik%2520Schmidt%2520and%2520Fernando%2520Acero%2520and%2520Nathan%2520Herr%2520and%2520Yilun%2520Du%2520and%2520Thomas%2520G.%2520Thuruthel%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Scaling%2520deep%2520learning%2520to%2520massive%2520and%2520diverse%2520internet%2520data%2520has%2520driven%250Aremarkable%2520breakthroughs%2520in%2520domains%2520such%2520as%2520video%2520generation%2520and%2520natural%250Alanguage%2520processing.%2520Robot%2520learning%252C%2520however%252C%2520has%2520thus%2520far%2520failed%2520to%2520replicate%250Athis%2520success%2520and%2520remains%2520constrained%2520by%2520a%2520scarcity%2520of%2520available%2520data.%2520Learning%250Afrom%2520videos%2520%2528LfV%2529%2520methods%2520aim%2520to%2520address%2520this%2520data%2520bottleneck%2520by%2520augmenting%250Atraditional%2520robot%2520data%2520with%2520large-scale%2520internet%2520video.%2520This%2520video%2520data%250Aprovides%2520foundational%2520information%2520regarding%2520physical%2520dynamics%252C%2520behaviours%252C%2520and%250Atasks%252C%2520and%2520can%2520be%2520highly%2520informative%2520for%2520general-purpose%2520robots.%250A%2520%2520This%2520survey%2520systematically%2520examines%2520the%2520emerging%2520field%2520of%2520LfV.%2520We%2520first%250Aoutline%2520essential%2520concepts%252C%2520including%2520detailing%2520fundamental%2520LfV%2520challenges%2520such%250Aas%2520distribution%2520shift%2520and%2520missing%2520action%2520labels%2520in%2520video%2520data.%2520Next%252C%2520we%250Acomprehensively%2520review%2520current%2520methods%2520for%2520extracting%2520knowledge%2520from%250Alarge-scale%2520internet%2520video%252C%2520overcoming%2520LfV%2520challenges%252C%2520and%2520improving%2520robot%250Alearning%2520through%2520video-informed%2520training.%2520The%2520survey%2520concludes%2520with%2520a%2520critical%250Adiscussion%2520of%2520future%2520opportunities.%2520Here%252C%2520we%2520emphasize%2520the%2520need%2520for%2520scalable%250Afoundation%2520model%2520approaches%2520that%2520can%2520leverage%2520the%2520full%2520range%2520of%2520available%250Ainternet%2520video%2520and%2520enhance%2520the%2520learning%2520of%2520robot%2520policies%2520and%2520dynamics%2520models.%250AOverall%252C%2520the%2520survey%2520aims%2520to%2520inform%2520and%2520catalyse%2520future%2520LfV%2520research%252C%2520driving%250Aprogress%2520towards%2520general-purpose%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19664v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&entry.906535625=Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li&entry.1292438233=%20%20Scaling%20deep%20learning%20to%20massive%20and%20diverse%20internet%20data%20has%20driven%0Aremarkable%20breakthroughs%20in%20domains%20such%20as%20video%20generation%20and%20natural%0Alanguage%20processing.%20Robot%20learning%2C%20however%2C%20has%20thus%20far%20failed%20to%20replicate%0Athis%20success%20and%20remains%20constrained%20by%20a%20scarcity%20of%20available%20data.%20Learning%0Afrom%20videos%20%28LfV%29%20methods%20aim%20to%20address%20this%20data%20bottleneck%20by%20augmenting%0Atraditional%20robot%20data%20with%20large-scale%20internet%20video.%20This%20video%20data%0Aprovides%20foundational%20information%20regarding%20physical%20dynamics%2C%20behaviours%2C%20and%0Atasks%2C%20and%20can%20be%20highly%20informative%20for%20general-purpose%20robots.%0A%20%20This%20survey%20systematically%20examines%20the%20emerging%20field%20of%20LfV.%20We%20first%0Aoutline%20essential%20concepts%2C%20including%20detailing%20fundamental%20LfV%20challenges%20such%0Aas%20distribution%20shift%20and%20missing%20action%20labels%20in%20video%20data.%20Next%2C%20we%0Acomprehensively%20review%20current%20methods%20for%20extracting%20knowledge%20from%0Alarge-scale%20internet%20video%2C%20overcoming%20LfV%20challenges%2C%20and%20improving%20robot%0Alearning%20through%20video-informed%20training.%20The%20survey%20concludes%20with%20a%20critical%0Adiscussion%20of%20future%20opportunities.%20Here%2C%20we%20emphasize%20the%20need%20for%20scalable%0Afoundation%20model%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%0Ainternet%20video%20and%20enhance%20the%20learning%20of%20robot%20policies%20and%20dynamics%20models.%0AOverall%2C%20the%20survey%20aims%20to%20inform%20and%20catalyse%20future%20LfV%20research%2C%20driving%0Aprogress%20towards%20general-purpose%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19664v5&entry.124074799=Read"},
{"title": "Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with\n  Sequential Mixture of Experts for Multi-View Mammography", "author": "Farnoush Bayatmakou and Reza Taleei and Nicole Simone and Arash Mohammadi", "abstract": "  Breast cancer (BC) remains one of the leading causes of cancer-related\nmortality among women, despite recent advances in Computer-Aided Diagnosis\n(CAD) systems. Accurate and efficient interpretation of multi-view mammograms\nis essential for early detection, driving a surge of interest in Artificial\nIntelligence (AI)-powered CAD models. While state-of-the-art multi-view\nmammogram classification models are largely based on Transformer architectures,\ntheir computational complexity scales quadratically with the number of image\npatches, highlighting the need for more efficient alternatives. To address this\nchallenge, we propose Mammo-Mamba, a novel framework that integrates Selective\nState-Space Models (SSMs), transformer-based attention, and expert-driven\nfeature refinement into a unified architecture. Mammo-Mamba extends the\nMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)\nmechanism through its customized SecMamba block. The SecMamba is a modified\nMambaVision block that enhances representation learning in high-resolution\nmammographic images by enabling content-adaptive feature refinement. These\nblocks are integrated into the deeper stages of MambaVision, allowing the model\nto progressively adjust feature emphasis through dynamic expert gating,\neffectively mitigating the limitations of traditional Transformer models.\nEvaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior\nclassification performance across all key metrics while maintaining\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2507.17662v1", "date": "2025-07-23", "relevancy": 2.1941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5422}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mammo-Mamba%3A%20A%20Hybrid%20State-Space%20and%20Transformer%20Architecture%20with%0A%20%20Sequential%20Mixture%20of%20Experts%20for%20Multi-View%20Mammography&body=Title%3A%20Mammo-Mamba%3A%20A%20Hybrid%20State-Space%20and%20Transformer%20Architecture%20with%0A%20%20Sequential%20Mixture%20of%20Experts%20for%20Multi-View%20Mammography%0AAuthor%3A%20Farnoush%20Bayatmakou%20and%20Reza%20Taleei%20and%20Nicole%20Simone%20and%20Arash%20Mohammadi%0AAbstract%3A%20%20%20Breast%20cancer%20%28BC%29%20remains%20one%20of%20the%20leading%20causes%20of%20cancer-related%0Amortality%20among%20women%2C%20despite%20recent%20advances%20in%20Computer-Aided%20Diagnosis%0A%28CAD%29%20systems.%20Accurate%20and%20efficient%20interpretation%20of%20multi-view%20mammograms%0Ais%20essential%20for%20early%20detection%2C%20driving%20a%20surge%20of%20interest%20in%20Artificial%0AIntelligence%20%28AI%29-powered%20CAD%20models.%20While%20state-of-the-art%20multi-view%0Amammogram%20classification%20models%20are%20largely%20based%20on%20Transformer%20architectures%2C%0Atheir%20computational%20complexity%20scales%20quadratically%20with%20the%20number%20of%20image%0Apatches%2C%20highlighting%20the%20need%20for%20more%20efficient%20alternatives.%20To%20address%20this%0Achallenge%2C%20we%20propose%20Mammo-Mamba%2C%20a%20novel%20framework%20that%20integrates%20Selective%0AState-Space%20Models%20%28SSMs%29%2C%20transformer-based%20attention%2C%20and%20expert-driven%0Afeature%20refinement%20into%20a%20unified%20architecture.%20Mammo-Mamba%20extends%20the%0AMambaVision%20backbone%20by%20introducing%20the%20Sequential%20Mixture%20of%20Experts%20%28SeqMoE%29%0Amechanism%20through%20its%20customized%20SecMamba%20block.%20The%20SecMamba%20is%20a%20modified%0AMambaVision%20block%20that%20enhances%20representation%20learning%20in%20high-resolution%0Amammographic%20images%20by%20enabling%20content-adaptive%20feature%20refinement.%20These%0Ablocks%20are%20integrated%20into%20the%20deeper%20stages%20of%20MambaVision%2C%20allowing%20the%20model%0Ato%20progressively%20adjust%20feature%20emphasis%20through%20dynamic%20expert%20gating%2C%0Aeffectively%20mitigating%20the%20limitations%20of%20traditional%20Transformer%20models.%0AEvaluated%20on%20the%20CBIS-DDSM%20benchmark%20dataset%2C%20Mammo-Mamba%20achieves%20superior%0Aclassification%20performance%20across%20all%20key%20metrics%20while%20maintaining%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMammo-Mamba%253A%2520A%2520Hybrid%2520State-Space%2520and%2520Transformer%2520Architecture%2520with%250A%2520%2520Sequential%2520Mixture%2520of%2520Experts%2520for%2520Multi-View%2520Mammography%26entry.906535625%3DFarnoush%2520Bayatmakou%2520and%2520Reza%2520Taleei%2520and%2520Nicole%2520Simone%2520and%2520Arash%2520Mohammadi%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520%2528BC%2529%2520remains%2520one%2520of%2520the%2520leading%2520causes%2520of%2520cancer-related%250Amortality%2520among%2520women%252C%2520despite%2520recent%2520advances%2520in%2520Computer-Aided%2520Diagnosis%250A%2528CAD%2529%2520systems.%2520Accurate%2520and%2520efficient%2520interpretation%2520of%2520multi-view%2520mammograms%250Ais%2520essential%2520for%2520early%2520detection%252C%2520driving%2520a%2520surge%2520of%2520interest%2520in%2520Artificial%250AIntelligence%2520%2528AI%2529-powered%2520CAD%2520models.%2520While%2520state-of-the-art%2520multi-view%250Amammogram%2520classification%2520models%2520are%2520largely%2520based%2520on%2520Transformer%2520architectures%252C%250Atheir%2520computational%2520complexity%2520scales%2520quadratically%2520with%2520the%2520number%2520of%2520image%250Apatches%252C%2520highlighting%2520the%2520need%2520for%2520more%2520efficient%2520alternatives.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520Mammo-Mamba%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520Selective%250AState-Space%2520Models%2520%2528SSMs%2529%252C%2520transformer-based%2520attention%252C%2520and%2520expert-driven%250Afeature%2520refinement%2520into%2520a%2520unified%2520architecture.%2520Mammo-Mamba%2520extends%2520the%250AMambaVision%2520backbone%2520by%2520introducing%2520the%2520Sequential%2520Mixture%2520of%2520Experts%2520%2528SeqMoE%2529%250Amechanism%2520through%2520its%2520customized%2520SecMamba%2520block.%2520The%2520SecMamba%2520is%2520a%2520modified%250AMambaVision%2520block%2520that%2520enhances%2520representation%2520learning%2520in%2520high-resolution%250Amammographic%2520images%2520by%2520enabling%2520content-adaptive%2520feature%2520refinement.%2520These%250Ablocks%2520are%2520integrated%2520into%2520the%2520deeper%2520stages%2520of%2520MambaVision%252C%2520allowing%2520the%2520model%250Ato%2520progressively%2520adjust%2520feature%2520emphasis%2520through%2520dynamic%2520expert%2520gating%252C%250Aeffectively%2520mitigating%2520the%2520limitations%2520of%2520traditional%2520Transformer%2520models.%250AEvaluated%2520on%2520the%2520CBIS-DDSM%2520benchmark%2520dataset%252C%2520Mammo-Mamba%2520achieves%2520superior%250Aclassification%2520performance%2520across%2520all%2520key%2520metrics%2520while%2520maintaining%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mammo-Mamba%3A%20A%20Hybrid%20State-Space%20and%20Transformer%20Architecture%20with%0A%20%20Sequential%20Mixture%20of%20Experts%20for%20Multi-View%20Mammography&entry.906535625=Farnoush%20Bayatmakou%20and%20Reza%20Taleei%20and%20Nicole%20Simone%20and%20Arash%20Mohammadi&entry.1292438233=%20%20Breast%20cancer%20%28BC%29%20remains%20one%20of%20the%20leading%20causes%20of%20cancer-related%0Amortality%20among%20women%2C%20despite%20recent%20advances%20in%20Computer-Aided%20Diagnosis%0A%28CAD%29%20systems.%20Accurate%20and%20efficient%20interpretation%20of%20multi-view%20mammograms%0Ais%20essential%20for%20early%20detection%2C%20driving%20a%20surge%20of%20interest%20in%20Artificial%0AIntelligence%20%28AI%29-powered%20CAD%20models.%20While%20state-of-the-art%20multi-view%0Amammogram%20classification%20models%20are%20largely%20based%20on%20Transformer%20architectures%2C%0Atheir%20computational%20complexity%20scales%20quadratically%20with%20the%20number%20of%20image%0Apatches%2C%20highlighting%20the%20need%20for%20more%20efficient%20alternatives.%20To%20address%20this%0Achallenge%2C%20we%20propose%20Mammo-Mamba%2C%20a%20novel%20framework%20that%20integrates%20Selective%0AState-Space%20Models%20%28SSMs%29%2C%20transformer-based%20attention%2C%20and%20expert-driven%0Afeature%20refinement%20into%20a%20unified%20architecture.%20Mammo-Mamba%20extends%20the%0AMambaVision%20backbone%20by%20introducing%20the%20Sequential%20Mixture%20of%20Experts%20%28SeqMoE%29%0Amechanism%20through%20its%20customized%20SecMamba%20block.%20The%20SecMamba%20is%20a%20modified%0AMambaVision%20block%20that%20enhances%20representation%20learning%20in%20high-resolution%0Amammographic%20images%20by%20enabling%20content-adaptive%20feature%20refinement.%20These%0Ablocks%20are%20integrated%20into%20the%20deeper%20stages%20of%20MambaVision%2C%20allowing%20the%20model%0Ato%20progressively%20adjust%20feature%20emphasis%20through%20dynamic%20expert%20gating%2C%0Aeffectively%20mitigating%20the%20limitations%20of%20traditional%20Transformer%20models.%0AEvaluated%20on%20the%20CBIS-DDSM%20benchmark%20dataset%2C%20Mammo-Mamba%20achieves%20superior%0Aclassification%20performance%20across%20all%20key%20metrics%20while%20maintaining%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17662v1&entry.124074799=Read"},
{"title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic\n  Learning", "author": "Li Jun and Wang Jinpeng and Tan Chaolei and Lian Niu and Chen Long and Zhang Min and Wang Yaowei and Xia Shu-Tao and Chen Bin", "abstract": "  Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.\n", "link": "http://arxiv.org/abs/2507.17402v1", "date": "2025-07-23", "relevancy": 2.1941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLFormer%3A%20Enhancing%20Partially%20Relevant%20Video%20Retrieval%20with%20Hyperbolic%0A%20%20Learning&body=Title%3A%20HLFormer%3A%20Enhancing%20Partially%20Relevant%20Video%20Retrieval%20with%20Hyperbolic%0A%20%20Learning%0AAuthor%3A%20Li%20Jun%20and%20Wang%20Jinpeng%20and%20Tan%20Chaolei%20and%20Lian%20Niu%20and%20Chen%20Long%20and%20Zhang%20Min%20and%20Wang%20Yaowei%20and%20Xia%20Shu-Tao%20and%20Chen%20Bin%0AAbstract%3A%20%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20addresses%20the%20critical%20challenge%20of%0Amatching%20untrimmed%20videos%20with%20text%20queries%20describing%20only%20partial%20content.%0AExisting%20methods%20suffer%20from%20geometric%20distortion%20in%20Euclidean%20space%20that%0Asometimes%20misrepresents%20the%20intrinsic%20hierarchical%20structure%20of%20videos%20and%0Aoverlooks%20certain%20hierarchical%20semantics%2C%20ultimately%20leading%20to%20suboptimal%0Atemporal%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20the%20first%20hyperbolic%0Amodeling%20framework%20for%20PRVR%2C%20namely%20HLFormer%2C%20which%20leverages%20hyperbolic%20space%0Alearning%20to%20compensate%20for%20the%20suboptimal%20hierarchical%20modeling%20capabilities%20of%0AEuclidean%20space.%20Specifically%2C%20HLFormer%20integrates%20the%20Lorentz%20Attention%20Block%0Aand%20Euclidean%20Attention%20Block%20to%20encode%20video%20embeddings%20in%20hybrid%20spaces%2C%0Ausing%20the%20Mean-Guided%20Adaptive%20Interaction%20Module%20to%20dynamically%20fuse%20features.%0AAdditionally%2C%20we%20introduce%20a%20Partial%20Order%20Preservation%20Loss%20to%20enforce%20%22text%20%3C%0Avideo%22%20hierarchy%20through%20Lorentzian%20cone%20constraints.%20This%20approach%20further%0Aenhances%20cross-modal%20matching%20by%20reinforcing%20partial%20relevance%20between%20video%0Acontent%20and%20text%20queries.%20Extensive%20experiments%20show%20that%20HLFormer%20outperforms%0Astate-of-the-art%20methods.%20Code%20is%20released%20at%0Ahttps%3A//github.com/lijun2005/ICCV25-HLFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLFormer%253A%2520Enhancing%2520Partially%2520Relevant%2520Video%2520Retrieval%2520with%2520Hyperbolic%250A%2520%2520Learning%26entry.906535625%3DLi%2520Jun%2520and%2520Wang%2520Jinpeng%2520and%2520Tan%2520Chaolei%2520and%2520Lian%2520Niu%2520and%2520Chen%2520Long%2520and%2520Zhang%2520Min%2520and%2520Wang%2520Yaowei%2520and%2520Xia%2520Shu-Tao%2520and%2520Chen%2520Bin%26entry.1292438233%3D%2520%2520Partially%2520Relevant%2520Video%2520Retrieval%2520%2528PRVR%2529%2520addresses%2520the%2520critical%2520challenge%2520of%250Amatching%2520untrimmed%2520videos%2520with%2520text%2520queries%2520describing%2520only%2520partial%2520content.%250AExisting%2520methods%2520suffer%2520from%2520geometric%2520distortion%2520in%2520Euclidean%2520space%2520that%250Asometimes%2520misrepresents%2520the%2520intrinsic%2520hierarchical%2520structure%2520of%2520videos%2520and%250Aoverlooks%2520certain%2520hierarchical%2520semantics%252C%2520ultimately%2520leading%2520to%2520suboptimal%250Atemporal%2520modeling.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520first%2520hyperbolic%250Amodeling%2520framework%2520for%2520PRVR%252C%2520namely%2520HLFormer%252C%2520which%2520leverages%2520hyperbolic%2520space%250Alearning%2520to%2520compensate%2520for%2520the%2520suboptimal%2520hierarchical%2520modeling%2520capabilities%2520of%250AEuclidean%2520space.%2520Specifically%252C%2520HLFormer%2520integrates%2520the%2520Lorentz%2520Attention%2520Block%250Aand%2520Euclidean%2520Attention%2520Block%2520to%2520encode%2520video%2520embeddings%2520in%2520hybrid%2520spaces%252C%250Ausing%2520the%2520Mean-Guided%2520Adaptive%2520Interaction%2520Module%2520to%2520dynamically%2520fuse%2520features.%250AAdditionally%252C%2520we%2520introduce%2520a%2520Partial%2520Order%2520Preservation%2520Loss%2520to%2520enforce%2520%2522text%2520%253C%250Avideo%2522%2520hierarchy%2520through%2520Lorentzian%2520cone%2520constraints.%2520This%2520approach%2520further%250Aenhances%2520cross-modal%2520matching%2520by%2520reinforcing%2520partial%2520relevance%2520between%2520video%250Acontent%2520and%2520text%2520queries.%2520Extensive%2520experiments%2520show%2520that%2520HLFormer%2520outperforms%250Astate-of-the-art%2520methods.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/lijun2005/ICCV25-HLFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLFormer%3A%20Enhancing%20Partially%20Relevant%20Video%20Retrieval%20with%20Hyperbolic%0A%20%20Learning&entry.906535625=Li%20Jun%20and%20Wang%20Jinpeng%20and%20Tan%20Chaolei%20and%20Lian%20Niu%20and%20Chen%20Long%20and%20Zhang%20Min%20and%20Wang%20Yaowei%20and%20Xia%20Shu-Tao%20and%20Chen%20Bin&entry.1292438233=%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20addresses%20the%20critical%20challenge%20of%0Amatching%20untrimmed%20videos%20with%20text%20queries%20describing%20only%20partial%20content.%0AExisting%20methods%20suffer%20from%20geometric%20distortion%20in%20Euclidean%20space%20that%0Asometimes%20misrepresents%20the%20intrinsic%20hierarchical%20structure%20of%20videos%20and%0Aoverlooks%20certain%20hierarchical%20semantics%2C%20ultimately%20leading%20to%20suboptimal%0Atemporal%20modeling.%20To%20address%20this%20issue%2C%20we%20propose%20the%20first%20hyperbolic%0Amodeling%20framework%20for%20PRVR%2C%20namely%20HLFormer%2C%20which%20leverages%20hyperbolic%20space%0Alearning%20to%20compensate%20for%20the%20suboptimal%20hierarchical%20modeling%20capabilities%20of%0AEuclidean%20space.%20Specifically%2C%20HLFormer%20integrates%20the%20Lorentz%20Attention%20Block%0Aand%20Euclidean%20Attention%20Block%20to%20encode%20video%20embeddings%20in%20hybrid%20spaces%2C%0Ausing%20the%20Mean-Guided%20Adaptive%20Interaction%20Module%20to%20dynamically%20fuse%20features.%0AAdditionally%2C%20we%20introduce%20a%20Partial%20Order%20Preservation%20Loss%20to%20enforce%20%22text%20%3C%0Avideo%22%20hierarchy%20through%20Lorentzian%20cone%20constraints.%20This%20approach%20further%0Aenhances%20cross-modal%20matching%20by%20reinforcing%20partial%20relevance%20between%20video%0Acontent%20and%20text%20queries.%20Extensive%20experiments%20show%20that%20HLFormer%20outperforms%0Astate-of-the-art%20methods.%20Code%20is%20released%20at%0Ahttps%3A//github.com/lijun2005/ICCV25-HLFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17402v1&entry.124074799=Read"},
{"title": "Exploring Spatial Diversity for Region-based Active Learning", "author": "Lile Cai and Xun Xu and Lining Zhang and Chuan-Sheng Foo", "abstract": "  State-of-the-art methods for semantic segmentation are based on deep neural\nnetworks trained on large-scale labeled datasets. Acquiring such datasets would\nincur large annotation costs, especially for dense pixel-level prediction tasks\nlike semantic segmentation. We consider region-based active learning as a\nstrategy to reduce annotation costs while maintaining high performance. In this\nsetting, batches of informative image regions instead of entire images are\nselected for labeling. Importantly, we propose that enforcing local spatial\ndiversity is beneficial for active learning in this case, and to incorporate\nspatial diversity along with the traditional active selection criterion, e.g.,\ndata sample uncertainty, in a unified optimization framework for region-based\nactive learning. We apply this framework to the Cityscapes and PASCAL VOC\ndatasets and demonstrate that the inclusion of spatial diversity effectively\nimproves the performance of uncertainty-based and feature diversity-based\nactive learning methods. Our framework achieves $95\\%$ performance of fully\nsupervised methods with only $5-9\\%$ of the labeled pixels, outperforming all\nstate-of-the-art region-based active learning methods for semantic\nsegmentation.\n", "link": "http://arxiv.org/abs/2507.17367v1", "date": "2025-07-23", "relevancy": 2.1827, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5396}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Spatial%20Diversity%20for%20Region-based%20Active%20Learning&body=Title%3A%20Exploring%20Spatial%20Diversity%20for%20Region-based%20Active%20Learning%0AAuthor%3A%20Lile%20Cai%20and%20Xun%20Xu%20and%20Lining%20Zhang%20and%20Chuan-Sheng%20Foo%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20semantic%20segmentation%20are%20based%20on%20deep%20neural%0Anetworks%20trained%20on%20large-scale%20labeled%20datasets.%20Acquiring%20such%20datasets%20would%0Aincur%20large%20annotation%20costs%2C%20especially%20for%20dense%20pixel-level%20prediction%20tasks%0Alike%20semantic%20segmentation.%20We%20consider%20region-based%20active%20learning%20as%20a%0Astrategy%20to%20reduce%20annotation%20costs%20while%20maintaining%20high%20performance.%20In%20this%0Asetting%2C%20batches%20of%20informative%20image%20regions%20instead%20of%20entire%20images%20are%0Aselected%20for%20labeling.%20Importantly%2C%20we%20propose%20that%20enforcing%20local%20spatial%0Adiversity%20is%20beneficial%20for%20active%20learning%20in%20this%20case%2C%20and%20to%20incorporate%0Aspatial%20diversity%20along%20with%20the%20traditional%20active%20selection%20criterion%2C%20e.g.%2C%0Adata%20sample%20uncertainty%2C%20in%20a%20unified%20optimization%20framework%20for%20region-based%0Aactive%20learning.%20We%20apply%20this%20framework%20to%20the%20Cityscapes%20and%20PASCAL%20VOC%0Adatasets%20and%20demonstrate%20that%20the%20inclusion%20of%20spatial%20diversity%20effectively%0Aimproves%20the%20performance%20of%20uncertainty-based%20and%20feature%20diversity-based%0Aactive%20learning%20methods.%20Our%20framework%20achieves%20%2495%5C%25%24%20performance%20of%20fully%0Asupervised%20methods%20with%20only%20%245-9%5C%25%24%20of%20the%20labeled%20pixels%2C%20outperforming%20all%0Astate-of-the-art%20region-based%20active%20learning%20methods%20for%20semantic%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Spatial%2520Diversity%2520for%2520Region-based%2520Active%2520Learning%26entry.906535625%3DLile%2520Cai%2520and%2520Xun%2520Xu%2520and%2520Lining%2520Zhang%2520and%2520Chuan-Sheng%2520Foo%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520semantic%2520segmentation%2520are%2520based%2520on%2520deep%2520neural%250Anetworks%2520trained%2520on%2520large-scale%2520labeled%2520datasets.%2520Acquiring%2520such%2520datasets%2520would%250Aincur%2520large%2520annotation%2520costs%252C%2520especially%2520for%2520dense%2520pixel-level%2520prediction%2520tasks%250Alike%2520semantic%2520segmentation.%2520We%2520consider%2520region-based%2520active%2520learning%2520as%2520a%250Astrategy%2520to%2520reduce%2520annotation%2520costs%2520while%2520maintaining%2520high%2520performance.%2520In%2520this%250Asetting%252C%2520batches%2520of%2520informative%2520image%2520regions%2520instead%2520of%2520entire%2520images%2520are%250Aselected%2520for%2520labeling.%2520Importantly%252C%2520we%2520propose%2520that%2520enforcing%2520local%2520spatial%250Adiversity%2520is%2520beneficial%2520for%2520active%2520learning%2520in%2520this%2520case%252C%2520and%2520to%2520incorporate%250Aspatial%2520diversity%2520along%2520with%2520the%2520traditional%2520active%2520selection%2520criterion%252C%2520e.g.%252C%250Adata%2520sample%2520uncertainty%252C%2520in%2520a%2520unified%2520optimization%2520framework%2520for%2520region-based%250Aactive%2520learning.%2520We%2520apply%2520this%2520framework%2520to%2520the%2520Cityscapes%2520and%2520PASCAL%2520VOC%250Adatasets%2520and%2520demonstrate%2520that%2520the%2520inclusion%2520of%2520spatial%2520diversity%2520effectively%250Aimproves%2520the%2520performance%2520of%2520uncertainty-based%2520and%2520feature%2520diversity-based%250Aactive%2520learning%2520methods.%2520Our%2520framework%2520achieves%2520%252495%255C%2525%2524%2520performance%2520of%2520fully%250Asupervised%2520methods%2520with%2520only%2520%25245-9%255C%2525%2524%2520of%2520the%2520labeled%2520pixels%252C%2520outperforming%2520all%250Astate-of-the-art%2520region-based%2520active%2520learning%2520methods%2520for%2520semantic%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Spatial%20Diversity%20for%20Region-based%20Active%20Learning&entry.906535625=Lile%20Cai%20and%20Xun%20Xu%20and%20Lining%20Zhang%20and%20Chuan-Sheng%20Foo&entry.1292438233=%20%20State-of-the-art%20methods%20for%20semantic%20segmentation%20are%20based%20on%20deep%20neural%0Anetworks%20trained%20on%20large-scale%20labeled%20datasets.%20Acquiring%20such%20datasets%20would%0Aincur%20large%20annotation%20costs%2C%20especially%20for%20dense%20pixel-level%20prediction%20tasks%0Alike%20semantic%20segmentation.%20We%20consider%20region-based%20active%20learning%20as%20a%0Astrategy%20to%20reduce%20annotation%20costs%20while%20maintaining%20high%20performance.%20In%20this%0Asetting%2C%20batches%20of%20informative%20image%20regions%20instead%20of%20entire%20images%20are%0Aselected%20for%20labeling.%20Importantly%2C%20we%20propose%20that%20enforcing%20local%20spatial%0Adiversity%20is%20beneficial%20for%20active%20learning%20in%20this%20case%2C%20and%20to%20incorporate%0Aspatial%20diversity%20along%20with%20the%20traditional%20active%20selection%20criterion%2C%20e.g.%2C%0Adata%20sample%20uncertainty%2C%20in%20a%20unified%20optimization%20framework%20for%20region-based%0Aactive%20learning.%20We%20apply%20this%20framework%20to%20the%20Cityscapes%20and%20PASCAL%20VOC%0Adatasets%20and%20demonstrate%20that%20the%20inclusion%20of%20spatial%20diversity%20effectively%0Aimproves%20the%20performance%20of%20uncertainty-based%20and%20feature%20diversity-based%0Aactive%20learning%20methods.%20Our%20framework%20achieves%20%2495%5C%25%24%20performance%20of%20fully%0Asupervised%20methods%20with%20only%20%245-9%5C%25%24%20of%20the%20labeled%20pixels%2C%20outperforming%20all%0Astate-of-the-art%20region-based%20active%20learning%20methods%20for%20semantic%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17367v1&entry.124074799=Read"},
{"title": "Confidence Calibration in Vision-Language-Action Models", "author": "Thomas P Zollo and Richard Zemel", "abstract": "  Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2507.17383v1", "date": "2025-07-23", "relevancy": 2.1712, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Calibration%20in%20Vision-Language-Action%20Models&body=Title%3A%20Confidence%20Calibration%20in%20Vision-Language-Action%20Models%0AAuthor%3A%20Thomas%20P%20Zollo%20and%20Richard%20Zemel%0AAbstract%3A%20%20%20Trustworthy%20robot%20behavior%20requires%20not%20only%20high%20levels%20of%20task%20success%20but%0Aalso%20that%20the%20robot%20can%20reliably%20quantify%20how%20likely%20it%20is%20to%20succeed.%20To%20this%0Aend%2C%20we%20present%20the%20first%20systematic%20study%20of%20confidence%20calibration%20in%0Avision-language-action%20%28VLA%29%20foundation%20models%2C%20which%20map%20visual%20observations%0Aand%20natural-language%20instructions%20to%20low-level%20robot%20motor%20commands.%20We%20begin%0Awith%20extensive%20benchmarking%20to%20understand%20the%20critical%20relationship%20between%0Atask%20success%20and%20calibration%20error%20across%20multiple%20datasets%20and%20VLA%20variants%2C%0Afinding%20that%20task%20performance%20and%20calibration%20are%20not%20in%20tension.%20Next%2C%20we%0Aintroduce%20prompt%20ensembles%20for%20VLAs%2C%20a%20lightweight%2C%20Bayesian-inspired%20algorithm%0Athat%20averages%20confidence%20across%20paraphrased%20instructions%20and%20consistently%0Aimproves%20calibration.%20We%20further%20analyze%20calibration%20over%20the%20task%20time%0Ahorizon%2C%20showing%20that%20confidence%20is%20often%20most%20reliable%20after%20making%20some%0Aprogress%2C%20suggesting%20natural%20points%20for%20risk-aware%20intervention.%20Finally%2C%20we%0Areveal%20differential%20miscalibration%20across%20action%20dimensions%20and%20propose%0Aaction-wise%20Platt%20scaling%2C%20a%20method%20to%20recalibrate%20each%20action%20dimension%0Aindependently%20to%20produce%20better%20confidence%20estimates.%20Our%20aim%20in%20this%20study%20is%0Ato%20begin%20to%20develop%20the%20tools%20and%20conceptual%20understanding%20necessary%20to%20render%0AVLAs%20both%20highly%20performant%20and%20highly%20trustworthy%20via%20reliable%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Calibration%2520in%2520Vision-Language-Action%2520Models%26entry.906535625%3DThomas%2520P%2520Zollo%2520and%2520Richard%2520Zemel%26entry.1292438233%3D%2520%2520Trustworthy%2520robot%2520behavior%2520requires%2520not%2520only%2520high%2520levels%2520of%2520task%2520success%2520but%250Aalso%2520that%2520the%2520robot%2520can%2520reliably%2520quantify%2520how%2520likely%2520it%2520is%2520to%2520succeed.%2520To%2520this%250Aend%252C%2520we%2520present%2520the%2520first%2520systematic%2520study%2520of%2520confidence%2520calibration%2520in%250Avision-language-action%2520%2528VLA%2529%2520foundation%2520models%252C%2520which%2520map%2520visual%2520observations%250Aand%2520natural-language%2520instructions%2520to%2520low-level%2520robot%2520motor%2520commands.%2520We%2520begin%250Awith%2520extensive%2520benchmarking%2520to%2520understand%2520the%2520critical%2520relationship%2520between%250Atask%2520success%2520and%2520calibration%2520error%2520across%2520multiple%2520datasets%2520and%2520VLA%2520variants%252C%250Afinding%2520that%2520task%2520performance%2520and%2520calibration%2520are%2520not%2520in%2520tension.%2520Next%252C%2520we%250Aintroduce%2520prompt%2520ensembles%2520for%2520VLAs%252C%2520a%2520lightweight%252C%2520Bayesian-inspired%2520algorithm%250Athat%2520averages%2520confidence%2520across%2520paraphrased%2520instructions%2520and%2520consistently%250Aimproves%2520calibration.%2520We%2520further%2520analyze%2520calibration%2520over%2520the%2520task%2520time%250Ahorizon%252C%2520showing%2520that%2520confidence%2520is%2520often%2520most%2520reliable%2520after%2520making%2520some%250Aprogress%252C%2520suggesting%2520natural%2520points%2520for%2520risk-aware%2520intervention.%2520Finally%252C%2520we%250Areveal%2520differential%2520miscalibration%2520across%2520action%2520dimensions%2520and%2520propose%250Aaction-wise%2520Platt%2520scaling%252C%2520a%2520method%2520to%2520recalibrate%2520each%2520action%2520dimension%250Aindependently%2520to%2520produce%2520better%2520confidence%2520estimates.%2520Our%2520aim%2520in%2520this%2520study%2520is%250Ato%2520begin%2520to%2520develop%2520the%2520tools%2520and%2520conceptual%2520understanding%2520necessary%2520to%2520render%250AVLAs%2520both%2520highly%2520performant%2520and%2520highly%2520trustworthy%2520via%2520reliable%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Calibration%20in%20Vision-Language-Action%20Models&entry.906535625=Thomas%20P%20Zollo%20and%20Richard%20Zemel&entry.1292438233=%20%20Trustworthy%20robot%20behavior%20requires%20not%20only%20high%20levels%20of%20task%20success%20but%0Aalso%20that%20the%20robot%20can%20reliably%20quantify%20how%20likely%20it%20is%20to%20succeed.%20To%20this%0Aend%2C%20we%20present%20the%20first%20systematic%20study%20of%20confidence%20calibration%20in%0Avision-language-action%20%28VLA%29%20foundation%20models%2C%20which%20map%20visual%20observations%0Aand%20natural-language%20instructions%20to%20low-level%20robot%20motor%20commands.%20We%20begin%0Awith%20extensive%20benchmarking%20to%20understand%20the%20critical%20relationship%20between%0Atask%20success%20and%20calibration%20error%20across%20multiple%20datasets%20and%20VLA%20variants%2C%0Afinding%20that%20task%20performance%20and%20calibration%20are%20not%20in%20tension.%20Next%2C%20we%0Aintroduce%20prompt%20ensembles%20for%20VLAs%2C%20a%20lightweight%2C%20Bayesian-inspired%20algorithm%0Athat%20averages%20confidence%20across%20paraphrased%20instructions%20and%20consistently%0Aimproves%20calibration.%20We%20further%20analyze%20calibration%20over%20the%20task%20time%0Ahorizon%2C%20showing%20that%20confidence%20is%20often%20most%20reliable%20after%20making%20some%0Aprogress%2C%20suggesting%20natural%20points%20for%20risk-aware%20intervention.%20Finally%2C%20we%0Areveal%20differential%20miscalibration%20across%20action%20dimensions%20and%20propose%0Aaction-wise%20Platt%20scaling%2C%20a%20method%20to%20recalibrate%20each%20action%20dimension%0Aindependently%20to%20produce%20better%20confidence%20estimates.%20Our%20aim%20in%20this%20study%20is%0Ato%20begin%20to%20develop%20the%20tools%20and%20conceptual%20understanding%20necessary%20to%20render%0AVLAs%20both%20highly%20performant%20and%20highly%20trustworthy%20via%20reliable%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17383v1&entry.124074799=Read"},
{"title": "DFDNet: Dynamic Frequency-Guided De-Flare Network", "author": "Minglong Xue and Aoxiang Ning and Shivakumara Palaiahnakote and Mingliang Zhou", "abstract": "  Strong light sources in nighttime photography frequently produce flares in\nimages, significantly degrading visual quality and impacting the performance of\ndownstream tasks. While some progress has been made, existing methods continue\nto struggle with removing large-scale flare artifacts and repairing structural\ndamage in regions near the light source. We observe that these challenging\nflare artifacts exhibit more significant discrepancies from the reference\nimages in the frequency domain compared to the spatial domain. Therefore, this\npaper presents a novel dynamic frequency-guided deflare network (DFDNet) that\ndecouples content information from flare artifacts in the frequency domain,\neffectively removing large-scale flare artifacts. Specifically, DFDNet consists\nmainly of a global dynamic frequency-domain guidance (GDFG) module and a local\ndetail guidance module (LDGM). The GDFG module guides the network to perceive\nthe frequency characteristics of flare artifacts by dynamically optimizing\nglobal frequency domain features, effectively separating flare information from\ncontent information. Additionally, we design an LDGM via a contrastive learning\nstrategy that aligns the local features of the light source with the reference\nimage, reduces local detail damage from flare removal, and improves\nfine-grained image restoration. The experimental results demonstrate that the\nproposed method outperforms existing state-of-the-art methods in terms of\nperformance. The code is available at\n\\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.\n", "link": "http://arxiv.org/abs/2507.17489v1", "date": "2025-07-23", "relevancy": 2.1671, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5449}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFDNet%3A%20Dynamic%20Frequency-Guided%20De-Flare%20Network&body=Title%3A%20DFDNet%3A%20Dynamic%20Frequency-Guided%20De-Flare%20Network%0AAuthor%3A%20Minglong%20Xue%20and%20Aoxiang%20Ning%20and%20Shivakumara%20Palaiahnakote%20and%20Mingliang%20Zhou%0AAbstract%3A%20%20%20Strong%20light%20sources%20in%20nighttime%20photography%20frequently%20produce%20flares%20in%0Aimages%2C%20significantly%20degrading%20visual%20quality%20and%20impacting%20the%20performance%20of%0Adownstream%20tasks.%20While%20some%20progress%20has%20been%20made%2C%20existing%20methods%20continue%0Ato%20struggle%20with%20removing%20large-scale%20flare%20artifacts%20and%20repairing%20structural%0Adamage%20in%20regions%20near%20the%20light%20source.%20We%20observe%20that%20these%20challenging%0Aflare%20artifacts%20exhibit%20more%20significant%20discrepancies%20from%20the%20reference%0Aimages%20in%20the%20frequency%20domain%20compared%20to%20the%20spatial%20domain.%20Therefore%2C%20this%0Apaper%20presents%20a%20novel%20dynamic%20frequency-guided%20deflare%20network%20%28DFDNet%29%20that%0Adecouples%20content%20information%20from%20flare%20artifacts%20in%20the%20frequency%20domain%2C%0Aeffectively%20removing%20large-scale%20flare%20artifacts.%20Specifically%2C%20DFDNet%20consists%0Amainly%20of%20a%20global%20dynamic%20frequency-domain%20guidance%20%28GDFG%29%20module%20and%20a%20local%0Adetail%20guidance%20module%20%28LDGM%29.%20The%20GDFG%20module%20guides%20the%20network%20to%20perceive%0Athe%20frequency%20characteristics%20of%20flare%20artifacts%20by%20dynamically%20optimizing%0Aglobal%20frequency%20domain%20features%2C%20effectively%20separating%20flare%20information%20from%0Acontent%20information.%20Additionally%2C%20we%20design%20an%20LDGM%20via%20a%20contrastive%20learning%0Astrategy%20that%20aligns%20the%20local%20features%20of%20the%20light%20source%20with%20the%20reference%0Aimage%2C%20reduces%20local%20detail%20damage%20from%20flare%20removal%2C%20and%20improves%0Afine-grained%20image%20restoration.%20The%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%0Aperformance.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/AXNing/DFDNet%7D%7Bhttps%3A//github.com/AXNing/DFDNet%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFDNet%253A%2520Dynamic%2520Frequency-Guided%2520De-Flare%2520Network%26entry.906535625%3DMinglong%2520Xue%2520and%2520Aoxiang%2520Ning%2520and%2520Shivakumara%2520Palaiahnakote%2520and%2520Mingliang%2520Zhou%26entry.1292438233%3D%2520%2520Strong%2520light%2520sources%2520in%2520nighttime%2520photography%2520frequently%2520produce%2520flares%2520in%250Aimages%252C%2520significantly%2520degrading%2520visual%2520quality%2520and%2520impacting%2520the%2520performance%2520of%250Adownstream%2520tasks.%2520While%2520some%2520progress%2520has%2520been%2520made%252C%2520existing%2520methods%2520continue%250Ato%2520struggle%2520with%2520removing%2520large-scale%2520flare%2520artifacts%2520and%2520repairing%2520structural%250Adamage%2520in%2520regions%2520near%2520the%2520light%2520source.%2520We%2520observe%2520that%2520these%2520challenging%250Aflare%2520artifacts%2520exhibit%2520more%2520significant%2520discrepancies%2520from%2520the%2520reference%250Aimages%2520in%2520the%2520frequency%2520domain%2520compared%2520to%2520the%2520spatial%2520domain.%2520Therefore%252C%2520this%250Apaper%2520presents%2520a%2520novel%2520dynamic%2520frequency-guided%2520deflare%2520network%2520%2528DFDNet%2529%2520that%250Adecouples%2520content%2520information%2520from%2520flare%2520artifacts%2520in%2520the%2520frequency%2520domain%252C%250Aeffectively%2520removing%2520large-scale%2520flare%2520artifacts.%2520Specifically%252C%2520DFDNet%2520consists%250Amainly%2520of%2520a%2520global%2520dynamic%2520frequency-domain%2520guidance%2520%2528GDFG%2529%2520module%2520and%2520a%2520local%250Adetail%2520guidance%2520module%2520%2528LDGM%2529.%2520The%2520GDFG%2520module%2520guides%2520the%2520network%2520to%2520perceive%250Athe%2520frequency%2520characteristics%2520of%2520flare%2520artifacts%2520by%2520dynamically%2520optimizing%250Aglobal%2520frequency%2520domain%2520features%252C%2520effectively%2520separating%2520flare%2520information%2520from%250Acontent%2520information.%2520Additionally%252C%2520we%2520design%2520an%2520LDGM%2520via%2520a%2520contrastive%2520learning%250Astrategy%2520that%2520aligns%2520the%2520local%2520features%2520of%2520the%2520light%2520source%2520with%2520the%2520reference%250Aimage%252C%2520reduces%2520local%2520detail%2520damage%2520from%2520flare%2520removal%252C%2520and%2520improves%250Afine-grained%2520image%2520restoration.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%250Aperformance.%2520The%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/AXNing/DFDNet%257D%257Bhttps%253A//github.com/AXNing/DFDNet%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFDNet%3A%20Dynamic%20Frequency-Guided%20De-Flare%20Network&entry.906535625=Minglong%20Xue%20and%20Aoxiang%20Ning%20and%20Shivakumara%20Palaiahnakote%20and%20Mingliang%20Zhou&entry.1292438233=%20%20Strong%20light%20sources%20in%20nighttime%20photography%20frequently%20produce%20flares%20in%0Aimages%2C%20significantly%20degrading%20visual%20quality%20and%20impacting%20the%20performance%20of%0Adownstream%20tasks.%20While%20some%20progress%20has%20been%20made%2C%20existing%20methods%20continue%0Ato%20struggle%20with%20removing%20large-scale%20flare%20artifacts%20and%20repairing%20structural%0Adamage%20in%20regions%20near%20the%20light%20source.%20We%20observe%20that%20these%20challenging%0Aflare%20artifacts%20exhibit%20more%20significant%20discrepancies%20from%20the%20reference%0Aimages%20in%20the%20frequency%20domain%20compared%20to%20the%20spatial%20domain.%20Therefore%2C%20this%0Apaper%20presents%20a%20novel%20dynamic%20frequency-guided%20deflare%20network%20%28DFDNet%29%20that%0Adecouples%20content%20information%20from%20flare%20artifacts%20in%20the%20frequency%20domain%2C%0Aeffectively%20removing%20large-scale%20flare%20artifacts.%20Specifically%2C%20DFDNet%20consists%0Amainly%20of%20a%20global%20dynamic%20frequency-domain%20guidance%20%28GDFG%29%20module%20and%20a%20local%0Adetail%20guidance%20module%20%28LDGM%29.%20The%20GDFG%20module%20guides%20the%20network%20to%20perceive%0Athe%20frequency%20characteristics%20of%20flare%20artifacts%20by%20dynamically%20optimizing%0Aglobal%20frequency%20domain%20features%2C%20effectively%20separating%20flare%20information%20from%0Acontent%20information.%20Additionally%2C%20we%20design%20an%20LDGM%20via%20a%20contrastive%20learning%0Astrategy%20that%20aligns%20the%20local%20features%20of%20the%20light%20source%20with%20the%20reference%0Aimage%2C%20reduces%20local%20detail%20damage%20from%20flare%20removal%2C%20and%20improves%0Afine-grained%20image%20restoration.%20The%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%0Aperformance.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/AXNing/DFDNet%7D%7Bhttps%3A//github.com/AXNing/DFDNet%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17489v1&entry.124074799=Read"},
{"title": "STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic\n  Point Clouds", "author": "Tian Guo and Hui Yuan and Xiaolong Mao and Shiqi Jiang and Raouf Hamzaoui and Sam Kwong", "abstract": "  Very few studies have addressed quality enhancement for compressed dynamic\npoint clouds. In particular, the effective exploitation of spatial-temporal\ncorrelations between point cloud frames remains largely unexplored. Addressing\nthis gap, we propose a spatial-temporal attribute quality enhancement (STQE)\nnetwork that exploits both spatial and temporal correlations to improve the\nvisual quality of G-PCC compressed dynamic point clouds. Our contributions\ninclude a recoloring-based motion compensation module that remaps reference\nattribute information to the current frame geometry to achieve precise\ninter-frame geometric alignment, a channel-aware temporal attention module that\ndynamically highlights relevant regions across bidirectional reference frames,\na Gaussian-guided neighborhood feature aggregation module that efficiently\ncaptures spatial dependencies between geometry and color attributes, and a\njoint loss function based on the Pearson correlation coefficient, designed to\nalleviate over-smoothing effects typical of point-wise mean squared error\noptimization. When applied to the latest G-PCC test model, STQE achieved\nimprovements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with\nBj{\\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%\nfor the Luma, Cb, and Cr components, respectively.\n", "link": "http://arxiv.org/abs/2507.17522v1", "date": "2025-07-23", "relevancy": 2.1623, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5508}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5389}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STQE%3A%20Spatial-Temporal%20Quality%20Enhancement%20for%20G-PCC%20Compressed%20Dynamic%0A%20%20Point%20Clouds&body=Title%3A%20STQE%3A%20Spatial-Temporal%20Quality%20Enhancement%20for%20G-PCC%20Compressed%20Dynamic%0A%20%20Point%20Clouds%0AAuthor%3A%20Tian%20Guo%20and%20Hui%20Yuan%20and%20Xiaolong%20Mao%20and%20Shiqi%20Jiang%20and%20Raouf%20Hamzaoui%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Very%20few%20studies%20have%20addressed%20quality%20enhancement%20for%20compressed%20dynamic%0Apoint%20clouds.%20In%20particular%2C%20the%20effective%20exploitation%20of%20spatial-temporal%0Acorrelations%20between%20point%20cloud%20frames%20remains%20largely%20unexplored.%20Addressing%0Athis%20gap%2C%20we%20propose%20a%20spatial-temporal%20attribute%20quality%20enhancement%20%28STQE%29%0Anetwork%20that%20exploits%20both%20spatial%20and%20temporal%20correlations%20to%20improve%20the%0Avisual%20quality%20of%20G-PCC%20compressed%20dynamic%20point%20clouds.%20Our%20contributions%0Ainclude%20a%20recoloring-based%20motion%20compensation%20module%20that%20remaps%20reference%0Aattribute%20information%20to%20the%20current%20frame%20geometry%20to%20achieve%20precise%0Ainter-frame%20geometric%20alignment%2C%20a%20channel-aware%20temporal%20attention%20module%20that%0Adynamically%20highlights%20relevant%20regions%20across%20bidirectional%20reference%20frames%2C%0Aa%20Gaussian-guided%20neighborhood%20feature%20aggregation%20module%20that%20efficiently%0Acaptures%20spatial%20dependencies%20between%20geometry%20and%20color%20attributes%2C%20and%20a%0Ajoint%20loss%20function%20based%20on%20the%20Pearson%20correlation%20coefficient%2C%20designed%20to%0Aalleviate%20over-smoothing%20effects%20typical%20of%20point-wise%20mean%20squared%20error%0Aoptimization.%20When%20applied%20to%20the%20latest%20G-PCC%20test%20model%2C%20STQE%20achieved%0Aimprovements%20of%200.855%20dB%2C%200.682%20dB%2C%20and%200.828%20dB%20in%20delta%20PSNR%2C%20with%0ABj%7B%5Co%7Dntegaard%20Delta%20rate%20%28BD-rate%29%20reductions%20of%20-25.2%25%2C%20-31.6%25%2C%20and%20-32.5%25%0Afor%20the%20Luma%2C%20Cb%2C%20and%20Cr%20components%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTQE%253A%2520Spatial-Temporal%2520Quality%2520Enhancement%2520for%2520G-PCC%2520Compressed%2520Dynamic%250A%2520%2520Point%2520Clouds%26entry.906535625%3DTian%2520Guo%2520and%2520Hui%2520Yuan%2520and%2520Xiaolong%2520Mao%2520and%2520Shiqi%2520Jiang%2520and%2520Raouf%2520Hamzaoui%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Very%2520few%2520studies%2520have%2520addressed%2520quality%2520enhancement%2520for%2520compressed%2520dynamic%250Apoint%2520clouds.%2520In%2520particular%252C%2520the%2520effective%2520exploitation%2520of%2520spatial-temporal%250Acorrelations%2520between%2520point%2520cloud%2520frames%2520remains%2520largely%2520unexplored.%2520Addressing%250Athis%2520gap%252C%2520we%2520propose%2520a%2520spatial-temporal%2520attribute%2520quality%2520enhancement%2520%2528STQE%2529%250Anetwork%2520that%2520exploits%2520both%2520spatial%2520and%2520temporal%2520correlations%2520to%2520improve%2520the%250Avisual%2520quality%2520of%2520G-PCC%2520compressed%2520dynamic%2520point%2520clouds.%2520Our%2520contributions%250Ainclude%2520a%2520recoloring-based%2520motion%2520compensation%2520module%2520that%2520remaps%2520reference%250Aattribute%2520information%2520to%2520the%2520current%2520frame%2520geometry%2520to%2520achieve%2520precise%250Ainter-frame%2520geometric%2520alignment%252C%2520a%2520channel-aware%2520temporal%2520attention%2520module%2520that%250Adynamically%2520highlights%2520relevant%2520regions%2520across%2520bidirectional%2520reference%2520frames%252C%250Aa%2520Gaussian-guided%2520neighborhood%2520feature%2520aggregation%2520module%2520that%2520efficiently%250Acaptures%2520spatial%2520dependencies%2520between%2520geometry%2520and%2520color%2520attributes%252C%2520and%2520a%250Ajoint%2520loss%2520function%2520based%2520on%2520the%2520Pearson%2520correlation%2520coefficient%252C%2520designed%2520to%250Aalleviate%2520over-smoothing%2520effects%2520typical%2520of%2520point-wise%2520mean%2520squared%2520error%250Aoptimization.%2520When%2520applied%2520to%2520the%2520latest%2520G-PCC%2520test%2520model%252C%2520STQE%2520achieved%250Aimprovements%2520of%25200.855%2520dB%252C%25200.682%2520dB%252C%2520and%25200.828%2520dB%2520in%2520delta%2520PSNR%252C%2520with%250ABj%257B%255Co%257Dntegaard%2520Delta%2520rate%2520%2528BD-rate%2529%2520reductions%2520of%2520-25.2%2525%252C%2520-31.6%2525%252C%2520and%2520-32.5%2525%250Afor%2520the%2520Luma%252C%2520Cb%252C%2520and%2520Cr%2520components%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STQE%3A%20Spatial-Temporal%20Quality%20Enhancement%20for%20G-PCC%20Compressed%20Dynamic%0A%20%20Point%20Clouds&entry.906535625=Tian%20Guo%20and%20Hui%20Yuan%20and%20Xiaolong%20Mao%20and%20Shiqi%20Jiang%20and%20Raouf%20Hamzaoui%20and%20Sam%20Kwong&entry.1292438233=%20%20Very%20few%20studies%20have%20addressed%20quality%20enhancement%20for%20compressed%20dynamic%0Apoint%20clouds.%20In%20particular%2C%20the%20effective%20exploitation%20of%20spatial-temporal%0Acorrelations%20between%20point%20cloud%20frames%20remains%20largely%20unexplored.%20Addressing%0Athis%20gap%2C%20we%20propose%20a%20spatial-temporal%20attribute%20quality%20enhancement%20%28STQE%29%0Anetwork%20that%20exploits%20both%20spatial%20and%20temporal%20correlations%20to%20improve%20the%0Avisual%20quality%20of%20G-PCC%20compressed%20dynamic%20point%20clouds.%20Our%20contributions%0Ainclude%20a%20recoloring-based%20motion%20compensation%20module%20that%20remaps%20reference%0Aattribute%20information%20to%20the%20current%20frame%20geometry%20to%20achieve%20precise%0Ainter-frame%20geometric%20alignment%2C%20a%20channel-aware%20temporal%20attention%20module%20that%0Adynamically%20highlights%20relevant%20regions%20across%20bidirectional%20reference%20frames%2C%0Aa%20Gaussian-guided%20neighborhood%20feature%20aggregation%20module%20that%20efficiently%0Acaptures%20spatial%20dependencies%20between%20geometry%20and%20color%20attributes%2C%20and%20a%0Ajoint%20loss%20function%20based%20on%20the%20Pearson%20correlation%20coefficient%2C%20designed%20to%0Aalleviate%20over-smoothing%20effects%20typical%20of%20point-wise%20mean%20squared%20error%0Aoptimization.%20When%20applied%20to%20the%20latest%20G-PCC%20test%20model%2C%20STQE%20achieved%0Aimprovements%20of%200.855%20dB%2C%200.682%20dB%2C%20and%200.828%20dB%20in%20delta%20PSNR%2C%20with%0ABj%7B%5Co%7Dntegaard%20Delta%20rate%20%28BD-rate%29%20reductions%20of%20-25.2%25%2C%20-31.6%25%2C%20and%20-32.5%25%0Afor%20the%20Luma%2C%20Cb%2C%20and%20Cr%20components%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17522v1&entry.124074799=Read"},
{"title": "C3RL: Rethinking the Combination of Channel-independence and\n  Channel-mixing from Representation Learning", "author": "Shusen Ma and Yun-Bo Zhao and Yu Kang", "abstract": "  Multivariate time series forecasting has drawn increasing attention due to\nits practical importance. Existing approaches typically adopt either\nchannel-mixing (CM) or channel-independence (CI) strategies. CM strategy can\ncapture inter-variable dependencies but fails to discern variable-specific\ntemporal patterns. CI strategy improves this aspect but fails to fully exploit\ncross-variable dependencies like CM. Hybrid strategies based on feature fusion\noffer limited generalization and interpretability. To address these issues, we\npropose C3RL, a novel representation learning framework that jointly models\nboth CM and CI strategies. Motivated by contrastive learning in computer\nvision, C3RL treats the inputs of the two strategies as transposed views and\nbuilds a siamese network architecture: one strategy serves as the backbone,\nwhile the other complements it. By jointly optimizing contrastive and\nprediction losses with adaptive weighting, C3RL balances representation and\nforecasting performance. Extensive experiments on seven models show that C3RL\nboosts the best-case performance rate to 81.4\\% for models based on CI strategy\nand to 76.3\\% for models based on CM strategy, demonstrating strong\ngeneralization and effectiveness. The code will be available once the paper is\naccepted.\n", "link": "http://arxiv.org/abs/2507.17454v1", "date": "2025-07-23", "relevancy": 2.1584, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%0A%20%20Channel-mixing%20from%20Representation%20Learning&body=Title%3A%20C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%0A%20%20Channel-mixing%20from%20Representation%20Learning%0AAuthor%3A%20Shusen%20Ma%20and%20Yun-Bo%20Zhao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20Multivariate%20time%20series%20forecasting%20has%20drawn%20increasing%20attention%20due%20to%0Aits%20practical%20importance.%20Existing%20approaches%20typically%20adopt%20either%0Achannel-mixing%20%28CM%29%20or%20channel-independence%20%28CI%29%20strategies.%20CM%20strategy%20can%0Acapture%20inter-variable%20dependencies%20but%20fails%20to%20discern%20variable-specific%0Atemporal%20patterns.%20CI%20strategy%20improves%20this%20aspect%20but%20fails%20to%20fully%20exploit%0Across-variable%20dependencies%20like%20CM.%20Hybrid%20strategies%20based%20on%20feature%20fusion%0Aoffer%20limited%20generalization%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%0Apropose%20C3RL%2C%20a%20novel%20representation%20learning%20framework%20that%20jointly%20models%0Aboth%20CM%20and%20CI%20strategies.%20Motivated%20by%20contrastive%20learning%20in%20computer%0Avision%2C%20C3RL%20treats%20the%20inputs%20of%20the%20two%20strategies%20as%20transposed%20views%20and%0Abuilds%20a%20siamese%20network%20architecture%3A%20one%20strategy%20serves%20as%20the%20backbone%2C%0Awhile%20the%20other%20complements%20it.%20By%20jointly%20optimizing%20contrastive%20and%0Aprediction%20losses%20with%20adaptive%20weighting%2C%20C3RL%20balances%20representation%20and%0Aforecasting%20performance.%20Extensive%20experiments%20on%20seven%20models%20show%20that%20C3RL%0Aboosts%20the%20best-case%20performance%20rate%20to%2081.4%5C%25%20for%20models%20based%20on%20CI%20strategy%0Aand%20to%2076.3%5C%25%20for%20models%20based%20on%20CM%20strategy%2C%20demonstrating%20strong%0Ageneralization%20and%20effectiveness.%20The%20code%20will%20be%20available%20once%20the%20paper%20is%0Aaccepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3RL%253A%2520Rethinking%2520the%2520Combination%2520of%2520Channel-independence%2520and%250A%2520%2520Channel-mixing%2520from%2520Representation%2520Learning%26entry.906535625%3DShusen%2520Ma%2520and%2520Yun-Bo%2520Zhao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520forecasting%2520has%2520drawn%2520increasing%2520attention%2520due%2520to%250Aits%2520practical%2520importance.%2520Existing%2520approaches%2520typically%2520adopt%2520either%250Achannel-mixing%2520%2528CM%2529%2520or%2520channel-independence%2520%2528CI%2529%2520strategies.%2520CM%2520strategy%2520can%250Acapture%2520inter-variable%2520dependencies%2520but%2520fails%2520to%2520discern%2520variable-specific%250Atemporal%2520patterns.%2520CI%2520strategy%2520improves%2520this%2520aspect%2520but%2520fails%2520to%2520fully%2520exploit%250Across-variable%2520dependencies%2520like%2520CM.%2520Hybrid%2520strategies%2520based%2520on%2520feature%2520fusion%250Aoffer%2520limited%2520generalization%2520and%2520interpretability.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520C3RL%252C%2520a%2520novel%2520representation%2520learning%2520framework%2520that%2520jointly%2520models%250Aboth%2520CM%2520and%2520CI%2520strategies.%2520Motivated%2520by%2520contrastive%2520learning%2520in%2520computer%250Avision%252C%2520C3RL%2520treats%2520the%2520inputs%2520of%2520the%2520two%2520strategies%2520as%2520transposed%2520views%2520and%250Abuilds%2520a%2520siamese%2520network%2520architecture%253A%2520one%2520strategy%2520serves%2520as%2520the%2520backbone%252C%250Awhile%2520the%2520other%2520complements%2520it.%2520By%2520jointly%2520optimizing%2520contrastive%2520and%250Aprediction%2520losses%2520with%2520adaptive%2520weighting%252C%2520C3RL%2520balances%2520representation%2520and%250Aforecasting%2520performance.%2520Extensive%2520experiments%2520on%2520seven%2520models%2520show%2520that%2520C3RL%250Aboosts%2520the%2520best-case%2520performance%2520rate%2520to%252081.4%255C%2525%2520for%2520models%2520based%2520on%2520CI%2520strategy%250Aand%2520to%252076.3%255C%2525%2520for%2520models%2520based%2520on%2520CM%2520strategy%252C%2520demonstrating%2520strong%250Ageneralization%2520and%2520effectiveness.%2520The%2520code%2520will%2520be%2520available%2520once%2520the%2520paper%2520is%250Aaccepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%0A%20%20Channel-mixing%20from%20Representation%20Learning&entry.906535625=Shusen%20Ma%20and%20Yun-Bo%20Zhao%20and%20Yu%20Kang&entry.1292438233=%20%20Multivariate%20time%20series%20forecasting%20has%20drawn%20increasing%20attention%20due%20to%0Aits%20practical%20importance.%20Existing%20approaches%20typically%20adopt%20either%0Achannel-mixing%20%28CM%29%20or%20channel-independence%20%28CI%29%20strategies.%20CM%20strategy%20can%0Acapture%20inter-variable%20dependencies%20but%20fails%20to%20discern%20variable-specific%0Atemporal%20patterns.%20CI%20strategy%20improves%20this%20aspect%20but%20fails%20to%20fully%20exploit%0Across-variable%20dependencies%20like%20CM.%20Hybrid%20strategies%20based%20on%20feature%20fusion%0Aoffer%20limited%20generalization%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%0Apropose%20C3RL%2C%20a%20novel%20representation%20learning%20framework%20that%20jointly%20models%0Aboth%20CM%20and%20CI%20strategies.%20Motivated%20by%20contrastive%20learning%20in%20computer%0Avision%2C%20C3RL%20treats%20the%20inputs%20of%20the%20two%20strategies%20as%20transposed%20views%20and%0Abuilds%20a%20siamese%20network%20architecture%3A%20one%20strategy%20serves%20as%20the%20backbone%2C%0Awhile%20the%20other%20complements%20it.%20By%20jointly%20optimizing%20contrastive%20and%0Aprediction%20losses%20with%20adaptive%20weighting%2C%20C3RL%20balances%20representation%20and%0Aforecasting%20performance.%20Extensive%20experiments%20on%20seven%20models%20show%20that%20C3RL%0Aboosts%20the%20best-case%20performance%20rate%20to%2081.4%5C%25%20for%20models%20based%20on%20CI%20strategy%0Aand%20to%2076.3%5C%25%20for%20models%20based%20on%20CM%20strategy%2C%20demonstrating%20strong%0Ageneralization%20and%20effectiveness.%20The%20code%20will%20be%20available%20once%20the%20paper%20is%0Aaccepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17454v1&entry.124074799=Read"},
{"title": "A Deep Learning Approach for Augmenting Perceptional Understanding of\n  Histopathology Images", "author": "Xiaoqian Hu", "abstract": "  In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.\n", "link": "http://arxiv.org/abs/2503.06894v3", "date": "2025-07-23", "relevancy": 2.1564, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Approach%20for%20Augmenting%20Perceptional%20Understanding%20of%0A%20%20Histopathology%20Images&body=Title%3A%20A%20Deep%20Learning%20Approach%20for%20Augmenting%20Perceptional%20Understanding%20of%0A%20%20Histopathology%20Images%0AAuthor%3A%20Xiaoqian%20Hu%0AAbstract%3A%20%20%20In%20Recent%20Years%2C%20Digital%20Technologies%20Have%20Made%20Significant%20Strides%20In%0AAugmenting-Human-Health%2C%20Cognition%2C%20And%20Perception%2C%20Particularly%20Within%20The%0AField%20Of%20Computational-Pathology.%20This%20Paper%20Presents%20A%20Novel%20Approach%20To%0AEnhancing%20The%20Analysis%20Of%20Histopathology%20Images%20By%20Leveraging%20A%0AMult-modal-Model%20That%20Combines%20Vision%20Transformers%20%28Vit%29%20With%20Gpt-2%20For%20Image%0ACaptioning.%20The%20Model%20Is%20Fine-Tuned%20On%20The%20Specialized%20Arch-Dataset%2C%20Which%0AIncludes%20Dense%20Image%20Captions%20Derived%20From%20Clinical%20And%20Academic%20Resources%2C%20To%0ACapture%20The%20Complexities%20Of%20Pathology%20Images%20Such%20As%20Tissue%20Morphologies%2C%0AStaining%20Variations%2C%20And%20Pathological%20Conditions.%20By%20Generating%20Accurate%2C%0AContextually%20Captions%2C%20The%20Model%20Augments%20The%20Cognitive%20Capabilities%20Of%0AHealthcare%20Professionals%2C%20Enabling%20More%20Efficient%20Disease%20Classification%2C%0ASegmentation%2C%20And%20Detection.%20The%20Model%20Enhances%20The%20Perception%20Of%20Subtle%0APathological%20Features%20In%20Images%20That%20Might%20Otherwise%20Go%20Unnoticed%2C%20Thereby%0AImproving%20Diagnostic%20Accuracy.%20Our%20Approach%20Demonstrates%20The%20Potential%20For%0ADigital%20Technologies%20To%20Augment%20Human%20Cognitive%20Abilities%20In%20Medical%20Image%0AAnalysis%2C%20Providing%20Steps%20Toward%20More%20Personalized%20And%20Accurate%20Healthcare%0AOutcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06894v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Approach%2520for%2520Augmenting%2520Perceptional%2520Understanding%2520of%250A%2520%2520Histopathology%2520Images%26entry.906535625%3DXiaoqian%2520Hu%26entry.1292438233%3D%2520%2520In%2520Recent%2520Years%252C%2520Digital%2520Technologies%2520Have%2520Made%2520Significant%2520Strides%2520In%250AAugmenting-Human-Health%252C%2520Cognition%252C%2520And%2520Perception%252C%2520Particularly%2520Within%2520The%250AField%2520Of%2520Computational-Pathology.%2520This%2520Paper%2520Presents%2520A%2520Novel%2520Approach%2520To%250AEnhancing%2520The%2520Analysis%2520Of%2520Histopathology%2520Images%2520By%2520Leveraging%2520A%250AMult-modal-Model%2520That%2520Combines%2520Vision%2520Transformers%2520%2528Vit%2529%2520With%2520Gpt-2%2520For%2520Image%250ACaptioning.%2520The%2520Model%2520Is%2520Fine-Tuned%2520On%2520The%2520Specialized%2520Arch-Dataset%252C%2520Which%250AIncludes%2520Dense%2520Image%2520Captions%2520Derived%2520From%2520Clinical%2520And%2520Academic%2520Resources%252C%2520To%250ACapture%2520The%2520Complexities%2520Of%2520Pathology%2520Images%2520Such%2520As%2520Tissue%2520Morphologies%252C%250AStaining%2520Variations%252C%2520And%2520Pathological%2520Conditions.%2520By%2520Generating%2520Accurate%252C%250AContextually%2520Captions%252C%2520The%2520Model%2520Augments%2520The%2520Cognitive%2520Capabilities%2520Of%250AHealthcare%2520Professionals%252C%2520Enabling%2520More%2520Efficient%2520Disease%2520Classification%252C%250ASegmentation%252C%2520And%2520Detection.%2520The%2520Model%2520Enhances%2520The%2520Perception%2520Of%2520Subtle%250APathological%2520Features%2520In%2520Images%2520That%2520Might%2520Otherwise%2520Go%2520Unnoticed%252C%2520Thereby%250AImproving%2520Diagnostic%2520Accuracy.%2520Our%2520Approach%2520Demonstrates%2520The%2520Potential%2520For%250ADigital%2520Technologies%2520To%2520Augment%2520Human%2520Cognitive%2520Abilities%2520In%2520Medical%2520Image%250AAnalysis%252C%2520Providing%2520Steps%2520Toward%2520More%2520Personalized%2520And%2520Accurate%2520Healthcare%250AOutcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06894v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Approach%20for%20Augmenting%20Perceptional%20Understanding%20of%0A%20%20Histopathology%20Images&entry.906535625=Xiaoqian%20Hu&entry.1292438233=%20%20In%20Recent%20Years%2C%20Digital%20Technologies%20Have%20Made%20Significant%20Strides%20In%0AAugmenting-Human-Health%2C%20Cognition%2C%20And%20Perception%2C%20Particularly%20Within%20The%0AField%20Of%20Computational-Pathology.%20This%20Paper%20Presents%20A%20Novel%20Approach%20To%0AEnhancing%20The%20Analysis%20Of%20Histopathology%20Images%20By%20Leveraging%20A%0AMult-modal-Model%20That%20Combines%20Vision%20Transformers%20%28Vit%29%20With%20Gpt-2%20For%20Image%0ACaptioning.%20The%20Model%20Is%20Fine-Tuned%20On%20The%20Specialized%20Arch-Dataset%2C%20Which%0AIncludes%20Dense%20Image%20Captions%20Derived%20From%20Clinical%20And%20Academic%20Resources%2C%20To%0ACapture%20The%20Complexities%20Of%20Pathology%20Images%20Such%20As%20Tissue%20Morphologies%2C%0AStaining%20Variations%2C%20And%20Pathological%20Conditions.%20By%20Generating%20Accurate%2C%0AContextually%20Captions%2C%20The%20Model%20Augments%20The%20Cognitive%20Capabilities%20Of%0AHealthcare%20Professionals%2C%20Enabling%20More%20Efficient%20Disease%20Classification%2C%0ASegmentation%2C%20And%20Detection.%20The%20Model%20Enhances%20The%20Perception%20Of%20Subtle%0APathological%20Features%20In%20Images%20That%20Might%20Otherwise%20Go%20Unnoticed%2C%20Thereby%0AImproving%20Diagnostic%20Accuracy.%20Our%20Approach%20Demonstrates%20The%20Potential%20For%0ADigital%20Technologies%20To%20Augment%20Human%20Cognitive%20Abilities%20In%20Medical%20Image%0AAnalysis%2C%20Providing%20Steps%20Toward%20More%20Personalized%20And%20Accurate%20Healthcare%0AOutcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06894v3&entry.124074799=Read"},
{"title": "Attention (as Discrete-Time Markov) Chains", "author": "Yotam Erel and Olaf D\u00fcnkel and Rishabh Dabral and Vladislav Golyanik and Christian Theobalt and Amit H. Bermano", "abstract": "  We introduce a new interpretation of the attention matrix as a discrete-time\nMarkov chain. Our interpretation sheds light on common operations involving\nattention scores such as selection, summation, and averaging in a unified\nframework. It further extends them by considering indirect attention,\npropagated through the Markov chain, as opposed to previous studies that only\nmodel immediate effects. Our main observation is that tokens corresponding to\nsemantically similar regions form a set of metastable states, where the\nattention clusters, while noisy attention scores tend to disperse. Metastable\nstates and their prevalence can be easily computed through simple matrix\nmultiplication and eigenanalysis, respectively. Using these lightweight tools,\nwe demonstrate state-of-the-art zero-shot segmentation. Lastly, we define\nTokenRank -- the steady state vector of the Markov chain, which measures global\ntoken importance. We demonstrate that using it brings improvements in\nunconditional image generation. We believe our framework offers a fresh view of\nhow tokens are being attended in modern visual transformers.\n", "link": "http://arxiv.org/abs/2507.17657v1", "date": "2025-07-23", "relevancy": 2.1482, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6098}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4957}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20%28as%20Discrete-Time%20Markov%29%20Chains&body=Title%3A%20Attention%20%28as%20Discrete-Time%20Markov%29%20Chains%0AAuthor%3A%20Yotam%20Erel%20and%20Olaf%20D%C3%BCnkel%20and%20Rishabh%20Dabral%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%20and%20Amit%20H.%20Bermano%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20interpretation%20of%20the%20attention%20matrix%20as%20a%20discrete-time%0AMarkov%20chain.%20Our%20interpretation%20sheds%20light%20on%20common%20operations%20involving%0Aattention%20scores%20such%20as%20selection%2C%20summation%2C%20and%20averaging%20in%20a%20unified%0Aframework.%20It%20further%20extends%20them%20by%20considering%20indirect%20attention%2C%0Apropagated%20through%20the%20Markov%20chain%2C%20as%20opposed%20to%20previous%20studies%20that%20only%0Amodel%20immediate%20effects.%20Our%20main%20observation%20is%20that%20tokens%20corresponding%20to%0Asemantically%20similar%20regions%20form%20a%20set%20of%20metastable%20states%2C%20where%20the%0Aattention%20clusters%2C%20while%20noisy%20attention%20scores%20tend%20to%20disperse.%20Metastable%0Astates%20and%20their%20prevalence%20can%20be%20easily%20computed%20through%20simple%20matrix%0Amultiplication%20and%20eigenanalysis%2C%20respectively.%20Using%20these%20lightweight%20tools%2C%0Awe%20demonstrate%20state-of-the-art%20zero-shot%20segmentation.%20Lastly%2C%20we%20define%0ATokenRank%20--%20the%20steady%20state%20vector%20of%20the%20Markov%20chain%2C%20which%20measures%20global%0Atoken%20importance.%20We%20demonstrate%20that%20using%20it%20brings%20improvements%20in%0Aunconditional%20image%20generation.%20We%20believe%20our%20framework%20offers%20a%20fresh%20view%20of%0Ahow%20tokens%20are%20being%20attended%20in%20modern%20visual%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520%2528as%2520Discrete-Time%2520Markov%2529%2520Chains%26entry.906535625%3DYotam%2520Erel%2520and%2520Olaf%2520D%25C3%25BCnkel%2520and%2520Rishabh%2520Dabral%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%2520and%2520Amit%2520H.%2520Bermano%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520interpretation%2520of%2520the%2520attention%2520matrix%2520as%2520a%2520discrete-time%250AMarkov%2520chain.%2520Our%2520interpretation%2520sheds%2520light%2520on%2520common%2520operations%2520involving%250Aattention%2520scores%2520such%2520as%2520selection%252C%2520summation%252C%2520and%2520averaging%2520in%2520a%2520unified%250Aframework.%2520It%2520further%2520extends%2520them%2520by%2520considering%2520indirect%2520attention%252C%250Apropagated%2520through%2520the%2520Markov%2520chain%252C%2520as%2520opposed%2520to%2520previous%2520studies%2520that%2520only%250Amodel%2520immediate%2520effects.%2520Our%2520main%2520observation%2520is%2520that%2520tokens%2520corresponding%2520to%250Asemantically%2520similar%2520regions%2520form%2520a%2520set%2520of%2520metastable%2520states%252C%2520where%2520the%250Aattention%2520clusters%252C%2520while%2520noisy%2520attention%2520scores%2520tend%2520to%2520disperse.%2520Metastable%250Astates%2520and%2520their%2520prevalence%2520can%2520be%2520easily%2520computed%2520through%2520simple%2520matrix%250Amultiplication%2520and%2520eigenanalysis%252C%2520respectively.%2520Using%2520these%2520lightweight%2520tools%252C%250Awe%2520demonstrate%2520state-of-the-art%2520zero-shot%2520segmentation.%2520Lastly%252C%2520we%2520define%250ATokenRank%2520--%2520the%2520steady%2520state%2520vector%2520of%2520the%2520Markov%2520chain%252C%2520which%2520measures%2520global%250Atoken%2520importance.%2520We%2520demonstrate%2520that%2520using%2520it%2520brings%2520improvements%2520in%250Aunconditional%2520image%2520generation.%2520We%2520believe%2520our%2520framework%2520offers%2520a%2520fresh%2520view%2520of%250Ahow%2520tokens%2520are%2520being%2520attended%2520in%2520modern%2520visual%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20%28as%20Discrete-Time%20Markov%29%20Chains&entry.906535625=Yotam%20Erel%20and%20Olaf%20D%C3%BCnkel%20and%20Rishabh%20Dabral%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%20and%20Amit%20H.%20Bermano&entry.1292438233=%20%20We%20introduce%20a%20new%20interpretation%20of%20the%20attention%20matrix%20as%20a%20discrete-time%0AMarkov%20chain.%20Our%20interpretation%20sheds%20light%20on%20common%20operations%20involving%0Aattention%20scores%20such%20as%20selection%2C%20summation%2C%20and%20averaging%20in%20a%20unified%0Aframework.%20It%20further%20extends%20them%20by%20considering%20indirect%20attention%2C%0Apropagated%20through%20the%20Markov%20chain%2C%20as%20opposed%20to%20previous%20studies%20that%20only%0Amodel%20immediate%20effects.%20Our%20main%20observation%20is%20that%20tokens%20corresponding%20to%0Asemantically%20similar%20regions%20form%20a%20set%20of%20metastable%20states%2C%20where%20the%0Aattention%20clusters%2C%20while%20noisy%20attention%20scores%20tend%20to%20disperse.%20Metastable%0Astates%20and%20their%20prevalence%20can%20be%20easily%20computed%20through%20simple%20matrix%0Amultiplication%20and%20eigenanalysis%2C%20respectively.%20Using%20these%20lightweight%20tools%2C%0Awe%20demonstrate%20state-of-the-art%20zero-shot%20segmentation.%20Lastly%2C%20we%20define%0ATokenRank%20--%20the%20steady%20state%20vector%20of%20the%20Markov%20chain%2C%20which%20measures%20global%0Atoken%20importance.%20We%20demonstrate%20that%20using%20it%20brings%20improvements%20in%0Aunconditional%20image%20generation.%20We%20believe%20our%20framework%20offers%20a%20fresh%20view%20of%0Ahow%20tokens%20are%20being%20attended%20in%20modern%20visual%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17657v1&entry.124074799=Read"},
{"title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision\n  Assessment on Semi-Structured Profiles", "author": "Junhua Liu and Roy Ka-Wei Lee and Kwan Hui Lim", "abstract": "  Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.\n", "link": "http://arxiv.org/abs/2507.17472v1", "date": "2025-07-23", "relevancy": 2.1404, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BGM-HAN%3A%20A%20Hierarchical%20Attention%20Network%20for%20Accurate%20and%20Fair%20Decision%0A%20%20Assessment%20on%20Semi-Structured%20Profiles&body=Title%3A%20BGM-HAN%3A%20A%20Hierarchical%20Attention%20Network%20for%20Accurate%20and%20Fair%20Decision%0A%20%20Assessment%20on%20Semi-Structured%20Profiles%0AAuthor%3A%20Junhua%20Liu%20and%20Roy%20Ka-Wei%20Lee%20and%20Kwan%20Hui%20Lim%0AAbstract%3A%20%20%20Human%20decision-making%20in%20high-stakes%20domains%20often%20relies%20on%20expertise%20and%0Aheuristics%2C%20but%20is%20vulnerable%20to%20hard-to-detect%20cognitive%20biases%20that%20threaten%0Afairness%20and%20long-term%20outcomes.%20This%20work%20presents%20a%20novel%20approach%20to%0Aenhancing%20complex%20decision-making%20workflows%20through%20the%20integration%20of%0Ahierarchical%20learning%20alongside%20various%20enhancements.%20Focusing%20on%20university%0Aadmissions%20as%20a%20representative%20high-stakes%20domain%2C%20we%20propose%20BGM-HAN%2C%20an%0Aenhanced%20Byte-Pair%20Encoded%2C%20Gated%20Multi-head%20Hierarchical%20Attention%20Network%2C%0Adesigned%20to%20effectively%20model%20semi-structured%20applicant%20data.%20BGM-HAN%20captures%0Amulti-level%20representations%20that%20are%20crucial%20for%20nuanced%20assessment%2C%20improving%0Aboth%20interpretability%20and%20predictive%20performance.%20Experimental%20results%20on%20real%0Aadmissions%20data%20demonstrate%20that%20our%20proposed%20model%20significantly%20outperforms%0Aboth%20state-of-the-art%20baselines%20from%20traditional%20machine%20learning%20to%20large%0Alanguage%20models%2C%20offering%20a%20promising%20framework%20for%20augmenting%20decision-making%0Ain%20domains%20where%20structure%2C%20context%2C%20and%20fairness%20matter.%20Source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/junhua/bgm-han.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBGM-HAN%253A%2520A%2520Hierarchical%2520Attention%2520Network%2520for%2520Accurate%2520and%2520Fair%2520Decision%250A%2520%2520Assessment%2520on%2520Semi-Structured%2520Profiles%26entry.906535625%3DJunhua%2520Liu%2520and%2520Roy%2520Ka-Wei%2520Lee%2520and%2520Kwan%2520Hui%2520Lim%26entry.1292438233%3D%2520%2520Human%2520decision-making%2520in%2520high-stakes%2520domains%2520often%2520relies%2520on%2520expertise%2520and%250Aheuristics%252C%2520but%2520is%2520vulnerable%2520to%2520hard-to-detect%2520cognitive%2520biases%2520that%2520threaten%250Afairness%2520and%2520long-term%2520outcomes.%2520This%2520work%2520presents%2520a%2520novel%2520approach%2520to%250Aenhancing%2520complex%2520decision-making%2520workflows%2520through%2520the%2520integration%2520of%250Ahierarchical%2520learning%2520alongside%2520various%2520enhancements.%2520Focusing%2520on%2520university%250Aadmissions%2520as%2520a%2520representative%2520high-stakes%2520domain%252C%2520we%2520propose%2520BGM-HAN%252C%2520an%250Aenhanced%2520Byte-Pair%2520Encoded%252C%2520Gated%2520Multi-head%2520Hierarchical%2520Attention%2520Network%252C%250Adesigned%2520to%2520effectively%2520model%2520semi-structured%2520applicant%2520data.%2520BGM-HAN%2520captures%250Amulti-level%2520representations%2520that%2520are%2520crucial%2520for%2520nuanced%2520assessment%252C%2520improving%250Aboth%2520interpretability%2520and%2520predictive%2520performance.%2520Experimental%2520results%2520on%2520real%250Aadmissions%2520data%2520demonstrate%2520that%2520our%2520proposed%2520model%2520significantly%2520outperforms%250Aboth%2520state-of-the-art%2520baselines%2520from%2520traditional%2520machine%2520learning%2520to%2520large%250Alanguage%2520models%252C%2520offering%2520a%2520promising%2520framework%2520for%2520augmenting%2520decision-making%250Ain%2520domains%2520where%2520structure%252C%2520context%252C%2520and%2520fairness%2520matter.%2520Source%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/junhua/bgm-han.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BGM-HAN%3A%20A%20Hierarchical%20Attention%20Network%20for%20Accurate%20and%20Fair%20Decision%0A%20%20Assessment%20on%20Semi-Structured%20Profiles&entry.906535625=Junhua%20Liu%20and%20Roy%20Ka-Wei%20Lee%20and%20Kwan%20Hui%20Lim&entry.1292438233=%20%20Human%20decision-making%20in%20high-stakes%20domains%20often%20relies%20on%20expertise%20and%0Aheuristics%2C%20but%20is%20vulnerable%20to%20hard-to-detect%20cognitive%20biases%20that%20threaten%0Afairness%20and%20long-term%20outcomes.%20This%20work%20presents%20a%20novel%20approach%20to%0Aenhancing%20complex%20decision-making%20workflows%20through%20the%20integration%20of%0Ahierarchical%20learning%20alongside%20various%20enhancements.%20Focusing%20on%20university%0Aadmissions%20as%20a%20representative%20high-stakes%20domain%2C%20we%20propose%20BGM-HAN%2C%20an%0Aenhanced%20Byte-Pair%20Encoded%2C%20Gated%20Multi-head%20Hierarchical%20Attention%20Network%2C%0Adesigned%20to%20effectively%20model%20semi-structured%20applicant%20data.%20BGM-HAN%20captures%0Amulti-level%20representations%20that%20are%20crucial%20for%20nuanced%20assessment%2C%20improving%0Aboth%20interpretability%20and%20predictive%20performance.%20Experimental%20results%20on%20real%0Aadmissions%20data%20demonstrate%20that%20our%20proposed%20model%20significantly%20outperforms%0Aboth%20state-of-the-art%20baselines%20from%20traditional%20machine%20learning%20to%20large%0Alanguage%20models%2C%20offering%20a%20promising%20framework%20for%20augmenting%20decision-making%0Ain%20domains%20where%20structure%2C%20context%2C%20and%20fairness%20matter.%20Source%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/junhua/bgm-han.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17472v1&entry.124074799=Read"},
{"title": "First, Learn What You Don't Know: Active Information Gathering for\n  Driving at the Limits of Handling", "author": "Alexander Davydov and Franck Djeumou and Marcus Greiff and Makoto Suminaka and Michael Thompson and John Subosits and Thomas Lew", "abstract": "  Combining data-driven models that adapt online and model predictive control\n(MPC) has enabled effective control of nonlinear systems. However, when\ndeployed on unstable systems, online adaptation may not be fast enough to\nensure reliable simultaneous learning and control. For example, a controller on\na vehicle executing highly dynamic maneuvers--such as drifting to avoid an\nobstacle--may push the vehicle's tires to their friction limits, destabilizing\nthe vehicle and allowing modeling errors to quickly compound and cause a loss\nof control. To address this challenge, we present an active information\ngathering framework for identifying vehicle dynamics as quickly as possible. We\npropose an expressive vehicle dynamics model that leverages Bayesian last-layer\nmeta-learning to enable rapid online adaptation. The model's uncertainty\nestimates are used to guide informative data collection and quickly improve the\nmodel prior to deployment. Dynamic drifting experiments on a Toyota Supra show\nthat (i) the framework enables reliable control of a vehicle at the edge of\nstability, (ii) online adaptation alone may not suffice for zero-shot control\nand can lead to undesirable transient errors or spin-outs, and (iii) active\ndata collection helps achieve reliable performance.\n", "link": "http://arxiv.org/abs/2411.00107v2", "date": "2025-07-23", "relevancy": 2.1398, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5347}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%2C%20Learn%20What%20You%20Don%27t%20Know%3A%20Active%20Information%20Gathering%20for%0A%20%20Driving%20at%20the%20Limits%20of%20Handling&body=Title%3A%20First%2C%20Learn%20What%20You%20Don%27t%20Know%3A%20Active%20Information%20Gathering%20for%0A%20%20Driving%20at%20the%20Limits%20of%20Handling%0AAuthor%3A%20Alexander%20Davydov%20and%20Franck%20Djeumou%20and%20Marcus%20Greiff%20and%20Makoto%20Suminaka%20and%20Michael%20Thompson%20and%20John%20Subosits%20and%20Thomas%20Lew%0AAbstract%3A%20%20%20Combining%20data-driven%20models%20that%20adapt%20online%20and%20model%20predictive%20control%0A%28MPC%29%20has%20enabled%20effective%20control%20of%20nonlinear%20systems.%20However%2C%20when%0Adeployed%20on%20unstable%20systems%2C%20online%20adaptation%20may%20not%20be%20fast%20enough%20to%0Aensure%20reliable%20simultaneous%20learning%20and%20control.%20For%20example%2C%20a%20controller%20on%0Aa%20vehicle%20executing%20highly%20dynamic%20maneuvers--such%20as%20drifting%20to%20avoid%20an%0Aobstacle--may%20push%20the%20vehicle%27s%20tires%20to%20their%20friction%20limits%2C%20destabilizing%0Athe%20vehicle%20and%20allowing%20modeling%20errors%20to%20quickly%20compound%20and%20cause%20a%20loss%0Aof%20control.%20To%20address%20this%20challenge%2C%20we%20present%20an%20active%20information%0Agathering%20framework%20for%20identifying%20vehicle%20dynamics%20as%20quickly%20as%20possible.%20We%0Apropose%20an%20expressive%20vehicle%20dynamics%20model%20that%20leverages%20Bayesian%20last-layer%0Ameta-learning%20to%20enable%20rapid%20online%20adaptation.%20The%20model%27s%20uncertainty%0Aestimates%20are%20used%20to%20guide%20informative%20data%20collection%20and%20quickly%20improve%20the%0Amodel%20prior%20to%20deployment.%20Dynamic%20drifting%20experiments%20on%20a%20Toyota%20Supra%20show%0Athat%20%28i%29%20the%20framework%20enables%20reliable%20control%20of%20a%20vehicle%20at%20the%20edge%20of%0Astability%2C%20%28ii%29%20online%20adaptation%20alone%20may%20not%20suffice%20for%20zero-shot%20control%0Aand%20can%20lead%20to%20undesirable%20transient%20errors%20or%20spin-outs%2C%20and%20%28iii%29%20active%0Adata%20collection%20helps%20achieve%20reliable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%252C%2520Learn%2520What%2520You%2520Don%2527t%2520Know%253A%2520Active%2520Information%2520Gathering%2520for%250A%2520%2520Driving%2520at%2520the%2520Limits%2520of%2520Handling%26entry.906535625%3DAlexander%2520Davydov%2520and%2520Franck%2520Djeumou%2520and%2520Marcus%2520Greiff%2520and%2520Makoto%2520Suminaka%2520and%2520Michael%2520Thompson%2520and%2520John%2520Subosits%2520and%2520Thomas%2520Lew%26entry.1292438233%3D%2520%2520Combining%2520data-driven%2520models%2520that%2520adapt%2520online%2520and%2520model%2520predictive%2520control%250A%2528MPC%2529%2520has%2520enabled%2520effective%2520control%2520of%2520nonlinear%2520systems.%2520However%252C%2520when%250Adeployed%2520on%2520unstable%2520systems%252C%2520online%2520adaptation%2520may%2520not%2520be%2520fast%2520enough%2520to%250Aensure%2520reliable%2520simultaneous%2520learning%2520and%2520control.%2520For%2520example%252C%2520a%2520controller%2520on%250Aa%2520vehicle%2520executing%2520highly%2520dynamic%2520maneuvers--such%2520as%2520drifting%2520to%2520avoid%2520an%250Aobstacle--may%2520push%2520the%2520vehicle%2527s%2520tires%2520to%2520their%2520friction%2520limits%252C%2520destabilizing%250Athe%2520vehicle%2520and%2520allowing%2520modeling%2520errors%2520to%2520quickly%2520compound%2520and%2520cause%2520a%2520loss%250Aof%2520control.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520an%2520active%2520information%250Agathering%2520framework%2520for%2520identifying%2520vehicle%2520dynamics%2520as%2520quickly%2520as%2520possible.%2520We%250Apropose%2520an%2520expressive%2520vehicle%2520dynamics%2520model%2520that%2520leverages%2520Bayesian%2520last-layer%250Ameta-learning%2520to%2520enable%2520rapid%2520online%2520adaptation.%2520The%2520model%2527s%2520uncertainty%250Aestimates%2520are%2520used%2520to%2520guide%2520informative%2520data%2520collection%2520and%2520quickly%2520improve%2520the%250Amodel%2520prior%2520to%2520deployment.%2520Dynamic%2520drifting%2520experiments%2520on%2520a%2520Toyota%2520Supra%2520show%250Athat%2520%2528i%2529%2520the%2520framework%2520enables%2520reliable%2520control%2520of%2520a%2520vehicle%2520at%2520the%2520edge%2520of%250Astability%252C%2520%2528ii%2529%2520online%2520adaptation%2520alone%2520may%2520not%2520suffice%2520for%2520zero-shot%2520control%250Aand%2520can%2520lead%2520to%2520undesirable%2520transient%2520errors%2520or%2520spin-outs%252C%2520and%2520%2528iii%2529%2520active%250Adata%2520collection%2520helps%2520achieve%2520reliable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%2C%20Learn%20What%20You%20Don%27t%20Know%3A%20Active%20Information%20Gathering%20for%0A%20%20Driving%20at%20the%20Limits%20of%20Handling&entry.906535625=Alexander%20Davydov%20and%20Franck%20Djeumou%20and%20Marcus%20Greiff%20and%20Makoto%20Suminaka%20and%20Michael%20Thompson%20and%20John%20Subosits%20and%20Thomas%20Lew&entry.1292438233=%20%20Combining%20data-driven%20models%20that%20adapt%20online%20and%20model%20predictive%20control%0A%28MPC%29%20has%20enabled%20effective%20control%20of%20nonlinear%20systems.%20However%2C%20when%0Adeployed%20on%20unstable%20systems%2C%20online%20adaptation%20may%20not%20be%20fast%20enough%20to%0Aensure%20reliable%20simultaneous%20learning%20and%20control.%20For%20example%2C%20a%20controller%20on%0Aa%20vehicle%20executing%20highly%20dynamic%20maneuvers--such%20as%20drifting%20to%20avoid%20an%0Aobstacle--may%20push%20the%20vehicle%27s%20tires%20to%20their%20friction%20limits%2C%20destabilizing%0Athe%20vehicle%20and%20allowing%20modeling%20errors%20to%20quickly%20compound%20and%20cause%20a%20loss%0Aof%20control.%20To%20address%20this%20challenge%2C%20we%20present%20an%20active%20information%0Agathering%20framework%20for%20identifying%20vehicle%20dynamics%20as%20quickly%20as%20possible.%20We%0Apropose%20an%20expressive%20vehicle%20dynamics%20model%20that%20leverages%20Bayesian%20last-layer%0Ameta-learning%20to%20enable%20rapid%20online%20adaptation.%20The%20model%27s%20uncertainty%0Aestimates%20are%20used%20to%20guide%20informative%20data%20collection%20and%20quickly%20improve%20the%0Amodel%20prior%20to%20deployment.%20Dynamic%20drifting%20experiments%20on%20a%20Toyota%20Supra%20show%0Athat%20%28i%29%20the%20framework%20enables%20reliable%20control%20of%20a%20vehicle%20at%20the%20edge%20of%0Astability%2C%20%28ii%29%20online%20adaptation%20alone%20may%20not%20suffice%20for%20zero-shot%20control%0Aand%20can%20lead%20to%20undesirable%20transient%20errors%20or%20spin-outs%2C%20and%20%28iii%29%20active%0Adata%20collection%20helps%20achieve%20reliable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00107v2&entry.124074799=Read"},
{"title": "CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality\n  Optimization in Computed Tomography", "author": "Sneha George Gnanakalavathy and Hairil Abdul Razak and Robert Meertens and Jonathan E. Fieldsend and Xujiong Ye and Mohammed M. Abdelsamea", "abstract": "  In computed tomography (CT), achieving high image quality while minimizing\nradiation exposure remains a key clinical challenge. This paper presents\nCAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and\nPredictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT\nintegrates image data with acquisition metadata (such as tube voltage, tube\ncurrent, and contrast agent types) to model the underlying causal relationships\nthat influence image quality. An ensemble of Variational Autoencoders (VAEs) is\nemployed to extract meaningful features and generate causal representations\nfrom observational data, including CT images and associated imaging parameters.\nThese input features are fused to predict the Signal-to-Noise Ratio (SNR) and\nsupport counterfactual inference, enabling what-if simulations, such as changes\nin contrast agents (types and concentrations) or scan parameters. CAPRI-CT is\ntrained and validated using an ensemble learning approach, achieving strong\npredictive performance. By facilitating both prediction and interpretability,\nCAPRI-CT provides actionable insights that could help radiologists and\ntechnicians design more efficient CT protocols without repeated physical scans.\nThe source code and dataset are publicly available at\nhttps://github.com/SnehaGeorge22/capri-ct.\n", "link": "http://arxiv.org/abs/2507.17420v1", "date": "2025-07-23", "relevancy": 2.1389, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.537}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.537}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPRI-CT%3A%20Causal%20Analysis%20and%20Predictive%20Reasoning%20for%20Image%20Quality%0A%20%20Optimization%20in%20Computed%20Tomography&body=Title%3A%20CAPRI-CT%3A%20Causal%20Analysis%20and%20Predictive%20Reasoning%20for%20Image%20Quality%0A%20%20Optimization%20in%20Computed%20Tomography%0AAuthor%3A%20Sneha%20George%20Gnanakalavathy%20and%20Hairil%20Abdul%20Razak%20and%20Robert%20Meertens%20and%20Jonathan%20E.%20Fieldsend%20and%20Xujiong%20Ye%20and%20Mohammed%20M.%20Abdelsamea%0AAbstract%3A%20%20%20In%20computed%20tomography%20%28CT%29%2C%20achieving%20high%20image%20quality%20while%20minimizing%0Aradiation%20exposure%20remains%20a%20key%20clinical%20challenge.%20This%20paper%20presents%0ACAPRI-CT%2C%20a%20novel%20causal-aware%20deep%20learning%20framework%20for%20Causal%20Analysis%20and%0APredictive%20Reasoning%20for%20Image%20Quality%20Optimization%20in%20CT%20imaging.%20CAPRI-CT%0Aintegrates%20image%20data%20with%20acquisition%20metadata%20%28such%20as%20tube%20voltage%2C%20tube%0Acurrent%2C%20and%20contrast%20agent%20types%29%20to%20model%20the%20underlying%20causal%20relationships%0Athat%20influence%20image%20quality.%20An%20ensemble%20of%20Variational%20Autoencoders%20%28VAEs%29%20is%0Aemployed%20to%20extract%20meaningful%20features%20and%20generate%20causal%20representations%0Afrom%20observational%20data%2C%20including%20CT%20images%20and%20associated%20imaging%20parameters.%0AThese%20input%20features%20are%20fused%20to%20predict%20the%20Signal-to-Noise%20Ratio%20%28SNR%29%20and%0Asupport%20counterfactual%20inference%2C%20enabling%20what-if%20simulations%2C%20such%20as%20changes%0Ain%20contrast%20agents%20%28types%20and%20concentrations%29%20or%20scan%20parameters.%20CAPRI-CT%20is%0Atrained%20and%20validated%20using%20an%20ensemble%20learning%20approach%2C%20achieving%20strong%0Apredictive%20performance.%20By%20facilitating%20both%20prediction%20and%20interpretability%2C%0ACAPRI-CT%20provides%20actionable%20insights%20that%20could%20help%20radiologists%20and%0Atechnicians%20design%20more%20efficient%20CT%20protocols%20without%20repeated%20physical%20scans.%0AThe%20source%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SnehaGeorge22/capri-ct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPRI-CT%253A%2520Causal%2520Analysis%2520and%2520Predictive%2520Reasoning%2520for%2520Image%2520Quality%250A%2520%2520Optimization%2520in%2520Computed%2520Tomography%26entry.906535625%3DSneha%2520George%2520Gnanakalavathy%2520and%2520Hairil%2520Abdul%2520Razak%2520and%2520Robert%2520Meertens%2520and%2520Jonathan%2520E.%2520Fieldsend%2520and%2520Xujiong%2520Ye%2520and%2520Mohammed%2520M.%2520Abdelsamea%26entry.1292438233%3D%2520%2520In%2520computed%2520tomography%2520%2528CT%2529%252C%2520achieving%2520high%2520image%2520quality%2520while%2520minimizing%250Aradiation%2520exposure%2520remains%2520a%2520key%2520clinical%2520challenge.%2520This%2520paper%2520presents%250ACAPRI-CT%252C%2520a%2520novel%2520causal-aware%2520deep%2520learning%2520framework%2520for%2520Causal%2520Analysis%2520and%250APredictive%2520Reasoning%2520for%2520Image%2520Quality%2520Optimization%2520in%2520CT%2520imaging.%2520CAPRI-CT%250Aintegrates%2520image%2520data%2520with%2520acquisition%2520metadata%2520%2528such%2520as%2520tube%2520voltage%252C%2520tube%250Acurrent%252C%2520and%2520contrast%2520agent%2520types%2529%2520to%2520model%2520the%2520underlying%2520causal%2520relationships%250Athat%2520influence%2520image%2520quality.%2520An%2520ensemble%2520of%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520is%250Aemployed%2520to%2520extract%2520meaningful%2520features%2520and%2520generate%2520causal%2520representations%250Afrom%2520observational%2520data%252C%2520including%2520CT%2520images%2520and%2520associated%2520imaging%2520parameters.%250AThese%2520input%2520features%2520are%2520fused%2520to%2520predict%2520the%2520Signal-to-Noise%2520Ratio%2520%2528SNR%2529%2520and%250Asupport%2520counterfactual%2520inference%252C%2520enabling%2520what-if%2520simulations%252C%2520such%2520as%2520changes%250Ain%2520contrast%2520agents%2520%2528types%2520and%2520concentrations%2529%2520or%2520scan%2520parameters.%2520CAPRI-CT%2520is%250Atrained%2520and%2520validated%2520using%2520an%2520ensemble%2520learning%2520approach%252C%2520achieving%2520strong%250Apredictive%2520performance.%2520By%2520facilitating%2520both%2520prediction%2520and%2520interpretability%252C%250ACAPRI-CT%2520provides%2520actionable%2520insights%2520that%2520could%2520help%2520radiologists%2520and%250Atechnicians%2520design%2520more%2520efficient%2520CT%2520protocols%2520without%2520repeated%2520physical%2520scans.%250AThe%2520source%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/SnehaGeorge22/capri-ct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPRI-CT%3A%20Causal%20Analysis%20and%20Predictive%20Reasoning%20for%20Image%20Quality%0A%20%20Optimization%20in%20Computed%20Tomography&entry.906535625=Sneha%20George%20Gnanakalavathy%20and%20Hairil%20Abdul%20Razak%20and%20Robert%20Meertens%20and%20Jonathan%20E.%20Fieldsend%20and%20Xujiong%20Ye%20and%20Mohammed%20M.%20Abdelsamea&entry.1292438233=%20%20In%20computed%20tomography%20%28CT%29%2C%20achieving%20high%20image%20quality%20while%20minimizing%0Aradiation%20exposure%20remains%20a%20key%20clinical%20challenge.%20This%20paper%20presents%0ACAPRI-CT%2C%20a%20novel%20causal-aware%20deep%20learning%20framework%20for%20Causal%20Analysis%20and%0APredictive%20Reasoning%20for%20Image%20Quality%20Optimization%20in%20CT%20imaging.%20CAPRI-CT%0Aintegrates%20image%20data%20with%20acquisition%20metadata%20%28such%20as%20tube%20voltage%2C%20tube%0Acurrent%2C%20and%20contrast%20agent%20types%29%20to%20model%20the%20underlying%20causal%20relationships%0Athat%20influence%20image%20quality.%20An%20ensemble%20of%20Variational%20Autoencoders%20%28VAEs%29%20is%0Aemployed%20to%20extract%20meaningful%20features%20and%20generate%20causal%20representations%0Afrom%20observational%20data%2C%20including%20CT%20images%20and%20associated%20imaging%20parameters.%0AThese%20input%20features%20are%20fused%20to%20predict%20the%20Signal-to-Noise%20Ratio%20%28SNR%29%20and%0Asupport%20counterfactual%20inference%2C%20enabling%20what-if%20simulations%2C%20such%20as%20changes%0Ain%20contrast%20agents%20%28types%20and%20concentrations%29%20or%20scan%20parameters.%20CAPRI-CT%20is%0Atrained%20and%20validated%20using%20an%20ensemble%20learning%20approach%2C%20achieving%20strong%0Apredictive%20performance.%20By%20facilitating%20both%20prediction%20and%20interpretability%2C%0ACAPRI-CT%20provides%20actionable%20insights%20that%20could%20help%20radiologists%20and%0Atechnicians%20design%20more%20efficient%20CT%20protocols%20without%20repeated%20physical%20scans.%0AThe%20source%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SnehaGeorge22/capri-ct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17420v1&entry.124074799=Read"},
{"title": "Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners", "author": "Kostas Karakontis and Thanos Petsanis and Athanasios Ch. Kapoutsis and Pavlos Ch. Kapoutsis and Elias B. Kosmatopoulos", "abstract": "  Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan\n", "link": "http://arxiv.org/abs/2507.17519v1", "date": "2025-07-23", "relevancy": 2.1325, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5361}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Terrain-Aware%20Adaptation%20for%20Two-Dimensional%20UAV%20Path%20Planners&body=Title%3A%20Terrain-Aware%20Adaptation%20for%20Two-Dimensional%20UAV%20Path%20Planners%0AAuthor%3A%20Kostas%20Karakontis%20and%20Thanos%20Petsanis%20and%20Athanasios%20Ch.%20Kapoutsis%20and%20Pavlos%20Ch.%20Kapoutsis%20and%20Elias%20B.%20Kosmatopoulos%0AAbstract%3A%20%20%20Multi-UAV%20Coverage%20Path%20Planning%20%28mCPP%29%20algorithms%20in%20popular%20commercial%0Asoftware%20typically%20treat%20a%20Region%20of%20Interest%20%28RoI%29%20only%20as%20a%202D%20plane%2C%0Aignoring%20important3D%20structure%20characteristics.%20This%20leads%20to%20incomplete%0A3Dreconstructions%2C%20especially%20around%20occluded%20or%20vertical%20surfaces.%20In%20this%0Apaper%2C%20we%20propose%20a%20modular%20algorithm%20that%20can%20extend%20commercial%0Atwo-dimensional%20path%20planners%20to%20facilitate%20terrain-aware%20planning%20by%20adjusting%0Aaltitude%20and%20camera%20orientations.%20To%20demonstrate%20it%2C%20we%20extend%20the%20well-known%0ADARP%20%28Divide%20Areas%20for%20Optimal%20Multi-Robot%20Coverage%20Path%20Planning%29%20algorithm%0Aand%20produce%20DARP-3D.%20We%20present%20simulation%20results%20in%20multiple%203D%20environments%0Aand%20a%20real-world%20flight%20test%20using%20DJI%20hardware.%20Compared%20to%20baseline%2C%20our%0Aapproach%20consistently%20captures%20improved%203D%20reconstructions%2C%20particularly%20in%0Aareas%20with%20significant%20vertical%20features.%20An%20open-source%20implementation%20of%20the%0Aalgorithm%20is%20available%20here%3Ahttps%3A//github.com/konskara/TerraPlan%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerrain-Aware%2520Adaptation%2520for%2520Two-Dimensional%2520UAV%2520Path%2520Planners%26entry.906535625%3DKostas%2520Karakontis%2520and%2520Thanos%2520Petsanis%2520and%2520Athanasios%2520Ch.%2520Kapoutsis%2520and%2520Pavlos%2520Ch.%2520Kapoutsis%2520and%2520Elias%2520B.%2520Kosmatopoulos%26entry.1292438233%3D%2520%2520Multi-UAV%2520Coverage%2520Path%2520Planning%2520%2528mCPP%2529%2520algorithms%2520in%2520popular%2520commercial%250Asoftware%2520typically%2520treat%2520a%2520Region%2520of%2520Interest%2520%2528RoI%2529%2520only%2520as%2520a%25202D%2520plane%252C%250Aignoring%2520important3D%2520structure%2520characteristics.%2520This%2520leads%2520to%2520incomplete%250A3Dreconstructions%252C%2520especially%2520around%2520occluded%2520or%2520vertical%2520surfaces.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520modular%2520algorithm%2520that%2520can%2520extend%2520commercial%250Atwo-dimensional%2520path%2520planners%2520to%2520facilitate%2520terrain-aware%2520planning%2520by%2520adjusting%250Aaltitude%2520and%2520camera%2520orientations.%2520To%2520demonstrate%2520it%252C%2520we%2520extend%2520the%2520well-known%250ADARP%2520%2528Divide%2520Areas%2520for%2520Optimal%2520Multi-Robot%2520Coverage%2520Path%2520Planning%2529%2520algorithm%250Aand%2520produce%2520DARP-3D.%2520We%2520present%2520simulation%2520results%2520in%2520multiple%25203D%2520environments%250Aand%2520a%2520real-world%2520flight%2520test%2520using%2520DJI%2520hardware.%2520Compared%2520to%2520baseline%252C%2520our%250Aapproach%2520consistently%2520captures%2520improved%25203D%2520reconstructions%252C%2520particularly%2520in%250Aareas%2520with%2520significant%2520vertical%2520features.%2520An%2520open-source%2520implementation%2520of%2520the%250Aalgorithm%2520is%2520available%2520here%253Ahttps%253A//github.com/konskara/TerraPlan%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Terrain-Aware%20Adaptation%20for%20Two-Dimensional%20UAV%20Path%20Planners&entry.906535625=Kostas%20Karakontis%20and%20Thanos%20Petsanis%20and%20Athanasios%20Ch.%20Kapoutsis%20and%20Pavlos%20Ch.%20Kapoutsis%20and%20Elias%20B.%20Kosmatopoulos&entry.1292438233=%20%20Multi-UAV%20Coverage%20Path%20Planning%20%28mCPP%29%20algorithms%20in%20popular%20commercial%0Asoftware%20typically%20treat%20a%20Region%20of%20Interest%20%28RoI%29%20only%20as%20a%202D%20plane%2C%0Aignoring%20important3D%20structure%20characteristics.%20This%20leads%20to%20incomplete%0A3Dreconstructions%2C%20especially%20around%20occluded%20or%20vertical%20surfaces.%20In%20this%0Apaper%2C%20we%20propose%20a%20modular%20algorithm%20that%20can%20extend%20commercial%0Atwo-dimensional%20path%20planners%20to%20facilitate%20terrain-aware%20planning%20by%20adjusting%0Aaltitude%20and%20camera%20orientations.%20To%20demonstrate%20it%2C%20we%20extend%20the%20well-known%0ADARP%20%28Divide%20Areas%20for%20Optimal%20Multi-Robot%20Coverage%20Path%20Planning%29%20algorithm%0Aand%20produce%20DARP-3D.%20We%20present%20simulation%20results%20in%20multiple%203D%20environments%0Aand%20a%20real-world%20flight%20test%20using%20DJI%20hardware.%20Compared%20to%20baseline%2C%20our%0Aapproach%20consistently%20captures%20improved%203D%20reconstructions%2C%20particularly%20in%0Aareas%20with%20significant%20vertical%20features.%20An%20open-source%20implementation%20of%20the%0Aalgorithm%20is%20available%20here%3Ahttps%3A//github.com/konskara/TerraPlan%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17519v1&entry.124074799=Read"},
{"title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven\n  Approach to QA Benchmarks", "author": "Linbo Cao and Jinman Zhao", "abstract": "  As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.\n", "link": "http://arxiv.org/abs/2507.17747v1", "date": "2025-07-23", "relevancy": 2.1267, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining%20on%20the%20Test%20Set%20Is%20No%20Longer%20All%20You%20Need%3A%20A%20Debate-Driven%0A%20%20Approach%20to%20QA%20Benchmarks&body=Title%3A%20Pretraining%20on%20the%20Test%20Set%20Is%20No%20Longer%20All%20You%20Need%3A%20A%20Debate-Driven%0A%20%20Approach%20to%20QA%20Benchmarks%0AAuthor%3A%20Linbo%20Cao%20and%20Jinman%20Zhao%0AAbstract%3A%20%20%20As%20frontier%20language%20models%20increasingly%20saturate%20standard%20QA%20benchmarks%2C%0Aconcerns%20about%20data%20contamination%2C%20memorization%2C%20and%20escalating%20dataset%0Acreation%20costs%20persist.%20We%20propose%20a%20debate-driven%20evaluation%20paradigm%20that%0Atransforms%20any%20existing%20QA%20dataset%20into%20structured%20adversarial%20debates--where%0Aone%20model%20is%20given%20the%20official%20answer%20to%20defend%2C%20and%20another%20constructs%20and%0Adefends%20an%20alternative%20answer--adjudicated%20by%20a%20judge%20model%20blind%20to%20the%0Acorrect%20solution.%20By%20forcing%20multi-round%20argumentation%2C%20this%20approach%0Asubstantially%20increases%20difficulty%20while%20penalizing%20shallow%20memorization%2C%20yet%0Areuses%20QA%20items%20to%20reduce%20curation%20overhead.%20We%20make%20two%20main%20contributions%3A%0A%281%29%20an%20evaluation%20pipeline%20to%20systematically%20convert%20QA%20tasks%20into%20debate-based%0Aassessments%2C%20and%20%282%29%20a%20public%20benchmark%20that%20demonstrates%20our%20paradigm%27s%0Aeffectiveness%20on%20a%20subset%20of%20MMLU-Pro%20questions%2C%20complete%20with%20standardized%0Aprotocols%20and%20reference%20models.%20Empirical%20results%20validate%20the%20robustness%20of%0Athe%20method%20and%20its%20effectiveness%20against%20data%20contamination--a%20Llama%203.1%20model%0Afine-tuned%20on%20test%20questions%20showed%20dramatic%20accuracy%20improvements%20%2850%25%20-%3E%2082%25%29%0Abut%20performed%20worse%20in%20debates.%20Results%20also%20show%20that%20even%20weaker%20judges%20can%0Areliably%20differentiate%20stronger%20debaters%2C%20highlighting%20how%20debate-based%0Aevaluation%20can%20scale%20to%20future%2C%20more%20capable%20systems%20while%20maintaining%20a%0Afraction%20of%20the%20cost%20of%20creating%20new%20benchmarks.%20Overall%2C%20our%20framework%0Aunderscores%20that%20%22pretraining%20on%20the%20test%20set%20is%20no%20longer%20all%20you%20need%2C%22%0Aoffering%20a%20sustainable%20path%20for%20measuring%20the%20genuine%20reasoning%20ability%20of%0Aadvanced%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining%2520on%2520the%2520Test%2520Set%2520Is%2520No%2520Longer%2520All%2520You%2520Need%253A%2520A%2520Debate-Driven%250A%2520%2520Approach%2520to%2520QA%2520Benchmarks%26entry.906535625%3DLinbo%2520Cao%2520and%2520Jinman%2520Zhao%26entry.1292438233%3D%2520%2520As%2520frontier%2520language%2520models%2520increasingly%2520saturate%2520standard%2520QA%2520benchmarks%252C%250Aconcerns%2520about%2520data%2520contamination%252C%2520memorization%252C%2520and%2520escalating%2520dataset%250Acreation%2520costs%2520persist.%2520We%2520propose%2520a%2520debate-driven%2520evaluation%2520paradigm%2520that%250Atransforms%2520any%2520existing%2520QA%2520dataset%2520into%2520structured%2520adversarial%2520debates--where%250Aone%2520model%2520is%2520given%2520the%2520official%2520answer%2520to%2520defend%252C%2520and%2520another%2520constructs%2520and%250Adefends%2520an%2520alternative%2520answer--adjudicated%2520by%2520a%2520judge%2520model%2520blind%2520to%2520the%250Acorrect%2520solution.%2520By%2520forcing%2520multi-round%2520argumentation%252C%2520this%2520approach%250Asubstantially%2520increases%2520difficulty%2520while%2520penalizing%2520shallow%2520memorization%252C%2520yet%250Areuses%2520QA%2520items%2520to%2520reduce%2520curation%2520overhead.%2520We%2520make%2520two%2520main%2520contributions%253A%250A%25281%2529%2520an%2520evaluation%2520pipeline%2520to%2520systematically%2520convert%2520QA%2520tasks%2520into%2520debate-based%250Aassessments%252C%2520and%2520%25282%2529%2520a%2520public%2520benchmark%2520that%2520demonstrates%2520our%2520paradigm%2527s%250Aeffectiveness%2520on%2520a%2520subset%2520of%2520MMLU-Pro%2520questions%252C%2520complete%2520with%2520standardized%250Aprotocols%2520and%2520reference%2520models.%2520Empirical%2520results%2520validate%2520the%2520robustness%2520of%250Athe%2520method%2520and%2520its%2520effectiveness%2520against%2520data%2520contamination--a%2520Llama%25203.1%2520model%250Afine-tuned%2520on%2520test%2520questions%2520showed%2520dramatic%2520accuracy%2520improvements%2520%252850%2525%2520-%253E%252082%2525%2529%250Abut%2520performed%2520worse%2520in%2520debates.%2520Results%2520also%2520show%2520that%2520even%2520weaker%2520judges%2520can%250Areliably%2520differentiate%2520stronger%2520debaters%252C%2520highlighting%2520how%2520debate-based%250Aevaluation%2520can%2520scale%2520to%2520future%252C%2520more%2520capable%2520systems%2520while%2520maintaining%2520a%250Afraction%2520of%2520the%2520cost%2520of%2520creating%2520new%2520benchmarks.%2520Overall%252C%2520our%2520framework%250Aunderscores%2520that%2520%2522pretraining%2520on%2520the%2520test%2520set%2520is%2520no%2520longer%2520all%2520you%2520need%252C%2522%250Aoffering%2520a%2520sustainable%2520path%2520for%2520measuring%2520the%2520genuine%2520reasoning%2520ability%2520of%250Aadvanced%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20on%20the%20Test%20Set%20Is%20No%20Longer%20All%20You%20Need%3A%20A%20Debate-Driven%0A%20%20Approach%20to%20QA%20Benchmarks&entry.906535625=Linbo%20Cao%20and%20Jinman%20Zhao&entry.1292438233=%20%20As%20frontier%20language%20models%20increasingly%20saturate%20standard%20QA%20benchmarks%2C%0Aconcerns%20about%20data%20contamination%2C%20memorization%2C%20and%20escalating%20dataset%0Acreation%20costs%20persist.%20We%20propose%20a%20debate-driven%20evaluation%20paradigm%20that%0Atransforms%20any%20existing%20QA%20dataset%20into%20structured%20adversarial%20debates--where%0Aone%20model%20is%20given%20the%20official%20answer%20to%20defend%2C%20and%20another%20constructs%20and%0Adefends%20an%20alternative%20answer--adjudicated%20by%20a%20judge%20model%20blind%20to%20the%0Acorrect%20solution.%20By%20forcing%20multi-round%20argumentation%2C%20this%20approach%0Asubstantially%20increases%20difficulty%20while%20penalizing%20shallow%20memorization%2C%20yet%0Areuses%20QA%20items%20to%20reduce%20curation%20overhead.%20We%20make%20two%20main%20contributions%3A%0A%281%29%20an%20evaluation%20pipeline%20to%20systematically%20convert%20QA%20tasks%20into%20debate-based%0Aassessments%2C%20and%20%282%29%20a%20public%20benchmark%20that%20demonstrates%20our%20paradigm%27s%0Aeffectiveness%20on%20a%20subset%20of%20MMLU-Pro%20questions%2C%20complete%20with%20standardized%0Aprotocols%20and%20reference%20models.%20Empirical%20results%20validate%20the%20robustness%20of%0Athe%20method%20and%20its%20effectiveness%20against%20data%20contamination--a%20Llama%203.1%20model%0Afine-tuned%20on%20test%20questions%20showed%20dramatic%20accuracy%20improvements%20%2850%25%20-%3E%2082%25%29%0Abut%20performed%20worse%20in%20debates.%20Results%20also%20show%20that%20even%20weaker%20judges%20can%0Areliably%20differentiate%20stronger%20debaters%2C%20highlighting%20how%20debate-based%0Aevaluation%20can%20scale%20to%20future%2C%20more%20capable%20systems%20while%20maintaining%20a%0Afraction%20of%20the%20cost%20of%20creating%20new%20benchmarks.%20Overall%2C%20our%20framework%0Aunderscores%20that%20%22pretraining%20on%20the%20test%20set%20is%20no%20longer%20all%20you%20need%2C%22%0Aoffering%20a%20sustainable%20path%20for%20measuring%20the%20genuine%20reasoning%20ability%20of%0Aadvanced%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17747v1&entry.124074799=Read"},
{"title": "Vision Transformer attention alignment with human visual perception in\n  aesthetic object evaluation", "author": "Miguel Carrasco and C\u00e9sar Gonz\u00e1lez-Mart\u00edn and Jos\u00e9 Aranda and Luis Oliveros", "abstract": "  Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.\n", "link": "http://arxiv.org/abs/2507.17616v1", "date": "2025-07-23", "relevancy": 2.1229, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5516}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5172}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformer%20attention%20alignment%20with%20human%20visual%20perception%20in%0A%20%20aesthetic%20object%20evaluation&body=Title%3A%20Vision%20Transformer%20attention%20alignment%20with%20human%20visual%20perception%20in%0A%20%20aesthetic%20object%20evaluation%0AAuthor%3A%20Miguel%20Carrasco%20and%20C%C3%A9sar%20Gonz%C3%A1lez-Mart%C3%ADn%20and%20Jos%C3%A9%20Aranda%20and%20Luis%20Oliveros%0AAbstract%3A%20%20%20Visual%20attention%20mechanisms%20play%20a%20crucial%20role%20in%20human%20perception%20and%0Aaesthetic%20evaluation.%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%0Ademonstrated%20remarkable%20capabilities%20in%20computer%20vision%20tasks%2C%20yet%20their%0Aalignment%20with%20human%20visual%20attention%20patterns%20remains%20underexplored%2C%0Aparticularly%20in%20aesthetic%20contexts.%20This%20study%20investigates%20the%20correlation%0Abetween%20human%20visual%20attention%20and%20ViT%20attention%20mechanisms%20when%20evaluating%0Ahandcrafted%20objects.%20We%20conducted%20an%20eye-tracking%20experiment%20with%2030%0Aparticipants%20%289%20female%2C%2021%20male%2C%20mean%20age%2024.6%20years%29%20who%20viewed%2020%20artisanal%0Aobjects%20comprising%20basketry%20bags%20and%20ginger%20jars.%20Using%20a%20Pupil%20Labs%0Aeye-tracker%2C%20we%20recorded%20gaze%20patterns%20and%20generated%20heat%20maps%20representing%0Ahuman%20visual%20attention.%20Simultaneously%2C%20we%20analyzed%20the%20same%20objects%20using%20a%0Apre-trained%20ViT%20model%20with%20DINO%20%28Self-DIstillation%20with%20NO%20Labels%29%2C%20extracting%0Aattention%20maps%20from%20each%20of%20the%2012%20attention%20heads.%20We%20compared%20human%20and%20ViT%0Aattention%20distributions%20using%20Kullback-Leibler%20divergence%20across%20varying%0AGaussian%20parameters%20%28sigma%3D0.1%20to%203.0%29.%20Statistical%20analysis%20revealed%20optimal%0Acorrelation%20at%20sigma%3D2.4%20%2B-0.03%2C%20with%20attention%20head%20%2312%20showing%20the%20strongest%0Aalignment%20with%20human%20visual%20patterns.%20Significant%20differences%20were%20found%0Abetween%20attention%20heads%2C%20with%20heads%20%237%20and%20%239%20demonstrating%20the%20greatest%0Adivergence%20from%20human%20attention%20%28p%3C%200.05%2C%20Tukey%20HSD%20test%29.%20Results%20indicate%0Athat%20while%20ViTs%20exhibit%20more%20global%20attention%20patterns%20compared%20to%20human%20focal%0Aattention%2C%20certain%20attention%20heads%20can%20approximate%20human%20visual%20behavior%2C%0Aparticularly%20for%20specific%20object%20features%20like%20buckles%20in%20basketry%20items.%20These%0Afindings%20suggest%20potential%20applications%20of%20ViT%20attention%20mechanisms%20in%20product%0Adesign%20and%20aesthetic%20evaluation%2C%20while%20highlighting%20fundamental%20differences%20in%0Aattention%20strategies%20between%20human%20perception%20and%20current%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformer%2520attention%2520alignment%2520with%2520human%2520visual%2520perception%2520in%250A%2520%2520aesthetic%2520object%2520evaluation%26entry.906535625%3DMiguel%2520Carrasco%2520and%2520C%25C3%25A9sar%2520Gonz%25C3%25A1lez-Mart%25C3%25ADn%2520and%2520Jos%25C3%25A9%2520Aranda%2520and%2520Luis%2520Oliveros%26entry.1292438233%3D%2520%2520Visual%2520attention%2520mechanisms%2520play%2520a%2520crucial%2520role%2520in%2520human%2520perception%2520and%250Aaesthetic%2520evaluation.%2520Recent%2520advances%2520in%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%250Ademonstrated%2520remarkable%2520capabilities%2520in%2520computer%2520vision%2520tasks%252C%2520yet%2520their%250Aalignment%2520with%2520human%2520visual%2520attention%2520patterns%2520remains%2520underexplored%252C%250Aparticularly%2520in%2520aesthetic%2520contexts.%2520This%2520study%2520investigates%2520the%2520correlation%250Abetween%2520human%2520visual%2520attention%2520and%2520ViT%2520attention%2520mechanisms%2520when%2520evaluating%250Ahandcrafted%2520objects.%2520We%2520conducted%2520an%2520eye-tracking%2520experiment%2520with%252030%250Aparticipants%2520%25289%2520female%252C%252021%2520male%252C%2520mean%2520age%252024.6%2520years%2529%2520who%2520viewed%252020%2520artisanal%250Aobjects%2520comprising%2520basketry%2520bags%2520and%2520ginger%2520jars.%2520Using%2520a%2520Pupil%2520Labs%250Aeye-tracker%252C%2520we%2520recorded%2520gaze%2520patterns%2520and%2520generated%2520heat%2520maps%2520representing%250Ahuman%2520visual%2520attention.%2520Simultaneously%252C%2520we%2520analyzed%2520the%2520same%2520objects%2520using%2520a%250Apre-trained%2520ViT%2520model%2520with%2520DINO%2520%2528Self-DIstillation%2520with%2520NO%2520Labels%2529%252C%2520extracting%250Aattention%2520maps%2520from%2520each%2520of%2520the%252012%2520attention%2520heads.%2520We%2520compared%2520human%2520and%2520ViT%250Aattention%2520distributions%2520using%2520Kullback-Leibler%2520divergence%2520across%2520varying%250AGaussian%2520parameters%2520%2528sigma%253D0.1%2520to%25203.0%2529.%2520Statistical%2520analysis%2520revealed%2520optimal%250Acorrelation%2520at%2520sigma%253D2.4%2520%252B-0.03%252C%2520with%2520attention%2520head%2520%252312%2520showing%2520the%2520strongest%250Aalignment%2520with%2520human%2520visual%2520patterns.%2520Significant%2520differences%2520were%2520found%250Abetween%2520attention%2520heads%252C%2520with%2520heads%2520%25237%2520and%2520%25239%2520demonstrating%2520the%2520greatest%250Adivergence%2520from%2520human%2520attention%2520%2528p%253C%25200.05%252C%2520Tukey%2520HSD%2520test%2529.%2520Results%2520indicate%250Athat%2520while%2520ViTs%2520exhibit%2520more%2520global%2520attention%2520patterns%2520compared%2520to%2520human%2520focal%250Aattention%252C%2520certain%2520attention%2520heads%2520can%2520approximate%2520human%2520visual%2520behavior%252C%250Aparticularly%2520for%2520specific%2520object%2520features%2520like%2520buckles%2520in%2520basketry%2520items.%2520These%250Afindings%2520suggest%2520potential%2520applications%2520of%2520ViT%2520attention%2520mechanisms%2520in%2520product%250Adesign%2520and%2520aesthetic%2520evaluation%252C%2520while%2520highlighting%2520fundamental%2520differences%2520in%250Aattention%2520strategies%2520between%2520human%2520perception%2520and%2520current%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformer%20attention%20alignment%20with%20human%20visual%20perception%20in%0A%20%20aesthetic%20object%20evaluation&entry.906535625=Miguel%20Carrasco%20and%20C%C3%A9sar%20Gonz%C3%A1lez-Mart%C3%ADn%20and%20Jos%C3%A9%20Aranda%20and%20Luis%20Oliveros&entry.1292438233=%20%20Visual%20attention%20mechanisms%20play%20a%20crucial%20role%20in%20human%20perception%20and%0Aaesthetic%20evaluation.%20Recent%20advances%20in%20Vision%20Transformers%20%28ViTs%29%20have%0Ademonstrated%20remarkable%20capabilities%20in%20computer%20vision%20tasks%2C%20yet%20their%0Aalignment%20with%20human%20visual%20attention%20patterns%20remains%20underexplored%2C%0Aparticularly%20in%20aesthetic%20contexts.%20This%20study%20investigates%20the%20correlation%0Abetween%20human%20visual%20attention%20and%20ViT%20attention%20mechanisms%20when%20evaluating%0Ahandcrafted%20objects.%20We%20conducted%20an%20eye-tracking%20experiment%20with%2030%0Aparticipants%20%289%20female%2C%2021%20male%2C%20mean%20age%2024.6%20years%29%20who%20viewed%2020%20artisanal%0Aobjects%20comprising%20basketry%20bags%20and%20ginger%20jars.%20Using%20a%20Pupil%20Labs%0Aeye-tracker%2C%20we%20recorded%20gaze%20patterns%20and%20generated%20heat%20maps%20representing%0Ahuman%20visual%20attention.%20Simultaneously%2C%20we%20analyzed%20the%20same%20objects%20using%20a%0Apre-trained%20ViT%20model%20with%20DINO%20%28Self-DIstillation%20with%20NO%20Labels%29%2C%20extracting%0Aattention%20maps%20from%20each%20of%20the%2012%20attention%20heads.%20We%20compared%20human%20and%20ViT%0Aattention%20distributions%20using%20Kullback-Leibler%20divergence%20across%20varying%0AGaussian%20parameters%20%28sigma%3D0.1%20to%203.0%29.%20Statistical%20analysis%20revealed%20optimal%0Acorrelation%20at%20sigma%3D2.4%20%2B-0.03%2C%20with%20attention%20head%20%2312%20showing%20the%20strongest%0Aalignment%20with%20human%20visual%20patterns.%20Significant%20differences%20were%20found%0Abetween%20attention%20heads%2C%20with%20heads%20%237%20and%20%239%20demonstrating%20the%20greatest%0Adivergence%20from%20human%20attention%20%28p%3C%200.05%2C%20Tukey%20HSD%20test%29.%20Results%20indicate%0Athat%20while%20ViTs%20exhibit%20more%20global%20attention%20patterns%20compared%20to%20human%20focal%0Aattention%2C%20certain%20attention%20heads%20can%20approximate%20human%20visual%20behavior%2C%0Aparticularly%20for%20specific%20object%20features%20like%20buckles%20in%20basketry%20items.%20These%0Afindings%20suggest%20potential%20applications%20of%20ViT%20attention%20mechanisms%20in%20product%0Adesign%20and%20aesthetic%20evaluation%2C%20while%20highlighting%20fundamental%20differences%20in%0Aattention%20strategies%20between%20human%20perception%20and%20current%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17616v1&entry.124074799=Read"},
{"title": "Trusted Multi-view Learning under Noisy Supervision", "author": "Yilin Zhang and Cai Xu and Han Jiang and Ziyu Guan and Wei Zhao and Xiaofei He and Murat Sensoy", "abstract": "  Multi-view learning methods often focus on improving decision accuracy while\nneglecting the decision uncertainty, which significantly restricts their\napplications in safety-critical scenarios. To address this, trusted multi-view\nlearning methods estimate prediction uncertainties by learning class\ndistributions from each instance. However, these methods heavily rely on high\nquality ground-truth labels. This motivates us to delve into a new problem: how\nto develop a reliable multi-view learning model under the guidance of noisy\nlabels? We propose the Trusted Multi view Noise Refining (TMNR) method to\naddress this challenge by modeling label noise arising from low-quality data\nfeatures and easily-confused classes. TMNR employs evidential deep neural\nnetworks to construct view-specific opinions that capture both beliefs and\nuncertainty. These opinions are then transformed through noise correlation\nmatrices to align with the noisy supervision, where matrix elements are\nconstrained by sample uncertainty to reflect label reliability. Furthermore,\nconsidering the challenge of jointly optimizing the evidence network and noise\ncorrelation matrices under noisy supervision, we further propose Trusted\nMulti-view Noise Re-Refining (TMNR^2 ), which disentangles this complex\nco-training problem by establishing different training objectives for distinct\nmodules. TMNR^2 identifies potentially mislabeled samples through\nevidence-label consistency and generates pseudo-labels from neighboring\ninformation. By assigning clean samples to optimize evidential networks and\nnoisy samples to guide noise correlation matrices, respectively, TMNR^2 reduces\nmapping interference and achieves stabilizes training. Experimental results\ndemonstrate that TMNR^2 significantly outperforms baseline methods, with\naverage accuracy improvements of 7% on datasets with 50% label noise.\n", "link": "http://arxiv.org/abs/2404.11944v3", "date": "2025-07-23", "relevancy": 2.1193, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trusted%20Multi-view%20Learning%20under%20Noisy%20Supervision&body=Title%3A%20Trusted%20Multi-view%20Learning%20under%20Noisy%20Supervision%0AAuthor%3A%20Yilin%20Zhang%20and%20Cai%20Xu%20and%20Han%20Jiang%20and%20Ziyu%20Guan%20and%20Wei%20Zhao%20and%20Xiaofei%20He%20and%20Murat%20Sensoy%0AAbstract%3A%20%20%20Multi-view%20learning%20methods%20often%20focus%20on%20improving%20decision%20accuracy%20while%0Aneglecting%20the%20decision%20uncertainty%2C%20which%20significantly%20restricts%20their%0Aapplications%20in%20safety-critical%20scenarios.%20To%20address%20this%2C%20trusted%20multi-view%0Alearning%20methods%20estimate%20prediction%20uncertainties%20by%20learning%20class%0Adistributions%20from%20each%20instance.%20However%2C%20these%20methods%20heavily%20rely%20on%20high%0Aquality%20ground-truth%20labels.%20This%20motivates%20us%20to%20delve%20into%20a%20new%20problem%3A%20how%0Ato%20develop%20a%20reliable%20multi-view%20learning%20model%20under%20the%20guidance%20of%20noisy%0Alabels%3F%20We%20propose%20the%20Trusted%20Multi%20view%20Noise%20Refining%20%28TMNR%29%20method%20to%0Aaddress%20this%20challenge%20by%20modeling%20label%20noise%20arising%20from%20low-quality%20data%0Afeatures%20and%20easily-confused%20classes.%20TMNR%20employs%20evidential%20deep%20neural%0Anetworks%20to%20construct%20view-specific%20opinions%20that%20capture%20both%20beliefs%20and%0Auncertainty.%20These%20opinions%20are%20then%20transformed%20through%20noise%20correlation%0Amatrices%20to%20align%20with%20the%20noisy%20supervision%2C%20where%20matrix%20elements%20are%0Aconstrained%20by%20sample%20uncertainty%20to%20reflect%20label%20reliability.%20Furthermore%2C%0Aconsidering%20the%20challenge%20of%20jointly%20optimizing%20the%20evidence%20network%20and%20noise%0Acorrelation%20matrices%20under%20noisy%20supervision%2C%20we%20further%20propose%20Trusted%0AMulti-view%20Noise%20Re-Refining%20%28TMNR%5E2%20%29%2C%20which%20disentangles%20this%20complex%0Aco-training%20problem%20by%20establishing%20different%20training%20objectives%20for%20distinct%0Amodules.%20TMNR%5E2%20identifies%20potentially%20mislabeled%20samples%20through%0Aevidence-label%20consistency%20and%20generates%20pseudo-labels%20from%20neighboring%0Ainformation.%20By%20assigning%20clean%20samples%20to%20optimize%20evidential%20networks%20and%0Anoisy%20samples%20to%20guide%20noise%20correlation%20matrices%2C%20respectively%2C%20TMNR%5E2%20reduces%0Amapping%20interference%20and%20achieves%20stabilizes%20training.%20Experimental%20results%0Ademonstrate%20that%20TMNR%5E2%20significantly%20outperforms%20baseline%20methods%2C%20with%0Aaverage%20accuracy%20improvements%20of%207%25%20on%20datasets%20with%2050%25%20label%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11944v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrusted%2520Multi-view%2520Learning%2520under%2520Noisy%2520Supervision%26entry.906535625%3DYilin%2520Zhang%2520and%2520Cai%2520Xu%2520and%2520Han%2520Jiang%2520and%2520Ziyu%2520Guan%2520and%2520Wei%2520Zhao%2520and%2520Xiaofei%2520He%2520and%2520Murat%2520Sensoy%26entry.1292438233%3D%2520%2520Multi-view%2520learning%2520methods%2520often%2520focus%2520on%2520improving%2520decision%2520accuracy%2520while%250Aneglecting%2520the%2520decision%2520uncertainty%252C%2520which%2520significantly%2520restricts%2520their%250Aapplications%2520in%2520safety-critical%2520scenarios.%2520To%2520address%2520this%252C%2520trusted%2520multi-view%250Alearning%2520methods%2520estimate%2520prediction%2520uncertainties%2520by%2520learning%2520class%250Adistributions%2520from%2520each%2520instance.%2520However%252C%2520these%2520methods%2520heavily%2520rely%2520on%2520high%250Aquality%2520ground-truth%2520labels.%2520This%2520motivates%2520us%2520to%2520delve%2520into%2520a%2520new%2520problem%253A%2520how%250Ato%2520develop%2520a%2520reliable%2520multi-view%2520learning%2520model%2520under%2520the%2520guidance%2520of%2520noisy%250Alabels%253F%2520We%2520propose%2520the%2520Trusted%2520Multi%2520view%2520Noise%2520Refining%2520%2528TMNR%2529%2520method%2520to%250Aaddress%2520this%2520challenge%2520by%2520modeling%2520label%2520noise%2520arising%2520from%2520low-quality%2520data%250Afeatures%2520and%2520easily-confused%2520classes.%2520TMNR%2520employs%2520evidential%2520deep%2520neural%250Anetworks%2520to%2520construct%2520view-specific%2520opinions%2520that%2520capture%2520both%2520beliefs%2520and%250Auncertainty.%2520These%2520opinions%2520are%2520then%2520transformed%2520through%2520noise%2520correlation%250Amatrices%2520to%2520align%2520with%2520the%2520noisy%2520supervision%252C%2520where%2520matrix%2520elements%2520are%250Aconstrained%2520by%2520sample%2520uncertainty%2520to%2520reflect%2520label%2520reliability.%2520Furthermore%252C%250Aconsidering%2520the%2520challenge%2520of%2520jointly%2520optimizing%2520the%2520evidence%2520network%2520and%2520noise%250Acorrelation%2520matrices%2520under%2520noisy%2520supervision%252C%2520we%2520further%2520propose%2520Trusted%250AMulti-view%2520Noise%2520Re-Refining%2520%2528TMNR%255E2%2520%2529%252C%2520which%2520disentangles%2520this%2520complex%250Aco-training%2520problem%2520by%2520establishing%2520different%2520training%2520objectives%2520for%2520distinct%250Amodules.%2520TMNR%255E2%2520identifies%2520potentially%2520mislabeled%2520samples%2520through%250Aevidence-label%2520consistency%2520and%2520generates%2520pseudo-labels%2520from%2520neighboring%250Ainformation.%2520By%2520assigning%2520clean%2520samples%2520to%2520optimize%2520evidential%2520networks%2520and%250Anoisy%2520samples%2520to%2520guide%2520noise%2520correlation%2520matrices%252C%2520respectively%252C%2520TMNR%255E2%2520reduces%250Amapping%2520interference%2520and%2520achieves%2520stabilizes%2520training.%2520Experimental%2520results%250Ademonstrate%2520that%2520TMNR%255E2%2520significantly%2520outperforms%2520baseline%2520methods%252C%2520with%250Aaverage%2520accuracy%2520improvements%2520of%25207%2525%2520on%2520datasets%2520with%252050%2525%2520label%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11944v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trusted%20Multi-view%20Learning%20under%20Noisy%20Supervision&entry.906535625=Yilin%20Zhang%20and%20Cai%20Xu%20and%20Han%20Jiang%20and%20Ziyu%20Guan%20and%20Wei%20Zhao%20and%20Xiaofei%20He%20and%20Murat%20Sensoy&entry.1292438233=%20%20Multi-view%20learning%20methods%20often%20focus%20on%20improving%20decision%20accuracy%20while%0Aneglecting%20the%20decision%20uncertainty%2C%20which%20significantly%20restricts%20their%0Aapplications%20in%20safety-critical%20scenarios.%20To%20address%20this%2C%20trusted%20multi-view%0Alearning%20methods%20estimate%20prediction%20uncertainties%20by%20learning%20class%0Adistributions%20from%20each%20instance.%20However%2C%20these%20methods%20heavily%20rely%20on%20high%0Aquality%20ground-truth%20labels.%20This%20motivates%20us%20to%20delve%20into%20a%20new%20problem%3A%20how%0Ato%20develop%20a%20reliable%20multi-view%20learning%20model%20under%20the%20guidance%20of%20noisy%0Alabels%3F%20We%20propose%20the%20Trusted%20Multi%20view%20Noise%20Refining%20%28TMNR%29%20method%20to%0Aaddress%20this%20challenge%20by%20modeling%20label%20noise%20arising%20from%20low-quality%20data%0Afeatures%20and%20easily-confused%20classes.%20TMNR%20employs%20evidential%20deep%20neural%0Anetworks%20to%20construct%20view-specific%20opinions%20that%20capture%20both%20beliefs%20and%0Auncertainty.%20These%20opinions%20are%20then%20transformed%20through%20noise%20correlation%0Amatrices%20to%20align%20with%20the%20noisy%20supervision%2C%20where%20matrix%20elements%20are%0Aconstrained%20by%20sample%20uncertainty%20to%20reflect%20label%20reliability.%20Furthermore%2C%0Aconsidering%20the%20challenge%20of%20jointly%20optimizing%20the%20evidence%20network%20and%20noise%0Acorrelation%20matrices%20under%20noisy%20supervision%2C%20we%20further%20propose%20Trusted%0AMulti-view%20Noise%20Re-Refining%20%28TMNR%5E2%20%29%2C%20which%20disentangles%20this%20complex%0Aco-training%20problem%20by%20establishing%20different%20training%20objectives%20for%20distinct%0Amodules.%20TMNR%5E2%20identifies%20potentially%20mislabeled%20samples%20through%0Aevidence-label%20consistency%20and%20generates%20pseudo-labels%20from%20neighboring%0Ainformation.%20By%20assigning%20clean%20samples%20to%20optimize%20evidential%20networks%20and%0Anoisy%20samples%20to%20guide%20noise%20correlation%20matrices%2C%20respectively%2C%20TMNR%5E2%20reduces%0Amapping%20interference%20and%20achieves%20stabilizes%20training.%20Experimental%20results%0Ademonstrate%20that%20TMNR%5E2%20significantly%20outperforms%20baseline%20methods%2C%20with%0Aaverage%20accuracy%20improvements%20of%207%25%20on%20datasets%20with%2050%25%20label%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11944v3&entry.124074799=Read"},
{"title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial\n  Networks", "author": "Ekaterina Dmitrieva and Maksim Kaledin", "abstract": "  Speech Enhancement techniques have become core technologies in mobile devices\nand voice software. Still, modern deep learning solutions often require high\namount of computational resources what makes their usage on low-resource\ndevices challenging. We present HiFi-Stream, an optimized version of recently\npublished HiFi++ model. Our experiments demonstrate that HiFi-Stream saves most\nof the qualities of the original model despite its size and computational\ncomplexity improved in comparison to the original HiFi++ making it one of the\nsmallest and fastest models available. The model is evaluated in streaming\nsetting where it demonstrates its superior performance in comparison to modern\nbaselines.\n", "link": "http://arxiv.org/abs/2503.17141v2", "date": "2025-07-23", "relevancy": 2.1164, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5309}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFi-Stream%3A%20Streaming%20Speech%20Enhancement%20with%20Generative%20Adversarial%0A%20%20Networks&body=Title%3A%20HiFi-Stream%3A%20Streaming%20Speech%20Enhancement%20with%20Generative%20Adversarial%0A%20%20Networks%0AAuthor%3A%20Ekaterina%20Dmitrieva%20and%20Maksim%20Kaledin%0AAbstract%3A%20%20%20Speech%20Enhancement%20techniques%20have%20become%20core%20technologies%20in%20mobile%20devices%0Aand%20voice%20software.%20Still%2C%20modern%20deep%20learning%20solutions%20often%20require%20high%0Aamount%20of%20computational%20resources%20what%20makes%20their%20usage%20on%20low-resource%0Adevices%20challenging.%20We%20present%20HiFi-Stream%2C%20an%20optimized%20version%20of%20recently%0Apublished%20HiFi%2B%2B%20model.%20Our%20experiments%20demonstrate%20that%20HiFi-Stream%20saves%20most%0Aof%20the%20qualities%20of%20the%20original%20model%20despite%20its%20size%20and%20computational%0Acomplexity%20improved%20in%20comparison%20to%20the%20original%20HiFi%2B%2B%20making%20it%20one%20of%20the%0Asmallest%20and%20fastest%20models%20available.%20The%20model%20is%20evaluated%20in%20streaming%0Asetting%20where%20it%20demonstrates%20its%20superior%20performance%20in%20comparison%20to%20modern%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFi-Stream%253A%2520Streaming%2520Speech%2520Enhancement%2520with%2520Generative%2520Adversarial%250A%2520%2520Networks%26entry.906535625%3DEkaterina%2520Dmitrieva%2520and%2520Maksim%2520Kaledin%26entry.1292438233%3D%2520%2520Speech%2520Enhancement%2520techniques%2520have%2520become%2520core%2520technologies%2520in%2520mobile%2520devices%250Aand%2520voice%2520software.%2520Still%252C%2520modern%2520deep%2520learning%2520solutions%2520often%2520require%2520high%250Aamount%2520of%2520computational%2520resources%2520what%2520makes%2520their%2520usage%2520on%2520low-resource%250Adevices%2520challenging.%2520We%2520present%2520HiFi-Stream%252C%2520an%2520optimized%2520version%2520of%2520recently%250Apublished%2520HiFi%252B%252B%2520model.%2520Our%2520experiments%2520demonstrate%2520that%2520HiFi-Stream%2520saves%2520most%250Aof%2520the%2520qualities%2520of%2520the%2520original%2520model%2520despite%2520its%2520size%2520and%2520computational%250Acomplexity%2520improved%2520in%2520comparison%2520to%2520the%2520original%2520HiFi%252B%252B%2520making%2520it%2520one%2520of%2520the%250Asmallest%2520and%2520fastest%2520models%2520available.%2520The%2520model%2520is%2520evaluated%2520in%2520streaming%250Asetting%2520where%2520it%2520demonstrates%2520its%2520superior%2520performance%2520in%2520comparison%2520to%2520modern%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-Stream%3A%20Streaming%20Speech%20Enhancement%20with%20Generative%20Adversarial%0A%20%20Networks&entry.906535625=Ekaterina%20Dmitrieva%20and%20Maksim%20Kaledin&entry.1292438233=%20%20Speech%20Enhancement%20techniques%20have%20become%20core%20technologies%20in%20mobile%20devices%0Aand%20voice%20software.%20Still%2C%20modern%20deep%20learning%20solutions%20often%20require%20high%0Aamount%20of%20computational%20resources%20what%20makes%20their%20usage%20on%20low-resource%0Adevices%20challenging.%20We%20present%20HiFi-Stream%2C%20an%20optimized%20version%20of%20recently%0Apublished%20HiFi%2B%2B%20model.%20Our%20experiments%20demonstrate%20that%20HiFi-Stream%20saves%20most%0Aof%20the%20qualities%20of%20the%20original%20model%20despite%20its%20size%20and%20computational%0Acomplexity%20improved%20in%20comparison%20to%20the%20original%20HiFi%2B%2B%20making%20it%20one%20of%20the%0Asmallest%20and%20fastest%20models%20available.%20The%20model%20is%20evaluated%20in%20streaming%0Asetting%20where%20it%20demonstrates%20its%20superior%20performance%20in%20comparison%20to%20modern%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17141v2&entry.124074799=Read"},
{"title": "SFUOD: Source-Free Unknown Object Detection", "author": "Keon-Hee Park and Seun-An Choe and Gyeong-Moon Park", "abstract": "  Source-free object detection adapts a detector pre-trained on a source domain\nto an unlabeled target domain without requiring access to labeled source data.\nWhile this setting is practical as it eliminates the need for the source\ndataset during domain adaptation, it operates under the restrictive assumption\nthat only pre-defined objects from the source domain exist in the target\ndomain. This closed-set setting prevents the detector from detecting undefined\nobjects. To ease this assumption, we propose Source-Free Unknown Object\nDetection (SFUOD), a novel scenario which enables the detector to not only\nrecognize known objects but also detect undefined objects as unknown objects.\nTo this end, we propose CollaPAUL (Collaborative tuning and Principal\nAxis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning\nenhances knowledge adaptation by integrating target-dependent knowledge from\nthe auxiliary encoder with source-dependent knowledge from the pre-trained\ndetector through a cross-domain attention mechanism. Additionally, principal\naxes-based unknown labeling assigns pseudo-labels to unknown objects by\nestimating objectness via principal axes projection and confidence scores from\nmodel predictions. The proposed CollaPAUL achieves state-of-the-art\nperformances on SFUOD benchmarks, and extensive experiments validate its\neffectiveness.\n", "link": "http://arxiv.org/abs/2507.17373v1", "date": "2025-07-23", "relevancy": 2.1148, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5657}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFUOD%3A%20Source-Free%20Unknown%20Object%20Detection&body=Title%3A%20SFUOD%3A%20Source-Free%20Unknown%20Object%20Detection%0AAuthor%3A%20Keon-Hee%20Park%20and%20Seun-An%20Choe%20and%20Gyeong-Moon%20Park%0AAbstract%3A%20%20%20Source-free%20object%20detection%20adapts%20a%20detector%20pre-trained%20on%20a%20source%20domain%0Ato%20an%20unlabeled%20target%20domain%20without%20requiring%20access%20to%20labeled%20source%20data.%0AWhile%20this%20setting%20is%20practical%20as%20it%20eliminates%20the%20need%20for%20the%20source%0Adataset%20during%20domain%20adaptation%2C%20it%20operates%20under%20the%20restrictive%20assumption%0Athat%20only%20pre-defined%20objects%20from%20the%20source%20domain%20exist%20in%20the%20target%0Adomain.%20This%20closed-set%20setting%20prevents%20the%20detector%20from%20detecting%20undefined%0Aobjects.%20To%20ease%20this%20assumption%2C%20we%20propose%20Source-Free%20Unknown%20Object%0ADetection%20%28SFUOD%29%2C%20a%20novel%20scenario%20which%20enables%20the%20detector%20to%20not%20only%0Arecognize%20known%20objects%20but%20also%20detect%20undefined%20objects%20as%20unknown%20objects.%0ATo%20this%20end%2C%20we%20propose%20CollaPAUL%20%28Collaborative%20tuning%20and%20Principal%0AAxis-based%20Unknown%20Labeling%29%2C%20a%20novel%20framework%20for%20SFUOD.%20Collaborative%20tuning%0Aenhances%20knowledge%20adaptation%20by%20integrating%20target-dependent%20knowledge%20from%0Athe%20auxiliary%20encoder%20with%20source-dependent%20knowledge%20from%20the%20pre-trained%0Adetector%20through%20a%20cross-domain%20attention%20mechanism.%20Additionally%2C%20principal%0Aaxes-based%20unknown%20labeling%20assigns%20pseudo-labels%20to%20unknown%20objects%20by%0Aestimating%20objectness%20via%20principal%20axes%20projection%20and%20confidence%20scores%20from%0Amodel%20predictions.%20The%20proposed%20CollaPAUL%20achieves%20state-of-the-art%0Aperformances%20on%20SFUOD%20benchmarks%2C%20and%20extensive%20experiments%20validate%20its%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFUOD%253A%2520Source-Free%2520Unknown%2520Object%2520Detection%26entry.906535625%3DKeon-Hee%2520Park%2520and%2520Seun-An%2520Choe%2520and%2520Gyeong-Moon%2520Park%26entry.1292438233%3D%2520%2520Source-free%2520object%2520detection%2520adapts%2520a%2520detector%2520pre-trained%2520on%2520a%2520source%2520domain%250Ato%2520an%2520unlabeled%2520target%2520domain%2520without%2520requiring%2520access%2520to%2520labeled%2520source%2520data.%250AWhile%2520this%2520setting%2520is%2520practical%2520as%2520it%2520eliminates%2520the%2520need%2520for%2520the%2520source%250Adataset%2520during%2520domain%2520adaptation%252C%2520it%2520operates%2520under%2520the%2520restrictive%2520assumption%250Athat%2520only%2520pre-defined%2520objects%2520from%2520the%2520source%2520domain%2520exist%2520in%2520the%2520target%250Adomain.%2520This%2520closed-set%2520setting%2520prevents%2520the%2520detector%2520from%2520detecting%2520undefined%250Aobjects.%2520To%2520ease%2520this%2520assumption%252C%2520we%2520propose%2520Source-Free%2520Unknown%2520Object%250ADetection%2520%2528SFUOD%2529%252C%2520a%2520novel%2520scenario%2520which%2520enables%2520the%2520detector%2520to%2520not%2520only%250Arecognize%2520known%2520objects%2520but%2520also%2520detect%2520undefined%2520objects%2520as%2520unknown%2520objects.%250ATo%2520this%2520end%252C%2520we%2520propose%2520CollaPAUL%2520%2528Collaborative%2520tuning%2520and%2520Principal%250AAxis-based%2520Unknown%2520Labeling%2529%252C%2520a%2520novel%2520framework%2520for%2520SFUOD.%2520Collaborative%2520tuning%250Aenhances%2520knowledge%2520adaptation%2520by%2520integrating%2520target-dependent%2520knowledge%2520from%250Athe%2520auxiliary%2520encoder%2520with%2520source-dependent%2520knowledge%2520from%2520the%2520pre-trained%250Adetector%2520through%2520a%2520cross-domain%2520attention%2520mechanism.%2520Additionally%252C%2520principal%250Aaxes-based%2520unknown%2520labeling%2520assigns%2520pseudo-labels%2520to%2520unknown%2520objects%2520by%250Aestimating%2520objectness%2520via%2520principal%2520axes%2520projection%2520and%2520confidence%2520scores%2520from%250Amodel%2520predictions.%2520The%2520proposed%2520CollaPAUL%2520achieves%2520state-of-the-art%250Aperformances%2520on%2520SFUOD%2520benchmarks%252C%2520and%2520extensive%2520experiments%2520validate%2520its%250Aeffectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFUOD%3A%20Source-Free%20Unknown%20Object%20Detection&entry.906535625=Keon-Hee%20Park%20and%20Seun-An%20Choe%20and%20Gyeong-Moon%20Park&entry.1292438233=%20%20Source-free%20object%20detection%20adapts%20a%20detector%20pre-trained%20on%20a%20source%20domain%0Ato%20an%20unlabeled%20target%20domain%20without%20requiring%20access%20to%20labeled%20source%20data.%0AWhile%20this%20setting%20is%20practical%20as%20it%20eliminates%20the%20need%20for%20the%20source%0Adataset%20during%20domain%20adaptation%2C%20it%20operates%20under%20the%20restrictive%20assumption%0Athat%20only%20pre-defined%20objects%20from%20the%20source%20domain%20exist%20in%20the%20target%0Adomain.%20This%20closed-set%20setting%20prevents%20the%20detector%20from%20detecting%20undefined%0Aobjects.%20To%20ease%20this%20assumption%2C%20we%20propose%20Source-Free%20Unknown%20Object%0ADetection%20%28SFUOD%29%2C%20a%20novel%20scenario%20which%20enables%20the%20detector%20to%20not%20only%0Arecognize%20known%20objects%20but%20also%20detect%20undefined%20objects%20as%20unknown%20objects.%0ATo%20this%20end%2C%20we%20propose%20CollaPAUL%20%28Collaborative%20tuning%20and%20Principal%0AAxis-based%20Unknown%20Labeling%29%2C%20a%20novel%20framework%20for%20SFUOD.%20Collaborative%20tuning%0Aenhances%20knowledge%20adaptation%20by%20integrating%20target-dependent%20knowledge%20from%0Athe%20auxiliary%20encoder%20with%20source-dependent%20knowledge%20from%20the%20pre-trained%0Adetector%20through%20a%20cross-domain%20attention%20mechanism.%20Additionally%2C%20principal%0Aaxes-based%20unknown%20labeling%20assigns%20pseudo-labels%20to%20unknown%20objects%20by%0Aestimating%20objectness%20via%20principal%20axes%20projection%20and%20confidence%20scores%20from%0Amodel%20predictions.%20The%20proposed%20CollaPAUL%20achieves%20state-of-the-art%0Aperformances%20on%20SFUOD%20benchmarks%2C%20and%20extensive%20experiments%20validate%20its%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17373v1&entry.124074799=Read"},
{"title": "Safety Assurance for Quadrotor Kinodynamic Motion Planning", "author": "Theodoros Tavoulareas and Marzia Cescon", "abstract": "  Autonomous drones have gained considerable attention for applications in\nreal-world scenarios, such as search and rescue, inspection, and delivery. As\ntheir use becomes ever more pervasive in civilian applications, failure to\nensure safe operation can lead to physical damage to the system, environmental\npollution, and even loss of human life. Recent work has demonstrated that\nmotion planning techniques effectively generate a collision-free trajectory\nduring navigation. However, these methods, while creating the motion plans, do\nnot inherently consider the safe operational region of the system, leading to\npotential safety constraints violation during deployment. In this paper, we\npropose a method that leverages run time safety assurance in a kinodynamic\nmotion planning scheme to satisfy the system's operational constraints. First,\nwe use a sampling-based geometric planner to determine a high-level\ncollision-free path within a user-defined space. Second, we design a low-level\nsafety assurance filter to provide safety guarantees to the control input of a\nLinear Quadratic Regulator (LQR) designed with the purpose of trajectory\ntracking. We demonstrate our proposed approach in a restricted 3D simulation\nenvironment using a model of the Crazyflie 2.0 drone.\n", "link": "http://arxiv.org/abs/2507.17679v1", "date": "2025-07-23", "relevancy": 2.1144, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Assurance%20for%20Quadrotor%20Kinodynamic%20Motion%20Planning&body=Title%3A%20Safety%20Assurance%20for%20Quadrotor%20Kinodynamic%20Motion%20Planning%0AAuthor%3A%20Theodoros%20Tavoulareas%20and%20Marzia%20Cescon%0AAbstract%3A%20%20%20Autonomous%20drones%20have%20gained%20considerable%20attention%20for%20applications%20in%0Areal-world%20scenarios%2C%20such%20as%20search%20and%20rescue%2C%20inspection%2C%20and%20delivery.%20As%0Atheir%20use%20becomes%20ever%20more%20pervasive%20in%20civilian%20applications%2C%20failure%20to%0Aensure%20safe%20operation%20can%20lead%20to%20physical%20damage%20to%20the%20system%2C%20environmental%0Apollution%2C%20and%20even%20loss%20of%20human%20life.%20Recent%20work%20has%20demonstrated%20that%0Amotion%20planning%20techniques%20effectively%20generate%20a%20collision-free%20trajectory%0Aduring%20navigation.%20However%2C%20these%20methods%2C%20while%20creating%20the%20motion%20plans%2C%20do%0Anot%20inherently%20consider%20the%20safe%20operational%20region%20of%20the%20system%2C%20leading%20to%0Apotential%20safety%20constraints%20violation%20during%20deployment.%20In%20this%20paper%2C%20we%0Apropose%20a%20method%20that%20leverages%20run%20time%20safety%20assurance%20in%20a%20kinodynamic%0Amotion%20planning%20scheme%20to%20satisfy%20the%20system%27s%20operational%20constraints.%20First%2C%0Awe%20use%20a%20sampling-based%20geometric%20planner%20to%20determine%20a%20high-level%0Acollision-free%20path%20within%20a%20user-defined%20space.%20Second%2C%20we%20design%20a%20low-level%0Asafety%20assurance%20filter%20to%20provide%20safety%20guarantees%20to%20the%20control%20input%20of%20a%0ALinear%20Quadratic%20Regulator%20%28LQR%29%20designed%20with%20the%20purpose%20of%20trajectory%0Atracking.%20We%20demonstrate%20our%20proposed%20approach%20in%20a%20restricted%203D%20simulation%0Aenvironment%20using%20a%20model%20of%20the%20Crazyflie%202.0%20drone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Assurance%2520for%2520Quadrotor%2520Kinodynamic%2520Motion%2520Planning%26entry.906535625%3DTheodoros%2520Tavoulareas%2520and%2520Marzia%2520Cescon%26entry.1292438233%3D%2520%2520Autonomous%2520drones%2520have%2520gained%2520considerable%2520attention%2520for%2520applications%2520in%250Areal-world%2520scenarios%252C%2520such%2520as%2520search%2520and%2520rescue%252C%2520inspection%252C%2520and%2520delivery.%2520As%250Atheir%2520use%2520becomes%2520ever%2520more%2520pervasive%2520in%2520civilian%2520applications%252C%2520failure%2520to%250Aensure%2520safe%2520operation%2520can%2520lead%2520to%2520physical%2520damage%2520to%2520the%2520system%252C%2520environmental%250Apollution%252C%2520and%2520even%2520loss%2520of%2520human%2520life.%2520Recent%2520work%2520has%2520demonstrated%2520that%250Amotion%2520planning%2520techniques%2520effectively%2520generate%2520a%2520collision-free%2520trajectory%250Aduring%2520navigation.%2520However%252C%2520these%2520methods%252C%2520while%2520creating%2520the%2520motion%2520plans%252C%2520do%250Anot%2520inherently%2520consider%2520the%2520safe%2520operational%2520region%2520of%2520the%2520system%252C%2520leading%2520to%250Apotential%2520safety%2520constraints%2520violation%2520during%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520method%2520that%2520leverages%2520run%2520time%2520safety%2520assurance%2520in%2520a%2520kinodynamic%250Amotion%2520planning%2520scheme%2520to%2520satisfy%2520the%2520system%2527s%2520operational%2520constraints.%2520First%252C%250Awe%2520use%2520a%2520sampling-based%2520geometric%2520planner%2520to%2520determine%2520a%2520high-level%250Acollision-free%2520path%2520within%2520a%2520user-defined%2520space.%2520Second%252C%2520we%2520design%2520a%2520low-level%250Asafety%2520assurance%2520filter%2520to%2520provide%2520safety%2520guarantees%2520to%2520the%2520control%2520input%2520of%2520a%250ALinear%2520Quadratic%2520Regulator%2520%2528LQR%2529%2520designed%2520with%2520the%2520purpose%2520of%2520trajectory%250Atracking.%2520We%2520demonstrate%2520our%2520proposed%2520approach%2520in%2520a%2520restricted%25203D%2520simulation%250Aenvironment%2520using%2520a%2520model%2520of%2520the%2520Crazyflie%25202.0%2520drone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Assurance%20for%20Quadrotor%20Kinodynamic%20Motion%20Planning&entry.906535625=Theodoros%20Tavoulareas%20and%20Marzia%20Cescon&entry.1292438233=%20%20Autonomous%20drones%20have%20gained%20considerable%20attention%20for%20applications%20in%0Areal-world%20scenarios%2C%20such%20as%20search%20and%20rescue%2C%20inspection%2C%20and%20delivery.%20As%0Atheir%20use%20becomes%20ever%20more%20pervasive%20in%20civilian%20applications%2C%20failure%20to%0Aensure%20safe%20operation%20can%20lead%20to%20physical%20damage%20to%20the%20system%2C%20environmental%0Apollution%2C%20and%20even%20loss%20of%20human%20life.%20Recent%20work%20has%20demonstrated%20that%0Amotion%20planning%20techniques%20effectively%20generate%20a%20collision-free%20trajectory%0Aduring%20navigation.%20However%2C%20these%20methods%2C%20while%20creating%20the%20motion%20plans%2C%20do%0Anot%20inherently%20consider%20the%20safe%20operational%20region%20of%20the%20system%2C%20leading%20to%0Apotential%20safety%20constraints%20violation%20during%20deployment.%20In%20this%20paper%2C%20we%0Apropose%20a%20method%20that%20leverages%20run%20time%20safety%20assurance%20in%20a%20kinodynamic%0Amotion%20planning%20scheme%20to%20satisfy%20the%20system%27s%20operational%20constraints.%20First%2C%0Awe%20use%20a%20sampling-based%20geometric%20planner%20to%20determine%20a%20high-level%0Acollision-free%20path%20within%20a%20user-defined%20space.%20Second%2C%20we%20design%20a%20low-level%0Asafety%20assurance%20filter%20to%20provide%20safety%20guarantees%20to%20the%20control%20input%20of%20a%0ALinear%20Quadratic%20Regulator%20%28LQR%29%20designed%20with%20the%20purpose%20of%20trajectory%0Atracking.%20We%20demonstrate%20our%20proposed%20approach%20in%20a%20restricted%203D%20simulation%0Aenvironment%20using%20a%20model%20of%20the%20Crazyflie%202.0%20drone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17679v1&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation on Quantization Techniques for Large Language\n  Models", "author": "Yutong Liu and Cairong Zhao and Guosheng Hu", "abstract": "  For large language models (LLMs), post-training quantization (PTQ) can\nsignificantly reduce memory footprint and computational overhead. Model\nquantization is a rapidly evolving research field. Though many papers have\nreported breakthrough performance, they may not conduct experiments on the same\nground since one quantization method usually contains multiple components. In\naddition, analyzing the theoretical connections among existing methods is\ncrucial for in-depth understanding. To bridge these gaps, we conduct an\nextensive review of state-of-the-art methods and perform comprehensive\nevaluations on the same ground to ensure fair comparisons. To our knowledge,\nthis fair and extensive investigation remains critically important yet\nunderexplored. To better understand the theoretical connections, we decouple\nthe published quantization methods into two steps: pre-quantization\ntransformation and quantization error mitigation. We define the former as a\npreprocessing step applied before quantization to reduce the impact of\noutliers, making the data distribution flatter and more suitable for\nquantization. Quantization error mitigation involves techniques that offset the\nerrors introduced during quantization, thereby enhancing model performance. We\nevaluate and analyze the impact of different components of quantization\nmethods. Additionally, we analyze and evaluate the latest MXFP4 data format and\nits performance. Our experimental results demonstrate that optimized rotation\nand scaling yield the best performance for pre-quantization transformation, and\ncombining low-rank compensation with GPTQ occasionally outperforms using GPTQ\nalone for quantization error mitigation. Furthermore, we explore the potential\nof the latest MXFP4 quantization and reveal that the optimal pre-quantization\ntransformation strategy for INT4 does not generalize well to MXFP4, inspiring\nfurther investigation.\n", "link": "http://arxiv.org/abs/2507.17417v1", "date": "2025-07-23", "relevancy": 2.1115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20on%20Quantization%20Techniques%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20A%20Comprehensive%20Evaluation%20on%20Quantization%20Techniques%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yutong%20Liu%20and%20Cairong%20Zhao%20and%20Guosheng%20Hu%0AAbstract%3A%20%20%20For%20large%20language%20models%20%28LLMs%29%2C%20post-training%20quantization%20%28PTQ%29%20can%0Asignificantly%20reduce%20memory%20footprint%20and%20computational%20overhead.%20Model%0Aquantization%20is%20a%20rapidly%20evolving%20research%20field.%20Though%20many%20papers%20have%0Areported%20breakthrough%20performance%2C%20they%20may%20not%20conduct%20experiments%20on%20the%20same%0Aground%20since%20one%20quantization%20method%20usually%20contains%20multiple%20components.%20In%0Aaddition%2C%20analyzing%20the%20theoretical%20connections%20among%20existing%20methods%20is%0Acrucial%20for%20in-depth%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20conduct%20an%0Aextensive%20review%20of%20state-of-the-art%20methods%20and%20perform%20comprehensive%0Aevaluations%20on%20the%20same%20ground%20to%20ensure%20fair%20comparisons.%20To%20our%20knowledge%2C%0Athis%20fair%20and%20extensive%20investigation%20remains%20critically%20important%20yet%0Aunderexplored.%20To%20better%20understand%20the%20theoretical%20connections%2C%20we%20decouple%0Athe%20published%20quantization%20methods%20into%20two%20steps%3A%20pre-quantization%0Atransformation%20and%20quantization%20error%20mitigation.%20We%20define%20the%20former%20as%20a%0Apreprocessing%20step%20applied%20before%20quantization%20to%20reduce%20the%20impact%20of%0Aoutliers%2C%20making%20the%20data%20distribution%20flatter%20and%20more%20suitable%20for%0Aquantization.%20Quantization%20error%20mitigation%20involves%20techniques%20that%20offset%20the%0Aerrors%20introduced%20during%20quantization%2C%20thereby%20enhancing%20model%20performance.%20We%0Aevaluate%20and%20analyze%20the%20impact%20of%20different%20components%20of%20quantization%0Amethods.%20Additionally%2C%20we%20analyze%20and%20evaluate%20the%20latest%20MXFP4%20data%20format%20and%0Aits%20performance.%20Our%20experimental%20results%20demonstrate%20that%20optimized%20rotation%0Aand%20scaling%20yield%20the%20best%20performance%20for%20pre-quantization%20transformation%2C%20and%0Acombining%20low-rank%20compensation%20with%20GPTQ%20occasionally%20outperforms%20using%20GPTQ%0Aalone%20for%20quantization%20error%20mitigation.%20Furthermore%2C%20we%20explore%20the%20potential%0Aof%20the%20latest%20MXFP4%20quantization%20and%20reveal%20that%20the%20optimal%20pre-quantization%0Atransformation%20strategy%20for%20INT4%20does%20not%20generalize%20well%20to%20MXFP4%2C%20inspiring%0Afurther%20investigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520on%2520Quantization%2520Techniques%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYutong%2520Liu%2520and%2520Cairong%2520Zhao%2520and%2520Guosheng%2520Hu%26entry.1292438233%3D%2520%2520For%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520post-training%2520quantization%2520%2528PTQ%2529%2520can%250Asignificantly%2520reduce%2520memory%2520footprint%2520and%2520computational%2520overhead.%2520Model%250Aquantization%2520is%2520a%2520rapidly%2520evolving%2520research%2520field.%2520Though%2520many%2520papers%2520have%250Areported%2520breakthrough%2520performance%252C%2520they%2520may%2520not%2520conduct%2520experiments%2520on%2520the%2520same%250Aground%2520since%2520one%2520quantization%2520method%2520usually%2520contains%2520multiple%2520components.%2520In%250Aaddition%252C%2520analyzing%2520the%2520theoretical%2520connections%2520among%2520existing%2520methods%2520is%250Acrucial%2520for%2520in-depth%2520understanding.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520conduct%2520an%250Aextensive%2520review%2520of%2520state-of-the-art%2520methods%2520and%2520perform%2520comprehensive%250Aevaluations%2520on%2520the%2520same%2520ground%2520to%2520ensure%2520fair%2520comparisons.%2520To%2520our%2520knowledge%252C%250Athis%2520fair%2520and%2520extensive%2520investigation%2520remains%2520critically%2520important%2520yet%250Aunderexplored.%2520To%2520better%2520understand%2520the%2520theoretical%2520connections%252C%2520we%2520decouple%250Athe%2520published%2520quantization%2520methods%2520into%2520two%2520steps%253A%2520pre-quantization%250Atransformation%2520and%2520quantization%2520error%2520mitigation.%2520We%2520define%2520the%2520former%2520as%2520a%250Apreprocessing%2520step%2520applied%2520before%2520quantization%2520to%2520reduce%2520the%2520impact%2520of%250Aoutliers%252C%2520making%2520the%2520data%2520distribution%2520flatter%2520and%2520more%2520suitable%2520for%250Aquantization.%2520Quantization%2520error%2520mitigation%2520involves%2520techniques%2520that%2520offset%2520the%250Aerrors%2520introduced%2520during%2520quantization%252C%2520thereby%2520enhancing%2520model%2520performance.%2520We%250Aevaluate%2520and%2520analyze%2520the%2520impact%2520of%2520different%2520components%2520of%2520quantization%250Amethods.%2520Additionally%252C%2520we%2520analyze%2520and%2520evaluate%2520the%2520latest%2520MXFP4%2520data%2520format%2520and%250Aits%2520performance.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520optimized%2520rotation%250Aand%2520scaling%2520yield%2520the%2520best%2520performance%2520for%2520pre-quantization%2520transformation%252C%2520and%250Acombining%2520low-rank%2520compensation%2520with%2520GPTQ%2520occasionally%2520outperforms%2520using%2520GPTQ%250Aalone%2520for%2520quantization%2520error%2520mitigation.%2520Furthermore%252C%2520we%2520explore%2520the%2520potential%250Aof%2520the%2520latest%2520MXFP4%2520quantization%2520and%2520reveal%2520that%2520the%2520optimal%2520pre-quantization%250Atransformation%2520strategy%2520for%2520INT4%2520does%2520not%2520generalize%2520well%2520to%2520MXFP4%252C%2520inspiring%250Afurther%2520investigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20on%20Quantization%20Techniques%20for%20Large%20Language%0A%20%20Models&entry.906535625=Yutong%20Liu%20and%20Cairong%20Zhao%20and%20Guosheng%20Hu&entry.1292438233=%20%20For%20large%20language%20models%20%28LLMs%29%2C%20post-training%20quantization%20%28PTQ%29%20can%0Asignificantly%20reduce%20memory%20footprint%20and%20computational%20overhead.%20Model%0Aquantization%20is%20a%20rapidly%20evolving%20research%20field.%20Though%20many%20papers%20have%0Areported%20breakthrough%20performance%2C%20they%20may%20not%20conduct%20experiments%20on%20the%20same%0Aground%20since%20one%20quantization%20method%20usually%20contains%20multiple%20components.%20In%0Aaddition%2C%20analyzing%20the%20theoretical%20connections%20among%20existing%20methods%20is%0Acrucial%20for%20in-depth%20understanding.%20To%20bridge%20these%20gaps%2C%20we%20conduct%20an%0Aextensive%20review%20of%20state-of-the-art%20methods%20and%20perform%20comprehensive%0Aevaluations%20on%20the%20same%20ground%20to%20ensure%20fair%20comparisons.%20To%20our%20knowledge%2C%0Athis%20fair%20and%20extensive%20investigation%20remains%20critically%20important%20yet%0Aunderexplored.%20To%20better%20understand%20the%20theoretical%20connections%2C%20we%20decouple%0Athe%20published%20quantization%20methods%20into%20two%20steps%3A%20pre-quantization%0Atransformation%20and%20quantization%20error%20mitigation.%20We%20define%20the%20former%20as%20a%0Apreprocessing%20step%20applied%20before%20quantization%20to%20reduce%20the%20impact%20of%0Aoutliers%2C%20making%20the%20data%20distribution%20flatter%20and%20more%20suitable%20for%0Aquantization.%20Quantization%20error%20mitigation%20involves%20techniques%20that%20offset%20the%0Aerrors%20introduced%20during%20quantization%2C%20thereby%20enhancing%20model%20performance.%20We%0Aevaluate%20and%20analyze%20the%20impact%20of%20different%20components%20of%20quantization%0Amethods.%20Additionally%2C%20we%20analyze%20and%20evaluate%20the%20latest%20MXFP4%20data%20format%20and%0Aits%20performance.%20Our%20experimental%20results%20demonstrate%20that%20optimized%20rotation%0Aand%20scaling%20yield%20the%20best%20performance%20for%20pre-quantization%20transformation%2C%20and%0Acombining%20low-rank%20compensation%20with%20GPTQ%20occasionally%20outperforms%20using%20GPTQ%0Aalone%20for%20quantization%20error%20mitigation.%20Furthermore%2C%20we%20explore%20the%20potential%0Aof%20the%20latest%20MXFP4%20quantization%20and%20reveal%20that%20the%20optimal%20pre-quantization%0Atransformation%20strategy%20for%20INT4%20does%20not%20generalize%20well%20to%20MXFP4%2C%20inspiring%0Afurther%20investigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17417v1&entry.124074799=Read"},
{"title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution\n  and Bottleneck Mamba", "author": "Yuhang Wang and Jun Li and Zhijian Wu and Jifeng Shen and Jianhua Xu and Wankou Yang", "abstract": "  Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba.\n", "link": "http://arxiv.org/abs/2506.08735v3", "date": "2025-07-23", "relevancy": 2.1008, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.511}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InceptionMamba%3A%20An%20Efficient%20Hybrid%20Network%20with%20Large%20Band%20Convolution%0A%20%20and%20Bottleneck%20Mamba&body=Title%3A%20InceptionMamba%3A%20An%20Efficient%20Hybrid%20Network%20with%20Large%20Band%20Convolution%0A%20%20and%20Bottleneck%20Mamba%0AAuthor%3A%20Yuhang%20Wang%20and%20Jun%20Li%20and%20Zhijian%20Wu%20and%20Jifeng%20Shen%20and%20Jianhua%20Xu%20and%20Wankou%20Yang%0AAbstract%3A%20%20%20Within%20the%20family%20of%20convolutional%20neural%20networks%2C%20InceptionNeXt%20has%20shown%0Aexcellent%20competitiveness%20in%20image%20classification%20and%20a%20number%20of%20downstream%0Atasks.%20Built%20on%20parallel%20one-dimensional%20strip%20convolutions%2C%20however%2C%20it%0Asuffers%20from%20limited%20ability%20of%20capturing%20spatial%20dependencies%20along%20different%0Adimensions%20and%20fails%20to%20fully%20explore%20spatial%20modeling%20in%20local%20neighborhood.%0ABesides%2C%20inherent%20locality%20constraints%20of%20convolution%20operations%20are%0Adetrimental%20to%20effective%20global%20context%20modeling.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20novel%20backbone%20architecture%20termed%20InceptionMamba%20in%0Athis%20study.%20More%20specifically%2C%20the%20traditional%20one-dimensional%20strip%0Aconvolutions%20are%20replaced%20by%20orthogonal%20band%20convolutions%20in%20our%20InceptionMamba%0Ato%20achieve%20cohesive%20spatial%20modeling.%20Furthermore%2C%20global%20contextual%20modeling%0Acan%20be%20achieved%20via%20a%20bottleneck%20Mamba%20module%2C%20facilitating%20enhanced%0Across-channel%20information%20fusion%20and%20enlarged%20receptive%20field.%20Extensive%0Aevaluations%20on%20classification%20and%20various%20downstream%20tasks%20demonstrate%20that%20the%0Aproposed%20InceptionMamba%20achieves%20state-of-the-art%20performance%20with%20superior%0Aparameter%20and%20computational%20efficiency.%20The%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Wake1021/InceptionMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInceptionMamba%253A%2520An%2520Efficient%2520Hybrid%2520Network%2520with%2520Large%2520Band%2520Convolution%250A%2520%2520and%2520Bottleneck%2520Mamba%26entry.906535625%3DYuhang%2520Wang%2520and%2520Jun%2520Li%2520and%2520Zhijian%2520Wu%2520and%2520Jifeng%2520Shen%2520and%2520Jianhua%2520Xu%2520and%2520Wankou%2520Yang%26entry.1292438233%3D%2520%2520Within%2520the%2520family%2520of%2520convolutional%2520neural%2520networks%252C%2520InceptionNeXt%2520has%2520shown%250Aexcellent%2520competitiveness%2520in%2520image%2520classification%2520and%2520a%2520number%2520of%2520downstream%250Atasks.%2520Built%2520on%2520parallel%2520one-dimensional%2520strip%2520convolutions%252C%2520however%252C%2520it%250Asuffers%2520from%2520limited%2520ability%2520of%2520capturing%2520spatial%2520dependencies%2520along%2520different%250Adimensions%2520and%2520fails%2520to%2520fully%2520explore%2520spatial%2520modeling%2520in%2520local%2520neighborhood.%250ABesides%252C%2520inherent%2520locality%2520constraints%2520of%2520convolution%2520operations%2520are%250Adetrimental%2520to%2520effective%2520global%2520context%2520modeling.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520backbone%2520architecture%2520termed%2520InceptionMamba%2520in%250Athis%2520study.%2520More%2520specifically%252C%2520the%2520traditional%2520one-dimensional%2520strip%250Aconvolutions%2520are%2520replaced%2520by%2520orthogonal%2520band%2520convolutions%2520in%2520our%2520InceptionMamba%250Ato%2520achieve%2520cohesive%2520spatial%2520modeling.%2520Furthermore%252C%2520global%2520contextual%2520modeling%250Acan%2520be%2520achieved%2520via%2520a%2520bottleneck%2520Mamba%2520module%252C%2520facilitating%2520enhanced%250Across-channel%2520information%2520fusion%2520and%2520enlarged%2520receptive%2520field.%2520Extensive%250Aevaluations%2520on%2520classification%2520and%2520various%2520downstream%2520tasks%2520demonstrate%2520that%2520the%250Aproposed%2520InceptionMamba%2520achieves%2520state-of-the-art%2520performance%2520with%2520superior%250Aparameter%2520and%2520computational%2520efficiency.%2520The%2520source%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Wake1021/InceptionMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InceptionMamba%3A%20An%20Efficient%20Hybrid%20Network%20with%20Large%20Band%20Convolution%0A%20%20and%20Bottleneck%20Mamba&entry.906535625=Yuhang%20Wang%20and%20Jun%20Li%20and%20Zhijian%20Wu%20and%20Jifeng%20Shen%20and%20Jianhua%20Xu%20and%20Wankou%20Yang&entry.1292438233=%20%20Within%20the%20family%20of%20convolutional%20neural%20networks%2C%20InceptionNeXt%20has%20shown%0Aexcellent%20competitiveness%20in%20image%20classification%20and%20a%20number%20of%20downstream%0Atasks.%20Built%20on%20parallel%20one-dimensional%20strip%20convolutions%2C%20however%2C%20it%0Asuffers%20from%20limited%20ability%20of%20capturing%20spatial%20dependencies%20along%20different%0Adimensions%20and%20fails%20to%20fully%20explore%20spatial%20modeling%20in%20local%20neighborhood.%0ABesides%2C%20inherent%20locality%20constraints%20of%20convolution%20operations%20are%0Adetrimental%20to%20effective%20global%20context%20modeling.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20novel%20backbone%20architecture%20termed%20InceptionMamba%20in%0Athis%20study.%20More%20specifically%2C%20the%20traditional%20one-dimensional%20strip%0Aconvolutions%20are%20replaced%20by%20orthogonal%20band%20convolutions%20in%20our%20InceptionMamba%0Ato%20achieve%20cohesive%20spatial%20modeling.%20Furthermore%2C%20global%20contextual%20modeling%0Acan%20be%20achieved%20via%20a%20bottleneck%20Mamba%20module%2C%20facilitating%20enhanced%0Across-channel%20information%20fusion%20and%20enlarged%20receptive%20field.%20Extensive%0Aevaluations%20on%20classification%20and%20various%20downstream%20tasks%20demonstrate%20that%20the%0Aproposed%20InceptionMamba%20achieves%20state-of-the-art%20performance%20with%20superior%0Aparameter%20and%20computational%20efficiency.%20The%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Wake1021/InceptionMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08735v3&entry.124074799=Read"},
{"title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation", "author": "Shuai Yang and Hao Li and Yilun Chen and Bin Wang and Yang Tian and Tai Wang and Hanqing Wang and Feng Zhao and Yiyi Liao and Jiangmiao Pang", "abstract": "  To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.\n", "link": "http://arxiv.org/abs/2507.17520v1", "date": "2025-07-23", "relevancy": 2.0986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructVLA%3A%20Vision-Language-Action%20Instruction%20Tuning%20from%0A%20%20Understanding%20to%20Manipulation&body=Title%3A%20InstructVLA%3A%20Vision-Language-Action%20Instruction%20Tuning%20from%0A%20%20Understanding%20to%20Manipulation%0AAuthor%3A%20Shuai%20Yang%20and%20Hao%20Li%20and%20Yilun%20Chen%20and%20Bin%20Wang%20and%20Yang%20Tian%20and%20Tai%20Wang%20and%20Hanqing%20Wang%20and%20Feng%20Zhao%20and%20Yiyi%20Liao%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20To%20operate%20effectively%20in%20the%20real%20world%2C%20robots%20must%20integrate%20multimodal%0Areasoning%20with%20precise%20action%20generation.%20However%2C%20existing%0Avision-language-action%20%28VLA%29%20models%20often%20sacrifice%20one%20for%20the%20other%2C%20narrow%0Atheir%20abilities%20to%20task-specific%20manipulation%20data%2C%20and%20suffer%20catastrophic%0Aforgetting%20of%20pre-trained%20vision-language%20capabilities.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20InstructVLA%2C%20an%20end-to-end%20VLA%20model%20that%20preserves%20the%20flexible%0Areasoning%20of%20large%20vision-language%20models%20%28VLMs%29%20while%20delivering%20leading%0Amanipulation%20performance.%20InstructVLA%20introduces%20a%20novel%20training%20paradigm%2C%0AVision-Language-Action%20Instruction%20Tuning%20%28VLA-IT%29%2C%20which%20employs%20multimodal%0Atraining%20with%20mixture-of-experts%20adaptation%20to%20jointly%20optimize%20textual%0Areasoning%20and%20action%20generation%20on%20both%20standard%20VLM%20corpora%20and%20a%20curated%0A650K-sample%20VLA-IT%20dataset.%20On%20in-domain%20SimplerEnv%20tasks%2C%20InstructVLA%20achieves%0A30.5%25%20improvement%20over%20SpatialVLA.%20To%20evaluate%20generalization%2C%20we%20introduce%0ASimplerEnv-Instruct%2C%20an%2080-task%20benchmark%20requiring%20closed-loop%20control%20and%0Ahigh-level%20instruction%20understanding%2C%20where%20it%20outperforms%20a%20fine-tuned%20OpenVLA%0Aby%2092%25%20and%20an%20action%20expert%20aided%20by%20GPT-4o%20by%2029%25.%20Additionally%2C%20InstructVLA%0Asurpasses%20baseline%20VLMs%20on%20multimodal%20tasks%20and%20exhibits%20inference-time%20scaling%0Aby%20leveraging%20textual%20reasoning%20to%20boost%20manipulation%20performance%20in%20both%0Asimulated%20and%20real-world%20settings.%20These%20results%20demonstrate%20InstructVLA%27s%0Apotential%20for%20bridging%20intuitive%20and%20steerable%20human-robot%20interaction%20with%0Aefficient%20policy%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructVLA%253A%2520Vision-Language-Action%2520Instruction%2520Tuning%2520from%250A%2520%2520Understanding%2520to%2520Manipulation%26entry.906535625%3DShuai%2520Yang%2520and%2520Hao%2520Li%2520and%2520Yilun%2520Chen%2520and%2520Bin%2520Wang%2520and%2520Yang%2520Tian%2520and%2520Tai%2520Wang%2520and%2520Hanqing%2520Wang%2520and%2520Feng%2520Zhao%2520and%2520Yiyi%2520Liao%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520To%2520operate%2520effectively%2520in%2520the%2520real%2520world%252C%2520robots%2520must%2520integrate%2520multimodal%250Areasoning%2520with%2520precise%2520action%2520generation.%2520However%252C%2520existing%250Avision-language-action%2520%2528VLA%2529%2520models%2520often%2520sacrifice%2520one%2520for%2520the%2520other%252C%2520narrow%250Atheir%2520abilities%2520to%2520task-specific%2520manipulation%2520data%252C%2520and%2520suffer%2520catastrophic%250Aforgetting%2520of%2520pre-trained%2520vision-language%2520capabilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520InstructVLA%252C%2520an%2520end-to-end%2520VLA%2520model%2520that%2520preserves%2520the%2520flexible%250Areasoning%2520of%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520while%2520delivering%2520leading%250Amanipulation%2520performance.%2520InstructVLA%2520introduces%2520a%2520novel%2520training%2520paradigm%252C%250AVision-Language-Action%2520Instruction%2520Tuning%2520%2528VLA-IT%2529%252C%2520which%2520employs%2520multimodal%250Atraining%2520with%2520mixture-of-experts%2520adaptation%2520to%2520jointly%2520optimize%2520textual%250Areasoning%2520and%2520action%2520generation%2520on%2520both%2520standard%2520VLM%2520corpora%2520and%2520a%2520curated%250A650K-sample%2520VLA-IT%2520dataset.%2520On%2520in-domain%2520SimplerEnv%2520tasks%252C%2520InstructVLA%2520achieves%250A30.5%2525%2520improvement%2520over%2520SpatialVLA.%2520To%2520evaluate%2520generalization%252C%2520we%2520introduce%250ASimplerEnv-Instruct%252C%2520an%252080-task%2520benchmark%2520requiring%2520closed-loop%2520control%2520and%250Ahigh-level%2520instruction%2520understanding%252C%2520where%2520it%2520outperforms%2520a%2520fine-tuned%2520OpenVLA%250Aby%252092%2525%2520and%2520an%2520action%2520expert%2520aided%2520by%2520GPT-4o%2520by%252029%2525.%2520Additionally%252C%2520InstructVLA%250Asurpasses%2520baseline%2520VLMs%2520on%2520multimodal%2520tasks%2520and%2520exhibits%2520inference-time%2520scaling%250Aby%2520leveraging%2520textual%2520reasoning%2520to%2520boost%2520manipulation%2520performance%2520in%2520both%250Asimulated%2520and%2520real-world%2520settings.%2520These%2520results%2520demonstrate%2520InstructVLA%2527s%250Apotential%2520for%2520bridging%2520intuitive%2520and%2520steerable%2520human-robot%2520interaction%2520with%250Aefficient%2520policy%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructVLA%3A%20Vision-Language-Action%20Instruction%20Tuning%20from%0A%20%20Understanding%20to%20Manipulation&entry.906535625=Shuai%20Yang%20and%20Hao%20Li%20and%20Yilun%20Chen%20and%20Bin%20Wang%20and%20Yang%20Tian%20and%20Tai%20Wang%20and%20Hanqing%20Wang%20and%20Feng%20Zhao%20and%20Yiyi%20Liao%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20To%20operate%20effectively%20in%20the%20real%20world%2C%20robots%20must%20integrate%20multimodal%0Areasoning%20with%20precise%20action%20generation.%20However%2C%20existing%0Avision-language-action%20%28VLA%29%20models%20often%20sacrifice%20one%20for%20the%20other%2C%20narrow%0Atheir%20abilities%20to%20task-specific%20manipulation%20data%2C%20and%20suffer%20catastrophic%0Aforgetting%20of%20pre-trained%20vision-language%20capabilities.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20InstructVLA%2C%20an%20end-to-end%20VLA%20model%20that%20preserves%20the%20flexible%0Areasoning%20of%20large%20vision-language%20models%20%28VLMs%29%20while%20delivering%20leading%0Amanipulation%20performance.%20InstructVLA%20introduces%20a%20novel%20training%20paradigm%2C%0AVision-Language-Action%20Instruction%20Tuning%20%28VLA-IT%29%2C%20which%20employs%20multimodal%0Atraining%20with%20mixture-of-experts%20adaptation%20to%20jointly%20optimize%20textual%0Areasoning%20and%20action%20generation%20on%20both%20standard%20VLM%20corpora%20and%20a%20curated%0A650K-sample%20VLA-IT%20dataset.%20On%20in-domain%20SimplerEnv%20tasks%2C%20InstructVLA%20achieves%0A30.5%25%20improvement%20over%20SpatialVLA.%20To%20evaluate%20generalization%2C%20we%20introduce%0ASimplerEnv-Instruct%2C%20an%2080-task%20benchmark%20requiring%20closed-loop%20control%20and%0Ahigh-level%20instruction%20understanding%2C%20where%20it%20outperforms%20a%20fine-tuned%20OpenVLA%0Aby%2092%25%20and%20an%20action%20expert%20aided%20by%20GPT-4o%20by%2029%25.%20Additionally%2C%20InstructVLA%0Asurpasses%20baseline%20VLMs%20on%20multimodal%20tasks%20and%20exhibits%20inference-time%20scaling%0Aby%20leveraging%20textual%20reasoning%20to%20boost%20manipulation%20performance%20in%20both%0Asimulated%20and%20real-world%20settings.%20These%20results%20demonstrate%20InstructVLA%27s%0Apotential%20for%20bridging%20intuitive%20and%20steerable%20human-robot%20interaction%20with%0Aefficient%20policy%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17520v1&entry.124074799=Read"},
{"title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "author": "Xianbiao Qi and Marco Chen and Wenjie Xiao and Jiaquan Ye and Yelin He and Chun-Guang Li and Zhouchen Lin", "abstract": "  Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.\n", "link": "http://arxiv.org/abs/2507.17501v1", "date": "2025-07-23", "relevancy": 2.0931, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5637}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5363}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNT%3A%20a%20Deeply%20Normalized%20Transformer%20that%20can%20be%20trained%20by%20Momentum%20SGD&body=Title%3A%20DNT%3A%20a%20Deeply%20Normalized%20Transformer%20that%20can%20be%20trained%20by%20Momentum%20SGD%0AAuthor%3A%20Xianbiao%20Qi%20and%20Marco%20Chen%20and%20Wenjie%20Xiao%20and%20Jiaquan%20Ye%20and%20Yelin%20He%20and%20Chun-Guang%20Li%20and%20Zhouchen%20Lin%0AAbstract%3A%20%20%20Transformers%20have%20become%20the%20de%20facto%20backbone%20of%20modern%20deep%20learning%2C%20yet%0Atheir%20training%20typically%20demands%20an%20advanced%20optimizer%20with%20adaptive%20learning%0Arate%20like%20AdamW%2C%20rather%20than%20a%20momentum%20SGDW%20%28mSGDW%29.%20Previous%20works%20show%20that%0Ait%20is%20mainly%20due%20to%20a%20heavy-tailed%20distribution%20of%20the%20gradients.%20In%20this%0Apaper%2C%20we%20introduce%20a%20Deeply%20Normalized%20Transformer%20%28DNT%29%2C%20which%20is%0Ameticulously%20engineered%20to%20overcome%20this%20limitation%20enabling%20seamless%20training%0Awith%20vanilla%20mSGDW%20while%20yielding%20comparable%20performance%20to%20the%20Transformers%0Atrained%20via%20AdamW.%20To%20be%20specific%2C%20in%20DNT%2C%20we%20strategically%20integrate%0Anormalization%20techniques%20at%20proper%20positions%20in%20the%20Transformers%20to%20effectively%0Amodulate%20the%20Jacobian%20matrices%20of%20each%20layer%2C%20balance%20the%20influence%20of%20weights%2C%0Aactivations%2C%20and%20their%20interactions%2C%20and%20thus%20enable%20the%20distributions%20of%0Agradients%20concentrated.%20We%20provide%20both%20theoretical%20justifications%20of%20the%0Anormalization%20technique%20used%20in%20our%20DNT%20and%20extensive%20empirical%20evaluation%20on%0Atwo%20popular%20Transformer%20architectures%20to%20validate%20that%3A%20a%29%20DNT%20outperforms%20its%0Acounterparts%20%28%5Cie%2C%20ViT%20and%20GPT%29%2C%20and%20b%29%20DNT%20can%20be%20effectively%20trained%20with%0Avanilla%20mSGDW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNT%253A%2520a%2520Deeply%2520Normalized%2520Transformer%2520that%2520can%2520be%2520trained%2520by%2520Momentum%2520SGD%26entry.906535625%3DXianbiao%2520Qi%2520and%2520Marco%2520Chen%2520and%2520Wenjie%2520Xiao%2520and%2520Jiaquan%2520Ye%2520and%2520Yelin%2520He%2520and%2520Chun-Guang%2520Li%2520and%2520Zhouchen%2520Lin%26entry.1292438233%3D%2520%2520Transformers%2520have%2520become%2520the%2520de%2520facto%2520backbone%2520of%2520modern%2520deep%2520learning%252C%2520yet%250Atheir%2520training%2520typically%2520demands%2520an%2520advanced%2520optimizer%2520with%2520adaptive%2520learning%250Arate%2520like%2520AdamW%252C%2520rather%2520than%2520a%2520momentum%2520SGDW%2520%2528mSGDW%2529.%2520Previous%2520works%2520show%2520that%250Ait%2520is%2520mainly%2520due%2520to%2520a%2520heavy-tailed%2520distribution%2520of%2520the%2520gradients.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520Deeply%2520Normalized%2520Transformer%2520%2528DNT%2529%252C%2520which%2520is%250Ameticulously%2520engineered%2520to%2520overcome%2520this%2520limitation%2520enabling%2520seamless%2520training%250Awith%2520vanilla%2520mSGDW%2520while%2520yielding%2520comparable%2520performance%2520to%2520the%2520Transformers%250Atrained%2520via%2520AdamW.%2520To%2520be%2520specific%252C%2520in%2520DNT%252C%2520we%2520strategically%2520integrate%250Anormalization%2520techniques%2520at%2520proper%2520positions%2520in%2520the%2520Transformers%2520to%2520effectively%250Amodulate%2520the%2520Jacobian%2520matrices%2520of%2520each%2520layer%252C%2520balance%2520the%2520influence%2520of%2520weights%252C%250Aactivations%252C%2520and%2520their%2520interactions%252C%2520and%2520thus%2520enable%2520the%2520distributions%2520of%250Agradients%2520concentrated.%2520We%2520provide%2520both%2520theoretical%2520justifications%2520of%2520the%250Anormalization%2520technique%2520used%2520in%2520our%2520DNT%2520and%2520extensive%2520empirical%2520evaluation%2520on%250Atwo%2520popular%2520Transformer%2520architectures%2520to%2520validate%2520that%253A%2520a%2529%2520DNT%2520outperforms%2520its%250Acounterparts%2520%2528%255Cie%252C%2520ViT%2520and%2520GPT%2529%252C%2520and%2520b%2529%2520DNT%2520can%2520be%2520effectively%2520trained%2520with%250Avanilla%2520mSGDW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNT%3A%20a%20Deeply%20Normalized%20Transformer%20that%20can%20be%20trained%20by%20Momentum%20SGD&entry.906535625=Xianbiao%20Qi%20and%20Marco%20Chen%20and%20Wenjie%20Xiao%20and%20Jiaquan%20Ye%20and%20Yelin%20He%20and%20Chun-Guang%20Li%20and%20Zhouchen%20Lin&entry.1292438233=%20%20Transformers%20have%20become%20the%20de%20facto%20backbone%20of%20modern%20deep%20learning%2C%20yet%0Atheir%20training%20typically%20demands%20an%20advanced%20optimizer%20with%20adaptive%20learning%0Arate%20like%20AdamW%2C%20rather%20than%20a%20momentum%20SGDW%20%28mSGDW%29.%20Previous%20works%20show%20that%0Ait%20is%20mainly%20due%20to%20a%20heavy-tailed%20distribution%20of%20the%20gradients.%20In%20this%0Apaper%2C%20we%20introduce%20a%20Deeply%20Normalized%20Transformer%20%28DNT%29%2C%20which%20is%0Ameticulously%20engineered%20to%20overcome%20this%20limitation%20enabling%20seamless%20training%0Awith%20vanilla%20mSGDW%20while%20yielding%20comparable%20performance%20to%20the%20Transformers%0Atrained%20via%20AdamW.%20To%20be%20specific%2C%20in%20DNT%2C%20we%20strategically%20integrate%0Anormalization%20techniques%20at%20proper%20positions%20in%20the%20Transformers%20to%20effectively%0Amodulate%20the%20Jacobian%20matrices%20of%20each%20layer%2C%20balance%20the%20influence%20of%20weights%2C%0Aactivations%2C%20and%20their%20interactions%2C%20and%20thus%20enable%20the%20distributions%20of%0Agradients%20concentrated.%20We%20provide%20both%20theoretical%20justifications%20of%20the%0Anormalization%20technique%20used%20in%20our%20DNT%20and%20extensive%20empirical%20evaluation%20on%0Atwo%20popular%20Transformer%20architectures%20to%20validate%20that%3A%20a%29%20DNT%20outperforms%20its%0Acounterparts%20%28%5Cie%2C%20ViT%20and%20GPT%29%2C%20and%20b%29%20DNT%20can%20be%20effectively%20trained%20with%0Avanilla%20mSGDW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17501v1&entry.124074799=Read"},
{"title": "Toward a Lightweight and Robust Design for Caching", "author": "Peng Chen and Hailiang Zhao and Jiaji Zhang and Xueyan Tang and Yixuan Wang and Shuiguang Deng", "abstract": "  The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.\n", "link": "http://arxiv.org/abs/2507.16242v2", "date": "2025-07-23", "relevancy": 2.0921, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4273}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Lightweight%20and%20Robust%20Design%20for%20Caching&body=Title%3A%20Toward%20a%20Lightweight%20and%20Robust%20Design%20for%20Caching%0AAuthor%3A%20Peng%20Chen%20and%20Hailiang%20Zhao%20and%20Jiaji%20Zhang%20and%20Xueyan%20Tang%20and%20Yixuan%20Wang%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20The%20online%20caching%20problem%20aims%20to%20minimize%20cache%20misses%20when%20serving%20a%0Asequence%20of%20requests%20under%20a%20limited%20cache%20size.%20While%20naive%20learning-augmented%0Acaching%20algorithms%20achieve%20ideal%20%241%24-consistency%2C%20they%20lack%20robustness%0Aguarantees.%20Existing%20robustification%20methods%20either%20sacrifice%20%241%24-consistency%0Aor%20introduce%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AGuard%2C%20a%20lightweight%20robustification%20framework%20that%20enhances%20the%20robustness%20of%0Aa%20broad%20class%20of%20learning-augmented%20caching%20algorithms%20to%20%242H_k%20%2B%202%24%2C%20while%0Apreserving%20their%20%241%24-consistency.%20Guard%20achieves%20the%20current%20best-known%0Atrade-off%20between%20consistency%20and%20robustness%2C%20with%20only%20%24O%281%29%24%20additional%0Aper-request%20overhead%2C%20thereby%20maintaining%20the%20original%20time%20complexity%20of%20the%0Abase%20algorithm.%20Extensive%20experiments%20across%20multiple%20real-world%20datasets%20and%0Aprediction%20models%20validate%20the%20effectiveness%20of%20Guard%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520a%2520Lightweight%2520and%2520Robust%2520Design%2520for%2520Caching%26entry.906535625%3DPeng%2520Chen%2520and%2520Hailiang%2520Zhao%2520and%2520Jiaji%2520Zhang%2520and%2520Xueyan%2520Tang%2520and%2520Yixuan%2520Wang%2520and%2520Shuiguang%2520Deng%26entry.1292438233%3D%2520%2520The%2520online%2520caching%2520problem%2520aims%2520to%2520minimize%2520cache%2520misses%2520when%2520serving%2520a%250Asequence%2520of%2520requests%2520under%2520a%2520limited%2520cache%2520size.%2520While%2520naive%2520learning-augmented%250Acaching%2520algorithms%2520achieve%2520ideal%2520%25241%2524-consistency%252C%2520they%2520lack%2520robustness%250Aguarantees.%2520Existing%2520robustification%2520methods%2520either%2520sacrifice%2520%25241%2524-consistency%250Aor%2520introduce%2520significant%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AGuard%252C%2520a%2520lightweight%2520robustification%2520framework%2520that%2520enhances%2520the%2520robustness%2520of%250Aa%2520broad%2520class%2520of%2520learning-augmented%2520caching%2520algorithms%2520to%2520%25242H_k%2520%252B%25202%2524%252C%2520while%250Apreserving%2520their%2520%25241%2524-consistency.%2520Guard%2520achieves%2520the%2520current%2520best-known%250Atrade-off%2520between%2520consistency%2520and%2520robustness%252C%2520with%2520only%2520%2524O%25281%2529%2524%2520additional%250Aper-request%2520overhead%252C%2520thereby%2520maintaining%2520the%2520original%2520time%2520complexity%2520of%2520the%250Abase%2520algorithm.%2520Extensive%2520experiments%2520across%2520multiple%2520real-world%2520datasets%2520and%250Aprediction%2520models%2520validate%2520the%2520effectiveness%2520of%2520Guard%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Lightweight%20and%20Robust%20Design%20for%20Caching&entry.906535625=Peng%20Chen%20and%20Hailiang%20Zhao%20and%20Jiaji%20Zhang%20and%20Xueyan%20Tang%20and%20Yixuan%20Wang%20and%20Shuiguang%20Deng&entry.1292438233=%20%20The%20online%20caching%20problem%20aims%20to%20minimize%20cache%20misses%20when%20serving%20a%0Asequence%20of%20requests%20under%20a%20limited%20cache%20size.%20While%20naive%20learning-augmented%0Acaching%20algorithms%20achieve%20ideal%20%241%24-consistency%2C%20they%20lack%20robustness%0Aguarantees.%20Existing%20robustification%20methods%20either%20sacrifice%20%241%24-consistency%0Aor%20introduce%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%0AGuard%2C%20a%20lightweight%20robustification%20framework%20that%20enhances%20the%20robustness%20of%0Aa%20broad%20class%20of%20learning-augmented%20caching%20algorithms%20to%20%242H_k%20%2B%202%24%2C%20while%0Apreserving%20their%20%241%24-consistency.%20Guard%20achieves%20the%20current%20best-known%0Atrade-off%20between%20consistency%20and%20robustness%2C%20with%20only%20%24O%281%29%24%20additional%0Aper-request%20overhead%2C%20thereby%20maintaining%20the%20original%20time%20complexity%20of%20the%0Abase%20algorithm.%20Extensive%20experiments%20across%20multiple%20real-world%20datasets%20and%0Aprediction%20models%20validate%20the%20effectiveness%20of%20Guard%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16242v2&entry.124074799=Read"},
{"title": "A Conditional Probability Framework for Compositional Zero-shot Learning", "author": "Peng Wu and Qiuxia Lai and Hao Fang and Guo-Sen Xie and Yilong Yin and Xiankai Lu and Wenguan Wang", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations\nof known objects and attributes by leveraging knowledge from previously seen\ncompositions. Traditional approaches primarily focus on disentangling\nattributes and objects, treating them as independent entities during learning.\nHowever, this assumption overlooks the semantic constraints and contextual\ndependencies inside a composition. For example, certain attributes naturally\npair with specific objects (e.g., \"striped\" applies to \"zebra\" or \"shirts\" but\nnot \"sky\" or \"water\"), while the same attribute can manifest differently\ndepending on context (e.g., \"young\" in \"young tree\" vs. \"young dog\"). Thus,\ncapturing attribute-object interdependence remains a fundamental yet\nlong-ignored challenge in CZSL. In this paper, we adopt a Conditional\nProbability Framework (CPF) to explicitly model attribute-object dependencies.\nWe decompose the probability of a composition into two components: the\nlikelihood of an object and the conditional likelihood of its attribute. To\nenhance object feature learning, we incorporate textual descriptors to\nhighlight semantically relevant image regions. These enhanced object features\nthen guide attribute learning through a cross-attention mechanism, ensuring\nbetter contextual alignment. By jointly optimizing object likelihood and\nconditional attribute likelihood, our method effectively captures compositional\ndependencies and generalizes well to unseen compositions. Extensive experiments\non multiple CZSL benchmarks demonstrate the superiority of our approach. Code\nis available at here.\n", "link": "http://arxiv.org/abs/2507.17377v1", "date": "2025-07-23", "relevancy": 2.0904, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5599}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conditional%20Probability%20Framework%20for%20Compositional%20Zero-shot%20Learning&body=Title%3A%20A%20Conditional%20Probability%20Framework%20for%20Compositional%20Zero-shot%20Learning%0AAuthor%3A%20Peng%20Wu%20and%20Qiuxia%20Lai%20and%20Hao%20Fang%20and%20Guo-Sen%20Xie%20and%20Yilong%20Yin%20and%20Xiankai%20Lu%20and%20Wenguan%20Wang%0AAbstract%3A%20%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20unseen%20combinations%0Aof%20known%20objects%20and%20attributes%20by%20leveraging%20knowledge%20from%20previously%20seen%0Acompositions.%20Traditional%20approaches%20primarily%20focus%20on%20disentangling%0Aattributes%20and%20objects%2C%20treating%20them%20as%20independent%20entities%20during%20learning.%0AHowever%2C%20this%20assumption%20overlooks%20the%20semantic%20constraints%20and%20contextual%0Adependencies%20inside%20a%20composition.%20For%20example%2C%20certain%20attributes%20naturally%0Apair%20with%20specific%20objects%20%28e.g.%2C%20%22striped%22%20applies%20to%20%22zebra%22%20or%20%22shirts%22%20but%0Anot%20%22sky%22%20or%20%22water%22%29%2C%20while%20the%20same%20attribute%20can%20manifest%20differently%0Adepending%20on%20context%20%28e.g.%2C%20%22young%22%20in%20%22young%20tree%22%20vs.%20%22young%20dog%22%29.%20Thus%2C%0Acapturing%20attribute-object%20interdependence%20remains%20a%20fundamental%20yet%0Along-ignored%20challenge%20in%20CZSL.%20In%20this%20paper%2C%20we%20adopt%20a%20Conditional%0AProbability%20Framework%20%28CPF%29%20to%20explicitly%20model%20attribute-object%20dependencies.%0AWe%20decompose%20the%20probability%20of%20a%20composition%20into%20two%20components%3A%20the%0Alikelihood%20of%20an%20object%20and%20the%20conditional%20likelihood%20of%20its%20attribute.%20To%0Aenhance%20object%20feature%20learning%2C%20we%20incorporate%20textual%20descriptors%20to%0Ahighlight%20semantically%20relevant%20image%20regions.%20These%20enhanced%20object%20features%0Athen%20guide%20attribute%20learning%20through%20a%20cross-attention%20mechanism%2C%20ensuring%0Abetter%20contextual%20alignment.%20By%20jointly%20optimizing%20object%20likelihood%20and%0Aconditional%20attribute%20likelihood%2C%20our%20method%20effectively%20captures%20compositional%0Adependencies%20and%20generalizes%20well%20to%20unseen%20compositions.%20Extensive%20experiments%0Aon%20multiple%20CZSL%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20approach.%20Code%0Ais%20available%20at%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conditional%2520Probability%2520Framework%2520for%2520Compositional%2520Zero-shot%2520Learning%26entry.906535625%3DPeng%2520Wu%2520and%2520Qiuxia%2520Lai%2520and%2520Hao%2520Fang%2520and%2520Guo-Sen%2520Xie%2520and%2520Yilong%2520Yin%2520and%2520Xiankai%2520Lu%2520and%2520Wenguan%2520Wang%26entry.1292438233%3D%2520%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520aims%2520to%2520recognize%2520unseen%2520combinations%250Aof%2520known%2520objects%2520and%2520attributes%2520by%2520leveraging%2520knowledge%2520from%2520previously%2520seen%250Acompositions.%2520Traditional%2520approaches%2520primarily%2520focus%2520on%2520disentangling%250Aattributes%2520and%2520objects%252C%2520treating%2520them%2520as%2520independent%2520entities%2520during%2520learning.%250AHowever%252C%2520this%2520assumption%2520overlooks%2520the%2520semantic%2520constraints%2520and%2520contextual%250Adependencies%2520inside%2520a%2520composition.%2520For%2520example%252C%2520certain%2520attributes%2520naturally%250Apair%2520with%2520specific%2520objects%2520%2528e.g.%252C%2520%2522striped%2522%2520applies%2520to%2520%2522zebra%2522%2520or%2520%2522shirts%2522%2520but%250Anot%2520%2522sky%2522%2520or%2520%2522water%2522%2529%252C%2520while%2520the%2520same%2520attribute%2520can%2520manifest%2520differently%250Adepending%2520on%2520context%2520%2528e.g.%252C%2520%2522young%2522%2520in%2520%2522young%2520tree%2522%2520vs.%2520%2522young%2520dog%2522%2529.%2520Thus%252C%250Acapturing%2520attribute-object%2520interdependence%2520remains%2520a%2520fundamental%2520yet%250Along-ignored%2520challenge%2520in%2520CZSL.%2520In%2520this%2520paper%252C%2520we%2520adopt%2520a%2520Conditional%250AProbability%2520Framework%2520%2528CPF%2529%2520to%2520explicitly%2520model%2520attribute-object%2520dependencies.%250AWe%2520decompose%2520the%2520probability%2520of%2520a%2520composition%2520into%2520two%2520components%253A%2520the%250Alikelihood%2520of%2520an%2520object%2520and%2520the%2520conditional%2520likelihood%2520of%2520its%2520attribute.%2520To%250Aenhance%2520object%2520feature%2520learning%252C%2520we%2520incorporate%2520textual%2520descriptors%2520to%250Ahighlight%2520semantically%2520relevant%2520image%2520regions.%2520These%2520enhanced%2520object%2520features%250Athen%2520guide%2520attribute%2520learning%2520through%2520a%2520cross-attention%2520mechanism%252C%2520ensuring%250Abetter%2520contextual%2520alignment.%2520By%2520jointly%2520optimizing%2520object%2520likelihood%2520and%250Aconditional%2520attribute%2520likelihood%252C%2520our%2520method%2520effectively%2520captures%2520compositional%250Adependencies%2520and%2520generalizes%2520well%2520to%2520unseen%2520compositions.%2520Extensive%2520experiments%250Aon%2520multiple%2520CZSL%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach.%2520Code%250Ais%2520available%2520at%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conditional%20Probability%20Framework%20for%20Compositional%20Zero-shot%20Learning&entry.906535625=Peng%20Wu%20and%20Qiuxia%20Lai%20and%20Hao%20Fang%20and%20Guo-Sen%20Xie%20and%20Yilong%20Yin%20and%20Xiankai%20Lu%20and%20Wenguan%20Wang&entry.1292438233=%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20unseen%20combinations%0Aof%20known%20objects%20and%20attributes%20by%20leveraging%20knowledge%20from%20previously%20seen%0Acompositions.%20Traditional%20approaches%20primarily%20focus%20on%20disentangling%0Aattributes%20and%20objects%2C%20treating%20them%20as%20independent%20entities%20during%20learning.%0AHowever%2C%20this%20assumption%20overlooks%20the%20semantic%20constraints%20and%20contextual%0Adependencies%20inside%20a%20composition.%20For%20example%2C%20certain%20attributes%20naturally%0Apair%20with%20specific%20objects%20%28e.g.%2C%20%22striped%22%20applies%20to%20%22zebra%22%20or%20%22shirts%22%20but%0Anot%20%22sky%22%20or%20%22water%22%29%2C%20while%20the%20same%20attribute%20can%20manifest%20differently%0Adepending%20on%20context%20%28e.g.%2C%20%22young%22%20in%20%22young%20tree%22%20vs.%20%22young%20dog%22%29.%20Thus%2C%0Acapturing%20attribute-object%20interdependence%20remains%20a%20fundamental%20yet%0Along-ignored%20challenge%20in%20CZSL.%20In%20this%20paper%2C%20we%20adopt%20a%20Conditional%0AProbability%20Framework%20%28CPF%29%20to%20explicitly%20model%20attribute-object%20dependencies.%0AWe%20decompose%20the%20probability%20of%20a%20composition%20into%20two%20components%3A%20the%0Alikelihood%20of%20an%20object%20and%20the%20conditional%20likelihood%20of%20its%20attribute.%20To%0Aenhance%20object%20feature%20learning%2C%20we%20incorporate%20textual%20descriptors%20to%0Ahighlight%20semantically%20relevant%20image%20regions.%20These%20enhanced%20object%20features%0Athen%20guide%20attribute%20learning%20through%20a%20cross-attention%20mechanism%2C%20ensuring%0Abetter%20contextual%20alignment.%20By%20jointly%20optimizing%20object%20likelihood%20and%0Aconditional%20attribute%20likelihood%2C%20our%20method%20effectively%20captures%20compositional%0Adependencies%20and%20generalizes%20well%20to%20unseen%20compositions.%20Extensive%20experiments%0Aon%20multiple%20CZSL%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20approach.%20Code%0Ais%20available%20at%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17377v1&entry.124074799=Read"},
{"title": "Generalized Dual Discriminator GANs", "author": "Penukonda Naga Chandana and Tejas Srivastava and Gowtham R. Kurri and V. Lalitha", "abstract": "  Dual discriminator generative adversarial networks (D2 GANs) were introduced\nto mitigate the problem of mode collapse in generative adversarial networks. In\nD2 GANs, two discriminators are employed alongside a generator: one\ndiscriminator rewards high scores for samples from the true data distribution,\nwhile the other favors samples from the generator. In this work, we first\nintroduce dual discriminator $\\alpha$-GANs (D2 $\\alpha$-GANs), which combines\nthe strengths of dual discriminators with the flexibility of a tunable loss\nfunction, $\\alpha$-loss. We further generalize this approach to arbitrary\nfunctions defined on positive reals, leading to a broader class of models we\nrefer to as generalized dual discriminator generative adversarial networks. For\neach of these proposed models, we provide theoretical analysis and show that\nthe associated min-max optimization reduces to the minimization of a linear\ncombination of an $f$-divergence and a reverse $f$-divergence. This generalizes\nthe known simplification for D2-GANs, where the objective reduces to a linear\ncombination of the KL-divergence and the reverse KL-divergence. Finally, we\nperform experiments on 2D synthetic data and use multiple performance metrics\nto capture various advantages of our GANs.\n", "link": "http://arxiv.org/abs/2507.17684v1", "date": "2025-07-23", "relevancy": 2.0842, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5441}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5089}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Dual%20Discriminator%20GANs&body=Title%3A%20Generalized%20Dual%20Discriminator%20GANs%0AAuthor%3A%20Penukonda%20Naga%20Chandana%20and%20Tejas%20Srivastava%20and%20Gowtham%20R.%20Kurri%20and%20V.%20Lalitha%0AAbstract%3A%20%20%20Dual%20discriminator%20generative%20adversarial%20networks%20%28D2%20GANs%29%20were%20introduced%0Ato%20mitigate%20the%20problem%20of%20mode%20collapse%20in%20generative%20adversarial%20networks.%20In%0AD2%20GANs%2C%20two%20discriminators%20are%20employed%20alongside%20a%20generator%3A%20one%0Adiscriminator%20rewards%20high%20scores%20for%20samples%20from%20the%20true%20data%20distribution%2C%0Awhile%20the%20other%20favors%20samples%20from%20the%20generator.%20In%20this%20work%2C%20we%20first%0Aintroduce%20dual%20discriminator%20%24%5Calpha%24-GANs%20%28D2%20%24%5Calpha%24-GANs%29%2C%20which%20combines%0Athe%20strengths%20of%20dual%20discriminators%20with%20the%20flexibility%20of%20a%20tunable%20loss%0Afunction%2C%20%24%5Calpha%24-loss.%20We%20further%20generalize%20this%20approach%20to%20arbitrary%0Afunctions%20defined%20on%20positive%20reals%2C%20leading%20to%20a%20broader%20class%20of%20models%20we%0Arefer%20to%20as%20generalized%20dual%20discriminator%20generative%20adversarial%20networks.%20For%0Aeach%20of%20these%20proposed%20models%2C%20we%20provide%20theoretical%20analysis%20and%20show%20that%0Athe%20associated%20min-max%20optimization%20reduces%20to%20the%20minimization%20of%20a%20linear%0Acombination%20of%20an%20%24f%24-divergence%20and%20a%20reverse%20%24f%24-divergence.%20This%20generalizes%0Athe%20known%20simplification%20for%20D2-GANs%2C%20where%20the%20objective%20reduces%20to%20a%20linear%0Acombination%20of%20the%20KL-divergence%20and%20the%20reverse%20KL-divergence.%20Finally%2C%20we%0Aperform%20experiments%20on%202D%20synthetic%20data%20and%20use%20multiple%20performance%20metrics%0Ato%20capture%20various%20advantages%20of%20our%20GANs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Dual%2520Discriminator%2520GANs%26entry.906535625%3DPenukonda%2520Naga%2520Chandana%2520and%2520Tejas%2520Srivastava%2520and%2520Gowtham%2520R.%2520Kurri%2520and%2520V.%2520Lalitha%26entry.1292438233%3D%2520%2520Dual%2520discriminator%2520generative%2520adversarial%2520networks%2520%2528D2%2520GANs%2529%2520were%2520introduced%250Ato%2520mitigate%2520the%2520problem%2520of%2520mode%2520collapse%2520in%2520generative%2520adversarial%2520networks.%2520In%250AD2%2520GANs%252C%2520two%2520discriminators%2520are%2520employed%2520alongside%2520a%2520generator%253A%2520one%250Adiscriminator%2520rewards%2520high%2520scores%2520for%2520samples%2520from%2520the%2520true%2520data%2520distribution%252C%250Awhile%2520the%2520other%2520favors%2520samples%2520from%2520the%2520generator.%2520In%2520this%2520work%252C%2520we%2520first%250Aintroduce%2520dual%2520discriminator%2520%2524%255Calpha%2524-GANs%2520%2528D2%2520%2524%255Calpha%2524-GANs%2529%252C%2520which%2520combines%250Athe%2520strengths%2520of%2520dual%2520discriminators%2520with%2520the%2520flexibility%2520of%2520a%2520tunable%2520loss%250Afunction%252C%2520%2524%255Calpha%2524-loss.%2520We%2520further%2520generalize%2520this%2520approach%2520to%2520arbitrary%250Afunctions%2520defined%2520on%2520positive%2520reals%252C%2520leading%2520to%2520a%2520broader%2520class%2520of%2520models%2520we%250Arefer%2520to%2520as%2520generalized%2520dual%2520discriminator%2520generative%2520adversarial%2520networks.%2520For%250Aeach%2520of%2520these%2520proposed%2520models%252C%2520we%2520provide%2520theoretical%2520analysis%2520and%2520show%2520that%250Athe%2520associated%2520min-max%2520optimization%2520reduces%2520to%2520the%2520minimization%2520of%2520a%2520linear%250Acombination%2520of%2520an%2520%2524f%2524-divergence%2520and%2520a%2520reverse%2520%2524f%2524-divergence.%2520This%2520generalizes%250Athe%2520known%2520simplification%2520for%2520D2-GANs%252C%2520where%2520the%2520objective%2520reduces%2520to%2520a%2520linear%250Acombination%2520of%2520the%2520KL-divergence%2520and%2520the%2520reverse%2520KL-divergence.%2520Finally%252C%2520we%250Aperform%2520experiments%2520on%25202D%2520synthetic%2520data%2520and%2520use%2520multiple%2520performance%2520metrics%250Ato%2520capture%2520various%2520advantages%2520of%2520our%2520GANs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Dual%20Discriminator%20GANs&entry.906535625=Penukonda%20Naga%20Chandana%20and%20Tejas%20Srivastava%20and%20Gowtham%20R.%20Kurri%20and%20V.%20Lalitha&entry.1292438233=%20%20Dual%20discriminator%20generative%20adversarial%20networks%20%28D2%20GANs%29%20were%20introduced%0Ato%20mitigate%20the%20problem%20of%20mode%20collapse%20in%20generative%20adversarial%20networks.%20In%0AD2%20GANs%2C%20two%20discriminators%20are%20employed%20alongside%20a%20generator%3A%20one%0Adiscriminator%20rewards%20high%20scores%20for%20samples%20from%20the%20true%20data%20distribution%2C%0Awhile%20the%20other%20favors%20samples%20from%20the%20generator.%20In%20this%20work%2C%20we%20first%0Aintroduce%20dual%20discriminator%20%24%5Calpha%24-GANs%20%28D2%20%24%5Calpha%24-GANs%29%2C%20which%20combines%0Athe%20strengths%20of%20dual%20discriminators%20with%20the%20flexibility%20of%20a%20tunable%20loss%0Afunction%2C%20%24%5Calpha%24-loss.%20We%20further%20generalize%20this%20approach%20to%20arbitrary%0Afunctions%20defined%20on%20positive%20reals%2C%20leading%20to%20a%20broader%20class%20of%20models%20we%0Arefer%20to%20as%20generalized%20dual%20discriminator%20generative%20adversarial%20networks.%20For%0Aeach%20of%20these%20proposed%20models%2C%20we%20provide%20theoretical%20analysis%20and%20show%20that%0Athe%20associated%20min-max%20optimization%20reduces%20to%20the%20minimization%20of%20a%20linear%0Acombination%20of%20an%20%24f%24-divergence%20and%20a%20reverse%20%24f%24-divergence.%20This%20generalizes%0Athe%20known%20simplification%20for%20D2-GANs%2C%20where%20the%20objective%20reduces%20to%20a%20linear%0Acombination%20of%20the%20KL-divergence%20and%20the%20reverse%20KL-divergence.%20Finally%2C%20we%0Aperform%20experiments%20on%202D%20synthetic%20data%20and%20use%20multiple%20performance%20metrics%0Ato%20capture%20various%20advantages%20of%20our%20GANs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17684v1&entry.124074799=Read"},
{"title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning", "author": "Yanjun Zheng and Xiyang Du and Longfei Liao and Xiaoke Zhao and Zhaowen Zhou and Bo Zhang and Jiawei Liu and Xiang Qi and Zhe Li and Zhiqiang Zhang and Wei Wang and Peng Zhang", "abstract": "  Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.\n", "link": "http://arxiv.org/abs/2507.16802v2", "date": "2025-07-23", "relevancy": 2.0841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&body=Title%3A%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning%0AAuthor%3A%20Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wei%20Wang%20and%20Peng%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20considerable%20promise%20in%20financial%0Aapplications%3B%20however%2C%20prevailing%20models%20frequently%20demonstrate%20limitations%0Awhen%20confronted%20with%20scenarios%20that%20necessitate%20sophisticated%20reasoning%0Acapabilities%2C%20stringent%20trustworthiness%20criteria%2C%20and%20efficient%20adaptation%20to%0Adomain-specific%20requirements.%20We%20introduce%20the%20Agentar-Fin-R1%20series%20of%0Afinancial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20label%20system%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20training%20pipeline%2C%20and%20dynamic%20attribution%20systems%2C%20we%0Aachieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20Fineva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA-diamond.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%20The%20Finova%20bench%20is%20available%20at%0Ahttps%3A//github.com/antgroup/Finova.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentar-Fin-R1%253A%2520Enhancing%2520Financial%2520Intelligence%2520through%2520Domain%250A%2520%2520Expertise%252C%2520Training%2520Efficiency%252C%2520and%2520Advanced%2520Reasoning%26entry.906535625%3DYanjun%2520Zheng%2520and%2520Xiyang%2520Du%2520and%2520Longfei%2520Liao%2520and%2520Xiaoke%2520Zhao%2520and%2520Zhaowen%2520Zhou%2520and%2520Bo%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Xiang%2520Qi%2520and%2520Zhe%2520Li%2520and%2520Zhiqiang%2520Zhang%2520and%2520Wei%2520Wang%2520and%2520Peng%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520considerable%2520promise%2520in%2520financial%250Aapplications%253B%2520however%252C%2520prevailing%2520models%2520frequently%2520demonstrate%2520limitations%250Awhen%2520confronted%2520with%2520scenarios%2520that%2520necessitate%2520sophisticated%2520reasoning%250Acapabilities%252C%2520stringent%2520trustworthiness%2520criteria%252C%2520and%2520efficient%2520adaptation%2520to%250Adomain-specific%2520requirements.%2520We%2520introduce%2520the%2520Agentar-Fin-R1%2520series%2520of%250Afinancial%2520large%2520language%2520models%2520%25288B%2520and%252032B%2520parameters%2529%252C%2520specifically%250Aengineered%2520based%2520on%2520the%2520Qwen3%2520foundation%2520model%2520to%2520enhance%2520reasoning%250Acapabilities%252C%2520reliability%252C%2520and%2520domain%2520specialization%2520for%2520financial%250Aapplications.%2520Our%2520optimization%2520approach%2520integrates%2520a%2520high-quality%252C%2520systematic%250Afinancial%2520task%2520label%2520system%2520with%2520a%2520comprehensive%2520multi-layered%2520trustworthiness%250Aassurance%2520framework.%2520This%2520framework%2520encompasses%2520high-quality%2520trustworthy%250Aknowledge%2520engineering%252C%2520multi-agent%2520trustworthy%2520data%2520synthesis%252C%2520and%2520rigorous%250Adata%2520validation%2520governance.%2520Through%2520label-guided%2520automated%2520difficulty-aware%250Aoptimization%252C%2520tow-stage%2520training%2520pipeline%252C%2520and%2520dynamic%2520attribution%2520systems%252C%2520we%250Aachieve%2520substantial%2520improvements%2520in%2520training%2520efficiency.%2520Our%2520models%2520undergo%250Acomprehensive%2520evaluation%2520on%2520mainstream%2520financial%2520benchmarks%2520including%2520Fineva%252C%250AFinEval%252C%2520and%2520FinanceIQ%252C%2520as%2520well%2520as%2520general%2520reasoning%2520datasets%2520such%2520as%2520MATH-500%250Aand%2520GPQA-diamond.%2520To%2520thoroughly%2520assess%2520real-world%2520deployment%2520capabilities%252C%2520we%250Ainnovatively%2520propose%2520the%2520Finova%2520evaluation%2520benchmark%252C%2520which%2520focuses%2520on%250Aagent-level%2520financial%2520reasoning%2520and%2520compliance%2520verification.%2520Experimental%250Aresults%2520demonstrate%2520that%2520Agentar-Fin-R1%2520not%2520only%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520financial%2520tasks%2520but%2520also%2520exhibits%2520exceptional%2520general%2520reasoning%250Acapabilities%252C%2520validating%2520its%2520effectiveness%2520as%2520a%2520trustworthy%2520solution%2520for%250Ahigh-stakes%2520financial%2520applications.%2520The%2520Finova%2520bench%2520is%2520available%2520at%250Ahttps%253A//github.com/antgroup/Finova.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&entry.906535625=Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wei%20Wang%20and%20Peng%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20considerable%20promise%20in%20financial%0Aapplications%3B%20however%2C%20prevailing%20models%20frequently%20demonstrate%20limitations%0Awhen%20confronted%20with%20scenarios%20that%20necessitate%20sophisticated%20reasoning%0Acapabilities%2C%20stringent%20trustworthiness%20criteria%2C%20and%20efficient%20adaptation%20to%0Adomain-specific%20requirements.%20We%20introduce%20the%20Agentar-Fin-R1%20series%20of%0Afinancial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20label%20system%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20training%20pipeline%2C%20and%20dynamic%20attribution%20systems%2C%20we%0Aachieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20Fineva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA-diamond.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%20The%20Finova%20bench%20is%20available%20at%0Ahttps%3A//github.com/antgroup/Finova.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16802v2&entry.124074799=Read"},
{"title": "Optimizing against Infeasible Inclusions from Data for Semantic\n  Segmentation through Morphology", "author": "Shamik Basu and Luc Van Gool and Christos Sakaridis", "abstract": "  State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel or per-segment classification\nobjectives on their training data. This purely data-driven paradigm often leads\nto absurd segmentations, especially when the domain of input images is shifted\nfrom the one encountered during training. For instance, state-of-the-art models\nmay assign the label \"road\" to a segment that is included by another segment\nthat is respectively labeled as \"sky\". However, the ground truth of the\nexisting dataset at hand dictates that such inclusion is not feasible. Our\nmethod, Infeasible Semantic Inclusions (InSeIn), first extracts explicit\ninclusion constraints that govern spatial class relations from the semantic\nsegmentation training set at hand in an offline, data-driven fashion, and then\nenforces a morphological yet differentiable loss that penalizes violations of\nthese constraints during training to promote prediction feasibility. InSeIn is\na light-weight plug-and-play method, constitutes a novel step towards\nminimizing infeasible semantic inclusions in the predictions of learned\nsegmentation models, and yields consistent and significant performance\nimprovements over diverse state-of-the-art networks across the ADE20K,\nCityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn/tree/main\n", "link": "http://arxiv.org/abs/2408.14672v4", "date": "2025-07-23", "relevancy": 2.0757, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology&body=Title%3A%20Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology%0AAuthor%3A%20Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis%0AAbstract%3A%20%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20or%20per-segment%20classification%0Aobjectives%20on%20their%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%0Ato%20absurd%20segmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%0Afrom%20the%20one%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%0Amay%20assign%20the%20label%20%22road%22%20to%20a%20segment%20that%20is%20included%20by%20another%20segment%0Athat%20is%20respectively%20labeled%20as%20%22sky%22.%20However%2C%20the%20ground%20truth%20of%20the%0Aexisting%20dataset%20at%20hand%20dictates%20that%20such%20inclusion%20is%20not%20feasible.%20Our%0Amethod%2C%20Infeasible%20Semantic%20Inclusions%20%28InSeIn%29%2C%20first%20extracts%20explicit%0Ainclusion%20constraints%20that%20govern%20spatial%20class%20relations%20from%20the%20semantic%0Asegmentation%20training%20set%20at%20hand%20in%20an%20offline%2C%20data-driven%20fashion%2C%20and%20then%0Aenforces%20a%20morphological%20yet%20differentiable%20loss%20that%20penalizes%20violations%20of%0Athese%20constraints%20during%20training%20to%20promote%20prediction%20feasibility.%20InSeIn%20is%0Aa%20light-weight%20plug-and-play%20method%2C%20constitutes%20a%20novel%20step%20towards%0Aminimizing%20infeasible%20semantic%20inclusions%20in%20the%20predictions%20of%20learned%0Asegmentation%20models%2C%20and%20yields%20consistent%20and%20significant%20performance%0Aimprovements%20over%20diverse%20state-of-the-art%20networks%20across%20the%20ADE20K%2C%0ACityscapes%2C%20and%20ACDC%20datasets.%20https%3A//github.com/SHAMIK-97/InSeIn/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14672v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520against%2520Infeasible%2520Inclusions%2520from%2520Data%2520for%2520Semantic%250A%2520%2520Segmentation%2520through%2520Morphology%26entry.906535625%3DShamik%2520Basu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Christos%2520Sakaridis%26entry.1292438233%3D%2520%2520State-of-the-art%2520semantic%2520segmentation%2520models%2520are%2520typically%2520optimized%2520in%2520a%250Adata-driven%2520fashion%252C%2520minimizing%2520solely%2520per-pixel%2520or%2520per-segment%2520classification%250Aobjectives%2520on%2520their%2520training%2520data.%2520This%2520purely%2520data-driven%2520paradigm%2520often%2520leads%250Ato%2520absurd%2520segmentations%252C%2520especially%2520when%2520the%2520domain%2520of%2520input%2520images%2520is%2520shifted%250Afrom%2520the%2520one%2520encountered%2520during%2520training.%2520For%2520instance%252C%2520state-of-the-art%2520models%250Amay%2520assign%2520the%2520label%2520%2522road%2522%2520to%2520a%2520segment%2520that%2520is%2520included%2520by%2520another%2520segment%250Athat%2520is%2520respectively%2520labeled%2520as%2520%2522sky%2522.%2520However%252C%2520the%2520ground%2520truth%2520of%2520the%250Aexisting%2520dataset%2520at%2520hand%2520dictates%2520that%2520such%2520inclusion%2520is%2520not%2520feasible.%2520Our%250Amethod%252C%2520Infeasible%2520Semantic%2520Inclusions%2520%2528InSeIn%2529%252C%2520first%2520extracts%2520explicit%250Ainclusion%2520constraints%2520that%2520govern%2520spatial%2520class%2520relations%2520from%2520the%2520semantic%250Asegmentation%2520training%2520set%2520at%2520hand%2520in%2520an%2520offline%252C%2520data-driven%2520fashion%252C%2520and%2520then%250Aenforces%2520a%2520morphological%2520yet%2520differentiable%2520loss%2520that%2520penalizes%2520violations%2520of%250Athese%2520constraints%2520during%2520training%2520to%2520promote%2520prediction%2520feasibility.%2520InSeIn%2520is%250Aa%2520light-weight%2520plug-and-play%2520method%252C%2520constitutes%2520a%2520novel%2520step%2520towards%250Aminimizing%2520infeasible%2520semantic%2520inclusions%2520in%2520the%2520predictions%2520of%2520learned%250Asegmentation%2520models%252C%2520and%2520yields%2520consistent%2520and%2520significant%2520performance%250Aimprovements%2520over%2520diverse%2520state-of-the-art%2520networks%2520across%2520the%2520ADE20K%252C%250ACityscapes%252C%2520and%2520ACDC%2520datasets.%2520https%253A//github.com/SHAMIK-97/InSeIn/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14672v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology&entry.906535625=Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis&entry.1292438233=%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20or%20per-segment%20classification%0Aobjectives%20on%20their%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%0Ato%20absurd%20segmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%0Afrom%20the%20one%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%0Amay%20assign%20the%20label%20%22road%22%20to%20a%20segment%20that%20is%20included%20by%20another%20segment%0Athat%20is%20respectively%20labeled%20as%20%22sky%22.%20However%2C%20the%20ground%20truth%20of%20the%0Aexisting%20dataset%20at%20hand%20dictates%20that%20such%20inclusion%20is%20not%20feasible.%20Our%0Amethod%2C%20Infeasible%20Semantic%20Inclusions%20%28InSeIn%29%2C%20first%20extracts%20explicit%0Ainclusion%20constraints%20that%20govern%20spatial%20class%20relations%20from%20the%20semantic%0Asegmentation%20training%20set%20at%20hand%20in%20an%20offline%2C%20data-driven%20fashion%2C%20and%20then%0Aenforces%20a%20morphological%20yet%20differentiable%20loss%20that%20penalizes%20violations%20of%0Athese%20constraints%20during%20training%20to%20promote%20prediction%20feasibility.%20InSeIn%20is%0Aa%20light-weight%20plug-and-play%20method%2C%20constitutes%20a%20novel%20step%20towards%0Aminimizing%20infeasible%20semantic%20inclusions%20in%20the%20predictions%20of%20learned%0Asegmentation%20models%2C%20and%20yields%20consistent%20and%20significant%20performance%0Aimprovements%20over%20diverse%20state-of-the-art%20networks%20across%20the%20ADE20K%2C%0ACityscapes%2C%20and%20ACDC%20datasets.%20https%3A//github.com/SHAMIK-97/InSeIn/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14672v4&entry.124074799=Read"},
{"title": "Application of YOLOv8 in monocular downward multiple Car Target\n  detection", "author": "Shijie Lyu", "abstract": "  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n", "link": "http://arxiv.org/abs/2505.10016v2", "date": "2025-07-23", "relevancy": 2.0709, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5053}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20YOLOv8%20in%20monocular%20downward%20multiple%20Car%20Target%0A%20%20detection&body=Title%3A%20Application%20of%20YOLOv8%20in%20monocular%20downward%20multiple%20Car%20Target%0A%20%20detection%0AAuthor%3A%20Shijie%20Lyu%0AAbstract%3A%20%20%20Autonomous%20driving%20technology%20is%20progressively%20transforming%20traditional%20car%0Adriving%20methods%2C%20marking%20a%20significant%20milestone%20in%20modern%20transportation.%0AObject%20detection%20serves%20as%20a%20cornerstone%20of%20autonomous%20systems%2C%20playing%20a%20vital%0Arole%20in%20enhancing%20driving%20safety%2C%20enabling%20autonomous%20functionality%2C%20improving%0Atraffic%20efficiency%2C%20and%20facilitating%20effective%20emergency%20responses.%20However%2C%0Acurrent%20technologies%20such%20as%20radar%20for%20environmental%20perception%2C%20cameras%20for%0Aroad%20perception%2C%20and%20vehicle%20sensor%20networks%20face%20notable%20challenges%2C%20including%0Ahigh%20costs%2C%20vulnerability%20to%20weather%20and%20lighting%20conditions%2C%20and%20limited%0Aresolution.To%20address%20these%20limitations%2C%20this%20paper%20presents%20an%20improved%0Aautonomous%20target%20detection%20network%20based%20on%20YOLOv8.%20By%20integrating%20structural%0Areparameterization%20technology%2C%20a%20bidirectional%20pyramid%20structure%20network%20model%2C%0Aand%20a%20novel%20detection%20pipeline%20into%20the%20YOLOv8%20framework%2C%20the%20proposed%20approach%0Aachieves%20highly%20efficient%20and%20precise%20detection%20of%20multi-scale%2C%20small%2C%20and%0Aremote%20objects.%20Experimental%20results%20demonstrate%20that%20the%20enhanced%20model%20can%0Aeffectively%20detect%20both%20large%20and%20small%20objects%20with%20a%20detection%20accuracy%20of%0A65%25%2C%20showcasing%20significant%20advancements%20over%20traditional%20methods.This%20improved%0Amodel%20holds%20substantial%20potential%20for%20real-world%20applications%20and%20is%0Awell-suited%20for%20autonomous%20driving%20competitions%2C%20such%20as%20the%20Formula%20Student%0AAutonomous%20China%20%28FSAC%29%2C%20particularly%20excelling%20in%20scenarios%20involving%0Asingle-target%20and%20small-object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520YOLOv8%2520in%2520monocular%2520downward%2520multiple%2520Car%2520Target%250A%2520%2520detection%26entry.906535625%3DShijie%2520Lyu%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520technology%2520is%2520progressively%2520transforming%2520traditional%2520car%250Adriving%2520methods%252C%2520marking%2520a%2520significant%2520milestone%2520in%2520modern%2520transportation.%250AObject%2520detection%2520serves%2520as%2520a%2520cornerstone%2520of%2520autonomous%2520systems%252C%2520playing%2520a%2520vital%250Arole%2520in%2520enhancing%2520driving%2520safety%252C%2520enabling%2520autonomous%2520functionality%252C%2520improving%250Atraffic%2520efficiency%252C%2520and%2520facilitating%2520effective%2520emergency%2520responses.%2520However%252C%250Acurrent%2520technologies%2520such%2520as%2520radar%2520for%2520environmental%2520perception%252C%2520cameras%2520for%250Aroad%2520perception%252C%2520and%2520vehicle%2520sensor%2520networks%2520face%2520notable%2520challenges%252C%2520including%250Ahigh%2520costs%252C%2520vulnerability%2520to%2520weather%2520and%2520lighting%2520conditions%252C%2520and%2520limited%250Aresolution.To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520presents%2520an%2520improved%250Aautonomous%2520target%2520detection%2520network%2520based%2520on%2520YOLOv8.%2520By%2520integrating%2520structural%250Areparameterization%2520technology%252C%2520a%2520bidirectional%2520pyramid%2520structure%2520network%2520model%252C%250Aand%2520a%2520novel%2520detection%2520pipeline%2520into%2520the%2520YOLOv8%2520framework%252C%2520the%2520proposed%2520approach%250Aachieves%2520highly%2520efficient%2520and%2520precise%2520detection%2520of%2520multi-scale%252C%2520small%252C%2520and%250Aremote%2520objects.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520enhanced%2520model%2520can%250Aeffectively%2520detect%2520both%2520large%2520and%2520small%2520objects%2520with%2520a%2520detection%2520accuracy%2520of%250A65%2525%252C%2520showcasing%2520significant%2520advancements%2520over%2520traditional%2520methods.This%2520improved%250Amodel%2520holds%2520substantial%2520potential%2520for%2520real-world%2520applications%2520and%2520is%250Awell-suited%2520for%2520autonomous%2520driving%2520competitions%252C%2520such%2520as%2520the%2520Formula%2520Student%250AAutonomous%2520China%2520%2528FSAC%2529%252C%2520particularly%2520excelling%2520in%2520scenarios%2520involving%250Asingle-target%2520and%2520small-object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20YOLOv8%20in%20monocular%20downward%20multiple%20Car%20Target%0A%20%20detection&entry.906535625=Shijie%20Lyu&entry.1292438233=%20%20Autonomous%20driving%20technology%20is%20progressively%20transforming%20traditional%20car%0Adriving%20methods%2C%20marking%20a%20significant%20milestone%20in%20modern%20transportation.%0AObject%20detection%20serves%20as%20a%20cornerstone%20of%20autonomous%20systems%2C%20playing%20a%20vital%0Arole%20in%20enhancing%20driving%20safety%2C%20enabling%20autonomous%20functionality%2C%20improving%0Atraffic%20efficiency%2C%20and%20facilitating%20effective%20emergency%20responses.%20However%2C%0Acurrent%20technologies%20such%20as%20radar%20for%20environmental%20perception%2C%20cameras%20for%0Aroad%20perception%2C%20and%20vehicle%20sensor%20networks%20face%20notable%20challenges%2C%20including%0Ahigh%20costs%2C%20vulnerability%20to%20weather%20and%20lighting%20conditions%2C%20and%20limited%0Aresolution.To%20address%20these%20limitations%2C%20this%20paper%20presents%20an%20improved%0Aautonomous%20target%20detection%20network%20based%20on%20YOLOv8.%20By%20integrating%20structural%0Areparameterization%20technology%2C%20a%20bidirectional%20pyramid%20structure%20network%20model%2C%0Aand%20a%20novel%20detection%20pipeline%20into%20the%20YOLOv8%20framework%2C%20the%20proposed%20approach%0Aachieves%20highly%20efficient%20and%20precise%20detection%20of%20multi-scale%2C%20small%2C%20and%0Aremote%20objects.%20Experimental%20results%20demonstrate%20that%20the%20enhanced%20model%20can%0Aeffectively%20detect%20both%20large%20and%20small%20objects%20with%20a%20detection%20accuracy%20of%0A65%25%2C%20showcasing%20significant%20advancements%20over%20traditional%20methods.This%20improved%0Amodel%20holds%20substantial%20potential%20for%20real-world%20applications%20and%20is%0Awell-suited%20for%20autonomous%20driving%20competitions%2C%20such%20as%20the%20Formula%20Student%0AAutonomous%20China%20%28FSAC%29%2C%20particularly%20excelling%20in%20scenarios%20involving%0Asingle-target%20and%20small-object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10016v2&entry.124074799=Read"},
{"title": "How to Adapt Control Barrier Functions? A Learning-Based Approach with\n  Applications to a VTOL Quadplane", "author": "Taekyung Kim and Randal W. Beard and Dimitra Panagou", "abstract": "  In this paper, we present a novel theoretical framework for online adaptation\nof Control Barrier Function (CBF) parameters, i.e., of the class K functions\nincluded in the CBF condition, under input constraints. We introduce the\nconcept of locally validated CBF parameters, which are adapted online to\nguarantee finite-horizon safety, based on conditions derived from Nagumo's\ntheorem and tangent cone analysis. To identify these parameters online, we\nintegrate a learning-based approach with an uncertainty-aware verification\nprocess that account for both epistemic and aleatoric uncertainties inherent in\nneural network predictions. Our method is demonstrated on a VTOL quadplane\nmodel during challenging transition and landing maneuvers, showcasing enhanced\nperformance while maintaining safety.\n", "link": "http://arxiv.org/abs/2504.03038v3", "date": "2025-07-23", "relevancy": 2.0658, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5302}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Adapt%20Control%20Barrier%20Functions%3F%20A%20Learning-Based%20Approach%20with%0A%20%20Applications%20to%20a%20VTOL%20Quadplane&body=Title%3A%20How%20to%20Adapt%20Control%20Barrier%20Functions%3F%20A%20Learning-Based%20Approach%20with%0A%20%20Applications%20to%20a%20VTOL%20Quadplane%0AAuthor%3A%20Taekyung%20Kim%20and%20Randal%20W.%20Beard%20and%20Dimitra%20Panagou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20theoretical%20framework%20for%20online%20adaptation%0Aof%20Control%20Barrier%20Function%20%28CBF%29%20parameters%2C%20i.e.%2C%20of%20the%20class%20K%20functions%0Aincluded%20in%20the%20CBF%20condition%2C%20under%20input%20constraints.%20We%20introduce%20the%0Aconcept%20of%20locally%20validated%20CBF%20parameters%2C%20which%20are%20adapted%20online%20to%0Aguarantee%20finite-horizon%20safety%2C%20based%20on%20conditions%20derived%20from%20Nagumo%27s%0Atheorem%20and%20tangent%20cone%20analysis.%20To%20identify%20these%20parameters%20online%2C%20we%0Aintegrate%20a%20learning-based%20approach%20with%20an%20uncertainty-aware%20verification%0Aprocess%20that%20account%20for%20both%20epistemic%20and%20aleatoric%20uncertainties%20inherent%20in%0Aneural%20network%20predictions.%20Our%20method%20is%20demonstrated%20on%20a%20VTOL%20quadplane%0Amodel%20during%20challenging%20transition%20and%20landing%20maneuvers%2C%20showcasing%20enhanced%0Aperformance%20while%20maintaining%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03038v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Adapt%2520Control%2520Barrier%2520Functions%253F%2520A%2520Learning-Based%2520Approach%2520with%250A%2520%2520Applications%2520to%2520a%2520VTOL%2520Quadplane%26entry.906535625%3DTaekyung%2520Kim%2520and%2520Randal%2520W.%2520Beard%2520and%2520Dimitra%2520Panagou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520theoretical%2520framework%2520for%2520online%2520adaptation%250Aof%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520parameters%252C%2520i.e.%252C%2520of%2520the%2520class%2520K%2520functions%250Aincluded%2520in%2520the%2520CBF%2520condition%252C%2520under%2520input%2520constraints.%2520We%2520introduce%2520the%250Aconcept%2520of%2520locally%2520validated%2520CBF%2520parameters%252C%2520which%2520are%2520adapted%2520online%2520to%250Aguarantee%2520finite-horizon%2520safety%252C%2520based%2520on%2520conditions%2520derived%2520from%2520Nagumo%2527s%250Atheorem%2520and%2520tangent%2520cone%2520analysis.%2520To%2520identify%2520these%2520parameters%2520online%252C%2520we%250Aintegrate%2520a%2520learning-based%2520approach%2520with%2520an%2520uncertainty-aware%2520verification%250Aprocess%2520that%2520account%2520for%2520both%2520epistemic%2520and%2520aleatoric%2520uncertainties%2520inherent%2520in%250Aneural%2520network%2520predictions.%2520Our%2520method%2520is%2520demonstrated%2520on%2520a%2520VTOL%2520quadplane%250Amodel%2520during%2520challenging%2520transition%2520and%2520landing%2520maneuvers%252C%2520showcasing%2520enhanced%250Aperformance%2520while%2520maintaining%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03038v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Adapt%20Control%20Barrier%20Functions%3F%20A%20Learning-Based%20Approach%20with%0A%20%20Applications%20to%20a%20VTOL%20Quadplane&entry.906535625=Taekyung%20Kim%20and%20Randal%20W.%20Beard%20and%20Dimitra%20Panagou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20theoretical%20framework%20for%20online%20adaptation%0Aof%20Control%20Barrier%20Function%20%28CBF%29%20parameters%2C%20i.e.%2C%20of%20the%20class%20K%20functions%0Aincluded%20in%20the%20CBF%20condition%2C%20under%20input%20constraints.%20We%20introduce%20the%0Aconcept%20of%20locally%20validated%20CBF%20parameters%2C%20which%20are%20adapted%20online%20to%0Aguarantee%20finite-horizon%20safety%2C%20based%20on%20conditions%20derived%20from%20Nagumo%27s%0Atheorem%20and%20tangent%20cone%20analysis.%20To%20identify%20these%20parameters%20online%2C%20we%0Aintegrate%20a%20learning-based%20approach%20with%20an%20uncertainty-aware%20verification%0Aprocess%20that%20account%20for%20both%20epistemic%20and%20aleatoric%20uncertainties%20inherent%20in%0Aneural%20network%20predictions.%20Our%20method%20is%20demonstrated%20on%20a%20VTOL%20quadplane%0Amodel%20during%20challenging%20transition%20and%20landing%20maneuvers%2C%20showcasing%20enhanced%0Aperformance%20while%20maintaining%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03038v3&entry.124074799=Read"},
{"title": "On the Lipschitz Constant of Deep Networks and Double Descent", "author": "Matteo Gamba and Hossein Azizpour and M\u00e5rten Bj\u00f6rkman", "abstract": "  Existing bounds on the generalization error of deep networks assume some form\nof smooth or bounded dependence on the input variable, falling short of\ninvestigating the mechanisms controlling such factors in practice. In this\nwork, we present an extensive experimental study of the empirical Lipschitz\nconstant of deep networks undergoing double descent, and highlight\nnon-monotonic trends strongly correlating with the test error. Building a\nconnection between parameter-space and input-space gradients for SGD around a\ncritical point, we isolate two important factors -- namely loss landscape\ncurvature and distance of parameters from initialization -- respectively\ncontrolling optimization dynamics around a critical point and bounding model\nfunction complexity, even beyond the training data. Our study presents novels\ninsights on implicit regularization via overparameterization, and effective\nmodel complexity for networks trained in practice.\n", "link": "http://arxiv.org/abs/2301.12309v5", "date": "2025-07-23", "relevancy": 1.9704, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4873}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Lipschitz%20Constant%20of%20Deep%20Networks%20and%20Double%20Descent&body=Title%3A%20On%20the%20Lipschitz%20Constant%20of%20Deep%20Networks%20and%20Double%20Descent%0AAuthor%3A%20Matteo%20Gamba%20and%20Hossein%20Azizpour%20and%20M%C3%A5rten%20Bj%C3%B6rkman%0AAbstract%3A%20%20%20Existing%20bounds%20on%20the%20generalization%20error%20of%20deep%20networks%20assume%20some%20form%0Aof%20smooth%20or%20bounded%20dependence%20on%20the%20input%20variable%2C%20falling%20short%20of%0Ainvestigating%20the%20mechanisms%20controlling%20such%20factors%20in%20practice.%20In%20this%0Awork%2C%20we%20present%20an%20extensive%20experimental%20study%20of%20the%20empirical%20Lipschitz%0Aconstant%20of%20deep%20networks%20undergoing%20double%20descent%2C%20and%20highlight%0Anon-monotonic%20trends%20strongly%20correlating%20with%20the%20test%20error.%20Building%20a%0Aconnection%20between%20parameter-space%20and%20input-space%20gradients%20for%20SGD%20around%20a%0Acritical%20point%2C%20we%20isolate%20two%20important%20factors%20--%20namely%20loss%20landscape%0Acurvature%20and%20distance%20of%20parameters%20from%20initialization%20--%20respectively%0Acontrolling%20optimization%20dynamics%20around%20a%20critical%20point%20and%20bounding%20model%0Afunction%20complexity%2C%20even%20beyond%20the%20training%20data.%20Our%20study%20presents%20novels%0Ainsights%20on%20implicit%20regularization%20via%20overparameterization%2C%20and%20effective%0Amodel%20complexity%20for%20networks%20trained%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12309v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Lipschitz%2520Constant%2520of%2520Deep%2520Networks%2520and%2520Double%2520Descent%26entry.906535625%3DMatteo%2520Gamba%2520and%2520Hossein%2520Azizpour%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%26entry.1292438233%3D%2520%2520Existing%2520bounds%2520on%2520the%2520generalization%2520error%2520of%2520deep%2520networks%2520assume%2520some%2520form%250Aof%2520smooth%2520or%2520bounded%2520dependence%2520on%2520the%2520input%2520variable%252C%2520falling%2520short%2520of%250Ainvestigating%2520the%2520mechanisms%2520controlling%2520such%2520factors%2520in%2520practice.%2520In%2520this%250Awork%252C%2520we%2520present%2520an%2520extensive%2520experimental%2520study%2520of%2520the%2520empirical%2520Lipschitz%250Aconstant%2520of%2520deep%2520networks%2520undergoing%2520double%2520descent%252C%2520and%2520highlight%250Anon-monotonic%2520trends%2520strongly%2520correlating%2520with%2520the%2520test%2520error.%2520Building%2520a%250Aconnection%2520between%2520parameter-space%2520and%2520input-space%2520gradients%2520for%2520SGD%2520around%2520a%250Acritical%2520point%252C%2520we%2520isolate%2520two%2520important%2520factors%2520--%2520namely%2520loss%2520landscape%250Acurvature%2520and%2520distance%2520of%2520parameters%2520from%2520initialization%2520--%2520respectively%250Acontrolling%2520optimization%2520dynamics%2520around%2520a%2520critical%2520point%2520and%2520bounding%2520model%250Afunction%2520complexity%252C%2520even%2520beyond%2520the%2520training%2520data.%2520Our%2520study%2520presents%2520novels%250Ainsights%2520on%2520implicit%2520regularization%2520via%2520overparameterization%252C%2520and%2520effective%250Amodel%2520complexity%2520for%2520networks%2520trained%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.12309v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Lipschitz%20Constant%20of%20Deep%20Networks%20and%20Double%20Descent&entry.906535625=Matteo%20Gamba%20and%20Hossein%20Azizpour%20and%20M%C3%A5rten%20Bj%C3%B6rkman&entry.1292438233=%20%20Existing%20bounds%20on%20the%20generalization%20error%20of%20deep%20networks%20assume%20some%20form%0Aof%20smooth%20or%20bounded%20dependence%20on%20the%20input%20variable%2C%20falling%20short%20of%0Ainvestigating%20the%20mechanisms%20controlling%20such%20factors%20in%20practice.%20In%20this%0Awork%2C%20we%20present%20an%20extensive%20experimental%20study%20of%20the%20empirical%20Lipschitz%0Aconstant%20of%20deep%20networks%20undergoing%20double%20descent%2C%20and%20highlight%0Anon-monotonic%20trends%20strongly%20correlating%20with%20the%20test%20error.%20Building%20a%0Aconnection%20between%20parameter-space%20and%20input-space%20gradients%20for%20SGD%20around%20a%0Acritical%20point%2C%20we%20isolate%20two%20important%20factors%20--%20namely%20loss%20landscape%0Acurvature%20and%20distance%20of%20parameters%20from%20initialization%20--%20respectively%0Acontrolling%20optimization%20dynamics%20around%20a%20critical%20point%20and%20bounding%20model%0Afunction%20complexity%2C%20even%20beyond%20the%20training%20data.%20Our%20study%20presents%20novels%0Ainsights%20on%20implicit%20regularization%20via%20overparameterization%2C%20and%20effective%0Amodel%20complexity%20for%20networks%20trained%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12309v5&entry.124074799=Read"},
{"title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and\n  PlanetScope Data", "author": "Ioannis Kontogiorgakis and Iason Tsardanidis and Dimitrios Bormpoudakis and Ilias Tsoumas and Dimitra A. Loka and Christos Noulas and Alexandros Tsitouras and Charalampos Kontoes", "abstract": "  Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as they commonly rely on\nground-based field surveys, which are often costly, time-consuming and subject\nto delays. In order to tackle this problem, we leverage earth observation data\nand Machine Learning (ML). Specifically, we developed separate ML models using\nSentinel-2 and PlanetScope satellite time series data, respectively, to\nclassify four distinct weed management methods (Mowing, Tillage,\nChemical-spraying, and No practice) in orchards. The findings demonstrate the\npotential of ML-driven remote sensing to enhance the efficiency and accuracy of\nweed management mapping in orchards.\n", "link": "http://arxiv.org/abs/2504.19991v2", "date": "2025-07-23", "relevancy": 1.3157, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20of%20Weed%20Management%20Methods%20in%20Orchards%20using%20Sentinel-2%20and%0A%20%20PlanetScope%20Data&body=Title%3A%20Mapping%20of%20Weed%20Management%20Methods%20in%20Orchards%20using%20Sentinel-2%20and%0A%20%20PlanetScope%20Data%0AAuthor%3A%20Ioannis%20Kontogiorgakis%20and%20Iason%20Tsardanidis%20and%20Dimitrios%20Bormpoudakis%20and%20Ilias%20Tsoumas%20and%20Dimitra%20A.%20Loka%20and%20Christos%20Noulas%20and%20Alexandros%20Tsitouras%20and%20Charalampos%20Kontoes%0AAbstract%3A%20%20%20Effective%20weed%20management%20is%20crucial%20for%20improving%20agricultural%20productivity%2C%0Aas%20weeds%20compete%20with%20crops%20for%20vital%20resources%20like%20nutrients%20and%20water.%0AAccurate%20maps%20of%20weed%20management%20methods%20are%20essential%20for%20policymakers%20to%0Aassess%20farmer%20practices%2C%20evaluate%20impacts%20on%20vegetation%20health%2C%20biodiversity%2C%0Aand%20climate%2C%20as%20well%20as%20ensure%20compliance%20with%20policies%20and%20subsidies.%20However%2C%0Amonitoring%20weed%20management%20methods%20is%20challenging%20as%20they%20commonly%20rely%20on%0Aground-based%20field%20surveys%2C%20which%20are%20often%20costly%2C%20time-consuming%20and%20subject%0Ato%20delays.%20In%20order%20to%20tackle%20this%20problem%2C%20we%20leverage%20earth%20observation%20data%0Aand%20Machine%20Learning%20%28ML%29.%20Specifically%2C%20we%20developed%20separate%20ML%20models%20using%0ASentinel-2%20and%20PlanetScope%20satellite%20time%20series%20data%2C%20respectively%2C%20to%0Aclassify%20four%20distinct%20weed%20management%20methods%20%28Mowing%2C%20Tillage%2C%0AChemical-spraying%2C%20and%20No%20practice%29%20in%20orchards.%20The%20findings%20demonstrate%20the%0Apotential%20of%20ML-driven%20remote%20sensing%20to%20enhance%20the%20efficiency%20and%20accuracy%20of%0Aweed%20management%20mapping%20in%20orchards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520of%2520Weed%2520Management%2520Methods%2520in%2520Orchards%2520using%2520Sentinel-2%2520and%250A%2520%2520PlanetScope%2520Data%26entry.906535625%3DIoannis%2520Kontogiorgakis%2520and%2520Iason%2520Tsardanidis%2520and%2520Dimitrios%2520Bormpoudakis%2520and%2520Ilias%2520Tsoumas%2520and%2520Dimitra%2520A.%2520Loka%2520and%2520Christos%2520Noulas%2520and%2520Alexandros%2520Tsitouras%2520and%2520Charalampos%2520Kontoes%26entry.1292438233%3D%2520%2520Effective%2520weed%2520management%2520is%2520crucial%2520for%2520improving%2520agricultural%2520productivity%252C%250Aas%2520weeds%2520compete%2520with%2520crops%2520for%2520vital%2520resources%2520like%2520nutrients%2520and%2520water.%250AAccurate%2520maps%2520of%2520weed%2520management%2520methods%2520are%2520essential%2520for%2520policymakers%2520to%250Aassess%2520farmer%2520practices%252C%2520evaluate%2520impacts%2520on%2520vegetation%2520health%252C%2520biodiversity%252C%250Aand%2520climate%252C%2520as%2520well%2520as%2520ensure%2520compliance%2520with%2520policies%2520and%2520subsidies.%2520However%252C%250Amonitoring%2520weed%2520management%2520methods%2520is%2520challenging%2520as%2520they%2520commonly%2520rely%2520on%250Aground-based%2520field%2520surveys%252C%2520which%2520are%2520often%2520costly%252C%2520time-consuming%2520and%2520subject%250Ato%2520delays.%2520In%2520order%2520to%2520tackle%2520this%2520problem%252C%2520we%2520leverage%2520earth%2520observation%2520data%250Aand%2520Machine%2520Learning%2520%2528ML%2529.%2520Specifically%252C%2520we%2520developed%2520separate%2520ML%2520models%2520using%250ASentinel-2%2520and%2520PlanetScope%2520satellite%2520time%2520series%2520data%252C%2520respectively%252C%2520to%250Aclassify%2520four%2520distinct%2520weed%2520management%2520methods%2520%2528Mowing%252C%2520Tillage%252C%250AChemical-spraying%252C%2520and%2520No%2520practice%2529%2520in%2520orchards.%2520The%2520findings%2520demonstrate%2520the%250Apotential%2520of%2520ML-driven%2520remote%2520sensing%2520to%2520enhance%2520the%2520efficiency%2520and%2520accuracy%2520of%250Aweed%2520management%2520mapping%2520in%2520orchards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20of%20Weed%20Management%20Methods%20in%20Orchards%20using%20Sentinel-2%20and%0A%20%20PlanetScope%20Data&entry.906535625=Ioannis%20Kontogiorgakis%20and%20Iason%20Tsardanidis%20and%20Dimitrios%20Bormpoudakis%20and%20Ilias%20Tsoumas%20and%20Dimitra%20A.%20Loka%20and%20Christos%20Noulas%20and%20Alexandros%20Tsitouras%20and%20Charalampos%20Kontoes&entry.1292438233=%20%20Effective%20weed%20management%20is%20crucial%20for%20improving%20agricultural%20productivity%2C%0Aas%20weeds%20compete%20with%20crops%20for%20vital%20resources%20like%20nutrients%20and%20water.%0AAccurate%20maps%20of%20weed%20management%20methods%20are%20essential%20for%20policymakers%20to%0Aassess%20farmer%20practices%2C%20evaluate%20impacts%20on%20vegetation%20health%2C%20biodiversity%2C%0Aand%20climate%2C%20as%20well%20as%20ensure%20compliance%20with%20policies%20and%20subsidies.%20However%2C%0Amonitoring%20weed%20management%20methods%20is%20challenging%20as%20they%20commonly%20rely%20on%0Aground-based%20field%20surveys%2C%20which%20are%20often%20costly%2C%20time-consuming%20and%20subject%0Ato%20delays.%20In%20order%20to%20tackle%20this%20problem%2C%20we%20leverage%20earth%20observation%20data%0Aand%20Machine%20Learning%20%28ML%29.%20Specifically%2C%20we%20developed%20separate%20ML%20models%20using%0ASentinel-2%20and%20PlanetScope%20satellite%20time%20series%20data%2C%20respectively%2C%20to%0Aclassify%20four%20distinct%20weed%20management%20methods%20%28Mowing%2C%20Tillage%2C%0AChemical-spraying%2C%20and%20No%20practice%29%20in%20orchards.%20The%20findings%20demonstrate%20the%0Apotential%20of%20ML-driven%20remote%20sensing%20to%20enhance%20the%20efficiency%20and%20accuracy%20of%0Aweed%20management%20mapping%20in%20orchards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19991v2&entry.124074799=Read"},
{"title": "Event Detection for Active Lower Limb Prosthesis", "author": "J. D. Clark and P. Ellison", "abstract": "  Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.\n", "link": "http://arxiv.org/abs/2507.17649v1", "date": "2025-07-23", "relevancy": 1.8123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4454}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event%20Detection%20for%20Active%20Lower%20Limb%20Prosthesis&body=Title%3A%20Event%20Detection%20for%20Active%20Lower%20Limb%20Prosthesis%0AAuthor%3A%20J.%20D.%20Clark%20and%20P.%20Ellison%0AAbstract%3A%20%20%20Accurate%20event%20detection%20is%20key%20to%20the%20successful%20design%20of%20semi-passive%20and%0Apowered%20prosthetics.%20Kinematically%2C%20the%20natural%20knee%20is%20complex%2C%20with%0Atranslation%20and%20rotation%20components%20that%20have%20a%20substantial%20impact%20on%20gait%0Acharacteristics.%20When%20simplified%20to%20a%20pin%20joint%2C%20some%20of%20this%20behaviour%20is%0Alost.%20This%20study%20investigates%20the%20role%20of%20cruciate%20ligament%20stretch%20in%20event%0Adetection.%20A%20bicondylar%20knee%20design%20was%20used%2C%20constrained%20by%20analogues%20of%20the%0Aanterior%20and%20posterior%20cruciate%20ligaments.%20This%20offers%20the%20ability%20to%0Acharacterize%20knee%20kinematics%20by%20the%20stretch%20of%20the%20ligaments.%20The%20ligament%0Astretch%20was%20recorded%20using%20LVDTs%20parallel%20to%20the%20ligaments%20of%20the%20Russell%20knee%0Aon%20a%20bent%20knee%20crutch.%20Which%20was%20used%20to%20capture%20data%20on%20a%20treadmill%20at%203%0Aspeeds.%20This%20study%20finds%20speed%20dependence%20within%20the%20stretch%20of%20the%20cruciate%0Aligaments%2C%20prominently%20around%205%5C%25%20and%2080%5C%25%20of%20the%20gait%20cycle%20for%20the%20posterior%0Aand%20anterior.%20The%20cycle%20profile%20remains%20consistent%20with%20speed%3B%20therefore%2C%20other%0Astatic%20events%20such%20as%20the%20turning%20point%20feature%20at%20around%2090%5C%25%20and%2095%5C%25%20of%20the%0Acycle%2C%20for%20the%20posterior%20and%20anterior%2C%20respectively%2C%20could%20be%20used%20as%20a%0Apredictive%20precursor%20for%20initial%20contact.%20Likewise%20at%2090%5C%25%20and%2095%5C%25%2C%20another%0Apair%20of%20turning%20points%20that%20in%20this%20case%20could%20be%20used%20to%20predict%20foot%20flat.%0AThis%20concludes%20that%20the%20use%20of%20a%20bicondylar%20knee%20design%20could%20improve%20the%0Adetection%20of%20events%20during%20the%20gait%20cycle%2C%20and%20therefore%20could%20increase%20the%0Aaccuracy%20of%20subsequent%20controllers%20for%20powered%20prosthetics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent%2520Detection%2520for%2520Active%2520Lower%2520Limb%2520Prosthesis%26entry.906535625%3DJ.%2520D.%2520Clark%2520and%2520P.%2520Ellison%26entry.1292438233%3D%2520%2520Accurate%2520event%2520detection%2520is%2520key%2520to%2520the%2520successful%2520design%2520of%2520semi-passive%2520and%250Apowered%2520prosthetics.%2520Kinematically%252C%2520the%2520natural%2520knee%2520is%2520complex%252C%2520with%250Atranslation%2520and%2520rotation%2520components%2520that%2520have%2520a%2520substantial%2520impact%2520on%2520gait%250Acharacteristics.%2520When%2520simplified%2520to%2520a%2520pin%2520joint%252C%2520some%2520of%2520this%2520behaviour%2520is%250Alost.%2520This%2520study%2520investigates%2520the%2520role%2520of%2520cruciate%2520ligament%2520stretch%2520in%2520event%250Adetection.%2520A%2520bicondylar%2520knee%2520design%2520was%2520used%252C%2520constrained%2520by%2520analogues%2520of%2520the%250Aanterior%2520and%2520posterior%2520cruciate%2520ligaments.%2520This%2520offers%2520the%2520ability%2520to%250Acharacterize%2520knee%2520kinematics%2520by%2520the%2520stretch%2520of%2520the%2520ligaments.%2520The%2520ligament%250Astretch%2520was%2520recorded%2520using%2520LVDTs%2520parallel%2520to%2520the%2520ligaments%2520of%2520the%2520Russell%2520knee%250Aon%2520a%2520bent%2520knee%2520crutch.%2520Which%2520was%2520used%2520to%2520capture%2520data%2520on%2520a%2520treadmill%2520at%25203%250Aspeeds.%2520This%2520study%2520finds%2520speed%2520dependence%2520within%2520the%2520stretch%2520of%2520the%2520cruciate%250Aligaments%252C%2520prominently%2520around%25205%255C%2525%2520and%252080%255C%2525%2520of%2520the%2520gait%2520cycle%2520for%2520the%2520posterior%250Aand%2520anterior.%2520The%2520cycle%2520profile%2520remains%2520consistent%2520with%2520speed%253B%2520therefore%252C%2520other%250Astatic%2520events%2520such%2520as%2520the%2520turning%2520point%2520feature%2520at%2520around%252090%255C%2525%2520and%252095%255C%2525%2520of%2520the%250Acycle%252C%2520for%2520the%2520posterior%2520and%2520anterior%252C%2520respectively%252C%2520could%2520be%2520used%2520as%2520a%250Apredictive%2520precursor%2520for%2520initial%2520contact.%2520Likewise%2520at%252090%255C%2525%2520and%252095%255C%2525%252C%2520another%250Apair%2520of%2520turning%2520points%2520that%2520in%2520this%2520case%2520could%2520be%2520used%2520to%2520predict%2520foot%2520flat.%250AThis%2520concludes%2520that%2520the%2520use%2520of%2520a%2520bicondylar%2520knee%2520design%2520could%2520improve%2520the%250Adetection%2520of%2520events%2520during%2520the%2520gait%2520cycle%252C%2520and%2520therefore%2520could%2520increase%2520the%250Aaccuracy%2520of%2520subsequent%2520controllers%2520for%2520powered%2520prosthetics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event%20Detection%20for%20Active%20Lower%20Limb%20Prosthesis&entry.906535625=J.%20D.%20Clark%20and%20P.%20Ellison&entry.1292438233=%20%20Accurate%20event%20detection%20is%20key%20to%20the%20successful%20design%20of%20semi-passive%20and%0Apowered%20prosthetics.%20Kinematically%2C%20the%20natural%20knee%20is%20complex%2C%20with%0Atranslation%20and%20rotation%20components%20that%20have%20a%20substantial%20impact%20on%20gait%0Acharacteristics.%20When%20simplified%20to%20a%20pin%20joint%2C%20some%20of%20this%20behaviour%20is%0Alost.%20This%20study%20investigates%20the%20role%20of%20cruciate%20ligament%20stretch%20in%20event%0Adetection.%20A%20bicondylar%20knee%20design%20was%20used%2C%20constrained%20by%20analogues%20of%20the%0Aanterior%20and%20posterior%20cruciate%20ligaments.%20This%20offers%20the%20ability%20to%0Acharacterize%20knee%20kinematics%20by%20the%20stretch%20of%20the%20ligaments.%20The%20ligament%0Astretch%20was%20recorded%20using%20LVDTs%20parallel%20to%20the%20ligaments%20of%20the%20Russell%20knee%0Aon%20a%20bent%20knee%20crutch.%20Which%20was%20used%20to%20capture%20data%20on%20a%20treadmill%20at%203%0Aspeeds.%20This%20study%20finds%20speed%20dependence%20within%20the%20stretch%20of%20the%20cruciate%0Aligaments%2C%20prominently%20around%205%5C%25%20and%2080%5C%25%20of%20the%20gait%20cycle%20for%20the%20posterior%0Aand%20anterior.%20The%20cycle%20profile%20remains%20consistent%20with%20speed%3B%20therefore%2C%20other%0Astatic%20events%20such%20as%20the%20turning%20point%20feature%20at%20around%2090%5C%25%20and%2095%5C%25%20of%20the%0Acycle%2C%20for%20the%20posterior%20and%20anterior%2C%20respectively%2C%20could%20be%20used%20as%20a%0Apredictive%20precursor%20for%20initial%20contact.%20Likewise%20at%2090%5C%25%20and%2095%5C%25%2C%20another%0Apair%20of%20turning%20points%20that%20in%20this%20case%20could%20be%20used%20to%20predict%20foot%20flat.%0AThis%20concludes%20that%20the%20use%20of%20a%20bicondylar%20knee%20design%20could%20improve%20the%0Adetection%20of%20events%20during%20the%20gait%20cycle%2C%20and%20therefore%20could%20increase%20the%0Aaccuracy%20of%20subsequent%20controllers%20for%20powered%20prosthetics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17649v1&entry.124074799=Read"},
{"title": "Learning from Scratch: Structurally-masked Transformer for Next\n  Generation Lib-free Simulation", "author": "Junlang Huang and Hao Chen and Zhong Guan", "abstract": "  This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors.\n", "link": "http://arxiv.org/abs/2507.17396v1", "date": "2025-07-23", "relevancy": 1.5508, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5658}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5157}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Scratch%3A%20Structurally-masked%20Transformer%20for%20Next%0A%20%20Generation%20Lib-free%20Simulation&body=Title%3A%20Learning%20from%20Scratch%3A%20Structurally-masked%20Transformer%20for%20Next%0A%20%20Generation%20Lib-free%20Simulation%0AAuthor%3A%20Junlang%20Huang%20and%20Hao%20Chen%20and%20Zhong%20Guan%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20neural%20framework%20for%20power%20and%20timing%20prediction%20of%0Amulti-stage%20data%20path%2C%20distinguishing%20itself%20from%20traditional%20lib-based%0Aanalytical%20methods%20dependent%20on%20driver%20characterization%20and%20load%0Asimplifications.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Alanguage-based%2C%20netlist-aware%20neural%20network%20designed%20explicitly%20for%20standard%0Acells.%20Our%20approach%20employs%20two%20pre-trained%20neural%20models%20of%20waveform%0Aprediction%20and%20delay%20estimation%20that%20directly%20infer%20transient%20waveforms%20and%0Apropagation%20delays%20from%20SPICE%20netlists%2C%20conditioned%20on%20critical%20physical%0Aparameters%20such%20as%20load%20capacitance%2C%20input%20slew%2C%20and%20gate%20size.%20This%20method%0Aaccurately%20captures%20both%20intrinsic%20and%20coupling-induced%20delay%20effects%20without%0Arequiring%20simplification%20or%20interpolation.%20For%20multi-stage%20timing%20prediction%2C%0Awe%20implement%20a%20recursive%20propagation%20strategy%20where%20predicted%20waveforms%20from%0Aeach%20stage%20feed%20into%20subsequent%20stages%2C%20cumulatively%20capturing%20delays%20across%0Athe%20logic%20chain.%20This%20approach%20ensures%20precise%20timing%20alignment%20and%20complete%0Awaveform%20visibility%20throughout%20complex%20signal%20pathways.%20The%20waveform%20prediction%0Autilizes%20a%20hybrid%20CNN-Transformer%20architecture%20with%20netlist-aware%20node-level%0Aencoding%2C%20addressing%20traditional%20Transformers%27%20fixed%20input%20dimensionality%0Aconstraints.%20Additionally%2C%20specialized%20subnetworks%20separately%20handle%20primary%0Adelay%20estimation%20and%20crosstalk%20correction.%20Experimental%20results%20demonstrate%0ASPICE-level%20accuracy%2C%20consistently%20achieving%20RMSE%20below%200.0098%20across%20diverse%0Aindustrial%20circuits.%20The%20proposed%20framework%20provides%20a%20scalable%2C%20structurally%0Aadaptable%20neural%20alternative%20to%20conventional%20power%20and%20timing%20engines%2C%0Ademonstrating%20high%20fidelity%20to%20physical%20circuit%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Scratch%253A%2520Structurally-masked%2520Transformer%2520for%2520Next%250A%2520%2520Generation%2520Lib-free%2520Simulation%26entry.906535625%3DJunlang%2520Huang%2520and%2520Hao%2520Chen%2520and%2520Zhong%2520Guan%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520neural%2520framework%2520for%2520power%2520and%2520timing%2520prediction%2520of%250Amulti-stage%2520data%2520path%252C%2520distinguishing%2520itself%2520from%2520traditional%2520lib-based%250Aanalytical%2520methods%2520dependent%2520on%2520driver%2520characterization%2520and%2520load%250Asimplifications.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Alanguage-based%252C%2520netlist-aware%2520neural%2520network%2520designed%2520explicitly%2520for%2520standard%250Acells.%2520Our%2520approach%2520employs%2520two%2520pre-trained%2520neural%2520models%2520of%2520waveform%250Aprediction%2520and%2520delay%2520estimation%2520that%2520directly%2520infer%2520transient%2520waveforms%2520and%250Apropagation%2520delays%2520from%2520SPICE%2520netlists%252C%2520conditioned%2520on%2520critical%2520physical%250Aparameters%2520such%2520as%2520load%2520capacitance%252C%2520input%2520slew%252C%2520and%2520gate%2520size.%2520This%2520method%250Aaccurately%2520captures%2520both%2520intrinsic%2520and%2520coupling-induced%2520delay%2520effects%2520without%250Arequiring%2520simplification%2520or%2520interpolation.%2520For%2520multi-stage%2520timing%2520prediction%252C%250Awe%2520implement%2520a%2520recursive%2520propagation%2520strategy%2520where%2520predicted%2520waveforms%2520from%250Aeach%2520stage%2520feed%2520into%2520subsequent%2520stages%252C%2520cumulatively%2520capturing%2520delays%2520across%250Athe%2520logic%2520chain.%2520This%2520approach%2520ensures%2520precise%2520timing%2520alignment%2520and%2520complete%250Awaveform%2520visibility%2520throughout%2520complex%2520signal%2520pathways.%2520The%2520waveform%2520prediction%250Autilizes%2520a%2520hybrid%2520CNN-Transformer%2520architecture%2520with%2520netlist-aware%2520node-level%250Aencoding%252C%2520addressing%2520traditional%2520Transformers%2527%2520fixed%2520input%2520dimensionality%250Aconstraints.%2520Additionally%252C%2520specialized%2520subnetworks%2520separately%2520handle%2520primary%250Adelay%2520estimation%2520and%2520crosstalk%2520correction.%2520Experimental%2520results%2520demonstrate%250ASPICE-level%2520accuracy%252C%2520consistently%2520achieving%2520RMSE%2520below%25200.0098%2520across%2520diverse%250Aindustrial%2520circuits.%2520The%2520proposed%2520framework%2520provides%2520a%2520scalable%252C%2520structurally%250Aadaptable%2520neural%2520alternative%2520to%2520conventional%2520power%2520and%2520timing%2520engines%252C%250Ademonstrating%2520high%2520fidelity%2520to%2520physical%2520circuit%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Scratch%3A%20Structurally-masked%20Transformer%20for%20Next%0A%20%20Generation%20Lib-free%20Simulation&entry.906535625=Junlang%20Huang%20and%20Hao%20Chen%20and%20Zhong%20Guan&entry.1292438233=%20%20This%20paper%20proposes%20a%20neural%20framework%20for%20power%20and%20timing%20prediction%20of%0Amulti-stage%20data%20path%2C%20distinguishing%20itself%20from%20traditional%20lib-based%0Aanalytical%20methods%20dependent%20on%20driver%20characterization%20and%20load%0Asimplifications.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Alanguage-based%2C%20netlist-aware%20neural%20network%20designed%20explicitly%20for%20standard%0Acells.%20Our%20approach%20employs%20two%20pre-trained%20neural%20models%20of%20waveform%0Aprediction%20and%20delay%20estimation%20that%20directly%20infer%20transient%20waveforms%20and%0Apropagation%20delays%20from%20SPICE%20netlists%2C%20conditioned%20on%20critical%20physical%0Aparameters%20such%20as%20load%20capacitance%2C%20input%20slew%2C%20and%20gate%20size.%20This%20method%0Aaccurately%20captures%20both%20intrinsic%20and%20coupling-induced%20delay%20effects%20without%0Arequiring%20simplification%20or%20interpolation.%20For%20multi-stage%20timing%20prediction%2C%0Awe%20implement%20a%20recursive%20propagation%20strategy%20where%20predicted%20waveforms%20from%0Aeach%20stage%20feed%20into%20subsequent%20stages%2C%20cumulatively%20capturing%20delays%20across%0Athe%20logic%20chain.%20This%20approach%20ensures%20precise%20timing%20alignment%20and%20complete%0Awaveform%20visibility%20throughout%20complex%20signal%20pathways.%20The%20waveform%20prediction%0Autilizes%20a%20hybrid%20CNN-Transformer%20architecture%20with%20netlist-aware%20node-level%0Aencoding%2C%20addressing%20traditional%20Transformers%27%20fixed%20input%20dimensionality%0Aconstraints.%20Additionally%2C%20specialized%20subnetworks%20separately%20handle%20primary%0Adelay%20estimation%20and%20crosstalk%20correction.%20Experimental%20results%20demonstrate%0ASPICE-level%20accuracy%2C%20consistently%20achieving%20RMSE%20below%200.0098%20across%20diverse%0Aindustrial%20circuits.%20The%20proposed%20framework%20provides%20a%20scalable%2C%20structurally%0Aadaptable%20neural%20alternative%20to%20conventional%20power%20and%20timing%20engines%2C%0Ademonstrating%20high%20fidelity%20to%20physical%20circuit%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17396v1&entry.124074799=Read"},
{"title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks", "author": "Ilias Chatzistefanidis and Navid Nikaein", "abstract": "  Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.\n", "link": "http://arxiv.org/abs/2507.17695v1", "date": "2025-07-23", "relevancy": 2.0216, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5244}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4972}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbiotic%20Agents%3A%20A%20Novel%20Paradigm%20for%20Trustworthy%20AGI-driven%20Networks&body=Title%3A%20Symbiotic%20Agents%3A%20A%20Novel%20Paradigm%20for%20Trustworthy%20AGI-driven%20Networks%0AAuthor%3A%20Ilias%20Chatzistefanidis%20and%20Navid%20Nikaein%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20autonomous%20agents%20are%20expected%20to%20play%20a%0Avital%20role%20in%20the%20evolution%20of%206G%20networks%2C%20by%20empowering%20real-time%0Adecision-making%20related%20to%20management%20and%20service%20provisioning%20to%20end-users.%0AThis%20shift%20facilitates%20the%20transition%20from%20a%20specialized%20intelligence%20approach%2C%0Awhere%20artificial%20intelligence%20%28AI%29%20algorithms%20handle%20isolated%20tasks%2C%20to%0Aartificial%20general%20intelligence%20%28AGI%29-driven%20networks%2C%20where%20agents%20possess%0Abroader%20reasoning%20capabilities%20and%20can%20manage%20diverse%20network%20functions.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20agentic%20paradigm%20that%20combines%20LLMs%20with%0Areal-time%20optimization%20algorithms%20towards%20Trustworthy%20AI%2C%20defined%20as%20symbiotic%0Aagents.%20Optimizers%20at%20the%20LLM%27s%20input-level%20provide%20bounded%20uncertainty%0Asteering%20for%20numerically%20precise%20tasks%2C%20whereas%20output-level%20optimizers%0Asupervised%20by%20the%20LLM%20enable%20adaptive%20real-time%20control.%20We%20design%20and%0Aimplement%20two%20novel%20agent%20types%20including%3A%20%28i%29%20Radio%20Access%20Network%20optimizers%2C%0Aand%20%28ii%29%20multi-agent%20negotiators%20for%20Service-Level%20Agreements%20%28SLAs%29.%20We%0Afurther%20propose%20an%20end-to-end%20architecture%20for%20AGI%20networks%20and%20evaluate%20it%20on%0Aa%205G%20testbed%20capturing%20channel%20fluctuations%20from%20moving%20vehicles.%20Results%20show%0Athat%20symbiotic%20agents%20reduce%20decision%20errors%20fivefold%20compared%20to%20standalone%0ALLM-based%20agents%2C%20while%20smaller%20language%20models%20%28SLM%29%20achieve%20similar%20accuracy%0Awith%20a%2099.9%25%20reduction%20in%20GPU%20resource%20overhead%20and%20in%20near-real-time%20loops%20of%0A82%20ms.%20A%20multi-agent%20demonstration%20for%20collaborative%20RAN%20on%20the%20real-world%0Atestbed%20highlights%20significant%20flexibility%20in%20service-level%20agreement%20and%0Aresource%20allocation%2C%20reducing%20RAN%20over-utilization%20by%20approximately%2044%25.%0ADrawing%20on%20our%20findings%20and%20open-source%20implementations%2C%20we%20introduce%20the%0Asymbiotic%20paradigm%20as%20the%20foundation%20for%20next-generation%2C%20AGI-driven%0Anetworks-systems%20designed%20to%20remain%20adaptable%2C%20efficient%2C%20and%20trustworthy%20even%0Aas%20LLMs%20advance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbiotic%2520Agents%253A%2520A%2520Novel%2520Paradigm%2520for%2520Trustworthy%2520AGI-driven%2520Networks%26entry.906535625%3DIlias%2520Chatzistefanidis%2520and%2520Navid%2520Nikaein%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520autonomous%2520agents%2520are%2520expected%2520to%2520play%2520a%250Avital%2520role%2520in%2520the%2520evolution%2520of%25206G%2520networks%252C%2520by%2520empowering%2520real-time%250Adecision-making%2520related%2520to%2520management%2520and%2520service%2520provisioning%2520to%2520end-users.%250AThis%2520shift%2520facilitates%2520the%2520transition%2520from%2520a%2520specialized%2520intelligence%2520approach%252C%250Awhere%2520artificial%2520intelligence%2520%2528AI%2529%2520algorithms%2520handle%2520isolated%2520tasks%252C%2520to%250Aartificial%2520general%2520intelligence%2520%2528AGI%2529-driven%2520networks%252C%2520where%2520agents%2520possess%250Abroader%2520reasoning%2520capabilities%2520and%2520can%2520manage%2520diverse%2520network%2520functions.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520agentic%2520paradigm%2520that%2520combines%2520LLMs%2520with%250Areal-time%2520optimization%2520algorithms%2520towards%2520Trustworthy%2520AI%252C%2520defined%2520as%2520symbiotic%250Aagents.%2520Optimizers%2520at%2520the%2520LLM%2527s%2520input-level%2520provide%2520bounded%2520uncertainty%250Asteering%2520for%2520numerically%2520precise%2520tasks%252C%2520whereas%2520output-level%2520optimizers%250Asupervised%2520by%2520the%2520LLM%2520enable%2520adaptive%2520real-time%2520control.%2520We%2520design%2520and%250Aimplement%2520two%2520novel%2520agent%2520types%2520including%253A%2520%2528i%2529%2520Radio%2520Access%2520Network%2520optimizers%252C%250Aand%2520%2528ii%2529%2520multi-agent%2520negotiators%2520for%2520Service-Level%2520Agreements%2520%2528SLAs%2529.%2520We%250Afurther%2520propose%2520an%2520end-to-end%2520architecture%2520for%2520AGI%2520networks%2520and%2520evaluate%2520it%2520on%250Aa%25205G%2520testbed%2520capturing%2520channel%2520fluctuations%2520from%2520moving%2520vehicles.%2520Results%2520show%250Athat%2520symbiotic%2520agents%2520reduce%2520decision%2520errors%2520fivefold%2520compared%2520to%2520standalone%250ALLM-based%2520agents%252C%2520while%2520smaller%2520language%2520models%2520%2528SLM%2529%2520achieve%2520similar%2520accuracy%250Awith%2520a%252099.9%2525%2520reduction%2520in%2520GPU%2520resource%2520overhead%2520and%2520in%2520near-real-time%2520loops%2520of%250A82%2520ms.%2520A%2520multi-agent%2520demonstration%2520for%2520collaborative%2520RAN%2520on%2520the%2520real-world%250Atestbed%2520highlights%2520significant%2520flexibility%2520in%2520service-level%2520agreement%2520and%250Aresource%2520allocation%252C%2520reducing%2520RAN%2520over-utilization%2520by%2520approximately%252044%2525.%250ADrawing%2520on%2520our%2520findings%2520and%2520open-source%2520implementations%252C%2520we%2520introduce%2520the%250Asymbiotic%2520paradigm%2520as%2520the%2520foundation%2520for%2520next-generation%252C%2520AGI-driven%250Anetworks-systems%2520designed%2520to%2520remain%2520adaptable%252C%2520efficient%252C%2520and%2520trustworthy%2520even%250Aas%2520LLMs%2520advance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbiotic%20Agents%3A%20A%20Novel%20Paradigm%20for%20Trustworthy%20AGI-driven%20Networks&entry.906535625=Ilias%20Chatzistefanidis%20and%20Navid%20Nikaein&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20autonomous%20agents%20are%20expected%20to%20play%20a%0Avital%20role%20in%20the%20evolution%20of%206G%20networks%2C%20by%20empowering%20real-time%0Adecision-making%20related%20to%20management%20and%20service%20provisioning%20to%20end-users.%0AThis%20shift%20facilitates%20the%20transition%20from%20a%20specialized%20intelligence%20approach%2C%0Awhere%20artificial%20intelligence%20%28AI%29%20algorithms%20handle%20isolated%20tasks%2C%20to%0Aartificial%20general%20intelligence%20%28AGI%29-driven%20networks%2C%20where%20agents%20possess%0Abroader%20reasoning%20capabilities%20and%20can%20manage%20diverse%20network%20functions.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20agentic%20paradigm%20that%20combines%20LLMs%20with%0Areal-time%20optimization%20algorithms%20towards%20Trustworthy%20AI%2C%20defined%20as%20symbiotic%0Aagents.%20Optimizers%20at%20the%20LLM%27s%20input-level%20provide%20bounded%20uncertainty%0Asteering%20for%20numerically%20precise%20tasks%2C%20whereas%20output-level%20optimizers%0Asupervised%20by%20the%20LLM%20enable%20adaptive%20real-time%20control.%20We%20design%20and%0Aimplement%20two%20novel%20agent%20types%20including%3A%20%28i%29%20Radio%20Access%20Network%20optimizers%2C%0Aand%20%28ii%29%20multi-agent%20negotiators%20for%20Service-Level%20Agreements%20%28SLAs%29.%20We%0Afurther%20propose%20an%20end-to-end%20architecture%20for%20AGI%20networks%20and%20evaluate%20it%20on%0Aa%205G%20testbed%20capturing%20channel%20fluctuations%20from%20moving%20vehicles.%20Results%20show%0Athat%20symbiotic%20agents%20reduce%20decision%20errors%20fivefold%20compared%20to%20standalone%0ALLM-based%20agents%2C%20while%20smaller%20language%20models%20%28SLM%29%20achieve%20similar%20accuracy%0Awith%20a%2099.9%25%20reduction%20in%20GPU%20resource%20overhead%20and%20in%20near-real-time%20loops%20of%0A82%20ms.%20A%20multi-agent%20demonstration%20for%20collaborative%20RAN%20on%20the%20real-world%0Atestbed%20highlights%20significant%20flexibility%20in%20service-level%20agreement%20and%0Aresource%20allocation%2C%20reducing%20RAN%20over-utilization%20by%20approximately%2044%25.%0ADrawing%20on%20our%20findings%20and%20open-source%20implementations%2C%20we%20introduce%20the%0Asymbiotic%20paradigm%20as%20the%20foundation%20for%20next-generation%2C%20AGI-driven%0Anetworks-systems%20designed%20to%20remain%20adaptable%2C%20efficient%2C%20and%20trustworthy%20even%0Aas%20LLMs%20advance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17695v1&entry.124074799=Read"},
{"title": "A Mathematical Theory of Discursive Networks", "author": "Juan B. Guti\u00e9rrez", "abstract": "  Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.\n", "link": "http://arxiv.org/abs/2507.06565v5", "date": "2025-07-23", "relevancy": 1.736, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4607}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mathematical%20Theory%20of%20Discursive%20Networks&body=Title%3A%20A%20Mathematical%20Theory%20of%20Discursive%20Networks%0AAuthor%3A%20Juan%20B.%20Guti%C3%A9rrez%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20turn%20writing%20into%20a%20live%20exchange%20between%20humans%0Aand%20software.%20We%20characterize%20this%20new%20medium%20as%20a%20discursive%20network%20that%0Atreats%20people%20and%20LLMs%20as%20equal%20nodes%20and%20tracks%20how%20their%20statements%0Acirculate.%20We%20define%20the%20generation%20of%20erroneous%20information%20as%20invalidation%0A%28any%20factual%2C%20logical%2C%20or%20structural%20breach%29%20and%20show%20it%20follows%20four%20hazards%3A%0Adrift%20from%20truth%2C%20self-repair%2C%20fresh%20fabrication%2C%20and%20external%20detection.%20We%0Adevelop%20a%20general%20mathematical%20model%20of%20discursive%20networks%20that%20shows%20that%20a%0Anetwork%20governed%20only%20by%20drift%20and%20self-repair%20stabilizes%20at%20a%20modest%20error%0Arate.%20Giving%20each%20false%20claim%20even%20a%20small%20chance%20of%20peer%20review%20shifts%20the%0Asystem%20to%20a%20truth-dominant%20state.%20We%20operationalize%20peer%20review%20with%20the%0Aopen-source%20Flaws-of-Others%20%28FOO%29%20algorithm%3A%20a%20configurable%20loop%20in%20which%20any%0Aset%20of%20agents%20critique%20one%20another%20while%20a%20harmonizer%20merges%20their%20verdicts.%20We%0Aidentify%20an%20ethical%20transgression%2C%20epithesis%2C%20that%20occurs%20when%20humans%20fail%20to%0Aengage%20in%20the%20discursive%20network.%20The%20takeaway%20is%20practical%20and%20cultural%3A%0Areliability%20in%20this%20new%20medium%20comes%20not%20from%20perfecting%20single%20models%20but%20from%0Aconnecting%20imperfect%20ones%20into%20networks%20that%20enforce%20mutual%20accountability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06565v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mathematical%2520Theory%2520of%2520Discursive%2520Networks%26entry.906535625%3DJuan%2520B.%2520Guti%25C3%25A9rrez%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520turn%2520writing%2520into%2520a%2520live%2520exchange%2520between%2520humans%250Aand%2520software.%2520We%2520characterize%2520this%2520new%2520medium%2520as%2520a%2520discursive%2520network%2520that%250Atreats%2520people%2520and%2520LLMs%2520as%2520equal%2520nodes%2520and%2520tracks%2520how%2520their%2520statements%250Acirculate.%2520We%2520define%2520the%2520generation%2520of%2520erroneous%2520information%2520as%2520invalidation%250A%2528any%2520factual%252C%2520logical%252C%2520or%2520structural%2520breach%2529%2520and%2520show%2520it%2520follows%2520four%2520hazards%253A%250Adrift%2520from%2520truth%252C%2520self-repair%252C%2520fresh%2520fabrication%252C%2520and%2520external%2520detection.%2520We%250Adevelop%2520a%2520general%2520mathematical%2520model%2520of%2520discursive%2520networks%2520that%2520shows%2520that%2520a%250Anetwork%2520governed%2520only%2520by%2520drift%2520and%2520self-repair%2520stabilizes%2520at%2520a%2520modest%2520error%250Arate.%2520Giving%2520each%2520false%2520claim%2520even%2520a%2520small%2520chance%2520of%2520peer%2520review%2520shifts%2520the%250Asystem%2520to%2520a%2520truth-dominant%2520state.%2520We%2520operationalize%2520peer%2520review%2520with%2520the%250Aopen-source%2520Flaws-of-Others%2520%2528FOO%2529%2520algorithm%253A%2520a%2520configurable%2520loop%2520in%2520which%2520any%250Aset%2520of%2520agents%2520critique%2520one%2520another%2520while%2520a%2520harmonizer%2520merges%2520their%2520verdicts.%2520We%250Aidentify%2520an%2520ethical%2520transgression%252C%2520epithesis%252C%2520that%2520occurs%2520when%2520humans%2520fail%2520to%250Aengage%2520in%2520the%2520discursive%2520network.%2520The%2520takeaway%2520is%2520practical%2520and%2520cultural%253A%250Areliability%2520in%2520this%2520new%2520medium%2520comes%2520not%2520from%2520perfecting%2520single%2520models%2520but%2520from%250Aconnecting%2520imperfect%2520ones%2520into%2520networks%2520that%2520enforce%2520mutual%2520accountability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06565v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mathematical%20Theory%20of%20Discursive%20Networks&entry.906535625=Juan%20B.%20Guti%C3%A9rrez&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20turn%20writing%20into%20a%20live%20exchange%20between%20humans%0Aand%20software.%20We%20characterize%20this%20new%20medium%20as%20a%20discursive%20network%20that%0Atreats%20people%20and%20LLMs%20as%20equal%20nodes%20and%20tracks%20how%20their%20statements%0Acirculate.%20We%20define%20the%20generation%20of%20erroneous%20information%20as%20invalidation%0A%28any%20factual%2C%20logical%2C%20or%20structural%20breach%29%20and%20show%20it%20follows%20four%20hazards%3A%0Adrift%20from%20truth%2C%20self-repair%2C%20fresh%20fabrication%2C%20and%20external%20detection.%20We%0Adevelop%20a%20general%20mathematical%20model%20of%20discursive%20networks%20that%20shows%20that%20a%0Anetwork%20governed%20only%20by%20drift%20and%20self-repair%20stabilizes%20at%20a%20modest%20error%0Arate.%20Giving%20each%20false%20claim%20even%20a%20small%20chance%20of%20peer%20review%20shifts%20the%0Asystem%20to%20a%20truth-dominant%20state.%20We%20operationalize%20peer%20review%20with%20the%0Aopen-source%20Flaws-of-Others%20%28FOO%29%20algorithm%3A%20a%20configurable%20loop%20in%20which%20any%0Aset%20of%20agents%20critique%20one%20another%20while%20a%20harmonizer%20merges%20their%20verdicts.%20We%0Aidentify%20an%20ethical%20transgression%2C%20epithesis%2C%20that%20occurs%20when%20humans%20fail%20to%0Aengage%20in%20the%20discursive%20network.%20The%20takeaway%20is%20practical%20and%20cultural%3A%0Areliability%20in%20this%20new%20medium%20comes%20not%20from%20perfecting%20single%20models%20but%20from%0Aconnecting%20imperfect%20ones%20into%20networks%20that%20enforce%20mutual%20accountability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06565v5&entry.124074799=Read"},
{"title": "Unsupervised anomaly detection using Bayesian flow networks: application\n  to brain FDG PET in the context of Alzheimer's disease", "author": "Hugues Roy and Reuben Dorent and Ninon Burgos", "abstract": "  Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for\nidentifying deviations from healthy subject data and thus facilitating the\ndiagnosis of neurological disorders. In this work, we focus on Bayesian flow\nnetworks (BFNs), a novel class of generative models, which have not yet been\napplied to medical imaging or anomaly detection. BFNs combine the strength of\ndiffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension\nof BFNs for UAD, designed to: i) perform conditional image generation under\nhigh levels of spatially correlated noise, and ii) preserve subject specificity\nby incorporating a recursive feedback from the input image throughout the\ngenerative process. We evaluate AnoBFN on the challenging task of Alzheimer's\ndisease-related anomaly detection in FDG PET images. Our approach outperforms\nother state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and\ndiffusion models (AnoDDPM), demonstrating its effectiveness at detecting\nanomalies while reducing false positive rates.\n", "link": "http://arxiv.org/abs/2507.17486v1", "date": "2025-07-23", "relevancy": 1.5112, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5076}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20anomaly%20detection%20using%20Bayesian%20flow%20networks%3A%20application%0A%20%20to%20brain%20FDG%20PET%20in%20the%20context%20of%20Alzheimer%27s%20disease&body=Title%3A%20Unsupervised%20anomaly%20detection%20using%20Bayesian%20flow%20networks%3A%20application%0A%20%20to%20brain%20FDG%20PET%20in%20the%20context%20of%20Alzheimer%27s%20disease%0AAuthor%3A%20Hugues%20Roy%20and%20Reuben%20Dorent%20and%20Ninon%20Burgos%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20plays%20a%20crucial%20role%20in%20neuroimaging%20for%0Aidentifying%20deviations%20from%20healthy%20subject%20data%20and%20thus%20facilitating%20the%0Adiagnosis%20of%20neurological%20disorders.%20In%20this%20work%2C%20we%20focus%20on%20Bayesian%20flow%0Anetworks%20%28BFNs%29%2C%20a%20novel%20class%20of%20generative%20models%2C%20which%20have%20not%20yet%20been%0Aapplied%20to%20medical%20imaging%20or%20anomaly%20detection.%20BFNs%20combine%20the%20strength%20of%0Adiffusion%20frameworks%20and%20Bayesian%20inference.%20We%20introduce%20AnoBFN%2C%20an%20extension%0Aof%20BFNs%20for%20UAD%2C%20designed%20to%3A%20i%29%20perform%20conditional%20image%20generation%20under%0Ahigh%20levels%20of%20spatially%20correlated%20noise%2C%20and%20ii%29%20preserve%20subject%20specificity%0Aby%20incorporating%20a%20recursive%20feedback%20from%20the%20input%20image%20throughout%20the%0Agenerative%20process.%20We%20evaluate%20AnoBFN%20on%20the%20challenging%20task%20of%20Alzheimer%27s%0Adisease-related%20anomaly%20detection%20in%20FDG%20PET%20images.%20Our%20approach%20outperforms%0Aother%20state-of-the-art%20methods%20based%20on%20VAEs%20%28beta-VAE%29%2C%20GANs%20%28f-AnoGAN%29%2C%20and%0Adiffusion%20models%20%28AnoDDPM%29%2C%20demonstrating%20its%20effectiveness%20at%20detecting%0Aanomalies%20while%20reducing%20false%20positive%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520anomaly%2520detection%2520using%2520Bayesian%2520flow%2520networks%253A%2520application%250A%2520%2520to%2520brain%2520FDG%2520PET%2520in%2520the%2520context%2520of%2520Alzheimer%2527s%2520disease%26entry.906535625%3DHugues%2520Roy%2520and%2520Reuben%2520Dorent%2520and%2520Ninon%2520Burgos%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520%2528UAD%2529%2520plays%2520a%2520crucial%2520role%2520in%2520neuroimaging%2520for%250Aidentifying%2520deviations%2520from%2520healthy%2520subject%2520data%2520and%2520thus%2520facilitating%2520the%250Adiagnosis%2520of%2520neurological%2520disorders.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520Bayesian%2520flow%250Anetworks%2520%2528BFNs%2529%252C%2520a%2520novel%2520class%2520of%2520generative%2520models%252C%2520which%2520have%2520not%2520yet%2520been%250Aapplied%2520to%2520medical%2520imaging%2520or%2520anomaly%2520detection.%2520BFNs%2520combine%2520the%2520strength%2520of%250Adiffusion%2520frameworks%2520and%2520Bayesian%2520inference.%2520We%2520introduce%2520AnoBFN%252C%2520an%2520extension%250Aof%2520BFNs%2520for%2520UAD%252C%2520designed%2520to%253A%2520i%2529%2520perform%2520conditional%2520image%2520generation%2520under%250Ahigh%2520levels%2520of%2520spatially%2520correlated%2520noise%252C%2520and%2520ii%2529%2520preserve%2520subject%2520specificity%250Aby%2520incorporating%2520a%2520recursive%2520feedback%2520from%2520the%2520input%2520image%2520throughout%2520the%250Agenerative%2520process.%2520We%2520evaluate%2520AnoBFN%2520on%2520the%2520challenging%2520task%2520of%2520Alzheimer%2527s%250Adisease-related%2520anomaly%2520detection%2520in%2520FDG%2520PET%2520images.%2520Our%2520approach%2520outperforms%250Aother%2520state-of-the-art%2520methods%2520based%2520on%2520VAEs%2520%2528beta-VAE%2529%252C%2520GANs%2520%2528f-AnoGAN%2529%252C%2520and%250Adiffusion%2520models%2520%2528AnoDDPM%2529%252C%2520demonstrating%2520its%2520effectiveness%2520at%2520detecting%250Aanomalies%2520while%2520reducing%2520false%2520positive%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20anomaly%20detection%20using%20Bayesian%20flow%20networks%3A%20application%0A%20%20to%20brain%20FDG%20PET%20in%20the%20context%20of%20Alzheimer%27s%20disease&entry.906535625=Hugues%20Roy%20and%20Reuben%20Dorent%20and%20Ninon%20Burgos&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20plays%20a%20crucial%20role%20in%20neuroimaging%20for%0Aidentifying%20deviations%20from%20healthy%20subject%20data%20and%20thus%20facilitating%20the%0Adiagnosis%20of%20neurological%20disorders.%20In%20this%20work%2C%20we%20focus%20on%20Bayesian%20flow%0Anetworks%20%28BFNs%29%2C%20a%20novel%20class%20of%20generative%20models%2C%20which%20have%20not%20yet%20been%0Aapplied%20to%20medical%20imaging%20or%20anomaly%20detection.%20BFNs%20combine%20the%20strength%20of%0Adiffusion%20frameworks%20and%20Bayesian%20inference.%20We%20introduce%20AnoBFN%2C%20an%20extension%0Aof%20BFNs%20for%20UAD%2C%20designed%20to%3A%20i%29%20perform%20conditional%20image%20generation%20under%0Ahigh%20levels%20of%20spatially%20correlated%20noise%2C%20and%20ii%29%20preserve%20subject%20specificity%0Aby%20incorporating%20a%20recursive%20feedback%20from%20the%20input%20image%20throughout%20the%0Agenerative%20process.%20We%20evaluate%20AnoBFN%20on%20the%20challenging%20task%20of%20Alzheimer%27s%0Adisease-related%20anomaly%20detection%20in%20FDG%20PET%20images.%20Our%20approach%20outperforms%0Aother%20state-of-the-art%20methods%20based%20on%20VAEs%20%28beta-VAE%29%2C%20GANs%20%28f-AnoGAN%29%2C%20and%0Adiffusion%20models%20%28AnoDDPM%29%2C%20demonstrating%20its%20effectiveness%20at%20detecting%0Aanomalies%20while%20reducing%20false%20positive%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17486v1&entry.124074799=Read"},
{"title": "Persistent Patterns in Eye Movements: A Topological Approach to Emotion\n  Recognition", "author": "Arsha Niksa and Hooman Zare and Ali Shahrabi and Hanieh Hatami and Mohammadreza Razvan", "abstract": "  We present a topological pipeline for automated multiclass emotion\nrecognition from eye-tracking data. Delay embeddings of gaze trajectories are\nanalyzed using persistent homology. From the resulting persistence diagrams, we\nextract shape-based features such as mean persistence, maximum persistence, and\nentropy. A random forest classifier trained on these features achieves up to\n$75.6\\%$ accuracy on four emotion classes, which are the quadrants the\nCircumplex Model of Affect. The results demonstrate that persistence diagram\ngeometry effectively encodes discriminative gaze dynamics, suggesting a\npromising topological approach for affective computing and human behavior\nanalysis.\n", "link": "http://arxiv.org/abs/2507.17450v1", "date": "2025-07-23", "relevancy": 1.9414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4858}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Patterns%20in%20Eye%20Movements%3A%20A%20Topological%20Approach%20to%20Emotion%0A%20%20Recognition&body=Title%3A%20Persistent%20Patterns%20in%20Eye%20Movements%3A%20A%20Topological%20Approach%20to%20Emotion%0A%20%20Recognition%0AAuthor%3A%20Arsha%20Niksa%20and%20Hooman%20Zare%20and%20Ali%20Shahrabi%20and%20Hanieh%20Hatami%20and%20Mohammadreza%20Razvan%0AAbstract%3A%20%20%20We%20present%20a%20topological%20pipeline%20for%20automated%20multiclass%20emotion%0Arecognition%20from%20eye-tracking%20data.%20Delay%20embeddings%20of%20gaze%20trajectories%20are%0Aanalyzed%20using%20persistent%20homology.%20From%20the%20resulting%20persistence%20diagrams%2C%20we%0Aextract%20shape-based%20features%20such%20as%20mean%20persistence%2C%20maximum%20persistence%2C%20and%0Aentropy.%20A%20random%20forest%20classifier%20trained%20on%20these%20features%20achieves%20up%20to%0A%2475.6%5C%25%24%20accuracy%20on%20four%20emotion%20classes%2C%20which%20are%20the%20quadrants%20the%0ACircumplex%20Model%20of%20Affect.%20The%20results%20demonstrate%20that%20persistence%20diagram%0Ageometry%20effectively%20encodes%20discriminative%20gaze%20dynamics%2C%20suggesting%20a%0Apromising%20topological%20approach%20for%20affective%20computing%20and%20human%20behavior%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Patterns%2520in%2520Eye%2520Movements%253A%2520A%2520Topological%2520Approach%2520to%2520Emotion%250A%2520%2520Recognition%26entry.906535625%3DArsha%2520Niksa%2520and%2520Hooman%2520Zare%2520and%2520Ali%2520Shahrabi%2520and%2520Hanieh%2520Hatami%2520and%2520Mohammadreza%2520Razvan%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520topological%2520pipeline%2520for%2520automated%2520multiclass%2520emotion%250Arecognition%2520from%2520eye-tracking%2520data.%2520Delay%2520embeddings%2520of%2520gaze%2520trajectories%2520are%250Aanalyzed%2520using%2520persistent%2520homology.%2520From%2520the%2520resulting%2520persistence%2520diagrams%252C%2520we%250Aextract%2520shape-based%2520features%2520such%2520as%2520mean%2520persistence%252C%2520maximum%2520persistence%252C%2520and%250Aentropy.%2520A%2520random%2520forest%2520classifier%2520trained%2520on%2520these%2520features%2520achieves%2520up%2520to%250A%252475.6%255C%2525%2524%2520accuracy%2520on%2520four%2520emotion%2520classes%252C%2520which%2520are%2520the%2520quadrants%2520the%250ACircumplex%2520Model%2520of%2520Affect.%2520The%2520results%2520demonstrate%2520that%2520persistence%2520diagram%250Ageometry%2520effectively%2520encodes%2520discriminative%2520gaze%2520dynamics%252C%2520suggesting%2520a%250Apromising%2520topological%2520approach%2520for%2520affective%2520computing%2520and%2520human%2520behavior%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Patterns%20in%20Eye%20Movements%3A%20A%20Topological%20Approach%20to%20Emotion%0A%20%20Recognition&entry.906535625=Arsha%20Niksa%20and%20Hooman%20Zare%20and%20Ali%20Shahrabi%20and%20Hanieh%20Hatami%20and%20Mohammadreza%20Razvan&entry.1292438233=%20%20We%20present%20a%20topological%20pipeline%20for%20automated%20multiclass%20emotion%0Arecognition%20from%20eye-tracking%20data.%20Delay%20embeddings%20of%20gaze%20trajectories%20are%0Aanalyzed%20using%20persistent%20homology.%20From%20the%20resulting%20persistence%20diagrams%2C%20we%0Aextract%20shape-based%20features%20such%20as%20mean%20persistence%2C%20maximum%20persistence%2C%20and%0Aentropy.%20A%20random%20forest%20classifier%20trained%20on%20these%20features%20achieves%20up%20to%0A%2475.6%5C%25%24%20accuracy%20on%20four%20emotion%20classes%2C%20which%20are%20the%20quadrants%20the%0ACircumplex%20Model%20of%20Affect.%20The%20results%20demonstrate%20that%20persistence%20diagram%0Ageometry%20effectively%20encodes%20discriminative%20gaze%20dynamics%2C%20suggesting%20a%0Apromising%20topological%20approach%20for%20affective%20computing%20and%20human%20behavior%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17450v1&entry.124074799=Read"},
{"title": "Mindfulness Meditation and Respiration: Accelerometer-Based Respiration\n  Rate and Mindfulness Progress Estimation to Enhance App Engagement and\n  Mindfulness Skills", "author": "Mohammad Nur Hossain Khan and David creswell and Jordan Albert and Patrick O'Connell and Shawn Fallon and Mathew Polowitz and Xuhai \"orson\" Xu and Bashima islam", "abstract": "  Mindfulness training is widely recognized for its benefits in reducing\ndepression, anxiety, and loneliness. With the rise of smartphone-based\nmindfulness apps, digital meditation has become more accessible, but sustaining\nlong-term user engagement remains a challenge. This paper explores whether\nrespiration biosignal feedback and mindfulness skill estimation enhance system\nusability and skill development. We develop a smartphone's accelerometer-based\nrespiration tracking algorithm, eliminating the need for additional wearables.\nUnlike existing methods, our approach accurately captures slow breathing\npatterns typical of mindfulness meditation. Additionally, we introduce the\nfirst quantitative framework to estimate mindfulness skills-concentration,\nsensory clarity, and equanimity-based on accelerometer-derived respiration\ndata. We develop and test our algorithms on 261 mindfulness sessions in both\ncontrolled and real-world settings. A user study comparing an experimental\ngroup receiving biosignal feedback with a control group using a standard app\nshows that respiration feedback enhances system usability. Our respiration\ntracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,\nclosely aligning with ground truth data, while our mindfulness skill estimation\nattains F1 scores of 80-84% in tracking skill progression. By integrating\nrespiration tracking and mindfulness estimation into a commercial app, we\ndemonstrate the potential of smartphone sensors to enhance digital mindfulness\ntraining.\n", "link": "http://arxiv.org/abs/2507.17688v1", "date": "2025-07-23", "relevancy": 1.68, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.427}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mindfulness%20Meditation%20and%20Respiration%3A%20Accelerometer-Based%20Respiration%0A%20%20Rate%20and%20Mindfulness%20Progress%20Estimation%20to%20Enhance%20App%20Engagement%20and%0A%20%20Mindfulness%20Skills&body=Title%3A%20Mindfulness%20Meditation%20and%20Respiration%3A%20Accelerometer-Based%20Respiration%0A%20%20Rate%20and%20Mindfulness%20Progress%20Estimation%20to%20Enhance%20App%20Engagement%20and%0A%20%20Mindfulness%20Skills%0AAuthor%3A%20Mohammad%20Nur%20Hossain%20Khan%20and%20David%20creswell%20and%20Jordan%20Albert%20and%20Patrick%20O%27Connell%20and%20Shawn%20Fallon%20and%20Mathew%20Polowitz%20and%20Xuhai%20%22orson%22%20Xu%20and%20Bashima%20islam%0AAbstract%3A%20%20%20Mindfulness%20training%20is%20widely%20recognized%20for%20its%20benefits%20in%20reducing%0Adepression%2C%20anxiety%2C%20and%20loneliness.%20With%20the%20rise%20of%20smartphone-based%0Amindfulness%20apps%2C%20digital%20meditation%20has%20become%20more%20accessible%2C%20but%20sustaining%0Along-term%20user%20engagement%20remains%20a%20challenge.%20This%20paper%20explores%20whether%0Arespiration%20biosignal%20feedback%20and%20mindfulness%20skill%20estimation%20enhance%20system%0Ausability%20and%20skill%20development.%20We%20develop%20a%20smartphone%27s%20accelerometer-based%0Arespiration%20tracking%20algorithm%2C%20eliminating%20the%20need%20for%20additional%20wearables.%0AUnlike%20existing%20methods%2C%20our%20approach%20accurately%20captures%20slow%20breathing%0Apatterns%20typical%20of%20mindfulness%20meditation.%20Additionally%2C%20we%20introduce%20the%0Afirst%20quantitative%20framework%20to%20estimate%20mindfulness%20skills-concentration%2C%0Asensory%20clarity%2C%20and%20equanimity-based%20on%20accelerometer-derived%20respiration%0Adata.%20We%20develop%20and%20test%20our%20algorithms%20on%20261%20mindfulness%20sessions%20in%20both%0Acontrolled%20and%20real-world%20settings.%20A%20user%20study%20comparing%20an%20experimental%0Agroup%20receiving%20biosignal%20feedback%20with%20a%20control%20group%20using%20a%20standard%20app%0Ashows%20that%20respiration%20feedback%20enhances%20system%20usability.%20Our%20respiration%0Atracking%20model%20achieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%201.6%20breaths%20per%20minute%2C%0Aclosely%20aligning%20with%20ground%20truth%20data%2C%20while%20our%20mindfulness%20skill%20estimation%0Aattains%20F1%20scores%20of%2080-84%25%20in%20tracking%20skill%20progression.%20By%20integrating%0Arespiration%20tracking%20and%20mindfulness%20estimation%20into%20a%20commercial%20app%2C%20we%0Ademonstrate%20the%20potential%20of%20smartphone%20sensors%20to%20enhance%20digital%20mindfulness%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindfulness%2520Meditation%2520and%2520Respiration%253A%2520Accelerometer-Based%2520Respiration%250A%2520%2520Rate%2520and%2520Mindfulness%2520Progress%2520Estimation%2520to%2520Enhance%2520App%2520Engagement%2520and%250A%2520%2520Mindfulness%2520Skills%26entry.906535625%3DMohammad%2520Nur%2520Hossain%2520Khan%2520and%2520David%2520creswell%2520and%2520Jordan%2520Albert%2520and%2520Patrick%2520O%2527Connell%2520and%2520Shawn%2520Fallon%2520and%2520Mathew%2520Polowitz%2520and%2520Xuhai%2520%2522orson%2522%2520Xu%2520and%2520Bashima%2520islam%26entry.1292438233%3D%2520%2520Mindfulness%2520training%2520is%2520widely%2520recognized%2520for%2520its%2520benefits%2520in%2520reducing%250Adepression%252C%2520anxiety%252C%2520and%2520loneliness.%2520With%2520the%2520rise%2520of%2520smartphone-based%250Amindfulness%2520apps%252C%2520digital%2520meditation%2520has%2520become%2520more%2520accessible%252C%2520but%2520sustaining%250Along-term%2520user%2520engagement%2520remains%2520a%2520challenge.%2520This%2520paper%2520explores%2520whether%250Arespiration%2520biosignal%2520feedback%2520and%2520mindfulness%2520skill%2520estimation%2520enhance%2520system%250Ausability%2520and%2520skill%2520development.%2520We%2520develop%2520a%2520smartphone%2527s%2520accelerometer-based%250Arespiration%2520tracking%2520algorithm%252C%2520eliminating%2520the%2520need%2520for%2520additional%2520wearables.%250AUnlike%2520existing%2520methods%252C%2520our%2520approach%2520accurately%2520captures%2520slow%2520breathing%250Apatterns%2520typical%2520of%2520mindfulness%2520meditation.%2520Additionally%252C%2520we%2520introduce%2520the%250Afirst%2520quantitative%2520framework%2520to%2520estimate%2520mindfulness%2520skills-concentration%252C%250Asensory%2520clarity%252C%2520and%2520equanimity-based%2520on%2520accelerometer-derived%2520respiration%250Adata.%2520We%2520develop%2520and%2520test%2520our%2520algorithms%2520on%2520261%2520mindfulness%2520sessions%2520in%2520both%250Acontrolled%2520and%2520real-world%2520settings.%2520A%2520user%2520study%2520comparing%2520an%2520experimental%250Agroup%2520receiving%2520biosignal%2520feedback%2520with%2520a%2520control%2520group%2520using%2520a%2520standard%2520app%250Ashows%2520that%2520respiration%2520feedback%2520enhances%2520system%2520usability.%2520Our%2520respiration%250Atracking%2520model%2520achieves%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25201.6%2520breaths%2520per%2520minute%252C%250Aclosely%2520aligning%2520with%2520ground%2520truth%2520data%252C%2520while%2520our%2520mindfulness%2520skill%2520estimation%250Aattains%2520F1%2520scores%2520of%252080-84%2525%2520in%2520tracking%2520skill%2520progression.%2520By%2520integrating%250Arespiration%2520tracking%2520and%2520mindfulness%2520estimation%2520into%2520a%2520commercial%2520app%252C%2520we%250Ademonstrate%2520the%2520potential%2520of%2520smartphone%2520sensors%2520to%2520enhance%2520digital%2520mindfulness%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mindfulness%20Meditation%20and%20Respiration%3A%20Accelerometer-Based%20Respiration%0A%20%20Rate%20and%20Mindfulness%20Progress%20Estimation%20to%20Enhance%20App%20Engagement%20and%0A%20%20Mindfulness%20Skills&entry.906535625=Mohammad%20Nur%20Hossain%20Khan%20and%20David%20creswell%20and%20Jordan%20Albert%20and%20Patrick%20O%27Connell%20and%20Shawn%20Fallon%20and%20Mathew%20Polowitz%20and%20Xuhai%20%22orson%22%20Xu%20and%20Bashima%20islam&entry.1292438233=%20%20Mindfulness%20training%20is%20widely%20recognized%20for%20its%20benefits%20in%20reducing%0Adepression%2C%20anxiety%2C%20and%20loneliness.%20With%20the%20rise%20of%20smartphone-based%0Amindfulness%20apps%2C%20digital%20meditation%20has%20become%20more%20accessible%2C%20but%20sustaining%0Along-term%20user%20engagement%20remains%20a%20challenge.%20This%20paper%20explores%20whether%0Arespiration%20biosignal%20feedback%20and%20mindfulness%20skill%20estimation%20enhance%20system%0Ausability%20and%20skill%20development.%20We%20develop%20a%20smartphone%27s%20accelerometer-based%0Arespiration%20tracking%20algorithm%2C%20eliminating%20the%20need%20for%20additional%20wearables.%0AUnlike%20existing%20methods%2C%20our%20approach%20accurately%20captures%20slow%20breathing%0Apatterns%20typical%20of%20mindfulness%20meditation.%20Additionally%2C%20we%20introduce%20the%0Afirst%20quantitative%20framework%20to%20estimate%20mindfulness%20skills-concentration%2C%0Asensory%20clarity%2C%20and%20equanimity-based%20on%20accelerometer-derived%20respiration%0Adata.%20We%20develop%20and%20test%20our%20algorithms%20on%20261%20mindfulness%20sessions%20in%20both%0Acontrolled%20and%20real-world%20settings.%20A%20user%20study%20comparing%20an%20experimental%0Agroup%20receiving%20biosignal%20feedback%20with%20a%20control%20group%20using%20a%20standard%20app%0Ashows%20that%20respiration%20feedback%20enhances%20system%20usability.%20Our%20respiration%0Atracking%20model%20achieves%20a%20mean%20absolute%20error%20%28MAE%29%20of%201.6%20breaths%20per%20minute%2C%0Aclosely%20aligning%20with%20ground%20truth%20data%2C%20while%20our%20mindfulness%20skill%20estimation%0Aattains%20F1%20scores%20of%2080-84%25%20in%20tracking%20skill%20progression.%20By%20integrating%0Arespiration%20tracking%20and%20mindfulness%20estimation%20into%20a%20commercial%20app%2C%20we%0Ademonstrate%20the%20potential%20of%20smartphone%20sensors%20to%20enhance%20digital%20mindfulness%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17688v1&entry.124074799=Read"},
{"title": "Flow Matching Meets Biology and Life Science: A Survey", "author": "Zihao Li and Zhichen Zeng and Xiao Lin and Feihao Fang and Yanru Qu and Zhe Xu and Zhining Liu and Xuying Ning and Tianxin Wei and Ge Liu and Hanghang Tong and Jingrui He", "abstract": "  Over the past decade, advances in generative modeling, such as generative\nadversarial networks, masked autoencoders, and diffusion models, have\nsignificantly transformed biological research and discovery, enabling\nbreakthroughs in molecule design, protein generation, drug discovery, and\nbeyond. At the same time, biological applications have served as valuable\ntestbeds for evaluating the capabilities of generative models. Recently, flow\nmatching has emerged as a powerful and efficient alternative to diffusion-based\ngenerative modeling, with growing interest in its application to problems in\nbiology and life sciences. This paper presents the first comprehensive survey\nof recent developments in flow matching and its applications in biological\ndomains. We begin by systematically reviewing the foundations and variants of\nflow matching, and then categorize its applications into three major areas:\nbiological sequence modeling, molecule generation and design, and peptide and\nprotein generation. For each, we provide an in-depth review of recent progress.\nWe also summarize commonly used datasets and software tools, and conclude with\na discussion of potential future directions. The corresponding curated\nresources are available at\nhttps://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.\n", "link": "http://arxiv.org/abs/2507.17731v1", "date": "2025-07-23", "relevancy": 1.4873, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5547}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4811}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Matching%20Meets%20Biology%20and%20Life%20Science%3A%20A%20Survey&body=Title%3A%20Flow%20Matching%20Meets%20Biology%20and%20Life%20Science%3A%20A%20Survey%0AAuthor%3A%20Zihao%20Li%20and%20Zhichen%20Zeng%20and%20Xiao%20Lin%20and%20Feihao%20Fang%20and%20Yanru%20Qu%20and%20Zhe%20Xu%20and%20Zhining%20Liu%20and%20Xuying%20Ning%20and%20Tianxin%20Wei%20and%20Ge%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20advances%20in%20generative%20modeling%2C%20such%20as%20generative%0Aadversarial%20networks%2C%20masked%20autoencoders%2C%20and%20diffusion%20models%2C%20have%0Asignificantly%20transformed%20biological%20research%20and%20discovery%2C%20enabling%0Abreakthroughs%20in%20molecule%20design%2C%20protein%20generation%2C%20drug%20discovery%2C%20and%0Abeyond.%20At%20the%20same%20time%2C%20biological%20applications%20have%20served%20as%20valuable%0Atestbeds%20for%20evaluating%20the%20capabilities%20of%20generative%20models.%20Recently%2C%20flow%0Amatching%20has%20emerged%20as%20a%20powerful%20and%20efficient%20alternative%20to%20diffusion-based%0Agenerative%20modeling%2C%20with%20growing%20interest%20in%20its%20application%20to%20problems%20in%0Abiology%20and%20life%20sciences.%20This%20paper%20presents%20the%20first%20comprehensive%20survey%0Aof%20recent%20developments%20in%20flow%20matching%20and%20its%20applications%20in%20biological%0Adomains.%20We%20begin%20by%20systematically%20reviewing%20the%20foundations%20and%20variants%20of%0Aflow%20matching%2C%20and%20then%20categorize%20its%20applications%20into%20three%20major%20areas%3A%0Abiological%20sequence%20modeling%2C%20molecule%20generation%20and%20design%2C%20and%20peptide%20and%0Aprotein%20generation.%20For%20each%2C%20we%20provide%20an%20in-depth%20review%20of%20recent%20progress.%0AWe%20also%20summarize%20commonly%20used%20datasets%20and%20software%20tools%2C%20and%20conclude%20with%0Aa%20discussion%20of%20potential%20future%20directions.%20The%20corresponding%20curated%0Aresources%20are%20available%20at%0Ahttps%3A//github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Matching%2520Meets%2520Biology%2520and%2520Life%2520Science%253A%2520A%2520Survey%26entry.906535625%3DZihao%2520Li%2520and%2520Zhichen%2520Zeng%2520and%2520Xiao%2520Lin%2520and%2520Feihao%2520Fang%2520and%2520Yanru%2520Qu%2520and%2520Zhe%2520Xu%2520and%2520Zhining%2520Liu%2520and%2520Xuying%2520Ning%2520and%2520Tianxin%2520Wei%2520and%2520Ge%2520Liu%2520and%2520Hanghang%2520Tong%2520and%2520Jingrui%2520He%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520advances%2520in%2520generative%2520modeling%252C%2520such%2520as%2520generative%250Aadversarial%2520networks%252C%2520masked%2520autoencoders%252C%2520and%2520diffusion%2520models%252C%2520have%250Asignificantly%2520transformed%2520biological%2520research%2520and%2520discovery%252C%2520enabling%250Abreakthroughs%2520in%2520molecule%2520design%252C%2520protein%2520generation%252C%2520drug%2520discovery%252C%2520and%250Abeyond.%2520At%2520the%2520same%2520time%252C%2520biological%2520applications%2520have%2520served%2520as%2520valuable%250Atestbeds%2520for%2520evaluating%2520the%2520capabilities%2520of%2520generative%2520models.%2520Recently%252C%2520flow%250Amatching%2520has%2520emerged%2520as%2520a%2520powerful%2520and%2520efficient%2520alternative%2520to%2520diffusion-based%250Agenerative%2520modeling%252C%2520with%2520growing%2520interest%2520in%2520its%2520application%2520to%2520problems%2520in%250Abiology%2520and%2520life%2520sciences.%2520This%2520paper%2520presents%2520the%2520first%2520comprehensive%2520survey%250Aof%2520recent%2520developments%2520in%2520flow%2520matching%2520and%2520its%2520applications%2520in%2520biological%250Adomains.%2520We%2520begin%2520by%2520systematically%2520reviewing%2520the%2520foundations%2520and%2520variants%2520of%250Aflow%2520matching%252C%2520and%2520then%2520categorize%2520its%2520applications%2520into%2520three%2520major%2520areas%253A%250Abiological%2520sequence%2520modeling%252C%2520molecule%2520generation%2520and%2520design%252C%2520and%2520peptide%2520and%250Aprotein%2520generation.%2520For%2520each%252C%2520we%2520provide%2520an%2520in-depth%2520review%2520of%2520recent%2520progress.%250AWe%2520also%2520summarize%2520commonly%2520used%2520datasets%2520and%2520software%2520tools%252C%2520and%2520conclude%2520with%250Aa%2520discussion%2520of%2520potential%2520future%2520directions.%2520The%2520corresponding%2520curated%250Aresources%2520are%2520available%2520at%250Ahttps%253A//github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Matching%20Meets%20Biology%20and%20Life%20Science%3A%20A%20Survey&entry.906535625=Zihao%20Li%20and%20Zhichen%20Zeng%20and%20Xiao%20Lin%20and%20Feihao%20Fang%20and%20Yanru%20Qu%20and%20Zhe%20Xu%20and%20Zhining%20Liu%20and%20Xuying%20Ning%20and%20Tianxin%20Wei%20and%20Ge%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20advances%20in%20generative%20modeling%2C%20such%20as%20generative%0Aadversarial%20networks%2C%20masked%20autoencoders%2C%20and%20diffusion%20models%2C%20have%0Asignificantly%20transformed%20biological%20research%20and%20discovery%2C%20enabling%0Abreakthroughs%20in%20molecule%20design%2C%20protein%20generation%2C%20drug%20discovery%2C%20and%0Abeyond.%20At%20the%20same%20time%2C%20biological%20applications%20have%20served%20as%20valuable%0Atestbeds%20for%20evaluating%20the%20capabilities%20of%20generative%20models.%20Recently%2C%20flow%0Amatching%20has%20emerged%20as%20a%20powerful%20and%20efficient%20alternative%20to%20diffusion-based%0Agenerative%20modeling%2C%20with%20growing%20interest%20in%20its%20application%20to%20problems%20in%0Abiology%20and%20life%20sciences.%20This%20paper%20presents%20the%20first%20comprehensive%20survey%0Aof%20recent%20developments%20in%20flow%20matching%20and%20its%20applications%20in%20biological%0Adomains.%20We%20begin%20by%20systematically%20reviewing%20the%20foundations%20and%20variants%20of%0Aflow%20matching%2C%20and%20then%20categorize%20its%20applications%20into%20three%20major%20areas%3A%0Abiological%20sequence%20modeling%2C%20molecule%20generation%20and%20design%2C%20and%20peptide%20and%0Aprotein%20generation.%20For%20each%2C%20we%20provide%20an%20in-depth%20review%20of%20recent%20progress.%0AWe%20also%20summarize%20commonly%20used%20datasets%20and%20software%20tools%2C%20and%20conclude%20with%0Aa%20discussion%20of%20potential%20future%20directions.%20The%20corresponding%20curated%0Aresources%20are%20available%20at%0Ahttps%3A//github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17731v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


