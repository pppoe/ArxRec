<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250831.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models", "author": "Ziwei Liao and Mohamed Sayed and Steven L. Waslander and Sara Vicente and Daniyar Turmukhambetov and Michael Firman", "abstract": "  Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.\n", "link": "http://arxiv.org/abs/2508.21542v1", "date": "2025-08-29", "relevancy": 3.3541, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6831}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.674}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complete%20Gaussian%20Splats%20from%20a%20Single%20Image%20with%20Denoising%20Diffusion%0A%20%20Models&body=Title%3A%20Complete%20Gaussian%20Splats%20from%20a%20Single%20Image%20with%20Denoising%20Diffusion%0A%20%20Models%0AAuthor%3A%20Ziwei%20Liao%20and%20Mohamed%20Sayed%20and%20Steven%20L.%20Waslander%20and%20Sara%20Vicente%20and%20Daniyar%20Turmukhambetov%20and%20Michael%20Firman%0AAbstract%3A%20%20%20Gaussian%20splatting%20typically%20requires%20dense%20observations%20of%20the%20scene%20and%20can%0Afail%20to%20reconstruct%20occluded%20and%20unobserved%20areas.%20We%20propose%20a%20latent%0Adiffusion%20model%20to%20reconstruct%20a%20complete%203D%20scene%20with%20Gaussian%20splats%2C%0Aincluding%20the%20occluded%20parts%2C%20from%20only%20a%20single%20image%20during%20inference.%0ACompleting%20the%20unobserved%20surfaces%20of%20a%20scene%20is%20challenging%20due%20to%20the%0Aambiguity%20of%20the%20plausible%20surfaces.%20Conventional%20methods%20use%20a%0Aregression-based%20formulation%20to%20predict%20a%20single%20%22mode%22%20for%20occluded%20and%0Aout-of-frustum%20surfaces%2C%20leading%20to%20blurriness%2C%20implausibility%2C%20and%20failure%20to%0Acapture%20multiple%20possible%20explanations.%20Thus%2C%20they%20often%20address%20this%20problem%0Apartially%2C%20focusing%20either%20on%20objects%20isolated%20from%20the%20background%2C%0Areconstructing%20only%20visible%20surfaces%2C%20or%20failing%20to%20extrapolate%20far%20from%20the%0Ainput%20views.%20In%20contrast%2C%20we%20propose%20a%20generative%20formulation%20to%20learn%20a%0Adistribution%20of%203D%20representations%20of%20Gaussian%20splats%20conditioned%20on%20a%20single%0Ainput%20image.%20To%20address%20the%20lack%20of%20ground-truth%20training%20data%2C%20we%20propose%20a%0AVariational%20AutoReconstructor%20to%20learn%20a%20latent%20space%20only%20from%202D%20images%20in%20a%0Aself-supervised%20manner%2C%20over%20which%20a%20diffusion%20model%20is%20trained.%20Our%20method%0Agenerates%20faithful%20reconstructions%20and%20diverse%20samples%20with%20the%20ability%20to%0Acomplete%20the%20occluded%20surfaces%20for%20high-quality%20360-degree%20renderings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplete%2520Gaussian%2520Splats%2520from%2520a%2520Single%2520Image%2520with%2520Denoising%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DZiwei%2520Liao%2520and%2520Mohamed%2520Sayed%2520and%2520Steven%2520L.%2520Waslander%2520and%2520Sara%2520Vicente%2520and%2520Daniyar%2520Turmukhambetov%2520and%2520Michael%2520Firman%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520typically%2520requires%2520dense%2520observations%2520of%2520the%2520scene%2520and%2520can%250Afail%2520to%2520reconstruct%2520occluded%2520and%2520unobserved%2520areas.%2520We%2520propose%2520a%2520latent%250Adiffusion%2520model%2520to%2520reconstruct%2520a%2520complete%25203D%2520scene%2520with%2520Gaussian%2520splats%252C%250Aincluding%2520the%2520occluded%2520parts%252C%2520from%2520only%2520a%2520single%2520image%2520during%2520inference.%250ACompleting%2520the%2520unobserved%2520surfaces%2520of%2520a%2520scene%2520is%2520challenging%2520due%2520to%2520the%250Aambiguity%2520of%2520the%2520plausible%2520surfaces.%2520Conventional%2520methods%2520use%2520a%250Aregression-based%2520formulation%2520to%2520predict%2520a%2520single%2520%2522mode%2522%2520for%2520occluded%2520and%250Aout-of-frustum%2520surfaces%252C%2520leading%2520to%2520blurriness%252C%2520implausibility%252C%2520and%2520failure%2520to%250Acapture%2520multiple%2520possible%2520explanations.%2520Thus%252C%2520they%2520often%2520address%2520this%2520problem%250Apartially%252C%2520focusing%2520either%2520on%2520objects%2520isolated%2520from%2520the%2520background%252C%250Areconstructing%2520only%2520visible%2520surfaces%252C%2520or%2520failing%2520to%2520extrapolate%2520far%2520from%2520the%250Ainput%2520views.%2520In%2520contrast%252C%2520we%2520propose%2520a%2520generative%2520formulation%2520to%2520learn%2520a%250Adistribution%2520of%25203D%2520representations%2520of%2520Gaussian%2520splats%2520conditioned%2520on%2520a%2520single%250Ainput%2520image.%2520To%2520address%2520the%2520lack%2520of%2520ground-truth%2520training%2520data%252C%2520we%2520propose%2520a%250AVariational%2520AutoReconstructor%2520to%2520learn%2520a%2520latent%2520space%2520only%2520from%25202D%2520images%2520in%2520a%250Aself-supervised%2520manner%252C%2520over%2520which%2520a%2520diffusion%2520model%2520is%2520trained.%2520Our%2520method%250Agenerates%2520faithful%2520reconstructions%2520and%2520diverse%2520samples%2520with%2520the%2520ability%2520to%250Acomplete%2520the%2520occluded%2520surfaces%2520for%2520high-quality%2520360-degree%2520renderings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complete%20Gaussian%20Splats%20from%20a%20Single%20Image%20with%20Denoising%20Diffusion%0A%20%20Models&entry.906535625=Ziwei%20Liao%20and%20Mohamed%20Sayed%20and%20Steven%20L.%20Waslander%20and%20Sara%20Vicente%20and%20Daniyar%20Turmukhambetov%20and%20Michael%20Firman&entry.1292438233=%20%20Gaussian%20splatting%20typically%20requires%20dense%20observations%20of%20the%20scene%20and%20can%0Afail%20to%20reconstruct%20occluded%20and%20unobserved%20areas.%20We%20propose%20a%20latent%0Adiffusion%20model%20to%20reconstruct%20a%20complete%203D%20scene%20with%20Gaussian%20splats%2C%0Aincluding%20the%20occluded%20parts%2C%20from%20only%20a%20single%20image%20during%20inference.%0ACompleting%20the%20unobserved%20surfaces%20of%20a%20scene%20is%20challenging%20due%20to%20the%0Aambiguity%20of%20the%20plausible%20surfaces.%20Conventional%20methods%20use%20a%0Aregression-based%20formulation%20to%20predict%20a%20single%20%22mode%22%20for%20occluded%20and%0Aout-of-frustum%20surfaces%2C%20leading%20to%20blurriness%2C%20implausibility%2C%20and%20failure%20to%0Acapture%20multiple%20possible%20explanations.%20Thus%2C%20they%20often%20address%20this%20problem%0Apartially%2C%20focusing%20either%20on%20objects%20isolated%20from%20the%20background%2C%0Areconstructing%20only%20visible%20surfaces%2C%20or%20failing%20to%20extrapolate%20far%20from%20the%0Ainput%20views.%20In%20contrast%2C%20we%20propose%20a%20generative%20formulation%20to%20learn%20a%0Adistribution%20of%203D%20representations%20of%20Gaussian%20splats%20conditioned%20on%20a%20single%0Ainput%20image.%20To%20address%20the%20lack%20of%20ground-truth%20training%20data%2C%20we%20propose%20a%0AVariational%20AutoReconstructor%20to%20learn%20a%20latent%20space%20only%20from%202D%20images%20in%20a%0Aself-supervised%20manner%2C%20over%20which%20a%20diffusion%20model%20is%20trained.%20Our%20method%0Agenerates%20faithful%20reconstructions%20and%20diverse%20samples%20with%20the%20ability%20to%0Acomplete%20the%20occluded%20surfaces%20for%20high-quality%20360-degree%20renderings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21542v1&entry.124074799=Read"},
{"title": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images", "author": "Juneyoung Ro and Namwoo Kim and Yoonjin Yoon", "abstract": "  Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.\n", "link": "http://arxiv.org/abs/2508.21565v1", "date": "2025-08-29", "relevancy": 3.1013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6585}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Well%20Do%20Vision--Language%20Models%20Understand%20Cities%3F%20A%20Comparative%0A%20%20Study%20on%20Spatial%20Reasoning%20from%20Street-View%20Images&body=Title%3A%20How%20Well%20Do%20Vision--Language%20Models%20Understand%20Cities%3F%20A%20Comparative%0A%20%20Study%20on%20Spatial%20Reasoning%20from%20Street-View%20Images%0AAuthor%3A%20Juneyoung%20Ro%20and%20Namwoo%20Kim%20and%20Yoonjin%20Yoon%0AAbstract%3A%20%20%20Effectively%20understanding%20urban%20scenes%20requires%20fine-grained%20spatial%0Areasoning%20about%20objects%2C%20layouts%2C%20and%20depth%20cues.%20However%2C%20how%20well%20current%0Avision-language%20models%20%28VLMs%29%2C%20pretrained%20on%20general%20scenes%2C%20transfer%20these%0Aabilities%20to%20urban%20domain%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%0Aconduct%20a%20comparative%20study%20of%20three%20off-the-shelf%20VLMs-BLIP-2%2C%20InstructBLIP%2C%0Aand%20LLaVA-1.5-evaluating%20both%20zero-shot%20performance%20and%20the%20effects%20of%0Afine-tuning%20with%20a%20synthetic%20VQA%20dataset%20specific%20to%20urban%20scenes.%20We%20construct%0Asuch%20dataset%20from%20segmentation%2C%20depth%2C%20and%20object%20detection%20predictions%20of%0Astreet-view%20images%2C%20pairing%20each%20question%20with%20LLM-generated%20Chain-of-Thought%0A%28CoT%29%20answers%20for%20step-by-step%20reasoning%20supervision.%20Results%20show%20that%20while%0AVLMs%20perform%20reasonably%20well%20in%20zero-shot%20settings%2C%20fine-tuning%20with%20our%0Asynthetic%20CoT-supervised%20dataset%20substantially%20boosts%20performance%2C%20especially%0Afor%20challenging%20question%20types%20such%20as%20negation%20and%20counterfactuals.%20This%20study%0Aintroduces%20urban%20spatial%20reasoning%20as%20a%20new%20challenge%20for%20VLMs%20and%20demonstrates%0Asynthetic%20dataset%20construction%20as%20a%20practical%20path%20for%20adapting%20general-purpose%0Amodels%20to%20specialized%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Well%2520Do%2520Vision--Language%2520Models%2520Understand%2520Cities%253F%2520A%2520Comparative%250A%2520%2520Study%2520on%2520Spatial%2520Reasoning%2520from%2520Street-View%2520Images%26entry.906535625%3DJuneyoung%2520Ro%2520and%2520Namwoo%2520Kim%2520and%2520Yoonjin%2520Yoon%26entry.1292438233%3D%2520%2520Effectively%2520understanding%2520urban%2520scenes%2520requires%2520fine-grained%2520spatial%250Areasoning%2520about%2520objects%252C%2520layouts%252C%2520and%2520depth%2520cues.%2520However%252C%2520how%2520well%2520current%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520pretrained%2520on%2520general%2520scenes%252C%2520transfer%2520these%250Aabilities%2520to%2520urban%2520domain%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%250Aconduct%2520a%2520comparative%2520study%2520of%2520three%2520off-the-shelf%2520VLMs-BLIP-2%252C%2520InstructBLIP%252C%250Aand%2520LLaVA-1.5-evaluating%2520both%2520zero-shot%2520performance%2520and%2520the%2520effects%2520of%250Afine-tuning%2520with%2520a%2520synthetic%2520VQA%2520dataset%2520specific%2520to%2520urban%2520scenes.%2520We%2520construct%250Asuch%2520dataset%2520from%2520segmentation%252C%2520depth%252C%2520and%2520object%2520detection%2520predictions%2520of%250Astreet-view%2520images%252C%2520pairing%2520each%2520question%2520with%2520LLM-generated%2520Chain-of-Thought%250A%2528CoT%2529%2520answers%2520for%2520step-by-step%2520reasoning%2520supervision.%2520Results%2520show%2520that%2520while%250AVLMs%2520perform%2520reasonably%2520well%2520in%2520zero-shot%2520settings%252C%2520fine-tuning%2520with%2520our%250Asynthetic%2520CoT-supervised%2520dataset%2520substantially%2520boosts%2520performance%252C%2520especially%250Afor%2520challenging%2520question%2520types%2520such%2520as%2520negation%2520and%2520counterfactuals.%2520This%2520study%250Aintroduces%2520urban%2520spatial%2520reasoning%2520as%2520a%2520new%2520challenge%2520for%2520VLMs%2520and%2520demonstrates%250Asynthetic%2520dataset%2520construction%2520as%2520a%2520practical%2520path%2520for%2520adapting%2520general-purpose%250Amodels%2520to%2520specialized%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Well%20Do%20Vision--Language%20Models%20Understand%20Cities%3F%20A%20Comparative%0A%20%20Study%20on%20Spatial%20Reasoning%20from%20Street-View%20Images&entry.906535625=Juneyoung%20Ro%20and%20Namwoo%20Kim%20and%20Yoonjin%20Yoon&entry.1292438233=%20%20Effectively%20understanding%20urban%20scenes%20requires%20fine-grained%20spatial%0Areasoning%20about%20objects%2C%20layouts%2C%20and%20depth%20cues.%20However%2C%20how%20well%20current%0Avision-language%20models%20%28VLMs%29%2C%20pretrained%20on%20general%20scenes%2C%20transfer%20these%0Aabilities%20to%20urban%20domain%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%0Aconduct%20a%20comparative%20study%20of%20three%20off-the-shelf%20VLMs-BLIP-2%2C%20InstructBLIP%2C%0Aand%20LLaVA-1.5-evaluating%20both%20zero-shot%20performance%20and%20the%20effects%20of%0Afine-tuning%20with%20a%20synthetic%20VQA%20dataset%20specific%20to%20urban%20scenes.%20We%20construct%0Asuch%20dataset%20from%20segmentation%2C%20depth%2C%20and%20object%20detection%20predictions%20of%0Astreet-view%20images%2C%20pairing%20each%20question%20with%20LLM-generated%20Chain-of-Thought%0A%28CoT%29%20answers%20for%20step-by-step%20reasoning%20supervision.%20Results%20show%20that%20while%0AVLMs%20perform%20reasonably%20well%20in%20zero-shot%20settings%2C%20fine-tuning%20with%20our%0Asynthetic%20CoT-supervised%20dataset%20substantially%20boosts%20performance%2C%20especially%0Afor%20challenging%20question%20types%20such%20as%20negation%20and%20counterfactuals.%20This%20study%0Aintroduces%20urban%20spatial%20reasoning%20as%20a%20new%20challenge%20for%20VLMs%20and%20demonstrates%0Asynthetic%20dataset%20construction%20as%20a%20practical%20path%20for%20adapting%20general-purpose%0Amodels%20to%20specialized%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21565v1&entry.124074799=Read"},
{"title": "What Can We Learn from Harry Potter? An Exploratory Study of Visual\n  Representation Learning from Atypical Videos", "author": "Qiyue Sun and Qiming Huang and Yang Yang and Hongjun Wang and Jianbo Jiao", "abstract": "  Humans usually show exceptional generalisation and discovery ability in the\nopen world, when being shown uncommon new concepts. Whereas most existing\nstudies in the literature focus on common typical data from closed sets,\nopen-world novel discovery is under-explored in videos. In this paper, we are\ninterested in asking: \\textit{What if atypical unusual videos are exposed in\nthe learning process?} To this end, we collect a new video dataset consisting\nof various types of unusual atypical data (\\eg sci-fi, animation, \\etc). To\nstudy how such atypical data may benefit open-world learning, we feed them into\nthe model training process for representation learning. Focusing on three key\ntasks in open-world learning: out-of-distribution (OOD) detection, novel\ncategory discovery (NCD), and zero-shot action recognition (ZSAR), we found\nthat even straightforward learning approaches with atypical data consistently\nimprove performance across various settings. Furthermore, we found that\nincreasing the categorical diversity of the atypical samples further boosts OOD\ndetection performance. Additionally, in the NCD task, using a smaller yet more\nsemantically diverse set of atypical samples leads to better performance\ncompared to using a larger but more typical dataset. In the ZSAR setting, the\nsemantic diversity of atypical videos helps the model generalise better to\nunseen action classes. These observations in our extensive experimental\nevaluations reveal the benefits of atypical videos for visual representation\nlearning in the open world, together with the newly proposed dataset,\nencouraging further studies in this direction.\n", "link": "http://arxiv.org/abs/2508.21770v1", "date": "2025-08-29", "relevancy": 2.9246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6055}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos&body=Title%3A%20What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos%0AAuthor%3A%20Qiyue%20Sun%20and%20Qiming%20Huang%20and%20Yang%20Yang%20and%20Hongjun%20Wang%20and%20Jianbo%20Jiao%0AAbstract%3A%20%20%20Humans%20usually%20show%20exceptional%20generalisation%20and%20discovery%20ability%20in%20the%0Aopen%20world%2C%20when%20being%20shown%20uncommon%20new%20concepts.%20Whereas%20most%20existing%0Astudies%20in%20the%20literature%20focus%20on%20common%20typical%20data%20from%20closed%20sets%2C%0Aopen-world%20novel%20discovery%20is%20under-explored%20in%20videos.%20In%20this%20paper%2C%20we%20are%0Ainterested%20in%20asking%3A%20%5Ctextit%7BWhat%20if%20atypical%20unusual%20videos%20are%20exposed%20in%0Athe%20learning%20process%3F%7D%20To%20this%20end%2C%20we%20collect%20a%20new%20video%20dataset%20consisting%0Aof%20various%20types%20of%20unusual%20atypical%20data%20%28%5Ceg%20sci-fi%2C%20animation%2C%20%5Cetc%29.%20To%0Astudy%20how%20such%20atypical%20data%20may%20benefit%20open-world%20learning%2C%20we%20feed%20them%20into%0Athe%20model%20training%20process%20for%20representation%20learning.%20Focusing%20on%20three%20key%0Atasks%20in%20open-world%20learning%3A%20out-of-distribution%20%28OOD%29%20detection%2C%20novel%0Acategory%20discovery%20%28NCD%29%2C%20and%20zero-shot%20action%20recognition%20%28ZSAR%29%2C%20we%20found%0Athat%20even%20straightforward%20learning%20approaches%20with%20atypical%20data%20consistently%0Aimprove%20performance%20across%20various%20settings.%20Furthermore%2C%20we%20found%20that%0Aincreasing%20the%20categorical%20diversity%20of%20the%20atypical%20samples%20further%20boosts%20OOD%0Adetection%20performance.%20Additionally%2C%20in%20the%20NCD%20task%2C%20using%20a%20smaller%20yet%20more%0Asemantically%20diverse%20set%20of%20atypical%20samples%20leads%20to%20better%20performance%0Acompared%20to%20using%20a%20larger%20but%20more%20typical%20dataset.%20In%20the%20ZSAR%20setting%2C%20the%0Asemantic%20diversity%20of%20atypical%20videos%20helps%20the%20model%20generalise%20better%20to%0Aunseen%20action%20classes.%20These%20observations%20in%20our%20extensive%20experimental%0Aevaluations%20reveal%20the%20benefits%20of%20atypical%20videos%20for%20visual%20representation%0Alearning%20in%20the%20open%20world%2C%20together%20with%20the%20newly%20proposed%20dataset%2C%0Aencouraging%20further%20studies%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520We%2520Learn%2520from%2520Harry%2520Potter%253F%2520An%2520Exploratory%2520Study%2520of%2520Visual%250A%2520%2520Representation%2520Learning%2520from%2520Atypical%2520Videos%26entry.906535625%3DQiyue%2520Sun%2520and%2520Qiming%2520Huang%2520and%2520Yang%2520Yang%2520and%2520Hongjun%2520Wang%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3D%2520%2520Humans%2520usually%2520show%2520exceptional%2520generalisation%2520and%2520discovery%2520ability%2520in%2520the%250Aopen%2520world%252C%2520when%2520being%2520shown%2520uncommon%2520new%2520concepts.%2520Whereas%2520most%2520existing%250Astudies%2520in%2520the%2520literature%2520focus%2520on%2520common%2520typical%2520data%2520from%2520closed%2520sets%252C%250Aopen-world%2520novel%2520discovery%2520is%2520under-explored%2520in%2520videos.%2520In%2520this%2520paper%252C%2520we%2520are%250Ainterested%2520in%2520asking%253A%2520%255Ctextit%257BWhat%2520if%2520atypical%2520unusual%2520videos%2520are%2520exposed%2520in%250Athe%2520learning%2520process%253F%257D%2520To%2520this%2520end%252C%2520we%2520collect%2520a%2520new%2520video%2520dataset%2520consisting%250Aof%2520various%2520types%2520of%2520unusual%2520atypical%2520data%2520%2528%255Ceg%2520sci-fi%252C%2520animation%252C%2520%255Cetc%2529.%2520To%250Astudy%2520how%2520such%2520atypical%2520data%2520may%2520benefit%2520open-world%2520learning%252C%2520we%2520feed%2520them%2520into%250Athe%2520model%2520training%2520process%2520for%2520representation%2520learning.%2520Focusing%2520on%2520three%2520key%250Atasks%2520in%2520open-world%2520learning%253A%2520out-of-distribution%2520%2528OOD%2529%2520detection%252C%2520novel%250Acategory%2520discovery%2520%2528NCD%2529%252C%2520and%2520zero-shot%2520action%2520recognition%2520%2528ZSAR%2529%252C%2520we%2520found%250Athat%2520even%2520straightforward%2520learning%2520approaches%2520with%2520atypical%2520data%2520consistently%250Aimprove%2520performance%2520across%2520various%2520settings.%2520Furthermore%252C%2520we%2520found%2520that%250Aincreasing%2520the%2520categorical%2520diversity%2520of%2520the%2520atypical%2520samples%2520further%2520boosts%2520OOD%250Adetection%2520performance.%2520Additionally%252C%2520in%2520the%2520NCD%2520task%252C%2520using%2520a%2520smaller%2520yet%2520more%250Asemantically%2520diverse%2520set%2520of%2520atypical%2520samples%2520leads%2520to%2520better%2520performance%250Acompared%2520to%2520using%2520a%2520larger%2520but%2520more%2520typical%2520dataset.%2520In%2520the%2520ZSAR%2520setting%252C%2520the%250Asemantic%2520diversity%2520of%2520atypical%2520videos%2520helps%2520the%2520model%2520generalise%2520better%2520to%250Aunseen%2520action%2520classes.%2520These%2520observations%2520in%2520our%2520extensive%2520experimental%250Aevaluations%2520reveal%2520the%2520benefits%2520of%2520atypical%2520videos%2520for%2520visual%2520representation%250Alearning%2520in%2520the%2520open%2520world%252C%2520together%2520with%2520the%2520newly%2520proposed%2520dataset%252C%250Aencouraging%2520further%2520studies%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20We%20Learn%20from%20Harry%20Potter%3F%20An%20Exploratory%20Study%20of%20Visual%0A%20%20Representation%20Learning%20from%20Atypical%20Videos&entry.906535625=Qiyue%20Sun%20and%20Qiming%20Huang%20and%20Yang%20Yang%20and%20Hongjun%20Wang%20and%20Jianbo%20Jiao&entry.1292438233=%20%20Humans%20usually%20show%20exceptional%20generalisation%20and%20discovery%20ability%20in%20the%0Aopen%20world%2C%20when%20being%20shown%20uncommon%20new%20concepts.%20Whereas%20most%20existing%0Astudies%20in%20the%20literature%20focus%20on%20common%20typical%20data%20from%20closed%20sets%2C%0Aopen-world%20novel%20discovery%20is%20under-explored%20in%20videos.%20In%20this%20paper%2C%20we%20are%0Ainterested%20in%20asking%3A%20%5Ctextit%7BWhat%20if%20atypical%20unusual%20videos%20are%20exposed%20in%0Athe%20learning%20process%3F%7D%20To%20this%20end%2C%20we%20collect%20a%20new%20video%20dataset%20consisting%0Aof%20various%20types%20of%20unusual%20atypical%20data%20%28%5Ceg%20sci-fi%2C%20animation%2C%20%5Cetc%29.%20To%0Astudy%20how%20such%20atypical%20data%20may%20benefit%20open-world%20learning%2C%20we%20feed%20them%20into%0Athe%20model%20training%20process%20for%20representation%20learning.%20Focusing%20on%20three%20key%0Atasks%20in%20open-world%20learning%3A%20out-of-distribution%20%28OOD%29%20detection%2C%20novel%0Acategory%20discovery%20%28NCD%29%2C%20and%20zero-shot%20action%20recognition%20%28ZSAR%29%2C%20we%20found%0Athat%20even%20straightforward%20learning%20approaches%20with%20atypical%20data%20consistently%0Aimprove%20performance%20across%20various%20settings.%20Furthermore%2C%20we%20found%20that%0Aincreasing%20the%20categorical%20diversity%20of%20the%20atypical%20samples%20further%20boosts%20OOD%0Adetection%20performance.%20Additionally%2C%20in%20the%20NCD%20task%2C%20using%20a%20smaller%20yet%20more%0Asemantically%20diverse%20set%20of%20atypical%20samples%20leads%20to%20better%20performance%0Acompared%20to%20using%20a%20larger%20but%20more%20typical%20dataset.%20In%20the%20ZSAR%20setting%2C%20the%0Asemantic%20diversity%20of%20atypical%20videos%20helps%20the%20model%20generalise%20better%20to%0Aunseen%20action%20classes.%20These%20observations%20in%20our%20extensive%20experimental%0Aevaluations%20reveal%20the%20benefits%20of%20atypical%20videos%20for%20visual%20representation%0Alearning%20in%20the%20open%20world%2C%20together%20with%20the%20newly%20proposed%20dataset%2C%0Aencouraging%20further%20studies%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21770v1&entry.124074799=Read"},
{"title": "ECHO: Ego-Centric modeling of Human-Object interactions", "author": "Ilya A. Petrov and Vladimir Guzov and Riccardo Marin and Emre Aksan and Xu Chen and Daniel Cremers and Thabo Beeler and Gerard Pons-Moll", "abstract": "  Modeling human-object interactions (HOI) from an egocentric perspective is a\nlargely unexplored yet important problem due to the increasing adoption of\nwearable devices, such as smart glasses and watches. We investigate how much\ninformation about interaction can be recovered from only head and wrists\ntracking. Our answer is ECHO (Ego-Centric modeling of Human-Object\ninteractions), which, for the first time, proposes a unified framework to\nrecover three modalities: human pose, object motion, and contact from such\nminimal observation. ECHO employs a Diffusion Transformer architecture and a\nunique three-variate diffusion process, which jointly models human motion,\nobject trajectory, and contact sequence, allowing for flexible input\nconfigurations. Our method operates in a head-centric canonical space,\nenhancing robustness to global orientation. We propose a conveyor-based\ninference, which progressively increases the diffusion timestamp with the frame\nposition, allowing us to process sequences of any length. Through extensive\nevaluation, we demonstrate that ECHO outperforms existing methods that do not\noffer the same flexibility, setting a state-of-the-art in egocentric HOI\nreconstruction.\n", "link": "http://arxiv.org/abs/2508.21556v1", "date": "2025-08-29", "relevancy": 2.9016, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6138}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5694}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECHO%3A%20Ego-Centric%20modeling%20of%20Human-Object%20interactions&body=Title%3A%20ECHO%3A%20Ego-Centric%20modeling%20of%20Human-Object%20interactions%0AAuthor%3A%20Ilya%20A.%20Petrov%20and%20Vladimir%20Guzov%20and%20Riccardo%20Marin%20and%20Emre%20Aksan%20and%20Xu%20Chen%20and%20Daniel%20Cremers%20and%20Thabo%20Beeler%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20Modeling%20human-object%20interactions%20%28HOI%29%20from%20an%20egocentric%20perspective%20is%20a%0Alargely%20unexplored%20yet%20important%20problem%20due%20to%20the%20increasing%20adoption%20of%0Awearable%20devices%2C%20such%20as%20smart%20glasses%20and%20watches.%20We%20investigate%20how%20much%0Ainformation%20about%20interaction%20can%20be%20recovered%20from%20only%20head%20and%20wrists%0Atracking.%20Our%20answer%20is%20ECHO%20%28Ego-Centric%20modeling%20of%20Human-Object%0Ainteractions%29%2C%20which%2C%20for%20the%20first%20time%2C%20proposes%20a%20unified%20framework%20to%0Arecover%20three%20modalities%3A%20human%20pose%2C%20object%20motion%2C%20and%20contact%20from%20such%0Aminimal%20observation.%20ECHO%20employs%20a%20Diffusion%20Transformer%20architecture%20and%20a%0Aunique%20three-variate%20diffusion%20process%2C%20which%20jointly%20models%20human%20motion%2C%0Aobject%20trajectory%2C%20and%20contact%20sequence%2C%20allowing%20for%20flexible%20input%0Aconfigurations.%20Our%20method%20operates%20in%20a%20head-centric%20canonical%20space%2C%0Aenhancing%20robustness%20to%20global%20orientation.%20We%20propose%20a%20conveyor-based%0Ainference%2C%20which%20progressively%20increases%20the%20diffusion%20timestamp%20with%20the%20frame%0Aposition%2C%20allowing%20us%20to%20process%20sequences%20of%20any%20length.%20Through%20extensive%0Aevaluation%2C%20we%20demonstrate%20that%20ECHO%20outperforms%20existing%20methods%20that%20do%20not%0Aoffer%20the%20same%20flexibility%2C%20setting%20a%20state-of-the-art%20in%20egocentric%20HOI%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECHO%253A%2520Ego-Centric%2520modeling%2520of%2520Human-Object%2520interactions%26entry.906535625%3DIlya%2520A.%2520Petrov%2520and%2520Vladimir%2520Guzov%2520and%2520Riccardo%2520Marin%2520and%2520Emre%2520Aksan%2520and%2520Xu%2520Chen%2520and%2520Daniel%2520Cremers%2520and%2520Thabo%2520Beeler%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520Modeling%2520human-object%2520interactions%2520%2528HOI%2529%2520from%2520an%2520egocentric%2520perspective%2520is%2520a%250Alargely%2520unexplored%2520yet%2520important%2520problem%2520due%2520to%2520the%2520increasing%2520adoption%2520of%250Awearable%2520devices%252C%2520such%2520as%2520smart%2520glasses%2520and%2520watches.%2520We%2520investigate%2520how%2520much%250Ainformation%2520about%2520interaction%2520can%2520be%2520recovered%2520from%2520only%2520head%2520and%2520wrists%250Atracking.%2520Our%2520answer%2520is%2520ECHO%2520%2528Ego-Centric%2520modeling%2520of%2520Human-Object%250Ainteractions%2529%252C%2520which%252C%2520for%2520the%2520first%2520time%252C%2520proposes%2520a%2520unified%2520framework%2520to%250Arecover%2520three%2520modalities%253A%2520human%2520pose%252C%2520object%2520motion%252C%2520and%2520contact%2520from%2520such%250Aminimal%2520observation.%2520ECHO%2520employs%2520a%2520Diffusion%2520Transformer%2520architecture%2520and%2520a%250Aunique%2520three-variate%2520diffusion%2520process%252C%2520which%2520jointly%2520models%2520human%2520motion%252C%250Aobject%2520trajectory%252C%2520and%2520contact%2520sequence%252C%2520allowing%2520for%2520flexible%2520input%250Aconfigurations.%2520Our%2520method%2520operates%2520in%2520a%2520head-centric%2520canonical%2520space%252C%250Aenhancing%2520robustness%2520to%2520global%2520orientation.%2520We%2520propose%2520a%2520conveyor-based%250Ainference%252C%2520which%2520progressively%2520increases%2520the%2520diffusion%2520timestamp%2520with%2520the%2520frame%250Aposition%252C%2520allowing%2520us%2520to%2520process%2520sequences%2520of%2520any%2520length.%2520Through%2520extensive%250Aevaluation%252C%2520we%2520demonstrate%2520that%2520ECHO%2520outperforms%2520existing%2520methods%2520that%2520do%2520not%250Aoffer%2520the%2520same%2520flexibility%252C%2520setting%2520a%2520state-of-the-art%2520in%2520egocentric%2520HOI%250Areconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECHO%3A%20Ego-Centric%20modeling%20of%20Human-Object%20interactions&entry.906535625=Ilya%20A.%20Petrov%20and%20Vladimir%20Guzov%20and%20Riccardo%20Marin%20and%20Emre%20Aksan%20and%20Xu%20Chen%20and%20Daniel%20Cremers%20and%20Thabo%20Beeler%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20Modeling%20human-object%20interactions%20%28HOI%29%20from%20an%20egocentric%20perspective%20is%20a%0Alargely%20unexplored%20yet%20important%20problem%20due%20to%20the%20increasing%20adoption%20of%0Awearable%20devices%2C%20such%20as%20smart%20glasses%20and%20watches.%20We%20investigate%20how%20much%0Ainformation%20about%20interaction%20can%20be%20recovered%20from%20only%20head%20and%20wrists%0Atracking.%20Our%20answer%20is%20ECHO%20%28Ego-Centric%20modeling%20of%20Human-Object%0Ainteractions%29%2C%20which%2C%20for%20the%20first%20time%2C%20proposes%20a%20unified%20framework%20to%0Arecover%20three%20modalities%3A%20human%20pose%2C%20object%20motion%2C%20and%20contact%20from%20such%0Aminimal%20observation.%20ECHO%20employs%20a%20Diffusion%20Transformer%20architecture%20and%20a%0Aunique%20three-variate%20diffusion%20process%2C%20which%20jointly%20models%20human%20motion%2C%0Aobject%20trajectory%2C%20and%20contact%20sequence%2C%20allowing%20for%20flexible%20input%0Aconfigurations.%20Our%20method%20operates%20in%20a%20head-centric%20canonical%20space%2C%0Aenhancing%20robustness%20to%20global%20orientation.%20We%20propose%20a%20conveyor-based%0Ainference%2C%20which%20progressively%20increases%20the%20diffusion%20timestamp%20with%20the%20frame%0Aposition%2C%20allowing%20us%20to%20process%20sequences%20of%20any%20length.%20Through%20extensive%0Aevaluation%2C%20we%20demonstrate%20that%20ECHO%20outperforms%20existing%20methods%20that%20do%20not%0Aoffer%20the%20same%20flexibility%2C%20setting%20a%20state-of-the-art%20in%20egocentric%20HOI%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21556v1&entry.124074799=Read"},
{"title": "VoCap: Video Object Captioning and Segmentation from Any Prompt", "author": "Jasper Uijlings and Xingyi Zhou and Xiuye Gu and Arsha Nagrani and Anurag Arnab and Alireza Fathi and David Ross and Cordelia Schmid", "abstract": "  Understanding objects in videos in terms of fine-grained localization masks\nand detailed semantic properties is a fundamental task in video understanding.\nIn this paper, we propose VoCap, a flexible video model that consumes a video\nand a prompt of various modalities (text, box or mask), and produces a\nspatio-temporal masklet with a corresponding object-centric caption. As such\nour model addresses simultaneously the tasks of promptable video object\nsegmentation, referring expression segmentation, and object captioning. Since\nobtaining data for this task is tedious and expensive, we propose to annotate\nan existing large-scale segmentation dataset (SAV) with pseudo object captions.\nWe do so by preprocessing videos with their ground-truth masks to highlight the\nobject of interest and feed this to a large Vision Language Model (VLM). For an\nunbiased evaluation, we collect manual annotations on the validation set. We\ncall the resulting dataset SAV-Caption. We train our VoCap model at scale on a\nSAV-Caption together with a mix of other image and video datasets. Our model\nyields state-of-the-art results on referring expression video object\nsegmentation, is competitive on semi-supervised video object segmentation, and\nestablishes a benchmark for video object captioning. Our dataset will be made\navailable at https://github.com/google-deepmind/vocap.\n", "link": "http://arxiv.org/abs/2508.21809v1", "date": "2025-08-29", "relevancy": 2.8623, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5763}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoCap%3A%20Video%20Object%20Captioning%20and%20Segmentation%20from%20Any%20Prompt&body=Title%3A%20VoCap%3A%20Video%20Object%20Captioning%20and%20Segmentation%20from%20Any%20Prompt%0AAuthor%3A%20Jasper%20Uijlings%20and%20Xingyi%20Zhou%20and%20Xiuye%20Gu%20and%20Arsha%20Nagrani%20and%20Anurag%20Arnab%20and%20Alireza%20Fathi%20and%20David%20Ross%20and%20Cordelia%20Schmid%0AAbstract%3A%20%20%20Understanding%20objects%20in%20videos%20in%20terms%20of%20fine-grained%20localization%20masks%0Aand%20detailed%20semantic%20properties%20is%20a%20fundamental%20task%20in%20video%20understanding.%0AIn%20this%20paper%2C%20we%20propose%20VoCap%2C%20a%20flexible%20video%20model%20that%20consumes%20a%20video%0Aand%20a%20prompt%20of%20various%20modalities%20%28text%2C%20box%20or%20mask%29%2C%20and%20produces%20a%0Aspatio-temporal%20masklet%20with%20a%20corresponding%20object-centric%20caption.%20As%20such%0Aour%20model%20addresses%20simultaneously%20the%20tasks%20of%20promptable%20video%20object%0Asegmentation%2C%20referring%20expression%20segmentation%2C%20and%20object%20captioning.%20Since%0Aobtaining%20data%20for%20this%20task%20is%20tedious%20and%20expensive%2C%20we%20propose%20to%20annotate%0Aan%20existing%20large-scale%20segmentation%20dataset%20%28SAV%29%20with%20pseudo%20object%20captions.%0AWe%20do%20so%20by%20preprocessing%20videos%20with%20their%20ground-truth%20masks%20to%20highlight%20the%0Aobject%20of%20interest%20and%20feed%20this%20to%20a%20large%20Vision%20Language%20Model%20%28VLM%29.%20For%20an%0Aunbiased%20evaluation%2C%20we%20collect%20manual%20annotations%20on%20the%20validation%20set.%20We%0Acall%20the%20resulting%20dataset%20SAV-Caption.%20We%20train%20our%20VoCap%20model%20at%20scale%20on%20a%0ASAV-Caption%20together%20with%20a%20mix%20of%20other%20image%20and%20video%20datasets.%20Our%20model%0Ayields%20state-of-the-art%20results%20on%20referring%20expression%20video%20object%0Asegmentation%2C%20is%20competitive%20on%20semi-supervised%20video%20object%20segmentation%2C%20and%0Aestablishes%20a%20benchmark%20for%20video%20object%20captioning.%20Our%20dataset%20will%20be%20made%0Aavailable%20at%20https%3A//github.com/google-deepmind/vocap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoCap%253A%2520Video%2520Object%2520Captioning%2520and%2520Segmentation%2520from%2520Any%2520Prompt%26entry.906535625%3DJasper%2520Uijlings%2520and%2520Xingyi%2520Zhou%2520and%2520Xiuye%2520Gu%2520and%2520Arsha%2520Nagrani%2520and%2520Anurag%2520Arnab%2520and%2520Alireza%2520Fathi%2520and%2520David%2520Ross%2520and%2520Cordelia%2520Schmid%26entry.1292438233%3D%2520%2520Understanding%2520objects%2520in%2520videos%2520in%2520terms%2520of%2520fine-grained%2520localization%2520masks%250Aand%2520detailed%2520semantic%2520properties%2520is%2520a%2520fundamental%2520task%2520in%2520video%2520understanding.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520VoCap%252C%2520a%2520flexible%2520video%2520model%2520that%2520consumes%2520a%2520video%250Aand%2520a%2520prompt%2520of%2520various%2520modalities%2520%2528text%252C%2520box%2520or%2520mask%2529%252C%2520and%2520produces%2520a%250Aspatio-temporal%2520masklet%2520with%2520a%2520corresponding%2520object-centric%2520caption.%2520As%2520such%250Aour%2520model%2520addresses%2520simultaneously%2520the%2520tasks%2520of%2520promptable%2520video%2520object%250Asegmentation%252C%2520referring%2520expression%2520segmentation%252C%2520and%2520object%2520captioning.%2520Since%250Aobtaining%2520data%2520for%2520this%2520task%2520is%2520tedious%2520and%2520expensive%252C%2520we%2520propose%2520to%2520annotate%250Aan%2520existing%2520large-scale%2520segmentation%2520dataset%2520%2528SAV%2529%2520with%2520pseudo%2520object%2520captions.%250AWe%2520do%2520so%2520by%2520preprocessing%2520videos%2520with%2520their%2520ground-truth%2520masks%2520to%2520highlight%2520the%250Aobject%2520of%2520interest%2520and%2520feed%2520this%2520to%2520a%2520large%2520Vision%2520Language%2520Model%2520%2528VLM%2529.%2520For%2520an%250Aunbiased%2520evaluation%252C%2520we%2520collect%2520manual%2520annotations%2520on%2520the%2520validation%2520set.%2520We%250Acall%2520the%2520resulting%2520dataset%2520SAV-Caption.%2520We%2520train%2520our%2520VoCap%2520model%2520at%2520scale%2520on%2520a%250ASAV-Caption%2520together%2520with%2520a%2520mix%2520of%2520other%2520image%2520and%2520video%2520datasets.%2520Our%2520model%250Ayields%2520state-of-the-art%2520results%2520on%2520referring%2520expression%2520video%2520object%250Asegmentation%252C%2520is%2520competitive%2520on%2520semi-supervised%2520video%2520object%2520segmentation%252C%2520and%250Aestablishes%2520a%2520benchmark%2520for%2520video%2520object%2520captioning.%2520Our%2520dataset%2520will%2520be%2520made%250Aavailable%2520at%2520https%253A//github.com/google-deepmind/vocap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoCap%3A%20Video%20Object%20Captioning%20and%20Segmentation%20from%20Any%20Prompt&entry.906535625=Jasper%20Uijlings%20and%20Xingyi%20Zhou%20and%20Xiuye%20Gu%20and%20Arsha%20Nagrani%20and%20Anurag%20Arnab%20and%20Alireza%20Fathi%20and%20David%20Ross%20and%20Cordelia%20Schmid&entry.1292438233=%20%20Understanding%20objects%20in%20videos%20in%20terms%20of%20fine-grained%20localization%20masks%0Aand%20detailed%20semantic%20properties%20is%20a%20fundamental%20task%20in%20video%20understanding.%0AIn%20this%20paper%2C%20we%20propose%20VoCap%2C%20a%20flexible%20video%20model%20that%20consumes%20a%20video%0Aand%20a%20prompt%20of%20various%20modalities%20%28text%2C%20box%20or%20mask%29%2C%20and%20produces%20a%0Aspatio-temporal%20masklet%20with%20a%20corresponding%20object-centric%20caption.%20As%20such%0Aour%20model%20addresses%20simultaneously%20the%20tasks%20of%20promptable%20video%20object%0Asegmentation%2C%20referring%20expression%20segmentation%2C%20and%20object%20captioning.%20Since%0Aobtaining%20data%20for%20this%20task%20is%20tedious%20and%20expensive%2C%20we%20propose%20to%20annotate%0Aan%20existing%20large-scale%20segmentation%20dataset%20%28SAV%29%20with%20pseudo%20object%20captions.%0AWe%20do%20so%20by%20preprocessing%20videos%20with%20their%20ground-truth%20masks%20to%20highlight%20the%0Aobject%20of%20interest%20and%20feed%20this%20to%20a%20large%20Vision%20Language%20Model%20%28VLM%29.%20For%20an%0Aunbiased%20evaluation%2C%20we%20collect%20manual%20annotations%20on%20the%20validation%20set.%20We%0Acall%20the%20resulting%20dataset%20SAV-Caption.%20We%20train%20our%20VoCap%20model%20at%20scale%20on%20a%0ASAV-Caption%20together%20with%20a%20mix%20of%20other%20image%20and%20video%20datasets.%20Our%20model%0Ayields%20state-of-the-art%20results%20on%20referring%20expression%20video%20object%0Asegmentation%2C%20is%20competitive%20on%20semi-supervised%20video%20object%20segmentation%2C%20and%0Aestablishes%20a%20benchmark%20for%20video%20object%20captioning.%20Our%20dataset%20will%20be%20made%0Aavailable%20at%20https%3A//github.com/google-deepmind/vocap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21809v1&entry.124074799=Read"},
{"title": "HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning\n  for Natural Language-Guided Drones", "author": "Hao Ruan and Jinliang Lin and Yingxin Lai and Zhiming Luo and Shaozi Li", "abstract": "  Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such\nas target matching and navigation. However, the wide field of view and complex\ncompositional semantics in drone scenarios pose challenges for vision-language\nunderstanding. Mainstream Vision-Language Models (VLMs) emphasize global\nalignment while lacking fine-grained semantics, and existing hierarchical\nmethods depend on precise entity partitioning and strict containment, limiting\neffectiveness in dynamic environments. To address this, we propose the\nHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)\nframework with two components: (1) Region-Global Image-Text Contrastive\nLearning (RG-ITC), which avoids precise scene partitioning and captures\nhierarchical local-to-global semantics by contrasting local visual regions with\nglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),\nwhich dispenses with rigid constraints and instead evaluates local semantic\nconsistency within global cross-modal representations, enhancing compositional\nreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,\ndestabilizing alignment. HCCM introduces a Momentum Contrast and Distillation\n(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM\nachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text\nretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot\ngeneralization with 39.93% mean recall (mR), outperforming fine-tuned\nbaselines.\n", "link": "http://arxiv.org/abs/2508.21539v1", "date": "2025-08-29", "relevancy": 2.7989, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HCCM%3A%20Hierarchical%20Cross-Granularity%20Contrastive%20and%20Matching%20Learning%0A%20%20for%20Natural%20Language-Guided%20Drones&body=Title%3A%20HCCM%3A%20Hierarchical%20Cross-Granularity%20Contrastive%20and%20Matching%20Learning%0A%20%20for%20Natural%20Language-Guided%20Drones%0AAuthor%3A%20Hao%20Ruan%20and%20Jinliang%20Lin%20and%20Yingxin%20Lai%20and%20Zhiming%20Luo%20and%20Shaozi%20Li%0AAbstract%3A%20%20%20Natural%20Language-Guided%20Drones%20%28NLGD%29%20provide%20a%20novel%20paradigm%20for%20tasks%20such%0Aas%20target%20matching%20and%20navigation.%20However%2C%20the%20wide%20field%20of%20view%20and%20complex%0Acompositional%20semantics%20in%20drone%20scenarios%20pose%20challenges%20for%20vision-language%0Aunderstanding.%20Mainstream%20Vision-Language%20Models%20%28VLMs%29%20emphasize%20global%0Aalignment%20while%20lacking%20fine-grained%20semantics%2C%20and%20existing%20hierarchical%0Amethods%20depend%20on%20precise%20entity%20partitioning%20and%20strict%20containment%2C%20limiting%0Aeffectiveness%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20propose%20the%0AHierarchical%20Cross-Granularity%20Contrastive%20and%20Matching%20learning%20%28HCCM%29%0Aframework%20with%20two%20components%3A%20%281%29%20Region-Global%20Image-Text%20Contrastive%0ALearning%20%28RG-ITC%29%2C%20which%20avoids%20precise%20scene%20partitioning%20and%20captures%0Ahierarchical%20local-to-global%20semantics%20by%20contrasting%20local%20visual%20regions%20with%0Aglobal%20text%20and%20vice%20versa%3B%20%282%29%20Region-Global%20Image-Text%20Matching%20%28RG-ITM%29%2C%0Awhich%20dispenses%20with%20rigid%20constraints%20and%20instead%20evaluates%20local%20semantic%0Aconsistency%20within%20global%20cross-modal%20representations%2C%20enhancing%20compositional%0Areasoning.%20Moreover%2C%20drone%20text%20descriptions%20are%20often%20incomplete%20or%20ambiguous%2C%0Adestabilizing%20alignment.%20HCCM%20introduces%20a%20Momentum%20Contrast%20and%20Distillation%0A%28MCD%29%20mechanism%20to%20improve%20robustness.%20Experiments%20on%20GeoText-1652%20show%20HCCM%0Aachieves%20state-of-the-art%20Recall%401%20of%2028.8%25%20%28image%20retrieval%29%20and%2014.7%25%20%28text%0Aretrieval%29.%20On%20the%20unseen%20ERA%20dataset%2C%20HCCM%20demonstrates%20strong%20zero-shot%0Ageneralization%20with%2039.93%25%20mean%20recall%20%28mR%29%2C%20outperforming%20fine-tuned%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHCCM%253A%2520Hierarchical%2520Cross-Granularity%2520Contrastive%2520and%2520Matching%2520Learning%250A%2520%2520for%2520Natural%2520Language-Guided%2520Drones%26entry.906535625%3DHao%2520Ruan%2520and%2520Jinliang%2520Lin%2520and%2520Yingxin%2520Lai%2520and%2520Zhiming%2520Luo%2520and%2520Shaozi%2520Li%26entry.1292438233%3D%2520%2520Natural%2520Language-Guided%2520Drones%2520%2528NLGD%2529%2520provide%2520a%2520novel%2520paradigm%2520for%2520tasks%2520such%250Aas%2520target%2520matching%2520and%2520navigation.%2520However%252C%2520the%2520wide%2520field%2520of%2520view%2520and%2520complex%250Acompositional%2520semantics%2520in%2520drone%2520scenarios%2520pose%2520challenges%2520for%2520vision-language%250Aunderstanding.%2520Mainstream%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520emphasize%2520global%250Aalignment%2520while%2520lacking%2520fine-grained%2520semantics%252C%2520and%2520existing%2520hierarchical%250Amethods%2520depend%2520on%2520precise%2520entity%2520partitioning%2520and%2520strict%2520containment%252C%2520limiting%250Aeffectiveness%2520in%2520dynamic%2520environments.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%250AHierarchical%2520Cross-Granularity%2520Contrastive%2520and%2520Matching%2520learning%2520%2528HCCM%2529%250Aframework%2520with%2520two%2520components%253A%2520%25281%2529%2520Region-Global%2520Image-Text%2520Contrastive%250ALearning%2520%2528RG-ITC%2529%252C%2520which%2520avoids%2520precise%2520scene%2520partitioning%2520and%2520captures%250Ahierarchical%2520local-to-global%2520semantics%2520by%2520contrasting%2520local%2520visual%2520regions%2520with%250Aglobal%2520text%2520and%2520vice%2520versa%253B%2520%25282%2529%2520Region-Global%2520Image-Text%2520Matching%2520%2528RG-ITM%2529%252C%250Awhich%2520dispenses%2520with%2520rigid%2520constraints%2520and%2520instead%2520evaluates%2520local%2520semantic%250Aconsistency%2520within%2520global%2520cross-modal%2520representations%252C%2520enhancing%2520compositional%250Areasoning.%2520Moreover%252C%2520drone%2520text%2520descriptions%2520are%2520often%2520incomplete%2520or%2520ambiguous%252C%250Adestabilizing%2520alignment.%2520HCCM%2520introduces%2520a%2520Momentum%2520Contrast%2520and%2520Distillation%250A%2528MCD%2529%2520mechanism%2520to%2520improve%2520robustness.%2520Experiments%2520on%2520GeoText-1652%2520show%2520HCCM%250Aachieves%2520state-of-the-art%2520Recall%25401%2520of%252028.8%2525%2520%2528image%2520retrieval%2529%2520and%252014.7%2525%2520%2528text%250Aretrieval%2529.%2520On%2520the%2520unseen%2520ERA%2520dataset%252C%2520HCCM%2520demonstrates%2520strong%2520zero-shot%250Ageneralization%2520with%252039.93%2525%2520mean%2520recall%2520%2528mR%2529%252C%2520outperforming%2520fine-tuned%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HCCM%3A%20Hierarchical%20Cross-Granularity%20Contrastive%20and%20Matching%20Learning%0A%20%20for%20Natural%20Language-Guided%20Drones&entry.906535625=Hao%20Ruan%20and%20Jinliang%20Lin%20and%20Yingxin%20Lai%20and%20Zhiming%20Luo%20and%20Shaozi%20Li&entry.1292438233=%20%20Natural%20Language-Guided%20Drones%20%28NLGD%29%20provide%20a%20novel%20paradigm%20for%20tasks%20such%0Aas%20target%20matching%20and%20navigation.%20However%2C%20the%20wide%20field%20of%20view%20and%20complex%0Acompositional%20semantics%20in%20drone%20scenarios%20pose%20challenges%20for%20vision-language%0Aunderstanding.%20Mainstream%20Vision-Language%20Models%20%28VLMs%29%20emphasize%20global%0Aalignment%20while%20lacking%20fine-grained%20semantics%2C%20and%20existing%20hierarchical%0Amethods%20depend%20on%20precise%20entity%20partitioning%20and%20strict%20containment%2C%20limiting%0Aeffectiveness%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20propose%20the%0AHierarchical%20Cross-Granularity%20Contrastive%20and%20Matching%20learning%20%28HCCM%29%0Aframework%20with%20two%20components%3A%20%281%29%20Region-Global%20Image-Text%20Contrastive%0ALearning%20%28RG-ITC%29%2C%20which%20avoids%20precise%20scene%20partitioning%20and%20captures%0Ahierarchical%20local-to-global%20semantics%20by%20contrasting%20local%20visual%20regions%20with%0Aglobal%20text%20and%20vice%20versa%3B%20%282%29%20Region-Global%20Image-Text%20Matching%20%28RG-ITM%29%2C%0Awhich%20dispenses%20with%20rigid%20constraints%20and%20instead%20evaluates%20local%20semantic%0Aconsistency%20within%20global%20cross-modal%20representations%2C%20enhancing%20compositional%0Areasoning.%20Moreover%2C%20drone%20text%20descriptions%20are%20often%20incomplete%20or%20ambiguous%2C%0Adestabilizing%20alignment.%20HCCM%20introduces%20a%20Momentum%20Contrast%20and%20Distillation%0A%28MCD%29%20mechanism%20to%20improve%20robustness.%20Experiments%20on%20GeoText-1652%20show%20HCCM%0Aachieves%20state-of-the-art%20Recall%401%20of%2028.8%25%20%28image%20retrieval%29%20and%2014.7%25%20%28text%0Aretrieval%29.%20On%20the%20unseen%20ERA%20dataset%2C%20HCCM%20demonstrates%20strong%20zero-shot%0Ageneralization%20with%2039.93%25%20mean%20recall%20%28mR%29%2C%20outperforming%20fine-tuned%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21539v1&entry.124074799=Read"},
{"title": "Learning from Silence and Noise for Visual Sound Source Localization", "author": "Xavier Juanola and Giovana Morais and Magdalena Fuentes and Gloria Haro", "abstract": "  Visual sound source localization is a fundamental perception task that aims\nto detect the location of sounding sources in a video given its audio. Despite\nrecent progress, we identify two shortcomings in current methods: 1) most\napproaches perform poorly in cases with low audio-visual semantic\ncorrespondence such as silence, noise, and offscreen sounds, i.e. in the\npresence of negative audio; and 2) most prior evaluations are limited to\npositive cases, where both datasets and metrics convey scenarios with a single\nvisible sound source in the scene. To address this, we introduce three key\ncontributions. First, we propose a new training strategy that incorporates\nsilence and noise, which improves performance in positive cases, while being\nmore robust against negative sounds. Our resulting self-supervised model,\nSSL-SaN, achieves state-of-the-art performance compared to other\nself-supervised models, both in sound localization and cross-modal retrieval.\nSecond, we propose a new metric that quantifies the trade-off between alignment\nand separability of auditory and visual features across positive and negative\naudio-visual pairs. Third, we present IS3+, an extended and improved version of\nthe IS3 synthetic dataset with negative audio.\n  Our data, metrics and code are available on the\nhttps://xavijuanola.github.io/SSL-SaN/.\n", "link": "http://arxiv.org/abs/2508.21761v1", "date": "2025-08-29", "relevancy": 2.7387, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Silence%20and%20Noise%20for%20Visual%20Sound%20Source%20Localization&body=Title%3A%20Learning%20from%20Silence%20and%20Noise%20for%20Visual%20Sound%20Source%20Localization%0AAuthor%3A%20Xavier%20Juanola%20and%20Giovana%20Morais%20and%20Magdalena%20Fuentes%20and%20Gloria%20Haro%0AAbstract%3A%20%20%20Visual%20sound%20source%20localization%20is%20a%20fundamental%20perception%20task%20that%20aims%0Ato%20detect%20the%20location%20of%20sounding%20sources%20in%20a%20video%20given%20its%20audio.%20Despite%0Arecent%20progress%2C%20we%20identify%20two%20shortcomings%20in%20current%20methods%3A%201%29%20most%0Aapproaches%20perform%20poorly%20in%20cases%20with%20low%20audio-visual%20semantic%0Acorrespondence%20such%20as%20silence%2C%20noise%2C%20and%20offscreen%20sounds%2C%20i.e.%20in%20the%0Apresence%20of%20negative%20audio%3B%20and%202%29%20most%20prior%20evaluations%20are%20limited%20to%0Apositive%20cases%2C%20where%20both%20datasets%20and%20metrics%20convey%20scenarios%20with%20a%20single%0Avisible%20sound%20source%20in%20the%20scene.%20To%20address%20this%2C%20we%20introduce%20three%20key%0Acontributions.%20First%2C%20we%20propose%20a%20new%20training%20strategy%20that%20incorporates%0Asilence%20and%20noise%2C%20which%20improves%20performance%20in%20positive%20cases%2C%20while%20being%0Amore%20robust%20against%20negative%20sounds.%20Our%20resulting%20self-supervised%20model%2C%0ASSL-SaN%2C%20achieves%20state-of-the-art%20performance%20compared%20to%20other%0Aself-supervised%20models%2C%20both%20in%20sound%20localization%20and%20cross-modal%20retrieval.%0ASecond%2C%20we%20propose%20a%20new%20metric%20that%20quantifies%20the%20trade-off%20between%20alignment%0Aand%20separability%20of%20auditory%20and%20visual%20features%20across%20positive%20and%20negative%0Aaudio-visual%20pairs.%20Third%2C%20we%20present%20IS3%2B%2C%20an%20extended%20and%20improved%20version%20of%0Athe%20IS3%20synthetic%20dataset%20with%20negative%20audio.%0A%20%20Our%20data%2C%20metrics%20and%20code%20are%20available%20on%20the%0Ahttps%3A//xavijuanola.github.io/SSL-SaN/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Silence%2520and%2520Noise%2520for%2520Visual%2520Sound%2520Source%2520Localization%26entry.906535625%3DXavier%2520Juanola%2520and%2520Giovana%2520Morais%2520and%2520Magdalena%2520Fuentes%2520and%2520Gloria%2520Haro%26entry.1292438233%3D%2520%2520Visual%2520sound%2520source%2520localization%2520is%2520a%2520fundamental%2520perception%2520task%2520that%2520aims%250Ato%2520detect%2520the%2520location%2520of%2520sounding%2520sources%2520in%2520a%2520video%2520given%2520its%2520audio.%2520Despite%250Arecent%2520progress%252C%2520we%2520identify%2520two%2520shortcomings%2520in%2520current%2520methods%253A%25201%2529%2520most%250Aapproaches%2520perform%2520poorly%2520in%2520cases%2520with%2520low%2520audio-visual%2520semantic%250Acorrespondence%2520such%2520as%2520silence%252C%2520noise%252C%2520and%2520offscreen%2520sounds%252C%2520i.e.%2520in%2520the%250Apresence%2520of%2520negative%2520audio%253B%2520and%25202%2529%2520most%2520prior%2520evaluations%2520are%2520limited%2520to%250Apositive%2520cases%252C%2520where%2520both%2520datasets%2520and%2520metrics%2520convey%2520scenarios%2520with%2520a%2520single%250Avisible%2520sound%2520source%2520in%2520the%2520scene.%2520To%2520address%2520this%252C%2520we%2520introduce%2520three%2520key%250Acontributions.%2520First%252C%2520we%2520propose%2520a%2520new%2520training%2520strategy%2520that%2520incorporates%250Asilence%2520and%2520noise%252C%2520which%2520improves%2520performance%2520in%2520positive%2520cases%252C%2520while%2520being%250Amore%2520robust%2520against%2520negative%2520sounds.%2520Our%2520resulting%2520self-supervised%2520model%252C%250ASSL-SaN%252C%2520achieves%2520state-of-the-art%2520performance%2520compared%2520to%2520other%250Aself-supervised%2520models%252C%2520both%2520in%2520sound%2520localization%2520and%2520cross-modal%2520retrieval.%250ASecond%252C%2520we%2520propose%2520a%2520new%2520metric%2520that%2520quantifies%2520the%2520trade-off%2520between%2520alignment%250Aand%2520separability%2520of%2520auditory%2520and%2520visual%2520features%2520across%2520positive%2520and%2520negative%250Aaudio-visual%2520pairs.%2520Third%252C%2520we%2520present%2520IS3%252B%252C%2520an%2520extended%2520and%2520improved%2520version%2520of%250Athe%2520IS3%2520synthetic%2520dataset%2520with%2520negative%2520audio.%250A%2520%2520Our%2520data%252C%2520metrics%2520and%2520code%2520are%2520available%2520on%2520the%250Ahttps%253A//xavijuanola.github.io/SSL-SaN/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Silence%20and%20Noise%20for%20Visual%20Sound%20Source%20Localization&entry.906535625=Xavier%20Juanola%20and%20Giovana%20Morais%20and%20Magdalena%20Fuentes%20and%20Gloria%20Haro&entry.1292438233=%20%20Visual%20sound%20source%20localization%20is%20a%20fundamental%20perception%20task%20that%20aims%0Ato%20detect%20the%20location%20of%20sounding%20sources%20in%20a%20video%20given%20its%20audio.%20Despite%0Arecent%20progress%2C%20we%20identify%20two%20shortcomings%20in%20current%20methods%3A%201%29%20most%0Aapproaches%20perform%20poorly%20in%20cases%20with%20low%20audio-visual%20semantic%0Acorrespondence%20such%20as%20silence%2C%20noise%2C%20and%20offscreen%20sounds%2C%20i.e.%20in%20the%0Apresence%20of%20negative%20audio%3B%20and%202%29%20most%20prior%20evaluations%20are%20limited%20to%0Apositive%20cases%2C%20where%20both%20datasets%20and%20metrics%20convey%20scenarios%20with%20a%20single%0Avisible%20sound%20source%20in%20the%20scene.%20To%20address%20this%2C%20we%20introduce%20three%20key%0Acontributions.%20First%2C%20we%20propose%20a%20new%20training%20strategy%20that%20incorporates%0Asilence%20and%20noise%2C%20which%20improves%20performance%20in%20positive%20cases%2C%20while%20being%0Amore%20robust%20against%20negative%20sounds.%20Our%20resulting%20self-supervised%20model%2C%0ASSL-SaN%2C%20achieves%20state-of-the-art%20performance%20compared%20to%20other%0Aself-supervised%20models%2C%20both%20in%20sound%20localization%20and%20cross-modal%20retrieval.%0ASecond%2C%20we%20propose%20a%20new%20metric%20that%20quantifies%20the%20trade-off%20between%20alignment%0Aand%20separability%20of%20auditory%20and%20visual%20features%20across%20positive%20and%20negative%0Aaudio-visual%20pairs.%20Third%2C%20we%20present%20IS3%2B%2C%20an%20extended%20and%20improved%20version%20of%0Athe%20IS3%20synthetic%20dataset%20with%20negative%20audio.%0A%20%20Our%20data%2C%20metrics%20and%20code%20are%20available%20on%20the%0Ahttps%3A//xavijuanola.github.io/SSL-SaN/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21761v1&entry.124074799=Read"},
{"title": "Domain Generalization in-the-Wild: Disentangling Classification from\n  Domain-Aware Representations", "author": "Ha Min Son and Zhe Zhao and Shahbaz Rezaei and Xin Liu", "abstract": "  Evaluating domain generalization (DG) for foundational models like CLIP is\nchallenging, as web-scale pretraining data potentially covers many existing\nbenchmarks. Consequently, current DG evaluation may neither be sufficiently\nchallenging nor adequately test genuinely unseen data scenarios. To better\nassess the performance of CLIP on DG in-the-wild, a scenario where CLIP\nencounters challenging unseen data, we consider two approaches: (1) evaluating\non 33 diverse datasets with quantified out-of-distribution (OOD) scores after\nfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'\nsome domains as an approximation. We observe that CLIP's performance\ndeteriorates significantly on more OOD datasets. To address this, we present\nCLIP-DCA (Disentangling Classification from enhanced domain Aware\nrepresentations). Our approach is motivated by the observation that while\nstandard domain invariance losses aim to make representations domain-invariant,\nthis can be harmful to foundation models by forcing the discarding of\ndomain-aware representations beneficial for generalization. We instead\nhypothesize that enhancing domain awareness is a prerequisite for effective\ndomain-invariant classification in foundation models. CLIP-DCA identifies and\nenhances domain awareness within CLIP's encoders using a separate domain head\nand synthetically generated diverse domain data. Simultaneously, it encourages\ndomain-invariant classification through disentanglement from the domain\nfeatures. CLIP-DCA shows significant improvements within this challenging\nevaluation compared to existing methods, particularly on datasets that are more\nOOD.\n", "link": "http://arxiv.org/abs/2508.21769v1", "date": "2025-08-29", "relevancy": 2.6808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20in-the-Wild%3A%20Disentangling%20Classification%20from%0A%20%20Domain-Aware%20Representations&body=Title%3A%20Domain%20Generalization%20in-the-Wild%3A%20Disentangling%20Classification%20from%0A%20%20Domain-Aware%20Representations%0AAuthor%3A%20Ha%20Min%20Son%20and%20Zhe%20Zhao%20and%20Shahbaz%20Rezaei%20and%20Xin%20Liu%0AAbstract%3A%20%20%20Evaluating%20domain%20generalization%20%28DG%29%20for%20foundational%20models%20like%20CLIP%20is%0Achallenging%2C%20as%20web-scale%20pretraining%20data%20potentially%20covers%20many%20existing%0Abenchmarks.%20Consequently%2C%20current%20DG%20evaluation%20may%20neither%20be%20sufficiently%0Achallenging%20nor%20adequately%20test%20genuinely%20unseen%20data%20scenarios.%20To%20better%0Aassess%20the%20performance%20of%20CLIP%20on%20DG%20in-the-wild%2C%20a%20scenario%20where%20CLIP%0Aencounters%20challenging%20unseen%20data%2C%20we%20consider%20two%20approaches%3A%20%281%29%20evaluating%0Aon%2033%20diverse%20datasets%20with%20quantified%20out-of-distribution%20%28OOD%29%20scores%20after%0Afine-tuning%20CLIP%20on%20ImageNet%2C%20and%20%282%29%20using%20unlearning%20to%20make%20CLIP%20%60forget%27%0Asome%20domains%20as%20an%20approximation.%20We%20observe%20that%20CLIP%27s%20performance%0Adeteriorates%20significantly%20on%20more%20OOD%20datasets.%20To%20address%20this%2C%20we%20present%0ACLIP-DCA%20%28Disentangling%20Classification%20from%20enhanced%20domain%20Aware%0Arepresentations%29.%20Our%20approach%20is%20motivated%20by%20the%20observation%20that%20while%0Astandard%20domain%20invariance%20losses%20aim%20to%20make%20representations%20domain-invariant%2C%0Athis%20can%20be%20harmful%20to%20foundation%20models%20by%20forcing%20the%20discarding%20of%0Adomain-aware%20representations%20beneficial%20for%20generalization.%20We%20instead%0Ahypothesize%20that%20enhancing%20domain%20awareness%20is%20a%20prerequisite%20for%20effective%0Adomain-invariant%20classification%20in%20foundation%20models.%20CLIP-DCA%20identifies%20and%0Aenhances%20domain%20awareness%20within%20CLIP%27s%20encoders%20using%20a%20separate%20domain%20head%0Aand%20synthetically%20generated%20diverse%20domain%20data.%20Simultaneously%2C%20it%20encourages%0Adomain-invariant%20classification%20through%20disentanglement%20from%20the%20domain%0Afeatures.%20CLIP-DCA%20shows%20significant%20improvements%20within%20this%20challenging%0Aevaluation%20compared%20to%20existing%20methods%2C%20particularly%20on%20datasets%20that%20are%20more%0AOOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520in-the-Wild%253A%2520Disentangling%2520Classification%2520from%250A%2520%2520Domain-Aware%2520Representations%26entry.906535625%3DHa%2520Min%2520Son%2520and%2520Zhe%2520Zhao%2520and%2520Shahbaz%2520Rezaei%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520Evaluating%2520domain%2520generalization%2520%2528DG%2529%2520for%2520foundational%2520models%2520like%2520CLIP%2520is%250Achallenging%252C%2520as%2520web-scale%2520pretraining%2520data%2520potentially%2520covers%2520many%2520existing%250Abenchmarks.%2520Consequently%252C%2520current%2520DG%2520evaluation%2520may%2520neither%2520be%2520sufficiently%250Achallenging%2520nor%2520adequately%2520test%2520genuinely%2520unseen%2520data%2520scenarios.%2520To%2520better%250Aassess%2520the%2520performance%2520of%2520CLIP%2520on%2520DG%2520in-the-wild%252C%2520a%2520scenario%2520where%2520CLIP%250Aencounters%2520challenging%2520unseen%2520data%252C%2520we%2520consider%2520two%2520approaches%253A%2520%25281%2529%2520evaluating%250Aon%252033%2520diverse%2520datasets%2520with%2520quantified%2520out-of-distribution%2520%2528OOD%2529%2520scores%2520after%250Afine-tuning%2520CLIP%2520on%2520ImageNet%252C%2520and%2520%25282%2529%2520using%2520unlearning%2520to%2520make%2520CLIP%2520%2560forget%2527%250Asome%2520domains%2520as%2520an%2520approximation.%2520We%2520observe%2520that%2520CLIP%2527s%2520performance%250Adeteriorates%2520significantly%2520on%2520more%2520OOD%2520datasets.%2520To%2520address%2520this%252C%2520we%2520present%250ACLIP-DCA%2520%2528Disentangling%2520Classification%2520from%2520enhanced%2520domain%2520Aware%250Arepresentations%2529.%2520Our%2520approach%2520is%2520motivated%2520by%2520the%2520observation%2520that%2520while%250Astandard%2520domain%2520invariance%2520losses%2520aim%2520to%2520make%2520representations%2520domain-invariant%252C%250Athis%2520can%2520be%2520harmful%2520to%2520foundation%2520models%2520by%2520forcing%2520the%2520discarding%2520of%250Adomain-aware%2520representations%2520beneficial%2520for%2520generalization.%2520We%2520instead%250Ahypothesize%2520that%2520enhancing%2520domain%2520awareness%2520is%2520a%2520prerequisite%2520for%2520effective%250Adomain-invariant%2520classification%2520in%2520foundation%2520models.%2520CLIP-DCA%2520identifies%2520and%250Aenhances%2520domain%2520awareness%2520within%2520CLIP%2527s%2520encoders%2520using%2520a%2520separate%2520domain%2520head%250Aand%2520synthetically%2520generated%2520diverse%2520domain%2520data.%2520Simultaneously%252C%2520it%2520encourages%250Adomain-invariant%2520classification%2520through%2520disentanglement%2520from%2520the%2520domain%250Afeatures.%2520CLIP-DCA%2520shows%2520significant%2520improvements%2520within%2520this%2520challenging%250Aevaluation%2520compared%2520to%2520existing%2520methods%252C%2520particularly%2520on%2520datasets%2520that%2520are%2520more%250AOOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20in-the-Wild%3A%20Disentangling%20Classification%20from%0A%20%20Domain-Aware%20Representations&entry.906535625=Ha%20Min%20Son%20and%20Zhe%20Zhao%20and%20Shahbaz%20Rezaei%20and%20Xin%20Liu&entry.1292438233=%20%20Evaluating%20domain%20generalization%20%28DG%29%20for%20foundational%20models%20like%20CLIP%20is%0Achallenging%2C%20as%20web-scale%20pretraining%20data%20potentially%20covers%20many%20existing%0Abenchmarks.%20Consequently%2C%20current%20DG%20evaluation%20may%20neither%20be%20sufficiently%0Achallenging%20nor%20adequately%20test%20genuinely%20unseen%20data%20scenarios.%20To%20better%0Aassess%20the%20performance%20of%20CLIP%20on%20DG%20in-the-wild%2C%20a%20scenario%20where%20CLIP%0Aencounters%20challenging%20unseen%20data%2C%20we%20consider%20two%20approaches%3A%20%281%29%20evaluating%0Aon%2033%20diverse%20datasets%20with%20quantified%20out-of-distribution%20%28OOD%29%20scores%20after%0Afine-tuning%20CLIP%20on%20ImageNet%2C%20and%20%282%29%20using%20unlearning%20to%20make%20CLIP%20%60forget%27%0Asome%20domains%20as%20an%20approximation.%20We%20observe%20that%20CLIP%27s%20performance%0Adeteriorates%20significantly%20on%20more%20OOD%20datasets.%20To%20address%20this%2C%20we%20present%0ACLIP-DCA%20%28Disentangling%20Classification%20from%20enhanced%20domain%20Aware%0Arepresentations%29.%20Our%20approach%20is%20motivated%20by%20the%20observation%20that%20while%0Astandard%20domain%20invariance%20losses%20aim%20to%20make%20representations%20domain-invariant%2C%0Athis%20can%20be%20harmful%20to%20foundation%20models%20by%20forcing%20the%20discarding%20of%0Adomain-aware%20representations%20beneficial%20for%20generalization.%20We%20instead%0Ahypothesize%20that%20enhancing%20domain%20awareness%20is%20a%20prerequisite%20for%20effective%0Adomain-invariant%20classification%20in%20foundation%20models.%20CLIP-DCA%20identifies%20and%0Aenhances%20domain%20awareness%20within%20CLIP%27s%20encoders%20using%20a%20separate%20domain%20head%0Aand%20synthetically%20generated%20diverse%20domain%20data.%20Simultaneously%2C%20it%20encourages%0Adomain-invariant%20classification%20through%20disentanglement%20from%20the%20domain%0Afeatures.%20CLIP-DCA%20shows%20significant%20improvements%20within%20this%20challenging%0Aevaluation%20compared%20to%20existing%20methods%2C%20particularly%20on%20datasets%20that%20are%20more%0AOOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21769v1&entry.124074799=Read"},
{"title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification", "author": "Yifei Yuan and Jiatong Li and Weijia Zhang and Mohammad Aliannejadi and Evangelos Kanoulas and Renjun Hu", "abstract": "  Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias.\n", "link": "http://arxiv.org/abs/2508.21561v1", "date": "2025-08-29", "relevancy": 2.6388, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Summarize-Exemplify-Reflect%3A%20Data-driven%20Insight%20Distillation%20Empowers%0A%20%20LLMs%20for%20Few-shot%20Tabular%20Classification&body=Title%3A%20Summarize-Exemplify-Reflect%3A%20Data-driven%20Insight%20Distillation%20Empowers%0A%20%20LLMs%20for%20Few-shot%20Tabular%20Classification%0AAuthor%3A%20Yifei%20Yuan%20and%20Jiatong%20Li%20and%20Weijia%20Zhang%20and%20Mohammad%20Aliannejadi%20and%20Evangelos%20Kanoulas%20and%20Renjun%20Hu%0AAbstract%3A%20%20%20Recent%20studies%20show%20the%20promise%20of%20large%20language%20models%20%28LLMs%29%20for%20few-shot%0Atabular%20classification%20but%20highlight%20challenges%20due%20to%20the%20variability%20in%0Astructured%20data.%20To%20address%20this%2C%20we%20propose%20distilling%20data%20into%20actionable%0Ainsights%20to%20enable%20robust%20and%20effective%20classification%20by%20LLMs.%20Drawing%0Ainspiration%20from%20human%20learning%20processes%2C%20we%20introduce%20InsightTab%2C%20an%20insight%0Adistillation%20framework%20guided%20by%20principles%20of%20divide-and-conquer%2C%20easy-first%2C%0Aand%20reflective%20learning.%20Our%20approach%20integrates%20rule%20summarization%2C%20strategic%0Aexemplification%2C%20and%20insight%20reflection%20through%20deep%20collaboration%20between%20LLMs%0Aand%20data%20modeling%20techniques.%20The%20obtained%20insights%20enable%20LLMs%20to%20better%20align%0Atheir%20general%20knowledge%20and%20capabilities%20with%20the%20particular%20requirements%20of%0Aspecific%20tabular%20tasks.%20We%20extensively%20evaluate%20InsightTab%20on%20nine%20datasets.%0AThe%20results%20demonstrate%20consistent%20improvement%20over%20state-of-the-art%20methods.%0AAblation%20studies%20further%20validate%20the%20principle-guided%20distillation%20process%2C%0Awhile%20analyses%20emphasize%20InsightTab%27s%20effectiveness%20in%20leveraging%20labeled%20data%0Aand%20managing%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSummarize-Exemplify-Reflect%253A%2520Data-driven%2520Insight%2520Distillation%2520Empowers%250A%2520%2520LLMs%2520for%2520Few-shot%2520Tabular%2520Classification%26entry.906535625%3DYifei%2520Yuan%2520and%2520Jiatong%2520Li%2520and%2520Weijia%2520Zhang%2520and%2520Mohammad%2520Aliannejadi%2520and%2520Evangelos%2520Kanoulas%2520and%2520Renjun%2520Hu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520the%2520promise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520few-shot%250Atabular%2520classification%2520but%2520highlight%2520challenges%2520due%2520to%2520the%2520variability%2520in%250Astructured%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520distilling%2520data%2520into%2520actionable%250Ainsights%2520to%2520enable%2520robust%2520and%2520effective%2520classification%2520by%2520LLMs.%2520Drawing%250Ainspiration%2520from%2520human%2520learning%2520processes%252C%2520we%2520introduce%2520InsightTab%252C%2520an%2520insight%250Adistillation%2520framework%2520guided%2520by%2520principles%2520of%2520divide-and-conquer%252C%2520easy-first%252C%250Aand%2520reflective%2520learning.%2520Our%2520approach%2520integrates%2520rule%2520summarization%252C%2520strategic%250Aexemplification%252C%2520and%2520insight%2520reflection%2520through%2520deep%2520collaboration%2520between%2520LLMs%250Aand%2520data%2520modeling%2520techniques.%2520The%2520obtained%2520insights%2520enable%2520LLMs%2520to%2520better%2520align%250Atheir%2520general%2520knowledge%2520and%2520capabilities%2520with%2520the%2520particular%2520requirements%2520of%250Aspecific%2520tabular%2520tasks.%2520We%2520extensively%2520evaluate%2520InsightTab%2520on%2520nine%2520datasets.%250AThe%2520results%2520demonstrate%2520consistent%2520improvement%2520over%2520state-of-the-art%2520methods.%250AAblation%2520studies%2520further%2520validate%2520the%2520principle-guided%2520distillation%2520process%252C%250Awhile%2520analyses%2520emphasize%2520InsightTab%2527s%2520effectiveness%2520in%2520leveraging%2520labeled%2520data%250Aand%2520managing%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Summarize-Exemplify-Reflect%3A%20Data-driven%20Insight%20Distillation%20Empowers%0A%20%20LLMs%20for%20Few-shot%20Tabular%20Classification&entry.906535625=Yifei%20Yuan%20and%20Jiatong%20Li%20and%20Weijia%20Zhang%20and%20Mohammad%20Aliannejadi%20and%20Evangelos%20Kanoulas%20and%20Renjun%20Hu&entry.1292438233=%20%20Recent%20studies%20show%20the%20promise%20of%20large%20language%20models%20%28LLMs%29%20for%20few-shot%0Atabular%20classification%20but%20highlight%20challenges%20due%20to%20the%20variability%20in%0Astructured%20data.%20To%20address%20this%2C%20we%20propose%20distilling%20data%20into%20actionable%0Ainsights%20to%20enable%20robust%20and%20effective%20classification%20by%20LLMs.%20Drawing%0Ainspiration%20from%20human%20learning%20processes%2C%20we%20introduce%20InsightTab%2C%20an%20insight%0Adistillation%20framework%20guided%20by%20principles%20of%20divide-and-conquer%2C%20easy-first%2C%0Aand%20reflective%20learning.%20Our%20approach%20integrates%20rule%20summarization%2C%20strategic%0Aexemplification%2C%20and%20insight%20reflection%20through%20deep%20collaboration%20between%20LLMs%0Aand%20data%20modeling%20techniques.%20The%20obtained%20insights%20enable%20LLMs%20to%20better%20align%0Atheir%20general%20knowledge%20and%20capabilities%20with%20the%20particular%20requirements%20of%0Aspecific%20tabular%20tasks.%20We%20extensively%20evaluate%20InsightTab%20on%20nine%20datasets.%0AThe%20results%20demonstrate%20consistent%20improvement%20over%20state-of-the-art%20methods.%0AAblation%20studies%20further%20validate%20the%20principle-guided%20distillation%20process%2C%0Awhile%20analyses%20emphasize%20InsightTab%27s%20effectiveness%20in%20leveraging%20labeled%20data%0Aand%20managing%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21561v1&entry.124074799=Read"},
{"title": "CHaRM: Conditioned Heatmap Regression Methodology for Accurate and Fast\n  Dental Landmark Localization", "author": "Jos\u00e9 Rodr\u00edguez-Ortega and Francisco P\u00e9rez-Hern\u00e1ndez and Siham Tabik", "abstract": "  Identifying anatomical landmarks in 3D dental models is essential for\northodontic treatment, yet manual placement is labor-intensive and requires\nexpert knowledge. While machine learning methods have been proposed for\nautomatic landmark detection in 3D Intraoral Scans (IOS), none provide a fully\nend-to-end solution that avoids costly tooth segmentation.\n  We present CHaRM (Conditioned Heatmap Regression Methodology), the first\nfully end-to-end deep learning approach for tooth landmark detection in 3D IOS.\nCHaRM integrates four components: a point cloud encoder, a decoder with a\nheatmap regression head, a teeth-presence classification head, and the novel\nCHaR module. The CHaR module leverages teeth-presence information to adapt to\nmissing teeth, improving detection accuracy in complex dental cases. Unlike\ntwo-stage workflows that segment teeth before landmarking, CHaRM operates\ndirectly on IOS point clouds, reducing complexity, avoiding error propagation,\nand lowering computational cost.\n  We evaluated CHaRM with five point cloud learning backbones on\nIOSLandmarks-1k, a new dataset of 1,214 annotated 3D dental models. Both the\ndataset and code will be publicly released to address the scarcity of open data\nin orthodontics and foster reproducible research.\n  CHaRM with PointMLP, named CHaRNet, achieved the best accuracy and\nefficiency. Compared to state-of-the-art methods (TSMDL and ALIIOS), CHaRNet\nreduced mean Euclidean distance error to 0.56 mm on standard dental models and\n1.12 mm across all dentition type, while delivering up to 14.8x faster\ninference on GPU. This end-to-end approach streamlines orthodontic workflows,\nenhances the precision of 3D IOS analysis, and enables efficient\ncomputer-assisted treatment planning.\n", "link": "http://arxiv.org/abs/2501.13073v5", "date": "2025-08-29", "relevancy": 2.5999, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5309}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHaRM%3A%20Conditioned%20Heatmap%20Regression%20Methodology%20for%20Accurate%20and%20Fast%0A%20%20Dental%20Landmark%20Localization&body=Title%3A%20CHaRM%3A%20Conditioned%20Heatmap%20Regression%20Methodology%20for%20Accurate%20and%20Fast%0A%20%20Dental%20Landmark%20Localization%0AAuthor%3A%20Jos%C3%A9%20Rodr%C3%ADguez-Ortega%20and%20Francisco%20P%C3%A9rez-Hern%C3%A1ndez%20and%20Siham%20Tabik%0AAbstract%3A%20%20%20Identifying%20anatomical%20landmarks%20in%203D%20dental%20models%20is%20essential%20for%0Aorthodontic%20treatment%2C%20yet%20manual%20placement%20is%20labor-intensive%20and%20requires%0Aexpert%20knowledge.%20While%20machine%20learning%20methods%20have%20been%20proposed%20for%0Aautomatic%20landmark%20detection%20in%203D%20Intraoral%20Scans%20%28IOS%29%2C%20none%20provide%20a%20fully%0Aend-to-end%20solution%20that%20avoids%20costly%20tooth%20segmentation.%0A%20%20We%20present%20CHaRM%20%28Conditioned%20Heatmap%20Regression%20Methodology%29%2C%20the%20first%0Afully%20end-to-end%20deep%20learning%20approach%20for%20tooth%20landmark%20detection%20in%203D%20IOS.%0ACHaRM%20integrates%20four%20components%3A%20a%20point%20cloud%20encoder%2C%20a%20decoder%20with%20a%0Aheatmap%20regression%20head%2C%20a%20teeth-presence%20classification%20head%2C%20and%20the%20novel%0ACHaR%20module.%20The%20CHaR%20module%20leverages%20teeth-presence%20information%20to%20adapt%20to%0Amissing%20teeth%2C%20improving%20detection%20accuracy%20in%20complex%20dental%20cases.%20Unlike%0Atwo-stage%20workflows%20that%20segment%20teeth%20before%20landmarking%2C%20CHaRM%20operates%0Adirectly%20on%20IOS%20point%20clouds%2C%20reducing%20complexity%2C%20avoiding%20error%20propagation%2C%0Aand%20lowering%20computational%20cost.%0A%20%20We%20evaluated%20CHaRM%20with%20five%20point%20cloud%20learning%20backbones%20on%0AIOSLandmarks-1k%2C%20a%20new%20dataset%20of%201%2C214%20annotated%203D%20dental%20models.%20Both%20the%0Adataset%20and%20code%20will%20be%20publicly%20released%20to%20address%20the%20scarcity%20of%20open%20data%0Ain%20orthodontics%20and%20foster%20reproducible%20research.%0A%20%20CHaRM%20with%20PointMLP%2C%20named%20CHaRNet%2C%20achieved%20the%20best%20accuracy%20and%0Aefficiency.%20Compared%20to%20state-of-the-art%20methods%20%28TSMDL%20and%20ALIIOS%29%2C%20CHaRNet%0Areduced%20mean%20Euclidean%20distance%20error%20to%200.56%20mm%20on%20standard%20dental%20models%20and%0A1.12%20mm%20across%20all%20dentition%20type%2C%20while%20delivering%20up%20to%2014.8x%20faster%0Ainference%20on%20GPU.%20This%20end-to-end%20approach%20streamlines%20orthodontic%20workflows%2C%0Aenhances%20the%20precision%20of%203D%20IOS%20analysis%2C%20and%20enables%20efficient%0Acomputer-assisted%20treatment%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13073v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHaRM%253A%2520Conditioned%2520Heatmap%2520Regression%2520Methodology%2520for%2520Accurate%2520and%2520Fast%250A%2520%2520Dental%2520Landmark%2520Localization%26entry.906535625%3DJos%25C3%25A9%2520Rodr%25C3%25ADguez-Ortega%2520and%2520Francisco%2520P%25C3%25A9rez-Hern%25C3%25A1ndez%2520and%2520Siham%2520Tabik%26entry.1292438233%3D%2520%2520Identifying%2520anatomical%2520landmarks%2520in%25203D%2520dental%2520models%2520is%2520essential%2520for%250Aorthodontic%2520treatment%252C%2520yet%2520manual%2520placement%2520is%2520labor-intensive%2520and%2520requires%250Aexpert%2520knowledge.%2520While%2520machine%2520learning%2520methods%2520have%2520been%2520proposed%2520for%250Aautomatic%2520landmark%2520detection%2520in%25203D%2520Intraoral%2520Scans%2520%2528IOS%2529%252C%2520none%2520provide%2520a%2520fully%250Aend-to-end%2520solution%2520that%2520avoids%2520costly%2520tooth%2520segmentation.%250A%2520%2520We%2520present%2520CHaRM%2520%2528Conditioned%2520Heatmap%2520Regression%2520Methodology%2529%252C%2520the%2520first%250Afully%2520end-to-end%2520deep%2520learning%2520approach%2520for%2520tooth%2520landmark%2520detection%2520in%25203D%2520IOS.%250ACHaRM%2520integrates%2520four%2520components%253A%2520a%2520point%2520cloud%2520encoder%252C%2520a%2520decoder%2520with%2520a%250Aheatmap%2520regression%2520head%252C%2520a%2520teeth-presence%2520classification%2520head%252C%2520and%2520the%2520novel%250ACHaR%2520module.%2520The%2520CHaR%2520module%2520leverages%2520teeth-presence%2520information%2520to%2520adapt%2520to%250Amissing%2520teeth%252C%2520improving%2520detection%2520accuracy%2520in%2520complex%2520dental%2520cases.%2520Unlike%250Atwo-stage%2520workflows%2520that%2520segment%2520teeth%2520before%2520landmarking%252C%2520CHaRM%2520operates%250Adirectly%2520on%2520IOS%2520point%2520clouds%252C%2520reducing%2520complexity%252C%2520avoiding%2520error%2520propagation%252C%250Aand%2520lowering%2520computational%2520cost.%250A%2520%2520We%2520evaluated%2520CHaRM%2520with%2520five%2520point%2520cloud%2520learning%2520backbones%2520on%250AIOSLandmarks-1k%252C%2520a%2520new%2520dataset%2520of%25201%252C214%2520annotated%25203D%2520dental%2520models.%2520Both%2520the%250Adataset%2520and%2520code%2520will%2520be%2520publicly%2520released%2520to%2520address%2520the%2520scarcity%2520of%2520open%2520data%250Ain%2520orthodontics%2520and%2520foster%2520reproducible%2520research.%250A%2520%2520CHaRM%2520with%2520PointMLP%252C%2520named%2520CHaRNet%252C%2520achieved%2520the%2520best%2520accuracy%2520and%250Aefficiency.%2520Compared%2520to%2520state-of-the-art%2520methods%2520%2528TSMDL%2520and%2520ALIIOS%2529%252C%2520CHaRNet%250Areduced%2520mean%2520Euclidean%2520distance%2520error%2520to%25200.56%2520mm%2520on%2520standard%2520dental%2520models%2520and%250A1.12%2520mm%2520across%2520all%2520dentition%2520type%252C%2520while%2520delivering%2520up%2520to%252014.8x%2520faster%250Ainference%2520on%2520GPU.%2520This%2520end-to-end%2520approach%2520streamlines%2520orthodontic%2520workflows%252C%250Aenhances%2520the%2520precision%2520of%25203D%2520IOS%2520analysis%252C%2520and%2520enables%2520efficient%250Acomputer-assisted%2520treatment%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13073v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHaRM%3A%20Conditioned%20Heatmap%20Regression%20Methodology%20for%20Accurate%20and%20Fast%0A%20%20Dental%20Landmark%20Localization&entry.906535625=Jos%C3%A9%20Rodr%C3%ADguez-Ortega%20and%20Francisco%20P%C3%A9rez-Hern%C3%A1ndez%20and%20Siham%20Tabik&entry.1292438233=%20%20Identifying%20anatomical%20landmarks%20in%203D%20dental%20models%20is%20essential%20for%0Aorthodontic%20treatment%2C%20yet%20manual%20placement%20is%20labor-intensive%20and%20requires%0Aexpert%20knowledge.%20While%20machine%20learning%20methods%20have%20been%20proposed%20for%0Aautomatic%20landmark%20detection%20in%203D%20Intraoral%20Scans%20%28IOS%29%2C%20none%20provide%20a%20fully%0Aend-to-end%20solution%20that%20avoids%20costly%20tooth%20segmentation.%0A%20%20We%20present%20CHaRM%20%28Conditioned%20Heatmap%20Regression%20Methodology%29%2C%20the%20first%0Afully%20end-to-end%20deep%20learning%20approach%20for%20tooth%20landmark%20detection%20in%203D%20IOS.%0ACHaRM%20integrates%20four%20components%3A%20a%20point%20cloud%20encoder%2C%20a%20decoder%20with%20a%0Aheatmap%20regression%20head%2C%20a%20teeth-presence%20classification%20head%2C%20and%20the%20novel%0ACHaR%20module.%20The%20CHaR%20module%20leverages%20teeth-presence%20information%20to%20adapt%20to%0Amissing%20teeth%2C%20improving%20detection%20accuracy%20in%20complex%20dental%20cases.%20Unlike%0Atwo-stage%20workflows%20that%20segment%20teeth%20before%20landmarking%2C%20CHaRM%20operates%0Adirectly%20on%20IOS%20point%20clouds%2C%20reducing%20complexity%2C%20avoiding%20error%20propagation%2C%0Aand%20lowering%20computational%20cost.%0A%20%20We%20evaluated%20CHaRM%20with%20five%20point%20cloud%20learning%20backbones%20on%0AIOSLandmarks-1k%2C%20a%20new%20dataset%20of%201%2C214%20annotated%203D%20dental%20models.%20Both%20the%0Adataset%20and%20code%20will%20be%20publicly%20released%20to%20address%20the%20scarcity%20of%20open%20data%0Ain%20orthodontics%20and%20foster%20reproducible%20research.%0A%20%20CHaRM%20with%20PointMLP%2C%20named%20CHaRNet%2C%20achieved%20the%20best%20accuracy%20and%0Aefficiency.%20Compared%20to%20state-of-the-art%20methods%20%28TSMDL%20and%20ALIIOS%29%2C%20CHaRNet%0Areduced%20mean%20Euclidean%20distance%20error%20to%200.56%20mm%20on%20standard%20dental%20models%20and%0A1.12%20mm%20across%20all%20dentition%20type%2C%20while%20delivering%20up%20to%2014.8x%20faster%0Ainference%20on%20GPU.%20This%20end-to-end%20approach%20streamlines%20orthodontic%20workflows%2C%0Aenhances%20the%20precision%20of%203D%20IOS%20analysis%2C%20and%20enables%20efficient%0Acomputer-assisted%20treatment%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13073v5&entry.124074799=Read"},
{"title": "QZhou-Embedding Technical Report", "author": "Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu", "abstract": "  We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.\n", "link": "http://arxiv.org/abs/2508.21632v1", "date": "2025-08-29", "relevancy": 2.5956, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QZhou-Embedding%20Technical%20Report&body=Title%3A%20QZhou-Embedding%20Technical%20Report%0AAuthor%3A%20Peng%20Yu%20and%20En%20Xu%20and%20Bin%20Chen%20and%20Haibiao%20Chen%20and%20Yinfei%20Xu%0AAbstract%3A%20%20%20We%20present%20QZhou-Embedding%2C%20a%20general-purpose%20contextual%20text%20embedding%20model%0Awith%20exceptional%20text%20representation%20capabilities.%20Built%20upon%20the%0AQwen2.5-7B-Instruct%20foundation%20model%2C%20we%20designed%20a%20unified%20multi-task%0Aframework%20comprising%20specialized%20data%20transformation%20and%20training%20strategies.%0AThe%20data%20transformation%20scheme%20enables%20the%20incorporation%20of%20more%20diverse%0Atextual%20training%20datasets%2C%20while%20the%20task-specific%20training%20strategies%20enhance%0Amodel%20learning%20efficiency.%20We%20developed%20a%20data%20synthesis%20pipeline%20leveraging%0ALLM%20API%2C%20incorporating%20techniques%20such%20as%20paraphrasing%2C%20augmentation%2C%20and%20hard%0Anegative%20example%20generation%20to%20improve%20the%20semantic%20richness%20and%20sample%0Adifficulty%20of%20the%20training%20set.%20Additionally%2C%20we%20employ%20a%20two-stage%20training%0Astrategy%2C%20comprising%20initial%20retrieval-focused%20pretraining%20followed%20by%0Afull-task%20fine-tuning%2C%20enabling%20the%20embedding%20model%20to%20extend%20its%20capabilities%0Abased%20on%20robust%20retrieval%20performance.%20Our%20model%20achieves%20state-of-the-art%0Aresults%20on%20the%20MTEB%20and%20CMTEB%20benchmarks%2C%20ranking%20first%20on%20both%20leaderboards%0A%28August%2027%202025%29%2C%20and%20simultaneously%20achieves%20state-of-the-art%20performance%20on%0Atasks%20including%20reranking%2C%20clustering%2C%20etc.%20Our%20findings%20demonstrate%20that%0Ahigher-quality%2C%20more%20diverse%20data%20is%20crucial%20for%20advancing%20retrieval%20model%0Aperformance%2C%20and%20that%20leveraging%20LLMs%20generative%20capabilities%20can%20further%0Aoptimize%20data%20quality%20for%20embedding%20model%20breakthroughs.%20Our%20model%20weights%20are%0Areleased%20on%20HuggingFace%20under%20Apache%202.0%20license.%20For%20reproducibility%2C%20we%0Aprovide%20evaluation%20code%20and%20instructions%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQZhou-Embedding%2520Technical%2520Report%26entry.906535625%3DPeng%2520Yu%2520and%2520En%2520Xu%2520and%2520Bin%2520Chen%2520and%2520Haibiao%2520Chen%2520and%2520Yinfei%2520Xu%26entry.1292438233%3D%2520%2520We%2520present%2520QZhou-Embedding%252C%2520a%2520general-purpose%2520contextual%2520text%2520embedding%2520model%250Awith%2520exceptional%2520text%2520representation%2520capabilities.%2520Built%2520upon%2520the%250AQwen2.5-7B-Instruct%2520foundation%2520model%252C%2520we%2520designed%2520a%2520unified%2520multi-task%250Aframework%2520comprising%2520specialized%2520data%2520transformation%2520and%2520training%2520strategies.%250AThe%2520data%2520transformation%2520scheme%2520enables%2520the%2520incorporation%2520of%2520more%2520diverse%250Atextual%2520training%2520datasets%252C%2520while%2520the%2520task-specific%2520training%2520strategies%2520enhance%250Amodel%2520learning%2520efficiency.%2520We%2520developed%2520a%2520data%2520synthesis%2520pipeline%2520leveraging%250ALLM%2520API%252C%2520incorporating%2520techniques%2520such%2520as%2520paraphrasing%252C%2520augmentation%252C%2520and%2520hard%250Anegative%2520example%2520generation%2520to%2520improve%2520the%2520semantic%2520richness%2520and%2520sample%250Adifficulty%2520of%2520the%2520training%2520set.%2520Additionally%252C%2520we%2520employ%2520a%2520two-stage%2520training%250Astrategy%252C%2520comprising%2520initial%2520retrieval-focused%2520pretraining%2520followed%2520by%250Afull-task%2520fine-tuning%252C%2520enabling%2520the%2520embedding%2520model%2520to%2520extend%2520its%2520capabilities%250Abased%2520on%2520robust%2520retrieval%2520performance.%2520Our%2520model%2520achieves%2520state-of-the-art%250Aresults%2520on%2520the%2520MTEB%2520and%2520CMTEB%2520benchmarks%252C%2520ranking%2520first%2520on%2520both%2520leaderboards%250A%2528August%252027%25202025%2529%252C%2520and%2520simultaneously%2520achieves%2520state-of-the-art%2520performance%2520on%250Atasks%2520including%2520reranking%252C%2520clustering%252C%2520etc.%2520Our%2520findings%2520demonstrate%2520that%250Ahigher-quality%252C%2520more%2520diverse%2520data%2520is%2520crucial%2520for%2520advancing%2520retrieval%2520model%250Aperformance%252C%2520and%2520that%2520leveraging%2520LLMs%2520generative%2520capabilities%2520can%2520further%250Aoptimize%2520data%2520quality%2520for%2520embedding%2520model%2520breakthroughs.%2520Our%2520model%2520weights%2520are%250Areleased%2520on%2520HuggingFace%2520under%2520Apache%25202.0%2520license.%2520For%2520reproducibility%252C%2520we%250Aprovide%2520evaluation%2520code%2520and%2520instructions%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QZhou-Embedding%20Technical%20Report&entry.906535625=Peng%20Yu%20and%20En%20Xu%20and%20Bin%20Chen%20and%20Haibiao%20Chen%20and%20Yinfei%20Xu&entry.1292438233=%20%20We%20present%20QZhou-Embedding%2C%20a%20general-purpose%20contextual%20text%20embedding%20model%0Awith%20exceptional%20text%20representation%20capabilities.%20Built%20upon%20the%0AQwen2.5-7B-Instruct%20foundation%20model%2C%20we%20designed%20a%20unified%20multi-task%0Aframework%20comprising%20specialized%20data%20transformation%20and%20training%20strategies.%0AThe%20data%20transformation%20scheme%20enables%20the%20incorporation%20of%20more%20diverse%0Atextual%20training%20datasets%2C%20while%20the%20task-specific%20training%20strategies%20enhance%0Amodel%20learning%20efficiency.%20We%20developed%20a%20data%20synthesis%20pipeline%20leveraging%0ALLM%20API%2C%20incorporating%20techniques%20such%20as%20paraphrasing%2C%20augmentation%2C%20and%20hard%0Anegative%20example%20generation%20to%20improve%20the%20semantic%20richness%20and%20sample%0Adifficulty%20of%20the%20training%20set.%20Additionally%2C%20we%20employ%20a%20two-stage%20training%0Astrategy%2C%20comprising%20initial%20retrieval-focused%20pretraining%20followed%20by%0Afull-task%20fine-tuning%2C%20enabling%20the%20embedding%20model%20to%20extend%20its%20capabilities%0Abased%20on%20robust%20retrieval%20performance.%20Our%20model%20achieves%20state-of-the-art%0Aresults%20on%20the%20MTEB%20and%20CMTEB%20benchmarks%2C%20ranking%20first%20on%20both%20leaderboards%0A%28August%2027%202025%29%2C%20and%20simultaneously%20achieves%20state-of-the-art%20performance%20on%0Atasks%20including%20reranking%2C%20clustering%2C%20etc.%20Our%20findings%20demonstrate%20that%0Ahigher-quality%2C%20more%20diverse%20data%20is%20crucial%20for%20advancing%20retrieval%20model%0Aperformance%2C%20and%20that%20leveraging%20LLMs%20generative%20capabilities%20can%20further%0Aoptimize%20data%20quality%20for%20embedding%20model%20breakthroughs.%20Our%20model%20weights%20are%0Areleased%20on%20HuggingFace%20under%20Apache%202.0%20license.%20For%20reproducibility%2C%20we%0Aprovide%20evaluation%20code%20and%20instructions%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21632v1&entry.124074799=Read"},
{"title": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional\n  Neural Networks", "author": "Amirhossein Nazeri and Wael Hafez", "abstract": "  Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance.\n", "link": "http://arxiv.org/abs/2508.21715v1", "date": "2025-08-29", "relevancy": 2.5297, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5108}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Based%20Non-Invasive%20Reliability%20Monitoring%20of%20Convolutional%0A%20%20Neural%20Networks&body=Title%3A%20Entropy-Based%20Non-Invasive%20Reliability%20Monitoring%20of%20Convolutional%0A%20%20Neural%20Networks%0AAuthor%3A%20Amirhossein%20Nazeri%20and%20Wael%20Hafez%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20become%20the%20foundation%20of%20modern%0Acomputer%20vision%2C%20achieving%20unprecedented%20accuracy%20across%20diverse%20image%0Arecognition%20tasks.%20While%20these%20networks%20excel%20on%20in-distribution%20data%2C%20they%0Aremain%20vulnerable%20to%20adversarial%20perturbations%20imperceptible%20input%0Amodifications%20that%20cause%20misclassification%20with%20high%20confidence.%20However%2C%0Aexisting%20detection%20methods%20either%20require%20expensive%20retraining%2C%20modify%20network%0Aarchitecture%2C%20or%20degrade%20performance%20on%20clean%20inputs.%20Here%20we%20show%20that%0Aadversarial%20perturbations%20create%20immediate%2C%20detectable%20entropy%20signatures%20in%0ACNN%20activations%20that%20can%20be%20monitored%20without%20any%20model%20modification.%20Using%0Aparallel%20entropy%20monitoring%20on%20VGG-16%2C%20we%20demonstrate%20that%20adversarial%20inputs%0Aconsistently%20shift%20activation%20entropy%20by%207%25%20in%20early%20convolutional%20layers%2C%0Aenabling%2090%25%20detection%20accuracy%20with%20false%20positives%20and%20false%20negative%20rates%0Abelow%2020%25.%20The%20complete%20separation%20between%20clean%20and%20adversarial%20entropy%0Adistributions%20reveals%20that%20CNNs%20inherently%20encode%20distribution%20shifts%20in%20their%0Aactivation%20patterns.%20This%20work%20establishes%20that%20CNN%20reliability%20can%20be%20assessed%0Athrough%20activation%20entropy%20alone%2C%20enabling%20practical%20deployment%20of%0Aself-diagnostic%20vision%20systems%20that%20detect%20adversarial%20inputs%20in%20real-time%0Awithout%20compromising%20original%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Based%2520Non-Invasive%2520Reliability%2520Monitoring%2520of%2520Convolutional%250A%2520%2520Neural%2520Networks%26entry.906535625%3DAmirhossein%2520Nazeri%2520and%2520Wael%2520Hafez%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520become%2520the%2520foundation%2520of%2520modern%250Acomputer%2520vision%252C%2520achieving%2520unprecedented%2520accuracy%2520across%2520diverse%2520image%250Arecognition%2520tasks.%2520While%2520these%2520networks%2520excel%2520on%2520in-distribution%2520data%252C%2520they%250Aremain%2520vulnerable%2520to%2520adversarial%2520perturbations%2520imperceptible%2520input%250Amodifications%2520that%2520cause%2520misclassification%2520with%2520high%2520confidence.%2520However%252C%250Aexisting%2520detection%2520methods%2520either%2520require%2520expensive%2520retraining%252C%2520modify%2520network%250Aarchitecture%252C%2520or%2520degrade%2520performance%2520on%2520clean%2520inputs.%2520Here%2520we%2520show%2520that%250Aadversarial%2520perturbations%2520create%2520immediate%252C%2520detectable%2520entropy%2520signatures%2520in%250ACNN%2520activations%2520that%2520can%2520be%2520monitored%2520without%2520any%2520model%2520modification.%2520Using%250Aparallel%2520entropy%2520monitoring%2520on%2520VGG-16%252C%2520we%2520demonstrate%2520that%2520adversarial%2520inputs%250Aconsistently%2520shift%2520activation%2520entropy%2520by%25207%2525%2520in%2520early%2520convolutional%2520layers%252C%250Aenabling%252090%2525%2520detection%2520accuracy%2520with%2520false%2520positives%2520and%2520false%2520negative%2520rates%250Abelow%252020%2525.%2520The%2520complete%2520separation%2520between%2520clean%2520and%2520adversarial%2520entropy%250Adistributions%2520reveals%2520that%2520CNNs%2520inherently%2520encode%2520distribution%2520shifts%2520in%2520their%250Aactivation%2520patterns.%2520This%2520work%2520establishes%2520that%2520CNN%2520reliability%2520can%2520be%2520assessed%250Athrough%2520activation%2520entropy%2520alone%252C%2520enabling%2520practical%2520deployment%2520of%250Aself-diagnostic%2520vision%2520systems%2520that%2520detect%2520adversarial%2520inputs%2520in%2520real-time%250Awithout%2520compromising%2520original%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Based%20Non-Invasive%20Reliability%20Monitoring%20of%20Convolutional%0A%20%20Neural%20Networks&entry.906535625=Amirhossein%20Nazeri%20and%20Wael%20Hafez&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20become%20the%20foundation%20of%20modern%0Acomputer%20vision%2C%20achieving%20unprecedented%20accuracy%20across%20diverse%20image%0Arecognition%20tasks.%20While%20these%20networks%20excel%20on%20in-distribution%20data%2C%20they%0Aremain%20vulnerable%20to%20adversarial%20perturbations%20imperceptible%20input%0Amodifications%20that%20cause%20misclassification%20with%20high%20confidence.%20However%2C%0Aexisting%20detection%20methods%20either%20require%20expensive%20retraining%2C%20modify%20network%0Aarchitecture%2C%20or%20degrade%20performance%20on%20clean%20inputs.%20Here%20we%20show%20that%0Aadversarial%20perturbations%20create%20immediate%2C%20detectable%20entropy%20signatures%20in%0ACNN%20activations%20that%20can%20be%20monitored%20without%20any%20model%20modification.%20Using%0Aparallel%20entropy%20monitoring%20on%20VGG-16%2C%20we%20demonstrate%20that%20adversarial%20inputs%0Aconsistently%20shift%20activation%20entropy%20by%207%25%20in%20early%20convolutional%20layers%2C%0Aenabling%2090%25%20detection%20accuracy%20with%20false%20positives%20and%20false%20negative%20rates%0Abelow%2020%25.%20The%20complete%20separation%20between%20clean%20and%20adversarial%20entropy%0Adistributions%20reveals%20that%20CNNs%20inherently%20encode%20distribution%20shifts%20in%20their%0Aactivation%20patterns.%20This%20work%20establishes%20that%20CNN%20reliability%20can%20be%20assessed%0Athrough%20activation%20entropy%20alone%2C%20enabling%20practical%20deployment%20of%0Aself-diagnostic%20vision%20systems%20that%20detect%20adversarial%20inputs%20in%20real-time%0Awithout%20compromising%20original%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21715v1&entry.124074799=Read"},
{"title": "Compression versus Accuracy: A Hierarchy of Lifted Models", "author": "Jan Speller and Malte Luttermann and Marcel Gehrke and Tanya Braun", "abstract": "  Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models.\n", "link": "http://arxiv.org/abs/2505.22288v2", "date": "2025-08-29", "relevancy": 2.5227, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20versus%20Accuracy%3A%20A%20Hierarchy%20of%20Lifted%20Models&body=Title%3A%20Compression%20versus%20Accuracy%3A%20A%20Hierarchy%20of%20Lifted%20Models%0AAuthor%3A%20Jan%20Speller%20and%20Malte%20Luttermann%20and%20Marcel%20Gehrke%20and%20Tanya%20Braun%0AAbstract%3A%20%20%20Probabilistic%20graphical%20models%20that%20encode%20indistinguishable%20objects%20and%0Arelations%20among%20them%20use%20first-order%20logic%20constructs%20to%20compress%20a%0Apropositional%20factorised%20model%20for%20more%20efficient%20%28lifted%29%20inference.%20To%20obtain%0Aa%20lifted%20representation%2C%20the%20state-of-the-art%20algorithm%20Advanced%20Colour%20Passing%0A%28ACP%29%20groups%20factors%20that%20represent%20matching%20distributions.%20In%20an%20approximate%0Aversion%20using%20%24%5Cvarepsilon%24%20as%20a%20hyperparameter%2C%20factors%20are%20grouped%20that%0Adiffer%20by%20a%20factor%20of%20at%20most%20%24%281%5Cpm%20%5Cvarepsilon%29%24.%20However%2C%20finding%20a%20suitable%0A%24%5Cvarepsilon%24%20is%20not%20obvious%20and%20may%20need%20a%20lot%20of%20exploration%2C%20possibly%0Arequiring%20many%20ACP%20runs%20with%20different%20%24%5Cvarepsilon%24%20values.%20Additionally%2C%0Avarying%20%24%5Cvarepsilon%24%20can%20yield%20wildly%20different%20models%2C%20leading%20to%20decreased%0Ainterpretability.%20Therefore%2C%20this%20paper%20presents%20a%20hierarchical%20approach%20to%0Alifted%20model%20construction%20that%20is%20hyperparameter-free.%20It%20efficiently%20computes%0Aa%20hierarchy%20of%20%24%5Cvarepsilon%24%20values%20that%20ensures%20a%20hierarchy%20of%20models%2C%20meaning%0Athat%20once%20factors%20are%20grouped%20together%20given%20some%20%24%5Cvarepsilon%24%2C%20these%20factors%0Awill%20be%20grouped%20together%20for%20larger%20%24%5Cvarepsilon%24%20as%20well.%20The%20hierarchy%20of%0A%24%5Cvarepsilon%24%20values%20also%20leads%20to%20a%20hierarchy%20of%20error%20bounds.%20This%20allows%20for%0Aexplicitly%20weighing%20compression%20versus%20accuracy%20when%20choosing%20specific%0A%24%5Cvarepsilon%24%20values%20to%20run%20ACP%20with%20and%20enables%20interpretability%20between%20the%0Adifferent%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520versus%2520Accuracy%253A%2520A%2520Hierarchy%2520of%2520Lifted%2520Models%26entry.906535625%3DJan%2520Speller%2520and%2520Malte%2520Luttermann%2520and%2520Marcel%2520Gehrke%2520and%2520Tanya%2520Braun%26entry.1292438233%3D%2520%2520Probabilistic%2520graphical%2520models%2520that%2520encode%2520indistinguishable%2520objects%2520and%250Arelations%2520among%2520them%2520use%2520first-order%2520logic%2520constructs%2520to%2520compress%2520a%250Apropositional%2520factorised%2520model%2520for%2520more%2520efficient%2520%2528lifted%2529%2520inference.%2520To%2520obtain%250Aa%2520lifted%2520representation%252C%2520the%2520state-of-the-art%2520algorithm%2520Advanced%2520Colour%2520Passing%250A%2528ACP%2529%2520groups%2520factors%2520that%2520represent%2520matching%2520distributions.%2520In%2520an%2520approximate%250Aversion%2520using%2520%2524%255Cvarepsilon%2524%2520as%2520a%2520hyperparameter%252C%2520factors%2520are%2520grouped%2520that%250Adiffer%2520by%2520a%2520factor%2520of%2520at%2520most%2520%2524%25281%255Cpm%2520%255Cvarepsilon%2529%2524.%2520However%252C%2520finding%2520a%2520suitable%250A%2524%255Cvarepsilon%2524%2520is%2520not%2520obvious%2520and%2520may%2520need%2520a%2520lot%2520of%2520exploration%252C%2520possibly%250Arequiring%2520many%2520ACP%2520runs%2520with%2520different%2520%2524%255Cvarepsilon%2524%2520values.%2520Additionally%252C%250Avarying%2520%2524%255Cvarepsilon%2524%2520can%2520yield%2520wildly%2520different%2520models%252C%2520leading%2520to%2520decreased%250Ainterpretability.%2520Therefore%252C%2520this%2520paper%2520presents%2520a%2520hierarchical%2520approach%2520to%250Alifted%2520model%2520construction%2520that%2520is%2520hyperparameter-free.%2520It%2520efficiently%2520computes%250Aa%2520hierarchy%2520of%2520%2524%255Cvarepsilon%2524%2520values%2520that%2520ensures%2520a%2520hierarchy%2520of%2520models%252C%2520meaning%250Athat%2520once%2520factors%2520are%2520grouped%2520together%2520given%2520some%2520%2524%255Cvarepsilon%2524%252C%2520these%2520factors%250Awill%2520be%2520grouped%2520together%2520for%2520larger%2520%2524%255Cvarepsilon%2524%2520as%2520well.%2520The%2520hierarchy%2520of%250A%2524%255Cvarepsilon%2524%2520values%2520also%2520leads%2520to%2520a%2520hierarchy%2520of%2520error%2520bounds.%2520This%2520allows%2520for%250Aexplicitly%2520weighing%2520compression%2520versus%2520accuracy%2520when%2520choosing%2520specific%250A%2524%255Cvarepsilon%2524%2520values%2520to%2520run%2520ACP%2520with%2520and%2520enables%2520interpretability%2520between%2520the%250Adifferent%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20versus%20Accuracy%3A%20A%20Hierarchy%20of%20Lifted%20Models&entry.906535625=Jan%20Speller%20and%20Malte%20Luttermann%20and%20Marcel%20Gehrke%20and%20Tanya%20Braun&entry.1292438233=%20%20Probabilistic%20graphical%20models%20that%20encode%20indistinguishable%20objects%20and%0Arelations%20among%20them%20use%20first-order%20logic%20constructs%20to%20compress%20a%0Apropositional%20factorised%20model%20for%20more%20efficient%20%28lifted%29%20inference.%20To%20obtain%0Aa%20lifted%20representation%2C%20the%20state-of-the-art%20algorithm%20Advanced%20Colour%20Passing%0A%28ACP%29%20groups%20factors%20that%20represent%20matching%20distributions.%20In%20an%20approximate%0Aversion%20using%20%24%5Cvarepsilon%24%20as%20a%20hyperparameter%2C%20factors%20are%20grouped%20that%0Adiffer%20by%20a%20factor%20of%20at%20most%20%24%281%5Cpm%20%5Cvarepsilon%29%24.%20However%2C%20finding%20a%20suitable%0A%24%5Cvarepsilon%24%20is%20not%20obvious%20and%20may%20need%20a%20lot%20of%20exploration%2C%20possibly%0Arequiring%20many%20ACP%20runs%20with%20different%20%24%5Cvarepsilon%24%20values.%20Additionally%2C%0Avarying%20%24%5Cvarepsilon%24%20can%20yield%20wildly%20different%20models%2C%20leading%20to%20decreased%0Ainterpretability.%20Therefore%2C%20this%20paper%20presents%20a%20hierarchical%20approach%20to%0Alifted%20model%20construction%20that%20is%20hyperparameter-free.%20It%20efficiently%20computes%0Aa%20hierarchy%20of%20%24%5Cvarepsilon%24%20values%20that%20ensures%20a%20hierarchy%20of%20models%2C%20meaning%0Athat%20once%20factors%20are%20grouped%20together%20given%20some%20%24%5Cvarepsilon%24%2C%20these%20factors%0Awill%20be%20grouped%20together%20for%20larger%20%24%5Cvarepsilon%24%20as%20well.%20The%20hierarchy%20of%0A%24%5Cvarepsilon%24%20values%20also%20leads%20to%20a%20hierarchy%20of%20error%20bounds.%20This%20allows%20for%0Aexplicitly%20weighing%20compression%20versus%20accuracy%20when%20choosing%20specific%0A%24%5Cvarepsilon%24%20values%20to%20run%20ACP%20with%20and%20enables%20interpretability%20between%20the%0Adifferent%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22288v2&entry.124074799=Read"},
{"title": "Adversarial Patch Attack for Ship Detection via Localized Augmentation", "author": "Chun Liu and Panpan Ding and Zheng Zheng and Hailong Wang and Bingqian Zhu and Tao Xu and Zhigang Han and Jiayao Wang", "abstract": "  Current ship detection techniques based on remote sensing imagery primarily\nrely on the object detection capabilities of deep neural networks (DNNs).\nHowever, DNNs are vulnerable to adversarial patch attacks, which can lead to\nmisclassification by the detection model or complete evasion of the targets.\nNumerous studies have demonstrated that data transformation-based methods can\nimprove the transferability of adversarial examples. However, excessive\naugmentation of image backgrounds or irrelevant regions may introduce\nunnecessary interference, resulting in false detections of the object detection\nmodel. These errors are not caused by the adversarial patches themselves but\nrather by the over-augmentation of background and non-target areas. This paper\nproposes a localized augmentation method that applies augmentation only to the\ntarget regions, avoiding any influence on non-target areas. By reducing\nbackground interference, this approach enables the loss function to focus more\ndirectly on the impact of the adversarial patch on the detection model, thereby\nimproving the attack success rate. Experiments conducted on the HRSC2016\ndataset demonstrate that the proposed method effectively increases the success\nrate of adversarial patch attacks and enhances their transferability.\n", "link": "http://arxiv.org/abs/2508.21472v1", "date": "2025-08-29", "relevancy": 2.4957, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.521}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4958}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Patch%20Attack%20for%20Ship%20Detection%20via%20Localized%20Augmentation&body=Title%3A%20Adversarial%20Patch%20Attack%20for%20Ship%20Detection%20via%20Localized%20Augmentation%0AAuthor%3A%20Chun%20Liu%20and%20Panpan%20Ding%20and%20Zheng%20Zheng%20and%20Hailong%20Wang%20and%20Bingqian%20Zhu%20and%20Tao%20Xu%20and%20Zhigang%20Han%20and%20Jiayao%20Wang%0AAbstract%3A%20%20%20Current%20ship%20detection%20techniques%20based%20on%20remote%20sensing%20imagery%20primarily%0Arely%20on%20the%20object%20detection%20capabilities%20of%20deep%20neural%20networks%20%28DNNs%29.%0AHowever%2C%20DNNs%20are%20vulnerable%20to%20adversarial%20patch%20attacks%2C%20which%20can%20lead%20to%0Amisclassification%20by%20the%20detection%20model%20or%20complete%20evasion%20of%20the%20targets.%0ANumerous%20studies%20have%20demonstrated%20that%20data%20transformation-based%20methods%20can%0Aimprove%20the%20transferability%20of%20adversarial%20examples.%20However%2C%20excessive%0Aaugmentation%20of%20image%20backgrounds%20or%20irrelevant%20regions%20may%20introduce%0Aunnecessary%20interference%2C%20resulting%20in%20false%20detections%20of%20the%20object%20detection%0Amodel.%20These%20errors%20are%20not%20caused%20by%20the%20adversarial%20patches%20themselves%20but%0Arather%20by%20the%20over-augmentation%20of%20background%20and%20non-target%20areas.%20This%20paper%0Aproposes%20a%20localized%20augmentation%20method%20that%20applies%20augmentation%20only%20to%20the%0Atarget%20regions%2C%20avoiding%20any%20influence%20on%20non-target%20areas.%20By%20reducing%0Abackground%20interference%2C%20this%20approach%20enables%20the%20loss%20function%20to%20focus%20more%0Adirectly%20on%20the%20impact%20of%20the%20adversarial%20patch%20on%20the%20detection%20model%2C%20thereby%0Aimproving%20the%20attack%20success%20rate.%20Experiments%20conducted%20on%20the%20HRSC2016%0Adataset%20demonstrate%20that%20the%20proposed%20method%20effectively%20increases%20the%20success%0Arate%20of%20adversarial%20patch%20attacks%20and%20enhances%20their%20transferability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Patch%2520Attack%2520for%2520Ship%2520Detection%2520via%2520Localized%2520Augmentation%26entry.906535625%3DChun%2520Liu%2520and%2520Panpan%2520Ding%2520and%2520Zheng%2520Zheng%2520and%2520Hailong%2520Wang%2520and%2520Bingqian%2520Zhu%2520and%2520Tao%2520Xu%2520and%2520Zhigang%2520Han%2520and%2520Jiayao%2520Wang%26entry.1292438233%3D%2520%2520Current%2520ship%2520detection%2520techniques%2520based%2520on%2520remote%2520sensing%2520imagery%2520primarily%250Arely%2520on%2520the%2520object%2520detection%2520capabilities%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%250AHowever%252C%2520DNNs%2520are%2520vulnerable%2520to%2520adversarial%2520patch%2520attacks%252C%2520which%2520can%2520lead%2520to%250Amisclassification%2520by%2520the%2520detection%2520model%2520or%2520complete%2520evasion%2520of%2520the%2520targets.%250ANumerous%2520studies%2520have%2520demonstrated%2520that%2520data%2520transformation-based%2520methods%2520can%250Aimprove%2520the%2520transferability%2520of%2520adversarial%2520examples.%2520However%252C%2520excessive%250Aaugmentation%2520of%2520image%2520backgrounds%2520or%2520irrelevant%2520regions%2520may%2520introduce%250Aunnecessary%2520interference%252C%2520resulting%2520in%2520false%2520detections%2520of%2520the%2520object%2520detection%250Amodel.%2520These%2520errors%2520are%2520not%2520caused%2520by%2520the%2520adversarial%2520patches%2520themselves%2520but%250Arather%2520by%2520the%2520over-augmentation%2520of%2520background%2520and%2520non-target%2520areas.%2520This%2520paper%250Aproposes%2520a%2520localized%2520augmentation%2520method%2520that%2520applies%2520augmentation%2520only%2520to%2520the%250Atarget%2520regions%252C%2520avoiding%2520any%2520influence%2520on%2520non-target%2520areas.%2520By%2520reducing%250Abackground%2520interference%252C%2520this%2520approach%2520enables%2520the%2520loss%2520function%2520to%2520focus%2520more%250Adirectly%2520on%2520the%2520impact%2520of%2520the%2520adversarial%2520patch%2520on%2520the%2520detection%2520model%252C%2520thereby%250Aimproving%2520the%2520attack%2520success%2520rate.%2520Experiments%2520conducted%2520on%2520the%2520HRSC2016%250Adataset%2520demonstrate%2520that%2520the%2520proposed%2520method%2520effectively%2520increases%2520the%2520success%250Arate%2520of%2520adversarial%2520patch%2520attacks%2520and%2520enhances%2520their%2520transferability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Patch%20Attack%20for%20Ship%20Detection%20via%20Localized%20Augmentation&entry.906535625=Chun%20Liu%20and%20Panpan%20Ding%20and%20Zheng%20Zheng%20and%20Hailong%20Wang%20and%20Bingqian%20Zhu%20and%20Tao%20Xu%20and%20Zhigang%20Han%20and%20Jiayao%20Wang&entry.1292438233=%20%20Current%20ship%20detection%20techniques%20based%20on%20remote%20sensing%20imagery%20primarily%0Arely%20on%20the%20object%20detection%20capabilities%20of%20deep%20neural%20networks%20%28DNNs%29.%0AHowever%2C%20DNNs%20are%20vulnerable%20to%20adversarial%20patch%20attacks%2C%20which%20can%20lead%20to%0Amisclassification%20by%20the%20detection%20model%20or%20complete%20evasion%20of%20the%20targets.%0ANumerous%20studies%20have%20demonstrated%20that%20data%20transformation-based%20methods%20can%0Aimprove%20the%20transferability%20of%20adversarial%20examples.%20However%2C%20excessive%0Aaugmentation%20of%20image%20backgrounds%20or%20irrelevant%20regions%20may%20introduce%0Aunnecessary%20interference%2C%20resulting%20in%20false%20detections%20of%20the%20object%20detection%0Amodel.%20These%20errors%20are%20not%20caused%20by%20the%20adversarial%20patches%20themselves%20but%0Arather%20by%20the%20over-augmentation%20of%20background%20and%20non-target%20areas.%20This%20paper%0Aproposes%20a%20localized%20augmentation%20method%20that%20applies%20augmentation%20only%20to%20the%0Atarget%20regions%2C%20avoiding%20any%20influence%20on%20non-target%20areas.%20By%20reducing%0Abackground%20interference%2C%20this%20approach%20enables%20the%20loss%20function%20to%20focus%20more%0Adirectly%20on%20the%20impact%20of%20the%20adversarial%20patch%20on%20the%20detection%20model%2C%20thereby%0Aimproving%20the%20attack%20success%20rate.%20Experiments%20conducted%20on%20the%20HRSC2016%0Adataset%20demonstrate%20that%20the%20proposed%20method%20effectively%20increases%20the%20success%0Arate%20of%20adversarial%20patch%20attacks%20and%20enhances%20their%20transferability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21472v1&entry.124074799=Read"},
{"title": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches", "author": "Israel Abebe Azime and Deborah D. Kanubala and Tejumade Afonja and Mario Fritz and Isabel Valera and Dietrich Klakow and Philipp Slusallek", "abstract": "  Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making.\n", "link": "http://arxiv.org/abs/2508.21512v1", "date": "2025-08-29", "relevancy": 2.4394, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accept%20or%20Deny%3F%20Evaluating%20LLM%20Fairness%20and%20Performance%20in%20Loan%20Approval%0A%20%20across%20Table-to-Text%20Serialization%20Approaches&body=Title%3A%20Accept%20or%20Deny%3F%20Evaluating%20LLM%20Fairness%20and%20Performance%20in%20Loan%20Approval%0A%20%20across%20Table-to-Text%20Serialization%20Approaches%0AAuthor%3A%20Israel%20Abebe%20Azime%20and%20Deborah%20D.%20Kanubala%20and%20Tejumade%20Afonja%20and%20Mario%20Fritz%20and%20Isabel%20Valera%20and%20Dietrich%20Klakow%20and%20Philipp%20Slusallek%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20in%20high-stakes%0Adecision-making%20tasks%2C%20such%20as%20loan%20approvals.%20While%20their%20applications%20expand%0Aacross%20domains%2C%20LLMs%20struggle%20to%20process%20tabular%20data%2C%20ensuring%20fairness%20and%0Adelivering%20reliable%20predictions.%20In%20this%20work%2C%20we%20assess%20the%20performance%20and%0Afairness%20of%20LLMs%20on%20serialized%20loan%20approval%20datasets%20from%20three%20geographically%0Adistinct%20regions%3A%20Ghana%2C%20Germany%2C%20and%20the%20United%20States.%20Our%20evaluation%20focuses%0Aon%20the%20model%27s%20zero-shot%20and%20in-context%20learning%20%28ICL%29%20capabilities.%20Our%0Aresults%20reveal%20that%20the%20choice%20of%20serialization%20%28Serialization%20refers%20to%20the%0Aprocess%20of%20converting%20tabular%20data%20into%20text%20formats%20suitable%20for%20processing%20by%0ALLMs.%29%20format%20significantly%20affects%20both%20performance%20and%20fairness%20in%20LLMs%2C%20with%0Acertain%20formats%20such%20as%20GReat%20and%20LIFT%20yielding%20higher%20F1%20scores%20but%0Aexacerbating%20fairness%20disparities.%20Notably%2C%20while%20ICL%20improved%20model%0Aperformance%20by%204.9-59.6%25%20relative%20to%20zero-shot%20baselines%2C%20its%20effect%20on%0Afairness%20varied%20considerably%20across%20datasets.%20Our%20work%20underscores%20the%0Aimportance%20of%20effective%20tabular%20data%20representation%20methods%20and%20fairness-aware%0Amodels%20to%20improve%20the%20reliability%20of%20LLMs%20in%20financial%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccept%2520or%2520Deny%253F%2520Evaluating%2520LLM%2520Fairness%2520and%2520Performance%2520in%2520Loan%2520Approval%250A%2520%2520across%2520Table-to-Text%2520Serialization%2520Approaches%26entry.906535625%3DIsrael%2520Abebe%2520Azime%2520and%2520Deborah%2520D.%2520Kanubala%2520and%2520Tejumade%2520Afonja%2520and%2520Mario%2520Fritz%2520and%2520Isabel%2520Valera%2520and%2520Dietrich%2520Klakow%2520and%2520Philipp%2520Slusallek%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520employed%2520in%2520high-stakes%250Adecision-making%2520tasks%252C%2520such%2520as%2520loan%2520approvals.%2520While%2520their%2520applications%2520expand%250Aacross%2520domains%252C%2520LLMs%2520struggle%2520to%2520process%2520tabular%2520data%252C%2520ensuring%2520fairness%2520and%250Adelivering%2520reliable%2520predictions.%2520In%2520this%2520work%252C%2520we%2520assess%2520the%2520performance%2520and%250Afairness%2520of%2520LLMs%2520on%2520serialized%2520loan%2520approval%2520datasets%2520from%2520three%2520geographically%250Adistinct%2520regions%253A%2520Ghana%252C%2520Germany%252C%2520and%2520the%2520United%2520States.%2520Our%2520evaluation%2520focuses%250Aon%2520the%2520model%2527s%2520zero-shot%2520and%2520in-context%2520learning%2520%2528ICL%2529%2520capabilities.%2520Our%250Aresults%2520reveal%2520that%2520the%2520choice%2520of%2520serialization%2520%2528Serialization%2520refers%2520to%2520the%250Aprocess%2520of%2520converting%2520tabular%2520data%2520into%2520text%2520formats%2520suitable%2520for%2520processing%2520by%250ALLMs.%2529%2520format%2520significantly%2520affects%2520both%2520performance%2520and%2520fairness%2520in%2520LLMs%252C%2520with%250Acertain%2520formats%2520such%2520as%2520GReat%2520and%2520LIFT%2520yielding%2520higher%2520F1%2520scores%2520but%250Aexacerbating%2520fairness%2520disparities.%2520Notably%252C%2520while%2520ICL%2520improved%2520model%250Aperformance%2520by%25204.9-59.6%2525%2520relative%2520to%2520zero-shot%2520baselines%252C%2520its%2520effect%2520on%250Afairness%2520varied%2520considerably%2520across%2520datasets.%2520Our%2520work%2520underscores%2520the%250Aimportance%2520of%2520effective%2520tabular%2520data%2520representation%2520methods%2520and%2520fairness-aware%250Amodels%2520to%2520improve%2520the%2520reliability%2520of%2520LLMs%2520in%2520financial%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accept%20or%20Deny%3F%20Evaluating%20LLM%20Fairness%20and%20Performance%20in%20Loan%20Approval%0A%20%20across%20Table-to-Text%20Serialization%20Approaches&entry.906535625=Israel%20Abebe%20Azime%20and%20Deborah%20D.%20Kanubala%20and%20Tejumade%20Afonja%20and%20Mario%20Fritz%20and%20Isabel%20Valera%20and%20Dietrich%20Klakow%20and%20Philipp%20Slusallek&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20in%20high-stakes%0Adecision-making%20tasks%2C%20such%20as%20loan%20approvals.%20While%20their%20applications%20expand%0Aacross%20domains%2C%20LLMs%20struggle%20to%20process%20tabular%20data%2C%20ensuring%20fairness%20and%0Adelivering%20reliable%20predictions.%20In%20this%20work%2C%20we%20assess%20the%20performance%20and%0Afairness%20of%20LLMs%20on%20serialized%20loan%20approval%20datasets%20from%20three%20geographically%0Adistinct%20regions%3A%20Ghana%2C%20Germany%2C%20and%20the%20United%20States.%20Our%20evaluation%20focuses%0Aon%20the%20model%27s%20zero-shot%20and%20in-context%20learning%20%28ICL%29%20capabilities.%20Our%0Aresults%20reveal%20that%20the%20choice%20of%20serialization%20%28Serialization%20refers%20to%20the%0Aprocess%20of%20converting%20tabular%20data%20into%20text%20formats%20suitable%20for%20processing%20by%0ALLMs.%29%20format%20significantly%20affects%20both%20performance%20and%20fairness%20in%20LLMs%2C%20with%0Acertain%20formats%20such%20as%20GReat%20and%20LIFT%20yielding%20higher%20F1%20scores%20but%0Aexacerbating%20fairness%20disparities.%20Notably%2C%20while%20ICL%20improved%20model%0Aperformance%20by%204.9-59.6%25%20relative%20to%20zero-shot%20baselines%2C%20its%20effect%20on%0Afairness%20varied%20considerably%20across%20datasets.%20Our%20work%20underscores%20the%0Aimportance%20of%20effective%20tabular%20data%20representation%20methods%20and%20fairness-aware%0Amodels%20to%20improve%20the%20reliability%20of%20LLMs%20in%20financial%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21512v1&entry.124074799=Read"},
{"title": "Revealing Fine-Grained Values and Opinions in Large Language Models", "author": "Dustin Wright and Arnav Arora and Nadav Borenstein and Srishti Yadav and Serge Belongie and Isabelle Augenstein", "abstract": "  Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.\n", "link": "http://arxiv.org/abs/2406.19238v3", "date": "2025-08-29", "relevancy": 2.4346, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models&body=Title%3A%20Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models%0AAuthor%3A%20Dustin%20Wright%20and%20Arnav%20Arora%20and%20Nadav%20Borenstein%20and%20Srishti%20Yadav%20and%20Serge%20Belongie%20and%20Isabelle%20Augenstein%0AAbstract%3A%20%20%20Uncovering%20latent%20values%20and%20opinions%20embedded%20in%20large%20language%20models%0A%28LLMs%29%20can%20help%20identify%20biases%20and%20mitigate%20potential%20harm.%20Recently%2C%20this%20has%0Abeen%20approached%20by%20prompting%20LLMs%20with%20survey%20questions%20and%20quantifying%20the%0Astances%20in%20the%20outputs%20towards%20morally%20and%20politically%20charged%20statements.%0AHowever%2C%20the%20stances%20generated%20by%20LLMs%20can%20vary%20greatly%20depending%20on%20how%20they%0Aare%20prompted%2C%20and%20there%20are%20many%20ways%20to%20argue%20for%20or%20against%20a%20given%20position.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20this%20by%20analysing%20a%20large%20and%20robust%0Adataset%20of%20156k%20LLM%20responses%20to%20the%2062%20propositions%20of%20the%20Political%20Compass%0ATest%20%28PCT%29%20generated%20by%206%20LLMs%20using%20420%20prompt%20variations.%20We%20perform%0Acoarse-grained%20analysis%20of%20their%20generated%20stances%20and%20fine-grained%20analysis%20of%0Athe%20plain%20text%20justifications%20for%20those%20stances.%20For%20fine-grained%20analysis%2C%20we%0Apropose%20to%20identify%20tropes%20in%20the%20responses%3A%20semantically%20similar%20phrases%20that%0Aare%20recurrent%20and%20consistent%20across%20different%20prompts%2C%20revealing%20natural%0Apatterns%20in%20the%20text%20that%20a%20given%20LLM%20is%20prone%20to%20produce.%20We%20find%20that%0Ademographic%20features%20added%20to%20prompts%20significantly%20affect%20outcomes%20on%20the%20PCT%2C%0Areflecting%20bias%2C%20as%20well%20as%20disparities%20between%20the%20results%20of%20tests%20when%0Aeliciting%20closed-form%20vs.%20open%20domain%20responses.%20Additionally%2C%20patterns%20in%20the%0Aplain%20text%20rationales%20via%20tropes%20show%20that%20similar%20justifications%20are%0Arepeatedly%20generated%20across%20models%20and%20prompts%20even%20with%20disparate%20stances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19238v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Fine-Grained%2520Values%2520and%2520Opinions%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDustin%2520Wright%2520and%2520Arnav%2520Arora%2520and%2520Nadav%2520Borenstein%2520and%2520Srishti%2520Yadav%2520and%2520Serge%2520Belongie%2520and%2520Isabelle%2520Augenstein%26entry.1292438233%3D%2520%2520Uncovering%2520latent%2520values%2520and%2520opinions%2520embedded%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520can%2520help%2520identify%2520biases%2520and%2520mitigate%2520potential%2520harm.%2520Recently%252C%2520this%2520has%250Abeen%2520approached%2520by%2520prompting%2520LLMs%2520with%2520survey%2520questions%2520and%2520quantifying%2520the%250Astances%2520in%2520the%2520outputs%2520towards%2520morally%2520and%2520politically%2520charged%2520statements.%250AHowever%252C%2520the%2520stances%2520generated%2520by%2520LLMs%2520can%2520vary%2520greatly%2520depending%2520on%2520how%2520they%250Aare%2520prompted%252C%2520and%2520there%2520are%2520many%2520ways%2520to%2520argue%2520for%2520or%2520against%2520a%2520given%2520position.%250AIn%2520this%2520work%252C%2520we%2520propose%2520to%2520address%2520this%2520by%2520analysing%2520a%2520large%2520and%2520robust%250Adataset%2520of%2520156k%2520LLM%2520responses%2520to%2520the%252062%2520propositions%2520of%2520the%2520Political%2520Compass%250ATest%2520%2528PCT%2529%2520generated%2520by%25206%2520LLMs%2520using%2520420%2520prompt%2520variations.%2520We%2520perform%250Acoarse-grained%2520analysis%2520of%2520their%2520generated%2520stances%2520and%2520fine-grained%2520analysis%2520of%250Athe%2520plain%2520text%2520justifications%2520for%2520those%2520stances.%2520For%2520fine-grained%2520analysis%252C%2520we%250Apropose%2520to%2520identify%2520tropes%2520in%2520the%2520responses%253A%2520semantically%2520similar%2520phrases%2520that%250Aare%2520recurrent%2520and%2520consistent%2520across%2520different%2520prompts%252C%2520revealing%2520natural%250Apatterns%2520in%2520the%2520text%2520that%2520a%2520given%2520LLM%2520is%2520prone%2520to%2520produce.%2520We%2520find%2520that%250Ademographic%2520features%2520added%2520to%2520prompts%2520significantly%2520affect%2520outcomes%2520on%2520the%2520PCT%252C%250Areflecting%2520bias%252C%2520as%2520well%2520as%2520disparities%2520between%2520the%2520results%2520of%2520tests%2520when%250Aeliciting%2520closed-form%2520vs.%2520open%2520domain%2520responses.%2520Additionally%252C%2520patterns%2520in%2520the%250Aplain%2520text%2520rationales%2520via%2520tropes%2520show%2520that%2520similar%2520justifications%2520are%250Arepeatedly%2520generated%2520across%2520models%2520and%2520prompts%2520even%2520with%2520disparate%2520stances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19238v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models&entry.906535625=Dustin%20Wright%20and%20Arnav%20Arora%20and%20Nadav%20Borenstein%20and%20Srishti%20Yadav%20and%20Serge%20Belongie%20and%20Isabelle%20Augenstein&entry.1292438233=%20%20Uncovering%20latent%20values%20and%20opinions%20embedded%20in%20large%20language%20models%0A%28LLMs%29%20can%20help%20identify%20biases%20and%20mitigate%20potential%20harm.%20Recently%2C%20this%20has%0Abeen%20approached%20by%20prompting%20LLMs%20with%20survey%20questions%20and%20quantifying%20the%0Astances%20in%20the%20outputs%20towards%20morally%20and%20politically%20charged%20statements.%0AHowever%2C%20the%20stances%20generated%20by%20LLMs%20can%20vary%20greatly%20depending%20on%20how%20they%0Aare%20prompted%2C%20and%20there%20are%20many%20ways%20to%20argue%20for%20or%20against%20a%20given%20position.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20this%20by%20analysing%20a%20large%20and%20robust%0Adataset%20of%20156k%20LLM%20responses%20to%20the%2062%20propositions%20of%20the%20Political%20Compass%0ATest%20%28PCT%29%20generated%20by%206%20LLMs%20using%20420%20prompt%20variations.%20We%20perform%0Acoarse-grained%20analysis%20of%20their%20generated%20stances%20and%20fine-grained%20analysis%20of%0Athe%20plain%20text%20justifications%20for%20those%20stances.%20For%20fine-grained%20analysis%2C%20we%0Apropose%20to%20identify%20tropes%20in%20the%20responses%3A%20semantically%20similar%20phrases%20that%0Aare%20recurrent%20and%20consistent%20across%20different%20prompts%2C%20revealing%20natural%0Apatterns%20in%20the%20text%20that%20a%20given%20LLM%20is%20prone%20to%20produce.%20We%20find%20that%0Ademographic%20features%20added%20to%20prompts%20significantly%20affect%20outcomes%20on%20the%20PCT%2C%0Areflecting%20bias%2C%20as%20well%20as%20disparities%20between%20the%20results%20of%20tests%20when%0Aeliciting%20closed-form%20vs.%20open%20domain%20responses.%20Additionally%2C%20patterns%20in%20the%0Aplain%20text%20rationales%20via%20tropes%20show%20that%20similar%20justifications%20are%0Arepeatedly%20generated%20across%20models%20and%20prompts%20even%20with%20disparate%20stances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19238v3&entry.124074799=Read"},
{"title": "C-Flat++: Towards a More Efficient and Powerful Framework for Continual\n  Learning", "author": "Wei Li and Hangjie Yuan and Zixiang Zhao and Yifan Zhu and Aojun Lu and Tao Feng and Yanan Sun", "abstract": "  Balancing sensitivity to new tasks and stability for retaining past knowledge\nis crucial in continual learning (CL). Recently, sharpness-aware minimization\nhas proven effective in transfer learning and has also been adopted in\ncontinual learning (CL) to improve memory retention and learning efficiency.\nHowever, relying on zeroth-order sharpness alone may favor sharper minima over\nflatter ones in certain settings, leading to less robust and potentially\nsuboptimal solutions. In this paper, we propose \\textbf{C}ontinual\n\\textbf{Flat}ness (\\textbf{C-Flat}), a method that promotes flatter loss\nlandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling\neasy integration with minimal modifications to the code pipeline. Besides, we\npresent a general framework that integrates C-Flat into all major CL paradigms\nand conduct comprehensive comparisons with loss-minima optimizers and\nflat-minima-based CL methods. Our results show that C-Flat consistently\nimproves performance across a wide range of settings. In addition, we introduce\nC-Flat++, an efficient yet effective framework that leverages selective\nflatness-driven promotion, significantly reducing the update cost required by\nC-Flat. Extensive experiments across multiple CL methods, datasets, and\nscenarios demonstrate the effectiveness and efficiency of our proposed\napproaches. Code is available at https://github.com/WanNaa/C-Flat.\n", "link": "http://arxiv.org/abs/2508.18860v2", "date": "2025-08-29", "relevancy": 2.4198, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4793}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-Flat%2B%2B%3A%20Towards%20a%20More%20Efficient%20and%20Powerful%20Framework%20for%20Continual%0A%20%20Learning&body=Title%3A%20C-Flat%2B%2B%3A%20Towards%20a%20More%20Efficient%20and%20Powerful%20Framework%20for%20Continual%0A%20%20Learning%0AAuthor%3A%20Wei%20Li%20and%20Hangjie%20Yuan%20and%20Zixiang%20Zhao%20and%20Yifan%20Zhu%20and%20Aojun%20Lu%20and%20Tao%20Feng%20and%20Yanan%20Sun%0AAbstract%3A%20%20%20Balancing%20sensitivity%20to%20new%20tasks%20and%20stability%20for%20retaining%20past%20knowledge%0Ais%20crucial%20in%20continual%20learning%20%28CL%29.%20Recently%2C%20sharpness-aware%20minimization%0Ahas%20proven%20effective%20in%20transfer%20learning%20and%20has%20also%20been%20adopted%20in%0Acontinual%20learning%20%28CL%29%20to%20improve%20memory%20retention%20and%20learning%20efficiency.%0AHowever%2C%20relying%20on%20zeroth-order%20sharpness%20alone%20may%20favor%20sharper%20minima%20over%0Aflatter%20ones%20in%20certain%20settings%2C%20leading%20to%20less%20robust%20and%20potentially%0Asuboptimal%20solutions.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BC%7Dontinual%0A%5Ctextbf%7BFlat%7Dness%20%28%5Ctextbf%7BC-Flat%7D%29%2C%20a%20method%20that%20promotes%20flatter%20loss%0Alandscapes%20tailored%20for%20CL.%20C-Flat%20offers%20plug-and-play%20compatibility%2C%20enabling%0Aeasy%20integration%20with%20minimal%20modifications%20to%20the%20code%20pipeline.%20Besides%2C%20we%0Apresent%20a%20general%20framework%20that%20integrates%20C-Flat%20into%20all%20major%20CL%20paradigms%0Aand%20conduct%20comprehensive%20comparisons%20with%20loss-minima%20optimizers%20and%0Aflat-minima-based%20CL%20methods.%20Our%20results%20show%20that%20C-Flat%20consistently%0Aimproves%20performance%20across%20a%20wide%20range%20of%20settings.%20In%20addition%2C%20we%20introduce%0AC-Flat%2B%2B%2C%20an%20efficient%20yet%20effective%20framework%20that%20leverages%20selective%0Aflatness-driven%20promotion%2C%20significantly%20reducing%20the%20update%20cost%20required%20by%0AC-Flat.%20Extensive%20experiments%20across%20multiple%20CL%20methods%2C%20datasets%2C%20and%0Ascenarios%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Aapproaches.%20Code%20is%20available%20at%20https%3A//github.com/WanNaa/C-Flat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-Flat%252B%252B%253A%2520Towards%2520a%2520More%2520Efficient%2520and%2520Powerful%2520Framework%2520for%2520Continual%250A%2520%2520Learning%26entry.906535625%3DWei%2520Li%2520and%2520Hangjie%2520Yuan%2520and%2520Zixiang%2520Zhao%2520and%2520Yifan%2520Zhu%2520and%2520Aojun%2520Lu%2520and%2520Tao%2520Feng%2520and%2520Yanan%2520Sun%26entry.1292438233%3D%2520%2520Balancing%2520sensitivity%2520to%2520new%2520tasks%2520and%2520stability%2520for%2520retaining%2520past%2520knowledge%250Ais%2520crucial%2520in%2520continual%2520learning%2520%2528CL%2529.%2520Recently%252C%2520sharpness-aware%2520minimization%250Ahas%2520proven%2520effective%2520in%2520transfer%2520learning%2520and%2520has%2520also%2520been%2520adopted%2520in%250Acontinual%2520learning%2520%2528CL%2529%2520to%2520improve%2520memory%2520retention%2520and%2520learning%2520efficiency.%250AHowever%252C%2520relying%2520on%2520zeroth-order%2520sharpness%2520alone%2520may%2520favor%2520sharper%2520minima%2520over%250Aflatter%2520ones%2520in%2520certain%2520settings%252C%2520leading%2520to%2520less%2520robust%2520and%2520potentially%250Asuboptimal%2520solutions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BC%257Dontinual%250A%255Ctextbf%257BFlat%257Dness%2520%2528%255Ctextbf%257BC-Flat%257D%2529%252C%2520a%2520method%2520that%2520promotes%2520flatter%2520loss%250Alandscapes%2520tailored%2520for%2520CL.%2520C-Flat%2520offers%2520plug-and-play%2520compatibility%252C%2520enabling%250Aeasy%2520integration%2520with%2520minimal%2520modifications%2520to%2520the%2520code%2520pipeline.%2520Besides%252C%2520we%250Apresent%2520a%2520general%2520framework%2520that%2520integrates%2520C-Flat%2520into%2520all%2520major%2520CL%2520paradigms%250Aand%2520conduct%2520comprehensive%2520comparisons%2520with%2520loss-minima%2520optimizers%2520and%250Aflat-minima-based%2520CL%2520methods.%2520Our%2520results%2520show%2520that%2520C-Flat%2520consistently%250Aimproves%2520performance%2520across%2520a%2520wide%2520range%2520of%2520settings.%2520In%2520addition%252C%2520we%2520introduce%250AC-Flat%252B%252B%252C%2520an%2520efficient%2520yet%2520effective%2520framework%2520that%2520leverages%2520selective%250Aflatness-driven%2520promotion%252C%2520significantly%2520reducing%2520the%2520update%2520cost%2520required%2520by%250AC-Flat.%2520Extensive%2520experiments%2520across%2520multiple%2520CL%2520methods%252C%2520datasets%252C%2520and%250Ascenarios%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520proposed%250Aapproaches.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/WanNaa/C-Flat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-Flat%2B%2B%3A%20Towards%20a%20More%20Efficient%20and%20Powerful%20Framework%20for%20Continual%0A%20%20Learning&entry.906535625=Wei%20Li%20and%20Hangjie%20Yuan%20and%20Zixiang%20Zhao%20and%20Yifan%20Zhu%20and%20Aojun%20Lu%20and%20Tao%20Feng%20and%20Yanan%20Sun&entry.1292438233=%20%20Balancing%20sensitivity%20to%20new%20tasks%20and%20stability%20for%20retaining%20past%20knowledge%0Ais%20crucial%20in%20continual%20learning%20%28CL%29.%20Recently%2C%20sharpness-aware%20minimization%0Ahas%20proven%20effective%20in%20transfer%20learning%20and%20has%20also%20been%20adopted%20in%0Acontinual%20learning%20%28CL%29%20to%20improve%20memory%20retention%20and%20learning%20efficiency.%0AHowever%2C%20relying%20on%20zeroth-order%20sharpness%20alone%20may%20favor%20sharper%20minima%20over%0Aflatter%20ones%20in%20certain%20settings%2C%20leading%20to%20less%20robust%20and%20potentially%0Asuboptimal%20solutions.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BC%7Dontinual%0A%5Ctextbf%7BFlat%7Dness%20%28%5Ctextbf%7BC-Flat%7D%29%2C%20a%20method%20that%20promotes%20flatter%20loss%0Alandscapes%20tailored%20for%20CL.%20C-Flat%20offers%20plug-and-play%20compatibility%2C%20enabling%0Aeasy%20integration%20with%20minimal%20modifications%20to%20the%20code%20pipeline.%20Besides%2C%20we%0Apresent%20a%20general%20framework%20that%20integrates%20C-Flat%20into%20all%20major%20CL%20paradigms%0Aand%20conduct%20comprehensive%20comparisons%20with%20loss-minima%20optimizers%20and%0Aflat-minima-based%20CL%20methods.%20Our%20results%20show%20that%20C-Flat%20consistently%0Aimproves%20performance%20across%20a%20wide%20range%20of%20settings.%20In%20addition%2C%20we%20introduce%0AC-Flat%2B%2B%2C%20an%20efficient%20yet%20effective%20framework%20that%20leverages%20selective%0Aflatness-driven%20promotion%2C%20significantly%20reducing%20the%20update%20cost%20required%20by%0AC-Flat.%20Extensive%20experiments%20across%20multiple%20CL%20methods%2C%20datasets%2C%20and%0Ascenarios%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20proposed%0Aapproaches.%20Code%20is%20available%20at%20https%3A//github.com/WanNaa/C-Flat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18860v2&entry.124074799=Read"},
{"title": "Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex\n  Optimization", "author": "Ekaterina Borodich and Dmitry Kovalev", "abstract": "  In this paper, we focus on the problem of minimizing a continuously\ndifferentiable convex objective function, $\\min_x f(x)$. Recently, Malitsky\n(2020); Alacaoglu et al.(2023) developed an adaptive first-order method, GRAAL.\nThis algorithm computes stepsizes by estimating the local curvature of the\nobjective function without any line search procedures or hyperparameter tuning,\nand attains the standard iteration complexity $\\mathcal{O}(L\\lVert\nx_0-x^*\\rVert^2/\\epsilon)$ of fixed-stepsize gradient descent for $L$-smooth\nfunctions. However, a natural question arises: is it possible to accelerate the\nconvergence of GRAAL to match the optimal complexity $\\mathcal{O}(\\sqrt{L\\lVert\nx_0-x^*\\rVert^2/\\epsilon})$ of the accelerated gradient descent of Nesterov\n(1983)? Although some attempts have been made by Li and Lan (2025); Suh and Ma\n(2025), the ability of existing accelerated algorithms to adapt to the local\ncurvature of the objective function is highly limited. We resolve this issue\nand develop GRAAL with Nesterov acceleration, which can adapt its stepsize to\nthe local curvature at a geometric, or linear, rate just like non-accelerated\nGRAAL. We demonstrate the adaptive capabilities of our algorithm by proving\nthat it achieves near-optimal iteration complexities for $L$-smooth functions,\nas well as under a more general $(L_0,L_1)$-smoothness assumption (Zhang et\nal., 2019).\n", "link": "http://arxiv.org/abs/2507.09823v2", "date": "2025-08-29", "relevancy": 2.3699, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4924}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4719}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nesterov%20Finds%20GRAAL%3A%20Optimal%20and%20Adaptive%20Gradient%20Method%20for%20Convex%0A%20%20Optimization&body=Title%3A%20Nesterov%20Finds%20GRAAL%3A%20Optimal%20and%20Adaptive%20Gradient%20Method%20for%20Convex%0A%20%20Optimization%0AAuthor%3A%20Ekaterina%20Borodich%20and%20Dmitry%20Kovalev%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20minimizing%20a%20continuously%0Adifferentiable%20convex%20objective%20function%2C%20%24%5Cmin_x%20f%28x%29%24.%20Recently%2C%20Malitsky%0A%282020%29%3B%20Alacaoglu%20et%20al.%282023%29%20developed%20an%20adaptive%20first-order%20method%2C%20GRAAL.%0AThis%20algorithm%20computes%20stepsizes%20by%20estimating%20the%20local%20curvature%20of%20the%0Aobjective%20function%20without%20any%20line%20search%20procedures%20or%20hyperparameter%20tuning%2C%0Aand%20attains%20the%20standard%20iteration%20complexity%20%24%5Cmathcal%7BO%7D%28L%5ClVert%0Ax_0-x%5E%2A%5CrVert%5E2/%5Cepsilon%29%24%20of%20fixed-stepsize%20gradient%20descent%20for%20%24L%24-smooth%0Afunctions.%20However%2C%20a%20natural%20question%20arises%3A%20is%20it%20possible%20to%20accelerate%20the%0Aconvergence%20of%20GRAAL%20to%20match%20the%20optimal%20complexity%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BL%5ClVert%0Ax_0-x%5E%2A%5CrVert%5E2/%5Cepsilon%7D%29%24%20of%20the%20accelerated%20gradient%20descent%20of%20Nesterov%0A%281983%29%3F%20Although%20some%20attempts%20have%20been%20made%20by%20Li%20and%20Lan%20%282025%29%3B%20Suh%20and%20Ma%0A%282025%29%2C%20the%20ability%20of%20existing%20accelerated%20algorithms%20to%20adapt%20to%20the%20local%0Acurvature%20of%20the%20objective%20function%20is%20highly%20limited.%20We%20resolve%20this%20issue%0Aand%20develop%20GRAAL%20with%20Nesterov%20acceleration%2C%20which%20can%20adapt%20its%20stepsize%20to%0Athe%20local%20curvature%20at%20a%20geometric%2C%20or%20linear%2C%20rate%20just%20like%20non-accelerated%0AGRAAL.%20We%20demonstrate%20the%20adaptive%20capabilities%20of%20our%20algorithm%20by%20proving%0Athat%20it%20achieves%20near-optimal%20iteration%20complexities%20for%20%24L%24-smooth%20functions%2C%0Aas%20well%20as%20under%20a%20more%20general%20%24%28L_0%2CL_1%29%24-smoothness%20assumption%20%28Zhang%20et%0Aal.%2C%202019%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNesterov%2520Finds%2520GRAAL%253A%2520Optimal%2520and%2520Adaptive%2520Gradient%2520Method%2520for%2520Convex%250A%2520%2520Optimization%26entry.906535625%3DEkaterina%2520Borodich%2520and%2520Dmitry%2520Kovalev%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520problem%2520of%2520minimizing%2520a%2520continuously%250Adifferentiable%2520convex%2520objective%2520function%252C%2520%2524%255Cmin_x%2520f%2528x%2529%2524.%2520Recently%252C%2520Malitsky%250A%25282020%2529%253B%2520Alacaoglu%2520et%2520al.%25282023%2529%2520developed%2520an%2520adaptive%2520first-order%2520method%252C%2520GRAAL.%250AThis%2520algorithm%2520computes%2520stepsizes%2520by%2520estimating%2520the%2520local%2520curvature%2520of%2520the%250Aobjective%2520function%2520without%2520any%2520line%2520search%2520procedures%2520or%2520hyperparameter%2520tuning%252C%250Aand%2520attains%2520the%2520standard%2520iteration%2520complexity%2520%2524%255Cmathcal%257BO%257D%2528L%255ClVert%250Ax_0-x%255E%252A%255CrVert%255E2/%255Cepsilon%2529%2524%2520of%2520fixed-stepsize%2520gradient%2520descent%2520for%2520%2524L%2524-smooth%250Afunctions.%2520However%252C%2520a%2520natural%2520question%2520arises%253A%2520is%2520it%2520possible%2520to%2520accelerate%2520the%250Aconvergence%2520of%2520GRAAL%2520to%2520match%2520the%2520optimal%2520complexity%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257BL%255ClVert%250Ax_0-x%255E%252A%255CrVert%255E2/%255Cepsilon%257D%2529%2524%2520of%2520the%2520accelerated%2520gradient%2520descent%2520of%2520Nesterov%250A%25281983%2529%253F%2520Although%2520some%2520attempts%2520have%2520been%2520made%2520by%2520Li%2520and%2520Lan%2520%25282025%2529%253B%2520Suh%2520and%2520Ma%250A%25282025%2529%252C%2520the%2520ability%2520of%2520existing%2520accelerated%2520algorithms%2520to%2520adapt%2520to%2520the%2520local%250Acurvature%2520of%2520the%2520objective%2520function%2520is%2520highly%2520limited.%2520We%2520resolve%2520this%2520issue%250Aand%2520develop%2520GRAAL%2520with%2520Nesterov%2520acceleration%252C%2520which%2520can%2520adapt%2520its%2520stepsize%2520to%250Athe%2520local%2520curvature%2520at%2520a%2520geometric%252C%2520or%2520linear%252C%2520rate%2520just%2520like%2520non-accelerated%250AGRAAL.%2520We%2520demonstrate%2520the%2520adaptive%2520capabilities%2520of%2520our%2520algorithm%2520by%2520proving%250Athat%2520it%2520achieves%2520near-optimal%2520iteration%2520complexities%2520for%2520%2524L%2524-smooth%2520functions%252C%250Aas%2520well%2520as%2520under%2520a%2520more%2520general%2520%2524%2528L_0%252CL_1%2529%2524-smoothness%2520assumption%2520%2528Zhang%2520et%250Aal.%252C%25202019%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nesterov%20Finds%20GRAAL%3A%20Optimal%20and%20Adaptive%20Gradient%20Method%20for%20Convex%0A%20%20Optimization&entry.906535625=Ekaterina%20Borodich%20and%20Dmitry%20Kovalev&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20problem%20of%20minimizing%20a%20continuously%0Adifferentiable%20convex%20objective%20function%2C%20%24%5Cmin_x%20f%28x%29%24.%20Recently%2C%20Malitsky%0A%282020%29%3B%20Alacaoglu%20et%20al.%282023%29%20developed%20an%20adaptive%20first-order%20method%2C%20GRAAL.%0AThis%20algorithm%20computes%20stepsizes%20by%20estimating%20the%20local%20curvature%20of%20the%0Aobjective%20function%20without%20any%20line%20search%20procedures%20or%20hyperparameter%20tuning%2C%0Aand%20attains%20the%20standard%20iteration%20complexity%20%24%5Cmathcal%7BO%7D%28L%5ClVert%0Ax_0-x%5E%2A%5CrVert%5E2/%5Cepsilon%29%24%20of%20fixed-stepsize%20gradient%20descent%20for%20%24L%24-smooth%0Afunctions.%20However%2C%20a%20natural%20question%20arises%3A%20is%20it%20possible%20to%20accelerate%20the%0Aconvergence%20of%20GRAAL%20to%20match%20the%20optimal%20complexity%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BL%5ClVert%0Ax_0-x%5E%2A%5CrVert%5E2/%5Cepsilon%7D%29%24%20of%20the%20accelerated%20gradient%20descent%20of%20Nesterov%0A%281983%29%3F%20Although%20some%20attempts%20have%20been%20made%20by%20Li%20and%20Lan%20%282025%29%3B%20Suh%20and%20Ma%0A%282025%29%2C%20the%20ability%20of%20existing%20accelerated%20algorithms%20to%20adapt%20to%20the%20local%0Acurvature%20of%20the%20objective%20function%20is%20highly%20limited.%20We%20resolve%20this%20issue%0Aand%20develop%20GRAAL%20with%20Nesterov%20acceleration%2C%20which%20can%20adapt%20its%20stepsize%20to%0Athe%20local%20curvature%20at%20a%20geometric%2C%20or%20linear%2C%20rate%20just%20like%20non-accelerated%0AGRAAL.%20We%20demonstrate%20the%20adaptive%20capabilities%20of%20our%20algorithm%20by%20proving%0Athat%20it%20achieves%20near-optimal%20iteration%20complexities%20for%20%24L%24-smooth%20functions%2C%0Aas%20well%20as%20under%20a%20more%20general%20%24%28L_0%2CL_1%29%24-smoothness%20assumption%20%28Zhang%20et%0Aal.%2C%202019%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09823v2&entry.124074799=Read"},
{"title": "Adaptive generative moment matching networks for improved learning of\n  dependence structures", "author": "Marius Hofert and Gan Yao", "abstract": "  An adaptive bandwidth selection procedure for the mixture kernel in the\nmaximum mean discrepancy (MMD) for fitting generative moment matching networks\n(GMMNs) is introduced, and its ability to improve the learning of copula random\nnumber generators is demonstrated. Based on the relative error of the training\nloss, the number of kernels is increased during training; additionally, the\nrelative error of the validation loss is used as an early stopping criterion.\nWhile training time of such adaptively trained GMMNs (AGMMNs) is similar to\nthat of GMMNs, training performance is increased significantly in comparison to\nGMMNs, which is assessed and shown based on validation MMD trajectories,\nsamples and validation MMD values. Superiority of AGMMNs over GMMNs, as well as\ntypical parametric copula models, is demonstrated in terms of three\napplications. First, convergence rates of quasi-random versus pseudo-random\nsamples from high-dimensional copulas are investigated for three functionals of\ninterest and in dimensions as large as 100 for the first time. Second,\nreplicated validation MMDs, as well as Monte Carlo and quasi-Monte Carlo\napplications based on the expected payoff of a basked call option and the risk\nmeasure expected shortfall as functionals are used to demonstrate the improved\ntraining of AGMMNs over GMMNs for a copula model fitted to the standardized\nresiduals of the 50 constituents of the S&P 500 index after deGARCHing. Last,\nboth the latter dataset and 50 constituents of the FTSE~100 are used to\ndemonstrate that the improved training of AGMMNs over GMMNs and in comparison\nto the fitting of classical parametric copula models indeed also translates to\nan improved model prediction.\n", "link": "http://arxiv.org/abs/2508.21531v1", "date": "2025-08-29", "relevancy": 2.3586, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4764}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4701}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20generative%20moment%20matching%20networks%20for%20improved%20learning%20of%0A%20%20dependence%20structures&body=Title%3A%20Adaptive%20generative%20moment%20matching%20networks%20for%20improved%20learning%20of%0A%20%20dependence%20structures%0AAuthor%3A%20Marius%20Hofert%20and%20Gan%20Yao%0AAbstract%3A%20%20%20An%20adaptive%20bandwidth%20selection%20procedure%20for%20the%20mixture%20kernel%20in%20the%0Amaximum%20mean%20discrepancy%20%28MMD%29%20for%20fitting%20generative%20moment%20matching%20networks%0A%28GMMNs%29%20is%20introduced%2C%20and%20its%20ability%20to%20improve%20the%20learning%20of%20copula%20random%0Anumber%20generators%20is%20demonstrated.%20Based%20on%20the%20relative%20error%20of%20the%20training%0Aloss%2C%20the%20number%20of%20kernels%20is%20increased%20during%20training%3B%20additionally%2C%20the%0Arelative%20error%20of%20the%20validation%20loss%20is%20used%20as%20an%20early%20stopping%20criterion.%0AWhile%20training%20time%20of%20such%20adaptively%20trained%20GMMNs%20%28AGMMNs%29%20is%20similar%20to%0Athat%20of%20GMMNs%2C%20training%20performance%20is%20increased%20significantly%20in%20comparison%20to%0AGMMNs%2C%20which%20is%20assessed%20and%20shown%20based%20on%20validation%20MMD%20trajectories%2C%0Asamples%20and%20validation%20MMD%20values.%20Superiority%20of%20AGMMNs%20over%20GMMNs%2C%20as%20well%20as%0Atypical%20parametric%20copula%20models%2C%20is%20demonstrated%20in%20terms%20of%20three%0Aapplications.%20First%2C%20convergence%20rates%20of%20quasi-random%20versus%20pseudo-random%0Asamples%20from%20high-dimensional%20copulas%20are%20investigated%20for%20three%20functionals%20of%0Ainterest%20and%20in%20dimensions%20as%20large%20as%20100%20for%20the%20first%20time.%20Second%2C%0Areplicated%20validation%20MMDs%2C%20as%20well%20as%20Monte%20Carlo%20and%20quasi-Monte%20Carlo%0Aapplications%20based%20on%20the%20expected%20payoff%20of%20a%20basked%20call%20option%20and%20the%20risk%0Ameasure%20expected%20shortfall%20as%20functionals%20are%20used%20to%20demonstrate%20the%20improved%0Atraining%20of%20AGMMNs%20over%20GMMNs%20for%20a%20copula%20model%20fitted%20to%20the%20standardized%0Aresiduals%20of%20the%2050%20constituents%20of%20the%20S%26P%20500%20index%20after%20deGARCHing.%20Last%2C%0Aboth%20the%20latter%20dataset%20and%2050%20constituents%20of%20the%20FTSE~100%20are%20used%20to%0Ademonstrate%20that%20the%20improved%20training%20of%20AGMMNs%20over%20GMMNs%20and%20in%20comparison%0Ato%20the%20fitting%20of%20classical%20parametric%20copula%20models%20indeed%20also%20translates%20to%0Aan%20improved%20model%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520generative%2520moment%2520matching%2520networks%2520for%2520improved%2520learning%2520of%250A%2520%2520dependence%2520structures%26entry.906535625%3DMarius%2520Hofert%2520and%2520Gan%2520Yao%26entry.1292438233%3D%2520%2520An%2520adaptive%2520bandwidth%2520selection%2520procedure%2520for%2520the%2520mixture%2520kernel%2520in%2520the%250Amaximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520for%2520fitting%2520generative%2520moment%2520matching%2520networks%250A%2528GMMNs%2529%2520is%2520introduced%252C%2520and%2520its%2520ability%2520to%2520improve%2520the%2520learning%2520of%2520copula%2520random%250Anumber%2520generators%2520is%2520demonstrated.%2520Based%2520on%2520the%2520relative%2520error%2520of%2520the%2520training%250Aloss%252C%2520the%2520number%2520of%2520kernels%2520is%2520increased%2520during%2520training%253B%2520additionally%252C%2520the%250Arelative%2520error%2520of%2520the%2520validation%2520loss%2520is%2520used%2520as%2520an%2520early%2520stopping%2520criterion.%250AWhile%2520training%2520time%2520of%2520such%2520adaptively%2520trained%2520GMMNs%2520%2528AGMMNs%2529%2520is%2520similar%2520to%250Athat%2520of%2520GMMNs%252C%2520training%2520performance%2520is%2520increased%2520significantly%2520in%2520comparison%2520to%250AGMMNs%252C%2520which%2520is%2520assessed%2520and%2520shown%2520based%2520on%2520validation%2520MMD%2520trajectories%252C%250Asamples%2520and%2520validation%2520MMD%2520values.%2520Superiority%2520of%2520AGMMNs%2520over%2520GMMNs%252C%2520as%2520well%2520as%250Atypical%2520parametric%2520copula%2520models%252C%2520is%2520demonstrated%2520in%2520terms%2520of%2520three%250Aapplications.%2520First%252C%2520convergence%2520rates%2520of%2520quasi-random%2520versus%2520pseudo-random%250Asamples%2520from%2520high-dimensional%2520copulas%2520are%2520investigated%2520for%2520three%2520functionals%2520of%250Ainterest%2520and%2520in%2520dimensions%2520as%2520large%2520as%2520100%2520for%2520the%2520first%2520time.%2520Second%252C%250Areplicated%2520validation%2520MMDs%252C%2520as%2520well%2520as%2520Monte%2520Carlo%2520and%2520quasi-Monte%2520Carlo%250Aapplications%2520based%2520on%2520the%2520expected%2520payoff%2520of%2520a%2520basked%2520call%2520option%2520and%2520the%2520risk%250Ameasure%2520expected%2520shortfall%2520as%2520functionals%2520are%2520used%2520to%2520demonstrate%2520the%2520improved%250Atraining%2520of%2520AGMMNs%2520over%2520GMMNs%2520for%2520a%2520copula%2520model%2520fitted%2520to%2520the%2520standardized%250Aresiduals%2520of%2520the%252050%2520constituents%2520of%2520the%2520S%2526P%2520500%2520index%2520after%2520deGARCHing.%2520Last%252C%250Aboth%2520the%2520latter%2520dataset%2520and%252050%2520constituents%2520of%2520the%2520FTSE~100%2520are%2520used%2520to%250Ademonstrate%2520that%2520the%2520improved%2520training%2520of%2520AGMMNs%2520over%2520GMMNs%2520and%2520in%2520comparison%250Ato%2520the%2520fitting%2520of%2520classical%2520parametric%2520copula%2520models%2520indeed%2520also%2520translates%2520to%250Aan%2520improved%2520model%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20generative%20moment%20matching%20networks%20for%20improved%20learning%20of%0A%20%20dependence%20structures&entry.906535625=Marius%20Hofert%20and%20Gan%20Yao&entry.1292438233=%20%20An%20adaptive%20bandwidth%20selection%20procedure%20for%20the%20mixture%20kernel%20in%20the%0Amaximum%20mean%20discrepancy%20%28MMD%29%20for%20fitting%20generative%20moment%20matching%20networks%0A%28GMMNs%29%20is%20introduced%2C%20and%20its%20ability%20to%20improve%20the%20learning%20of%20copula%20random%0Anumber%20generators%20is%20demonstrated.%20Based%20on%20the%20relative%20error%20of%20the%20training%0Aloss%2C%20the%20number%20of%20kernels%20is%20increased%20during%20training%3B%20additionally%2C%20the%0Arelative%20error%20of%20the%20validation%20loss%20is%20used%20as%20an%20early%20stopping%20criterion.%0AWhile%20training%20time%20of%20such%20adaptively%20trained%20GMMNs%20%28AGMMNs%29%20is%20similar%20to%0Athat%20of%20GMMNs%2C%20training%20performance%20is%20increased%20significantly%20in%20comparison%20to%0AGMMNs%2C%20which%20is%20assessed%20and%20shown%20based%20on%20validation%20MMD%20trajectories%2C%0Asamples%20and%20validation%20MMD%20values.%20Superiority%20of%20AGMMNs%20over%20GMMNs%2C%20as%20well%20as%0Atypical%20parametric%20copula%20models%2C%20is%20demonstrated%20in%20terms%20of%20three%0Aapplications.%20First%2C%20convergence%20rates%20of%20quasi-random%20versus%20pseudo-random%0Asamples%20from%20high-dimensional%20copulas%20are%20investigated%20for%20three%20functionals%20of%0Ainterest%20and%20in%20dimensions%20as%20large%20as%20100%20for%20the%20first%20time.%20Second%2C%0Areplicated%20validation%20MMDs%2C%20as%20well%20as%20Monte%20Carlo%20and%20quasi-Monte%20Carlo%0Aapplications%20based%20on%20the%20expected%20payoff%20of%20a%20basked%20call%20option%20and%20the%20risk%0Ameasure%20expected%20shortfall%20as%20functionals%20are%20used%20to%20demonstrate%20the%20improved%0Atraining%20of%20AGMMNs%20over%20GMMNs%20for%20a%20copula%20model%20fitted%20to%20the%20standardized%0Aresiduals%20of%20the%2050%20constituents%20of%20the%20S%26P%20500%20index%20after%20deGARCHing.%20Last%2C%0Aboth%20the%20latter%20dataset%20and%2050%20constituents%20of%20the%20FTSE~100%20are%20used%20to%0Ademonstrate%20that%20the%20improved%20training%20of%20AGMMNs%20over%20GMMNs%20and%20in%20comparison%0Ato%20the%20fitting%20of%20classical%20parametric%20copula%20models%20indeed%20also%20translates%20to%0Aan%20improved%20model%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21531v1&entry.124074799=Read"},
{"title": "From Drone Imagery to Livability Mapping: AI-powered Environment\n  Perception in Rural China", "author": "Weihuan Deng and Yaofu Huang and Luan Chen and Xun Li and Yao Yao", "abstract": "  With the deepening of poverty alleviation and rural revitalization\nstrategies, improving the rural living environment and enhancing the quality of\nlife have become key priorities. Rural livability is a key indicator for\nmeasuring the effectiveness of these efforts. Current measurement approaches\nface significant limitations, as questionnaire-based methods are difficult to\nscale, while urban-oriented visual perception methods are poorly suited for\nrural contexts. In this paper, a rural-specific livability assessment framework\nwas proposed based on drone imagery and multimodal large language models\n(MLLMs). To comprehensively assess village livability, this study first used a\ntop-down approach to collect large-scale drone imagery of 1,766 villages in 146\ncounties across China. In terms of the model framework, an efficient image\ncomparison mechanism was developed, incorporating binary search interpolation\nto determine effective image pairs while reducing comparison iterations.\nBuilding on expert knowledge, a chain-of-thought prompting suitable for\nnationwide rural livability measurement was constructed, considering both\nliving quality and ecological habitability dimensions. This approach enhanced\nthe rationality and reliability of the livability assessment. Finally, this\nstudy characterized the spatial heterogeneity of rural livability across China\nand thoroughly analyzed its influential factors. The results show that: (1) The\nrural livability in China demonstrates a dual-core-periphery spatial pattern,\nradiating outward from Sichuan and Zhejiang provinces with declining gradients;\n(2) Among various influential factors, government fiscal expenditure emerged as\nthe core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit\nenhancement in livability. The findings provide valuable insights for rural\nconstruction policy-making.\n", "link": "http://arxiv.org/abs/2508.21738v1", "date": "2025-08-29", "relevancy": 2.3371, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Drone%20Imagery%20to%20Livability%20Mapping%3A%20AI-powered%20Environment%0A%20%20Perception%20in%20Rural%20China&body=Title%3A%20From%20Drone%20Imagery%20to%20Livability%20Mapping%3A%20AI-powered%20Environment%0A%20%20Perception%20in%20Rural%20China%0AAuthor%3A%20Weihuan%20Deng%20and%20Yaofu%20Huang%20and%20Luan%20Chen%20and%20Xun%20Li%20and%20Yao%20Yao%0AAbstract%3A%20%20%20With%20the%20deepening%20of%20poverty%20alleviation%20and%20rural%20revitalization%0Astrategies%2C%20improving%20the%20rural%20living%20environment%20and%20enhancing%20the%20quality%20of%0Alife%20have%20become%20key%20priorities.%20Rural%20livability%20is%20a%20key%20indicator%20for%0Ameasuring%20the%20effectiveness%20of%20these%20efforts.%20Current%20measurement%20approaches%0Aface%20significant%20limitations%2C%20as%20questionnaire-based%20methods%20are%20difficult%20to%0Ascale%2C%20while%20urban-oriented%20visual%20perception%20methods%20are%20poorly%20suited%20for%0Arural%20contexts.%20In%20this%20paper%2C%20a%20rural-specific%20livability%20assessment%20framework%0Awas%20proposed%20based%20on%20drone%20imagery%20and%20multimodal%20large%20language%20models%0A%28MLLMs%29.%20To%20comprehensively%20assess%20village%20livability%2C%20this%20study%20first%20used%20a%0Atop-down%20approach%20to%20collect%20large-scale%20drone%20imagery%20of%201%2C766%20villages%20in%20146%0Acounties%20across%20China.%20In%20terms%20of%20the%20model%20framework%2C%20an%20efficient%20image%0Acomparison%20mechanism%20was%20developed%2C%20incorporating%20binary%20search%20interpolation%0Ato%20determine%20effective%20image%20pairs%20while%20reducing%20comparison%20iterations.%0ABuilding%20on%20expert%20knowledge%2C%20a%20chain-of-thought%20prompting%20suitable%20for%0Anationwide%20rural%20livability%20measurement%20was%20constructed%2C%20considering%20both%0Aliving%20quality%20and%20ecological%20habitability%20dimensions.%20This%20approach%20enhanced%0Athe%20rationality%20and%20reliability%20of%20the%20livability%20assessment.%20Finally%2C%20this%0Astudy%20characterized%20the%20spatial%20heterogeneity%20of%20rural%20livability%20across%20China%0Aand%20thoroughly%20analyzed%20its%20influential%20factors.%20The%20results%20show%20that%3A%20%281%29%20The%0Arural%20livability%20in%20China%20demonstrates%20a%20dual-core-periphery%20spatial%20pattern%2C%0Aradiating%20outward%20from%20Sichuan%20and%20Zhejiang%20provinces%20with%20declining%20gradients%3B%0A%282%29%20Among%20various%20influential%20factors%2C%20government%20fiscal%20expenditure%20emerged%20as%0Athe%20core%20determinant%2C%20with%20each%20unit%20increase%20corresponding%20to%20a%203.9%20-%204.9%20unit%0Aenhancement%20in%20livability.%20The%20findings%20provide%20valuable%20insights%20for%20rural%0Aconstruction%20policy-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Drone%2520Imagery%2520to%2520Livability%2520Mapping%253A%2520AI-powered%2520Environment%250A%2520%2520Perception%2520in%2520Rural%2520China%26entry.906535625%3DWeihuan%2520Deng%2520and%2520Yaofu%2520Huang%2520and%2520Luan%2520Chen%2520and%2520Xun%2520Li%2520and%2520Yao%2520Yao%26entry.1292438233%3D%2520%2520With%2520the%2520deepening%2520of%2520poverty%2520alleviation%2520and%2520rural%2520revitalization%250Astrategies%252C%2520improving%2520the%2520rural%2520living%2520environment%2520and%2520enhancing%2520the%2520quality%2520of%250Alife%2520have%2520become%2520key%2520priorities.%2520Rural%2520livability%2520is%2520a%2520key%2520indicator%2520for%250Ameasuring%2520the%2520effectiveness%2520of%2520these%2520efforts.%2520Current%2520measurement%2520approaches%250Aface%2520significant%2520limitations%252C%2520as%2520questionnaire-based%2520methods%2520are%2520difficult%2520to%250Ascale%252C%2520while%2520urban-oriented%2520visual%2520perception%2520methods%2520are%2520poorly%2520suited%2520for%250Arural%2520contexts.%2520In%2520this%2520paper%252C%2520a%2520rural-specific%2520livability%2520assessment%2520framework%250Awas%2520proposed%2520based%2520on%2520drone%2520imagery%2520and%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529.%2520To%2520comprehensively%2520assess%2520village%2520livability%252C%2520this%2520study%2520first%2520used%2520a%250Atop-down%2520approach%2520to%2520collect%2520large-scale%2520drone%2520imagery%2520of%25201%252C766%2520villages%2520in%2520146%250Acounties%2520across%2520China.%2520In%2520terms%2520of%2520the%2520model%2520framework%252C%2520an%2520efficient%2520image%250Acomparison%2520mechanism%2520was%2520developed%252C%2520incorporating%2520binary%2520search%2520interpolation%250Ato%2520determine%2520effective%2520image%2520pairs%2520while%2520reducing%2520comparison%2520iterations.%250ABuilding%2520on%2520expert%2520knowledge%252C%2520a%2520chain-of-thought%2520prompting%2520suitable%2520for%250Anationwide%2520rural%2520livability%2520measurement%2520was%2520constructed%252C%2520considering%2520both%250Aliving%2520quality%2520and%2520ecological%2520habitability%2520dimensions.%2520This%2520approach%2520enhanced%250Athe%2520rationality%2520and%2520reliability%2520of%2520the%2520livability%2520assessment.%2520Finally%252C%2520this%250Astudy%2520characterized%2520the%2520spatial%2520heterogeneity%2520of%2520rural%2520livability%2520across%2520China%250Aand%2520thoroughly%2520analyzed%2520its%2520influential%2520factors.%2520The%2520results%2520show%2520that%253A%2520%25281%2529%2520The%250Arural%2520livability%2520in%2520China%2520demonstrates%2520a%2520dual-core-periphery%2520spatial%2520pattern%252C%250Aradiating%2520outward%2520from%2520Sichuan%2520and%2520Zhejiang%2520provinces%2520with%2520declining%2520gradients%253B%250A%25282%2529%2520Among%2520various%2520influential%2520factors%252C%2520government%2520fiscal%2520expenditure%2520emerged%2520as%250Athe%2520core%2520determinant%252C%2520with%2520each%2520unit%2520increase%2520corresponding%2520to%2520a%25203.9%2520-%25204.9%2520unit%250Aenhancement%2520in%2520livability.%2520The%2520findings%2520provide%2520valuable%2520insights%2520for%2520rural%250Aconstruction%2520policy-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Drone%20Imagery%20to%20Livability%20Mapping%3A%20AI-powered%20Environment%0A%20%20Perception%20in%20Rural%20China&entry.906535625=Weihuan%20Deng%20and%20Yaofu%20Huang%20and%20Luan%20Chen%20and%20Xun%20Li%20and%20Yao%20Yao&entry.1292438233=%20%20With%20the%20deepening%20of%20poverty%20alleviation%20and%20rural%20revitalization%0Astrategies%2C%20improving%20the%20rural%20living%20environment%20and%20enhancing%20the%20quality%20of%0Alife%20have%20become%20key%20priorities.%20Rural%20livability%20is%20a%20key%20indicator%20for%0Ameasuring%20the%20effectiveness%20of%20these%20efforts.%20Current%20measurement%20approaches%0Aface%20significant%20limitations%2C%20as%20questionnaire-based%20methods%20are%20difficult%20to%0Ascale%2C%20while%20urban-oriented%20visual%20perception%20methods%20are%20poorly%20suited%20for%0Arural%20contexts.%20In%20this%20paper%2C%20a%20rural-specific%20livability%20assessment%20framework%0Awas%20proposed%20based%20on%20drone%20imagery%20and%20multimodal%20large%20language%20models%0A%28MLLMs%29.%20To%20comprehensively%20assess%20village%20livability%2C%20this%20study%20first%20used%20a%0Atop-down%20approach%20to%20collect%20large-scale%20drone%20imagery%20of%201%2C766%20villages%20in%20146%0Acounties%20across%20China.%20In%20terms%20of%20the%20model%20framework%2C%20an%20efficient%20image%0Acomparison%20mechanism%20was%20developed%2C%20incorporating%20binary%20search%20interpolation%0Ato%20determine%20effective%20image%20pairs%20while%20reducing%20comparison%20iterations.%0ABuilding%20on%20expert%20knowledge%2C%20a%20chain-of-thought%20prompting%20suitable%20for%0Anationwide%20rural%20livability%20measurement%20was%20constructed%2C%20considering%20both%0Aliving%20quality%20and%20ecological%20habitability%20dimensions.%20This%20approach%20enhanced%0Athe%20rationality%20and%20reliability%20of%20the%20livability%20assessment.%20Finally%2C%20this%0Astudy%20characterized%20the%20spatial%20heterogeneity%20of%20rural%20livability%20across%20China%0Aand%20thoroughly%20analyzed%20its%20influential%20factors.%20The%20results%20show%20that%3A%20%281%29%20The%0Arural%20livability%20in%20China%20demonstrates%20a%20dual-core-periphery%20spatial%20pattern%2C%0Aradiating%20outward%20from%20Sichuan%20and%20Zhejiang%20provinces%20with%20declining%20gradients%3B%0A%282%29%20Among%20various%20influential%20factors%2C%20government%20fiscal%20expenditure%20emerged%20as%0Athe%20core%20determinant%2C%20with%20each%20unit%20increase%20corresponding%20to%20a%203.9%20-%204.9%20unit%0Aenhancement%20in%20livability.%20The%20findings%20provide%20valuable%20insights%20for%20rural%0Aconstruction%20policy-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21738v1&entry.124074799=Read"},
{"title": "Consistent and Invariant Generalization Learning for Short-video\n  Misinformation Detection", "author": "Hanghui Guo and Weijie Shi and Mengze Li and Juncheng Li and Hao Chen and Yue Cui and Jiajie Xu and Jia Zhu and Jiawei Shen and Zhangze Chen and Sirui Han", "abstract": "  Short-video misinformation detection has attracted wide attention in the\nmulti-modal domain, aiming to accurately identify the misinformation in the\nvideo format accompanied by the corresponding audio. Despite significant\nadvancements, current models in this field, trained on particular domains\n(source domains), often exhibit unsatisfactory performance on unseen domains\n(target domains) due to domain gaps. To effectively realize such domain\ngeneralization on the short-video misinformation detection task, we propose\ndeep insights into the characteristics of different domains: (1) The detection\non various domains may mainly rely on different modalities (i.e., mainly\nfocusing on videos or audios). To enhance domain generalization, it is crucial\nto achieve optimal model performance on all modalities simultaneously. (2) For\nsome domains focusing on cross-modal joint fraud, a comprehensive analysis\nrelying on cross-modal fusion is necessary. However, domain biases located in\neach modality (especially in each frame of videos) will be accumulated in this\nfusion process, which may seriously damage the final identification of\nmisinformation. To address these issues, we propose a new DOmain generalization\nmodel via ConsisTency and invariance learning for shORt-video misinformation\ndetection (named DOCTOR), which contains two characteristic modules: (1) We\ninvolve the cross-modal feature interpolation to map multiple modalities into a\nshared space and the interpolation distillation to synchronize multi-modal\nlearning; (2) We design the diffusion model to add noise to retain core\nfeatures of multi modal and enhance domain invariant features through\ncross-modal guided denoising. Extensive experiments demonstrate the\neffectiveness of our proposed DOCTOR model. Our code is public available at\nhttps://github.com/ghh1125/DOCTOR.\n", "link": "http://arxiv.org/abs/2507.04061v3", "date": "2025-08-29", "relevancy": 2.3255, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5861}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5789}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20and%20Invariant%20Generalization%20Learning%20for%20Short-video%0A%20%20Misinformation%20Detection&body=Title%3A%20Consistent%20and%20Invariant%20Generalization%20Learning%20for%20Short-video%0A%20%20Misinformation%20Detection%0AAuthor%3A%20Hanghui%20Guo%20and%20Weijie%20Shi%20and%20Mengze%20Li%20and%20Juncheng%20Li%20and%20Hao%20Chen%20and%20Yue%20Cui%20and%20Jiajie%20Xu%20and%20Jia%20Zhu%20and%20Jiawei%20Shen%20and%20Zhangze%20Chen%20and%20Sirui%20Han%0AAbstract%3A%20%20%20Short-video%20misinformation%20detection%20has%20attracted%20wide%20attention%20in%20the%0Amulti-modal%20domain%2C%20aiming%20to%20accurately%20identify%20the%20misinformation%20in%20the%0Avideo%20format%20accompanied%20by%20the%20corresponding%20audio.%20Despite%20significant%0Aadvancements%2C%20current%20models%20in%20this%20field%2C%20trained%20on%20particular%20domains%0A%28source%20domains%29%2C%20often%20exhibit%20unsatisfactory%20performance%20on%20unseen%20domains%0A%28target%20domains%29%20due%20to%20domain%20gaps.%20To%20effectively%20realize%20such%20domain%0Ageneralization%20on%20the%20short-video%20misinformation%20detection%20task%2C%20we%20propose%0Adeep%20insights%20into%20the%20characteristics%20of%20different%20domains%3A%20%281%29%20The%20detection%0Aon%20various%20domains%20may%20mainly%20rely%20on%20different%20modalities%20%28i.e.%2C%20mainly%0Afocusing%20on%20videos%20or%20audios%29.%20To%20enhance%20domain%20generalization%2C%20it%20is%20crucial%0Ato%20achieve%20optimal%20model%20performance%20on%20all%20modalities%20simultaneously.%20%282%29%20For%0Asome%20domains%20focusing%20on%20cross-modal%20joint%20fraud%2C%20a%20comprehensive%20analysis%0Arelying%20on%20cross-modal%20fusion%20is%20necessary.%20However%2C%20domain%20biases%20located%20in%0Aeach%20modality%20%28especially%20in%20each%20frame%20of%20videos%29%20will%20be%20accumulated%20in%20this%0Afusion%20process%2C%20which%20may%20seriously%20damage%20the%20final%20identification%20of%0Amisinformation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20DOmain%20generalization%0Amodel%20via%20ConsisTency%20and%20invariance%20learning%20for%20shORt-video%20misinformation%0Adetection%20%28named%20DOCTOR%29%2C%20which%20contains%20two%20characteristic%20modules%3A%20%281%29%20We%0Ainvolve%20the%20cross-modal%20feature%20interpolation%20to%20map%20multiple%20modalities%20into%20a%0Ashared%20space%20and%20the%20interpolation%20distillation%20to%20synchronize%20multi-modal%0Alearning%3B%20%282%29%20We%20design%20the%20diffusion%20model%20to%20add%20noise%20to%20retain%20core%0Afeatures%20of%20multi%20modal%20and%20enhance%20domain%20invariant%20features%20through%0Across-modal%20guided%20denoising.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20DOCTOR%20model.%20Our%20code%20is%20public%20available%20at%0Ahttps%3A//github.com/ghh1125/DOCTOR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520and%2520Invariant%2520Generalization%2520Learning%2520for%2520Short-video%250A%2520%2520Misinformation%2520Detection%26entry.906535625%3DHanghui%2520Guo%2520and%2520Weijie%2520Shi%2520and%2520Mengze%2520Li%2520and%2520Juncheng%2520Li%2520and%2520Hao%2520Chen%2520and%2520Yue%2520Cui%2520and%2520Jiajie%2520Xu%2520and%2520Jia%2520Zhu%2520and%2520Jiawei%2520Shen%2520and%2520Zhangze%2520Chen%2520and%2520Sirui%2520Han%26entry.1292438233%3D%2520%2520Short-video%2520misinformation%2520detection%2520has%2520attracted%2520wide%2520attention%2520in%2520the%250Amulti-modal%2520domain%252C%2520aiming%2520to%2520accurately%2520identify%2520the%2520misinformation%2520in%2520the%250Avideo%2520format%2520accompanied%2520by%2520the%2520corresponding%2520audio.%2520Despite%2520significant%250Aadvancements%252C%2520current%2520models%2520in%2520this%2520field%252C%2520trained%2520on%2520particular%2520domains%250A%2528source%2520domains%2529%252C%2520often%2520exhibit%2520unsatisfactory%2520performance%2520on%2520unseen%2520domains%250A%2528target%2520domains%2529%2520due%2520to%2520domain%2520gaps.%2520To%2520effectively%2520realize%2520such%2520domain%250Ageneralization%2520on%2520the%2520short-video%2520misinformation%2520detection%2520task%252C%2520we%2520propose%250Adeep%2520insights%2520into%2520the%2520characteristics%2520of%2520different%2520domains%253A%2520%25281%2529%2520The%2520detection%250Aon%2520various%2520domains%2520may%2520mainly%2520rely%2520on%2520different%2520modalities%2520%2528i.e.%252C%2520mainly%250Afocusing%2520on%2520videos%2520or%2520audios%2529.%2520To%2520enhance%2520domain%2520generalization%252C%2520it%2520is%2520crucial%250Ato%2520achieve%2520optimal%2520model%2520performance%2520on%2520all%2520modalities%2520simultaneously.%2520%25282%2529%2520For%250Asome%2520domains%2520focusing%2520on%2520cross-modal%2520joint%2520fraud%252C%2520a%2520comprehensive%2520analysis%250Arelying%2520on%2520cross-modal%2520fusion%2520is%2520necessary.%2520However%252C%2520domain%2520biases%2520located%2520in%250Aeach%2520modality%2520%2528especially%2520in%2520each%2520frame%2520of%2520videos%2529%2520will%2520be%2520accumulated%2520in%2520this%250Afusion%2520process%252C%2520which%2520may%2520seriously%2520damage%2520the%2520final%2520identification%2520of%250Amisinformation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520DOmain%2520generalization%250Amodel%2520via%2520ConsisTency%2520and%2520invariance%2520learning%2520for%2520shORt-video%2520misinformation%250Adetection%2520%2528named%2520DOCTOR%2529%252C%2520which%2520contains%2520two%2520characteristic%2520modules%253A%2520%25281%2529%2520We%250Ainvolve%2520the%2520cross-modal%2520feature%2520interpolation%2520to%2520map%2520multiple%2520modalities%2520into%2520a%250Ashared%2520space%2520and%2520the%2520interpolation%2520distillation%2520to%2520synchronize%2520multi-modal%250Alearning%253B%2520%25282%2529%2520We%2520design%2520the%2520diffusion%2520model%2520to%2520add%2520noise%2520to%2520retain%2520core%250Afeatures%2520of%2520multi%2520modal%2520and%2520enhance%2520domain%2520invariant%2520features%2520through%250Across-modal%2520guided%2520denoising.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520DOCTOR%2520model.%2520Our%2520code%2520is%2520public%2520available%2520at%250Ahttps%253A//github.com/ghh1125/DOCTOR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20and%20Invariant%20Generalization%20Learning%20for%20Short-video%0A%20%20Misinformation%20Detection&entry.906535625=Hanghui%20Guo%20and%20Weijie%20Shi%20and%20Mengze%20Li%20and%20Juncheng%20Li%20and%20Hao%20Chen%20and%20Yue%20Cui%20and%20Jiajie%20Xu%20and%20Jia%20Zhu%20and%20Jiawei%20Shen%20and%20Zhangze%20Chen%20and%20Sirui%20Han&entry.1292438233=%20%20Short-video%20misinformation%20detection%20has%20attracted%20wide%20attention%20in%20the%0Amulti-modal%20domain%2C%20aiming%20to%20accurately%20identify%20the%20misinformation%20in%20the%0Avideo%20format%20accompanied%20by%20the%20corresponding%20audio.%20Despite%20significant%0Aadvancements%2C%20current%20models%20in%20this%20field%2C%20trained%20on%20particular%20domains%0A%28source%20domains%29%2C%20often%20exhibit%20unsatisfactory%20performance%20on%20unseen%20domains%0A%28target%20domains%29%20due%20to%20domain%20gaps.%20To%20effectively%20realize%20such%20domain%0Ageneralization%20on%20the%20short-video%20misinformation%20detection%20task%2C%20we%20propose%0Adeep%20insights%20into%20the%20characteristics%20of%20different%20domains%3A%20%281%29%20The%20detection%0Aon%20various%20domains%20may%20mainly%20rely%20on%20different%20modalities%20%28i.e.%2C%20mainly%0Afocusing%20on%20videos%20or%20audios%29.%20To%20enhance%20domain%20generalization%2C%20it%20is%20crucial%0Ato%20achieve%20optimal%20model%20performance%20on%20all%20modalities%20simultaneously.%20%282%29%20For%0Asome%20domains%20focusing%20on%20cross-modal%20joint%20fraud%2C%20a%20comprehensive%20analysis%0Arelying%20on%20cross-modal%20fusion%20is%20necessary.%20However%2C%20domain%20biases%20located%20in%0Aeach%20modality%20%28especially%20in%20each%20frame%20of%20videos%29%20will%20be%20accumulated%20in%20this%0Afusion%20process%2C%20which%20may%20seriously%20damage%20the%20final%20identification%20of%0Amisinformation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20new%20DOmain%20generalization%0Amodel%20via%20ConsisTency%20and%20invariance%20learning%20for%20shORt-video%20misinformation%0Adetection%20%28named%20DOCTOR%29%2C%20which%20contains%20two%20characteristic%20modules%3A%20%281%29%20We%0Ainvolve%20the%20cross-modal%20feature%20interpolation%20to%20map%20multiple%20modalities%20into%20a%0Ashared%20space%20and%20the%20interpolation%20distillation%20to%20synchronize%20multi-modal%0Alearning%3B%20%282%29%20We%20design%20the%20diffusion%20model%20to%20add%20noise%20to%20retain%20core%0Afeatures%20of%20multi%20modal%20and%20enhance%20domain%20invariant%20features%20through%0Across-modal%20guided%20denoising.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20DOCTOR%20model.%20Our%20code%20is%20public%20available%20at%0Ahttps%3A//github.com/ghh1125/DOCTOR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04061v3&entry.124074799=Read"},
{"title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization", "author": "Jaewook Lee and Alexander Scarlatos and Andrew Lan", "abstract": "  Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.\n", "link": "http://arxiv.org/abs/2507.05137v2", "date": "2025-08-29", "relevancy": 2.3126, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization&body=Title%3A%20Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization%0AAuthor%3A%20Jaewook%20Lee%20and%20Alexander%20Scarlatos%20and%20Andrew%20Lan%0AAbstract%3A%20%20%20Learning%20Japanese%20vocabulary%20is%20a%20challenge%20for%20learners%20from%20Roman%20alphabet%0Abackgrounds%20due%20to%20script%20differences.%20Japanese%20combines%20syllabaries%20like%0Ahiragana%20with%20kanji%2C%20which%20are%20logographic%20characters%20of%20Chinese%20origin.%20Kanji%0Aare%20also%20complicated%20due%20to%20their%20complexity%20and%20volume.%20Keyword%20mnemonics%20are%0Aa%20common%20strategy%20to%20aid%20memorization%2C%20often%20using%20the%20compositional%20structure%0Aof%20kanji%20to%20form%20vivid%20associations.%20Despite%20recent%20efforts%20to%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20assist%20learners%2C%20existing%20methods%20for%20LLM-based%0Akeyword%20mnemonic%20generation%20function%20as%20a%20black%20box%2C%20offering%20limited%0Ainterpretability.%20We%20propose%20a%20generative%20framework%20that%20explicitly%20models%20the%0Amnemonic%20construction%20process%20as%20driven%20by%20a%20set%20of%20common%20rules%2C%20and%20learn%0Athem%20using%20a%20novel%20Expectation-Maximization-type%20algorithm.%20Trained%20on%0Alearner-authored%20mnemonics%20from%20an%20online%20platform%2C%20our%20method%20learns%20latent%0Astructures%20and%20compositional%20rules%2C%20enabling%20interpretable%20and%20systematic%0Amnemonics%20generation.%20Experiments%20show%20that%20our%20method%20performs%20well%20in%20the%0Acold-start%20setting%20for%20new%20learners%20while%20providing%20insight%20into%20the%20mechanisms%0Abehind%20effective%20mnemonic%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Mnemonic%2520Generation%2520for%2520Kanji%2520Learning%2520via%250A%2520%2520Expectation-Maximization%26entry.906535625%3DJaewook%2520Lee%2520and%2520Alexander%2520Scarlatos%2520and%2520Andrew%2520Lan%26entry.1292438233%3D%2520%2520Learning%2520Japanese%2520vocabulary%2520is%2520a%2520challenge%2520for%2520learners%2520from%2520Roman%2520alphabet%250Abackgrounds%2520due%2520to%2520script%2520differences.%2520Japanese%2520combines%2520syllabaries%2520like%250Ahiragana%2520with%2520kanji%252C%2520which%2520are%2520logographic%2520characters%2520of%2520Chinese%2520origin.%2520Kanji%250Aare%2520also%2520complicated%2520due%2520to%2520their%2520complexity%2520and%2520volume.%2520Keyword%2520mnemonics%2520are%250Aa%2520common%2520strategy%2520to%2520aid%2520memorization%252C%2520often%2520using%2520the%2520compositional%2520structure%250Aof%2520kanji%2520to%2520form%2520vivid%2520associations.%2520Despite%2520recent%2520efforts%2520to%2520use%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520assist%2520learners%252C%2520existing%2520methods%2520for%2520LLM-based%250Akeyword%2520mnemonic%2520generation%2520function%2520as%2520a%2520black%2520box%252C%2520offering%2520limited%250Ainterpretability.%2520We%2520propose%2520a%2520generative%2520framework%2520that%2520explicitly%2520models%2520the%250Amnemonic%2520construction%2520process%2520as%2520driven%2520by%2520a%2520set%2520of%2520common%2520rules%252C%2520and%2520learn%250Athem%2520using%2520a%2520novel%2520Expectation-Maximization-type%2520algorithm.%2520Trained%2520on%250Alearner-authored%2520mnemonics%2520from%2520an%2520online%2520platform%252C%2520our%2520method%2520learns%2520latent%250Astructures%2520and%2520compositional%2520rules%252C%2520enabling%2520interpretable%2520and%2520systematic%250Amnemonics%2520generation.%2520Experiments%2520show%2520that%2520our%2520method%2520performs%2520well%2520in%2520the%250Acold-start%2520setting%2520for%2520new%2520learners%2520while%2520providing%2520insight%2520into%2520the%2520mechanisms%250Abehind%2520effective%2520mnemonic%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization&entry.906535625=Jaewook%20Lee%20and%20Alexander%20Scarlatos%20and%20Andrew%20Lan&entry.1292438233=%20%20Learning%20Japanese%20vocabulary%20is%20a%20challenge%20for%20learners%20from%20Roman%20alphabet%0Abackgrounds%20due%20to%20script%20differences.%20Japanese%20combines%20syllabaries%20like%0Ahiragana%20with%20kanji%2C%20which%20are%20logographic%20characters%20of%20Chinese%20origin.%20Kanji%0Aare%20also%20complicated%20due%20to%20their%20complexity%20and%20volume.%20Keyword%20mnemonics%20are%0Aa%20common%20strategy%20to%20aid%20memorization%2C%20often%20using%20the%20compositional%20structure%0Aof%20kanji%20to%20form%20vivid%20associations.%20Despite%20recent%20efforts%20to%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20assist%20learners%2C%20existing%20methods%20for%20LLM-based%0Akeyword%20mnemonic%20generation%20function%20as%20a%20black%20box%2C%20offering%20limited%0Ainterpretability.%20We%20propose%20a%20generative%20framework%20that%20explicitly%20models%20the%0Amnemonic%20construction%20process%20as%20driven%20by%20a%20set%20of%20common%20rules%2C%20and%20learn%0Athem%20using%20a%20novel%20Expectation-Maximization-type%20algorithm.%20Trained%20on%0Alearner-authored%20mnemonics%20from%20an%20online%20platform%2C%20our%20method%20learns%20latent%0Astructures%20and%20compositional%20rules%2C%20enabling%20interpretable%20and%20systematic%0Amnemonics%20generation.%20Experiments%20show%20that%20our%20method%20performs%20well%20in%20the%0Acold-start%20setting%20for%20new%20learners%20while%20providing%20insight%20into%20the%20mechanisms%0Abehind%20effective%20mnemonic%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05137v2&entry.124074799=Read"},
{"title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD\n  Model Datasets for fine-tuning Large Vision-Language Models", "author": "Jo\u00e3o Valente and Atabak Dehban and Rodrigo Ventura", "abstract": "  Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive capabilities across various multimodal tasks. They continue,\nhowever, to struggle with trivial scenarios such as reading values from Digital\nMeasurement Devices (DMDs), particularly in real-world conditions involving\nclutter, occlusions, extreme viewpoints, and motion blur; common in\nhead-mounted cameras and Augmented Reality (AR) applications. Motivated by\nthese limitations, this work introduces CAD2DMD-SET, a synthetic data\ngeneration tool designed to support visual question answering (VQA) tasks\ninvolving DMDs. By leveraging 3D CAD models, advanced rendering, and\nhigh-fidelity image composition, our tool produces diverse, VQA-labelled\nsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present\nDMDBench, a curated validation set of 1,000 annotated real-world images\ndesigned to evaluate model performance under practical constraints.\nBenchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein\nSimilarity (ANLS) and further fine-tuning LoRA's of these models with\nCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL\nshowcasing a score increase of 200% without degrading on other tasks. This\ndemonstrates that the CAD2DMD-SET training dataset substantially improves the\nrobustness and performance of LVLMs when operating under the previously stated\nchallenging conditions. The CAD2DMD-SET tool is expected to be released as\nopen-source once the final version of this manuscript is prepared, allowing the\ncommunity to add different measurement devices and generate their own datasets.\n", "link": "http://arxiv.org/abs/2508.21732v1", "date": "2025-08-29", "relevancy": 2.3104, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD2DMD-SET%3A%20Synthetic%20Generation%20Tool%20of%20Digital%20Measurement%20Device%20CAD%0A%20%20Model%20Datasets%20for%20fine-tuning%20Large%20Vision-Language%20Models&body=Title%3A%20CAD2DMD-SET%3A%20Synthetic%20Generation%20Tool%20of%20Digital%20Measurement%20Device%20CAD%0A%20%20Model%20Datasets%20for%20fine-tuning%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jo%C3%A3o%20Valente%20and%20Atabak%20Dehban%20and%20Rodrigo%20Ventura%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%0Aimpressive%20capabilities%20across%20various%20multimodal%20tasks.%20They%20continue%2C%0Ahowever%2C%20to%20struggle%20with%20trivial%20scenarios%20such%20as%20reading%20values%20from%20Digital%0AMeasurement%20Devices%20%28DMDs%29%2C%20particularly%20in%20real-world%20conditions%20involving%0Aclutter%2C%20occlusions%2C%20extreme%20viewpoints%2C%20and%20motion%20blur%3B%20common%20in%0Ahead-mounted%20cameras%20and%20Augmented%20Reality%20%28AR%29%20applications.%20Motivated%20by%0Athese%20limitations%2C%20this%20work%20introduces%20CAD2DMD-SET%2C%20a%20synthetic%20data%0Ageneration%20tool%20designed%20to%20support%20visual%20question%20answering%20%28VQA%29%20tasks%0Ainvolving%20DMDs.%20By%20leveraging%203D%20CAD%20models%2C%20advanced%20rendering%2C%20and%0Ahigh-fidelity%20image%20composition%2C%20our%20tool%20produces%20diverse%2C%20VQA-labelled%0Asynthetic%20DMD%20datasets%20suitable%20for%20fine-tuning%20LVLMs.%20Additionally%2C%20we%20present%0ADMDBench%2C%20a%20curated%20validation%20set%20of%201%2C000%20annotated%20real-world%20images%0Adesigned%20to%20evaluate%20model%20performance%20under%20practical%20constraints.%0ABenchmarking%20three%20state-of-the-art%20LVLMs%20using%20Average%20Normalised%20Levenshtein%0ASimilarity%20%28ANLS%29%20and%20further%20fine-tuning%20LoRA%27s%20of%20these%20models%20with%0ACAD2DMD-SET%27s%20generated%20dataset%20yielded%20substantial%20improvements%2C%20with%20InternVL%0Ashowcasing%20a%20score%20increase%20of%20200%25%20without%20degrading%20on%20other%20tasks.%20This%0Ademonstrates%20that%20the%20CAD2DMD-SET%20training%20dataset%20substantially%20improves%20the%0Arobustness%20and%20performance%20of%20LVLMs%20when%20operating%20under%20the%20previously%20stated%0Achallenging%20conditions.%20The%20CAD2DMD-SET%20tool%20is%20expected%20to%20be%20released%20as%0Aopen-source%20once%20the%20final%20version%20of%20this%20manuscript%20is%20prepared%2C%20allowing%20the%0Acommunity%20to%20add%20different%20measurement%20devices%20and%20generate%20their%20own%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD2DMD-SET%253A%2520Synthetic%2520Generation%2520Tool%2520of%2520Digital%2520Measurement%2520Device%2520CAD%250A%2520%2520Model%2520Datasets%2520for%2520fine-tuning%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJo%25C3%25A3o%2520Valente%2520and%2520Atabak%2520Dehban%2520and%2520Rodrigo%2520Ventura%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%250Aimpressive%2520capabilities%2520across%2520various%2520multimodal%2520tasks.%2520They%2520continue%252C%250Ahowever%252C%2520to%2520struggle%2520with%2520trivial%2520scenarios%2520such%2520as%2520reading%2520values%2520from%2520Digital%250AMeasurement%2520Devices%2520%2528DMDs%2529%252C%2520particularly%2520in%2520real-world%2520conditions%2520involving%250Aclutter%252C%2520occlusions%252C%2520extreme%2520viewpoints%252C%2520and%2520motion%2520blur%253B%2520common%2520in%250Ahead-mounted%2520cameras%2520and%2520Augmented%2520Reality%2520%2528AR%2529%2520applications.%2520Motivated%2520by%250Athese%2520limitations%252C%2520this%2520work%2520introduces%2520CAD2DMD-SET%252C%2520a%2520synthetic%2520data%250Ageneration%2520tool%2520designed%2520to%2520support%2520visual%2520question%2520answering%2520%2528VQA%2529%2520tasks%250Ainvolving%2520DMDs.%2520By%2520leveraging%25203D%2520CAD%2520models%252C%2520advanced%2520rendering%252C%2520and%250Ahigh-fidelity%2520image%2520composition%252C%2520our%2520tool%2520produces%2520diverse%252C%2520VQA-labelled%250Asynthetic%2520DMD%2520datasets%2520suitable%2520for%2520fine-tuning%2520LVLMs.%2520Additionally%252C%2520we%2520present%250ADMDBench%252C%2520a%2520curated%2520validation%2520set%2520of%25201%252C000%2520annotated%2520real-world%2520images%250Adesigned%2520to%2520evaluate%2520model%2520performance%2520under%2520practical%2520constraints.%250ABenchmarking%2520three%2520state-of-the-art%2520LVLMs%2520using%2520Average%2520Normalised%2520Levenshtein%250ASimilarity%2520%2528ANLS%2529%2520and%2520further%2520fine-tuning%2520LoRA%2527s%2520of%2520these%2520models%2520with%250ACAD2DMD-SET%2527s%2520generated%2520dataset%2520yielded%2520substantial%2520improvements%252C%2520with%2520InternVL%250Ashowcasing%2520a%2520score%2520increase%2520of%2520200%2525%2520without%2520degrading%2520on%2520other%2520tasks.%2520This%250Ademonstrates%2520that%2520the%2520CAD2DMD-SET%2520training%2520dataset%2520substantially%2520improves%2520the%250Arobustness%2520and%2520performance%2520of%2520LVLMs%2520when%2520operating%2520under%2520the%2520previously%2520stated%250Achallenging%2520conditions.%2520The%2520CAD2DMD-SET%2520tool%2520is%2520expected%2520to%2520be%2520released%2520as%250Aopen-source%2520once%2520the%2520final%2520version%2520of%2520this%2520manuscript%2520is%2520prepared%252C%2520allowing%2520the%250Acommunity%2520to%2520add%2520different%2520measurement%2520devices%2520and%2520generate%2520their%2520own%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD2DMD-SET%3A%20Synthetic%20Generation%20Tool%20of%20Digital%20Measurement%20Device%20CAD%0A%20%20Model%20Datasets%20for%20fine-tuning%20Large%20Vision-Language%20Models&entry.906535625=Jo%C3%A3o%20Valente%20and%20Atabak%20Dehban%20and%20Rodrigo%20Ventura&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%0Aimpressive%20capabilities%20across%20various%20multimodal%20tasks.%20They%20continue%2C%0Ahowever%2C%20to%20struggle%20with%20trivial%20scenarios%20such%20as%20reading%20values%20from%20Digital%0AMeasurement%20Devices%20%28DMDs%29%2C%20particularly%20in%20real-world%20conditions%20involving%0Aclutter%2C%20occlusions%2C%20extreme%20viewpoints%2C%20and%20motion%20blur%3B%20common%20in%0Ahead-mounted%20cameras%20and%20Augmented%20Reality%20%28AR%29%20applications.%20Motivated%20by%0Athese%20limitations%2C%20this%20work%20introduces%20CAD2DMD-SET%2C%20a%20synthetic%20data%0Ageneration%20tool%20designed%20to%20support%20visual%20question%20answering%20%28VQA%29%20tasks%0Ainvolving%20DMDs.%20By%20leveraging%203D%20CAD%20models%2C%20advanced%20rendering%2C%20and%0Ahigh-fidelity%20image%20composition%2C%20our%20tool%20produces%20diverse%2C%20VQA-labelled%0Asynthetic%20DMD%20datasets%20suitable%20for%20fine-tuning%20LVLMs.%20Additionally%2C%20we%20present%0ADMDBench%2C%20a%20curated%20validation%20set%20of%201%2C000%20annotated%20real-world%20images%0Adesigned%20to%20evaluate%20model%20performance%20under%20practical%20constraints.%0ABenchmarking%20three%20state-of-the-art%20LVLMs%20using%20Average%20Normalised%20Levenshtein%0ASimilarity%20%28ANLS%29%20and%20further%20fine-tuning%20LoRA%27s%20of%20these%20models%20with%0ACAD2DMD-SET%27s%20generated%20dataset%20yielded%20substantial%20improvements%2C%20with%20InternVL%0Ashowcasing%20a%20score%20increase%20of%20200%25%20without%20degrading%20on%20other%20tasks.%20This%0Ademonstrates%20that%20the%20CAD2DMD-SET%20training%20dataset%20substantially%20improves%20the%0Arobustness%20and%20performance%20of%20LVLMs%20when%20operating%20under%20the%20previously%20stated%0Achallenging%20conditions.%20The%20CAD2DMD-SET%20tool%20is%20expected%20to%20be%20released%20as%0Aopen-source%20once%20the%20final%20version%20of%20this%20manuscript%20is%20prepared%2C%20allowing%20the%0Acommunity%20to%20add%20different%20measurement%20devices%20and%20generate%20their%20own%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21732v1&entry.124074799=Read"},
{"title": "Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D\n  Longitudinal Medical Imaging", "author": "Nico Albert Disch and Yannick Kirchhoff and Robin Peretzke and Maximilian Rokuss and Saikat Roy and Constantin Ulrich and David Zimmerer and Klaus Maier-Hein", "abstract": "  Understanding temporal dynamics in medical imaging is crucial for\napplications such as disease progression modeling, treatment planning and\nanatomical development tracking. However, most deep learning methods either\nconsider only single temporal contexts, or focus on tasks like classification\nor regression, limiting their ability for fine-grained spatial predictions.\nWhile some approaches have been explored, they are often limited to single\ntimepoints, specific diseases or have other technical restrictions. To address\nthis fundamental gap, we introduce Temporal Flow Matching (TFM), a unified\ngenerative trajectory method that (i) aims to learn the underlying temporal\ndistribution, (ii) by design can fall back to a nearest image predictor, i.e.\npredicting the last context image (LCI), as a special case, and (iii) supports\n$3D$ volumes, multiple prior scans, and irregular sampling. Extensive\nbenchmarks on three public longitudinal datasets show that TFM consistently\nsurpasses spatio-temporal methods from natural imaging, establishing a new\nstate-of-the-art and robust baseline for $4D$ medical image prediction.\n", "link": "http://arxiv.org/abs/2508.21580v1", "date": "2025-08-29", "relevancy": 2.2761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5804}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Flow%20Matching%20for%20Learning%20Spatio-Temporal%20Trajectories%20in%204D%0A%20%20Longitudinal%20Medical%20Imaging&body=Title%3A%20Temporal%20Flow%20Matching%20for%20Learning%20Spatio-Temporal%20Trajectories%20in%204D%0A%20%20Longitudinal%20Medical%20Imaging%0AAuthor%3A%20Nico%20Albert%20Disch%20and%20Yannick%20Kirchhoff%20and%20Robin%20Peretzke%20and%20Maximilian%20Rokuss%20and%20Saikat%20Roy%20and%20Constantin%20Ulrich%20and%20David%20Zimmerer%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Understanding%20temporal%20dynamics%20in%20medical%20imaging%20is%20crucial%20for%0Aapplications%20such%20as%20disease%20progression%20modeling%2C%20treatment%20planning%20and%0Aanatomical%20development%20tracking.%20However%2C%20most%20deep%20learning%20methods%20either%0Aconsider%20only%20single%20temporal%20contexts%2C%20or%20focus%20on%20tasks%20like%20classification%0Aor%20regression%2C%20limiting%20their%20ability%20for%20fine-grained%20spatial%20predictions.%0AWhile%20some%20approaches%20have%20been%20explored%2C%20they%20are%20often%20limited%20to%20single%0Atimepoints%2C%20specific%20diseases%20or%20have%20other%20technical%20restrictions.%20To%20address%0Athis%20fundamental%20gap%2C%20we%20introduce%20Temporal%20Flow%20Matching%20%28TFM%29%2C%20a%20unified%0Agenerative%20trajectory%20method%20that%20%28i%29%20aims%20to%20learn%20the%20underlying%20temporal%0Adistribution%2C%20%28ii%29%20by%20design%20can%20fall%20back%20to%20a%20nearest%20image%20predictor%2C%20i.e.%0Apredicting%20the%20last%20context%20image%20%28LCI%29%2C%20as%20a%20special%20case%2C%20and%20%28iii%29%20supports%0A%243D%24%20volumes%2C%20multiple%20prior%20scans%2C%20and%20irregular%20sampling.%20Extensive%0Abenchmarks%20on%20three%20public%20longitudinal%20datasets%20show%20that%20TFM%20consistently%0Asurpasses%20spatio-temporal%20methods%20from%20natural%20imaging%2C%20establishing%20a%20new%0Astate-of-the-art%20and%20robust%20baseline%20for%20%244D%24%20medical%20image%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Flow%2520Matching%2520for%2520Learning%2520Spatio-Temporal%2520Trajectories%2520in%25204D%250A%2520%2520Longitudinal%2520Medical%2520Imaging%26entry.906535625%3DNico%2520Albert%2520Disch%2520and%2520Yannick%2520Kirchhoff%2520and%2520Robin%2520Peretzke%2520and%2520Maximilian%2520Rokuss%2520and%2520Saikat%2520Roy%2520and%2520Constantin%2520Ulrich%2520and%2520David%2520Zimmerer%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Understanding%2520temporal%2520dynamics%2520in%2520medical%2520imaging%2520is%2520crucial%2520for%250Aapplications%2520such%2520as%2520disease%2520progression%2520modeling%252C%2520treatment%2520planning%2520and%250Aanatomical%2520development%2520tracking.%2520However%252C%2520most%2520deep%2520learning%2520methods%2520either%250Aconsider%2520only%2520single%2520temporal%2520contexts%252C%2520or%2520focus%2520on%2520tasks%2520like%2520classification%250Aor%2520regression%252C%2520limiting%2520their%2520ability%2520for%2520fine-grained%2520spatial%2520predictions.%250AWhile%2520some%2520approaches%2520have%2520been%2520explored%252C%2520they%2520are%2520often%2520limited%2520to%2520single%250Atimepoints%252C%2520specific%2520diseases%2520or%2520have%2520other%2520technical%2520restrictions.%2520To%2520address%250Athis%2520fundamental%2520gap%252C%2520we%2520introduce%2520Temporal%2520Flow%2520Matching%2520%2528TFM%2529%252C%2520a%2520unified%250Agenerative%2520trajectory%2520method%2520that%2520%2528i%2529%2520aims%2520to%2520learn%2520the%2520underlying%2520temporal%250Adistribution%252C%2520%2528ii%2529%2520by%2520design%2520can%2520fall%2520back%2520to%2520a%2520nearest%2520image%2520predictor%252C%2520i.e.%250Apredicting%2520the%2520last%2520context%2520image%2520%2528LCI%2529%252C%2520as%2520a%2520special%2520case%252C%2520and%2520%2528iii%2529%2520supports%250A%25243D%2524%2520volumes%252C%2520multiple%2520prior%2520scans%252C%2520and%2520irregular%2520sampling.%2520Extensive%250Abenchmarks%2520on%2520three%2520public%2520longitudinal%2520datasets%2520show%2520that%2520TFM%2520consistently%250Asurpasses%2520spatio-temporal%2520methods%2520from%2520natural%2520imaging%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520and%2520robust%2520baseline%2520for%2520%25244D%2524%2520medical%2520image%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Flow%20Matching%20for%20Learning%20Spatio-Temporal%20Trajectories%20in%204D%0A%20%20Longitudinal%20Medical%20Imaging&entry.906535625=Nico%20Albert%20Disch%20and%20Yannick%20Kirchhoff%20and%20Robin%20Peretzke%20and%20Maximilian%20Rokuss%20and%20Saikat%20Roy%20and%20Constantin%20Ulrich%20and%20David%20Zimmerer%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Understanding%20temporal%20dynamics%20in%20medical%20imaging%20is%20crucial%20for%0Aapplications%20such%20as%20disease%20progression%20modeling%2C%20treatment%20planning%20and%0Aanatomical%20development%20tracking.%20However%2C%20most%20deep%20learning%20methods%20either%0Aconsider%20only%20single%20temporal%20contexts%2C%20or%20focus%20on%20tasks%20like%20classification%0Aor%20regression%2C%20limiting%20their%20ability%20for%20fine-grained%20spatial%20predictions.%0AWhile%20some%20approaches%20have%20been%20explored%2C%20they%20are%20often%20limited%20to%20single%0Atimepoints%2C%20specific%20diseases%20or%20have%20other%20technical%20restrictions.%20To%20address%0Athis%20fundamental%20gap%2C%20we%20introduce%20Temporal%20Flow%20Matching%20%28TFM%29%2C%20a%20unified%0Agenerative%20trajectory%20method%20that%20%28i%29%20aims%20to%20learn%20the%20underlying%20temporal%0Adistribution%2C%20%28ii%29%20by%20design%20can%20fall%20back%20to%20a%20nearest%20image%20predictor%2C%20i.e.%0Apredicting%20the%20last%20context%20image%20%28LCI%29%2C%20as%20a%20special%20case%2C%20and%20%28iii%29%20supports%0A%243D%24%20volumes%2C%20multiple%20prior%20scans%2C%20and%20irregular%20sampling.%20Extensive%0Abenchmarks%20on%20three%20public%20longitudinal%20datasets%20show%20that%20TFM%20consistently%0Asurpasses%20spatio-temporal%20methods%20from%20natural%20imaging%2C%20establishing%20a%20new%0Astate-of-the-art%20and%20robust%20baseline%20for%20%244D%24%20medical%20image%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21580v1&entry.124074799=Read"},
{"title": "Stochastic Control for Fine-tuning Diffusion Models: Optimality,\n  Regularity, and Convergence", "author": "Yinbin Han and Meisam Razaviyayn and Renyuan Xu", "abstract": "  Diffusion models have emerged as powerful tools for generative modeling,\ndemonstrating exceptional capability in capturing target data distributions\nfrom large datasets. However, fine-tuning these massive models for specific\ndownstream tasks, constraints, and human preferences remains a critical\nchallenge. While recent advances have leveraged reinforcement learning\nalgorithms to tackle this problem, much of the progress has been empirical,\nwith limited theoretical understanding. To bridge this gap, we propose a\nstochastic control framework for fine-tuning diffusion models. Building on\ndenoising diffusion probabilistic models as the pre-trained reference dynamics,\nour approach integrates linear dynamics control with Kullback-Leibler\nregularization. We establish the well-posedness and regularity of the\nstochastic control problem and develop a policy iteration algorithm (PI-FT) for\nnumerical solution. We show that PI-FT achieves global convergence at a linear\nrate. Unlike existing work that assumes regularities throughout training, we\nprove that the control and value sequences generated by the algorithm maintain\nthe regularity. Additionally, we explore extensions of our framework to\nparametric settings and continuous-time formulations, and demonstrate the\npractical effectiveness of the proposed PI-FT algorithm through numerical\nexperiments. Our code is available at\nhttps://github.com/yinbinhan/fine-tuning-of-diffusion-models.\n", "link": "http://arxiv.org/abs/2412.18164v4", "date": "2025-08-29", "relevancy": 2.2497, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5748}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Control%20for%20Fine-tuning%20Diffusion%20Models%3A%20Optimality%2C%0A%20%20Regularity%2C%20and%20Convergence&body=Title%3A%20Stochastic%20Control%20for%20Fine-tuning%20Diffusion%20Models%3A%20Optimality%2C%0A%20%20Regularity%2C%20and%20Convergence%0AAuthor%3A%20Yinbin%20Han%20and%20Meisam%20Razaviyayn%20and%20Renyuan%20Xu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20powerful%20tools%20for%20generative%20modeling%2C%0Ademonstrating%20exceptional%20capability%20in%20capturing%20target%20data%20distributions%0Afrom%20large%20datasets.%20However%2C%20fine-tuning%20these%20massive%20models%20for%20specific%0Adownstream%20tasks%2C%20constraints%2C%20and%20human%20preferences%20remains%20a%20critical%0Achallenge.%20While%20recent%20advances%20have%20leveraged%20reinforcement%20learning%0Aalgorithms%20to%20tackle%20this%20problem%2C%20much%20of%20the%20progress%20has%20been%20empirical%2C%0Awith%20limited%20theoretical%20understanding.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%0Astochastic%20control%20framework%20for%20fine-tuning%20diffusion%20models.%20Building%20on%0Adenoising%20diffusion%20probabilistic%20models%20as%20the%20pre-trained%20reference%20dynamics%2C%0Aour%20approach%20integrates%20linear%20dynamics%20control%20with%20Kullback-Leibler%0Aregularization.%20We%20establish%20the%20well-posedness%20and%20regularity%20of%20the%0Astochastic%20control%20problem%20and%20develop%20a%20policy%20iteration%20algorithm%20%28PI-FT%29%20for%0Anumerical%20solution.%20We%20show%20that%20PI-FT%20achieves%20global%20convergence%20at%20a%20linear%0Arate.%20Unlike%20existing%20work%20that%20assumes%20regularities%20throughout%20training%2C%20we%0Aprove%20that%20the%20control%20and%20value%20sequences%20generated%20by%20the%20algorithm%20maintain%0Athe%20regularity.%20Additionally%2C%20we%20explore%20extensions%20of%20our%20framework%20to%0Aparametric%20settings%20and%20continuous-time%20formulations%2C%20and%20demonstrate%20the%0Apractical%20effectiveness%20of%20the%20proposed%20PI-FT%20algorithm%20through%20numerical%0Aexperiments.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yinbinhan/fine-tuning-of-diffusion-models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18164v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Control%2520for%2520Fine-tuning%2520Diffusion%2520Models%253A%2520Optimality%252C%250A%2520%2520Regularity%252C%2520and%2520Convergence%26entry.906535625%3DYinbin%2520Han%2520and%2520Meisam%2520Razaviyayn%2520and%2520Renyuan%2520Xu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520generative%2520modeling%252C%250Ademonstrating%2520exceptional%2520capability%2520in%2520capturing%2520target%2520data%2520distributions%250Afrom%2520large%2520datasets.%2520However%252C%2520fine-tuning%2520these%2520massive%2520models%2520for%2520specific%250Adownstream%2520tasks%252C%2520constraints%252C%2520and%2520human%2520preferences%2520remains%2520a%2520critical%250Achallenge.%2520While%2520recent%2520advances%2520have%2520leveraged%2520reinforcement%2520learning%250Aalgorithms%2520to%2520tackle%2520this%2520problem%252C%2520much%2520of%2520the%2520progress%2520has%2520been%2520empirical%252C%250Awith%2520limited%2520theoretical%2520understanding.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%250Astochastic%2520control%2520framework%2520for%2520fine-tuning%2520diffusion%2520models.%2520Building%2520on%250Adenoising%2520diffusion%2520probabilistic%2520models%2520as%2520the%2520pre-trained%2520reference%2520dynamics%252C%250Aour%2520approach%2520integrates%2520linear%2520dynamics%2520control%2520with%2520Kullback-Leibler%250Aregularization.%2520We%2520establish%2520the%2520well-posedness%2520and%2520regularity%2520of%2520the%250Astochastic%2520control%2520problem%2520and%2520develop%2520a%2520policy%2520iteration%2520algorithm%2520%2528PI-FT%2529%2520for%250Anumerical%2520solution.%2520We%2520show%2520that%2520PI-FT%2520achieves%2520global%2520convergence%2520at%2520a%2520linear%250Arate.%2520Unlike%2520existing%2520work%2520that%2520assumes%2520regularities%2520throughout%2520training%252C%2520we%250Aprove%2520that%2520the%2520control%2520and%2520value%2520sequences%2520generated%2520by%2520the%2520algorithm%2520maintain%250Athe%2520regularity.%2520Additionally%252C%2520we%2520explore%2520extensions%2520of%2520our%2520framework%2520to%250Aparametric%2520settings%2520and%2520continuous-time%2520formulations%252C%2520and%2520demonstrate%2520the%250Apractical%2520effectiveness%2520of%2520the%2520proposed%2520PI-FT%2520algorithm%2520through%2520numerical%250Aexperiments.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/yinbinhan/fine-tuning-of-diffusion-models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18164v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Control%20for%20Fine-tuning%20Diffusion%20Models%3A%20Optimality%2C%0A%20%20Regularity%2C%20and%20Convergence&entry.906535625=Yinbin%20Han%20and%20Meisam%20Razaviyayn%20and%20Renyuan%20Xu&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20powerful%20tools%20for%20generative%20modeling%2C%0Ademonstrating%20exceptional%20capability%20in%20capturing%20target%20data%20distributions%0Afrom%20large%20datasets.%20However%2C%20fine-tuning%20these%20massive%20models%20for%20specific%0Adownstream%20tasks%2C%20constraints%2C%20and%20human%20preferences%20remains%20a%20critical%0Achallenge.%20While%20recent%20advances%20have%20leveraged%20reinforcement%20learning%0Aalgorithms%20to%20tackle%20this%20problem%2C%20much%20of%20the%20progress%20has%20been%20empirical%2C%0Awith%20limited%20theoretical%20understanding.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%0Astochastic%20control%20framework%20for%20fine-tuning%20diffusion%20models.%20Building%20on%0Adenoising%20diffusion%20probabilistic%20models%20as%20the%20pre-trained%20reference%20dynamics%2C%0Aour%20approach%20integrates%20linear%20dynamics%20control%20with%20Kullback-Leibler%0Aregularization.%20We%20establish%20the%20well-posedness%20and%20regularity%20of%20the%0Astochastic%20control%20problem%20and%20develop%20a%20policy%20iteration%20algorithm%20%28PI-FT%29%20for%0Anumerical%20solution.%20We%20show%20that%20PI-FT%20achieves%20global%20convergence%20at%20a%20linear%0Arate.%20Unlike%20existing%20work%20that%20assumes%20regularities%20throughout%20training%2C%20we%0Aprove%20that%20the%20control%20and%20value%20sequences%20generated%20by%20the%20algorithm%20maintain%0Athe%20regularity.%20Additionally%2C%20we%20explore%20extensions%20of%20our%20framework%20to%0Aparametric%20settings%20and%20continuous-time%20formulations%2C%20and%20demonstrate%20the%0Apractical%20effectiveness%20of%20the%20proposed%20PI-FT%20algorithm%20through%20numerical%0Aexperiments.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/yinbinhan/fine-tuning-of-diffusion-models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18164v4&entry.124074799=Read"},
{"title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time\n  Optimization", "author": "Jiazheng Xing and Hai Ci and Hongbin Xu and Hangjie Yuan and Yong Liu and Mike Zheng Shou", "abstract": "  Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks.\n", "link": "http://arxiv.org/abs/2508.21727v1", "date": "2025-08-29", "relevancy": 2.2489, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5685}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5629}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptMark%3A%20Robust%20Multi-bit%20Diffusion%20Watermarking%20via%20Inference%20Time%0A%20%20Optimization&body=Title%3A%20OptMark%3A%20Robust%20Multi-bit%20Diffusion%20Watermarking%20via%20Inference%20Time%0A%20%20Optimization%0AAuthor%3A%20Jiazheng%20Xing%20and%20Hai%20Ci%20and%20Hongbin%20Xu%20and%20Hangjie%20Yuan%20and%20Yong%20Liu%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Watermarking%20diffusion-generated%20images%20is%20crucial%20for%20copyright%20protection%0Aand%20user%20tracking.%20However%2C%20current%20diffusion%20watermarking%20methods%20face%0Asignificant%20limitations%3A%20zero-bit%20watermarking%20systems%20lack%20the%20capacity%20for%0Alarge-scale%20user%20tracking%2C%20while%20multi-bit%20methods%20are%20highly%20sensitive%20to%0Acertain%20image%20transformations%20or%20generative%20attacks%2C%20resulting%20in%20a%20lack%20of%0Acomprehensive%20robustness.%20In%20this%20paper%2C%20we%20propose%20OptMark%2C%20an%0Aoptimization-based%20approach%20that%20embeds%20a%20robust%20multi-bit%20watermark%20into%20the%0Aintermediate%20latents%20of%20the%20diffusion%20denoising%20process.%20OptMark%20strategically%0Ainserts%20a%20structural%20watermark%20early%20to%20resist%20generative%20attacks%20and%20a%20detail%0Awatermark%20late%20to%20withstand%20image%20transformations%2C%20with%20tailored%20regularization%0Aterms%20to%20preserve%20image%20quality%20and%20ensure%20imperceptibility.%20To%20address%20the%0Achallenge%20of%20memory%20consumption%20growing%20linearly%20with%20the%20number%20of%20denoising%0Asteps%20during%20optimization%2C%20OptMark%20incorporates%20adjoint%20gradient%20methods%2C%0Areducing%20memory%20usage%20from%20O%28N%29%20to%20O%281%29.%20Experimental%20results%20demonstrate%20that%0AOptMark%20achieves%20invisible%20multi-bit%20watermarking%20while%20ensuring%20robust%0Aresilience%20against%20valuemetric%20transformations%2C%20geometric%20transformations%2C%0Aediting%2C%20and%20regeneration%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptMark%253A%2520Robust%2520Multi-bit%2520Diffusion%2520Watermarking%2520via%2520Inference%2520Time%250A%2520%2520Optimization%26entry.906535625%3DJiazheng%2520Xing%2520and%2520Hai%2520Ci%2520and%2520Hongbin%2520Xu%2520and%2520Hangjie%2520Yuan%2520and%2520Yong%2520Liu%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Watermarking%2520diffusion-generated%2520images%2520is%2520crucial%2520for%2520copyright%2520protection%250Aand%2520user%2520tracking.%2520However%252C%2520current%2520diffusion%2520watermarking%2520methods%2520face%250Asignificant%2520limitations%253A%2520zero-bit%2520watermarking%2520systems%2520lack%2520the%2520capacity%2520for%250Alarge-scale%2520user%2520tracking%252C%2520while%2520multi-bit%2520methods%2520are%2520highly%2520sensitive%2520to%250Acertain%2520image%2520transformations%2520or%2520generative%2520attacks%252C%2520resulting%2520in%2520a%2520lack%2520of%250Acomprehensive%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OptMark%252C%2520an%250Aoptimization-based%2520approach%2520that%2520embeds%2520a%2520robust%2520multi-bit%2520watermark%2520into%2520the%250Aintermediate%2520latents%2520of%2520the%2520diffusion%2520denoising%2520process.%2520OptMark%2520strategically%250Ainserts%2520a%2520structural%2520watermark%2520early%2520to%2520resist%2520generative%2520attacks%2520and%2520a%2520detail%250Awatermark%2520late%2520to%2520withstand%2520image%2520transformations%252C%2520with%2520tailored%2520regularization%250Aterms%2520to%2520preserve%2520image%2520quality%2520and%2520ensure%2520imperceptibility.%2520To%2520address%2520the%250Achallenge%2520of%2520memory%2520consumption%2520growing%2520linearly%2520with%2520the%2520number%2520of%2520denoising%250Asteps%2520during%2520optimization%252C%2520OptMark%2520incorporates%2520adjoint%2520gradient%2520methods%252C%250Areducing%2520memory%2520usage%2520from%2520O%2528N%2529%2520to%2520O%25281%2529.%2520Experimental%2520results%2520demonstrate%2520that%250AOptMark%2520achieves%2520invisible%2520multi-bit%2520watermarking%2520while%2520ensuring%2520robust%250Aresilience%2520against%2520valuemetric%2520transformations%252C%2520geometric%2520transformations%252C%250Aediting%252C%2520and%2520regeneration%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptMark%3A%20Robust%20Multi-bit%20Diffusion%20Watermarking%20via%20Inference%20Time%0A%20%20Optimization&entry.906535625=Jiazheng%20Xing%20and%20Hai%20Ci%20and%20Hongbin%20Xu%20and%20Hangjie%20Yuan%20and%20Yong%20Liu%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Watermarking%20diffusion-generated%20images%20is%20crucial%20for%20copyright%20protection%0Aand%20user%20tracking.%20However%2C%20current%20diffusion%20watermarking%20methods%20face%0Asignificant%20limitations%3A%20zero-bit%20watermarking%20systems%20lack%20the%20capacity%20for%0Alarge-scale%20user%20tracking%2C%20while%20multi-bit%20methods%20are%20highly%20sensitive%20to%0Acertain%20image%20transformations%20or%20generative%20attacks%2C%20resulting%20in%20a%20lack%20of%0Acomprehensive%20robustness.%20In%20this%20paper%2C%20we%20propose%20OptMark%2C%20an%0Aoptimization-based%20approach%20that%20embeds%20a%20robust%20multi-bit%20watermark%20into%20the%0Aintermediate%20latents%20of%20the%20diffusion%20denoising%20process.%20OptMark%20strategically%0Ainserts%20a%20structural%20watermark%20early%20to%20resist%20generative%20attacks%20and%20a%20detail%0Awatermark%20late%20to%20withstand%20image%20transformations%2C%20with%20tailored%20regularization%0Aterms%20to%20preserve%20image%20quality%20and%20ensure%20imperceptibility.%20To%20address%20the%0Achallenge%20of%20memory%20consumption%20growing%20linearly%20with%20the%20number%20of%20denoising%0Asteps%20during%20optimization%2C%20OptMark%20incorporates%20adjoint%20gradient%20methods%2C%0Areducing%20memory%20usage%20from%20O%28N%29%20to%20O%281%29.%20Experimental%20results%20demonstrate%20that%0AOptMark%20achieves%20invisible%20multi-bit%20watermarking%20while%20ensuring%20robust%0Aresilience%20against%20valuemetric%20transformations%2C%20geometric%20transformations%2C%0Aediting%2C%20and%20regeneration%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21727v1&entry.124074799=Read"},
{"title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single\n  Positive Multi-Label Learning", "author": "Yiming Lin and Yuchen Niu and Shang Wang and Kaizhu Huang and Qiufeng Wang and Xiao-Bo Jin", "abstract": "  Context recognition (SR) is a fundamental task in computer vision that aims\nto extract structured semantic summaries from images by identifying key events\nand their associated entities. Specifically, given an input image, the model\nmust first classify the main visual events (verb classification), then identify\nthe participating entities and their semantic roles (semantic role labeling),\nand finally localize these entities in the image (semantic role localization).\nExisting methods treat verb classification as a single-label problem, but we\nshow through a comprehensive analysis that this formulation fails to address\nthe inherent ambiguity in visual event recognition, as multiple verb categories\nmay reasonably describe the same image. This paper makes three key\ncontributions: First, we reveal through empirical analysis that verb\nclassification is inherently a multi-label problem due to the ubiquitous\nsemantic overlap between verb categories. Second, given the impracticality of\nfully annotating large-scale datasets with multiple labels, we propose to\nreformulate verb classification as a single positive multi-label learning\n(SPMLL) problem - a novel perspective in SR research. Third, we design a\ncomprehensive multi-label evaluation benchmark for SR that is carefully\ndesigned to fairly evaluate model performance in a multi-label setting. To\naddress the challenges of SPMLL, we futher develop the Graph Enhanced Verb\nMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks to\ncapture label correlations and adversarial training to optimize decision\nboundaries. Extensive experiments on real-world datasets show that our approach\nachieves more than 3\\% MAP improvement while remaining competitive on\ntraditional top-1 and top-5 accuracy metrics.\n", "link": "http://arxiv.org/abs/2508.21816v1", "date": "2025-08-29", "relevancy": 2.23, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5563}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Demon%20is%20in%20Ambiguity%3A%20Revisiting%20Situation%20Recognition%20with%20Single%0A%20%20Positive%20Multi-Label%20Learning&body=Title%3A%20The%20Demon%20is%20in%20Ambiguity%3A%20Revisiting%20Situation%20Recognition%20with%20Single%0A%20%20Positive%20Multi-Label%20Learning%0AAuthor%3A%20Yiming%20Lin%20and%20Yuchen%20Niu%20and%20Shang%20Wang%20and%20Kaizhu%20Huang%20and%20Qiufeng%20Wang%20and%20Xiao-Bo%20Jin%0AAbstract%3A%20%20%20Context%20recognition%20%28SR%29%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20aims%0Ato%20extract%20structured%20semantic%20summaries%20from%20images%20by%20identifying%20key%20events%0Aand%20their%20associated%20entities.%20Specifically%2C%20given%20an%20input%20image%2C%20the%20model%0Amust%20first%20classify%20the%20main%20visual%20events%20%28verb%20classification%29%2C%20then%20identify%0Athe%20participating%20entities%20and%20their%20semantic%20roles%20%28semantic%20role%20labeling%29%2C%0Aand%20finally%20localize%20these%20entities%20in%20the%20image%20%28semantic%20role%20localization%29.%0AExisting%20methods%20treat%20verb%20classification%20as%20a%20single-label%20problem%2C%20but%20we%0Ashow%20through%20a%20comprehensive%20analysis%20that%20this%20formulation%20fails%20to%20address%0Athe%20inherent%20ambiguity%20in%20visual%20event%20recognition%2C%20as%20multiple%20verb%20categories%0Amay%20reasonably%20describe%20the%20same%20image.%20This%20paper%20makes%20three%20key%0Acontributions%3A%20First%2C%20we%20reveal%20through%20empirical%20analysis%20that%20verb%0Aclassification%20is%20inherently%20a%20multi-label%20problem%20due%20to%20the%20ubiquitous%0Asemantic%20overlap%20between%20verb%20categories.%20Second%2C%20given%20the%20impracticality%20of%0Afully%20annotating%20large-scale%20datasets%20with%20multiple%20labels%2C%20we%20propose%20to%0Areformulate%20verb%20classification%20as%20a%20single%20positive%20multi-label%20learning%0A%28SPMLL%29%20problem%20-%20a%20novel%20perspective%20in%20SR%20research.%20Third%2C%20we%20design%20a%0Acomprehensive%20multi-label%20evaluation%20benchmark%20for%20SR%20that%20is%20carefully%0Adesigned%20to%20fairly%20evaluate%20model%20performance%20in%20a%20multi-label%20setting.%20To%0Aaddress%20the%20challenges%20of%20SPMLL%2C%20we%20futher%20develop%20the%20Graph%20Enhanced%20Verb%0AMultilayer%20Perceptron%20%28GE-VerbMLP%29%2C%20which%20combines%20graph%20neural%20networks%20to%0Acapture%20label%20correlations%20and%20adversarial%20training%20to%20optimize%20decision%0Aboundaries.%20Extensive%20experiments%20on%20real-world%20datasets%20show%20that%20our%20approach%0Aachieves%20more%20than%203%5C%25%20MAP%20improvement%20while%20remaining%20competitive%20on%0Atraditional%20top-1%20and%20top-5%20accuracy%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Demon%2520is%2520in%2520Ambiguity%253A%2520Revisiting%2520Situation%2520Recognition%2520with%2520Single%250A%2520%2520Positive%2520Multi-Label%2520Learning%26entry.906535625%3DYiming%2520Lin%2520and%2520Yuchen%2520Niu%2520and%2520Shang%2520Wang%2520and%2520Kaizhu%2520Huang%2520and%2520Qiufeng%2520Wang%2520and%2520Xiao-Bo%2520Jin%26entry.1292438233%3D%2520%2520Context%2520recognition%2520%2528SR%2529%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%2520that%2520aims%250Ato%2520extract%2520structured%2520semantic%2520summaries%2520from%2520images%2520by%2520identifying%2520key%2520events%250Aand%2520their%2520associated%2520entities.%2520Specifically%252C%2520given%2520an%2520input%2520image%252C%2520the%2520model%250Amust%2520first%2520classify%2520the%2520main%2520visual%2520events%2520%2528verb%2520classification%2529%252C%2520then%2520identify%250Athe%2520participating%2520entities%2520and%2520their%2520semantic%2520roles%2520%2528semantic%2520role%2520labeling%2529%252C%250Aand%2520finally%2520localize%2520these%2520entities%2520in%2520the%2520image%2520%2528semantic%2520role%2520localization%2529.%250AExisting%2520methods%2520treat%2520verb%2520classification%2520as%2520a%2520single-label%2520problem%252C%2520but%2520we%250Ashow%2520through%2520a%2520comprehensive%2520analysis%2520that%2520this%2520formulation%2520fails%2520to%2520address%250Athe%2520inherent%2520ambiguity%2520in%2520visual%2520event%2520recognition%252C%2520as%2520multiple%2520verb%2520categories%250Amay%2520reasonably%2520describe%2520the%2520same%2520image.%2520This%2520paper%2520makes%2520three%2520key%250Acontributions%253A%2520First%252C%2520we%2520reveal%2520through%2520empirical%2520analysis%2520that%2520verb%250Aclassification%2520is%2520inherently%2520a%2520multi-label%2520problem%2520due%2520to%2520the%2520ubiquitous%250Asemantic%2520overlap%2520between%2520verb%2520categories.%2520Second%252C%2520given%2520the%2520impracticality%2520of%250Afully%2520annotating%2520large-scale%2520datasets%2520with%2520multiple%2520labels%252C%2520we%2520propose%2520to%250Areformulate%2520verb%2520classification%2520as%2520a%2520single%2520positive%2520multi-label%2520learning%250A%2528SPMLL%2529%2520problem%2520-%2520a%2520novel%2520perspective%2520in%2520SR%2520research.%2520Third%252C%2520we%2520design%2520a%250Acomprehensive%2520multi-label%2520evaluation%2520benchmark%2520for%2520SR%2520that%2520is%2520carefully%250Adesigned%2520to%2520fairly%2520evaluate%2520model%2520performance%2520in%2520a%2520multi-label%2520setting.%2520To%250Aaddress%2520the%2520challenges%2520of%2520SPMLL%252C%2520we%2520futher%2520develop%2520the%2520Graph%2520Enhanced%2520Verb%250AMultilayer%2520Perceptron%2520%2528GE-VerbMLP%2529%252C%2520which%2520combines%2520graph%2520neural%2520networks%2520to%250Acapture%2520label%2520correlations%2520and%2520adversarial%2520training%2520to%2520optimize%2520decision%250Aboundaries.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520our%2520approach%250Aachieves%2520more%2520than%25203%255C%2525%2520MAP%2520improvement%2520while%2520remaining%2520competitive%2520on%250Atraditional%2520top-1%2520and%2520top-5%2520accuracy%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Demon%20is%20in%20Ambiguity%3A%20Revisiting%20Situation%20Recognition%20with%20Single%0A%20%20Positive%20Multi-Label%20Learning&entry.906535625=Yiming%20Lin%20and%20Yuchen%20Niu%20and%20Shang%20Wang%20and%20Kaizhu%20Huang%20and%20Qiufeng%20Wang%20and%20Xiao-Bo%20Jin&entry.1292438233=%20%20Context%20recognition%20%28SR%29%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20aims%0Ato%20extract%20structured%20semantic%20summaries%20from%20images%20by%20identifying%20key%20events%0Aand%20their%20associated%20entities.%20Specifically%2C%20given%20an%20input%20image%2C%20the%20model%0Amust%20first%20classify%20the%20main%20visual%20events%20%28verb%20classification%29%2C%20then%20identify%0Athe%20participating%20entities%20and%20their%20semantic%20roles%20%28semantic%20role%20labeling%29%2C%0Aand%20finally%20localize%20these%20entities%20in%20the%20image%20%28semantic%20role%20localization%29.%0AExisting%20methods%20treat%20verb%20classification%20as%20a%20single-label%20problem%2C%20but%20we%0Ashow%20through%20a%20comprehensive%20analysis%20that%20this%20formulation%20fails%20to%20address%0Athe%20inherent%20ambiguity%20in%20visual%20event%20recognition%2C%20as%20multiple%20verb%20categories%0Amay%20reasonably%20describe%20the%20same%20image.%20This%20paper%20makes%20three%20key%0Acontributions%3A%20First%2C%20we%20reveal%20through%20empirical%20analysis%20that%20verb%0Aclassification%20is%20inherently%20a%20multi-label%20problem%20due%20to%20the%20ubiquitous%0Asemantic%20overlap%20between%20verb%20categories.%20Second%2C%20given%20the%20impracticality%20of%0Afully%20annotating%20large-scale%20datasets%20with%20multiple%20labels%2C%20we%20propose%20to%0Areformulate%20verb%20classification%20as%20a%20single%20positive%20multi-label%20learning%0A%28SPMLL%29%20problem%20-%20a%20novel%20perspective%20in%20SR%20research.%20Third%2C%20we%20design%20a%0Acomprehensive%20multi-label%20evaluation%20benchmark%20for%20SR%20that%20is%20carefully%0Adesigned%20to%20fairly%20evaluate%20model%20performance%20in%20a%20multi-label%20setting.%20To%0Aaddress%20the%20challenges%20of%20SPMLL%2C%20we%20futher%20develop%20the%20Graph%20Enhanced%20Verb%0AMultilayer%20Perceptron%20%28GE-VerbMLP%29%2C%20which%20combines%20graph%20neural%20networks%20to%0Acapture%20label%20correlations%20and%20adversarial%20training%20to%20optimize%20decision%0Aboundaries.%20Extensive%20experiments%20on%20real-world%20datasets%20show%20that%20our%20approach%0Aachieves%20more%20than%203%5C%25%20MAP%20improvement%20while%20remaining%20competitive%20on%0Atraditional%20top-1%20and%20top-5%20accuracy%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21816v1&entry.124074799=Read"},
{"title": "UItron: Foundational GUI Agent with Advanced Perception and Planning", "author": "Zhixiong Zeng and Jing Huang and Liming Zheng and Wenkang Han and Yufeng Zhong and Lei Chen and Longrong Yang and Yingjie Chu and Yuzhi He and Lin Ma", "abstract": "  GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.\n", "link": "http://arxiv.org/abs/2508.21767v1", "date": "2025-08-29", "relevancy": 2.2219, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5707}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.553}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UItron%3A%20Foundational%20GUI%20Agent%20with%20Advanced%20Perception%20and%20Planning&body=Title%3A%20UItron%3A%20Foundational%20GUI%20Agent%20with%20Advanced%20Perception%20and%20Planning%0AAuthor%3A%20Zhixiong%20Zeng%20and%20Jing%20Huang%20and%20Liming%20Zheng%20and%20Wenkang%20Han%20and%20Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Longrong%20Yang%20and%20Yingjie%20Chu%20and%20Yuzhi%20He%20and%20Lin%20Ma%0AAbstract%3A%20%20%20GUI%20agent%20aims%20to%20enable%20automated%20operations%20on%20Mobile/PC%20devices%2C%20which%20is%0Aan%20important%20task%20toward%20achieving%20artificial%20general%20intelligence.%20The%20rapid%0Aadvancement%20of%20VLMs%20accelerates%20the%20development%20of%20GUI%20agents%2C%20owing%20to%20their%0Apowerful%20capabilities%20in%20visual%20understanding%20and%20task%20planning.%20However%2C%0Abuilding%20a%20GUI%20agent%20remains%20a%20challenging%20task%20due%20to%20the%20scarcity%20of%0Aoperation%20trajectories%2C%20the%20availability%20of%20interactive%20infrastructure%2C%20and%20the%0Alimitation%20of%20initial%20capabilities%20in%20foundation%20models.%20In%20this%20work%2C%20we%0Aintroduce%20UItron%2C%20an%20open-source%20foundational%20model%20for%20automatic%20GUI%20agents%2C%0Afeaturing%20advanced%20GUI%20perception%2C%20grounding%2C%20and%20planning%20capabilities.%20UItron%0Ahighlights%20the%20necessity%20of%20systemic%20data%20engineering%20and%20interactive%0Ainfrastructure%20as%20foundational%20components%20for%20advancing%20GUI%20agent%20development.%0AIt%20not%20only%20systematically%20studies%20a%20series%20of%20data%20engineering%20strategies%20to%0Aenhance%20training%20effects%2C%20but%20also%20establishes%20an%20interactive%20environment%0Aconnecting%20both%20Mobile%20and%20PC%20devices.%20In%20training%2C%20UItron%20adopts%20supervised%0Afinetuning%20over%20perception%20and%20planning%20tasks%20in%20various%20GUI%20scenarios%2C%20and%0Athen%20develop%20a%20curriculum%20reinforcement%20learning%20framework%20to%20enable%20complex%0Areasoning%20and%20exploration%20for%20online%20environments.%20As%20a%20result%2C%20UItron%20achieves%0Asuperior%20performance%20in%20benchmarks%20of%20GUI%20perception%2C%20grounding%2C%20and%20planning.%0AIn%20particular%2C%20UItron%20highlights%20the%20interaction%20proficiency%20with%20top-tier%0AChinese%20mobile%20APPs%2C%20as%20we%20identified%20a%20general%20lack%20of%20Chinese%20capabilities%0Aeven%20in%20state-of-the-art%20solutions.%20To%20this%20end%2C%20we%20manually%20collect%20over%20one%0Amillion%20steps%20of%20operation%20trajectories%20across%20the%20top%20100%20most%20popular%20apps%2C%0Aand%20build%20the%20offline%20and%20online%20agent%20evaluation%20environments.%20Experimental%0Aresults%20demonstrate%20that%20UItron%20achieves%20significant%20progress%20in%20Chinese%20app%0Ascenarios%2C%20propelling%20GUI%20agents%20one%20step%20closer%20to%20real-world%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUItron%253A%2520Foundational%2520GUI%2520Agent%2520with%2520Advanced%2520Perception%2520and%2520Planning%26entry.906535625%3DZhixiong%2520Zeng%2520and%2520Jing%2520Huang%2520and%2520Liming%2520Zheng%2520and%2520Wenkang%2520Han%2520and%2520Yufeng%2520Zhong%2520and%2520Lei%2520Chen%2520and%2520Longrong%2520Yang%2520and%2520Yingjie%2520Chu%2520and%2520Yuzhi%2520He%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520GUI%2520agent%2520aims%2520to%2520enable%2520automated%2520operations%2520on%2520Mobile/PC%2520devices%252C%2520which%2520is%250Aan%2520important%2520task%2520toward%2520achieving%2520artificial%2520general%2520intelligence.%2520The%2520rapid%250Aadvancement%2520of%2520VLMs%2520accelerates%2520the%2520development%2520of%2520GUI%2520agents%252C%2520owing%2520to%2520their%250Apowerful%2520capabilities%2520in%2520visual%2520understanding%2520and%2520task%2520planning.%2520However%252C%250Abuilding%2520a%2520GUI%2520agent%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520scarcity%2520of%250Aoperation%2520trajectories%252C%2520the%2520availability%2520of%2520interactive%2520infrastructure%252C%2520and%2520the%250Alimitation%2520of%2520initial%2520capabilities%2520in%2520foundation%2520models.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520UItron%252C%2520an%2520open-source%2520foundational%2520model%2520for%2520automatic%2520GUI%2520agents%252C%250Afeaturing%2520advanced%2520GUI%2520perception%252C%2520grounding%252C%2520and%2520planning%2520capabilities.%2520UItron%250Ahighlights%2520the%2520necessity%2520of%2520systemic%2520data%2520engineering%2520and%2520interactive%250Ainfrastructure%2520as%2520foundational%2520components%2520for%2520advancing%2520GUI%2520agent%2520development.%250AIt%2520not%2520only%2520systematically%2520studies%2520a%2520series%2520of%2520data%2520engineering%2520strategies%2520to%250Aenhance%2520training%2520effects%252C%2520but%2520also%2520establishes%2520an%2520interactive%2520environment%250Aconnecting%2520both%2520Mobile%2520and%2520PC%2520devices.%2520In%2520training%252C%2520UItron%2520adopts%2520supervised%250Afinetuning%2520over%2520perception%2520and%2520planning%2520tasks%2520in%2520various%2520GUI%2520scenarios%252C%2520and%250Athen%2520develop%2520a%2520curriculum%2520reinforcement%2520learning%2520framework%2520to%2520enable%2520complex%250Areasoning%2520and%2520exploration%2520for%2520online%2520environments.%2520As%2520a%2520result%252C%2520UItron%2520achieves%250Asuperior%2520performance%2520in%2520benchmarks%2520of%2520GUI%2520perception%252C%2520grounding%252C%2520and%2520planning.%250AIn%2520particular%252C%2520UItron%2520highlights%2520the%2520interaction%2520proficiency%2520with%2520top-tier%250AChinese%2520mobile%2520APPs%252C%2520as%2520we%2520identified%2520a%2520general%2520lack%2520of%2520Chinese%2520capabilities%250Aeven%2520in%2520state-of-the-art%2520solutions.%2520To%2520this%2520end%252C%2520we%2520manually%2520collect%2520over%2520one%250Amillion%2520steps%2520of%2520operation%2520trajectories%2520across%2520the%2520top%2520100%2520most%2520popular%2520apps%252C%250Aand%2520build%2520the%2520offline%2520and%2520online%2520agent%2520evaluation%2520environments.%2520Experimental%250Aresults%2520demonstrate%2520that%2520UItron%2520achieves%2520significant%2520progress%2520in%2520Chinese%2520app%250Ascenarios%252C%2520propelling%2520GUI%2520agents%2520one%2520step%2520closer%2520to%2520real-world%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UItron%3A%20Foundational%20GUI%20Agent%20with%20Advanced%20Perception%20and%20Planning&entry.906535625=Zhixiong%20Zeng%20and%20Jing%20Huang%20and%20Liming%20Zheng%20and%20Wenkang%20Han%20and%20Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Longrong%20Yang%20and%20Yingjie%20Chu%20and%20Yuzhi%20He%20and%20Lin%20Ma&entry.1292438233=%20%20GUI%20agent%20aims%20to%20enable%20automated%20operations%20on%20Mobile/PC%20devices%2C%20which%20is%0Aan%20important%20task%20toward%20achieving%20artificial%20general%20intelligence.%20The%20rapid%0Aadvancement%20of%20VLMs%20accelerates%20the%20development%20of%20GUI%20agents%2C%20owing%20to%20their%0Apowerful%20capabilities%20in%20visual%20understanding%20and%20task%20planning.%20However%2C%0Abuilding%20a%20GUI%20agent%20remains%20a%20challenging%20task%20due%20to%20the%20scarcity%20of%0Aoperation%20trajectories%2C%20the%20availability%20of%20interactive%20infrastructure%2C%20and%20the%0Alimitation%20of%20initial%20capabilities%20in%20foundation%20models.%20In%20this%20work%2C%20we%0Aintroduce%20UItron%2C%20an%20open-source%20foundational%20model%20for%20automatic%20GUI%20agents%2C%0Afeaturing%20advanced%20GUI%20perception%2C%20grounding%2C%20and%20planning%20capabilities.%20UItron%0Ahighlights%20the%20necessity%20of%20systemic%20data%20engineering%20and%20interactive%0Ainfrastructure%20as%20foundational%20components%20for%20advancing%20GUI%20agent%20development.%0AIt%20not%20only%20systematically%20studies%20a%20series%20of%20data%20engineering%20strategies%20to%0Aenhance%20training%20effects%2C%20but%20also%20establishes%20an%20interactive%20environment%0Aconnecting%20both%20Mobile%20and%20PC%20devices.%20In%20training%2C%20UItron%20adopts%20supervised%0Afinetuning%20over%20perception%20and%20planning%20tasks%20in%20various%20GUI%20scenarios%2C%20and%0Athen%20develop%20a%20curriculum%20reinforcement%20learning%20framework%20to%20enable%20complex%0Areasoning%20and%20exploration%20for%20online%20environments.%20As%20a%20result%2C%20UItron%20achieves%0Asuperior%20performance%20in%20benchmarks%20of%20GUI%20perception%2C%20grounding%2C%20and%20planning.%0AIn%20particular%2C%20UItron%20highlights%20the%20interaction%20proficiency%20with%20top-tier%0AChinese%20mobile%20APPs%2C%20as%20we%20identified%20a%20general%20lack%20of%20Chinese%20capabilities%0Aeven%20in%20state-of-the-art%20solutions.%20To%20this%20end%2C%20we%20manually%20collect%20over%20one%0Amillion%20steps%20of%20operation%20trajectories%20across%20the%20top%20100%20most%20popular%20apps%2C%0Aand%20build%20the%20offline%20and%20online%20agent%20evaluation%20environments.%20Experimental%0Aresults%20demonstrate%20that%20UItron%20achieves%20significant%20progress%20in%20Chinese%20app%0Ascenarios%2C%20propelling%20GUI%20agents%20one%20step%20closer%20to%20real-world%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21767v1&entry.124074799=Read"},
{"title": "MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal\n  Browsing Agents", "author": "Xijia Tao and Yihua Teng and Xinxing Su and Xinyu Fu and Jihao Wu and Chaofan Tao and Ziru Liu and Haoli Bai and Rui Liu and Lingpeng Kong", "abstract": "  Large multimodal language models (MLLMs) are increasingly deployed as web\nagents, yet many multimodal browsing benchmarks can be solved by shallow, fixed\nworkflows that lean on high-recall image search and nearby text-masking the\ngenuinely multimodal challenges of fine-grained visual reasoning, provenance\nverification, and long-horizon tool use. We introduce MMSearch-Plus, a\nbenchmark of 311 tasks that highly demand multimodal understanding while\npreserving the difficulty profile of strong text-only browsing suites. Each\nitem is constructed to contain multiple weak, localized visual signals that\nmust be extracted, propagated through iterative text-image search, and\ncross-validated under retrieval noise before answering. Our curation procedure,\nSpatial-Temporal Extrapolation, seeds questions whose answers require\nextrapolating from spatial cues (micro-text, part-level appearance, layouts,\nsignage) and temporal traces (broadcast overlays, seasonal context) to\nout-of-image facts such as events, dates, and venues. We provide a\nmodel-agnostic agent framework with browsing tools and evaluate a range of\nclosed and open MLLMs. The strongest agent (o3) attains 15.1% without search\nand 36.0% accuracy with rollout under our framework, while a strong open-source\nmodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20\nrounds of search. Beyond answer accuracy, we assess bounding-box production and\ncropped-image search, and conduct an error analysis that surfaces failures in\nsource verification, part-based reasoning, and long-horizon planning.\n", "link": "http://arxiv.org/abs/2508.21475v1", "date": "2025-08-29", "relevancy": 2.2208, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMSearch-Plus%3A%20A%20Simple%20Yet%20Challenging%20Benchmark%20for%20Multimodal%0A%20%20Browsing%20Agents&body=Title%3A%20MMSearch-Plus%3A%20A%20Simple%20Yet%20Challenging%20Benchmark%20for%20Multimodal%0A%20%20Browsing%20Agents%0AAuthor%3A%20Xijia%20Tao%20and%20Yihua%20Teng%20and%20Xinxing%20Su%20and%20Xinyu%20Fu%20and%20Jihao%20Wu%20and%20Chaofan%20Tao%20and%20Ziru%20Liu%20and%20Haoli%20Bai%20and%20Rui%20Liu%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20Large%20multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20deployed%20as%20web%0Aagents%2C%20yet%20many%20multimodal%20browsing%20benchmarks%20can%20be%20solved%20by%20shallow%2C%20fixed%0Aworkflows%20that%20lean%20on%20high-recall%20image%20search%20and%20nearby%20text-masking%20the%0Agenuinely%20multimodal%20challenges%20of%20fine-grained%20visual%20reasoning%2C%20provenance%0Averification%2C%20and%20long-horizon%20tool%20use.%20We%20introduce%20MMSearch-Plus%2C%20a%0Abenchmark%20of%20311%20tasks%20that%20highly%20demand%20multimodal%20understanding%20while%0Apreserving%20the%20difficulty%20profile%20of%20strong%20text-only%20browsing%20suites.%20Each%0Aitem%20is%20constructed%20to%20contain%20multiple%20weak%2C%20localized%20visual%20signals%20that%0Amust%20be%20extracted%2C%20propagated%20through%20iterative%20text-image%20search%2C%20and%0Across-validated%20under%20retrieval%20noise%20before%20answering.%20Our%20curation%20procedure%2C%0ASpatial-Temporal%20Extrapolation%2C%20seeds%20questions%20whose%20answers%20require%0Aextrapolating%20from%20spatial%20cues%20%28micro-text%2C%20part-level%20appearance%2C%20layouts%2C%0Asignage%29%20and%20temporal%20traces%20%28broadcast%20overlays%2C%20seasonal%20context%29%20to%0Aout-of-image%20facts%20such%20as%20events%2C%20dates%2C%20and%20venues.%20We%20provide%20a%0Amodel-agnostic%20agent%20framework%20with%20browsing%20tools%20and%20evaluate%20a%20range%20of%0Aclosed%20and%20open%20MLLMs.%20The%20strongest%20agent%20%28o3%29%20attains%2015.1%25%20without%20search%0Aand%2036.0%25%20accuracy%20with%20rollout%20under%20our%20framework%2C%20while%20a%20strong%20open-source%0Amodel%20%28Qwen-2.5-VL-72B-Instruct%29%20achieves%200.0%25%20without%20search%20and%206.9%25%20after%2020%0Arounds%20of%20search.%20Beyond%20answer%20accuracy%2C%20we%20assess%20bounding-box%20production%20and%0Acropped-image%20search%2C%20and%20conduct%20an%20error%20analysis%20that%20surfaces%20failures%20in%0Asource%20verification%2C%20part-based%20reasoning%2C%20and%20long-horizon%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMSearch-Plus%253A%2520A%2520Simple%2520Yet%2520Challenging%2520Benchmark%2520for%2520Multimodal%250A%2520%2520Browsing%2520Agents%26entry.906535625%3DXijia%2520Tao%2520and%2520Yihua%2520Teng%2520and%2520Xinxing%2520Su%2520and%2520Xinyu%2520Fu%2520and%2520Jihao%2520Wu%2520and%2520Chaofan%2520Tao%2520and%2520Ziru%2520Liu%2520and%2520Haoli%2520Bai%2520and%2520Rui%2520Liu%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520language%2520models%2520%2528MLLMs%2529%2520are%2520increasingly%2520deployed%2520as%2520web%250Aagents%252C%2520yet%2520many%2520multimodal%2520browsing%2520benchmarks%2520can%2520be%2520solved%2520by%2520shallow%252C%2520fixed%250Aworkflows%2520that%2520lean%2520on%2520high-recall%2520image%2520search%2520and%2520nearby%2520text-masking%2520the%250Agenuinely%2520multimodal%2520challenges%2520of%2520fine-grained%2520visual%2520reasoning%252C%2520provenance%250Averification%252C%2520and%2520long-horizon%2520tool%2520use.%2520We%2520introduce%2520MMSearch-Plus%252C%2520a%250Abenchmark%2520of%2520311%2520tasks%2520that%2520highly%2520demand%2520multimodal%2520understanding%2520while%250Apreserving%2520the%2520difficulty%2520profile%2520of%2520strong%2520text-only%2520browsing%2520suites.%2520Each%250Aitem%2520is%2520constructed%2520to%2520contain%2520multiple%2520weak%252C%2520localized%2520visual%2520signals%2520that%250Amust%2520be%2520extracted%252C%2520propagated%2520through%2520iterative%2520text-image%2520search%252C%2520and%250Across-validated%2520under%2520retrieval%2520noise%2520before%2520answering.%2520Our%2520curation%2520procedure%252C%250ASpatial-Temporal%2520Extrapolation%252C%2520seeds%2520questions%2520whose%2520answers%2520require%250Aextrapolating%2520from%2520spatial%2520cues%2520%2528micro-text%252C%2520part-level%2520appearance%252C%2520layouts%252C%250Asignage%2529%2520and%2520temporal%2520traces%2520%2528broadcast%2520overlays%252C%2520seasonal%2520context%2529%2520to%250Aout-of-image%2520facts%2520such%2520as%2520events%252C%2520dates%252C%2520and%2520venues.%2520We%2520provide%2520a%250Amodel-agnostic%2520agent%2520framework%2520with%2520browsing%2520tools%2520and%2520evaluate%2520a%2520range%2520of%250Aclosed%2520and%2520open%2520MLLMs.%2520The%2520strongest%2520agent%2520%2528o3%2529%2520attains%252015.1%2525%2520without%2520search%250Aand%252036.0%2525%2520accuracy%2520with%2520rollout%2520under%2520our%2520framework%252C%2520while%2520a%2520strong%2520open-source%250Amodel%2520%2528Qwen-2.5-VL-72B-Instruct%2529%2520achieves%25200.0%2525%2520without%2520search%2520and%25206.9%2525%2520after%252020%250Arounds%2520of%2520search.%2520Beyond%2520answer%2520accuracy%252C%2520we%2520assess%2520bounding-box%2520production%2520and%250Acropped-image%2520search%252C%2520and%2520conduct%2520an%2520error%2520analysis%2520that%2520surfaces%2520failures%2520in%250Asource%2520verification%252C%2520part-based%2520reasoning%252C%2520and%2520long-horizon%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSearch-Plus%3A%20A%20Simple%20Yet%20Challenging%20Benchmark%20for%20Multimodal%0A%20%20Browsing%20Agents&entry.906535625=Xijia%20Tao%20and%20Yihua%20Teng%20and%20Xinxing%20Su%20and%20Xinyu%20Fu%20and%20Jihao%20Wu%20and%20Chaofan%20Tao%20and%20Ziru%20Liu%20and%20Haoli%20Bai%20and%20Rui%20Liu%20and%20Lingpeng%20Kong&entry.1292438233=%20%20Large%20multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20deployed%20as%20web%0Aagents%2C%20yet%20many%20multimodal%20browsing%20benchmarks%20can%20be%20solved%20by%20shallow%2C%20fixed%0Aworkflows%20that%20lean%20on%20high-recall%20image%20search%20and%20nearby%20text-masking%20the%0Agenuinely%20multimodal%20challenges%20of%20fine-grained%20visual%20reasoning%2C%20provenance%0Averification%2C%20and%20long-horizon%20tool%20use.%20We%20introduce%20MMSearch-Plus%2C%20a%0Abenchmark%20of%20311%20tasks%20that%20highly%20demand%20multimodal%20understanding%20while%0Apreserving%20the%20difficulty%20profile%20of%20strong%20text-only%20browsing%20suites.%20Each%0Aitem%20is%20constructed%20to%20contain%20multiple%20weak%2C%20localized%20visual%20signals%20that%0Amust%20be%20extracted%2C%20propagated%20through%20iterative%20text-image%20search%2C%20and%0Across-validated%20under%20retrieval%20noise%20before%20answering.%20Our%20curation%20procedure%2C%0ASpatial-Temporal%20Extrapolation%2C%20seeds%20questions%20whose%20answers%20require%0Aextrapolating%20from%20spatial%20cues%20%28micro-text%2C%20part-level%20appearance%2C%20layouts%2C%0Asignage%29%20and%20temporal%20traces%20%28broadcast%20overlays%2C%20seasonal%20context%29%20to%0Aout-of-image%20facts%20such%20as%20events%2C%20dates%2C%20and%20venues.%20We%20provide%20a%0Amodel-agnostic%20agent%20framework%20with%20browsing%20tools%20and%20evaluate%20a%20range%20of%0Aclosed%20and%20open%20MLLMs.%20The%20strongest%20agent%20%28o3%29%20attains%2015.1%25%20without%20search%0Aand%2036.0%25%20accuracy%20with%20rollout%20under%20our%20framework%2C%20while%20a%20strong%20open-source%0Amodel%20%28Qwen-2.5-VL-72B-Instruct%29%20achieves%200.0%25%20without%20search%20and%206.9%25%20after%2020%0Arounds%20of%20search.%20Beyond%20answer%20accuracy%2C%20we%20assess%20bounding-box%20production%20and%0Acropped-image%20search%2C%20and%20conduct%20an%20error%20analysis%20that%20surfaces%20failures%20in%0Asource%20verification%2C%20part-based%20reasoning%2C%20and%20long-horizon%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21475v1&entry.124074799=Read"},
{"title": "On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph\n  Ricci Curvature", "author": "Geri Skenderi", "abstract": "  Graph Neural Networks (GNNs) have recently shown promise as solvers for\nBoolean Satisfiability Problems (SATs) by operating on graph representations of\nlogical formulas. However, their performance degrades sharply on harder\ninstances, raising the question of whether this reflects fundamental\narchitectural limitations. In this work, we provide a geometric explanation\nthrough the lens of graph Ricci Curvature (RC), which quantifies local\nconnectivity bottlenecks. We prove that bipartite graphs derived from random\nk-SAT formulas are inherently negatively curved, and that this curvature\ndecreases with instance difficulty. Building on this, we show that GNN-based\nSAT solvers are affected by oversquashing, a phenomenon where long-range\ndependencies become impossible to compress into fixed-length representations.\nWe validate our claims empirically across different SAT benchmarks and confirm\nthat curvature is both a strong indicator of problem complexity and can be used\nto predict performance. Finally, we connect our findings to design principles\nof existing solvers and outline promising directions for future work.\n", "link": "http://arxiv.org/abs/2508.21513v1", "date": "2025-08-29", "relevancy": 2.1965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4564}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4377}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Hardness%20of%20Learning%20GNN-based%20SAT%20Solvers%3A%20The%20Role%20of%20Graph%0A%20%20Ricci%20Curvature&body=Title%3A%20On%20the%20Hardness%20of%20Learning%20GNN-based%20SAT%20Solvers%3A%20The%20Role%20of%20Graph%0A%20%20Ricci%20Curvature%0AAuthor%3A%20Geri%20Skenderi%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20shown%20promise%20as%20solvers%20for%0ABoolean%20Satisfiability%20Problems%20%28SATs%29%20by%20operating%20on%20graph%20representations%20of%0Alogical%20formulas.%20However%2C%20their%20performance%20degrades%20sharply%20on%20harder%0Ainstances%2C%20raising%20the%20question%20of%20whether%20this%20reflects%20fundamental%0Aarchitectural%20limitations.%20In%20this%20work%2C%20we%20provide%20a%20geometric%20explanation%0Athrough%20the%20lens%20of%20graph%20Ricci%20Curvature%20%28RC%29%2C%20which%20quantifies%20local%0Aconnectivity%20bottlenecks.%20We%20prove%20that%20bipartite%20graphs%20derived%20from%20random%0Ak-SAT%20formulas%20are%20inherently%20negatively%20curved%2C%20and%20that%20this%20curvature%0Adecreases%20with%20instance%20difficulty.%20Building%20on%20this%2C%20we%20show%20that%20GNN-based%0ASAT%20solvers%20are%20affected%20by%20oversquashing%2C%20a%20phenomenon%20where%20long-range%0Adependencies%20become%20impossible%20to%20compress%20into%20fixed-length%20representations.%0AWe%20validate%20our%20claims%20empirically%20across%20different%20SAT%20benchmarks%20and%20confirm%0Athat%20curvature%20is%20both%20a%20strong%20indicator%20of%20problem%20complexity%20and%20can%20be%20used%0Ato%20predict%20performance.%20Finally%2C%20we%20connect%20our%20findings%20to%20design%20principles%0Aof%20existing%20solvers%20and%20outline%20promising%20directions%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Hardness%2520of%2520Learning%2520GNN-based%2520SAT%2520Solvers%253A%2520The%2520Role%2520of%2520Graph%250A%2520%2520Ricci%2520Curvature%26entry.906535625%3DGeri%2520Skenderi%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520recently%2520shown%2520promise%2520as%2520solvers%2520for%250ABoolean%2520Satisfiability%2520Problems%2520%2528SATs%2529%2520by%2520operating%2520on%2520graph%2520representations%2520of%250Alogical%2520formulas.%2520However%252C%2520their%2520performance%2520degrades%2520sharply%2520on%2520harder%250Ainstances%252C%2520raising%2520the%2520question%2520of%2520whether%2520this%2520reflects%2520fundamental%250Aarchitectural%2520limitations.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520geometric%2520explanation%250Athrough%2520the%2520lens%2520of%2520graph%2520Ricci%2520Curvature%2520%2528RC%2529%252C%2520which%2520quantifies%2520local%250Aconnectivity%2520bottlenecks.%2520We%2520prove%2520that%2520bipartite%2520graphs%2520derived%2520from%2520random%250Ak-SAT%2520formulas%2520are%2520inherently%2520negatively%2520curved%252C%2520and%2520that%2520this%2520curvature%250Adecreases%2520with%2520instance%2520difficulty.%2520Building%2520on%2520this%252C%2520we%2520show%2520that%2520GNN-based%250ASAT%2520solvers%2520are%2520affected%2520by%2520oversquashing%252C%2520a%2520phenomenon%2520where%2520long-range%250Adependencies%2520become%2520impossible%2520to%2520compress%2520into%2520fixed-length%2520representations.%250AWe%2520validate%2520our%2520claims%2520empirically%2520across%2520different%2520SAT%2520benchmarks%2520and%2520confirm%250Athat%2520curvature%2520is%2520both%2520a%2520strong%2520indicator%2520of%2520problem%2520complexity%2520and%2520can%2520be%2520used%250Ato%2520predict%2520performance.%2520Finally%252C%2520we%2520connect%2520our%2520findings%2520to%2520design%2520principles%250Aof%2520existing%2520solvers%2520and%2520outline%2520promising%2520directions%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Hardness%20of%20Learning%20GNN-based%20SAT%20Solvers%3A%20The%20Role%20of%20Graph%0A%20%20Ricci%20Curvature&entry.906535625=Geri%20Skenderi&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20recently%20shown%20promise%20as%20solvers%20for%0ABoolean%20Satisfiability%20Problems%20%28SATs%29%20by%20operating%20on%20graph%20representations%20of%0Alogical%20formulas.%20However%2C%20their%20performance%20degrades%20sharply%20on%20harder%0Ainstances%2C%20raising%20the%20question%20of%20whether%20this%20reflects%20fundamental%0Aarchitectural%20limitations.%20In%20this%20work%2C%20we%20provide%20a%20geometric%20explanation%0Athrough%20the%20lens%20of%20graph%20Ricci%20Curvature%20%28RC%29%2C%20which%20quantifies%20local%0Aconnectivity%20bottlenecks.%20We%20prove%20that%20bipartite%20graphs%20derived%20from%20random%0Ak-SAT%20formulas%20are%20inherently%20negatively%20curved%2C%20and%20that%20this%20curvature%0Adecreases%20with%20instance%20difficulty.%20Building%20on%20this%2C%20we%20show%20that%20GNN-based%0ASAT%20solvers%20are%20affected%20by%20oversquashing%2C%20a%20phenomenon%20where%20long-range%0Adependencies%20become%20impossible%20to%20compress%20into%20fixed-length%20representations.%0AWe%20validate%20our%20claims%20empirically%20across%20different%20SAT%20benchmarks%20and%20confirm%0Athat%20curvature%20is%20both%20a%20strong%20indicator%20of%20problem%20complexity%20and%20can%20be%20used%0Ato%20predict%20performance.%20Finally%2C%20we%20connect%20our%20findings%20to%20design%20principles%0Aof%20existing%20solvers%20and%20outline%20promising%20directions%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21513v1&entry.124074799=Read"},
{"title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long\n  Video Understanding", "author": "Hao Lu and Jiahao Wang and Yaolun Zhang and Ruohui Wang and Xuanyu Zheng and Yepeng Tang and Dahua Lin and Lewei Lu", "abstract": "  Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.\n", "link": "http://arxiv.org/abs/2508.21496v1", "date": "2025-08-29", "relevancy": 2.1753, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELV-Halluc%3A%20Benchmarking%20Semantic%20Aggregation%20Hallucinations%20in%20Long%0A%20%20Video%20Understanding&body=Title%3A%20ELV-Halluc%3A%20Benchmarking%20Semantic%20Aggregation%20Hallucinations%20in%20Long%0A%20%20Video%20Understanding%0AAuthor%3A%20Hao%20Lu%20and%20Jiahao%20Wang%20and%20Yaolun%20Zhang%20and%20Ruohui%20Wang%20and%20Xuanyu%20Zheng%20and%20Yepeng%20Tang%20and%20Dahua%20Lin%20and%20Lewei%20Lu%0AAbstract%3A%20%20%20Video%20multimodal%20large%20language%20models%20%28Video-MLLMs%29%20have%20achieved%20remarkable%0Aprogress%20in%20video%20understanding.%20However%2C%20they%20remain%20vulnerable%20to%0Ahallucination-producing%20content%20inconsistent%20with%20or%20unrelated%20to%20video%20inputs.%0APrevious%20video%20hallucination%20benchmarks%20primarily%20focus%20on%20short-videos.%20They%0Aattribute%20hallucinations%20to%20factors%20such%20as%20strong%20language%20priors%2C%20missing%0Aframes%2C%20or%20vision-language%20biases%20introduced%20by%20the%20visual%20encoder.%20While%20these%0Acauses%20indeed%20account%20for%20most%20hallucinations%20in%20short%20videos%2C%20they%20still%0Aoversimplify%20the%20cause%20of%20hallucinations.%20Sometimes%2C%20models%20generate%20incorrect%0Aoutputs%20but%20with%20correct%20frame-level%20semantics.%20We%20refer%20to%20this%20type%20of%0Ahallucination%20as%20Semantic%20Aggregation%20Hallucination%20%28SAH%29%2C%20which%20arises%20during%0Athe%20process%20of%20aggregating%20frame-level%20semantics%20into%20event-level%20semantic%0Agroups.%20Given%20that%20SAH%20becomes%20particularly%20critical%20in%20long%20videos%20due%20to%0Aincreased%20semantic%20complexity%20across%20multiple%20events%2C%20it%20is%20essential%20to%0Aseparate%20and%20thoroughly%20investigate%20the%20causes%20of%20this%20type%20of%20hallucination.%0ATo%20address%20the%20above%20issues%2C%20we%20introduce%20ELV-Halluc%2C%20the%20first%20benchmark%0Adedicated%20to%20long-video%20hallucination%2C%20enabling%20a%20systematic%20investigation%20of%0ASAH.%20Our%20experiments%20confirm%20the%20existence%20of%20SAH%20and%20show%20that%20it%20increases%0Awith%20semantic%20complexity.%20Additionally%2C%20we%20find%20that%20models%20are%20more%20prone%20to%0ASAH%20on%20rapidly%20changing%20semantics.%20Moreover%2C%20we%20discuss%20potential%20approaches%20to%0Amitigate%20SAH.%20We%20demonstrate%20that%20positional%20encoding%20strategy%20contributes%20to%0Aalleviating%20SAH%2C%20and%20further%20adopt%20DPO%20strategy%20to%20enhance%20the%20model%27s%20ability%0Ato%20distinguish%20semantics%20within%20and%20across%20events.%20To%20support%20this%2C%20we%20curate%20a%0Adataset%20of%208K%20adversarial%20data%20pairs%20and%20achieve%20improvements%20on%20both%0AELV-Halluc%20and%20Video-MME%2C%20including%20a%20substantial%2027.7%25%20reduction%20in%20SAH%20ratio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELV-Halluc%253A%2520Benchmarking%2520Semantic%2520Aggregation%2520Hallucinations%2520in%2520Long%250A%2520%2520Video%2520Understanding%26entry.906535625%3DHao%2520Lu%2520and%2520Jiahao%2520Wang%2520and%2520Yaolun%2520Zhang%2520and%2520Ruohui%2520Wang%2520and%2520Xuanyu%2520Zheng%2520and%2520Yepeng%2520Tang%2520and%2520Dahua%2520Lin%2520and%2520Lewei%2520Lu%26entry.1292438233%3D%2520%2520Video%2520multimodal%2520large%2520language%2520models%2520%2528Video-MLLMs%2529%2520have%2520achieved%2520remarkable%250Aprogress%2520in%2520video%2520understanding.%2520However%252C%2520they%2520remain%2520vulnerable%2520to%250Ahallucination-producing%2520content%2520inconsistent%2520with%2520or%2520unrelated%2520to%2520video%2520inputs.%250APrevious%2520video%2520hallucination%2520benchmarks%2520primarily%2520focus%2520on%2520short-videos.%2520They%250Aattribute%2520hallucinations%2520to%2520factors%2520such%2520as%2520strong%2520language%2520priors%252C%2520missing%250Aframes%252C%2520or%2520vision-language%2520biases%2520introduced%2520by%2520the%2520visual%2520encoder.%2520While%2520these%250Acauses%2520indeed%2520account%2520for%2520most%2520hallucinations%2520in%2520short%2520videos%252C%2520they%2520still%250Aoversimplify%2520the%2520cause%2520of%2520hallucinations.%2520Sometimes%252C%2520models%2520generate%2520incorrect%250Aoutputs%2520but%2520with%2520correct%2520frame-level%2520semantics.%2520We%2520refer%2520to%2520this%2520type%2520of%250Ahallucination%2520as%2520Semantic%2520Aggregation%2520Hallucination%2520%2528SAH%2529%252C%2520which%2520arises%2520during%250Athe%2520process%2520of%2520aggregating%2520frame-level%2520semantics%2520into%2520event-level%2520semantic%250Agroups.%2520Given%2520that%2520SAH%2520becomes%2520particularly%2520critical%2520in%2520long%2520videos%2520due%2520to%250Aincreased%2520semantic%2520complexity%2520across%2520multiple%2520events%252C%2520it%2520is%2520essential%2520to%250Aseparate%2520and%2520thoroughly%2520investigate%2520the%2520causes%2520of%2520this%2520type%2520of%2520hallucination.%250ATo%2520address%2520the%2520above%2520issues%252C%2520we%2520introduce%2520ELV-Halluc%252C%2520the%2520first%2520benchmark%250Adedicated%2520to%2520long-video%2520hallucination%252C%2520enabling%2520a%2520systematic%2520investigation%2520of%250ASAH.%2520Our%2520experiments%2520confirm%2520the%2520existence%2520of%2520SAH%2520and%2520show%2520that%2520it%2520increases%250Awith%2520semantic%2520complexity.%2520Additionally%252C%2520we%2520find%2520that%2520models%2520are%2520more%2520prone%2520to%250ASAH%2520on%2520rapidly%2520changing%2520semantics.%2520Moreover%252C%2520we%2520discuss%2520potential%2520approaches%2520to%250Amitigate%2520SAH.%2520We%2520demonstrate%2520that%2520positional%2520encoding%2520strategy%2520contributes%2520to%250Aalleviating%2520SAH%252C%2520and%2520further%2520adopt%2520DPO%2520strategy%2520to%2520enhance%2520the%2520model%2527s%2520ability%250Ato%2520distinguish%2520semantics%2520within%2520and%2520across%2520events.%2520To%2520support%2520this%252C%2520we%2520curate%2520a%250Adataset%2520of%25208K%2520adversarial%2520data%2520pairs%2520and%2520achieve%2520improvements%2520on%2520both%250AELV-Halluc%2520and%2520Video-MME%252C%2520including%2520a%2520substantial%252027.7%2525%2520reduction%2520in%2520SAH%2520ratio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELV-Halluc%3A%20Benchmarking%20Semantic%20Aggregation%20Hallucinations%20in%20Long%0A%20%20Video%20Understanding&entry.906535625=Hao%20Lu%20and%20Jiahao%20Wang%20and%20Yaolun%20Zhang%20and%20Ruohui%20Wang%20and%20Xuanyu%20Zheng%20and%20Yepeng%20Tang%20and%20Dahua%20Lin%20and%20Lewei%20Lu&entry.1292438233=%20%20Video%20multimodal%20large%20language%20models%20%28Video-MLLMs%29%20have%20achieved%20remarkable%0Aprogress%20in%20video%20understanding.%20However%2C%20they%20remain%20vulnerable%20to%0Ahallucination-producing%20content%20inconsistent%20with%20or%20unrelated%20to%20video%20inputs.%0APrevious%20video%20hallucination%20benchmarks%20primarily%20focus%20on%20short-videos.%20They%0Aattribute%20hallucinations%20to%20factors%20such%20as%20strong%20language%20priors%2C%20missing%0Aframes%2C%20or%20vision-language%20biases%20introduced%20by%20the%20visual%20encoder.%20While%20these%0Acauses%20indeed%20account%20for%20most%20hallucinations%20in%20short%20videos%2C%20they%20still%0Aoversimplify%20the%20cause%20of%20hallucinations.%20Sometimes%2C%20models%20generate%20incorrect%0Aoutputs%20but%20with%20correct%20frame-level%20semantics.%20We%20refer%20to%20this%20type%20of%0Ahallucination%20as%20Semantic%20Aggregation%20Hallucination%20%28SAH%29%2C%20which%20arises%20during%0Athe%20process%20of%20aggregating%20frame-level%20semantics%20into%20event-level%20semantic%0Agroups.%20Given%20that%20SAH%20becomes%20particularly%20critical%20in%20long%20videos%20due%20to%0Aincreased%20semantic%20complexity%20across%20multiple%20events%2C%20it%20is%20essential%20to%0Aseparate%20and%20thoroughly%20investigate%20the%20causes%20of%20this%20type%20of%20hallucination.%0ATo%20address%20the%20above%20issues%2C%20we%20introduce%20ELV-Halluc%2C%20the%20first%20benchmark%0Adedicated%20to%20long-video%20hallucination%2C%20enabling%20a%20systematic%20investigation%20of%0ASAH.%20Our%20experiments%20confirm%20the%20existence%20of%20SAH%20and%20show%20that%20it%20increases%0Awith%20semantic%20complexity.%20Additionally%2C%20we%20find%20that%20models%20are%20more%20prone%20to%0ASAH%20on%20rapidly%20changing%20semantics.%20Moreover%2C%20we%20discuss%20potential%20approaches%20to%0Amitigate%20SAH.%20We%20demonstrate%20that%20positional%20encoding%20strategy%20contributes%20to%0Aalleviating%20SAH%2C%20and%20further%20adopt%20DPO%20strategy%20to%20enhance%20the%20model%27s%20ability%0Ato%20distinguish%20semantics%20within%20and%20across%20events.%20To%20support%20this%2C%20we%20curate%20a%0Adataset%20of%208K%20adversarial%20data%20pairs%20and%20achieve%20improvements%20on%20both%0AELV-Halluc%20and%20Video-MME%2C%20including%20a%20substantial%2027.7%25%20reduction%20in%20SAH%20ratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21496v1&entry.124074799=Read"},
{"title": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking", "author": "TianChen Huang and Wei Gao and Runchen Xu and Shiwu Zhang", "abstract": "  Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.\n", "link": "http://arxiv.org/abs/2508.20661v2", "date": "2025-08-29", "relevancy": 2.1246, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5722}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5291}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traversing%20the%20Narrow%20Path%3A%20A%20Two-Stage%20Reinforcement%20Learning%20Framework%0A%20%20for%20Humanoid%20Beam%20Walking&body=Title%3A%20Traversing%20the%20Narrow%20Path%3A%20A%20Two-Stage%20Reinforcement%20Learning%20Framework%0A%20%20for%20Humanoid%20Beam%20Walking%0AAuthor%3A%20TianChen%20Huang%20and%20Wei%20Gao%20and%20Runchen%20Xu%20and%20Shiwu%20Zhang%0AAbstract%3A%20%20%20Traversing%20narrow%20beams%20is%20challenging%20for%20humanoids%20due%20to%20sparse%2C%0Asafety-critical%20contacts%20and%20the%20fragility%20of%20purely%20learned%20policies.%20We%0Apropose%20a%20physically%20grounded%2C%20two-stage%20framework%20that%20couples%20an%20XCoM/LIPM%0Afootstep%20template%20with%20a%20lightweight%20residual%20planner%20and%20a%20simple%20low-level%0Atracker.%20Stage-1%20is%20trained%20on%20flat%20ground%3A%20the%20tracker%20learns%20to%20robustly%0Afollow%20footstep%20targets%20by%20adding%20small%20random%20perturbations%20to%20heuristic%0Afootsteps%2C%20without%20any%20hand-crafted%20centerline%20locking%2C%20so%20it%20acquires%20stable%0Acontact%20scheduling%20and%20strong%20target-tracking%20robustness.%20Stage-2%20is%20trained%20in%0Asimulation%20on%20a%20beam%3A%20a%20high-level%20planner%20predicts%20a%20body-frame%20residual%0A%28Delta%20x%2C%20Delta%20y%2C%20Delta%20psi%29%20for%20the%20swing%20foot%20only%2C%20refining%20the%20template%0Astep%20to%20prioritize%20safe%2C%20precise%20placement%20under%20narrow%20support%20while%0Apreserving%20interpretability.%20To%20ease%20deployment%2C%20sensing%20is%20kept%20minimal%20and%0Aconsistent%20between%20simulation%20and%20hardware%3A%20the%20planner%20consumes%20compact%2C%0Aforward-facing%20elevation%20cues%20together%20with%20onboard%20IMU%20and%20joint%20signals.%20On%20a%0AUnitree%20G1%2C%20our%20system%20reliably%20traverses%20a%200.2%20m-wide%2C%203%20m-long%20beam.%20Across%0Asimulation%20and%20real-world%20studies%2C%20residual%20refinement%20consistently%20outperforms%0Atemplate-only%20and%20monolithic%20baselines%20in%20success%20rate%2C%20centerline%20adherence%2C%0Aand%20safety%20margins%2C%20while%20the%20structured%20footstep%20interface%20enables%20transparent%0Aanalysis%20and%20low-friction%20sim-to-real%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraversing%2520the%2520Narrow%2520Path%253A%2520A%2520Two-Stage%2520Reinforcement%2520Learning%2520Framework%250A%2520%2520for%2520Humanoid%2520Beam%2520Walking%26entry.906535625%3DTianChen%2520Huang%2520and%2520Wei%2520Gao%2520and%2520Runchen%2520Xu%2520and%2520Shiwu%2520Zhang%26entry.1292438233%3D%2520%2520Traversing%2520narrow%2520beams%2520is%2520challenging%2520for%2520humanoids%2520due%2520to%2520sparse%252C%250Asafety-critical%2520contacts%2520and%2520the%2520fragility%2520of%2520purely%2520learned%2520policies.%2520We%250Apropose%2520a%2520physically%2520grounded%252C%2520two-stage%2520framework%2520that%2520couples%2520an%2520XCoM/LIPM%250Afootstep%2520template%2520with%2520a%2520lightweight%2520residual%2520planner%2520and%2520a%2520simple%2520low-level%250Atracker.%2520Stage-1%2520is%2520trained%2520on%2520flat%2520ground%253A%2520the%2520tracker%2520learns%2520to%2520robustly%250Afollow%2520footstep%2520targets%2520by%2520adding%2520small%2520random%2520perturbations%2520to%2520heuristic%250Afootsteps%252C%2520without%2520any%2520hand-crafted%2520centerline%2520locking%252C%2520so%2520it%2520acquires%2520stable%250Acontact%2520scheduling%2520and%2520strong%2520target-tracking%2520robustness.%2520Stage-2%2520is%2520trained%2520in%250Asimulation%2520on%2520a%2520beam%253A%2520a%2520high-level%2520planner%2520predicts%2520a%2520body-frame%2520residual%250A%2528Delta%2520x%252C%2520Delta%2520y%252C%2520Delta%2520psi%2529%2520for%2520the%2520swing%2520foot%2520only%252C%2520refining%2520the%2520template%250Astep%2520to%2520prioritize%2520safe%252C%2520precise%2520placement%2520under%2520narrow%2520support%2520while%250Apreserving%2520interpretability.%2520To%2520ease%2520deployment%252C%2520sensing%2520is%2520kept%2520minimal%2520and%250Aconsistent%2520between%2520simulation%2520and%2520hardware%253A%2520the%2520planner%2520consumes%2520compact%252C%250Aforward-facing%2520elevation%2520cues%2520together%2520with%2520onboard%2520IMU%2520and%2520joint%2520signals.%2520On%2520a%250AUnitree%2520G1%252C%2520our%2520system%2520reliably%2520traverses%2520a%25200.2%2520m-wide%252C%25203%2520m-long%2520beam.%2520Across%250Asimulation%2520and%2520real-world%2520studies%252C%2520residual%2520refinement%2520consistently%2520outperforms%250Atemplate-only%2520and%2520monolithic%2520baselines%2520in%2520success%2520rate%252C%2520centerline%2520adherence%252C%250Aand%2520safety%2520margins%252C%2520while%2520the%2520structured%2520footstep%2520interface%2520enables%2520transparent%250Aanalysis%2520and%2520low-friction%2520sim-to-real%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traversing%20the%20Narrow%20Path%3A%20A%20Two-Stage%20Reinforcement%20Learning%20Framework%0A%20%20for%20Humanoid%20Beam%20Walking&entry.906535625=TianChen%20Huang%20and%20Wei%20Gao%20and%20Runchen%20Xu%20and%20Shiwu%20Zhang&entry.1292438233=%20%20Traversing%20narrow%20beams%20is%20challenging%20for%20humanoids%20due%20to%20sparse%2C%0Asafety-critical%20contacts%20and%20the%20fragility%20of%20purely%20learned%20policies.%20We%0Apropose%20a%20physically%20grounded%2C%20two-stage%20framework%20that%20couples%20an%20XCoM/LIPM%0Afootstep%20template%20with%20a%20lightweight%20residual%20planner%20and%20a%20simple%20low-level%0Atracker.%20Stage-1%20is%20trained%20on%20flat%20ground%3A%20the%20tracker%20learns%20to%20robustly%0Afollow%20footstep%20targets%20by%20adding%20small%20random%20perturbations%20to%20heuristic%0Afootsteps%2C%20without%20any%20hand-crafted%20centerline%20locking%2C%20so%20it%20acquires%20stable%0Acontact%20scheduling%20and%20strong%20target-tracking%20robustness.%20Stage-2%20is%20trained%20in%0Asimulation%20on%20a%20beam%3A%20a%20high-level%20planner%20predicts%20a%20body-frame%20residual%0A%28Delta%20x%2C%20Delta%20y%2C%20Delta%20psi%29%20for%20the%20swing%20foot%20only%2C%20refining%20the%20template%0Astep%20to%20prioritize%20safe%2C%20precise%20placement%20under%20narrow%20support%20while%0Apreserving%20interpretability.%20To%20ease%20deployment%2C%20sensing%20is%20kept%20minimal%20and%0Aconsistent%20between%20simulation%20and%20hardware%3A%20the%20planner%20consumes%20compact%2C%0Aforward-facing%20elevation%20cues%20together%20with%20onboard%20IMU%20and%20joint%20signals.%20On%20a%0AUnitree%20G1%2C%20our%20system%20reliably%20traverses%20a%200.2%20m-wide%2C%203%20m-long%20beam.%20Across%0Asimulation%20and%20real-world%20studies%2C%20residual%20refinement%20consistently%20outperforms%0Atemplate-only%20and%20monolithic%20baselines%20in%20success%20rate%2C%20centerline%20adherence%2C%0Aand%20safety%20margins%2C%20while%20the%20structured%20footstep%20interface%20enables%20transparent%0Aanalysis%20and%20low-friction%20sim-to-real%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20661v2&entry.124074799=Read"},
{"title": "Comprehensive Signal Quality Evaluation of a Wearable Textile ECG\n  Garment: A Sex-Balanced Study", "author": "Maximilian P. Oppelt and Tobias S. Zech and Sarah H. Lorenz and Laurenz Ottmann and Jan Steffan and Bjoern M. Eskofier and Nadine R. Lang-Richter and Norman Pfeiffer", "abstract": "  We introduce a novel wearable textile-garment featuring an innovative\nelectrode placement aimed at minimizing noise and motion artifacts, thereby\nenhancing signal fidelity in Electrocardiography (ECG) recordings. We present a\ncomprehensive, sex-balanced evaluation involving 15 healthy males and 15\nhealthy female participants to ensure the device's suitability across\nanatomical and physiological variations. The assessment framework encompasses\ndistinct evaluation approaches: quantitative signal quality indices to\nobjectively benchmark device performance; rhythm-based analyzes of\nphysiological parameters such as heart rate and heart rate variability; machine\nlearning classification tasks to assess application-relevant predictive\nutility; morphological analysis of ECG features including amplitude and\ninterval parameters; and investigations of the effects of electrode projection\nangle given by the textile / body shape, with all analyzes stratified by sex to\nelucidate sex-specific influences. Evaluations were conducted across various\nactivity phases representing real-world conditions. The results demonstrate\nthat the textile system achieves signal quality highly concordant with\nreference devices in both rhythm and morphological analyses, exhibits robust\nclassification performance, and enables identification of key sex-specific\ndeterminants affecting signal acquisition. These findings underscore the\npractical viability of textile-based ECG garments for physiological monitoring\nas well as psychophysiological state detection. Moreover, we identify the\nimportance of incorporating sex-specific design considerations to ensure\nequitable and reliable cardiac diagnostics in wearable health technologies.\n", "link": "http://arxiv.org/abs/2508.21554v1", "date": "2025-08-29", "relevancy": 2.1223, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.562}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5178}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Signal%20Quality%20Evaluation%20of%20a%20Wearable%20Textile%20ECG%0A%20%20Garment%3A%20A%20Sex-Balanced%20Study&body=Title%3A%20Comprehensive%20Signal%20Quality%20Evaluation%20of%20a%20Wearable%20Textile%20ECG%0A%20%20Garment%3A%20A%20Sex-Balanced%20Study%0AAuthor%3A%20Maximilian%20P.%20Oppelt%20and%20Tobias%20S.%20Zech%20and%20Sarah%20H.%20Lorenz%20and%20Laurenz%20Ottmann%20and%20Jan%20Steffan%20and%20Bjoern%20M.%20Eskofier%20and%20Nadine%20R.%20Lang-Richter%20and%20Norman%20Pfeiffer%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20wearable%20textile-garment%20featuring%20an%20innovative%0Aelectrode%20placement%20aimed%20at%20minimizing%20noise%20and%20motion%20artifacts%2C%20thereby%0Aenhancing%20signal%20fidelity%20in%20Electrocardiography%20%28ECG%29%20recordings.%20We%20present%20a%0Acomprehensive%2C%20sex-balanced%20evaluation%20involving%2015%20healthy%20males%20and%2015%0Ahealthy%20female%20participants%20to%20ensure%20the%20device%27s%20suitability%20across%0Aanatomical%20and%20physiological%20variations.%20The%20assessment%20framework%20encompasses%0Adistinct%20evaluation%20approaches%3A%20quantitative%20signal%20quality%20indices%20to%0Aobjectively%20benchmark%20device%20performance%3B%20rhythm-based%20analyzes%20of%0Aphysiological%20parameters%20such%20as%20heart%20rate%20and%20heart%20rate%20variability%3B%20machine%0Alearning%20classification%20tasks%20to%20assess%20application-relevant%20predictive%0Autility%3B%20morphological%20analysis%20of%20ECG%20features%20including%20amplitude%20and%0Ainterval%20parameters%3B%20and%20investigations%20of%20the%20effects%20of%20electrode%20projection%0Aangle%20given%20by%20the%20textile%20/%20body%20shape%2C%20with%20all%20analyzes%20stratified%20by%20sex%20to%0Aelucidate%20sex-specific%20influences.%20Evaluations%20were%20conducted%20across%20various%0Aactivity%20phases%20representing%20real-world%20conditions.%20The%20results%20demonstrate%0Athat%20the%20textile%20system%20achieves%20signal%20quality%20highly%20concordant%20with%0Areference%20devices%20in%20both%20rhythm%20and%20morphological%20analyses%2C%20exhibits%20robust%0Aclassification%20performance%2C%20and%20enables%20identification%20of%20key%20sex-specific%0Adeterminants%20affecting%20signal%20acquisition.%20These%20findings%20underscore%20the%0Apractical%20viability%20of%20textile-based%20ECG%20garments%20for%20physiological%20monitoring%0Aas%20well%20as%20psychophysiological%20state%20detection.%20Moreover%2C%20we%20identify%20the%0Aimportance%20of%20incorporating%20sex-specific%20design%20considerations%20to%20ensure%0Aequitable%20and%20reliable%20cardiac%20diagnostics%20in%20wearable%20health%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Signal%2520Quality%2520Evaluation%2520of%2520a%2520Wearable%2520Textile%2520ECG%250A%2520%2520Garment%253A%2520A%2520Sex-Balanced%2520Study%26entry.906535625%3DMaximilian%2520P.%2520Oppelt%2520and%2520Tobias%2520S.%2520Zech%2520and%2520Sarah%2520H.%2520Lorenz%2520and%2520Laurenz%2520Ottmann%2520and%2520Jan%2520Steffan%2520and%2520Bjoern%2520M.%2520Eskofier%2520and%2520Nadine%2520R.%2520Lang-Richter%2520and%2520Norman%2520Pfeiffer%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520wearable%2520textile-garment%2520featuring%2520an%2520innovative%250Aelectrode%2520placement%2520aimed%2520at%2520minimizing%2520noise%2520and%2520motion%2520artifacts%252C%2520thereby%250Aenhancing%2520signal%2520fidelity%2520in%2520Electrocardiography%2520%2528ECG%2529%2520recordings.%2520We%2520present%2520a%250Acomprehensive%252C%2520sex-balanced%2520evaluation%2520involving%252015%2520healthy%2520males%2520and%252015%250Ahealthy%2520female%2520participants%2520to%2520ensure%2520the%2520device%2527s%2520suitability%2520across%250Aanatomical%2520and%2520physiological%2520variations.%2520The%2520assessment%2520framework%2520encompasses%250Adistinct%2520evaluation%2520approaches%253A%2520quantitative%2520signal%2520quality%2520indices%2520to%250Aobjectively%2520benchmark%2520device%2520performance%253B%2520rhythm-based%2520analyzes%2520of%250Aphysiological%2520parameters%2520such%2520as%2520heart%2520rate%2520and%2520heart%2520rate%2520variability%253B%2520machine%250Alearning%2520classification%2520tasks%2520to%2520assess%2520application-relevant%2520predictive%250Autility%253B%2520morphological%2520analysis%2520of%2520ECG%2520features%2520including%2520amplitude%2520and%250Ainterval%2520parameters%253B%2520and%2520investigations%2520of%2520the%2520effects%2520of%2520electrode%2520projection%250Aangle%2520given%2520by%2520the%2520textile%2520/%2520body%2520shape%252C%2520with%2520all%2520analyzes%2520stratified%2520by%2520sex%2520to%250Aelucidate%2520sex-specific%2520influences.%2520Evaluations%2520were%2520conducted%2520across%2520various%250Aactivity%2520phases%2520representing%2520real-world%2520conditions.%2520The%2520results%2520demonstrate%250Athat%2520the%2520textile%2520system%2520achieves%2520signal%2520quality%2520highly%2520concordant%2520with%250Areference%2520devices%2520in%2520both%2520rhythm%2520and%2520morphological%2520analyses%252C%2520exhibits%2520robust%250Aclassification%2520performance%252C%2520and%2520enables%2520identification%2520of%2520key%2520sex-specific%250Adeterminants%2520affecting%2520signal%2520acquisition.%2520These%2520findings%2520underscore%2520the%250Apractical%2520viability%2520of%2520textile-based%2520ECG%2520garments%2520for%2520physiological%2520monitoring%250Aas%2520well%2520as%2520psychophysiological%2520state%2520detection.%2520Moreover%252C%2520we%2520identify%2520the%250Aimportance%2520of%2520incorporating%2520sex-specific%2520design%2520considerations%2520to%2520ensure%250Aequitable%2520and%2520reliable%2520cardiac%2520diagnostics%2520in%2520wearable%2520health%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Signal%20Quality%20Evaluation%20of%20a%20Wearable%20Textile%20ECG%0A%20%20Garment%3A%20A%20Sex-Balanced%20Study&entry.906535625=Maximilian%20P.%20Oppelt%20and%20Tobias%20S.%20Zech%20and%20Sarah%20H.%20Lorenz%20and%20Laurenz%20Ottmann%20and%20Jan%20Steffan%20and%20Bjoern%20M.%20Eskofier%20and%20Nadine%20R.%20Lang-Richter%20and%20Norman%20Pfeiffer&entry.1292438233=%20%20We%20introduce%20a%20novel%20wearable%20textile-garment%20featuring%20an%20innovative%0Aelectrode%20placement%20aimed%20at%20minimizing%20noise%20and%20motion%20artifacts%2C%20thereby%0Aenhancing%20signal%20fidelity%20in%20Electrocardiography%20%28ECG%29%20recordings.%20We%20present%20a%0Acomprehensive%2C%20sex-balanced%20evaluation%20involving%2015%20healthy%20males%20and%2015%0Ahealthy%20female%20participants%20to%20ensure%20the%20device%27s%20suitability%20across%0Aanatomical%20and%20physiological%20variations.%20The%20assessment%20framework%20encompasses%0Adistinct%20evaluation%20approaches%3A%20quantitative%20signal%20quality%20indices%20to%0Aobjectively%20benchmark%20device%20performance%3B%20rhythm-based%20analyzes%20of%0Aphysiological%20parameters%20such%20as%20heart%20rate%20and%20heart%20rate%20variability%3B%20machine%0Alearning%20classification%20tasks%20to%20assess%20application-relevant%20predictive%0Autility%3B%20morphological%20analysis%20of%20ECG%20features%20including%20amplitude%20and%0Ainterval%20parameters%3B%20and%20investigations%20of%20the%20effects%20of%20electrode%20projection%0Aangle%20given%20by%20the%20textile%20/%20body%20shape%2C%20with%20all%20analyzes%20stratified%20by%20sex%20to%0Aelucidate%20sex-specific%20influences.%20Evaluations%20were%20conducted%20across%20various%0Aactivity%20phases%20representing%20real-world%20conditions.%20The%20results%20demonstrate%0Athat%20the%20textile%20system%20achieves%20signal%20quality%20highly%20concordant%20with%0Areference%20devices%20in%20both%20rhythm%20and%20morphological%20analyses%2C%20exhibits%20robust%0Aclassification%20performance%2C%20and%20enables%20identification%20of%20key%20sex-specific%0Adeterminants%20affecting%20signal%20acquisition.%20These%20findings%20underscore%20the%0Apractical%20viability%20of%20textile-based%20ECG%20garments%20for%20physiological%20monitoring%0Aas%20well%20as%20psychophysiological%20state%20detection.%20Moreover%2C%20we%20identify%20the%0Aimportance%20of%20incorporating%20sex-specific%20design%20considerations%20to%20ensure%0Aequitable%20and%20reliable%20cardiac%20diagnostics%20in%20wearable%20health%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21554v1&entry.124074799=Read"},
{"title": "L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models", "author": "Aishwarya Mirashi and Ananya Joshi and Raviraj Joshi", "abstract": "  We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)\ndataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT\nmodel optimized for regression-based similarity scoring. The MahaSTS dataset\nconsists of 16,860 Marathi sentence pairs labeled with continuous similarity\nscores in the range of 0-5. To ensure balanced supervision, the dataset is\nuniformly distributed across six score-based buckets spanning the full 0-5\nrange, thus reducing label bias and enhancing model stability. We fine-tune the\nMahaSBERT model on this dataset and benchmark its performance against other\nalternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments\ndemonstrate that MahaSTS enables effective training for sentence similarity\ntasks in Marathi, highlighting the impact of human-curated annotations,\ntargeted fine-tuning, and structured supervision in low-resource settings. The\ndataset and model are publicly shared at\nhttps://github.com/l3cube-pune/MarathiNLP\n", "link": "http://arxiv.org/abs/2508.21569v1", "date": "2025-08-29", "relevancy": 2.1167, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.41}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L3Cube-MahaSTS%3A%20A%20Marathi%20Sentence%20Similarity%20Dataset%20and%20Models&body=Title%3A%20L3Cube-MahaSTS%3A%20A%20Marathi%20Sentence%20Similarity%20Dataset%20and%20Models%0AAuthor%3A%20Aishwarya%20Mirashi%20and%20Ananya%20Joshi%20and%20Raviraj%20Joshi%0AAbstract%3A%20%20%20We%20present%20MahaSTS%2C%20a%20human-annotated%20Sentence%20Textual%20Similarity%20%28STS%29%0Adataset%20for%20Marathi%2C%20along%20with%20MahaSBERT-STS-v2%2C%20a%20fine-tuned%20Sentence-BERT%0Amodel%20optimized%20for%20regression-based%20similarity%20scoring.%20The%20MahaSTS%20dataset%0Aconsists%20of%2016%2C860%20Marathi%20sentence%20pairs%20labeled%20with%20continuous%20similarity%0Ascores%20in%20the%20range%20of%200-5.%20To%20ensure%20balanced%20supervision%2C%20the%20dataset%20is%0Auniformly%20distributed%20across%20six%20score-based%20buckets%20spanning%20the%20full%200-5%0Arange%2C%20thus%20reducing%20label%20bias%20and%20enhancing%20model%20stability.%20We%20fine-tune%20the%0AMahaSBERT%20model%20on%20this%20dataset%20and%20benchmark%20its%20performance%20against%20other%0Aalternatives%20like%20MahaBERT%2C%20MuRIL%2C%20IndicBERT%2C%20and%20IndicSBERT.%20Our%20experiments%0Ademonstrate%20that%20MahaSTS%20enables%20effective%20training%20for%20sentence%20similarity%0Atasks%20in%20Marathi%2C%20highlighting%20the%20impact%20of%20human-curated%20annotations%2C%0Atargeted%20fine-tuning%2C%20and%20structured%20supervision%20in%20low-resource%20settings.%20The%0Adataset%20and%20model%20are%20publicly%20shared%20at%0Ahttps%3A//github.com/l3cube-pune/MarathiNLP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL3Cube-MahaSTS%253A%2520A%2520Marathi%2520Sentence%2520Similarity%2520Dataset%2520and%2520Models%26entry.906535625%3DAishwarya%2520Mirashi%2520and%2520Ananya%2520Joshi%2520and%2520Raviraj%2520Joshi%26entry.1292438233%3D%2520%2520We%2520present%2520MahaSTS%252C%2520a%2520human-annotated%2520Sentence%2520Textual%2520Similarity%2520%2528STS%2529%250Adataset%2520for%2520Marathi%252C%2520along%2520with%2520MahaSBERT-STS-v2%252C%2520a%2520fine-tuned%2520Sentence-BERT%250Amodel%2520optimized%2520for%2520regression-based%2520similarity%2520scoring.%2520The%2520MahaSTS%2520dataset%250Aconsists%2520of%252016%252C860%2520Marathi%2520sentence%2520pairs%2520labeled%2520with%2520continuous%2520similarity%250Ascores%2520in%2520the%2520range%2520of%25200-5.%2520To%2520ensure%2520balanced%2520supervision%252C%2520the%2520dataset%2520is%250Auniformly%2520distributed%2520across%2520six%2520score-based%2520buckets%2520spanning%2520the%2520full%25200-5%250Arange%252C%2520thus%2520reducing%2520label%2520bias%2520and%2520enhancing%2520model%2520stability.%2520We%2520fine-tune%2520the%250AMahaSBERT%2520model%2520on%2520this%2520dataset%2520and%2520benchmark%2520its%2520performance%2520against%2520other%250Aalternatives%2520like%2520MahaBERT%252C%2520MuRIL%252C%2520IndicBERT%252C%2520and%2520IndicSBERT.%2520Our%2520experiments%250Ademonstrate%2520that%2520MahaSTS%2520enables%2520effective%2520training%2520for%2520sentence%2520similarity%250Atasks%2520in%2520Marathi%252C%2520highlighting%2520the%2520impact%2520of%2520human-curated%2520annotations%252C%250Atargeted%2520fine-tuning%252C%2520and%2520structured%2520supervision%2520in%2520low-resource%2520settings.%2520The%250Adataset%2520and%2520model%2520are%2520publicly%2520shared%2520at%250Ahttps%253A//github.com/l3cube-pune/MarathiNLP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L3Cube-MahaSTS%3A%20A%20Marathi%20Sentence%20Similarity%20Dataset%20and%20Models&entry.906535625=Aishwarya%20Mirashi%20and%20Ananya%20Joshi%20and%20Raviraj%20Joshi&entry.1292438233=%20%20We%20present%20MahaSTS%2C%20a%20human-annotated%20Sentence%20Textual%20Similarity%20%28STS%29%0Adataset%20for%20Marathi%2C%20along%20with%20MahaSBERT-STS-v2%2C%20a%20fine-tuned%20Sentence-BERT%0Amodel%20optimized%20for%20regression-based%20similarity%20scoring.%20The%20MahaSTS%20dataset%0Aconsists%20of%2016%2C860%20Marathi%20sentence%20pairs%20labeled%20with%20continuous%20similarity%0Ascores%20in%20the%20range%20of%200-5.%20To%20ensure%20balanced%20supervision%2C%20the%20dataset%20is%0Auniformly%20distributed%20across%20six%20score-based%20buckets%20spanning%20the%20full%200-5%0Arange%2C%20thus%20reducing%20label%20bias%20and%20enhancing%20model%20stability.%20We%20fine-tune%20the%0AMahaSBERT%20model%20on%20this%20dataset%20and%20benchmark%20its%20performance%20against%20other%0Aalternatives%20like%20MahaBERT%2C%20MuRIL%2C%20IndicBERT%2C%20and%20IndicSBERT.%20Our%20experiments%0Ademonstrate%20that%20MahaSTS%20enables%20effective%20training%20for%20sentence%20similarity%0Atasks%20in%20Marathi%2C%20highlighting%20the%20impact%20of%20human-curated%20annotations%2C%0Atargeted%20fine-tuning%2C%20and%20structured%20supervision%20in%20low-resource%20settings.%20The%0Adataset%20and%20model%20are%20publicly%20shared%20at%0Ahttps%3A//github.com/l3cube-pune/MarathiNLP%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21569v1&entry.124074799=Read"},
{"title": "DriveQA: Passing the Driving Knowledge Test", "author": "Maolin Wei and Wanzhou Liu and Eshed Ohn-Bar", "abstract": "  If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.\n", "link": "http://arxiv.org/abs/2508.21824v1", "date": "2025-08-29", "relevancy": 2.1071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveQA%3A%20Passing%20the%20Driving%20Knowledge%20Test&body=Title%3A%20DriveQA%3A%20Passing%20the%20Driving%20Knowledge%20Test%0AAuthor%3A%20Maolin%20Wei%20and%20Wanzhou%20Liu%20and%20Eshed%20Ohn-Bar%0AAbstract%3A%20%20%20If%20a%20Large%20Language%20Model%20%28LLM%29%20were%20to%20take%20a%20driving%20knowledge%20test%20today%2C%0Awould%20it%20pass%3F%20Beyond%20standard%20spatial%20and%20visual%20question-answering%20%28QA%29%20tasks%0Aon%20current%20autonomous%20driving%20benchmarks%2C%20driving%20knowledge%20tests%20require%20a%0Acomplete%20understanding%20of%20all%20traffic%20rules%2C%20signage%2C%20and%20right-of-way%0Aprinciples.%20To%20pass%20this%20test%2C%20human%20drivers%20must%20discern%20various%20edge%20cases%0Athat%20rarely%20appear%20in%20real-world%20datasets.%20In%20this%20work%2C%20we%20present%20DriveQA%2C%20an%0Aextensive%20open-source%20text%20and%20vision-based%20benchmark%20that%20exhaustively%20covers%0Atraffic%20regulations%20and%20scenarios.%20Through%20our%20experiments%20using%20DriveQA%2C%20we%0Ashow%20that%20%281%29%20state-of-the-art%20LLMs%20and%20Multimodal%20LLMs%20%28MLLMs%29%20perform%20well%20on%0Abasic%20traffic%20rules%20but%20exhibit%20significant%20weaknesses%20in%20numerical%20reasoning%0Aand%20complex%20right-of-way%20scenarios%2C%20traffic%20sign%20variations%2C%20and%20spatial%0Alayouts%2C%20%282%29%20fine-tuning%20on%20DriveQA%20improves%20accuracy%20across%20multiple%0Acategories%2C%20particularly%20in%20regulatory%20sign%20recognition%20and%20intersection%0Adecision-making%2C%20%283%29%20controlled%20variations%20in%20DriveQA-V%20provide%20insights%20into%0Amodel%20sensitivity%20to%20environmental%20factors%20such%20as%20lighting%2C%20perspective%2C%0Adistance%2C%20and%20weather%20conditions%2C%20and%20%284%29%20pretraining%20on%20DriveQA%20enhances%0Adownstream%20driving%20task%20performance%2C%20leading%20to%20improved%20results%20on%20real-world%0Adatasets%20such%20as%20nuScenes%20and%20BDD%2C%20while%20also%20demonstrating%20that%20models%20can%0Ainternalize%20text%20and%20synthetic%20traffic%20knowledge%20to%20generalize%20effectively%0Aacross%20downstream%20QA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveQA%253A%2520Passing%2520the%2520Driving%2520Knowledge%2520Test%26entry.906535625%3DMaolin%2520Wei%2520and%2520Wanzhou%2520Liu%2520and%2520Eshed%2520Ohn-Bar%26entry.1292438233%3D%2520%2520If%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520were%2520to%2520take%2520a%2520driving%2520knowledge%2520test%2520today%252C%250Awould%2520it%2520pass%253F%2520Beyond%2520standard%2520spatial%2520and%2520visual%2520question-answering%2520%2528QA%2529%2520tasks%250Aon%2520current%2520autonomous%2520driving%2520benchmarks%252C%2520driving%2520knowledge%2520tests%2520require%2520a%250Acomplete%2520understanding%2520of%2520all%2520traffic%2520rules%252C%2520signage%252C%2520and%2520right-of-way%250Aprinciples.%2520To%2520pass%2520this%2520test%252C%2520human%2520drivers%2520must%2520discern%2520various%2520edge%2520cases%250Athat%2520rarely%2520appear%2520in%2520real-world%2520datasets.%2520In%2520this%2520work%252C%2520we%2520present%2520DriveQA%252C%2520an%250Aextensive%2520open-source%2520text%2520and%2520vision-based%2520benchmark%2520that%2520exhaustively%2520covers%250Atraffic%2520regulations%2520and%2520scenarios.%2520Through%2520our%2520experiments%2520using%2520DriveQA%252C%2520we%250Ashow%2520that%2520%25281%2529%2520state-of-the-art%2520LLMs%2520and%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520perform%2520well%2520on%250Abasic%2520traffic%2520rules%2520but%2520exhibit%2520significant%2520weaknesses%2520in%2520numerical%2520reasoning%250Aand%2520complex%2520right-of-way%2520scenarios%252C%2520traffic%2520sign%2520variations%252C%2520and%2520spatial%250Alayouts%252C%2520%25282%2529%2520fine-tuning%2520on%2520DriveQA%2520improves%2520accuracy%2520across%2520multiple%250Acategories%252C%2520particularly%2520in%2520regulatory%2520sign%2520recognition%2520and%2520intersection%250Adecision-making%252C%2520%25283%2529%2520controlled%2520variations%2520in%2520DriveQA-V%2520provide%2520insights%2520into%250Amodel%2520sensitivity%2520to%2520environmental%2520factors%2520such%2520as%2520lighting%252C%2520perspective%252C%250Adistance%252C%2520and%2520weather%2520conditions%252C%2520and%2520%25284%2529%2520pretraining%2520on%2520DriveQA%2520enhances%250Adownstream%2520driving%2520task%2520performance%252C%2520leading%2520to%2520improved%2520results%2520on%2520real-world%250Adatasets%2520such%2520as%2520nuScenes%2520and%2520BDD%252C%2520while%2520also%2520demonstrating%2520that%2520models%2520can%250Ainternalize%2520text%2520and%2520synthetic%2520traffic%2520knowledge%2520to%2520generalize%2520effectively%250Aacross%2520downstream%2520QA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveQA%3A%20Passing%20the%20Driving%20Knowledge%20Test&entry.906535625=Maolin%20Wei%20and%20Wanzhou%20Liu%20and%20Eshed%20Ohn-Bar&entry.1292438233=%20%20If%20a%20Large%20Language%20Model%20%28LLM%29%20were%20to%20take%20a%20driving%20knowledge%20test%20today%2C%0Awould%20it%20pass%3F%20Beyond%20standard%20spatial%20and%20visual%20question-answering%20%28QA%29%20tasks%0Aon%20current%20autonomous%20driving%20benchmarks%2C%20driving%20knowledge%20tests%20require%20a%0Acomplete%20understanding%20of%20all%20traffic%20rules%2C%20signage%2C%20and%20right-of-way%0Aprinciples.%20To%20pass%20this%20test%2C%20human%20drivers%20must%20discern%20various%20edge%20cases%0Athat%20rarely%20appear%20in%20real-world%20datasets.%20In%20this%20work%2C%20we%20present%20DriveQA%2C%20an%0Aextensive%20open-source%20text%20and%20vision-based%20benchmark%20that%20exhaustively%20covers%0Atraffic%20regulations%20and%20scenarios.%20Through%20our%20experiments%20using%20DriveQA%2C%20we%0Ashow%20that%20%281%29%20state-of-the-art%20LLMs%20and%20Multimodal%20LLMs%20%28MLLMs%29%20perform%20well%20on%0Abasic%20traffic%20rules%20but%20exhibit%20significant%20weaknesses%20in%20numerical%20reasoning%0Aand%20complex%20right-of-way%20scenarios%2C%20traffic%20sign%20variations%2C%20and%20spatial%0Alayouts%2C%20%282%29%20fine-tuning%20on%20DriveQA%20improves%20accuracy%20across%20multiple%0Acategories%2C%20particularly%20in%20regulatory%20sign%20recognition%20and%20intersection%0Adecision-making%2C%20%283%29%20controlled%20variations%20in%20DriveQA-V%20provide%20insights%20into%0Amodel%20sensitivity%20to%20environmental%20factors%20such%20as%20lighting%2C%20perspective%2C%0Adistance%2C%20and%20weather%20conditions%2C%20and%20%284%29%20pretraining%20on%20DriveQA%20enhances%0Adownstream%20driving%20task%20performance%2C%20leading%20to%20improved%20results%20on%20real-world%0Adatasets%20such%20as%20nuScenes%20and%20BDD%2C%20while%20also%20demonstrating%20that%20models%20can%0Ainternalize%20text%20and%20synthetic%20traffic%20knowledge%20to%20generalize%20effectively%0Aacross%20downstream%20QA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21824v1&entry.124074799=Read"},
{"title": "A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor\n  Segmentation in Diagnostic and Therapeutic MRI", "author": "Omer Faruk Durugol and Maximilian Rokuss and Yannick Kirchhoff and Klaus H. Maier-Hein", "abstract": "  Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is\ncritical for clinical workflows but is hindered by poor tumor-tissue contrast\nand a scarcity of annotated data. This paper details our submission to the\nPANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and\ntherapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the\nnnU-Net framework and leverages a deep, multi-stage cascaded pre-training\nstrategy, starting from a general anatomical foundation model and sequentially\nfine-tuning on CT pancreatic lesion datasets and the target MRI modalities.\nThrough extensive five-fold cross-validation, we systematically evaluated data\naugmentation schemes and training schedules. Our analysis revealed a critical\ntrade-off, where aggressive data augmentation produced the highest volumetric\naccuracy, while default augmentations yielded superior boundary precision\n(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).\nFor our final submission, we exploited this finding by constructing custom,\nheterogeneous ensembles of specialist models, essentially creating a mix of\nexperts. This metric-aware ensembling strategy proved highly effective,\nachieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523\nfor Task 2. Our work presents a robust methodology for developing specialized,\nhigh-performance models in the context of limited data and complex medical\nimaging tasks (Team MIC-DKFZ).\n", "link": "http://arxiv.org/abs/2508.21775v1", "date": "2025-08-29", "relevancy": 2.0954, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5261}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stage%20Fine-Tuning%20and%20Ensembling%20Strategy%20for%20Pancreatic%20Tumor%0A%20%20Segmentation%20in%20Diagnostic%20and%20Therapeutic%20MRI&body=Title%3A%20A%20Multi-Stage%20Fine-Tuning%20and%20Ensembling%20Strategy%20for%20Pancreatic%20Tumor%0A%20%20Segmentation%20in%20Diagnostic%20and%20Therapeutic%20MRI%0AAuthor%3A%20Omer%20Faruk%20Durugol%20and%20Maximilian%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Klaus%20H.%20Maier-Hein%0AAbstract%3A%20%20%20Automated%20segmentation%20of%20Pancreatic%20Ductal%20Adenocarcinoma%20%28PDAC%29%20from%20MRI%20is%0Acritical%20for%20clinical%20workflows%20but%20is%20hindered%20by%20poor%20tumor-tissue%20contrast%0Aand%20a%20scarcity%20of%20annotated%20data.%20This%20paper%20details%20our%20submission%20to%20the%0APANTHER%20challenge%2C%20addressing%20both%20diagnostic%20T1-weighted%20%28Task%201%29%20and%0Atherapeutic%20T2-weighted%20%28Task%202%29%20segmentation.%20Our%20approach%20is%20built%20upon%20the%0AnnU-Net%20framework%20and%20leverages%20a%20deep%2C%20multi-stage%20cascaded%20pre-training%0Astrategy%2C%20starting%20from%20a%20general%20anatomical%20foundation%20model%20and%20sequentially%0Afine-tuning%20on%20CT%20pancreatic%20lesion%20datasets%20and%20the%20target%20MRI%20modalities.%0AThrough%20extensive%20five-fold%20cross-validation%2C%20we%20systematically%20evaluated%20data%0Aaugmentation%20schemes%20and%20training%20schedules.%20Our%20analysis%20revealed%20a%20critical%0Atrade-off%2C%20where%20aggressive%20data%20augmentation%20produced%20the%20highest%20volumetric%0Aaccuracy%2C%20while%20default%20augmentations%20yielded%20superior%20boundary%20precision%0A%28achieving%20a%20state-of-the-art%20MASD%20of%205.46%20mm%20and%20HD95%20of%2017.33%20mm%20for%20Task%201%29.%0AFor%20our%20final%20submission%2C%20we%20exploited%20this%20finding%20by%20constructing%20custom%2C%0Aheterogeneous%20ensembles%20of%20specialist%20models%2C%20essentially%20creating%20a%20mix%20of%0Aexperts.%20This%20metric-aware%20ensembling%20strategy%20proved%20highly%20effective%2C%0Aachieving%20a%20top%20cross-validation%20Tumor%20Dice%20score%20of%200.661%20for%20Task%201%20and%200.523%0Afor%20Task%202.%20Our%20work%20presents%20a%20robust%20methodology%20for%20developing%20specialized%2C%0Ahigh-performance%20models%20in%20the%20context%20of%20limited%20data%20and%20complex%20medical%0Aimaging%20tasks%20%28Team%20MIC-DKFZ%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stage%2520Fine-Tuning%2520and%2520Ensembling%2520Strategy%2520for%2520Pancreatic%2520Tumor%250A%2520%2520Segmentation%2520in%2520Diagnostic%2520and%2520Therapeutic%2520MRI%26entry.906535625%3DOmer%2520Faruk%2520Durugol%2520and%2520Maximilian%2520Rokuss%2520and%2520Yannick%2520Kirchhoff%2520and%2520Klaus%2520H.%2520Maier-Hein%26entry.1292438233%3D%2520%2520Automated%2520segmentation%2520of%2520Pancreatic%2520Ductal%2520Adenocarcinoma%2520%2528PDAC%2529%2520from%2520MRI%2520is%250Acritical%2520for%2520clinical%2520workflows%2520but%2520is%2520hindered%2520by%2520poor%2520tumor-tissue%2520contrast%250Aand%2520a%2520scarcity%2520of%2520annotated%2520data.%2520This%2520paper%2520details%2520our%2520submission%2520to%2520the%250APANTHER%2520challenge%252C%2520addressing%2520both%2520diagnostic%2520T1-weighted%2520%2528Task%25201%2529%2520and%250Atherapeutic%2520T2-weighted%2520%2528Task%25202%2529%2520segmentation.%2520Our%2520approach%2520is%2520built%2520upon%2520the%250AnnU-Net%2520framework%2520and%2520leverages%2520a%2520deep%252C%2520multi-stage%2520cascaded%2520pre-training%250Astrategy%252C%2520starting%2520from%2520a%2520general%2520anatomical%2520foundation%2520model%2520and%2520sequentially%250Afine-tuning%2520on%2520CT%2520pancreatic%2520lesion%2520datasets%2520and%2520the%2520target%2520MRI%2520modalities.%250AThrough%2520extensive%2520five-fold%2520cross-validation%252C%2520we%2520systematically%2520evaluated%2520data%250Aaugmentation%2520schemes%2520and%2520training%2520schedules.%2520Our%2520analysis%2520revealed%2520a%2520critical%250Atrade-off%252C%2520where%2520aggressive%2520data%2520augmentation%2520produced%2520the%2520highest%2520volumetric%250Aaccuracy%252C%2520while%2520default%2520augmentations%2520yielded%2520superior%2520boundary%2520precision%250A%2528achieving%2520a%2520state-of-the-art%2520MASD%2520of%25205.46%2520mm%2520and%2520HD95%2520of%252017.33%2520mm%2520for%2520Task%25201%2529.%250AFor%2520our%2520final%2520submission%252C%2520we%2520exploited%2520this%2520finding%2520by%2520constructing%2520custom%252C%250Aheterogeneous%2520ensembles%2520of%2520specialist%2520models%252C%2520essentially%2520creating%2520a%2520mix%2520of%250Aexperts.%2520This%2520metric-aware%2520ensembling%2520strategy%2520proved%2520highly%2520effective%252C%250Aachieving%2520a%2520top%2520cross-validation%2520Tumor%2520Dice%2520score%2520of%25200.661%2520for%2520Task%25201%2520and%25200.523%250Afor%2520Task%25202.%2520Our%2520work%2520presents%2520a%2520robust%2520methodology%2520for%2520developing%2520specialized%252C%250Ahigh-performance%2520models%2520in%2520the%2520context%2520of%2520limited%2520data%2520and%2520complex%2520medical%250Aimaging%2520tasks%2520%2528Team%2520MIC-DKFZ%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stage%20Fine-Tuning%20and%20Ensembling%20Strategy%20for%20Pancreatic%20Tumor%0A%20%20Segmentation%20in%20Diagnostic%20and%20Therapeutic%20MRI&entry.906535625=Omer%20Faruk%20Durugol%20and%20Maximilian%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Klaus%20H.%20Maier-Hein&entry.1292438233=%20%20Automated%20segmentation%20of%20Pancreatic%20Ductal%20Adenocarcinoma%20%28PDAC%29%20from%20MRI%20is%0Acritical%20for%20clinical%20workflows%20but%20is%20hindered%20by%20poor%20tumor-tissue%20contrast%0Aand%20a%20scarcity%20of%20annotated%20data.%20This%20paper%20details%20our%20submission%20to%20the%0APANTHER%20challenge%2C%20addressing%20both%20diagnostic%20T1-weighted%20%28Task%201%29%20and%0Atherapeutic%20T2-weighted%20%28Task%202%29%20segmentation.%20Our%20approach%20is%20built%20upon%20the%0AnnU-Net%20framework%20and%20leverages%20a%20deep%2C%20multi-stage%20cascaded%20pre-training%0Astrategy%2C%20starting%20from%20a%20general%20anatomical%20foundation%20model%20and%20sequentially%0Afine-tuning%20on%20CT%20pancreatic%20lesion%20datasets%20and%20the%20target%20MRI%20modalities.%0AThrough%20extensive%20five-fold%20cross-validation%2C%20we%20systematically%20evaluated%20data%0Aaugmentation%20schemes%20and%20training%20schedules.%20Our%20analysis%20revealed%20a%20critical%0Atrade-off%2C%20where%20aggressive%20data%20augmentation%20produced%20the%20highest%20volumetric%0Aaccuracy%2C%20while%20default%20augmentations%20yielded%20superior%20boundary%20precision%0A%28achieving%20a%20state-of-the-art%20MASD%20of%205.46%20mm%20and%20HD95%20of%2017.33%20mm%20for%20Task%201%29.%0AFor%20our%20final%20submission%2C%20we%20exploited%20this%20finding%20by%20constructing%20custom%2C%0Aheterogeneous%20ensembles%20of%20specialist%20models%2C%20essentially%20creating%20a%20mix%20of%0Aexperts.%20This%20metric-aware%20ensembling%20strategy%20proved%20highly%20effective%2C%0Aachieving%20a%20top%20cross-validation%20Tumor%20Dice%20score%20of%200.661%20for%20Task%201%20and%200.523%0Afor%20Task%202.%20Our%20work%20presents%20a%20robust%20methodology%20for%20developing%20specialized%2C%0Ahigh-performance%20models%20in%20the%20context%20of%20limited%20data%20and%20complex%20medical%0Aimaging%20tasks%20%28Team%20MIC-DKFZ%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21775v1&entry.124074799=Read"},
{"title": "Reasoning-Intensive Regression", "author": "Diane Tchuindjo and Omar Khattab", "abstract": "  AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.\n", "link": "http://arxiv.org/abs/2508.21762v1", "date": "2025-08-29", "relevancy": 2.0838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning-Intensive%20Regression&body=Title%3A%20Reasoning-Intensive%20Regression%0AAuthor%3A%20Diane%20Tchuindjo%20and%20Omar%20Khattab%0AAbstract%3A%20%20%20AI%20researchers%20and%20practitioners%20increasingly%20apply%20large%20language%20models%0A%28LLMs%29%20to%20what%20we%20call%20reasoning-intensive%20regression%20%28RiR%29%2C%20i.e.%20deducing%0Asubtle%20numerical%20properties%20from%20text.%20Unlike%20standard%20language%20regression%0Atasks%2C%20e.g.%20for%20sentiment%20or%20similarity%2C%20RiR%20often%20appears%20instead%20in%20ad-hoc%0Aproblems%20like%20rubric-based%20scoring%20or%20domain-specific%20retrieval%2C%20where%20much%0Adeeper%20analysis%20of%20text%20is%20required%20while%20only%20limited%20task-specific%20training%0Adata%20and%20computation%20are%20available.%20We%20cast%20three%20realistic%20problems%20as%20RiR%0Atasks%20to%20establish%20an%20initial%20benchmark%2C%20and%20use%20that%20to%20test%20our%20hypothesis%0Athat%20prompting%20frozen%20LLMs%20and%20finetuning%20Transformer%20encoders%20via%20gradient%0Adescent%20will%20both%20often%20struggle%20in%20RiR.%20We%20then%20propose%20MENTAT%2C%20a%20simple%20and%0Alightweight%20method%20that%20combines%20batch-reflective%20prompt%20optimization%20with%0Aneural%20ensemble%20learning.%20MENTAT%20achieves%20up%20to%2065%25%20improvement%20over%20both%0Abaselines%2C%20though%20substantial%20room%20remains%20for%20future%20advances%20in%20RiR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning-Intensive%2520Regression%26entry.906535625%3DDiane%2520Tchuindjo%2520and%2520Omar%2520Khattab%26entry.1292438233%3D%2520%2520AI%2520researchers%2520and%2520practitioners%2520increasingly%2520apply%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520what%2520we%2520call%2520reasoning-intensive%2520regression%2520%2528RiR%2529%252C%2520i.e.%2520deducing%250Asubtle%2520numerical%2520properties%2520from%2520text.%2520Unlike%2520standard%2520language%2520regression%250Atasks%252C%2520e.g.%2520for%2520sentiment%2520or%2520similarity%252C%2520RiR%2520often%2520appears%2520instead%2520in%2520ad-hoc%250Aproblems%2520like%2520rubric-based%2520scoring%2520or%2520domain-specific%2520retrieval%252C%2520where%2520much%250Adeeper%2520analysis%2520of%2520text%2520is%2520required%2520while%2520only%2520limited%2520task-specific%2520training%250Adata%2520and%2520computation%2520are%2520available.%2520We%2520cast%2520three%2520realistic%2520problems%2520as%2520RiR%250Atasks%2520to%2520establish%2520an%2520initial%2520benchmark%252C%2520and%2520use%2520that%2520to%2520test%2520our%2520hypothesis%250Athat%2520prompting%2520frozen%2520LLMs%2520and%2520finetuning%2520Transformer%2520encoders%2520via%2520gradient%250Adescent%2520will%2520both%2520often%2520struggle%2520in%2520RiR.%2520We%2520then%2520propose%2520MENTAT%252C%2520a%2520simple%2520and%250Alightweight%2520method%2520that%2520combines%2520batch-reflective%2520prompt%2520optimization%2520with%250Aneural%2520ensemble%2520learning.%2520MENTAT%2520achieves%2520up%2520to%252065%2525%2520improvement%2520over%2520both%250Abaselines%252C%2520though%2520substantial%2520room%2520remains%2520for%2520future%2520advances%2520in%2520RiR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning-Intensive%20Regression&entry.906535625=Diane%20Tchuindjo%20and%20Omar%20Khattab&entry.1292438233=%20%20AI%20researchers%20and%20practitioners%20increasingly%20apply%20large%20language%20models%0A%28LLMs%29%20to%20what%20we%20call%20reasoning-intensive%20regression%20%28RiR%29%2C%20i.e.%20deducing%0Asubtle%20numerical%20properties%20from%20text.%20Unlike%20standard%20language%20regression%0Atasks%2C%20e.g.%20for%20sentiment%20or%20similarity%2C%20RiR%20often%20appears%20instead%20in%20ad-hoc%0Aproblems%20like%20rubric-based%20scoring%20or%20domain-specific%20retrieval%2C%20where%20much%0Adeeper%20analysis%20of%20text%20is%20required%20while%20only%20limited%20task-specific%20training%0Adata%20and%20computation%20are%20available.%20We%20cast%20three%20realistic%20problems%20as%20RiR%0Atasks%20to%20establish%20an%20initial%20benchmark%2C%20and%20use%20that%20to%20test%20our%20hypothesis%0Athat%20prompting%20frozen%20LLMs%20and%20finetuning%20Transformer%20encoders%20via%20gradient%0Adescent%20will%20both%20often%20struggle%20in%20RiR.%20We%20then%20propose%20MENTAT%2C%20a%20simple%20and%0Alightweight%20method%20that%20combines%20batch-reflective%20prompt%20optimization%20with%0Aneural%20ensemble%20learning.%20MENTAT%20achieves%20up%20to%2065%25%20improvement%20over%20both%0Abaselines%2C%20though%20substantial%20room%20remains%20for%20future%20advances%20in%20RiR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21762v1&entry.124074799=Read"},
{"title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR", "author": "Shashank Vempati and Nishit Anand and Gaurav Talebailkar and Arpan Garai and Chetan Arora", "abstract": "  Conventional optical character recognition (OCR) techniques segmented each\ncharacter and then recognized. This made them prone to error in character\nsegmentation, and devoid of context to exploit language models. Advances in\nsequence to sequence translation in last decade led to modern techniques first\ndetecting words and then inputting one word at a time to a model to directly\noutput full words as sequence of characters. This allowed better utilization of\nlanguage models and bypass error-prone character segmentation step. We observe\nthat the above transition in style has moved the bottleneck in accuracy to word\nsegmentation. Hence, in this paper, we propose a natural and logical\nprogression from word level OCR to line-level OCR. The proposal allows to\nbypass errors in word detection, and provides larger sentence context for\nbetter utilization of language models. We show that the proposed technique not\nonly improves the accuracy but also efficiency of OCR. Despite our thorough\nliterature survey, we did not find any public dataset to train and benchmark\nsuch shift from word to line-level OCR. Hence, we also contribute a\nmeticulously curated dataset of 251 English page images with line-level\nannotations. Our experimentation revealed a notable end-to-end accuracy\nimprovement of 5.4%, underscoring the potential benefits of transitioning\ntowards line-level OCR, especially for document images. We also report a 4\ntimes improvement in efficiency compared to word-based pipelines. With\ncontinuous improvements in large language models, our methodology also holds\npotential to exploit such advances. Project Website:\nhttps://nishitanand.github.io/line-level-ocr-website\n", "link": "http://arxiv.org/abs/2508.21693v1", "date": "2025-08-29", "relevancy": 2.0779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Stop%20at%20Words%3F%20Unveiling%20the%20Bigger%20Picture%20through%20Line-Level%20OCR&body=Title%3A%20Why%20Stop%20at%20Words%3F%20Unveiling%20the%20Bigger%20Picture%20through%20Line-Level%20OCR%0AAuthor%3A%20Shashank%20Vempati%20and%20Nishit%20Anand%20and%20Gaurav%20Talebailkar%20and%20Arpan%20Garai%20and%20Chetan%20Arora%0AAbstract%3A%20%20%20Conventional%20optical%20character%20recognition%20%28OCR%29%20techniques%20segmented%20each%0Acharacter%20and%20then%20recognized.%20This%20made%20them%20prone%20to%20error%20in%20character%0Asegmentation%2C%20and%20devoid%20of%20context%20to%20exploit%20language%20models.%20Advances%20in%0Asequence%20to%20sequence%20translation%20in%20last%20decade%20led%20to%20modern%20techniques%20first%0Adetecting%20words%20and%20then%20inputting%20one%20word%20at%20a%20time%20to%20a%20model%20to%20directly%0Aoutput%20full%20words%20as%20sequence%20of%20characters.%20This%20allowed%20better%20utilization%20of%0Alanguage%20models%20and%20bypass%20error-prone%20character%20segmentation%20step.%20We%20observe%0Athat%20the%20above%20transition%20in%20style%20has%20moved%20the%20bottleneck%20in%20accuracy%20to%20word%0Asegmentation.%20Hence%2C%20in%20this%20paper%2C%20we%20propose%20a%20natural%20and%20logical%0Aprogression%20from%20word%20level%20OCR%20to%20line-level%20OCR.%20The%20proposal%20allows%20to%0Abypass%20errors%20in%20word%20detection%2C%20and%20provides%20larger%20sentence%20context%20for%0Abetter%20utilization%20of%20language%20models.%20We%20show%20that%20the%20proposed%20technique%20not%0Aonly%20improves%20the%20accuracy%20but%20also%20efficiency%20of%20OCR.%20Despite%20our%20thorough%0Aliterature%20survey%2C%20we%20did%20not%20find%20any%20public%20dataset%20to%20train%20and%20benchmark%0Asuch%20shift%20from%20word%20to%20line-level%20OCR.%20Hence%2C%20we%20also%20contribute%20a%0Ameticulously%20curated%20dataset%20of%20251%20English%20page%20images%20with%20line-level%0Aannotations.%20Our%20experimentation%20revealed%20a%20notable%20end-to-end%20accuracy%0Aimprovement%20of%205.4%25%2C%20underscoring%20the%20potential%20benefits%20of%20transitioning%0Atowards%20line-level%20OCR%2C%20especially%20for%20document%20images.%20We%20also%20report%20a%204%0Atimes%20improvement%20in%20efficiency%20compared%20to%20word-based%20pipelines.%20With%0Acontinuous%20improvements%20in%20large%20language%20models%2C%20our%20methodology%20also%20holds%0Apotential%20to%20exploit%20such%20advances.%20Project%20Website%3A%0Ahttps%3A//nishitanand.github.io/line-level-ocr-website%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Stop%2520at%2520Words%253F%2520Unveiling%2520the%2520Bigger%2520Picture%2520through%2520Line-Level%2520OCR%26entry.906535625%3DShashank%2520Vempati%2520and%2520Nishit%2520Anand%2520and%2520Gaurav%2520Talebailkar%2520and%2520Arpan%2520Garai%2520and%2520Chetan%2520Arora%26entry.1292438233%3D%2520%2520Conventional%2520optical%2520character%2520recognition%2520%2528OCR%2529%2520techniques%2520segmented%2520each%250Acharacter%2520and%2520then%2520recognized.%2520This%2520made%2520them%2520prone%2520to%2520error%2520in%2520character%250Asegmentation%252C%2520and%2520devoid%2520of%2520context%2520to%2520exploit%2520language%2520models.%2520Advances%2520in%250Asequence%2520to%2520sequence%2520translation%2520in%2520last%2520decade%2520led%2520to%2520modern%2520techniques%2520first%250Adetecting%2520words%2520and%2520then%2520inputting%2520one%2520word%2520at%2520a%2520time%2520to%2520a%2520model%2520to%2520directly%250Aoutput%2520full%2520words%2520as%2520sequence%2520of%2520characters.%2520This%2520allowed%2520better%2520utilization%2520of%250Alanguage%2520models%2520and%2520bypass%2520error-prone%2520character%2520segmentation%2520step.%2520We%2520observe%250Athat%2520the%2520above%2520transition%2520in%2520style%2520has%2520moved%2520the%2520bottleneck%2520in%2520accuracy%2520to%2520word%250Asegmentation.%2520Hence%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520natural%2520and%2520logical%250Aprogression%2520from%2520word%2520level%2520OCR%2520to%2520line-level%2520OCR.%2520The%2520proposal%2520allows%2520to%250Abypass%2520errors%2520in%2520word%2520detection%252C%2520and%2520provides%2520larger%2520sentence%2520context%2520for%250Abetter%2520utilization%2520of%2520language%2520models.%2520We%2520show%2520that%2520the%2520proposed%2520technique%2520not%250Aonly%2520improves%2520the%2520accuracy%2520but%2520also%2520efficiency%2520of%2520OCR.%2520Despite%2520our%2520thorough%250Aliterature%2520survey%252C%2520we%2520did%2520not%2520find%2520any%2520public%2520dataset%2520to%2520train%2520and%2520benchmark%250Asuch%2520shift%2520from%2520word%2520to%2520line-level%2520OCR.%2520Hence%252C%2520we%2520also%2520contribute%2520a%250Ameticulously%2520curated%2520dataset%2520of%2520251%2520English%2520page%2520images%2520with%2520line-level%250Aannotations.%2520Our%2520experimentation%2520revealed%2520a%2520notable%2520end-to-end%2520accuracy%250Aimprovement%2520of%25205.4%2525%252C%2520underscoring%2520the%2520potential%2520benefits%2520of%2520transitioning%250Atowards%2520line-level%2520OCR%252C%2520especially%2520for%2520document%2520images.%2520We%2520also%2520report%2520a%25204%250Atimes%2520improvement%2520in%2520efficiency%2520compared%2520to%2520word-based%2520pipelines.%2520With%250Acontinuous%2520improvements%2520in%2520large%2520language%2520models%252C%2520our%2520methodology%2520also%2520holds%250Apotential%2520to%2520exploit%2520such%2520advances.%2520Project%2520Website%253A%250Ahttps%253A//nishitanand.github.io/line-level-ocr-website%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Stop%20at%20Words%3F%20Unveiling%20the%20Bigger%20Picture%20through%20Line-Level%20OCR&entry.906535625=Shashank%20Vempati%20and%20Nishit%20Anand%20and%20Gaurav%20Talebailkar%20and%20Arpan%20Garai%20and%20Chetan%20Arora&entry.1292438233=%20%20Conventional%20optical%20character%20recognition%20%28OCR%29%20techniques%20segmented%20each%0Acharacter%20and%20then%20recognized.%20This%20made%20them%20prone%20to%20error%20in%20character%0Asegmentation%2C%20and%20devoid%20of%20context%20to%20exploit%20language%20models.%20Advances%20in%0Asequence%20to%20sequence%20translation%20in%20last%20decade%20led%20to%20modern%20techniques%20first%0Adetecting%20words%20and%20then%20inputting%20one%20word%20at%20a%20time%20to%20a%20model%20to%20directly%0Aoutput%20full%20words%20as%20sequence%20of%20characters.%20This%20allowed%20better%20utilization%20of%0Alanguage%20models%20and%20bypass%20error-prone%20character%20segmentation%20step.%20We%20observe%0Athat%20the%20above%20transition%20in%20style%20has%20moved%20the%20bottleneck%20in%20accuracy%20to%20word%0Asegmentation.%20Hence%2C%20in%20this%20paper%2C%20we%20propose%20a%20natural%20and%20logical%0Aprogression%20from%20word%20level%20OCR%20to%20line-level%20OCR.%20The%20proposal%20allows%20to%0Abypass%20errors%20in%20word%20detection%2C%20and%20provides%20larger%20sentence%20context%20for%0Abetter%20utilization%20of%20language%20models.%20We%20show%20that%20the%20proposed%20technique%20not%0Aonly%20improves%20the%20accuracy%20but%20also%20efficiency%20of%20OCR.%20Despite%20our%20thorough%0Aliterature%20survey%2C%20we%20did%20not%20find%20any%20public%20dataset%20to%20train%20and%20benchmark%0Asuch%20shift%20from%20word%20to%20line-level%20OCR.%20Hence%2C%20we%20also%20contribute%20a%0Ameticulously%20curated%20dataset%20of%20251%20English%20page%20images%20with%20line-level%0Aannotations.%20Our%20experimentation%20revealed%20a%20notable%20end-to-end%20accuracy%0Aimprovement%20of%205.4%25%2C%20underscoring%20the%20potential%20benefits%20of%20transitioning%0Atowards%20line-level%20OCR%2C%20especially%20for%20document%20images.%20We%20also%20report%20a%204%0Atimes%20improvement%20in%20efficiency%20compared%20to%20word-based%20pipelines.%20With%0Acontinuous%20improvements%20in%20large%20language%20models%2C%20our%20methodology%20also%20holds%0Apotential%20to%20exploit%20such%20advances.%20Project%20Website%3A%0Ahttps%3A//nishitanand.github.io/line-level-ocr-website%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21693v1&entry.124074799=Read"},
{"title": "Maximising Kidney Glomeruli Segmentation using Minimal Labels via\n  Self-Supervision", "author": "Zeeshan Nisar and Thomas Lampert", "abstract": "  Histopathology, the microscopic examination of tissue samples, is essential\nfor disease diagnosis and prognosis. Accurate segmentation and identification\nof key regions in histopathology images are crucial for developing automated\nsolutions. However, state-of-art deep learning segmentation methods like UNet\nrequire extensive labels, which is both costly and time-consuming, particularly\nwhen dealing with multiple stainings. To mitigate this, various multi-stain\nsegmentation methods such as UDA-GAN have been developed, which reduce the need\nfor labels by requiring only one (source) stain to be labelled. Nonetheless,\nobtaining source stain labels can still be challenging, and segmentation models\nfail when they are unavailable. This article shows that through self-supervised\npre-training$-$including SimCLR, BYOL, and a novel approach, HR-CS-CO$-$the\nperformance of these segmentation methods (UNet, and UDAGAN) can be retained\neven with 95% fewer labels. Notably, with self-supervised pre-training and\nusing only 5% labels, the performance drops are minimal: 5.9% for UNet and 6.2%\nfor UDAGAN, compared to their respective fully supervised counterparts (without\npre-training, using 100% labels). Furthermore, these findings are shown to\ngeneralise beyond their training distribution to public benchmark datasets. Im-\n", "link": "http://arxiv.org/abs/2412.15389v2", "date": "2025-08-29", "relevancy": 2.077, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5273}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maximising%20Kidney%20Glomeruli%20Segmentation%20using%20Minimal%20Labels%20via%0A%20%20Self-Supervision&body=Title%3A%20Maximising%20Kidney%20Glomeruli%20Segmentation%20using%20Minimal%20Labels%20via%0A%20%20Self-Supervision%0AAuthor%3A%20Zeeshan%20Nisar%20and%20Thomas%20Lampert%0AAbstract%3A%20%20%20Histopathology%2C%20the%20microscopic%20examination%20of%20tissue%20samples%2C%20is%20essential%0Afor%20disease%20diagnosis%20and%20prognosis.%20Accurate%20segmentation%20and%20identification%0Aof%20key%20regions%20in%20histopathology%20images%20are%20crucial%20for%20developing%20automated%0Asolutions.%20However%2C%20state-of-art%20deep%20learning%20segmentation%20methods%20like%20UNet%0Arequire%20extensive%20labels%2C%20which%20is%20both%20costly%20and%20time-consuming%2C%20particularly%0Awhen%20dealing%20with%20multiple%20stainings.%20To%20mitigate%20this%2C%20various%20multi-stain%0Asegmentation%20methods%20such%20as%20UDA-GAN%20have%20been%20developed%2C%20which%20reduce%20the%20need%0Afor%20labels%20by%20requiring%20only%20one%20%28source%29%20stain%20to%20be%20labelled.%20Nonetheless%2C%0Aobtaining%20source%20stain%20labels%20can%20still%20be%20challenging%2C%20and%20segmentation%20models%0Afail%20when%20they%20are%20unavailable.%20This%20article%20shows%20that%20through%20self-supervised%0Apre-training%24-%24including%20SimCLR%2C%20BYOL%2C%20and%20a%20novel%20approach%2C%20HR-CS-CO%24-%24the%0Aperformance%20of%20these%20segmentation%20methods%20%28UNet%2C%20and%20UDAGAN%29%20can%20be%20retained%0Aeven%20with%2095%25%20fewer%20labels.%20Notably%2C%20with%20self-supervised%20pre-training%20and%0Ausing%20only%205%25%20labels%2C%20the%20performance%20drops%20are%20minimal%3A%205.9%25%20for%20UNet%20and%206.2%25%0Afor%20UDAGAN%2C%20compared%20to%20their%20respective%20fully%20supervised%20counterparts%20%28without%0Apre-training%2C%20using%20100%25%20labels%29.%20Furthermore%2C%20these%20findings%20are%20shown%20to%0Ageneralise%20beyond%20their%20training%20distribution%20to%20public%20benchmark%20datasets.%20Im-%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaximising%2520Kidney%2520Glomeruli%2520Segmentation%2520using%2520Minimal%2520Labels%2520via%250A%2520%2520Self-Supervision%26entry.906535625%3DZeeshan%2520Nisar%2520and%2520Thomas%2520Lampert%26entry.1292438233%3D%2520%2520Histopathology%252C%2520the%2520microscopic%2520examination%2520of%2520tissue%2520samples%252C%2520is%2520essential%250Afor%2520disease%2520diagnosis%2520and%2520prognosis.%2520Accurate%2520segmentation%2520and%2520identification%250Aof%2520key%2520regions%2520in%2520histopathology%2520images%2520are%2520crucial%2520for%2520developing%2520automated%250Asolutions.%2520However%252C%2520state-of-art%2520deep%2520learning%2520segmentation%2520methods%2520like%2520UNet%250Arequire%2520extensive%2520labels%252C%2520which%2520is%2520both%2520costly%2520and%2520time-consuming%252C%2520particularly%250Awhen%2520dealing%2520with%2520multiple%2520stainings.%2520To%2520mitigate%2520this%252C%2520various%2520multi-stain%250Asegmentation%2520methods%2520such%2520as%2520UDA-GAN%2520have%2520been%2520developed%252C%2520which%2520reduce%2520the%2520need%250Afor%2520labels%2520by%2520requiring%2520only%2520one%2520%2528source%2529%2520stain%2520to%2520be%2520labelled.%2520Nonetheless%252C%250Aobtaining%2520source%2520stain%2520labels%2520can%2520still%2520be%2520challenging%252C%2520and%2520segmentation%2520models%250Afail%2520when%2520they%2520are%2520unavailable.%2520This%2520article%2520shows%2520that%2520through%2520self-supervised%250Apre-training%2524-%2524including%2520SimCLR%252C%2520BYOL%252C%2520and%2520a%2520novel%2520approach%252C%2520HR-CS-CO%2524-%2524the%250Aperformance%2520of%2520these%2520segmentation%2520methods%2520%2528UNet%252C%2520and%2520UDAGAN%2529%2520can%2520be%2520retained%250Aeven%2520with%252095%2525%2520fewer%2520labels.%2520Notably%252C%2520with%2520self-supervised%2520pre-training%2520and%250Ausing%2520only%25205%2525%2520labels%252C%2520the%2520performance%2520drops%2520are%2520minimal%253A%25205.9%2525%2520for%2520UNet%2520and%25206.2%2525%250Afor%2520UDAGAN%252C%2520compared%2520to%2520their%2520respective%2520fully%2520supervised%2520counterparts%2520%2528without%250Apre-training%252C%2520using%2520100%2525%2520labels%2529.%2520Furthermore%252C%2520these%2520findings%2520are%2520shown%2520to%250Ageneralise%2520beyond%2520their%2520training%2520distribution%2520to%2520public%2520benchmark%2520datasets.%2520Im-%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maximising%20Kidney%20Glomeruli%20Segmentation%20using%20Minimal%20Labels%20via%0A%20%20Self-Supervision&entry.906535625=Zeeshan%20Nisar%20and%20Thomas%20Lampert&entry.1292438233=%20%20Histopathology%2C%20the%20microscopic%20examination%20of%20tissue%20samples%2C%20is%20essential%0Afor%20disease%20diagnosis%20and%20prognosis.%20Accurate%20segmentation%20and%20identification%0Aof%20key%20regions%20in%20histopathology%20images%20are%20crucial%20for%20developing%20automated%0Asolutions.%20However%2C%20state-of-art%20deep%20learning%20segmentation%20methods%20like%20UNet%0Arequire%20extensive%20labels%2C%20which%20is%20both%20costly%20and%20time-consuming%2C%20particularly%0Awhen%20dealing%20with%20multiple%20stainings.%20To%20mitigate%20this%2C%20various%20multi-stain%0Asegmentation%20methods%20such%20as%20UDA-GAN%20have%20been%20developed%2C%20which%20reduce%20the%20need%0Afor%20labels%20by%20requiring%20only%20one%20%28source%29%20stain%20to%20be%20labelled.%20Nonetheless%2C%0Aobtaining%20source%20stain%20labels%20can%20still%20be%20challenging%2C%20and%20segmentation%20models%0Afail%20when%20they%20are%20unavailable.%20This%20article%20shows%20that%20through%20self-supervised%0Apre-training%24-%24including%20SimCLR%2C%20BYOL%2C%20and%20a%20novel%20approach%2C%20HR-CS-CO%24-%24the%0Aperformance%20of%20these%20segmentation%20methods%20%28UNet%2C%20and%20UDAGAN%29%20can%20be%20retained%0Aeven%20with%2095%25%20fewer%20labels.%20Notably%2C%20with%20self-supervised%20pre-training%20and%0Ausing%20only%205%25%20labels%2C%20the%20performance%20drops%20are%20minimal%3A%205.9%25%20for%20UNet%20and%206.2%25%0Afor%20UDAGAN%2C%20compared%20to%20their%20respective%20fully%20supervised%20counterparts%20%28without%0Apre-training%2C%20using%20100%25%20labels%29.%20Furthermore%2C%20these%20findings%20are%20shown%20to%0Ageneralise%20beyond%20their%20training%20distribution%20to%20public%20benchmark%20datasets.%20Im-%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15389v2&entry.124074799=Read"},
{"title": "Towards Interactive Lesion Segmentation in Whole-Body PET/CT with\n  Promptable Models", "author": "Maximilian Rokuss and Yannick Kirchhoff and Fabian Isensee and Klaus H. Maier-Hein", "abstract": "  Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate\nlesion segmentation remains challenging due to tracer heterogeneity,\nphysiological uptake, and multi-center variability. While fully automated\nmethods have advanced substantially, clinical practice benefits from approaches\nthat keep humans in the loop to efficiently refine predicted masks. The\nautoPET/CT IV challenge addresses this need by introducing interactive\nsegmentation tasks based on simulated user prompts. In this work, we present\nour submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,\nwe extend the framework with promptable capabilities by encoding user-provided\nforeground and background clicks as additional input channels. We\nsystematically investigate representations for spatial prompts and demonstrate\nthat Euclidean Distance Transform (EDT) encodings consistently outperform\nGaussian kernels. Furthermore, we propose online simulation of user\ninteractions and a custom point sampling strategy to improve robustness under\nrealistic prompting conditions. Our ensemble of EDT-based models, trained with\nand without external data, achieves the strongest cross-validation performance,\nreducing both false positives and false negatives compared to baseline models.\nThese results highlight the potential of promptable models to enable efficient,\nuser-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code\nis publicly available at https://github.com/MIC-DKFZ/autoPET-interactive\n", "link": "http://arxiv.org/abs/2508.21680v1", "date": "2025-08-29", "relevancy": 2.0573, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5372}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interactive%20Lesion%20Segmentation%20in%20Whole-Body%20PET/CT%20with%0A%20%20Promptable%20Models&body=Title%3A%20Towards%20Interactive%20Lesion%20Segmentation%20in%20Whole-Body%20PET/CT%20with%0A%20%20Promptable%20Models%0AAuthor%3A%20Maximilian%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Klaus%20H.%20Maier-Hein%0AAbstract%3A%20%20%20Whole-body%20PET/CT%20is%20a%20cornerstone%20of%20oncological%20imaging%2C%20yet%20accurate%0Alesion%20segmentation%20remains%20challenging%20due%20to%20tracer%20heterogeneity%2C%0Aphysiological%20uptake%2C%20and%20multi-center%20variability.%20While%20fully%20automated%0Amethods%20have%20advanced%20substantially%2C%20clinical%20practice%20benefits%20from%20approaches%0Athat%20keep%20humans%20in%20the%20loop%20to%20efficiently%20refine%20predicted%20masks.%20The%0AautoPET/CT%20IV%20challenge%20addresses%20this%20need%20by%20introducing%20interactive%0Asegmentation%20tasks%20based%20on%20simulated%20user%20prompts.%20In%20this%20work%2C%20we%20present%0Aour%20submission%20to%20Task%201.%20Building%20on%20the%20winning%20autoPET%20III%20nnU-Net%20pipeline%2C%0Awe%20extend%20the%20framework%20with%20promptable%20capabilities%20by%20encoding%20user-provided%0Aforeground%20and%20background%20clicks%20as%20additional%20input%20channels.%20We%0Asystematically%20investigate%20representations%20for%20spatial%20prompts%20and%20demonstrate%0Athat%20Euclidean%20Distance%20Transform%20%28EDT%29%20encodings%20consistently%20outperform%0AGaussian%20kernels.%20Furthermore%2C%20we%20propose%20online%20simulation%20of%20user%0Ainteractions%20and%20a%20custom%20point%20sampling%20strategy%20to%20improve%20robustness%20under%0Arealistic%20prompting%20conditions.%20Our%20ensemble%20of%20EDT-based%20models%2C%20trained%20with%0Aand%20without%20external%20data%2C%20achieves%20the%20strongest%20cross-validation%20performance%2C%0Areducing%20both%20false%20positives%20and%20false%20negatives%20compared%20to%20baseline%20models.%0AThese%20results%20highlight%20the%20potential%20of%20promptable%20models%20to%20enable%20efficient%2C%0Auser-guided%20segmentation%20workflows%20in%20multi-tracer%2C%20multi-center%20PET/CT.%20Code%0Ais%20publicly%20available%20at%20https%3A//github.com/MIC-DKFZ/autoPET-interactive%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interactive%2520Lesion%2520Segmentation%2520in%2520Whole-Body%2520PET/CT%2520with%250A%2520%2520Promptable%2520Models%26entry.906535625%3DMaximilian%2520Rokuss%2520and%2520Yannick%2520Kirchhoff%2520and%2520Fabian%2520Isensee%2520and%2520Klaus%2520H.%2520Maier-Hein%26entry.1292438233%3D%2520%2520Whole-body%2520PET/CT%2520is%2520a%2520cornerstone%2520of%2520oncological%2520imaging%252C%2520yet%2520accurate%250Alesion%2520segmentation%2520remains%2520challenging%2520due%2520to%2520tracer%2520heterogeneity%252C%250Aphysiological%2520uptake%252C%2520and%2520multi-center%2520variability.%2520While%2520fully%2520automated%250Amethods%2520have%2520advanced%2520substantially%252C%2520clinical%2520practice%2520benefits%2520from%2520approaches%250Athat%2520keep%2520humans%2520in%2520the%2520loop%2520to%2520efficiently%2520refine%2520predicted%2520masks.%2520The%250AautoPET/CT%2520IV%2520challenge%2520addresses%2520this%2520need%2520by%2520introducing%2520interactive%250Asegmentation%2520tasks%2520based%2520on%2520simulated%2520user%2520prompts.%2520In%2520this%2520work%252C%2520we%2520present%250Aour%2520submission%2520to%2520Task%25201.%2520Building%2520on%2520the%2520winning%2520autoPET%2520III%2520nnU-Net%2520pipeline%252C%250Awe%2520extend%2520the%2520framework%2520with%2520promptable%2520capabilities%2520by%2520encoding%2520user-provided%250Aforeground%2520and%2520background%2520clicks%2520as%2520additional%2520input%2520channels.%2520We%250Asystematically%2520investigate%2520representations%2520for%2520spatial%2520prompts%2520and%2520demonstrate%250Athat%2520Euclidean%2520Distance%2520Transform%2520%2528EDT%2529%2520encodings%2520consistently%2520outperform%250AGaussian%2520kernels.%2520Furthermore%252C%2520we%2520propose%2520online%2520simulation%2520of%2520user%250Ainteractions%2520and%2520a%2520custom%2520point%2520sampling%2520strategy%2520to%2520improve%2520robustness%2520under%250Arealistic%2520prompting%2520conditions.%2520Our%2520ensemble%2520of%2520EDT-based%2520models%252C%2520trained%2520with%250Aand%2520without%2520external%2520data%252C%2520achieves%2520the%2520strongest%2520cross-validation%2520performance%252C%250Areducing%2520both%2520false%2520positives%2520and%2520false%2520negatives%2520compared%2520to%2520baseline%2520models.%250AThese%2520results%2520highlight%2520the%2520potential%2520of%2520promptable%2520models%2520to%2520enable%2520efficient%252C%250Auser-guided%2520segmentation%2520workflows%2520in%2520multi-tracer%252C%2520multi-center%2520PET/CT.%2520Code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/MIC-DKFZ/autoPET-interactive%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interactive%20Lesion%20Segmentation%20in%20Whole-Body%20PET/CT%20with%0A%20%20Promptable%20Models&entry.906535625=Maximilian%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Klaus%20H.%20Maier-Hein&entry.1292438233=%20%20Whole-body%20PET/CT%20is%20a%20cornerstone%20of%20oncological%20imaging%2C%20yet%20accurate%0Alesion%20segmentation%20remains%20challenging%20due%20to%20tracer%20heterogeneity%2C%0Aphysiological%20uptake%2C%20and%20multi-center%20variability.%20While%20fully%20automated%0Amethods%20have%20advanced%20substantially%2C%20clinical%20practice%20benefits%20from%20approaches%0Athat%20keep%20humans%20in%20the%20loop%20to%20efficiently%20refine%20predicted%20masks.%20The%0AautoPET/CT%20IV%20challenge%20addresses%20this%20need%20by%20introducing%20interactive%0Asegmentation%20tasks%20based%20on%20simulated%20user%20prompts.%20In%20this%20work%2C%20we%20present%0Aour%20submission%20to%20Task%201.%20Building%20on%20the%20winning%20autoPET%20III%20nnU-Net%20pipeline%2C%0Awe%20extend%20the%20framework%20with%20promptable%20capabilities%20by%20encoding%20user-provided%0Aforeground%20and%20background%20clicks%20as%20additional%20input%20channels.%20We%0Asystematically%20investigate%20representations%20for%20spatial%20prompts%20and%20demonstrate%0Athat%20Euclidean%20Distance%20Transform%20%28EDT%29%20encodings%20consistently%20outperform%0AGaussian%20kernels.%20Furthermore%2C%20we%20propose%20online%20simulation%20of%20user%0Ainteractions%20and%20a%20custom%20point%20sampling%20strategy%20to%20improve%20robustness%20under%0Arealistic%20prompting%20conditions.%20Our%20ensemble%20of%20EDT-based%20models%2C%20trained%20with%0Aand%20without%20external%20data%2C%20achieves%20the%20strongest%20cross-validation%20performance%2C%0Areducing%20both%20false%20positives%20and%20false%20negatives%20compared%20to%20baseline%20models.%0AThese%20results%20highlight%20the%20potential%20of%20promptable%20models%20to%20enable%20efficient%2C%0Auser-guided%20segmentation%20workflows%20in%20multi-tracer%2C%20multi-center%20PET/CT.%20Code%0Ais%20publicly%20available%20at%20https%3A//github.com/MIC-DKFZ/autoPET-interactive%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21680v1&entry.124074799=Read"},
{"title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding", "author": "Junyu Xiong and Yonghui Wang and Weichao Zhao and Chenyu Liu and Bing Yin and Wengang Zhou and Houqiang Li", "abstract": "  Understanding multi-page documents poses a significant challenge for\nmultimodal large language models (MLLMs), as it requires fine-grained visual\ncomprehension and multi-hop reasoning across pages. While prior work has\nexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,\nits application to multi-page document understanding remains underexplored. In\nthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,\nEvidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware\nreward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the\nmodel to first retrieve relevant pages before generating answers. This training\nparadigm enables us to build high-quality models with limited supervision. To\nsupport this, we design a two-stage annotation pipeline and a curriculum\nlearning strategy, based on which we construct two datasets: EviBench, a\nhigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluation\nbenchmark with 8.6k QA pairs based on scientific papers. Extensive experiments\nacross a wide range of benchmarks demonstrate that DocR1 achieves\nstate-of-the-art performance on multi-page tasks, while consistently\nmaintaining strong results on single-page benchmarks.\n", "link": "http://arxiv.org/abs/2508.07313v2", "date": "2025-08-29", "relevancy": 2.0491, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocR1%3A%20Evidence%20Page-Guided%20GRPO%20for%20Multi-Page%20Document%20Understanding&body=Title%3A%20DocR1%3A%20Evidence%20Page-Guided%20GRPO%20for%20Multi-Page%20Document%20Understanding%0AAuthor%3A%20Junyu%20Xiong%20and%20Yonghui%20Wang%20and%20Weichao%20Zhao%20and%20Chenyu%20Liu%20and%20Bing%20Yin%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20Understanding%20multi-page%20documents%20poses%20a%20significant%20challenge%20for%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20as%20it%20requires%20fine-grained%20visual%0Acomprehension%20and%20multi-hop%20reasoning%20across%20pages.%20While%20prior%20work%20has%0Aexplored%20reinforcement%20learning%20%28RL%29%20for%20enhancing%20advanced%20reasoning%20in%20MLLMs%2C%0Aits%20application%20to%20multi-page%20document%20understanding%20remains%20underexplored.%20In%0Athis%20paper%2C%20we%20introduce%20DocR1%2C%20an%20MLLM%20trained%20with%20a%20novel%20RL%20framework%2C%0AEvidence%20Page-Guided%20GRPO%20%28EviGRPO%29.%20EviGRPO%20incorporates%20an%20evidence-aware%0Areward%20mechanism%20that%20promotes%20a%20coarse-to-fine%20reasoning%20strategy%2C%20guiding%20the%0Amodel%20to%20first%20retrieve%20relevant%20pages%20before%20generating%20answers.%20This%20training%0Aparadigm%20enables%20us%20to%20build%20high-quality%20models%20with%20limited%20supervision.%20To%0Asupport%20this%2C%20we%20design%20a%20two-stage%20annotation%20pipeline%20and%20a%20curriculum%0Alearning%20strategy%2C%20based%20on%20which%20we%20construct%20two%20datasets%3A%20EviBench%2C%20a%0Ahigh-quality%20training%20set%20with%204.8k%20examples%2C%20and%20ArxivFullQA%2C%20an%20evaluation%0Abenchmark%20with%208.6k%20QA%20pairs%20based%20on%20scientific%20papers.%20Extensive%20experiments%0Aacross%20a%20wide%20range%20of%20benchmarks%20demonstrate%20that%20DocR1%20achieves%0Astate-of-the-art%20performance%20on%20multi-page%20tasks%2C%20while%20consistently%0Amaintaining%20strong%20results%20on%20single-page%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocR1%253A%2520Evidence%2520Page-Guided%2520GRPO%2520for%2520Multi-Page%2520Document%2520Understanding%26entry.906535625%3DJunyu%2520Xiong%2520and%2520Yonghui%2520Wang%2520and%2520Weichao%2520Zhao%2520and%2520Chenyu%2520Liu%2520and%2520Bing%2520Yin%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%26entry.1292438233%3D%2520%2520Understanding%2520multi-page%2520documents%2520poses%2520a%2520significant%2520challenge%2520for%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520as%2520it%2520requires%2520fine-grained%2520visual%250Acomprehension%2520and%2520multi-hop%2520reasoning%2520across%2520pages.%2520While%2520prior%2520work%2520has%250Aexplored%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520enhancing%2520advanced%2520reasoning%2520in%2520MLLMs%252C%250Aits%2520application%2520to%2520multi-page%2520document%2520understanding%2520remains%2520underexplored.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520DocR1%252C%2520an%2520MLLM%2520trained%2520with%2520a%2520novel%2520RL%2520framework%252C%250AEvidence%2520Page-Guided%2520GRPO%2520%2528EviGRPO%2529.%2520EviGRPO%2520incorporates%2520an%2520evidence-aware%250Areward%2520mechanism%2520that%2520promotes%2520a%2520coarse-to-fine%2520reasoning%2520strategy%252C%2520guiding%2520the%250Amodel%2520to%2520first%2520retrieve%2520relevant%2520pages%2520before%2520generating%2520answers.%2520This%2520training%250Aparadigm%2520enables%2520us%2520to%2520build%2520high-quality%2520models%2520with%2520limited%2520supervision.%2520To%250Asupport%2520this%252C%2520we%2520design%2520a%2520two-stage%2520annotation%2520pipeline%2520and%2520a%2520curriculum%250Alearning%2520strategy%252C%2520based%2520on%2520which%2520we%2520construct%2520two%2520datasets%253A%2520EviBench%252C%2520a%250Ahigh-quality%2520training%2520set%2520with%25204.8k%2520examples%252C%2520and%2520ArxivFullQA%252C%2520an%2520evaluation%250Abenchmark%2520with%25208.6k%2520QA%2520pairs%2520based%2520on%2520scientific%2520papers.%2520Extensive%2520experiments%250Aacross%2520a%2520wide%2520range%2520of%2520benchmarks%2520demonstrate%2520that%2520DocR1%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multi-page%2520tasks%252C%2520while%2520consistently%250Amaintaining%2520strong%2520results%2520on%2520single-page%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocR1%3A%20Evidence%20Page-Guided%20GRPO%20for%20Multi-Page%20Document%20Understanding&entry.906535625=Junyu%20Xiong%20and%20Yonghui%20Wang%20and%20Weichao%20Zhao%20and%20Chenyu%20Liu%20and%20Bing%20Yin%20and%20Wengang%20Zhou%20and%20Houqiang%20Li&entry.1292438233=%20%20Understanding%20multi-page%20documents%20poses%20a%20significant%20challenge%20for%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20as%20it%20requires%20fine-grained%20visual%0Acomprehension%20and%20multi-hop%20reasoning%20across%20pages.%20While%20prior%20work%20has%0Aexplored%20reinforcement%20learning%20%28RL%29%20for%20enhancing%20advanced%20reasoning%20in%20MLLMs%2C%0Aits%20application%20to%20multi-page%20document%20understanding%20remains%20underexplored.%20In%0Athis%20paper%2C%20we%20introduce%20DocR1%2C%20an%20MLLM%20trained%20with%20a%20novel%20RL%20framework%2C%0AEvidence%20Page-Guided%20GRPO%20%28EviGRPO%29.%20EviGRPO%20incorporates%20an%20evidence-aware%0Areward%20mechanism%20that%20promotes%20a%20coarse-to-fine%20reasoning%20strategy%2C%20guiding%20the%0Amodel%20to%20first%20retrieve%20relevant%20pages%20before%20generating%20answers.%20This%20training%0Aparadigm%20enables%20us%20to%20build%20high-quality%20models%20with%20limited%20supervision.%20To%0Asupport%20this%2C%20we%20design%20a%20two-stage%20annotation%20pipeline%20and%20a%20curriculum%0Alearning%20strategy%2C%20based%20on%20which%20we%20construct%20two%20datasets%3A%20EviBench%2C%20a%0Ahigh-quality%20training%20set%20with%204.8k%20examples%2C%20and%20ArxivFullQA%2C%20an%20evaluation%0Abenchmark%20with%208.6k%20QA%20pairs%20based%20on%20scientific%20papers.%20Extensive%20experiments%0Aacross%20a%20wide%20range%20of%20benchmarks%20demonstrate%20that%20DocR1%20achieves%0Astate-of-the-art%20performance%20on%20multi-page%20tasks%2C%20while%20consistently%0Amaintaining%20strong%20results%20on%20single-page%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07313v2&entry.124074799=Read"},
{"title": "NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic\n  pruning and dendritic integration", "author": "Wuque Cai and Hongze Sun and Jiayi He and Qianqian Liao and Yunliang Zang and Duo Chen and Dezhong Yao and Daqing Guo", "abstract": "  Spiking neural networks (SNNs) are artificial neural networks based on\nsimulated biological neurons and have attracted much attention in recent\nartificial intelligence technology studies. The dendrites in biological neurons\nhave efficient information processing ability and computational power; however,\nthe neurons of SNNs rarely match the complex structure of the dendrites.\nInspired by the nonlinear structure and highly sparse properties of neuronal\ndendrites, in this study, we propose an efficient, lightweight SNN method with\nnonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we\nintroduce nonlinear dendritic integration (NDI) to improve the representation\nof the spatiotemporal information of neurons. We implement heterogeneous state\ntransition ratios of dendritic spines and construct a new and flexible\nnonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We\nconducted systematic experiments on three benchmark datasets (DVS128 Gesture,\nCIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks\n(speech recognition and reinforcement learning-based maze navigation task).\nAcross all tasks, NSPDI-SNN consistently achieved high sparsity with minimal\nperformance degradation. In particular, our method achieved the best\nexperimental results on all three event stream datasets. Further analysis\nshowed that NSPDI significantly improved the efficiency of synaptic information\ntransfer as sparsity increased. In conclusion, our results indicate that the\ncomplex structure and nonlinear computation of neuronal dendrites provide a\npromising approach for developing efficient SNN methods.\n", "link": "http://arxiv.org/abs/2508.21566v1", "date": "2025-08-29", "relevancy": 2.0462, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NSPDI-SNN%3A%20An%20efficient%20lightweight%20SNN%20based%20on%20nonlinear%20synaptic%0A%20%20pruning%20and%20dendritic%20integration&body=Title%3A%20NSPDI-SNN%3A%20An%20efficient%20lightweight%20SNN%20based%20on%20nonlinear%20synaptic%0A%20%20pruning%20and%20dendritic%20integration%0AAuthor%3A%20Wuque%20Cai%20and%20Hongze%20Sun%20and%20Jiayi%20He%20and%20Qianqian%20Liao%20and%20Yunliang%20Zang%20and%20Duo%20Chen%20and%20Dezhong%20Yao%20and%20Daqing%20Guo%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20are%20artificial%20neural%20networks%20based%20on%0Asimulated%20biological%20neurons%20and%20have%20attracted%20much%20attention%20in%20recent%0Aartificial%20intelligence%20technology%20studies.%20The%20dendrites%20in%20biological%20neurons%0Ahave%20efficient%20information%20processing%20ability%20and%20computational%20power%3B%20however%2C%0Athe%20neurons%20of%20SNNs%20rarely%20match%20the%20complex%20structure%20of%20the%20dendrites.%0AInspired%20by%20the%20nonlinear%20structure%20and%20highly%20sparse%20properties%20of%20neuronal%0Adendrites%2C%20in%20this%20study%2C%20we%20propose%20an%20efficient%2C%20lightweight%20SNN%20method%20with%0Anonlinear%20pruning%20and%20dendritic%20integration%20%28NSPDI-SNN%29.%20In%20this%20method%2C%20we%0Aintroduce%20nonlinear%20dendritic%20integration%20%28NDI%29%20to%20improve%20the%20representation%0Aof%20the%20spatiotemporal%20information%20of%20neurons.%20We%20implement%20heterogeneous%20state%0Atransition%20ratios%20of%20dendritic%20spines%20and%20construct%20a%20new%20and%20flexible%0Anonlinear%20synaptic%20pruning%20%28NSP%29%20method%20to%20achieve%20the%20high%20sparsity%20of%20SNN.%20We%0Aconducted%20systematic%20experiments%20on%20three%20benchmark%20datasets%20%28DVS128%20Gesture%2C%0ACIFAR10-DVS%2C%20and%20CIFAR10%29%20and%20extended%20the%20evaluation%20to%20two%20complex%20tasks%0A%28speech%20recognition%20and%20reinforcement%20learning-based%20maze%20navigation%20task%29.%0AAcross%20all%20tasks%2C%20NSPDI-SNN%20consistently%20achieved%20high%20sparsity%20with%20minimal%0Aperformance%20degradation.%20In%20particular%2C%20our%20method%20achieved%20the%20best%0Aexperimental%20results%20on%20all%20three%20event%20stream%20datasets.%20Further%20analysis%0Ashowed%20that%20NSPDI%20significantly%20improved%20the%20efficiency%20of%20synaptic%20information%0Atransfer%20as%20sparsity%20increased.%20In%20conclusion%2C%20our%20results%20indicate%20that%20the%0Acomplex%20structure%20and%20nonlinear%20computation%20of%20neuronal%20dendrites%20provide%20a%0Apromising%20approach%20for%20developing%20efficient%20SNN%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNSPDI-SNN%253A%2520An%2520efficient%2520lightweight%2520SNN%2520based%2520on%2520nonlinear%2520synaptic%250A%2520%2520pruning%2520and%2520dendritic%2520integration%26entry.906535625%3DWuque%2520Cai%2520and%2520Hongze%2520Sun%2520and%2520Jiayi%2520He%2520and%2520Qianqian%2520Liao%2520and%2520Yunliang%2520Zang%2520and%2520Duo%2520Chen%2520and%2520Dezhong%2520Yao%2520and%2520Daqing%2520Guo%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520are%2520artificial%2520neural%2520networks%2520based%2520on%250Asimulated%2520biological%2520neurons%2520and%2520have%2520attracted%2520much%2520attention%2520in%2520recent%250Aartificial%2520intelligence%2520technology%2520studies.%2520The%2520dendrites%2520in%2520biological%2520neurons%250Ahave%2520efficient%2520information%2520processing%2520ability%2520and%2520computational%2520power%253B%2520however%252C%250Athe%2520neurons%2520of%2520SNNs%2520rarely%2520match%2520the%2520complex%2520structure%2520of%2520the%2520dendrites.%250AInspired%2520by%2520the%2520nonlinear%2520structure%2520and%2520highly%2520sparse%2520properties%2520of%2520neuronal%250Adendrites%252C%2520in%2520this%2520study%252C%2520we%2520propose%2520an%2520efficient%252C%2520lightweight%2520SNN%2520method%2520with%250Anonlinear%2520pruning%2520and%2520dendritic%2520integration%2520%2528NSPDI-SNN%2529.%2520In%2520this%2520method%252C%2520we%250Aintroduce%2520nonlinear%2520dendritic%2520integration%2520%2528NDI%2529%2520to%2520improve%2520the%2520representation%250Aof%2520the%2520spatiotemporal%2520information%2520of%2520neurons.%2520We%2520implement%2520heterogeneous%2520state%250Atransition%2520ratios%2520of%2520dendritic%2520spines%2520and%2520construct%2520a%2520new%2520and%2520flexible%250Anonlinear%2520synaptic%2520pruning%2520%2528NSP%2529%2520method%2520to%2520achieve%2520the%2520high%2520sparsity%2520of%2520SNN.%2520We%250Aconducted%2520systematic%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520%2528DVS128%2520Gesture%252C%250ACIFAR10-DVS%252C%2520and%2520CIFAR10%2529%2520and%2520extended%2520the%2520evaluation%2520to%2520two%2520complex%2520tasks%250A%2528speech%2520recognition%2520and%2520reinforcement%2520learning-based%2520maze%2520navigation%2520task%2529.%250AAcross%2520all%2520tasks%252C%2520NSPDI-SNN%2520consistently%2520achieved%2520high%2520sparsity%2520with%2520minimal%250Aperformance%2520degradation.%2520In%2520particular%252C%2520our%2520method%2520achieved%2520the%2520best%250Aexperimental%2520results%2520on%2520all%2520three%2520event%2520stream%2520datasets.%2520Further%2520analysis%250Ashowed%2520that%2520NSPDI%2520significantly%2520improved%2520the%2520efficiency%2520of%2520synaptic%2520information%250Atransfer%2520as%2520sparsity%2520increased.%2520In%2520conclusion%252C%2520our%2520results%2520indicate%2520that%2520the%250Acomplex%2520structure%2520and%2520nonlinear%2520computation%2520of%2520neuronal%2520dendrites%2520provide%2520a%250Apromising%2520approach%2520for%2520developing%2520efficient%2520SNN%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NSPDI-SNN%3A%20An%20efficient%20lightweight%20SNN%20based%20on%20nonlinear%20synaptic%0A%20%20pruning%20and%20dendritic%20integration&entry.906535625=Wuque%20Cai%20and%20Hongze%20Sun%20and%20Jiayi%20He%20and%20Qianqian%20Liao%20and%20Yunliang%20Zang%20and%20Duo%20Chen%20and%20Dezhong%20Yao%20and%20Daqing%20Guo&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20are%20artificial%20neural%20networks%20based%20on%0Asimulated%20biological%20neurons%20and%20have%20attracted%20much%20attention%20in%20recent%0Aartificial%20intelligence%20technology%20studies.%20The%20dendrites%20in%20biological%20neurons%0Ahave%20efficient%20information%20processing%20ability%20and%20computational%20power%3B%20however%2C%0Athe%20neurons%20of%20SNNs%20rarely%20match%20the%20complex%20structure%20of%20the%20dendrites.%0AInspired%20by%20the%20nonlinear%20structure%20and%20highly%20sparse%20properties%20of%20neuronal%0Adendrites%2C%20in%20this%20study%2C%20we%20propose%20an%20efficient%2C%20lightweight%20SNN%20method%20with%0Anonlinear%20pruning%20and%20dendritic%20integration%20%28NSPDI-SNN%29.%20In%20this%20method%2C%20we%0Aintroduce%20nonlinear%20dendritic%20integration%20%28NDI%29%20to%20improve%20the%20representation%0Aof%20the%20spatiotemporal%20information%20of%20neurons.%20We%20implement%20heterogeneous%20state%0Atransition%20ratios%20of%20dendritic%20spines%20and%20construct%20a%20new%20and%20flexible%0Anonlinear%20synaptic%20pruning%20%28NSP%29%20method%20to%20achieve%20the%20high%20sparsity%20of%20SNN.%20We%0Aconducted%20systematic%20experiments%20on%20three%20benchmark%20datasets%20%28DVS128%20Gesture%2C%0ACIFAR10-DVS%2C%20and%20CIFAR10%29%20and%20extended%20the%20evaluation%20to%20two%20complex%20tasks%0A%28speech%20recognition%20and%20reinforcement%20learning-based%20maze%20navigation%20task%29.%0AAcross%20all%20tasks%2C%20NSPDI-SNN%20consistently%20achieved%20high%20sparsity%20with%20minimal%0Aperformance%20degradation.%20In%20particular%2C%20our%20method%20achieved%20the%20best%0Aexperimental%20results%20on%20all%20three%20event%20stream%20datasets.%20Further%20analysis%0Ashowed%20that%20NSPDI%20significantly%20improved%20the%20efficiency%20of%20synaptic%20information%0Atransfer%20as%20sparsity%20increased.%20In%20conclusion%2C%20our%20results%20indicate%20that%20the%0Acomplex%20structure%20and%20nonlinear%20computation%20of%20neuronal%20dendrites%20provide%20a%0Apromising%20approach%20for%20developing%20efficient%20SNN%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21566v1&entry.124074799=Read"},
{"title": "Single Domain Generalization for Multimodal Cross-Cancer Prognosis via\n  Dirac Rebalancer and Distribution Entanglement", "author": "Jia-Xuan Jiang and Jiashuai Liu and Hongtao Wu and Yifeng Wu and Zhong Wang and Qi Bi and Yefeng Zheng", "abstract": "  Deep learning has shown remarkable performance in integrating multimodal data\nfor survival prediction. However, existing multimodal methods mainly focus on\nsingle cancer types and overlook the challenge of generalization across\ncancers. In this work, we are the first to reveal that multimodal prognosis\nmodels often generalize worse than unimodal ones in cross-cancer scenarios,\ndespite the critical need for such robustness in clinical practice. To address\nthis, we propose a new task: Cross-Cancer Single Domain Generalization for\nMultimodal Prognosis, which evaluates whether models trained on a single cancer\ntype can generalize to unseen cancers. We identify two key challenges: degraded\nfeatures from weaker modalities and ineffective multimodal integration. To\ntackle these, we introduce two plug-and-play modules: Sparse Dirac Information\nRebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR\nmitigates the dominance of strong features by applying Bernoulli-based\nsparsification and Dirac-inspired stabilization to enhance weaker modality\nsignals. CADE, designed to synthesize the target domain distribution, fuses\nlocal morphological cues and global gene expression in latent space.\nExperiments on a four-cancer-type benchmark demonstrate superior\ngeneralization, laying the foundation for practical, robust cross-cancer\nmultimodal prognosis. Code is available at\nhttps://github.com/HopkinsKwong/MCCSDG\n", "link": "http://arxiv.org/abs/2507.08340v2", "date": "2025-08-29", "relevancy": 2.0314, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5229}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Domain%20Generalization%20for%20Multimodal%20Cross-Cancer%20Prognosis%20via%0A%20%20Dirac%20Rebalancer%20and%20Distribution%20Entanglement&body=Title%3A%20Single%20Domain%20Generalization%20for%20Multimodal%20Cross-Cancer%20Prognosis%20via%0A%20%20Dirac%20Rebalancer%20and%20Distribution%20Entanglement%0AAuthor%3A%20Jia-Xuan%20Jiang%20and%20Jiashuai%20Liu%20and%20Hongtao%20Wu%20and%20Yifeng%20Wu%20and%20Zhong%20Wang%20and%20Qi%20Bi%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Deep%20learning%20has%20shown%20remarkable%20performance%20in%20integrating%20multimodal%20data%0Afor%20survival%20prediction.%20However%2C%20existing%20multimodal%20methods%20mainly%20focus%20on%0Asingle%20cancer%20types%20and%20overlook%20the%20challenge%20of%20generalization%20across%0Acancers.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20reveal%20that%20multimodal%20prognosis%0Amodels%20often%20generalize%20worse%20than%20unimodal%20ones%20in%20cross-cancer%20scenarios%2C%0Adespite%20the%20critical%20need%20for%20such%20robustness%20in%20clinical%20practice.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20task%3A%20Cross-Cancer%20Single%20Domain%20Generalization%20for%0AMultimodal%20Prognosis%2C%20which%20evaluates%20whether%20models%20trained%20on%20a%20single%20cancer%0Atype%20can%20generalize%20to%20unseen%20cancers.%20We%20identify%20two%20key%20challenges%3A%20degraded%0Afeatures%20from%20weaker%20modalities%20and%20ineffective%20multimodal%20integration.%20To%0Atackle%20these%2C%20we%20introduce%20two%20plug-and-play%20modules%3A%20Sparse%20Dirac%20Information%0ARebalancer%20%28SDIR%29%20and%20Cancer-aware%20Distribution%20Entanglement%20%28CADE%29.%20SDIR%0Amitigates%20the%20dominance%20of%20strong%20features%20by%20applying%20Bernoulli-based%0Asparsification%20and%20Dirac-inspired%20stabilization%20to%20enhance%20weaker%20modality%0Asignals.%20CADE%2C%20designed%20to%20synthesize%20the%20target%20domain%20distribution%2C%20fuses%0Alocal%20morphological%20cues%20and%20global%20gene%20expression%20in%20latent%20space.%0AExperiments%20on%20a%20four-cancer-type%20benchmark%20demonstrate%20superior%0Ageneralization%2C%20laying%20the%20foundation%20for%20practical%2C%20robust%20cross-cancer%0Amultimodal%20prognosis.%20Code%20is%20available%20at%0Ahttps%3A//github.com/HopkinsKwong/MCCSDG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Domain%2520Generalization%2520for%2520Multimodal%2520Cross-Cancer%2520Prognosis%2520via%250A%2520%2520Dirac%2520Rebalancer%2520and%2520Distribution%2520Entanglement%26entry.906535625%3DJia-Xuan%2520Jiang%2520and%2520Jiashuai%2520Liu%2520and%2520Hongtao%2520Wu%2520and%2520Yifeng%2520Wu%2520and%2520Zhong%2520Wang%2520and%2520Qi%2520Bi%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520shown%2520remarkable%2520performance%2520in%2520integrating%2520multimodal%2520data%250Afor%2520survival%2520prediction.%2520However%252C%2520existing%2520multimodal%2520methods%2520mainly%2520focus%2520on%250Asingle%2520cancer%2520types%2520and%2520overlook%2520the%2520challenge%2520of%2520generalization%2520across%250Acancers.%2520In%2520this%2520work%252C%2520we%2520are%2520the%2520first%2520to%2520reveal%2520that%2520multimodal%2520prognosis%250Amodels%2520often%2520generalize%2520worse%2520than%2520unimodal%2520ones%2520in%2520cross-cancer%2520scenarios%252C%250Adespite%2520the%2520critical%2520need%2520for%2520such%2520robustness%2520in%2520clinical%2520practice.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520new%2520task%253A%2520Cross-Cancer%2520Single%2520Domain%2520Generalization%2520for%250AMultimodal%2520Prognosis%252C%2520which%2520evaluates%2520whether%2520models%2520trained%2520on%2520a%2520single%2520cancer%250Atype%2520can%2520generalize%2520to%2520unseen%2520cancers.%2520We%2520identify%2520two%2520key%2520challenges%253A%2520degraded%250Afeatures%2520from%2520weaker%2520modalities%2520and%2520ineffective%2520multimodal%2520integration.%2520To%250Atackle%2520these%252C%2520we%2520introduce%2520two%2520plug-and-play%2520modules%253A%2520Sparse%2520Dirac%2520Information%250ARebalancer%2520%2528SDIR%2529%2520and%2520Cancer-aware%2520Distribution%2520Entanglement%2520%2528CADE%2529.%2520SDIR%250Amitigates%2520the%2520dominance%2520of%2520strong%2520features%2520by%2520applying%2520Bernoulli-based%250Asparsification%2520and%2520Dirac-inspired%2520stabilization%2520to%2520enhance%2520weaker%2520modality%250Asignals.%2520CADE%252C%2520designed%2520to%2520synthesize%2520the%2520target%2520domain%2520distribution%252C%2520fuses%250Alocal%2520morphological%2520cues%2520and%2520global%2520gene%2520expression%2520in%2520latent%2520space.%250AExperiments%2520on%2520a%2520four-cancer-type%2520benchmark%2520demonstrate%2520superior%250Ageneralization%252C%2520laying%2520the%2520foundation%2520for%2520practical%252C%2520robust%2520cross-cancer%250Amultimodal%2520prognosis.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/HopkinsKwong/MCCSDG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Domain%20Generalization%20for%20Multimodal%20Cross-Cancer%20Prognosis%20via%0A%20%20Dirac%20Rebalancer%20and%20Distribution%20Entanglement&entry.906535625=Jia-Xuan%20Jiang%20and%20Jiashuai%20Liu%20and%20Hongtao%20Wu%20and%20Yifeng%20Wu%20and%20Zhong%20Wang%20and%20Qi%20Bi%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Deep%20learning%20has%20shown%20remarkable%20performance%20in%20integrating%20multimodal%20data%0Afor%20survival%20prediction.%20However%2C%20existing%20multimodal%20methods%20mainly%20focus%20on%0Asingle%20cancer%20types%20and%20overlook%20the%20challenge%20of%20generalization%20across%0Acancers.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20reveal%20that%20multimodal%20prognosis%0Amodels%20often%20generalize%20worse%20than%20unimodal%20ones%20in%20cross-cancer%20scenarios%2C%0Adespite%20the%20critical%20need%20for%20such%20robustness%20in%20clinical%20practice.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20task%3A%20Cross-Cancer%20Single%20Domain%20Generalization%20for%0AMultimodal%20Prognosis%2C%20which%20evaluates%20whether%20models%20trained%20on%20a%20single%20cancer%0Atype%20can%20generalize%20to%20unseen%20cancers.%20We%20identify%20two%20key%20challenges%3A%20degraded%0Afeatures%20from%20weaker%20modalities%20and%20ineffective%20multimodal%20integration.%20To%0Atackle%20these%2C%20we%20introduce%20two%20plug-and-play%20modules%3A%20Sparse%20Dirac%20Information%0ARebalancer%20%28SDIR%29%20and%20Cancer-aware%20Distribution%20Entanglement%20%28CADE%29.%20SDIR%0Amitigates%20the%20dominance%20of%20strong%20features%20by%20applying%20Bernoulli-based%0Asparsification%20and%20Dirac-inspired%20stabilization%20to%20enhance%20weaker%20modality%0Asignals.%20CADE%2C%20designed%20to%20synthesize%20the%20target%20domain%20distribution%2C%20fuses%0Alocal%20morphological%20cues%20and%20global%20gene%20expression%20in%20latent%20space.%0AExperiments%20on%20a%20four-cancer-type%20benchmark%20demonstrate%20superior%0Ageneralization%2C%20laying%20the%20foundation%20for%20practical%2C%20robust%20cross-cancer%0Amultimodal%20prognosis.%20Code%20is%20available%20at%0Ahttps%3A//github.com/HopkinsKwong/MCCSDG%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08340v2&entry.124074799=Read"},
{"title": "Achieving Hilbert-Schmidt Independence Under R\u00e9nyi Differential\n  Privacy for Fair and Private Data Generation", "author": "Tobias Hyrup and Emmanouil Panagiotou and Arjun Roy and Arthur Zimek and Eirini Ntoutsi and Peter Schneider-Kamp", "abstract": "  As privacy regulations such as the GDPR and HIPAA and responsibility\nframeworks for artificial intelligence such as the AI Act gain traction, the\nethical and responsible use of real-world data faces increasing constraints.\nSynthetic data generation has emerged as a promising solution to risk-aware\ndata sharing and model development, particularly for tabular datasets that are\nfoundational to sensitive domains such as healthcare. To address both privacy\nand fairness concerns in this setting, we propose FLIP (Fair Latent\nIntervention under Privacy guarantees), a transformer-based variational\nautoencoder augmented with latent diffusion to generate heterogeneous tabular\ndata. Unlike the typical setup in fairness-aware data generation, we assume a\ntask-agnostic setup, not reliant on a fixed, defined downstream task, thus\noffering broader applicability. To ensure privacy, FLIP employs R\\'enyi\ndifferential privacy (RDP) constraints during training and addresses fairness\nin the input space with RDP-compatible balanced sampling that accounts for\ngroup-specific noise levels across multiple sampling rates. In the latent\nspace, we promote fairness by aligning neuron activation patterns across\nprotected groups using Centered Kernel Alignment (CKA), a similarity measure\nextending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment\nencourages statistical independence between latent representations and the\nprotected feature. Empirical results demonstrate that FLIP effectively provides\nsignificant fairness improvements for task-agnostic fairness and across diverse\ndownstream tasks under differential privacy constraints.\n", "link": "http://arxiv.org/abs/2508.21815v1", "date": "2025-08-29", "relevancy": 2.0266, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5064}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20Hilbert-Schmidt%20Independence%20Under%20R%C3%A9nyi%20Differential%0A%20%20Privacy%20for%20Fair%20and%20Private%20Data%20Generation&body=Title%3A%20Achieving%20Hilbert-Schmidt%20Independence%20Under%20R%C3%A9nyi%20Differential%0A%20%20Privacy%20for%20Fair%20and%20Private%20Data%20Generation%0AAuthor%3A%20Tobias%20Hyrup%20and%20Emmanouil%20Panagiotou%20and%20Arjun%20Roy%20and%20Arthur%20Zimek%20and%20Eirini%20Ntoutsi%20and%20Peter%20Schneider-Kamp%0AAbstract%3A%20%20%20As%20privacy%20regulations%20such%20as%20the%20GDPR%20and%20HIPAA%20and%20responsibility%0Aframeworks%20for%20artificial%20intelligence%20such%20as%20the%20AI%20Act%20gain%20traction%2C%20the%0Aethical%20and%20responsible%20use%20of%20real-world%20data%20faces%20increasing%20constraints.%0ASynthetic%20data%20generation%20has%20emerged%20as%20a%20promising%20solution%20to%20risk-aware%0Adata%20sharing%20and%20model%20development%2C%20particularly%20for%20tabular%20datasets%20that%20are%0Afoundational%20to%20sensitive%20domains%20such%20as%20healthcare.%20To%20address%20both%20privacy%0Aand%20fairness%20concerns%20in%20this%20setting%2C%20we%20propose%20FLIP%20%28Fair%20Latent%0AIntervention%20under%20Privacy%20guarantees%29%2C%20a%20transformer-based%20variational%0Aautoencoder%20augmented%20with%20latent%20diffusion%20to%20generate%20heterogeneous%20tabular%0Adata.%20Unlike%20the%20typical%20setup%20in%20fairness-aware%20data%20generation%2C%20we%20assume%20a%0Atask-agnostic%20setup%2C%20not%20reliant%20on%20a%20fixed%2C%20defined%20downstream%20task%2C%20thus%0Aoffering%20broader%20applicability.%20To%20ensure%20privacy%2C%20FLIP%20employs%20R%5C%27enyi%0Adifferential%20privacy%20%28RDP%29%20constraints%20during%20training%20and%20addresses%20fairness%0Ain%20the%20input%20space%20with%20RDP-compatible%20balanced%20sampling%20that%20accounts%20for%0Agroup-specific%20noise%20levels%20across%20multiple%20sampling%20rates.%20In%20the%20latent%0Aspace%2C%20we%20promote%20fairness%20by%20aligning%20neuron%20activation%20patterns%20across%0Aprotected%20groups%20using%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20measure%0Aextending%20the%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29.%20This%20alignment%0Aencourages%20statistical%20independence%20between%20latent%20representations%20and%20the%0Aprotected%20feature.%20Empirical%20results%20demonstrate%20that%20FLIP%20effectively%20provides%0Asignificant%20fairness%20improvements%20for%20task-agnostic%20fairness%20and%20across%20diverse%0Adownstream%20tasks%20under%20differential%20privacy%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520Hilbert-Schmidt%2520Independence%2520Under%2520R%25C3%25A9nyi%2520Differential%250A%2520%2520Privacy%2520for%2520Fair%2520and%2520Private%2520Data%2520Generation%26entry.906535625%3DTobias%2520Hyrup%2520and%2520Emmanouil%2520Panagiotou%2520and%2520Arjun%2520Roy%2520and%2520Arthur%2520Zimek%2520and%2520Eirini%2520Ntoutsi%2520and%2520Peter%2520Schneider-Kamp%26entry.1292438233%3D%2520%2520As%2520privacy%2520regulations%2520such%2520as%2520the%2520GDPR%2520and%2520HIPAA%2520and%2520responsibility%250Aframeworks%2520for%2520artificial%2520intelligence%2520such%2520as%2520the%2520AI%2520Act%2520gain%2520traction%252C%2520the%250Aethical%2520and%2520responsible%2520use%2520of%2520real-world%2520data%2520faces%2520increasing%2520constraints.%250ASynthetic%2520data%2520generation%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520risk-aware%250Adata%2520sharing%2520and%2520model%2520development%252C%2520particularly%2520for%2520tabular%2520datasets%2520that%2520are%250Afoundational%2520to%2520sensitive%2520domains%2520such%2520as%2520healthcare.%2520To%2520address%2520both%2520privacy%250Aand%2520fairness%2520concerns%2520in%2520this%2520setting%252C%2520we%2520propose%2520FLIP%2520%2528Fair%2520Latent%250AIntervention%2520under%2520Privacy%2520guarantees%2529%252C%2520a%2520transformer-based%2520variational%250Aautoencoder%2520augmented%2520with%2520latent%2520diffusion%2520to%2520generate%2520heterogeneous%2520tabular%250Adata.%2520Unlike%2520the%2520typical%2520setup%2520in%2520fairness-aware%2520data%2520generation%252C%2520we%2520assume%2520a%250Atask-agnostic%2520setup%252C%2520not%2520reliant%2520on%2520a%2520fixed%252C%2520defined%2520downstream%2520task%252C%2520thus%250Aoffering%2520broader%2520applicability.%2520To%2520ensure%2520privacy%252C%2520FLIP%2520employs%2520R%255C%2527enyi%250Adifferential%2520privacy%2520%2528RDP%2529%2520constraints%2520during%2520training%2520and%2520addresses%2520fairness%250Ain%2520the%2520input%2520space%2520with%2520RDP-compatible%2520balanced%2520sampling%2520that%2520accounts%2520for%250Agroup-specific%2520noise%2520levels%2520across%2520multiple%2520sampling%2520rates.%2520In%2520the%2520latent%250Aspace%252C%2520we%2520promote%2520fairness%2520by%2520aligning%2520neuron%2520activation%2520patterns%2520across%250Aprotected%2520groups%2520using%2520Centered%2520Kernel%2520Alignment%2520%2528CKA%2529%252C%2520a%2520similarity%2520measure%250Aextending%2520the%2520Hilbert-Schmidt%2520Independence%2520Criterion%2520%2528HSIC%2529.%2520This%2520alignment%250Aencourages%2520statistical%2520independence%2520between%2520latent%2520representations%2520and%2520the%250Aprotected%2520feature.%2520Empirical%2520results%2520demonstrate%2520that%2520FLIP%2520effectively%2520provides%250Asignificant%2520fairness%2520improvements%2520for%2520task-agnostic%2520fairness%2520and%2520across%2520diverse%250Adownstream%2520tasks%2520under%2520differential%2520privacy%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Hilbert-Schmidt%20Independence%20Under%20R%C3%A9nyi%20Differential%0A%20%20Privacy%20for%20Fair%20and%20Private%20Data%20Generation&entry.906535625=Tobias%20Hyrup%20and%20Emmanouil%20Panagiotou%20and%20Arjun%20Roy%20and%20Arthur%20Zimek%20and%20Eirini%20Ntoutsi%20and%20Peter%20Schneider-Kamp&entry.1292438233=%20%20As%20privacy%20regulations%20such%20as%20the%20GDPR%20and%20HIPAA%20and%20responsibility%0Aframeworks%20for%20artificial%20intelligence%20such%20as%20the%20AI%20Act%20gain%20traction%2C%20the%0Aethical%20and%20responsible%20use%20of%20real-world%20data%20faces%20increasing%20constraints.%0ASynthetic%20data%20generation%20has%20emerged%20as%20a%20promising%20solution%20to%20risk-aware%0Adata%20sharing%20and%20model%20development%2C%20particularly%20for%20tabular%20datasets%20that%20are%0Afoundational%20to%20sensitive%20domains%20such%20as%20healthcare.%20To%20address%20both%20privacy%0Aand%20fairness%20concerns%20in%20this%20setting%2C%20we%20propose%20FLIP%20%28Fair%20Latent%0AIntervention%20under%20Privacy%20guarantees%29%2C%20a%20transformer-based%20variational%0Aautoencoder%20augmented%20with%20latent%20diffusion%20to%20generate%20heterogeneous%20tabular%0Adata.%20Unlike%20the%20typical%20setup%20in%20fairness-aware%20data%20generation%2C%20we%20assume%20a%0Atask-agnostic%20setup%2C%20not%20reliant%20on%20a%20fixed%2C%20defined%20downstream%20task%2C%20thus%0Aoffering%20broader%20applicability.%20To%20ensure%20privacy%2C%20FLIP%20employs%20R%5C%27enyi%0Adifferential%20privacy%20%28RDP%29%20constraints%20during%20training%20and%20addresses%20fairness%0Ain%20the%20input%20space%20with%20RDP-compatible%20balanced%20sampling%20that%20accounts%20for%0Agroup-specific%20noise%20levels%20across%20multiple%20sampling%20rates.%20In%20the%20latent%0Aspace%2C%20we%20promote%20fairness%20by%20aligning%20neuron%20activation%20patterns%20across%0Aprotected%20groups%20using%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20measure%0Aextending%20the%20Hilbert-Schmidt%20Independence%20Criterion%20%28HSIC%29.%20This%20alignment%0Aencourages%20statistical%20independence%20between%20latent%20representations%20and%20the%0Aprotected%20feature.%20Empirical%20results%20demonstrate%20that%20FLIP%20effectively%20provides%0Asignificant%20fairness%20improvements%20for%20task-agnostic%20fairness%20and%20across%20diverse%0Adownstream%20tasks%20under%20differential%20privacy%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21815v1&entry.124074799=Read"},
{"title": "Revisiting Landmarks: Learning from Previous Plans to Generalize over\n  Problem Instances", "author": "Issa Hanou and Sebastijan Duman\u010di\u0107 and Mathijs de Weerdt", "abstract": "  We propose a new framework for discovering landmarks that automatically\ngeneralize across a domain. These generalized landmarks are learned from a set\nof solved instances and describe intermediate goals for planning problems where\ntraditional landmark extraction algorithms fall short. Our generalized\nlandmarks extend beyond the predicates of a domain by using state functions\nthat are independent of the objects of a specific problem and apply to all\nsimilar objects, thus capturing repetition. Based on these functions, we\nconstruct a directed generalized landmark graph that defines the landmark\nprogression, including loop possibilities for repetitive subplans. We show how\nto use this graph in a heuristic to solve new problem instances of the same\ndomain. Our results show that the generalized landmark graphs learned from a\nfew small instances are also effective for larger instances in the same domain.\nIf a loop that indicates repetition is identified, we see a significant\nimprovement in heuristic performance over the baseline. Generalized landmarks\ncapture domain information that is interpretable and useful to an automated\nplanner. This information can be discovered from a small set of plans for the\nsame domain.\n", "link": "http://arxiv.org/abs/2508.21564v1", "date": "2025-08-29", "relevancy": 2.0145, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5315}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Landmarks%3A%20Learning%20from%20Previous%20Plans%20to%20Generalize%20over%0A%20%20Problem%20Instances&body=Title%3A%20Revisiting%20Landmarks%3A%20Learning%20from%20Previous%20Plans%20to%20Generalize%20over%0A%20%20Problem%20Instances%0AAuthor%3A%20Issa%20Hanou%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Mathijs%20de%20Weerdt%0AAbstract%3A%20%20%20We%20propose%20a%20new%20framework%20for%20discovering%20landmarks%20that%20automatically%0Ageneralize%20across%20a%20domain.%20These%20generalized%20landmarks%20are%20learned%20from%20a%20set%0Aof%20solved%20instances%20and%20describe%20intermediate%20goals%20for%20planning%20problems%20where%0Atraditional%20landmark%20extraction%20algorithms%20fall%20short.%20Our%20generalized%0Alandmarks%20extend%20beyond%20the%20predicates%20of%20a%20domain%20by%20using%20state%20functions%0Athat%20are%20independent%20of%20the%20objects%20of%20a%20specific%20problem%20and%20apply%20to%20all%0Asimilar%20objects%2C%20thus%20capturing%20repetition.%20Based%20on%20these%20functions%2C%20we%0Aconstruct%20a%20directed%20generalized%20landmark%20graph%20that%20defines%20the%20landmark%0Aprogression%2C%20including%20loop%20possibilities%20for%20repetitive%20subplans.%20We%20show%20how%0Ato%20use%20this%20graph%20in%20a%20heuristic%20to%20solve%20new%20problem%20instances%20of%20the%20same%0Adomain.%20Our%20results%20show%20that%20the%20generalized%20landmark%20graphs%20learned%20from%20a%0Afew%20small%20instances%20are%20also%20effective%20for%20larger%20instances%20in%20the%20same%20domain.%0AIf%20a%20loop%20that%20indicates%20repetition%20is%20identified%2C%20we%20see%20a%20significant%0Aimprovement%20in%20heuristic%20performance%20over%20the%20baseline.%20Generalized%20landmarks%0Acapture%20domain%20information%20that%20is%20interpretable%20and%20useful%20to%20an%20automated%0Aplanner.%20This%20information%20can%20be%20discovered%20from%20a%20small%20set%20of%20plans%20for%20the%0Asame%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Landmarks%253A%2520Learning%2520from%2520Previous%2520Plans%2520to%2520Generalize%2520over%250A%2520%2520Problem%2520Instances%26entry.906535625%3DIssa%2520Hanou%2520and%2520Sebastijan%2520Duman%25C4%258Di%25C4%2587%2520and%2520Mathijs%2520de%2520Weerdt%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520framework%2520for%2520discovering%2520landmarks%2520that%2520automatically%250Ageneralize%2520across%2520a%2520domain.%2520These%2520generalized%2520landmarks%2520are%2520learned%2520from%2520a%2520set%250Aof%2520solved%2520instances%2520and%2520describe%2520intermediate%2520goals%2520for%2520planning%2520problems%2520where%250Atraditional%2520landmark%2520extraction%2520algorithms%2520fall%2520short.%2520Our%2520generalized%250Alandmarks%2520extend%2520beyond%2520the%2520predicates%2520of%2520a%2520domain%2520by%2520using%2520state%2520functions%250Athat%2520are%2520independent%2520of%2520the%2520objects%2520of%2520a%2520specific%2520problem%2520and%2520apply%2520to%2520all%250Asimilar%2520objects%252C%2520thus%2520capturing%2520repetition.%2520Based%2520on%2520these%2520functions%252C%2520we%250Aconstruct%2520a%2520directed%2520generalized%2520landmark%2520graph%2520that%2520defines%2520the%2520landmark%250Aprogression%252C%2520including%2520loop%2520possibilities%2520for%2520repetitive%2520subplans.%2520We%2520show%2520how%250Ato%2520use%2520this%2520graph%2520in%2520a%2520heuristic%2520to%2520solve%2520new%2520problem%2520instances%2520of%2520the%2520same%250Adomain.%2520Our%2520results%2520show%2520that%2520the%2520generalized%2520landmark%2520graphs%2520learned%2520from%2520a%250Afew%2520small%2520instances%2520are%2520also%2520effective%2520for%2520larger%2520instances%2520in%2520the%2520same%2520domain.%250AIf%2520a%2520loop%2520that%2520indicates%2520repetition%2520is%2520identified%252C%2520we%2520see%2520a%2520significant%250Aimprovement%2520in%2520heuristic%2520performance%2520over%2520the%2520baseline.%2520Generalized%2520landmarks%250Acapture%2520domain%2520information%2520that%2520is%2520interpretable%2520and%2520useful%2520to%2520an%2520automated%250Aplanner.%2520This%2520information%2520can%2520be%2520discovered%2520from%2520a%2520small%2520set%2520of%2520plans%2520for%2520the%250Asame%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Landmarks%3A%20Learning%20from%20Previous%20Plans%20to%20Generalize%20over%0A%20%20Problem%20Instances&entry.906535625=Issa%20Hanou%20and%20Sebastijan%20Duman%C4%8Di%C4%87%20and%20Mathijs%20de%20Weerdt&entry.1292438233=%20%20We%20propose%20a%20new%20framework%20for%20discovering%20landmarks%20that%20automatically%0Ageneralize%20across%20a%20domain.%20These%20generalized%20landmarks%20are%20learned%20from%20a%20set%0Aof%20solved%20instances%20and%20describe%20intermediate%20goals%20for%20planning%20problems%20where%0Atraditional%20landmark%20extraction%20algorithms%20fall%20short.%20Our%20generalized%0Alandmarks%20extend%20beyond%20the%20predicates%20of%20a%20domain%20by%20using%20state%20functions%0Athat%20are%20independent%20of%20the%20objects%20of%20a%20specific%20problem%20and%20apply%20to%20all%0Asimilar%20objects%2C%20thus%20capturing%20repetition.%20Based%20on%20these%20functions%2C%20we%0Aconstruct%20a%20directed%20generalized%20landmark%20graph%20that%20defines%20the%20landmark%0Aprogression%2C%20including%20loop%20possibilities%20for%20repetitive%20subplans.%20We%20show%20how%0Ato%20use%20this%20graph%20in%20a%20heuristic%20to%20solve%20new%20problem%20instances%20of%20the%20same%0Adomain.%20Our%20results%20show%20that%20the%20generalized%20landmark%20graphs%20learned%20from%20a%0Afew%20small%20instances%20are%20also%20effective%20for%20larger%20instances%20in%20the%20same%20domain.%0AIf%20a%20loop%20that%20indicates%20repetition%20is%20identified%2C%20we%20see%20a%20significant%0Aimprovement%20in%20heuristic%20performance%20over%20the%20baseline.%20Generalized%20landmarks%0Acapture%20domain%20information%20that%20is%20interpretable%20and%20useful%20to%20an%20automated%0Aplanner.%20This%20information%20can%20be%20discovered%20from%20a%20small%20set%20of%20plans%20for%20the%0Asame%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21564v1&entry.124074799=Read"},
{"title": "Scientifically-Interpretable Reasoning Network (ScIReN): Discovering\n  Hidden Relationships in the Carbon Cycle and Beyond", "author": "Joshua Fan and Haodi Xu and Feng Tao and Md Nasim and Marc Grimson and Yiqi Luo and Carla P. Gomes", "abstract": "  Understanding how carbon flows through the soil is crucial for mitigating the\neffects of climate change. While soils have potential to sequester carbon from\nthe atmosphere, the soil carbon cycle remains poorly understood. Scientists\nhave developed mathematical process-based models of the soil carbon cycle based\non existing knowledge, but they contain numerous unknown parameters that must\nbe set in an ad-hoc manner, and often fit observations poorly. On the other\nhand, neural networks can learn patterns from data, but do not respect known\nscientific laws, nor can they reveal novel scientific relationships due to\ntheir black-box nature. We thus propose Scientifically-Interpretable Reasoning\nNetwork (ScIReN), a fully-transparent framework that combines interpretable\nneural and process-based reasoning. An interpretable encoder predicts\nscientifically-meaningful latent parameters, which are then passed through a\ndifferentiable process-based decoder to predict labeled output variables.\nScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is\nfully interpretable and reveals relationships between input features and latent\nparameters; it uses novel smoothness penalties to balance expressivity and\nsimplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict\nlatent parameters to meaningful ranges defined by scientific prior knowledge.\nWhile the process-based decoder enforces established scientific knowledge, the\nKAN-based encoder reveals new scientific relationships hidden in conventional\nblack-box models. We apply ScIReN on two tasks: simulating the flow of organic\ncarbon through soils, and modeling ecosystem respiration from plants. In both\ntasks, ScIReN outperforms black-box networks in predictive accuracy while\nproviding substantial scientific interpretability -- it can infer latent\nscientific mechanisms and their relationships with input features.\n", "link": "http://arxiv.org/abs/2506.14054v3", "date": "2025-08-29", "relevancy": 2.0096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scientifically-Interpretable%20Reasoning%20Network%20%28ScIReN%29%3A%20Discovering%0A%20%20Hidden%20Relationships%20in%20the%20Carbon%20Cycle%20and%20Beyond&body=Title%3A%20Scientifically-Interpretable%20Reasoning%20Network%20%28ScIReN%29%3A%20Discovering%0A%20%20Hidden%20Relationships%20in%20the%20Carbon%20Cycle%20and%20Beyond%0AAuthor%3A%20Joshua%20Fan%20and%20Haodi%20Xu%20and%20Feng%20Tao%20and%20Md%20Nasim%20and%20Marc%20Grimson%20and%20Yiqi%20Luo%20and%20Carla%20P.%20Gomes%0AAbstract%3A%20%20%20Understanding%20how%20carbon%20flows%20through%20the%20soil%20is%20crucial%20for%20mitigating%20the%0Aeffects%20of%20climate%20change.%20While%20soils%20have%20potential%20to%20sequester%20carbon%20from%0Athe%20atmosphere%2C%20the%20soil%20carbon%20cycle%20remains%20poorly%20understood.%20Scientists%0Ahave%20developed%20mathematical%20process-based%20models%20of%20the%20soil%20carbon%20cycle%20based%0Aon%20existing%20knowledge%2C%20but%20they%20contain%20numerous%20unknown%20parameters%20that%20must%0Abe%20set%20in%20an%20ad-hoc%20manner%2C%20and%20often%20fit%20observations%20poorly.%20On%20the%20other%0Ahand%2C%20neural%20networks%20can%20learn%20patterns%20from%20data%2C%20but%20do%20not%20respect%20known%0Ascientific%20laws%2C%20nor%20can%20they%20reveal%20novel%20scientific%20relationships%20due%20to%0Atheir%20black-box%20nature.%20We%20thus%20propose%20Scientifically-Interpretable%20Reasoning%0ANetwork%20%28ScIReN%29%2C%20a%20fully-transparent%20framework%20that%20combines%20interpretable%0Aneural%20and%20process-based%20reasoning.%20An%20interpretable%20encoder%20predicts%0Ascientifically-meaningful%20latent%20parameters%2C%20which%20are%20then%20passed%20through%20a%0Adifferentiable%20process-based%20decoder%20to%20predict%20labeled%20output%20variables.%0AScIReN%20leverages%20Kolmogorov-Arnold%20networks%20%28KAN%29%20to%20ensure%20the%20encoder%20is%0Afully%20interpretable%20and%20reveals%20relationships%20between%20input%20features%20and%20latent%0Aparameters%3B%20it%20uses%20novel%20smoothness%20penalties%20to%20balance%20expressivity%20and%0Asimplicity.%20ScIReN%20also%20uses%20a%20novel%20hard-sigmoid%20constraint%20layer%20to%20restrict%0Alatent%20parameters%20to%20meaningful%20ranges%20defined%20by%20scientific%20prior%20knowledge.%0AWhile%20the%20process-based%20decoder%20enforces%20established%20scientific%20knowledge%2C%20the%0AKAN-based%20encoder%20reveals%20new%20scientific%20relationships%20hidden%20in%20conventional%0Ablack-box%20models.%20We%20apply%20ScIReN%20on%20two%20tasks%3A%20simulating%20the%20flow%20of%20organic%0Acarbon%20through%20soils%2C%20and%20modeling%20ecosystem%20respiration%20from%20plants.%20In%20both%0Atasks%2C%20ScIReN%20outperforms%20black-box%20networks%20in%20predictive%20accuracy%20while%0Aproviding%20substantial%20scientific%20interpretability%20--%20it%20can%20infer%20latent%0Ascientific%20mechanisms%20and%20their%20relationships%20with%20input%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14054v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScientifically-Interpretable%2520Reasoning%2520Network%2520%2528ScIReN%2529%253A%2520Discovering%250A%2520%2520Hidden%2520Relationships%2520in%2520the%2520Carbon%2520Cycle%2520and%2520Beyond%26entry.906535625%3DJoshua%2520Fan%2520and%2520Haodi%2520Xu%2520and%2520Feng%2520Tao%2520and%2520Md%2520Nasim%2520and%2520Marc%2520Grimson%2520and%2520Yiqi%2520Luo%2520and%2520Carla%2520P.%2520Gomes%26entry.1292438233%3D%2520%2520Understanding%2520how%2520carbon%2520flows%2520through%2520the%2520soil%2520is%2520crucial%2520for%2520mitigating%2520the%250Aeffects%2520of%2520climate%2520change.%2520While%2520soils%2520have%2520potential%2520to%2520sequester%2520carbon%2520from%250Athe%2520atmosphere%252C%2520the%2520soil%2520carbon%2520cycle%2520remains%2520poorly%2520understood.%2520Scientists%250Ahave%2520developed%2520mathematical%2520process-based%2520models%2520of%2520the%2520soil%2520carbon%2520cycle%2520based%250Aon%2520existing%2520knowledge%252C%2520but%2520they%2520contain%2520numerous%2520unknown%2520parameters%2520that%2520must%250Abe%2520set%2520in%2520an%2520ad-hoc%2520manner%252C%2520and%2520often%2520fit%2520observations%2520poorly.%2520On%2520the%2520other%250Ahand%252C%2520neural%2520networks%2520can%2520learn%2520patterns%2520from%2520data%252C%2520but%2520do%2520not%2520respect%2520known%250Ascientific%2520laws%252C%2520nor%2520can%2520they%2520reveal%2520novel%2520scientific%2520relationships%2520due%2520to%250Atheir%2520black-box%2520nature.%2520We%2520thus%2520propose%2520Scientifically-Interpretable%2520Reasoning%250ANetwork%2520%2528ScIReN%2529%252C%2520a%2520fully-transparent%2520framework%2520that%2520combines%2520interpretable%250Aneural%2520and%2520process-based%2520reasoning.%2520An%2520interpretable%2520encoder%2520predicts%250Ascientifically-meaningful%2520latent%2520parameters%252C%2520which%2520are%2520then%2520passed%2520through%2520a%250Adifferentiable%2520process-based%2520decoder%2520to%2520predict%2520labeled%2520output%2520variables.%250AScIReN%2520leverages%2520Kolmogorov-Arnold%2520networks%2520%2528KAN%2529%2520to%2520ensure%2520the%2520encoder%2520is%250Afully%2520interpretable%2520and%2520reveals%2520relationships%2520between%2520input%2520features%2520and%2520latent%250Aparameters%253B%2520it%2520uses%2520novel%2520smoothness%2520penalties%2520to%2520balance%2520expressivity%2520and%250Asimplicity.%2520ScIReN%2520also%2520uses%2520a%2520novel%2520hard-sigmoid%2520constraint%2520layer%2520to%2520restrict%250Alatent%2520parameters%2520to%2520meaningful%2520ranges%2520defined%2520by%2520scientific%2520prior%2520knowledge.%250AWhile%2520the%2520process-based%2520decoder%2520enforces%2520established%2520scientific%2520knowledge%252C%2520the%250AKAN-based%2520encoder%2520reveals%2520new%2520scientific%2520relationships%2520hidden%2520in%2520conventional%250Ablack-box%2520models.%2520We%2520apply%2520ScIReN%2520on%2520two%2520tasks%253A%2520simulating%2520the%2520flow%2520of%2520organic%250Acarbon%2520through%2520soils%252C%2520and%2520modeling%2520ecosystem%2520respiration%2520from%2520plants.%2520In%2520both%250Atasks%252C%2520ScIReN%2520outperforms%2520black-box%2520networks%2520in%2520predictive%2520accuracy%2520while%250Aproviding%2520substantial%2520scientific%2520interpretability%2520--%2520it%2520can%2520infer%2520latent%250Ascientific%2520mechanisms%2520and%2520their%2520relationships%2520with%2520input%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14054v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scientifically-Interpretable%20Reasoning%20Network%20%28ScIReN%29%3A%20Discovering%0A%20%20Hidden%20Relationships%20in%20the%20Carbon%20Cycle%20and%20Beyond&entry.906535625=Joshua%20Fan%20and%20Haodi%20Xu%20and%20Feng%20Tao%20and%20Md%20Nasim%20and%20Marc%20Grimson%20and%20Yiqi%20Luo%20and%20Carla%20P.%20Gomes&entry.1292438233=%20%20Understanding%20how%20carbon%20flows%20through%20the%20soil%20is%20crucial%20for%20mitigating%20the%0Aeffects%20of%20climate%20change.%20While%20soils%20have%20potential%20to%20sequester%20carbon%20from%0Athe%20atmosphere%2C%20the%20soil%20carbon%20cycle%20remains%20poorly%20understood.%20Scientists%0Ahave%20developed%20mathematical%20process-based%20models%20of%20the%20soil%20carbon%20cycle%20based%0Aon%20existing%20knowledge%2C%20but%20they%20contain%20numerous%20unknown%20parameters%20that%20must%0Abe%20set%20in%20an%20ad-hoc%20manner%2C%20and%20often%20fit%20observations%20poorly.%20On%20the%20other%0Ahand%2C%20neural%20networks%20can%20learn%20patterns%20from%20data%2C%20but%20do%20not%20respect%20known%0Ascientific%20laws%2C%20nor%20can%20they%20reveal%20novel%20scientific%20relationships%20due%20to%0Atheir%20black-box%20nature.%20We%20thus%20propose%20Scientifically-Interpretable%20Reasoning%0ANetwork%20%28ScIReN%29%2C%20a%20fully-transparent%20framework%20that%20combines%20interpretable%0Aneural%20and%20process-based%20reasoning.%20An%20interpretable%20encoder%20predicts%0Ascientifically-meaningful%20latent%20parameters%2C%20which%20are%20then%20passed%20through%20a%0Adifferentiable%20process-based%20decoder%20to%20predict%20labeled%20output%20variables.%0AScIReN%20leverages%20Kolmogorov-Arnold%20networks%20%28KAN%29%20to%20ensure%20the%20encoder%20is%0Afully%20interpretable%20and%20reveals%20relationships%20between%20input%20features%20and%20latent%0Aparameters%3B%20it%20uses%20novel%20smoothness%20penalties%20to%20balance%20expressivity%20and%0Asimplicity.%20ScIReN%20also%20uses%20a%20novel%20hard-sigmoid%20constraint%20layer%20to%20restrict%0Alatent%20parameters%20to%20meaningful%20ranges%20defined%20by%20scientific%20prior%20knowledge.%0AWhile%20the%20process-based%20decoder%20enforces%20established%20scientific%20knowledge%2C%20the%0AKAN-based%20encoder%20reveals%20new%20scientific%20relationships%20hidden%20in%20conventional%0Ablack-box%20models.%20We%20apply%20ScIReN%20on%20two%20tasks%3A%20simulating%20the%20flow%20of%20organic%0Acarbon%20through%20soils%2C%20and%20modeling%20ecosystem%20respiration%20from%20plants.%20In%20both%0Atasks%2C%20ScIReN%20outperforms%20black-box%20networks%20in%20predictive%20accuracy%20while%0Aproviding%20substantial%20scientific%20interpretability%20--%20it%20can%20infer%20latent%0Ascientific%20mechanisms%20and%20their%20relationships%20with%20input%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14054v3&entry.124074799=Read"},
{"title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward", "author": "Yong Deng and Guoqing Wang and Zhenzhe Ying and Xiaofeng Wu and Jinzhen Lin and Wenwen Xiong and Yuqin Dai and Shuo Yang and Zhanwei Zhang and Qiwen Wang and Yang Qin and Yuan Wang and Quanxing Zha and Sunhao Dai and Changhua Meng", "abstract": "  Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.\n", "link": "http://arxiv.org/abs/2508.12800v3", "date": "2025-08-29", "relevancy": 2.0074, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward&body=Title%3A%20Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward%0AAuthor%3A%20Yong%20Deng%20and%20Guoqing%20Wang%20and%20Zhenzhe%20Ying%20and%20Xiaofeng%20Wu%20and%20Jinzhen%20Lin%20and%20Wenwen%20Xiong%20and%20Yuqin%20Dai%20and%20Shuo%20Yang%20and%20Zhanwei%20Zhang%20and%20Qiwen%20Wang%20and%20Yang%20Qin%20and%20Yuan%20Wang%20and%20Quanxing%20Zha%20and%20Sunhao%20Dai%20and%20Changhua%20Meng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20problem-solving%20abilities%2C%0Abut%20struggle%20with%20complex%20tasks%20due%20to%20static%20internal%20knowledge.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20enhances%20access%20to%20external%20information%2C%0Ayet%20remains%20limited%20in%20multi-hop%20reasoning%20and%20strategic%20search%20due%20to%20rigid%0Aworkflows.%20Recent%20advancements%20in%20agentic%20deep%20research%20empower%20LLMs%20to%0Aautonomously%20reason%2C%20search%2C%20and%20synthesize%20information.%20However%2C%20current%0Aapproaches%20relying%20on%20outcome-based%20reinforcement%20learning%20%28RL%29%20face%20critical%0Aissues%20such%20as%20conflicting%20gradients%20and%20reward%20sparsity%2C%20limiting%20performance%0Agains%20and%20training%20efficiency.%20To%20address%20these%2C%20we%20first%20propose%20Atomic%0AThought%2C%20a%20novel%20LLM%20thinking%20paradigm%20that%20decomposes%20reasoning%20into%0Afine-grained%20functional%20units.%20These%20units%20are%20supervised%20by%20Reasoning%20Reward%0AModels%20%28RRMs%29%2C%20which%20provide%20Atomic%20Thought%20Rewards%20%28ATR%29%20for%20fine-grained%0Aguidance.%20Building%20on%20this%2C%20we%20propose%20Atom-Searcher%2C%20a%20novel%20RL%20framework%20for%0Aagentic%20deep%20research%20that%20integrates%20Atomic%20Thought%20and%20ATR.%20Atom-Searcher%0Auses%20a%20curriculum-inspired%20reward%20schedule%2C%20prioritizing%20process-level%20ATR%0Aearly%20and%20transitioning%20to%20outcome%20rewards%2C%20accelerating%20convergence%20on%0Aeffective%20reasoning%20paths.%20Experiments%20on%20seven%20benchmarks%20show%20consistent%0Aimprovements%20over%20the%20state-of-the-art.%20Key%20advantages%20include%3A%20%281%29%0AAtom-Searcher%20scales%20computation%20at%20test-time.%20%282%29%20Atomic%20Thought%20provides%0Asupervision%20anchors%20for%20RRMs%2C%20bridging%20deep%20research%20tasks%20and%20RRMs.%20%283%29%0AAtom-Searcher%20exhibits%20more%20interpretable%2C%20human-like%20reasoning%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12800v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtom-Searcher%253A%2520Enhancing%2520Agentic%2520Deep%2520Research%2520via%2520Fine-Grained%2520Atomic%250A%2520%2520Thought%2520Reward%26entry.906535625%3DYong%2520Deng%2520and%2520Guoqing%2520Wang%2520and%2520Zhenzhe%2520Ying%2520and%2520Xiaofeng%2520Wu%2520and%2520Jinzhen%2520Lin%2520and%2520Wenwen%2520Xiong%2520and%2520Yuqin%2520Dai%2520and%2520Shuo%2520Yang%2520and%2520Zhanwei%2520Zhang%2520and%2520Qiwen%2520Wang%2520and%2520Yang%2520Qin%2520and%2520Yuan%2520Wang%2520and%2520Quanxing%2520Zha%2520and%2520Sunhao%2520Dai%2520and%2520Changhua%2520Meng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520problem-solving%2520abilities%252C%250Abut%2520struggle%2520with%2520complex%2520tasks%2520due%2520to%2520static%2520internal%2520knowledge.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520enhances%2520access%2520to%2520external%2520information%252C%250Ayet%2520remains%2520limited%2520in%2520multi-hop%2520reasoning%2520and%2520strategic%2520search%2520due%2520to%2520rigid%250Aworkflows.%2520Recent%2520advancements%2520in%2520agentic%2520deep%2520research%2520empower%2520LLMs%2520to%250Aautonomously%2520reason%252C%2520search%252C%2520and%2520synthesize%2520information.%2520However%252C%2520current%250Aapproaches%2520relying%2520on%2520outcome-based%2520reinforcement%2520learning%2520%2528RL%2529%2520face%2520critical%250Aissues%2520such%2520as%2520conflicting%2520gradients%2520and%2520reward%2520sparsity%252C%2520limiting%2520performance%250Agains%2520and%2520training%2520efficiency.%2520To%2520address%2520these%252C%2520we%2520first%2520propose%2520Atomic%250AThought%252C%2520a%2520novel%2520LLM%2520thinking%2520paradigm%2520that%2520decomposes%2520reasoning%2520into%250Afine-grained%2520functional%2520units.%2520These%2520units%2520are%2520supervised%2520by%2520Reasoning%2520Reward%250AModels%2520%2528RRMs%2529%252C%2520which%2520provide%2520Atomic%2520Thought%2520Rewards%2520%2528ATR%2529%2520for%2520fine-grained%250Aguidance.%2520Building%2520on%2520this%252C%2520we%2520propose%2520Atom-Searcher%252C%2520a%2520novel%2520RL%2520framework%2520for%250Aagentic%2520deep%2520research%2520that%2520integrates%2520Atomic%2520Thought%2520and%2520ATR.%2520Atom-Searcher%250Auses%2520a%2520curriculum-inspired%2520reward%2520schedule%252C%2520prioritizing%2520process-level%2520ATR%250Aearly%2520and%2520transitioning%2520to%2520outcome%2520rewards%252C%2520accelerating%2520convergence%2520on%250Aeffective%2520reasoning%2520paths.%2520Experiments%2520on%2520seven%2520benchmarks%2520show%2520consistent%250Aimprovements%2520over%2520the%2520state-of-the-art.%2520Key%2520advantages%2520include%253A%2520%25281%2529%250AAtom-Searcher%2520scales%2520computation%2520at%2520test-time.%2520%25282%2529%2520Atomic%2520Thought%2520provides%250Asupervision%2520anchors%2520for%2520RRMs%252C%2520bridging%2520deep%2520research%2520tasks%2520and%2520RRMs.%2520%25283%2529%250AAtom-Searcher%2520exhibits%2520more%2520interpretable%252C%2520human-like%2520reasoning%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12800v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward&entry.906535625=Yong%20Deng%20and%20Guoqing%20Wang%20and%20Zhenzhe%20Ying%20and%20Xiaofeng%20Wu%20and%20Jinzhen%20Lin%20and%20Wenwen%20Xiong%20and%20Yuqin%20Dai%20and%20Shuo%20Yang%20and%20Zhanwei%20Zhang%20and%20Qiwen%20Wang%20and%20Yang%20Qin%20and%20Yuan%20Wang%20and%20Quanxing%20Zha%20and%20Sunhao%20Dai%20and%20Changhua%20Meng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20problem-solving%20abilities%2C%0Abut%20struggle%20with%20complex%20tasks%20due%20to%20static%20internal%20knowledge.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20enhances%20access%20to%20external%20information%2C%0Ayet%20remains%20limited%20in%20multi-hop%20reasoning%20and%20strategic%20search%20due%20to%20rigid%0Aworkflows.%20Recent%20advancements%20in%20agentic%20deep%20research%20empower%20LLMs%20to%0Aautonomously%20reason%2C%20search%2C%20and%20synthesize%20information.%20However%2C%20current%0Aapproaches%20relying%20on%20outcome-based%20reinforcement%20learning%20%28RL%29%20face%20critical%0Aissues%20such%20as%20conflicting%20gradients%20and%20reward%20sparsity%2C%20limiting%20performance%0Agains%20and%20training%20efficiency.%20To%20address%20these%2C%20we%20first%20propose%20Atomic%0AThought%2C%20a%20novel%20LLM%20thinking%20paradigm%20that%20decomposes%20reasoning%20into%0Afine-grained%20functional%20units.%20These%20units%20are%20supervised%20by%20Reasoning%20Reward%0AModels%20%28RRMs%29%2C%20which%20provide%20Atomic%20Thought%20Rewards%20%28ATR%29%20for%20fine-grained%0Aguidance.%20Building%20on%20this%2C%20we%20propose%20Atom-Searcher%2C%20a%20novel%20RL%20framework%20for%0Aagentic%20deep%20research%20that%20integrates%20Atomic%20Thought%20and%20ATR.%20Atom-Searcher%0Auses%20a%20curriculum-inspired%20reward%20schedule%2C%20prioritizing%20process-level%20ATR%0Aearly%20and%20transitioning%20to%20outcome%20rewards%2C%20accelerating%20convergence%20on%0Aeffective%20reasoning%20paths.%20Experiments%20on%20seven%20benchmarks%20show%20consistent%0Aimprovements%20over%20the%20state-of-the-art.%20Key%20advantages%20include%3A%20%281%29%0AAtom-Searcher%20scales%20computation%20at%20test-time.%20%282%29%20Atomic%20Thought%20provides%0Asupervision%20anchors%20for%20RRMs%2C%20bridging%20deep%20research%20tasks%20and%20RRMs.%20%283%29%0AAtom-Searcher%20exhibits%20more%20interpretable%2C%20human-like%20reasoning%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12800v3&entry.124074799=Read"},
{"title": "From stability of Langevin diffusion to convergence of proximal MCMC for\n  non-log-concave sampling", "author": "Marien Renaud and Valentin De Bortoli and Arthur Leclaire and Nicolas Papadakis", "abstract": "  We consider the problem of sampling distributions stemming from non-convex\npotentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of\nthe discrete-time ULA to drift approximations under the assumption that the\npotential is strongly convex at infinity. In many context, e.g. imaging inverse\nproblems, potentials are non-convex and non-smooth. Proximal Stochastic\nGradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such\npotentials. It combines the forward-backward optimization algorithm with a ULA\nstep. Our main stability result combined with properties of the Moreau envelope\nallows us to derive the first proof of convergence of the PSGLA for non-convex\npotentials. We empirically validate our methodology on synthetic data and in\nthe context of imaging inverse problems. In particular, we observe that PSGLA\nexhibits faster convergence rates than Stochastic Gradient Langevin Algorithm\nfor posterior sampling while preserving its restoration properties.\n", "link": "http://arxiv.org/abs/2505.14177v2", "date": "2025-08-29", "relevancy": 1.9884, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20stability%20of%20Langevin%20diffusion%20to%20convergence%20of%20proximal%20MCMC%20for%0A%20%20non-log-concave%20sampling&body=Title%3A%20From%20stability%20of%20Langevin%20diffusion%20to%20convergence%20of%20proximal%20MCMC%20for%0A%20%20non-log-concave%20sampling%0AAuthor%3A%20Marien%20Renaud%20and%20Valentin%20De%20Bortoli%20and%20Arthur%20Leclaire%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20sampling%20distributions%20stemming%20from%20non-convex%0Apotentials%20with%20Unadjusted%20Langevin%20Algorithm%20%28ULA%29.%20We%20prove%20the%20stability%20of%0Athe%20discrete-time%20ULA%20to%20drift%20approximations%20under%20the%20assumption%20that%20the%0Apotential%20is%20strongly%20convex%20at%20infinity.%20In%20many%20context%2C%20e.g.%20imaging%20inverse%0Aproblems%2C%20potentials%20are%20non-convex%20and%20non-smooth.%20Proximal%20Stochastic%0AGradient%20Langevin%20Algorithm%20%28PSGLA%29%20is%20a%20popular%20algorithm%20to%20handle%20such%0Apotentials.%20It%20combines%20the%20forward-backward%20optimization%20algorithm%20with%20a%20ULA%0Astep.%20Our%20main%20stability%20result%20combined%20with%20properties%20of%20the%20Moreau%20envelope%0Aallows%20us%20to%20derive%20the%20first%20proof%20of%20convergence%20of%20the%20PSGLA%20for%20non-convex%0Apotentials.%20We%20empirically%20validate%20our%20methodology%20on%20synthetic%20data%20and%20in%0Athe%20context%20of%20imaging%20inverse%20problems.%20In%20particular%2C%20we%20observe%20that%20PSGLA%0Aexhibits%20faster%20convergence%20rates%20than%20Stochastic%20Gradient%20Langevin%20Algorithm%0Afor%20posterior%20sampling%20while%20preserving%20its%20restoration%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520stability%2520of%2520Langevin%2520diffusion%2520to%2520convergence%2520of%2520proximal%2520MCMC%2520for%250A%2520%2520non-log-concave%2520sampling%26entry.906535625%3DMarien%2520Renaud%2520and%2520Valentin%2520De%2520Bortoli%2520and%2520Arthur%2520Leclaire%2520and%2520Nicolas%2520Papadakis%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520sampling%2520distributions%2520stemming%2520from%2520non-convex%250Apotentials%2520with%2520Unadjusted%2520Langevin%2520Algorithm%2520%2528ULA%2529.%2520We%2520prove%2520the%2520stability%2520of%250Athe%2520discrete-time%2520ULA%2520to%2520drift%2520approximations%2520under%2520the%2520assumption%2520that%2520the%250Apotential%2520is%2520strongly%2520convex%2520at%2520infinity.%2520In%2520many%2520context%252C%2520e.g.%2520imaging%2520inverse%250Aproblems%252C%2520potentials%2520are%2520non-convex%2520and%2520non-smooth.%2520Proximal%2520Stochastic%250AGradient%2520Langevin%2520Algorithm%2520%2528PSGLA%2529%2520is%2520a%2520popular%2520algorithm%2520to%2520handle%2520such%250Apotentials.%2520It%2520combines%2520the%2520forward-backward%2520optimization%2520algorithm%2520with%2520a%2520ULA%250Astep.%2520Our%2520main%2520stability%2520result%2520combined%2520with%2520properties%2520of%2520the%2520Moreau%2520envelope%250Aallows%2520us%2520to%2520derive%2520the%2520first%2520proof%2520of%2520convergence%2520of%2520the%2520PSGLA%2520for%2520non-convex%250Apotentials.%2520We%2520empirically%2520validate%2520our%2520methodology%2520on%2520synthetic%2520data%2520and%2520in%250Athe%2520context%2520of%2520imaging%2520inverse%2520problems.%2520In%2520particular%252C%2520we%2520observe%2520that%2520PSGLA%250Aexhibits%2520faster%2520convergence%2520rates%2520than%2520Stochastic%2520Gradient%2520Langevin%2520Algorithm%250Afor%2520posterior%2520sampling%2520while%2520preserving%2520its%2520restoration%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20stability%20of%20Langevin%20diffusion%20to%20convergence%20of%20proximal%20MCMC%20for%0A%20%20non-log-concave%20sampling&entry.906535625=Marien%20Renaud%20and%20Valentin%20De%20Bortoli%20and%20Arthur%20Leclaire%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20sampling%20distributions%20stemming%20from%20non-convex%0Apotentials%20with%20Unadjusted%20Langevin%20Algorithm%20%28ULA%29.%20We%20prove%20the%20stability%20of%0Athe%20discrete-time%20ULA%20to%20drift%20approximations%20under%20the%20assumption%20that%20the%0Apotential%20is%20strongly%20convex%20at%20infinity.%20In%20many%20context%2C%20e.g.%20imaging%20inverse%0Aproblems%2C%20potentials%20are%20non-convex%20and%20non-smooth.%20Proximal%20Stochastic%0AGradient%20Langevin%20Algorithm%20%28PSGLA%29%20is%20a%20popular%20algorithm%20to%20handle%20such%0Apotentials.%20It%20combines%20the%20forward-backward%20optimization%20algorithm%20with%20a%20ULA%0Astep.%20Our%20main%20stability%20result%20combined%20with%20properties%20of%20the%20Moreau%20envelope%0Aallows%20us%20to%20derive%20the%20first%20proof%20of%20convergence%20of%20the%20PSGLA%20for%20non-convex%0Apotentials.%20We%20empirically%20validate%20our%20methodology%20on%20synthetic%20data%20and%20in%0Athe%20context%20of%20imaging%20inverse%20problems.%20In%20particular%2C%20we%20observe%20that%20PSGLA%0Aexhibits%20faster%20convergence%20rates%20than%20Stochastic%20Gradient%20Langevin%20Algorithm%0Afor%20posterior%20sampling%20while%20preserving%20its%20restoration%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14177v2&entry.124074799=Read"},
{"title": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning", "author": "Jiaan Wang and Fandong Meng and Jie Zhou", "abstract": "  Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation.\n", "link": "http://arxiv.org/abs/2504.10187v2", "date": "2025-08-29", "relevancy": 1.9819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepTrans%3A%20Deep%20Reasoning%20Translation%20via%20Reinforcement%20Learning&body=Title%3A%20DeepTrans%3A%20Deep%20Reasoning%20Translation%20via%20Reinforcement%20Learning%0AAuthor%3A%20Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20deep%20reasoning%20LLMs%20%28e.g.%2C%20OpenAI%20o1%20and%20DeepSeek-R1%29%20have%20shown%0Apromising%20performance%20in%20various%20downstream%20tasks.%20Free%20translation%20is%20an%0Aimportant%20and%20interesting%20task%20in%20the%20multilingual%20world%2C%20which%20requires%20going%0Abeyond%20word-for-word%20translation.%20However%2C%20the%20task%20is%20still%20under-explored%20in%0Adeep%20reasoning%20LLMs.%20In%20this%20paper%2C%20we%20introduce%20DeepTrans%2C%20a%20deep%20reasoning%0Atranslation%20model%20that%20learns%20free%20translation%20via%20reinforcement%20learning%20%28RL%29.%0ASpecifically%2C%20we%20carefully%20build%20a%20reward%20model%20with%20pre-defined%20scoring%0Acriteria%20on%20both%20the%20translation%20results%20and%20the%20thought%20processes.%20The%20reward%0Amodel%20teaches%20DeepTrans%20how%20to%20think%20and%20free-translate%20the%20given%20sentences%0Aduring%20RL.%20Besides%2C%20our%20RL%20training%20does%20not%20need%20any%20labeled%20translations%2C%0Aavoiding%20the%20human-intensive%20annotation%20or%20resource-intensive%20data%20synthesis.%0AExperimental%20results%20show%20the%20effectiveness%20of%20DeepTrans.%20Using%20Qwen2.5-7B%20as%0Athe%20backbone%2C%20DeepTrans%20improves%20performance%20by%2016.3%25%20in%20literature%0Atranslation%2C%20and%20outperforms%20strong%20deep%20reasoning%20LLMs.%20Moreover%2C%20we%20summarize%0Athe%20failures%20and%20interesting%20findings%20during%20our%20RL%20exploration.%20We%20hope%20this%0Awork%20could%20inspire%20other%20researchers%20in%20free%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepTrans%253A%2520Deep%2520Reasoning%2520Translation%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DJiaan%2520Wang%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520reasoning%2520LLMs%2520%2528e.g.%252C%2520OpenAI%2520o1%2520and%2520DeepSeek-R1%2529%2520have%2520shown%250Apromising%2520performance%2520in%2520various%2520downstream%2520tasks.%2520Free%2520translation%2520is%2520an%250Aimportant%2520and%2520interesting%2520task%2520in%2520the%2520multilingual%2520world%252C%2520which%2520requires%2520going%250Abeyond%2520word-for-word%2520translation.%2520However%252C%2520the%2520task%2520is%2520still%2520under-explored%2520in%250Adeep%2520reasoning%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DeepTrans%252C%2520a%2520deep%2520reasoning%250Atranslation%2520model%2520that%2520learns%2520free%2520translation%2520via%2520reinforcement%2520learning%2520%2528RL%2529.%250ASpecifically%252C%2520we%2520carefully%2520build%2520a%2520reward%2520model%2520with%2520pre-defined%2520scoring%250Acriteria%2520on%2520both%2520the%2520translation%2520results%2520and%2520the%2520thought%2520processes.%2520The%2520reward%250Amodel%2520teaches%2520DeepTrans%2520how%2520to%2520think%2520and%2520free-translate%2520the%2520given%2520sentences%250Aduring%2520RL.%2520Besides%252C%2520our%2520RL%2520training%2520does%2520not%2520need%2520any%2520labeled%2520translations%252C%250Aavoiding%2520the%2520human-intensive%2520annotation%2520or%2520resource-intensive%2520data%2520synthesis.%250AExperimental%2520results%2520show%2520the%2520effectiveness%2520of%2520DeepTrans.%2520Using%2520Qwen2.5-7B%2520as%250Athe%2520backbone%252C%2520DeepTrans%2520improves%2520performance%2520by%252016.3%2525%2520in%2520literature%250Atranslation%252C%2520and%2520outperforms%2520strong%2520deep%2520reasoning%2520LLMs.%2520Moreover%252C%2520we%2520summarize%250Athe%2520failures%2520and%2520interesting%2520findings%2520during%2520our%2520RL%2520exploration.%2520We%2520hope%2520this%250Awork%2520could%2520inspire%2520other%2520researchers%2520in%2520free%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepTrans%3A%20Deep%20Reasoning%20Translation%20via%20Reinforcement%20Learning&entry.906535625=Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Jie%20Zhou&entry.1292438233=%20%20Recently%2C%20deep%20reasoning%20LLMs%20%28e.g.%2C%20OpenAI%20o1%20and%20DeepSeek-R1%29%20have%20shown%0Apromising%20performance%20in%20various%20downstream%20tasks.%20Free%20translation%20is%20an%0Aimportant%20and%20interesting%20task%20in%20the%20multilingual%20world%2C%20which%20requires%20going%0Abeyond%20word-for-word%20translation.%20However%2C%20the%20task%20is%20still%20under-explored%20in%0Adeep%20reasoning%20LLMs.%20In%20this%20paper%2C%20we%20introduce%20DeepTrans%2C%20a%20deep%20reasoning%0Atranslation%20model%20that%20learns%20free%20translation%20via%20reinforcement%20learning%20%28RL%29.%0ASpecifically%2C%20we%20carefully%20build%20a%20reward%20model%20with%20pre-defined%20scoring%0Acriteria%20on%20both%20the%20translation%20results%20and%20the%20thought%20processes.%20The%20reward%0Amodel%20teaches%20DeepTrans%20how%20to%20think%20and%20free-translate%20the%20given%20sentences%0Aduring%20RL.%20Besides%2C%20our%20RL%20training%20does%20not%20need%20any%20labeled%20translations%2C%0Aavoiding%20the%20human-intensive%20annotation%20or%20resource-intensive%20data%20synthesis.%0AExperimental%20results%20show%20the%20effectiveness%20of%20DeepTrans.%20Using%20Qwen2.5-7B%20as%0Athe%20backbone%2C%20DeepTrans%20improves%20performance%20by%2016.3%25%20in%20literature%0Atranslation%2C%20and%20outperforms%20strong%20deep%20reasoning%20LLMs.%20Moreover%2C%20we%20summarize%0Athe%20failures%20and%20interesting%20findings%20during%20our%20RL%20exploration.%20We%20hope%20this%0Awork%20could%20inspire%20other%20researchers%20in%20free%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10187v2&entry.124074799=Read"},
{"title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval", "author": "In\u00e9s Altemir Marinas and Anastasiia Kucherenko and Andrei Kucharavy", "abstract": "  Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.\n", "link": "http://arxiv.org/abs/2508.21788v1", "date": "2025-08-29", "relevancy": 1.9699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Going%20over%20Fine%20Web%20with%20a%20Fine-Tooth%20Comb%3A%20Technical%20Report%20of%20Indexing%0A%20%20Fine%20Web%20for%20Problematic%20Content%20Search%20and%20Retrieval&body=Title%3A%20Going%20over%20Fine%20Web%20with%20a%20Fine-Tooth%20Comb%3A%20Technical%20Report%20of%20Indexing%0A%20%20Fine%20Web%20for%20Problematic%20Content%20Search%20and%20Retrieval%0AAuthor%3A%20In%C3%A9s%20Altemir%20Marinas%20and%20Anastasiia%20Kucherenko%20and%20Andrei%20Kucharavy%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20rely%20heavily%20on%20web-scale%20datasets%20like%20Common%0ACrawl%2C%20which%20provides%20over%2080%5C%25%20of%20training%20data%20for%20some%20modern%20models.%0AHowever%2C%20the%20indiscriminate%20nature%20of%20web%20crawling%20raises%20challenges%20in%20data%0Aquality%2C%20safety%2C%20and%20ethics.%20Despite%20the%20critical%20importance%20of%20training%20data%0Aquality%2C%20prior%20research%20on%20harmful%20content%20has%20been%20limited%20to%20small%20samples%0Adue%20to%20computational%20constraints.%20This%20project%20presents%20a%20framework%20for%0Aindexing%20and%20analyzing%20LLM%20training%20datasets%20using%20an%20ElasticSearch-based%0Apipeline.%20We%20apply%20it%20to%20SwissAI%27s%20FineWeb-2%20corpus%20%281.5TB%2C%20four%20languages%29%2C%0Aachieving%20fast%20query%20performance--most%20searches%20in%20milliseconds%2C%20all%20under%202%0Aseconds.%20Our%20work%20demonstrates%20real-time%20dataset%20analysis%2C%20offering%20practical%0Atools%20for%20safer%2C%20more%20accountable%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoing%2520over%2520Fine%2520Web%2520with%2520a%2520Fine-Tooth%2520Comb%253A%2520Technical%2520Report%2520of%2520Indexing%250A%2520%2520Fine%2520Web%2520for%2520Problematic%2520Content%2520Search%2520and%2520Retrieval%26entry.906535625%3DIn%25C3%25A9s%2520Altemir%2520Marinas%2520and%2520Anastasiia%2520Kucherenko%2520and%2520Andrei%2520Kucharavy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520rely%2520heavily%2520on%2520web-scale%2520datasets%2520like%2520Common%250ACrawl%252C%2520which%2520provides%2520over%252080%255C%2525%2520of%2520training%2520data%2520for%2520some%2520modern%2520models.%250AHowever%252C%2520the%2520indiscriminate%2520nature%2520of%2520web%2520crawling%2520raises%2520challenges%2520in%2520data%250Aquality%252C%2520safety%252C%2520and%2520ethics.%2520Despite%2520the%2520critical%2520importance%2520of%2520training%2520data%250Aquality%252C%2520prior%2520research%2520on%2520harmful%2520content%2520has%2520been%2520limited%2520to%2520small%2520samples%250Adue%2520to%2520computational%2520constraints.%2520This%2520project%2520presents%2520a%2520framework%2520for%250Aindexing%2520and%2520analyzing%2520LLM%2520training%2520datasets%2520using%2520an%2520ElasticSearch-based%250Apipeline.%2520We%2520apply%2520it%2520to%2520SwissAI%2527s%2520FineWeb-2%2520corpus%2520%25281.5TB%252C%2520four%2520languages%2529%252C%250Aachieving%2520fast%2520query%2520performance--most%2520searches%2520in%2520milliseconds%252C%2520all%2520under%25202%250Aseconds.%2520Our%2520work%2520demonstrates%2520real-time%2520dataset%2520analysis%252C%2520offering%2520practical%250Atools%2520for%2520safer%252C%2520more%2520accountable%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Going%20over%20Fine%20Web%20with%20a%20Fine-Tooth%20Comb%3A%20Technical%20Report%20of%20Indexing%0A%20%20Fine%20Web%20for%20Problematic%20Content%20Search%20and%20Retrieval&entry.906535625=In%C3%A9s%20Altemir%20Marinas%20and%20Anastasiia%20Kucherenko%20and%20Andrei%20Kucharavy&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20rely%20heavily%20on%20web-scale%20datasets%20like%20Common%0ACrawl%2C%20which%20provides%20over%2080%5C%25%20of%20training%20data%20for%20some%20modern%20models.%0AHowever%2C%20the%20indiscriminate%20nature%20of%20web%20crawling%20raises%20challenges%20in%20data%0Aquality%2C%20safety%2C%20and%20ethics.%20Despite%20the%20critical%20importance%20of%20training%20data%0Aquality%2C%20prior%20research%20on%20harmful%20content%20has%20been%20limited%20to%20small%20samples%0Adue%20to%20computational%20constraints.%20This%20project%20presents%20a%20framework%20for%0Aindexing%20and%20analyzing%20LLM%20training%20datasets%20using%20an%20ElasticSearch-based%0Apipeline.%20We%20apply%20it%20to%20SwissAI%27s%20FineWeb-2%20corpus%20%281.5TB%2C%20four%20languages%29%2C%0Aachieving%20fast%20query%20performance--most%20searches%20in%20milliseconds%2C%20all%20under%202%0Aseconds.%20Our%20work%20demonstrates%20real-time%20dataset%20analysis%2C%20offering%20practical%0Atools%20for%20safer%2C%20more%20accountable%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21788v1&entry.124074799=Read"},
{"title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,\n  Rogue Software and Auto-SNL", "author": "Hamza Ezzaoui Rahali and Abhilasha Dave and Larry Ruckman and Mohammad Mehdi Rahimifar and Audrey C. Therrien and James J. Russel and Ryan T. Herbst", "abstract": "  The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more.\n", "link": "http://arxiv.org/abs/2508.21739v1", "date": "2025-08-29", "relevancy": 1.968, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5487}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4992}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Acceleration%20on%20MPSoC%20board%3A%20Integrating%20SLAC%27s%20SNL%2C%0A%20%20Rogue%20Software%20and%20Auto-SNL&body=Title%3A%20Neural%20Network%20Acceleration%20on%20MPSoC%20board%3A%20Integrating%20SLAC%27s%20SNL%2C%0A%20%20Rogue%20Software%20and%20Auto-SNL%0AAuthor%3A%20Hamza%20Ezzaoui%20Rahali%20and%20Abhilasha%20Dave%20and%20Larry%20Ruckman%20and%20Mohammad%20Mehdi%20Rahimifar%20and%20Audrey%20C.%20Therrien%20and%20James%20J.%20Russel%20and%20Ryan%20T.%20Herbst%0AAbstract%3A%20%20%20The%20LCLS-II%20Free%20Electron%20Laser%20%28FEL%29%20will%20generate%20X-ray%20pulses%20for%20beamline%0Aexperiments%20at%20rates%20of%20up%20to%201~MHz%2C%20with%20detectors%20producing%20data%20throughputs%0Aexceeding%201%20TB/s.%20Managing%20such%20massive%20data%20streams%20presents%20significant%0Achallenges%2C%20as%20transmission%20and%20storage%20infrastructures%20become%20prohibitively%0Aexpensive.%20Machine%20learning%20%28ML%29%20offers%20a%20promising%20solution%20for%20real-time%20data%0Areduction%2C%20but%20conventional%20implementations%20introduce%20excessive%20latency%2C%20making%0Athem%20unsuitable%20for%20high-speed%20experimental%20environments.%20To%20address%20these%0Achallenges%2C%20SLAC%20developed%20the%20SLAC%20Neural%20Network%20Library%20%28SNL%29%2C%20a%20specialized%0Aframework%20designed%20to%20deploy%20real-time%20ML%20inference%20models%20on%0AField-Programmable%20Gate%20Arrays%20%28FPGA%29.%20SNL%27s%20key%20feature%20is%20the%20ability%20to%0Adynamically%20update%20model%20weights%20without%20requiring%20FPGA%20resynthesis%2C%20enhancing%0Aflexibility%20for%20adaptive%20learning%20applications.%20To%20further%20enhance%20usability%0Aand%20accessibility%2C%20we%20introduce%20Auto-SNL%2C%20a%20Python%20extension%20that%20streamlines%0Athe%20process%20of%20converting%20Python-based%20neural%20network%20models%20into%0ASNL-compatible%20high-level%20synthesis%20code.%20This%20paper%20presents%20a%20benchmark%0Acomparison%20against%20hls4ml%2C%20the%20current%20state-of-the-art%20tool%2C%20across%20multiple%0Aneural%20network%20architectures%2C%20fixed-point%20precisions%2C%20and%20synthesis%0Aconfigurations%20targeting%20a%20Xilinx%20ZCU102%20FPGA.%20The%20results%20showed%20that%20SNL%0Aachieves%20competitive%20or%20superior%20latency%20in%20most%20tested%20architectures%2C%20while%20in%0Asome%20cases%20also%20offering%20FPGA%20resource%20savings.%20This%20adaptation%20demonstrates%0ASNL%27s%20versatility%2C%20opening%20new%20opportunities%20for%20researchers%20and%20academics%20in%0Afields%20such%20as%20high-energy%20physics%2C%20medical%20imaging%2C%20robotics%2C%20and%20many%20more.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Acceleration%2520on%2520MPSoC%2520board%253A%2520Integrating%2520SLAC%2527s%2520SNL%252C%250A%2520%2520Rogue%2520Software%2520and%2520Auto-SNL%26entry.906535625%3DHamza%2520Ezzaoui%2520Rahali%2520and%2520Abhilasha%2520Dave%2520and%2520Larry%2520Ruckman%2520and%2520Mohammad%2520Mehdi%2520Rahimifar%2520and%2520Audrey%2520C.%2520Therrien%2520and%2520James%2520J.%2520Russel%2520and%2520Ryan%2520T.%2520Herbst%26entry.1292438233%3D%2520%2520The%2520LCLS-II%2520Free%2520Electron%2520Laser%2520%2528FEL%2529%2520will%2520generate%2520X-ray%2520pulses%2520for%2520beamline%250Aexperiments%2520at%2520rates%2520of%2520up%2520to%25201~MHz%252C%2520with%2520detectors%2520producing%2520data%2520throughputs%250Aexceeding%25201%2520TB/s.%2520Managing%2520such%2520massive%2520data%2520streams%2520presents%2520significant%250Achallenges%252C%2520as%2520transmission%2520and%2520storage%2520infrastructures%2520become%2520prohibitively%250Aexpensive.%2520Machine%2520learning%2520%2528ML%2529%2520offers%2520a%2520promising%2520solution%2520for%2520real-time%2520data%250Areduction%252C%2520but%2520conventional%2520implementations%2520introduce%2520excessive%2520latency%252C%2520making%250Athem%2520unsuitable%2520for%2520high-speed%2520experimental%2520environments.%2520To%2520address%2520these%250Achallenges%252C%2520SLAC%2520developed%2520the%2520SLAC%2520Neural%2520Network%2520Library%2520%2528SNL%2529%252C%2520a%2520specialized%250Aframework%2520designed%2520to%2520deploy%2520real-time%2520ML%2520inference%2520models%2520on%250AField-Programmable%2520Gate%2520Arrays%2520%2528FPGA%2529.%2520SNL%2527s%2520key%2520feature%2520is%2520the%2520ability%2520to%250Adynamically%2520update%2520model%2520weights%2520without%2520requiring%2520FPGA%2520resynthesis%252C%2520enhancing%250Aflexibility%2520for%2520adaptive%2520learning%2520applications.%2520To%2520further%2520enhance%2520usability%250Aand%2520accessibility%252C%2520we%2520introduce%2520Auto-SNL%252C%2520a%2520Python%2520extension%2520that%2520streamlines%250Athe%2520process%2520of%2520converting%2520Python-based%2520neural%2520network%2520models%2520into%250ASNL-compatible%2520high-level%2520synthesis%2520code.%2520This%2520paper%2520presents%2520a%2520benchmark%250Acomparison%2520against%2520hls4ml%252C%2520the%2520current%2520state-of-the-art%2520tool%252C%2520across%2520multiple%250Aneural%2520network%2520architectures%252C%2520fixed-point%2520precisions%252C%2520and%2520synthesis%250Aconfigurations%2520targeting%2520a%2520Xilinx%2520ZCU102%2520FPGA.%2520The%2520results%2520showed%2520that%2520SNL%250Aachieves%2520competitive%2520or%2520superior%2520latency%2520in%2520most%2520tested%2520architectures%252C%2520while%2520in%250Asome%2520cases%2520also%2520offering%2520FPGA%2520resource%2520savings.%2520This%2520adaptation%2520demonstrates%250ASNL%2527s%2520versatility%252C%2520opening%2520new%2520opportunities%2520for%2520researchers%2520and%2520academics%2520in%250Afields%2520such%2520as%2520high-energy%2520physics%252C%2520medical%2520imaging%252C%2520robotics%252C%2520and%2520many%2520more.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Acceleration%20on%20MPSoC%20board%3A%20Integrating%20SLAC%27s%20SNL%2C%0A%20%20Rogue%20Software%20and%20Auto-SNL&entry.906535625=Hamza%20Ezzaoui%20Rahali%20and%20Abhilasha%20Dave%20and%20Larry%20Ruckman%20and%20Mohammad%20Mehdi%20Rahimifar%20and%20Audrey%20C.%20Therrien%20and%20James%20J.%20Russel%20and%20Ryan%20T.%20Herbst&entry.1292438233=%20%20The%20LCLS-II%20Free%20Electron%20Laser%20%28FEL%29%20will%20generate%20X-ray%20pulses%20for%20beamline%0Aexperiments%20at%20rates%20of%20up%20to%201~MHz%2C%20with%20detectors%20producing%20data%20throughputs%0Aexceeding%201%20TB/s.%20Managing%20such%20massive%20data%20streams%20presents%20significant%0Achallenges%2C%20as%20transmission%20and%20storage%20infrastructures%20become%20prohibitively%0Aexpensive.%20Machine%20learning%20%28ML%29%20offers%20a%20promising%20solution%20for%20real-time%20data%0Areduction%2C%20but%20conventional%20implementations%20introduce%20excessive%20latency%2C%20making%0Athem%20unsuitable%20for%20high-speed%20experimental%20environments.%20To%20address%20these%0Achallenges%2C%20SLAC%20developed%20the%20SLAC%20Neural%20Network%20Library%20%28SNL%29%2C%20a%20specialized%0Aframework%20designed%20to%20deploy%20real-time%20ML%20inference%20models%20on%0AField-Programmable%20Gate%20Arrays%20%28FPGA%29.%20SNL%27s%20key%20feature%20is%20the%20ability%20to%0Adynamically%20update%20model%20weights%20without%20requiring%20FPGA%20resynthesis%2C%20enhancing%0Aflexibility%20for%20adaptive%20learning%20applications.%20To%20further%20enhance%20usability%0Aand%20accessibility%2C%20we%20introduce%20Auto-SNL%2C%20a%20Python%20extension%20that%20streamlines%0Athe%20process%20of%20converting%20Python-based%20neural%20network%20models%20into%0ASNL-compatible%20high-level%20synthesis%20code.%20This%20paper%20presents%20a%20benchmark%0Acomparison%20against%20hls4ml%2C%20the%20current%20state-of-the-art%20tool%2C%20across%20multiple%0Aneural%20network%20architectures%2C%20fixed-point%20precisions%2C%20and%20synthesis%0Aconfigurations%20targeting%20a%20Xilinx%20ZCU102%20FPGA.%20The%20results%20showed%20that%20SNL%0Aachieves%20competitive%20or%20superior%20latency%20in%20most%20tested%20architectures%2C%20while%20in%0Asome%20cases%20also%20offering%20FPGA%20resource%20savings.%20This%20adaptation%20demonstrates%0ASNL%27s%20versatility%2C%20opening%20new%20opportunities%20for%20researchers%20and%20academics%20in%0Afields%20such%20as%20high-energy%20physics%2C%20medical%20imaging%2C%20robotics%2C%20and%20many%20more.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21739v1&entry.124074799=Read"},
{"title": "Soft Manipulation Surface With Reduced Actuator Density For\n  Heterogeneous Object Manipulation", "author": "Pratik Ingle and Kasper St\u00f8y and Andres Fai\u00f1a", "abstract": "  Object manipulation in robotics faces challenges due to diverse object\nshapes, sizes, and fragility. Gripper-based methods offer precision and low\ndegrees of freedom (DOF) but the gripper limits the kind of objects to grasp.\nOn the other hand, surface-based approaches provide flexibility for handling\nfragile and heterogeneous objects but require numerous actuators, increasing\ncomplexity. We propose new manipulation hardware that utilizes equally spaced\nlinear actuators placed vertically and connected by a soft surface. In this\nsetup, object manipulation occurs on the soft surface through coordinated\nmovements of the surrounding actuators. This approach requires fewer actuators\nto cover a large manipulation area, offering a cost-effective solution with a\nlower DOF compared to dense actuator arrays. It also effectively handles\nheterogeneous objects of varying shapes and weights, even when they are\nsignificantly smaller than the distance between actuators. This method is\nparticularly suitable for managing highly fragile objects in the food industry.\n", "link": "http://arxiv.org/abs/2411.14290v2", "date": "2025-08-29", "relevancy": 1.9632, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5312}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4934}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Manipulation%20Surface%20With%20Reduced%20Actuator%20Density%20For%0A%20%20Heterogeneous%20Object%20Manipulation&body=Title%3A%20Soft%20Manipulation%20Surface%20With%20Reduced%20Actuator%20Density%20For%0A%20%20Heterogeneous%20Object%20Manipulation%0AAuthor%3A%20Pratik%20Ingle%20and%20Kasper%20St%C3%B8y%20and%20Andres%20Fai%C3%B1a%0AAbstract%3A%20%20%20Object%20manipulation%20in%20robotics%20faces%20challenges%20due%20to%20diverse%20object%0Ashapes%2C%20sizes%2C%20and%20fragility.%20Gripper-based%20methods%20offer%20precision%20and%20low%0Adegrees%20of%20freedom%20%28DOF%29%20but%20the%20gripper%20limits%20the%20kind%20of%20objects%20to%20grasp.%0AOn%20the%20other%20hand%2C%20surface-based%20approaches%20provide%20flexibility%20for%20handling%0Afragile%20and%20heterogeneous%20objects%20but%20require%20numerous%20actuators%2C%20increasing%0Acomplexity.%20We%20propose%20new%20manipulation%20hardware%20that%20utilizes%20equally%20spaced%0Alinear%20actuators%20placed%20vertically%20and%20connected%20by%20a%20soft%20surface.%20In%20this%0Asetup%2C%20object%20manipulation%20occurs%20on%20the%20soft%20surface%20through%20coordinated%0Amovements%20of%20the%20surrounding%20actuators.%20This%20approach%20requires%20fewer%20actuators%0Ato%20cover%20a%20large%20manipulation%20area%2C%20offering%20a%20cost-effective%20solution%20with%20a%0Alower%20DOF%20compared%20to%20dense%20actuator%20arrays.%20It%20also%20effectively%20handles%0Aheterogeneous%20objects%20of%20varying%20shapes%20and%20weights%2C%20even%20when%20they%20are%0Asignificantly%20smaller%20than%20the%20distance%20between%20actuators.%20This%20method%20is%0Aparticularly%20suitable%20for%20managing%20highly%20fragile%20objects%20in%20the%20food%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14290v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Manipulation%2520Surface%2520With%2520Reduced%2520Actuator%2520Density%2520For%250A%2520%2520Heterogeneous%2520Object%2520Manipulation%26entry.906535625%3DPratik%2520Ingle%2520and%2520Kasper%2520St%25C3%25B8y%2520and%2520Andres%2520Fai%25C3%25B1a%26entry.1292438233%3D%2520%2520Object%2520manipulation%2520in%2520robotics%2520faces%2520challenges%2520due%2520to%2520diverse%2520object%250Ashapes%252C%2520sizes%252C%2520and%2520fragility.%2520Gripper-based%2520methods%2520offer%2520precision%2520and%2520low%250Adegrees%2520of%2520freedom%2520%2528DOF%2529%2520but%2520the%2520gripper%2520limits%2520the%2520kind%2520of%2520objects%2520to%2520grasp.%250AOn%2520the%2520other%2520hand%252C%2520surface-based%2520approaches%2520provide%2520flexibility%2520for%2520handling%250Afragile%2520and%2520heterogeneous%2520objects%2520but%2520require%2520numerous%2520actuators%252C%2520increasing%250Acomplexity.%2520We%2520propose%2520new%2520manipulation%2520hardware%2520that%2520utilizes%2520equally%2520spaced%250Alinear%2520actuators%2520placed%2520vertically%2520and%2520connected%2520by%2520a%2520soft%2520surface.%2520In%2520this%250Asetup%252C%2520object%2520manipulation%2520occurs%2520on%2520the%2520soft%2520surface%2520through%2520coordinated%250Amovements%2520of%2520the%2520surrounding%2520actuators.%2520This%2520approach%2520requires%2520fewer%2520actuators%250Ato%2520cover%2520a%2520large%2520manipulation%2520area%252C%2520offering%2520a%2520cost-effective%2520solution%2520with%2520a%250Alower%2520DOF%2520compared%2520to%2520dense%2520actuator%2520arrays.%2520It%2520also%2520effectively%2520handles%250Aheterogeneous%2520objects%2520of%2520varying%2520shapes%2520and%2520weights%252C%2520even%2520when%2520they%2520are%250Asignificantly%2520smaller%2520than%2520the%2520distance%2520between%2520actuators.%2520This%2520method%2520is%250Aparticularly%2520suitable%2520for%2520managing%2520highly%2520fragile%2520objects%2520in%2520the%2520food%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14290v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Manipulation%20Surface%20With%20Reduced%20Actuator%20Density%20For%0A%20%20Heterogeneous%20Object%20Manipulation&entry.906535625=Pratik%20Ingle%20and%20Kasper%20St%C3%B8y%20and%20Andres%20Fai%C3%B1a&entry.1292438233=%20%20Object%20manipulation%20in%20robotics%20faces%20challenges%20due%20to%20diverse%20object%0Ashapes%2C%20sizes%2C%20and%20fragility.%20Gripper-based%20methods%20offer%20precision%20and%20low%0Adegrees%20of%20freedom%20%28DOF%29%20but%20the%20gripper%20limits%20the%20kind%20of%20objects%20to%20grasp.%0AOn%20the%20other%20hand%2C%20surface-based%20approaches%20provide%20flexibility%20for%20handling%0Afragile%20and%20heterogeneous%20objects%20but%20require%20numerous%20actuators%2C%20increasing%0Acomplexity.%20We%20propose%20new%20manipulation%20hardware%20that%20utilizes%20equally%20spaced%0Alinear%20actuators%20placed%20vertically%20and%20connected%20by%20a%20soft%20surface.%20In%20this%0Asetup%2C%20object%20manipulation%20occurs%20on%20the%20soft%20surface%20through%20coordinated%0Amovements%20of%20the%20surrounding%20actuators.%20This%20approach%20requires%20fewer%20actuators%0Ato%20cover%20a%20large%20manipulation%20area%2C%20offering%20a%20cost-effective%20solution%20with%20a%0Alower%20DOF%20compared%20to%20dense%20actuator%20arrays.%20It%20also%20effectively%20handles%0Aheterogeneous%20objects%20of%20varying%20shapes%20and%20weights%2C%20even%20when%20they%20are%0Asignificantly%20smaller%20than%20the%20distance%20between%20actuators.%20This%20method%20is%0Aparticularly%20suitable%20for%20managing%20highly%20fragile%20objects%20in%20the%20food%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14290v2&entry.124074799=Read"},
{"title": "Endmember Extraction from Hyperspectral Images Using Self-Dictionary\n  Approach with Linear Programming", "author": "Tomohiko Mizutani", "abstract": "  Hyperspectral imaging technology has a wide range of applications, including\nforest management, mineral resource exploration, and Earth surface monitoring.\nA key step in utilizing this technology is endmember extraction, which aims to\nidentify the spectral signatures of materials in observed scenes. Theoretical\nstudies suggest that self-dictionary methods using linear programming (LP),\nknown as Hottopixx methods, are effective in extracting endmembers. However,\ntheir practical application is hindered by high computational costs, as they\nrequire solving LP problems whose size grows quadratically with the number of\npixels in the image. As a result, their actual effectiveness remains unclear.\nTo address this issue, we propose an enhanced implementation of Hottopixx\ndesigned to reduce computational time and improve endmember extraction\nperformance. We demonstrate its effectiveness through experiments. The results\nsuggest that our implementation enables the application of Hottopixx for\nendmember extraction from real hyperspectral images and allows us to achieve\nreasonably high accuracy in estimating endmember signatures.\n", "link": "http://arxiv.org/abs/2404.13098v3", "date": "2025-08-29", "relevancy": 1.9593, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5178}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4749}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Endmember%20Extraction%20from%20Hyperspectral%20Images%20Using%20Self-Dictionary%0A%20%20Approach%20with%20Linear%20Programming&body=Title%3A%20Endmember%20Extraction%20from%20Hyperspectral%20Images%20Using%20Self-Dictionary%0A%20%20Approach%20with%20Linear%20Programming%0AAuthor%3A%20Tomohiko%20Mizutani%0AAbstract%3A%20%20%20Hyperspectral%20imaging%20technology%20has%20a%20wide%20range%20of%20applications%2C%20including%0Aforest%20management%2C%20mineral%20resource%20exploration%2C%20and%20Earth%20surface%20monitoring.%0AA%20key%20step%20in%20utilizing%20this%20technology%20is%20endmember%20extraction%2C%20which%20aims%20to%0Aidentify%20the%20spectral%20signatures%20of%20materials%20in%20observed%20scenes.%20Theoretical%0Astudies%20suggest%20that%20self-dictionary%20methods%20using%20linear%20programming%20%28LP%29%2C%0Aknown%20as%20Hottopixx%20methods%2C%20are%20effective%20in%20extracting%20endmembers.%20However%2C%0Atheir%20practical%20application%20is%20hindered%20by%20high%20computational%20costs%2C%20as%20they%0Arequire%20solving%20LP%20problems%20whose%20size%20grows%20quadratically%20with%20the%20number%20of%0Apixels%20in%20the%20image.%20As%20a%20result%2C%20their%20actual%20effectiveness%20remains%20unclear.%0ATo%20address%20this%20issue%2C%20we%20propose%20an%20enhanced%20implementation%20of%20Hottopixx%0Adesigned%20to%20reduce%20computational%20time%20and%20improve%20endmember%20extraction%0Aperformance.%20We%20demonstrate%20its%20effectiveness%20through%20experiments.%20The%20results%0Asuggest%20that%20our%20implementation%20enables%20the%20application%20of%20Hottopixx%20for%0Aendmember%20extraction%20from%20real%20hyperspectral%20images%20and%20allows%20us%20to%20achieve%0Areasonably%20high%20accuracy%20in%20estimating%20endmember%20signatures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndmember%2520Extraction%2520from%2520Hyperspectral%2520Images%2520Using%2520Self-Dictionary%250A%2520%2520Approach%2520with%2520Linear%2520Programming%26entry.906535625%3DTomohiko%2520Mizutani%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%2520technology%2520has%2520a%2520wide%2520range%2520of%2520applications%252C%2520including%250Aforest%2520management%252C%2520mineral%2520resource%2520exploration%252C%2520and%2520Earth%2520surface%2520monitoring.%250AA%2520key%2520step%2520in%2520utilizing%2520this%2520technology%2520is%2520endmember%2520extraction%252C%2520which%2520aims%2520to%250Aidentify%2520the%2520spectral%2520signatures%2520of%2520materials%2520in%2520observed%2520scenes.%2520Theoretical%250Astudies%2520suggest%2520that%2520self-dictionary%2520methods%2520using%2520linear%2520programming%2520%2528LP%2529%252C%250Aknown%2520as%2520Hottopixx%2520methods%252C%2520are%2520effective%2520in%2520extracting%2520endmembers.%2520However%252C%250Atheir%2520practical%2520application%2520is%2520hindered%2520by%2520high%2520computational%2520costs%252C%2520as%2520they%250Arequire%2520solving%2520LP%2520problems%2520whose%2520size%2520grows%2520quadratically%2520with%2520the%2520number%2520of%250Apixels%2520in%2520the%2520image.%2520As%2520a%2520result%252C%2520their%2520actual%2520effectiveness%2520remains%2520unclear.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520enhanced%2520implementation%2520of%2520Hottopixx%250Adesigned%2520to%2520reduce%2520computational%2520time%2520and%2520improve%2520endmember%2520extraction%250Aperformance.%2520We%2520demonstrate%2520its%2520effectiveness%2520through%2520experiments.%2520The%2520results%250Asuggest%2520that%2520our%2520implementation%2520enables%2520the%2520application%2520of%2520Hottopixx%2520for%250Aendmember%2520extraction%2520from%2520real%2520hyperspectral%2520images%2520and%2520allows%2520us%2520to%2520achieve%250Areasonably%2520high%2520accuracy%2520in%2520estimating%2520endmember%2520signatures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Endmember%20Extraction%20from%20Hyperspectral%20Images%20Using%20Self-Dictionary%0A%20%20Approach%20with%20Linear%20Programming&entry.906535625=Tomohiko%20Mizutani&entry.1292438233=%20%20Hyperspectral%20imaging%20technology%20has%20a%20wide%20range%20of%20applications%2C%20including%0Aforest%20management%2C%20mineral%20resource%20exploration%2C%20and%20Earth%20surface%20monitoring.%0AA%20key%20step%20in%20utilizing%20this%20technology%20is%20endmember%20extraction%2C%20which%20aims%20to%0Aidentify%20the%20spectral%20signatures%20of%20materials%20in%20observed%20scenes.%20Theoretical%0Astudies%20suggest%20that%20self-dictionary%20methods%20using%20linear%20programming%20%28LP%29%2C%0Aknown%20as%20Hottopixx%20methods%2C%20are%20effective%20in%20extracting%20endmembers.%20However%2C%0Atheir%20practical%20application%20is%20hindered%20by%20high%20computational%20costs%2C%20as%20they%0Arequire%20solving%20LP%20problems%20whose%20size%20grows%20quadratically%20with%20the%20number%20of%0Apixels%20in%20the%20image.%20As%20a%20result%2C%20their%20actual%20effectiveness%20remains%20unclear.%0ATo%20address%20this%20issue%2C%20we%20propose%20an%20enhanced%20implementation%20of%20Hottopixx%0Adesigned%20to%20reduce%20computational%20time%20and%20improve%20endmember%20extraction%0Aperformance.%20We%20demonstrate%20its%20effectiveness%20through%20experiments.%20The%20results%0Asuggest%20that%20our%20implementation%20enables%20the%20application%20of%20Hottopixx%20for%0Aendmember%20extraction%20from%20real%20hyperspectral%20images%20and%20allows%20us%20to%20achieve%0Areasonably%20high%20accuracy%20in%20estimating%20endmember%20signatures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13098v3&entry.124074799=Read"},
{"title": "PiCSAR: Probabilistic Confidence Selection And Ranking", "author": "Joshua Ong Jun Leang and Zheng Zhao and Aryo Pradipta Gema and Sohee Yang and Wai-Chung Kwan and Xuanli He and Wenda Li and Pasquale Minervini and Eleonora Giunchiglia and Shay B. Cohen", "abstract": "  Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.\n", "link": "http://arxiv.org/abs/2508.21787v1", "date": "2025-08-29", "relevancy": 1.9454, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4796}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiCSAR%3A%20Probabilistic%20Confidence%20Selection%20And%20Ranking&body=Title%3A%20PiCSAR%3A%20Probabilistic%20Confidence%20Selection%20And%20Ranking%0AAuthor%3A%20Joshua%20Ong%20Jun%20Leang%20and%20Zheng%20Zhao%20and%20Aryo%20Pradipta%20Gema%20and%20Sohee%20Yang%20and%20Wai-Chung%20Kwan%20and%20Xuanli%20He%20and%20Wenda%20Li%20and%20Pasquale%20Minervini%20and%20Eleonora%20Giunchiglia%20and%20Shay%20B.%20Cohen%0AAbstract%3A%20%20%20Best-of-n%20sampling%20improves%20the%20accuracy%20of%20large%20language%20models%20%28LLMs%29%20and%0Alarge%20reasoning%20models%20%28LRMs%29%20by%20generating%20multiple%20candidate%20solutions%20and%0Aselecting%20the%20one%20with%20the%20highest%20reward.%20The%20key%20challenge%20for%20reasoning%0Atasks%20is%20designing%20a%20scoring%20function%20that%20can%20identify%20correct%20reasoning%0Achains%20without%20access%20to%20ground-truth%20answers.%20We%20propose%20Probabilistic%0AConfidence%20Selection%20And%20Ranking%20%28PiCSAR%29%3A%20a%20simple%2C%20training-free%20method%20that%0Ascores%20each%20candidate%20generation%20using%20the%20joint%20log-likelihood%20of%20the%0Areasoning%20and%20final%20answer.%20The%20joint%20log-likelihood%20of%20the%20reasoning%20and%20final%0Aanswer%20naturally%20decomposes%20into%20reasoning%20confidence%20and%20answer%20confidence.%0APiCSAR%20achieves%20substantial%20gains%20across%20diverse%20benchmarks%20%28%2B10.18%20on%20MATH500%2C%0A%2B9.81%20on%20AIME2025%29%2C%20outperforming%20baselines%20with%20at%20least%202x%20fewer%20samples%20in%0A16%20out%20of%2020%20comparisons.%20Our%20analysis%20reveals%20that%20correct%20reasoning%20chains%0Aexhibit%20significantly%20higher%20reasoning%20and%20answer%20confidence%2C%20justifying%20the%0Aeffectiveness%20of%20PiCSAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiCSAR%253A%2520Probabilistic%2520Confidence%2520Selection%2520And%2520Ranking%26entry.906535625%3DJoshua%2520Ong%2520Jun%2520Leang%2520and%2520Zheng%2520Zhao%2520and%2520Aryo%2520Pradipta%2520Gema%2520and%2520Sohee%2520Yang%2520and%2520Wai-Chung%2520Kwan%2520and%2520Xuanli%2520He%2520and%2520Wenda%2520Li%2520and%2520Pasquale%2520Minervini%2520and%2520Eleonora%2520Giunchiglia%2520and%2520Shay%2520B.%2520Cohen%26entry.1292438233%3D%2520%2520Best-of-n%2520sampling%2520improves%2520the%2520accuracy%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Alarge%2520reasoning%2520models%2520%2528LRMs%2529%2520by%2520generating%2520multiple%2520candidate%2520solutions%2520and%250Aselecting%2520the%2520one%2520with%2520the%2520highest%2520reward.%2520The%2520key%2520challenge%2520for%2520reasoning%250Atasks%2520is%2520designing%2520a%2520scoring%2520function%2520that%2520can%2520identify%2520correct%2520reasoning%250Achains%2520without%2520access%2520to%2520ground-truth%2520answers.%2520We%2520propose%2520Probabilistic%250AConfidence%2520Selection%2520And%2520Ranking%2520%2528PiCSAR%2529%253A%2520a%2520simple%252C%2520training-free%2520method%2520that%250Ascores%2520each%2520candidate%2520generation%2520using%2520the%2520joint%2520log-likelihood%2520of%2520the%250Areasoning%2520and%2520final%2520answer.%2520The%2520joint%2520log-likelihood%2520of%2520the%2520reasoning%2520and%2520final%250Aanswer%2520naturally%2520decomposes%2520into%2520reasoning%2520confidence%2520and%2520answer%2520confidence.%250APiCSAR%2520achieves%2520substantial%2520gains%2520across%2520diverse%2520benchmarks%2520%2528%252B10.18%2520on%2520MATH500%252C%250A%252B9.81%2520on%2520AIME2025%2529%252C%2520outperforming%2520baselines%2520with%2520at%2520least%25202x%2520fewer%2520samples%2520in%250A16%2520out%2520of%252020%2520comparisons.%2520Our%2520analysis%2520reveals%2520that%2520correct%2520reasoning%2520chains%250Aexhibit%2520significantly%2520higher%2520reasoning%2520and%2520answer%2520confidence%252C%2520justifying%2520the%250Aeffectiveness%2520of%2520PiCSAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiCSAR%3A%20Probabilistic%20Confidence%20Selection%20And%20Ranking&entry.906535625=Joshua%20Ong%20Jun%20Leang%20and%20Zheng%20Zhao%20and%20Aryo%20Pradipta%20Gema%20and%20Sohee%20Yang%20and%20Wai-Chung%20Kwan%20and%20Xuanli%20He%20and%20Wenda%20Li%20and%20Pasquale%20Minervini%20and%20Eleonora%20Giunchiglia%20and%20Shay%20B.%20Cohen&entry.1292438233=%20%20Best-of-n%20sampling%20improves%20the%20accuracy%20of%20large%20language%20models%20%28LLMs%29%20and%0Alarge%20reasoning%20models%20%28LRMs%29%20by%20generating%20multiple%20candidate%20solutions%20and%0Aselecting%20the%20one%20with%20the%20highest%20reward.%20The%20key%20challenge%20for%20reasoning%0Atasks%20is%20designing%20a%20scoring%20function%20that%20can%20identify%20correct%20reasoning%0Achains%20without%20access%20to%20ground-truth%20answers.%20We%20propose%20Probabilistic%0AConfidence%20Selection%20And%20Ranking%20%28PiCSAR%29%3A%20a%20simple%2C%20training-free%20method%20that%0Ascores%20each%20candidate%20generation%20using%20the%20joint%20log-likelihood%20of%20the%0Areasoning%20and%20final%20answer.%20The%20joint%20log-likelihood%20of%20the%20reasoning%20and%20final%0Aanswer%20naturally%20decomposes%20into%20reasoning%20confidence%20and%20answer%20confidence.%0APiCSAR%20achieves%20substantial%20gains%20across%20diverse%20benchmarks%20%28%2B10.18%20on%20MATH500%2C%0A%2B9.81%20on%20AIME2025%29%2C%20outperforming%20baselines%20with%20at%20least%202x%20fewer%20samples%20in%0A16%20out%20of%2020%20comparisons.%20Our%20analysis%20reveals%20that%20correct%20reasoning%20chains%0Aexhibit%20significantly%20higher%20reasoning%20and%20answer%20confidence%2C%20justifying%20the%0Aeffectiveness%20of%20PiCSAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21787v1&entry.124074799=Read"},
{"title": "Activation Subspaces for Out-of-Distribution Detection", "author": "Bar\u0131\u015f Z\u00f6ng\u00fcr and Robin Hesse and Stefan Roth", "abstract": "  To ensure the reliability of deep models in real-world applications,\nout-of-distribution (OOD) detection methods aim to distinguish samples close to\nthe training distribution (in-distribution, ID) from those farther away (OOD).\nIn this work, we propose a novel OOD detection method that utilizes singular\nvalue decomposition of the weight matrix of the classification head to\ndecompose the model's activations into decisive and insignificant components,\nwhich contribute maximally, respectively minimally, to the final classifier\noutput. We find that the subspace of insignificant components more effectively\ndistinguishes ID from OOD data than raw activations in regimes of large\ndistribution shifts (Far-OOD). This occurs because the classification objective\nleaves the insignificant subspace largely unaffected, yielding features that\nare ''untainted'' by the target classification task. Conversely, in regimes of\nsmaller distribution shifts (Near-OOD), we find that activation shaping methods\nprofit from only considering the decisive subspace, as the insignificant\ncomponent can cause interference in the activation space. By combining two\nfindings into a single approach, termed ActSub, we achieve state-of-the-art\nresults in various standard OOD benchmarks.\n", "link": "http://arxiv.org/abs/2508.21695v1", "date": "2025-08-29", "relevancy": 1.9452, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Subspaces%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Activation%20Subspaces%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Robin%20Hesse%20and%20Stefan%20Roth%0AAbstract%3A%20%20%20To%20ensure%20the%20reliability%20of%20deep%20models%20in%20real-world%20applications%2C%0Aout-of-distribution%20%28OOD%29%20detection%20methods%20aim%20to%20distinguish%20samples%20close%20to%0Athe%20training%20distribution%20%28in-distribution%2C%20ID%29%20from%20those%20farther%20away%20%28OOD%29.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20OOD%20detection%20method%20that%20utilizes%20singular%0Avalue%20decomposition%20of%20the%20weight%20matrix%20of%20the%20classification%20head%20to%0Adecompose%20the%20model%27s%20activations%20into%20decisive%20and%20insignificant%20components%2C%0Awhich%20contribute%20maximally%2C%20respectively%20minimally%2C%20to%20the%20final%20classifier%0Aoutput.%20We%20find%20that%20the%20subspace%20of%20insignificant%20components%20more%20effectively%0Adistinguishes%20ID%20from%20OOD%20data%20than%20raw%20activations%20in%20regimes%20of%20large%0Adistribution%20shifts%20%28Far-OOD%29.%20This%20occurs%20because%20the%20classification%20objective%0Aleaves%20the%20insignificant%20subspace%20largely%20unaffected%2C%20yielding%20features%20that%0Aare%20%27%27untainted%27%27%20by%20the%20target%20classification%20task.%20Conversely%2C%20in%20regimes%20of%0Asmaller%20distribution%20shifts%20%28Near-OOD%29%2C%20we%20find%20that%20activation%20shaping%20methods%0Aprofit%20from%20only%20considering%20the%20decisive%20subspace%2C%20as%20the%20insignificant%0Acomponent%20can%20cause%20interference%20in%20the%20activation%20space.%20By%20combining%20two%0Afindings%20into%20a%20single%20approach%2C%20termed%20ActSub%2C%20we%20achieve%20state-of-the-art%0Aresults%20in%20various%20standard%20OOD%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Subspaces%2520for%2520Out-of-Distribution%2520Detection%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520Z%25C3%25B6ng%25C3%25BCr%2520and%2520Robin%2520Hesse%2520and%2520Stefan%2520Roth%26entry.1292438233%3D%2520%2520To%2520ensure%2520the%2520reliability%2520of%2520deep%2520models%2520in%2520real-world%2520applications%252C%250Aout-of-distribution%2520%2528OOD%2529%2520detection%2520methods%2520aim%2520to%2520distinguish%2520samples%2520close%2520to%250Athe%2520training%2520distribution%2520%2528in-distribution%252C%2520ID%2529%2520from%2520those%2520farther%2520away%2520%2528OOD%2529.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520OOD%2520detection%2520method%2520that%2520utilizes%2520singular%250Avalue%2520decomposition%2520of%2520the%2520weight%2520matrix%2520of%2520the%2520classification%2520head%2520to%250Adecompose%2520the%2520model%2527s%2520activations%2520into%2520decisive%2520and%2520insignificant%2520components%252C%250Awhich%2520contribute%2520maximally%252C%2520respectively%2520minimally%252C%2520to%2520the%2520final%2520classifier%250Aoutput.%2520We%2520find%2520that%2520the%2520subspace%2520of%2520insignificant%2520components%2520more%2520effectively%250Adistinguishes%2520ID%2520from%2520OOD%2520data%2520than%2520raw%2520activations%2520in%2520regimes%2520of%2520large%250Adistribution%2520shifts%2520%2528Far-OOD%2529.%2520This%2520occurs%2520because%2520the%2520classification%2520objective%250Aleaves%2520the%2520insignificant%2520subspace%2520largely%2520unaffected%252C%2520yielding%2520features%2520that%250Aare%2520%2527%2527untainted%2527%2527%2520by%2520the%2520target%2520classification%2520task.%2520Conversely%252C%2520in%2520regimes%2520of%250Asmaller%2520distribution%2520shifts%2520%2528Near-OOD%2529%252C%2520we%2520find%2520that%2520activation%2520shaping%2520methods%250Aprofit%2520from%2520only%2520considering%2520the%2520decisive%2520subspace%252C%2520as%2520the%2520insignificant%250Acomponent%2520can%2520cause%2520interference%2520in%2520the%2520activation%2520space.%2520By%2520combining%2520two%250Afindings%2520into%2520a%2520single%2520approach%252C%2520termed%2520ActSub%252C%2520we%2520achieve%2520state-of-the-art%250Aresults%2520in%2520various%2520standard%2520OOD%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Subspaces%20for%20Out-of-Distribution%20Detection&entry.906535625=Bar%C4%B1%C5%9F%20Z%C3%B6ng%C3%BCr%20and%20Robin%20Hesse%20and%20Stefan%20Roth&entry.1292438233=%20%20To%20ensure%20the%20reliability%20of%20deep%20models%20in%20real-world%20applications%2C%0Aout-of-distribution%20%28OOD%29%20detection%20methods%20aim%20to%20distinguish%20samples%20close%20to%0Athe%20training%20distribution%20%28in-distribution%2C%20ID%29%20from%20those%20farther%20away%20%28OOD%29.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20OOD%20detection%20method%20that%20utilizes%20singular%0Avalue%20decomposition%20of%20the%20weight%20matrix%20of%20the%20classification%20head%20to%0Adecompose%20the%20model%27s%20activations%20into%20decisive%20and%20insignificant%20components%2C%0Awhich%20contribute%20maximally%2C%20respectively%20minimally%2C%20to%20the%20final%20classifier%0Aoutput.%20We%20find%20that%20the%20subspace%20of%20insignificant%20components%20more%20effectively%0Adistinguishes%20ID%20from%20OOD%20data%20than%20raw%20activations%20in%20regimes%20of%20large%0Adistribution%20shifts%20%28Far-OOD%29.%20This%20occurs%20because%20the%20classification%20objective%0Aleaves%20the%20insignificant%20subspace%20largely%20unaffected%2C%20yielding%20features%20that%0Aare%20%27%27untainted%27%27%20by%20the%20target%20classification%20task.%20Conversely%2C%20in%20regimes%20of%0Asmaller%20distribution%20shifts%20%28Near-OOD%29%2C%20we%20find%20that%20activation%20shaping%20methods%0Aprofit%20from%20only%20considering%20the%20decisive%20subspace%2C%20as%20the%20insignificant%0Acomponent%20can%20cause%20interference%20in%20the%20activation%20space.%20By%20combining%20two%0Afindings%20into%20a%20single%20approach%2C%20termed%20ActSub%2C%20we%20achieve%20state-of-the-art%0Aresults%20in%20various%20standard%20OOD%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21695v1&entry.124074799=Read"},
{"title": "Is this chart lying to me? Automating the detection of misleading\n  visualizations", "author": "Jonathan Tonglet and Jan Zimny and Tinne Tuytelaars and Iryna Gurevych", "abstract": "  Misleading visualizations are a potent driver of misinformation on social\nmedia and the web. By violating chart design principles, they distort data and\nlead readers to draw inaccurate conclusions. Prior work has shown that both\nhumans and multimodal large language models (MLLMs) are frequently deceived by\nsuch visualizations. Automatically detecting misleading visualizations and\nidentifying the specific design rules they violate could help protect readers\nand reduce the spread of misinformation. However, the training and evaluation\nof AI models has been limited by the absence of large, diverse, and openly\navailable datasets. In this work, we introduce Misviz, a benchmark of 2,604\nreal-world visualizations annotated with 12 types of misleaders. To support\nmodel training, we also release Misviz-synth, a synthetic dataset of 81,814\nvisualizations generated using Matplotlib and based on real-world data tables.\nWe perform a comprehensive evaluation on both datasets using state-of-the-art\nMLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that\nthe task remains highly challenging. We release Misviz, Misviz-synth, and the\naccompanying code.\n", "link": "http://arxiv.org/abs/2508.21675v1", "date": "2025-08-29", "relevancy": 1.9443, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%0A%20%20visualizations&body=Title%3A%20Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%0A%20%20visualizations%0AAuthor%3A%20Jonathan%20Tonglet%20and%20Jan%20Zimny%20and%20Tinne%20Tuytelaars%20and%20Iryna%20Gurevych%0AAbstract%3A%20%20%20Misleading%20visualizations%20are%20a%20potent%20driver%20of%20misinformation%20on%20social%0Amedia%20and%20the%20web.%20By%20violating%20chart%20design%20principles%2C%20they%20distort%20data%20and%0Alead%20readers%20to%20draw%20inaccurate%20conclusions.%20Prior%20work%20has%20shown%20that%20both%0Ahumans%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20frequently%20deceived%20by%0Asuch%20visualizations.%20Automatically%20detecting%20misleading%20visualizations%20and%0Aidentifying%20the%20specific%20design%20rules%20they%20violate%20could%20help%20protect%20readers%0Aand%20reduce%20the%20spread%20of%20misinformation.%20However%2C%20the%20training%20and%20evaluation%0Aof%20AI%20models%20has%20been%20limited%20by%20the%20absence%20of%20large%2C%20diverse%2C%20and%20openly%0Aavailable%20datasets.%20In%20this%20work%2C%20we%20introduce%20Misviz%2C%20a%20benchmark%20of%202%2C604%0Areal-world%20visualizations%20annotated%20with%2012%20types%20of%20misleaders.%20To%20support%0Amodel%20training%2C%20we%20also%20release%20Misviz-synth%2C%20a%20synthetic%20dataset%20of%2081%2C814%0Avisualizations%20generated%20using%20Matplotlib%20and%20based%20on%20real-world%20data%20tables.%0AWe%20perform%20a%20comprehensive%20evaluation%20on%20both%20datasets%20using%20state-of-the-art%0AMLLMs%2C%20rule-based%20systems%2C%20and%20fine-tuned%20classifiers.%20Our%20results%20reveal%20that%0Athe%20task%20remains%20highly%20challenging.%20We%20release%20Misviz%2C%20Misviz-synth%2C%20and%20the%0Aaccompanying%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520this%2520chart%2520lying%2520to%2520me%253F%2520Automating%2520the%2520detection%2520of%2520misleading%250A%2520%2520visualizations%26entry.906535625%3DJonathan%2520Tonglet%2520and%2520Jan%2520Zimny%2520and%2520Tinne%2520Tuytelaars%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3D%2520%2520Misleading%2520visualizations%2520are%2520a%2520potent%2520driver%2520of%2520misinformation%2520on%2520social%250Amedia%2520and%2520the%2520web.%2520By%2520violating%2520chart%2520design%2520principles%252C%2520they%2520distort%2520data%2520and%250Alead%2520readers%2520to%2520draw%2520inaccurate%2520conclusions.%2520Prior%2520work%2520has%2520shown%2520that%2520both%250Ahumans%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520frequently%2520deceived%2520by%250Asuch%2520visualizations.%2520Automatically%2520detecting%2520misleading%2520visualizations%2520and%250Aidentifying%2520the%2520specific%2520design%2520rules%2520they%2520violate%2520could%2520help%2520protect%2520readers%250Aand%2520reduce%2520the%2520spread%2520of%2520misinformation.%2520However%252C%2520the%2520training%2520and%2520evaluation%250Aof%2520AI%2520models%2520has%2520been%2520limited%2520by%2520the%2520absence%2520of%2520large%252C%2520diverse%252C%2520and%2520openly%250Aavailable%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Misviz%252C%2520a%2520benchmark%2520of%25202%252C604%250Areal-world%2520visualizations%2520annotated%2520with%252012%2520types%2520of%2520misleaders.%2520To%2520support%250Amodel%2520training%252C%2520we%2520also%2520release%2520Misviz-synth%252C%2520a%2520synthetic%2520dataset%2520of%252081%252C814%250Avisualizations%2520generated%2520using%2520Matplotlib%2520and%2520based%2520on%2520real-world%2520data%2520tables.%250AWe%2520perform%2520a%2520comprehensive%2520evaluation%2520on%2520both%2520datasets%2520using%2520state-of-the-art%250AMLLMs%252C%2520rule-based%2520systems%252C%2520and%2520fine-tuned%2520classifiers.%2520Our%2520results%2520reveal%2520that%250Athe%2520task%2520remains%2520highly%2520challenging.%2520We%2520release%2520Misviz%252C%2520Misviz-synth%252C%2520and%2520the%250Aaccompanying%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20this%20chart%20lying%20to%20me%3F%20Automating%20the%20detection%20of%20misleading%0A%20%20visualizations&entry.906535625=Jonathan%20Tonglet%20and%20Jan%20Zimny%20and%20Tinne%20Tuytelaars%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Misleading%20visualizations%20are%20a%20potent%20driver%20of%20misinformation%20on%20social%0Amedia%20and%20the%20web.%20By%20violating%20chart%20design%20principles%2C%20they%20distort%20data%20and%0Alead%20readers%20to%20draw%20inaccurate%20conclusions.%20Prior%20work%20has%20shown%20that%20both%0Ahumans%20and%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20frequently%20deceived%20by%0Asuch%20visualizations.%20Automatically%20detecting%20misleading%20visualizations%20and%0Aidentifying%20the%20specific%20design%20rules%20they%20violate%20could%20help%20protect%20readers%0Aand%20reduce%20the%20spread%20of%20misinformation.%20However%2C%20the%20training%20and%20evaluation%0Aof%20AI%20models%20has%20been%20limited%20by%20the%20absence%20of%20large%2C%20diverse%2C%20and%20openly%0Aavailable%20datasets.%20In%20this%20work%2C%20we%20introduce%20Misviz%2C%20a%20benchmark%20of%202%2C604%0Areal-world%20visualizations%20annotated%20with%2012%20types%20of%20misleaders.%20To%20support%0Amodel%20training%2C%20we%20also%20release%20Misviz-synth%2C%20a%20synthetic%20dataset%20of%2081%2C814%0Avisualizations%20generated%20using%20Matplotlib%20and%20based%20on%20real-world%20data%20tables.%0AWe%20perform%20a%20comprehensive%20evaluation%20on%20both%20datasets%20using%20state-of-the-art%0AMLLMs%2C%20rule-based%20systems%2C%20and%20fine-tuned%20classifiers.%20Our%20results%20reveal%20that%0Athe%20task%20remains%20highly%20challenging.%20We%20release%20Misviz%2C%20Misviz-synth%2C%20and%20the%0Aaccompanying%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21675v1&entry.124074799=Read"},
{"title": "HSFN: Hierarchical Selection for Fake News Detection building\n  Heterogeneous Ensemble", "author": "Sara B. Coutinho and Rafael M. O. Cruz and Francimaria R. S. Nascimento and George D. C. Cavalcanti", "abstract": "  Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .\n", "link": "http://arxiv.org/abs/2508.21482v1", "date": "2025-08-29", "relevancy": 1.9404, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4924}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4843}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSFN%3A%20Hierarchical%20Selection%20for%20Fake%20News%20Detection%20building%0A%20%20Heterogeneous%20Ensemble&body=Title%3A%20HSFN%3A%20Hierarchical%20Selection%20for%20Fake%20News%20Detection%20building%0A%20%20Heterogeneous%20Ensemble%0AAuthor%3A%20Sara%20B.%20Coutinho%20and%20Rafael%20M.%20O.%20Cruz%20and%20Francimaria%20R.%20S.%20Nascimento%20and%20George%20D.%20C.%20Cavalcanti%0AAbstract%3A%20%20%20Psychological%20biases%2C%20such%20as%20confirmation%20bias%2C%20make%20individuals%0Aparticularly%20vulnerable%20to%20believing%20and%20spreading%20fake%20news%20on%20social%20media%2C%0Aleading%20to%20significant%20consequences%20in%20domains%20such%20as%20public%20health%20and%0Apolitics.%20Machine%20learning-based%20fact-checking%20systems%20have%20been%20widely%20studied%0Ato%20mitigate%20this%20problem.%20Among%20them%2C%20ensemble%20methods%20are%20particularly%0Aeffective%20in%20combining%20multiple%20classifiers%20to%20improve%20robustness.%20However%2C%0Atheir%20performance%20heavily%20depends%20on%20the%20diversity%20of%20the%20constituent%0Aclassifiers-selecting%20genuinely%20diverse%20models%20remains%20a%20key%20challenge%2C%0Aespecially%20when%20models%20tend%20to%20learn%20redundant%20patterns.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20automatic%20classifier%20selection%20approach%20that%20prioritizes%0Adiversity%2C%20also%20extended%20by%20performance.%20The%20method%20first%20computes%20pairwise%0Adiversity%20between%20classifiers%20and%20applies%20hierarchical%20clustering%20to%20organize%0Athem%20into%20groups%20at%20different%20levels%20of%20granularity.%20A%20HierarchySelect%20then%0Aexplores%20these%20hierarchical%20levels%20to%20select%20one%20pool%20of%20classifiers%20per%20level%2C%0Aeach%20representing%20a%20distinct%20intra-pool%20diversity.%20The%20most%20diverse%20pool%20is%0Aidentified%20and%20selected%20for%20ensemble%20construction%20from%20these.%20The%20selection%0Aprocess%20incorporates%20an%20evaluation%20metric%20reflecting%20each%20classifiers%27s%0Aperformance%20to%20ensure%20the%20ensemble%20also%20generalises%20well.%20We%20conduct%0Aexperiments%20with%2040%20heterogeneous%20classifiers%20across%20six%20datasets%20from%0Adifferent%20application%20domains%20and%20with%20varying%20numbers%20of%20classes.%20Our%20method%0Ais%20compared%20against%20the%20Elbow%20heuristic%20and%20state-of-the-art%20baselines.%20Results%0Ashow%20that%20our%20approach%20achieves%20the%20highest%20accuracy%20on%20two%20of%20six%20datasets.%0AThe%20implementation%20details%20are%20available%20on%20the%20project%27s%20repository%3A%0Ahttps%3A//github.com/SaraBCoutinho/HSFN%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSFN%253A%2520Hierarchical%2520Selection%2520for%2520Fake%2520News%2520Detection%2520building%250A%2520%2520Heterogeneous%2520Ensemble%26entry.906535625%3DSara%2520B.%2520Coutinho%2520and%2520Rafael%2520M.%2520O.%2520Cruz%2520and%2520Francimaria%2520R.%2520S.%2520Nascimento%2520and%2520George%2520D.%2520C.%2520Cavalcanti%26entry.1292438233%3D%2520%2520Psychological%2520biases%252C%2520such%2520as%2520confirmation%2520bias%252C%2520make%2520individuals%250Aparticularly%2520vulnerable%2520to%2520believing%2520and%2520spreading%2520fake%2520news%2520on%2520social%2520media%252C%250Aleading%2520to%2520significant%2520consequences%2520in%2520domains%2520such%2520as%2520public%2520health%2520and%250Apolitics.%2520Machine%2520learning-based%2520fact-checking%2520systems%2520have%2520been%2520widely%2520studied%250Ato%2520mitigate%2520this%2520problem.%2520Among%2520them%252C%2520ensemble%2520methods%2520are%2520particularly%250Aeffective%2520in%2520combining%2520multiple%2520classifiers%2520to%2520improve%2520robustness.%2520However%252C%250Atheir%2520performance%2520heavily%2520depends%2520on%2520the%2520diversity%2520of%2520the%2520constituent%250Aclassifiers-selecting%2520genuinely%2520diverse%2520models%2520remains%2520a%2520key%2520challenge%252C%250Aespecially%2520when%2520models%2520tend%2520to%2520learn%2520redundant%2520patterns.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520automatic%2520classifier%2520selection%2520approach%2520that%2520prioritizes%250Adiversity%252C%2520also%2520extended%2520by%2520performance.%2520The%2520method%2520first%2520computes%2520pairwise%250Adiversity%2520between%2520classifiers%2520and%2520applies%2520hierarchical%2520clustering%2520to%2520organize%250Athem%2520into%2520groups%2520at%2520different%2520levels%2520of%2520granularity.%2520A%2520HierarchySelect%2520then%250Aexplores%2520these%2520hierarchical%2520levels%2520to%2520select%2520one%2520pool%2520of%2520classifiers%2520per%2520level%252C%250Aeach%2520representing%2520a%2520distinct%2520intra-pool%2520diversity.%2520The%2520most%2520diverse%2520pool%2520is%250Aidentified%2520and%2520selected%2520for%2520ensemble%2520construction%2520from%2520these.%2520The%2520selection%250Aprocess%2520incorporates%2520an%2520evaluation%2520metric%2520reflecting%2520each%2520classifiers%2527s%250Aperformance%2520to%2520ensure%2520the%2520ensemble%2520also%2520generalises%2520well.%2520We%2520conduct%250Aexperiments%2520with%252040%2520heterogeneous%2520classifiers%2520across%2520six%2520datasets%2520from%250Adifferent%2520application%2520domains%2520and%2520with%2520varying%2520numbers%2520of%2520classes.%2520Our%2520method%250Ais%2520compared%2520against%2520the%2520Elbow%2520heuristic%2520and%2520state-of-the-art%2520baselines.%2520Results%250Ashow%2520that%2520our%2520approach%2520achieves%2520the%2520highest%2520accuracy%2520on%2520two%2520of%2520six%2520datasets.%250AThe%2520implementation%2520details%2520are%2520available%2520on%2520the%2520project%2527s%2520repository%253A%250Ahttps%253A//github.com/SaraBCoutinho/HSFN%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSFN%3A%20Hierarchical%20Selection%20for%20Fake%20News%20Detection%20building%0A%20%20Heterogeneous%20Ensemble&entry.906535625=Sara%20B.%20Coutinho%20and%20Rafael%20M.%20O.%20Cruz%20and%20Francimaria%20R.%20S.%20Nascimento%20and%20George%20D.%20C.%20Cavalcanti&entry.1292438233=%20%20Psychological%20biases%2C%20such%20as%20confirmation%20bias%2C%20make%20individuals%0Aparticularly%20vulnerable%20to%20believing%20and%20spreading%20fake%20news%20on%20social%20media%2C%0Aleading%20to%20significant%20consequences%20in%20domains%20such%20as%20public%20health%20and%0Apolitics.%20Machine%20learning-based%20fact-checking%20systems%20have%20been%20widely%20studied%0Ato%20mitigate%20this%20problem.%20Among%20them%2C%20ensemble%20methods%20are%20particularly%0Aeffective%20in%20combining%20multiple%20classifiers%20to%20improve%20robustness.%20However%2C%0Atheir%20performance%20heavily%20depends%20on%20the%20diversity%20of%20the%20constituent%0Aclassifiers-selecting%20genuinely%20diverse%20models%20remains%20a%20key%20challenge%2C%0Aespecially%20when%20models%20tend%20to%20learn%20redundant%20patterns.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20automatic%20classifier%20selection%20approach%20that%20prioritizes%0Adiversity%2C%20also%20extended%20by%20performance.%20The%20method%20first%20computes%20pairwise%0Adiversity%20between%20classifiers%20and%20applies%20hierarchical%20clustering%20to%20organize%0Athem%20into%20groups%20at%20different%20levels%20of%20granularity.%20A%20HierarchySelect%20then%0Aexplores%20these%20hierarchical%20levels%20to%20select%20one%20pool%20of%20classifiers%20per%20level%2C%0Aeach%20representing%20a%20distinct%20intra-pool%20diversity.%20The%20most%20diverse%20pool%20is%0Aidentified%20and%20selected%20for%20ensemble%20construction%20from%20these.%20The%20selection%0Aprocess%20incorporates%20an%20evaluation%20metric%20reflecting%20each%20classifiers%27s%0Aperformance%20to%20ensure%20the%20ensemble%20also%20generalises%20well.%20We%20conduct%0Aexperiments%20with%2040%20heterogeneous%20classifiers%20across%20six%20datasets%20from%0Adifferent%20application%20domains%20and%20with%20varying%20numbers%20of%20classes.%20Our%20method%0Ais%20compared%20against%20the%20Elbow%20heuristic%20and%20state-of-the-art%20baselines.%20Results%0Ashow%20that%20our%20approach%20achieves%20the%20highest%20accuracy%20on%20two%20of%20six%20datasets.%0AThe%20implementation%20details%20are%20available%20on%20the%20project%27s%20repository%3A%0Ahttps%3A//github.com/SaraBCoutinho/HSFN%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21482v1&entry.124074799=Read"},
{"title": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study", "author": "Saravanan Venkatachalam", "abstract": "  This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.\n", "link": "http://arxiv.org/abs/2508.21622v1", "date": "2025-08-29", "relevancy": 1.9261, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Large%20Language%20Models%20with%20Network%20Optimization%20for%0A%20%20Interactive%20and%20Explainable%20Supply%20Chain%20Planning%3A%20A%20Real-World%20Case%20Study&body=Title%3A%20Integrating%20Large%20Language%20Models%20with%20Network%20Optimization%20for%0A%20%20Interactive%20and%20Explainable%20Supply%20Chain%20Planning%3A%20A%20Real-World%20Case%20Study%0AAuthor%3A%20Saravanan%20Venkatachalam%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20integrated%20framework%20that%20combines%20traditional%20network%0Aoptimization%20models%20with%20large%20language%20models%20%28LLMs%29%20to%20deliver%20interactive%2C%0Aexplainable%2C%20and%20role-aware%20decision%20support%20for%20supply%20chain%20planning.%20The%0Aproposed%20system%20bridges%20the%20gap%20between%20complex%20operations%20research%20outputs%20and%0Abusiness%20stakeholder%20understanding%20by%20generating%20natural%20language%20summaries%2C%0Acontextual%20visualizations%2C%20and%20tailored%20key%20performance%20indicators%20%28KPIs%29.%20The%0Acore%20optimization%20model%20addresses%20tactical%20inventory%20redistribution%20across%20a%0Anetwork%20of%20distribution%20centers%20for%20multi-period%20and%20multi-item%2C%20using%20a%0Amixed-integer%20formulation.%20The%20technical%20architecture%20incorporates%20AI%20agents%2C%0ARESTful%20APIs%2C%20and%20a%20dynamic%20user%20interface%20to%20support%20real-time%20interaction%2C%0Aconfiguration%20updates%2C%20and%20simulation-based%20insights.%20A%20case%20study%20demonstrates%0Ahow%20the%20system%20improves%20planning%20outcomes%20by%20preventing%20stockouts%2C%20reducing%0Acosts%2C%20and%20maintaining%20service%20levels.%20Future%20extensions%20include%20integrating%0Aprivate%20LLMs%2C%20transfer%20learning%2C%20reinforcement%20learning%2C%20and%20Bayesian%20neural%0Anetworks%20to%20enhance%20explainability%2C%20adaptability%2C%20and%20real-time%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Large%2520Language%2520Models%2520with%2520Network%2520Optimization%2520for%250A%2520%2520Interactive%2520and%2520Explainable%2520Supply%2520Chain%2520Planning%253A%2520A%2520Real-World%2520Case%2520Study%26entry.906535625%3DSaravanan%2520Venkatachalam%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520integrated%2520framework%2520that%2520combines%2520traditional%2520network%250Aoptimization%2520models%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520deliver%2520interactive%252C%250Aexplainable%252C%2520and%2520role-aware%2520decision%2520support%2520for%2520supply%2520chain%2520planning.%2520The%250Aproposed%2520system%2520bridges%2520the%2520gap%2520between%2520complex%2520operations%2520research%2520outputs%2520and%250Abusiness%2520stakeholder%2520understanding%2520by%2520generating%2520natural%2520language%2520summaries%252C%250Acontextual%2520visualizations%252C%2520and%2520tailored%2520key%2520performance%2520indicators%2520%2528KPIs%2529.%2520The%250Acore%2520optimization%2520model%2520addresses%2520tactical%2520inventory%2520redistribution%2520across%2520a%250Anetwork%2520of%2520distribution%2520centers%2520for%2520multi-period%2520and%2520multi-item%252C%2520using%2520a%250Amixed-integer%2520formulation.%2520The%2520technical%2520architecture%2520incorporates%2520AI%2520agents%252C%250ARESTful%2520APIs%252C%2520and%2520a%2520dynamic%2520user%2520interface%2520to%2520support%2520real-time%2520interaction%252C%250Aconfiguration%2520updates%252C%2520and%2520simulation-based%2520insights.%2520A%2520case%2520study%2520demonstrates%250Ahow%2520the%2520system%2520improves%2520planning%2520outcomes%2520by%2520preventing%2520stockouts%252C%2520reducing%250Acosts%252C%2520and%2520maintaining%2520service%2520levels.%2520Future%2520extensions%2520include%2520integrating%250Aprivate%2520LLMs%252C%2520transfer%2520learning%252C%2520reinforcement%2520learning%252C%2520and%2520Bayesian%2520neural%250Anetworks%2520to%2520enhance%2520explainability%252C%2520adaptability%252C%2520and%2520real-time%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Large%20Language%20Models%20with%20Network%20Optimization%20for%0A%20%20Interactive%20and%20Explainable%20Supply%20Chain%20Planning%3A%20A%20Real-World%20Case%20Study&entry.906535625=Saravanan%20Venkatachalam&entry.1292438233=%20%20This%20paper%20presents%20an%20integrated%20framework%20that%20combines%20traditional%20network%0Aoptimization%20models%20with%20large%20language%20models%20%28LLMs%29%20to%20deliver%20interactive%2C%0Aexplainable%2C%20and%20role-aware%20decision%20support%20for%20supply%20chain%20planning.%20The%0Aproposed%20system%20bridges%20the%20gap%20between%20complex%20operations%20research%20outputs%20and%0Abusiness%20stakeholder%20understanding%20by%20generating%20natural%20language%20summaries%2C%0Acontextual%20visualizations%2C%20and%20tailored%20key%20performance%20indicators%20%28KPIs%29.%20The%0Acore%20optimization%20model%20addresses%20tactical%20inventory%20redistribution%20across%20a%0Anetwork%20of%20distribution%20centers%20for%20multi-period%20and%20multi-item%2C%20using%20a%0Amixed-integer%20formulation.%20The%20technical%20architecture%20incorporates%20AI%20agents%2C%0ARESTful%20APIs%2C%20and%20a%20dynamic%20user%20interface%20to%20support%20real-time%20interaction%2C%0Aconfiguration%20updates%2C%20and%20simulation-based%20insights.%20A%20case%20study%20demonstrates%0Ahow%20the%20system%20improves%20planning%20outcomes%20by%20preventing%20stockouts%2C%20reducing%0Acosts%2C%20and%20maintaining%20service%20levels.%20Future%20extensions%20include%20integrating%0Aprivate%20LLMs%2C%20transfer%20learning%2C%20reinforcement%20learning%2C%20and%20Bayesian%20neural%0Anetworks%20to%20enhance%20explainability%2C%20adaptability%2C%20and%20real-time%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21622v1&entry.124074799=Read"},
{"title": "FORGE: Foundational Optimization Representations from Graph Embeddings", "author": "Zohair Shafi and Serdar Kadioglu", "abstract": "  Combinatorial optimization problems are ubiquitous in science and\nengineering, yet learning-based approaches to accelerate their solution often\nrequire solving a large number of hard-to-solve optimization instances to\ncollect training data, incurring significant computational overhead. Existing\nmethods require training dedicated models for each problem distribution for\neach downstream task, severely limiting their scalability and generalization.\nIn this work, we introduce Forge, a method of pre-training a vector-quantized\ngraph autoencoder on a large and diverse collection of mixed-integer\nprogramming (MIP) instances in an unsupervised fashion without dependency on\ntheir solution. The vector quantization process creates discrete code\nassignments that act as a vocabulary to represent optimization instances. We\nevaluate our approach under both supervised and unsupervised settings. For the\nunsupervised setting, we demonstrate that Forge embeddings effectively\ndifferentiate and cluster unseen instances. For the supervised setting, we\nfine-tuneForge embeddings and show that a single model predicts both the\nvariables for warm-starts and integrality gaps for cut-generation across\nmultiple problem type distributions. Both predictions help improve performance\nof a state-of-the-art, commercial optimization solver. Finally, we release our\ncode and pre-trained Forge weights to encourage further research and practical\nuse of instance-level MIP embeddings at https://github.com/skadio/forge/.\n", "link": "http://arxiv.org/abs/2508.20330v2", "date": "2025-08-29", "relevancy": 1.925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORGE%3A%20Foundational%20Optimization%20Representations%20from%20Graph%20Embeddings&body=Title%3A%20FORGE%3A%20Foundational%20Optimization%20Representations%20from%20Graph%20Embeddings%0AAuthor%3A%20Zohair%20Shafi%20and%20Serdar%20Kadioglu%0AAbstract%3A%20%20%20Combinatorial%20optimization%20problems%20are%20ubiquitous%20in%20science%20and%0Aengineering%2C%20yet%20learning-based%20approaches%20to%20accelerate%20their%20solution%20often%0Arequire%20solving%20a%20large%20number%20of%20hard-to-solve%20optimization%20instances%20to%0Acollect%20training%20data%2C%20incurring%20significant%20computational%20overhead.%20Existing%0Amethods%20require%20training%20dedicated%20models%20for%20each%20problem%20distribution%20for%0Aeach%20downstream%20task%2C%20severely%20limiting%20their%20scalability%20and%20generalization.%0AIn%20this%20work%2C%20we%20introduce%20Forge%2C%20a%20method%20of%20pre-training%20a%20vector-quantized%0Agraph%20autoencoder%20on%20a%20large%20and%20diverse%20collection%20of%20mixed-integer%0Aprogramming%20%28MIP%29%20instances%20in%20an%20unsupervised%20fashion%20without%20dependency%20on%0Atheir%20solution.%20The%20vector%20quantization%20process%20creates%20discrete%20code%0Aassignments%20that%20act%20as%20a%20vocabulary%20to%20represent%20optimization%20instances.%20We%0Aevaluate%20our%20approach%20under%20both%20supervised%20and%20unsupervised%20settings.%20For%20the%0Aunsupervised%20setting%2C%20we%20demonstrate%20that%20Forge%20embeddings%20effectively%0Adifferentiate%20and%20cluster%20unseen%20instances.%20For%20the%20supervised%20setting%2C%20we%0Afine-tuneForge%20embeddings%20and%20show%20that%20a%20single%20model%20predicts%20both%20the%0Avariables%20for%20warm-starts%20and%20integrality%20gaps%20for%20cut-generation%20across%0Amultiple%20problem%20type%20distributions.%20Both%20predictions%20help%20improve%20performance%0Aof%20a%20state-of-the-art%2C%20commercial%20optimization%20solver.%20Finally%2C%20we%20release%20our%0Acode%20and%20pre-trained%20Forge%20weights%20to%20encourage%20further%20research%20and%20practical%0Ause%20of%20instance-level%20MIP%20embeddings%20at%20https%3A//github.com/skadio/forge/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORGE%253A%2520Foundational%2520Optimization%2520Representations%2520from%2520Graph%2520Embeddings%26entry.906535625%3DZohair%2520Shafi%2520and%2520Serdar%2520Kadioglu%26entry.1292438233%3D%2520%2520Combinatorial%2520optimization%2520problems%2520are%2520ubiquitous%2520in%2520science%2520and%250Aengineering%252C%2520yet%2520learning-based%2520approaches%2520to%2520accelerate%2520their%2520solution%2520often%250Arequire%2520solving%2520a%2520large%2520number%2520of%2520hard-to-solve%2520optimization%2520instances%2520to%250Acollect%2520training%2520data%252C%2520incurring%2520significant%2520computational%2520overhead.%2520Existing%250Amethods%2520require%2520training%2520dedicated%2520models%2520for%2520each%2520problem%2520distribution%2520for%250Aeach%2520downstream%2520task%252C%2520severely%2520limiting%2520their%2520scalability%2520and%2520generalization.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520Forge%252C%2520a%2520method%2520of%2520pre-training%2520a%2520vector-quantized%250Agraph%2520autoencoder%2520on%2520a%2520large%2520and%2520diverse%2520collection%2520of%2520mixed-integer%250Aprogramming%2520%2528MIP%2529%2520instances%2520in%2520an%2520unsupervised%2520fashion%2520without%2520dependency%2520on%250Atheir%2520solution.%2520The%2520vector%2520quantization%2520process%2520creates%2520discrete%2520code%250Aassignments%2520that%2520act%2520as%2520a%2520vocabulary%2520to%2520represent%2520optimization%2520instances.%2520We%250Aevaluate%2520our%2520approach%2520under%2520both%2520supervised%2520and%2520unsupervised%2520settings.%2520For%2520the%250Aunsupervised%2520setting%252C%2520we%2520demonstrate%2520that%2520Forge%2520embeddings%2520effectively%250Adifferentiate%2520and%2520cluster%2520unseen%2520instances.%2520For%2520the%2520supervised%2520setting%252C%2520we%250Afine-tuneForge%2520embeddings%2520and%2520show%2520that%2520a%2520single%2520model%2520predicts%2520both%2520the%250Avariables%2520for%2520warm-starts%2520and%2520integrality%2520gaps%2520for%2520cut-generation%2520across%250Amultiple%2520problem%2520type%2520distributions.%2520Both%2520predictions%2520help%2520improve%2520performance%250Aof%2520a%2520state-of-the-art%252C%2520commercial%2520optimization%2520solver.%2520Finally%252C%2520we%2520release%2520our%250Acode%2520and%2520pre-trained%2520Forge%2520weights%2520to%2520encourage%2520further%2520research%2520and%2520practical%250Ause%2520of%2520instance-level%2520MIP%2520embeddings%2520at%2520https%253A//github.com/skadio/forge/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORGE%3A%20Foundational%20Optimization%20Representations%20from%20Graph%20Embeddings&entry.906535625=Zohair%20Shafi%20and%20Serdar%20Kadioglu&entry.1292438233=%20%20Combinatorial%20optimization%20problems%20are%20ubiquitous%20in%20science%20and%0Aengineering%2C%20yet%20learning-based%20approaches%20to%20accelerate%20their%20solution%20often%0Arequire%20solving%20a%20large%20number%20of%20hard-to-solve%20optimization%20instances%20to%0Acollect%20training%20data%2C%20incurring%20significant%20computational%20overhead.%20Existing%0Amethods%20require%20training%20dedicated%20models%20for%20each%20problem%20distribution%20for%0Aeach%20downstream%20task%2C%20severely%20limiting%20their%20scalability%20and%20generalization.%0AIn%20this%20work%2C%20we%20introduce%20Forge%2C%20a%20method%20of%20pre-training%20a%20vector-quantized%0Agraph%20autoencoder%20on%20a%20large%20and%20diverse%20collection%20of%20mixed-integer%0Aprogramming%20%28MIP%29%20instances%20in%20an%20unsupervised%20fashion%20without%20dependency%20on%0Atheir%20solution.%20The%20vector%20quantization%20process%20creates%20discrete%20code%0Aassignments%20that%20act%20as%20a%20vocabulary%20to%20represent%20optimization%20instances.%20We%0Aevaluate%20our%20approach%20under%20both%20supervised%20and%20unsupervised%20settings.%20For%20the%0Aunsupervised%20setting%2C%20we%20demonstrate%20that%20Forge%20embeddings%20effectively%0Adifferentiate%20and%20cluster%20unseen%20instances.%20For%20the%20supervised%20setting%2C%20we%0Afine-tuneForge%20embeddings%20and%20show%20that%20a%20single%20model%20predicts%20both%20the%0Avariables%20for%20warm-starts%20and%20integrality%20gaps%20for%20cut-generation%20across%0Amultiple%20problem%20type%20distributions.%20Both%20predictions%20help%20improve%20performance%0Aof%20a%20state-of-the-art%2C%20commercial%20optimization%20solver.%20Finally%2C%20we%20release%20our%0Acode%20and%20pre-trained%20Forge%20weights%20to%20encourage%20further%20research%20and%20practical%0Ause%20of%20instance-level%20MIP%20embeddings%20at%20https%3A//github.com/skadio/forge/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20330v2&entry.124074799=Read"},
{"title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards", "author": "Xiaolong Wei and Bo Lu and Xingyu Zhang and Zhejun Zhao and Dongdong Shen and Long Xia and Dawei Yin", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.\n", "link": "http://arxiv.org/abs/2508.21476v1", "date": "2025-08-29", "relevancy": 1.9207, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Igniting%20Creative%20Writing%20in%20Small%20Language%20Models%3A%20LLM-as-a-Judge%0A%20%20versus%20Multi-Agent%20Refined%20Rewards&body=Title%3A%20Igniting%20Creative%20Writing%20in%20Small%20Language%20Models%3A%20LLM-as-a-Judge%0A%20%20versus%20Multi-Agent%20Refined%20Rewards%0AAuthor%3A%20Xiaolong%20Wei%20and%20Bo%20Lu%20and%20Xingyu%20Zhang%20and%20Zhejun%20Zhao%20and%20Dongdong%20Shen%20and%20Long%20Xia%20and%20Dawei%20Yin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20creative%20writing%0Acapabilities%2C%20yet%20their%20substantial%20computational%20demands%20hinder%20widespread%0Ause.%20Enhancing%20Small%20Language%20Models%20%28SLMs%29%20offers%20a%20promising%20alternative%2C%20but%0Acurrent%20methods%20like%20Supervised%20Fine-Tuning%20%28SFT%29%20struggle%20with%20novelty%2C%20and%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20costly.%20This%20paper%0Aexplores%20two%20distinct%20AI-driven%20reward%20strategies%20within%20a%20Reinforcement%0ALearning%20from%20AI%20Feedback%20%28RLAIF%29%20framework%20to%20ignite%20the%20creative%20writing%20of%20a%0A7B-parameter%20SLM%2C%20specifically%20for%20generating%20Chinese%20greetings.%20The%20first%0Astrategy%20employs%20a%20RM%20trained%20on%20high-quality%20preference%20data%20curated%20by%20a%0Anovel%20multi-agent%20rejection%20sampling%20framework%20designed%20for%20creative%20tasks.%20The%0Asecond%2C%20more%20novel%20strategy%20utilizes%20a%20principle-guided%20LLM-as-a-Judge%2C%20whose%0Areward%20function%20is%20optimized%20via%20an%20adversarial%20training%20scheme%20with%20a%0Areflection%20mechanism%2C%20to%20directly%20provide%20reward%20signals.%20Comprehensive%0Aexperiments%20reveal%20that%20while%20both%20approaches%20significantly%20enhance%20creative%0Aoutput%20over%20baselines%2C%20the%20principle-guided%20LLM-as-a-Judge%20demonstrably%20yields%0Asuperior%20generation%20quality.%20Furthermore%2C%20it%20offers%20notable%20advantages%20in%0Atraining%20efficiency%20and%20reduced%20dependency%20on%20human-annotated%20data%2C%20presenting%0Aa%20more%20scalable%20and%20effective%20path%20towards%20creative%20SLMs.%20Our%20automated%0Aevaluation%20methods%20also%20exhibit%20strong%20alignment%20with%20human%20judgments.%20Our%20code%0Aand%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIgniting%2520Creative%2520Writing%2520in%2520Small%2520Language%2520Models%253A%2520LLM-as-a-Judge%250A%2520%2520versus%2520Multi-Agent%2520Refined%2520Rewards%26entry.906535625%3DXiaolong%2520Wei%2520and%2520Bo%2520Lu%2520and%2520Xingyu%2520Zhang%2520and%2520Zhejun%2520Zhao%2520and%2520Dongdong%2520Shen%2520and%2520Long%2520Xia%2520and%2520Dawei%2520Yin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520creative%2520writing%250Acapabilities%252C%2520yet%2520their%2520substantial%2520computational%2520demands%2520hinder%2520widespread%250Ause.%2520Enhancing%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520offers%2520a%2520promising%2520alternative%252C%2520but%250Acurrent%2520methods%2520like%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520struggle%2520with%2520novelty%252C%2520and%250AReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520costly.%2520This%2520paper%250Aexplores%2520two%2520distinct%2520AI-driven%2520reward%2520strategies%2520within%2520a%2520Reinforcement%250ALearning%2520from%2520AI%2520Feedback%2520%2528RLAIF%2529%2520framework%2520to%2520ignite%2520the%2520creative%2520writing%2520of%2520a%250A7B-parameter%2520SLM%252C%2520specifically%2520for%2520generating%2520Chinese%2520greetings.%2520The%2520first%250Astrategy%2520employs%2520a%2520RM%2520trained%2520on%2520high-quality%2520preference%2520data%2520curated%2520by%2520a%250Anovel%2520multi-agent%2520rejection%2520sampling%2520framework%2520designed%2520for%2520creative%2520tasks.%2520The%250Asecond%252C%2520more%2520novel%2520strategy%2520utilizes%2520a%2520principle-guided%2520LLM-as-a-Judge%252C%2520whose%250Areward%2520function%2520is%2520optimized%2520via%2520an%2520adversarial%2520training%2520scheme%2520with%2520a%250Areflection%2520mechanism%252C%2520to%2520directly%2520provide%2520reward%2520signals.%2520Comprehensive%250Aexperiments%2520reveal%2520that%2520while%2520both%2520approaches%2520significantly%2520enhance%2520creative%250Aoutput%2520over%2520baselines%252C%2520the%2520principle-guided%2520LLM-as-a-Judge%2520demonstrably%2520yields%250Asuperior%2520generation%2520quality.%2520Furthermore%252C%2520it%2520offers%2520notable%2520advantages%2520in%250Atraining%2520efficiency%2520and%2520reduced%2520dependency%2520on%2520human-annotated%2520data%252C%2520presenting%250Aa%2520more%2520scalable%2520and%2520effective%2520path%2520towards%2520creative%2520SLMs.%2520Our%2520automated%250Aevaluation%2520methods%2520also%2520exhibit%2520strong%2520alignment%2520with%2520human%2520judgments.%2520Our%2520code%250Aand%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Igniting%20Creative%20Writing%20in%20Small%20Language%20Models%3A%20LLM-as-a-Judge%0A%20%20versus%20Multi-Agent%20Refined%20Rewards&entry.906535625=Xiaolong%20Wei%20and%20Bo%20Lu%20and%20Xingyu%20Zhang%20and%20Zhejun%20Zhao%20and%20Dongdong%20Shen%20and%20Long%20Xia%20and%20Dawei%20Yin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20creative%20writing%0Acapabilities%2C%20yet%20their%20substantial%20computational%20demands%20hinder%20widespread%0Ause.%20Enhancing%20Small%20Language%20Models%20%28SLMs%29%20offers%20a%20promising%20alternative%2C%20but%0Acurrent%20methods%20like%20Supervised%20Fine-Tuning%20%28SFT%29%20struggle%20with%20novelty%2C%20and%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20costly.%20This%20paper%0Aexplores%20two%20distinct%20AI-driven%20reward%20strategies%20within%20a%20Reinforcement%0ALearning%20from%20AI%20Feedback%20%28RLAIF%29%20framework%20to%20ignite%20the%20creative%20writing%20of%20a%0A7B-parameter%20SLM%2C%20specifically%20for%20generating%20Chinese%20greetings.%20The%20first%0Astrategy%20employs%20a%20RM%20trained%20on%20high-quality%20preference%20data%20curated%20by%20a%0Anovel%20multi-agent%20rejection%20sampling%20framework%20designed%20for%20creative%20tasks.%20The%0Asecond%2C%20more%20novel%20strategy%20utilizes%20a%20principle-guided%20LLM-as-a-Judge%2C%20whose%0Areward%20function%20is%20optimized%20via%20an%20adversarial%20training%20scheme%20with%20a%0Areflection%20mechanism%2C%20to%20directly%20provide%20reward%20signals.%20Comprehensive%0Aexperiments%20reveal%20that%20while%20both%20approaches%20significantly%20enhance%20creative%0Aoutput%20over%20baselines%2C%20the%20principle-guided%20LLM-as-a-Judge%20demonstrably%20yields%0Asuperior%20generation%20quality.%20Furthermore%2C%20it%20offers%20notable%20advantages%20in%0Atraining%20efficiency%20and%20reduced%20dependency%20on%20human-annotated%20data%2C%20presenting%0Aa%20more%20scalable%20and%20effective%20path%20towards%20creative%20SLMs.%20Our%20automated%0Aevaluation%20methods%20also%20exhibit%20strong%20alignment%20with%20human%20judgments.%20Our%20code%0Aand%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21476v1&entry.124074799=Read"},
{"title": "WebInject: Prompt Injection Attack to Web Agents", "author": "Xilong Wang and John Bloch and Zedian Shao and Yuepeng Hu and Shuyan Zhou and Neil Zhenqiang Gong", "abstract": "  Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. In this work, we propose WebInject, a prompt injection attack that\nmanipulates the webpage environment to induce a web agent to perform an\nattacker-specified action. Our attack adds a perturbation to the raw pixel\nvalues of the rendered webpage. After these perturbed pixels are mapped into a\nscreenshot, the perturbation induces the web agent to perform the\nattacker-specified action. We formulate the task of finding the perturbation as\nan optimization problem. A key challenge in solving this problem is that the\nmapping between raw pixel values and screenshot is non-differentiable, making\nit difficult to backpropagate gradients to the perturbation. To overcome this,\nwe train a neural network to approximate the mapping and apply projected\ngradient descent to solve the reformulated optimization problem. Extensive\nevaluation on multiple datasets shows that WebInject is highly effective and\nsignificantly outperforms baselines.\n", "link": "http://arxiv.org/abs/2505.11717v3", "date": "2025-08-29", "relevancy": 1.9142, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4865}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebInject%3A%20Prompt%20Injection%20Attack%20to%20Web%20Agents&body=Title%3A%20WebInject%3A%20Prompt%20Injection%20Attack%20to%20Web%20Agents%0AAuthor%3A%20Xilong%20Wang%20and%20John%20Bloch%20and%20Zedian%20Shao%20and%20Yuepeng%20Hu%20and%20Shuyan%20Zhou%20and%20Neil%20Zhenqiang%20Gong%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20model%20%28MLLM%29-based%20web%20agents%20interact%20with%0Awebpage%20environments%20by%20generating%20actions%20based%20on%20screenshots%20of%20the%0Awebpages.%20In%20this%20work%2C%20we%20propose%20WebInject%2C%20a%20prompt%20injection%20attack%20that%0Amanipulates%20the%20webpage%20environment%20to%20induce%20a%20web%20agent%20to%20perform%20an%0Aattacker-specified%20action.%20Our%20attack%20adds%20a%20perturbation%20to%20the%20raw%20pixel%0Avalues%20of%20the%20rendered%20webpage.%20After%20these%20perturbed%20pixels%20are%20mapped%20into%20a%0Ascreenshot%2C%20the%20perturbation%20induces%20the%20web%20agent%20to%20perform%20the%0Aattacker-specified%20action.%20We%20formulate%20the%20task%20of%20finding%20the%20perturbation%20as%0Aan%20optimization%20problem.%20A%20key%20challenge%20in%20solving%20this%20problem%20is%20that%20the%0Amapping%20between%20raw%20pixel%20values%20and%20screenshot%20is%20non-differentiable%2C%20making%0Ait%20difficult%20to%20backpropagate%20gradients%20to%20the%20perturbation.%20To%20overcome%20this%2C%0Awe%20train%20a%20neural%20network%20to%20approximate%20the%20mapping%20and%20apply%20projected%0Agradient%20descent%20to%20solve%20the%20reformulated%20optimization%20problem.%20Extensive%0Aevaluation%20on%20multiple%20datasets%20shows%20that%20WebInject%20is%20highly%20effective%20and%0Asignificantly%20outperforms%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11717v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebInject%253A%2520Prompt%2520Injection%2520Attack%2520to%2520Web%2520Agents%26entry.906535625%3DXilong%2520Wang%2520and%2520John%2520Bloch%2520and%2520Zedian%2520Shao%2520and%2520Yuepeng%2520Hu%2520and%2520Shuyan%2520Zhou%2520and%2520Neil%2520Zhenqiang%2520Gong%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520model%2520%2528MLLM%2529-based%2520web%2520agents%2520interact%2520with%250Awebpage%2520environments%2520by%2520generating%2520actions%2520based%2520on%2520screenshots%2520of%2520the%250Awebpages.%2520In%2520this%2520work%252C%2520we%2520propose%2520WebInject%252C%2520a%2520prompt%2520injection%2520attack%2520that%250Amanipulates%2520the%2520webpage%2520environment%2520to%2520induce%2520a%2520web%2520agent%2520to%2520perform%2520an%250Aattacker-specified%2520action.%2520Our%2520attack%2520adds%2520a%2520perturbation%2520to%2520the%2520raw%2520pixel%250Avalues%2520of%2520the%2520rendered%2520webpage.%2520After%2520these%2520perturbed%2520pixels%2520are%2520mapped%2520into%2520a%250Ascreenshot%252C%2520the%2520perturbation%2520induces%2520the%2520web%2520agent%2520to%2520perform%2520the%250Aattacker-specified%2520action.%2520We%2520formulate%2520the%2520task%2520of%2520finding%2520the%2520perturbation%2520as%250Aan%2520optimization%2520problem.%2520A%2520key%2520challenge%2520in%2520solving%2520this%2520problem%2520is%2520that%2520the%250Amapping%2520between%2520raw%2520pixel%2520values%2520and%2520screenshot%2520is%2520non-differentiable%252C%2520making%250Ait%2520difficult%2520to%2520backpropagate%2520gradients%2520to%2520the%2520perturbation.%2520To%2520overcome%2520this%252C%250Awe%2520train%2520a%2520neural%2520network%2520to%2520approximate%2520the%2520mapping%2520and%2520apply%2520projected%250Agradient%2520descent%2520to%2520solve%2520the%2520reformulated%2520optimization%2520problem.%2520Extensive%250Aevaluation%2520on%2520multiple%2520datasets%2520shows%2520that%2520WebInject%2520is%2520highly%2520effective%2520and%250Asignificantly%2520outperforms%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11717v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebInject%3A%20Prompt%20Injection%20Attack%20to%20Web%20Agents&entry.906535625=Xilong%20Wang%20and%20John%20Bloch%20and%20Zedian%20Shao%20and%20Yuepeng%20Hu%20and%20Shuyan%20Zhou%20and%20Neil%20Zhenqiang%20Gong&entry.1292438233=%20%20Multi-modal%20large%20language%20model%20%28MLLM%29-based%20web%20agents%20interact%20with%0Awebpage%20environments%20by%20generating%20actions%20based%20on%20screenshots%20of%20the%0Awebpages.%20In%20this%20work%2C%20we%20propose%20WebInject%2C%20a%20prompt%20injection%20attack%20that%0Amanipulates%20the%20webpage%20environment%20to%20induce%20a%20web%20agent%20to%20perform%20an%0Aattacker-specified%20action.%20Our%20attack%20adds%20a%20perturbation%20to%20the%20raw%20pixel%0Avalues%20of%20the%20rendered%20webpage.%20After%20these%20perturbed%20pixels%20are%20mapped%20into%20a%0Ascreenshot%2C%20the%20perturbation%20induces%20the%20web%20agent%20to%20perform%20the%0Aattacker-specified%20action.%20We%20formulate%20the%20task%20of%20finding%20the%20perturbation%20as%0Aan%20optimization%20problem.%20A%20key%20challenge%20in%20solving%20this%20problem%20is%20that%20the%0Amapping%20between%20raw%20pixel%20values%20and%20screenshot%20is%20non-differentiable%2C%20making%0Ait%20difficult%20to%20backpropagate%20gradients%20to%20the%20perturbation.%20To%20overcome%20this%2C%0Awe%20train%20a%20neural%20network%20to%20approximate%20the%20mapping%20and%20apply%20projected%0Agradient%20descent%20to%20solve%20the%20reformulated%20optimization%20problem.%20Extensive%0Aevaluation%20on%20multiple%20datasets%20shows%20that%20WebInject%20is%20highly%20effective%20and%0Asignificantly%20outperforms%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11717v3&entry.124074799=Read"},
{"title": "Robustness is Important: Limitations of LLMs for Data Fitting", "author": "Hejia Liu and Mochen Yang and Gediminas Adomavicius", "abstract": "  Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.\n", "link": "http://arxiv.org/abs/2508.19563v2", "date": "2025-08-29", "relevancy": 1.9106, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20is%20Important%3A%20Limitations%20of%20LLMs%20for%20Data%20Fitting&body=Title%3A%20Robustness%20is%20Important%3A%20Limitations%20of%20LLMs%20for%20Data%20Fitting%0AAuthor%3A%20Hejia%20Liu%20and%20Mochen%20Yang%20and%20Gediminas%20Adomavicius%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20applied%20in%20a%20wide%20array%20of%20settings%2C%0Awell%20beyond%20the%20typical%20language-oriented%20use%20cases.%20In%20particular%2C%20LLMs%20are%0Aincreasingly%20used%20as%20a%20plug-and-play%20method%20for%20fitting%20data%20and%20generating%0Apredictions.%20Prior%20work%20has%20shown%20that%20LLMs%2C%20via%20in-context%20learning%20or%0Asupervised%20fine-tuning%2C%20can%20perform%20competitively%20with%20many%20tabular%20supervised%0Alearning%20techniques%20in%20terms%20of%20predictive%20performance.%20However%2C%20we%20identify%20a%0Acritical%20vulnerability%20of%20using%20LLMs%20for%20data%20fitting%20--%20making%20changes%20to%20data%0Arepresentation%20that%20are%20completely%20irrelevant%20to%20the%20underlying%20learning%20task%0Acan%20drastically%20alter%20LLMs%27%20predictions%20on%20the%20same%20data.%20For%20example%2C%20simply%0Achanging%20variable%20names%20can%20sway%20the%20size%20of%20prediction%20error%20by%20as%20much%20as%2082%25%0Ain%20certain%20settings.%20Such%20prediction%20sensitivity%20with%20respect%20to%0Atask-irrelevant%20variations%20manifests%20under%20both%20in-context%20learning%20and%0Asupervised%20fine-tuning%2C%20for%20both%20close-weight%20and%20open-weight%20general-purpose%0ALLMs.%20Moreover%2C%20by%20examining%20the%20attention%20scores%20of%20an%20open-weight%20LLM%2C%20we%0Adiscover%20a%20non-uniform%20attention%20pattern%3A%20training%20examples%20and%20variable%0Anames/values%20which%20happen%20to%20occupy%20certain%20positions%20in%20the%20prompt%20receive%0Amore%20attention%20when%20output%20tokens%20are%20generated%2C%20even%20though%20different%0Apositions%20are%20expected%20to%20receive%20roughly%20the%20same%20attention.%20This%20partially%0Aexplains%20the%20sensitivity%20in%20the%20presence%20of%20task-irrelevant%20variations.%20We%20also%0Aconsider%20a%20state-of-the-art%20tabular%20foundation%20model%20%28TabPFN%29%20trained%0Aspecifically%20for%20data%20fitting.%20Despite%20being%20explicitly%20designed%20to%20achieve%0Aprediction%20robustness%2C%20TabPFN%20is%20still%20not%20immune%20to%20task-irrelevant%0Avariations.%20Overall%2C%20despite%20LLMs%27%20impressive%20predictive%20capabilities%2C%0Acurrently%20they%20lack%20even%20the%20basic%20level%20of%20robustness%20to%20be%20used%20as%20a%0Aprincipled%20data-fitting%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520is%2520Important%253A%2520Limitations%2520of%2520LLMs%2520for%2520Data%2520Fitting%26entry.906535625%3DHejia%2520Liu%2520and%2520Mochen%2520Yang%2520and%2520Gediminas%2520Adomavicius%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520applied%2520in%2520a%2520wide%2520array%2520of%2520settings%252C%250Awell%2520beyond%2520the%2520typical%2520language-oriented%2520use%2520cases.%2520In%2520particular%252C%2520LLMs%2520are%250Aincreasingly%2520used%2520as%2520a%2520plug-and-play%2520method%2520for%2520fitting%2520data%2520and%2520generating%250Apredictions.%2520Prior%2520work%2520has%2520shown%2520that%2520LLMs%252C%2520via%2520in-context%2520learning%2520or%250Asupervised%2520fine-tuning%252C%2520can%2520perform%2520competitively%2520with%2520many%2520tabular%2520supervised%250Alearning%2520techniques%2520in%2520terms%2520of%2520predictive%2520performance.%2520However%252C%2520we%2520identify%2520a%250Acritical%2520vulnerability%2520of%2520using%2520LLMs%2520for%2520data%2520fitting%2520--%2520making%2520changes%2520to%2520data%250Arepresentation%2520that%2520are%2520completely%2520irrelevant%2520to%2520the%2520underlying%2520learning%2520task%250Acan%2520drastically%2520alter%2520LLMs%2527%2520predictions%2520on%2520the%2520same%2520data.%2520For%2520example%252C%2520simply%250Achanging%2520variable%2520names%2520can%2520sway%2520the%2520size%2520of%2520prediction%2520error%2520by%2520as%2520much%2520as%252082%2525%250Ain%2520certain%2520settings.%2520Such%2520prediction%2520sensitivity%2520with%2520respect%2520to%250Atask-irrelevant%2520variations%2520manifests%2520under%2520both%2520in-context%2520learning%2520and%250Asupervised%2520fine-tuning%252C%2520for%2520both%2520close-weight%2520and%2520open-weight%2520general-purpose%250ALLMs.%2520Moreover%252C%2520by%2520examining%2520the%2520attention%2520scores%2520of%2520an%2520open-weight%2520LLM%252C%2520we%250Adiscover%2520a%2520non-uniform%2520attention%2520pattern%253A%2520training%2520examples%2520and%2520variable%250Anames/values%2520which%2520happen%2520to%2520occupy%2520certain%2520positions%2520in%2520the%2520prompt%2520receive%250Amore%2520attention%2520when%2520output%2520tokens%2520are%2520generated%252C%2520even%2520though%2520different%250Apositions%2520are%2520expected%2520to%2520receive%2520roughly%2520the%2520same%2520attention.%2520This%2520partially%250Aexplains%2520the%2520sensitivity%2520in%2520the%2520presence%2520of%2520task-irrelevant%2520variations.%2520We%2520also%250Aconsider%2520a%2520state-of-the-art%2520tabular%2520foundation%2520model%2520%2528TabPFN%2529%2520trained%250Aspecifically%2520for%2520data%2520fitting.%2520Despite%2520being%2520explicitly%2520designed%2520to%2520achieve%250Aprediction%2520robustness%252C%2520TabPFN%2520is%2520still%2520not%2520immune%2520to%2520task-irrelevant%250Avariations.%2520Overall%252C%2520despite%2520LLMs%2527%2520impressive%2520predictive%2520capabilities%252C%250Acurrently%2520they%2520lack%2520even%2520the%2520basic%2520level%2520of%2520robustness%2520to%2520be%2520used%2520as%2520a%250Aprincipled%2520data-fitting%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20is%20Important%3A%20Limitations%20of%20LLMs%20for%20Data%20Fitting&entry.906535625=Hejia%20Liu%20and%20Mochen%20Yang%20and%20Gediminas%20Adomavicius&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20applied%20in%20a%20wide%20array%20of%20settings%2C%0Awell%20beyond%20the%20typical%20language-oriented%20use%20cases.%20In%20particular%2C%20LLMs%20are%0Aincreasingly%20used%20as%20a%20plug-and-play%20method%20for%20fitting%20data%20and%20generating%0Apredictions.%20Prior%20work%20has%20shown%20that%20LLMs%2C%20via%20in-context%20learning%20or%0Asupervised%20fine-tuning%2C%20can%20perform%20competitively%20with%20many%20tabular%20supervised%0Alearning%20techniques%20in%20terms%20of%20predictive%20performance.%20However%2C%20we%20identify%20a%0Acritical%20vulnerability%20of%20using%20LLMs%20for%20data%20fitting%20--%20making%20changes%20to%20data%0Arepresentation%20that%20are%20completely%20irrelevant%20to%20the%20underlying%20learning%20task%0Acan%20drastically%20alter%20LLMs%27%20predictions%20on%20the%20same%20data.%20For%20example%2C%20simply%0Achanging%20variable%20names%20can%20sway%20the%20size%20of%20prediction%20error%20by%20as%20much%20as%2082%25%0Ain%20certain%20settings.%20Such%20prediction%20sensitivity%20with%20respect%20to%0Atask-irrelevant%20variations%20manifests%20under%20both%20in-context%20learning%20and%0Asupervised%20fine-tuning%2C%20for%20both%20close-weight%20and%20open-weight%20general-purpose%0ALLMs.%20Moreover%2C%20by%20examining%20the%20attention%20scores%20of%20an%20open-weight%20LLM%2C%20we%0Adiscover%20a%20non-uniform%20attention%20pattern%3A%20training%20examples%20and%20variable%0Anames/values%20which%20happen%20to%20occupy%20certain%20positions%20in%20the%20prompt%20receive%0Amore%20attention%20when%20output%20tokens%20are%20generated%2C%20even%20though%20different%0Apositions%20are%20expected%20to%20receive%20roughly%20the%20same%20attention.%20This%20partially%0Aexplains%20the%20sensitivity%20in%20the%20presence%20of%20task-irrelevant%20variations.%20We%20also%0Aconsider%20a%20state-of-the-art%20tabular%20foundation%20model%20%28TabPFN%29%20trained%0Aspecifically%20for%20data%20fitting.%20Despite%20being%20explicitly%20designed%20to%20achieve%0Aprediction%20robustness%2C%20TabPFN%20is%20still%20not%20immune%20to%20task-irrelevant%0Avariations.%20Overall%2C%20despite%20LLMs%27%20impressive%20predictive%20capabilities%2C%0Acurrently%20they%20lack%20even%20the%20basic%20level%20of%20robustness%20to%20be%20used%20as%20a%0Aprincipled%20data-fitting%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19563v2&entry.124074799=Read"},
{"title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective", "author": "Weijie Xu and Yiwen Wang and Chi Xue and Xiangkun Hu and Xi Fang and Guimin Dong and Chandan K. Reddy", "abstract": "  Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose FiSCo\n(Fine-grained Semantic Comparison), a novel statistical framework to evaluate\ngroup-level fairness in LLMs by detecting subtle semantic differences in\nlong-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSCo more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.\n", "link": "http://arxiv.org/abs/2506.19028v4", "date": "2025-08-29", "relevancy": 1.9064, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Fairness%20in%20LLMs%20Beyond%20Tokens%3A%20A%20Semantic%20and%20Statistical%0A%20%20Perspective&body=Title%3A%20Quantifying%20Fairness%20in%20LLMs%20Beyond%20Tokens%3A%20A%20Semantic%20and%20Statistical%0A%20%20Perspective%0AAuthor%3A%20Weijie%20Xu%20and%20Yiwen%20Wang%20and%20Chi%20Xue%20and%20Xiangkun%20Hu%20and%20Xi%20Fang%20and%20Guimin%20Dong%20and%20Chandan%20K.%20Reddy%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20generate%20responses%20with%20inherent%20biases%2C%0Aundermining%20their%20reliability%20in%20real-world%20applications.%20Existing%20evaluation%0Amethods%20often%20overlook%20biases%20in%20long-form%20responses%20and%20the%20intrinsic%0Avariability%20of%20LLM%20outputs.%20To%20address%20these%20challenges%2C%20we%20propose%20FiSCo%0A%28Fine-grained%20Semantic%20Comparison%29%2C%20a%20novel%20statistical%20framework%20to%20evaluate%0Agroup-level%20fairness%20in%20LLMs%20by%20detecting%20subtle%20semantic%20differences%20in%0Along-form%20responses%20across%20demographic%20groups.%20Unlike%20prior%20work%20focusing%20on%0Asentiment%20or%20token-level%20comparisons%2C%20FiSCo%20goes%20beyond%20surface-level%20analysis%0Aby%20operating%20at%20the%20claim%20level%2C%20leveraging%20entailment%20checks%20to%20assess%20the%0Aconsistency%20of%20meaning%20across%20responses.%20We%20decompose%20model%20outputs%20into%0Asemantically%20distinct%20claims%20and%20apply%20statistical%20hypothesis%20testing%20to%0Acompare%20inter-%20and%20intra-group%20similarities%2C%20enabling%20robust%20detection%20of%0Asubtle%20biases.%20We%20formalize%20a%20new%20group%20counterfactual%20fairness%20definition%20and%0Avalidate%20FiSCo%20on%20both%20synthetic%20and%20human-annotated%20datasets%20spanning%20gender%2C%0Arace%2C%20and%20age.%20Experiments%20show%20that%20FiSCo%20more%20reliably%20identifies%20nuanced%0Abiases%20while%20reducing%20the%20impact%20of%20stochastic%20LLM%20variability%2C%20outperforming%0Avarious%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19028v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Fairness%2520in%2520LLMs%2520Beyond%2520Tokens%253A%2520A%2520Semantic%2520and%2520Statistical%250A%2520%2520Perspective%26entry.906535625%3DWeijie%2520Xu%2520and%2520Yiwen%2520Wang%2520and%2520Chi%2520Xue%2520and%2520Xiangkun%2520Hu%2520and%2520Xi%2520Fang%2520and%2520Guimin%2520Dong%2520and%2520Chandan%2520K.%2520Reddy%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520generate%2520responses%2520with%2520inherent%2520biases%252C%250Aundermining%2520their%2520reliability%2520in%2520real-world%2520applications.%2520Existing%2520evaluation%250Amethods%2520often%2520overlook%2520biases%2520in%2520long-form%2520responses%2520and%2520the%2520intrinsic%250Avariability%2520of%2520LLM%2520outputs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FiSCo%250A%2528Fine-grained%2520Semantic%2520Comparison%2529%252C%2520a%2520novel%2520statistical%2520framework%2520to%2520evaluate%250Agroup-level%2520fairness%2520in%2520LLMs%2520by%2520detecting%2520subtle%2520semantic%2520differences%2520in%250Along-form%2520responses%2520across%2520demographic%2520groups.%2520Unlike%2520prior%2520work%2520focusing%2520on%250Asentiment%2520or%2520token-level%2520comparisons%252C%2520FiSCo%2520goes%2520beyond%2520surface-level%2520analysis%250Aby%2520operating%2520at%2520the%2520claim%2520level%252C%2520leveraging%2520entailment%2520checks%2520to%2520assess%2520the%250Aconsistency%2520of%2520meaning%2520across%2520responses.%2520We%2520decompose%2520model%2520outputs%2520into%250Asemantically%2520distinct%2520claims%2520and%2520apply%2520statistical%2520hypothesis%2520testing%2520to%250Acompare%2520inter-%2520and%2520intra-group%2520similarities%252C%2520enabling%2520robust%2520detection%2520of%250Asubtle%2520biases.%2520We%2520formalize%2520a%2520new%2520group%2520counterfactual%2520fairness%2520definition%2520and%250Avalidate%2520FiSCo%2520on%2520both%2520synthetic%2520and%2520human-annotated%2520datasets%2520spanning%2520gender%252C%250Arace%252C%2520and%2520age.%2520Experiments%2520show%2520that%2520FiSCo%2520more%2520reliably%2520identifies%2520nuanced%250Abiases%2520while%2520reducing%2520the%2520impact%2520of%2520stochastic%2520LLM%2520variability%252C%2520outperforming%250Avarious%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19028v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Fairness%20in%20LLMs%20Beyond%20Tokens%3A%20A%20Semantic%20and%20Statistical%0A%20%20Perspective&entry.906535625=Weijie%20Xu%20and%20Yiwen%20Wang%20and%20Chi%20Xue%20and%20Xiangkun%20Hu%20and%20Xi%20Fang%20and%20Guimin%20Dong%20and%20Chandan%20K.%20Reddy&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20generate%20responses%20with%20inherent%20biases%2C%0Aundermining%20their%20reliability%20in%20real-world%20applications.%20Existing%20evaluation%0Amethods%20often%20overlook%20biases%20in%20long-form%20responses%20and%20the%20intrinsic%0Avariability%20of%20LLM%20outputs.%20To%20address%20these%20challenges%2C%20we%20propose%20FiSCo%0A%28Fine-grained%20Semantic%20Comparison%29%2C%20a%20novel%20statistical%20framework%20to%20evaluate%0Agroup-level%20fairness%20in%20LLMs%20by%20detecting%20subtle%20semantic%20differences%20in%0Along-form%20responses%20across%20demographic%20groups.%20Unlike%20prior%20work%20focusing%20on%0Asentiment%20or%20token-level%20comparisons%2C%20FiSCo%20goes%20beyond%20surface-level%20analysis%0Aby%20operating%20at%20the%20claim%20level%2C%20leveraging%20entailment%20checks%20to%20assess%20the%0Aconsistency%20of%20meaning%20across%20responses.%20We%20decompose%20model%20outputs%20into%0Asemantically%20distinct%20claims%20and%20apply%20statistical%20hypothesis%20testing%20to%0Acompare%20inter-%20and%20intra-group%20similarities%2C%20enabling%20robust%20detection%20of%0Asubtle%20biases.%20We%20formalize%20a%20new%20group%20counterfactual%20fairness%20definition%20and%0Avalidate%20FiSCo%20on%20both%20synthetic%20and%20human-annotated%20datasets%20spanning%20gender%2C%0Arace%2C%20and%20age.%20Experiments%20show%20that%20FiSCo%20more%20reliably%20identifies%20nuanced%0Abiases%20while%20reducing%20the%20impact%20of%20stochastic%20LLM%20variability%2C%20outperforming%0Avarious%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19028v4&entry.124074799=Read"},
{"title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics", "author": "Weida Wang and Dongchen Huang and Jiatong Li and Tengchao Yang and Ziyang Zheng and Di Zhang and Dong Han and Benteng Chen and Binzhao Luo and Zhiyu Liu and Kunling Liu and Zhiyuan Gao and Shiqi Geng and Wei Ma and Jiaming Su and Xin Li and Shuchen Pu and Yuhan Shui and Qianjia Cheng and Zhihao Dou and Dongfei Cui and Changyong He and Jin Zeng and Zeke Xie and Mao Su and Dongzhan Zhou and Yuqiang Li and Wanli Ouyang and Yunqi Cai and Xi Dai and Shufei Zhang and Lei Bai and Jinguang Cheng and Zhong Fang and Hongming Weng", "abstract": "  We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.\n", "link": "http://arxiv.org/abs/2508.18124v3", "date": "2025-08-29", "relevancy": 1.9064, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMPhysBench%3A%20A%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Condensed%20Matter%20Physics&body=Title%3A%20CMPhysBench%3A%20A%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Condensed%20Matter%20Physics%0AAuthor%3A%20Weida%20Wang%20and%20Dongchen%20Huang%20and%20Jiatong%20Li%20and%20Tengchao%20Yang%20and%20Ziyang%20Zheng%20and%20Di%20Zhang%20and%20Dong%20Han%20and%20Benteng%20Chen%20and%20Binzhao%20Luo%20and%20Zhiyu%20Liu%20and%20Kunling%20Liu%20and%20Zhiyuan%20Gao%20and%20Shiqi%20Geng%20and%20Wei%20Ma%20and%20Jiaming%20Su%20and%20Xin%20Li%20and%20Shuchen%20Pu%20and%20Yuhan%20Shui%20and%20Qianjia%20Cheng%20and%20Zhihao%20Dou%20and%20Dongfei%20Cui%20and%20Changyong%20He%20and%20Jin%20Zeng%20and%20Zeke%20Xie%20and%20Mao%20Su%20and%20Dongzhan%20Zhou%20and%20Yuqiang%20Li%20and%20Wanli%20Ouyang%20and%20Yunqi%20Cai%20and%20Xi%20Dai%20and%20Shufei%20Zhang%20and%20Lei%20Bai%20and%20Jinguang%20Cheng%20and%20Zhong%20Fang%20and%20Hongming%20Weng%0AAbstract%3A%20%20%20We%20introduce%20CMPhysBench%2C%20designed%20to%20assess%20the%20proficiency%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20in%20Condensed%20Matter%20Physics%2C%20as%20a%20novel%20Benchmark.%0ACMPhysBench%20is%20composed%20of%20more%20than%20520%20graduate-level%20meticulously%20curated%0Aquestions%20covering%20both%20representative%20subfields%20and%20foundational%20theoretical%0Aframeworks%20of%20condensed%20matter%20physics%2C%20such%20as%20magnetism%2C%20superconductivity%2C%0Astrongly%20correlated%20systems%2C%20etc.%20To%20ensure%20a%20deep%20understanding%20of%20the%0Aproblem-solving%20process%2Cwe%20focus%20exclusively%20on%20calculation%20problems%2C%20requiring%0ALLMs%20to%20independently%20generate%20comprehensive%20solutions.%20Meanwhile%2C%20leveraging%0Atree-based%20representations%20of%20expressions%2C%20we%20introduce%20the%20Scalable%20Expression%0AEdit%20Distance%20%28SEED%29%20score%2C%20which%20provides%20fine-grained%20%28non-binary%29%20partial%0Acredit%20and%20yields%20a%20more%20accurate%20assessment%20of%20similarity%20between%20prediction%0Aand%20ground-truth.%20Our%20results%20show%20that%20even%20the%20best%20models%2C%20Grok-4%2C%20reach%0Aonly%2036%20average%20SEED%20score%20and%2028%25%20accuracy%20on%20CMPhysBench%2C%20underscoring%20a%0Asignificant%20capability%20gap%2C%20especially%20for%20this%20practical%20and%20frontier%20domain%0Arelative%20to%20traditional%20physics.%20The%20code%20anddataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/CMPhysBench/CMPhysBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18124v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMPhysBench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Large%2520Language%2520Models%2520in%250A%2520%2520Condensed%2520Matter%2520Physics%26entry.906535625%3DWeida%2520Wang%2520and%2520Dongchen%2520Huang%2520and%2520Jiatong%2520Li%2520and%2520Tengchao%2520Yang%2520and%2520Ziyang%2520Zheng%2520and%2520Di%2520Zhang%2520and%2520Dong%2520Han%2520and%2520Benteng%2520Chen%2520and%2520Binzhao%2520Luo%2520and%2520Zhiyu%2520Liu%2520and%2520Kunling%2520Liu%2520and%2520Zhiyuan%2520Gao%2520and%2520Shiqi%2520Geng%2520and%2520Wei%2520Ma%2520and%2520Jiaming%2520Su%2520and%2520Xin%2520Li%2520and%2520Shuchen%2520Pu%2520and%2520Yuhan%2520Shui%2520and%2520Qianjia%2520Cheng%2520and%2520Zhihao%2520Dou%2520and%2520Dongfei%2520Cui%2520and%2520Changyong%2520He%2520and%2520Jin%2520Zeng%2520and%2520Zeke%2520Xie%2520and%2520Mao%2520Su%2520and%2520Dongzhan%2520Zhou%2520and%2520Yuqiang%2520Li%2520and%2520Wanli%2520Ouyang%2520and%2520Yunqi%2520Cai%2520and%2520Xi%2520Dai%2520and%2520Shufei%2520Zhang%2520and%2520Lei%2520Bai%2520and%2520Jinguang%2520Cheng%2520and%2520Zhong%2520Fang%2520and%2520Hongming%2520Weng%26entry.1292438233%3D%2520%2520We%2520introduce%2520CMPhysBench%252C%2520designed%2520to%2520assess%2520the%2520proficiency%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520in%2520Condensed%2520Matter%2520Physics%252C%2520as%2520a%2520novel%2520Benchmark.%250ACMPhysBench%2520is%2520composed%2520of%2520more%2520than%2520520%2520graduate-level%2520meticulously%2520curated%250Aquestions%2520covering%2520both%2520representative%2520subfields%2520and%2520foundational%2520theoretical%250Aframeworks%2520of%2520condensed%2520matter%2520physics%252C%2520such%2520as%2520magnetism%252C%2520superconductivity%252C%250Astrongly%2520correlated%2520systems%252C%2520etc.%2520To%2520ensure%2520a%2520deep%2520understanding%2520of%2520the%250Aproblem-solving%2520process%252Cwe%2520focus%2520exclusively%2520on%2520calculation%2520problems%252C%2520requiring%250ALLMs%2520to%2520independently%2520generate%2520comprehensive%2520solutions.%2520Meanwhile%252C%2520leveraging%250Atree-based%2520representations%2520of%2520expressions%252C%2520we%2520introduce%2520the%2520Scalable%2520Expression%250AEdit%2520Distance%2520%2528SEED%2529%2520score%252C%2520which%2520provides%2520fine-grained%2520%2528non-binary%2529%2520partial%250Acredit%2520and%2520yields%2520a%2520more%2520accurate%2520assessment%2520of%2520similarity%2520between%2520prediction%250Aand%2520ground-truth.%2520Our%2520results%2520show%2520that%2520even%2520the%2520best%2520models%252C%2520Grok-4%252C%2520reach%250Aonly%252036%2520average%2520SEED%2520score%2520and%252028%2525%2520accuracy%2520on%2520CMPhysBench%252C%2520underscoring%2520a%250Asignificant%2520capability%2520gap%252C%2520especially%2520for%2520this%2520practical%2520and%2520frontier%2520domain%250Arelative%2520to%2520traditional%2520physics.%2520The%2520code%2520anddataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/CMPhysBench/CMPhysBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18124v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMPhysBench%3A%20A%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Condensed%20Matter%20Physics&entry.906535625=Weida%20Wang%20and%20Dongchen%20Huang%20and%20Jiatong%20Li%20and%20Tengchao%20Yang%20and%20Ziyang%20Zheng%20and%20Di%20Zhang%20and%20Dong%20Han%20and%20Benteng%20Chen%20and%20Binzhao%20Luo%20and%20Zhiyu%20Liu%20and%20Kunling%20Liu%20and%20Zhiyuan%20Gao%20and%20Shiqi%20Geng%20and%20Wei%20Ma%20and%20Jiaming%20Su%20and%20Xin%20Li%20and%20Shuchen%20Pu%20and%20Yuhan%20Shui%20and%20Qianjia%20Cheng%20and%20Zhihao%20Dou%20and%20Dongfei%20Cui%20and%20Changyong%20He%20and%20Jin%20Zeng%20and%20Zeke%20Xie%20and%20Mao%20Su%20and%20Dongzhan%20Zhou%20and%20Yuqiang%20Li%20and%20Wanli%20Ouyang%20and%20Yunqi%20Cai%20and%20Xi%20Dai%20and%20Shufei%20Zhang%20and%20Lei%20Bai%20and%20Jinguang%20Cheng%20and%20Zhong%20Fang%20and%20Hongming%20Weng&entry.1292438233=%20%20We%20introduce%20CMPhysBench%2C%20designed%20to%20assess%20the%20proficiency%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20in%20Condensed%20Matter%20Physics%2C%20as%20a%20novel%20Benchmark.%0ACMPhysBench%20is%20composed%20of%20more%20than%20520%20graduate-level%20meticulously%20curated%0Aquestions%20covering%20both%20representative%20subfields%20and%20foundational%20theoretical%0Aframeworks%20of%20condensed%20matter%20physics%2C%20such%20as%20magnetism%2C%20superconductivity%2C%0Astrongly%20correlated%20systems%2C%20etc.%20To%20ensure%20a%20deep%20understanding%20of%20the%0Aproblem-solving%20process%2Cwe%20focus%20exclusively%20on%20calculation%20problems%2C%20requiring%0ALLMs%20to%20independently%20generate%20comprehensive%20solutions.%20Meanwhile%2C%20leveraging%0Atree-based%20representations%20of%20expressions%2C%20we%20introduce%20the%20Scalable%20Expression%0AEdit%20Distance%20%28SEED%29%20score%2C%20which%20provides%20fine-grained%20%28non-binary%29%20partial%0Acredit%20and%20yields%20a%20more%20accurate%20assessment%20of%20similarity%20between%20prediction%0Aand%20ground-truth.%20Our%20results%20show%20that%20even%20the%20best%20models%2C%20Grok-4%2C%20reach%0Aonly%2036%20average%20SEED%20score%20and%2028%25%20accuracy%20on%20CMPhysBench%2C%20underscoring%20a%0Asignificant%20capability%20gap%2C%20especially%20for%20this%20practical%20and%20frontier%20domain%0Arelative%20to%20traditional%20physics.%20The%20code%20anddataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/CMPhysBench/CMPhysBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18124v3&entry.124074799=Read"},
{"title": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models", "author": "Jessica Liang and Anirudh Bharadwaj", "abstract": "  The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups.\n", "link": "http://arxiv.org/abs/2508.21810v1", "date": "2025-08-29", "relevancy": 1.904, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4918}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4739}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QR-LoRA%3A%20QR-Based%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20QR-LoRA%3A%20QR-Based%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Jessica%20Liang%20and%20Anirudh%20Bharadwaj%0AAbstract%3A%20%20%20The%20growing%20scale%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20necessitated%20the%0Adevelopment%20of%20parameter-efficient%20fine-tuning%20techniques.%20Low-Rank%20Adaptation%0A%28LoRA%29%20has%20emerged%20as%20a%20promising%20approach%2C%20reducing%20the%20number%20of%20trainable%0Aparameters%20by%20applying%20low-rank%20updates%20to%20pretrained%20weights.%20While%20standard%0ALoRA%20learns%20both%20update%20factors%20directly%2C%20several%20recent%20variants%20first%0Ainitialize%20those%20matrices%20via%20an%20SVD%20of%20the%20pretrained%20weights%20--%20an%20operation%0Athat%20can%20be%20expensive%20on%20large%20models%20and%20yields%20singular%20vectors%20that%20are%20not%0Aalways%20easy%20to%20interpret.%20In%20this%20work%2C%20we%20extract%20an%20orthonormal%20basis%20from%0Athe%20pretrained%20weight%20matrix%20using%20QR%20decomposition%20with%20column%20pivoting%2C%20and%0Athen%20express%20the%20LoRA%20update%20as%20a%20linear%20combination%20of%20these%20basis%20vectors%20--%0Atraining%20only%20the%20scalar%20coefficients%2C%20which%20imposes%20clear%20structure%20on%0Aadaptation%20and%20drastically%20reduces%20parameter%20count.%20Experiments%20across%20GLUE%0Atasks%20show%20that%20QR-LoRA%20matches%20or%20exceeds%20the%20performance%20of%20full%20fine-tuning%2C%0Astandard%20LoRA%2C%20and%20SVD-LoRA%20%28LoRA%20with%20update%20matrices%20initialized%20via%20singular%0Avalue%20decomposition%29%20with%20as%20few%20as%20601%20parameters%20--%20a%20reduction%20of%20over%201000x%0Acompared%20to%20full%20fine-tuning%20and%2077x%20fewer%20than%20typical%20LoRA%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQR-LoRA%253A%2520QR-Based%2520Low-Rank%2520Adaptation%2520for%2520Efficient%2520Fine-Tuning%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DJessica%2520Liang%2520and%2520Anirudh%2520Bharadwaj%26entry.1292438233%3D%2520%2520The%2520growing%2520scale%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520necessitated%2520the%250Adevelopment%2520of%2520parameter-efficient%2520fine-tuning%2520techniques.%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%252C%2520reducing%2520the%2520number%2520of%2520trainable%250Aparameters%2520by%2520applying%2520low-rank%2520updates%2520to%2520pretrained%2520weights.%2520While%2520standard%250ALoRA%2520learns%2520both%2520update%2520factors%2520directly%252C%2520several%2520recent%2520variants%2520first%250Ainitialize%2520those%2520matrices%2520via%2520an%2520SVD%2520of%2520the%2520pretrained%2520weights%2520--%2520an%2520operation%250Athat%2520can%2520be%2520expensive%2520on%2520large%2520models%2520and%2520yields%2520singular%2520vectors%2520that%2520are%2520not%250Aalways%2520easy%2520to%2520interpret.%2520In%2520this%2520work%252C%2520we%2520extract%2520an%2520orthonormal%2520basis%2520from%250Athe%2520pretrained%2520weight%2520matrix%2520using%2520QR%2520decomposition%2520with%2520column%2520pivoting%252C%2520and%250Athen%2520express%2520the%2520LoRA%2520update%2520as%2520a%2520linear%2520combination%2520of%2520these%2520basis%2520vectors%2520--%250Atraining%2520only%2520the%2520scalar%2520coefficients%252C%2520which%2520imposes%2520clear%2520structure%2520on%250Aadaptation%2520and%2520drastically%2520reduces%2520parameter%2520count.%2520Experiments%2520across%2520GLUE%250Atasks%2520show%2520that%2520QR-LoRA%2520matches%2520or%2520exceeds%2520the%2520performance%2520of%2520full%2520fine-tuning%252C%250Astandard%2520LoRA%252C%2520and%2520SVD-LoRA%2520%2528LoRA%2520with%2520update%2520matrices%2520initialized%2520via%2520singular%250Avalue%2520decomposition%2529%2520with%2520as%2520few%2520as%2520601%2520parameters%2520--%2520a%2520reduction%2520of%2520over%25201000x%250Acompared%2520to%2520full%2520fine-tuning%2520and%252077x%2520fewer%2520than%2520typical%2520LoRA%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QR-LoRA%3A%20QR-Based%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20of%20Large%0A%20%20Language%20Models&entry.906535625=Jessica%20Liang%20and%20Anirudh%20Bharadwaj&entry.1292438233=%20%20The%20growing%20scale%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20necessitated%20the%0Adevelopment%20of%20parameter-efficient%20fine-tuning%20techniques.%20Low-Rank%20Adaptation%0A%28LoRA%29%20has%20emerged%20as%20a%20promising%20approach%2C%20reducing%20the%20number%20of%20trainable%0Aparameters%20by%20applying%20low-rank%20updates%20to%20pretrained%20weights.%20While%20standard%0ALoRA%20learns%20both%20update%20factors%20directly%2C%20several%20recent%20variants%20first%0Ainitialize%20those%20matrices%20via%20an%20SVD%20of%20the%20pretrained%20weights%20--%20an%20operation%0Athat%20can%20be%20expensive%20on%20large%20models%20and%20yields%20singular%20vectors%20that%20are%20not%0Aalways%20easy%20to%20interpret.%20In%20this%20work%2C%20we%20extract%20an%20orthonormal%20basis%20from%0Athe%20pretrained%20weight%20matrix%20using%20QR%20decomposition%20with%20column%20pivoting%2C%20and%0Athen%20express%20the%20LoRA%20update%20as%20a%20linear%20combination%20of%20these%20basis%20vectors%20--%0Atraining%20only%20the%20scalar%20coefficients%2C%20which%20imposes%20clear%20structure%20on%0Aadaptation%20and%20drastically%20reduces%20parameter%20count.%20Experiments%20across%20GLUE%0Atasks%20show%20that%20QR-LoRA%20matches%20or%20exceeds%20the%20performance%20of%20full%20fine-tuning%2C%0Astandard%20LoRA%2C%20and%20SVD-LoRA%20%28LoRA%20with%20update%20matrices%20initialized%20via%20singular%0Avalue%20decomposition%29%20with%20as%20few%20as%20601%20parameters%20--%20a%20reduction%20of%20over%201000x%0Acompared%20to%20full%20fine-tuning%20and%2077x%20fewer%20than%20typical%20LoRA%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21810v1&entry.124074799=Read"},
{"title": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of\n  the Land", "author": "Simone Scardapane", "abstract": "  Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures.\n", "link": "http://arxiv.org/abs/2404.17625v3", "date": "2025-08-29", "relevancy": 1.9014, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4811}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alice%27s%20Adventures%20in%20a%20Differentiable%20Wonderland%20--%20Volume%20I%2C%20A%20Tour%20of%0A%20%20the%20Land&body=Title%3A%20Alice%27s%20Adventures%20in%20a%20Differentiable%20Wonderland%20--%20Volume%20I%2C%20A%20Tour%20of%0A%20%20the%20Land%0AAuthor%3A%20Simone%20Scardapane%0AAbstract%3A%20%20%20Neural%20networks%20surround%20us%2C%20in%20the%20form%20of%20large%20language%20models%2C%20speech%0Atranscription%20systems%2C%20molecular%20discovery%20algorithms%2C%20robotics%2C%20and%20much%20more.%0AStripped%20of%20anything%20else%2C%20neural%20networks%20are%20compositions%20of%20differentiable%0Aprimitives%2C%20and%20studying%20them%20means%20learning%20how%20to%20program%20and%20how%20to%20interact%0Awith%20these%20models%2C%20a%20particular%20example%20of%20what%20is%20called%20differentiable%0Aprogramming.%0A%20%20This%20primer%20is%20an%20introduction%20to%20this%20fascinating%20field%20imagined%20for%0Asomeone%2C%20like%20Alice%2C%20who%20has%20just%20ventured%20into%20this%20strange%20differentiable%0Awonderland.%20I%20overview%20the%20basics%20of%20optimizing%20a%20function%20via%20automatic%0Adifferentiation%2C%20and%20a%20selection%20of%20the%20most%20common%20designs%20for%20handling%0Asequences%2C%20graphs%2C%20texts%2C%20and%20audios.%20The%20focus%20is%20on%20a%20intuitive%2C%0Aself-contained%20introduction%20to%20the%20most%20important%20design%20techniques%2C%20including%0Aconvolutional%2C%20attentional%2C%20and%20recurrent%20blocks%2C%20hoping%20to%20bridge%20the%20gap%0Abetween%20theory%20and%20code%20%28PyTorch%20and%20JAX%29%20and%20leaving%20the%20reader%20capable%20of%0Aunderstanding%20some%20of%20the%20most%20advanced%20models%20out%20there%2C%20such%20as%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17625v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlice%2527s%2520Adventures%2520in%2520a%2520Differentiable%2520Wonderland%2520--%2520Volume%2520I%252C%2520A%2520Tour%2520of%250A%2520%2520the%2520Land%26entry.906535625%3DSimone%2520Scardapane%26entry.1292438233%3D%2520%2520Neural%2520networks%2520surround%2520us%252C%2520in%2520the%2520form%2520of%2520large%2520language%2520models%252C%2520speech%250Atranscription%2520systems%252C%2520molecular%2520discovery%2520algorithms%252C%2520robotics%252C%2520and%2520much%2520more.%250AStripped%2520of%2520anything%2520else%252C%2520neural%2520networks%2520are%2520compositions%2520of%2520differentiable%250Aprimitives%252C%2520and%2520studying%2520them%2520means%2520learning%2520how%2520to%2520program%2520and%2520how%2520to%2520interact%250Awith%2520these%2520models%252C%2520a%2520particular%2520example%2520of%2520what%2520is%2520called%2520differentiable%250Aprogramming.%250A%2520%2520This%2520primer%2520is%2520an%2520introduction%2520to%2520this%2520fascinating%2520field%2520imagined%2520for%250Asomeone%252C%2520like%2520Alice%252C%2520who%2520has%2520just%2520ventured%2520into%2520this%2520strange%2520differentiable%250Awonderland.%2520I%2520overview%2520the%2520basics%2520of%2520optimizing%2520a%2520function%2520via%2520automatic%250Adifferentiation%252C%2520and%2520a%2520selection%2520of%2520the%2520most%2520common%2520designs%2520for%2520handling%250Asequences%252C%2520graphs%252C%2520texts%252C%2520and%2520audios.%2520The%2520focus%2520is%2520on%2520a%2520intuitive%252C%250Aself-contained%2520introduction%2520to%2520the%2520most%2520important%2520design%2520techniques%252C%2520including%250Aconvolutional%252C%2520attentional%252C%2520and%2520recurrent%2520blocks%252C%2520hoping%2520to%2520bridge%2520the%2520gap%250Abetween%2520theory%2520and%2520code%2520%2528PyTorch%2520and%2520JAX%2529%2520and%2520leaving%2520the%2520reader%2520capable%2520of%250Aunderstanding%2520some%2520of%2520the%2520most%2520advanced%2520models%2520out%2520there%252C%2520such%2520as%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17625v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alice%27s%20Adventures%20in%20a%20Differentiable%20Wonderland%20--%20Volume%20I%2C%20A%20Tour%20of%0A%20%20the%20Land&entry.906535625=Simone%20Scardapane&entry.1292438233=%20%20Neural%20networks%20surround%20us%2C%20in%20the%20form%20of%20large%20language%20models%2C%20speech%0Atranscription%20systems%2C%20molecular%20discovery%20algorithms%2C%20robotics%2C%20and%20much%20more.%0AStripped%20of%20anything%20else%2C%20neural%20networks%20are%20compositions%20of%20differentiable%0Aprimitives%2C%20and%20studying%20them%20means%20learning%20how%20to%20program%20and%20how%20to%20interact%0Awith%20these%20models%2C%20a%20particular%20example%20of%20what%20is%20called%20differentiable%0Aprogramming.%0A%20%20This%20primer%20is%20an%20introduction%20to%20this%20fascinating%20field%20imagined%20for%0Asomeone%2C%20like%20Alice%2C%20who%20has%20just%20ventured%20into%20this%20strange%20differentiable%0Awonderland.%20I%20overview%20the%20basics%20of%20optimizing%20a%20function%20via%20automatic%0Adifferentiation%2C%20and%20a%20selection%20of%20the%20most%20common%20designs%20for%20handling%0Asequences%2C%20graphs%2C%20texts%2C%20and%20audios.%20The%20focus%20is%20on%20a%20intuitive%2C%0Aself-contained%20introduction%20to%20the%20most%20important%20design%20techniques%2C%20including%0Aconvolutional%2C%20attentional%2C%20and%20recurrent%20blocks%2C%20hoping%20to%20bridge%20the%20gap%0Abetween%20theory%20and%20code%20%28PyTorch%20and%20JAX%29%20and%20leaving%20the%20reader%20capable%20of%0Aunderstanding%20some%20of%20the%20most%20advanced%20models%20out%20there%2C%20such%20as%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17625v3&entry.124074799=Read"},
{"title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning", "author": "Yang Wu and Huayi Zhang and Yizheng Jiao and Lin Ma and Xiaozhong Liu and Jinhong Yu and Dongyu Zhang and Dezhi Yu and Wei Xu", "abstract": "  Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5\\% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.\n", "link": "http://arxiv.org/abs/2412.00631v2", "date": "2025-08-29", "relevancy": 1.8888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4736}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4731}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROSE%3A%20A%20Reward-Oriented%20Data%20Selection%20Framework%20for%20LLM%20Task-Specific%0A%20%20Instruction%20Tuning&body=Title%3A%20ROSE%3A%20A%20Reward-Oriented%20Data%20Selection%20Framework%20for%20LLM%20Task-Specific%0A%20%20Instruction%20Tuning%0AAuthor%3A%20Yang%20Wu%20and%20Huayi%20Zhang%20and%20Yizheng%20Jiao%20and%20Lin%20Ma%20and%20Xiaozhong%20Liu%20and%20Jinhong%20Yu%20and%20Dongyu%20Zhang%20and%20Dezhi%20Yu%20and%20Wei%20Xu%0AAbstract%3A%20%20%20Instruction%20tuning%20has%20underscored%20the%20significant%20potential%20of%20large%0Alanguage%20models%20%28LLMs%29%20in%20producing%20more%20human%20controllable%20and%20effective%0Aoutputs%20in%20various%20domains.%20In%20this%20work%2C%20we%20focus%20on%20the%20data%20selection%0Aproblem%20for%20task-specific%20instruction%20tuning%20of%20LLMs.%20Prevailing%20methods%0Aprimarily%20rely%20on%20the%20crafted%20similarity%20metrics%20to%20select%20training%20data%20that%0Aaligns%20with%20the%20test%20data%20distribution.%20The%20goal%20is%20to%20minimize%20instruction%0Atuning%20loss%20on%20the%20test%20data%2C%20ultimately%20improving%20performance%20on%20the%20target%0Atask.%20However%2C%20it%20has%20been%20widely%20observed%20that%20instruction%20tuning%20loss%20%28i.e.%2C%0Across-entropy%20loss%20for%20next%20token%20prediction%29%20in%20LLMs%20often%20fails%20to%20exhibit%20a%0Amonotonic%20relationship%20with%20actual%20task%20performance.%20This%20misalignment%0Aundermines%20the%20effectiveness%20of%20current%20data%20selection%20methods%20for%0Atask-specific%20instruction%20tuning.%20To%20address%20this%20issue%2C%20we%20introduce%20ROSE%2C%20a%0Anovel%20Reward-Oriented%20inStruction%20data%20sElection%20method%20which%20leverages%0Apairwise%20preference%20loss%20as%20a%20reward%20signal%20to%20optimize%20data%20selection%20for%0Atask-specific%20instruction%20tuning.%20Specifically%2C%20ROSE%20adapts%20an%20influence%0Aformulation%20to%20approximate%20the%20influence%20of%20training%20data%20points%20relative%20to%20a%0Afew-shot%20preference%20validation%20set%20to%20select%20the%20most%20task-related%20training%0Adata%20points.%20Experimental%20results%20show%20that%20by%20selecting%20just%205%5C%25%20of%20the%0Atraining%20data%20using%20ROSE%2C%20our%20approach%20can%20achieve%20competitive%20results%20compared%0Ato%20fine-tuning%20with%20the%20full%20training%20dataset%2C%20and%20it%20surpasses%20other%0Astate-of-the-art%20data%20selection%20methods%20for%20task-specific%20instruction%20tuning.%0AOur%20qualitative%20analysis%20further%20confirms%20the%20robust%20generalizability%20of%20our%0Amethod%20across%20multiple%20benchmark%20datasets%20and%20diverse%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROSE%253A%2520A%2520Reward-Oriented%2520Data%2520Selection%2520Framework%2520for%2520LLM%2520Task-Specific%250A%2520%2520Instruction%2520Tuning%26entry.906535625%3DYang%2520Wu%2520and%2520Huayi%2520Zhang%2520and%2520Yizheng%2520Jiao%2520and%2520Lin%2520Ma%2520and%2520Xiaozhong%2520Liu%2520and%2520Jinhong%2520Yu%2520and%2520Dongyu%2520Zhang%2520and%2520Dezhi%2520Yu%2520and%2520Wei%2520Xu%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520has%2520underscored%2520the%2520significant%2520potential%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520in%2520producing%2520more%2520human%2520controllable%2520and%2520effective%250Aoutputs%2520in%2520various%2520domains.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520data%2520selection%250Aproblem%2520for%2520task-specific%2520instruction%2520tuning%2520of%2520LLMs.%2520Prevailing%2520methods%250Aprimarily%2520rely%2520on%2520the%2520crafted%2520similarity%2520metrics%2520to%2520select%2520training%2520data%2520that%250Aaligns%2520with%2520the%2520test%2520data%2520distribution.%2520The%2520goal%2520is%2520to%2520minimize%2520instruction%250Atuning%2520loss%2520on%2520the%2520test%2520data%252C%2520ultimately%2520improving%2520performance%2520on%2520the%2520target%250Atask.%2520However%252C%2520it%2520has%2520been%2520widely%2520observed%2520that%2520instruction%2520tuning%2520loss%2520%2528i.e.%252C%250Across-entropy%2520loss%2520for%2520next%2520token%2520prediction%2529%2520in%2520LLMs%2520often%2520fails%2520to%2520exhibit%2520a%250Amonotonic%2520relationship%2520with%2520actual%2520task%2520performance.%2520This%2520misalignment%250Aundermines%2520the%2520effectiveness%2520of%2520current%2520data%2520selection%2520methods%2520for%250Atask-specific%2520instruction%2520tuning.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520ROSE%252C%2520a%250Anovel%2520Reward-Oriented%2520inStruction%2520data%2520sElection%2520method%2520which%2520leverages%250Apairwise%2520preference%2520loss%2520as%2520a%2520reward%2520signal%2520to%2520optimize%2520data%2520selection%2520for%250Atask-specific%2520instruction%2520tuning.%2520Specifically%252C%2520ROSE%2520adapts%2520an%2520influence%250Aformulation%2520to%2520approximate%2520the%2520influence%2520of%2520training%2520data%2520points%2520relative%2520to%2520a%250Afew-shot%2520preference%2520validation%2520set%2520to%2520select%2520the%2520most%2520task-related%2520training%250Adata%2520points.%2520Experimental%2520results%2520show%2520that%2520by%2520selecting%2520just%25205%255C%2525%2520of%2520the%250Atraining%2520data%2520using%2520ROSE%252C%2520our%2520approach%2520can%2520achieve%2520competitive%2520results%2520compared%250Ato%2520fine-tuning%2520with%2520the%2520full%2520training%2520dataset%252C%2520and%2520it%2520surpasses%2520other%250Astate-of-the-art%2520data%2520selection%2520methods%2520for%2520task-specific%2520instruction%2520tuning.%250AOur%2520qualitative%2520analysis%2520further%2520confirms%2520the%2520robust%2520generalizability%2520of%2520our%250Amethod%2520across%2520multiple%2520benchmark%2520datasets%2520and%2520diverse%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROSE%3A%20A%20Reward-Oriented%20Data%20Selection%20Framework%20for%20LLM%20Task-Specific%0A%20%20Instruction%20Tuning&entry.906535625=Yang%20Wu%20and%20Huayi%20Zhang%20and%20Yizheng%20Jiao%20and%20Lin%20Ma%20and%20Xiaozhong%20Liu%20and%20Jinhong%20Yu%20and%20Dongyu%20Zhang%20and%20Dezhi%20Yu%20and%20Wei%20Xu&entry.1292438233=%20%20Instruction%20tuning%20has%20underscored%20the%20significant%20potential%20of%20large%0Alanguage%20models%20%28LLMs%29%20in%20producing%20more%20human%20controllable%20and%20effective%0Aoutputs%20in%20various%20domains.%20In%20this%20work%2C%20we%20focus%20on%20the%20data%20selection%0Aproblem%20for%20task-specific%20instruction%20tuning%20of%20LLMs.%20Prevailing%20methods%0Aprimarily%20rely%20on%20the%20crafted%20similarity%20metrics%20to%20select%20training%20data%20that%0Aaligns%20with%20the%20test%20data%20distribution.%20The%20goal%20is%20to%20minimize%20instruction%0Atuning%20loss%20on%20the%20test%20data%2C%20ultimately%20improving%20performance%20on%20the%20target%0Atask.%20However%2C%20it%20has%20been%20widely%20observed%20that%20instruction%20tuning%20loss%20%28i.e.%2C%0Across-entropy%20loss%20for%20next%20token%20prediction%29%20in%20LLMs%20often%20fails%20to%20exhibit%20a%0Amonotonic%20relationship%20with%20actual%20task%20performance.%20This%20misalignment%0Aundermines%20the%20effectiveness%20of%20current%20data%20selection%20methods%20for%0Atask-specific%20instruction%20tuning.%20To%20address%20this%20issue%2C%20we%20introduce%20ROSE%2C%20a%0Anovel%20Reward-Oriented%20inStruction%20data%20sElection%20method%20which%20leverages%0Apairwise%20preference%20loss%20as%20a%20reward%20signal%20to%20optimize%20data%20selection%20for%0Atask-specific%20instruction%20tuning.%20Specifically%2C%20ROSE%20adapts%20an%20influence%0Aformulation%20to%20approximate%20the%20influence%20of%20training%20data%20points%20relative%20to%20a%0Afew-shot%20preference%20validation%20set%20to%20select%20the%20most%20task-related%20training%0Adata%20points.%20Experimental%20results%20show%20that%20by%20selecting%20just%205%5C%25%20of%20the%0Atraining%20data%20using%20ROSE%2C%20our%20approach%20can%20achieve%20competitive%20results%20compared%0Ato%20fine-tuning%20with%20the%20full%20training%20dataset%2C%20and%20it%20surpasses%20other%0Astate-of-the-art%20data%20selection%20methods%20for%20task-specific%20instruction%20tuning.%0AOur%20qualitative%20analysis%20further%20confirms%20the%20robust%20generalizability%20of%20our%0Amethod%20across%20multiple%20benchmark%20datasets%20and%20diverse%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00631v2&entry.124074799=Read"},
{"title": "Documenting Deployment with Fabric: A Repository of Real-World AI\n  Governance", "author": "Mackenzie Jorgensen and Kendall Brogle and Katherine M. Collins and Lujain Ibrahim and Arina Shah and Petra Ivanovic and Noah Broestl and Gabriel Piles and Paul Dongha and Hatim Abdulhussein and Adrian Weller and Jillian Powers and Umang Bhatt", "abstract": "  Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance.\n", "link": "http://arxiv.org/abs/2508.14119v4", "date": "2025-08-29", "relevancy": 1.8754, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4855}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4712}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Documenting%20Deployment%20with%20Fabric%3A%20A%20Repository%20of%20Real-World%20AI%0A%20%20Governance&body=Title%3A%20Documenting%20Deployment%20with%20Fabric%3A%20A%20Repository%20of%20Real-World%20AI%0A%20%20Governance%0AAuthor%3A%20Mackenzie%20Jorgensen%20and%20Kendall%20Brogle%20and%20Katherine%20M.%20Collins%20and%20Lujain%20Ibrahim%20and%20Arina%20Shah%20and%20Petra%20Ivanovic%20and%20Noah%20Broestl%20and%20Gabriel%20Piles%20and%20Paul%20Dongha%20and%20Hatim%20Abdulhussein%20and%20Adrian%20Weller%20and%20Jillian%20Powers%20and%20Umang%20Bhatt%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20is%20increasingly%20integrated%20into%20society%2C%20from%0Afinancial%20services%20and%20traffic%20management%20to%20creative%20writing.%20Academic%0Aliterature%20on%20the%20deployment%20of%20AI%20has%20mostly%20focused%20on%20the%20risks%20and%20harms%0Athat%20result%20from%20the%20use%20of%20AI.%20We%20introduce%20Fabric%2C%20a%20publicly%20available%0Arepository%20of%20deployed%20AI%20use%20cases%20to%20outline%20their%20governance%20mechanisms.%0AThrough%20semi-structured%20interviews%20with%20practitioners%2C%20we%20collect%20an%20initial%0Aset%20of%2020%20AI%20use%20cases.%20In%20addition%2C%20we%20co-design%20diagrams%20of%20the%20AI%20workflow%0Awith%20the%20practitioners.%20We%20discuss%20the%20oversight%20mechanisms%20and%20guardrails%20used%0Ain%20practice%20to%20safeguard%20AI%20use.%20The%20Fabric%20repository%20includes%20visual%20diagrams%0Aof%20AI%20use%20cases%20and%20descriptions%20of%20the%20deployed%20systems.%20Using%20the%20repository%2C%0Awe%20surface%20gaps%20in%20governance%20and%20find%20common%20patterns%20in%20human%20oversight%20of%0Adeployed%20AI%20systems.%20We%20intend%20for%20Fabric%20to%20serve%20as%20an%20extendable%2C%20evolving%0Atool%20for%20researchers%20to%20study%20the%20effectiveness%20of%20AI%20governance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14119v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocumenting%2520Deployment%2520with%2520Fabric%253A%2520A%2520Repository%2520of%2520Real-World%2520AI%250A%2520%2520Governance%26entry.906535625%3DMackenzie%2520Jorgensen%2520and%2520Kendall%2520Brogle%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Lujain%2520Ibrahim%2520and%2520Arina%2520Shah%2520and%2520Petra%2520Ivanovic%2520and%2520Noah%2520Broestl%2520and%2520Gabriel%2520Piles%2520and%2520Paul%2520Dongha%2520and%2520Hatim%2520Abdulhussein%2520and%2520Adrian%2520Weller%2520and%2520Jillian%2520Powers%2520and%2520Umang%2520Bhatt%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520is%2520increasingly%2520integrated%2520into%2520society%252C%2520from%250Afinancial%2520services%2520and%2520traffic%2520management%2520to%2520creative%2520writing.%2520Academic%250Aliterature%2520on%2520the%2520deployment%2520of%2520AI%2520has%2520mostly%2520focused%2520on%2520the%2520risks%2520and%2520harms%250Athat%2520result%2520from%2520the%2520use%2520of%2520AI.%2520We%2520introduce%2520Fabric%252C%2520a%2520publicly%2520available%250Arepository%2520of%2520deployed%2520AI%2520use%2520cases%2520to%2520outline%2520their%2520governance%2520mechanisms.%250AThrough%2520semi-structured%2520interviews%2520with%2520practitioners%252C%2520we%2520collect%2520an%2520initial%250Aset%2520of%252020%2520AI%2520use%2520cases.%2520In%2520addition%252C%2520we%2520co-design%2520diagrams%2520of%2520the%2520AI%2520workflow%250Awith%2520the%2520practitioners.%2520We%2520discuss%2520the%2520oversight%2520mechanisms%2520and%2520guardrails%2520used%250Ain%2520practice%2520to%2520safeguard%2520AI%2520use.%2520The%2520Fabric%2520repository%2520includes%2520visual%2520diagrams%250Aof%2520AI%2520use%2520cases%2520and%2520descriptions%2520of%2520the%2520deployed%2520systems.%2520Using%2520the%2520repository%252C%250Awe%2520surface%2520gaps%2520in%2520governance%2520and%2520find%2520common%2520patterns%2520in%2520human%2520oversight%2520of%250Adeployed%2520AI%2520systems.%2520We%2520intend%2520for%2520Fabric%2520to%2520serve%2520as%2520an%2520extendable%252C%2520evolving%250Atool%2520for%2520researchers%2520to%2520study%2520the%2520effectiveness%2520of%2520AI%2520governance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14119v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Documenting%20Deployment%20with%20Fabric%3A%20A%20Repository%20of%20Real-World%20AI%0A%20%20Governance&entry.906535625=Mackenzie%20Jorgensen%20and%20Kendall%20Brogle%20and%20Katherine%20M.%20Collins%20and%20Lujain%20Ibrahim%20and%20Arina%20Shah%20and%20Petra%20Ivanovic%20and%20Noah%20Broestl%20and%20Gabriel%20Piles%20and%20Paul%20Dongha%20and%20Hatim%20Abdulhussein%20and%20Adrian%20Weller%20and%20Jillian%20Powers%20and%20Umang%20Bhatt&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20is%20increasingly%20integrated%20into%20society%2C%20from%0Afinancial%20services%20and%20traffic%20management%20to%20creative%20writing.%20Academic%0Aliterature%20on%20the%20deployment%20of%20AI%20has%20mostly%20focused%20on%20the%20risks%20and%20harms%0Athat%20result%20from%20the%20use%20of%20AI.%20We%20introduce%20Fabric%2C%20a%20publicly%20available%0Arepository%20of%20deployed%20AI%20use%20cases%20to%20outline%20their%20governance%20mechanisms.%0AThrough%20semi-structured%20interviews%20with%20practitioners%2C%20we%20collect%20an%20initial%0Aset%20of%2020%20AI%20use%20cases.%20In%20addition%2C%20we%20co-design%20diagrams%20of%20the%20AI%20workflow%0Awith%20the%20practitioners.%20We%20discuss%20the%20oversight%20mechanisms%20and%20guardrails%20used%0Ain%20practice%20to%20safeguard%20AI%20use.%20The%20Fabric%20repository%20includes%20visual%20diagrams%0Aof%20AI%20use%20cases%20and%20descriptions%20of%20the%20deployed%20systems.%20Using%20the%20repository%2C%0Awe%20surface%20gaps%20in%20governance%20and%20find%20common%20patterns%20in%20human%20oversight%20of%0Adeployed%20AI%20systems.%20We%20intend%20for%20Fabric%20to%20serve%20as%20an%20extendable%2C%20evolving%0Atool%20for%20researchers%20to%20study%20the%20effectiveness%20of%20AI%20governance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14119v4&entry.124074799=Read"},
{"title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with\n  Synthetic Annotations using CoTR prompting and Large Language Models", "author": "Nidhi Kowtal and Raviraj Joshi", "abstract": "  Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP\n", "link": "http://arxiv.org/abs/2506.00863v2", "date": "2025-08-29", "relevancy": 1.8678, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L3Cube-MahaEmotions%3A%20A%20Marathi%20Emotion%20Recognition%20Dataset%20with%0A%20%20Synthetic%20Annotations%20using%20CoTR%20prompting%20and%20Large%20Language%20Models&body=Title%3A%20L3Cube-MahaEmotions%3A%20A%20Marathi%20Emotion%20Recognition%20Dataset%20with%0A%20%20Synthetic%20Annotations%20using%20CoTR%20prompting%20and%20Large%20Language%20Models%0AAuthor%3A%20Nidhi%20Kowtal%20and%20Raviraj%20Joshi%0AAbstract%3A%20%20%20Emotion%20recognition%20in%20low-resource%20languages%20like%20Marathi%20remains%0Achallenging%20due%20to%20limited%20annotated%20data.%20We%20present%20L3Cube-MahaEmotions%2C%20a%0Ahigh-quality%20Marathi%20emotion%20recognition%20dataset%20with%2011%20fine-grained%20emotion%0Alabels.%20The%20training%20data%20is%20synthetically%20annotated%20using%20large%20language%0Amodels%20%28LLMs%29%2C%20while%20the%20validation%20and%20test%20sets%20are%20manually%20labeled%20to%20serve%0Aas%20a%20reliable%20gold-standard%20benchmark.%20Building%20on%20the%20MahaSent%20dataset%2C%20we%0Aapply%20the%20Chain-of-Translation%20%28CoTR%29%20prompting%20technique%2C%20where%20Marathi%0Asentences%20are%20translated%20into%20English%20and%20emotion%20labeled%20via%20a%20single%20prompt.%0AGPT-4%20and%20Llama3-405B%20were%20evaluated%2C%20with%20GPT-4%20selected%20for%20training%20data%0Aannotation%20due%20to%20superior%20label%20quality.%20We%20evaluate%20model%20performance%20using%0Astandard%20metrics%20and%20explore%20label%20aggregation%20strategies%20%28e.g.%2C%20Union%2C%0AIntersection%29.%20While%20GPT-4%20predictions%20outperform%20fine-tuned%20BERT%20models%2C%0ABERT-based%20models%20trained%20on%20synthetic%20labels%20fail%20to%20surpass%20GPT-4.%20This%0Ahighlights%20both%20the%20importance%20of%20high-quality%20human-labeled%20data%20and%20the%0Ainherent%20complexity%20of%20emotion%20recognition.%20An%20important%20finding%20of%20this%20work%0Ais%20that%20generic%20LLMs%20like%20GPT-4%20and%20Llama3-405B%20generalize%20better%20than%0Afine-tuned%20BERT%20for%20complex%20low-resource%20emotion%20recognition%20tasks.%20The%20dataset%0Aand%20model%20are%20shared%20publicly%20at%20https%3A//github.com/l3cube-pune/MarathiNLP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL3Cube-MahaEmotions%253A%2520A%2520Marathi%2520Emotion%2520Recognition%2520Dataset%2520with%250A%2520%2520Synthetic%2520Annotations%2520using%2520CoTR%2520prompting%2520and%2520Large%2520Language%2520Models%26entry.906535625%3DNidhi%2520Kowtal%2520and%2520Raviraj%2520Joshi%26entry.1292438233%3D%2520%2520Emotion%2520recognition%2520in%2520low-resource%2520languages%2520like%2520Marathi%2520remains%250Achallenging%2520due%2520to%2520limited%2520annotated%2520data.%2520We%2520present%2520L3Cube-MahaEmotions%252C%2520a%250Ahigh-quality%2520Marathi%2520emotion%2520recognition%2520dataset%2520with%252011%2520fine-grained%2520emotion%250Alabels.%2520The%2520training%2520data%2520is%2520synthetically%2520annotated%2520using%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520while%2520the%2520validation%2520and%2520test%2520sets%2520are%2520manually%2520labeled%2520to%2520serve%250Aas%2520a%2520reliable%2520gold-standard%2520benchmark.%2520Building%2520on%2520the%2520MahaSent%2520dataset%252C%2520we%250Aapply%2520the%2520Chain-of-Translation%2520%2528CoTR%2529%2520prompting%2520technique%252C%2520where%2520Marathi%250Asentences%2520are%2520translated%2520into%2520English%2520and%2520emotion%2520labeled%2520via%2520a%2520single%2520prompt.%250AGPT-4%2520and%2520Llama3-405B%2520were%2520evaluated%252C%2520with%2520GPT-4%2520selected%2520for%2520training%2520data%250Aannotation%2520due%2520to%2520superior%2520label%2520quality.%2520We%2520evaluate%2520model%2520performance%2520using%250Astandard%2520metrics%2520and%2520explore%2520label%2520aggregation%2520strategies%2520%2528e.g.%252C%2520Union%252C%250AIntersection%2529.%2520While%2520GPT-4%2520predictions%2520outperform%2520fine-tuned%2520BERT%2520models%252C%250ABERT-based%2520models%2520trained%2520on%2520synthetic%2520labels%2520fail%2520to%2520surpass%2520GPT-4.%2520This%250Ahighlights%2520both%2520the%2520importance%2520of%2520high-quality%2520human-labeled%2520data%2520and%2520the%250Ainherent%2520complexity%2520of%2520emotion%2520recognition.%2520An%2520important%2520finding%2520of%2520this%2520work%250Ais%2520that%2520generic%2520LLMs%2520like%2520GPT-4%2520and%2520Llama3-405B%2520generalize%2520better%2520than%250Afine-tuned%2520BERT%2520for%2520complex%2520low-resource%2520emotion%2520recognition%2520tasks.%2520The%2520dataset%250Aand%2520model%2520are%2520shared%2520publicly%2520at%2520https%253A//github.com/l3cube-pune/MarathiNLP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L3Cube-MahaEmotions%3A%20A%20Marathi%20Emotion%20Recognition%20Dataset%20with%0A%20%20Synthetic%20Annotations%20using%20CoTR%20prompting%20and%20Large%20Language%20Models&entry.906535625=Nidhi%20Kowtal%20and%20Raviraj%20Joshi&entry.1292438233=%20%20Emotion%20recognition%20in%20low-resource%20languages%20like%20Marathi%20remains%0Achallenging%20due%20to%20limited%20annotated%20data.%20We%20present%20L3Cube-MahaEmotions%2C%20a%0Ahigh-quality%20Marathi%20emotion%20recognition%20dataset%20with%2011%20fine-grained%20emotion%0Alabels.%20The%20training%20data%20is%20synthetically%20annotated%20using%20large%20language%0Amodels%20%28LLMs%29%2C%20while%20the%20validation%20and%20test%20sets%20are%20manually%20labeled%20to%20serve%0Aas%20a%20reliable%20gold-standard%20benchmark.%20Building%20on%20the%20MahaSent%20dataset%2C%20we%0Aapply%20the%20Chain-of-Translation%20%28CoTR%29%20prompting%20technique%2C%20where%20Marathi%0Asentences%20are%20translated%20into%20English%20and%20emotion%20labeled%20via%20a%20single%20prompt.%0AGPT-4%20and%20Llama3-405B%20were%20evaluated%2C%20with%20GPT-4%20selected%20for%20training%20data%0Aannotation%20due%20to%20superior%20label%20quality.%20We%20evaluate%20model%20performance%20using%0Astandard%20metrics%20and%20explore%20label%20aggregation%20strategies%20%28e.g.%2C%20Union%2C%0AIntersection%29.%20While%20GPT-4%20predictions%20outperform%20fine-tuned%20BERT%20models%2C%0ABERT-based%20models%20trained%20on%20synthetic%20labels%20fail%20to%20surpass%20GPT-4.%20This%0Ahighlights%20both%20the%20importance%20of%20high-quality%20human-labeled%20data%20and%20the%0Ainherent%20complexity%20of%20emotion%20recognition.%20An%20important%20finding%20of%20this%20work%0Ais%20that%20generic%20LLMs%20like%20GPT-4%20and%20Llama3-405B%20generalize%20better%20than%0Afine-tuned%20BERT%20for%20complex%20low-resource%20emotion%20recognition%20tasks.%20The%20dataset%0Aand%20model%20are%20shared%20publicly%20at%20https%3A//github.com/l3cube-pune/MarathiNLP%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00863v2&entry.124074799=Read"},
{"title": "EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based\n  Pre-Ordering and Human-in-the-Loop Sorting", "author": "Yujin Park and Haejun Chung and Ikbeom Jang", "abstract": "  Pairwise comparison is often favored over absolute rating or ordinal\nclassification in subjective or difficult annotation tasks due to its improved\nreliability. However, exhaustive comparisons require a massive number of\nannotations (O(n^2)). Recent work has greatly reduced the annotation burden\n(O(n log n)) by actively sampling pairwise comparisons using a sorting\nalgorithm. We further improve annotation efficiency by (1) roughly pre-ordering\nitems using the Contrastive Language-Image Pre-training (CLIP) model\nhierarchically without training, and (2) replacing easy, obvious human\ncomparisons with automated comparisons. The proposed EZ-Sort first produces a\nCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,\nand finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation\nwas conducted using various datasets: face-age estimation (FGNET), historical\nimage chronology (DHCI), and retinal image quality assessment (EyePACS). It\nshowed that EZ-Sort reduced human annotation cost by 90.5% compared to\nexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =\n100), while improving or maintaining inter-rater reliability. These results\ndemonstrate that combining CLIP-based priors with uncertainty-aware sampling\nyields an efficient and scalable solution for pairwise ranking.\n", "link": "http://arxiv.org/abs/2508.21550v1", "date": "2025-08-29", "relevancy": 1.8655, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5062}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EZ-Sort%3A%20Efficient%20Pairwise%20Comparison%20via%20Zero-Shot%20CLIP-Based%0A%20%20Pre-Ordering%20and%20Human-in-the-Loop%20Sorting&body=Title%3A%20EZ-Sort%3A%20Efficient%20Pairwise%20Comparison%20via%20Zero-Shot%20CLIP-Based%0A%20%20Pre-Ordering%20and%20Human-in-the-Loop%20Sorting%0AAuthor%3A%20Yujin%20Park%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang%0AAbstract%3A%20%20%20Pairwise%20comparison%20is%20often%20favored%20over%20absolute%20rating%20or%20ordinal%0Aclassification%20in%20subjective%20or%20difficult%20annotation%20tasks%20due%20to%20its%20improved%0Areliability.%20However%2C%20exhaustive%20comparisons%20require%20a%20massive%20number%20of%0Aannotations%20%28O%28n%5E2%29%29.%20Recent%20work%20has%20greatly%20reduced%20the%20annotation%20burden%0A%28O%28n%20log%20n%29%29%20by%20actively%20sampling%20pairwise%20comparisons%20using%20a%20sorting%0Aalgorithm.%20We%20further%20improve%20annotation%20efficiency%20by%20%281%29%20roughly%20pre-ordering%0Aitems%20using%20the%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20model%0Ahierarchically%20without%20training%2C%20and%20%282%29%20replacing%20easy%2C%20obvious%20human%0Acomparisons%20with%20automated%20comparisons.%20The%20proposed%20EZ-Sort%20first%20produces%20a%0ACLIP-based%20zero-shot%20pre-ordering%2C%20then%20initializes%20bucket-aware%20Elo%20scores%2C%0Aand%20finally%20runs%20an%20uncertainty-guided%20human-in-the-loop%20MergeSort.%20Validation%0Awas%20conducted%20using%20various%20datasets%3A%20face-age%20estimation%20%28FGNET%29%2C%20historical%0Aimage%20chronology%20%28DHCI%29%2C%20and%20retinal%20image%20quality%20assessment%20%28EyePACS%29.%20It%0Ashowed%20that%20EZ-Sort%20reduced%20human%20annotation%20cost%20by%2090.5%25%20compared%20to%0Aexhaustive%20pairwise%20comparisons%20and%20by%2019.8%25%20compared%20to%20prior%20work%20%28when%20n%20%3D%0A100%29%2C%20while%20improving%20or%20maintaining%20inter-rater%20reliability.%20These%20results%0Ademonstrate%20that%20combining%20CLIP-based%20priors%20with%20uncertainty-aware%20sampling%0Ayields%20an%20efficient%20and%20scalable%20solution%20for%20pairwise%20ranking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEZ-Sort%253A%2520Efficient%2520Pairwise%2520Comparison%2520via%2520Zero-Shot%2520CLIP-Based%250A%2520%2520Pre-Ordering%2520and%2520Human-in-the-Loop%2520Sorting%26entry.906535625%3DYujin%2520Park%2520and%2520Haejun%2520Chung%2520and%2520Ikbeom%2520Jang%26entry.1292438233%3D%2520%2520Pairwise%2520comparison%2520is%2520often%2520favored%2520over%2520absolute%2520rating%2520or%2520ordinal%250Aclassification%2520in%2520subjective%2520or%2520difficult%2520annotation%2520tasks%2520due%2520to%2520its%2520improved%250Areliability.%2520However%252C%2520exhaustive%2520comparisons%2520require%2520a%2520massive%2520number%2520of%250Aannotations%2520%2528O%2528n%255E2%2529%2529.%2520Recent%2520work%2520has%2520greatly%2520reduced%2520the%2520annotation%2520burden%250A%2528O%2528n%2520log%2520n%2529%2529%2520by%2520actively%2520sampling%2520pairwise%2520comparisons%2520using%2520a%2520sorting%250Aalgorithm.%2520We%2520further%2520improve%2520annotation%2520efficiency%2520by%2520%25281%2529%2520roughly%2520pre-ordering%250Aitems%2520using%2520the%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520model%250Ahierarchically%2520without%2520training%252C%2520and%2520%25282%2529%2520replacing%2520easy%252C%2520obvious%2520human%250Acomparisons%2520with%2520automated%2520comparisons.%2520The%2520proposed%2520EZ-Sort%2520first%2520produces%2520a%250ACLIP-based%2520zero-shot%2520pre-ordering%252C%2520then%2520initializes%2520bucket-aware%2520Elo%2520scores%252C%250Aand%2520finally%2520runs%2520an%2520uncertainty-guided%2520human-in-the-loop%2520MergeSort.%2520Validation%250Awas%2520conducted%2520using%2520various%2520datasets%253A%2520face-age%2520estimation%2520%2528FGNET%2529%252C%2520historical%250Aimage%2520chronology%2520%2528DHCI%2529%252C%2520and%2520retinal%2520image%2520quality%2520assessment%2520%2528EyePACS%2529.%2520It%250Ashowed%2520that%2520EZ-Sort%2520reduced%2520human%2520annotation%2520cost%2520by%252090.5%2525%2520compared%2520to%250Aexhaustive%2520pairwise%2520comparisons%2520and%2520by%252019.8%2525%2520compared%2520to%2520prior%2520work%2520%2528when%2520n%2520%253D%250A100%2529%252C%2520while%2520improving%2520or%2520maintaining%2520inter-rater%2520reliability.%2520These%2520results%250Ademonstrate%2520that%2520combining%2520CLIP-based%2520priors%2520with%2520uncertainty-aware%2520sampling%250Ayields%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%2520pairwise%2520ranking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EZ-Sort%3A%20Efficient%20Pairwise%20Comparison%20via%20Zero-Shot%20CLIP-Based%0A%20%20Pre-Ordering%20and%20Human-in-the-Loop%20Sorting&entry.906535625=Yujin%20Park%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang&entry.1292438233=%20%20Pairwise%20comparison%20is%20often%20favored%20over%20absolute%20rating%20or%20ordinal%0Aclassification%20in%20subjective%20or%20difficult%20annotation%20tasks%20due%20to%20its%20improved%0Areliability.%20However%2C%20exhaustive%20comparisons%20require%20a%20massive%20number%20of%0Aannotations%20%28O%28n%5E2%29%29.%20Recent%20work%20has%20greatly%20reduced%20the%20annotation%20burden%0A%28O%28n%20log%20n%29%29%20by%20actively%20sampling%20pairwise%20comparisons%20using%20a%20sorting%0Aalgorithm.%20We%20further%20improve%20annotation%20efficiency%20by%20%281%29%20roughly%20pre-ordering%0Aitems%20using%20the%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20model%0Ahierarchically%20without%20training%2C%20and%20%282%29%20replacing%20easy%2C%20obvious%20human%0Acomparisons%20with%20automated%20comparisons.%20The%20proposed%20EZ-Sort%20first%20produces%20a%0ACLIP-based%20zero-shot%20pre-ordering%2C%20then%20initializes%20bucket-aware%20Elo%20scores%2C%0Aand%20finally%20runs%20an%20uncertainty-guided%20human-in-the-loop%20MergeSort.%20Validation%0Awas%20conducted%20using%20various%20datasets%3A%20face-age%20estimation%20%28FGNET%29%2C%20historical%0Aimage%20chronology%20%28DHCI%29%2C%20and%20retinal%20image%20quality%20assessment%20%28EyePACS%29.%20It%0Ashowed%20that%20EZ-Sort%20reduced%20human%20annotation%20cost%20by%2090.5%25%20compared%20to%0Aexhaustive%20pairwise%20comparisons%20and%20by%2019.8%25%20compared%20to%20prior%20work%20%28when%20n%20%3D%0A100%29%2C%20while%20improving%20or%20maintaining%20inter-rater%20reliability.%20These%20results%0Ademonstrate%20that%20combining%20CLIP-based%20priors%20with%20uncertainty-aware%20sampling%0Ayields%20an%20efficient%20and%20scalable%20solution%20for%20pairwise%20ranking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21550v1&entry.124074799=Read"},
{"title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models", "author": "Neel Jain and Aditya Shrivastava and Chenyang Zhu and Daben Liu and Alfy Samuel and Ashwinee Panda and Anoop Kumar and Micah Goldblum and Tom Goldstein", "abstract": "  A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration.\n", "link": "http://arxiv.org/abs/2412.06748v2", "date": "2025-08-29", "relevancy": 1.8609, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5033}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4441}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refusal%20Tokens%3A%20A%20Simple%20Way%20to%20Calibrate%20Refusals%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Refusal%20Tokens%3A%20A%20Simple%20Way%20to%20Calibrate%20Refusals%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Neel%20Jain%20and%20Aditya%20Shrivastava%20and%20Chenyang%20Zhu%20and%20Daben%20Liu%20and%20Alfy%20Samuel%20and%20Ashwinee%20Panda%20and%20Anoop%20Kumar%20and%20Micah%20Goldblum%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20A%20key%20component%20of%20building%20safe%20and%20reliable%20language%20models%20is%20enabling%20the%0Amodels%20to%20appropriately%20refuse%20to%20follow%20certain%20instructions%20or%20answer%20certain%0Aquestions.%20We%20may%20want%20models%20to%20output%20refusal%20messages%20for%20various%20categories%0Aof%20user%20queries%2C%20for%20example%2C%20ill-posed%20questions%2C%20instructions%20for%20committing%0Aillegal%20acts%2C%20or%20queries%20which%20require%20information%20past%20the%20model%27s%20knowledge%0Ahorizon.%20Engineering%20models%20that%20refuse%20to%20answer%20such%20questions%20is%20complicated%0Aby%20the%20fact%20that%20an%20individual%20may%20want%20their%20model%20to%20exhibit%20varying%20levels%0Aof%20sensitivity%20for%20refusing%20queries%20of%20various%20categories%2C%20and%20different%20users%0Amay%20want%20different%20refusal%20rates.%20The%20current%20default%20approach%20involves%0Atraining%20multiple%20models%20with%20varying%20proportions%20of%20refusal%20messages%20from%20each%0Acategory%20to%20achieve%20the%20desired%20refusal%20rates%2C%20which%20is%20computationally%0Aexpensive%20and%20may%20require%20training%20a%20new%20model%20to%20accommodate%20each%20user%27s%0Adesired%20preference%20over%20refusal%20rates.%20To%20address%20these%20challenges%2C%20we%20propose%0Arefusal%20tokens%2C%20one%20such%20token%20for%20each%20refusal%20category%20or%20a%20single%20refusal%0Atoken%2C%20which%20are%20prepended%20to%20the%20model%27s%20responses%20during%20training.%20We%20then%0Ashow%20how%20to%20increase%20or%20decrease%20the%20probability%20of%20generating%20the%20refusal%0Atoken%20for%20each%20category%20during%20inference%20to%20steer%20the%20model%27s%20refusal%20behavior.%0ARefusal%20tokens%20enable%20controlling%20a%20single%20model%27s%20refusal%20rates%20without%20the%0Aneed%20of%20any%20further%20fine-tuning%2C%20but%20only%20by%20selectively%20intervening%20during%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefusal%2520Tokens%253A%2520A%2520Simple%2520Way%2520to%2520Calibrate%2520Refusals%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DNeel%2520Jain%2520and%2520Aditya%2520Shrivastava%2520and%2520Chenyang%2520Zhu%2520and%2520Daben%2520Liu%2520and%2520Alfy%2520Samuel%2520and%2520Ashwinee%2520Panda%2520and%2520Anoop%2520Kumar%2520and%2520Micah%2520Goldblum%2520and%2520Tom%2520Goldstein%26entry.1292438233%3D%2520%2520A%2520key%2520component%2520of%2520building%2520safe%2520and%2520reliable%2520language%2520models%2520is%2520enabling%2520the%250Amodels%2520to%2520appropriately%2520refuse%2520to%2520follow%2520certain%2520instructions%2520or%2520answer%2520certain%250Aquestions.%2520We%2520may%2520want%2520models%2520to%2520output%2520refusal%2520messages%2520for%2520various%2520categories%250Aof%2520user%2520queries%252C%2520for%2520example%252C%2520ill-posed%2520questions%252C%2520instructions%2520for%2520committing%250Aillegal%2520acts%252C%2520or%2520queries%2520which%2520require%2520information%2520past%2520the%2520model%2527s%2520knowledge%250Ahorizon.%2520Engineering%2520models%2520that%2520refuse%2520to%2520answer%2520such%2520questions%2520is%2520complicated%250Aby%2520the%2520fact%2520that%2520an%2520individual%2520may%2520want%2520their%2520model%2520to%2520exhibit%2520varying%2520levels%250Aof%2520sensitivity%2520for%2520refusing%2520queries%2520of%2520various%2520categories%252C%2520and%2520different%2520users%250Amay%2520want%2520different%2520refusal%2520rates.%2520The%2520current%2520default%2520approach%2520involves%250Atraining%2520multiple%2520models%2520with%2520varying%2520proportions%2520of%2520refusal%2520messages%2520from%2520each%250Acategory%2520to%2520achieve%2520the%2520desired%2520refusal%2520rates%252C%2520which%2520is%2520computationally%250Aexpensive%2520and%2520may%2520require%2520training%2520a%2520new%2520model%2520to%2520accommodate%2520each%2520user%2527s%250Adesired%2520preference%2520over%2520refusal%2520rates.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250Arefusal%2520tokens%252C%2520one%2520such%2520token%2520for%2520each%2520refusal%2520category%2520or%2520a%2520single%2520refusal%250Atoken%252C%2520which%2520are%2520prepended%2520to%2520the%2520model%2527s%2520responses%2520during%2520training.%2520We%2520then%250Ashow%2520how%2520to%2520increase%2520or%2520decrease%2520the%2520probability%2520of%2520generating%2520the%2520refusal%250Atoken%2520for%2520each%2520category%2520during%2520inference%2520to%2520steer%2520the%2520model%2527s%2520refusal%2520behavior.%250ARefusal%2520tokens%2520enable%2520controlling%2520a%2520single%2520model%2527s%2520refusal%2520rates%2520without%2520the%250Aneed%2520of%2520any%2520further%2520fine-tuning%252C%2520but%2520only%2520by%2520selectively%2520intervening%2520during%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refusal%20Tokens%3A%20A%20Simple%20Way%20to%20Calibrate%20Refusals%20in%20Large%20Language%0A%20%20Models&entry.906535625=Neel%20Jain%20and%20Aditya%20Shrivastava%20and%20Chenyang%20Zhu%20and%20Daben%20Liu%20and%20Alfy%20Samuel%20and%20Ashwinee%20Panda%20and%20Anoop%20Kumar%20and%20Micah%20Goldblum%20and%20Tom%20Goldstein&entry.1292438233=%20%20A%20key%20component%20of%20building%20safe%20and%20reliable%20language%20models%20is%20enabling%20the%0Amodels%20to%20appropriately%20refuse%20to%20follow%20certain%20instructions%20or%20answer%20certain%0Aquestions.%20We%20may%20want%20models%20to%20output%20refusal%20messages%20for%20various%20categories%0Aof%20user%20queries%2C%20for%20example%2C%20ill-posed%20questions%2C%20instructions%20for%20committing%0Aillegal%20acts%2C%20or%20queries%20which%20require%20information%20past%20the%20model%27s%20knowledge%0Ahorizon.%20Engineering%20models%20that%20refuse%20to%20answer%20such%20questions%20is%20complicated%0Aby%20the%20fact%20that%20an%20individual%20may%20want%20their%20model%20to%20exhibit%20varying%20levels%0Aof%20sensitivity%20for%20refusing%20queries%20of%20various%20categories%2C%20and%20different%20users%0Amay%20want%20different%20refusal%20rates.%20The%20current%20default%20approach%20involves%0Atraining%20multiple%20models%20with%20varying%20proportions%20of%20refusal%20messages%20from%20each%0Acategory%20to%20achieve%20the%20desired%20refusal%20rates%2C%20which%20is%20computationally%0Aexpensive%20and%20may%20require%20training%20a%20new%20model%20to%20accommodate%20each%20user%27s%0Adesired%20preference%20over%20refusal%20rates.%20To%20address%20these%20challenges%2C%20we%20propose%0Arefusal%20tokens%2C%20one%20such%20token%20for%20each%20refusal%20category%20or%20a%20single%20refusal%0Atoken%2C%20which%20are%20prepended%20to%20the%20model%27s%20responses%20during%20training.%20We%20then%0Ashow%20how%20to%20increase%20or%20decrease%20the%20probability%20of%20generating%20the%20refusal%0Atoken%20for%20each%20category%20during%20inference%20to%20steer%20the%20model%27s%20refusal%20behavior.%0ARefusal%20tokens%20enable%20controlling%20a%20single%20model%27s%20refusal%20rates%20without%20the%0Aneed%20of%20any%20further%20fine-tuning%2C%20but%20only%20by%20selectively%20intervening%20during%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06748v2&entry.124074799=Read"},
{"title": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by\n  Phronesis", "author": "Sweta Kaman and Ankita Sharma and Romi Banerjee", "abstract": "  Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment.\n", "link": "http://arxiv.org/abs/2508.21517v1", "date": "2025-08-29", "relevancy": 1.86, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Wise%20Decision%20Making%3A%20A%20Z-Number%20Fuzzy%20Framework%20Inspired%20by%0A%20%20Phronesis&body=Title%3A%20Modeling%20Wise%20Decision%20Making%3A%20A%20Z-Number%20Fuzzy%20Framework%20Inspired%20by%0A%20%20Phronesis%0AAuthor%3A%20Sweta%20Kaman%20and%20Ankita%20Sharma%20and%20Romi%20Banerjee%0AAbstract%3A%20%20%20Background%3A%20Wisdom%20is%20a%20superordinate%20construct%20that%20embraces%20perspective%0Ataking%2C%20reflectiveness%2C%20prosocial%20orientation%2C%20reflective%20empathetic%20action%2C%0Aand%20intellectual%20humility.%20Unlike%20conventional%20models%20of%20reasoning%20that%20are%0Arigidly%20bound%20by%20binary%20thinking%2C%20wisdom%20unfolds%20in%20shades%20of%20ambiguity%2C%0Arequiring%20both%20graded%20evaluation%20and%20self-reflective%20humility.%20Current%20measures%0Adepend%20on%20self-reports%20and%20seldom%20reflect%20the%20humility%20and%20uncertainty%20inherent%0Ain%20wise%20reasoning.%20A%20computational%20framework%20that%20takes%20into%20account%20both%0Amultidimensionality%20and%20confidence%20has%20the%20potential%20to%20improve%20psychological%0Ascience%20and%20allow%20humane%20AI.%20Method%3A%20We%20present%20a%20fuzzy%20inference%20system%20with%20Z%0Anumbers%2C%20each%20of%20the%20decisions%20being%20expressed%20in%20terms%20of%20a%20wisdom%20score%0A%28restriction%29%20and%20confidence%20score%20%28certainty%29.%20As%20part%20of%20this%20study%2C%0Aparticipants%20%28N%20%3D%20100%29%20were%20exposed%20to%20culturally%20neutral%20pictorial%20moral%0Adilemma%20tasks%20to%20which%20they%20generated%20think-aloud%20linguistic%20responses%2C%20which%0Awere%20mapped%20into%20five%20theoretically%20based%20components%20of%20wisdom.%20The%20scores%20of%0Aeach%20individual%20component%20were%20combined%20using%20a%20base%20of%2021%20rules%2C%20with%0Amembership%20functions%20tuned%20via%20Gaussian%20kernel%20density%20estimation.%20Results%3A%20In%0Aa%20proof%20of%20concept%20study%2C%20the%20system%20produced%20dual%20attribute%20wisdom%0Arepresentations%20that%20correlated%20modestly%20but%20significantly%20with%20established%0Ascales%20while%20showing%20negligible%20relations%20with%20unrelated%20traits%2C%20supporting%0Aconvergent%20and%20divergent%20validity.%20Contribution%3A%20The%20contribution%20is%20to%0Aformalize%20wisdom%20as%20a%20multidimensional%2C%20uncertainty-conscious%20construct%2C%0Aoperationalized%20in%20the%20form%20of%20Z-numbers.%20In%20addition%20to%20progressing%0Ameasurement%20in%20psychology%2C%20it%20calculates%20how%20fuzzy%20Z%20numbers%20can%20provide%20AI%0Asystems%20with%20interpretable%2C%20confidence-sensitive%20reasoning%20that%20affords%20a%20safe%2C%0Amiddle%20ground%20between%20rigorous%20computation%20and%20human-like%20judgment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Wise%2520Decision%2520Making%253A%2520A%2520Z-Number%2520Fuzzy%2520Framework%2520Inspired%2520by%250A%2520%2520Phronesis%26entry.906535625%3DSweta%2520Kaman%2520and%2520Ankita%2520Sharma%2520and%2520Romi%2520Banerjee%26entry.1292438233%3D%2520%2520Background%253A%2520Wisdom%2520is%2520a%2520superordinate%2520construct%2520that%2520embraces%2520perspective%250Ataking%252C%2520reflectiveness%252C%2520prosocial%2520orientation%252C%2520reflective%2520empathetic%2520action%252C%250Aand%2520intellectual%2520humility.%2520Unlike%2520conventional%2520models%2520of%2520reasoning%2520that%2520are%250Arigidly%2520bound%2520by%2520binary%2520thinking%252C%2520wisdom%2520unfolds%2520in%2520shades%2520of%2520ambiguity%252C%250Arequiring%2520both%2520graded%2520evaluation%2520and%2520self-reflective%2520humility.%2520Current%2520measures%250Adepend%2520on%2520self-reports%2520and%2520seldom%2520reflect%2520the%2520humility%2520and%2520uncertainty%2520inherent%250Ain%2520wise%2520reasoning.%2520A%2520computational%2520framework%2520that%2520takes%2520into%2520account%2520both%250Amultidimensionality%2520and%2520confidence%2520has%2520the%2520potential%2520to%2520improve%2520psychological%250Ascience%2520and%2520allow%2520humane%2520AI.%2520Method%253A%2520We%2520present%2520a%2520fuzzy%2520inference%2520system%2520with%2520Z%250Anumbers%252C%2520each%2520of%2520the%2520decisions%2520being%2520expressed%2520in%2520terms%2520of%2520a%2520wisdom%2520score%250A%2528restriction%2529%2520and%2520confidence%2520score%2520%2528certainty%2529.%2520As%2520part%2520of%2520this%2520study%252C%250Aparticipants%2520%2528N%2520%253D%2520100%2529%2520were%2520exposed%2520to%2520culturally%2520neutral%2520pictorial%2520moral%250Adilemma%2520tasks%2520to%2520which%2520they%2520generated%2520think-aloud%2520linguistic%2520responses%252C%2520which%250Awere%2520mapped%2520into%2520five%2520theoretically%2520based%2520components%2520of%2520wisdom.%2520The%2520scores%2520of%250Aeach%2520individual%2520component%2520were%2520combined%2520using%2520a%2520base%2520of%252021%2520rules%252C%2520with%250Amembership%2520functions%2520tuned%2520via%2520Gaussian%2520kernel%2520density%2520estimation.%2520Results%253A%2520In%250Aa%2520proof%2520of%2520concept%2520study%252C%2520the%2520system%2520produced%2520dual%2520attribute%2520wisdom%250Arepresentations%2520that%2520correlated%2520modestly%2520but%2520significantly%2520with%2520established%250Ascales%2520while%2520showing%2520negligible%2520relations%2520with%2520unrelated%2520traits%252C%2520supporting%250Aconvergent%2520and%2520divergent%2520validity.%2520Contribution%253A%2520The%2520contribution%2520is%2520to%250Aformalize%2520wisdom%2520as%2520a%2520multidimensional%252C%2520uncertainty-conscious%2520construct%252C%250Aoperationalized%2520in%2520the%2520form%2520of%2520Z-numbers.%2520In%2520addition%2520to%2520progressing%250Ameasurement%2520in%2520psychology%252C%2520it%2520calculates%2520how%2520fuzzy%2520Z%2520numbers%2520can%2520provide%2520AI%250Asystems%2520with%2520interpretable%252C%2520confidence-sensitive%2520reasoning%2520that%2520affords%2520a%2520safe%252C%250Amiddle%2520ground%2520between%2520rigorous%2520computation%2520and%2520human-like%2520judgment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Wise%20Decision%20Making%3A%20A%20Z-Number%20Fuzzy%20Framework%20Inspired%20by%0A%20%20Phronesis&entry.906535625=Sweta%20Kaman%20and%20Ankita%20Sharma%20and%20Romi%20Banerjee&entry.1292438233=%20%20Background%3A%20Wisdom%20is%20a%20superordinate%20construct%20that%20embraces%20perspective%0Ataking%2C%20reflectiveness%2C%20prosocial%20orientation%2C%20reflective%20empathetic%20action%2C%0Aand%20intellectual%20humility.%20Unlike%20conventional%20models%20of%20reasoning%20that%20are%0Arigidly%20bound%20by%20binary%20thinking%2C%20wisdom%20unfolds%20in%20shades%20of%20ambiguity%2C%0Arequiring%20both%20graded%20evaluation%20and%20self-reflective%20humility.%20Current%20measures%0Adepend%20on%20self-reports%20and%20seldom%20reflect%20the%20humility%20and%20uncertainty%20inherent%0Ain%20wise%20reasoning.%20A%20computational%20framework%20that%20takes%20into%20account%20both%0Amultidimensionality%20and%20confidence%20has%20the%20potential%20to%20improve%20psychological%0Ascience%20and%20allow%20humane%20AI.%20Method%3A%20We%20present%20a%20fuzzy%20inference%20system%20with%20Z%0Anumbers%2C%20each%20of%20the%20decisions%20being%20expressed%20in%20terms%20of%20a%20wisdom%20score%0A%28restriction%29%20and%20confidence%20score%20%28certainty%29.%20As%20part%20of%20this%20study%2C%0Aparticipants%20%28N%20%3D%20100%29%20were%20exposed%20to%20culturally%20neutral%20pictorial%20moral%0Adilemma%20tasks%20to%20which%20they%20generated%20think-aloud%20linguistic%20responses%2C%20which%0Awere%20mapped%20into%20five%20theoretically%20based%20components%20of%20wisdom.%20The%20scores%20of%0Aeach%20individual%20component%20were%20combined%20using%20a%20base%20of%2021%20rules%2C%20with%0Amembership%20functions%20tuned%20via%20Gaussian%20kernel%20density%20estimation.%20Results%3A%20In%0Aa%20proof%20of%20concept%20study%2C%20the%20system%20produced%20dual%20attribute%20wisdom%0Arepresentations%20that%20correlated%20modestly%20but%20significantly%20with%20established%0Ascales%20while%20showing%20negligible%20relations%20with%20unrelated%20traits%2C%20supporting%0Aconvergent%20and%20divergent%20validity.%20Contribution%3A%20The%20contribution%20is%20to%0Aformalize%20wisdom%20as%20a%20multidimensional%2C%20uncertainty-conscious%20construct%2C%0Aoperationalized%20in%20the%20form%20of%20Z-numbers.%20In%20addition%20to%20progressing%0Ameasurement%20in%20psychology%2C%20it%20calculates%20how%20fuzzy%20Z%20numbers%20can%20provide%20AI%0Asystems%20with%20interpretable%2C%20confidence-sensitive%20reasoning%20that%20affords%20a%20safe%2C%0Amiddle%20ground%20between%20rigorous%20computation%20and%20human-like%20judgment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21517v1&entry.124074799=Read"},
{"title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic\n  Evolutionary Approximation", "author": "Vojtech Mrazek and Konstantinos Balaskas and Paula Carolina Lozano Duarte and Zdenek Vasicek and Mehdi B. Tahoori and Georgios Zervakis", "abstract": "  Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs.\n", "link": "http://arxiv.org/abs/2508.19660v2", "date": "2025-08-29", "relevancy": 1.8545, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arbitrary%20Precision%20Printed%20Ternary%20Neural%20Networks%20with%20Holistic%0A%20%20Evolutionary%20Approximation&body=Title%3A%20Arbitrary%20Precision%20Printed%20Ternary%20Neural%20Networks%20with%20Holistic%0A%20%20Evolutionary%20Approximation%0AAuthor%3A%20Vojtech%20Mrazek%20and%20Konstantinos%20Balaskas%20and%20Paula%20Carolina%20Lozano%20Duarte%20and%20Zdenek%20Vasicek%20and%20Mehdi%20B.%20Tahoori%20and%20Georgios%20Zervakis%0AAbstract%3A%20%20%20Printed%20electronics%20offer%20a%20promising%20alternative%20for%20applications%20beyond%0Asilicon-based%20systems%2C%20requiring%20properties%20like%20flexibility%2C%20stretchability%2C%0Aconformality%2C%20and%20ultra-low%20fabrication%20costs.%20Despite%20the%20large%20feature%20sizes%0Ain%20printed%20electronics%2C%20printed%20neural%20networks%20have%20attracted%20attention%20for%0Ameeting%20target%20application%20requirements%2C%20though%20realizing%20complex%20circuits%0Aremains%20challenging.%20This%20work%20bridges%20the%20gap%20between%20classification%20accuracy%0Aand%20area%20efficiency%20in%20printed%20neural%20networks%2C%20covering%20the%20entire%0Aprocessing-near-sensor%20system%20design%20and%20co-optimization%20from%20the%0Aanalog-to-digital%20interface-a%20major%20area%20and%20power%20bottleneck-to%20the%20digital%0Aclassifier.%20We%20propose%20an%20automated%20framework%20for%20designing%20printed%20Ternary%0ANeural%20Networks%20with%20arbitrary%20input%20precision%2C%20utilizing%20multi-objective%0Aoptimization%20and%20holistic%20approximation.%20Our%20circuits%20outperform%20existing%0Aapproximate%20printed%20neural%20networks%20by%2017x%20in%20area%20and%2059x%20in%20power%20on%20average%2C%0Abeing%20the%20first%20to%20enable%20printed-battery-powered%20operation%20with%20under%205%25%0Aaccuracy%20loss%20while%20accounting%20for%20analog-to-digital%20interfacing%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbitrary%2520Precision%2520Printed%2520Ternary%2520Neural%2520Networks%2520with%2520Holistic%250A%2520%2520Evolutionary%2520Approximation%26entry.906535625%3DVojtech%2520Mrazek%2520and%2520Konstantinos%2520Balaskas%2520and%2520Paula%2520Carolina%2520Lozano%2520Duarte%2520and%2520Zdenek%2520Vasicek%2520and%2520Mehdi%2520B.%2520Tahoori%2520and%2520Georgios%2520Zervakis%26entry.1292438233%3D%2520%2520Printed%2520electronics%2520offer%2520a%2520promising%2520alternative%2520for%2520applications%2520beyond%250Asilicon-based%2520systems%252C%2520requiring%2520properties%2520like%2520flexibility%252C%2520stretchability%252C%250Aconformality%252C%2520and%2520ultra-low%2520fabrication%2520costs.%2520Despite%2520the%2520large%2520feature%2520sizes%250Ain%2520printed%2520electronics%252C%2520printed%2520neural%2520networks%2520have%2520attracted%2520attention%2520for%250Ameeting%2520target%2520application%2520requirements%252C%2520though%2520realizing%2520complex%2520circuits%250Aremains%2520challenging.%2520This%2520work%2520bridges%2520the%2520gap%2520between%2520classification%2520accuracy%250Aand%2520area%2520efficiency%2520in%2520printed%2520neural%2520networks%252C%2520covering%2520the%2520entire%250Aprocessing-near-sensor%2520system%2520design%2520and%2520co-optimization%2520from%2520the%250Aanalog-to-digital%2520interface-a%2520major%2520area%2520and%2520power%2520bottleneck-to%2520the%2520digital%250Aclassifier.%2520We%2520propose%2520an%2520automated%2520framework%2520for%2520designing%2520printed%2520Ternary%250ANeural%2520Networks%2520with%2520arbitrary%2520input%2520precision%252C%2520utilizing%2520multi-objective%250Aoptimization%2520and%2520holistic%2520approximation.%2520Our%2520circuits%2520outperform%2520existing%250Aapproximate%2520printed%2520neural%2520networks%2520by%252017x%2520in%2520area%2520and%252059x%2520in%2520power%2520on%2520average%252C%250Abeing%2520the%2520first%2520to%2520enable%2520printed-battery-powered%2520operation%2520with%2520under%25205%2525%250Aaccuracy%2520loss%2520while%2520accounting%2520for%2520analog-to-digital%2520interfacing%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arbitrary%20Precision%20Printed%20Ternary%20Neural%20Networks%20with%20Holistic%0A%20%20Evolutionary%20Approximation&entry.906535625=Vojtech%20Mrazek%20and%20Konstantinos%20Balaskas%20and%20Paula%20Carolina%20Lozano%20Duarte%20and%20Zdenek%20Vasicek%20and%20Mehdi%20B.%20Tahoori%20and%20Georgios%20Zervakis&entry.1292438233=%20%20Printed%20electronics%20offer%20a%20promising%20alternative%20for%20applications%20beyond%0Asilicon-based%20systems%2C%20requiring%20properties%20like%20flexibility%2C%20stretchability%2C%0Aconformality%2C%20and%20ultra-low%20fabrication%20costs.%20Despite%20the%20large%20feature%20sizes%0Ain%20printed%20electronics%2C%20printed%20neural%20networks%20have%20attracted%20attention%20for%0Ameeting%20target%20application%20requirements%2C%20though%20realizing%20complex%20circuits%0Aremains%20challenging.%20This%20work%20bridges%20the%20gap%20between%20classification%20accuracy%0Aand%20area%20efficiency%20in%20printed%20neural%20networks%2C%20covering%20the%20entire%0Aprocessing-near-sensor%20system%20design%20and%20co-optimization%20from%20the%0Aanalog-to-digital%20interface-a%20major%20area%20and%20power%20bottleneck-to%20the%20digital%0Aclassifier.%20We%20propose%20an%20automated%20framework%20for%20designing%20printed%20Ternary%0ANeural%20Networks%20with%20arbitrary%20input%20precision%2C%20utilizing%20multi-objective%0Aoptimization%20and%20holistic%20approximation.%20Our%20circuits%20outperform%20existing%0Aapproximate%20printed%20neural%20networks%20by%2017x%20in%20area%20and%2059x%20in%20power%20on%20average%2C%0Abeing%20the%20first%20to%20enable%20printed-battery-powered%20operation%20with%20under%205%25%0Aaccuracy%20loss%20while%20accounting%20for%20analog-to-digital%20interfacing%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19660v2&entry.124074799=Read"},
{"title": "Integrating Pathology and CT Imaging for Personalized Recurrence Risk\n  Prediction in Renal Cancer", "author": "Dani\u00ebl Boeke and Cedrik Blommestijn and Rebecca N. Wray and Kalina Chupetlovska and Shangqi Gao and Zeyu Gao and Regina G. H. Beets-Tan and Mireia Crispin-Ortuzar and James O. Jones and Wilson Silva and Ines P. Machado", "abstract": "  Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is\nessential for guiding postoperative surveillance and treatment. The Leibovich\nscore remains widely used for stratifying distant recurrence risk but offers\nlimited patient-level resolution and excludes imaging information. This study\nevaluates multimodal recurrence prediction by integrating preoperative computed\ntomography (CT) and postoperative histopathology whole-slide images (WSIs). A\nmodular deep learning framework with pretrained encoders and Cox-based survival\nmodeling was tested across unimodal, late fusion, and intermediate fusion\nsetups. In a real-world ccRCC cohort, WSI-based models consistently\noutperformed CT-only models, underscoring the prognostic strength of pathology.\nIntermediate fusion further improved performance, with the best model\n(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random\ntie-breaking narrowed the gap between the clinical baseline and learned models,\nsuggesting discretization may overstate individualized performance. Using\nsimple embedding concatenation, radiology added value primarily through fusion.\nThese findings demonstrate the feasibility of foundation model-based multimodal\nintegration for personalized ccRCC risk prediction. Future work should explore\nmore expressive fusion strategies, larger multimodal datasets, and\ngeneral-purpose CT encoders to better match pathology modeling capacity.\n", "link": "http://arxiv.org/abs/2508.21581v1", "date": "2025-08-29", "relevancy": 1.8515, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Pathology%20and%20CT%20Imaging%20for%20Personalized%20Recurrence%20Risk%0A%20%20Prediction%20in%20Renal%20Cancer&body=Title%3A%20Integrating%20Pathology%20and%20CT%20Imaging%20for%20Personalized%20Recurrence%20Risk%0A%20%20Prediction%20in%20Renal%20Cancer%0AAuthor%3A%20Dani%C3%ABl%20Boeke%20and%20Cedrik%20Blommestijn%20and%20Rebecca%20N.%20Wray%20and%20Kalina%20Chupetlovska%20and%20Shangqi%20Gao%20and%20Zeyu%20Gao%20and%20Regina%20G.%20H.%20Beets-Tan%20and%20Mireia%20Crispin-Ortuzar%20and%20James%20O.%20Jones%20and%20Wilson%20Silva%20and%20Ines%20P.%20Machado%0AAbstract%3A%20%20%20Recurrence%20risk%20estimation%20in%20clear%20cell%20renal%20cell%20carcinoma%20%28ccRCC%29%20is%0Aessential%20for%20guiding%20postoperative%20surveillance%20and%20treatment.%20The%20Leibovich%0Ascore%20remains%20widely%20used%20for%20stratifying%20distant%20recurrence%20risk%20but%20offers%0Alimited%20patient-level%20resolution%20and%20excludes%20imaging%20information.%20This%20study%0Aevaluates%20multimodal%20recurrence%20prediction%20by%20integrating%20preoperative%20computed%0Atomography%20%28CT%29%20and%20postoperative%20histopathology%20whole-slide%20images%20%28WSIs%29.%20A%0Amodular%20deep%20learning%20framework%20with%20pretrained%20encoders%20and%20Cox-based%20survival%0Amodeling%20was%20tested%20across%20unimodal%2C%20late%20fusion%2C%20and%20intermediate%20fusion%0Asetups.%20In%20a%20real-world%20ccRCC%20cohort%2C%20WSI-based%20models%20consistently%0Aoutperformed%20CT-only%20models%2C%20underscoring%20the%20prognostic%20strength%20of%20pathology.%0AIntermediate%20fusion%20further%20improved%20performance%2C%20with%20the%20best%20model%0A%28TITAN-CONCH%20with%20ResNet-18%29%20approaching%20the%20adjusted%20Leibovich%20score.%20Random%0Atie-breaking%20narrowed%20the%20gap%20between%20the%20clinical%20baseline%20and%20learned%20models%2C%0Asuggesting%20discretization%20may%20overstate%20individualized%20performance.%20Using%0Asimple%20embedding%20concatenation%2C%20radiology%20added%20value%20primarily%20through%20fusion.%0AThese%20findings%20demonstrate%20the%20feasibility%20of%20foundation%20model-based%20multimodal%0Aintegration%20for%20personalized%20ccRCC%20risk%20prediction.%20Future%20work%20should%20explore%0Amore%20expressive%20fusion%20strategies%2C%20larger%20multimodal%20datasets%2C%20and%0Ageneral-purpose%20CT%20encoders%20to%20better%20match%20pathology%20modeling%20capacity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Pathology%2520and%2520CT%2520Imaging%2520for%2520Personalized%2520Recurrence%2520Risk%250A%2520%2520Prediction%2520in%2520Renal%2520Cancer%26entry.906535625%3DDani%25C3%25ABl%2520Boeke%2520and%2520Cedrik%2520Blommestijn%2520and%2520Rebecca%2520N.%2520Wray%2520and%2520Kalina%2520Chupetlovska%2520and%2520Shangqi%2520Gao%2520and%2520Zeyu%2520Gao%2520and%2520Regina%2520G.%2520H.%2520Beets-Tan%2520and%2520Mireia%2520Crispin-Ortuzar%2520and%2520James%2520O.%2520Jones%2520and%2520Wilson%2520Silva%2520and%2520Ines%2520P.%2520Machado%26entry.1292438233%3D%2520%2520Recurrence%2520risk%2520estimation%2520in%2520clear%2520cell%2520renal%2520cell%2520carcinoma%2520%2528ccRCC%2529%2520is%250Aessential%2520for%2520guiding%2520postoperative%2520surveillance%2520and%2520treatment.%2520The%2520Leibovich%250Ascore%2520remains%2520widely%2520used%2520for%2520stratifying%2520distant%2520recurrence%2520risk%2520but%2520offers%250Alimited%2520patient-level%2520resolution%2520and%2520excludes%2520imaging%2520information.%2520This%2520study%250Aevaluates%2520multimodal%2520recurrence%2520prediction%2520by%2520integrating%2520preoperative%2520computed%250Atomography%2520%2528CT%2529%2520and%2520postoperative%2520histopathology%2520whole-slide%2520images%2520%2528WSIs%2529.%2520A%250Amodular%2520deep%2520learning%2520framework%2520with%2520pretrained%2520encoders%2520and%2520Cox-based%2520survival%250Amodeling%2520was%2520tested%2520across%2520unimodal%252C%2520late%2520fusion%252C%2520and%2520intermediate%2520fusion%250Asetups.%2520In%2520a%2520real-world%2520ccRCC%2520cohort%252C%2520WSI-based%2520models%2520consistently%250Aoutperformed%2520CT-only%2520models%252C%2520underscoring%2520the%2520prognostic%2520strength%2520of%2520pathology.%250AIntermediate%2520fusion%2520further%2520improved%2520performance%252C%2520with%2520the%2520best%2520model%250A%2528TITAN-CONCH%2520with%2520ResNet-18%2529%2520approaching%2520the%2520adjusted%2520Leibovich%2520score.%2520Random%250Atie-breaking%2520narrowed%2520the%2520gap%2520between%2520the%2520clinical%2520baseline%2520and%2520learned%2520models%252C%250Asuggesting%2520discretization%2520may%2520overstate%2520individualized%2520performance.%2520Using%250Asimple%2520embedding%2520concatenation%252C%2520radiology%2520added%2520value%2520primarily%2520through%2520fusion.%250AThese%2520findings%2520demonstrate%2520the%2520feasibility%2520of%2520foundation%2520model-based%2520multimodal%250Aintegration%2520for%2520personalized%2520ccRCC%2520risk%2520prediction.%2520Future%2520work%2520should%2520explore%250Amore%2520expressive%2520fusion%2520strategies%252C%2520larger%2520multimodal%2520datasets%252C%2520and%250Ageneral-purpose%2520CT%2520encoders%2520to%2520better%2520match%2520pathology%2520modeling%2520capacity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Pathology%20and%20CT%20Imaging%20for%20Personalized%20Recurrence%20Risk%0A%20%20Prediction%20in%20Renal%20Cancer&entry.906535625=Dani%C3%ABl%20Boeke%20and%20Cedrik%20Blommestijn%20and%20Rebecca%20N.%20Wray%20and%20Kalina%20Chupetlovska%20and%20Shangqi%20Gao%20and%20Zeyu%20Gao%20and%20Regina%20G.%20H.%20Beets-Tan%20and%20Mireia%20Crispin-Ortuzar%20and%20James%20O.%20Jones%20and%20Wilson%20Silva%20and%20Ines%20P.%20Machado&entry.1292438233=%20%20Recurrence%20risk%20estimation%20in%20clear%20cell%20renal%20cell%20carcinoma%20%28ccRCC%29%20is%0Aessential%20for%20guiding%20postoperative%20surveillance%20and%20treatment.%20The%20Leibovich%0Ascore%20remains%20widely%20used%20for%20stratifying%20distant%20recurrence%20risk%20but%20offers%0Alimited%20patient-level%20resolution%20and%20excludes%20imaging%20information.%20This%20study%0Aevaluates%20multimodal%20recurrence%20prediction%20by%20integrating%20preoperative%20computed%0Atomography%20%28CT%29%20and%20postoperative%20histopathology%20whole-slide%20images%20%28WSIs%29.%20A%0Amodular%20deep%20learning%20framework%20with%20pretrained%20encoders%20and%20Cox-based%20survival%0Amodeling%20was%20tested%20across%20unimodal%2C%20late%20fusion%2C%20and%20intermediate%20fusion%0Asetups.%20In%20a%20real-world%20ccRCC%20cohort%2C%20WSI-based%20models%20consistently%0Aoutperformed%20CT-only%20models%2C%20underscoring%20the%20prognostic%20strength%20of%20pathology.%0AIntermediate%20fusion%20further%20improved%20performance%2C%20with%20the%20best%20model%0A%28TITAN-CONCH%20with%20ResNet-18%29%20approaching%20the%20adjusted%20Leibovich%20score.%20Random%0Atie-breaking%20narrowed%20the%20gap%20between%20the%20clinical%20baseline%20and%20learned%20models%2C%0Asuggesting%20discretization%20may%20overstate%20individualized%20performance.%20Using%0Asimple%20embedding%20concatenation%2C%20radiology%20added%20value%20primarily%20through%20fusion.%0AThese%20findings%20demonstrate%20the%20feasibility%20of%20foundation%20model-based%20multimodal%0Aintegration%20for%20personalized%20ccRCC%20risk%20prediction.%20Future%20work%20should%20explore%0Amore%20expressive%20fusion%20strategies%2C%20larger%20multimodal%20datasets%2C%20and%0Ageneral-purpose%20CT%20encoders%20to%20better%20match%20pathology%20modeling%20capacity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21581v1&entry.124074799=Read"},
{"title": "Mapping like a Skeptic: Probabilistic BEV Projection for Online HD\n  Mapping", "author": "Fatih Erdo\u011fan and Merve Rabia Bar\u0131n and Fatma G\u00fcney", "abstract": "  Constructing high-definition (HD) maps from sensory input requires accurately\nmapping the road elements in image space to the Bird's Eye View (BEV) space.\nThe precision of this mapping directly impacts the quality of the final\nvectorized HD map. Existing HD mapping approaches outsource the projection to\nstandard mapping techniques, such as attention-based ones. However, these\nmethods struggle with accuracy due to generalization problems, often\nhallucinating non-existent road elements. Our key idea is to start with a\ngeometric mapping based on camera parameters and adapt it to the scene to\nextract relevant map information from camera images. To implement this, we\npropose a novel probabilistic projection mechanism with confidence scores to\n(i) refine the mapping to better align with the scene and (ii) filter out\nirrelevant elements that should not influence HD map generation. In addition,\nwe improve temporal processing by using confidence scores to selectively\naccumulate reliable information over time. Experiments on new splits of the\nnuScenes and Argoverse2 datasets demonstrate improved performance over\nstate-of-the-art approaches, indicating better generalization. The improvements\nare particularly pronounced on nuScenes and in the challenging long perception\nrange. Our code and model checkpoints are available at\nhttps://github.com/Fatih-Erdogan/mapping-like-skeptic .\n", "link": "http://arxiv.org/abs/2508.21689v1", "date": "2025-08-29", "relevancy": 1.8419, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5998}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20like%20a%20Skeptic%3A%20Probabilistic%20BEV%20Projection%20for%20Online%20HD%0A%20%20Mapping&body=Title%3A%20Mapping%20like%20a%20Skeptic%3A%20Probabilistic%20BEV%20Projection%20for%20Online%20HD%0A%20%20Mapping%0AAuthor%3A%20Fatih%20Erdo%C4%9Fan%20and%20Merve%20Rabia%20Bar%C4%B1n%20and%20Fatma%20G%C3%BCney%0AAbstract%3A%20%20%20Constructing%20high-definition%20%28HD%29%20maps%20from%20sensory%20input%20requires%20accurately%0Amapping%20the%20road%20elements%20in%20image%20space%20to%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20space.%0AThe%20precision%20of%20this%20mapping%20directly%20impacts%20the%20quality%20of%20the%20final%0Avectorized%20HD%20map.%20Existing%20HD%20mapping%20approaches%20outsource%20the%20projection%20to%0Astandard%20mapping%20techniques%2C%20such%20as%20attention-based%20ones.%20However%2C%20these%0Amethods%20struggle%20with%20accuracy%20due%20to%20generalization%20problems%2C%20often%0Ahallucinating%20non-existent%20road%20elements.%20Our%20key%20idea%20is%20to%20start%20with%20a%0Ageometric%20mapping%20based%20on%20camera%20parameters%20and%20adapt%20it%20to%20the%20scene%20to%0Aextract%20relevant%20map%20information%20from%20camera%20images.%20To%20implement%20this%2C%20we%0Apropose%20a%20novel%20probabilistic%20projection%20mechanism%20with%20confidence%20scores%20to%0A%28i%29%20refine%20the%20mapping%20to%20better%20align%20with%20the%20scene%20and%20%28ii%29%20filter%20out%0Airrelevant%20elements%20that%20should%20not%20influence%20HD%20map%20generation.%20In%20addition%2C%0Awe%20improve%20temporal%20processing%20by%20using%20confidence%20scores%20to%20selectively%0Aaccumulate%20reliable%20information%20over%20time.%20Experiments%20on%20new%20splits%20of%20the%0AnuScenes%20and%20Argoverse2%20datasets%20demonstrate%20improved%20performance%20over%0Astate-of-the-art%20approaches%2C%20indicating%20better%20generalization.%20The%20improvements%0Aare%20particularly%20pronounced%20on%20nuScenes%20and%20in%20the%20challenging%20long%20perception%0Arange.%20Our%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/Fatih-Erdogan/mapping-like-skeptic%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520like%2520a%2520Skeptic%253A%2520Probabilistic%2520BEV%2520Projection%2520for%2520Online%2520HD%250A%2520%2520Mapping%26entry.906535625%3DFatih%2520Erdo%25C4%259Fan%2520and%2520Merve%2520Rabia%2520Bar%25C4%25B1n%2520and%2520Fatma%2520G%25C3%25BCney%26entry.1292438233%3D%2520%2520Constructing%2520high-definition%2520%2528HD%2529%2520maps%2520from%2520sensory%2520input%2520requires%2520accurately%250Amapping%2520the%2520road%2520elements%2520in%2520image%2520space%2520to%2520the%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520space.%250AThe%2520precision%2520of%2520this%2520mapping%2520directly%2520impacts%2520the%2520quality%2520of%2520the%2520final%250Avectorized%2520HD%2520map.%2520Existing%2520HD%2520mapping%2520approaches%2520outsource%2520the%2520projection%2520to%250Astandard%2520mapping%2520techniques%252C%2520such%2520as%2520attention-based%2520ones.%2520However%252C%2520these%250Amethods%2520struggle%2520with%2520accuracy%2520due%2520to%2520generalization%2520problems%252C%2520often%250Ahallucinating%2520non-existent%2520road%2520elements.%2520Our%2520key%2520idea%2520is%2520to%2520start%2520with%2520a%250Ageometric%2520mapping%2520based%2520on%2520camera%2520parameters%2520and%2520adapt%2520it%2520to%2520the%2520scene%2520to%250Aextract%2520relevant%2520map%2520information%2520from%2520camera%2520images.%2520To%2520implement%2520this%252C%2520we%250Apropose%2520a%2520novel%2520probabilistic%2520projection%2520mechanism%2520with%2520confidence%2520scores%2520to%250A%2528i%2529%2520refine%2520the%2520mapping%2520to%2520better%2520align%2520with%2520the%2520scene%2520and%2520%2528ii%2529%2520filter%2520out%250Airrelevant%2520elements%2520that%2520should%2520not%2520influence%2520HD%2520map%2520generation.%2520In%2520addition%252C%250Awe%2520improve%2520temporal%2520processing%2520by%2520using%2520confidence%2520scores%2520to%2520selectively%250Aaccumulate%2520reliable%2520information%2520over%2520time.%2520Experiments%2520on%2520new%2520splits%2520of%2520the%250AnuScenes%2520and%2520Argoverse2%2520datasets%2520demonstrate%2520improved%2520performance%2520over%250Astate-of-the-art%2520approaches%252C%2520indicating%2520better%2520generalization.%2520The%2520improvements%250Aare%2520particularly%2520pronounced%2520on%2520nuScenes%2520and%2520in%2520the%2520challenging%2520long%2520perception%250Arange.%2520Our%2520code%2520and%2520model%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/Fatih-Erdogan/mapping-like-skeptic%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20like%20a%20Skeptic%3A%20Probabilistic%20BEV%20Projection%20for%20Online%20HD%0A%20%20Mapping&entry.906535625=Fatih%20Erdo%C4%9Fan%20and%20Merve%20Rabia%20Bar%C4%B1n%20and%20Fatma%20G%C3%BCney&entry.1292438233=%20%20Constructing%20high-definition%20%28HD%29%20maps%20from%20sensory%20input%20requires%20accurately%0Amapping%20the%20road%20elements%20in%20image%20space%20to%20the%20Bird%27s%20Eye%20View%20%28BEV%29%20space.%0AThe%20precision%20of%20this%20mapping%20directly%20impacts%20the%20quality%20of%20the%20final%0Avectorized%20HD%20map.%20Existing%20HD%20mapping%20approaches%20outsource%20the%20projection%20to%0Astandard%20mapping%20techniques%2C%20such%20as%20attention-based%20ones.%20However%2C%20these%0Amethods%20struggle%20with%20accuracy%20due%20to%20generalization%20problems%2C%20often%0Ahallucinating%20non-existent%20road%20elements.%20Our%20key%20idea%20is%20to%20start%20with%20a%0Ageometric%20mapping%20based%20on%20camera%20parameters%20and%20adapt%20it%20to%20the%20scene%20to%0Aextract%20relevant%20map%20information%20from%20camera%20images.%20To%20implement%20this%2C%20we%0Apropose%20a%20novel%20probabilistic%20projection%20mechanism%20with%20confidence%20scores%20to%0A%28i%29%20refine%20the%20mapping%20to%20better%20align%20with%20the%20scene%20and%20%28ii%29%20filter%20out%0Airrelevant%20elements%20that%20should%20not%20influence%20HD%20map%20generation.%20In%20addition%2C%0Awe%20improve%20temporal%20processing%20by%20using%20confidence%20scores%20to%20selectively%0Aaccumulate%20reliable%20information%20over%20time.%20Experiments%20on%20new%20splits%20of%20the%0AnuScenes%20and%20Argoverse2%20datasets%20demonstrate%20improved%20performance%20over%0Astate-of-the-art%20approaches%2C%20indicating%20better%20generalization.%20The%20improvements%0Aare%20particularly%20pronounced%20on%20nuScenes%20and%20in%20the%20challenging%20long%20perception%0Arange.%20Our%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/Fatih-Erdogan/mapping-like-skeptic%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21689v1&entry.124074799=Read"},
{"title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of\n  Model Stealing Attacks", "author": "Daryna Oliynyk and Rudolf Mayer and Kathrin Grosse and Andreas Rauber", "abstract": "  Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks.\n", "link": "http://arxiv.org/abs/2508.21654v1", "date": "2025-08-29", "relevancy": 1.8395, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4739}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Stolenly%20Swear%20That%20I%20Am%20Up%20to%20%28No%29%20Good%3A%20Design%20and%20Evaluation%20of%0A%20%20Model%20Stealing%20Attacks&body=Title%3A%20I%20Stolenly%20Swear%20That%20I%20Am%20Up%20to%20%28No%29%20Good%3A%20Design%20and%20Evaluation%20of%0A%20%20Model%20Stealing%20Attacks%0AAuthor%3A%20Daryna%20Oliynyk%20and%20Rudolf%20Mayer%20and%20Kathrin%20Grosse%20and%20Andreas%20Rauber%0AAbstract%3A%20%20%20Model%20stealing%20attacks%20endanger%20the%20confidentiality%20of%20machine%20learning%0Amodels%20offered%20as%20a%20service.%20Although%20these%20models%20are%20kept%20secret%2C%20a%20malicious%0Aparty%20can%20query%20a%20model%20to%20label%20data%20samples%20and%20train%20their%20own%20substitute%0Amodel%2C%20violating%20intellectual%20property.%20While%20novel%20attacks%20in%20the%20field%20are%0Acontinually%20being%20published%2C%20their%20design%20and%20evaluations%20are%20not%20standardised%2C%0Amaking%20it%20challenging%20to%20compare%20prior%20works%20and%20assess%20progress%20in%20the%20field.%0AThis%20paper%20is%20the%20first%20to%20address%20this%20gap%20by%20providing%20recommendations%20for%0Adesigning%20and%20evaluating%20model%20stealing%20attacks.%20To%20this%20end%2C%20we%20study%20the%0Alargest%20group%20of%20attacks%20that%20rely%20on%20training%20a%20substitute%20model%20--%20those%0Aattacking%20image%20classification%20models.%20We%20propose%20the%20first%20comprehensive%0Athreat%20model%20and%20develop%20a%20framework%20for%20attack%20comparison.%20Further%2C%20we%20analyse%0Aattack%20setups%20from%20related%20works%20to%20understand%20which%20tasks%20and%20models%20have%20been%0Astudied%20the%20most.%20Based%20on%20our%20findings%2C%20we%20present%20best%20practices%20for%20attack%0Adevelopment%20before%2C%20during%2C%20and%20beyond%20experiments%20and%20derive%20an%20extensive%20list%0Aof%20open%20research%20questions%20regarding%20the%20evaluation%20of%20model%20stealing%20attacks.%0AOur%20findings%20and%20recommendations%20also%20transfer%20to%20other%20problem%20domains%2C%20hence%0Aestablishing%20the%20first%20generic%20evaluation%20methodology%20for%20model%20stealing%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Stolenly%2520Swear%2520That%2520I%2520Am%2520Up%2520to%2520%2528No%2529%2520Good%253A%2520Design%2520and%2520Evaluation%2520of%250A%2520%2520Model%2520Stealing%2520Attacks%26entry.906535625%3DDaryna%2520Oliynyk%2520and%2520Rudolf%2520Mayer%2520and%2520Kathrin%2520Grosse%2520and%2520Andreas%2520Rauber%26entry.1292438233%3D%2520%2520Model%2520stealing%2520attacks%2520endanger%2520the%2520confidentiality%2520of%2520machine%2520learning%250Amodels%2520offered%2520as%2520a%2520service.%2520Although%2520these%2520models%2520are%2520kept%2520secret%252C%2520a%2520malicious%250Aparty%2520can%2520query%2520a%2520model%2520to%2520label%2520data%2520samples%2520and%2520train%2520their%2520own%2520substitute%250Amodel%252C%2520violating%2520intellectual%2520property.%2520While%2520novel%2520attacks%2520in%2520the%2520field%2520are%250Acontinually%2520being%2520published%252C%2520their%2520design%2520and%2520evaluations%2520are%2520not%2520standardised%252C%250Amaking%2520it%2520challenging%2520to%2520compare%2520prior%2520works%2520and%2520assess%2520progress%2520in%2520the%2520field.%250AThis%2520paper%2520is%2520the%2520first%2520to%2520address%2520this%2520gap%2520by%2520providing%2520recommendations%2520for%250Adesigning%2520and%2520evaluating%2520model%2520stealing%2520attacks.%2520To%2520this%2520end%252C%2520we%2520study%2520the%250Alargest%2520group%2520of%2520attacks%2520that%2520rely%2520on%2520training%2520a%2520substitute%2520model%2520--%2520those%250Aattacking%2520image%2520classification%2520models.%2520We%2520propose%2520the%2520first%2520comprehensive%250Athreat%2520model%2520and%2520develop%2520a%2520framework%2520for%2520attack%2520comparison.%2520Further%252C%2520we%2520analyse%250Aattack%2520setups%2520from%2520related%2520works%2520to%2520understand%2520which%2520tasks%2520and%2520models%2520have%2520been%250Astudied%2520the%2520most.%2520Based%2520on%2520our%2520findings%252C%2520we%2520present%2520best%2520practices%2520for%2520attack%250Adevelopment%2520before%252C%2520during%252C%2520and%2520beyond%2520experiments%2520and%2520derive%2520an%2520extensive%2520list%250Aof%2520open%2520research%2520questions%2520regarding%2520the%2520evaluation%2520of%2520model%2520stealing%2520attacks.%250AOur%2520findings%2520and%2520recommendations%2520also%2520transfer%2520to%2520other%2520problem%2520domains%252C%2520hence%250Aestablishing%2520the%2520first%2520generic%2520evaluation%2520methodology%2520for%2520model%2520stealing%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Stolenly%20Swear%20That%20I%20Am%20Up%20to%20%28No%29%20Good%3A%20Design%20and%20Evaluation%20of%0A%20%20Model%20Stealing%20Attacks&entry.906535625=Daryna%20Oliynyk%20and%20Rudolf%20Mayer%20and%20Kathrin%20Grosse%20and%20Andreas%20Rauber&entry.1292438233=%20%20Model%20stealing%20attacks%20endanger%20the%20confidentiality%20of%20machine%20learning%0Amodels%20offered%20as%20a%20service.%20Although%20these%20models%20are%20kept%20secret%2C%20a%20malicious%0Aparty%20can%20query%20a%20model%20to%20label%20data%20samples%20and%20train%20their%20own%20substitute%0Amodel%2C%20violating%20intellectual%20property.%20While%20novel%20attacks%20in%20the%20field%20are%0Acontinually%20being%20published%2C%20their%20design%20and%20evaluations%20are%20not%20standardised%2C%0Amaking%20it%20challenging%20to%20compare%20prior%20works%20and%20assess%20progress%20in%20the%20field.%0AThis%20paper%20is%20the%20first%20to%20address%20this%20gap%20by%20providing%20recommendations%20for%0Adesigning%20and%20evaluating%20model%20stealing%20attacks.%20To%20this%20end%2C%20we%20study%20the%0Alargest%20group%20of%20attacks%20that%20rely%20on%20training%20a%20substitute%20model%20--%20those%0Aattacking%20image%20classification%20models.%20We%20propose%20the%20first%20comprehensive%0Athreat%20model%20and%20develop%20a%20framework%20for%20attack%20comparison.%20Further%2C%20we%20analyse%0Aattack%20setups%20from%20related%20works%20to%20understand%20which%20tasks%20and%20models%20have%20been%0Astudied%20the%20most.%20Based%20on%20our%20findings%2C%20we%20present%20best%20practices%20for%20attack%0Adevelopment%20before%2C%20during%2C%20and%20beyond%20experiments%20and%20derive%20an%20extensive%20list%0Aof%20open%20research%20questions%20regarding%20the%20evaluation%20of%20model%20stealing%20attacks.%0AOur%20findings%20and%20recommendations%20also%20transfer%20to%20other%20problem%20domains%2C%20hence%0Aestablishing%20the%20first%20generic%20evaluation%20methodology%20for%20model%20stealing%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21654v1&entry.124074799=Read"},
{"title": "SAGA: A Security Architecture for Governing AI Agentic Systems", "author": "Georgios Syros and Anshuman Suri and Jacob Ginesin and Cristina Nita-Rotaru and Alina Oprea", "abstract": "  Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management.\n  To address this gap, we propose SAGA, a scalable Security Architecture for\nGoverning Agentic systems, that offers user oversight over their agents'\nlifecycle. In our design, users register their agents with a central entity,\nthe Provider, that maintains agent contact information, user-defined access\ncontrol policies, and helps agents enforce these policies on inter-agent\ncommunication. We introduce a cryptographic mechanism for deriving access\ncontrol tokens, that offers fine-grained control over an agent's interaction\nwith other agents, providing formal security guarantees. We evaluate SAGA on\nseveral agentic tasks, using agents in different geolocations, and multiple\non-device and cloud LLMs, demonstrating minimal performance overhead with no\nimpact on underlying task utility in a wide range of conditions. Our\narchitecture enables secure and trustworthy deployment of autonomous agents,\naccelerating the responsible adoption of this technology in sensitive\nenvironments.\n", "link": "http://arxiv.org/abs/2504.21034v2", "date": "2025-08-29", "relevancy": 1.8386, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4953}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4724}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGA%3A%20A%20Security%20Architecture%20for%20Governing%20AI%20Agentic%20Systems&body=Title%3A%20SAGA%3A%20A%20Security%20Architecture%20for%20Governing%20AI%20Agentic%20Systems%0AAuthor%3A%20Georgios%20Syros%20and%20Anshuman%20Suri%20and%20Jacob%20Ginesin%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20agents%20increasingly%20interact%2C%20collaborate%2C%0Aand%20delegate%20tasks%20to%20one%20another%20autonomously%20with%20minimal%20human%20interaction.%0AIndustry%20guidelines%20for%20agentic%20system%20governance%20emphasize%20the%20need%20for%20users%0Ato%20maintain%20comprehensive%20control%20over%20their%20agents%2C%20mitigating%20potential%0Adamage%20from%20malicious%20agents.%20Several%20proposed%20agentic%20system%20designs%20address%0Aagent%20identity%2C%20authorization%2C%20and%20delegation%2C%20but%20remain%20purely%20theoretical%2C%0Awithout%20concrete%20implementation%20and%20evaluation.%20Most%20importantly%2C%20they%20do%20not%0Aprovide%20user-controlled%20agent%20management.%0A%20%20To%20address%20this%20gap%2C%20we%20propose%20SAGA%2C%20a%20scalable%20Security%20Architecture%20for%0AGoverning%20Agentic%20systems%2C%20that%20offers%20user%20oversight%20over%20their%20agents%27%0Alifecycle.%20In%20our%20design%2C%20users%20register%20their%20agents%20with%20a%20central%20entity%2C%0Athe%20Provider%2C%20that%20maintains%20agent%20contact%20information%2C%20user-defined%20access%0Acontrol%20policies%2C%20and%20helps%20agents%20enforce%20these%20policies%20on%20inter-agent%0Acommunication.%20We%20introduce%20a%20cryptographic%20mechanism%20for%20deriving%20access%0Acontrol%20tokens%2C%20that%20offers%20fine-grained%20control%20over%20an%20agent%27s%20interaction%0Awith%20other%20agents%2C%20providing%20formal%20security%20guarantees.%20We%20evaluate%20SAGA%20on%0Aseveral%20agentic%20tasks%2C%20using%20agents%20in%20different%20geolocations%2C%20and%20multiple%0Aon-device%20and%20cloud%20LLMs%2C%20demonstrating%20minimal%20performance%20overhead%20with%20no%0Aimpact%20on%20underlying%20task%20utility%20in%20a%20wide%20range%20of%20conditions.%20Our%0Aarchitecture%20enables%20secure%20and%20trustworthy%20deployment%20of%20autonomous%20agents%2C%0Aaccelerating%20the%20responsible%20adoption%20of%20this%20technology%20in%20sensitive%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGA%253A%2520A%2520Security%2520Architecture%2520for%2520Governing%2520AI%2520Agentic%2520Systems%26entry.906535625%3DGeorgios%2520Syros%2520and%2520Anshuman%2520Suri%2520and%2520Jacob%2520Ginesin%2520and%2520Cristina%2520Nita-Rotaru%2520and%2520Alina%2520Oprea%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520agents%2520increasingly%2520interact%252C%2520collaborate%252C%250Aand%2520delegate%2520tasks%2520to%2520one%2520another%2520autonomously%2520with%2520minimal%2520human%2520interaction.%250AIndustry%2520guidelines%2520for%2520agentic%2520system%2520governance%2520emphasize%2520the%2520need%2520for%2520users%250Ato%2520maintain%2520comprehensive%2520control%2520over%2520their%2520agents%252C%2520mitigating%2520potential%250Adamage%2520from%2520malicious%2520agents.%2520Several%2520proposed%2520agentic%2520system%2520designs%2520address%250Aagent%2520identity%252C%2520authorization%252C%2520and%2520delegation%252C%2520but%2520remain%2520purely%2520theoretical%252C%250Awithout%2520concrete%2520implementation%2520and%2520evaluation.%2520Most%2520importantly%252C%2520they%2520do%2520not%250Aprovide%2520user-controlled%2520agent%2520management.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520SAGA%252C%2520a%2520scalable%2520Security%2520Architecture%2520for%250AGoverning%2520Agentic%2520systems%252C%2520that%2520offers%2520user%2520oversight%2520over%2520their%2520agents%2527%250Alifecycle.%2520In%2520our%2520design%252C%2520users%2520register%2520their%2520agents%2520with%2520a%2520central%2520entity%252C%250Athe%2520Provider%252C%2520that%2520maintains%2520agent%2520contact%2520information%252C%2520user-defined%2520access%250Acontrol%2520policies%252C%2520and%2520helps%2520agents%2520enforce%2520these%2520policies%2520on%2520inter-agent%250Acommunication.%2520We%2520introduce%2520a%2520cryptographic%2520mechanism%2520for%2520deriving%2520access%250Acontrol%2520tokens%252C%2520that%2520offers%2520fine-grained%2520control%2520over%2520an%2520agent%2527s%2520interaction%250Awith%2520other%2520agents%252C%2520providing%2520formal%2520security%2520guarantees.%2520We%2520evaluate%2520SAGA%2520on%250Aseveral%2520agentic%2520tasks%252C%2520using%2520agents%2520in%2520different%2520geolocations%252C%2520and%2520multiple%250Aon-device%2520and%2520cloud%2520LLMs%252C%2520demonstrating%2520minimal%2520performance%2520overhead%2520with%2520no%250Aimpact%2520on%2520underlying%2520task%2520utility%2520in%2520a%2520wide%2520range%2520of%2520conditions.%2520Our%250Aarchitecture%2520enables%2520secure%2520and%2520trustworthy%2520deployment%2520of%2520autonomous%2520agents%252C%250Aaccelerating%2520the%2520responsible%2520adoption%2520of%2520this%2520technology%2520in%2520sensitive%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGA%3A%20A%20Security%20Architecture%20for%20Governing%20AI%20Agentic%20Systems&entry.906535625=Georgios%20Syros%20and%20Anshuman%20Suri%20and%20Jacob%20Ginesin%20and%20Cristina%20Nita-Rotaru%20and%20Alina%20Oprea&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20agents%20increasingly%20interact%2C%20collaborate%2C%0Aand%20delegate%20tasks%20to%20one%20another%20autonomously%20with%20minimal%20human%20interaction.%0AIndustry%20guidelines%20for%20agentic%20system%20governance%20emphasize%20the%20need%20for%20users%0Ato%20maintain%20comprehensive%20control%20over%20their%20agents%2C%20mitigating%20potential%0Adamage%20from%20malicious%20agents.%20Several%20proposed%20agentic%20system%20designs%20address%0Aagent%20identity%2C%20authorization%2C%20and%20delegation%2C%20but%20remain%20purely%20theoretical%2C%0Awithout%20concrete%20implementation%20and%20evaluation.%20Most%20importantly%2C%20they%20do%20not%0Aprovide%20user-controlled%20agent%20management.%0A%20%20To%20address%20this%20gap%2C%20we%20propose%20SAGA%2C%20a%20scalable%20Security%20Architecture%20for%0AGoverning%20Agentic%20systems%2C%20that%20offers%20user%20oversight%20over%20their%20agents%27%0Alifecycle.%20In%20our%20design%2C%20users%20register%20their%20agents%20with%20a%20central%20entity%2C%0Athe%20Provider%2C%20that%20maintains%20agent%20contact%20information%2C%20user-defined%20access%0Acontrol%20policies%2C%20and%20helps%20agents%20enforce%20these%20policies%20on%20inter-agent%0Acommunication.%20We%20introduce%20a%20cryptographic%20mechanism%20for%20deriving%20access%0Acontrol%20tokens%2C%20that%20offers%20fine-grained%20control%20over%20an%20agent%27s%20interaction%0Awith%20other%20agents%2C%20providing%20formal%20security%20guarantees.%20We%20evaluate%20SAGA%20on%0Aseveral%20agentic%20tasks%2C%20using%20agents%20in%20different%20geolocations%2C%20and%20multiple%0Aon-device%20and%20cloud%20LLMs%2C%20demonstrating%20minimal%20performance%20overhead%20with%20no%0Aimpact%20on%20underlying%20task%20utility%20in%20a%20wide%20range%20of%20conditions.%20Our%0Aarchitecture%20enables%20secure%20and%20trustworthy%20deployment%20of%20autonomous%20agents%2C%0Aaccelerating%20the%20responsible%20adoption%20of%20this%20technology%20in%20sensitive%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21034v2&entry.124074799=Read"},
{"title": "Physics-Informed Spectral Modeling for Hyperspectral Imaging", "author": "Zuzanna Gawrysiak and Krzysztof Krawiec", "abstract": "  We present PhISM, a physics-informed deep learning architecture that learns\nwithout supervision to explicitly disentangle hyperspectral observations and\nmodel them with continuous basis functions. \\mname outperforms prior methods on\nseveral classification and regression benchmarks, requires limited labeled\ndata, and provides additional insights thanks to interpretable latent\nrepresentation.\n", "link": "http://arxiv.org/abs/2508.21618v1", "date": "2025-08-29", "relevancy": 1.8362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Spectral%20Modeling%20for%20Hyperspectral%20Imaging&body=Title%3A%20Physics-Informed%20Spectral%20Modeling%20for%20Hyperspectral%20Imaging%0AAuthor%3A%20Zuzanna%20Gawrysiak%20and%20Krzysztof%20Krawiec%0AAbstract%3A%20%20%20We%20present%20PhISM%2C%20a%20physics-informed%20deep%20learning%20architecture%20that%20learns%0Awithout%20supervision%20to%20explicitly%20disentangle%20hyperspectral%20observations%20and%0Amodel%20them%20with%20continuous%20basis%20functions.%20%5Cmname%20outperforms%20prior%20methods%20on%0Aseveral%20classification%20and%20regression%20benchmarks%2C%20requires%20limited%20labeled%0Adata%2C%20and%20provides%20additional%20insights%20thanks%20to%20interpretable%20latent%0Arepresentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Spectral%2520Modeling%2520for%2520Hyperspectral%2520Imaging%26entry.906535625%3DZuzanna%2520Gawrysiak%2520and%2520Krzysztof%2520Krawiec%26entry.1292438233%3D%2520%2520We%2520present%2520PhISM%252C%2520a%2520physics-informed%2520deep%2520learning%2520architecture%2520that%2520learns%250Awithout%2520supervision%2520to%2520explicitly%2520disentangle%2520hyperspectral%2520observations%2520and%250Amodel%2520them%2520with%2520continuous%2520basis%2520functions.%2520%255Cmname%2520outperforms%2520prior%2520methods%2520on%250Aseveral%2520classification%2520and%2520regression%2520benchmarks%252C%2520requires%2520limited%2520labeled%250Adata%252C%2520and%2520provides%2520additional%2520insights%2520thanks%2520to%2520interpretable%2520latent%250Arepresentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Spectral%20Modeling%20for%20Hyperspectral%20Imaging&entry.906535625=Zuzanna%20Gawrysiak%20and%20Krzysztof%20Krawiec&entry.1292438233=%20%20We%20present%20PhISM%2C%20a%20physics-informed%20deep%20learning%20architecture%20that%20learns%0Awithout%20supervision%20to%20explicitly%20disentangle%20hyperspectral%20observations%20and%0Amodel%20them%20with%20continuous%20basis%20functions.%20%5Cmname%20outperforms%20prior%20methods%20on%0Aseveral%20classification%20and%20regression%20benchmarks%2C%20requires%20limited%20labeled%0Adata%2C%20and%20provides%20additional%20insights%20thanks%20to%20interpretable%20latent%0Arepresentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21618v1&entry.124074799=Read"},
{"title": "What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning\n  under Incomplete Knowledge", "author": "Dongzhuoran Zhou and Yuqicheng Zhu and Xiaxia Wang and Hongkuan Zhou and Yuan He and Jiaoyan Chen and Steffen Staab and Evgeny Kharlamov", "abstract": "  Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an\nincreasingly explored approach for combining the reasoning capabilities of\nlarge language models with the structured evidence of knowledge graphs.\nHowever, current evaluation practices fall short: existing benchmarks often\ninclude questions that can be directly answered using existing triples in KG,\nmaking it unclear whether models perform reasoning or simply retrieve answers\ndirectly. Moreover, inconsistent evaluation metrics and lenient answer matching\ncriteria further obscure meaningful comparisons. In this work, we introduce a\ngeneral method for constructing benchmarks, together with an evaluation\nprotocol, to systematically assess KG-RAG methods under knowledge\nincompleteness. Our empirical results show that current KG-RAG methods have\nlimited reasoning ability under missing knowledge, often rely on internal\nmemorization, and exhibit varying degrees of generalization depending on their\ndesign.\n", "link": "http://arxiv.org/abs/2508.08344v2", "date": "2025-08-29", "relevancy": 1.8331, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Breaks%20Knowledge%20Graph%20based%20RAG%3F%20Empirical%20Insights%20into%20Reasoning%0A%20%20under%20Incomplete%20Knowledge&body=Title%3A%20What%20Breaks%20Knowledge%20Graph%20based%20RAG%3F%20Empirical%20Insights%20into%20Reasoning%0A%20%20under%20Incomplete%20Knowledge%0AAuthor%3A%20Dongzhuoran%20Zhou%20and%20Yuqicheng%20Zhu%20and%20Xiaxia%20Wang%20and%20Hongkuan%20Zhou%20and%20Yuan%20He%20and%20Jiaoyan%20Chen%20and%20Steffen%20Staab%20and%20Evgeny%20Kharlamov%0AAbstract%3A%20%20%20Knowledge%20Graph-based%20Retrieval-Augmented%20Generation%20%28KG-RAG%29%20is%20an%0Aincreasingly%20explored%20approach%20for%20combining%20the%20reasoning%20capabilities%20of%0Alarge%20language%20models%20with%20the%20structured%20evidence%20of%20knowledge%20graphs.%0AHowever%2C%20current%20evaluation%20practices%20fall%20short%3A%20existing%20benchmarks%20often%0Ainclude%20questions%20that%20can%20be%20directly%20answered%20using%20existing%20triples%20in%20KG%2C%0Amaking%20it%20unclear%20whether%20models%20perform%20reasoning%20or%20simply%20retrieve%20answers%0Adirectly.%20Moreover%2C%20inconsistent%20evaluation%20metrics%20and%20lenient%20answer%20matching%0Acriteria%20further%20obscure%20meaningful%20comparisons.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneral%20method%20for%20constructing%20benchmarks%2C%20together%20with%20an%20evaluation%0Aprotocol%2C%20to%20systematically%20assess%20KG-RAG%20methods%20under%20knowledge%0Aincompleteness.%20Our%20empirical%20results%20show%20that%20current%20KG-RAG%20methods%20have%0Alimited%20reasoning%20ability%20under%20missing%20knowledge%2C%20often%20rely%20on%20internal%0Amemorization%2C%20and%20exhibit%20varying%20degrees%20of%20generalization%20depending%20on%20their%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Breaks%2520Knowledge%2520Graph%2520based%2520RAG%253F%2520Empirical%2520Insights%2520into%2520Reasoning%250A%2520%2520under%2520Incomplete%2520Knowledge%26entry.906535625%3DDongzhuoran%2520Zhou%2520and%2520Yuqicheng%2520Zhu%2520and%2520Xiaxia%2520Wang%2520and%2520Hongkuan%2520Zhou%2520and%2520Yuan%2520He%2520and%2520Jiaoyan%2520Chen%2520and%2520Steffen%2520Staab%2520and%2520Evgeny%2520Kharlamov%26entry.1292438233%3D%2520%2520Knowledge%2520Graph-based%2520Retrieval-Augmented%2520Generation%2520%2528KG-RAG%2529%2520is%2520an%250Aincreasingly%2520explored%2520approach%2520for%2520combining%2520the%2520reasoning%2520capabilities%2520of%250Alarge%2520language%2520models%2520with%2520the%2520structured%2520evidence%2520of%2520knowledge%2520graphs.%250AHowever%252C%2520current%2520evaluation%2520practices%2520fall%2520short%253A%2520existing%2520benchmarks%2520often%250Ainclude%2520questions%2520that%2520can%2520be%2520directly%2520answered%2520using%2520existing%2520triples%2520in%2520KG%252C%250Amaking%2520it%2520unclear%2520whether%2520models%2520perform%2520reasoning%2520or%2520simply%2520retrieve%2520answers%250Adirectly.%2520Moreover%252C%2520inconsistent%2520evaluation%2520metrics%2520and%2520lenient%2520answer%2520matching%250Acriteria%2520further%2520obscure%2520meaningful%2520comparisons.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Ageneral%2520method%2520for%2520constructing%2520benchmarks%252C%2520together%2520with%2520an%2520evaluation%250Aprotocol%252C%2520to%2520systematically%2520assess%2520KG-RAG%2520methods%2520under%2520knowledge%250Aincompleteness.%2520Our%2520empirical%2520results%2520show%2520that%2520current%2520KG-RAG%2520methods%2520have%250Alimited%2520reasoning%2520ability%2520under%2520missing%2520knowledge%252C%2520often%2520rely%2520on%2520internal%250Amemorization%252C%2520and%2520exhibit%2520varying%2520degrees%2520of%2520generalization%2520depending%2520on%2520their%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Breaks%20Knowledge%20Graph%20based%20RAG%3F%20Empirical%20Insights%20into%20Reasoning%0A%20%20under%20Incomplete%20Knowledge&entry.906535625=Dongzhuoran%20Zhou%20and%20Yuqicheng%20Zhu%20and%20Xiaxia%20Wang%20and%20Hongkuan%20Zhou%20and%20Yuan%20He%20and%20Jiaoyan%20Chen%20and%20Steffen%20Staab%20and%20Evgeny%20Kharlamov&entry.1292438233=%20%20Knowledge%20Graph-based%20Retrieval-Augmented%20Generation%20%28KG-RAG%29%20is%20an%0Aincreasingly%20explored%20approach%20for%20combining%20the%20reasoning%20capabilities%20of%0Alarge%20language%20models%20with%20the%20structured%20evidence%20of%20knowledge%20graphs.%0AHowever%2C%20current%20evaluation%20practices%20fall%20short%3A%20existing%20benchmarks%20often%0Ainclude%20questions%20that%20can%20be%20directly%20answered%20using%20existing%20triples%20in%20KG%2C%0Amaking%20it%20unclear%20whether%20models%20perform%20reasoning%20or%20simply%20retrieve%20answers%0Adirectly.%20Moreover%2C%20inconsistent%20evaluation%20metrics%20and%20lenient%20answer%20matching%0Acriteria%20further%20obscure%20meaningful%20comparisons.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneral%20method%20for%20constructing%20benchmarks%2C%20together%20with%20an%20evaluation%0Aprotocol%2C%20to%20systematically%20assess%20KG-RAG%20methods%20under%20knowledge%0Aincompleteness.%20Our%20empirical%20results%20show%20that%20current%20KG-RAG%20methods%20have%0Alimited%20reasoning%20ability%20under%20missing%20knowledge%2C%20often%20rely%20on%20internal%0Amemorization%2C%20and%20exhibit%20varying%20degrees%20of%20generalization%20depending%20on%20their%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08344v2&entry.124074799=Read"},
{"title": "Beyond Frequency: The Role of Redundancy in Large Language Model\n  Memorization", "author": "Jie Zhang and Qinghua Zhao and Chi-ho Lin and Zhongfeng Kang and Lei Li", "abstract": "  Memorization in large language models poses critical risks for privacy and\nfairness as these systems scale to billions of parameters. While previous\nstudies established correlations between memorization and factors like token\nfrequency and repetition patterns, we revealed distinct response patterns:\nfrequency increases minimally impact memorized samples (e.g. 0.09) while\nsubstantially affecting non-memorized samples (e.g., 0.25), with consistency\nobserved across model scales. Through counterfactual analysis by perturbing\nsample prefixes and quantifying perturbation strength through token positional\nchanges, we demonstrate that redundancy correlates with memorization patterns.\nOur findings establish that: about 79% of memorized samples are low-redundancy,\nthese low-redundancy samples exhibit 2-fold higher vulnerability than\nhigh-redundancy ones, and consequently memorized samples drop by 0.6 under\nperturbation while non-memorized samples drop by only 0.01, indicating that\nmore redundant content becomes both more memorable and more fragile. These\nfindings suggest potential redundancy-guided approaches for data preprocessing,\nthereby reducing privacy risks and mitigating bias to ensure fairness in model\ndeployments.\n", "link": "http://arxiv.org/abs/2506.12321v2", "date": "2025-08-29", "relevancy": 1.8276, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.476}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Frequency%3A%20The%20Role%20of%20Redundancy%20in%20Large%20Language%20Model%0A%20%20Memorization&body=Title%3A%20Beyond%20Frequency%3A%20The%20Role%20of%20Redundancy%20in%20Large%20Language%20Model%0A%20%20Memorization%0AAuthor%3A%20Jie%20Zhang%20and%20Qinghua%20Zhao%20and%20Chi-ho%20Lin%20and%20Zhongfeng%20Kang%20and%20Lei%20Li%0AAbstract%3A%20%20%20Memorization%20in%20large%20language%20models%20poses%20critical%20risks%20for%20privacy%20and%0Afairness%20as%20these%20systems%20scale%20to%20billions%20of%20parameters.%20While%20previous%0Astudies%20established%20correlations%20between%20memorization%20and%20factors%20like%20token%0Afrequency%20and%20repetition%20patterns%2C%20we%20revealed%20distinct%20response%20patterns%3A%0Afrequency%20increases%20minimally%20impact%20memorized%20samples%20%28e.g.%200.09%29%20while%0Asubstantially%20affecting%20non-memorized%20samples%20%28e.g.%2C%200.25%29%2C%20with%20consistency%0Aobserved%20across%20model%20scales.%20Through%20counterfactual%20analysis%20by%20perturbing%0Asample%20prefixes%20and%20quantifying%20perturbation%20strength%20through%20token%20positional%0Achanges%2C%20we%20demonstrate%20that%20redundancy%20correlates%20with%20memorization%20patterns.%0AOur%20findings%20establish%20that%3A%20about%2079%25%20of%20memorized%20samples%20are%20low-redundancy%2C%0Athese%20low-redundancy%20samples%20exhibit%202-fold%20higher%20vulnerability%20than%0Ahigh-redundancy%20ones%2C%20and%20consequently%20memorized%20samples%20drop%20by%200.6%20under%0Aperturbation%20while%20non-memorized%20samples%20drop%20by%20only%200.01%2C%20indicating%20that%0Amore%20redundant%20content%20becomes%20both%20more%20memorable%20and%20more%20fragile.%20These%0Afindings%20suggest%20potential%20redundancy-guided%20approaches%20for%20data%20preprocessing%2C%0Athereby%20reducing%20privacy%20risks%20and%20mitigating%20bias%20to%20ensure%20fairness%20in%20model%0Adeployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Frequency%253A%2520The%2520Role%2520of%2520Redundancy%2520in%2520Large%2520Language%2520Model%250A%2520%2520Memorization%26entry.906535625%3DJie%2520Zhang%2520and%2520Qinghua%2520Zhao%2520and%2520Chi-ho%2520Lin%2520and%2520Zhongfeng%2520Kang%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520Memorization%2520in%2520large%2520language%2520models%2520poses%2520critical%2520risks%2520for%2520privacy%2520and%250Afairness%2520as%2520these%2520systems%2520scale%2520to%2520billions%2520of%2520parameters.%2520While%2520previous%250Astudies%2520established%2520correlations%2520between%2520memorization%2520and%2520factors%2520like%2520token%250Afrequency%2520and%2520repetition%2520patterns%252C%2520we%2520revealed%2520distinct%2520response%2520patterns%253A%250Afrequency%2520increases%2520minimally%2520impact%2520memorized%2520samples%2520%2528e.g.%25200.09%2529%2520while%250Asubstantially%2520affecting%2520non-memorized%2520samples%2520%2528e.g.%252C%25200.25%2529%252C%2520with%2520consistency%250Aobserved%2520across%2520model%2520scales.%2520Through%2520counterfactual%2520analysis%2520by%2520perturbing%250Asample%2520prefixes%2520and%2520quantifying%2520perturbation%2520strength%2520through%2520token%2520positional%250Achanges%252C%2520we%2520demonstrate%2520that%2520redundancy%2520correlates%2520with%2520memorization%2520patterns.%250AOur%2520findings%2520establish%2520that%253A%2520about%252079%2525%2520of%2520memorized%2520samples%2520are%2520low-redundancy%252C%250Athese%2520low-redundancy%2520samples%2520exhibit%25202-fold%2520higher%2520vulnerability%2520than%250Ahigh-redundancy%2520ones%252C%2520and%2520consequently%2520memorized%2520samples%2520drop%2520by%25200.6%2520under%250Aperturbation%2520while%2520non-memorized%2520samples%2520drop%2520by%2520only%25200.01%252C%2520indicating%2520that%250Amore%2520redundant%2520content%2520becomes%2520both%2520more%2520memorable%2520and%2520more%2520fragile.%2520These%250Afindings%2520suggest%2520potential%2520redundancy-guided%2520approaches%2520for%2520data%2520preprocessing%252C%250Athereby%2520reducing%2520privacy%2520risks%2520and%2520mitigating%2520bias%2520to%2520ensure%2520fairness%2520in%2520model%250Adeployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Frequency%3A%20The%20Role%20of%20Redundancy%20in%20Large%20Language%20Model%0A%20%20Memorization&entry.906535625=Jie%20Zhang%20and%20Qinghua%20Zhao%20and%20Chi-ho%20Lin%20and%20Zhongfeng%20Kang%20and%20Lei%20Li&entry.1292438233=%20%20Memorization%20in%20large%20language%20models%20poses%20critical%20risks%20for%20privacy%20and%0Afairness%20as%20these%20systems%20scale%20to%20billions%20of%20parameters.%20While%20previous%0Astudies%20established%20correlations%20between%20memorization%20and%20factors%20like%20token%0Afrequency%20and%20repetition%20patterns%2C%20we%20revealed%20distinct%20response%20patterns%3A%0Afrequency%20increases%20minimally%20impact%20memorized%20samples%20%28e.g.%200.09%29%20while%0Asubstantially%20affecting%20non-memorized%20samples%20%28e.g.%2C%200.25%29%2C%20with%20consistency%0Aobserved%20across%20model%20scales.%20Through%20counterfactual%20analysis%20by%20perturbing%0Asample%20prefixes%20and%20quantifying%20perturbation%20strength%20through%20token%20positional%0Achanges%2C%20we%20demonstrate%20that%20redundancy%20correlates%20with%20memorization%20patterns.%0AOur%20findings%20establish%20that%3A%20about%2079%25%20of%20memorized%20samples%20are%20low-redundancy%2C%0Athese%20low-redundancy%20samples%20exhibit%202-fold%20higher%20vulnerability%20than%0Ahigh-redundancy%20ones%2C%20and%20consequently%20memorized%20samples%20drop%20by%200.6%20under%0Aperturbation%20while%20non-memorized%20samples%20drop%20by%20only%200.01%2C%20indicating%20that%0Amore%20redundant%20content%20becomes%20both%20more%20memorable%20and%20more%20fragile.%20These%0Afindings%20suggest%20potential%20redundancy-guided%20approaches%20for%20data%20preprocessing%2C%0Athereby%20reducing%20privacy%20risks%20and%20mitigating%20bias%20to%20ensure%20fairness%20in%20model%0Adeployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12321v2&entry.124074799=Read"},
{"title": "Failure Prediction Is a Better Performance Proxy for Early-Exit Networks\n  Than Calibration", "author": "Piotr Kubaty and Filip Szatkowski and Metod Jazbec and Bartosz W\u00f3jcik", "abstract": "  Early-exit models speed up inference by attaching internal classifiers to\nintermediate layers of the model and allowing computation to stop once a\nprediction satisfies an exit criterion. Most early-exit methods rely on\nconfidence-based exit strategies, which motivated some works to calibrate\nintermediate classifiers to improve the performance of the entire model. In\nthis paper, we show that calibration measures can be misleading indicators of\nthe performance of multi-exit models: a well-calibrated classifier may still\nwaste computation, and common calibration methods do not preserve the sample\nranking within a classifier. We demonstrate empirical cases where miscalibrated\nnetworks outperform calibrated ones. As an alternative, we propose to use\nfailure prediction as a more useful proxy for early-exit model performance.\nUnlike calibration, failure prediction accounts for changes in the ranking of\nsamples and shows a strong correlation with efficiency improvements, making it\na more dependable basis for designing and evaluating early-exit models.\n", "link": "http://arxiv.org/abs/2508.21495v1", "date": "2025-08-29", "relevancy": 1.824, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Failure%20Prediction%20Is%20a%20Better%20Performance%20Proxy%20for%20Early-Exit%20Networks%0A%20%20Than%20Calibration&body=Title%3A%20Failure%20Prediction%20Is%20a%20Better%20Performance%20Proxy%20for%20Early-Exit%20Networks%0A%20%20Than%20Calibration%0AAuthor%3A%20Piotr%20Kubaty%20and%20Filip%20Szatkowski%20and%20Metod%20Jazbec%20and%20Bartosz%20W%C3%B3jcik%0AAbstract%3A%20%20%20Early-exit%20models%20speed%20up%20inference%20by%20attaching%20internal%20classifiers%20to%0Aintermediate%20layers%20of%20the%20model%20and%20allowing%20computation%20to%20stop%20once%20a%0Aprediction%20satisfies%20an%20exit%20criterion.%20Most%20early-exit%20methods%20rely%20on%0Aconfidence-based%20exit%20strategies%2C%20which%20motivated%20some%20works%20to%20calibrate%0Aintermediate%20classifiers%20to%20improve%20the%20performance%20of%20the%20entire%20model.%20In%0Athis%20paper%2C%20we%20show%20that%20calibration%20measures%20can%20be%20misleading%20indicators%20of%0Athe%20performance%20of%20multi-exit%20models%3A%20a%20well-calibrated%20classifier%20may%20still%0Awaste%20computation%2C%20and%20common%20calibration%20methods%20do%20not%20preserve%20the%20sample%0Aranking%20within%20a%20classifier.%20We%20demonstrate%20empirical%20cases%20where%20miscalibrated%0Anetworks%20outperform%20calibrated%20ones.%20As%20an%20alternative%2C%20we%20propose%20to%20use%0Afailure%20prediction%20as%20a%20more%20useful%20proxy%20for%20early-exit%20model%20performance.%0AUnlike%20calibration%2C%20failure%20prediction%20accounts%20for%20changes%20in%20the%20ranking%20of%0Asamples%20and%20shows%20a%20strong%20correlation%20with%20efficiency%20improvements%2C%20making%20it%0Aa%20more%20dependable%20basis%20for%20designing%20and%20evaluating%20early-exit%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFailure%2520Prediction%2520Is%2520a%2520Better%2520Performance%2520Proxy%2520for%2520Early-Exit%2520Networks%250A%2520%2520Than%2520Calibration%26entry.906535625%3DPiotr%2520Kubaty%2520and%2520Filip%2520Szatkowski%2520and%2520Metod%2520Jazbec%2520and%2520Bartosz%2520W%25C3%25B3jcik%26entry.1292438233%3D%2520%2520Early-exit%2520models%2520speed%2520up%2520inference%2520by%2520attaching%2520internal%2520classifiers%2520to%250Aintermediate%2520layers%2520of%2520the%2520model%2520and%2520allowing%2520computation%2520to%2520stop%2520once%2520a%250Aprediction%2520satisfies%2520an%2520exit%2520criterion.%2520Most%2520early-exit%2520methods%2520rely%2520on%250Aconfidence-based%2520exit%2520strategies%252C%2520which%2520motivated%2520some%2520works%2520to%2520calibrate%250Aintermediate%2520classifiers%2520to%2520improve%2520the%2520performance%2520of%2520the%2520entire%2520model.%2520In%250Athis%2520paper%252C%2520we%2520show%2520that%2520calibration%2520measures%2520can%2520be%2520misleading%2520indicators%2520of%250Athe%2520performance%2520of%2520multi-exit%2520models%253A%2520a%2520well-calibrated%2520classifier%2520may%2520still%250Awaste%2520computation%252C%2520and%2520common%2520calibration%2520methods%2520do%2520not%2520preserve%2520the%2520sample%250Aranking%2520within%2520a%2520classifier.%2520We%2520demonstrate%2520empirical%2520cases%2520where%2520miscalibrated%250Anetworks%2520outperform%2520calibrated%2520ones.%2520As%2520an%2520alternative%252C%2520we%2520propose%2520to%2520use%250Afailure%2520prediction%2520as%2520a%2520more%2520useful%2520proxy%2520for%2520early-exit%2520model%2520performance.%250AUnlike%2520calibration%252C%2520failure%2520prediction%2520accounts%2520for%2520changes%2520in%2520the%2520ranking%2520of%250Asamples%2520and%2520shows%2520a%2520strong%2520correlation%2520with%2520efficiency%2520improvements%252C%2520making%2520it%250Aa%2520more%2520dependable%2520basis%2520for%2520designing%2520and%2520evaluating%2520early-exit%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Failure%20Prediction%20Is%20a%20Better%20Performance%20Proxy%20for%20Early-Exit%20Networks%0A%20%20Than%20Calibration&entry.906535625=Piotr%20Kubaty%20and%20Filip%20Szatkowski%20and%20Metod%20Jazbec%20and%20Bartosz%20W%C3%B3jcik&entry.1292438233=%20%20Early-exit%20models%20speed%20up%20inference%20by%20attaching%20internal%20classifiers%20to%0Aintermediate%20layers%20of%20the%20model%20and%20allowing%20computation%20to%20stop%20once%20a%0Aprediction%20satisfies%20an%20exit%20criterion.%20Most%20early-exit%20methods%20rely%20on%0Aconfidence-based%20exit%20strategies%2C%20which%20motivated%20some%20works%20to%20calibrate%0Aintermediate%20classifiers%20to%20improve%20the%20performance%20of%20the%20entire%20model.%20In%0Athis%20paper%2C%20we%20show%20that%20calibration%20measures%20can%20be%20misleading%20indicators%20of%0Athe%20performance%20of%20multi-exit%20models%3A%20a%20well-calibrated%20classifier%20may%20still%0Awaste%20computation%2C%20and%20common%20calibration%20methods%20do%20not%20preserve%20the%20sample%0Aranking%20within%20a%20classifier.%20We%20demonstrate%20empirical%20cases%20where%20miscalibrated%0Anetworks%20outperform%20calibrated%20ones.%20As%20an%20alternative%2C%20we%20propose%20to%20use%0Afailure%20prediction%20as%20a%20more%20useful%20proxy%20for%20early-exit%20model%20performance.%0AUnlike%20calibration%2C%20failure%20prediction%20accounts%20for%20changes%20in%20the%20ranking%20of%0Asamples%20and%20shows%20a%20strong%20correlation%20with%20efficiency%20improvements%2C%20making%20it%0Aa%20more%20dependable%20basis%20for%20designing%20and%20evaluating%20early-exit%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21495v1&entry.124074799=Read"},
{"title": "Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data\n  Analysis", "author": "Adam Sandler and Diego Klabjan and Yuan Luo", "abstract": "  We analyze large, multi-dimensional, sparse counting data sets, finding\nunsupervised groups to provide unique insights into genetic data. We create\ngene and biological pathway groups based on patients' variants to find common\nrisk factors for four common types of cancer (breast, lung, prostate, and\ncolorectal) and autism spectrum disorder. To accomplish this, we extend latent\nDirichlet allocation to multiple dimensions and design distinct methods for\nhierarchical topic modeling. We find that our conditional hierarchical Bayesian\nTucker decomposition models are more coherent than baseline models.\n", "link": "http://arxiv.org/abs/1911.12426v9", "date": "2025-08-29", "relevancy": 1.8185, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Hierarchical%20Bayesian%20Tucker%20Decomposition%20for%20Genetic%20Data%0A%20%20Analysis&body=Title%3A%20Conditional%20Hierarchical%20Bayesian%20Tucker%20Decomposition%20for%20Genetic%20Data%0A%20%20Analysis%0AAuthor%3A%20Adam%20Sandler%20and%20Diego%20Klabjan%20and%20Yuan%20Luo%0AAbstract%3A%20%20%20We%20analyze%20large%2C%20multi-dimensional%2C%20sparse%20counting%20data%20sets%2C%20finding%0Aunsupervised%20groups%20to%20provide%20unique%20insights%20into%20genetic%20data.%20We%20create%0Agene%20and%20biological%20pathway%20groups%20based%20on%20patients%27%20variants%20to%20find%20common%0Arisk%20factors%20for%20four%20common%20types%20of%20cancer%20%28breast%2C%20lung%2C%20prostate%2C%20and%0Acolorectal%29%20and%20autism%20spectrum%20disorder.%20To%20accomplish%20this%2C%20we%20extend%20latent%0ADirichlet%20allocation%20to%20multiple%20dimensions%20and%20design%20distinct%20methods%20for%0Ahierarchical%20topic%20modeling.%20We%20find%20that%20our%20conditional%20hierarchical%20Bayesian%0ATucker%20decomposition%20models%20are%20more%20coherent%20than%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1911.12426v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Hierarchical%2520Bayesian%2520Tucker%2520Decomposition%2520for%2520Genetic%2520Data%250A%2520%2520Analysis%26entry.906535625%3DAdam%2520Sandler%2520and%2520Diego%2520Klabjan%2520and%2520Yuan%2520Luo%26entry.1292438233%3D%2520%2520We%2520analyze%2520large%252C%2520multi-dimensional%252C%2520sparse%2520counting%2520data%2520sets%252C%2520finding%250Aunsupervised%2520groups%2520to%2520provide%2520unique%2520insights%2520into%2520genetic%2520data.%2520We%2520create%250Agene%2520and%2520biological%2520pathway%2520groups%2520based%2520on%2520patients%2527%2520variants%2520to%2520find%2520common%250Arisk%2520factors%2520for%2520four%2520common%2520types%2520of%2520cancer%2520%2528breast%252C%2520lung%252C%2520prostate%252C%2520and%250Acolorectal%2529%2520and%2520autism%2520spectrum%2520disorder.%2520To%2520accomplish%2520this%252C%2520we%2520extend%2520latent%250ADirichlet%2520allocation%2520to%2520multiple%2520dimensions%2520and%2520design%2520distinct%2520methods%2520for%250Ahierarchical%2520topic%2520modeling.%2520We%2520find%2520that%2520our%2520conditional%2520hierarchical%2520Bayesian%250ATucker%2520decomposition%2520models%2520are%2520more%2520coherent%2520than%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1911.12426v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Hierarchical%20Bayesian%20Tucker%20Decomposition%20for%20Genetic%20Data%0A%20%20Analysis&entry.906535625=Adam%20Sandler%20and%20Diego%20Klabjan%20and%20Yuan%20Luo&entry.1292438233=%20%20We%20analyze%20large%2C%20multi-dimensional%2C%20sparse%20counting%20data%20sets%2C%20finding%0Aunsupervised%20groups%20to%20provide%20unique%20insights%20into%20genetic%20data.%20We%20create%0Agene%20and%20biological%20pathway%20groups%20based%20on%20patients%27%20variants%20to%20find%20common%0Arisk%20factors%20for%20four%20common%20types%20of%20cancer%20%28breast%2C%20lung%2C%20prostate%2C%20and%0Acolorectal%29%20and%20autism%20spectrum%20disorder.%20To%20accomplish%20this%2C%20we%20extend%20latent%0ADirichlet%20allocation%20to%20multiple%20dimensions%20and%20design%20distinct%20methods%20for%0Ahierarchical%20topic%20modeling.%20We%20find%20that%20our%20conditional%20hierarchical%20Bayesian%0ATucker%20decomposition%20models%20are%20more%20coherent%20than%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/1911.12426v9&entry.124074799=Read"},
{"title": "BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens", "author": "Hao Wen and Xinrui Wu and Yi Sun and Feifei Zhang and Liye Chen and Jie Wang and Yunxin Liu and Yunhao Liu and Ya-Qin Zhang and Yuanchun Li", "abstract": "  Recent advancements in Large Language Models (LLMs) have leveraged increased\ntest-time computation to enhance reasoning capabilities, a strategy that, while\neffective, incurs significant latency and resource costs, limiting their\napplicability in real-world time-constrained or cost-sensitive scenarios. This\npaper introduces BudgetThinker, a novel framework designed to empower LLMs with\nbudget-aware reasoning, enabling precise control over the length of their\nthought processes. We propose a methodology that periodically inserts special\ncontrol tokens during inference to continuously inform the model of its\nremaining token budget. This approach is coupled with a comprehensive two-stage\ntraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize\nthe model with budget constraints, followed by a curriculum-based Reinforcement\nLearning (RL) phase that utilizes a length-aware reward function to optimize\nfor both accuracy and budget adherence. We demonstrate that BudgetThinker\nsignificantly surpasses strong baselines in maintaining performance across a\nvariety of reasoning budgets on challenging mathematical benchmarks. Our method\nprovides a scalable and effective solution for developing efficient and\ncontrollable LLM reasoning, making advanced models more practical for\ndeployment in resource-constrained and real-time environments.\n", "link": "http://arxiv.org/abs/2508.17196v2", "date": "2025-08-29", "relevancy": 1.8099, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4657}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4433}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BudgetThinker%3A%20Empowering%20Budget-aware%20LLM%20Reasoning%20with%20Control%20Tokens&body=Title%3A%20BudgetThinker%3A%20Empowering%20Budget-aware%20LLM%20Reasoning%20with%20Control%20Tokens%0AAuthor%3A%20Hao%20Wen%20and%20Xinrui%20Wu%20and%20Yi%20Sun%20and%20Feifei%20Zhang%20and%20Liye%20Chen%20and%20Jie%20Wang%20and%20Yunxin%20Liu%20and%20Yunhao%20Liu%20and%20Ya-Qin%20Zhang%20and%20Yuanchun%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20leveraged%20increased%0Atest-time%20computation%20to%20enhance%20reasoning%20capabilities%2C%20a%20strategy%20that%2C%20while%0Aeffective%2C%20incurs%20significant%20latency%20and%20resource%20costs%2C%20limiting%20their%0Aapplicability%20in%20real-world%20time-constrained%20or%20cost-sensitive%20scenarios.%20This%0Apaper%20introduces%20BudgetThinker%2C%20a%20novel%20framework%20designed%20to%20empower%20LLMs%20with%0Abudget-aware%20reasoning%2C%20enabling%20precise%20control%20over%20the%20length%20of%20their%0Athought%20processes.%20We%20propose%20a%20methodology%20that%20periodically%20inserts%20special%0Acontrol%20tokens%20during%20inference%20to%20continuously%20inform%20the%20model%20of%20its%0Aremaining%20token%20budget.%20This%20approach%20is%20coupled%20with%20a%20comprehensive%20two-stage%0Atraining%20pipeline%2C%20beginning%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20familiarize%0Athe%20model%20with%20budget%20constraints%2C%20followed%20by%20a%20curriculum-based%20Reinforcement%0ALearning%20%28RL%29%20phase%20that%20utilizes%20a%20length-aware%20reward%20function%20to%20optimize%0Afor%20both%20accuracy%20and%20budget%20adherence.%20We%20demonstrate%20that%20BudgetThinker%0Asignificantly%20surpasses%20strong%20baselines%20in%20maintaining%20performance%20across%20a%0Avariety%20of%20reasoning%20budgets%20on%20challenging%20mathematical%20benchmarks.%20Our%20method%0Aprovides%20a%20scalable%20and%20effective%20solution%20for%20developing%20efficient%20and%0Acontrollable%20LLM%20reasoning%2C%20making%20advanced%20models%20more%20practical%20for%0Adeployment%20in%20resource-constrained%20and%20real-time%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBudgetThinker%253A%2520Empowering%2520Budget-aware%2520LLM%2520Reasoning%2520with%2520Control%2520Tokens%26entry.906535625%3DHao%2520Wen%2520and%2520Xinrui%2520Wu%2520and%2520Yi%2520Sun%2520and%2520Feifei%2520Zhang%2520and%2520Liye%2520Chen%2520and%2520Jie%2520Wang%2520and%2520Yunxin%2520Liu%2520and%2520Yunhao%2520Liu%2520and%2520Ya-Qin%2520Zhang%2520and%2520Yuanchun%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520leveraged%2520increased%250Atest-time%2520computation%2520to%2520enhance%2520reasoning%2520capabilities%252C%2520a%2520strategy%2520that%252C%2520while%250Aeffective%252C%2520incurs%2520significant%2520latency%2520and%2520resource%2520costs%252C%2520limiting%2520their%250Aapplicability%2520in%2520real-world%2520time-constrained%2520or%2520cost-sensitive%2520scenarios.%2520This%250Apaper%2520introduces%2520BudgetThinker%252C%2520a%2520novel%2520framework%2520designed%2520to%2520empower%2520LLMs%2520with%250Abudget-aware%2520reasoning%252C%2520enabling%2520precise%2520control%2520over%2520the%2520length%2520of%2520their%250Athought%2520processes.%2520We%2520propose%2520a%2520methodology%2520that%2520periodically%2520inserts%2520special%250Acontrol%2520tokens%2520during%2520inference%2520to%2520continuously%2520inform%2520the%2520model%2520of%2520its%250Aremaining%2520token%2520budget.%2520This%2520approach%2520is%2520coupled%2520with%2520a%2520comprehensive%2520two-stage%250Atraining%2520pipeline%252C%2520beginning%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520to%2520familiarize%250Athe%2520model%2520with%2520budget%2520constraints%252C%2520followed%2520by%2520a%2520curriculum-based%2520Reinforcement%250ALearning%2520%2528RL%2529%2520phase%2520that%2520utilizes%2520a%2520length-aware%2520reward%2520function%2520to%2520optimize%250Afor%2520both%2520accuracy%2520and%2520budget%2520adherence.%2520We%2520demonstrate%2520that%2520BudgetThinker%250Asignificantly%2520surpasses%2520strong%2520baselines%2520in%2520maintaining%2520performance%2520across%2520a%250Avariety%2520of%2520reasoning%2520budgets%2520on%2520challenging%2520mathematical%2520benchmarks.%2520Our%2520method%250Aprovides%2520a%2520scalable%2520and%2520effective%2520solution%2520for%2520developing%2520efficient%2520and%250Acontrollable%2520LLM%2520reasoning%252C%2520making%2520advanced%2520models%2520more%2520practical%2520for%250Adeployment%2520in%2520resource-constrained%2520and%2520real-time%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BudgetThinker%3A%20Empowering%20Budget-aware%20LLM%20Reasoning%20with%20Control%20Tokens&entry.906535625=Hao%20Wen%20and%20Xinrui%20Wu%20and%20Yi%20Sun%20and%20Feifei%20Zhang%20and%20Liye%20Chen%20and%20Jie%20Wang%20and%20Yunxin%20Liu%20and%20Yunhao%20Liu%20and%20Ya-Qin%20Zhang%20and%20Yuanchun%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20leveraged%20increased%0Atest-time%20computation%20to%20enhance%20reasoning%20capabilities%2C%20a%20strategy%20that%2C%20while%0Aeffective%2C%20incurs%20significant%20latency%20and%20resource%20costs%2C%20limiting%20their%0Aapplicability%20in%20real-world%20time-constrained%20or%20cost-sensitive%20scenarios.%20This%0Apaper%20introduces%20BudgetThinker%2C%20a%20novel%20framework%20designed%20to%20empower%20LLMs%20with%0Abudget-aware%20reasoning%2C%20enabling%20precise%20control%20over%20the%20length%20of%20their%0Athought%20processes.%20We%20propose%20a%20methodology%20that%20periodically%20inserts%20special%0Acontrol%20tokens%20during%20inference%20to%20continuously%20inform%20the%20model%20of%20its%0Aremaining%20token%20budget.%20This%20approach%20is%20coupled%20with%20a%20comprehensive%20two-stage%0Atraining%20pipeline%2C%20beginning%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20familiarize%0Athe%20model%20with%20budget%20constraints%2C%20followed%20by%20a%20curriculum-based%20Reinforcement%0ALearning%20%28RL%29%20phase%20that%20utilizes%20a%20length-aware%20reward%20function%20to%20optimize%0Afor%20both%20accuracy%20and%20budget%20adherence.%20We%20demonstrate%20that%20BudgetThinker%0Asignificantly%20surpasses%20strong%20baselines%20in%20maintaining%20performance%20across%20a%0Avariety%20of%20reasoning%20budgets%20on%20challenging%20mathematical%20benchmarks.%20Our%20method%0Aprovides%20a%20scalable%20and%20effective%20solution%20for%20developing%20efficient%20and%0Acontrollable%20LLM%20reasoning%2C%20making%20advanced%20models%20more%20practical%20for%0Adeployment%20in%20resource-constrained%20and%20real-time%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17196v2&entry.124074799=Read"},
{"title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in\n  Climate Resilience Education", "author": "Imran S. A. Khan and Emmanuel G. Blanchard and S\u00e9bastien George", "abstract": "  This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness.\n", "link": "http://arxiv.org/abs/2508.21666v1", "date": "2025-08-29", "relevancy": 1.7859, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4685}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20IoT%20and%20Generative%20AI%20for%20Weather-Adaptive%20Learning%20in%0A%20%20Climate%20Resilience%20Education&body=Title%3A%20Harnessing%20IoT%20and%20Generative%20AI%20for%20Weather-Adaptive%20Learning%20in%0A%20%20Climate%20Resilience%20Education%0AAuthor%3A%20Imran%20S.%20A.%20Khan%20and%20Emmanuel%20G.%20Blanchard%20and%20S%C3%A9bastien%20George%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Future%20Atmospheric%20Conditions%20Training%20System%0A%28FACTS%29%2C%20a%20novel%20platform%20that%20advances%20climate%20resilience%20education%20through%0Aplace-based%2C%20adaptive%20learning%20experiences.%20FACTS%20combines%20real-time%0Aatmospheric%20data%20collected%20by%20IoT%20sensors%20with%20curated%20resources%20from%20a%0AKnowledge%20Base%20to%20dynamically%20generate%20localized%20learning%20challenges.%20Learner%0Aresponses%20are%20analyzed%20by%20a%20Generative%20AI%20powered%20server%2C%20which%20delivers%0Apersonalized%20feedback%20and%20adaptive%20support.%20Results%20from%20a%20user%20evaluation%0Aindicate%20that%20participants%20found%20the%20system%20both%20easy%20to%20use%20and%20effective%20for%0Abuilding%20knowledge%20related%20to%20climate%20resilience.%20These%20findings%20suggest%20that%0Aintegrating%20IoT%20and%20Generative%20AI%20into%20atmospherically%20adaptive%20learning%0Atechnologies%20holds%20significant%20promise%20for%20enhancing%20educational%20engagement%20and%0Afostering%20climate%20awareness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520IoT%2520and%2520Generative%2520AI%2520for%2520Weather-Adaptive%2520Learning%2520in%250A%2520%2520Climate%2520Resilience%2520Education%26entry.906535625%3DImran%2520S.%2520A.%2520Khan%2520and%2520Emmanuel%2520G.%2520Blanchard%2520and%2520S%25C3%25A9bastien%2520George%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Future%2520Atmospheric%2520Conditions%2520Training%2520System%250A%2528FACTS%2529%252C%2520a%2520novel%2520platform%2520that%2520advances%2520climate%2520resilience%2520education%2520through%250Aplace-based%252C%2520adaptive%2520learning%2520experiences.%2520FACTS%2520combines%2520real-time%250Aatmospheric%2520data%2520collected%2520by%2520IoT%2520sensors%2520with%2520curated%2520resources%2520from%2520a%250AKnowledge%2520Base%2520to%2520dynamically%2520generate%2520localized%2520learning%2520challenges.%2520Learner%250Aresponses%2520are%2520analyzed%2520by%2520a%2520Generative%2520AI%2520powered%2520server%252C%2520which%2520delivers%250Apersonalized%2520feedback%2520and%2520adaptive%2520support.%2520Results%2520from%2520a%2520user%2520evaluation%250Aindicate%2520that%2520participants%2520found%2520the%2520system%2520both%2520easy%2520to%2520use%2520and%2520effective%2520for%250Abuilding%2520knowledge%2520related%2520to%2520climate%2520resilience.%2520These%2520findings%2520suggest%2520that%250Aintegrating%2520IoT%2520and%2520Generative%2520AI%2520into%2520atmospherically%2520adaptive%2520learning%250Atechnologies%2520holds%2520significant%2520promise%2520for%2520enhancing%2520educational%2520engagement%2520and%250Afostering%2520climate%2520awareness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20IoT%20and%20Generative%20AI%20for%20Weather-Adaptive%20Learning%20in%0A%20%20Climate%20Resilience%20Education&entry.906535625=Imran%20S.%20A.%20Khan%20and%20Emmanuel%20G.%20Blanchard%20and%20S%C3%A9bastien%20George&entry.1292438233=%20%20This%20paper%20introduces%20the%20Future%20Atmospheric%20Conditions%20Training%20System%0A%28FACTS%29%2C%20a%20novel%20platform%20that%20advances%20climate%20resilience%20education%20through%0Aplace-based%2C%20adaptive%20learning%20experiences.%20FACTS%20combines%20real-time%0Aatmospheric%20data%20collected%20by%20IoT%20sensors%20with%20curated%20resources%20from%20a%0AKnowledge%20Base%20to%20dynamically%20generate%20localized%20learning%20challenges.%20Learner%0Aresponses%20are%20analyzed%20by%20a%20Generative%20AI%20powered%20server%2C%20which%20delivers%0Apersonalized%20feedback%20and%20adaptive%20support.%20Results%20from%20a%20user%20evaluation%0Aindicate%20that%20participants%20found%20the%20system%20both%20easy%20to%20use%20and%20effective%20for%0Abuilding%20knowledge%20related%20to%20climate%20resilience.%20These%20findings%20suggest%20that%0Aintegrating%20IoT%20and%20Generative%20AI%20into%20atmospherically%20adaptive%20learning%0Atechnologies%20holds%20significant%20promise%20for%20enhancing%20educational%20engagement%20and%0Afostering%20climate%20awareness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21666v1&entry.124074799=Read"},
{"title": "A Hybrid Artificial Intelligence Method for Estimating Flicker in Power\n  Systems", "author": "Javad Enayati and Pedram Asef and Alexandre Benoit", "abstract": "  This paper introduces a novel hybrid AI method combining H filtering and an\nadaptive linear neuron network for flicker component estimation in power\ndistribution systems.The proposed method leverages the robustness of the H\nfilter to extract the voltage envelope under uncertain and noisy conditions\nfollowed by the use of ADALINE to accurately identify flicker frequencies\nembedded in the envelope.This synergy enables efficient time domain estimation\nwith rapid convergence and noise resilience addressing key limitations of\nexisting frequency domain approaches.Unlike conventional techniques this hybrid\nAI model handles complex power disturbances without prior knowledge of noise\ncharacteristics or extensive training.To validate the method performance we\nconduct simulation studies based on IEC Standard 61000 4 15 supported by\nstatistical analysis Monte Carlo simulations and real world data.Results\ndemonstrate superior accuracy robustness and reduced computational load\ncompared to Fast Fourier Transform and Discrete Wavelet Transform based\nestimators.\n", "link": "http://arxiv.org/abs/2506.13611v3", "date": "2025-08-29", "relevancy": 1.7804, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4791}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4244}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Artificial%20Intelligence%20Method%20for%20Estimating%20Flicker%20in%20Power%0A%20%20Systems&body=Title%3A%20A%20Hybrid%20Artificial%20Intelligence%20Method%20for%20Estimating%20Flicker%20in%20Power%0A%20%20Systems%0AAuthor%3A%20Javad%20Enayati%20and%20Pedram%20Asef%20and%20Alexandre%20Benoit%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20hybrid%20AI%20method%20combining%20H%20filtering%20and%20an%0Aadaptive%20linear%20neuron%20network%20for%20flicker%20component%20estimation%20in%20power%0Adistribution%20systems.The%20proposed%20method%20leverages%20the%20robustness%20of%20the%20H%0Afilter%20to%20extract%20the%20voltage%20envelope%20under%20uncertain%20and%20noisy%20conditions%0Afollowed%20by%20the%20use%20of%20ADALINE%20to%20accurately%20identify%20flicker%20frequencies%0Aembedded%20in%20the%20envelope.This%20synergy%20enables%20efficient%20time%20domain%20estimation%0Awith%20rapid%20convergence%20and%20noise%20resilience%20addressing%20key%20limitations%20of%0Aexisting%20frequency%20domain%20approaches.Unlike%20conventional%20techniques%20this%20hybrid%0AAI%20model%20handles%20complex%20power%20disturbances%20without%20prior%20knowledge%20of%20noise%0Acharacteristics%20or%20extensive%20training.To%20validate%20the%20method%20performance%20we%0Aconduct%20simulation%20studies%20based%20on%20IEC%20Standard%2061000%204%2015%20supported%20by%0Astatistical%20analysis%20Monte%20Carlo%20simulations%20and%20real%20world%20data.Results%0Ademonstrate%20superior%20accuracy%20robustness%20and%20reduced%20computational%20load%0Acompared%20to%20Fast%20Fourier%20Transform%20and%20Discrete%20Wavelet%20Transform%20based%0Aestimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13611v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Artificial%2520Intelligence%2520Method%2520for%2520Estimating%2520Flicker%2520in%2520Power%250A%2520%2520Systems%26entry.906535625%3DJavad%2520Enayati%2520and%2520Pedram%2520Asef%2520and%2520Alexandre%2520Benoit%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520hybrid%2520AI%2520method%2520combining%2520H%2520filtering%2520and%2520an%250Aadaptive%2520linear%2520neuron%2520network%2520for%2520flicker%2520component%2520estimation%2520in%2520power%250Adistribution%2520systems.The%2520proposed%2520method%2520leverages%2520the%2520robustness%2520of%2520the%2520H%250Afilter%2520to%2520extract%2520the%2520voltage%2520envelope%2520under%2520uncertain%2520and%2520noisy%2520conditions%250Afollowed%2520by%2520the%2520use%2520of%2520ADALINE%2520to%2520accurately%2520identify%2520flicker%2520frequencies%250Aembedded%2520in%2520the%2520envelope.This%2520synergy%2520enables%2520efficient%2520time%2520domain%2520estimation%250Awith%2520rapid%2520convergence%2520and%2520noise%2520resilience%2520addressing%2520key%2520limitations%2520of%250Aexisting%2520frequency%2520domain%2520approaches.Unlike%2520conventional%2520techniques%2520this%2520hybrid%250AAI%2520model%2520handles%2520complex%2520power%2520disturbances%2520without%2520prior%2520knowledge%2520of%2520noise%250Acharacteristics%2520or%2520extensive%2520training.To%2520validate%2520the%2520method%2520performance%2520we%250Aconduct%2520simulation%2520studies%2520based%2520on%2520IEC%2520Standard%252061000%25204%252015%2520supported%2520by%250Astatistical%2520analysis%2520Monte%2520Carlo%2520simulations%2520and%2520real%2520world%2520data.Results%250Ademonstrate%2520superior%2520accuracy%2520robustness%2520and%2520reduced%2520computational%2520load%250Acompared%2520to%2520Fast%2520Fourier%2520Transform%2520and%2520Discrete%2520Wavelet%2520Transform%2520based%250Aestimators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13611v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Artificial%20Intelligence%20Method%20for%20Estimating%20Flicker%20in%20Power%0A%20%20Systems&entry.906535625=Javad%20Enayati%20and%20Pedram%20Asef%20and%20Alexandre%20Benoit&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20hybrid%20AI%20method%20combining%20H%20filtering%20and%20an%0Aadaptive%20linear%20neuron%20network%20for%20flicker%20component%20estimation%20in%20power%0Adistribution%20systems.The%20proposed%20method%20leverages%20the%20robustness%20of%20the%20H%0Afilter%20to%20extract%20the%20voltage%20envelope%20under%20uncertain%20and%20noisy%20conditions%0Afollowed%20by%20the%20use%20of%20ADALINE%20to%20accurately%20identify%20flicker%20frequencies%0Aembedded%20in%20the%20envelope.This%20synergy%20enables%20efficient%20time%20domain%20estimation%0Awith%20rapid%20convergence%20and%20noise%20resilience%20addressing%20key%20limitations%20of%0Aexisting%20frequency%20domain%20approaches.Unlike%20conventional%20techniques%20this%20hybrid%0AAI%20model%20handles%20complex%20power%20disturbances%20without%20prior%20knowledge%20of%20noise%0Acharacteristics%20or%20extensive%20training.To%20validate%20the%20method%20performance%20we%0Aconduct%20simulation%20studies%20based%20on%20IEC%20Standard%2061000%204%2015%20supported%20by%0Astatistical%20analysis%20Monte%20Carlo%20simulations%20and%20real%20world%20data.Results%0Ademonstrate%20superior%20accuracy%20robustness%20and%20reduced%20computational%20load%0Acompared%20to%20Fast%20Fourier%20Transform%20and%20Discrete%20Wavelet%20Transform%20based%0Aestimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13611v3&entry.124074799=Read"},
{"title": "TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection\n  Models with a Text Memory Bank", "author": "Jiawei Liu and Jiahe Hou and Wei Wang and Jinsong Du and Yang Cong and Huijie Fan", "abstract": "  Anomaly detection, which aims to identify anomalies deviating from normal\npatterns, is challenging due to the limited amount of normal data available.\nUnlike most existing unified methods that rely on carefully designed image\nfeature extractors and memory banks to capture logical relationships between\nobjects, we introduce a text memory bank to enhance the detection of logical\nanomalies. Specifically, we propose a Three-Memory framework for Unified\nstructural and logical Anomaly Detection (TMUAD). First, we build a class-level\ntext memory bank for logical anomaly detection by the proposed logic-aware text\nextractor, which can capture rich logical descriptions of objects from input\nimages. Second, we construct an object-level image memory bank that preserves\ncomplete object contours by extracting features from segmented objects. Third,\nwe employ visual encoders to extract patch-level image features for\nconstructing a patch-level memory bank for structural anomaly detection. These\nthree complementary memory banks are used to retrieve and compare normal images\nthat are most similar to the query image, compute anomaly scores at multiple\nlevels, and fuse them into a final anomaly score. By unifying structural and\nlogical anomaly detection through collaborative memory banks, TMUAD achieves\nstate-of-the-art performance across seven publicly available datasets involving\nindustrial and medical domains. The model and code are available at\nhttps://github.com/SIA-IDE/TMUAD.\n", "link": "http://arxiv.org/abs/2508.21795v1", "date": "2025-08-29", "relevancy": 1.7616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6038}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5914}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TMUAD%3A%20Enhancing%20Logical%20Capabilities%20in%20Unified%20Anomaly%20Detection%0A%20%20Models%20with%20a%20Text%20Memory%20Bank&body=Title%3A%20TMUAD%3A%20Enhancing%20Logical%20Capabilities%20in%20Unified%20Anomaly%20Detection%0A%20%20Models%20with%20a%20Text%20Memory%20Bank%0AAuthor%3A%20Jiawei%20Liu%20and%20Jiahe%20Hou%20and%20Wei%20Wang%20and%20Jinsong%20Du%20and%20Yang%20Cong%20and%20Huijie%20Fan%0AAbstract%3A%20%20%20Anomaly%20detection%2C%20which%20aims%20to%20identify%20anomalies%20deviating%20from%20normal%0Apatterns%2C%20is%20challenging%20due%20to%20the%20limited%20amount%20of%20normal%20data%20available.%0AUnlike%20most%20existing%20unified%20methods%20that%20rely%20on%20carefully%20designed%20image%0Afeature%20extractors%20and%20memory%20banks%20to%20capture%20logical%20relationships%20between%0Aobjects%2C%20we%20introduce%20a%20text%20memory%20bank%20to%20enhance%20the%20detection%20of%20logical%0Aanomalies.%20Specifically%2C%20we%20propose%20a%20Three-Memory%20framework%20for%20Unified%0Astructural%20and%20logical%20Anomaly%20Detection%20%28TMUAD%29.%20First%2C%20we%20build%20a%20class-level%0Atext%20memory%20bank%20for%20logical%20anomaly%20detection%20by%20the%20proposed%20logic-aware%20text%0Aextractor%2C%20which%20can%20capture%20rich%20logical%20descriptions%20of%20objects%20from%20input%0Aimages.%20Second%2C%20we%20construct%20an%20object-level%20image%20memory%20bank%20that%20preserves%0Acomplete%20object%20contours%20by%20extracting%20features%20from%20segmented%20objects.%20Third%2C%0Awe%20employ%20visual%20encoders%20to%20extract%20patch-level%20image%20features%20for%0Aconstructing%20a%20patch-level%20memory%20bank%20for%20structural%20anomaly%20detection.%20These%0Athree%20complementary%20memory%20banks%20are%20used%20to%20retrieve%20and%20compare%20normal%20images%0Athat%20are%20most%20similar%20to%20the%20query%20image%2C%20compute%20anomaly%20scores%20at%20multiple%0Alevels%2C%20and%20fuse%20them%20into%20a%20final%20anomaly%20score.%20By%20unifying%20structural%20and%0Alogical%20anomaly%20detection%20through%20collaborative%20memory%20banks%2C%20TMUAD%20achieves%0Astate-of-the-art%20performance%20across%20seven%20publicly%20available%20datasets%20involving%0Aindustrial%20and%20medical%20domains.%20The%20model%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/SIA-IDE/TMUAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTMUAD%253A%2520Enhancing%2520Logical%2520Capabilities%2520in%2520Unified%2520Anomaly%2520Detection%250A%2520%2520Models%2520with%2520a%2520Text%2520Memory%2520Bank%26entry.906535625%3DJiawei%2520Liu%2520and%2520Jiahe%2520Hou%2520and%2520Wei%2520Wang%2520and%2520Jinsong%2520Du%2520and%2520Yang%2520Cong%2520and%2520Huijie%2520Fan%26entry.1292438233%3D%2520%2520Anomaly%2520detection%252C%2520which%2520aims%2520to%2520identify%2520anomalies%2520deviating%2520from%2520normal%250Apatterns%252C%2520is%2520challenging%2520due%2520to%2520the%2520limited%2520amount%2520of%2520normal%2520data%2520available.%250AUnlike%2520most%2520existing%2520unified%2520methods%2520that%2520rely%2520on%2520carefully%2520designed%2520image%250Afeature%2520extractors%2520and%2520memory%2520banks%2520to%2520capture%2520logical%2520relationships%2520between%250Aobjects%252C%2520we%2520introduce%2520a%2520text%2520memory%2520bank%2520to%2520enhance%2520the%2520detection%2520of%2520logical%250Aanomalies.%2520Specifically%252C%2520we%2520propose%2520a%2520Three-Memory%2520framework%2520for%2520Unified%250Astructural%2520and%2520logical%2520Anomaly%2520Detection%2520%2528TMUAD%2529.%2520First%252C%2520we%2520build%2520a%2520class-level%250Atext%2520memory%2520bank%2520for%2520logical%2520anomaly%2520detection%2520by%2520the%2520proposed%2520logic-aware%2520text%250Aextractor%252C%2520which%2520can%2520capture%2520rich%2520logical%2520descriptions%2520of%2520objects%2520from%2520input%250Aimages.%2520Second%252C%2520we%2520construct%2520an%2520object-level%2520image%2520memory%2520bank%2520that%2520preserves%250Acomplete%2520object%2520contours%2520by%2520extracting%2520features%2520from%2520segmented%2520objects.%2520Third%252C%250Awe%2520employ%2520visual%2520encoders%2520to%2520extract%2520patch-level%2520image%2520features%2520for%250Aconstructing%2520a%2520patch-level%2520memory%2520bank%2520for%2520structural%2520anomaly%2520detection.%2520These%250Athree%2520complementary%2520memory%2520banks%2520are%2520used%2520to%2520retrieve%2520and%2520compare%2520normal%2520images%250Athat%2520are%2520most%2520similar%2520to%2520the%2520query%2520image%252C%2520compute%2520anomaly%2520scores%2520at%2520multiple%250Alevels%252C%2520and%2520fuse%2520them%2520into%2520a%2520final%2520anomaly%2520score.%2520By%2520unifying%2520structural%2520and%250Alogical%2520anomaly%2520detection%2520through%2520collaborative%2520memory%2520banks%252C%2520TMUAD%2520achieves%250Astate-of-the-art%2520performance%2520across%2520seven%2520publicly%2520available%2520datasets%2520involving%250Aindustrial%2520and%2520medical%2520domains.%2520The%2520model%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/SIA-IDE/TMUAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TMUAD%3A%20Enhancing%20Logical%20Capabilities%20in%20Unified%20Anomaly%20Detection%0A%20%20Models%20with%20a%20Text%20Memory%20Bank&entry.906535625=Jiawei%20Liu%20and%20Jiahe%20Hou%20and%20Wei%20Wang%20and%20Jinsong%20Du%20and%20Yang%20Cong%20and%20Huijie%20Fan&entry.1292438233=%20%20Anomaly%20detection%2C%20which%20aims%20to%20identify%20anomalies%20deviating%20from%20normal%0Apatterns%2C%20is%20challenging%20due%20to%20the%20limited%20amount%20of%20normal%20data%20available.%0AUnlike%20most%20existing%20unified%20methods%20that%20rely%20on%20carefully%20designed%20image%0Afeature%20extractors%20and%20memory%20banks%20to%20capture%20logical%20relationships%20between%0Aobjects%2C%20we%20introduce%20a%20text%20memory%20bank%20to%20enhance%20the%20detection%20of%20logical%0Aanomalies.%20Specifically%2C%20we%20propose%20a%20Three-Memory%20framework%20for%20Unified%0Astructural%20and%20logical%20Anomaly%20Detection%20%28TMUAD%29.%20First%2C%20we%20build%20a%20class-level%0Atext%20memory%20bank%20for%20logical%20anomaly%20detection%20by%20the%20proposed%20logic-aware%20text%0Aextractor%2C%20which%20can%20capture%20rich%20logical%20descriptions%20of%20objects%20from%20input%0Aimages.%20Second%2C%20we%20construct%20an%20object-level%20image%20memory%20bank%20that%20preserves%0Acomplete%20object%20contours%20by%20extracting%20features%20from%20segmented%20objects.%20Third%2C%0Awe%20employ%20visual%20encoders%20to%20extract%20patch-level%20image%20features%20for%0Aconstructing%20a%20patch-level%20memory%20bank%20for%20structural%20anomaly%20detection.%20These%0Athree%20complementary%20memory%20banks%20are%20used%20to%20retrieve%20and%20compare%20normal%20images%0Athat%20are%20most%20similar%20to%20the%20query%20image%2C%20compute%20anomaly%20scores%20at%20multiple%0Alevels%2C%20and%20fuse%20them%20into%20a%20final%20anomaly%20score.%20By%20unifying%20structural%20and%0Alogical%20anomaly%20detection%20through%20collaborative%20memory%20banks%2C%20TMUAD%20achieves%0Astate-of-the-art%20performance%20across%20seven%20publicly%20available%20datasets%20involving%0Aindustrial%20and%20medical%20domains.%20The%20model%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/SIA-IDE/TMUAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21795v1&entry.124074799=Read"},
{"title": "Predicting Social Media Engagement from Emotional and Temporal Features", "author": "Yunwoo Kim and Junhyuk Hwang", "abstract": "  We present a machine learning approach for predicting social media engagement\n(comments and likes) from emotional and temporal features. The dataset contains\n600 songs with annotations for valence, arousal, and related sentiment metrics.\nA multi target regression model based on HistGradientBoostingRegressor is\ntrained on log transformed engagement ratios to address skewed targets.\nPerformance is evaluated with both a custom order of magnitude accuracy and\nstandard regression metrics, including the coefficient of determination (R^2).\nResults show that emotional and temporal metadata, together with existing view\ncounts, predict future engagement effectively. The model attains R^2 = 0.98 for\nlikes but only R^2 = 0.41 for comments. This gap indicates that likes are\nlargely driven by readily captured affective and exposure signals, whereas\ncomments depend on additional factors not represented in the current feature\nset.\n", "link": "http://arxiv.org/abs/2508.21650v1", "date": "2025-08-29", "relevancy": 1.755, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4331}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Social%20Media%20Engagement%20from%20Emotional%20and%20Temporal%20Features&body=Title%3A%20Predicting%20Social%20Media%20Engagement%20from%20Emotional%20and%20Temporal%20Features%0AAuthor%3A%20Yunwoo%20Kim%20and%20Junhyuk%20Hwang%0AAbstract%3A%20%20%20We%20present%20a%20machine%20learning%20approach%20for%20predicting%20social%20media%20engagement%0A%28comments%20and%20likes%29%20from%20emotional%20and%20temporal%20features.%20The%20dataset%20contains%0A600%20songs%20with%20annotations%20for%20valence%2C%20arousal%2C%20and%20related%20sentiment%20metrics.%0AA%20multi%20target%20regression%20model%20based%20on%20HistGradientBoostingRegressor%20is%0Atrained%20on%20log%20transformed%20engagement%20ratios%20to%20address%20skewed%20targets.%0APerformance%20is%20evaluated%20with%20both%20a%20custom%20order%20of%20magnitude%20accuracy%20and%0Astandard%20regression%20metrics%2C%20including%20the%20coefficient%20of%20determination%20%28R%5E2%29.%0AResults%20show%20that%20emotional%20and%20temporal%20metadata%2C%20together%20with%20existing%20view%0Acounts%2C%20predict%20future%20engagement%20effectively.%20The%20model%20attains%20R%5E2%20%3D%200.98%20for%0Alikes%20but%20only%20R%5E2%20%3D%200.41%20for%20comments.%20This%20gap%20indicates%20that%20likes%20are%0Alargely%20driven%20by%20readily%20captured%20affective%20and%20exposure%20signals%2C%20whereas%0Acomments%20depend%20on%20additional%20factors%20not%20represented%20in%20the%20current%20feature%0Aset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Social%2520Media%2520Engagement%2520from%2520Emotional%2520and%2520Temporal%2520Features%26entry.906535625%3DYunwoo%2520Kim%2520and%2520Junhyuk%2520Hwang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520machine%2520learning%2520approach%2520for%2520predicting%2520social%2520media%2520engagement%250A%2528comments%2520and%2520likes%2529%2520from%2520emotional%2520and%2520temporal%2520features.%2520The%2520dataset%2520contains%250A600%2520songs%2520with%2520annotations%2520for%2520valence%252C%2520arousal%252C%2520and%2520related%2520sentiment%2520metrics.%250AA%2520multi%2520target%2520regression%2520model%2520based%2520on%2520HistGradientBoostingRegressor%2520is%250Atrained%2520on%2520log%2520transformed%2520engagement%2520ratios%2520to%2520address%2520skewed%2520targets.%250APerformance%2520is%2520evaluated%2520with%2520both%2520a%2520custom%2520order%2520of%2520magnitude%2520accuracy%2520and%250Astandard%2520regression%2520metrics%252C%2520including%2520the%2520coefficient%2520of%2520determination%2520%2528R%255E2%2529.%250AResults%2520show%2520that%2520emotional%2520and%2520temporal%2520metadata%252C%2520together%2520with%2520existing%2520view%250Acounts%252C%2520predict%2520future%2520engagement%2520effectively.%2520The%2520model%2520attains%2520R%255E2%2520%253D%25200.98%2520for%250Alikes%2520but%2520only%2520R%255E2%2520%253D%25200.41%2520for%2520comments.%2520This%2520gap%2520indicates%2520that%2520likes%2520are%250Alargely%2520driven%2520by%2520readily%2520captured%2520affective%2520and%2520exposure%2520signals%252C%2520whereas%250Acomments%2520depend%2520on%2520additional%2520factors%2520not%2520represented%2520in%2520the%2520current%2520feature%250Aset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Social%20Media%20Engagement%20from%20Emotional%20and%20Temporal%20Features&entry.906535625=Yunwoo%20Kim%20and%20Junhyuk%20Hwang&entry.1292438233=%20%20We%20present%20a%20machine%20learning%20approach%20for%20predicting%20social%20media%20engagement%0A%28comments%20and%20likes%29%20from%20emotional%20and%20temporal%20features.%20The%20dataset%20contains%0A600%20songs%20with%20annotations%20for%20valence%2C%20arousal%2C%20and%20related%20sentiment%20metrics.%0AA%20multi%20target%20regression%20model%20based%20on%20HistGradientBoostingRegressor%20is%0Atrained%20on%20log%20transformed%20engagement%20ratios%20to%20address%20skewed%20targets.%0APerformance%20is%20evaluated%20with%20both%20a%20custom%20order%20of%20magnitude%20accuracy%20and%0Astandard%20regression%20metrics%2C%20including%20the%20coefficient%20of%20determination%20%28R%5E2%29.%0AResults%20show%20that%20emotional%20and%20temporal%20metadata%2C%20together%20with%20existing%20view%0Acounts%2C%20predict%20future%20engagement%20effectively.%20The%20model%20attains%20R%5E2%20%3D%200.98%20for%0Alikes%20but%20only%20R%5E2%20%3D%200.41%20for%20comments.%20This%20gap%20indicates%20that%20likes%20are%0Alargely%20driven%20by%20readily%20captured%20affective%20and%20exposure%20signals%2C%20whereas%0Acomments%20depend%20on%20additional%20factors%20not%20represented%20in%20the%20current%20feature%0Aset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21650v1&entry.124074799=Read"},
{"title": "A Survey on Current Trends and Recent Advances in Text Anonymization", "author": "Tobias Deu\u00dfer and Lorenz Sparrenberg and Armin Berger and Max Hahnb\u00fcck and Christian Bauckhage and Rafet Sifa", "abstract": "  The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.\n", "link": "http://arxiv.org/abs/2508.21587v1", "date": "2025-08-29", "relevancy": 1.7548, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4481}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4343}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Current%20Trends%20and%20Recent%20Advances%20in%20Text%20Anonymization&body=Title%3A%20A%20Survey%20on%20Current%20Trends%20and%20Recent%20Advances%20in%20Text%20Anonymization%0AAuthor%3A%20Tobias%20Deu%C3%9Fer%20and%20Lorenz%20Sparrenberg%20and%20Armin%20Berger%20and%20Max%20Hahnb%C3%BCck%20and%20Christian%20Bauckhage%20and%20Rafet%20Sifa%0AAbstract%3A%20%20%20The%20proliferation%20of%20textual%20data%20containing%20sensitive%20personal%20information%0Aacross%20various%20domains%20requires%20robust%20anonymization%20techniques%20to%20protect%0Aprivacy%20and%20comply%20with%20regulations%2C%20while%20preserving%20data%20usability%20for%0Adiverse%20and%20crucial%20downstream%20tasks.%20This%20survey%20provides%20a%20comprehensive%0Aoverview%20of%20current%20trends%20and%20recent%20advances%20in%20text%20anonymization%0Atechniques.%20We%20begin%20by%20discussing%20foundational%20approaches%2C%20primarily%20centered%0Aon%20Named%20Entity%20Recognition%2C%20before%20examining%20the%20transformative%20impact%20of%0ALarge%20Language%20Models%2C%20detailing%20their%20dual%20role%20as%20sophisticated%20anonymizers%0Aand%20potent%20de-anonymization%20threats.%20The%20survey%20further%20explores%0Adomain-specific%20challenges%20and%20tailored%20solutions%20in%20critical%20sectors%20such%20as%0Ahealthcare%2C%20law%2C%20finance%2C%20and%20education.%20We%20investigate%20advanced%20methodologies%0Aincorporating%20formal%20privacy%20models%20and%20risk-aware%20frameworks%2C%20and%20address%20the%0Aspecialized%20subfield%20of%20authorship%20anonymization.%20Additionally%2C%20we%20review%0Aevaluation%20frameworks%2C%20comprehensive%20metrics%2C%20benchmarks%2C%20and%20practical%0Atoolkits%20for%20real-world%20deployment%20of%20anonymization%20solutions.%20This%20review%0Aconsolidates%20current%20knowledge%2C%20identifies%20emerging%20trends%20and%20persistent%0Achallenges%2C%20including%20the%20evolving%20privacy-utility%20trade-off%2C%20the%20need%20to%0Aaddress%20quasi-identifiers%2C%20and%20the%20implications%20of%20LLM%20capabilities%2C%20and%20aims%0Ato%20guide%20future%20research%20directions%20for%20both%20academics%20and%20practitioners%20in%0Athis%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Current%2520Trends%2520and%2520Recent%2520Advances%2520in%2520Text%2520Anonymization%26entry.906535625%3DTobias%2520Deu%25C3%259Fer%2520and%2520Lorenz%2520Sparrenberg%2520and%2520Armin%2520Berger%2520and%2520Max%2520Hahnb%25C3%25BCck%2520and%2520Christian%2520Bauckhage%2520and%2520Rafet%2520Sifa%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520textual%2520data%2520containing%2520sensitive%2520personal%2520information%250Aacross%2520various%2520domains%2520requires%2520robust%2520anonymization%2520techniques%2520to%2520protect%250Aprivacy%2520and%2520comply%2520with%2520regulations%252C%2520while%2520preserving%2520data%2520usability%2520for%250Adiverse%2520and%2520crucial%2520downstream%2520tasks.%2520This%2520survey%2520provides%2520a%2520comprehensive%250Aoverview%2520of%2520current%2520trends%2520and%2520recent%2520advances%2520in%2520text%2520anonymization%250Atechniques.%2520We%2520begin%2520by%2520discussing%2520foundational%2520approaches%252C%2520primarily%2520centered%250Aon%2520Named%2520Entity%2520Recognition%252C%2520before%2520examining%2520the%2520transformative%2520impact%2520of%250ALarge%2520Language%2520Models%252C%2520detailing%2520their%2520dual%2520role%2520as%2520sophisticated%2520anonymizers%250Aand%2520potent%2520de-anonymization%2520threats.%2520The%2520survey%2520further%2520explores%250Adomain-specific%2520challenges%2520and%2520tailored%2520solutions%2520in%2520critical%2520sectors%2520such%2520as%250Ahealthcare%252C%2520law%252C%2520finance%252C%2520and%2520education.%2520We%2520investigate%2520advanced%2520methodologies%250Aincorporating%2520formal%2520privacy%2520models%2520and%2520risk-aware%2520frameworks%252C%2520and%2520address%2520the%250Aspecialized%2520subfield%2520of%2520authorship%2520anonymization.%2520Additionally%252C%2520we%2520review%250Aevaluation%2520frameworks%252C%2520comprehensive%2520metrics%252C%2520benchmarks%252C%2520and%2520practical%250Atoolkits%2520for%2520real-world%2520deployment%2520of%2520anonymization%2520solutions.%2520This%2520review%250Aconsolidates%2520current%2520knowledge%252C%2520identifies%2520emerging%2520trends%2520and%2520persistent%250Achallenges%252C%2520including%2520the%2520evolving%2520privacy-utility%2520trade-off%252C%2520the%2520need%2520to%250Aaddress%2520quasi-identifiers%252C%2520and%2520the%2520implications%2520of%2520LLM%2520capabilities%252C%2520and%2520aims%250Ato%2520guide%2520future%2520research%2520directions%2520for%2520both%2520academics%2520and%2520practitioners%2520in%250Athis%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Current%20Trends%20and%20Recent%20Advances%20in%20Text%20Anonymization&entry.906535625=Tobias%20Deu%C3%9Fer%20and%20Lorenz%20Sparrenberg%20and%20Armin%20Berger%20and%20Max%20Hahnb%C3%BCck%20and%20Christian%20Bauckhage%20and%20Rafet%20Sifa&entry.1292438233=%20%20The%20proliferation%20of%20textual%20data%20containing%20sensitive%20personal%20information%0Aacross%20various%20domains%20requires%20robust%20anonymization%20techniques%20to%20protect%0Aprivacy%20and%20comply%20with%20regulations%2C%20while%20preserving%20data%20usability%20for%0Adiverse%20and%20crucial%20downstream%20tasks.%20This%20survey%20provides%20a%20comprehensive%0Aoverview%20of%20current%20trends%20and%20recent%20advances%20in%20text%20anonymization%0Atechniques.%20We%20begin%20by%20discussing%20foundational%20approaches%2C%20primarily%20centered%0Aon%20Named%20Entity%20Recognition%2C%20before%20examining%20the%20transformative%20impact%20of%0ALarge%20Language%20Models%2C%20detailing%20their%20dual%20role%20as%20sophisticated%20anonymizers%0Aand%20potent%20de-anonymization%20threats.%20The%20survey%20further%20explores%0Adomain-specific%20challenges%20and%20tailored%20solutions%20in%20critical%20sectors%20such%20as%0Ahealthcare%2C%20law%2C%20finance%2C%20and%20education.%20We%20investigate%20advanced%20methodologies%0Aincorporating%20formal%20privacy%20models%20and%20risk-aware%20frameworks%2C%20and%20address%20the%0Aspecialized%20subfield%20of%20authorship%20anonymization.%20Additionally%2C%20we%20review%0Aevaluation%20frameworks%2C%20comprehensive%20metrics%2C%20benchmarks%2C%20and%20practical%0Atoolkits%20for%20real-world%20deployment%20of%20anonymization%20solutions.%20This%20review%0Aconsolidates%20current%20knowledge%2C%20identifies%20emerging%20trends%20and%20persistent%0Achallenges%2C%20including%20the%20evolving%20privacy-utility%20trade-off%2C%20the%20need%20to%0Aaddress%20quasi-identifiers%2C%20and%20the%20implications%20of%20LLM%20capabilities%2C%20and%20aims%0Ato%20guide%20future%20research%20directions%20for%20both%20academics%20and%20practitioners%20in%0Athis%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21587v1&entry.124074799=Read"},
{"title": "Developer Insights into Designing AI-Based Computer Perception Tools", "author": "Maya Guhan and Meghan E. Hurley and Eric A. Storch and John Herrington and Casey Zampella and Julia Parish-Morris and Gabriel L\u00e1zaro-Mu\u00f1oz and Kristin Kostick-Quenet", "abstract": "  Artificial intelligence (AI)-based computer perception (CP) technologies use\nmobile sensors to collect behavioral and physiological data for clinical\ndecision-making. These tools can reshape how clinical knowledge is generated\nand interpreted. However, effective integration of these tools into clinical\nworkflows depends on how developers balance clinical utility with user\nacceptability and trustworthiness. Our study presents findings from 20 in-depth\ninterviews with developers of AI-based CP tools. Interviews were transcribed\nand inductive, thematic analysis was performed to identify 4 key design\npriorities: 1) to account for context and ensure explainability for both\npatients and clinicians; 2) align tools with existing clinical workflows; 3)\nappropriately customize to relevant stakeholders for usability and\nacceptability; and 4) push the boundaries of innovation while aligning with\nestablished paradigms. Our findings highlight that developers view themselves\nas not merely technical architects but also ethical stewards, designing tools\nthat are both acceptable by users and epistemically responsible (prioritizing\nobjectivity and pushing clinical knowledge forward). We offer the following\nsuggestions to help achieve this balance: documenting how design choices around\ncustomization are made, defining limits for customization choices,\ntransparently conveying information about outputs, and investing in user\ntraining. Achieving these goals will require interdisciplinary collaboration\nbetween developers, clinicians, and ethicists.\n", "link": "http://arxiv.org/abs/2508.21733v1", "date": "2025-08-29", "relevancy": 1.7478, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4172}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developer%20Insights%20into%20Designing%20AI-Based%20Computer%20Perception%20Tools&body=Title%3A%20Developer%20Insights%20into%20Designing%20AI-Based%20Computer%20Perception%20Tools%0AAuthor%3A%20Maya%20Guhan%20and%20Meghan%20E.%20Hurley%20and%20Eric%20A.%20Storch%20and%20John%20Herrington%20and%20Casey%20Zampella%20and%20Julia%20Parish-Morris%20and%20Gabriel%20L%C3%A1zaro-Mu%C3%B1oz%20and%20Kristin%20Kostick-Quenet%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29-based%20computer%20perception%20%28CP%29%20technologies%20use%0Amobile%20sensors%20to%20collect%20behavioral%20and%20physiological%20data%20for%20clinical%0Adecision-making.%20These%20tools%20can%20reshape%20how%20clinical%20knowledge%20is%20generated%0Aand%20interpreted.%20However%2C%20effective%20integration%20of%20these%20tools%20into%20clinical%0Aworkflows%20depends%20on%20how%20developers%20balance%20clinical%20utility%20with%20user%0Aacceptability%20and%20trustworthiness.%20Our%20study%20presents%20findings%20from%2020%20in-depth%0Ainterviews%20with%20developers%20of%20AI-based%20CP%20tools.%20Interviews%20were%20transcribed%0Aand%20inductive%2C%20thematic%20analysis%20was%20performed%20to%20identify%204%20key%20design%0Apriorities%3A%201%29%20to%20account%20for%20context%20and%20ensure%20explainability%20for%20both%0Apatients%20and%20clinicians%3B%202%29%20align%20tools%20with%20existing%20clinical%20workflows%3B%203%29%0Aappropriately%20customize%20to%20relevant%20stakeholders%20for%20usability%20and%0Aacceptability%3B%20and%204%29%20push%20the%20boundaries%20of%20innovation%20while%20aligning%20with%0Aestablished%20paradigms.%20Our%20findings%20highlight%20that%20developers%20view%20themselves%0Aas%20not%20merely%20technical%20architects%20but%20also%20ethical%20stewards%2C%20designing%20tools%0Athat%20are%20both%20acceptable%20by%20users%20and%20epistemically%20responsible%20%28prioritizing%0Aobjectivity%20and%20pushing%20clinical%20knowledge%20forward%29.%20We%20offer%20the%20following%0Asuggestions%20to%20help%20achieve%20this%20balance%3A%20documenting%20how%20design%20choices%20around%0Acustomization%20are%20made%2C%20defining%20limits%20for%20customization%20choices%2C%0Atransparently%20conveying%20information%20about%20outputs%2C%20and%20investing%20in%20user%0Atraining.%20Achieving%20these%20goals%20will%20require%20interdisciplinary%20collaboration%0Abetween%20developers%2C%20clinicians%2C%20and%20ethicists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloper%2520Insights%2520into%2520Designing%2520AI-Based%2520Computer%2520Perception%2520Tools%26entry.906535625%3DMaya%2520Guhan%2520and%2520Meghan%2520E.%2520Hurley%2520and%2520Eric%2520A.%2520Storch%2520and%2520John%2520Herrington%2520and%2520Casey%2520Zampella%2520and%2520Julia%2520Parish-Morris%2520and%2520Gabriel%2520L%25C3%25A1zaro-Mu%25C3%25B1oz%2520and%2520Kristin%2520Kostick-Quenet%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529-based%2520computer%2520perception%2520%2528CP%2529%2520technologies%2520use%250Amobile%2520sensors%2520to%2520collect%2520behavioral%2520and%2520physiological%2520data%2520for%2520clinical%250Adecision-making.%2520These%2520tools%2520can%2520reshape%2520how%2520clinical%2520knowledge%2520is%2520generated%250Aand%2520interpreted.%2520However%252C%2520effective%2520integration%2520of%2520these%2520tools%2520into%2520clinical%250Aworkflows%2520depends%2520on%2520how%2520developers%2520balance%2520clinical%2520utility%2520with%2520user%250Aacceptability%2520and%2520trustworthiness.%2520Our%2520study%2520presents%2520findings%2520from%252020%2520in-depth%250Ainterviews%2520with%2520developers%2520of%2520AI-based%2520CP%2520tools.%2520Interviews%2520were%2520transcribed%250Aand%2520inductive%252C%2520thematic%2520analysis%2520was%2520performed%2520to%2520identify%25204%2520key%2520design%250Apriorities%253A%25201%2529%2520to%2520account%2520for%2520context%2520and%2520ensure%2520explainability%2520for%2520both%250Apatients%2520and%2520clinicians%253B%25202%2529%2520align%2520tools%2520with%2520existing%2520clinical%2520workflows%253B%25203%2529%250Aappropriately%2520customize%2520to%2520relevant%2520stakeholders%2520for%2520usability%2520and%250Aacceptability%253B%2520and%25204%2529%2520push%2520the%2520boundaries%2520of%2520innovation%2520while%2520aligning%2520with%250Aestablished%2520paradigms.%2520Our%2520findings%2520highlight%2520that%2520developers%2520view%2520themselves%250Aas%2520not%2520merely%2520technical%2520architects%2520but%2520also%2520ethical%2520stewards%252C%2520designing%2520tools%250Athat%2520are%2520both%2520acceptable%2520by%2520users%2520and%2520epistemically%2520responsible%2520%2528prioritizing%250Aobjectivity%2520and%2520pushing%2520clinical%2520knowledge%2520forward%2529.%2520We%2520offer%2520the%2520following%250Asuggestions%2520to%2520help%2520achieve%2520this%2520balance%253A%2520documenting%2520how%2520design%2520choices%2520around%250Acustomization%2520are%2520made%252C%2520defining%2520limits%2520for%2520customization%2520choices%252C%250Atransparently%2520conveying%2520information%2520about%2520outputs%252C%2520and%2520investing%2520in%2520user%250Atraining.%2520Achieving%2520these%2520goals%2520will%2520require%2520interdisciplinary%2520collaboration%250Abetween%2520developers%252C%2520clinicians%252C%2520and%2520ethicists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developer%20Insights%20into%20Designing%20AI-Based%20Computer%20Perception%20Tools&entry.906535625=Maya%20Guhan%20and%20Meghan%20E.%20Hurley%20and%20Eric%20A.%20Storch%20and%20John%20Herrington%20and%20Casey%20Zampella%20and%20Julia%20Parish-Morris%20and%20Gabriel%20L%C3%A1zaro-Mu%C3%B1oz%20and%20Kristin%20Kostick-Quenet&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29-based%20computer%20perception%20%28CP%29%20technologies%20use%0Amobile%20sensors%20to%20collect%20behavioral%20and%20physiological%20data%20for%20clinical%0Adecision-making.%20These%20tools%20can%20reshape%20how%20clinical%20knowledge%20is%20generated%0Aand%20interpreted.%20However%2C%20effective%20integration%20of%20these%20tools%20into%20clinical%0Aworkflows%20depends%20on%20how%20developers%20balance%20clinical%20utility%20with%20user%0Aacceptability%20and%20trustworthiness.%20Our%20study%20presents%20findings%20from%2020%20in-depth%0Ainterviews%20with%20developers%20of%20AI-based%20CP%20tools.%20Interviews%20were%20transcribed%0Aand%20inductive%2C%20thematic%20analysis%20was%20performed%20to%20identify%204%20key%20design%0Apriorities%3A%201%29%20to%20account%20for%20context%20and%20ensure%20explainability%20for%20both%0Apatients%20and%20clinicians%3B%202%29%20align%20tools%20with%20existing%20clinical%20workflows%3B%203%29%0Aappropriately%20customize%20to%20relevant%20stakeholders%20for%20usability%20and%0Aacceptability%3B%20and%204%29%20push%20the%20boundaries%20of%20innovation%20while%20aligning%20with%0Aestablished%20paradigms.%20Our%20findings%20highlight%20that%20developers%20view%20themselves%0Aas%20not%20merely%20technical%20architects%20but%20also%20ethical%20stewards%2C%20designing%20tools%0Athat%20are%20both%20acceptable%20by%20users%20and%20epistemically%20responsible%20%28prioritizing%0Aobjectivity%20and%20pushing%20clinical%20knowledge%20forward%29.%20We%20offer%20the%20following%0Asuggestions%20to%20help%20achieve%20this%20balance%3A%20documenting%20how%20design%20choices%20around%0Acustomization%20are%20made%2C%20defining%20limits%20for%20customization%20choices%2C%0Atransparently%20conveying%20information%20about%20outputs%2C%20and%20investing%20in%20user%0Atraining.%20Achieving%20these%20goals%20will%20require%20interdisciplinary%20collaboration%0Abetween%20developers%2C%20clinicians%2C%20and%20ethicists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21733v1&entry.124074799=Read"},
{"title": "Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman\n  Problem", "author": "Fabrizio Fagiolo and Nicolo' Vescera", "abstract": "  In this paper we present a variational algorithm for the Traveling Salesman\nProblem (TSP) that combines (i) a compact encoding of permutations, which\nreduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:\nwhere the circuit topology (``Ansatz'') is first optimized on a training\ninstance by Simulated Annealing (SA), then ``frozen'' and re-used on novel\ninstances, limited to a rapid re-optimization of only the circuit parameters.\nThis pipeline eliminates costly structural research in testing, making the\nprocedure immediately implementable on NISQ hardware.\n  On a set of $40$ randomly generated symmetric instances that span $4 - 7$\ncities, the resulting Ansatz achieves an average optimal trip sampling\nprobability of $100\\%$ for 4 city cases, $90\\%$ for 5 city cases and $80\\%$ for\n6 city cases. With 7 cities the success rate drops markedly to an average of\n$\\sim 20\\%$, revealing the onset of scalability limitations of the proposed\nmethod.\n  The results show robust generalization ability for moderate problem sizes and\nindicate how freezing the Ansatz can dramatically reduce time-to-solution\nwithout degrading solution quality. The paper also discusses scalability\nlimitations, the impact of ``warm-start'' initialization of parameters, and\nprospects for extension to more complex problems, such as Vehicle Routing and\nJob-Shop Scheduling.\n", "link": "http://arxiv.org/abs/2508.21730v1", "date": "2025-08-29", "relevancy": 1.7456, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.442}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freeze%20and%20Conquer%3A%20Reusable%20Ansatz%20for%20Solving%20the%20Traveling%20Salesman%0A%20%20Problem&body=Title%3A%20Freeze%20and%20Conquer%3A%20Reusable%20Ansatz%20for%20Solving%20the%20Traveling%20Salesman%0A%20%20Problem%0AAuthor%3A%20Fabrizio%20Fagiolo%20and%20Nicolo%27%20Vescera%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20a%20variational%20algorithm%20for%20the%20Traveling%20Salesman%0AProblem%20%28TSP%29%20that%20combines%20%28i%29%20a%20compact%20encoding%20of%20permutations%2C%20which%0Areduces%20the%20qubit%20requirement%20too%2C%20%28ii%29%20an%20optimize-freeze-reuse%20strategy%3A%0Awhere%20the%20circuit%20topology%20%28%60%60Ansatz%27%27%29%20is%20first%20optimized%20on%20a%20training%0Ainstance%20by%20Simulated%20Annealing%20%28SA%29%2C%20then%20%60%60frozen%27%27%20and%20re-used%20on%20novel%0Ainstances%2C%20limited%20to%20a%20rapid%20re-optimization%20of%20only%20the%20circuit%20parameters.%0AThis%20pipeline%20eliminates%20costly%20structural%20research%20in%20testing%2C%20making%20the%0Aprocedure%20immediately%20implementable%20on%20NISQ%20hardware.%0A%20%20On%20a%20set%20of%20%2440%24%20randomly%20generated%20symmetric%20instances%20that%20span%20%244%20-%207%24%0Acities%2C%20the%20resulting%20Ansatz%20achieves%20an%20average%20optimal%20trip%20sampling%0Aprobability%20of%20%24100%5C%25%24%20for%204%20city%20cases%2C%20%2490%5C%25%24%20for%205%20city%20cases%20and%20%2480%5C%25%24%20for%0A6%20city%20cases.%20With%207%20cities%20the%20success%20rate%20drops%20markedly%20to%20an%20average%20of%0A%24%5Csim%2020%5C%25%24%2C%20revealing%20the%20onset%20of%20scalability%20limitations%20of%20the%20proposed%0Amethod.%0A%20%20The%20results%20show%20robust%20generalization%20ability%20for%20moderate%20problem%20sizes%20and%0Aindicate%20how%20freezing%20the%20Ansatz%20can%20dramatically%20reduce%20time-to-solution%0Awithout%20degrading%20solution%20quality.%20The%20paper%20also%20discusses%20scalability%0Alimitations%2C%20the%20impact%20of%20%60%60warm-start%27%27%20initialization%20of%20parameters%2C%20and%0Aprospects%20for%20extension%20to%20more%20complex%20problems%2C%20such%20as%20Vehicle%20Routing%20and%0AJob-Shop%20Scheduling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeze%2520and%2520Conquer%253A%2520Reusable%2520Ansatz%2520for%2520Solving%2520the%2520Traveling%2520Salesman%250A%2520%2520Problem%26entry.906535625%3DFabrizio%2520Fagiolo%2520and%2520Nicolo%2527%2520Vescera%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520a%2520variational%2520algorithm%2520for%2520the%2520Traveling%2520Salesman%250AProblem%2520%2528TSP%2529%2520that%2520combines%2520%2528i%2529%2520a%2520compact%2520encoding%2520of%2520permutations%252C%2520which%250Areduces%2520the%2520qubit%2520requirement%2520too%252C%2520%2528ii%2529%2520an%2520optimize-freeze-reuse%2520strategy%253A%250Awhere%2520the%2520circuit%2520topology%2520%2528%2560%2560Ansatz%2527%2527%2529%2520is%2520first%2520optimized%2520on%2520a%2520training%250Ainstance%2520by%2520Simulated%2520Annealing%2520%2528SA%2529%252C%2520then%2520%2560%2560frozen%2527%2527%2520and%2520re-used%2520on%2520novel%250Ainstances%252C%2520limited%2520to%2520a%2520rapid%2520re-optimization%2520of%2520only%2520the%2520circuit%2520parameters.%250AThis%2520pipeline%2520eliminates%2520costly%2520structural%2520research%2520in%2520testing%252C%2520making%2520the%250Aprocedure%2520immediately%2520implementable%2520on%2520NISQ%2520hardware.%250A%2520%2520On%2520a%2520set%2520of%2520%252440%2524%2520randomly%2520generated%2520symmetric%2520instances%2520that%2520span%2520%25244%2520-%25207%2524%250Acities%252C%2520the%2520resulting%2520Ansatz%2520achieves%2520an%2520average%2520optimal%2520trip%2520sampling%250Aprobability%2520of%2520%2524100%255C%2525%2524%2520for%25204%2520city%2520cases%252C%2520%252490%255C%2525%2524%2520for%25205%2520city%2520cases%2520and%2520%252480%255C%2525%2524%2520for%250A6%2520city%2520cases.%2520With%25207%2520cities%2520the%2520success%2520rate%2520drops%2520markedly%2520to%2520an%2520average%2520of%250A%2524%255Csim%252020%255C%2525%2524%252C%2520revealing%2520the%2520onset%2520of%2520scalability%2520limitations%2520of%2520the%2520proposed%250Amethod.%250A%2520%2520The%2520results%2520show%2520robust%2520generalization%2520ability%2520for%2520moderate%2520problem%2520sizes%2520and%250Aindicate%2520how%2520freezing%2520the%2520Ansatz%2520can%2520dramatically%2520reduce%2520time-to-solution%250Awithout%2520degrading%2520solution%2520quality.%2520The%2520paper%2520also%2520discusses%2520scalability%250Alimitations%252C%2520the%2520impact%2520of%2520%2560%2560warm-start%2527%2527%2520initialization%2520of%2520parameters%252C%2520and%250Aprospects%2520for%2520extension%2520to%2520more%2520complex%2520problems%252C%2520such%2520as%2520Vehicle%2520Routing%2520and%250AJob-Shop%2520Scheduling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freeze%20and%20Conquer%3A%20Reusable%20Ansatz%20for%20Solving%20the%20Traveling%20Salesman%0A%20%20Problem&entry.906535625=Fabrizio%20Fagiolo%20and%20Nicolo%27%20Vescera&entry.1292438233=%20%20In%20this%20paper%20we%20present%20a%20variational%20algorithm%20for%20the%20Traveling%20Salesman%0AProblem%20%28TSP%29%20that%20combines%20%28i%29%20a%20compact%20encoding%20of%20permutations%2C%20which%0Areduces%20the%20qubit%20requirement%20too%2C%20%28ii%29%20an%20optimize-freeze-reuse%20strategy%3A%0Awhere%20the%20circuit%20topology%20%28%60%60Ansatz%27%27%29%20is%20first%20optimized%20on%20a%20training%0Ainstance%20by%20Simulated%20Annealing%20%28SA%29%2C%20then%20%60%60frozen%27%27%20and%20re-used%20on%20novel%0Ainstances%2C%20limited%20to%20a%20rapid%20re-optimization%20of%20only%20the%20circuit%20parameters.%0AThis%20pipeline%20eliminates%20costly%20structural%20research%20in%20testing%2C%20making%20the%0Aprocedure%20immediately%20implementable%20on%20NISQ%20hardware.%0A%20%20On%20a%20set%20of%20%2440%24%20randomly%20generated%20symmetric%20instances%20that%20span%20%244%20-%207%24%0Acities%2C%20the%20resulting%20Ansatz%20achieves%20an%20average%20optimal%20trip%20sampling%0Aprobability%20of%20%24100%5C%25%24%20for%204%20city%20cases%2C%20%2490%5C%25%24%20for%205%20city%20cases%20and%20%2480%5C%25%24%20for%0A6%20city%20cases.%20With%207%20cities%20the%20success%20rate%20drops%20markedly%20to%20an%20average%20of%0A%24%5Csim%2020%5C%25%24%2C%20revealing%20the%20onset%20of%20scalability%20limitations%20of%20the%20proposed%0Amethod.%0A%20%20The%20results%20show%20robust%20generalization%20ability%20for%20moderate%20problem%20sizes%20and%0Aindicate%20how%20freezing%20the%20Ansatz%20can%20dramatically%20reduce%20time-to-solution%0Awithout%20degrading%20solution%20quality.%20The%20paper%20also%20discusses%20scalability%0Alimitations%2C%20the%20impact%20of%20%60%60warm-start%27%27%20initialization%20of%20parameters%2C%20and%0Aprospects%20for%20extension%20to%20more%20complex%20problems%2C%20such%20as%20Vehicle%20Routing%20and%0AJob-Shop%20Scheduling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21730v1&entry.124074799=Read"},
{"title": "Limitations of Physics-Informed Neural Networks: a Study on Smart Grid\n  Surrogation", "author": "Julen Cestero and Carmine Delle Femine and Kenji S. Muro and Marco Quartulli and Marcello Restelli", "abstract": "  Physics-Informed Neural Networks (PINNs) present a transformative approach\nfor smart grid modeling by integrating physical laws directly into learning\nframeworks, addressing critical challenges of data scarcity and physical\nconsistency in conventional data-driven methods. This paper evaluates PINNs'\ncapabilities as surrogate models for smart grid dynamics, comparing their\nperformance against XGBoost, Random Forest, and Linear Regression across three\nkey experiments: interpolation, cross-validation, and episodic trajectory\nprediction. By training PINNs exclusively through physics-based loss functions\n(enforcing power balance, operational constraints, and grid stability) we\ndemonstrate their superior generalization, outperforming data-driven models in\nerror reduction. Notably, PINNs maintain comparatively lower MAE in dynamic\ngrid operations, reliably capturing state transitions in both random and\nexpert-driven control scenarios, while traditional models exhibit erratic\nperformance. Despite slight degradation in extreme operational regimes, PINNs\nconsistently enforce physical feasibility, proving vital for safety-critical\napplications. Our results contribute to establishing PINNs as a\nparadigm-shifting tool for smart grid surrogation, bridging data-driven\nflexibility with first-principles rigor. This work advances real-time grid\ncontrol and scalable digital twins, emphasizing the necessity of physics-aware\narchitectures in mission-critical energy systems.\n", "link": "http://arxiv.org/abs/2508.21559v1", "date": "2025-08-29", "relevancy": 1.4532, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4817}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limitations%20of%20Physics-Informed%20Neural%20Networks%3A%20a%20Study%20on%20Smart%20Grid%0A%20%20Surrogation&body=Title%3A%20Limitations%20of%20Physics-Informed%20Neural%20Networks%3A%20a%20Study%20on%20Smart%20Grid%0A%20%20Surrogation%0AAuthor%3A%20Julen%20Cestero%20and%20Carmine%20Delle%20Femine%20and%20Kenji%20S.%20Muro%20and%20Marco%20Quartulli%20and%20Marcello%20Restelli%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20present%20a%20transformative%20approach%0Afor%20smart%20grid%20modeling%20by%20integrating%20physical%20laws%20directly%20into%20learning%0Aframeworks%2C%20addressing%20critical%20challenges%20of%20data%20scarcity%20and%20physical%0Aconsistency%20in%20conventional%20data-driven%20methods.%20This%20paper%20evaluates%20PINNs%27%0Acapabilities%20as%20surrogate%20models%20for%20smart%20grid%20dynamics%2C%20comparing%20their%0Aperformance%20against%20XGBoost%2C%20Random%20Forest%2C%20and%20Linear%20Regression%20across%20three%0Akey%20experiments%3A%20interpolation%2C%20cross-validation%2C%20and%20episodic%20trajectory%0Aprediction.%20By%20training%20PINNs%20exclusively%20through%20physics-based%20loss%20functions%0A%28enforcing%20power%20balance%2C%20operational%20constraints%2C%20and%20grid%20stability%29%20we%0Ademonstrate%20their%20superior%20generalization%2C%20outperforming%20data-driven%20models%20in%0Aerror%20reduction.%20Notably%2C%20PINNs%20maintain%20comparatively%20lower%20MAE%20in%20dynamic%0Agrid%20operations%2C%20reliably%20capturing%20state%20transitions%20in%20both%20random%20and%0Aexpert-driven%20control%20scenarios%2C%20while%20traditional%20models%20exhibit%20erratic%0Aperformance.%20Despite%20slight%20degradation%20in%20extreme%20operational%20regimes%2C%20PINNs%0Aconsistently%20enforce%20physical%20feasibility%2C%20proving%20vital%20for%20safety-critical%0Aapplications.%20Our%20results%20contribute%20to%20establishing%20PINNs%20as%20a%0Aparadigm-shifting%20tool%20for%20smart%20grid%20surrogation%2C%20bridging%20data-driven%0Aflexibility%20with%20first-principles%20rigor.%20This%20work%20advances%20real-time%20grid%0Acontrol%20and%20scalable%20digital%20twins%2C%20emphasizing%20the%20necessity%20of%20physics-aware%0Aarchitectures%20in%20mission-critical%20energy%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimitations%2520of%2520Physics-Informed%2520Neural%2520Networks%253A%2520a%2520Study%2520on%2520Smart%2520Grid%250A%2520%2520Surrogation%26entry.906535625%3DJulen%2520Cestero%2520and%2520Carmine%2520Delle%2520Femine%2520and%2520Kenji%2520S.%2520Muro%2520and%2520Marco%2520Quartulli%2520and%2520Marcello%2520Restelli%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520present%2520a%2520transformative%2520approach%250Afor%2520smart%2520grid%2520modeling%2520by%2520integrating%2520physical%2520laws%2520directly%2520into%2520learning%250Aframeworks%252C%2520addressing%2520critical%2520challenges%2520of%2520data%2520scarcity%2520and%2520physical%250Aconsistency%2520in%2520conventional%2520data-driven%2520methods.%2520This%2520paper%2520evaluates%2520PINNs%2527%250Acapabilities%2520as%2520surrogate%2520models%2520for%2520smart%2520grid%2520dynamics%252C%2520comparing%2520their%250Aperformance%2520against%2520XGBoost%252C%2520Random%2520Forest%252C%2520and%2520Linear%2520Regression%2520across%2520three%250Akey%2520experiments%253A%2520interpolation%252C%2520cross-validation%252C%2520and%2520episodic%2520trajectory%250Aprediction.%2520By%2520training%2520PINNs%2520exclusively%2520through%2520physics-based%2520loss%2520functions%250A%2528enforcing%2520power%2520balance%252C%2520operational%2520constraints%252C%2520and%2520grid%2520stability%2529%2520we%250Ademonstrate%2520their%2520superior%2520generalization%252C%2520outperforming%2520data-driven%2520models%2520in%250Aerror%2520reduction.%2520Notably%252C%2520PINNs%2520maintain%2520comparatively%2520lower%2520MAE%2520in%2520dynamic%250Agrid%2520operations%252C%2520reliably%2520capturing%2520state%2520transitions%2520in%2520both%2520random%2520and%250Aexpert-driven%2520control%2520scenarios%252C%2520while%2520traditional%2520models%2520exhibit%2520erratic%250Aperformance.%2520Despite%2520slight%2520degradation%2520in%2520extreme%2520operational%2520regimes%252C%2520PINNs%250Aconsistently%2520enforce%2520physical%2520feasibility%252C%2520proving%2520vital%2520for%2520safety-critical%250Aapplications.%2520Our%2520results%2520contribute%2520to%2520establishing%2520PINNs%2520as%2520a%250Aparadigm-shifting%2520tool%2520for%2520smart%2520grid%2520surrogation%252C%2520bridging%2520data-driven%250Aflexibility%2520with%2520first-principles%2520rigor.%2520This%2520work%2520advances%2520real-time%2520grid%250Acontrol%2520and%2520scalable%2520digital%2520twins%252C%2520emphasizing%2520the%2520necessity%2520of%2520physics-aware%250Aarchitectures%2520in%2520mission-critical%2520energy%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limitations%20of%20Physics-Informed%20Neural%20Networks%3A%20a%20Study%20on%20Smart%20Grid%0A%20%20Surrogation&entry.906535625=Julen%20Cestero%20and%20Carmine%20Delle%20Femine%20and%20Kenji%20S.%20Muro%20and%20Marco%20Quartulli%20and%20Marcello%20Restelli&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20present%20a%20transformative%20approach%0Afor%20smart%20grid%20modeling%20by%20integrating%20physical%20laws%20directly%20into%20learning%0Aframeworks%2C%20addressing%20critical%20challenges%20of%20data%20scarcity%20and%20physical%0Aconsistency%20in%20conventional%20data-driven%20methods.%20This%20paper%20evaluates%20PINNs%27%0Acapabilities%20as%20surrogate%20models%20for%20smart%20grid%20dynamics%2C%20comparing%20their%0Aperformance%20against%20XGBoost%2C%20Random%20Forest%2C%20and%20Linear%20Regression%20across%20three%0Akey%20experiments%3A%20interpolation%2C%20cross-validation%2C%20and%20episodic%20trajectory%0Aprediction.%20By%20training%20PINNs%20exclusively%20through%20physics-based%20loss%20functions%0A%28enforcing%20power%20balance%2C%20operational%20constraints%2C%20and%20grid%20stability%29%20we%0Ademonstrate%20their%20superior%20generalization%2C%20outperforming%20data-driven%20models%20in%0Aerror%20reduction.%20Notably%2C%20PINNs%20maintain%20comparatively%20lower%20MAE%20in%20dynamic%0Agrid%20operations%2C%20reliably%20capturing%20state%20transitions%20in%20both%20random%20and%0Aexpert-driven%20control%20scenarios%2C%20while%20traditional%20models%20exhibit%20erratic%0Aperformance.%20Despite%20slight%20degradation%20in%20extreme%20operational%20regimes%2C%20PINNs%0Aconsistently%20enforce%20physical%20feasibility%2C%20proving%20vital%20for%20safety-critical%0Aapplications.%20Our%20results%20contribute%20to%20establishing%20PINNs%20as%20a%0Aparadigm-shifting%20tool%20for%20smart%20grid%20surrogation%2C%20bridging%20data-driven%0Aflexibility%20with%20first-principles%20rigor.%20This%20work%20advances%20real-time%20grid%0Acontrol%20and%20scalable%20digital%20twins%2C%20emphasizing%20the%20necessity%20of%20physics-aware%0Aarchitectures%20in%20mission-critical%20energy%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21559v1&entry.124074799=Read"},
{"title": "Bayesian Double Descent", "author": "Nick Polson and Vadim Sokolov", "abstract": "  Double descent is a phenomenon of over-parameterized statistical models. Our\ngoal is to view double descent from a Bayesian perspective. Over-parameterized\nmodels such as deep neural networks have an interesting re-descending property\nin their risk characteristics. This is a recent phenomenon in machine learning\nand has been the subject of many studies. As the complexity of the model\nincreases, there is a U-shaped region corresponding to the traditional\nbias-variance trade-off, but then as the number of parameters equals the number\nof observations and the model becomes one of interpolation, the risk can become\ninfinite and then, in the over-parameterized region, it re-descends -- the\ndouble descent effect. We show that this has a natural Bayesian interpretation.\nMoreover, we show that it is not in conflict with the traditional Occam's razor\nthat Bayesian models possess, in that they tend to prefer simpler models when\npossible. We develop comprehensive theoretical foundations including Dawid's\nmodel comparison theory, Dickey-Savage results, and connections to generalized\nridge regression and shrinkage methods. We illustrate the approach with\nexamples of Bayesian model selection in neural networks and provide detailed\ntreatments of infinite Gaussian means models and non-parametric regression.\nFinally, we conclude with directions for future research.\n", "link": "http://arxiv.org/abs/2507.07338v2", "date": "2025-08-29", "relevancy": 1.4969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5055}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Double%20Descent&body=Title%3A%20Bayesian%20Double%20Descent%0AAuthor%3A%20Nick%20Polson%20and%20Vadim%20Sokolov%0AAbstract%3A%20%20%20Double%20descent%20is%20a%20phenomenon%20of%20over-parameterized%20statistical%20models.%20Our%0Agoal%20is%20to%20view%20double%20descent%20from%20a%20Bayesian%20perspective.%20Over-parameterized%0Amodels%20such%20as%20deep%20neural%20networks%20have%20an%20interesting%20re-descending%20property%0Ain%20their%20risk%20characteristics.%20This%20is%20a%20recent%20phenomenon%20in%20machine%20learning%0Aand%20has%20been%20the%20subject%20of%20many%20studies.%20As%20the%20complexity%20of%20the%20model%0Aincreases%2C%20there%20is%20a%20U-shaped%20region%20corresponding%20to%20the%20traditional%0Abias-variance%20trade-off%2C%20but%20then%20as%20the%20number%20of%20parameters%20equals%20the%20number%0Aof%20observations%20and%20the%20model%20becomes%20one%20of%20interpolation%2C%20the%20risk%20can%20become%0Ainfinite%20and%20then%2C%20in%20the%20over-parameterized%20region%2C%20it%20re-descends%20--%20the%0Adouble%20descent%20effect.%20We%20show%20that%20this%20has%20a%20natural%20Bayesian%20interpretation.%0AMoreover%2C%20we%20show%20that%20it%20is%20not%20in%20conflict%20with%20the%20traditional%20Occam%27s%20razor%0Athat%20Bayesian%20models%20possess%2C%20in%20that%20they%20tend%20to%20prefer%20simpler%20models%20when%0Apossible.%20We%20develop%20comprehensive%20theoretical%20foundations%20including%20Dawid%27s%0Amodel%20comparison%20theory%2C%20Dickey-Savage%20results%2C%20and%20connections%20to%20generalized%0Aridge%20regression%20and%20shrinkage%20methods.%20We%20illustrate%20the%20approach%20with%0Aexamples%20of%20Bayesian%20model%20selection%20in%20neural%20networks%20and%20provide%20detailed%0Atreatments%20of%20infinite%20Gaussian%20means%20models%20and%20non-parametric%20regression.%0AFinally%2C%20we%20conclude%20with%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Double%2520Descent%26entry.906535625%3DNick%2520Polson%2520and%2520Vadim%2520Sokolov%26entry.1292438233%3D%2520%2520Double%2520descent%2520is%2520a%2520phenomenon%2520of%2520over-parameterized%2520statistical%2520models.%2520Our%250Agoal%2520is%2520to%2520view%2520double%2520descent%2520from%2520a%2520Bayesian%2520perspective.%2520Over-parameterized%250Amodels%2520such%2520as%2520deep%2520neural%2520networks%2520have%2520an%2520interesting%2520re-descending%2520property%250Ain%2520their%2520risk%2520characteristics.%2520This%2520is%2520a%2520recent%2520phenomenon%2520in%2520machine%2520learning%250Aand%2520has%2520been%2520the%2520subject%2520of%2520many%2520studies.%2520As%2520the%2520complexity%2520of%2520the%2520model%250Aincreases%252C%2520there%2520is%2520a%2520U-shaped%2520region%2520corresponding%2520to%2520the%2520traditional%250Abias-variance%2520trade-off%252C%2520but%2520then%2520as%2520the%2520number%2520of%2520parameters%2520equals%2520the%2520number%250Aof%2520observations%2520and%2520the%2520model%2520becomes%2520one%2520of%2520interpolation%252C%2520the%2520risk%2520can%2520become%250Ainfinite%2520and%2520then%252C%2520in%2520the%2520over-parameterized%2520region%252C%2520it%2520re-descends%2520--%2520the%250Adouble%2520descent%2520effect.%2520We%2520show%2520that%2520this%2520has%2520a%2520natural%2520Bayesian%2520interpretation.%250AMoreover%252C%2520we%2520show%2520that%2520it%2520is%2520not%2520in%2520conflict%2520with%2520the%2520traditional%2520Occam%2527s%2520razor%250Athat%2520Bayesian%2520models%2520possess%252C%2520in%2520that%2520they%2520tend%2520to%2520prefer%2520simpler%2520models%2520when%250Apossible.%2520We%2520develop%2520comprehensive%2520theoretical%2520foundations%2520including%2520Dawid%2527s%250Amodel%2520comparison%2520theory%252C%2520Dickey-Savage%2520results%252C%2520and%2520connections%2520to%2520generalized%250Aridge%2520regression%2520and%2520shrinkage%2520methods.%2520We%2520illustrate%2520the%2520approach%2520with%250Aexamples%2520of%2520Bayesian%2520model%2520selection%2520in%2520neural%2520networks%2520and%2520provide%2520detailed%250Atreatments%2520of%2520infinite%2520Gaussian%2520means%2520models%2520and%2520non-parametric%2520regression.%250AFinally%252C%2520we%2520conclude%2520with%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Double%20Descent&entry.906535625=Nick%20Polson%20and%20Vadim%20Sokolov&entry.1292438233=%20%20Double%20descent%20is%20a%20phenomenon%20of%20over-parameterized%20statistical%20models.%20Our%0Agoal%20is%20to%20view%20double%20descent%20from%20a%20Bayesian%20perspective.%20Over-parameterized%0Amodels%20such%20as%20deep%20neural%20networks%20have%20an%20interesting%20re-descending%20property%0Ain%20their%20risk%20characteristics.%20This%20is%20a%20recent%20phenomenon%20in%20machine%20learning%0Aand%20has%20been%20the%20subject%20of%20many%20studies.%20As%20the%20complexity%20of%20the%20model%0Aincreases%2C%20there%20is%20a%20U-shaped%20region%20corresponding%20to%20the%20traditional%0Abias-variance%20trade-off%2C%20but%20then%20as%20the%20number%20of%20parameters%20equals%20the%20number%0Aof%20observations%20and%20the%20model%20becomes%20one%20of%20interpolation%2C%20the%20risk%20can%20become%0Ainfinite%20and%20then%2C%20in%20the%20over-parameterized%20region%2C%20it%20re-descends%20--%20the%0Adouble%20descent%20effect.%20We%20show%20that%20this%20has%20a%20natural%20Bayesian%20interpretation.%0AMoreover%2C%20we%20show%20that%20it%20is%20not%20in%20conflict%20with%20the%20traditional%20Occam%27s%20razor%0Athat%20Bayesian%20models%20possess%2C%20in%20that%20they%20tend%20to%20prefer%20simpler%20models%20when%0Apossible.%20We%20develop%20comprehensive%20theoretical%20foundations%20including%20Dawid%27s%0Amodel%20comparison%20theory%2C%20Dickey-Savage%20results%2C%20and%20connections%20to%20generalized%0Aridge%20regression%20and%20shrinkage%20methods.%20We%20illustrate%20the%20approach%20with%0Aexamples%20of%20Bayesian%20model%20selection%20in%20neural%20networks%20and%20provide%20detailed%0Atreatments%20of%20infinite%20Gaussian%20means%20models%20and%20non-parametric%20regression.%0AFinally%2C%20we%20conclude%20with%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07338v2&entry.124074799=Read"},
{"title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and\n  Acting", "author": "Pierrick Lorang and Hong Lu and Johannes Huemer and Patrik Zips and Matthias Scheutz", "abstract": "  Imitation learning enables intelligent systems to acquire complex behaviors\nwith minimal supervision. However, existing methods often focus on\nshort-horizon skills, require large datasets, and struggle to solve\nlong-horizon tasks or generalize across task variations and distribution\nshifts. We propose a novel neuro-symbolic framework that jointly learns\ncontinuous control policies and symbolic domain abstractions from a few skill\ndemonstrations. Our method abstracts high-level task structures into a graph,\ndiscovers symbolic rules via an Answer Set Programming solver, and trains\nlow-level controllers using diffusion policy imitation learning. A high-level\noracle filters task-relevant information to focus each controller on a minimal\nobservation and action space. Our graph-based neuro-symbolic framework enables\ncapturing complex state transitions, including non-spatial and temporal\nrelations, that data-driven learning or clustering techniques often fail to\ndiscover in limited demonstration datasets. We validate our approach in six\ndomains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers\nof Hanoi environments, and a distinct Automated Forklift domain with two\nenvironments. The results demonstrate high data efficiency with as few as five\nskill demonstrations, strong zero- and few-shot generalizations, and\ninterpretable decision making.\n", "link": "http://arxiv.org/abs/2508.21501v1", "date": "2025-08-29", "relevancy": 1.6983, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6039}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Neuro-Symbolic%20Imitation%20Learning%20for%20Long-Horizon%20Planning%20and%0A%20%20Acting&body=Title%3A%20Few-Shot%20Neuro-Symbolic%20Imitation%20Learning%20for%20Long-Horizon%20Planning%20and%0A%20%20Acting%0AAuthor%3A%20Pierrick%20Lorang%20and%20Hong%20Lu%20and%20Johannes%20Huemer%20and%20Patrik%20Zips%20and%20Matthias%20Scheutz%0AAbstract%3A%20%20%20Imitation%20learning%20enables%20intelligent%20systems%20to%20acquire%20complex%20behaviors%0Awith%20minimal%20supervision.%20However%2C%20existing%20methods%20often%20focus%20on%0Ashort-horizon%20skills%2C%20require%20large%20datasets%2C%20and%20struggle%20to%20solve%0Along-horizon%20tasks%20or%20generalize%20across%20task%20variations%20and%20distribution%0Ashifts.%20We%20propose%20a%20novel%20neuro-symbolic%20framework%20that%20jointly%20learns%0Acontinuous%20control%20policies%20and%20symbolic%20domain%20abstractions%20from%20a%20few%20skill%0Ademonstrations.%20Our%20method%20abstracts%20high-level%20task%20structures%20into%20a%20graph%2C%0Adiscovers%20symbolic%20rules%20via%20an%20Answer%20Set%20Programming%20solver%2C%20and%20trains%0Alow-level%20controllers%20using%20diffusion%20policy%20imitation%20learning.%20A%20high-level%0Aoracle%20filters%20task-relevant%20information%20to%20focus%20each%20controller%20on%20a%20minimal%0Aobservation%20and%20action%20space.%20Our%20graph-based%20neuro-symbolic%20framework%20enables%0Acapturing%20complex%20state%20transitions%2C%20including%20non-spatial%20and%20temporal%0Arelations%2C%20that%20data-driven%20learning%20or%20clustering%20techniques%20often%20fail%20to%0Adiscover%20in%20limited%20demonstration%20datasets.%20We%20validate%20our%20approach%20in%20six%0Adomains%20that%20involve%20four%20robotic%20arms%2C%20Stacking%2C%20Kitchen%2C%20Assembly%2C%20and%20Towers%0Aof%20Hanoi%20environments%2C%20and%20a%20distinct%20Automated%20Forklift%20domain%20with%20two%0Aenvironments.%20The%20results%20demonstrate%20high%20data%20efficiency%20with%20as%20few%20as%20five%0Askill%20demonstrations%2C%20strong%20zero-%20and%20few-shot%20generalizations%2C%20and%0Ainterpretable%20decision%20making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Neuro-Symbolic%2520Imitation%2520Learning%2520for%2520Long-Horizon%2520Planning%2520and%250A%2520%2520Acting%26entry.906535625%3DPierrick%2520Lorang%2520and%2520Hong%2520Lu%2520and%2520Johannes%2520Huemer%2520and%2520Patrik%2520Zips%2520and%2520Matthias%2520Scheutz%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520enables%2520intelligent%2520systems%2520to%2520acquire%2520complex%2520behaviors%250Awith%2520minimal%2520supervision.%2520However%252C%2520existing%2520methods%2520often%2520focus%2520on%250Ashort-horizon%2520skills%252C%2520require%2520large%2520datasets%252C%2520and%2520struggle%2520to%2520solve%250Along-horizon%2520tasks%2520or%2520generalize%2520across%2520task%2520variations%2520and%2520distribution%250Ashifts.%2520We%2520propose%2520a%2520novel%2520neuro-symbolic%2520framework%2520that%2520jointly%2520learns%250Acontinuous%2520control%2520policies%2520and%2520symbolic%2520domain%2520abstractions%2520from%2520a%2520few%2520skill%250Ademonstrations.%2520Our%2520method%2520abstracts%2520high-level%2520task%2520structures%2520into%2520a%2520graph%252C%250Adiscovers%2520symbolic%2520rules%2520via%2520an%2520Answer%2520Set%2520Programming%2520solver%252C%2520and%2520trains%250Alow-level%2520controllers%2520using%2520diffusion%2520policy%2520imitation%2520learning.%2520A%2520high-level%250Aoracle%2520filters%2520task-relevant%2520information%2520to%2520focus%2520each%2520controller%2520on%2520a%2520minimal%250Aobservation%2520and%2520action%2520space.%2520Our%2520graph-based%2520neuro-symbolic%2520framework%2520enables%250Acapturing%2520complex%2520state%2520transitions%252C%2520including%2520non-spatial%2520and%2520temporal%250Arelations%252C%2520that%2520data-driven%2520learning%2520or%2520clustering%2520techniques%2520often%2520fail%2520to%250Adiscover%2520in%2520limited%2520demonstration%2520datasets.%2520We%2520validate%2520our%2520approach%2520in%2520six%250Adomains%2520that%2520involve%2520four%2520robotic%2520arms%252C%2520Stacking%252C%2520Kitchen%252C%2520Assembly%252C%2520and%2520Towers%250Aof%2520Hanoi%2520environments%252C%2520and%2520a%2520distinct%2520Automated%2520Forklift%2520domain%2520with%2520two%250Aenvironments.%2520The%2520results%2520demonstrate%2520high%2520data%2520efficiency%2520with%2520as%2520few%2520as%2520five%250Askill%2520demonstrations%252C%2520strong%2520zero-%2520and%2520few-shot%2520generalizations%252C%2520and%250Ainterpretable%2520decision%2520making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Neuro-Symbolic%20Imitation%20Learning%20for%20Long-Horizon%20Planning%20and%0A%20%20Acting&entry.906535625=Pierrick%20Lorang%20and%20Hong%20Lu%20and%20Johannes%20Huemer%20and%20Patrik%20Zips%20and%20Matthias%20Scheutz&entry.1292438233=%20%20Imitation%20learning%20enables%20intelligent%20systems%20to%20acquire%20complex%20behaviors%0Awith%20minimal%20supervision.%20However%2C%20existing%20methods%20often%20focus%20on%0Ashort-horizon%20skills%2C%20require%20large%20datasets%2C%20and%20struggle%20to%20solve%0Along-horizon%20tasks%20or%20generalize%20across%20task%20variations%20and%20distribution%0Ashifts.%20We%20propose%20a%20novel%20neuro-symbolic%20framework%20that%20jointly%20learns%0Acontinuous%20control%20policies%20and%20symbolic%20domain%20abstractions%20from%20a%20few%20skill%0Ademonstrations.%20Our%20method%20abstracts%20high-level%20task%20structures%20into%20a%20graph%2C%0Adiscovers%20symbolic%20rules%20via%20an%20Answer%20Set%20Programming%20solver%2C%20and%20trains%0Alow-level%20controllers%20using%20diffusion%20policy%20imitation%20learning.%20A%20high-level%0Aoracle%20filters%20task-relevant%20information%20to%20focus%20each%20controller%20on%20a%20minimal%0Aobservation%20and%20action%20space.%20Our%20graph-based%20neuro-symbolic%20framework%20enables%0Acapturing%20complex%20state%20transitions%2C%20including%20non-spatial%20and%20temporal%0Arelations%2C%20that%20data-driven%20learning%20or%20clustering%20techniques%20often%20fail%20to%0Adiscover%20in%20limited%20demonstration%20datasets.%20We%20validate%20our%20approach%20in%20six%0Adomains%20that%20involve%20four%20robotic%20arms%2C%20Stacking%2C%20Kitchen%2C%20Assembly%2C%20and%20Towers%0Aof%20Hanoi%20environments%2C%20and%20a%20distinct%20Automated%20Forklift%20domain%20with%20two%0Aenvironments.%20The%20results%20demonstrate%20high%20data%20efficiency%20with%20as%20few%20as%20five%0Askill%20demonstrations%2C%20strong%20zero-%20and%20few-shot%20generalizations%2C%20and%0Ainterpretable%20decision%20making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21501v1&entry.124074799=Read"},
{"title": "FLORA: Efficient Synthetic Data Generation for Object Detection in\n  Low-Data Regimes via finetuning Flux LoRA", "author": "Alvaro Patricio and Atabak Dehban and Rodrigo Ventura", "abstract": "  Recent advances in diffusion-based generative models have demonstrated\nsignificant potential in augmenting scarce datasets for object detection tasks.\nNevertheless, most recent models rely on resource-intensive full fine-tuning of\nlarge-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA\nV100) and thousands of synthetic images. To address these limitations, we\npropose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation\npipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned\nexclusively through Low-Rank Adaptation (LoRA). This dramatically reduces\ncomputational requirements, enabling synthetic dataset generation with a\nconsumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our\napproach on seven diverse object detection datasets. Our results demonstrate\nthat training object detectors with just 500 synthetic images generated by our\napproach yields superior detection performance compared to models trained on\n5000 synthetic images from the ODGEN baseline, achieving improvements of up to\n21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass\nstate-of-the-art performance with far greater efficiency, as FLORA achieves\nsuperior results using only 10% of the data and a fraction of the computational\ncost. This work demonstrates that a quality and efficiency-focused approach is\nmore effective than brute-force generation, making advanced synthetic data\ncreation more practical and accessible for real-world scenarios.\n", "link": "http://arxiv.org/abs/2508.21712v1", "date": "2025-08-29", "relevancy": 1.7068, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5849}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5644}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLORA%3A%20Efficient%20Synthetic%20Data%20Generation%20for%20Object%20Detection%20in%0A%20%20Low-Data%20Regimes%20via%20finetuning%20Flux%20LoRA&body=Title%3A%20FLORA%3A%20Efficient%20Synthetic%20Data%20Generation%20for%20Object%20Detection%20in%0A%20%20Low-Data%20Regimes%20via%20finetuning%20Flux%20LoRA%0AAuthor%3A%20Alvaro%20Patricio%20and%20Atabak%20Dehban%20and%20Rodrigo%20Ventura%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion-based%20generative%20models%20have%20demonstrated%0Asignificant%20potential%20in%20augmenting%20scarce%20datasets%20for%20object%20detection%20tasks.%0ANevertheless%2C%20most%20recent%20models%20rely%20on%20resource-intensive%20full%20fine-tuning%20of%0Alarge-scale%20diffusion%20models%2C%20requiring%20enterprise-grade%20GPUs%20%28e.g.%2C%20NVIDIA%0AV100%29%20and%20thousands%20of%20synthetic%20images.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Flux%20LoRA%20Augmentation%20%28FLORA%29%2C%20a%20lightweight%20synthetic%20data%20generation%0Apipeline.%20Our%20approach%20uses%20the%20Flux%201.1%20Dev%20diffusion%20model%2C%20fine-tuned%0Aexclusively%20through%20Low-Rank%20Adaptation%20%28LoRA%29.%20This%20dramatically%20reduces%0Acomputational%20requirements%2C%20enabling%20synthetic%20dataset%20generation%20with%20a%0Aconsumer-grade%20GPU%20%28e.g.%2C%20NVIDIA%20RTX%204090%29.%20We%20empirically%20evaluate%20our%0Aapproach%20on%20seven%20diverse%20object%20detection%20datasets.%20Our%20results%20demonstrate%0Athat%20training%20object%20detectors%20with%20just%20500%20synthetic%20images%20generated%20by%20our%0Aapproach%20yields%20superior%20detection%20performance%20compared%20to%20models%20trained%20on%0A5000%20synthetic%20images%20from%20the%20ODGEN%20baseline%2C%20achieving%20improvements%20of%20up%20to%0A21.3%25%20in%20mAP%40.50%3A.95.%20This%20work%20demonstrates%20that%20it%20is%20possible%20to%20surpass%0Astate-of-the-art%20performance%20with%20far%20greater%20efficiency%2C%20as%20FLORA%20achieves%0Asuperior%20results%20using%20only%2010%25%20of%20the%20data%20and%20a%20fraction%20of%20the%20computational%0Acost.%20This%20work%20demonstrates%20that%20a%20quality%20and%20efficiency-focused%20approach%20is%0Amore%20effective%20than%20brute-force%20generation%2C%20making%20advanced%20synthetic%20data%0Acreation%20more%20practical%20and%20accessible%20for%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLORA%253A%2520Efficient%2520Synthetic%2520Data%2520Generation%2520for%2520Object%2520Detection%2520in%250A%2520%2520Low-Data%2520Regimes%2520via%2520finetuning%2520Flux%2520LoRA%26entry.906535625%3DAlvaro%2520Patricio%2520and%2520Atabak%2520Dehban%2520and%2520Rodrigo%2520Ventura%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion-based%2520generative%2520models%2520have%2520demonstrated%250Asignificant%2520potential%2520in%2520augmenting%2520scarce%2520datasets%2520for%2520object%2520detection%2520tasks.%250ANevertheless%252C%2520most%2520recent%2520models%2520rely%2520on%2520resource-intensive%2520full%2520fine-tuning%2520of%250Alarge-scale%2520diffusion%2520models%252C%2520requiring%2520enterprise-grade%2520GPUs%2520%2528e.g.%252C%2520NVIDIA%250AV100%2529%2520and%2520thousands%2520of%2520synthetic%2520images.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Flux%2520LoRA%2520Augmentation%2520%2528FLORA%2529%252C%2520a%2520lightweight%2520synthetic%2520data%2520generation%250Apipeline.%2520Our%2520approach%2520uses%2520the%2520Flux%25201.1%2520Dev%2520diffusion%2520model%252C%2520fine-tuned%250Aexclusively%2520through%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520This%2520dramatically%2520reduces%250Acomputational%2520requirements%252C%2520enabling%2520synthetic%2520dataset%2520generation%2520with%2520a%250Aconsumer-grade%2520GPU%2520%2528e.g.%252C%2520NVIDIA%2520RTX%25204090%2529.%2520We%2520empirically%2520evaluate%2520our%250Aapproach%2520on%2520seven%2520diverse%2520object%2520detection%2520datasets.%2520Our%2520results%2520demonstrate%250Athat%2520training%2520object%2520detectors%2520with%2520just%2520500%2520synthetic%2520images%2520generated%2520by%2520our%250Aapproach%2520yields%2520superior%2520detection%2520performance%2520compared%2520to%2520models%2520trained%2520on%250A5000%2520synthetic%2520images%2520from%2520the%2520ODGEN%2520baseline%252C%2520achieving%2520improvements%2520of%2520up%2520to%250A21.3%2525%2520in%2520mAP%2540.50%253A.95.%2520This%2520work%2520demonstrates%2520that%2520it%2520is%2520possible%2520to%2520surpass%250Astate-of-the-art%2520performance%2520with%2520far%2520greater%2520efficiency%252C%2520as%2520FLORA%2520achieves%250Asuperior%2520results%2520using%2520only%252010%2525%2520of%2520the%2520data%2520and%2520a%2520fraction%2520of%2520the%2520computational%250Acost.%2520This%2520work%2520demonstrates%2520that%2520a%2520quality%2520and%2520efficiency-focused%2520approach%2520is%250Amore%2520effective%2520than%2520brute-force%2520generation%252C%2520making%2520advanced%2520synthetic%2520data%250Acreation%2520more%2520practical%2520and%2520accessible%2520for%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLORA%3A%20Efficient%20Synthetic%20Data%20Generation%20for%20Object%20Detection%20in%0A%20%20Low-Data%20Regimes%20via%20finetuning%20Flux%20LoRA&entry.906535625=Alvaro%20Patricio%20and%20Atabak%20Dehban%20and%20Rodrigo%20Ventura&entry.1292438233=%20%20Recent%20advances%20in%20diffusion-based%20generative%20models%20have%20demonstrated%0Asignificant%20potential%20in%20augmenting%20scarce%20datasets%20for%20object%20detection%20tasks.%0ANevertheless%2C%20most%20recent%20models%20rely%20on%20resource-intensive%20full%20fine-tuning%20of%0Alarge-scale%20diffusion%20models%2C%20requiring%20enterprise-grade%20GPUs%20%28e.g.%2C%20NVIDIA%0AV100%29%20and%20thousands%20of%20synthetic%20images.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Flux%20LoRA%20Augmentation%20%28FLORA%29%2C%20a%20lightweight%20synthetic%20data%20generation%0Apipeline.%20Our%20approach%20uses%20the%20Flux%201.1%20Dev%20diffusion%20model%2C%20fine-tuned%0Aexclusively%20through%20Low-Rank%20Adaptation%20%28LoRA%29.%20This%20dramatically%20reduces%0Acomputational%20requirements%2C%20enabling%20synthetic%20dataset%20generation%20with%20a%0Aconsumer-grade%20GPU%20%28e.g.%2C%20NVIDIA%20RTX%204090%29.%20We%20empirically%20evaluate%20our%0Aapproach%20on%20seven%20diverse%20object%20detection%20datasets.%20Our%20results%20demonstrate%0Athat%20training%20object%20detectors%20with%20just%20500%20synthetic%20images%20generated%20by%20our%0Aapproach%20yields%20superior%20detection%20performance%20compared%20to%20models%20trained%20on%0A5000%20synthetic%20images%20from%20the%20ODGEN%20baseline%2C%20achieving%20improvements%20of%20up%20to%0A21.3%25%20in%20mAP%40.50%3A.95.%20This%20work%20demonstrates%20that%20it%20is%20possible%20to%20surpass%0Astate-of-the-art%20performance%20with%20far%20greater%20efficiency%2C%20as%20FLORA%20achieves%0Asuperior%20results%20using%20only%2010%25%20of%20the%20data%20and%20a%20fraction%20of%20the%20computational%0Acost.%20This%20work%20demonstrates%20that%20a%20quality%20and%20efficiency-focused%20approach%20is%0Amore%20effective%20than%20brute-force%20generation%2C%20making%20advanced%20synthetic%20data%0Acreation%20more%20practical%20and%20accessible%20for%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21712v1&entry.124074799=Read"},
{"title": "Adapting to Change: A Comparison of Continual and Transfer Learning for\n  Modeling Building Thermal Dynamics under Concept Drifts", "author": "Fabian Raisch and Max Langtry and Felix Koch and Ruchi Choudhary and Christoph Goebel and Benjamin Tischler", "abstract": "  Transfer Learning (TL) is currently the most effective approach for modeling\nbuilding thermal dynamics when only limited data are available. TL uses a\npretrained model that is fine-tuned to a specific target building. However, it\nremains unclear how to proceed after initial fine-tuning, as more operational\nmeasurement data are collected over time. This challenge becomes even more\ncomplex when the dynamics of the building change, for example, after a retrofit\nor a change in occupancy. In Machine Learning literature, Continual Learning\n(CL) methods are used to update models of changing systems. TL approaches can\nalso address this challenge by reusing the pretrained model at each update step\nand fine-tuning it with new measurement data. A comprehensive study on how to\nincorporate new measurement data over time to improve prediction accuracy and\naddress the challenges of concept drifts (changes in dynamics) for building\nthermal dynamics is still missing.\n  Therefore, this study compares several CL and TL strategies, as well as a\nmodel trained from scratch, for thermal dynamics modeling during building\noperation. The methods are evaluated using 5--7 years of simulated data\nrepresentative of single-family houses in Central Europe, including scenarios\nwith concept drifts from retrofits and changes in occupancy. We propose a CL\nstrategy (Seasonal Memory Learning) that provides greater accuracy improvements\nthan existing CL and TL methods, while maintaining low computational effort.\nSML outperformed the benchmark of initial fine-tuning by 28.1\\% without concept\ndrifts and 34.9\\% with concept drifts.\n", "link": "http://arxiv.org/abs/2508.21615v1", "date": "2025-08-29", "relevancy": 1.5101, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20to%20Change%3A%20A%20Comparison%20of%20Continual%20and%20Transfer%20Learning%20for%0A%20%20Modeling%20Building%20Thermal%20Dynamics%20under%20Concept%20Drifts&body=Title%3A%20Adapting%20to%20Change%3A%20A%20Comparison%20of%20Continual%20and%20Transfer%20Learning%20for%0A%20%20Modeling%20Building%20Thermal%20Dynamics%20under%20Concept%20Drifts%0AAuthor%3A%20Fabian%20Raisch%20and%20Max%20Langtry%20and%20Felix%20Koch%20and%20Ruchi%20Choudhary%20and%20Christoph%20Goebel%20and%20Benjamin%20Tischler%0AAbstract%3A%20%20%20Transfer%20Learning%20%28TL%29%20is%20currently%20the%20most%20effective%20approach%20for%20modeling%0Abuilding%20thermal%20dynamics%20when%20only%20limited%20data%20are%20available.%20TL%20uses%20a%0Apretrained%20model%20that%20is%20fine-tuned%20to%20a%20specific%20target%20building.%20However%2C%20it%0Aremains%20unclear%20how%20to%20proceed%20after%20initial%20fine-tuning%2C%20as%20more%20operational%0Ameasurement%20data%20are%20collected%20over%20time.%20This%20challenge%20becomes%20even%20more%0Acomplex%20when%20the%20dynamics%20of%20the%20building%20change%2C%20for%20example%2C%20after%20a%20retrofit%0Aor%20a%20change%20in%20occupancy.%20In%20Machine%20Learning%20literature%2C%20Continual%20Learning%0A%28CL%29%20methods%20are%20used%20to%20update%20models%20of%20changing%20systems.%20TL%20approaches%20can%0Aalso%20address%20this%20challenge%20by%20reusing%20the%20pretrained%20model%20at%20each%20update%20step%0Aand%20fine-tuning%20it%20with%20new%20measurement%20data.%20A%20comprehensive%20study%20on%20how%20to%0Aincorporate%20new%20measurement%20data%20over%20time%20to%20improve%20prediction%20accuracy%20and%0Aaddress%20the%20challenges%20of%20concept%20drifts%20%28changes%20in%20dynamics%29%20for%20building%0Athermal%20dynamics%20is%20still%20missing.%0A%20%20Therefore%2C%20this%20study%20compares%20several%20CL%20and%20TL%20strategies%2C%20as%20well%20as%20a%0Amodel%20trained%20from%20scratch%2C%20for%20thermal%20dynamics%20modeling%20during%20building%0Aoperation.%20The%20methods%20are%20evaluated%20using%205--7%20years%20of%20simulated%20data%0Arepresentative%20of%20single-family%20houses%20in%20Central%20Europe%2C%20including%20scenarios%0Awith%20concept%20drifts%20from%20retrofits%20and%20changes%20in%20occupancy.%20We%20propose%20a%20CL%0Astrategy%20%28Seasonal%20Memory%20Learning%29%20that%20provides%20greater%20accuracy%20improvements%0Athan%20existing%20CL%20and%20TL%20methods%2C%20while%20maintaining%20low%20computational%20effort.%0ASML%20outperformed%20the%20benchmark%20of%20initial%20fine-tuning%20by%2028.1%5C%25%20without%20concept%0Adrifts%20and%2034.9%5C%25%20with%20concept%20drifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520to%2520Change%253A%2520A%2520Comparison%2520of%2520Continual%2520and%2520Transfer%2520Learning%2520for%250A%2520%2520Modeling%2520Building%2520Thermal%2520Dynamics%2520under%2520Concept%2520Drifts%26entry.906535625%3DFabian%2520Raisch%2520and%2520Max%2520Langtry%2520and%2520Felix%2520Koch%2520and%2520Ruchi%2520Choudhary%2520and%2520Christoph%2520Goebel%2520and%2520Benjamin%2520Tischler%26entry.1292438233%3D%2520%2520Transfer%2520Learning%2520%2528TL%2529%2520is%2520currently%2520the%2520most%2520effective%2520approach%2520for%2520modeling%250Abuilding%2520thermal%2520dynamics%2520when%2520only%2520limited%2520data%2520are%2520available.%2520TL%2520uses%2520a%250Apretrained%2520model%2520that%2520is%2520fine-tuned%2520to%2520a%2520specific%2520target%2520building.%2520However%252C%2520it%250Aremains%2520unclear%2520how%2520to%2520proceed%2520after%2520initial%2520fine-tuning%252C%2520as%2520more%2520operational%250Ameasurement%2520data%2520are%2520collected%2520over%2520time.%2520This%2520challenge%2520becomes%2520even%2520more%250Acomplex%2520when%2520the%2520dynamics%2520of%2520the%2520building%2520change%252C%2520for%2520example%252C%2520after%2520a%2520retrofit%250Aor%2520a%2520change%2520in%2520occupancy.%2520In%2520Machine%2520Learning%2520literature%252C%2520Continual%2520Learning%250A%2528CL%2529%2520methods%2520are%2520used%2520to%2520update%2520models%2520of%2520changing%2520systems.%2520TL%2520approaches%2520can%250Aalso%2520address%2520this%2520challenge%2520by%2520reusing%2520the%2520pretrained%2520model%2520at%2520each%2520update%2520step%250Aand%2520fine-tuning%2520it%2520with%2520new%2520measurement%2520data.%2520A%2520comprehensive%2520study%2520on%2520how%2520to%250Aincorporate%2520new%2520measurement%2520data%2520over%2520time%2520to%2520improve%2520prediction%2520accuracy%2520and%250Aaddress%2520the%2520challenges%2520of%2520concept%2520drifts%2520%2528changes%2520in%2520dynamics%2529%2520for%2520building%250Athermal%2520dynamics%2520is%2520still%2520missing.%250A%2520%2520Therefore%252C%2520this%2520study%2520compares%2520several%2520CL%2520and%2520TL%2520strategies%252C%2520as%2520well%2520as%2520a%250Amodel%2520trained%2520from%2520scratch%252C%2520for%2520thermal%2520dynamics%2520modeling%2520during%2520building%250Aoperation.%2520The%2520methods%2520are%2520evaluated%2520using%25205--7%2520years%2520of%2520simulated%2520data%250Arepresentative%2520of%2520single-family%2520houses%2520in%2520Central%2520Europe%252C%2520including%2520scenarios%250Awith%2520concept%2520drifts%2520from%2520retrofits%2520and%2520changes%2520in%2520occupancy.%2520We%2520propose%2520a%2520CL%250Astrategy%2520%2528Seasonal%2520Memory%2520Learning%2529%2520that%2520provides%2520greater%2520accuracy%2520improvements%250Athan%2520existing%2520CL%2520and%2520TL%2520methods%252C%2520while%2520maintaining%2520low%2520computational%2520effort.%250ASML%2520outperformed%2520the%2520benchmark%2520of%2520initial%2520fine-tuning%2520by%252028.1%255C%2525%2520without%2520concept%250Adrifts%2520and%252034.9%255C%2525%2520with%2520concept%2520drifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20to%20Change%3A%20A%20Comparison%20of%20Continual%20and%20Transfer%20Learning%20for%0A%20%20Modeling%20Building%20Thermal%20Dynamics%20under%20Concept%20Drifts&entry.906535625=Fabian%20Raisch%20and%20Max%20Langtry%20and%20Felix%20Koch%20and%20Ruchi%20Choudhary%20and%20Christoph%20Goebel%20and%20Benjamin%20Tischler&entry.1292438233=%20%20Transfer%20Learning%20%28TL%29%20is%20currently%20the%20most%20effective%20approach%20for%20modeling%0Abuilding%20thermal%20dynamics%20when%20only%20limited%20data%20are%20available.%20TL%20uses%20a%0Apretrained%20model%20that%20is%20fine-tuned%20to%20a%20specific%20target%20building.%20However%2C%20it%0Aremains%20unclear%20how%20to%20proceed%20after%20initial%20fine-tuning%2C%20as%20more%20operational%0Ameasurement%20data%20are%20collected%20over%20time.%20This%20challenge%20becomes%20even%20more%0Acomplex%20when%20the%20dynamics%20of%20the%20building%20change%2C%20for%20example%2C%20after%20a%20retrofit%0Aor%20a%20change%20in%20occupancy.%20In%20Machine%20Learning%20literature%2C%20Continual%20Learning%0A%28CL%29%20methods%20are%20used%20to%20update%20models%20of%20changing%20systems.%20TL%20approaches%20can%0Aalso%20address%20this%20challenge%20by%20reusing%20the%20pretrained%20model%20at%20each%20update%20step%0Aand%20fine-tuning%20it%20with%20new%20measurement%20data.%20A%20comprehensive%20study%20on%20how%20to%0Aincorporate%20new%20measurement%20data%20over%20time%20to%20improve%20prediction%20accuracy%20and%0Aaddress%20the%20challenges%20of%20concept%20drifts%20%28changes%20in%20dynamics%29%20for%20building%0Athermal%20dynamics%20is%20still%20missing.%0A%20%20Therefore%2C%20this%20study%20compares%20several%20CL%20and%20TL%20strategies%2C%20as%20well%20as%20a%0Amodel%20trained%20from%20scratch%2C%20for%20thermal%20dynamics%20modeling%20during%20building%0Aoperation.%20The%20methods%20are%20evaluated%20using%205--7%20years%20of%20simulated%20data%0Arepresentative%20of%20single-family%20houses%20in%20Central%20Europe%2C%20including%20scenarios%0Awith%20concept%20drifts%20from%20retrofits%20and%20changes%20in%20occupancy.%20We%20propose%20a%20CL%0Astrategy%20%28Seasonal%20Memory%20Learning%29%20that%20provides%20greater%20accuracy%20improvements%0Athan%20existing%20CL%20and%20TL%20methods%2C%20while%20maintaining%20low%20computational%20effort.%0ASML%20outperformed%20the%20benchmark%20of%20initial%20fine-tuning%20by%2028.1%5C%25%20without%20concept%0Adrifts%20and%2034.9%5C%25%20with%20concept%20drifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21615v1&entry.124074799=Read"},
{"title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk\n  salsa?", "author": "Olger Siebinga and David Abbink", "abstract": "  Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.\n", "link": "http://arxiv.org/abs/2508.21690v1", "date": "2025-08-29", "relevancy": 1.6697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5943}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5765}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20a%20mobile%20robot%20learn%20from%20a%20pedestrian%20model%20to%20prevent%20the%20sidewalk%0A%20%20salsa%3F&body=Title%3A%20Can%20a%20mobile%20robot%20learn%20from%20a%20pedestrian%20model%20to%20prevent%20the%20sidewalk%0A%20%20salsa%3F%0AAuthor%3A%20Olger%20Siebinga%20and%20David%20Abbink%0AAbstract%3A%20%20%20Pedestrians%20approaching%20each%20other%20on%20a%20sidewalk%20sometimes%20end%20up%20in%20an%0Aawkward%20interaction%20known%20as%20the%20%22sidewalk%20salsa%22%3A%20they%20both%20%28repeatedly%29%0Adeviate%20to%20the%20same%20side%20to%20avoid%20a%20collision.%20This%20provides%20an%20interesting%20use%0Acase%20to%20study%20interactions%20between%20pedestrians%20and%20mobile%20robots%20because%2C%20in%0Athe%20vast%20majority%20of%20cases%2C%20this%20phenomenon%20is%20avoided%20through%20a%20negotiation%0Abased%20on%20implicit%20communication.%20Understanding%20how%20it%20goes%20wrong%20and%20how%0Apedestrians%20end%20up%20in%20the%20sidewalk%20salsa%20will%20therefore%20provide%20insight%20into%0Athe%20implicit%20communication.%20This%20understanding%20can%20be%20used%20to%20design%20safe%20and%0Aacceptable%20robotic%20behaviour.%20In%20a%20previous%20attempt%20to%20gain%20this%20understanding%2C%0Aa%20model%20of%20pedestrian%20behaviour%20based%20on%20the%20Communication-Enabled%20Interaction%0A%28CEI%29%20framework%20was%20developed%20that%20can%20replicate%20the%20sidewalk%20salsa.%20However%2C%0Ait%20is%20unclear%20how%20to%20leverage%20this%20model%20in%20robotic%20planning%20and%0Adecision-making%20since%20it%20violates%20the%20assumptions%20of%20game%20theory%2C%20a%20much-used%0Aframework%20in%20planning%20and%20decision-making.%20Here%2C%20we%20present%20a%20proof-of-concept%0Afor%20an%20approach%20where%20a%20Reinforcement%20Learning%20%28RL%29%20agent%20leverages%20the%20model%0Ato%20learn%20how%20to%20interact%20with%20pedestrians.%20The%20results%20show%20that%20a%20basic%20RL%0Aagent%20successfully%20learned%20to%20interact%20with%20the%20CEI%20model.%20Furthermore%2C%20a%0Arisk-averse%20RL%20agent%20that%20had%20access%20to%20the%20perceived%20risk%20of%20the%20CEI%20model%0Alearned%20how%20to%20effectively%20communicate%20its%20intention%20through%20its%20motion%20and%0Athereby%20substantially%20lowered%20the%20perceived%20risk%2C%20and%20displayed%20effort%20by%20the%0Amodelled%20pedestrian.%20These%20results%20show%20this%20is%20a%20promising%20approach%20and%0Aencourage%20further%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520a%2520mobile%2520robot%2520learn%2520from%2520a%2520pedestrian%2520model%2520to%2520prevent%2520the%2520sidewalk%250A%2520%2520salsa%253F%26entry.906535625%3DOlger%2520Siebinga%2520and%2520David%2520Abbink%26entry.1292438233%3D%2520%2520Pedestrians%2520approaching%2520each%2520other%2520on%2520a%2520sidewalk%2520sometimes%2520end%2520up%2520in%2520an%250Aawkward%2520interaction%2520known%2520as%2520the%2520%2522sidewalk%2520salsa%2522%253A%2520they%2520both%2520%2528repeatedly%2529%250Adeviate%2520to%2520the%2520same%2520side%2520to%2520avoid%2520a%2520collision.%2520This%2520provides%2520an%2520interesting%2520use%250Acase%2520to%2520study%2520interactions%2520between%2520pedestrians%2520and%2520mobile%2520robots%2520because%252C%2520in%250Athe%2520vast%2520majority%2520of%2520cases%252C%2520this%2520phenomenon%2520is%2520avoided%2520through%2520a%2520negotiation%250Abased%2520on%2520implicit%2520communication.%2520Understanding%2520how%2520it%2520goes%2520wrong%2520and%2520how%250Apedestrians%2520end%2520up%2520in%2520the%2520sidewalk%2520salsa%2520will%2520therefore%2520provide%2520insight%2520into%250Athe%2520implicit%2520communication.%2520This%2520understanding%2520can%2520be%2520used%2520to%2520design%2520safe%2520and%250Aacceptable%2520robotic%2520behaviour.%2520In%2520a%2520previous%2520attempt%2520to%2520gain%2520this%2520understanding%252C%250Aa%2520model%2520of%2520pedestrian%2520behaviour%2520based%2520on%2520the%2520Communication-Enabled%2520Interaction%250A%2528CEI%2529%2520framework%2520was%2520developed%2520that%2520can%2520replicate%2520the%2520sidewalk%2520salsa.%2520However%252C%250Ait%2520is%2520unclear%2520how%2520to%2520leverage%2520this%2520model%2520in%2520robotic%2520planning%2520and%250Adecision-making%2520since%2520it%2520violates%2520the%2520assumptions%2520of%2520game%2520theory%252C%2520a%2520much-used%250Aframework%2520in%2520planning%2520and%2520decision-making.%2520Here%252C%2520we%2520present%2520a%2520proof-of-concept%250Afor%2520an%2520approach%2520where%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529%2520agent%2520leverages%2520the%2520model%250Ato%2520learn%2520how%2520to%2520interact%2520with%2520pedestrians.%2520The%2520results%2520show%2520that%2520a%2520basic%2520RL%250Aagent%2520successfully%2520learned%2520to%2520interact%2520with%2520the%2520CEI%2520model.%2520Furthermore%252C%2520a%250Arisk-averse%2520RL%2520agent%2520that%2520had%2520access%2520to%2520the%2520perceived%2520risk%2520of%2520the%2520CEI%2520model%250Alearned%2520how%2520to%2520effectively%2520communicate%2520its%2520intention%2520through%2520its%2520motion%2520and%250Athereby%2520substantially%2520lowered%2520the%2520perceived%2520risk%252C%2520and%2520displayed%2520effort%2520by%2520the%250Amodelled%2520pedestrian.%2520These%2520results%2520show%2520this%2520is%2520a%2520promising%2520approach%2520and%250Aencourage%2520further%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20a%20mobile%20robot%20learn%20from%20a%20pedestrian%20model%20to%20prevent%20the%20sidewalk%0A%20%20salsa%3F&entry.906535625=Olger%20Siebinga%20and%20David%20Abbink&entry.1292438233=%20%20Pedestrians%20approaching%20each%20other%20on%20a%20sidewalk%20sometimes%20end%20up%20in%20an%0Aawkward%20interaction%20known%20as%20the%20%22sidewalk%20salsa%22%3A%20they%20both%20%28repeatedly%29%0Adeviate%20to%20the%20same%20side%20to%20avoid%20a%20collision.%20This%20provides%20an%20interesting%20use%0Acase%20to%20study%20interactions%20between%20pedestrians%20and%20mobile%20robots%20because%2C%20in%0Athe%20vast%20majority%20of%20cases%2C%20this%20phenomenon%20is%20avoided%20through%20a%20negotiation%0Abased%20on%20implicit%20communication.%20Understanding%20how%20it%20goes%20wrong%20and%20how%0Apedestrians%20end%20up%20in%20the%20sidewalk%20salsa%20will%20therefore%20provide%20insight%20into%0Athe%20implicit%20communication.%20This%20understanding%20can%20be%20used%20to%20design%20safe%20and%0Aacceptable%20robotic%20behaviour.%20In%20a%20previous%20attempt%20to%20gain%20this%20understanding%2C%0Aa%20model%20of%20pedestrian%20behaviour%20based%20on%20the%20Communication-Enabled%20Interaction%0A%28CEI%29%20framework%20was%20developed%20that%20can%20replicate%20the%20sidewalk%20salsa.%20However%2C%0Ait%20is%20unclear%20how%20to%20leverage%20this%20model%20in%20robotic%20planning%20and%0Adecision-making%20since%20it%20violates%20the%20assumptions%20of%20game%20theory%2C%20a%20much-used%0Aframework%20in%20planning%20and%20decision-making.%20Here%2C%20we%20present%20a%20proof-of-concept%0Afor%20an%20approach%20where%20a%20Reinforcement%20Learning%20%28RL%29%20agent%20leverages%20the%20model%0Ato%20learn%20how%20to%20interact%20with%20pedestrians.%20The%20results%20show%20that%20a%20basic%20RL%0Aagent%20successfully%20learned%20to%20interact%20with%20the%20CEI%20model.%20Furthermore%2C%20a%0Arisk-averse%20RL%20agent%20that%20had%20access%20to%20the%20perceived%20risk%20of%20the%20CEI%20model%0Alearned%20how%20to%20effectively%20communicate%20its%20intention%20through%20its%20motion%20and%0Athereby%20substantially%20lowered%20the%20perceived%20risk%2C%20and%20displayed%20effort%20by%20the%0Amodelled%20pedestrian.%20These%20results%20show%20this%20is%20a%20promising%20approach%20and%0Aencourage%20further%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21690v1&entry.124074799=Read"},
{"title": "A-MHA*: Anytime Multi-Heuristic A*", "author": "Ramkumar Natarajan and Muhammad Suhail Saleem and William Xiao and Sandip Aine and Howie Choset and Maxim Likhachev", "abstract": "  Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.\n", "link": "http://arxiv.org/abs/2508.21637v1", "date": "2025-08-29", "relevancy": 1.2647, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.453}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A-MHA%2A%3A%20Anytime%20Multi-Heuristic%20A%2A&body=Title%3A%20A-MHA%2A%3A%20Anytime%20Multi-Heuristic%20A%2A%0AAuthor%3A%20Ramkumar%20Natarajan%20and%20Muhammad%20Suhail%20Saleem%20and%20William%20Xiao%20and%20Sandip%20Aine%20and%20Howie%20Choset%20and%20Maxim%20Likhachev%0AAbstract%3A%20%20%20Designing%20good%20heuristic%20functions%20for%20graph%20search%20requires%20adequate%20domain%0Aknowledge.%20It%20is%20often%20easy%20to%20design%20heuristics%20that%20perform%20well%20and%0Acorrelate%20with%20the%20underlying%20true%20cost-to-go%20values%20in%20certain%20parts%20of%20the%0Asearch%20space%20but%20these%20may%20not%20be%20admissible%20throughout%20the%20domain%20thereby%0Aaffecting%20the%20optimality%20guarantees%20of%20the%20search.%20Bounded%20suboptimal%20search%0Ausing%20several%20such%20partially%20good%20but%20inadmissible%20heuristics%20was%20developed%20in%0AMulti-Heuristic%20A%2A%20%28MHA%2A%29.%20Although%20MHA%2A%20leverages%20multiple%20inadmissible%0Aheuristics%20to%20potentially%20generate%20a%20faster%20suboptimal%20solution%2C%20the%20original%0Aversion%20does%20not%20improve%20the%20solution%20over%20time.%20It%20is%20a%20one%20shot%20algorithm%0Athat%20requires%20careful%20setting%20of%20inflation%20factors%20to%20obtain%20a%20desired%20one%20time%0Asolution.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20extending%20MHA%2A%20to%20an%20anytime%0Aversion%20that%20finds%20a%20feasible%20suboptimal%20solution%20quickly%20and%20continually%0Aimproves%20it%20until%20time%20runs%20out.%20Our%20work%20is%20inspired%20from%20the%20Anytime%0ARepairing%20A%2A%20%28ARA%2A%29%20algorithm.%20We%20prove%20that%20our%20precise%20adaptation%20of%20ARA%2A%0Aconcepts%20in%20the%20MHA%2A%20framework%20preserves%20the%20original%20suboptimal%20and%0Acompleteness%20guarantees%20and%20enhances%20MHA%2A%20to%20perform%20in%20an%20anytime%20fashion.%0AFurthermore%2C%20we%20report%20the%20performance%20of%20A-MHA%2A%20in%203-D%20path%20planning%20domain%0Aand%20sliding%20tiles%20puzzle%20and%20compare%20against%20MHA%2A%20and%20other%20anytime%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA-MHA%252A%253A%2520Anytime%2520Multi-Heuristic%2520A%252A%26entry.906535625%3DRamkumar%2520Natarajan%2520and%2520Muhammad%2520Suhail%2520Saleem%2520and%2520William%2520Xiao%2520and%2520Sandip%2520Aine%2520and%2520Howie%2520Choset%2520and%2520Maxim%2520Likhachev%26entry.1292438233%3D%2520%2520Designing%2520good%2520heuristic%2520functions%2520for%2520graph%2520search%2520requires%2520adequate%2520domain%250Aknowledge.%2520It%2520is%2520often%2520easy%2520to%2520design%2520heuristics%2520that%2520perform%2520well%2520and%250Acorrelate%2520with%2520the%2520underlying%2520true%2520cost-to-go%2520values%2520in%2520certain%2520parts%2520of%2520the%250Asearch%2520space%2520but%2520these%2520may%2520not%2520be%2520admissible%2520throughout%2520the%2520domain%2520thereby%250Aaffecting%2520the%2520optimality%2520guarantees%2520of%2520the%2520search.%2520Bounded%2520suboptimal%2520search%250Ausing%2520several%2520such%2520partially%2520good%2520but%2520inadmissible%2520heuristics%2520was%2520developed%2520in%250AMulti-Heuristic%2520A%252A%2520%2528MHA%252A%2529.%2520Although%2520MHA%252A%2520leverages%2520multiple%2520inadmissible%250Aheuristics%2520to%2520potentially%2520generate%2520a%2520faster%2520suboptimal%2520solution%252C%2520the%2520original%250Aversion%2520does%2520not%2520improve%2520the%2520solution%2520over%2520time.%2520It%2520is%2520a%2520one%2520shot%2520algorithm%250Athat%2520requires%2520careful%2520setting%2520of%2520inflation%2520factors%2520to%2520obtain%2520a%2520desired%2520one%2520time%250Asolution.%2520In%2520this%2520work%252C%2520we%2520tackle%2520this%2520issue%2520by%2520extending%2520MHA%252A%2520to%2520an%2520anytime%250Aversion%2520that%2520finds%2520a%2520feasible%2520suboptimal%2520solution%2520quickly%2520and%2520continually%250Aimproves%2520it%2520until%2520time%2520runs%2520out.%2520Our%2520work%2520is%2520inspired%2520from%2520the%2520Anytime%250ARepairing%2520A%252A%2520%2528ARA%252A%2529%2520algorithm.%2520We%2520prove%2520that%2520our%2520precise%2520adaptation%2520of%2520ARA%252A%250Aconcepts%2520in%2520the%2520MHA%252A%2520framework%2520preserves%2520the%2520original%2520suboptimal%2520and%250Acompleteness%2520guarantees%2520and%2520enhances%2520MHA%252A%2520to%2520perform%2520in%2520an%2520anytime%2520fashion.%250AFurthermore%252C%2520we%2520report%2520the%2520performance%2520of%2520A-MHA%252A%2520in%25203-D%2520path%2520planning%2520domain%250Aand%2520sliding%2520tiles%2520puzzle%2520and%2520compare%2520against%2520MHA%252A%2520and%2520other%2520anytime%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A-MHA%2A%3A%20Anytime%20Multi-Heuristic%20A%2A&entry.906535625=Ramkumar%20Natarajan%20and%20Muhammad%20Suhail%20Saleem%20and%20William%20Xiao%20and%20Sandip%20Aine%20and%20Howie%20Choset%20and%20Maxim%20Likhachev&entry.1292438233=%20%20Designing%20good%20heuristic%20functions%20for%20graph%20search%20requires%20adequate%20domain%0Aknowledge.%20It%20is%20often%20easy%20to%20design%20heuristics%20that%20perform%20well%20and%0Acorrelate%20with%20the%20underlying%20true%20cost-to-go%20values%20in%20certain%20parts%20of%20the%0Asearch%20space%20but%20these%20may%20not%20be%20admissible%20throughout%20the%20domain%20thereby%0Aaffecting%20the%20optimality%20guarantees%20of%20the%20search.%20Bounded%20suboptimal%20search%0Ausing%20several%20such%20partially%20good%20but%20inadmissible%20heuristics%20was%20developed%20in%0AMulti-Heuristic%20A%2A%20%28MHA%2A%29.%20Although%20MHA%2A%20leverages%20multiple%20inadmissible%0Aheuristics%20to%20potentially%20generate%20a%20faster%20suboptimal%20solution%2C%20the%20original%0Aversion%20does%20not%20improve%20the%20solution%20over%20time.%20It%20is%20a%20one%20shot%20algorithm%0Athat%20requires%20careful%20setting%20of%20inflation%20factors%20to%20obtain%20a%20desired%20one%20time%0Asolution.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20extending%20MHA%2A%20to%20an%20anytime%0Aversion%20that%20finds%20a%20feasible%20suboptimal%20solution%20quickly%20and%20continually%0Aimproves%20it%20until%20time%20runs%20out.%20Our%20work%20is%20inspired%20from%20the%20Anytime%0ARepairing%20A%2A%20%28ARA%2A%29%20algorithm.%20We%20prove%20that%20our%20precise%20adaptation%20of%20ARA%2A%0Aconcepts%20in%20the%20MHA%2A%20framework%20preserves%20the%20original%20suboptimal%20and%0Acompleteness%20guarantees%20and%20enhances%20MHA%2A%20to%20perform%20in%20an%20anytime%20fashion.%0AFurthermore%2C%20we%20report%20the%20performance%20of%20A-MHA%2A%20in%203-D%20path%20planning%20domain%0Aand%20sliding%20tiles%20puzzle%20and%20compare%20against%20MHA%2A%20and%20other%20anytime%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21637v1&entry.124074799=Read"},
{"title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning", "author": "Zinan Tang and Xin Gao and Qizhi Pei and Zhuoshi Pan and Mengzhang Cai and Jiang Wu and Conghui He and Lijun Wu", "abstract": "  Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.\n", "link": "http://arxiv.org/abs/2508.21589v1", "date": "2025-08-29", "relevancy": 1.5844, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.547}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning&body=Title%3A%20Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning%0AAuthor%3A%20Zinan%20Tang%20and%20Xin%20Gao%20and%20Qizhi%20Pei%20and%20Zhuoshi%20Pan%20and%20Mengzhang%20Cai%20and%20Jiang%20Wu%20and%20Conghui%20He%20and%20Lijun%20Wu%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20Large%20Language%20Models%20%28LLM%29%20fundamentally%20rely%0Aon%20high-quality%20training%20data.%20While%20data%20selection%20and%20data%20synthesis%20are%20two%0Acommon%20strategies%20to%20improve%20data%20quality%2C%20existing%20approaches%20often%20face%0Alimitations%20in%20static%20dataset%20curation%20that%20fail%20to%20adapt%20to%20evolving%20model%0Acapabilities.%20In%20this%20paper%2C%20we%20introduce%20Middo%2C%20a%20self-evolving%20Model-informed%0Adynamic%20data%20optimization%20framework%20that%20uses%20model-aware%20data%20selection%20and%0Acontext-preserving%20data%20refinement.%20Unlike%20conventional%20one-off%0Afiltering/synthesis%20methods%2C%20our%20framework%20establishes%20a%20closed-loop%0Aoptimization%20system%3A%20%281%29%20A%20self-referential%20diagnostic%20module%20proactively%0Aidentifies%20suboptimal%20samples%20through%20tri-axial%20model%20signals%20-%20loss%20patterns%0A%28complexity%29%2C%20embedding%20cluster%20dynamics%20%28diversity%29%2C%20and%20self-alignment%20scores%0A%28quality%29%3B%20%282%29%20An%20adaptive%20optimization%20engine%20then%20transforms%20suboptimal%0Asamples%20into%20pedagogically%20valuable%20training%20points%20while%20preserving%20semantic%0Aintegrity%3B%20%283%29%20This%20optimization%20process%20continuously%20evolves%20with%20model%0Acapability%20through%20dynamic%20learning%20principles.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20%5Cmethod%20consistently%20enhances%20the%20quality%20of%0Aseed%20data%20and%20boosts%20LLM%27s%20performance%20with%20improving%20accuracy%20by%207.15%25%20on%0Aaverage%20while%20maintaining%20the%20original%20dataset%20scale.%20This%20work%20establishes%20a%0Anew%20paradigm%20for%20sustainable%20LLM%20training%20through%20dynamic%20human-AI%20co-evolution%0Aof%20data%20and%20models.%20Our%20datasets%2C%20models%2C%20and%20code%20are%20coming%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiddo%253A%2520Model-Informed%2520Dynamic%2520Data%2520Optimization%2520for%2520Enhanced%2520LLM%250A%2520%2520Fine-Tuning%2520via%2520Closed-Loop%2520Learning%26entry.906535625%3DZinan%2520Tang%2520and%2520Xin%2520Gao%2520and%2520Qizhi%2520Pei%2520and%2520Zhuoshi%2520Pan%2520and%2520Mengzhang%2520Cai%2520and%2520Jiang%2520Wu%2520and%2520Conghui%2520He%2520and%2520Lijun%2520Wu%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520fundamentally%2520rely%250Aon%2520high-quality%2520training%2520data.%2520While%2520data%2520selection%2520and%2520data%2520synthesis%2520are%2520two%250Acommon%2520strategies%2520to%2520improve%2520data%2520quality%252C%2520existing%2520approaches%2520often%2520face%250Alimitations%2520in%2520static%2520dataset%2520curation%2520that%2520fail%2520to%2520adapt%2520to%2520evolving%2520model%250Acapabilities.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Middo%252C%2520a%2520self-evolving%2520Model-informed%250Adynamic%2520data%2520optimization%2520framework%2520that%2520uses%2520model-aware%2520data%2520selection%2520and%250Acontext-preserving%2520data%2520refinement.%2520Unlike%2520conventional%2520one-off%250Afiltering/synthesis%2520methods%252C%2520our%2520framework%2520establishes%2520a%2520closed-loop%250Aoptimization%2520system%253A%2520%25281%2529%2520A%2520self-referential%2520diagnostic%2520module%2520proactively%250Aidentifies%2520suboptimal%2520samples%2520through%2520tri-axial%2520model%2520signals%2520-%2520loss%2520patterns%250A%2528complexity%2529%252C%2520embedding%2520cluster%2520dynamics%2520%2528diversity%2529%252C%2520and%2520self-alignment%2520scores%250A%2528quality%2529%253B%2520%25282%2529%2520An%2520adaptive%2520optimization%2520engine%2520then%2520transforms%2520suboptimal%250Asamples%2520into%2520pedagogically%2520valuable%2520training%2520points%2520while%2520preserving%2520semantic%250Aintegrity%253B%2520%25283%2529%2520This%2520optimization%2520process%2520continuously%2520evolves%2520with%2520model%250Acapability%2520through%2520dynamic%2520learning%2520principles.%2520Experiments%2520on%2520multiple%250Abenchmarks%2520demonstrate%2520that%2520our%2520%255Cmethod%2520consistently%2520enhances%2520the%2520quality%2520of%250Aseed%2520data%2520and%2520boosts%2520LLM%2527s%2520performance%2520with%2520improving%2520accuracy%2520by%25207.15%2525%2520on%250Aaverage%2520while%2520maintaining%2520the%2520original%2520dataset%2520scale.%2520This%2520work%2520establishes%2520a%250Anew%2520paradigm%2520for%2520sustainable%2520LLM%2520training%2520through%2520dynamic%2520human-AI%2520co-evolution%250Aof%2520data%2520and%2520models.%2520Our%2520datasets%252C%2520models%252C%2520and%2520code%2520are%2520coming%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Middo%3A%20Model-Informed%20Dynamic%20Data%20Optimization%20for%20Enhanced%20LLM%0A%20%20Fine-Tuning%20via%20Closed-Loop%20Learning&entry.906535625=Zinan%20Tang%20and%20Xin%20Gao%20and%20Qizhi%20Pei%20and%20Zhuoshi%20Pan%20and%20Mengzhang%20Cai%20and%20Jiang%20Wu%20and%20Conghui%20He%20and%20Lijun%20Wu&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20Large%20Language%20Models%20%28LLM%29%20fundamentally%20rely%0Aon%20high-quality%20training%20data.%20While%20data%20selection%20and%20data%20synthesis%20are%20two%0Acommon%20strategies%20to%20improve%20data%20quality%2C%20existing%20approaches%20often%20face%0Alimitations%20in%20static%20dataset%20curation%20that%20fail%20to%20adapt%20to%20evolving%20model%0Acapabilities.%20In%20this%20paper%2C%20we%20introduce%20Middo%2C%20a%20self-evolving%20Model-informed%0Adynamic%20data%20optimization%20framework%20that%20uses%20model-aware%20data%20selection%20and%0Acontext-preserving%20data%20refinement.%20Unlike%20conventional%20one-off%0Afiltering/synthesis%20methods%2C%20our%20framework%20establishes%20a%20closed-loop%0Aoptimization%20system%3A%20%281%29%20A%20self-referential%20diagnostic%20module%20proactively%0Aidentifies%20suboptimal%20samples%20through%20tri-axial%20model%20signals%20-%20loss%20patterns%0A%28complexity%29%2C%20embedding%20cluster%20dynamics%20%28diversity%29%2C%20and%20self-alignment%20scores%0A%28quality%29%3B%20%282%29%20An%20adaptive%20optimization%20engine%20then%20transforms%20suboptimal%0Asamples%20into%20pedagogically%20valuable%20training%20points%20while%20preserving%20semantic%0Aintegrity%3B%20%283%29%20This%20optimization%20process%20continuously%20evolves%20with%20model%0Acapability%20through%20dynamic%20learning%20principles.%20Experiments%20on%20multiple%0Abenchmarks%20demonstrate%20that%20our%20%5Cmethod%20consistently%20enhances%20the%20quality%20of%0Aseed%20data%20and%20boosts%20LLM%27s%20performance%20with%20improving%20accuracy%20by%207.15%25%20on%0Aaverage%20while%20maintaining%20the%20original%20dataset%20scale.%20This%20work%20establishes%20a%0Anew%20paradigm%20for%20sustainable%20LLM%20training%20through%20dynamic%20human-AI%20co-evolution%0Aof%20data%20and%20models.%20Our%20datasets%2C%20models%2C%20and%20code%20are%20coming%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21589v1&entry.124074799=Read"},
{"title": "Orientability of Causal Relations in Time Series using Summary Causal\n  Graphs and Faithful Distributions", "author": "Timoth\u00e9e Loranchet and Charles K. Assaad", "abstract": "  Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.\n", "link": "http://arxiv.org/abs/2508.21742v1", "date": "2025-08-29", "relevancy": 1.1903, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4381}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4188}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orientability%20of%20Causal%20Relations%20in%20Time%20Series%20using%20Summary%20Causal%0A%20%20Graphs%20and%20Faithful%20Distributions&body=Title%3A%20Orientability%20of%20Causal%20Relations%20in%20Time%20Series%20using%20Summary%20Causal%0A%20%20Graphs%20and%20Faithful%20Distributions%0AAuthor%3A%20Timoth%C3%A9e%20Loranchet%20and%20Charles%20K.%20Assaad%0AAbstract%3A%20%20%20Understanding%20causal%20relations%20between%20temporal%20variables%20is%20a%20central%0Achallenge%20in%20time%20series%20analysis%2C%20particularly%20when%20the%20full%20causal%20structure%0Ais%20unknown.%20Even%20when%20the%20full%20causal%20structure%20cannot%20be%20fully%20specified%2C%0Aexperts%20often%20succeed%20in%20providing%20a%20high-level%20abstraction%20of%20the%20causal%0Agraph%2C%20known%20as%20a%20summary%20causal%20graph%2C%20which%20captures%20the%20main%20causal%0Arelations%20between%20different%20time%20series%20while%20abstracting%20away%20micro-level%0Adetails.%20In%20this%20work%2C%20we%20present%20conditions%20that%20guarantee%20the%20orientability%0Aof%20micro-level%20edges%20between%20temporal%20variables%20given%20the%20background%20knowledge%0Aencoded%20in%20a%20summary%20causal%20graph%20and%20assuming%20having%20access%20to%20a%20faithful%20and%0Acausally%20sufficient%20distribution%20with%20respect%20to%20the%20true%20unknown%20graph.%20Our%0Aresults%20provide%20theoretical%20guarantees%20for%20edge%20orientation%20at%20the%20micro-level%2C%0Aeven%20in%20the%20presence%20of%20cycles%20or%20bidirected%20edges%20at%20the%20macro-level.%20These%0Afindings%20offer%20practical%20guidance%20for%20leveraging%20SCGs%20to%20inform%20causal%0Adiscovery%20in%20complex%20temporal%20systems%20and%20highlight%20the%20value%20of%20incorporating%0Aexpert%20knowledge%20to%20improve%20causal%20inference%20from%20observational%20time%20series%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrientability%2520of%2520Causal%2520Relations%2520in%2520Time%2520Series%2520using%2520Summary%2520Causal%250A%2520%2520Graphs%2520and%2520Faithful%2520Distributions%26entry.906535625%3DTimoth%25C3%25A9e%2520Loranchet%2520and%2520Charles%2520K.%2520Assaad%26entry.1292438233%3D%2520%2520Understanding%2520causal%2520relations%2520between%2520temporal%2520variables%2520is%2520a%2520central%250Achallenge%2520in%2520time%2520series%2520analysis%252C%2520particularly%2520when%2520the%2520full%2520causal%2520structure%250Ais%2520unknown.%2520Even%2520when%2520the%2520full%2520causal%2520structure%2520cannot%2520be%2520fully%2520specified%252C%250Aexperts%2520often%2520succeed%2520in%2520providing%2520a%2520high-level%2520abstraction%2520of%2520the%2520causal%250Agraph%252C%2520known%2520as%2520a%2520summary%2520causal%2520graph%252C%2520which%2520captures%2520the%2520main%2520causal%250Arelations%2520between%2520different%2520time%2520series%2520while%2520abstracting%2520away%2520micro-level%250Adetails.%2520In%2520this%2520work%252C%2520we%2520present%2520conditions%2520that%2520guarantee%2520the%2520orientability%250Aof%2520micro-level%2520edges%2520between%2520temporal%2520variables%2520given%2520the%2520background%2520knowledge%250Aencoded%2520in%2520a%2520summary%2520causal%2520graph%2520and%2520assuming%2520having%2520access%2520to%2520a%2520faithful%2520and%250Acausally%2520sufficient%2520distribution%2520with%2520respect%2520to%2520the%2520true%2520unknown%2520graph.%2520Our%250Aresults%2520provide%2520theoretical%2520guarantees%2520for%2520edge%2520orientation%2520at%2520the%2520micro-level%252C%250Aeven%2520in%2520the%2520presence%2520of%2520cycles%2520or%2520bidirected%2520edges%2520at%2520the%2520macro-level.%2520These%250Afindings%2520offer%2520practical%2520guidance%2520for%2520leveraging%2520SCGs%2520to%2520inform%2520causal%250Adiscovery%2520in%2520complex%2520temporal%2520systems%2520and%2520highlight%2520the%2520value%2520of%2520incorporating%250Aexpert%2520knowledge%2520to%2520improve%2520causal%2520inference%2520from%2520observational%2520time%2520series%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orientability%20of%20Causal%20Relations%20in%20Time%20Series%20using%20Summary%20Causal%0A%20%20Graphs%20and%20Faithful%20Distributions&entry.906535625=Timoth%C3%A9e%20Loranchet%20and%20Charles%20K.%20Assaad&entry.1292438233=%20%20Understanding%20causal%20relations%20between%20temporal%20variables%20is%20a%20central%0Achallenge%20in%20time%20series%20analysis%2C%20particularly%20when%20the%20full%20causal%20structure%0Ais%20unknown.%20Even%20when%20the%20full%20causal%20structure%20cannot%20be%20fully%20specified%2C%0Aexperts%20often%20succeed%20in%20providing%20a%20high-level%20abstraction%20of%20the%20causal%0Agraph%2C%20known%20as%20a%20summary%20causal%20graph%2C%20which%20captures%20the%20main%20causal%0Arelations%20between%20different%20time%20series%20while%20abstracting%20away%20micro-level%0Adetails.%20In%20this%20work%2C%20we%20present%20conditions%20that%20guarantee%20the%20orientability%0Aof%20micro-level%20edges%20between%20temporal%20variables%20given%20the%20background%20knowledge%0Aencoded%20in%20a%20summary%20causal%20graph%20and%20assuming%20having%20access%20to%20a%20faithful%20and%0Acausally%20sufficient%20distribution%20with%20respect%20to%20the%20true%20unknown%20graph.%20Our%0Aresults%20provide%20theoretical%20guarantees%20for%20edge%20orientation%20at%20the%20micro-level%2C%0Aeven%20in%20the%20presence%20of%20cycles%20or%20bidirected%20edges%20at%20the%20macro-level.%20These%0Afindings%20offer%20practical%20guidance%20for%20leveraging%20SCGs%20to%20inform%20causal%0Adiscovery%20in%20complex%20temporal%20systems%20and%20highlight%20the%20value%20of%20incorporating%0Aexpert%20knowledge%20to%20improve%20causal%20inference%20from%20observational%20time%20series%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21742v1&entry.124074799=Read"},
{"title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction", "author": "Junxiang Wang and Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak and Rana Soltani Zarrin and Zackory Erickson", "abstract": "  Clear communication of robot intent fosters transparency and interpretability\nin physical human-robot interaction (pHRI), particularly during assistive tasks\ninvolving direct human-robot contact. We introduce CoRI, a pipeline that\nautomatically generates natural language communication of a robot's upcoming\nactions directly from its motion plan and visual perception. Our pipeline first\nprocesses the robot's image view to identify human poses and key environmental\nfeatures. It then encodes the planned 3D spatial trajectory (including velocity\nand force) onto this view, visually grounding the path and its dynamics. CoRI\nqueries a vision-language model with this visual representation to interpret\nthe planned action within the visual context before generating concise,\nuser-directed statements, without relying on task-specific information. Results\nfrom a user study involving robot-assisted feeding, bathing, and shaving tasks\nacross two different robots indicate that CoRI leads to statistically\nsignificant difference in communication clarity compared to a baseline\ncommunication strategy. Specifically, CoRI effectively conveys not only the\nrobot's high-level intentions but also crucial details about its motion and any\ncollaborative user action needed. Video and code of our project can be found on\nour project website: https://cori-phri.github.io/\n", "link": "http://arxiv.org/abs/2505.20537v2", "date": "2025-08-29", "relevancy": 1.5502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5152}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRI%3A%20Communication%20of%20Robot%20Intent%20for%20Physical%20Human-Robot%20Interaction&body=Title%3A%20CoRI%3A%20Communication%20of%20Robot%20Intent%20for%20Physical%20Human-Robot%20Interaction%0AAuthor%3A%20Junxiang%20Wang%20and%20Emek%20Bar%C4%B1%C5%9F%20K%C3%BC%C3%A7%C3%BCktabak%20and%20Rana%20Soltani%20Zarrin%20and%20Zackory%20Erickson%0AAbstract%3A%20%20%20Clear%20communication%20of%20robot%20intent%20fosters%20transparency%20and%20interpretability%0Ain%20physical%20human-robot%20interaction%20%28pHRI%29%2C%20particularly%20during%20assistive%20tasks%0Ainvolving%20direct%20human-robot%20contact.%20We%20introduce%20CoRI%2C%20a%20pipeline%20that%0Aautomatically%20generates%20natural%20language%20communication%20of%20a%20robot%27s%20upcoming%0Aactions%20directly%20from%20its%20motion%20plan%20and%20visual%20perception.%20Our%20pipeline%20first%0Aprocesses%20the%20robot%27s%20image%20view%20to%20identify%20human%20poses%20and%20key%20environmental%0Afeatures.%20It%20then%20encodes%20the%20planned%203D%20spatial%20trajectory%20%28including%20velocity%0Aand%20force%29%20onto%20this%20view%2C%20visually%20grounding%20the%20path%20and%20its%20dynamics.%20CoRI%0Aqueries%20a%20vision-language%20model%20with%20this%20visual%20representation%20to%20interpret%0Athe%20planned%20action%20within%20the%20visual%20context%20before%20generating%20concise%2C%0Auser-directed%20statements%2C%20without%20relying%20on%20task-specific%20information.%20Results%0Afrom%20a%20user%20study%20involving%20robot-assisted%20feeding%2C%20bathing%2C%20and%20shaving%20tasks%0Aacross%20two%20different%20robots%20indicate%20that%20CoRI%20leads%20to%20statistically%0Asignificant%20difference%20in%20communication%20clarity%20compared%20to%20a%20baseline%0Acommunication%20strategy.%20Specifically%2C%20CoRI%20effectively%20conveys%20not%20only%20the%0Arobot%27s%20high-level%20intentions%20but%20also%20crucial%20details%20about%20its%20motion%20and%20any%0Acollaborative%20user%20action%20needed.%20Video%20and%20code%20of%20our%20project%20can%20be%20found%20on%0Aour%20project%20website%3A%20https%3A//cori-phri.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRI%253A%2520Communication%2520of%2520Robot%2520Intent%2520for%2520Physical%2520Human-Robot%2520Interaction%26entry.906535625%3DJunxiang%2520Wang%2520and%2520Emek%2520Bar%25C4%25B1%25C5%259F%2520K%25C3%25BC%25C3%25A7%25C3%25BCktabak%2520and%2520Rana%2520Soltani%2520Zarrin%2520and%2520Zackory%2520Erickson%26entry.1292438233%3D%2520%2520Clear%2520communication%2520of%2520robot%2520intent%2520fosters%2520transparency%2520and%2520interpretability%250Ain%2520physical%2520human-robot%2520interaction%2520%2528pHRI%2529%252C%2520particularly%2520during%2520assistive%2520tasks%250Ainvolving%2520direct%2520human-robot%2520contact.%2520We%2520introduce%2520CoRI%252C%2520a%2520pipeline%2520that%250Aautomatically%2520generates%2520natural%2520language%2520communication%2520of%2520a%2520robot%2527s%2520upcoming%250Aactions%2520directly%2520from%2520its%2520motion%2520plan%2520and%2520visual%2520perception.%2520Our%2520pipeline%2520first%250Aprocesses%2520the%2520robot%2527s%2520image%2520view%2520to%2520identify%2520human%2520poses%2520and%2520key%2520environmental%250Afeatures.%2520It%2520then%2520encodes%2520the%2520planned%25203D%2520spatial%2520trajectory%2520%2528including%2520velocity%250Aand%2520force%2529%2520onto%2520this%2520view%252C%2520visually%2520grounding%2520the%2520path%2520and%2520its%2520dynamics.%2520CoRI%250Aqueries%2520a%2520vision-language%2520model%2520with%2520this%2520visual%2520representation%2520to%2520interpret%250Athe%2520planned%2520action%2520within%2520the%2520visual%2520context%2520before%2520generating%2520concise%252C%250Auser-directed%2520statements%252C%2520without%2520relying%2520on%2520task-specific%2520information.%2520Results%250Afrom%2520a%2520user%2520study%2520involving%2520robot-assisted%2520feeding%252C%2520bathing%252C%2520and%2520shaving%2520tasks%250Aacross%2520two%2520different%2520robots%2520indicate%2520that%2520CoRI%2520leads%2520to%2520statistically%250Asignificant%2520difference%2520in%2520communication%2520clarity%2520compared%2520to%2520a%2520baseline%250Acommunication%2520strategy.%2520Specifically%252C%2520CoRI%2520effectively%2520conveys%2520not%2520only%2520the%250Arobot%2527s%2520high-level%2520intentions%2520but%2520also%2520crucial%2520details%2520about%2520its%2520motion%2520and%2520any%250Acollaborative%2520user%2520action%2520needed.%2520Video%2520and%2520code%2520of%2520our%2520project%2520can%2520be%2520found%2520on%250Aour%2520project%2520website%253A%2520https%253A//cori-phri.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRI%3A%20Communication%20of%20Robot%20Intent%20for%20Physical%20Human-Robot%20Interaction&entry.906535625=Junxiang%20Wang%20and%20Emek%20Bar%C4%B1%C5%9F%20K%C3%BC%C3%A7%C3%BCktabak%20and%20Rana%20Soltani%20Zarrin%20and%20Zackory%20Erickson&entry.1292438233=%20%20Clear%20communication%20of%20robot%20intent%20fosters%20transparency%20and%20interpretability%0Ain%20physical%20human-robot%20interaction%20%28pHRI%29%2C%20particularly%20during%20assistive%20tasks%0Ainvolving%20direct%20human-robot%20contact.%20We%20introduce%20CoRI%2C%20a%20pipeline%20that%0Aautomatically%20generates%20natural%20language%20communication%20of%20a%20robot%27s%20upcoming%0Aactions%20directly%20from%20its%20motion%20plan%20and%20visual%20perception.%20Our%20pipeline%20first%0Aprocesses%20the%20robot%27s%20image%20view%20to%20identify%20human%20poses%20and%20key%20environmental%0Afeatures.%20It%20then%20encodes%20the%20planned%203D%20spatial%20trajectory%20%28including%20velocity%0Aand%20force%29%20onto%20this%20view%2C%20visually%20grounding%20the%20path%20and%20its%20dynamics.%20CoRI%0Aqueries%20a%20vision-language%20model%20with%20this%20visual%20representation%20to%20interpret%0Athe%20planned%20action%20within%20the%20visual%20context%20before%20generating%20concise%2C%0Auser-directed%20statements%2C%20without%20relying%20on%20task-specific%20information.%20Results%0Afrom%20a%20user%20study%20involving%20robot-assisted%20feeding%2C%20bathing%2C%20and%20shaving%20tasks%0Aacross%20two%20different%20robots%20indicate%20that%20CoRI%20leads%20to%20statistically%0Asignificant%20difference%20in%20communication%20clarity%20compared%20to%20a%20baseline%0Acommunication%20strategy.%20Specifically%2C%20CoRI%20effectively%20conveys%20not%20only%20the%0Arobot%27s%20high-level%20intentions%20but%20also%20crucial%20details%20about%20its%20motion%20and%20any%0Acollaborative%20user%20action%20needed.%20Video%20and%20code%20of%20our%20project%20can%20be%20found%20on%0Aour%20project%20website%3A%20https%3A//cori-phri.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20537v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


