<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241114.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment\n  in Multi-Modal Models", "author": "Wei Wang and Zhaowei Li and Qi Xu and Linfeng Li and YiQing Cai and Botian Jiang and Hang Song and Xingcan Hu and Pengyu Wang and Li Xiao", "abstract": "  Multi-modal large language models (MLLMs) have achieved remarkable success in\nfine-grained visual understanding across a range of tasks. However, they often\nencounter significant challenges due to inadequate alignment for fine-grained\nknowledge, which restricts their ability to accurately capture local details\nand attain a comprehensive global perception. While recent advancements have\nfocused on aligning object expressions with grounding information, they\ntypically lack explicit integration of object images, which contain affluent\ninformation beyond mere texts or coordinates. To bridge this gap, we introduce\na novel fine-grained visual knowledge alignment method that effectively aligns\nand integrates multi-scale knowledge of objects, including texts, coordinates,\nand images. This innovative method is underpinned by our multi-scale\nfine-grained enhancement data synthesis pipeline, which provides over 300K\nessential training data to enhance alignment and improve overall performance.\nFurthermore, we present TinyGroundingGPT, a series of compact models optimized\nfor high-level alignments. With a scale of approximately 3B parameters,\nTinyGroundingGPT achieves outstanding results in grounding tasks while\ndelivering performance comparable to larger MLLMs in complex visual scenarios.\n", "link": "http://arxiv.org/abs/2411.09691v1", "date": "2024-11-14", "relevancy": 3.0726, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Fine-Grained%20Visual%20Understanding%20with%20Multi-Scale%20Alignment%0A%20%20in%20Multi-Modal%20Models&body=Title%3A%20Advancing%20Fine-Grained%20Visual%20Understanding%20with%20Multi-Scale%20Alignment%0A%20%20in%20Multi-Modal%20Models%0AAuthor%3A%20Wei%20Wang%20and%20Zhaowei%20Li%20and%20Qi%20Xu%20and%20Linfeng%20Li%20and%20YiQing%20Cai%20and%20Botian%20Jiang%20and%20Hang%20Song%20and%20Xingcan%20Hu%20and%20Pengyu%20Wang%20and%20Li%20Xiao%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Afine-grained%20visual%20understanding%20across%20a%20range%20of%20tasks.%20However%2C%20they%20often%0Aencounter%20significant%20challenges%20due%20to%20inadequate%20alignment%20for%20fine-grained%0Aknowledge%2C%20which%20restricts%20their%20ability%20to%20accurately%20capture%20local%20details%0Aand%20attain%20a%20comprehensive%20global%20perception.%20While%20recent%20advancements%20have%0Afocused%20on%20aligning%20object%20expressions%20with%20grounding%20information%2C%20they%0Atypically%20lack%20explicit%20integration%20of%20object%20images%2C%20which%20contain%20affluent%0Ainformation%20beyond%20mere%20texts%20or%20coordinates.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Aa%20novel%20fine-grained%20visual%20knowledge%20alignment%20method%20that%20effectively%20aligns%0Aand%20integrates%20multi-scale%20knowledge%20of%20objects%2C%20including%20texts%2C%20coordinates%2C%0Aand%20images.%20This%20innovative%20method%20is%20underpinned%20by%20our%20multi-scale%0Afine-grained%20enhancement%20data%20synthesis%20pipeline%2C%20which%20provides%20over%20300K%0Aessential%20training%20data%20to%20enhance%20alignment%20and%20improve%20overall%20performance.%0AFurthermore%2C%20we%20present%20TinyGroundingGPT%2C%20a%20series%20of%20compact%20models%20optimized%0Afor%20high-level%20alignments.%20With%20a%20scale%20of%20approximately%203B%20parameters%2C%0ATinyGroundingGPT%20achieves%20outstanding%20results%20in%20grounding%20tasks%20while%0Adelivering%20performance%20comparable%20to%20larger%20MLLMs%20in%20complex%20visual%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Fine-Grained%2520Visual%2520Understanding%2520with%2520Multi-Scale%2520Alignment%250A%2520%2520in%2520Multi-Modal%2520Models%26entry.906535625%3DWei%2520Wang%2520and%2520Zhaowei%2520Li%2520and%2520Qi%2520Xu%2520and%2520Linfeng%2520Li%2520and%2520YiQing%2520Cai%2520and%2520Botian%2520Jiang%2520and%2520Hang%2520Song%2520and%2520Xingcan%2520Hu%2520and%2520Pengyu%2520Wang%2520and%2520Li%2520Xiao%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%250Afine-grained%2520visual%2520understanding%2520across%2520a%2520range%2520of%2520tasks.%2520However%252C%2520they%2520often%250Aencounter%2520significant%2520challenges%2520due%2520to%2520inadequate%2520alignment%2520for%2520fine-grained%250Aknowledge%252C%2520which%2520restricts%2520their%2520ability%2520to%2520accurately%2520capture%2520local%2520details%250Aand%2520attain%2520a%2520comprehensive%2520global%2520perception.%2520While%2520recent%2520advancements%2520have%250Afocused%2520on%2520aligning%2520object%2520expressions%2520with%2520grounding%2520information%252C%2520they%250Atypically%2520lack%2520explicit%2520integration%2520of%2520object%2520images%252C%2520which%2520contain%2520affluent%250Ainformation%2520beyond%2520mere%2520texts%2520or%2520coordinates.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250Aa%2520novel%2520fine-grained%2520visual%2520knowledge%2520alignment%2520method%2520that%2520effectively%2520aligns%250Aand%2520integrates%2520multi-scale%2520knowledge%2520of%2520objects%252C%2520including%2520texts%252C%2520coordinates%252C%250Aand%2520images.%2520This%2520innovative%2520method%2520is%2520underpinned%2520by%2520our%2520multi-scale%250Afine-grained%2520enhancement%2520data%2520synthesis%2520pipeline%252C%2520which%2520provides%2520over%2520300K%250Aessential%2520training%2520data%2520to%2520enhance%2520alignment%2520and%2520improve%2520overall%2520performance.%250AFurthermore%252C%2520we%2520present%2520TinyGroundingGPT%252C%2520a%2520series%2520of%2520compact%2520models%2520optimized%250Afor%2520high-level%2520alignments.%2520With%2520a%2520scale%2520of%2520approximately%25203B%2520parameters%252C%250ATinyGroundingGPT%2520achieves%2520outstanding%2520results%2520in%2520grounding%2520tasks%2520while%250Adelivering%2520performance%2520comparable%2520to%2520larger%2520MLLMs%2520in%2520complex%2520visual%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Fine-Grained%20Visual%20Understanding%20with%20Multi-Scale%20Alignment%0A%20%20in%20Multi-Modal%20Models&entry.906535625=Wei%20Wang%20and%20Zhaowei%20Li%20and%20Qi%20Xu%20and%20Linfeng%20Li%20and%20YiQing%20Cai%20and%20Botian%20Jiang%20and%20Hang%20Song%20and%20Xingcan%20Hu%20and%20Pengyu%20Wang%20and%20Li%20Xiao&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20in%0Afine-grained%20visual%20understanding%20across%20a%20range%20of%20tasks.%20However%2C%20they%20often%0Aencounter%20significant%20challenges%20due%20to%20inadequate%20alignment%20for%20fine-grained%0Aknowledge%2C%20which%20restricts%20their%20ability%20to%20accurately%20capture%20local%20details%0Aand%20attain%20a%20comprehensive%20global%20perception.%20While%20recent%20advancements%20have%0Afocused%20on%20aligning%20object%20expressions%20with%20grounding%20information%2C%20they%0Atypically%20lack%20explicit%20integration%20of%20object%20images%2C%20which%20contain%20affluent%0Ainformation%20beyond%20mere%20texts%20or%20coordinates.%20To%20bridge%20this%20gap%2C%20we%20introduce%0Aa%20novel%20fine-grained%20visual%20knowledge%20alignment%20method%20that%20effectively%20aligns%0Aand%20integrates%20multi-scale%20knowledge%20of%20objects%2C%20including%20texts%2C%20coordinates%2C%0Aand%20images.%20This%20innovative%20method%20is%20underpinned%20by%20our%20multi-scale%0Afine-grained%20enhancement%20data%20synthesis%20pipeline%2C%20which%20provides%20over%20300K%0Aessential%20training%20data%20to%20enhance%20alignment%20and%20improve%20overall%20performance.%0AFurthermore%2C%20we%20present%20TinyGroundingGPT%2C%20a%20series%20of%20compact%20models%20optimized%0Afor%20high-level%20alignments.%20With%20a%20scale%20of%20approximately%203B%20parameters%2C%0ATinyGroundingGPT%20achieves%20outstanding%20results%20in%20grounding%20tasks%20while%0Adelivering%20performance%20comparable%20to%20larger%20MLLMs%20in%20complex%20visual%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09691v1&entry.124074799=Read"},
{"title": "Long-Tailed Object Detection Pre-training: Dynamic Rebalancing\n  Contrastive Learning with Dual Reconstruction", "author": "Chen-Long Duan and Yong Li and Xiu-Shen Wei and Lin Zhao", "abstract": "  Pre-training plays a vital role in various vision tasks, such as object\nrecognition and detection. Commonly used pre-training methods, which typically\nrely on randomized approaches like uniform or Gaussian distributions to\ninitialize model parameters, often fall short when confronted with long-tailed\ndistributions, especially in detection tasks. This is largely due to extreme\ndata imbalance and the issue of simplicity bias. In this paper, we introduce a\nnovel pre-training framework for object detection, called Dynamic Rebalancing\nContrastive Learning with Dual Reconstruction (2DRCL). Our method builds on a\nHolistic-Local Contrastive Learning mechanism, which aligns pre-training with\nobject detection by capturing both global contextual semantics and detailed\nlocal patterns. To tackle the imbalance inherent in long-tailed data, we design\na dynamic rebalancing strategy that adjusts the sampling of underrepresented\ninstances throughout the pre-training process, ensuring better representation\nof tail classes. Moreover, Dual Reconstruction addresses simplicity bias by\nenforcing a reconstruction task aligned with the self-consistency principle,\nspecifically benefiting underrepresented tail classes. Experiments on COCO and\nLVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in\nimproving the mAP/AP scores for tail classes.\n", "link": "http://arxiv.org/abs/2411.09453v1", "date": "2024-11-14", "relevancy": 2.9364, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%20Object%20Detection%20Pre-training%3A%20Dynamic%20Rebalancing%0A%20%20Contrastive%20Learning%20with%20Dual%20Reconstruction&body=Title%3A%20Long-Tailed%20Object%20Detection%20Pre-training%3A%20Dynamic%20Rebalancing%0A%20%20Contrastive%20Learning%20with%20Dual%20Reconstruction%0AAuthor%3A%20Chen-Long%20Duan%20and%20Yong%20Li%20and%20Xiu-Shen%20Wei%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Pre-training%20plays%20a%20vital%20role%20in%20various%20vision%20tasks%2C%20such%20as%20object%0Arecognition%20and%20detection.%20Commonly%20used%20pre-training%20methods%2C%20which%20typically%0Arely%20on%20randomized%20approaches%20like%20uniform%20or%20Gaussian%20distributions%20to%0Ainitialize%20model%20parameters%2C%20often%20fall%20short%20when%20confronted%20with%20long-tailed%0Adistributions%2C%20especially%20in%20detection%20tasks.%20This%20is%20largely%20due%20to%20extreme%0Adata%20imbalance%20and%20the%20issue%20of%20simplicity%20bias.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20pre-training%20framework%20for%20object%20detection%2C%20called%20Dynamic%20Rebalancing%0AContrastive%20Learning%20with%20Dual%20Reconstruction%20%282DRCL%29.%20Our%20method%20builds%20on%20a%0AHolistic-Local%20Contrastive%20Learning%20mechanism%2C%20which%20aligns%20pre-training%20with%0Aobject%20detection%20by%20capturing%20both%20global%20contextual%20semantics%20and%20detailed%0Alocal%20patterns.%20To%20tackle%20the%20imbalance%20inherent%20in%20long-tailed%20data%2C%20we%20design%0Aa%20dynamic%20rebalancing%20strategy%20that%20adjusts%20the%20sampling%20of%20underrepresented%0Ainstances%20throughout%20the%20pre-training%20process%2C%20ensuring%20better%20representation%0Aof%20tail%20classes.%20Moreover%2C%20Dual%20Reconstruction%20addresses%20simplicity%20bias%20by%0Aenforcing%20a%20reconstruction%20task%20aligned%20with%20the%20self-consistency%20principle%2C%0Aspecifically%20benefiting%20underrepresented%20tail%20classes.%20Experiments%20on%20COCO%20and%0ALVIS%20v1.0%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20particularly%20in%0Aimproving%20the%20mAP/AP%20scores%20for%20tail%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tailed%2520Object%2520Detection%2520Pre-training%253A%2520Dynamic%2520Rebalancing%250A%2520%2520Contrastive%2520Learning%2520with%2520Dual%2520Reconstruction%26entry.906535625%3DChen-Long%2520Duan%2520and%2520Yong%2520Li%2520and%2520Xiu-Shen%2520Wei%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Pre-training%2520plays%2520a%2520vital%2520role%2520in%2520various%2520vision%2520tasks%252C%2520such%2520as%2520object%250Arecognition%2520and%2520detection.%2520Commonly%2520used%2520pre-training%2520methods%252C%2520which%2520typically%250Arely%2520on%2520randomized%2520approaches%2520like%2520uniform%2520or%2520Gaussian%2520distributions%2520to%250Ainitialize%2520model%2520parameters%252C%2520often%2520fall%2520short%2520when%2520confronted%2520with%2520long-tailed%250Adistributions%252C%2520especially%2520in%2520detection%2520tasks.%2520This%2520is%2520largely%2520due%2520to%2520extreme%250Adata%2520imbalance%2520and%2520the%2520issue%2520of%2520simplicity%2520bias.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520pre-training%2520framework%2520for%2520object%2520detection%252C%2520called%2520Dynamic%2520Rebalancing%250AContrastive%2520Learning%2520with%2520Dual%2520Reconstruction%2520%25282DRCL%2529.%2520Our%2520method%2520builds%2520on%2520a%250AHolistic-Local%2520Contrastive%2520Learning%2520mechanism%252C%2520which%2520aligns%2520pre-training%2520with%250Aobject%2520detection%2520by%2520capturing%2520both%2520global%2520contextual%2520semantics%2520and%2520detailed%250Alocal%2520patterns.%2520To%2520tackle%2520the%2520imbalance%2520inherent%2520in%2520long-tailed%2520data%252C%2520we%2520design%250Aa%2520dynamic%2520rebalancing%2520strategy%2520that%2520adjusts%2520the%2520sampling%2520of%2520underrepresented%250Ainstances%2520throughout%2520the%2520pre-training%2520process%252C%2520ensuring%2520better%2520representation%250Aof%2520tail%2520classes.%2520Moreover%252C%2520Dual%2520Reconstruction%2520addresses%2520simplicity%2520bias%2520by%250Aenforcing%2520a%2520reconstruction%2520task%2520aligned%2520with%2520the%2520self-consistency%2520principle%252C%250Aspecifically%2520benefiting%2520underrepresented%2520tail%2520classes.%2520Experiments%2520on%2520COCO%2520and%250ALVIS%2520v1.0%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520particularly%2520in%250Aimproving%2520the%2520mAP/AP%2520scores%2520for%2520tail%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%20Object%20Detection%20Pre-training%3A%20Dynamic%20Rebalancing%0A%20%20Contrastive%20Learning%20with%20Dual%20Reconstruction&entry.906535625=Chen-Long%20Duan%20and%20Yong%20Li%20and%20Xiu-Shen%20Wei%20and%20Lin%20Zhao&entry.1292438233=%20%20Pre-training%20plays%20a%20vital%20role%20in%20various%20vision%20tasks%2C%20such%20as%20object%0Arecognition%20and%20detection.%20Commonly%20used%20pre-training%20methods%2C%20which%20typically%0Arely%20on%20randomized%20approaches%20like%20uniform%20or%20Gaussian%20distributions%20to%0Ainitialize%20model%20parameters%2C%20often%20fall%20short%20when%20confronted%20with%20long-tailed%0Adistributions%2C%20especially%20in%20detection%20tasks.%20This%20is%20largely%20due%20to%20extreme%0Adata%20imbalance%20and%20the%20issue%20of%20simplicity%20bias.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20pre-training%20framework%20for%20object%20detection%2C%20called%20Dynamic%20Rebalancing%0AContrastive%20Learning%20with%20Dual%20Reconstruction%20%282DRCL%29.%20Our%20method%20builds%20on%20a%0AHolistic-Local%20Contrastive%20Learning%20mechanism%2C%20which%20aligns%20pre-training%20with%0Aobject%20detection%20by%20capturing%20both%20global%20contextual%20semantics%20and%20detailed%0Alocal%20patterns.%20To%20tackle%20the%20imbalance%20inherent%20in%20long-tailed%20data%2C%20we%20design%0Aa%20dynamic%20rebalancing%20strategy%20that%20adjusts%20the%20sampling%20of%20underrepresented%0Ainstances%20throughout%20the%20pre-training%20process%2C%20ensuring%20better%20representation%0Aof%20tail%20classes.%20Moreover%2C%20Dual%20Reconstruction%20addresses%20simplicity%20bias%20by%0Aenforcing%20a%20reconstruction%20task%20aligned%20with%20the%20self-consistency%20principle%2C%0Aspecifically%20benefiting%20underrepresented%20tail%20classes.%20Experiments%20on%20COCO%20and%0ALVIS%20v1.0%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20particularly%20in%0Aimproving%20the%20mAP/AP%20scores%20for%20tail%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09453v1&entry.124074799=Read"},
{"title": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery", "author": "Gu\u00e9nol\u00e9 Fiche and Simon Leglaive and Xavier Alameda-Pineda and Francesc Moreno-Noguer", "abstract": "  Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous\nproblem, as an infinite set of 3D interpretations can explain the 2D\nobservation equally well. Nevertheless, most HMR methods overlook this issue\nand make a single prediction without accounting for this ambiguity. A few\napproaches generate a distribution of human meshes, enabling the sampling of\nmultiple predictions; however, none of them is competitive with the latest\nsingle-output model when making a single prediction. This work proposes a new\napproach based on masked generative modeling. By tokenizing the human pose and\nshape, we formulate the HMR task as generating a sequence of discrete tokens\nconditioned on an input image. We introduce MEGA, a MaskEd Generative\nAutoencoder trained to recover human meshes from images and partial human mesh\ntoken sequences. Given an image, our flexible generation scheme allows us to\npredict a single human mesh in deterministic mode or to generate multiple human\nmeshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA\nachieves state-of-the-art performance in deterministic and stochastic modes,\noutperforming single-output and multi-output approaches.\n", "link": "http://arxiv.org/abs/2405.18839v3", "date": "2024-11-14", "relevancy": 2.9259, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6097}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5905}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery&body=Title%3A%20MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery%0AAuthor%3A%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Francesc%20Moreno-Noguer%0AAbstract%3A%20%20%20Human%20Mesh%20Recovery%20%28HMR%29%20from%20a%20single%20RGB%20image%20is%20a%20highly%20ambiguous%0Aproblem%2C%20as%20an%20infinite%20set%20of%203D%20interpretations%20can%20explain%20the%202D%0Aobservation%20equally%20well.%20Nevertheless%2C%20most%20HMR%20methods%20overlook%20this%20issue%0Aand%20make%20a%20single%20prediction%20without%20accounting%20for%20this%20ambiguity.%20A%20few%0Aapproaches%20generate%20a%20distribution%20of%20human%20meshes%2C%20enabling%20the%20sampling%20of%0Amultiple%20predictions%3B%20however%2C%20none%20of%20them%20is%20competitive%20with%20the%20latest%0Asingle-output%20model%20when%20making%20a%20single%20prediction.%20This%20work%20proposes%20a%20new%0Aapproach%20based%20on%20masked%20generative%20modeling.%20By%20tokenizing%20the%20human%20pose%20and%0Ashape%2C%20we%20formulate%20the%20HMR%20task%20as%20generating%20a%20sequence%20of%20discrete%20tokens%0Aconditioned%20on%20an%20input%20image.%20We%20introduce%20MEGA%2C%20a%20MaskEd%20Generative%0AAutoencoder%20trained%20to%20recover%20human%20meshes%20from%20images%20and%20partial%20human%20mesh%0Atoken%20sequences.%20Given%20an%20image%2C%20our%20flexible%20generation%20scheme%20allows%20us%20to%0Apredict%20a%20single%20human%20mesh%20in%20deterministic%20mode%20or%20to%20generate%20multiple%20human%0Ameshes%20in%20stochastic%20mode.%20Experiments%20on%20in-the-wild%20benchmarks%20show%20that%20MEGA%0Aachieves%20state-of-the-art%20performance%20in%20deterministic%20and%20stochastic%20modes%2C%0Aoutperforming%20single-output%20and%20multi-output%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18839v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Masked%2520Generative%2520Autoencoder%2520for%2520Human%2520Mesh%2520Recovery%26entry.906535625%3DGu%25C3%25A9nol%25C3%25A9%2520Fiche%2520and%2520Simon%2520Leglaive%2520and%2520Xavier%2520Alameda-Pineda%2520and%2520Francesc%2520Moreno-Noguer%26entry.1292438233%3D%2520%2520Human%2520Mesh%2520Recovery%2520%2528HMR%2529%2520from%2520a%2520single%2520RGB%2520image%2520is%2520a%2520highly%2520ambiguous%250Aproblem%252C%2520as%2520an%2520infinite%2520set%2520of%25203D%2520interpretations%2520can%2520explain%2520the%25202D%250Aobservation%2520equally%2520well.%2520Nevertheless%252C%2520most%2520HMR%2520methods%2520overlook%2520this%2520issue%250Aand%2520make%2520a%2520single%2520prediction%2520without%2520accounting%2520for%2520this%2520ambiguity.%2520A%2520few%250Aapproaches%2520generate%2520a%2520distribution%2520of%2520human%2520meshes%252C%2520enabling%2520the%2520sampling%2520of%250Amultiple%2520predictions%253B%2520however%252C%2520none%2520of%2520them%2520is%2520competitive%2520with%2520the%2520latest%250Asingle-output%2520model%2520when%2520making%2520a%2520single%2520prediction.%2520This%2520work%2520proposes%2520a%2520new%250Aapproach%2520based%2520on%2520masked%2520generative%2520modeling.%2520By%2520tokenizing%2520the%2520human%2520pose%2520and%250Ashape%252C%2520we%2520formulate%2520the%2520HMR%2520task%2520as%2520generating%2520a%2520sequence%2520of%2520discrete%2520tokens%250Aconditioned%2520on%2520an%2520input%2520image.%2520We%2520introduce%2520MEGA%252C%2520a%2520MaskEd%2520Generative%250AAutoencoder%2520trained%2520to%2520recover%2520human%2520meshes%2520from%2520images%2520and%2520partial%2520human%2520mesh%250Atoken%2520sequences.%2520Given%2520an%2520image%252C%2520our%2520flexible%2520generation%2520scheme%2520allows%2520us%2520to%250Apredict%2520a%2520single%2520human%2520mesh%2520in%2520deterministic%2520mode%2520or%2520to%2520generate%2520multiple%2520human%250Ameshes%2520in%2520stochastic%2520mode.%2520Experiments%2520on%2520in-the-wild%2520benchmarks%2520show%2520that%2520MEGA%250Aachieves%2520state-of-the-art%2520performance%2520in%2520deterministic%2520and%2520stochastic%2520modes%252C%250Aoutperforming%2520single-output%2520and%2520multi-output%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18839v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery&entry.906535625=Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Francesc%20Moreno-Noguer&entry.1292438233=%20%20Human%20Mesh%20Recovery%20%28HMR%29%20from%20a%20single%20RGB%20image%20is%20a%20highly%20ambiguous%0Aproblem%2C%20as%20an%20infinite%20set%20of%203D%20interpretations%20can%20explain%20the%202D%0Aobservation%20equally%20well.%20Nevertheless%2C%20most%20HMR%20methods%20overlook%20this%20issue%0Aand%20make%20a%20single%20prediction%20without%20accounting%20for%20this%20ambiguity.%20A%20few%0Aapproaches%20generate%20a%20distribution%20of%20human%20meshes%2C%20enabling%20the%20sampling%20of%0Amultiple%20predictions%3B%20however%2C%20none%20of%20them%20is%20competitive%20with%20the%20latest%0Asingle-output%20model%20when%20making%20a%20single%20prediction.%20This%20work%20proposes%20a%20new%0Aapproach%20based%20on%20masked%20generative%20modeling.%20By%20tokenizing%20the%20human%20pose%20and%0Ashape%2C%20we%20formulate%20the%20HMR%20task%20as%20generating%20a%20sequence%20of%20discrete%20tokens%0Aconditioned%20on%20an%20input%20image.%20We%20introduce%20MEGA%2C%20a%20MaskEd%20Generative%0AAutoencoder%20trained%20to%20recover%20human%20meshes%20from%20images%20and%20partial%20human%20mesh%0Atoken%20sequences.%20Given%20an%20image%2C%20our%20flexible%20generation%20scheme%20allows%20us%20to%0Apredict%20a%20single%20human%20mesh%20in%20deterministic%20mode%20or%20to%20generate%20multiple%20human%0Ameshes%20in%20stochastic%20mode.%20Experiments%20on%20in-the-wild%20benchmarks%20show%20that%20MEGA%0Aachieves%20state-of-the-art%20performance%20in%20deterministic%20and%20stochastic%20modes%2C%0Aoutperforming%20single-output%20and%20multi-output%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18839v3&entry.124074799=Read"},
{"title": "Dynamic Reconstruction of Hand-Object Interaction with Distributed\n  Force-aware Contact Representation", "author": "Zhenjun Yu and Wenqiang Xu and Pengfei Xie and Yutong Li and Cewu Lu", "abstract": "  We present ViTaM-D, a novel visual-tactile framework for dynamic hand-object\ninteraction reconstruction, integrating distributed tactile sensing for more\naccurate contact modeling. While existing methods focus primarily on visual\ninputs, they struggle with capturing detailed contact interactions such as\nobject deformation. Our approach leverages distributed tactile sensors to\naddress this limitation by introducing DF-Field. This distributed force-aware\ncontact representation models both kinetic and potential energy in hand-object\ninteraction. ViTaM-D first reconstructs hand-object interactions using a\nvisual-only network, VDT-Net, and then refines contact details through a\nforce-aware optimization (FO) process, enhancing object deformation modeling.\nTo benchmark our approach, we introduce the HOT dataset, which features 600\nsequences of hand-object interactions, including deformable objects, built in a\nhigh-precision simulation environment. Extensive experiments on both the DexYCB\nand HOT datasets demonstrate significant improvements in accuracy over previous\nstate-of-the-art methods such as gSDF and HOTrack. Our results highlight the\nsuperior performance of ViTaM-D in both rigid and deformable object\nreconstruction, as well as the effectiveness of DF-Field in refining hand\nposes. This work offers a comprehensive solution to dynamic hand-object\ninteraction reconstruction by seamlessly integrating visual and tactile data.\nCodes, models, and datasets will be available.\n", "link": "http://arxiv.org/abs/2411.09572v1", "date": "2024-11-14", "relevancy": 2.8891, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5977}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5765}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Reconstruction%20of%20Hand-Object%20Interaction%20with%20Distributed%0A%20%20Force-aware%20Contact%20Representation&body=Title%3A%20Dynamic%20Reconstruction%20of%20Hand-Object%20Interaction%20with%20Distributed%0A%20%20Force-aware%20Contact%20Representation%0AAuthor%3A%20Zhenjun%20Yu%20and%20Wenqiang%20Xu%20and%20Pengfei%20Xie%20and%20Yutong%20Li%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20We%20present%20ViTaM-D%2C%20a%20novel%20visual-tactile%20framework%20for%20dynamic%20hand-object%0Ainteraction%20reconstruction%2C%20integrating%20distributed%20tactile%20sensing%20for%20more%0Aaccurate%20contact%20modeling.%20While%20existing%20methods%20focus%20primarily%20on%20visual%0Ainputs%2C%20they%20struggle%20with%20capturing%20detailed%20contact%20interactions%20such%20as%0Aobject%20deformation.%20Our%20approach%20leverages%20distributed%20tactile%20sensors%20to%0Aaddress%20this%20limitation%20by%20introducing%20DF-Field.%20This%20distributed%20force-aware%0Acontact%20representation%20models%20both%20kinetic%20and%20potential%20energy%20in%20hand-object%0Ainteraction.%20ViTaM-D%20first%20reconstructs%20hand-object%20interactions%20using%20a%0Avisual-only%20network%2C%20VDT-Net%2C%20and%20then%20refines%20contact%20details%20through%20a%0Aforce-aware%20optimization%20%28FO%29%20process%2C%20enhancing%20object%20deformation%20modeling.%0ATo%20benchmark%20our%20approach%2C%20we%20introduce%20the%20HOT%20dataset%2C%20which%20features%20600%0Asequences%20of%20hand-object%20interactions%2C%20including%20deformable%20objects%2C%20built%20in%20a%0Ahigh-precision%20simulation%20environment.%20Extensive%20experiments%20on%20both%20the%20DexYCB%0Aand%20HOT%20datasets%20demonstrate%20significant%20improvements%20in%20accuracy%20over%20previous%0Astate-of-the-art%20methods%20such%20as%20gSDF%20and%20HOTrack.%20Our%20results%20highlight%20the%0Asuperior%20performance%20of%20ViTaM-D%20in%20both%20rigid%20and%20deformable%20object%0Areconstruction%2C%20as%20well%20as%20the%20effectiveness%20of%20DF-Field%20in%20refining%20hand%0Aposes.%20This%20work%20offers%20a%20comprehensive%20solution%20to%20dynamic%20hand-object%0Ainteraction%20reconstruction%20by%20seamlessly%20integrating%20visual%20and%20tactile%20data.%0ACodes%2C%20models%2C%20and%20datasets%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Reconstruction%2520of%2520Hand-Object%2520Interaction%2520with%2520Distributed%250A%2520%2520Force-aware%2520Contact%2520Representation%26entry.906535625%3DZhenjun%2520Yu%2520and%2520Wenqiang%2520Xu%2520and%2520Pengfei%2520Xie%2520and%2520Yutong%2520Li%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520We%2520present%2520ViTaM-D%252C%2520a%2520novel%2520visual-tactile%2520framework%2520for%2520dynamic%2520hand-object%250Ainteraction%2520reconstruction%252C%2520integrating%2520distributed%2520tactile%2520sensing%2520for%2520more%250Aaccurate%2520contact%2520modeling.%2520While%2520existing%2520methods%2520focus%2520primarily%2520on%2520visual%250Ainputs%252C%2520they%2520struggle%2520with%2520capturing%2520detailed%2520contact%2520interactions%2520such%2520as%250Aobject%2520deformation.%2520Our%2520approach%2520leverages%2520distributed%2520tactile%2520sensors%2520to%250Aaddress%2520this%2520limitation%2520by%2520introducing%2520DF-Field.%2520This%2520distributed%2520force-aware%250Acontact%2520representation%2520models%2520both%2520kinetic%2520and%2520potential%2520energy%2520in%2520hand-object%250Ainteraction.%2520ViTaM-D%2520first%2520reconstructs%2520hand-object%2520interactions%2520using%2520a%250Avisual-only%2520network%252C%2520VDT-Net%252C%2520and%2520then%2520refines%2520contact%2520details%2520through%2520a%250Aforce-aware%2520optimization%2520%2528FO%2529%2520process%252C%2520enhancing%2520object%2520deformation%2520modeling.%250ATo%2520benchmark%2520our%2520approach%252C%2520we%2520introduce%2520the%2520HOT%2520dataset%252C%2520which%2520features%2520600%250Asequences%2520of%2520hand-object%2520interactions%252C%2520including%2520deformable%2520objects%252C%2520built%2520in%2520a%250Ahigh-precision%2520simulation%2520environment.%2520Extensive%2520experiments%2520on%2520both%2520the%2520DexYCB%250Aand%2520HOT%2520datasets%2520demonstrate%2520significant%2520improvements%2520in%2520accuracy%2520over%2520previous%250Astate-of-the-art%2520methods%2520such%2520as%2520gSDF%2520and%2520HOTrack.%2520Our%2520results%2520highlight%2520the%250Asuperior%2520performance%2520of%2520ViTaM-D%2520in%2520both%2520rigid%2520and%2520deformable%2520object%250Areconstruction%252C%2520as%2520well%2520as%2520the%2520effectiveness%2520of%2520DF-Field%2520in%2520refining%2520hand%250Aposes.%2520This%2520work%2520offers%2520a%2520comprehensive%2520solution%2520to%2520dynamic%2520hand-object%250Ainteraction%2520reconstruction%2520by%2520seamlessly%2520integrating%2520visual%2520and%2520tactile%2520data.%250ACodes%252C%2520models%252C%2520and%2520datasets%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Reconstruction%20of%20Hand-Object%20Interaction%20with%20Distributed%0A%20%20Force-aware%20Contact%20Representation&entry.906535625=Zhenjun%20Yu%20and%20Wenqiang%20Xu%20and%20Pengfei%20Xie%20and%20Yutong%20Li%20and%20Cewu%20Lu&entry.1292438233=%20%20We%20present%20ViTaM-D%2C%20a%20novel%20visual-tactile%20framework%20for%20dynamic%20hand-object%0Ainteraction%20reconstruction%2C%20integrating%20distributed%20tactile%20sensing%20for%20more%0Aaccurate%20contact%20modeling.%20While%20existing%20methods%20focus%20primarily%20on%20visual%0Ainputs%2C%20they%20struggle%20with%20capturing%20detailed%20contact%20interactions%20such%20as%0Aobject%20deformation.%20Our%20approach%20leverages%20distributed%20tactile%20sensors%20to%0Aaddress%20this%20limitation%20by%20introducing%20DF-Field.%20This%20distributed%20force-aware%0Acontact%20representation%20models%20both%20kinetic%20and%20potential%20energy%20in%20hand-object%0Ainteraction.%20ViTaM-D%20first%20reconstructs%20hand-object%20interactions%20using%20a%0Avisual-only%20network%2C%20VDT-Net%2C%20and%20then%20refines%20contact%20details%20through%20a%0Aforce-aware%20optimization%20%28FO%29%20process%2C%20enhancing%20object%20deformation%20modeling.%0ATo%20benchmark%20our%20approach%2C%20we%20introduce%20the%20HOT%20dataset%2C%20which%20features%20600%0Asequences%20of%20hand-object%20interactions%2C%20including%20deformable%20objects%2C%20built%20in%20a%0Ahigh-precision%20simulation%20environment.%20Extensive%20experiments%20on%20both%20the%20DexYCB%0Aand%20HOT%20datasets%20demonstrate%20significant%20improvements%20in%20accuracy%20over%20previous%0Astate-of-the-art%20methods%20such%20as%20gSDF%20and%20HOTrack.%20Our%20results%20highlight%20the%0Asuperior%20performance%20of%20ViTaM-D%20in%20both%20rigid%20and%20deformable%20object%0Areconstruction%2C%20as%20well%20as%20the%20effectiveness%20of%20DF-Field%20in%20refining%20hand%0Aposes.%20This%20work%20offers%20a%20comprehensive%20solution%20to%20dynamic%20hand-object%0Ainteraction%20reconstruction%20by%20seamlessly%20integrating%20visual%20and%20tactile%20data.%0ACodes%2C%20models%2C%20and%20datasets%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09572v1&entry.124074799=Read"},
{"title": "Image Matching Filtering and Refinement by Planes and Beyond", "author": "Fabio Bellavia and Zhenjun Zhao and Luca Morelli and Fabio Remondino", "abstract": "  This paper introduces a modular, non-deep learning method for filtering and\nrefining sparse correspondences in image matching. Assuming that motion flow\nwithin the scene can be approximated by local homography transformations,\nmatches are aggregated into overlapping clusters corresponding to virtual\nplanes using an iterative RANSAC-based approach, with non-conforming\ncorrespondences discarded. Moreover, the underlying planar structural design\nprovides an explicit map between local patches associated with the matches,\nenabling optional refinement of keypoint positions through cross-correlation\ntemplate matching after patch reprojection. Finally, to enhance robustness and\nfault-tolerance against violations of the piece-wise planar approximation\nassumption, a further strategy is designed for minimizing relative patch\ndistortion in the plane reprojection by introducing an intermediate homography\nthat projects both patches into a common plane. The proposed method is\nextensively evaluated on standard datasets and image matching pipelines, and\ncompared with state-of-the-art approaches. Unlike other current comparisons,\nthe proposed benchmark also takes into account the more general, real, and\npractical cases where camera intrinsics are unavailable. Experimental results\ndemonstrate that our proposed non-deep learning, geometry-based approach\nachieves performances that are either superior to or on par with recent\nstate-of-the-art deep learning methods. Finally, this study suggests that there\nare still development potential in actual image matching solutions in the\nconsidered research direction, which could be in the future incorporated in\nnovel deep image matching architectures.\n", "link": "http://arxiv.org/abs/2411.09484v1", "date": "2024-11-14", "relevancy": 2.8861, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6133}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5711}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond&body=Title%3A%20Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond%0AAuthor%3A%20Fabio%20Bellavia%20and%20Zhenjun%20Zhao%20and%20Luca%20Morelli%20and%20Fabio%20Remondino%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20modular%2C%20non-deep%20learning%20method%20for%20filtering%20and%0Arefining%20sparse%20correspondences%20in%20image%20matching.%20Assuming%20that%20motion%20flow%0Awithin%20the%20scene%20can%20be%20approximated%20by%20local%20homography%20transformations%2C%0Amatches%20are%20aggregated%20into%20overlapping%20clusters%20corresponding%20to%20virtual%0Aplanes%20using%20an%20iterative%20RANSAC-based%20approach%2C%20with%20non-conforming%0Acorrespondences%20discarded.%20Moreover%2C%20the%20underlying%20planar%20structural%20design%0Aprovides%20an%20explicit%20map%20between%20local%20patches%20associated%20with%20the%20matches%2C%0Aenabling%20optional%20refinement%20of%20keypoint%20positions%20through%20cross-correlation%0Atemplate%20matching%20after%20patch%20reprojection.%20Finally%2C%20to%20enhance%20robustness%20and%0Afault-tolerance%20against%20violations%20of%20the%20piece-wise%20planar%20approximation%0Aassumption%2C%20a%20further%20strategy%20is%20designed%20for%20minimizing%20relative%20patch%0Adistortion%20in%20the%20plane%20reprojection%20by%20introducing%20an%20intermediate%20homography%0Athat%20projects%20both%20patches%20into%20a%20common%20plane.%20The%20proposed%20method%20is%0Aextensively%20evaluated%20on%20standard%20datasets%20and%20image%20matching%20pipelines%2C%20and%0Acompared%20with%20state-of-the-art%20approaches.%20Unlike%20other%20current%20comparisons%2C%0Athe%20proposed%20benchmark%20also%20takes%20into%20account%20the%20more%20general%2C%20real%2C%20and%0Apractical%20cases%20where%20camera%20intrinsics%20are%20unavailable.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20non-deep%20learning%2C%20geometry-based%20approach%0Aachieves%20performances%20that%20are%20either%20superior%20to%20or%20on%20par%20with%20recent%0Astate-of-the-art%20deep%20learning%20methods.%20Finally%2C%20this%20study%20suggests%20that%20there%0Aare%20still%20development%20potential%20in%20actual%20image%20matching%20solutions%20in%20the%0Aconsidered%20research%20direction%2C%20which%20could%20be%20in%20the%20future%20incorporated%20in%0Anovel%20deep%20image%20matching%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Matching%2520Filtering%2520and%2520Refinement%2520by%2520Planes%2520and%2520Beyond%26entry.906535625%3DFabio%2520Bellavia%2520and%2520Zhenjun%2520Zhao%2520and%2520Luca%2520Morelli%2520and%2520Fabio%2520Remondino%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520modular%252C%2520non-deep%2520learning%2520method%2520for%2520filtering%2520and%250Arefining%2520sparse%2520correspondences%2520in%2520image%2520matching.%2520Assuming%2520that%2520motion%2520flow%250Awithin%2520the%2520scene%2520can%2520be%2520approximated%2520by%2520local%2520homography%2520transformations%252C%250Amatches%2520are%2520aggregated%2520into%2520overlapping%2520clusters%2520corresponding%2520to%2520virtual%250Aplanes%2520using%2520an%2520iterative%2520RANSAC-based%2520approach%252C%2520with%2520non-conforming%250Acorrespondences%2520discarded.%2520Moreover%252C%2520the%2520underlying%2520planar%2520structural%2520design%250Aprovides%2520an%2520explicit%2520map%2520between%2520local%2520patches%2520associated%2520with%2520the%2520matches%252C%250Aenabling%2520optional%2520refinement%2520of%2520keypoint%2520positions%2520through%2520cross-correlation%250Atemplate%2520matching%2520after%2520patch%2520reprojection.%2520Finally%252C%2520to%2520enhance%2520robustness%2520and%250Afault-tolerance%2520against%2520violations%2520of%2520the%2520piece-wise%2520planar%2520approximation%250Aassumption%252C%2520a%2520further%2520strategy%2520is%2520designed%2520for%2520minimizing%2520relative%2520patch%250Adistortion%2520in%2520the%2520plane%2520reprojection%2520by%2520introducing%2520an%2520intermediate%2520homography%250Athat%2520projects%2520both%2520patches%2520into%2520a%2520common%2520plane.%2520The%2520proposed%2520method%2520is%250Aextensively%2520evaluated%2520on%2520standard%2520datasets%2520and%2520image%2520matching%2520pipelines%252C%2520and%250Acompared%2520with%2520state-of-the-art%2520approaches.%2520Unlike%2520other%2520current%2520comparisons%252C%250Athe%2520proposed%2520benchmark%2520also%2520takes%2520into%2520account%2520the%2520more%2520general%252C%2520real%252C%2520and%250Apractical%2520cases%2520where%2520camera%2520intrinsics%2520are%2520unavailable.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520non-deep%2520learning%252C%2520geometry-based%2520approach%250Aachieves%2520performances%2520that%2520are%2520either%2520superior%2520to%2520or%2520on%2520par%2520with%2520recent%250Astate-of-the-art%2520deep%2520learning%2520methods.%2520Finally%252C%2520this%2520study%2520suggests%2520that%2520there%250Aare%2520still%2520development%2520potential%2520in%2520actual%2520image%2520matching%2520solutions%2520in%2520the%250Aconsidered%2520research%2520direction%252C%2520which%2520could%2520be%2520in%2520the%2520future%2520incorporated%2520in%250Anovel%2520deep%2520image%2520matching%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Matching%20Filtering%20and%20Refinement%20by%20Planes%20and%20Beyond&entry.906535625=Fabio%20Bellavia%20and%20Zhenjun%20Zhao%20and%20Luca%20Morelli%20and%20Fabio%20Remondino&entry.1292438233=%20%20This%20paper%20introduces%20a%20modular%2C%20non-deep%20learning%20method%20for%20filtering%20and%0Arefining%20sparse%20correspondences%20in%20image%20matching.%20Assuming%20that%20motion%20flow%0Awithin%20the%20scene%20can%20be%20approximated%20by%20local%20homography%20transformations%2C%0Amatches%20are%20aggregated%20into%20overlapping%20clusters%20corresponding%20to%20virtual%0Aplanes%20using%20an%20iterative%20RANSAC-based%20approach%2C%20with%20non-conforming%0Acorrespondences%20discarded.%20Moreover%2C%20the%20underlying%20planar%20structural%20design%0Aprovides%20an%20explicit%20map%20between%20local%20patches%20associated%20with%20the%20matches%2C%0Aenabling%20optional%20refinement%20of%20keypoint%20positions%20through%20cross-correlation%0Atemplate%20matching%20after%20patch%20reprojection.%20Finally%2C%20to%20enhance%20robustness%20and%0Afault-tolerance%20against%20violations%20of%20the%20piece-wise%20planar%20approximation%0Aassumption%2C%20a%20further%20strategy%20is%20designed%20for%20minimizing%20relative%20patch%0Adistortion%20in%20the%20plane%20reprojection%20by%20introducing%20an%20intermediate%20homography%0Athat%20projects%20both%20patches%20into%20a%20common%20plane.%20The%20proposed%20method%20is%0Aextensively%20evaluated%20on%20standard%20datasets%20and%20image%20matching%20pipelines%2C%20and%0Acompared%20with%20state-of-the-art%20approaches.%20Unlike%20other%20current%20comparisons%2C%0Athe%20proposed%20benchmark%20also%20takes%20into%20account%20the%20more%20general%2C%20real%2C%20and%0Apractical%20cases%20where%20camera%20intrinsics%20are%20unavailable.%20Experimental%20results%0Ademonstrate%20that%20our%20proposed%20non-deep%20learning%2C%20geometry-based%20approach%0Aachieves%20performances%20that%20are%20either%20superior%20to%20or%20on%20par%20with%20recent%0Astate-of-the-art%20deep%20learning%20methods.%20Finally%2C%20this%20study%20suggests%20that%20there%0Aare%20still%20development%20potential%20in%20actual%20image%20matching%20solutions%20in%20the%0Aconsidered%20research%20direction%2C%20which%20could%20be%20in%20the%20future%20incorporated%20in%0Anovel%20deep%20image%20matching%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09484v1&entry.124074799=Read"},
{"title": "Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature\n  Integration", "author": "Yifan Shao", "abstract": "  In recent years, attention mechanisms have significantly enhanced the\nperformance of object detection by focusing on key feature information.\nHowever, prevalent methods still encounter difficulties in effectively\nbalancing local and global features. This imbalance hampers their ability to\ncapture both fine-grained details and broader contextual information-two\ncritical elements for achieving accurate object detection.To address these\nchallenges, we propose a novel attention mechanism, termed Local-Global\nAttention, which is designed to better integrate both local and global\ncontextual features. Specifically, our approach combines multi-scale\nconvolutions with positional encoding, enabling the model to focus on local\ndetails while concurrently considering the broader global context.\nAdditionally, we introduce a learnable parameters, which allow the model to\ndynamically adjust the relative importance of local and global attention,\ndepending on the specific requirements of the task, thereby optimizing feature\nrepresentations across multiple scales.We have thoroughly evaluated the\nLocal-Global Attention mechanism on several widely used object detection and\nclassification datasets. Our experimental results demonstrate that this\napproach significantly enhances the detection of objects at various scales,\nwith particularly strong performance on multi-class and small object detection\ntasks. In comparison to existing attention mechanisms, Local-Global Attention\nconsistently outperforms them across several key metrics, all while maintaining\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2411.09604v1", "date": "2024-11-14", "relevancy": 2.866, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5901}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-Global%20Attention%3A%20An%20Adaptive%20Mechanism%20for%20Multi-Scale%20Feature%0A%20%20Integration&body=Title%3A%20Local-Global%20Attention%3A%20An%20Adaptive%20Mechanism%20for%20Multi-Scale%20Feature%0A%20%20Integration%0AAuthor%3A%20Yifan%20Shao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20attention%20mechanisms%20have%20significantly%20enhanced%20the%0Aperformance%20of%20object%20detection%20by%20focusing%20on%20key%20feature%20information.%0AHowever%2C%20prevalent%20methods%20still%20encounter%20difficulties%20in%20effectively%0Abalancing%20local%20and%20global%20features.%20This%20imbalance%20hampers%20their%20ability%20to%0Acapture%20both%20fine-grained%20details%20and%20broader%20contextual%20information-two%0Acritical%20elements%20for%20achieving%20accurate%20object%20detection.To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20attention%20mechanism%2C%20termed%20Local-Global%0AAttention%2C%20which%20is%20designed%20to%20better%20integrate%20both%20local%20and%20global%0Acontextual%20features.%20Specifically%2C%20our%20approach%20combines%20multi-scale%0Aconvolutions%20with%20positional%20encoding%2C%20enabling%20the%20model%20to%20focus%20on%20local%0Adetails%20while%20concurrently%20considering%20the%20broader%20global%20context.%0AAdditionally%2C%20we%20introduce%20a%20learnable%20parameters%2C%20which%20allow%20the%20model%20to%0Adynamically%20adjust%20the%20relative%20importance%20of%20local%20and%20global%20attention%2C%0Adepending%20on%20the%20specific%20requirements%20of%20the%20task%2C%20thereby%20optimizing%20feature%0Arepresentations%20across%20multiple%20scales.We%20have%20thoroughly%20evaluated%20the%0ALocal-Global%20Attention%20mechanism%20on%20several%20widely%20used%20object%20detection%20and%0Aclassification%20datasets.%20Our%20experimental%20results%20demonstrate%20that%20this%0Aapproach%20significantly%20enhances%20the%20detection%20of%20objects%20at%20various%20scales%2C%0Awith%20particularly%20strong%20performance%20on%20multi-class%20and%20small%20object%20detection%0Atasks.%20In%20comparison%20to%20existing%20attention%20mechanisms%2C%20Local-Global%20Attention%0Aconsistently%20outperforms%20them%20across%20several%20key%20metrics%2C%20all%20while%20maintaining%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-Global%2520Attention%253A%2520An%2520Adaptive%2520Mechanism%2520for%2520Multi-Scale%2520Feature%250A%2520%2520Integration%26entry.906535625%3DYifan%2520Shao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520attention%2520mechanisms%2520have%2520significantly%2520enhanced%2520the%250Aperformance%2520of%2520object%2520detection%2520by%2520focusing%2520on%2520key%2520feature%2520information.%250AHowever%252C%2520prevalent%2520methods%2520still%2520encounter%2520difficulties%2520in%2520effectively%250Abalancing%2520local%2520and%2520global%2520features.%2520This%2520imbalance%2520hampers%2520their%2520ability%2520to%250Acapture%2520both%2520fine-grained%2520details%2520and%2520broader%2520contextual%2520information-two%250Acritical%2520elements%2520for%2520achieving%2520accurate%2520object%2520detection.To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520attention%2520mechanism%252C%2520termed%2520Local-Global%250AAttention%252C%2520which%2520is%2520designed%2520to%2520better%2520integrate%2520both%2520local%2520and%2520global%250Acontextual%2520features.%2520Specifically%252C%2520our%2520approach%2520combines%2520multi-scale%250Aconvolutions%2520with%2520positional%2520encoding%252C%2520enabling%2520the%2520model%2520to%2520focus%2520on%2520local%250Adetails%2520while%2520concurrently%2520considering%2520the%2520broader%2520global%2520context.%250AAdditionally%252C%2520we%2520introduce%2520a%2520learnable%2520parameters%252C%2520which%2520allow%2520the%2520model%2520to%250Adynamically%2520adjust%2520the%2520relative%2520importance%2520of%2520local%2520and%2520global%2520attention%252C%250Adepending%2520on%2520the%2520specific%2520requirements%2520of%2520the%2520task%252C%2520thereby%2520optimizing%2520feature%250Arepresentations%2520across%2520multiple%2520scales.We%2520have%2520thoroughly%2520evaluated%2520the%250ALocal-Global%2520Attention%2520mechanism%2520on%2520several%2520widely%2520used%2520object%2520detection%2520and%250Aclassification%2520datasets.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520this%250Aapproach%2520significantly%2520enhances%2520the%2520detection%2520of%2520objects%2520at%2520various%2520scales%252C%250Awith%2520particularly%2520strong%2520performance%2520on%2520multi-class%2520and%2520small%2520object%2520detection%250Atasks.%2520In%2520comparison%2520to%2520existing%2520attention%2520mechanisms%252C%2520Local-Global%2520Attention%250Aconsistently%2520outperforms%2520them%2520across%2520several%2520key%2520metrics%252C%2520all%2520while%2520maintaining%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-Global%20Attention%3A%20An%20Adaptive%20Mechanism%20for%20Multi-Scale%20Feature%0A%20%20Integration&entry.906535625=Yifan%20Shao&entry.1292438233=%20%20In%20recent%20years%2C%20attention%20mechanisms%20have%20significantly%20enhanced%20the%0Aperformance%20of%20object%20detection%20by%20focusing%20on%20key%20feature%20information.%0AHowever%2C%20prevalent%20methods%20still%20encounter%20difficulties%20in%20effectively%0Abalancing%20local%20and%20global%20features.%20This%20imbalance%20hampers%20their%20ability%20to%0Acapture%20both%20fine-grained%20details%20and%20broader%20contextual%20information-two%0Acritical%20elements%20for%20achieving%20accurate%20object%20detection.To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20attention%20mechanism%2C%20termed%20Local-Global%0AAttention%2C%20which%20is%20designed%20to%20better%20integrate%20both%20local%20and%20global%0Acontextual%20features.%20Specifically%2C%20our%20approach%20combines%20multi-scale%0Aconvolutions%20with%20positional%20encoding%2C%20enabling%20the%20model%20to%20focus%20on%20local%0Adetails%20while%20concurrently%20considering%20the%20broader%20global%20context.%0AAdditionally%2C%20we%20introduce%20a%20learnable%20parameters%2C%20which%20allow%20the%20model%20to%0Adynamically%20adjust%20the%20relative%20importance%20of%20local%20and%20global%20attention%2C%0Adepending%20on%20the%20specific%20requirements%20of%20the%20task%2C%20thereby%20optimizing%20feature%0Arepresentations%20across%20multiple%20scales.We%20have%20thoroughly%20evaluated%20the%0ALocal-Global%20Attention%20mechanism%20on%20several%20widely%20used%20object%20detection%20and%0Aclassification%20datasets.%20Our%20experimental%20results%20demonstrate%20that%20this%0Aapproach%20significantly%20enhances%20the%20detection%20of%20objects%20at%20various%20scales%2C%0Awith%20particularly%20strong%20performance%20on%20multi-class%20and%20small%20object%20detection%0Atasks.%20In%20comparison%20to%20existing%20attention%20mechanisms%2C%20Local-Global%20Attention%0Aconsistently%20outperforms%20them%20across%20several%20key%20metrics%2C%20all%20while%20maintaining%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09604v1&entry.124074799=Read"},
{"title": "Advancements in Visual Language Models for Remote Sensing: Datasets,\n  Capabilities, and Enhancement Techniques", "author": "Lijie Tao and Haokui Zhang and Haizhao Jing and Yu Liu and Kelu Yao and Chao Li and Xizhe Xue", "abstract": "  Recently, the remarkable success of ChatGPT has sparked a renewed wave of\ninterest in artificial intelligence (AI), and the advancements in visual\nlanguage models (VLMs) have pushed this enthusiasm to new heights. Differring\nfrom previous AI approaches that generally formulated different tasks as\ndiscriminative models, VLMs frame tasks as generative models and align language\nwith visual information, enabling the handling of more challenging problems.\nThe remote sensing (RS) field, a highly practical domain, has also embraced\nthis new trend and introduced several VLM-based RS methods that have\ndemonstrated promising performance and enormous potential. In this paper, we\nfirst review the fundamental theories related to VLM, then summarize the\ndatasets constructed for VLMs in remote sensing and the various tasks they\naddressed. Finally, we categorize the improvement methods into three main parts\naccording to the core components of VLMs and provide a detailed introduction\nand comparison of these methods. A project associated with this review has been\ncreated at https://github.com/taolijie11111/VLMs-in-RS-review.\n", "link": "http://arxiv.org/abs/2410.17283v2", "date": "2024-11-14", "relevancy": 2.8293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Visual%20Language%20Models%20for%20Remote%20Sensing%3A%20Datasets%2C%0A%20%20Capabilities%2C%20and%20Enhancement%20Techniques&body=Title%3A%20Advancements%20in%20Visual%20Language%20Models%20for%20Remote%20Sensing%3A%20Datasets%2C%0A%20%20Capabilities%2C%20and%20Enhancement%20Techniques%0AAuthor%3A%20Lijie%20Tao%20and%20Haokui%20Zhang%20and%20Haizhao%20Jing%20and%20Yu%20Liu%20and%20Kelu%20Yao%20and%20Chao%20Li%20and%20Xizhe%20Xue%0AAbstract%3A%20%20%20Recently%2C%20the%20remarkable%20success%20of%20ChatGPT%20has%20sparked%20a%20renewed%20wave%20of%0Ainterest%20in%20artificial%20intelligence%20%28AI%29%2C%20and%20the%20advancements%20in%20visual%0Alanguage%20models%20%28VLMs%29%20have%20pushed%20this%20enthusiasm%20to%20new%20heights.%20Differring%0Afrom%20previous%20AI%20approaches%20that%20generally%20formulated%20different%20tasks%20as%0Adiscriminative%20models%2C%20VLMs%20frame%20tasks%20as%20generative%20models%20and%20align%20language%0Awith%20visual%20information%2C%20enabling%20the%20handling%20of%20more%20challenging%20problems.%0AThe%20remote%20sensing%20%28RS%29%20field%2C%20a%20highly%20practical%20domain%2C%20has%20also%20embraced%0Athis%20new%20trend%20and%20introduced%20several%20VLM-based%20RS%20methods%20that%20have%0Ademonstrated%20promising%20performance%20and%20enormous%20potential.%20In%20this%20paper%2C%20we%0Afirst%20review%20the%20fundamental%20theories%20related%20to%20VLM%2C%20then%20summarize%20the%0Adatasets%20constructed%20for%20VLMs%20in%20remote%20sensing%20and%20the%20various%20tasks%20they%0Aaddressed.%20Finally%2C%20we%20categorize%20the%20improvement%20methods%20into%20three%20main%20parts%0Aaccording%20to%20the%20core%20components%20of%20VLMs%20and%20provide%20a%20detailed%20introduction%0Aand%20comparison%20of%20these%20methods.%20A%20project%20associated%20with%20this%20review%20has%20been%0Acreated%20at%20https%3A//github.com/taolijie11111/VLMs-in-RS-review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520in%2520Visual%2520Language%2520Models%2520for%2520Remote%2520Sensing%253A%2520Datasets%252C%250A%2520%2520Capabilities%252C%2520and%2520Enhancement%2520Techniques%26entry.906535625%3DLijie%2520Tao%2520and%2520Haokui%2520Zhang%2520and%2520Haizhao%2520Jing%2520and%2520Yu%2520Liu%2520and%2520Kelu%2520Yao%2520and%2520Chao%2520Li%2520and%2520Xizhe%2520Xue%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520remarkable%2520success%2520of%2520ChatGPT%2520has%2520sparked%2520a%2520renewed%2520wave%2520of%250Ainterest%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520and%2520the%2520advancements%2520in%2520visual%250Alanguage%2520models%2520%2528VLMs%2529%2520have%2520pushed%2520this%2520enthusiasm%2520to%2520new%2520heights.%2520Differring%250Afrom%2520previous%2520AI%2520approaches%2520that%2520generally%2520formulated%2520different%2520tasks%2520as%250Adiscriminative%2520models%252C%2520VLMs%2520frame%2520tasks%2520as%2520generative%2520models%2520and%2520align%2520language%250Awith%2520visual%2520information%252C%2520enabling%2520the%2520handling%2520of%2520more%2520challenging%2520problems.%250AThe%2520remote%2520sensing%2520%2528RS%2529%2520field%252C%2520a%2520highly%2520practical%2520domain%252C%2520has%2520also%2520embraced%250Athis%2520new%2520trend%2520and%2520introduced%2520several%2520VLM-based%2520RS%2520methods%2520that%2520have%250Ademonstrated%2520promising%2520performance%2520and%2520enormous%2520potential.%2520In%2520this%2520paper%252C%2520we%250Afirst%2520review%2520the%2520fundamental%2520theories%2520related%2520to%2520VLM%252C%2520then%2520summarize%2520the%250Adatasets%2520constructed%2520for%2520VLMs%2520in%2520remote%2520sensing%2520and%2520the%2520various%2520tasks%2520they%250Aaddressed.%2520Finally%252C%2520we%2520categorize%2520the%2520improvement%2520methods%2520into%2520three%2520main%2520parts%250Aaccording%2520to%2520the%2520core%2520components%2520of%2520VLMs%2520and%2520provide%2520a%2520detailed%2520introduction%250Aand%2520comparison%2520of%2520these%2520methods.%2520A%2520project%2520associated%2520with%2520this%2520review%2520has%2520been%250Acreated%2520at%2520https%253A//github.com/taolijie11111/VLMs-in-RS-review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Visual%20Language%20Models%20for%20Remote%20Sensing%3A%20Datasets%2C%0A%20%20Capabilities%2C%20and%20Enhancement%20Techniques&entry.906535625=Lijie%20Tao%20and%20Haokui%20Zhang%20and%20Haizhao%20Jing%20and%20Yu%20Liu%20and%20Kelu%20Yao%20and%20Chao%20Li%20and%20Xizhe%20Xue&entry.1292438233=%20%20Recently%2C%20the%20remarkable%20success%20of%20ChatGPT%20has%20sparked%20a%20renewed%20wave%20of%0Ainterest%20in%20artificial%20intelligence%20%28AI%29%2C%20and%20the%20advancements%20in%20visual%0Alanguage%20models%20%28VLMs%29%20have%20pushed%20this%20enthusiasm%20to%20new%20heights.%20Differring%0Afrom%20previous%20AI%20approaches%20that%20generally%20formulated%20different%20tasks%20as%0Adiscriminative%20models%2C%20VLMs%20frame%20tasks%20as%20generative%20models%20and%20align%20language%0Awith%20visual%20information%2C%20enabling%20the%20handling%20of%20more%20challenging%20problems.%0AThe%20remote%20sensing%20%28RS%29%20field%2C%20a%20highly%20practical%20domain%2C%20has%20also%20embraced%0Athis%20new%20trend%20and%20introduced%20several%20VLM-based%20RS%20methods%20that%20have%0Ademonstrated%20promising%20performance%20and%20enormous%20potential.%20In%20this%20paper%2C%20we%0Afirst%20review%20the%20fundamental%20theories%20related%20to%20VLM%2C%20then%20summarize%20the%0Adatasets%20constructed%20for%20VLMs%20in%20remote%20sensing%20and%20the%20various%20tasks%20they%0Aaddressed.%20Finally%2C%20we%20categorize%20the%20improvement%20methods%20into%20three%20main%20parts%0Aaccording%20to%20the%20core%20components%20of%20VLMs%20and%20provide%20a%20detailed%20introduction%0Aand%20comparison%20of%20these%20methods.%20A%20project%20associated%20with%20this%20review%20has%20been%0Acreated%20at%20https%3A//github.com/taolijie11111/VLMs-in-RS-review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17283v2&entry.124074799=Read"},
{"title": "ViTGaze: Gaze Following with Interaction Features in Vision Transformers", "author": "Yuehao Song and Xinggang Wang and Jingfeng Yao and Wenyu Liu and Jinglin Zhang and Xiangmin Xu", "abstract": "  Gaze following aims to interpret human-scene interactions by predicting the\nperson's focal point of gaze. Prevailing approaches often adopt a two-stage\nframework, whereby multi-modality information is extracted in the initial stage\nfor gaze target prediction. Consequently, the efficacy of these methods highly\ndepends on the precision of the preceding modality extraction. Others use a\nsingle-modality approach with complex decoders, increasing network\ncomputational load. Inspired by the remarkable success of pre-trained plain\nvision transformers (ViTs), we introduce a novel single-modality gaze following\nframework called ViTGaze. In contrast to previous methods, it creates a novel\ngaze following framework based mainly on powerful encoders (relative decoder\nparameters less than 1%). Our principal insight is that the inter-token\ninteractions within self-attention can be transferred to interactions between\nhumans and scenes. Leveraging this presumption, we formulate a framework\nconsisting of a 4D interaction encoder and a 2D spatial guidance module to\nextract human-scene interaction information from self-attention maps.\nFurthermore, our investigation reveals that ViT with self-supervised\npre-training has an enhanced ability to extract correlation information. Many\nexperiments have been conducted to demonstrate the performance of the proposed\nmethod. Our method achieves state-of-the-art (SOTA) performance among all\nsingle-modality methods (3.4% improvement in the area under curve (AUC) score,\n5.1% improvement in the average precision (AP)) and very comparable performance\nagainst multi-modality methods with 59% number of parameters less.\n", "link": "http://arxiv.org/abs/2403.12778v2", "date": "2024-11-14", "relevancy": 2.8289, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5681}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers&body=Title%3A%20ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers%0AAuthor%3A%20Yuehao%20Song%20and%20Xinggang%20Wang%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Jinglin%20Zhang%20and%20Xiangmin%20Xu%0AAbstract%3A%20%20%20Gaze%20following%20aims%20to%20interpret%20human-scene%20interactions%20by%20predicting%20the%0Aperson%27s%20focal%20point%20of%20gaze.%20Prevailing%20approaches%20often%20adopt%20a%20two-stage%0Aframework%2C%20whereby%20multi-modality%20information%20is%20extracted%20in%20the%20initial%20stage%0Afor%20gaze%20target%20prediction.%20Consequently%2C%20the%20efficacy%20of%20these%20methods%20highly%0Adepends%20on%20the%20precision%20of%20the%20preceding%20modality%20extraction.%20Others%20use%20a%0Asingle-modality%20approach%20with%20complex%20decoders%2C%20increasing%20network%0Acomputational%20load.%20Inspired%20by%20the%20remarkable%20success%20of%20pre-trained%20plain%0Avision%20transformers%20%28ViTs%29%2C%20we%20introduce%20a%20novel%20single-modality%20gaze%20following%0Aframework%20called%20ViTGaze.%20In%20contrast%20to%20previous%20methods%2C%20it%20creates%20a%20novel%0Agaze%20following%20framework%20based%20mainly%20on%20powerful%20encoders%20%28relative%20decoder%0Aparameters%20less%20than%201%25%29.%20Our%20principal%20insight%20is%20that%20the%20inter-token%0Ainteractions%20within%20self-attention%20can%20be%20transferred%20to%20interactions%20between%0Ahumans%20and%20scenes.%20Leveraging%20this%20presumption%2C%20we%20formulate%20a%20framework%0Aconsisting%20of%20a%204D%20interaction%20encoder%20and%20a%202D%20spatial%20guidance%20module%20to%0Aextract%20human-scene%20interaction%20information%20from%20self-attention%20maps.%0AFurthermore%2C%20our%20investigation%20reveals%20that%20ViT%20with%20self-supervised%0Apre-training%20has%20an%20enhanced%20ability%20to%20extract%20correlation%20information.%20Many%0Aexperiments%20have%20been%20conducted%20to%20demonstrate%20the%20performance%20of%20the%20proposed%0Amethod.%20Our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20all%0Asingle-modality%20methods%20%283.4%25%20improvement%20in%20the%20area%20under%20curve%20%28AUC%29%20score%2C%0A5.1%25%20improvement%20in%20the%20average%20precision%20%28AP%29%29%20and%20very%20comparable%20performance%0Aagainst%20multi-modality%20methods%20with%2059%25%20number%20of%20parameters%20less.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTGaze%253A%2520Gaze%2520Following%2520with%2520Interaction%2520Features%2520in%2520Vision%2520Transformers%26entry.906535625%3DYuehao%2520Song%2520and%2520Xinggang%2520Wang%2520and%2520Jingfeng%2520Yao%2520and%2520Wenyu%2520Liu%2520and%2520Jinglin%2520Zhang%2520and%2520Xiangmin%2520Xu%26entry.1292438233%3D%2520%2520Gaze%2520following%2520aims%2520to%2520interpret%2520human-scene%2520interactions%2520by%2520predicting%2520the%250Aperson%2527s%2520focal%2520point%2520of%2520gaze.%2520Prevailing%2520approaches%2520often%2520adopt%2520a%2520two-stage%250Aframework%252C%2520whereby%2520multi-modality%2520information%2520is%2520extracted%2520in%2520the%2520initial%2520stage%250Afor%2520gaze%2520target%2520prediction.%2520Consequently%252C%2520the%2520efficacy%2520of%2520these%2520methods%2520highly%250Adepends%2520on%2520the%2520precision%2520of%2520the%2520preceding%2520modality%2520extraction.%2520Others%2520use%2520a%250Asingle-modality%2520approach%2520with%2520complex%2520decoders%252C%2520increasing%2520network%250Acomputational%2520load.%2520Inspired%2520by%2520the%2520remarkable%2520success%2520of%2520pre-trained%2520plain%250Avision%2520transformers%2520%2528ViTs%2529%252C%2520we%2520introduce%2520a%2520novel%2520single-modality%2520gaze%2520following%250Aframework%2520called%2520ViTGaze.%2520In%2520contrast%2520to%2520previous%2520methods%252C%2520it%2520creates%2520a%2520novel%250Agaze%2520following%2520framework%2520based%2520mainly%2520on%2520powerful%2520encoders%2520%2528relative%2520decoder%250Aparameters%2520less%2520than%25201%2525%2529.%2520Our%2520principal%2520insight%2520is%2520that%2520the%2520inter-token%250Ainteractions%2520within%2520self-attention%2520can%2520be%2520transferred%2520to%2520interactions%2520between%250Ahumans%2520and%2520scenes.%2520Leveraging%2520this%2520presumption%252C%2520we%2520formulate%2520a%2520framework%250Aconsisting%2520of%2520a%25204D%2520interaction%2520encoder%2520and%2520a%25202D%2520spatial%2520guidance%2520module%2520to%250Aextract%2520human-scene%2520interaction%2520information%2520from%2520self-attention%2520maps.%250AFurthermore%252C%2520our%2520investigation%2520reveals%2520that%2520ViT%2520with%2520self-supervised%250Apre-training%2520has%2520an%2520enhanced%2520ability%2520to%2520extract%2520correlation%2520information.%2520Many%250Aexperiments%2520have%2520been%2520conducted%2520to%2520demonstrate%2520the%2520performance%2520of%2520the%2520proposed%250Amethod.%2520Our%2520method%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520among%2520all%250Asingle-modality%2520methods%2520%25283.4%2525%2520improvement%2520in%2520the%2520area%2520under%2520curve%2520%2528AUC%2529%2520score%252C%250A5.1%2525%2520improvement%2520in%2520the%2520average%2520precision%2520%2528AP%2529%2529%2520and%2520very%2520comparable%2520performance%250Aagainst%2520multi-modality%2520methods%2520with%252059%2525%2520number%2520of%2520parameters%2520less.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers&entry.906535625=Yuehao%20Song%20and%20Xinggang%20Wang%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Jinglin%20Zhang%20and%20Xiangmin%20Xu&entry.1292438233=%20%20Gaze%20following%20aims%20to%20interpret%20human-scene%20interactions%20by%20predicting%20the%0Aperson%27s%20focal%20point%20of%20gaze.%20Prevailing%20approaches%20often%20adopt%20a%20two-stage%0Aframework%2C%20whereby%20multi-modality%20information%20is%20extracted%20in%20the%20initial%20stage%0Afor%20gaze%20target%20prediction.%20Consequently%2C%20the%20efficacy%20of%20these%20methods%20highly%0Adepends%20on%20the%20precision%20of%20the%20preceding%20modality%20extraction.%20Others%20use%20a%0Asingle-modality%20approach%20with%20complex%20decoders%2C%20increasing%20network%0Acomputational%20load.%20Inspired%20by%20the%20remarkable%20success%20of%20pre-trained%20plain%0Avision%20transformers%20%28ViTs%29%2C%20we%20introduce%20a%20novel%20single-modality%20gaze%20following%0Aframework%20called%20ViTGaze.%20In%20contrast%20to%20previous%20methods%2C%20it%20creates%20a%20novel%0Agaze%20following%20framework%20based%20mainly%20on%20powerful%20encoders%20%28relative%20decoder%0Aparameters%20less%20than%201%25%29.%20Our%20principal%20insight%20is%20that%20the%20inter-token%0Ainteractions%20within%20self-attention%20can%20be%20transferred%20to%20interactions%20between%0Ahumans%20and%20scenes.%20Leveraging%20this%20presumption%2C%20we%20formulate%20a%20framework%0Aconsisting%20of%20a%204D%20interaction%20encoder%20and%20a%202D%20spatial%20guidance%20module%20to%0Aextract%20human-scene%20interaction%20information%20from%20self-attention%20maps.%0AFurthermore%2C%20our%20investigation%20reveals%20that%20ViT%20with%20self-supervised%0Apre-training%20has%20an%20enhanced%20ability%20to%20extract%20correlation%20information.%20Many%0Aexperiments%20have%20been%20conducted%20to%20demonstrate%20the%20performance%20of%20the%20proposed%0Amethod.%20Our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20all%0Asingle-modality%20methods%20%283.4%25%20improvement%20in%20the%20area%20under%20curve%20%28AUC%29%20score%2C%0A5.1%25%20improvement%20in%20the%20average%20precision%20%28AP%29%29%20and%20very%20comparable%20performance%0Aagainst%20multi-modality%20methods%20with%2059%25%20number%20of%20parameters%20less.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12778v2&entry.124074799=Read"},
{"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "author": "Zhengyi Wang and Jonathan Lorraine and Yikai Wang and Hang Su and Jun Zhu and Sanja Fidler and Xiaohui Zeng", "abstract": "  This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.\n", "link": "http://arxiv.org/abs/2411.09595v1", "date": "2024-11-14", "relevancy": 2.8254, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMA-Mesh%3A%20Unifying%203D%20Mesh%20Generation%20with%20Language%20Models&body=Title%3A%20LLaMA-Mesh%3A%20Unifying%203D%20Mesh%20Generation%20with%20Language%20Models%0AAuthor%3A%20Zhengyi%20Wang%20and%20Jonathan%20Lorraine%20and%20Yikai%20Wang%20and%20Hang%20Su%20and%20Jun%20Zhu%20and%20Sanja%20Fidler%20and%20Xiaohui%20Zeng%0AAbstract%3A%20%20%20This%20work%20explores%20expanding%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Apretrained%20on%20text%20to%20generate%203D%20meshes%20within%20a%20unified%20model.%20This%20offers%0Akey%20advantages%20of%20%281%29%20leveraging%20spatial%20knowledge%20already%20embedded%20in%20LLMs%2C%0Aderived%20from%20textual%20sources%20like%203D%20tutorials%2C%20and%20%282%29%20enabling%20conversational%0A3D%20generation%20and%20mesh%20understanding.%20A%20primary%20challenge%20is%20effectively%0Atokenizing%203D%20mesh%20data%20into%20discrete%20tokens%20that%20LLMs%20can%20process%20seamlessly.%0ATo%20address%20this%2C%20we%20introduce%20LLaMA-Mesh%2C%20a%20novel%20approach%20that%20represents%20the%0Avertex%20coordinates%20and%20face%20definitions%20of%203D%20meshes%20as%20plain%20text%2C%20allowing%0Adirect%20integration%20with%20LLMs%20without%20expanding%20the%20vocabulary.%20We%20construct%20a%0Asupervised%20fine-tuning%20%28SFT%29%20dataset%20enabling%20pretrained%20LLMs%20to%20%281%29%20generate%0A3D%20meshes%20from%20text%20prompts%2C%20%282%29%20produce%20interleaved%20text%20and%203D%20mesh%20outputs%0Aas%20required%2C%20and%20%283%29%20understand%20and%20interpret%203D%20meshes.%20Our%20work%20is%20the%20first%0Ato%20demonstrate%20that%20LLMs%20can%20be%20fine-tuned%20to%20acquire%20complex%20spatial%20knowledge%0Afor%203D%20mesh%20generation%20in%20a%20text-based%20format%2C%20effectively%20unifying%20the%203D%20and%0Atext%20modalities.%20LLaMA-Mesh%20achieves%20mesh%20generation%20quality%20on%20par%20with%20models%0Atrained%20from%20scratch%20while%20maintaining%20strong%20text%20generation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMA-Mesh%253A%2520Unifying%25203D%2520Mesh%2520Generation%2520with%2520Language%2520Models%26entry.906535625%3DZhengyi%2520Wang%2520and%2520Jonathan%2520Lorraine%2520and%2520Yikai%2520Wang%2520and%2520Hang%2520Su%2520and%2520Jun%2520Zhu%2520and%2520Sanja%2520Fidler%2520and%2520Xiaohui%2520Zeng%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520expanding%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Apretrained%2520on%2520text%2520to%2520generate%25203D%2520meshes%2520within%2520a%2520unified%2520model.%2520This%2520offers%250Akey%2520advantages%2520of%2520%25281%2529%2520leveraging%2520spatial%2520knowledge%2520already%2520embedded%2520in%2520LLMs%252C%250Aderived%2520from%2520textual%2520sources%2520like%25203D%2520tutorials%252C%2520and%2520%25282%2529%2520enabling%2520conversational%250A3D%2520generation%2520and%2520mesh%2520understanding.%2520A%2520primary%2520challenge%2520is%2520effectively%250Atokenizing%25203D%2520mesh%2520data%2520into%2520discrete%2520tokens%2520that%2520LLMs%2520can%2520process%2520seamlessly.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520LLaMA-Mesh%252C%2520a%2520novel%2520approach%2520that%2520represents%2520the%250Avertex%2520coordinates%2520and%2520face%2520definitions%2520of%25203D%2520meshes%2520as%2520plain%2520text%252C%2520allowing%250Adirect%2520integration%2520with%2520LLMs%2520without%2520expanding%2520the%2520vocabulary.%2520We%2520construct%2520a%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520dataset%2520enabling%2520pretrained%2520LLMs%2520to%2520%25281%2529%2520generate%250A3D%2520meshes%2520from%2520text%2520prompts%252C%2520%25282%2529%2520produce%2520interleaved%2520text%2520and%25203D%2520mesh%2520outputs%250Aas%2520required%252C%2520and%2520%25283%2529%2520understand%2520and%2520interpret%25203D%2520meshes.%2520Our%2520work%2520is%2520the%2520first%250Ato%2520demonstrate%2520that%2520LLMs%2520can%2520be%2520fine-tuned%2520to%2520acquire%2520complex%2520spatial%2520knowledge%250Afor%25203D%2520mesh%2520generation%2520in%2520a%2520text-based%2520format%252C%2520effectively%2520unifying%2520the%25203D%2520and%250Atext%2520modalities.%2520LLaMA-Mesh%2520achieves%2520mesh%2520generation%2520quality%2520on%2520par%2520with%2520models%250Atrained%2520from%2520scratch%2520while%2520maintaining%2520strong%2520text%2520generation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMA-Mesh%3A%20Unifying%203D%20Mesh%20Generation%20with%20Language%20Models&entry.906535625=Zhengyi%20Wang%20and%20Jonathan%20Lorraine%20and%20Yikai%20Wang%20and%20Hang%20Su%20and%20Jun%20Zhu%20and%20Sanja%20Fidler%20and%20Xiaohui%20Zeng&entry.1292438233=%20%20This%20work%20explores%20expanding%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Apretrained%20on%20text%20to%20generate%203D%20meshes%20within%20a%20unified%20model.%20This%20offers%0Akey%20advantages%20of%20%281%29%20leveraging%20spatial%20knowledge%20already%20embedded%20in%20LLMs%2C%0Aderived%20from%20textual%20sources%20like%203D%20tutorials%2C%20and%20%282%29%20enabling%20conversational%0A3D%20generation%20and%20mesh%20understanding.%20A%20primary%20challenge%20is%20effectively%0Atokenizing%203D%20mesh%20data%20into%20discrete%20tokens%20that%20LLMs%20can%20process%20seamlessly.%0ATo%20address%20this%2C%20we%20introduce%20LLaMA-Mesh%2C%20a%20novel%20approach%20that%20represents%20the%0Avertex%20coordinates%20and%20face%20definitions%20of%203D%20meshes%20as%20plain%20text%2C%20allowing%0Adirect%20integration%20with%20LLMs%20without%20expanding%20the%20vocabulary.%20We%20construct%20a%0Asupervised%20fine-tuning%20%28SFT%29%20dataset%20enabling%20pretrained%20LLMs%20to%20%281%29%20generate%0A3D%20meshes%20from%20text%20prompts%2C%20%282%29%20produce%20interleaved%20text%20and%203D%20mesh%20outputs%0Aas%20required%2C%20and%20%283%29%20understand%20and%20interpret%203D%20meshes.%20Our%20work%20is%20the%20first%0Ato%20demonstrate%20that%20LLMs%20can%20be%20fine-tuned%20to%20acquire%20complex%20spatial%20knowledge%0Afor%203D%20mesh%20generation%20in%20a%20text-based%20format%2C%20effectively%20unifying%20the%203D%20and%0Atext%20modalities.%20LLaMA-Mesh%20achieves%20mesh%20generation%20quality%20on%20par%20with%20models%0Atrained%20from%20scratch%20while%20maintaining%20strong%20text%20generation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09595v1&entry.124074799=Read"},
{"title": "TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for\n  Traversability Estimation", "author": "Antonio Santo and Juan J. Cabrera and David Valiente and Carlos Viegas and Arturo Gil", "abstract": "  This paper presents TE-NeXt, a novel and efficient architecture for\nTraversability Estimation (TE) from sparse LiDAR point clouds based on a\nresidual convolution block. TE-NeXt block fuses notions of current trends such\nas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate\nhigh capacity for generalisation in a variety of urban and natural\nenvironments, using well-known and accessible datasets such as SemanticKITTI,\nRellis-3D and SemanticUSL. Thus, the designed architecture ouperforms\nstate-of-the-art methods in the problem of semantic segmentation, demonstrating\nbetter results in unstructured environments and maintaining high reliability\nand robustness in urbans environments, which leads to better abstraction.\nImplementation is available in a open repository to the scientific community\nwith the aim of ensuring the reproducibility of results.\n", "link": "http://arxiv.org/abs/2406.01395v3", "date": "2024-11-14", "relevancy": 2.8246, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation&body=Title%3A%20TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation%0AAuthor%3A%20Antonio%20Santo%20and%20Juan%20J.%20Cabrera%20and%20David%20Valiente%20and%20Carlos%20Viegas%20and%20Arturo%20Gil%0AAbstract%3A%20%20%20This%20paper%20presents%20TE-NeXt%2C%20a%20novel%20and%20efficient%20architecture%20for%0ATraversability%20Estimation%20%28TE%29%20from%20sparse%20LiDAR%20point%20clouds%20based%20on%20a%0Aresidual%20convolution%20block.%20TE-NeXt%20block%20fuses%20notions%20of%20current%20trends%20such%0Aas%20attention%20mechanisms%20and%203D%20sparse%20convolutions.%20TE-NeXt%20aims%20to%20demonstrate%0Ahigh%20capacity%20for%20generalisation%20in%20a%20variety%20of%20urban%20and%20natural%0Aenvironments%2C%20using%20well-known%20and%20accessible%20datasets%20such%20as%20SemanticKITTI%2C%0ARellis-3D%20and%20SemanticUSL.%20Thus%2C%20the%20designed%20architecture%20ouperforms%0Astate-of-the-art%20methods%20in%20the%20problem%20of%20semantic%20segmentation%2C%20demonstrating%0Abetter%20results%20in%20unstructured%20environments%20and%20maintaining%20high%20reliability%0Aand%20robustness%20in%20urbans%20environments%2C%20which%20leads%20to%20better%20abstraction.%0AImplementation%20is%20available%20in%20a%20open%20repository%20to%20the%20scientific%20community%0Awith%20the%20aim%20of%20ensuring%20the%20reproducibility%20of%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01395v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTE-NeXt%253A%2520A%2520LiDAR-Based%25203D%2520Sparse%2520Convolutional%2520Network%2520for%250A%2520%2520Traversability%2520Estimation%26entry.906535625%3DAntonio%2520Santo%2520and%2520Juan%2520J.%2520Cabrera%2520and%2520David%2520Valiente%2520and%2520Carlos%2520Viegas%2520and%2520Arturo%2520Gil%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520TE-NeXt%252C%2520a%2520novel%2520and%2520efficient%2520architecture%2520for%250ATraversability%2520Estimation%2520%2528TE%2529%2520from%2520sparse%2520LiDAR%2520point%2520clouds%2520based%2520on%2520a%250Aresidual%2520convolution%2520block.%2520TE-NeXt%2520block%2520fuses%2520notions%2520of%2520current%2520trends%2520such%250Aas%2520attention%2520mechanisms%2520and%25203D%2520sparse%2520convolutions.%2520TE-NeXt%2520aims%2520to%2520demonstrate%250Ahigh%2520capacity%2520for%2520generalisation%2520in%2520a%2520variety%2520of%2520urban%2520and%2520natural%250Aenvironments%252C%2520using%2520well-known%2520and%2520accessible%2520datasets%2520such%2520as%2520SemanticKITTI%252C%250ARellis-3D%2520and%2520SemanticUSL.%2520Thus%252C%2520the%2520designed%2520architecture%2520ouperforms%250Astate-of-the-art%2520methods%2520in%2520the%2520problem%2520of%2520semantic%2520segmentation%252C%2520demonstrating%250Abetter%2520results%2520in%2520unstructured%2520environments%2520and%2520maintaining%2520high%2520reliability%250Aand%2520robustness%2520in%2520urbans%2520environments%252C%2520which%2520leads%2520to%2520better%2520abstraction.%250AImplementation%2520is%2520available%2520in%2520a%2520open%2520repository%2520to%2520the%2520scientific%2520community%250Awith%2520the%2520aim%2520of%2520ensuring%2520the%2520reproducibility%2520of%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01395v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation&entry.906535625=Antonio%20Santo%20and%20Juan%20J.%20Cabrera%20and%20David%20Valiente%20and%20Carlos%20Viegas%20and%20Arturo%20Gil&entry.1292438233=%20%20This%20paper%20presents%20TE-NeXt%2C%20a%20novel%20and%20efficient%20architecture%20for%0ATraversability%20Estimation%20%28TE%29%20from%20sparse%20LiDAR%20point%20clouds%20based%20on%20a%0Aresidual%20convolution%20block.%20TE-NeXt%20block%20fuses%20notions%20of%20current%20trends%20such%0Aas%20attention%20mechanisms%20and%203D%20sparse%20convolutions.%20TE-NeXt%20aims%20to%20demonstrate%0Ahigh%20capacity%20for%20generalisation%20in%20a%20variety%20of%20urban%20and%20natural%0Aenvironments%2C%20using%20well-known%20and%20accessible%20datasets%20such%20as%20SemanticKITTI%2C%0ARellis-3D%20and%20SemanticUSL.%20Thus%2C%20the%20designed%20architecture%20ouperforms%0Astate-of-the-art%20methods%20in%20the%20problem%20of%20semantic%20segmentation%2C%20demonstrating%0Abetter%20results%20in%20unstructured%20environments%20and%20maintaining%20high%20reliability%0Aand%20robustness%20in%20urbans%20environments%2C%20which%20leads%20to%20better%20abstraction.%0AImplementation%20is%20available%20in%20a%20open%20repository%20to%20the%20scientific%20community%0Awith%20the%20aim%20of%20ensuring%20the%20reproducibility%20of%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01395v3&entry.124074799=Read"},
{"title": "DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous\n  Vehicle Testing", "author": "Junjie Zhou and Lin Wang and Qiang Meng and Xiaofan Wang", "abstract": "  Generating realistic and diverse road scenarios is essential for autonomous\nvehicle testing and validation. Nevertheless, owing to the complexity and\nvariability of real-world road environments, creating authentic and varied\nscenarios for intelligent driving testing is challenging. In this paper, we\npropose DiffRoad, a novel diffusion model designed to produce controllable and\nhigh-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities\nof diffusion models to synthesize road layouts from white noise through an\ninverse denoising process, preserving real-world spatial features. To enhance\nthe quality of generated scenarios, we design the Road-UNet architecture,\noptimizing the balance between backbone and skip connections for high-realism\nscenario generation. Furthermore, we introduce a road scenario evaluation\nmodule that screens adequate and reasonable scenarios for intelligent driving\ntesting using two critical metrics: road continuity and road reasonableness.\nExperimental results on multiple real-world datasets demonstrate DiffRoad's\nability to generate realistic and smooth road structures while maintaining the\noriginal distribution. Additionally, the generated scenarios can be fully\nautomated into the OpenDRIVE format, facilitating generalized autonomous\nvehicle simulation testing. DiffRoad provides a rich and diverse scenario\nlibrary for large-scale autonomous vehicle testing and offers valuable insights\nfor future infrastructure designs that are better suited for autonomous\nvehicles.\n", "link": "http://arxiv.org/abs/2411.09451v1", "date": "2024-11-14", "relevancy": 2.7213, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5537}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5537}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffRoad%3A%20Realistic%20and%20Diverse%20Road%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicle%20Testing&body=Title%3A%20DiffRoad%3A%20Realistic%20and%20Diverse%20Road%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicle%20Testing%0AAuthor%3A%20Junjie%20Zhou%20and%20Lin%20Wang%20and%20Qiang%20Meng%20and%20Xiaofan%20Wang%0AAbstract%3A%20%20%20Generating%20realistic%20and%20diverse%20road%20scenarios%20is%20essential%20for%20autonomous%0Avehicle%20testing%20and%20validation.%20Nevertheless%2C%20owing%20to%20the%20complexity%20and%0Avariability%20of%20real-world%20road%20environments%2C%20creating%20authentic%20and%20varied%0Ascenarios%20for%20intelligent%20driving%20testing%20is%20challenging.%20In%20this%20paper%2C%20we%0Apropose%20DiffRoad%2C%20a%20novel%20diffusion%20model%20designed%20to%20produce%20controllable%20and%0Ahigh-fidelity%203D%20road%20scenarios.%20DiffRoad%20leverages%20the%20generative%20capabilities%0Aof%20diffusion%20models%20to%20synthesize%20road%20layouts%20from%20white%20noise%20through%20an%0Ainverse%20denoising%20process%2C%20preserving%20real-world%20spatial%20features.%20To%20enhance%0Athe%20quality%20of%20generated%20scenarios%2C%20we%20design%20the%20Road-UNet%20architecture%2C%0Aoptimizing%20the%20balance%20between%20backbone%20and%20skip%20connections%20for%20high-realism%0Ascenario%20generation.%20Furthermore%2C%20we%20introduce%20a%20road%20scenario%20evaluation%0Amodule%20that%20screens%20adequate%20and%20reasonable%20scenarios%20for%20intelligent%20driving%0Atesting%20using%20two%20critical%20metrics%3A%20road%20continuity%20and%20road%20reasonableness.%0AExperimental%20results%20on%20multiple%20real-world%20datasets%20demonstrate%20DiffRoad%27s%0Aability%20to%20generate%20realistic%20and%20smooth%20road%20structures%20while%20maintaining%20the%0Aoriginal%20distribution.%20Additionally%2C%20the%20generated%20scenarios%20can%20be%20fully%0Aautomated%20into%20the%20OpenDRIVE%20format%2C%20facilitating%20generalized%20autonomous%0Avehicle%20simulation%20testing.%20DiffRoad%20provides%20a%20rich%20and%20diverse%20scenario%0Alibrary%20for%20large-scale%20autonomous%20vehicle%20testing%20and%20offers%20valuable%20insights%0Afor%20future%20infrastructure%20designs%20that%20are%20better%20suited%20for%20autonomous%0Avehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffRoad%253A%2520Realistic%2520and%2520Diverse%2520Road%2520Scenario%2520Generation%2520for%2520Autonomous%250A%2520%2520Vehicle%2520Testing%26entry.906535625%3DJunjie%2520Zhou%2520and%2520Lin%2520Wang%2520and%2520Qiang%2520Meng%2520and%2520Xiaofan%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520and%2520diverse%2520road%2520scenarios%2520is%2520essential%2520for%2520autonomous%250Avehicle%2520testing%2520and%2520validation.%2520Nevertheless%252C%2520owing%2520to%2520the%2520complexity%2520and%250Avariability%2520of%2520real-world%2520road%2520environments%252C%2520creating%2520authentic%2520and%2520varied%250Ascenarios%2520for%2520intelligent%2520driving%2520testing%2520is%2520challenging.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520DiffRoad%252C%2520a%2520novel%2520diffusion%2520model%2520designed%2520to%2520produce%2520controllable%2520and%250Ahigh-fidelity%25203D%2520road%2520scenarios.%2520DiffRoad%2520leverages%2520the%2520generative%2520capabilities%250Aof%2520diffusion%2520models%2520to%2520synthesize%2520road%2520layouts%2520from%2520white%2520noise%2520through%2520an%250Ainverse%2520denoising%2520process%252C%2520preserving%2520real-world%2520spatial%2520features.%2520To%2520enhance%250Athe%2520quality%2520of%2520generated%2520scenarios%252C%2520we%2520design%2520the%2520Road-UNet%2520architecture%252C%250Aoptimizing%2520the%2520balance%2520between%2520backbone%2520and%2520skip%2520connections%2520for%2520high-realism%250Ascenario%2520generation.%2520Furthermore%252C%2520we%2520introduce%2520a%2520road%2520scenario%2520evaluation%250Amodule%2520that%2520screens%2520adequate%2520and%2520reasonable%2520scenarios%2520for%2520intelligent%2520driving%250Atesting%2520using%2520two%2520critical%2520metrics%253A%2520road%2520continuity%2520and%2520road%2520reasonableness.%250AExperimental%2520results%2520on%2520multiple%2520real-world%2520datasets%2520demonstrate%2520DiffRoad%2527s%250Aability%2520to%2520generate%2520realistic%2520and%2520smooth%2520road%2520structures%2520while%2520maintaining%2520the%250Aoriginal%2520distribution.%2520Additionally%252C%2520the%2520generated%2520scenarios%2520can%2520be%2520fully%250Aautomated%2520into%2520the%2520OpenDRIVE%2520format%252C%2520facilitating%2520generalized%2520autonomous%250Avehicle%2520simulation%2520testing.%2520DiffRoad%2520provides%2520a%2520rich%2520and%2520diverse%2520scenario%250Alibrary%2520for%2520large-scale%2520autonomous%2520vehicle%2520testing%2520and%2520offers%2520valuable%2520insights%250Afor%2520future%2520infrastructure%2520designs%2520that%2520are%2520better%2520suited%2520for%2520autonomous%250Avehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffRoad%3A%20Realistic%20and%20Diverse%20Road%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicle%20Testing&entry.906535625=Junjie%20Zhou%20and%20Lin%20Wang%20and%20Qiang%20Meng%20and%20Xiaofan%20Wang&entry.1292438233=%20%20Generating%20realistic%20and%20diverse%20road%20scenarios%20is%20essential%20for%20autonomous%0Avehicle%20testing%20and%20validation.%20Nevertheless%2C%20owing%20to%20the%20complexity%20and%0Avariability%20of%20real-world%20road%20environments%2C%20creating%20authentic%20and%20varied%0Ascenarios%20for%20intelligent%20driving%20testing%20is%20challenging.%20In%20this%20paper%2C%20we%0Apropose%20DiffRoad%2C%20a%20novel%20diffusion%20model%20designed%20to%20produce%20controllable%20and%0Ahigh-fidelity%203D%20road%20scenarios.%20DiffRoad%20leverages%20the%20generative%20capabilities%0Aof%20diffusion%20models%20to%20synthesize%20road%20layouts%20from%20white%20noise%20through%20an%0Ainverse%20denoising%20process%2C%20preserving%20real-world%20spatial%20features.%20To%20enhance%0Athe%20quality%20of%20generated%20scenarios%2C%20we%20design%20the%20Road-UNet%20architecture%2C%0Aoptimizing%20the%20balance%20between%20backbone%20and%20skip%20connections%20for%20high-realism%0Ascenario%20generation.%20Furthermore%2C%20we%20introduce%20a%20road%20scenario%20evaluation%0Amodule%20that%20screens%20adequate%20and%20reasonable%20scenarios%20for%20intelligent%20driving%0Atesting%20using%20two%20critical%20metrics%3A%20road%20continuity%20and%20road%20reasonableness.%0AExperimental%20results%20on%20multiple%20real-world%20datasets%20demonstrate%20DiffRoad%27s%0Aability%20to%20generate%20realistic%20and%20smooth%20road%20structures%20while%20maintaining%20the%0Aoriginal%20distribution.%20Additionally%2C%20the%20generated%20scenarios%20can%20be%20fully%0Aautomated%20into%20the%20OpenDRIVE%20format%2C%20facilitating%20generalized%20autonomous%0Avehicle%20simulation%20testing.%20DiffRoad%20provides%20a%20rich%20and%20diverse%20scenario%0Alibrary%20for%20large-scale%20autonomous%20vehicle%20testing%20and%20offers%20valuable%20insights%0Afor%20future%20infrastructure%20designs%20that%20are%20better%20suited%20for%20autonomous%0Avehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09451v1&entry.124074799=Read"},
{"title": "Information-driven design of imaging systems", "author": "Henry Pinkard and Leyla Kabuli and Eric Markley and Tiffany Chien and Jiantao Jiao and Laura Waller", "abstract": "  Most modern imaging systems process the data they capture algorithmically\nbefore-or instead of-human viewing. As a result, performance depends not on how\ninterpretable the measurements appear, but how effectively they encode details\nfor algorithmic processing. Information theory provides mathematical tools to\nanalyze this, but developing methods that can handle the complexity of\nreal-world measurements yet remain practical enough for widespread use has\nproven challenging. We introduce a data-driven approach for estimating the\ninformation content of imaging system measurements. Our framework requires only\nexperimental measurements and noise characterization, with no need for ground\ntruth data. We demonstrate that these information estimates reliably predict\nsystem performance across diverse imaging modalities, including color\nphotography, radio astronomy, lensless imaging, and label-free microscopy. To\nautomate the process of designing imaging systems that maximize information\ncapture we introduce an optimization technique called Information-Driven\nEncoder Analysis Learning (IDEAL). The tools we develop in this work unlock\ninformation theory as a powerful, practical tool for analyzing and designing\nimaging systems across a broad range of applications.\n  A video summarizing this work can be found at\nhttps://waller-lab.github.io/EncodingInformationWebsite/\n", "link": "http://arxiv.org/abs/2405.20559v2", "date": "2024-11-14", "relevancy": 2.7129, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-driven%20design%20of%20imaging%20systems&body=Title%3A%20Information-driven%20design%20of%20imaging%20systems%0AAuthor%3A%20Henry%20Pinkard%20and%20Leyla%20Kabuli%20and%20Eric%20Markley%20and%20Tiffany%20Chien%20and%20Jiantao%20Jiao%20and%20Laura%20Waller%0AAbstract%3A%20%20%20Most%20modern%20imaging%20systems%20process%20the%20data%20they%20capture%20algorithmically%0Abefore-or%20instead%20of-human%20viewing.%20As%20a%20result%2C%20performance%20depends%20not%20on%20how%0Ainterpretable%20the%20measurements%20appear%2C%20but%20how%20effectively%20they%20encode%20details%0Afor%20algorithmic%20processing.%20Information%20theory%20provides%20mathematical%20tools%20to%0Aanalyze%20this%2C%20but%20developing%20methods%20that%20can%20handle%20the%20complexity%20of%0Areal-world%20measurements%20yet%20remain%20practical%20enough%20for%20widespread%20use%20has%0Aproven%20challenging.%20We%20introduce%20a%20data-driven%20approach%20for%20estimating%20the%0Ainformation%20content%20of%20imaging%20system%20measurements.%20Our%20framework%20requires%20only%0Aexperimental%20measurements%20and%20noise%20characterization%2C%20with%20no%20need%20for%20ground%0Atruth%20data.%20We%20demonstrate%20that%20these%20information%20estimates%20reliably%20predict%0Asystem%20performance%20across%20diverse%20imaging%20modalities%2C%20including%20color%0Aphotography%2C%20radio%20astronomy%2C%20lensless%20imaging%2C%20and%20label-free%20microscopy.%20To%0Aautomate%20the%20process%20of%20designing%20imaging%20systems%20that%20maximize%20information%0Acapture%20we%20introduce%20an%20optimization%20technique%20called%20Information-Driven%0AEncoder%20Analysis%20Learning%20%28IDEAL%29.%20The%20tools%20we%20develop%20in%20this%20work%20unlock%0Ainformation%20theory%20as%20a%20powerful%2C%20practical%20tool%20for%20analyzing%20and%20designing%0Aimaging%20systems%20across%20a%20broad%20range%20of%20applications.%0A%20%20A%20video%20summarizing%20this%20work%20can%20be%20found%20at%0Ahttps%3A//waller-lab.github.io/EncodingInformationWebsite/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-driven%2520design%2520of%2520imaging%2520systems%26entry.906535625%3DHenry%2520Pinkard%2520and%2520Leyla%2520Kabuli%2520and%2520Eric%2520Markley%2520and%2520Tiffany%2520Chien%2520and%2520Jiantao%2520Jiao%2520and%2520Laura%2520Waller%26entry.1292438233%3D%2520%2520Most%2520modern%2520imaging%2520systems%2520process%2520the%2520data%2520they%2520capture%2520algorithmically%250Abefore-or%2520instead%2520of-human%2520viewing.%2520As%2520a%2520result%252C%2520performance%2520depends%2520not%2520on%2520how%250Ainterpretable%2520the%2520measurements%2520appear%252C%2520but%2520how%2520effectively%2520they%2520encode%2520details%250Afor%2520algorithmic%2520processing.%2520Information%2520theory%2520provides%2520mathematical%2520tools%2520to%250Aanalyze%2520this%252C%2520but%2520developing%2520methods%2520that%2520can%2520handle%2520the%2520complexity%2520of%250Areal-world%2520measurements%2520yet%2520remain%2520practical%2520enough%2520for%2520widespread%2520use%2520has%250Aproven%2520challenging.%2520We%2520introduce%2520a%2520data-driven%2520approach%2520for%2520estimating%2520the%250Ainformation%2520content%2520of%2520imaging%2520system%2520measurements.%2520Our%2520framework%2520requires%2520only%250Aexperimental%2520measurements%2520and%2520noise%2520characterization%252C%2520with%2520no%2520need%2520for%2520ground%250Atruth%2520data.%2520We%2520demonstrate%2520that%2520these%2520information%2520estimates%2520reliably%2520predict%250Asystem%2520performance%2520across%2520diverse%2520imaging%2520modalities%252C%2520including%2520color%250Aphotography%252C%2520radio%2520astronomy%252C%2520lensless%2520imaging%252C%2520and%2520label-free%2520microscopy.%2520To%250Aautomate%2520the%2520process%2520of%2520designing%2520imaging%2520systems%2520that%2520maximize%2520information%250Acapture%2520we%2520introduce%2520an%2520optimization%2520technique%2520called%2520Information-Driven%250AEncoder%2520Analysis%2520Learning%2520%2528IDEAL%2529.%2520The%2520tools%2520we%2520develop%2520in%2520this%2520work%2520unlock%250Ainformation%2520theory%2520as%2520a%2520powerful%252C%2520practical%2520tool%2520for%2520analyzing%2520and%2520designing%250Aimaging%2520systems%2520across%2520a%2520broad%2520range%2520of%2520applications.%250A%2520%2520A%2520video%2520summarizing%2520this%2520work%2520can%2520be%2520found%2520at%250Ahttps%253A//waller-lab.github.io/EncodingInformationWebsite/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-driven%20design%20of%20imaging%20systems&entry.906535625=Henry%20Pinkard%20and%20Leyla%20Kabuli%20and%20Eric%20Markley%20and%20Tiffany%20Chien%20and%20Jiantao%20Jiao%20and%20Laura%20Waller&entry.1292438233=%20%20Most%20modern%20imaging%20systems%20process%20the%20data%20they%20capture%20algorithmically%0Abefore-or%20instead%20of-human%20viewing.%20As%20a%20result%2C%20performance%20depends%20not%20on%20how%0Ainterpretable%20the%20measurements%20appear%2C%20but%20how%20effectively%20they%20encode%20details%0Afor%20algorithmic%20processing.%20Information%20theory%20provides%20mathematical%20tools%20to%0Aanalyze%20this%2C%20but%20developing%20methods%20that%20can%20handle%20the%20complexity%20of%0Areal-world%20measurements%20yet%20remain%20practical%20enough%20for%20widespread%20use%20has%0Aproven%20challenging.%20We%20introduce%20a%20data-driven%20approach%20for%20estimating%20the%0Ainformation%20content%20of%20imaging%20system%20measurements.%20Our%20framework%20requires%20only%0Aexperimental%20measurements%20and%20noise%20characterization%2C%20with%20no%20need%20for%20ground%0Atruth%20data.%20We%20demonstrate%20that%20these%20information%20estimates%20reliably%20predict%0Asystem%20performance%20across%20diverse%20imaging%20modalities%2C%20including%20color%0Aphotography%2C%20radio%20astronomy%2C%20lensless%20imaging%2C%20and%20label-free%20microscopy.%20To%0Aautomate%20the%20process%20of%20designing%20imaging%20systems%20that%20maximize%20information%0Acapture%20we%20introduce%20an%20optimization%20technique%20called%20Information-Driven%0AEncoder%20Analysis%20Learning%20%28IDEAL%29.%20The%20tools%20we%20develop%20in%20this%20work%20unlock%0Ainformation%20theory%20as%20a%20powerful%2C%20practical%20tool%20for%20analyzing%20and%20designing%0Aimaging%20systems%20across%20a%20broad%20range%20of%20applications.%0A%20%20A%20video%20summarizing%20this%20work%20can%20be%20found%20at%0Ahttps%3A//waller-lab.github.io/EncodingInformationWebsite/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20559v2&entry.124074799=Read"},
{"title": "An interpretable generative multimodal neuroimaging-genomics framework\n  for decoding Alzheimer's disease", "author": "Giorgio Dolci and Federica Cruciani and Md Abdur Rahaman and Anees Abrol and Jiayu Chen and Zening Fu and Ilaria Boscolo Galazzo and Gloria Menegaz and Vince D. Calhoun", "abstract": "  Alzheimer's disease (AD) is the most prevalent form of dementia with a\nprogressive decline in cognitive abilities. The AD continuum encompasses a\nprodromal stage known as MCI, where patients may either progress to AD (MCIc)\nor remain stable (MCInc). Understanding AD mechanisms requires complementary\nanalyses relying on different data sources, leading to the development of\nmultimodal DL models. We leveraged structural and functional MRI to investigate\nthe disease-induced GM and functional network connectivity changes. Moreover,\nconsidering AD's strong genetic component, we introduced SNPs as a third\nchannel. Missing one or more modalities is a typical concern of multimodal\nmethods. We hence propose a novel DL-based classification framework where a\ngenerative module employing Cycle GAN was adopted for imputing missing data in\nthe latent space. Additionally, we adopted an XAI method, Integrated Gradients,\nto extract features' relevance, enhancing our understanding of the learned\nrepresentations. Two tasks were addressed: AD detection and MCI conversion\nprediction. Experimental results showed that our framework reached the SOA in\nthe classification of CN/AD with an average test accuracy of $0.926\\pm0.02$.\nFor the MCInc/MCIc task, we achieved an average prediction accuracy of\n$0.711\\pm0.01$ using the pre-trained model for CN and AD. The interpretability\nanalysis revealed that significant GM modulations led the classification\nperformance in cortical and subcortical brain areas well known for their\nassociation with AD. Impairments in sensory-motor and visual functional network\nconnectivity along AD, as well as mutations in SNPs defining biological\nprocesses linked to endocytosis, amyloid-beta, and cholesterol, were identified\nas contributors to the results. Overall, our integrative DL model shows promise\nfor AD detection and MCI prediction, while shading light on important\nbiological insights.\n", "link": "http://arxiv.org/abs/2406.13292v2", "date": "2024-11-14", "relevancy": 2.7023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease&body=Title%3A%20An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease%0AAuthor%3A%20Giorgio%20Dolci%20and%20Federica%20Cruciani%20and%20Md%20Abdur%20Rahaman%20and%20Anees%20Abrol%20and%20Jiayu%20Chen%20and%20Zening%20Fu%20and%20Ilaria%20Boscolo%20Galazzo%20and%20Gloria%20Menegaz%20and%20Vince%20D.%20Calhoun%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20prevalent%20form%20of%20dementia%20with%20a%0Aprogressive%20decline%20in%20cognitive%20abilities.%20The%20AD%20continuum%20encompasses%20a%0Aprodromal%20stage%20known%20as%20MCI%2C%20where%20patients%20may%20either%20progress%20to%20AD%20%28MCIc%29%0Aor%20remain%20stable%20%28MCInc%29.%20Understanding%20AD%20mechanisms%20requires%20complementary%0Aanalyses%20relying%20on%20different%20data%20sources%2C%20leading%20to%20the%20development%20of%0Amultimodal%20DL%20models.%20We%20leveraged%20structural%20and%20functional%20MRI%20to%20investigate%0Athe%20disease-induced%20GM%20and%20functional%20network%20connectivity%20changes.%20Moreover%2C%0Aconsidering%20AD%27s%20strong%20genetic%20component%2C%20we%20introduced%20SNPs%20as%20a%20third%0Achannel.%20Missing%20one%20or%20more%20modalities%20is%20a%20typical%20concern%20of%20multimodal%0Amethods.%20We%20hence%20propose%20a%20novel%20DL-based%20classification%20framework%20where%20a%0Agenerative%20module%20employing%20Cycle%20GAN%20was%20adopted%20for%20imputing%20missing%20data%20in%0Athe%20latent%20space.%20Additionally%2C%20we%20adopted%20an%20XAI%20method%2C%20Integrated%20Gradients%2C%0Ato%20extract%20features%27%20relevance%2C%20enhancing%20our%20understanding%20of%20the%20learned%0Arepresentations.%20Two%20tasks%20were%20addressed%3A%20AD%20detection%20and%20MCI%20conversion%0Aprediction.%20Experimental%20results%20showed%20that%20our%20framework%20reached%20the%20SOA%20in%0Athe%20classification%20of%20CN/AD%20with%20an%20average%20test%20accuracy%20of%20%240.926%5Cpm0.02%24.%0AFor%20the%20MCInc/MCIc%20task%2C%20we%20achieved%20an%20average%20prediction%20accuracy%20of%0A%240.711%5Cpm0.01%24%20using%20the%20pre-trained%20model%20for%20CN%20and%20AD.%20The%20interpretability%0Aanalysis%20revealed%20that%20significant%20GM%20modulations%20led%20the%20classification%0Aperformance%20in%20cortical%20and%20subcortical%20brain%20areas%20well%20known%20for%20their%0Aassociation%20with%20AD.%20Impairments%20in%20sensory-motor%20and%20visual%20functional%20network%0Aconnectivity%20along%20AD%2C%20as%20well%20as%20mutations%20in%20SNPs%20defining%20biological%0Aprocesses%20linked%20to%20endocytosis%2C%20amyloid-beta%2C%20and%20cholesterol%2C%20were%20identified%0Aas%20contributors%20to%20the%20results.%20Overall%2C%20our%20integrative%20DL%20model%20shows%20promise%0Afor%20AD%20detection%20and%20MCI%20prediction%2C%20while%20shading%20light%20on%20important%0Abiological%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520interpretable%2520generative%2520multimodal%2520neuroimaging-genomics%2520framework%250A%2520%2520for%2520decoding%2520Alzheimer%2527s%2520disease%26entry.906535625%3DGiorgio%2520Dolci%2520and%2520Federica%2520Cruciani%2520and%2520Md%2520Abdur%2520Rahaman%2520and%2520Anees%2520Abrol%2520and%2520Jiayu%2520Chen%2520and%2520Zening%2520Fu%2520and%2520Ilaria%2520Boscolo%2520Galazzo%2520and%2520Gloria%2520Menegaz%2520and%2520Vince%2520D.%2520Calhoun%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520is%2520the%2520most%2520prevalent%2520form%2520of%2520dementia%2520with%2520a%250Aprogressive%2520decline%2520in%2520cognitive%2520abilities.%2520The%2520AD%2520continuum%2520encompasses%2520a%250Aprodromal%2520stage%2520known%2520as%2520MCI%252C%2520where%2520patients%2520may%2520either%2520progress%2520to%2520AD%2520%2528MCIc%2529%250Aor%2520remain%2520stable%2520%2528MCInc%2529.%2520Understanding%2520AD%2520mechanisms%2520requires%2520complementary%250Aanalyses%2520relying%2520on%2520different%2520data%2520sources%252C%2520leading%2520to%2520the%2520development%2520of%250Amultimodal%2520DL%2520models.%2520We%2520leveraged%2520structural%2520and%2520functional%2520MRI%2520to%2520investigate%250Athe%2520disease-induced%2520GM%2520and%2520functional%2520network%2520connectivity%2520changes.%2520Moreover%252C%250Aconsidering%2520AD%2527s%2520strong%2520genetic%2520component%252C%2520we%2520introduced%2520SNPs%2520as%2520a%2520third%250Achannel.%2520Missing%2520one%2520or%2520more%2520modalities%2520is%2520a%2520typical%2520concern%2520of%2520multimodal%250Amethods.%2520We%2520hence%2520propose%2520a%2520novel%2520DL-based%2520classification%2520framework%2520where%2520a%250Agenerative%2520module%2520employing%2520Cycle%2520GAN%2520was%2520adopted%2520for%2520imputing%2520missing%2520data%2520in%250Athe%2520latent%2520space.%2520Additionally%252C%2520we%2520adopted%2520an%2520XAI%2520method%252C%2520Integrated%2520Gradients%252C%250Ato%2520extract%2520features%2527%2520relevance%252C%2520enhancing%2520our%2520understanding%2520of%2520the%2520learned%250Arepresentations.%2520Two%2520tasks%2520were%2520addressed%253A%2520AD%2520detection%2520and%2520MCI%2520conversion%250Aprediction.%2520Experimental%2520results%2520showed%2520that%2520our%2520framework%2520reached%2520the%2520SOA%2520in%250Athe%2520classification%2520of%2520CN/AD%2520with%2520an%2520average%2520test%2520accuracy%2520of%2520%25240.926%255Cpm0.02%2524.%250AFor%2520the%2520MCInc/MCIc%2520task%252C%2520we%2520achieved%2520an%2520average%2520prediction%2520accuracy%2520of%250A%25240.711%255Cpm0.01%2524%2520using%2520the%2520pre-trained%2520model%2520for%2520CN%2520and%2520AD.%2520The%2520interpretability%250Aanalysis%2520revealed%2520that%2520significant%2520GM%2520modulations%2520led%2520the%2520classification%250Aperformance%2520in%2520cortical%2520and%2520subcortical%2520brain%2520areas%2520well%2520known%2520for%2520their%250Aassociation%2520with%2520AD.%2520Impairments%2520in%2520sensory-motor%2520and%2520visual%2520functional%2520network%250Aconnectivity%2520along%2520AD%252C%2520as%2520well%2520as%2520mutations%2520in%2520SNPs%2520defining%2520biological%250Aprocesses%2520linked%2520to%2520endocytosis%252C%2520amyloid-beta%252C%2520and%2520cholesterol%252C%2520were%2520identified%250Aas%2520contributors%2520to%2520the%2520results.%2520Overall%252C%2520our%2520integrative%2520DL%2520model%2520shows%2520promise%250Afor%2520AD%2520detection%2520and%2520MCI%2520prediction%252C%2520while%2520shading%2520light%2520on%2520important%250Abiological%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20interpretable%20generative%20multimodal%20neuroimaging-genomics%20framework%0A%20%20for%20decoding%20Alzheimer%27s%20disease&entry.906535625=Giorgio%20Dolci%20and%20Federica%20Cruciani%20and%20Md%20Abdur%20Rahaman%20and%20Anees%20Abrol%20and%20Jiayu%20Chen%20and%20Zening%20Fu%20and%20Ilaria%20Boscolo%20Galazzo%20and%20Gloria%20Menegaz%20and%20Vince%20D.%20Calhoun&entry.1292438233=%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20prevalent%20form%20of%20dementia%20with%20a%0Aprogressive%20decline%20in%20cognitive%20abilities.%20The%20AD%20continuum%20encompasses%20a%0Aprodromal%20stage%20known%20as%20MCI%2C%20where%20patients%20may%20either%20progress%20to%20AD%20%28MCIc%29%0Aor%20remain%20stable%20%28MCInc%29.%20Understanding%20AD%20mechanisms%20requires%20complementary%0Aanalyses%20relying%20on%20different%20data%20sources%2C%20leading%20to%20the%20development%20of%0Amultimodal%20DL%20models.%20We%20leveraged%20structural%20and%20functional%20MRI%20to%20investigate%0Athe%20disease-induced%20GM%20and%20functional%20network%20connectivity%20changes.%20Moreover%2C%0Aconsidering%20AD%27s%20strong%20genetic%20component%2C%20we%20introduced%20SNPs%20as%20a%20third%0Achannel.%20Missing%20one%20or%20more%20modalities%20is%20a%20typical%20concern%20of%20multimodal%0Amethods.%20We%20hence%20propose%20a%20novel%20DL-based%20classification%20framework%20where%20a%0Agenerative%20module%20employing%20Cycle%20GAN%20was%20adopted%20for%20imputing%20missing%20data%20in%0Athe%20latent%20space.%20Additionally%2C%20we%20adopted%20an%20XAI%20method%2C%20Integrated%20Gradients%2C%0Ato%20extract%20features%27%20relevance%2C%20enhancing%20our%20understanding%20of%20the%20learned%0Arepresentations.%20Two%20tasks%20were%20addressed%3A%20AD%20detection%20and%20MCI%20conversion%0Aprediction.%20Experimental%20results%20showed%20that%20our%20framework%20reached%20the%20SOA%20in%0Athe%20classification%20of%20CN/AD%20with%20an%20average%20test%20accuracy%20of%20%240.926%5Cpm0.02%24.%0AFor%20the%20MCInc/MCIc%20task%2C%20we%20achieved%20an%20average%20prediction%20accuracy%20of%0A%240.711%5Cpm0.01%24%20using%20the%20pre-trained%20model%20for%20CN%20and%20AD.%20The%20interpretability%0Aanalysis%20revealed%20that%20significant%20GM%20modulations%20led%20the%20classification%0Aperformance%20in%20cortical%20and%20subcortical%20brain%20areas%20well%20known%20for%20their%0Aassociation%20with%20AD.%20Impairments%20in%20sensory-motor%20and%20visual%20functional%20network%0Aconnectivity%20along%20AD%2C%20as%20well%20as%20mutations%20in%20SNPs%20defining%20biological%0Aprocesses%20linked%20to%20endocytosis%2C%20amyloid-beta%2C%20and%20cholesterol%2C%20were%20identified%0Aas%20contributors%20to%20the%20results.%20Overall%2C%20our%20integrative%20DL%20model%20shows%20promise%0Afor%20AD%20detection%20and%20MCI%20prediction%2C%20while%20shading%20light%20on%20important%0Abiological%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13292v2&entry.124074799=Read"},
{"title": "Adaptively Augmented Consistency Learning: A Semi-supervised\n  Segmentation Framework for Remote Sensing", "author": "Hui Ye and Haodong Chen and Xiaoming Chen and Vera Chung", "abstract": "  Remote sensing (RS) involves the acquisition of data about objects or areas\nfrom a distance, primarily to monitor environmental changes, manage resources,\nand support planning and disaster response. A significant challenge in RS\nsegmentation is the scarcity of high-quality labeled images due to the\ndiversity and complexity of RS image, which makes pixel-level annotation\ndifficult and hinders the development of effective supervised segmentation\nalgorithms. To solve this problem, we propose Adaptively Augmented Consistency\nLearning (AACL), a semi-supervised segmentation framework designed to enhances\nRS segmentation accuracy under condictions of limited labeled data. AACL\nextracts additional information embedded in unlabeled images through the use of\nUniform Strength Augmentation (USAug) and Adaptive Cut-Mix (AdaCM). Evaluations\nacross various RS datasets demonstrate that AACL achieves competitive\nperformance in semi-supervised segmentation, showing up to a 20% improvement in\nspecific categories and 2% increase in overall performance compared to\nstate-of-the-art frameworks.\n", "link": "http://arxiv.org/abs/2411.09344v1", "date": "2024-11-14", "relevancy": 2.6972, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5374}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptively%20Augmented%20Consistency%20Learning%3A%20A%20Semi-supervised%0A%20%20Segmentation%20Framework%20for%20Remote%20Sensing&body=Title%3A%20Adaptively%20Augmented%20Consistency%20Learning%3A%20A%20Semi-supervised%0A%20%20Segmentation%20Framework%20for%20Remote%20Sensing%0AAuthor%3A%20Hui%20Ye%20and%20Haodong%20Chen%20and%20Xiaoming%20Chen%20and%20Vera%20Chung%0AAbstract%3A%20%20%20Remote%20sensing%20%28RS%29%20involves%20the%20acquisition%20of%20data%20about%20objects%20or%20areas%0Afrom%20a%20distance%2C%20primarily%20to%20monitor%20environmental%20changes%2C%20manage%20resources%2C%0Aand%20support%20planning%20and%20disaster%20response.%20A%20significant%20challenge%20in%20RS%0Asegmentation%20is%20the%20scarcity%20of%20high-quality%20labeled%20images%20due%20to%20the%0Adiversity%20and%20complexity%20of%20RS%20image%2C%20which%20makes%20pixel-level%20annotation%0Adifficult%20and%20hinders%20the%20development%20of%20effective%20supervised%20segmentation%0Aalgorithms.%20To%20solve%20this%20problem%2C%20we%20propose%20Adaptively%20Augmented%20Consistency%0ALearning%20%28AACL%29%2C%20a%20semi-supervised%20segmentation%20framework%20designed%20to%20enhances%0ARS%20segmentation%20accuracy%20under%20condictions%20of%20limited%20labeled%20data.%20AACL%0Aextracts%20additional%20information%20embedded%20in%20unlabeled%20images%20through%20the%20use%20of%0AUniform%20Strength%20Augmentation%20%28USAug%29%20and%20Adaptive%20Cut-Mix%20%28AdaCM%29.%20Evaluations%0Aacross%20various%20RS%20datasets%20demonstrate%20that%20AACL%20achieves%20competitive%0Aperformance%20in%20semi-supervised%20segmentation%2C%20showing%20up%20to%20a%2020%25%20improvement%20in%0Aspecific%20categories%20and%202%25%20increase%20in%20overall%20performance%20compared%20to%0Astate-of-the-art%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptively%2520Augmented%2520Consistency%2520Learning%253A%2520A%2520Semi-supervised%250A%2520%2520Segmentation%2520Framework%2520for%2520Remote%2520Sensing%26entry.906535625%3DHui%2520Ye%2520and%2520Haodong%2520Chen%2520and%2520Xiaoming%2520Chen%2520and%2520Vera%2520Chung%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520%2528RS%2529%2520involves%2520the%2520acquisition%2520of%2520data%2520about%2520objects%2520or%2520areas%250Afrom%2520a%2520distance%252C%2520primarily%2520to%2520monitor%2520environmental%2520changes%252C%2520manage%2520resources%252C%250Aand%2520support%2520planning%2520and%2520disaster%2520response.%2520A%2520significant%2520challenge%2520in%2520RS%250Asegmentation%2520is%2520the%2520scarcity%2520of%2520high-quality%2520labeled%2520images%2520due%2520to%2520the%250Adiversity%2520and%2520complexity%2520of%2520RS%2520image%252C%2520which%2520makes%2520pixel-level%2520annotation%250Adifficult%2520and%2520hinders%2520the%2520development%2520of%2520effective%2520supervised%2520segmentation%250Aalgorithms.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520Adaptively%2520Augmented%2520Consistency%250ALearning%2520%2528AACL%2529%252C%2520a%2520semi-supervised%2520segmentation%2520framework%2520designed%2520to%2520enhances%250ARS%2520segmentation%2520accuracy%2520under%2520condictions%2520of%2520limited%2520labeled%2520data.%2520AACL%250Aextracts%2520additional%2520information%2520embedded%2520in%2520unlabeled%2520images%2520through%2520the%2520use%2520of%250AUniform%2520Strength%2520Augmentation%2520%2528USAug%2529%2520and%2520Adaptive%2520Cut-Mix%2520%2528AdaCM%2529.%2520Evaluations%250Aacross%2520various%2520RS%2520datasets%2520demonstrate%2520that%2520AACL%2520achieves%2520competitive%250Aperformance%2520in%2520semi-supervised%2520segmentation%252C%2520showing%2520up%2520to%2520a%252020%2525%2520improvement%2520in%250Aspecific%2520categories%2520and%25202%2525%2520increase%2520in%2520overall%2520performance%2520compared%2520to%250Astate-of-the-art%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptively%20Augmented%20Consistency%20Learning%3A%20A%20Semi-supervised%0A%20%20Segmentation%20Framework%20for%20Remote%20Sensing&entry.906535625=Hui%20Ye%20and%20Haodong%20Chen%20and%20Xiaoming%20Chen%20and%20Vera%20Chung&entry.1292438233=%20%20Remote%20sensing%20%28RS%29%20involves%20the%20acquisition%20of%20data%20about%20objects%20or%20areas%0Afrom%20a%20distance%2C%20primarily%20to%20monitor%20environmental%20changes%2C%20manage%20resources%2C%0Aand%20support%20planning%20and%20disaster%20response.%20A%20significant%20challenge%20in%20RS%0Asegmentation%20is%20the%20scarcity%20of%20high-quality%20labeled%20images%20due%20to%20the%0Adiversity%20and%20complexity%20of%20RS%20image%2C%20which%20makes%20pixel-level%20annotation%0Adifficult%20and%20hinders%20the%20development%20of%20effective%20supervised%20segmentation%0Aalgorithms.%20To%20solve%20this%20problem%2C%20we%20propose%20Adaptively%20Augmented%20Consistency%0ALearning%20%28AACL%29%2C%20a%20semi-supervised%20segmentation%20framework%20designed%20to%20enhances%0ARS%20segmentation%20accuracy%20under%20condictions%20of%20limited%20labeled%20data.%20AACL%0Aextracts%20additional%20information%20embedded%20in%20unlabeled%20images%20through%20the%20use%20of%0AUniform%20Strength%20Augmentation%20%28USAug%29%20and%20Adaptive%20Cut-Mix%20%28AdaCM%29.%20Evaluations%0Aacross%20various%20RS%20datasets%20demonstrate%20that%20AACL%20achieves%20competitive%0Aperformance%20in%20semi-supervised%20segmentation%2C%20showing%20up%20to%20a%2020%25%20improvement%20in%0Aspecific%20categories%20and%202%25%20increase%20in%20overall%20performance%20compared%20to%0Astate-of-the-art%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09344v1&entry.124074799=Read"},
{"title": "IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images", "author": "Vincent Roca and Gr\u00e9gory Kuchcinski and Jean-Pierre Pruvo and Dorian Manouvriez and Renaud Lopes", "abstract": "  In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN by\nintegrating an arbitrary number of domains for training through a many-to-one\narchitecture. The framework based on domain pairs enables the implementation of\nsampling strategies that prevent confusion between site-related and biological\nvariabilities. During inference, the model can be applied to any image, even\nfrom an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$'$s disease (AD),\nand the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities. IGUANe is available at\nhttps://github.com/RocaVincent/iguane_harmonization.git.\n", "link": "http://arxiv.org/abs/2402.03227v4", "date": "2024-11-14", "relevancy": 2.6786, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5358}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGUANe%3A%20a%203D%20generalizable%20CycleGAN%20for%20multicenter%20harmonization%20of%0A%20%20brain%20MR%20images&body=Title%3A%20IGUANe%3A%20a%203D%20generalizable%20CycleGAN%20for%20multicenter%20harmonization%20of%0A%20%20brain%20MR%20images%0AAuthor%3A%20Vincent%20Roca%20and%20Gr%C3%A9gory%20Kuchcinski%20and%20Jean-Pierre%20Pruvo%20and%20Dorian%20Manouvriez%20and%20Renaud%20Lopes%0AAbstract%3A%20%20%20In%20MRI%20studies%2C%20the%20aggregation%20of%20imaging%20data%20from%20multiple%20acquisition%0Asites%20enhances%20sample%20size%20but%20may%20introduce%20site-related%20variabilities%20that%0Ahinder%20consistency%20in%20subsequent%20analyses.%20Deep%20learning%20methods%20for%20image%0Atranslation%20have%20emerged%20as%20a%20solution%20for%20harmonizing%20MR%20images%20across%20sites.%0AIn%20this%20study%2C%20we%20introduce%20IGUANe%20%28Image%20Generation%20with%20Unified%20Adversarial%0ANetworks%29%2C%20an%20original%203D%20model%20that%20leverages%20the%20strengths%20of%20domain%0Atranslation%20and%20straightforward%20application%20of%20style%20transfer%20methods%20for%0Amulticenter%20brain%20MR%20image%20harmonization.%20IGUANe%20extends%20CycleGAN%20by%0Aintegrating%20an%20arbitrary%20number%20of%20domains%20for%20training%20through%20a%20many-to-one%0Aarchitecture.%20The%20framework%20based%20on%20domain%20pairs%20enables%20the%20implementation%20of%0Asampling%20strategies%20that%20prevent%20confusion%20between%20site-related%20and%20biological%0Avariabilities.%20During%20inference%2C%20the%20model%20can%20be%20applied%20to%20any%20image%2C%20even%0Afrom%20an%20unknown%20acquisition%20site%2C%20making%20it%20a%20universal%20generator%20for%0Aharmonization.%20Trained%20on%20a%20dataset%20comprising%20T1-weighted%20images%20from%2011%0Adifferent%20scanners%2C%20IGUANe%20was%20evaluated%20on%20data%20from%20unseen%20sites.%20The%0Aassessments%20included%20the%20transformation%20of%20MR%20images%20with%20traveling%20subjects%2C%0Athe%20preservation%20of%20pairwise%20distances%20between%20MR%20images%20within%20domains%2C%20the%0Aevolution%20of%20volumetric%20patterns%20related%20to%20age%20and%20Alzheimer%24%27%24s%20disease%20%28AD%29%2C%0Aand%20the%20performance%20in%20age%20regression%20and%20patient%20classification%20tasks.%0AComparisons%20with%20other%20harmonization%20and%20normalization%20methods%20suggest%20that%0AIGUANe%20better%20preserves%20individual%20information%20in%20MR%20images%20and%20is%20more%0Asuitable%20for%20maintaining%20and%20reinforcing%20variabilities%20related%20to%20age%20and%20AD.%0AFuture%20studies%20may%20further%20assess%20IGUANe%20in%20other%20multicenter%20contexts%2C%20either%0Ausing%20the%20same%20model%20or%20retraining%20it%20for%20applications%20to%20different%20image%0Amodalities.%20IGUANe%20is%20available%20at%0Ahttps%3A//github.com/RocaVincent/iguane_harmonization.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03227v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGUANe%253A%2520a%25203D%2520generalizable%2520CycleGAN%2520for%2520multicenter%2520harmonization%2520of%250A%2520%2520brain%2520MR%2520images%26entry.906535625%3DVincent%2520Roca%2520and%2520Gr%25C3%25A9gory%2520Kuchcinski%2520and%2520Jean-Pierre%2520Pruvo%2520and%2520Dorian%2520Manouvriez%2520and%2520Renaud%2520Lopes%26entry.1292438233%3D%2520%2520In%2520MRI%2520studies%252C%2520the%2520aggregation%2520of%2520imaging%2520data%2520from%2520multiple%2520acquisition%250Asites%2520enhances%2520sample%2520size%2520but%2520may%2520introduce%2520site-related%2520variabilities%2520that%250Ahinder%2520consistency%2520in%2520subsequent%2520analyses.%2520Deep%2520learning%2520methods%2520for%2520image%250Atranslation%2520have%2520emerged%2520as%2520a%2520solution%2520for%2520harmonizing%2520MR%2520images%2520across%2520sites.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520IGUANe%2520%2528Image%2520Generation%2520with%2520Unified%2520Adversarial%250ANetworks%2529%252C%2520an%2520original%25203D%2520model%2520that%2520leverages%2520the%2520strengths%2520of%2520domain%250Atranslation%2520and%2520straightforward%2520application%2520of%2520style%2520transfer%2520methods%2520for%250Amulticenter%2520brain%2520MR%2520image%2520harmonization.%2520IGUANe%2520extends%2520CycleGAN%2520by%250Aintegrating%2520an%2520arbitrary%2520number%2520of%2520domains%2520for%2520training%2520through%2520a%2520many-to-one%250Aarchitecture.%2520The%2520framework%2520based%2520on%2520domain%2520pairs%2520enables%2520the%2520implementation%2520of%250Asampling%2520strategies%2520that%2520prevent%2520confusion%2520between%2520site-related%2520and%2520biological%250Avariabilities.%2520During%2520inference%252C%2520the%2520model%2520can%2520be%2520applied%2520to%2520any%2520image%252C%2520even%250Afrom%2520an%2520unknown%2520acquisition%2520site%252C%2520making%2520it%2520a%2520universal%2520generator%2520for%250Aharmonization.%2520Trained%2520on%2520a%2520dataset%2520comprising%2520T1-weighted%2520images%2520from%252011%250Adifferent%2520scanners%252C%2520IGUANe%2520was%2520evaluated%2520on%2520data%2520from%2520unseen%2520sites.%2520The%250Aassessments%2520included%2520the%2520transformation%2520of%2520MR%2520images%2520with%2520traveling%2520subjects%252C%250Athe%2520preservation%2520of%2520pairwise%2520distances%2520between%2520MR%2520images%2520within%2520domains%252C%2520the%250Aevolution%2520of%2520volumetric%2520patterns%2520related%2520to%2520age%2520and%2520Alzheimer%2524%2527%2524s%2520disease%2520%2528AD%2529%252C%250Aand%2520the%2520performance%2520in%2520age%2520regression%2520and%2520patient%2520classification%2520tasks.%250AComparisons%2520with%2520other%2520harmonization%2520and%2520normalization%2520methods%2520suggest%2520that%250AIGUANe%2520better%2520preserves%2520individual%2520information%2520in%2520MR%2520images%2520and%2520is%2520more%250Asuitable%2520for%2520maintaining%2520and%2520reinforcing%2520variabilities%2520related%2520to%2520age%2520and%2520AD.%250AFuture%2520studies%2520may%2520further%2520assess%2520IGUANe%2520in%2520other%2520multicenter%2520contexts%252C%2520either%250Ausing%2520the%2520same%2520model%2520or%2520retraining%2520it%2520for%2520applications%2520to%2520different%2520image%250Amodalities.%2520IGUANe%2520is%2520available%2520at%250Ahttps%253A//github.com/RocaVincent/iguane_harmonization.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03227v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGUANe%3A%20a%203D%20generalizable%20CycleGAN%20for%20multicenter%20harmonization%20of%0A%20%20brain%20MR%20images&entry.906535625=Vincent%20Roca%20and%20Gr%C3%A9gory%20Kuchcinski%20and%20Jean-Pierre%20Pruvo%20and%20Dorian%20Manouvriez%20and%20Renaud%20Lopes&entry.1292438233=%20%20In%20MRI%20studies%2C%20the%20aggregation%20of%20imaging%20data%20from%20multiple%20acquisition%0Asites%20enhances%20sample%20size%20but%20may%20introduce%20site-related%20variabilities%20that%0Ahinder%20consistency%20in%20subsequent%20analyses.%20Deep%20learning%20methods%20for%20image%0Atranslation%20have%20emerged%20as%20a%20solution%20for%20harmonizing%20MR%20images%20across%20sites.%0AIn%20this%20study%2C%20we%20introduce%20IGUANe%20%28Image%20Generation%20with%20Unified%20Adversarial%0ANetworks%29%2C%20an%20original%203D%20model%20that%20leverages%20the%20strengths%20of%20domain%0Atranslation%20and%20straightforward%20application%20of%20style%20transfer%20methods%20for%0Amulticenter%20brain%20MR%20image%20harmonization.%20IGUANe%20extends%20CycleGAN%20by%0Aintegrating%20an%20arbitrary%20number%20of%20domains%20for%20training%20through%20a%20many-to-one%0Aarchitecture.%20The%20framework%20based%20on%20domain%20pairs%20enables%20the%20implementation%20of%0Asampling%20strategies%20that%20prevent%20confusion%20between%20site-related%20and%20biological%0Avariabilities.%20During%20inference%2C%20the%20model%20can%20be%20applied%20to%20any%20image%2C%20even%0Afrom%20an%20unknown%20acquisition%20site%2C%20making%20it%20a%20universal%20generator%20for%0Aharmonization.%20Trained%20on%20a%20dataset%20comprising%20T1-weighted%20images%20from%2011%0Adifferent%20scanners%2C%20IGUANe%20was%20evaluated%20on%20data%20from%20unseen%20sites.%20The%0Aassessments%20included%20the%20transformation%20of%20MR%20images%20with%20traveling%20subjects%2C%0Athe%20preservation%20of%20pairwise%20distances%20between%20MR%20images%20within%20domains%2C%20the%0Aevolution%20of%20volumetric%20patterns%20related%20to%20age%20and%20Alzheimer%24%27%24s%20disease%20%28AD%29%2C%0Aand%20the%20performance%20in%20age%20regression%20and%20patient%20classification%20tasks.%0AComparisons%20with%20other%20harmonization%20and%20normalization%20methods%20suggest%20that%0AIGUANe%20better%20preserves%20individual%20information%20in%20MR%20images%20and%20is%20more%0Asuitable%20for%20maintaining%20and%20reinforcing%20variabilities%20related%20to%20age%20and%20AD.%0AFuture%20studies%20may%20further%20assess%20IGUANe%20in%20other%20multicenter%20contexts%2C%20either%0Ausing%20the%20same%20model%20or%20retraining%20it%20for%20applications%20to%20different%20image%0Amodalities.%20IGUANe%20is%20available%20at%0Ahttps%3A//github.com/RocaVincent/iguane_harmonization.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03227v4&entry.124074799=Read"},
{"title": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System", "author": "Wen-Dong Jiang and Chih-Yung Chang and Ssu-Chi Kuai and Diptendu Sinha Roy", "abstract": "  Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, most of them were\nblack-box systems which faced challenges regarding explainability during\ntraining and inference processes. An important question is how to incorporate\nexplicit knowledge into these implicit models, thereby designing expertdriven\nand interpretable violence surveillance systems. This paper proposes a new\nparadigm for weakly supervised violence monitoring (WSVM) called Rule base\nViolence Monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure\nwith different designs for images and text. One of the branches is called the\nimplicit branch, which uses only visual features for coarse-grained binary\nclassification. In this branch, image feature extraction is divided into two\nchannels: one responsible for extracting scene frames and the other focusing on\nextracting actions. The other branch is called the explicit branch, which\nutilizes language-image alignment to perform fine-grained classification. For\nthe language channel design in the explicit branch, the proposed RuleVM uses\nthe state-of-the-art YOLOWorld model to detect objects in video frames, and\nassociation rules are identified through data mining methods as descriptions of\nthe video. Leveraging the dual-branch architecture, RuleVM achieves\ninterpretable coarse-grained and fine-grained violence surveillance. Extensive\nexperiments were conducted on two commonly used benchmarks, and the results\nshow that RuleVM achieved the best performance in both coarse-grained and\nfinegrained monitoring, significantly outperforming existing state-ofthe-art\nmethods. Moreover, interpretability experiments uncovered some interesting\nrules, such as the observation that as the number of people increases, the risk\nlevel of violent behavior also rises.\n", "link": "http://arxiv.org/abs/2410.21991v5", "date": "2024-11-14", "relevancy": 2.6741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System&body=Title%3A%20From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System%0AAuthor%3A%20Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Ssu-Chi%20Kuai%20and%20Diptendu%20Sinha%20Roy%0AAbstract%3A%20%20%20Recently%2C%20research%20based%20on%20pre-trained%20models%20has%20demonstrated%20outstanding%0Aperformance%20in%20violence%20surveillance%20tasks.%20However%2C%20most%20of%20them%20were%0Ablack-box%20systems%20which%20faced%20challenges%20regarding%20explainability%20during%0Atraining%20and%20inference%20processes.%20An%20important%20question%20is%20how%20to%20incorporate%0Aexplicit%20knowledge%20into%20these%20implicit%20models%2C%20thereby%20designing%20expertdriven%0Aand%20interpretable%20violence%20surveillance%20systems.%20This%20paper%20proposes%20a%20new%0Aparadigm%20for%20weakly%20supervised%20violence%20monitoring%20%28WSVM%29%20called%20Rule%20base%0AViolence%20Monitoring%20%28RuleVM%29.%20The%20proposed%20RuleVM%20uses%20a%20dual-branch%20structure%0Awith%20different%20designs%20for%20images%20and%20text.%20One%20of%20the%20branches%20is%20called%20the%0Aimplicit%20branch%2C%20which%20uses%20only%20visual%20features%20for%20coarse-grained%20binary%0Aclassification.%20In%20this%20branch%2C%20image%20feature%20extraction%20is%20divided%20into%20two%0Achannels%3A%20one%20responsible%20for%20extracting%20scene%20frames%20and%20the%20other%20focusing%20on%0Aextracting%20actions.%20The%20other%20branch%20is%20called%20the%20explicit%20branch%2C%20which%0Autilizes%20language-image%20alignment%20to%20perform%20fine-grained%20classification.%20For%0Athe%20language%20channel%20design%20in%20the%20explicit%20branch%2C%20the%20proposed%20RuleVM%20uses%0Athe%20state-of-the-art%20YOLOWorld%20model%20to%20detect%20objects%20in%20video%20frames%2C%20and%0Aassociation%20rules%20are%20identified%20through%20data%20mining%20methods%20as%20descriptions%20of%0Athe%20video.%20Leveraging%20the%20dual-branch%20architecture%2C%20RuleVM%20achieves%0Ainterpretable%20coarse-grained%20and%20fine-grained%20violence%20surveillance.%20Extensive%0Aexperiments%20were%20conducted%20on%20two%20commonly%20used%20benchmarks%2C%20and%20the%20results%0Ashow%20that%20RuleVM%20achieved%20the%20best%20performance%20in%20both%20coarse-grained%20and%0Afinegrained%20monitoring%2C%20significantly%20outperforming%20existing%20state-ofthe-art%0Amethods.%20Moreover%2C%20interpretability%20experiments%20uncovered%20some%20interesting%0Arules%2C%20such%20as%20the%20observation%20that%20as%20the%20number%20of%20people%20increases%2C%20the%20risk%0Alevel%20of%20violent%20behavior%20also%20rises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21991v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Explicit%2520Rules%2520to%2520Implicit%2520Reasoning%2520in%2520an%2520Interpretable%2520Violence%250A%2520%2520Monitoring%2520System%26entry.906535625%3DWen-Dong%2520Jiang%2520and%2520Chih-Yung%2520Chang%2520and%2520Ssu-Chi%2520Kuai%2520and%2520Diptendu%2520Sinha%2520Roy%26entry.1292438233%3D%2520%2520Recently%252C%2520research%2520based%2520on%2520pre-trained%2520models%2520has%2520demonstrated%2520outstanding%250Aperformance%2520in%2520violence%2520surveillance%2520tasks.%2520However%252C%2520most%2520of%2520them%2520were%250Ablack-box%2520systems%2520which%2520faced%2520challenges%2520regarding%2520explainability%2520during%250Atraining%2520and%2520inference%2520processes.%2520An%2520important%2520question%2520is%2520how%2520to%2520incorporate%250Aexplicit%2520knowledge%2520into%2520these%2520implicit%2520models%252C%2520thereby%2520designing%2520expertdriven%250Aand%2520interpretable%2520violence%2520surveillance%2520systems.%2520This%2520paper%2520proposes%2520a%2520new%250Aparadigm%2520for%2520weakly%2520supervised%2520violence%2520monitoring%2520%2528WSVM%2529%2520called%2520Rule%2520base%250AViolence%2520Monitoring%2520%2528RuleVM%2529.%2520The%2520proposed%2520RuleVM%2520uses%2520a%2520dual-branch%2520structure%250Awith%2520different%2520designs%2520for%2520images%2520and%2520text.%2520One%2520of%2520the%2520branches%2520is%2520called%2520the%250Aimplicit%2520branch%252C%2520which%2520uses%2520only%2520visual%2520features%2520for%2520coarse-grained%2520binary%250Aclassification.%2520In%2520this%2520branch%252C%2520image%2520feature%2520extraction%2520is%2520divided%2520into%2520two%250Achannels%253A%2520one%2520responsible%2520for%2520extracting%2520scene%2520frames%2520and%2520the%2520other%2520focusing%2520on%250Aextracting%2520actions.%2520The%2520other%2520branch%2520is%2520called%2520the%2520explicit%2520branch%252C%2520which%250Autilizes%2520language-image%2520alignment%2520to%2520perform%2520fine-grained%2520classification.%2520For%250Athe%2520language%2520channel%2520design%2520in%2520the%2520explicit%2520branch%252C%2520the%2520proposed%2520RuleVM%2520uses%250Athe%2520state-of-the-art%2520YOLOWorld%2520model%2520to%2520detect%2520objects%2520in%2520video%2520frames%252C%2520and%250Aassociation%2520rules%2520are%2520identified%2520through%2520data%2520mining%2520methods%2520as%2520descriptions%2520of%250Athe%2520video.%2520Leveraging%2520the%2520dual-branch%2520architecture%252C%2520RuleVM%2520achieves%250Ainterpretable%2520coarse-grained%2520and%2520fine-grained%2520violence%2520surveillance.%2520Extensive%250Aexperiments%2520were%2520conducted%2520on%2520two%2520commonly%2520used%2520benchmarks%252C%2520and%2520the%2520results%250Ashow%2520that%2520RuleVM%2520achieved%2520the%2520best%2520performance%2520in%2520both%2520coarse-grained%2520and%250Afinegrained%2520monitoring%252C%2520significantly%2520outperforming%2520existing%2520state-ofthe-art%250Amethods.%2520Moreover%252C%2520interpretability%2520experiments%2520uncovered%2520some%2520interesting%250Arules%252C%2520such%2520as%2520the%2520observation%2520that%2520as%2520the%2520number%2520of%2520people%2520increases%252C%2520the%2520risk%250Alevel%2520of%2520violent%2520behavior%2520also%2520rises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21991v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Explicit%20Rules%20to%20Implicit%20Reasoning%20in%20an%20Interpretable%20Violence%0A%20%20Monitoring%20System&entry.906535625=Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Ssu-Chi%20Kuai%20and%20Diptendu%20Sinha%20Roy&entry.1292438233=%20%20Recently%2C%20research%20based%20on%20pre-trained%20models%20has%20demonstrated%20outstanding%0Aperformance%20in%20violence%20surveillance%20tasks.%20However%2C%20most%20of%20them%20were%0Ablack-box%20systems%20which%20faced%20challenges%20regarding%20explainability%20during%0Atraining%20and%20inference%20processes.%20An%20important%20question%20is%20how%20to%20incorporate%0Aexplicit%20knowledge%20into%20these%20implicit%20models%2C%20thereby%20designing%20expertdriven%0Aand%20interpretable%20violence%20surveillance%20systems.%20This%20paper%20proposes%20a%20new%0Aparadigm%20for%20weakly%20supervised%20violence%20monitoring%20%28WSVM%29%20called%20Rule%20base%0AViolence%20Monitoring%20%28RuleVM%29.%20The%20proposed%20RuleVM%20uses%20a%20dual-branch%20structure%0Awith%20different%20designs%20for%20images%20and%20text.%20One%20of%20the%20branches%20is%20called%20the%0Aimplicit%20branch%2C%20which%20uses%20only%20visual%20features%20for%20coarse-grained%20binary%0Aclassification.%20In%20this%20branch%2C%20image%20feature%20extraction%20is%20divided%20into%20two%0Achannels%3A%20one%20responsible%20for%20extracting%20scene%20frames%20and%20the%20other%20focusing%20on%0Aextracting%20actions.%20The%20other%20branch%20is%20called%20the%20explicit%20branch%2C%20which%0Autilizes%20language-image%20alignment%20to%20perform%20fine-grained%20classification.%20For%0Athe%20language%20channel%20design%20in%20the%20explicit%20branch%2C%20the%20proposed%20RuleVM%20uses%0Athe%20state-of-the-art%20YOLOWorld%20model%20to%20detect%20objects%20in%20video%20frames%2C%20and%0Aassociation%20rules%20are%20identified%20through%20data%20mining%20methods%20as%20descriptions%20of%0Athe%20video.%20Leveraging%20the%20dual-branch%20architecture%2C%20RuleVM%20achieves%0Ainterpretable%20coarse-grained%20and%20fine-grained%20violence%20surveillance.%20Extensive%0Aexperiments%20were%20conducted%20on%20two%20commonly%20used%20benchmarks%2C%20and%20the%20results%0Ashow%20that%20RuleVM%20achieved%20the%20best%20performance%20in%20both%20coarse-grained%20and%0Afinegrained%20monitoring%2C%20significantly%20outperforming%20existing%20state-ofthe-art%0Amethods.%20Moreover%2C%20interpretability%20experiments%20uncovered%20some%20interesting%0Arules%2C%20such%20as%20the%20observation%20that%20as%20the%20number%20of%20people%20increases%2C%20the%20risk%0Alevel%20of%20violent%20behavior%20also%20rises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21991v5&entry.124074799=Read"},
{"title": "Spider: Any-to-Many Multimodal LLM", "author": "Jinxiang Lai and Jie Zhang and Jun Liu and Jian Li and Xiaocheng Lu and Song Guo", "abstract": "  Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field.\n", "link": "http://arxiv.org/abs/2411.09439v1", "date": "2024-11-14", "relevancy": 2.6008, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spider%3A%20Any-to-Many%20Multimodal%20LLM&body=Title%3A%20Spider%3A%20Any-to-Many%20Multimodal%20LLM%0AAuthor%3A%20Jinxiang%20Lai%20and%20Jie%20Zhang%20and%20Jun%20Liu%20and%20Jian%20Li%20and%20Xiaocheng%20Lu%20and%20Song%20Guo%0AAbstract%3A%20%20%20Multimodal%20LLMs%20%28MLLMs%29%20have%20emerged%20as%20an%20extension%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20enabling%20the%20integration%20of%20various%20modalities.%20However%2C%20Any-to-Any%0AMLLMs%20are%20limited%20to%20generating%20pairwise%20modalities%20%27Text%20%2B%20X%27%20within%20a%20single%0Aresponse%2C%20such%20as%20Text%20%2B%20%7BImage%20or%20Audio%20or%20Video%7D.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20Spider%2C%20a%20novel%20efficient%20Any-to-Many%20Modalities%20Generation%20%28AMMG%29%0Aframework%2C%20which%20can%20generate%20an%20arbitrary%20combination%20of%20modalities%20%27Text%20%2B%0AXs%27%2C%20such%20as%20Text%20%2B%20%7BImage%20and%20Audio%20and%20Video%7D.%20To%20achieve%20efficient%20AMMG%2C%20our%0ASpider%20integrates%20three%20core%20components%3A%20a%20Base%20Model%20for%20basic%20X-to-X%20%28i.e.%2C%0AAny-to-Any%29%20modality%20processing%2C%20a%20novel%20Efficient%20Decoders-Controller%20for%0Acontrolling%20multimodal%20Decoders%20to%20generate%20Xs%20%28many-modal%29%20contents%2C%20and%20an%0AAny-to-Many%20Instruction%20Template%20designed%20for%20producing%20Xs%20signal%20prompts.%20To%0Atrain%20Spider%2C%20we%20constructed%20a%20novel%20Text-formatted%20Many-Modal%20%28TMM%29%20dataset%2C%0Awhich%20facilitates%20the%20learning%20of%20the%20X-to-Xs%20%28i.e.%2C%20Any-to-Many%29%20capability%0Anecessary%20for%20AMMG.%20Ultimately%2C%20the%20well-trained%20Spider%20generates%20a%20pseudo%0AX-to-Xs%20dataset%2C%20the%20first-ever%20X-to-Xs%20many-modal%20dataset%2C%20enhancing%20the%0Apotential%20for%20AMMG%20task%20in%20future%20research.%20Overall%2C%20this%20work%20not%20only%20pushes%0Athe%20boundary%20of%20multimodal%20interaction%20but%20also%20provides%20rich%20data%20support%20for%0Aadvancing%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpider%253A%2520Any-to-Many%2520Multimodal%2520LLM%26entry.906535625%3DJinxiang%2520Lai%2520and%2520Jie%2520Zhang%2520and%2520Jun%2520Liu%2520and%2520Jian%2520Li%2520and%2520Xiaocheng%2520Lu%2520and%2520Song%2520Guo%26entry.1292438233%3D%2520%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520an%2520extension%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520enabling%2520the%2520integration%2520of%2520various%2520modalities.%2520However%252C%2520Any-to-Any%250AMLLMs%2520are%2520limited%2520to%2520generating%2520pairwise%2520modalities%2520%2527Text%2520%252B%2520X%2527%2520within%2520a%2520single%250Aresponse%252C%2520such%2520as%2520Text%2520%252B%2520%257BImage%2520or%2520Audio%2520or%2520Video%257D.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520introduce%2520Spider%252C%2520a%2520novel%2520efficient%2520Any-to-Many%2520Modalities%2520Generation%2520%2528AMMG%2529%250Aframework%252C%2520which%2520can%2520generate%2520an%2520arbitrary%2520combination%2520of%2520modalities%2520%2527Text%2520%252B%250AXs%2527%252C%2520such%2520as%2520Text%2520%252B%2520%257BImage%2520and%2520Audio%2520and%2520Video%257D.%2520To%2520achieve%2520efficient%2520AMMG%252C%2520our%250ASpider%2520integrates%2520three%2520core%2520components%253A%2520a%2520Base%2520Model%2520for%2520basic%2520X-to-X%2520%2528i.e.%252C%250AAny-to-Any%2529%2520modality%2520processing%252C%2520a%2520novel%2520Efficient%2520Decoders-Controller%2520for%250Acontrolling%2520multimodal%2520Decoders%2520to%2520generate%2520Xs%2520%2528many-modal%2529%2520contents%252C%2520and%2520an%250AAny-to-Many%2520Instruction%2520Template%2520designed%2520for%2520producing%2520Xs%2520signal%2520prompts.%2520To%250Atrain%2520Spider%252C%2520we%2520constructed%2520a%2520novel%2520Text-formatted%2520Many-Modal%2520%2528TMM%2529%2520dataset%252C%250Awhich%2520facilitates%2520the%2520learning%2520of%2520the%2520X-to-Xs%2520%2528i.e.%252C%2520Any-to-Many%2529%2520capability%250Anecessary%2520for%2520AMMG.%2520Ultimately%252C%2520the%2520well-trained%2520Spider%2520generates%2520a%2520pseudo%250AX-to-Xs%2520dataset%252C%2520the%2520first-ever%2520X-to-Xs%2520many-modal%2520dataset%252C%2520enhancing%2520the%250Apotential%2520for%2520AMMG%2520task%2520in%2520future%2520research.%2520Overall%252C%2520this%2520work%2520not%2520only%2520pushes%250Athe%2520boundary%2520of%2520multimodal%2520interaction%2520but%2520also%2520provides%2520rich%2520data%2520support%2520for%250Aadvancing%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spider%3A%20Any-to-Many%20Multimodal%20LLM&entry.906535625=Jinxiang%20Lai%20and%20Jie%20Zhang%20and%20Jun%20Liu%20and%20Jian%20Li%20and%20Xiaocheng%20Lu%20and%20Song%20Guo&entry.1292438233=%20%20Multimodal%20LLMs%20%28MLLMs%29%20have%20emerged%20as%20an%20extension%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20enabling%20the%20integration%20of%20various%20modalities.%20However%2C%20Any-to-Any%0AMLLMs%20are%20limited%20to%20generating%20pairwise%20modalities%20%27Text%20%2B%20X%27%20within%20a%20single%0Aresponse%2C%20such%20as%20Text%20%2B%20%7BImage%20or%20Audio%20or%20Video%7D.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20Spider%2C%20a%20novel%20efficient%20Any-to-Many%20Modalities%20Generation%20%28AMMG%29%0Aframework%2C%20which%20can%20generate%20an%20arbitrary%20combination%20of%20modalities%20%27Text%20%2B%0AXs%27%2C%20such%20as%20Text%20%2B%20%7BImage%20and%20Audio%20and%20Video%7D.%20To%20achieve%20efficient%20AMMG%2C%20our%0ASpider%20integrates%20three%20core%20components%3A%20a%20Base%20Model%20for%20basic%20X-to-X%20%28i.e.%2C%0AAny-to-Any%29%20modality%20processing%2C%20a%20novel%20Efficient%20Decoders-Controller%20for%0Acontrolling%20multimodal%20Decoders%20to%20generate%20Xs%20%28many-modal%29%20contents%2C%20and%20an%0AAny-to-Many%20Instruction%20Template%20designed%20for%20producing%20Xs%20signal%20prompts.%20To%0Atrain%20Spider%2C%20we%20constructed%20a%20novel%20Text-formatted%20Many-Modal%20%28TMM%29%20dataset%2C%0Awhich%20facilitates%20the%20learning%20of%20the%20X-to-Xs%20%28i.e.%2C%20Any-to-Many%29%20capability%0Anecessary%20for%20AMMG.%20Ultimately%2C%20the%20well-trained%20Spider%20generates%20a%20pseudo%0AX-to-Xs%20dataset%2C%20the%20first-ever%20X-to-Xs%20many-modal%20dataset%2C%20enhancing%20the%0Apotential%20for%20AMMG%20task%20in%20future%20research.%20Overall%2C%20this%20work%20not%20only%20pushes%0Athe%20boundary%20of%20multimodal%20interaction%20but%20also%20provides%20rich%20data%20support%20for%0Aadvancing%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09439v1&entry.124074799=Read"},
{"title": "V2X Cooperative Perception for Autonomous Driving: Recent Advances and\n  Challenges", "author": "Tao Huang and Jianan Liu and Xi Zhou and Dinh C. Nguyen and Mostafa Rahimi Azghadi and Yuxuan Xia and Qing-Long Han and Sumei Sun", "abstract": "  Achieving fully autonomous driving with heightened safety and efficiency\ndepends on vehicle-to-everything (V2X) cooperative perception (CP), which\nallows vehicles to share perception data, thereby enhancing situational\nawareness and overcoming the limitations of the sensing ability of individual\nvehicles. V2X CP is crucial for extending perception range, improving accuracy,\nand strengthening the decision-making and control capabilities of autonomous\nvehicles in complex environments. This paper provides a comprehensive survey of\nrecent advances in V2X CP, introducing mathematical models of CP processes\nacross various collaboration strategies. We examine essential techniques for\nreliable perception sharing, including agent selection, data alignment, and\nfusion methods. Key issues are analyzed, such as agent and model heterogeneity,\nperception uncertainty, and the impact of V2X communication constraints like\ndelays and data loss on CP effectiveness. To inspire further advancements in\nV2X CP, we outline promising avenues, including privacy-preserving artificial\nintelligence (AI), collaborative AI, and integrated sensing frameworks, as\npathways to enhance CP capabilities.\n", "link": "http://arxiv.org/abs/2310.03525v4", "date": "2024-11-14", "relevancy": 2.5852, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges&body=Title%3A%20V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges%0AAuthor%3A%20Tao%20Huang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Dinh%20C.%20Nguyen%20and%20Mostafa%20Rahimi%20Azghadi%20and%20Yuxuan%20Xia%20and%20Qing-Long%20Han%20and%20Sumei%20Sun%0AAbstract%3A%20%20%20Achieving%20fully%20autonomous%20driving%20with%20heightened%20safety%20and%20efficiency%0Adepends%20on%20vehicle-to-everything%20%28V2X%29%20cooperative%20perception%20%28CP%29%2C%20which%0Aallows%20vehicles%20to%20share%20perception%20data%2C%20thereby%20enhancing%20situational%0Aawareness%20and%20overcoming%20the%20limitations%20of%20the%20sensing%20ability%20of%20individual%0Avehicles.%20V2X%20CP%20is%20crucial%20for%20extending%20perception%20range%2C%20improving%20accuracy%2C%0Aand%20strengthening%20the%20decision-making%20and%20control%20capabilities%20of%20autonomous%0Avehicles%20in%20complex%20environments.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%0Arecent%20advances%20in%20V2X%20CP%2C%20introducing%20mathematical%20models%20of%20CP%20processes%0Aacross%20various%20collaboration%20strategies.%20We%20examine%20essential%20techniques%20for%0Areliable%20perception%20sharing%2C%20including%20agent%20selection%2C%20data%20alignment%2C%20and%0Afusion%20methods.%20Key%20issues%20are%20analyzed%2C%20such%20as%20agent%20and%20model%20heterogeneity%2C%0Aperception%20uncertainty%2C%20and%20the%20impact%20of%20V2X%20communication%20constraints%20like%0Adelays%20and%20data%20loss%20on%20CP%20effectiveness.%20To%20inspire%20further%20advancements%20in%0AV2X%20CP%2C%20we%20outline%20promising%20avenues%2C%20including%20privacy-preserving%20artificial%0Aintelligence%20%28AI%29%2C%20collaborative%20AI%2C%20and%20integrated%20sensing%20frameworks%2C%20as%0Apathways%20to%20enhance%20CP%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03525v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2X%2520Cooperative%2520Perception%2520for%2520Autonomous%2520Driving%253A%2520Recent%2520Advances%2520and%250A%2520%2520Challenges%26entry.906535625%3DTao%2520Huang%2520and%2520Jianan%2520Liu%2520and%2520Xi%2520Zhou%2520and%2520Dinh%2520C.%2520Nguyen%2520and%2520Mostafa%2520Rahimi%2520Azghadi%2520and%2520Yuxuan%2520Xia%2520and%2520Qing-Long%2520Han%2520and%2520Sumei%2520Sun%26entry.1292438233%3D%2520%2520Achieving%2520fully%2520autonomous%2520driving%2520with%2520heightened%2520safety%2520and%2520efficiency%250Adepends%2520on%2520vehicle-to-everything%2520%2528V2X%2529%2520cooperative%2520perception%2520%2528CP%2529%252C%2520which%250Aallows%2520vehicles%2520to%2520share%2520perception%2520data%252C%2520thereby%2520enhancing%2520situational%250Aawareness%2520and%2520overcoming%2520the%2520limitations%2520of%2520the%2520sensing%2520ability%2520of%2520individual%250Avehicles.%2520V2X%2520CP%2520is%2520crucial%2520for%2520extending%2520perception%2520range%252C%2520improving%2520accuracy%252C%250Aand%2520strengthening%2520the%2520decision-making%2520and%2520control%2520capabilities%2520of%2520autonomous%250Avehicles%2520in%2520complex%2520environments.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520survey%2520of%250Arecent%2520advances%2520in%2520V2X%2520CP%252C%2520introducing%2520mathematical%2520models%2520of%2520CP%2520processes%250Aacross%2520various%2520collaboration%2520strategies.%2520We%2520examine%2520essential%2520techniques%2520for%250Areliable%2520perception%2520sharing%252C%2520including%2520agent%2520selection%252C%2520data%2520alignment%252C%2520and%250Afusion%2520methods.%2520Key%2520issues%2520are%2520analyzed%252C%2520such%2520as%2520agent%2520and%2520model%2520heterogeneity%252C%250Aperception%2520uncertainty%252C%2520and%2520the%2520impact%2520of%2520V2X%2520communication%2520constraints%2520like%250Adelays%2520and%2520data%2520loss%2520on%2520CP%2520effectiveness.%2520To%2520inspire%2520further%2520advancements%2520in%250AV2X%2520CP%252C%2520we%2520outline%2520promising%2520avenues%252C%2520including%2520privacy-preserving%2520artificial%250Aintelligence%2520%2528AI%2529%252C%2520collaborative%2520AI%252C%2520and%2520integrated%2520sensing%2520frameworks%252C%2520as%250Apathways%2520to%2520enhance%2520CP%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03525v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges&entry.906535625=Tao%20Huang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Dinh%20C.%20Nguyen%20and%20Mostafa%20Rahimi%20Azghadi%20and%20Yuxuan%20Xia%20and%20Qing-Long%20Han%20and%20Sumei%20Sun&entry.1292438233=%20%20Achieving%20fully%20autonomous%20driving%20with%20heightened%20safety%20and%20efficiency%0Adepends%20on%20vehicle-to-everything%20%28V2X%29%20cooperative%20perception%20%28CP%29%2C%20which%0Aallows%20vehicles%20to%20share%20perception%20data%2C%20thereby%20enhancing%20situational%0Aawareness%20and%20overcoming%20the%20limitations%20of%20the%20sensing%20ability%20of%20individual%0Avehicles.%20V2X%20CP%20is%20crucial%20for%20extending%20perception%20range%2C%20improving%20accuracy%2C%0Aand%20strengthening%20the%20decision-making%20and%20control%20capabilities%20of%20autonomous%0Avehicles%20in%20complex%20environments.%20This%20paper%20provides%20a%20comprehensive%20survey%20of%0Arecent%20advances%20in%20V2X%20CP%2C%20introducing%20mathematical%20models%20of%20CP%20processes%0Aacross%20various%20collaboration%20strategies.%20We%20examine%20essential%20techniques%20for%0Areliable%20perception%20sharing%2C%20including%20agent%20selection%2C%20data%20alignment%2C%20and%0Afusion%20methods.%20Key%20issues%20are%20analyzed%2C%20such%20as%20agent%20and%20model%20heterogeneity%2C%0Aperception%20uncertainty%2C%20and%20the%20impact%20of%20V2X%20communication%20constraints%20like%0Adelays%20and%20data%20loss%20on%20CP%20effectiveness.%20To%20inspire%20further%20advancements%20in%0AV2X%20CP%2C%20we%20outline%20promising%20avenues%2C%20including%20privacy-preserving%20artificial%0Aintelligence%20%28AI%29%2C%20collaborative%20AI%2C%20and%20integrated%20sensing%20frameworks%2C%20as%0Apathways%20to%20enhance%20CP%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03525v4&entry.124074799=Read"},
{"title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons", "author": "Dan Shi and Renren Jin and Tianhao Shen and Weilong Dong and Xinwei Wu and Deyi Xiong", "abstract": "  It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN.\n", "link": "http://arxiv.org/abs/2406.18406v2", "date": "2024-11-14", "relevancy": 2.5467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons&body=Title%3A%20IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons%0AAuthor%3A%20Dan%20Shi%20and%20Renren%20Jin%20and%20Tianhao%20Shen%20and%20Weilong%20Dong%20and%20Xinwei%20Wu%20and%20Deyi%20Xiong%0AAbstract%3A%20%20%20It%20is%20widely%20acknowledged%20that%20large%20language%20models%20%28LLMs%29%20encode%20a%20vast%0Areservoir%20of%20knowledge%20after%20being%20trained%20on%20mass%20data.%20Recent%20studies%0Adisclose%20knowledge%20conflicts%20in%20LLM%20generation%2C%20wherein%20outdated%20or%20incorrect%0Aparametric%20knowledge%20%28i.e.%2C%20encoded%20knowledge%29%20contradicts%20new%20knowledge%0Aprovided%20in%20the%20context.%20To%20mitigate%20such%20knowledge%20conflicts%2C%20we%20propose%20a%0Anovel%20framework%2C%20IRCAN%20%28Identifying%20and%20Reweighting%20Context-Aware%20Neurons%29%20to%0Acapitalize%20on%20neurons%20that%20are%20crucial%20in%20processing%20contextual%20cues.%0ASpecifically%2C%20IRCAN%20first%20identifies%20neurons%20that%20significantly%20contribute%20to%0Acontext%20processing%2C%20utilizing%20a%20context-aware%20attribution%20score%20derived%20from%0Aintegrated%20gradients.%20Subsequently%2C%20the%20identified%20context-aware%20neurons%20are%0Astrengthened%20via%20reweighting.%20In%20doing%20so%2C%20we%20steer%20LLMs%20to%20generate%0Acontext-sensitive%20outputs%20with%20respect%20to%20the%20new%20knowledge%20provided%20in%20the%0Acontext.%20Extensive%20experiments%20conducted%20across%20a%20variety%20of%20models%20and%20tasks%0Ademonstrate%20that%20IRCAN%20not%20only%20achieves%20remarkable%20improvements%20in%20handling%0Aknowledge%20conflicts%20but%20also%20offers%20a%20scalable%2C%20plug-and-play%20solution%20that%20can%0Abe%20integrated%20seamlessly%20with%20existing%20models.%20Our%20codes%20are%20released%20at%0Ahttps%3A//github.com/danshi777/IRCAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRCAN%253A%2520Mitigating%2520Knowledge%2520Conflicts%2520in%2520LLM%2520Generation%2520via%2520Identifying%250A%2520%2520and%2520Reweighting%2520Context-Aware%2520Neurons%26entry.906535625%3DDan%2520Shi%2520and%2520Renren%2520Jin%2520and%2520Tianhao%2520Shen%2520and%2520Weilong%2520Dong%2520and%2520Xinwei%2520Wu%2520and%2520Deyi%2520Xiong%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520acknowledged%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520encode%2520a%2520vast%250Areservoir%2520of%2520knowledge%2520after%2520being%2520trained%2520on%2520mass%2520data.%2520Recent%2520studies%250Adisclose%2520knowledge%2520conflicts%2520in%2520LLM%2520generation%252C%2520wherein%2520outdated%2520or%2520incorrect%250Aparametric%2520knowledge%2520%2528i.e.%252C%2520encoded%2520knowledge%2529%2520contradicts%2520new%2520knowledge%250Aprovided%2520in%2520the%2520context.%2520To%2520mitigate%2520such%2520knowledge%2520conflicts%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520IRCAN%2520%2528Identifying%2520and%2520Reweighting%2520Context-Aware%2520Neurons%2529%2520to%250Acapitalize%2520on%2520neurons%2520that%2520are%2520crucial%2520in%2520processing%2520contextual%2520cues.%250ASpecifically%252C%2520IRCAN%2520first%2520identifies%2520neurons%2520that%2520significantly%2520contribute%2520to%250Acontext%2520processing%252C%2520utilizing%2520a%2520context-aware%2520attribution%2520score%2520derived%2520from%250Aintegrated%2520gradients.%2520Subsequently%252C%2520the%2520identified%2520context-aware%2520neurons%2520are%250Astrengthened%2520via%2520reweighting.%2520In%2520doing%2520so%252C%2520we%2520steer%2520LLMs%2520to%2520generate%250Acontext-sensitive%2520outputs%2520with%2520respect%2520to%2520the%2520new%2520knowledge%2520provided%2520in%2520the%250Acontext.%2520Extensive%2520experiments%2520conducted%2520across%2520a%2520variety%2520of%2520models%2520and%2520tasks%250Ademonstrate%2520that%2520IRCAN%2520not%2520only%2520achieves%2520remarkable%2520improvements%2520in%2520handling%250Aknowledge%2520conflicts%2520but%2520also%2520offers%2520a%2520scalable%252C%2520plug-and-play%2520solution%2520that%2520can%250Abe%2520integrated%2520seamlessly%2520with%2520existing%2520models.%2520Our%2520codes%2520are%2520released%2520at%250Ahttps%253A//github.com/danshi777/IRCAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRCAN%3A%20Mitigating%20Knowledge%20Conflicts%20in%20LLM%20Generation%20via%20Identifying%0A%20%20and%20Reweighting%20Context-Aware%20Neurons&entry.906535625=Dan%20Shi%20and%20Renren%20Jin%20and%20Tianhao%20Shen%20and%20Weilong%20Dong%20and%20Xinwei%20Wu%20and%20Deyi%20Xiong&entry.1292438233=%20%20It%20is%20widely%20acknowledged%20that%20large%20language%20models%20%28LLMs%29%20encode%20a%20vast%0Areservoir%20of%20knowledge%20after%20being%20trained%20on%20mass%20data.%20Recent%20studies%0Adisclose%20knowledge%20conflicts%20in%20LLM%20generation%2C%20wherein%20outdated%20or%20incorrect%0Aparametric%20knowledge%20%28i.e.%2C%20encoded%20knowledge%29%20contradicts%20new%20knowledge%0Aprovided%20in%20the%20context.%20To%20mitigate%20such%20knowledge%20conflicts%2C%20we%20propose%20a%0Anovel%20framework%2C%20IRCAN%20%28Identifying%20and%20Reweighting%20Context-Aware%20Neurons%29%20to%0Acapitalize%20on%20neurons%20that%20are%20crucial%20in%20processing%20contextual%20cues.%0ASpecifically%2C%20IRCAN%20first%20identifies%20neurons%20that%20significantly%20contribute%20to%0Acontext%20processing%2C%20utilizing%20a%20context-aware%20attribution%20score%20derived%20from%0Aintegrated%20gradients.%20Subsequently%2C%20the%20identified%20context-aware%20neurons%20are%0Astrengthened%20via%20reweighting.%20In%20doing%20so%2C%20we%20steer%20LLMs%20to%20generate%0Acontext-sensitive%20outputs%20with%20respect%20to%20the%20new%20knowledge%20provided%20in%20the%0Acontext.%20Extensive%20experiments%20conducted%20across%20a%20variety%20of%20models%20and%20tasks%0Ademonstrate%20that%20IRCAN%20not%20only%20achieves%20remarkable%20improvements%20in%20handling%0Aknowledge%20conflicts%20but%20also%20offers%20a%20scalable%2C%20plug-and-play%20solution%20that%20can%0Abe%20integrated%20seamlessly%20with%20existing%20models.%20Our%20codes%20are%20released%20at%0Ahttps%3A//github.com/danshi777/IRCAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18406v2&entry.124074799=Read"},
{"title": "Building Height Estimation Using Shadow Length in Satellite Imagery", "author": "Mahd Qureshi and Shayaan Chaudhry and Sana Jabba and Murtaza Taj", "abstract": "  Estimating building height from satellite imagery poses significant\nchallenges, especially when monocular images are employed, resulting in a loss\nof essential 3D information during imaging. This loss of spatial depth further\ncomplicates the height estimation process. We addressed this issue by using\nshadow length as an additional cue to compensate for the loss of building\nheight estimation using single-view imagery. We proposed a novel method that\nfirst localized a building and its shadow in the given satellite image. After\nlocalization, the shadow length is estimated using a regression model. To\nestimate the final height of each building, we utilize the principles of\nphotogrammetry, specifically considering the relationship between the solar\nelevation angle, the vertical edge length of the building, and the length of\nthe building's shadow. For the localization of buildings in our model, we\nutilized a modified YOLOv7 detector, and to regress the shadow length for each\nbuilding we utilized the ResNet18 as backbone architecture. Finally, we\nestimated the associated building height using solar elevation with shadow\nlength through analytical formulation. We evaluated our method on 42 different\ncities and the results showed that the proposed framework surpasses the\nstate-of-the-art methods with a suitable margin.\n", "link": "http://arxiv.org/abs/2411.09411v1", "date": "2024-11-14", "relevancy": 2.4871, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5011}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Height%20Estimation%20Using%20Shadow%20Length%20in%20Satellite%20Imagery&body=Title%3A%20Building%20Height%20Estimation%20Using%20Shadow%20Length%20in%20Satellite%20Imagery%0AAuthor%3A%20Mahd%20Qureshi%20and%20Shayaan%20Chaudhry%20and%20Sana%20Jabba%20and%20Murtaza%20Taj%0AAbstract%3A%20%20%20Estimating%20building%20height%20from%20satellite%20imagery%20poses%20significant%0Achallenges%2C%20especially%20when%20monocular%20images%20are%20employed%2C%20resulting%20in%20a%20loss%0Aof%20essential%203D%20information%20during%20imaging.%20This%20loss%20of%20spatial%20depth%20further%0Acomplicates%20the%20height%20estimation%20process.%20We%20addressed%20this%20issue%20by%20using%0Ashadow%20length%20as%20an%20additional%20cue%20to%20compensate%20for%20the%20loss%20of%20building%0Aheight%20estimation%20using%20single-view%20imagery.%20We%20proposed%20a%20novel%20method%20that%0Afirst%20localized%20a%20building%20and%20its%20shadow%20in%20the%20given%20satellite%20image.%20After%0Alocalization%2C%20the%20shadow%20length%20is%20estimated%20using%20a%20regression%20model.%20To%0Aestimate%20the%20final%20height%20of%20each%20building%2C%20we%20utilize%20the%20principles%20of%0Aphotogrammetry%2C%20specifically%20considering%20the%20relationship%20between%20the%20solar%0Aelevation%20angle%2C%20the%20vertical%20edge%20length%20of%20the%20building%2C%20and%20the%20length%20of%0Athe%20building%27s%20shadow.%20For%20the%20localization%20of%20buildings%20in%20our%20model%2C%20we%0Autilized%20a%20modified%20YOLOv7%20detector%2C%20and%20to%20regress%20the%20shadow%20length%20for%20each%0Abuilding%20we%20utilized%20the%20ResNet18%20as%20backbone%20architecture.%20Finally%2C%20we%0Aestimated%20the%20associated%20building%20height%20using%20solar%20elevation%20with%20shadow%0Alength%20through%20analytical%20formulation.%20We%20evaluated%20our%20method%20on%2042%20different%0Acities%20and%20the%20results%20showed%20that%20the%20proposed%20framework%20surpasses%20the%0Astate-of-the-art%20methods%20with%20a%20suitable%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Height%2520Estimation%2520Using%2520Shadow%2520Length%2520in%2520Satellite%2520Imagery%26entry.906535625%3DMahd%2520Qureshi%2520and%2520Shayaan%2520Chaudhry%2520and%2520Sana%2520Jabba%2520and%2520Murtaza%2520Taj%26entry.1292438233%3D%2520%2520Estimating%2520building%2520height%2520from%2520satellite%2520imagery%2520poses%2520significant%250Achallenges%252C%2520especially%2520when%2520monocular%2520images%2520are%2520employed%252C%2520resulting%2520in%2520a%2520loss%250Aof%2520essential%25203D%2520information%2520during%2520imaging.%2520This%2520loss%2520of%2520spatial%2520depth%2520further%250Acomplicates%2520the%2520height%2520estimation%2520process.%2520We%2520addressed%2520this%2520issue%2520by%2520using%250Ashadow%2520length%2520as%2520an%2520additional%2520cue%2520to%2520compensate%2520for%2520the%2520loss%2520of%2520building%250Aheight%2520estimation%2520using%2520single-view%2520imagery.%2520We%2520proposed%2520a%2520novel%2520method%2520that%250Afirst%2520localized%2520a%2520building%2520and%2520its%2520shadow%2520in%2520the%2520given%2520satellite%2520image.%2520After%250Alocalization%252C%2520the%2520shadow%2520length%2520is%2520estimated%2520using%2520a%2520regression%2520model.%2520To%250Aestimate%2520the%2520final%2520height%2520of%2520each%2520building%252C%2520we%2520utilize%2520the%2520principles%2520of%250Aphotogrammetry%252C%2520specifically%2520considering%2520the%2520relationship%2520between%2520the%2520solar%250Aelevation%2520angle%252C%2520the%2520vertical%2520edge%2520length%2520of%2520the%2520building%252C%2520and%2520the%2520length%2520of%250Athe%2520building%2527s%2520shadow.%2520For%2520the%2520localization%2520of%2520buildings%2520in%2520our%2520model%252C%2520we%250Autilized%2520a%2520modified%2520YOLOv7%2520detector%252C%2520and%2520to%2520regress%2520the%2520shadow%2520length%2520for%2520each%250Abuilding%2520we%2520utilized%2520the%2520ResNet18%2520as%2520backbone%2520architecture.%2520Finally%252C%2520we%250Aestimated%2520the%2520associated%2520building%2520height%2520using%2520solar%2520elevation%2520with%2520shadow%250Alength%2520through%2520analytical%2520formulation.%2520We%2520evaluated%2520our%2520method%2520on%252042%2520different%250Acities%2520and%2520the%2520results%2520showed%2520that%2520the%2520proposed%2520framework%2520surpasses%2520the%250Astate-of-the-art%2520methods%2520with%2520a%2520suitable%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Height%20Estimation%20Using%20Shadow%20Length%20in%20Satellite%20Imagery&entry.906535625=Mahd%20Qureshi%20and%20Shayaan%20Chaudhry%20and%20Sana%20Jabba%20and%20Murtaza%20Taj&entry.1292438233=%20%20Estimating%20building%20height%20from%20satellite%20imagery%20poses%20significant%0Achallenges%2C%20especially%20when%20monocular%20images%20are%20employed%2C%20resulting%20in%20a%20loss%0Aof%20essential%203D%20information%20during%20imaging.%20This%20loss%20of%20spatial%20depth%20further%0Acomplicates%20the%20height%20estimation%20process.%20We%20addressed%20this%20issue%20by%20using%0Ashadow%20length%20as%20an%20additional%20cue%20to%20compensate%20for%20the%20loss%20of%20building%0Aheight%20estimation%20using%20single-view%20imagery.%20We%20proposed%20a%20novel%20method%20that%0Afirst%20localized%20a%20building%20and%20its%20shadow%20in%20the%20given%20satellite%20image.%20After%0Alocalization%2C%20the%20shadow%20length%20is%20estimated%20using%20a%20regression%20model.%20To%0Aestimate%20the%20final%20height%20of%20each%20building%2C%20we%20utilize%20the%20principles%20of%0Aphotogrammetry%2C%20specifically%20considering%20the%20relationship%20between%20the%20solar%0Aelevation%20angle%2C%20the%20vertical%20edge%20length%20of%20the%20building%2C%20and%20the%20length%20of%0Athe%20building%27s%20shadow.%20For%20the%20localization%20of%20buildings%20in%20our%20model%2C%20we%0Autilized%20a%20modified%20YOLOv7%20detector%2C%20and%20to%20regress%20the%20shadow%20length%20for%20each%0Abuilding%20we%20utilized%20the%20ResNet18%20as%20backbone%20architecture.%20Finally%2C%20we%0Aestimated%20the%20associated%20building%20height%20using%20solar%20elevation%20with%20shadow%0Alength%20through%20analytical%20formulation.%20We%20evaluated%20our%20method%20on%2042%20different%0Acities%20and%20the%20results%20showed%20that%20the%20proposed%20framework%20surpasses%20the%0Astate-of-the-art%20methods%20with%20a%20suitable%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09411v1&entry.124074799=Read"},
{"title": "MotionDreamer: Exploring Semantic Video Diffusion features for Zero-Shot\n  3D Mesh Animation", "author": "Lukas Uzolas and Elmar Eisemann and Petr Kellnhofer", "abstract": "  Animation techniques bring digital 3D worlds and characters to life. However,\nmanual animation is tedious and automated techniques are often specialized to\nnarrow shape classes. In our work, we propose a technique for automatic\nre-animation of various 3D shapes based on a motion prior extracted from a\nvideo diffusion model. Unlike existing 4D generation methods, we focus solely\non the motion, and we leverage an explicit mesh-based representation compatible\nwith existing computer-graphics pipelines. Furthermore, our utilization of\ndiffusion features enhances accuracy of our motion fitting. We analyze efficacy\nof these features for animation fitting and we experimentally validate our\napproach for two different diffusion models and four animation models. Finally,\nwe demonstrate that our time-efficient zero-shot method achieves a superior\nperformance re-animating a diverse set of 3D shapes when compared to existing\ntechniques in a user study. The project website is located at\nhttps://lukas.uzolas.com/MotionDreamer.\n", "link": "http://arxiv.org/abs/2405.20155v2", "date": "2024-11-14", "relevancy": 2.4765, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6582}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6292}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionDreamer%3A%20Exploring%20Semantic%20Video%20Diffusion%20features%20for%20Zero-Shot%0A%20%203D%20Mesh%20Animation&body=Title%3A%20MotionDreamer%3A%20Exploring%20Semantic%20Video%20Diffusion%20features%20for%20Zero-Shot%0A%20%203D%20Mesh%20Animation%0AAuthor%3A%20Lukas%20Uzolas%20and%20Elmar%20Eisemann%20and%20Petr%20Kellnhofer%0AAbstract%3A%20%20%20Animation%20techniques%20bring%20digital%203D%20worlds%20and%20characters%20to%20life.%20However%2C%0Amanual%20animation%20is%20tedious%20and%20automated%20techniques%20are%20often%20specialized%20to%0Anarrow%20shape%20classes.%20In%20our%20work%2C%20we%20propose%20a%20technique%20for%20automatic%0Are-animation%20of%20various%203D%20shapes%20based%20on%20a%20motion%20prior%20extracted%20from%20a%0Avideo%20diffusion%20model.%20Unlike%20existing%204D%20generation%20methods%2C%20we%20focus%20solely%0Aon%20the%20motion%2C%20and%20we%20leverage%20an%20explicit%20mesh-based%20representation%20compatible%0Awith%20existing%20computer-graphics%20pipelines.%20Furthermore%2C%20our%20utilization%20of%0Adiffusion%20features%20enhances%20accuracy%20of%20our%20motion%20fitting.%20We%20analyze%20efficacy%0Aof%20these%20features%20for%20animation%20fitting%20and%20we%20experimentally%20validate%20our%0Aapproach%20for%20two%20different%20diffusion%20models%20and%20four%20animation%20models.%20Finally%2C%0Awe%20demonstrate%20that%20our%20time-efficient%20zero-shot%20method%20achieves%20a%20superior%0Aperformance%20re-animating%20a%20diverse%20set%20of%203D%20shapes%20when%20compared%20to%20existing%0Atechniques%20in%20a%20user%20study.%20The%20project%20website%20is%20located%20at%0Ahttps%3A//lukas.uzolas.com/MotionDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionDreamer%253A%2520Exploring%2520Semantic%2520Video%2520Diffusion%2520features%2520for%2520Zero-Shot%250A%2520%25203D%2520Mesh%2520Animation%26entry.906535625%3DLukas%2520Uzolas%2520and%2520Elmar%2520Eisemann%2520and%2520Petr%2520Kellnhofer%26entry.1292438233%3D%2520%2520Animation%2520techniques%2520bring%2520digital%25203D%2520worlds%2520and%2520characters%2520to%2520life.%2520However%252C%250Amanual%2520animation%2520is%2520tedious%2520and%2520automated%2520techniques%2520are%2520often%2520specialized%2520to%250Anarrow%2520shape%2520classes.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520technique%2520for%2520automatic%250Are-animation%2520of%2520various%25203D%2520shapes%2520based%2520on%2520a%2520motion%2520prior%2520extracted%2520from%2520a%250Avideo%2520diffusion%2520model.%2520Unlike%2520existing%25204D%2520generation%2520methods%252C%2520we%2520focus%2520solely%250Aon%2520the%2520motion%252C%2520and%2520we%2520leverage%2520an%2520explicit%2520mesh-based%2520representation%2520compatible%250Awith%2520existing%2520computer-graphics%2520pipelines.%2520Furthermore%252C%2520our%2520utilization%2520of%250Adiffusion%2520features%2520enhances%2520accuracy%2520of%2520our%2520motion%2520fitting.%2520We%2520analyze%2520efficacy%250Aof%2520these%2520features%2520for%2520animation%2520fitting%2520and%2520we%2520experimentally%2520validate%2520our%250Aapproach%2520for%2520two%2520different%2520diffusion%2520models%2520and%2520four%2520animation%2520models.%2520Finally%252C%250Awe%2520demonstrate%2520that%2520our%2520time-efficient%2520zero-shot%2520method%2520achieves%2520a%2520superior%250Aperformance%2520re-animating%2520a%2520diverse%2520set%2520of%25203D%2520shapes%2520when%2520compared%2520to%2520existing%250Atechniques%2520in%2520a%2520user%2520study.%2520The%2520project%2520website%2520is%2520located%2520at%250Ahttps%253A//lukas.uzolas.com/MotionDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionDreamer%3A%20Exploring%20Semantic%20Video%20Diffusion%20features%20for%20Zero-Shot%0A%20%203D%20Mesh%20Animation&entry.906535625=Lukas%20Uzolas%20and%20Elmar%20Eisemann%20and%20Petr%20Kellnhofer&entry.1292438233=%20%20Animation%20techniques%20bring%20digital%203D%20worlds%20and%20characters%20to%20life.%20However%2C%0Amanual%20animation%20is%20tedious%20and%20automated%20techniques%20are%20often%20specialized%20to%0Anarrow%20shape%20classes.%20In%20our%20work%2C%20we%20propose%20a%20technique%20for%20automatic%0Are-animation%20of%20various%203D%20shapes%20based%20on%20a%20motion%20prior%20extracted%20from%20a%0Avideo%20diffusion%20model.%20Unlike%20existing%204D%20generation%20methods%2C%20we%20focus%20solely%0Aon%20the%20motion%2C%20and%20we%20leverage%20an%20explicit%20mesh-based%20representation%20compatible%0Awith%20existing%20computer-graphics%20pipelines.%20Furthermore%2C%20our%20utilization%20of%0Adiffusion%20features%20enhances%20accuracy%20of%20our%20motion%20fitting.%20We%20analyze%20efficacy%0Aof%20these%20features%20for%20animation%20fitting%20and%20we%20experimentally%20validate%20our%0Aapproach%20for%20two%20different%20diffusion%20models%20and%20four%20animation%20models.%20Finally%2C%0Awe%20demonstrate%20that%20our%20time-efficient%20zero-shot%20method%20achieves%20a%20superior%0Aperformance%20re-animating%20a%20diverse%20set%20of%203D%20shapes%20when%20compared%20to%20existing%0Atechniques%20in%20a%20user%20study.%20The%20project%20website%20is%20located%20at%0Ahttps%3A//lukas.uzolas.com/MotionDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20155v2&entry.124074799=Read"},
{"title": "Renal Cell Carcinoma subtyping: learning from multi-resolution\n  localization", "author": "Mohamad Mohamad and Francesco Ponzio and Santa Di Cataldo and Damien Ambrosetti and Xavier Descombes", "abstract": "  Renal Cell Carcinoma is typically asymptomatic at the early stages for many\npatients. This leads to a late diagnosis of the tumor, where the curability\nlikelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high,\nwith respect to its incidence rate. To increase the survival chance, a fast and\ncorrect categorization of the tumor subtype is paramount. Nowadays,\ncomputerized methods, based on artificial intelligence, represent an\ninteresting opportunity to improve the productivity and the objectivity of the\nmicroscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their\nexploitation is hampered by the paucity of annotated dataset, essential for a\nproficient training of supervised machine learning technologies. This study\nsets out to investigate a novel self supervised training strategy for machine\nlearning diagnostic tools, based on the multi-resolution nature of the\nhistological samples. We aim at reducing the need of annotated dataset, without\nsignificantly reducing the accuracy of the tool. We demonstrate the\nclassification capability of our tool on a whole slide imaging dataset for\nRenal Cancer subtyping, and we compare our solution with several\nstate-of-the-art classification counterparts.\n", "link": "http://arxiv.org/abs/2411.09471v1", "date": "2024-11-14", "relevancy": 2.4547, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%0A%20%20localization&body=Title%3A%20Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%0A%20%20localization%0AAuthor%3A%20Mohamad%20Mohamad%20and%20Francesco%20Ponzio%20and%20Santa%20Di%20Cataldo%20and%20Damien%20Ambrosetti%20and%20Xavier%20Descombes%0AAbstract%3A%20%20%20Renal%20Cell%20Carcinoma%20is%20typically%20asymptomatic%20at%20the%20early%20stages%20for%20many%0Apatients.%20This%20leads%20to%20a%20late%20diagnosis%20of%20the%20tumor%2C%20where%20the%20curability%0Alikelihood%20is%20lower%2C%20and%20makes%20the%20mortality%20rate%20of%20Renal%20Cell%20Carcinoma%20high%2C%0Awith%20respect%20to%20its%20incidence%20rate.%20To%20increase%20the%20survival%20chance%2C%20a%20fast%20and%0Acorrect%20categorization%20of%20the%20tumor%20subtype%20is%20paramount.%20Nowadays%2C%0Acomputerized%20methods%2C%20based%20on%20artificial%20intelligence%2C%20represent%20an%0Ainteresting%20opportunity%20to%20improve%20the%20productivity%20and%20the%20objectivity%20of%20the%0Amicroscopy-based%20Renal%20Cell%20Carcinoma%20diagnosis.%20Nonetheless%2C%20much%20of%20their%0Aexploitation%20is%20hampered%20by%20the%20paucity%20of%20annotated%20dataset%2C%20essential%20for%20a%0Aproficient%20training%20of%20supervised%20machine%20learning%20technologies.%20This%20study%0Asets%20out%20to%20investigate%20a%20novel%20self%20supervised%20training%20strategy%20for%20machine%0Alearning%20diagnostic%20tools%2C%20based%20on%20the%20multi-resolution%20nature%20of%20the%0Ahistological%20samples.%20We%20aim%20at%20reducing%20the%20need%20of%20annotated%20dataset%2C%20without%0Asignificantly%20reducing%20the%20accuracy%20of%20the%20tool.%20We%20demonstrate%20the%0Aclassification%20capability%20of%20our%20tool%20on%20a%20whole%20slide%20imaging%20dataset%20for%0ARenal%20Cancer%20subtyping%2C%20and%20we%20compare%20our%20solution%20with%20several%0Astate-of-the-art%20classification%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRenal%2520Cell%2520Carcinoma%2520subtyping%253A%2520learning%2520from%2520multi-resolution%250A%2520%2520localization%26entry.906535625%3DMohamad%2520Mohamad%2520and%2520Francesco%2520Ponzio%2520and%2520Santa%2520Di%2520Cataldo%2520and%2520Damien%2520Ambrosetti%2520and%2520Xavier%2520Descombes%26entry.1292438233%3D%2520%2520Renal%2520Cell%2520Carcinoma%2520is%2520typically%2520asymptomatic%2520at%2520the%2520early%2520stages%2520for%2520many%250Apatients.%2520This%2520leads%2520to%2520a%2520late%2520diagnosis%2520of%2520the%2520tumor%252C%2520where%2520the%2520curability%250Alikelihood%2520is%2520lower%252C%2520and%2520makes%2520the%2520mortality%2520rate%2520of%2520Renal%2520Cell%2520Carcinoma%2520high%252C%250Awith%2520respect%2520to%2520its%2520incidence%2520rate.%2520To%2520increase%2520the%2520survival%2520chance%252C%2520a%2520fast%2520and%250Acorrect%2520categorization%2520of%2520the%2520tumor%2520subtype%2520is%2520paramount.%2520Nowadays%252C%250Acomputerized%2520methods%252C%2520based%2520on%2520artificial%2520intelligence%252C%2520represent%2520an%250Ainteresting%2520opportunity%2520to%2520improve%2520the%2520productivity%2520and%2520the%2520objectivity%2520of%2520the%250Amicroscopy-based%2520Renal%2520Cell%2520Carcinoma%2520diagnosis.%2520Nonetheless%252C%2520much%2520of%2520their%250Aexploitation%2520is%2520hampered%2520by%2520the%2520paucity%2520of%2520annotated%2520dataset%252C%2520essential%2520for%2520a%250Aproficient%2520training%2520of%2520supervised%2520machine%2520learning%2520technologies.%2520This%2520study%250Asets%2520out%2520to%2520investigate%2520a%2520novel%2520self%2520supervised%2520training%2520strategy%2520for%2520machine%250Alearning%2520diagnostic%2520tools%252C%2520based%2520on%2520the%2520multi-resolution%2520nature%2520of%2520the%250Ahistological%2520samples.%2520We%2520aim%2520at%2520reducing%2520the%2520need%2520of%2520annotated%2520dataset%252C%2520without%250Asignificantly%2520reducing%2520the%2520accuracy%2520of%2520the%2520tool.%2520We%2520demonstrate%2520the%250Aclassification%2520capability%2520of%2520our%2520tool%2520on%2520a%2520whole%2520slide%2520imaging%2520dataset%2520for%250ARenal%2520Cancer%2520subtyping%252C%2520and%2520we%2520compare%2520our%2520solution%2520with%2520several%250Astate-of-the-art%2520classification%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%0A%20%20localization&entry.906535625=Mohamad%20Mohamad%20and%20Francesco%20Ponzio%20and%20Santa%20Di%20Cataldo%20and%20Damien%20Ambrosetti%20and%20Xavier%20Descombes&entry.1292438233=%20%20Renal%20Cell%20Carcinoma%20is%20typically%20asymptomatic%20at%20the%20early%20stages%20for%20many%0Apatients.%20This%20leads%20to%20a%20late%20diagnosis%20of%20the%20tumor%2C%20where%20the%20curability%0Alikelihood%20is%20lower%2C%20and%20makes%20the%20mortality%20rate%20of%20Renal%20Cell%20Carcinoma%20high%2C%0Awith%20respect%20to%20its%20incidence%20rate.%20To%20increase%20the%20survival%20chance%2C%20a%20fast%20and%0Acorrect%20categorization%20of%20the%20tumor%20subtype%20is%20paramount.%20Nowadays%2C%0Acomputerized%20methods%2C%20based%20on%20artificial%20intelligence%2C%20represent%20an%0Ainteresting%20opportunity%20to%20improve%20the%20productivity%20and%20the%20objectivity%20of%20the%0Amicroscopy-based%20Renal%20Cell%20Carcinoma%20diagnosis.%20Nonetheless%2C%20much%20of%20their%0Aexploitation%20is%20hampered%20by%20the%20paucity%20of%20annotated%20dataset%2C%20essential%20for%20a%0Aproficient%20training%20of%20supervised%20machine%20learning%20technologies.%20This%20study%0Asets%20out%20to%20investigate%20a%20novel%20self%20supervised%20training%20strategy%20for%20machine%0Alearning%20diagnostic%20tools%2C%20based%20on%20the%20multi-resolution%20nature%20of%20the%0Ahistological%20samples.%20We%20aim%20at%20reducing%20the%20need%20of%20annotated%20dataset%2C%20without%0Asignificantly%20reducing%20the%20accuracy%20of%20the%20tool.%20We%20demonstrate%20the%0Aclassification%20capability%20of%20our%20tool%20on%20a%20whole%20slide%20imaging%20dataset%20for%0ARenal%20Cancer%20subtyping%2C%20and%20we%20compare%20our%20solution%20with%20several%0Astate-of-the-art%20classification%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09471v1&entry.124074799=Read"},
{"title": "Spatial Re-parameterization for N:M Sparsity", "author": "Yuxin Zhang and Mingliang Xu and Yonghong Tian and Rongrong Ji", "abstract": "  This paper presents a Spatial Re-parameterization (SpRe) method for the N:M\nsparsity in CNNs. SpRe is stemmed from an observation regarding the restricted\nvariety in spatial sparsity present in N:M sparsity compared with unstructured\nsparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within the\nspatial domains due to its distinctive pattern that mandates N non-zero\ncomponents among M successive weights in the input channel dimension of\nconvolution filters. On the contrary, we observe that unstructured sparsity\ndisplays a substantial divergence in sparsity across the spatial domains, which\nwe experimentally verified to be very crucial for its robust performance\nretention compared with N:M sparsity. Therefore, SpRe employs the\nspatial-sparsity distribution of unstructured sparsity to assign an extra\nbranch in conjunction with the original N:M branch at training time, which\nallows the N:M sparse network to sustain a similar distribution of spatial\nsparsity with unstructured sparsity. During inference, the extra branch can be\nfurther re-parameterized into the main N:M branch, without exerting any\ndistortion on the sparse pattern or additional computation costs. SpRe has\nachieved a commendable feat by matching the performance of N:M sparsity methods\nwith state-of-the-art unstructured sparsity methods across various benchmarks.\nCode and models are anonymously available at\n\\url{https://github.com/zyxxmu/SpRe}.\n", "link": "http://arxiv.org/abs/2306.05612v2", "date": "2024-11-14", "relevancy": 2.4123, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5036}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4746}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Re-parameterization%20for%20N%3AM%20Sparsity&body=Title%3A%20Spatial%20Re-parameterization%20for%20N%3AM%20Sparsity%0AAuthor%3A%20Yuxin%20Zhang%20and%20Mingliang%20Xu%20and%20Yonghong%20Tian%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Spatial%20Re-parameterization%20%28SpRe%29%20method%20for%20the%20N%3AM%0Asparsity%20in%20CNNs.%20SpRe%20is%20stemmed%20from%20an%20observation%20regarding%20the%20restricted%0Avariety%20in%20spatial%20sparsity%20present%20in%20N%3AM%20sparsity%20compared%20with%20unstructured%0Asparsity.%20Particularly%2C%20N%3AM%20sparsity%20exhibits%20a%20fixed%20sparsity%20rate%20within%20the%0Aspatial%20domains%20due%20to%20its%20distinctive%20pattern%20that%20mandates%20N%20non-zero%0Acomponents%20among%20M%20successive%20weights%20in%20the%20input%20channel%20dimension%20of%0Aconvolution%20filters.%20On%20the%20contrary%2C%20we%20observe%20that%20unstructured%20sparsity%0Adisplays%20a%20substantial%20divergence%20in%20sparsity%20across%20the%20spatial%20domains%2C%20which%0Awe%20experimentally%20verified%20to%20be%20very%20crucial%20for%20its%20robust%20performance%0Aretention%20compared%20with%20N%3AM%20sparsity.%20Therefore%2C%20SpRe%20employs%20the%0Aspatial-sparsity%20distribution%20of%20unstructured%20sparsity%20to%20assign%20an%20extra%0Abranch%20in%20conjunction%20with%20the%20original%20N%3AM%20branch%20at%20training%20time%2C%20which%0Aallows%20the%20N%3AM%20sparse%20network%20to%20sustain%20a%20similar%20distribution%20of%20spatial%0Asparsity%20with%20unstructured%20sparsity.%20During%20inference%2C%20the%20extra%20branch%20can%20be%0Afurther%20re-parameterized%20into%20the%20main%20N%3AM%20branch%2C%20without%20exerting%20any%0Adistortion%20on%20the%20sparse%20pattern%20or%20additional%20computation%20costs.%20SpRe%20has%0Aachieved%20a%20commendable%20feat%20by%20matching%20the%20performance%20of%20N%3AM%20sparsity%20methods%0Awith%20state-of-the-art%20unstructured%20sparsity%20methods%20across%20various%20benchmarks.%0ACode%20and%20models%20are%20anonymously%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zyxxmu/SpRe%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Re-parameterization%2520for%2520N%253AM%2520Sparsity%26entry.906535625%3DYuxin%2520Zhang%2520and%2520Mingliang%2520Xu%2520and%2520Yonghong%2520Tian%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Spatial%2520Re-parameterization%2520%2528SpRe%2529%2520method%2520for%2520the%2520N%253AM%250Asparsity%2520in%2520CNNs.%2520SpRe%2520is%2520stemmed%2520from%2520an%2520observation%2520regarding%2520the%2520restricted%250Avariety%2520in%2520spatial%2520sparsity%2520present%2520in%2520N%253AM%2520sparsity%2520compared%2520with%2520unstructured%250Asparsity.%2520Particularly%252C%2520N%253AM%2520sparsity%2520exhibits%2520a%2520fixed%2520sparsity%2520rate%2520within%2520the%250Aspatial%2520domains%2520due%2520to%2520its%2520distinctive%2520pattern%2520that%2520mandates%2520N%2520non-zero%250Acomponents%2520among%2520M%2520successive%2520weights%2520in%2520the%2520input%2520channel%2520dimension%2520of%250Aconvolution%2520filters.%2520On%2520the%2520contrary%252C%2520we%2520observe%2520that%2520unstructured%2520sparsity%250Adisplays%2520a%2520substantial%2520divergence%2520in%2520sparsity%2520across%2520the%2520spatial%2520domains%252C%2520which%250Awe%2520experimentally%2520verified%2520to%2520be%2520very%2520crucial%2520for%2520its%2520robust%2520performance%250Aretention%2520compared%2520with%2520N%253AM%2520sparsity.%2520Therefore%252C%2520SpRe%2520employs%2520the%250Aspatial-sparsity%2520distribution%2520of%2520unstructured%2520sparsity%2520to%2520assign%2520an%2520extra%250Abranch%2520in%2520conjunction%2520with%2520the%2520original%2520N%253AM%2520branch%2520at%2520training%2520time%252C%2520which%250Aallows%2520the%2520N%253AM%2520sparse%2520network%2520to%2520sustain%2520a%2520similar%2520distribution%2520of%2520spatial%250Asparsity%2520with%2520unstructured%2520sparsity.%2520During%2520inference%252C%2520the%2520extra%2520branch%2520can%2520be%250Afurther%2520re-parameterized%2520into%2520the%2520main%2520N%253AM%2520branch%252C%2520without%2520exerting%2520any%250Adistortion%2520on%2520the%2520sparse%2520pattern%2520or%2520additional%2520computation%2520costs.%2520SpRe%2520has%250Aachieved%2520a%2520commendable%2520feat%2520by%2520matching%2520the%2520performance%2520of%2520N%253AM%2520sparsity%2520methods%250Awith%2520state-of-the-art%2520unstructured%2520sparsity%2520methods%2520across%2520various%2520benchmarks.%250ACode%2520and%2520models%2520are%2520anonymously%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/zyxxmu/SpRe%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Re-parameterization%20for%20N%3AM%20Sparsity&entry.906535625=Yuxin%20Zhang%20and%20Mingliang%20Xu%20and%20Yonghong%20Tian%20and%20Rongrong%20Ji&entry.1292438233=%20%20This%20paper%20presents%20a%20Spatial%20Re-parameterization%20%28SpRe%29%20method%20for%20the%20N%3AM%0Asparsity%20in%20CNNs.%20SpRe%20is%20stemmed%20from%20an%20observation%20regarding%20the%20restricted%0Avariety%20in%20spatial%20sparsity%20present%20in%20N%3AM%20sparsity%20compared%20with%20unstructured%0Asparsity.%20Particularly%2C%20N%3AM%20sparsity%20exhibits%20a%20fixed%20sparsity%20rate%20within%20the%0Aspatial%20domains%20due%20to%20its%20distinctive%20pattern%20that%20mandates%20N%20non-zero%0Acomponents%20among%20M%20successive%20weights%20in%20the%20input%20channel%20dimension%20of%0Aconvolution%20filters.%20On%20the%20contrary%2C%20we%20observe%20that%20unstructured%20sparsity%0Adisplays%20a%20substantial%20divergence%20in%20sparsity%20across%20the%20spatial%20domains%2C%20which%0Awe%20experimentally%20verified%20to%20be%20very%20crucial%20for%20its%20robust%20performance%0Aretention%20compared%20with%20N%3AM%20sparsity.%20Therefore%2C%20SpRe%20employs%20the%0Aspatial-sparsity%20distribution%20of%20unstructured%20sparsity%20to%20assign%20an%20extra%0Abranch%20in%20conjunction%20with%20the%20original%20N%3AM%20branch%20at%20training%20time%2C%20which%0Aallows%20the%20N%3AM%20sparse%20network%20to%20sustain%20a%20similar%20distribution%20of%20spatial%0Asparsity%20with%20unstructured%20sparsity.%20During%20inference%2C%20the%20extra%20branch%20can%20be%0Afurther%20re-parameterized%20into%20the%20main%20N%3AM%20branch%2C%20without%20exerting%20any%0Adistortion%20on%20the%20sparse%20pattern%20or%20additional%20computation%20costs.%20SpRe%20has%0Aachieved%20a%20commendable%20feat%20by%20matching%20the%20performance%20of%20N%3AM%20sparsity%20methods%0Awith%20state-of-the-art%20unstructured%20sparsity%20methods%20across%20various%20benchmarks.%0ACode%20and%20models%20are%20anonymously%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zyxxmu/SpRe%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05612v2&entry.124074799=Read"},
{"title": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping", "author": "Siang Chen and Pengwei Xie and Wei Tang and Dingchang Hu and Yixiang Dai and Guijin Wang", "abstract": "  A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios.\n", "link": "http://arxiv.org/abs/2406.01767v3", "date": "2024-11-14", "relevancy": 2.3953, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6136}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6006}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping&body=Title%3A%20Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping%0AAuthor%3A%20Siang%20Chen%20and%20Pengwei%20Xie%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20A%20series%20of%20region-based%20methods%20succeed%20in%20extracting%20regional%20features%20and%0Aenhancing%20grasp%20detection%20quality.%20However%2C%20faced%20with%20a%20cluttered%20scene%20with%0Apotential%20collision%2C%20the%20definition%20of%20the%20grasp-relevant%20region%20stays%0Ainconsistent%2C%20and%20the%20relationship%20between%20grasps%20and%20regional%20spaces%20remains%0Aincompletely%20investigated.%20In%20this%20paper%2C%20we%20propose%20Normalized%20Grasp%20Space%0A%28NGS%29%20from%20a%20novel%20region-aware%20viewpoint%2C%20unifying%20the%20grasp%20representation%0Awithin%20a%20normalized%20regional%20space%20and%20benefiting%20the%20generalizability%20of%0Amethods.%20Leveraging%20the%20NGS%2C%20we%20find%20that%20CNNs%20are%20underestimated%20for%203D%0Afeature%20extraction%20and%206-DoF%20grasp%20detection%20in%20clutter%20scenes%20and%20build%20a%0Ahighly%20efficient%20Region-aware%20Normalized%20Grasp%20Network%20%28RNGNet%29.%20Experiments%20on%0Athe%20public%20benchmark%20show%20that%20our%20method%20achieves%20significant%20%3E20%25%20performance%0Agains%20while%20attaining%20a%20real-time%20inference%20speed%20of%20approximately%2050%20FPS.%0AReal-world%20cluttered%20scene%20clearance%20experiments%20underscore%20the%20effectiveness%0Aof%20our%20method.%20Further%2C%20human-to-robot%20handover%20and%20dynamic%20object%20grasping%0Aexperiments%20demonstrate%20the%20potential%20of%20our%20proposed%20method%20for%20closed-loop%0Agrasping%20in%20dynamic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01767v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion-aware%2520Grasp%2520Framework%2520with%2520Normalized%2520Grasp%2520Space%2520for%2520Efficient%250A%2520%25206-DoF%2520Grasping%26entry.906535625%3DSiang%2520Chen%2520and%2520Pengwei%2520Xie%2520and%2520Wei%2520Tang%2520and%2520Dingchang%2520Hu%2520and%2520Yixiang%2520Dai%2520and%2520Guijin%2520Wang%26entry.1292438233%3D%2520%2520A%2520series%2520of%2520region-based%2520methods%2520succeed%2520in%2520extracting%2520regional%2520features%2520and%250Aenhancing%2520grasp%2520detection%2520quality.%2520However%252C%2520faced%2520with%2520a%2520cluttered%2520scene%2520with%250Apotential%2520collision%252C%2520the%2520definition%2520of%2520the%2520grasp-relevant%2520region%2520stays%250Ainconsistent%252C%2520and%2520the%2520relationship%2520between%2520grasps%2520and%2520regional%2520spaces%2520remains%250Aincompletely%2520investigated.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Normalized%2520Grasp%2520Space%250A%2528NGS%2529%2520from%2520a%2520novel%2520region-aware%2520viewpoint%252C%2520unifying%2520the%2520grasp%2520representation%250Awithin%2520a%2520normalized%2520regional%2520space%2520and%2520benefiting%2520the%2520generalizability%2520of%250Amethods.%2520Leveraging%2520the%2520NGS%252C%2520we%2520find%2520that%2520CNNs%2520are%2520underestimated%2520for%25203D%250Afeature%2520extraction%2520and%25206-DoF%2520grasp%2520detection%2520in%2520clutter%2520scenes%2520and%2520build%2520a%250Ahighly%2520efficient%2520Region-aware%2520Normalized%2520Grasp%2520Network%2520%2528RNGNet%2529.%2520Experiments%2520on%250Athe%2520public%2520benchmark%2520show%2520that%2520our%2520method%2520achieves%2520significant%2520%253E20%2525%2520performance%250Agains%2520while%2520attaining%2520a%2520real-time%2520inference%2520speed%2520of%2520approximately%252050%2520FPS.%250AReal-world%2520cluttered%2520scene%2520clearance%2520experiments%2520underscore%2520the%2520effectiveness%250Aof%2520our%2520method.%2520Further%252C%2520human-to-robot%2520handover%2520and%2520dynamic%2520object%2520grasping%250Aexperiments%2520demonstrate%2520the%2520potential%2520of%2520our%2520proposed%2520method%2520for%2520closed-loop%250Agrasping%2520in%2520dynamic%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01767v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region-aware%20Grasp%20Framework%20with%20Normalized%20Grasp%20Space%20for%20Efficient%0A%20%206-DoF%20Grasping&entry.906535625=Siang%20Chen%20and%20Pengwei%20Xie%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Guijin%20Wang&entry.1292438233=%20%20A%20series%20of%20region-based%20methods%20succeed%20in%20extracting%20regional%20features%20and%0Aenhancing%20grasp%20detection%20quality.%20However%2C%20faced%20with%20a%20cluttered%20scene%20with%0Apotential%20collision%2C%20the%20definition%20of%20the%20grasp-relevant%20region%20stays%0Ainconsistent%2C%20and%20the%20relationship%20between%20grasps%20and%20regional%20spaces%20remains%0Aincompletely%20investigated.%20In%20this%20paper%2C%20we%20propose%20Normalized%20Grasp%20Space%0A%28NGS%29%20from%20a%20novel%20region-aware%20viewpoint%2C%20unifying%20the%20grasp%20representation%0Awithin%20a%20normalized%20regional%20space%20and%20benefiting%20the%20generalizability%20of%0Amethods.%20Leveraging%20the%20NGS%2C%20we%20find%20that%20CNNs%20are%20underestimated%20for%203D%0Afeature%20extraction%20and%206-DoF%20grasp%20detection%20in%20clutter%20scenes%20and%20build%20a%0Ahighly%20efficient%20Region-aware%20Normalized%20Grasp%20Network%20%28RNGNet%29.%20Experiments%20on%0Athe%20public%20benchmark%20show%20that%20our%20method%20achieves%20significant%20%3E20%25%20performance%0Agains%20while%20attaining%20a%20real-time%20inference%20speed%20of%20approximately%2050%20FPS.%0AReal-world%20cluttered%20scene%20clearance%20experiments%20underscore%20the%20effectiveness%0Aof%20our%20method.%20Further%2C%20human-to-robot%20handover%20and%20dynamic%20object%20grasping%0Aexperiments%20demonstrate%20the%20potential%20of%20our%20proposed%20method%20for%20closed-loop%0Agrasping%20in%20dynamic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01767v3&entry.124074799=Read"},
{"title": "CropCraft: Inverse Procedural Modeling for 3D Reconstruction of Crop\n  Plants", "author": "Albert J. Zhai and Xinlei Wang and Kaiyuan Li and Zhao Jiang and Junxiong Zhou and Sheng Wang and Zhenong Jin and Kaiyu Guan and Shenlong Wang", "abstract": "  The ability to automatically build 3D digital twins of plants from images has\ncountless applications in agriculture, environmental science, robotics, and\nother fields. However, current 3D reconstruction methods fail to recover\ncomplete shapes of plants due to heavy occlusion and complex geometries. In\nthis work, we present a novel method for 3D reconstruction of agricultural\ncrops based on optimizing a parametric model of plant morphology via inverse\nprocedural modeling. Our method first estimates depth maps by fitting a neural\nradiance field and then employs Bayesian optimization to estimate plant\nmorphological parameters that result in consistent depth renderings. The\nresulting 3D model is complete and biologically plausible. We validate our\nmethod on a dataset of real images of agricultural fields, and demonstrate that\nthe reconstructions can be used for a variety of monitoring and simulation\napplications.\n", "link": "http://arxiv.org/abs/2411.09693v1", "date": "2024-11-14", "relevancy": 2.3904, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CropCraft%3A%20Inverse%20Procedural%20Modeling%20for%203D%20Reconstruction%20of%20Crop%0A%20%20Plants&body=Title%3A%20CropCraft%3A%20Inverse%20Procedural%20Modeling%20for%203D%20Reconstruction%20of%20Crop%0A%20%20Plants%0AAuthor%3A%20Albert%20J.%20Zhai%20and%20Xinlei%20Wang%20and%20Kaiyuan%20Li%20and%20Zhao%20Jiang%20and%20Junxiong%20Zhou%20and%20Sheng%20Wang%20and%20Zhenong%20Jin%20and%20Kaiyu%20Guan%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20The%20ability%20to%20automatically%20build%203D%20digital%20twins%20of%20plants%20from%20images%20has%0Acountless%20applications%20in%20agriculture%2C%20environmental%20science%2C%20robotics%2C%20and%0Aother%20fields.%20However%2C%20current%203D%20reconstruction%20methods%20fail%20to%20recover%0Acomplete%20shapes%20of%20plants%20due%20to%20heavy%20occlusion%20and%20complex%20geometries.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20method%20for%203D%20reconstruction%20of%20agricultural%0Acrops%20based%20on%20optimizing%20a%20parametric%20model%20of%20plant%20morphology%20via%20inverse%0Aprocedural%20modeling.%20Our%20method%20first%20estimates%20depth%20maps%20by%20fitting%20a%20neural%0Aradiance%20field%20and%20then%20employs%20Bayesian%20optimization%20to%20estimate%20plant%0Amorphological%20parameters%20that%20result%20in%20consistent%20depth%20renderings.%20The%0Aresulting%203D%20model%20is%20complete%20and%20biologically%20plausible.%20We%20validate%20our%0Amethod%20on%20a%20dataset%20of%20real%20images%20of%20agricultural%20fields%2C%20and%20demonstrate%20that%0Athe%20reconstructions%20can%20be%20used%20for%20a%20variety%20of%20monitoring%20and%20simulation%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCropCraft%253A%2520Inverse%2520Procedural%2520Modeling%2520for%25203D%2520Reconstruction%2520of%2520Crop%250A%2520%2520Plants%26entry.906535625%3DAlbert%2520J.%2520Zhai%2520and%2520Xinlei%2520Wang%2520and%2520Kaiyuan%2520Li%2520and%2520Zhao%2520Jiang%2520and%2520Junxiong%2520Zhou%2520and%2520Sheng%2520Wang%2520and%2520Zhenong%2520Jin%2520and%2520Kaiyu%2520Guan%2520and%2520Shenlong%2520Wang%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520automatically%2520build%25203D%2520digital%2520twins%2520of%2520plants%2520from%2520images%2520has%250Acountless%2520applications%2520in%2520agriculture%252C%2520environmental%2520science%252C%2520robotics%252C%2520and%250Aother%2520fields.%2520However%252C%2520current%25203D%2520reconstruction%2520methods%2520fail%2520to%2520recover%250Acomplete%2520shapes%2520of%2520plants%2520due%2520to%2520heavy%2520occlusion%2520and%2520complex%2520geometries.%2520In%250Athis%2520work%252C%2520we%2520present%2520a%2520novel%2520method%2520for%25203D%2520reconstruction%2520of%2520agricultural%250Acrops%2520based%2520on%2520optimizing%2520a%2520parametric%2520model%2520of%2520plant%2520morphology%2520via%2520inverse%250Aprocedural%2520modeling.%2520Our%2520method%2520first%2520estimates%2520depth%2520maps%2520by%2520fitting%2520a%2520neural%250Aradiance%2520field%2520and%2520then%2520employs%2520Bayesian%2520optimization%2520to%2520estimate%2520plant%250Amorphological%2520parameters%2520that%2520result%2520in%2520consistent%2520depth%2520renderings.%2520The%250Aresulting%25203D%2520model%2520is%2520complete%2520and%2520biologically%2520plausible.%2520We%2520validate%2520our%250Amethod%2520on%2520a%2520dataset%2520of%2520real%2520images%2520of%2520agricultural%2520fields%252C%2520and%2520demonstrate%2520that%250Athe%2520reconstructions%2520can%2520be%2520used%2520for%2520a%2520variety%2520of%2520monitoring%2520and%2520simulation%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CropCraft%3A%20Inverse%20Procedural%20Modeling%20for%203D%20Reconstruction%20of%20Crop%0A%20%20Plants&entry.906535625=Albert%20J.%20Zhai%20and%20Xinlei%20Wang%20and%20Kaiyuan%20Li%20and%20Zhao%20Jiang%20and%20Junxiong%20Zhou%20and%20Sheng%20Wang%20and%20Zhenong%20Jin%20and%20Kaiyu%20Guan%20and%20Shenlong%20Wang&entry.1292438233=%20%20The%20ability%20to%20automatically%20build%203D%20digital%20twins%20of%20plants%20from%20images%20has%0Acountless%20applications%20in%20agriculture%2C%20environmental%20science%2C%20robotics%2C%20and%0Aother%20fields.%20However%2C%20current%203D%20reconstruction%20methods%20fail%20to%20recover%0Acomplete%20shapes%20of%20plants%20due%20to%20heavy%20occlusion%20and%20complex%20geometries.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20method%20for%203D%20reconstruction%20of%20agricultural%0Acrops%20based%20on%20optimizing%20a%20parametric%20model%20of%20plant%20morphology%20via%20inverse%0Aprocedural%20modeling.%20Our%20method%20first%20estimates%20depth%20maps%20by%20fitting%20a%20neural%0Aradiance%20field%20and%20then%20employs%20Bayesian%20optimization%20to%20estimate%20plant%0Amorphological%20parameters%20that%20result%20in%20consistent%20depth%20renderings.%20The%0Aresulting%203D%20model%20is%20complete%20and%20biologically%20plausible.%20We%20validate%20our%0Amethod%20on%20a%20dataset%20of%20real%20images%20of%20agricultural%20fields%2C%20and%20demonstrate%20that%0Athe%20reconstructions%20can%20be%20used%20for%20a%20variety%20of%20monitoring%20and%20simulation%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09693v1&entry.124074799=Read"},
{"title": "V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation\n  Localization and Copyright Protection", "author": "Xuanyu Zhang and Youmin Xu and Runyi Li and Jiwen Yu and Weiqi Li and Zhipei Xu and Jian Zhang", "abstract": "  AI-generated video has revolutionized short video production, filmmaking, and\npersonalized media, making video local editing an essential tool. However, this\nprogress also blurs the line between reality and fiction, posing challenges in\nmultimedia forensics. To solve this urgent issue, V2A-Mark is proposed to\naddress the limitations of current video tampering forensics, such as poor\ngeneralizability, singular function, and single modality focus. Combining the\nfragility of video-into-video steganography with deep robust watermarking, our\nmethod can embed invisible visual-audio localization watermarks and copyright\nwatermarks into the original video frames and audio, enabling precise\nmanipulation localization and copyright protection. We also design a temporal\nalignment and fusion module and degradation prompt learning to enhance the\nlocalization accuracy and decoding robustness. Meanwhile, we introduce a\nsample-level audio localization method and a cross-modal copyright extraction\nmechanism to couple the information of audio and video frames. The\neffectiveness of V2A-Mark has been verified on a visual-audio tampering\ndataset, emphasizing its superiority in localization precision and copyright\naccuracy, crucial for the sustainable development of video editing in the AIGC\nvideo era.\n", "link": "http://arxiv.org/abs/2404.16824v4", "date": "2024-11-14", "relevancy": 2.3786, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6067}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection&body=Title%3A%20V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection%0AAuthor%3A%20Xuanyu%20Zhang%20and%20Youmin%20Xu%20and%20Runyi%20Li%20and%20Jiwen%20Yu%20and%20Weiqi%20Li%20and%20Zhipei%20Xu%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20AI-generated%20video%20has%20revolutionized%20short%20video%20production%2C%20filmmaking%2C%20and%0Apersonalized%20media%2C%20making%20video%20local%20editing%20an%20essential%20tool.%20However%2C%20this%0Aprogress%20also%20blurs%20the%20line%20between%20reality%20and%20fiction%2C%20posing%20challenges%20in%0Amultimedia%20forensics.%20To%20solve%20this%20urgent%20issue%2C%20V2A-Mark%20is%20proposed%20to%0Aaddress%20the%20limitations%20of%20current%20video%20tampering%20forensics%2C%20such%20as%20poor%0Ageneralizability%2C%20singular%20function%2C%20and%20single%20modality%20focus.%20Combining%20the%0Afragility%20of%20video-into-video%20steganography%20with%20deep%20robust%20watermarking%2C%20our%0Amethod%20can%20embed%20invisible%20visual-audio%20localization%20watermarks%20and%20copyright%0Awatermarks%20into%20the%20original%20video%20frames%20and%20audio%2C%20enabling%20precise%0Amanipulation%20localization%20and%20copyright%20protection.%20We%20also%20design%20a%20temporal%0Aalignment%20and%20fusion%20module%20and%20degradation%20prompt%20learning%20to%20enhance%20the%0Alocalization%20accuracy%20and%20decoding%20robustness.%20Meanwhile%2C%20we%20introduce%20a%0Asample-level%20audio%20localization%20method%20and%20a%20cross-modal%20copyright%20extraction%0Amechanism%20to%20couple%20the%20information%20of%20audio%20and%20video%20frames.%20The%0Aeffectiveness%20of%20V2A-Mark%20has%20been%20verified%20on%20a%20visual-audio%20tampering%0Adataset%2C%20emphasizing%20its%20superiority%20in%20localization%20precision%20and%20copyright%0Aaccuracy%2C%20crucial%20for%20the%20sustainable%20development%20of%20video%20editing%20in%20the%20AIGC%0Avideo%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16824v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2A-Mark%253A%2520Versatile%2520Deep%2520Visual-Audio%2520Watermarking%2520for%2520Manipulation%250A%2520%2520Localization%2520and%2520Copyright%2520Protection%26entry.906535625%3DXuanyu%2520Zhang%2520and%2520Youmin%2520Xu%2520and%2520Runyi%2520Li%2520and%2520Jiwen%2520Yu%2520and%2520Weiqi%2520Li%2520and%2520Zhipei%2520Xu%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520AI-generated%2520video%2520has%2520revolutionized%2520short%2520video%2520production%252C%2520filmmaking%252C%2520and%250Apersonalized%2520media%252C%2520making%2520video%2520local%2520editing%2520an%2520essential%2520tool.%2520However%252C%2520this%250Aprogress%2520also%2520blurs%2520the%2520line%2520between%2520reality%2520and%2520fiction%252C%2520posing%2520challenges%2520in%250Amultimedia%2520forensics.%2520To%2520solve%2520this%2520urgent%2520issue%252C%2520V2A-Mark%2520is%2520proposed%2520to%250Aaddress%2520the%2520limitations%2520of%2520current%2520video%2520tampering%2520forensics%252C%2520such%2520as%2520poor%250Ageneralizability%252C%2520singular%2520function%252C%2520and%2520single%2520modality%2520focus.%2520Combining%2520the%250Afragility%2520of%2520video-into-video%2520steganography%2520with%2520deep%2520robust%2520watermarking%252C%2520our%250Amethod%2520can%2520embed%2520invisible%2520visual-audio%2520localization%2520watermarks%2520and%2520copyright%250Awatermarks%2520into%2520the%2520original%2520video%2520frames%2520and%2520audio%252C%2520enabling%2520precise%250Amanipulation%2520localization%2520and%2520copyright%2520protection.%2520We%2520also%2520design%2520a%2520temporal%250Aalignment%2520and%2520fusion%2520module%2520and%2520degradation%2520prompt%2520learning%2520to%2520enhance%2520the%250Alocalization%2520accuracy%2520and%2520decoding%2520robustness.%2520Meanwhile%252C%2520we%2520introduce%2520a%250Asample-level%2520audio%2520localization%2520method%2520and%2520a%2520cross-modal%2520copyright%2520extraction%250Amechanism%2520to%2520couple%2520the%2520information%2520of%2520audio%2520and%2520video%2520frames.%2520The%250Aeffectiveness%2520of%2520V2A-Mark%2520has%2520been%2520verified%2520on%2520a%2520visual-audio%2520tampering%250Adataset%252C%2520emphasizing%2520its%2520superiority%2520in%2520localization%2520precision%2520and%2520copyright%250Aaccuracy%252C%2520crucial%2520for%2520the%2520sustainable%2520development%2520of%2520video%2520editing%2520in%2520the%2520AIGC%250Avideo%2520era.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16824v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2A-Mark%3A%20Versatile%20Deep%20Visual-Audio%20Watermarking%20for%20Manipulation%0A%20%20Localization%20and%20Copyright%20Protection&entry.906535625=Xuanyu%20Zhang%20and%20Youmin%20Xu%20and%20Runyi%20Li%20and%20Jiwen%20Yu%20and%20Weiqi%20Li%20and%20Zhipei%20Xu%20and%20Jian%20Zhang&entry.1292438233=%20%20AI-generated%20video%20has%20revolutionized%20short%20video%20production%2C%20filmmaking%2C%20and%0Apersonalized%20media%2C%20making%20video%20local%20editing%20an%20essential%20tool.%20However%2C%20this%0Aprogress%20also%20blurs%20the%20line%20between%20reality%20and%20fiction%2C%20posing%20challenges%20in%0Amultimedia%20forensics.%20To%20solve%20this%20urgent%20issue%2C%20V2A-Mark%20is%20proposed%20to%0Aaddress%20the%20limitations%20of%20current%20video%20tampering%20forensics%2C%20such%20as%20poor%0Ageneralizability%2C%20singular%20function%2C%20and%20single%20modality%20focus.%20Combining%20the%0Afragility%20of%20video-into-video%20steganography%20with%20deep%20robust%20watermarking%2C%20our%0Amethod%20can%20embed%20invisible%20visual-audio%20localization%20watermarks%20and%20copyright%0Awatermarks%20into%20the%20original%20video%20frames%20and%20audio%2C%20enabling%20precise%0Amanipulation%20localization%20and%20copyright%20protection.%20We%20also%20design%20a%20temporal%0Aalignment%20and%20fusion%20module%20and%20degradation%20prompt%20learning%20to%20enhance%20the%0Alocalization%20accuracy%20and%20decoding%20robustness.%20Meanwhile%2C%20we%20introduce%20a%0Asample-level%20audio%20localization%20method%20and%20a%20cross-modal%20copyright%20extraction%0Amechanism%20to%20couple%20the%20information%20of%20audio%20and%20video%20frames.%20The%0Aeffectiveness%20of%20V2A-Mark%20has%20been%20verified%20on%20a%20visual-audio%20tampering%0Adataset%2C%20emphasizing%20its%20superiority%20in%20localization%20precision%20and%20copyright%0Aaccuracy%2C%20crucial%20for%20the%20sustainable%20development%20of%20video%20editing%20in%20the%20AIGC%0Avideo%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16824v4&entry.124074799=Read"},
{"title": "Local deployment of large-scale music AI models on commodity hardware", "author": "Xun Zhou and Charlie Ruan and Zihe Zhao and Tianqi Chen and Chris Donahue", "abstract": "  We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering.\n", "link": "http://arxiv.org/abs/2411.09625v1", "date": "2024-11-14", "relevancy": 2.3783, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5126}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20deployment%20of%20large-scale%20music%20AI%20models%20on%20commodity%20hardware&body=Title%3A%20Local%20deployment%20of%20large-scale%20music%20AI%20models%20on%20commodity%20hardware%0AAuthor%3A%20Xun%20Zhou%20and%20Charlie%20Ruan%20and%20Zihe%20Zhao%20and%20Tianqi%20Chen%20and%20Chris%20Donahue%0AAbstract%3A%20%20%20We%20present%20the%20MIDInfinite%2C%20a%20web%20application%20capable%20of%20generating%20symbolic%0Amusic%20using%20a%20large-scale%20generative%20AI%20model%20locally%20on%20commodity%20hardware.%0ACreating%20this%20demo%20involved%20porting%20the%20Anticipatory%20Music%20Transformer%2C%20a%20large%0Alanguage%20model%20%28LLM%29%20pre-trained%20on%20the%20Lakh%20MIDI%20dataset%2C%20to%20the%20Machine%0ALearning%20Compilation%20%28MLC%29%20framework.%20Once%20the%20model%20is%20ported%2C%20MLC%20facilitates%0Ainference%20on%20a%20variety%20of%20runtimes%20including%20C%2B%2B%2C%20mobile%2C%20and%20the%20browser.%20We%0Aenvision%20that%20MLC%20has%20the%20potential%20to%20bridge%20the%20gap%20between%20the%20landscape%20of%0Aincreasingly%20capable%20music%20AI%20models%20and%20technology%20more%20familiar%20to%20music%0Asoftware%20developers.%20As%20a%20proof%20of%20concept%2C%20we%20build%20a%20web%20application%20that%0Aallows%20users%20to%20generate%20endless%20streams%20of%20multi-instrumental%20MIDI%20in%20the%0Abrowser%2C%20either%20from%20scratch%20or%20conditioned%20on%20a%20prompt.%20On%20commodity%20hardware%0A%28an%20M3%20Macbook%20Pro%29%2C%20our%20demo%20can%20generate%2051%20notes%20per%20second%2C%20which%20is%20faster%0Athan%20real-time%20playback%20for%2072.9%25%20of%20generations%2C%20and%20increases%20to%2086.3%25%20with%202%0Aseconds%20of%20upfront%20buffering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520deployment%2520of%2520large-scale%2520music%2520AI%2520models%2520on%2520commodity%2520hardware%26entry.906535625%3DXun%2520Zhou%2520and%2520Charlie%2520Ruan%2520and%2520Zihe%2520Zhao%2520and%2520Tianqi%2520Chen%2520and%2520Chris%2520Donahue%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520MIDInfinite%252C%2520a%2520web%2520application%2520capable%2520of%2520generating%2520symbolic%250Amusic%2520using%2520a%2520large-scale%2520generative%2520AI%2520model%2520locally%2520on%2520commodity%2520hardware.%250ACreating%2520this%2520demo%2520involved%2520porting%2520the%2520Anticipatory%2520Music%2520Transformer%252C%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520pre-trained%2520on%2520the%2520Lakh%2520MIDI%2520dataset%252C%2520to%2520the%2520Machine%250ALearning%2520Compilation%2520%2528MLC%2529%2520framework.%2520Once%2520the%2520model%2520is%2520ported%252C%2520MLC%2520facilitates%250Ainference%2520on%2520a%2520variety%2520of%2520runtimes%2520including%2520C%252B%252B%252C%2520mobile%252C%2520and%2520the%2520browser.%2520We%250Aenvision%2520that%2520MLC%2520has%2520the%2520potential%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520landscape%2520of%250Aincreasingly%2520capable%2520music%2520AI%2520models%2520and%2520technology%2520more%2520familiar%2520to%2520music%250Asoftware%2520developers.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520build%2520a%2520web%2520application%2520that%250Aallows%2520users%2520to%2520generate%2520endless%2520streams%2520of%2520multi-instrumental%2520MIDI%2520in%2520the%250Abrowser%252C%2520either%2520from%2520scratch%2520or%2520conditioned%2520on%2520a%2520prompt.%2520On%2520commodity%2520hardware%250A%2528an%2520M3%2520Macbook%2520Pro%2529%252C%2520our%2520demo%2520can%2520generate%252051%2520notes%2520per%2520second%252C%2520which%2520is%2520faster%250Athan%2520real-time%2520playback%2520for%252072.9%2525%2520of%2520generations%252C%2520and%2520increases%2520to%252086.3%2525%2520with%25202%250Aseconds%2520of%2520upfront%2520buffering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20deployment%20of%20large-scale%20music%20AI%20models%20on%20commodity%20hardware&entry.906535625=Xun%20Zhou%20and%20Charlie%20Ruan%20and%20Zihe%20Zhao%20and%20Tianqi%20Chen%20and%20Chris%20Donahue&entry.1292438233=%20%20We%20present%20the%20MIDInfinite%2C%20a%20web%20application%20capable%20of%20generating%20symbolic%0Amusic%20using%20a%20large-scale%20generative%20AI%20model%20locally%20on%20commodity%20hardware.%0ACreating%20this%20demo%20involved%20porting%20the%20Anticipatory%20Music%20Transformer%2C%20a%20large%0Alanguage%20model%20%28LLM%29%20pre-trained%20on%20the%20Lakh%20MIDI%20dataset%2C%20to%20the%20Machine%0ALearning%20Compilation%20%28MLC%29%20framework.%20Once%20the%20model%20is%20ported%2C%20MLC%20facilitates%0Ainference%20on%20a%20variety%20of%20runtimes%20including%20C%2B%2B%2C%20mobile%2C%20and%20the%20browser.%20We%0Aenvision%20that%20MLC%20has%20the%20potential%20to%20bridge%20the%20gap%20between%20the%20landscape%20of%0Aincreasingly%20capable%20music%20AI%20models%20and%20technology%20more%20familiar%20to%20music%0Asoftware%20developers.%20As%20a%20proof%20of%20concept%2C%20we%20build%20a%20web%20application%20that%0Aallows%20users%20to%20generate%20endless%20streams%20of%20multi-instrumental%20MIDI%20in%20the%0Abrowser%2C%20either%20from%20scratch%20or%20conditioned%20on%20a%20prompt.%20On%20commodity%20hardware%0A%28an%20M3%20Macbook%20Pro%29%2C%20our%20demo%20can%20generate%2051%20notes%20per%20second%2C%20which%20is%20faster%0Athan%20real-time%20playback%20for%2072.9%25%20of%20generations%2C%20and%20increases%20to%2086.3%25%20with%202%0Aseconds%20of%20upfront%20buffering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09625v1&entry.124074799=Read"},
{"title": "Stable Consistency Tuning: Understanding and Improving Consistency\n  Models", "author": "Fu-Yun Wang and Zhengyang Geng and Hongsheng Li", "abstract": "  Diffusion models achieve superior generation quality but suffer from slow\ngeneration speed due to the iterative nature of denoising. In contrast,\nconsistency models, a new generative family, achieve competitive performance\nwith significantly faster sampling. These models are trained either through\nconsistency distillation, which leverages pretrained diffusion models, or\nconsistency training/tuning directly from raw data. In this work, we propose a\nnovel framework for understanding consistency models by modeling the denoising\nprocess of the diffusion model as a Markov Decision Process (MDP) and framing\nconsistency model training as the value estimation through Temporal\nDifference~(TD) Learning. More importantly, this framework allows us to analyze\nthe limitations of current consistency training/tuning strategies. Built upon\nEasy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),\nwhich incorporates variance-reduced learning using the score identity. SCT\nleads to significant performance improvements on benchmarks such as CIFAR-10\nand ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID\n1.55, a new SoTA for consistency models.\n", "link": "http://arxiv.org/abs/2410.18958v2", "date": "2024-11-14", "relevancy": 2.3733, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5884}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models&body=Title%3A%20Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models%0AAuthor%3A%20Fu-Yun%20Wang%20and%20Zhengyang%20Geng%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20achieve%20superior%20generation%20quality%20but%20suffer%20from%20slow%0Ageneration%20speed%20due%20to%20the%20iterative%20nature%20of%20denoising.%20In%20contrast%2C%0Aconsistency%20models%2C%20a%20new%20generative%20family%2C%20achieve%20competitive%20performance%0Awith%20significantly%20faster%20sampling.%20These%20models%20are%20trained%20either%20through%0Aconsistency%20distillation%2C%20which%20leverages%20pretrained%20diffusion%20models%2C%20or%0Aconsistency%20training/tuning%20directly%20from%20raw%20data.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20for%20understanding%20consistency%20models%20by%20modeling%20the%20denoising%0Aprocess%20of%20the%20diffusion%20model%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20framing%0Aconsistency%20model%20training%20as%20the%20value%20estimation%20through%20Temporal%0ADifference~%28TD%29%20Learning.%20More%20importantly%2C%20this%20framework%20allows%20us%20to%20analyze%0Athe%20limitations%20of%20current%20consistency%20training/tuning%20strategies.%20Built%20upon%0AEasy%20Consistency%20Tuning%20%28ECT%29%2C%20we%20propose%20Stable%20Consistency%20Tuning%20%28SCT%29%2C%0Awhich%20incorporates%20variance-reduced%20learning%20using%20the%20score%20identity.%20SCT%0Aleads%20to%20significant%20performance%20improvements%20on%20benchmarks%20such%20as%20CIFAR-10%0Aand%20ImageNet-64.%20On%20ImageNet-64%2C%20SCT%20achieves%201-step%20FID%202.42%20and%202-step%20FID%0A1.55%2C%20a%20new%20SoTA%20for%20consistency%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Consistency%2520Tuning%253A%2520Understanding%2520and%2520Improving%2520Consistency%250A%2520%2520Models%26entry.906535625%3DFu-Yun%2520Wang%2520and%2520Zhengyang%2520Geng%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520achieve%2520superior%2520generation%2520quality%2520but%2520suffer%2520from%2520slow%250Ageneration%2520speed%2520due%2520to%2520the%2520iterative%2520nature%2520of%2520denoising.%2520In%2520contrast%252C%250Aconsistency%2520models%252C%2520a%2520new%2520generative%2520family%252C%2520achieve%2520competitive%2520performance%250Awith%2520significantly%2520faster%2520sampling.%2520These%2520models%2520are%2520trained%2520either%2520through%250Aconsistency%2520distillation%252C%2520which%2520leverages%2520pretrained%2520diffusion%2520models%252C%2520or%250Aconsistency%2520training/tuning%2520directly%2520from%2520raw%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520for%2520understanding%2520consistency%2520models%2520by%2520modeling%2520the%2520denoising%250Aprocess%2520of%2520the%2520diffusion%2520model%2520as%2520a%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520and%2520framing%250Aconsistency%2520model%2520training%2520as%2520the%2520value%2520estimation%2520through%2520Temporal%250ADifference~%2528TD%2529%2520Learning.%2520More%2520importantly%252C%2520this%2520framework%2520allows%2520us%2520to%2520analyze%250Athe%2520limitations%2520of%2520current%2520consistency%2520training/tuning%2520strategies.%2520Built%2520upon%250AEasy%2520Consistency%2520Tuning%2520%2528ECT%2529%252C%2520we%2520propose%2520Stable%2520Consistency%2520Tuning%2520%2528SCT%2529%252C%250Awhich%2520incorporates%2520variance-reduced%2520learning%2520using%2520the%2520score%2520identity.%2520SCT%250Aleads%2520to%2520significant%2520performance%2520improvements%2520on%2520benchmarks%2520such%2520as%2520CIFAR-10%250Aand%2520ImageNet-64.%2520On%2520ImageNet-64%252C%2520SCT%2520achieves%25201-step%2520FID%25202.42%2520and%25202-step%2520FID%250A1.55%252C%2520a%2520new%2520SoTA%2520for%2520consistency%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models&entry.906535625=Fu-Yun%20Wang%20and%20Zhengyang%20Geng%20and%20Hongsheng%20Li&entry.1292438233=%20%20Diffusion%20models%20achieve%20superior%20generation%20quality%20but%20suffer%20from%20slow%0Ageneration%20speed%20due%20to%20the%20iterative%20nature%20of%20denoising.%20In%20contrast%2C%0Aconsistency%20models%2C%20a%20new%20generative%20family%2C%20achieve%20competitive%20performance%0Awith%20significantly%20faster%20sampling.%20These%20models%20are%20trained%20either%20through%0Aconsistency%20distillation%2C%20which%20leverages%20pretrained%20diffusion%20models%2C%20or%0Aconsistency%20training/tuning%20directly%20from%20raw%20data.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20for%20understanding%20consistency%20models%20by%20modeling%20the%20denoising%0Aprocess%20of%20the%20diffusion%20model%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20framing%0Aconsistency%20model%20training%20as%20the%20value%20estimation%20through%20Temporal%0ADifference~%28TD%29%20Learning.%20More%20importantly%2C%20this%20framework%20allows%20us%20to%20analyze%0Athe%20limitations%20of%20current%20consistency%20training/tuning%20strategies.%20Built%20upon%0AEasy%20Consistency%20Tuning%20%28ECT%29%2C%20we%20propose%20Stable%20Consistency%20Tuning%20%28SCT%29%2C%0Awhich%20incorporates%20variance-reduced%20learning%20using%20the%20score%20identity.%20SCT%0Aleads%20to%20significant%20performance%20improvements%20on%20benchmarks%20such%20as%20CIFAR-10%0Aand%20ImageNet-64.%20On%20ImageNet-64%2C%20SCT%20achieves%201-step%20FID%202.42%20and%202-step%20FID%0A1.55%2C%20a%20new%20SoTA%20for%20consistency%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18958v2&entry.124074799=Read"},
{"title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation", "author": "Kaiqin Yang and Yixiang Dai and Guijin Wang and Siang Chen", "abstract": "  6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.\n", "link": "http://arxiv.org/abs/2410.22980v2", "date": "2024-11-14", "relevancy": 2.3657, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6219}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5829}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20End-to-End%206-Dof%20Grasp%20Detection%20Framework%20for%20Edge%20Devices%0A%20%20with%20Hierarchical%20Heatmaps%20and%20Feature%20Propagation&body=Title%3A%20Efficient%20End-to-End%206-Dof%20Grasp%20Detection%20Framework%20for%20Edge%20Devices%0A%20%20with%20Hierarchical%20Heatmaps%20and%20Feature%20Propagation%0AAuthor%3A%20Kaiqin%20Yang%20and%20Yixiang%20Dai%20and%20Guijin%20Wang%20and%20Siang%20Chen%0AAbstract%3A%20%20%206-DoF%20grasp%20detection%20is%20critically%20important%20for%20the%20advancement%20of%0Aintelligent%20embodied%20systems%2C%20as%20it%20provides%20feasible%20robot%20poses%20for%20object%0Agrasping.%20Various%20methods%20have%20been%20proposed%20to%20detect%206-DoF%20grasps%20through%20the%0Aextraction%20of%203D%20geometric%20features%20from%20RGBD%20or%20point%20cloud%20data.%20However%2C%0Amost%20of%20these%20approaches%20encounter%20challenges%20during%20real%20robot%20deployment%20due%0Ato%20their%20significant%20computational%20demands%2C%20which%20can%20be%20particularly%0Aproblematic%20for%20mobile%20robot%20platforms%2C%20especially%20those%20reliant%20on%20edge%0Acomputing%20devices.%20This%20paper%20presents%20an%20Efficient%20End-to-End%20Grasp%20Detection%0ANetwork%20%28E3GNet%29%20for%206-DoF%20grasp%20detection%20utilizing%20hierarchical%20heatmap%0Arepresentations.%20E3GNet%20effectively%20identifies%20high-quality%20and%20diverse%20grasps%0Ain%20cluttered%20real-world%20environments.%20Benefiting%20from%20our%20end-to-end%0Amethodology%20and%20efficient%20network%20design%2C%20our%20approach%20surpasses%20previous%0Amethods%20in%20model%20inference%20efficiency%20and%20achieves%20real-time%206-Dof%20grasp%0Adetection%20on%20edge%20devices.%20Furthermore%2C%20real-world%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20method%2C%20achieving%20a%20satisfactory%2094%25%20object%20grasping%0Asuccess%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520End-to-End%25206-Dof%2520Grasp%2520Detection%2520Framework%2520for%2520Edge%2520Devices%250A%2520%2520with%2520Hierarchical%2520Heatmaps%2520and%2520Feature%2520Propagation%26entry.906535625%3DKaiqin%2520Yang%2520and%2520Yixiang%2520Dai%2520and%2520Guijin%2520Wang%2520and%2520Siang%2520Chen%26entry.1292438233%3D%2520%25206-DoF%2520grasp%2520detection%2520is%2520critically%2520important%2520for%2520the%2520advancement%2520of%250Aintelligent%2520embodied%2520systems%252C%2520as%2520it%2520provides%2520feasible%2520robot%2520poses%2520for%2520object%250Agrasping.%2520Various%2520methods%2520have%2520been%2520proposed%2520to%2520detect%25206-DoF%2520grasps%2520through%2520the%250Aextraction%2520of%25203D%2520geometric%2520features%2520from%2520RGBD%2520or%2520point%2520cloud%2520data.%2520However%252C%250Amost%2520of%2520these%2520approaches%2520encounter%2520challenges%2520during%2520real%2520robot%2520deployment%2520due%250Ato%2520their%2520significant%2520computational%2520demands%252C%2520which%2520can%2520be%2520particularly%250Aproblematic%2520for%2520mobile%2520robot%2520platforms%252C%2520especially%2520those%2520reliant%2520on%2520edge%250Acomputing%2520devices.%2520This%2520paper%2520presents%2520an%2520Efficient%2520End-to-End%2520Grasp%2520Detection%250ANetwork%2520%2528E3GNet%2529%2520for%25206-DoF%2520grasp%2520detection%2520utilizing%2520hierarchical%2520heatmap%250Arepresentations.%2520E3GNet%2520effectively%2520identifies%2520high-quality%2520and%2520diverse%2520grasps%250Ain%2520cluttered%2520real-world%2520environments.%2520Benefiting%2520from%2520our%2520end-to-end%250Amethodology%2520and%2520efficient%2520network%2520design%252C%2520our%2520approach%2520surpasses%2520previous%250Amethods%2520in%2520model%2520inference%2520efficiency%2520and%2520achieves%2520real-time%25206-Dof%2520grasp%250Adetection%2520on%2520edge%2520devices.%2520Furthermore%252C%2520real-world%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method%252C%2520achieving%2520a%2520satisfactory%252094%2525%2520object%2520grasping%250Asuccess%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20End-to-End%206-Dof%20Grasp%20Detection%20Framework%20for%20Edge%20Devices%0A%20%20with%20Hierarchical%20Heatmaps%20and%20Feature%20Propagation&entry.906535625=Kaiqin%20Yang%20and%20Yixiang%20Dai%20and%20Guijin%20Wang%20and%20Siang%20Chen&entry.1292438233=%20%206-DoF%20grasp%20detection%20is%20critically%20important%20for%20the%20advancement%20of%0Aintelligent%20embodied%20systems%2C%20as%20it%20provides%20feasible%20robot%20poses%20for%20object%0Agrasping.%20Various%20methods%20have%20been%20proposed%20to%20detect%206-DoF%20grasps%20through%20the%0Aextraction%20of%203D%20geometric%20features%20from%20RGBD%20or%20point%20cloud%20data.%20However%2C%0Amost%20of%20these%20approaches%20encounter%20challenges%20during%20real%20robot%20deployment%20due%0Ato%20their%20significant%20computational%20demands%2C%20which%20can%20be%20particularly%0Aproblematic%20for%20mobile%20robot%20platforms%2C%20especially%20those%20reliant%20on%20edge%0Acomputing%20devices.%20This%20paper%20presents%20an%20Efficient%20End-to-End%20Grasp%20Detection%0ANetwork%20%28E3GNet%29%20for%206-DoF%20grasp%20detection%20utilizing%20hierarchical%20heatmap%0Arepresentations.%20E3GNet%20effectively%20identifies%20high-quality%20and%20diverse%20grasps%0Ain%20cluttered%20real-world%20environments.%20Benefiting%20from%20our%20end-to-end%0Amethodology%20and%20efficient%20network%20design%2C%20our%20approach%20surpasses%20previous%0Amethods%20in%20model%20inference%20efficiency%20and%20achieves%20real-time%206-Dof%20grasp%0Adetection%20on%20edge%20devices.%20Furthermore%2C%20real-world%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20method%2C%20achieving%20a%20satisfactory%2094%25%20object%20grasping%0Asuccess%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22980v2&entry.124074799=Read"},
{"title": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models", "author": "Cogan Shimizu and Pascal Hitzler", "abstract": "  Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.\n", "link": "http://arxiv.org/abs/2411.09601v1", "date": "2024-11-14", "relevancy": 2.3407, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Knowledge%20Graph%20and%20Ontology%20Engineering%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Accelerating%20Knowledge%20Graph%20and%20Ontology%20Engineering%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Cogan%20Shimizu%20and%20Pascal%20Hitzler%0AAbstract%3A%20%20%20Large%20Language%20Models%20bear%20the%20promise%20of%20significant%20acceleration%20of%20key%0AKnowledge%20Graph%20and%20Ontology%20Engineering%20tasks%2C%20including%20ontology%20modeling%2C%0Aextension%2C%20modification%2C%20population%2C%20alignment%2C%20as%20well%20as%20entity%0Adisambiguation.%20We%20lay%20out%20LLM-based%20Knowledge%20Graph%20and%20Ontology%20Engineering%0Aas%20a%20new%20and%20coming%20area%20of%20research%2C%20and%20argue%20that%20modular%20approaches%20to%0Aontologies%20will%20be%20of%20central%20importance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Knowledge%2520Graph%2520and%2520Ontology%2520Engineering%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DCogan%2520Shimizu%2520and%2520Pascal%2520Hitzler%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520bear%2520the%2520promise%2520of%2520significant%2520acceleration%2520of%2520key%250AKnowledge%2520Graph%2520and%2520Ontology%2520Engineering%2520tasks%252C%2520including%2520ontology%2520modeling%252C%250Aextension%252C%2520modification%252C%2520population%252C%2520alignment%252C%2520as%2520well%2520as%2520entity%250Adisambiguation.%2520We%2520lay%2520out%2520LLM-based%2520Knowledge%2520Graph%2520and%2520Ontology%2520Engineering%250Aas%2520a%2520new%2520and%2520coming%2520area%2520of%2520research%252C%2520and%2520argue%2520that%2520modular%2520approaches%2520to%250Aontologies%2520will%2520be%2520of%2520central%2520importance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Knowledge%20Graph%20and%20Ontology%20Engineering%20with%20Large%0A%20%20Language%20Models&entry.906535625=Cogan%20Shimizu%20and%20Pascal%20Hitzler&entry.1292438233=%20%20Large%20Language%20Models%20bear%20the%20promise%20of%20significant%20acceleration%20of%20key%0AKnowledge%20Graph%20and%20Ontology%20Engineering%20tasks%2C%20including%20ontology%20modeling%2C%0Aextension%2C%20modification%2C%20population%2C%20alignment%2C%20as%20well%20as%20entity%0Adisambiguation.%20We%20lay%20out%20LLM-based%20Knowledge%20Graph%20and%20Ontology%20Engineering%0Aas%20a%20new%20and%20coming%20area%20of%20research%2C%20and%20argue%20that%20modular%20approaches%20to%0Aontologies%20will%20be%20of%20central%20importance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09601v1&entry.124074799=Read"},
{"title": "Uncovering communities of pipelines in the task-fMRI analytical space", "author": "Elodie Germani and Elisa Fromont and Camille Maumet", "abstract": "  Analytical workflows in functional magnetic resonance imaging are highly\nflexible with limited best practices as to how to choose a pipeline. While it\nhas been shown that the use of different pipelines might lead to different\nresults, there is still a lack of understanding of the factors that drive these\ndifferences and of the stability of these differences across contexts. We use\ncommunity detection algorithms to explore the pipeline space and assess the\nstability of pipeline relationships across different contexts. We show that\nthere are subsets of pipelines that give similar results, especially those\nsharing specific parameters (e.g. number of motion regressors, software\npackages, etc.). Those pipeline-to-pipeline patterns are stable across groups\nof participants but not across different tasks. By visualizing the differences\nbetween communities, we show that the pipeline space is mainly driven by the\nsize of the activation area in the brain and the scale of statistic values in\nstatistic maps.\n", "link": "http://arxiv.org/abs/2312.06231v4", "date": "2024-11-14", "relevancy": 2.3355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20communities%20of%20pipelines%20in%20the%20task-fMRI%20analytical%20space&body=Title%3A%20Uncovering%20communities%20of%20pipelines%20in%20the%20task-fMRI%20analytical%20space%0AAuthor%3A%20Elodie%20Germani%20and%20Elisa%20Fromont%20and%20Camille%20Maumet%0AAbstract%3A%20%20%20Analytical%20workflows%20in%20functional%20magnetic%20resonance%20imaging%20are%20highly%0Aflexible%20with%20limited%20best%20practices%20as%20to%20how%20to%20choose%20a%20pipeline.%20While%20it%0Ahas%20been%20shown%20that%20the%20use%20of%20different%20pipelines%20might%20lead%20to%20different%0Aresults%2C%20there%20is%20still%20a%20lack%20of%20understanding%20of%20the%20factors%20that%20drive%20these%0Adifferences%20and%20of%20the%20stability%20of%20these%20differences%20across%20contexts.%20We%20use%0Acommunity%20detection%20algorithms%20to%20explore%20the%20pipeline%20space%20and%20assess%20the%0Astability%20of%20pipeline%20relationships%20across%20different%20contexts.%20We%20show%20that%0Athere%20are%20subsets%20of%20pipelines%20that%20give%20similar%20results%2C%20especially%20those%0Asharing%20specific%20parameters%20%28e.g.%20number%20of%20motion%20regressors%2C%20software%0Apackages%2C%20etc.%29.%20Those%20pipeline-to-pipeline%20patterns%20are%20stable%20across%20groups%0Aof%20participants%20but%20not%20across%20different%20tasks.%20By%20visualizing%20the%20differences%0Abetween%20communities%2C%20we%20show%20that%20the%20pipeline%20space%20is%20mainly%20driven%20by%20the%0Asize%20of%20the%20activation%20area%20in%20the%20brain%20and%20the%20scale%20of%20statistic%20values%20in%0Astatistic%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06231v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520communities%2520of%2520pipelines%2520in%2520the%2520task-fMRI%2520analytical%2520space%26entry.906535625%3DElodie%2520Germani%2520and%2520Elisa%2520Fromont%2520and%2520Camille%2520Maumet%26entry.1292438233%3D%2520%2520Analytical%2520workflows%2520in%2520functional%2520magnetic%2520resonance%2520imaging%2520are%2520highly%250Aflexible%2520with%2520limited%2520best%2520practices%2520as%2520to%2520how%2520to%2520choose%2520a%2520pipeline.%2520While%2520it%250Ahas%2520been%2520shown%2520that%2520the%2520use%2520of%2520different%2520pipelines%2520might%2520lead%2520to%2520different%250Aresults%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520understanding%2520of%2520the%2520factors%2520that%2520drive%2520these%250Adifferences%2520and%2520of%2520the%2520stability%2520of%2520these%2520differences%2520across%2520contexts.%2520We%2520use%250Acommunity%2520detection%2520algorithms%2520to%2520explore%2520the%2520pipeline%2520space%2520and%2520assess%2520the%250Astability%2520of%2520pipeline%2520relationships%2520across%2520different%2520contexts.%2520We%2520show%2520that%250Athere%2520are%2520subsets%2520of%2520pipelines%2520that%2520give%2520similar%2520results%252C%2520especially%2520those%250Asharing%2520specific%2520parameters%2520%2528e.g.%2520number%2520of%2520motion%2520regressors%252C%2520software%250Apackages%252C%2520etc.%2529.%2520Those%2520pipeline-to-pipeline%2520patterns%2520are%2520stable%2520across%2520groups%250Aof%2520participants%2520but%2520not%2520across%2520different%2520tasks.%2520By%2520visualizing%2520the%2520differences%250Abetween%2520communities%252C%2520we%2520show%2520that%2520the%2520pipeline%2520space%2520is%2520mainly%2520driven%2520by%2520the%250Asize%2520of%2520the%2520activation%2520area%2520in%2520the%2520brain%2520and%2520the%2520scale%2520of%2520statistic%2520values%2520in%250Astatistic%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06231v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20communities%20of%20pipelines%20in%20the%20task-fMRI%20analytical%20space&entry.906535625=Elodie%20Germani%20and%20Elisa%20Fromont%20and%20Camille%20Maumet&entry.1292438233=%20%20Analytical%20workflows%20in%20functional%20magnetic%20resonance%20imaging%20are%20highly%0Aflexible%20with%20limited%20best%20practices%20as%20to%20how%20to%20choose%20a%20pipeline.%20While%20it%0Ahas%20been%20shown%20that%20the%20use%20of%20different%20pipelines%20might%20lead%20to%20different%0Aresults%2C%20there%20is%20still%20a%20lack%20of%20understanding%20of%20the%20factors%20that%20drive%20these%0Adifferences%20and%20of%20the%20stability%20of%20these%20differences%20across%20contexts.%20We%20use%0Acommunity%20detection%20algorithms%20to%20explore%20the%20pipeline%20space%20and%20assess%20the%0Astability%20of%20pipeline%20relationships%20across%20different%20contexts.%20We%20show%20that%0Athere%20are%20subsets%20of%20pipelines%20that%20give%20similar%20results%2C%20especially%20those%0Asharing%20specific%20parameters%20%28e.g.%20number%20of%20motion%20regressors%2C%20software%0Apackages%2C%20etc.%29.%20Those%20pipeline-to-pipeline%20patterns%20are%20stable%20across%20groups%0Aof%20participants%20but%20not%20across%20different%20tasks.%20By%20visualizing%20the%20differences%0Abetween%20communities%2C%20we%20show%20that%20the%20pipeline%20space%20is%20mainly%20driven%20by%20the%0Asize%20of%20the%20activation%20area%20in%20the%20brain%20and%20the%20scale%20of%20statistic%20values%20in%0Astatistic%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06231v4&entry.124074799=Read"},
{"title": "ShanghaiTech Mapping Robot is All You Need: Robot System for Collecting\n  Universal Ground Vehicle Datasets", "author": "Bowen Xu and Xiting Zhao and Delin Feng and Yuanyuan Yang and S\u00f6ren Schwertfeger", "abstract": "  This paper presents the ShanghaiTech Mapping Robot, a state-of-the-art\nunmanned ground vehicle (UGV) designed for collecting comprehensive\nmulti-sensor datasets to support research in robotics, Simultaneous\nLocalization and Mapping (SLAM), computer vision, and autonomous driving. The\nrobot is equipped with a wide array of sensors including RGB cameras, RGB-D\ncameras, event-based cameras, IR cameras, LiDARs, mmWave radars, IMUs,\nultrasonic range finders, and a GNSS RTK receiver. The sensor suite is\nintegrated onto a specially designed mechanical structure with a centralized\npower system and a synchronization mechanism to ensure spatial and temporal\nalignment of the sensor data. A 16-node on-board computing cluster handles\nsensor control, data collection, and storage. We describe the hardware and\nsoftware architecture of the robot in detail and discuss the calibration\nprocedures for the various sensors and investigate the interference for LiDAR\nand RGB-D sensors. The capabilities of the platform are demonstrated through an\nextensive outdoor dataset collected in a diverse campus environment.\nExperiments with two LiDAR-based and two RGB-based SLAM approaches showcase the\npotential of the dataset to support development and benchmarking for robotics.\nTo facilitate research, we make the dataset publicly available along with the\nassociated robot sensor calibration data:\nhttps://slam-hive.net/wiki/ShanghaiTech_Datasets\n", "link": "http://arxiv.org/abs/2406.16713v4", "date": "2024-11-14", "relevancy": 2.3288, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&body=Title%3A%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets%0AAuthor%3A%20Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%2C%20computer%20vision%2C%20and%20autonomous%20driving.%20The%0Arobot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%20including%20RGB%20cameras%2C%20RGB-D%0Acameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%20mmWave%20radars%2C%20IMUs%2C%0Aultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%20sensor%20suite%20is%0Aintegrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%20a%20centralized%0Apower%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%20and%20temporal%0Aalignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%20handles%0Asensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%20and%0Asoftware%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors%20and%20investigate%20the%20interference%20for%20LiDAR%0Aand%20RGB-D%20sensors.%20The%20capabilities%20of%20the%20platform%20are%20demonstrated%20through%20an%0Aextensive%20outdoor%20dataset%20collected%20in%20a%20diverse%20campus%20environment.%0AExperiments%20with%20two%20LiDAR-based%20and%20two%20RGB-based%20SLAM%20approaches%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20development%20and%20benchmarking%20for%20robotics.%0ATo%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%20along%20with%20the%0Aassociated%20robot%20sensor%20calibration%20data%3A%0Ahttps%3A//slam-hive.net/wiki/ShanghaiTech_Datasets%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16713v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShanghaiTech%2520Mapping%2520Robot%2520is%2520All%2520You%2520Need%253A%2520Robot%2520System%2520for%2520Collecting%250A%2520%2520Universal%2520Ground%2520Vehicle%2520Datasets%26entry.906535625%3DBowen%2520Xu%2520and%2520Xiting%2520Zhao%2520and%2520Delin%2520Feng%2520and%2520Yuanyuan%2520Yang%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520ShanghaiTech%2520Mapping%2520Robot%252C%2520a%2520state-of-the-art%250Aunmanned%2520ground%2520vehicle%2520%2528UGV%2529%2520designed%2520for%2520collecting%2520comprehensive%250Amulti-sensor%2520datasets%2520to%2520support%2520research%2520in%2520robotics%252C%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520computer%2520vision%252C%2520and%2520autonomous%2520driving.%2520The%250Arobot%2520is%2520equipped%2520with%2520a%2520wide%2520array%2520of%2520sensors%2520including%2520RGB%2520cameras%252C%2520RGB-D%250Acameras%252C%2520event-based%2520cameras%252C%2520IR%2520cameras%252C%2520LiDARs%252C%2520mmWave%2520radars%252C%2520IMUs%252C%250Aultrasonic%2520range%2520finders%252C%2520and%2520a%2520GNSS%2520RTK%2520receiver.%2520The%2520sensor%2520suite%2520is%250Aintegrated%2520onto%2520a%2520specially%2520designed%2520mechanical%2520structure%2520with%2520a%2520centralized%250Apower%2520system%2520and%2520a%2520synchronization%2520mechanism%2520to%2520ensure%2520spatial%2520and%2520temporal%250Aalignment%2520of%2520the%2520sensor%2520data.%2520A%252016-node%2520on-board%2520computing%2520cluster%2520handles%250Asensor%2520control%252C%2520data%2520collection%252C%2520and%2520storage.%2520We%2520describe%2520the%2520hardware%2520and%250Asoftware%2520architecture%2520of%2520the%2520robot%2520in%2520detail%2520and%2520discuss%2520the%2520calibration%250Aprocedures%2520for%2520the%2520various%2520sensors%2520and%2520investigate%2520the%2520interference%2520for%2520LiDAR%250Aand%2520RGB-D%2520sensors.%2520The%2520capabilities%2520of%2520the%2520platform%2520are%2520demonstrated%2520through%2520an%250Aextensive%2520outdoor%2520dataset%2520collected%2520in%2520a%2520diverse%2520campus%2520environment.%250AExperiments%2520with%2520two%2520LiDAR-based%2520and%2520two%2520RGB-based%2520SLAM%2520approaches%2520showcase%2520the%250Apotential%2520of%2520the%2520dataset%2520to%2520support%2520development%2520and%2520benchmarking%2520for%2520robotics.%250ATo%2520facilitate%2520research%252C%2520we%2520make%2520the%2520dataset%2520publicly%2520available%2520along%2520with%2520the%250Aassociated%2520robot%2520sensor%2520calibration%2520data%253A%250Ahttps%253A//slam-hive.net/wiki/ShanghaiTech_Datasets%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16713v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&entry.906535625=Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%2C%20computer%20vision%2C%20and%20autonomous%20driving.%20The%0Arobot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%20including%20RGB%20cameras%2C%20RGB-D%0Acameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%20mmWave%20radars%2C%20IMUs%2C%0Aultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%20sensor%20suite%20is%0Aintegrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%20a%20centralized%0Apower%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%20and%20temporal%0Aalignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%20handles%0Asensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%20and%0Asoftware%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors%20and%20investigate%20the%20interference%20for%20LiDAR%0Aand%20RGB-D%20sensors.%20The%20capabilities%20of%20the%20platform%20are%20demonstrated%20through%20an%0Aextensive%20outdoor%20dataset%20collected%20in%20a%20diverse%20campus%20environment.%0AExperiments%20with%20two%20LiDAR-based%20and%20two%20RGB-based%20SLAM%20approaches%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20development%20and%20benchmarking%20for%20robotics.%0ATo%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%20along%20with%20the%0Aassociated%20robot%20sensor%20calibration%20data%3A%0Ahttps%3A//slam-hive.net/wiki/ShanghaiTech_Datasets%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16713v4&entry.124074799=Read"},
{"title": "On the Surprising Effectiveness of Attention Transfer for Vision\n  Transformers", "author": "Alexander C. Li and Yuandong Tian and Beidi Chen and Deepak Pathak and Xinlei Chen", "abstract": "  Conventional wisdom suggests that pre-training Vision Transformers (ViT)\nimproves downstream performance by learning useful representations. Is this\nactually true? We investigate this question and find that the features and\nrepresentations learned during pre-training are not essential. Surprisingly,\nusing only the attention patterns from pre-training (i.e., guiding how\ninformation flows between tokens) is sufficient for models to learn high\nquality features from scratch and achieve comparable downstream performance. We\nshow this by introducing a simple method called attention transfer, where only\nthe attention patterns from a pre-trained teacher ViT are transferred to a\nstudent, either by copying or distilling the attention maps. Since attention\ntransfer lets the student learn its own features, ensembling it with a\nfine-tuned teacher also further improves accuracy on ImageNet. We\nsystematically study various aspects of our findings on the sufficiency of\nattention maps, including distribution shift settings where they underperform\nfine-tuning. We hope our exploration provides a better understanding of what\npre-training accomplishes and leads to a useful alternative to the standard\npractice of fine-tuning\n", "link": "http://arxiv.org/abs/2411.09702v1", "date": "2024-11-14", "relevancy": 2.3014, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5907}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Surprising%20Effectiveness%20of%20Attention%20Transfer%20for%20Vision%0A%20%20Transformers&body=Title%3A%20On%20the%20Surprising%20Effectiveness%20of%20Attention%20Transfer%20for%20Vision%0A%20%20Transformers%0AAuthor%3A%20Alexander%20C.%20Li%20and%20Yuandong%20Tian%20and%20Beidi%20Chen%20and%20Deepak%20Pathak%20and%20Xinlei%20Chen%0AAbstract%3A%20%20%20Conventional%20wisdom%20suggests%20that%20pre-training%20Vision%20Transformers%20%28ViT%29%0Aimproves%20downstream%20performance%20by%20learning%20useful%20representations.%20Is%20this%0Aactually%20true%3F%20We%20investigate%20this%20question%20and%20find%20that%20the%20features%20and%0Arepresentations%20learned%20during%20pre-training%20are%20not%20essential.%20Surprisingly%2C%0Ausing%20only%20the%20attention%20patterns%20from%20pre-training%20%28i.e.%2C%20guiding%20how%0Ainformation%20flows%20between%20tokens%29%20is%20sufficient%20for%20models%20to%20learn%20high%0Aquality%20features%20from%20scratch%20and%20achieve%20comparable%20downstream%20performance.%20We%0Ashow%20this%20by%20introducing%20a%20simple%20method%20called%20attention%20transfer%2C%20where%20only%0Athe%20attention%20patterns%20from%20a%20pre-trained%20teacher%20ViT%20are%20transferred%20to%20a%0Astudent%2C%20either%20by%20copying%20or%20distilling%20the%20attention%20maps.%20Since%20attention%0Atransfer%20lets%20the%20student%20learn%20its%20own%20features%2C%20ensembling%20it%20with%20a%0Afine-tuned%20teacher%20also%20further%20improves%20accuracy%20on%20ImageNet.%20We%0Asystematically%20study%20various%20aspects%20of%20our%20findings%20on%20the%20sufficiency%20of%0Aattention%20maps%2C%20including%20distribution%20shift%20settings%20where%20they%20underperform%0Afine-tuning.%20We%20hope%20our%20exploration%20provides%20a%20better%20understanding%20of%20what%0Apre-training%20accomplishes%20and%20leads%20to%20a%20useful%20alternative%20to%20the%20standard%0Apractice%20of%20fine-tuning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Surprising%2520Effectiveness%2520of%2520Attention%2520Transfer%2520for%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DAlexander%2520C.%2520Li%2520and%2520Yuandong%2520Tian%2520and%2520Beidi%2520Chen%2520and%2520Deepak%2520Pathak%2520and%2520Xinlei%2520Chen%26entry.1292438233%3D%2520%2520Conventional%2520wisdom%2520suggests%2520that%2520pre-training%2520Vision%2520Transformers%2520%2528ViT%2529%250Aimproves%2520downstream%2520performance%2520by%2520learning%2520useful%2520representations.%2520Is%2520this%250Aactually%2520true%253F%2520We%2520investigate%2520this%2520question%2520and%2520find%2520that%2520the%2520features%2520and%250Arepresentations%2520learned%2520during%2520pre-training%2520are%2520not%2520essential.%2520Surprisingly%252C%250Ausing%2520only%2520the%2520attention%2520patterns%2520from%2520pre-training%2520%2528i.e.%252C%2520guiding%2520how%250Ainformation%2520flows%2520between%2520tokens%2529%2520is%2520sufficient%2520for%2520models%2520to%2520learn%2520high%250Aquality%2520features%2520from%2520scratch%2520and%2520achieve%2520comparable%2520downstream%2520performance.%2520We%250Ashow%2520this%2520by%2520introducing%2520a%2520simple%2520method%2520called%2520attention%2520transfer%252C%2520where%2520only%250Athe%2520attention%2520patterns%2520from%2520a%2520pre-trained%2520teacher%2520ViT%2520are%2520transferred%2520to%2520a%250Astudent%252C%2520either%2520by%2520copying%2520or%2520distilling%2520the%2520attention%2520maps.%2520Since%2520attention%250Atransfer%2520lets%2520the%2520student%2520learn%2520its%2520own%2520features%252C%2520ensembling%2520it%2520with%2520a%250Afine-tuned%2520teacher%2520also%2520further%2520improves%2520accuracy%2520on%2520ImageNet.%2520We%250Asystematically%2520study%2520various%2520aspects%2520of%2520our%2520findings%2520on%2520the%2520sufficiency%2520of%250Aattention%2520maps%252C%2520including%2520distribution%2520shift%2520settings%2520where%2520they%2520underperform%250Afine-tuning.%2520We%2520hope%2520our%2520exploration%2520provides%2520a%2520better%2520understanding%2520of%2520what%250Apre-training%2520accomplishes%2520and%2520leads%2520to%2520a%2520useful%2520alternative%2520to%2520the%2520standard%250Apractice%2520of%2520fine-tuning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Surprising%20Effectiveness%20of%20Attention%20Transfer%20for%20Vision%0A%20%20Transformers&entry.906535625=Alexander%20C.%20Li%20and%20Yuandong%20Tian%20and%20Beidi%20Chen%20and%20Deepak%20Pathak%20and%20Xinlei%20Chen&entry.1292438233=%20%20Conventional%20wisdom%20suggests%20that%20pre-training%20Vision%20Transformers%20%28ViT%29%0Aimproves%20downstream%20performance%20by%20learning%20useful%20representations.%20Is%20this%0Aactually%20true%3F%20We%20investigate%20this%20question%20and%20find%20that%20the%20features%20and%0Arepresentations%20learned%20during%20pre-training%20are%20not%20essential.%20Surprisingly%2C%0Ausing%20only%20the%20attention%20patterns%20from%20pre-training%20%28i.e.%2C%20guiding%20how%0Ainformation%20flows%20between%20tokens%29%20is%20sufficient%20for%20models%20to%20learn%20high%0Aquality%20features%20from%20scratch%20and%20achieve%20comparable%20downstream%20performance.%20We%0Ashow%20this%20by%20introducing%20a%20simple%20method%20called%20attention%20transfer%2C%20where%20only%0Athe%20attention%20patterns%20from%20a%20pre-trained%20teacher%20ViT%20are%20transferred%20to%20a%0Astudent%2C%20either%20by%20copying%20or%20distilling%20the%20attention%20maps.%20Since%20attention%0Atransfer%20lets%20the%20student%20learn%20its%20own%20features%2C%20ensembling%20it%20with%20a%0Afine-tuned%20teacher%20also%20further%20improves%20accuracy%20on%20ImageNet.%20We%0Asystematically%20study%20various%20aspects%20of%20our%20findings%20on%20the%20sufficiency%20of%0Aattention%20maps%2C%20including%20distribution%20shift%20settings%20where%20they%20underperform%0Afine-tuning.%20We%20hope%20our%20exploration%20provides%20a%20better%20understanding%20of%20what%0Apre-training%20accomplishes%20and%20leads%20to%20a%20useful%20alternative%20to%20the%20standard%0Apractice%20of%20fine-tuning%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09702v1&entry.124074799=Read"},
{"title": "ResBit: Residual Bit Vector for Categorical Values", "author": "Masane Fuchi and Amar Zanashir and Hiroto Minami and Tomohiro Takagi", "abstract": "  One-hot vectors, a common method for representing discrete/categorical data,\nin machine learning are widely used because of their simplicity and\nintuitiveness. However, one-hot vectors suffer from a linear increase in\ndimensionality, posing computational and memory challenges, especially when\ndealing with datasets containing numerous categories. In this paper, we focus\non tabular data generation, and reveal the multinomial diffusion faces the mode\ncollapse phenomenon when the cardinality is high. Moreover, due to the\nlimitations of one-hot vectors, the training phase takes time longer in such a\nsituation. To address these issues, we propose Residual Bit Vectors (ResBit), a\ntechnique for densely representing categorical data. ResBit is an extension of\nanalog bits and overcomes limitations of analog bits when applied to tabular\ndata generation. Our experiments demonstrate that ResBit not only accelerates\ntraining but also maintains performance when compared with the situations\nbefore applying ResBit. Furthermore, our results indicate that many existing\nmethods struggle with high-cardinality data, underscoring the need for\nlower-dimensional representations, such as ResBit and latent vectors.\n", "link": "http://arxiv.org/abs/2309.17196v4", "date": "2024-11-14", "relevancy": 2.2971, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4752}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4567}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResBit%3A%20Residual%20Bit%20Vector%20for%20Categorical%20Values&body=Title%3A%20ResBit%3A%20Residual%20Bit%20Vector%20for%20Categorical%20Values%0AAuthor%3A%20Masane%20Fuchi%20and%20Amar%20Zanashir%20and%20Hiroto%20Minami%20and%20Tomohiro%20Takagi%0AAbstract%3A%20%20%20One-hot%20vectors%2C%20a%20common%20method%20for%20representing%20discrete/categorical%20data%2C%0Ain%20machine%20learning%20are%20widely%20used%20because%20of%20their%20simplicity%20and%0Aintuitiveness.%20However%2C%20one-hot%20vectors%20suffer%20from%20a%20linear%20increase%20in%0Adimensionality%2C%20posing%20computational%20and%20memory%20challenges%2C%20especially%20when%0Adealing%20with%20datasets%20containing%20numerous%20categories.%20In%20this%20paper%2C%20we%20focus%0Aon%20tabular%20data%20generation%2C%20and%20reveal%20the%20multinomial%20diffusion%20faces%20the%20mode%0Acollapse%20phenomenon%20when%20the%20cardinality%20is%20high.%20Moreover%2C%20due%20to%20the%0Alimitations%20of%20one-hot%20vectors%2C%20the%20training%20phase%20takes%20time%20longer%20in%20such%20a%0Asituation.%20To%20address%20these%20issues%2C%20we%20propose%20Residual%20Bit%20Vectors%20%28ResBit%29%2C%20a%0Atechnique%20for%20densely%20representing%20categorical%20data.%20ResBit%20is%20an%20extension%20of%0Aanalog%20bits%20and%20overcomes%20limitations%20of%20analog%20bits%20when%20applied%20to%20tabular%0Adata%20generation.%20Our%20experiments%20demonstrate%20that%20ResBit%20not%20only%20accelerates%0Atraining%20but%20also%20maintains%20performance%20when%20compared%20with%20the%20situations%0Abefore%20applying%20ResBit.%20Furthermore%2C%20our%20results%20indicate%20that%20many%20existing%0Amethods%20struggle%20with%20high-cardinality%20data%2C%20underscoring%20the%20need%20for%0Alower-dimensional%20representations%2C%20such%20as%20ResBit%20and%20latent%20vectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17196v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResBit%253A%2520Residual%2520Bit%2520Vector%2520for%2520Categorical%2520Values%26entry.906535625%3DMasane%2520Fuchi%2520and%2520Amar%2520Zanashir%2520and%2520Hiroto%2520Minami%2520and%2520Tomohiro%2520Takagi%26entry.1292438233%3D%2520%2520One-hot%2520vectors%252C%2520a%2520common%2520method%2520for%2520representing%2520discrete/categorical%2520data%252C%250Ain%2520machine%2520learning%2520are%2520widely%2520used%2520because%2520of%2520their%2520simplicity%2520and%250Aintuitiveness.%2520However%252C%2520one-hot%2520vectors%2520suffer%2520from%2520a%2520linear%2520increase%2520in%250Adimensionality%252C%2520posing%2520computational%2520and%2520memory%2520challenges%252C%2520especially%2520when%250Adealing%2520with%2520datasets%2520containing%2520numerous%2520categories.%2520In%2520this%2520paper%252C%2520we%2520focus%250Aon%2520tabular%2520data%2520generation%252C%2520and%2520reveal%2520the%2520multinomial%2520diffusion%2520faces%2520the%2520mode%250Acollapse%2520phenomenon%2520when%2520the%2520cardinality%2520is%2520high.%2520Moreover%252C%2520due%2520to%2520the%250Alimitations%2520of%2520one-hot%2520vectors%252C%2520the%2520training%2520phase%2520takes%2520time%2520longer%2520in%2520such%2520a%250Asituation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Residual%2520Bit%2520Vectors%2520%2528ResBit%2529%252C%2520a%250Atechnique%2520for%2520densely%2520representing%2520categorical%2520data.%2520ResBit%2520is%2520an%2520extension%2520of%250Aanalog%2520bits%2520and%2520overcomes%2520limitations%2520of%2520analog%2520bits%2520when%2520applied%2520to%2520tabular%250Adata%2520generation.%2520Our%2520experiments%2520demonstrate%2520that%2520ResBit%2520not%2520only%2520accelerates%250Atraining%2520but%2520also%2520maintains%2520performance%2520when%2520compared%2520with%2520the%2520situations%250Abefore%2520applying%2520ResBit.%2520Furthermore%252C%2520our%2520results%2520indicate%2520that%2520many%2520existing%250Amethods%2520struggle%2520with%2520high-cardinality%2520data%252C%2520underscoring%2520the%2520need%2520for%250Alower-dimensional%2520representations%252C%2520such%2520as%2520ResBit%2520and%2520latent%2520vectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17196v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResBit%3A%20Residual%20Bit%20Vector%20for%20Categorical%20Values&entry.906535625=Masane%20Fuchi%20and%20Amar%20Zanashir%20and%20Hiroto%20Minami%20and%20Tomohiro%20Takagi&entry.1292438233=%20%20One-hot%20vectors%2C%20a%20common%20method%20for%20representing%20discrete/categorical%20data%2C%0Ain%20machine%20learning%20are%20widely%20used%20because%20of%20their%20simplicity%20and%0Aintuitiveness.%20However%2C%20one-hot%20vectors%20suffer%20from%20a%20linear%20increase%20in%0Adimensionality%2C%20posing%20computational%20and%20memory%20challenges%2C%20especially%20when%0Adealing%20with%20datasets%20containing%20numerous%20categories.%20In%20this%20paper%2C%20we%20focus%0Aon%20tabular%20data%20generation%2C%20and%20reveal%20the%20multinomial%20diffusion%20faces%20the%20mode%0Acollapse%20phenomenon%20when%20the%20cardinality%20is%20high.%20Moreover%2C%20due%20to%20the%0Alimitations%20of%20one-hot%20vectors%2C%20the%20training%20phase%20takes%20time%20longer%20in%20such%20a%0Asituation.%20To%20address%20these%20issues%2C%20we%20propose%20Residual%20Bit%20Vectors%20%28ResBit%29%2C%20a%0Atechnique%20for%20densely%20representing%20categorical%20data.%20ResBit%20is%20an%20extension%20of%0Aanalog%20bits%20and%20overcomes%20limitations%20of%20analog%20bits%20when%20applied%20to%20tabular%0Adata%20generation.%20Our%20experiments%20demonstrate%20that%20ResBit%20not%20only%20accelerates%0Atraining%20but%20also%20maintains%20performance%20when%20compared%20with%20the%20situations%0Abefore%20applying%20ResBit.%20Furthermore%2C%20our%20results%20indicate%20that%20many%20existing%0Amethods%20struggle%20with%20high-cardinality%20data%2C%20underscoring%20the%20need%20for%0Alower-dimensional%20representations%2C%20such%20as%20ResBit%20and%20latent%20vectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17196v4&entry.124074799=Read"},
{"title": "Time-to-Event Pretraining for 3D Medical Imaging", "author": "Zepeng Huo and Jason Alan Fries and Alejandro Lozano and Jeya Maria Jose Valanarasu and Ethan Steinberg and Louis Blankemeier and Akshay S. Chaudhari and Curtis Langlotz and Nigam H. Shah", "abstract": "  With the rise of medical foundation models and the growing availability of\nimaging data, scalable pretraining techniques offer a promising way to identify\nimaging biomarkers predictive of future disease risk. While current\nself-supervised methods for 3D medical imaging models capture local structural\nfeatures like organ morphology, they fail to link pixel biomarkers with\nlong-term health outcomes due to a missing context problem. Current approaches\nlack the temporal context necessary to identify biomarkers correlated with\ndisease progression, as they rely on supervision derived only from images and\nconcurrent text descriptions. To address this, we introduce time-to-event\npretraining, a pretraining framework for 3D medical imaging models that\nleverages large-scale temporal supervision from paired, longitudinal electronic\nhealth records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D\nimages) and time-to-event distributions across thousands of EHR-derived tasks,\nour method improves outcome prediction, achieving an average AUROC increase of\n23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks.\nImportantly, these gains are achieved without sacrificing diagnostic\nclassification performance. This study lays the foundation for integrating\nlongitudinal EHR and 3D imaging data to advance clinical risk prediction.\n", "link": "http://arxiv.org/abs/2411.09361v1", "date": "2024-11-14", "relevancy": 2.237, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5662}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-to-Event%20Pretraining%20for%203D%20Medical%20Imaging&body=Title%3A%20Time-to-Event%20Pretraining%20for%203D%20Medical%20Imaging%0AAuthor%3A%20Zepeng%20Huo%20and%20Jason%20Alan%20Fries%20and%20Alejandro%20Lozano%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Ethan%20Steinberg%20and%20Louis%20Blankemeier%20and%20Akshay%20S.%20Chaudhari%20and%20Curtis%20Langlotz%20and%20Nigam%20H.%20Shah%0AAbstract%3A%20%20%20With%20the%20rise%20of%20medical%20foundation%20models%20and%20the%20growing%20availability%20of%0Aimaging%20data%2C%20scalable%20pretraining%20techniques%20offer%20a%20promising%20way%20to%20identify%0Aimaging%20biomarkers%20predictive%20of%20future%20disease%20risk.%20While%20current%0Aself-supervised%20methods%20for%203D%20medical%20imaging%20models%20capture%20local%20structural%0Afeatures%20like%20organ%20morphology%2C%20they%20fail%20to%20link%20pixel%20biomarkers%20with%0Along-term%20health%20outcomes%20due%20to%20a%20missing%20context%20problem.%20Current%20approaches%0Alack%20the%20temporal%20context%20necessary%20to%20identify%20biomarkers%20correlated%20with%0Adisease%20progression%2C%20as%20they%20rely%20on%20supervision%20derived%20only%20from%20images%20and%0Aconcurrent%20text%20descriptions.%20To%20address%20this%2C%20we%20introduce%20time-to-event%0Apretraining%2C%20a%20pretraining%20framework%20for%203D%20medical%20imaging%20models%20that%0Aleverages%20large-scale%20temporal%20supervision%20from%20paired%2C%20longitudinal%20electronic%0Ahealth%20records%20%28EHRs%29.%20Using%20a%20dataset%20of%2018%2C945%20CT%20scans%20%284.2%20million%202D%0Aimages%29%20and%20time-to-event%20distributions%20across%20thousands%20of%20EHR-derived%20tasks%2C%0Aour%20method%20improves%20outcome%20prediction%2C%20achieving%20an%20average%20AUROC%20increase%20of%0A23.7%25%20and%20a%2029.4%25%20gain%20in%20Harrell%27s%20C-index%20across%208%20benchmark%20tasks.%0AImportantly%2C%20these%20gains%20are%20achieved%20without%20sacrificing%20diagnostic%0Aclassification%20performance.%20This%20study%20lays%20the%20foundation%20for%20integrating%0Alongitudinal%20EHR%20and%203D%20imaging%20data%20to%20advance%20clinical%20risk%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-to-Event%2520Pretraining%2520for%25203D%2520Medical%2520Imaging%26entry.906535625%3DZepeng%2520Huo%2520and%2520Jason%2520Alan%2520Fries%2520and%2520Alejandro%2520Lozano%2520and%2520Jeya%2520Maria%2520Jose%2520Valanarasu%2520and%2520Ethan%2520Steinberg%2520and%2520Louis%2520Blankemeier%2520and%2520Akshay%2520S.%2520Chaudhari%2520and%2520Curtis%2520Langlotz%2520and%2520Nigam%2520H.%2520Shah%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520medical%2520foundation%2520models%2520and%2520the%2520growing%2520availability%2520of%250Aimaging%2520data%252C%2520scalable%2520pretraining%2520techniques%2520offer%2520a%2520promising%2520way%2520to%2520identify%250Aimaging%2520biomarkers%2520predictive%2520of%2520future%2520disease%2520risk.%2520While%2520current%250Aself-supervised%2520methods%2520for%25203D%2520medical%2520imaging%2520models%2520capture%2520local%2520structural%250Afeatures%2520like%2520organ%2520morphology%252C%2520they%2520fail%2520to%2520link%2520pixel%2520biomarkers%2520with%250Along-term%2520health%2520outcomes%2520due%2520to%2520a%2520missing%2520context%2520problem.%2520Current%2520approaches%250Alack%2520the%2520temporal%2520context%2520necessary%2520to%2520identify%2520biomarkers%2520correlated%2520with%250Adisease%2520progression%252C%2520as%2520they%2520rely%2520on%2520supervision%2520derived%2520only%2520from%2520images%2520and%250Aconcurrent%2520text%2520descriptions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520time-to-event%250Apretraining%252C%2520a%2520pretraining%2520framework%2520for%25203D%2520medical%2520imaging%2520models%2520that%250Aleverages%2520large-scale%2520temporal%2520supervision%2520from%2520paired%252C%2520longitudinal%2520electronic%250Ahealth%2520records%2520%2528EHRs%2529.%2520Using%2520a%2520dataset%2520of%252018%252C945%2520CT%2520scans%2520%25284.2%2520million%25202D%250Aimages%2529%2520and%2520time-to-event%2520distributions%2520across%2520thousands%2520of%2520EHR-derived%2520tasks%252C%250Aour%2520method%2520improves%2520outcome%2520prediction%252C%2520achieving%2520an%2520average%2520AUROC%2520increase%2520of%250A23.7%2525%2520and%2520a%252029.4%2525%2520gain%2520in%2520Harrell%2527s%2520C-index%2520across%25208%2520benchmark%2520tasks.%250AImportantly%252C%2520these%2520gains%2520are%2520achieved%2520without%2520sacrificing%2520diagnostic%250Aclassification%2520performance.%2520This%2520study%2520lays%2520the%2520foundation%2520for%2520integrating%250Alongitudinal%2520EHR%2520and%25203D%2520imaging%2520data%2520to%2520advance%2520clinical%2520risk%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-to-Event%20Pretraining%20for%203D%20Medical%20Imaging&entry.906535625=Zepeng%20Huo%20and%20Jason%20Alan%20Fries%20and%20Alejandro%20Lozano%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Ethan%20Steinberg%20and%20Louis%20Blankemeier%20and%20Akshay%20S.%20Chaudhari%20and%20Curtis%20Langlotz%20and%20Nigam%20H.%20Shah&entry.1292438233=%20%20With%20the%20rise%20of%20medical%20foundation%20models%20and%20the%20growing%20availability%20of%0Aimaging%20data%2C%20scalable%20pretraining%20techniques%20offer%20a%20promising%20way%20to%20identify%0Aimaging%20biomarkers%20predictive%20of%20future%20disease%20risk.%20While%20current%0Aself-supervised%20methods%20for%203D%20medical%20imaging%20models%20capture%20local%20structural%0Afeatures%20like%20organ%20morphology%2C%20they%20fail%20to%20link%20pixel%20biomarkers%20with%0Along-term%20health%20outcomes%20due%20to%20a%20missing%20context%20problem.%20Current%20approaches%0Alack%20the%20temporal%20context%20necessary%20to%20identify%20biomarkers%20correlated%20with%0Adisease%20progression%2C%20as%20they%20rely%20on%20supervision%20derived%20only%20from%20images%20and%0Aconcurrent%20text%20descriptions.%20To%20address%20this%2C%20we%20introduce%20time-to-event%0Apretraining%2C%20a%20pretraining%20framework%20for%203D%20medical%20imaging%20models%20that%0Aleverages%20large-scale%20temporal%20supervision%20from%20paired%2C%20longitudinal%20electronic%0Ahealth%20records%20%28EHRs%29.%20Using%20a%20dataset%20of%2018%2C945%20CT%20scans%20%284.2%20million%202D%0Aimages%29%20and%20time-to-event%20distributions%20across%20thousands%20of%20EHR-derived%20tasks%2C%0Aour%20method%20improves%20outcome%20prediction%2C%20achieving%20an%20average%20AUROC%20increase%20of%0A23.7%25%20and%20a%2029.4%25%20gain%20in%20Harrell%27s%20C-index%20across%208%20benchmark%20tasks.%0AImportantly%2C%20these%20gains%20are%20achieved%20without%20sacrificing%20diagnostic%0Aclassification%20performance.%20This%20study%20lays%20the%20foundation%20for%20integrating%0Alongitudinal%20EHR%20and%203D%20imaging%20data%20to%20advance%20clinical%20risk%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09361v1&entry.124074799=Read"},
{"title": "Instruction-Driven Fusion of Infrared-Visible Images: Tailoring for\n  Diverse Downstream Tasks", "author": "Zengyi Yang and Yafei Zhang and Huafeng Li and Yu Liu", "abstract": "  The primary value of infrared and visible image fusion technology lies in\napplying the fusion results to downstream tasks. However, existing methods face\nchallenges such as increased training complexity and significantly compromised\nperformance of individual tasks when addressing multiple downstream tasks\nsimultaneously. To tackle this, we propose Task-Oriented Adaptive Regulation\n(T-OAR), an adaptive mechanism specifically designed for multi-task\nenvironments. Additionally, we introduce the Task-related Dynamic Prompt\nInjection (T-DPI) module, which generates task-specific dynamic prompts from\nuser-input text instructions and integrates them into target representations.\nThis guides the feature extraction module to produce representations that are\nmore closely aligned with the specific requirements of downstream tasks. By\nincorporating the T-DPI module into the T-OAR framework, our approach generates\nfusion images tailored to task-specific requirements without the need for\nseparate training or task-specific weights. This not only reduces computational\ncosts but also enhances adaptability and performance across multiple tasks.\nExperimental results show that our method excels in object detection, semantic\nsegmentation, and salient object detection, demonstrating its strong\nadaptability, flexibility, and task specificity. This provides an efficient\nsolution for image fusion in multi-task environments, highlighting the\ntechnology's potential across diverse applications.\n", "link": "http://arxiv.org/abs/2411.09387v1", "date": "2024-11-14", "relevancy": 2.2001, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.54}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Driven%20Fusion%20of%20Infrared-Visible%20Images%3A%20Tailoring%20for%0A%20%20Diverse%20Downstream%20Tasks&body=Title%3A%20Instruction-Driven%20Fusion%20of%20Infrared-Visible%20Images%3A%20Tailoring%20for%0A%20%20Diverse%20Downstream%20Tasks%0AAuthor%3A%20Zengyi%20Yang%20and%20Yafei%20Zhang%20and%20Huafeng%20Li%20and%20Yu%20Liu%0AAbstract%3A%20%20%20The%20primary%20value%20of%20infrared%20and%20visible%20image%20fusion%20technology%20lies%20in%0Aapplying%20the%20fusion%20results%20to%20downstream%20tasks.%20However%2C%20existing%20methods%20face%0Achallenges%20such%20as%20increased%20training%20complexity%20and%20significantly%20compromised%0Aperformance%20of%20individual%20tasks%20when%20addressing%20multiple%20downstream%20tasks%0Asimultaneously.%20To%20tackle%20this%2C%20we%20propose%20Task-Oriented%20Adaptive%20Regulation%0A%28T-OAR%29%2C%20an%20adaptive%20mechanism%20specifically%20designed%20for%20multi-task%0Aenvironments.%20Additionally%2C%20we%20introduce%20the%20Task-related%20Dynamic%20Prompt%0AInjection%20%28T-DPI%29%20module%2C%20which%20generates%20task-specific%20dynamic%20prompts%20from%0Auser-input%20text%20instructions%20and%20integrates%20them%20into%20target%20representations.%0AThis%20guides%20the%20feature%20extraction%20module%20to%20produce%20representations%20that%20are%0Amore%20closely%20aligned%20with%20the%20specific%20requirements%20of%20downstream%20tasks.%20By%0Aincorporating%20the%20T-DPI%20module%20into%20the%20T-OAR%20framework%2C%20our%20approach%20generates%0Afusion%20images%20tailored%20to%20task-specific%20requirements%20without%20the%20need%20for%0Aseparate%20training%20or%20task-specific%20weights.%20This%20not%20only%20reduces%20computational%0Acosts%20but%20also%20enhances%20adaptability%20and%20performance%20across%20multiple%20tasks.%0AExperimental%20results%20show%20that%20our%20method%20excels%20in%20object%20detection%2C%20semantic%0Asegmentation%2C%20and%20salient%20object%20detection%2C%20demonstrating%20its%20strong%0Aadaptability%2C%20flexibility%2C%20and%20task%20specificity.%20This%20provides%20an%20efficient%0Asolution%20for%20image%20fusion%20in%20multi-task%20environments%2C%20highlighting%20the%0Atechnology%27s%20potential%20across%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Driven%2520Fusion%2520of%2520Infrared-Visible%2520Images%253A%2520Tailoring%2520for%250A%2520%2520Diverse%2520Downstream%2520Tasks%26entry.906535625%3DZengyi%2520Yang%2520and%2520Yafei%2520Zhang%2520and%2520Huafeng%2520Li%2520and%2520Yu%2520Liu%26entry.1292438233%3D%2520%2520The%2520primary%2520value%2520of%2520infrared%2520and%2520visible%2520image%2520fusion%2520technology%2520lies%2520in%250Aapplying%2520the%2520fusion%2520results%2520to%2520downstream%2520tasks.%2520However%252C%2520existing%2520methods%2520face%250Achallenges%2520such%2520as%2520increased%2520training%2520complexity%2520and%2520significantly%2520compromised%250Aperformance%2520of%2520individual%2520tasks%2520when%2520addressing%2520multiple%2520downstream%2520tasks%250Asimultaneously.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520Task-Oriented%2520Adaptive%2520Regulation%250A%2528T-OAR%2529%252C%2520an%2520adaptive%2520mechanism%2520specifically%2520designed%2520for%2520multi-task%250Aenvironments.%2520Additionally%252C%2520we%2520introduce%2520the%2520Task-related%2520Dynamic%2520Prompt%250AInjection%2520%2528T-DPI%2529%2520module%252C%2520which%2520generates%2520task-specific%2520dynamic%2520prompts%2520from%250Auser-input%2520text%2520instructions%2520and%2520integrates%2520them%2520into%2520target%2520representations.%250AThis%2520guides%2520the%2520feature%2520extraction%2520module%2520to%2520produce%2520representations%2520that%2520are%250Amore%2520closely%2520aligned%2520with%2520the%2520specific%2520requirements%2520of%2520downstream%2520tasks.%2520By%250Aincorporating%2520the%2520T-DPI%2520module%2520into%2520the%2520T-OAR%2520framework%252C%2520our%2520approach%2520generates%250Afusion%2520images%2520tailored%2520to%2520task-specific%2520requirements%2520without%2520the%2520need%2520for%250Aseparate%2520training%2520or%2520task-specific%2520weights.%2520This%2520not%2520only%2520reduces%2520computational%250Acosts%2520but%2520also%2520enhances%2520adaptability%2520and%2520performance%2520across%2520multiple%2520tasks.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520excels%2520in%2520object%2520detection%252C%2520semantic%250Asegmentation%252C%2520and%2520salient%2520object%2520detection%252C%2520demonstrating%2520its%2520strong%250Aadaptability%252C%2520flexibility%252C%2520and%2520task%2520specificity.%2520This%2520provides%2520an%2520efficient%250Asolution%2520for%2520image%2520fusion%2520in%2520multi-task%2520environments%252C%2520highlighting%2520the%250Atechnology%2527s%2520potential%2520across%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Driven%20Fusion%20of%20Infrared-Visible%20Images%3A%20Tailoring%20for%0A%20%20Diverse%20Downstream%20Tasks&entry.906535625=Zengyi%20Yang%20and%20Yafei%20Zhang%20and%20Huafeng%20Li%20and%20Yu%20Liu&entry.1292438233=%20%20The%20primary%20value%20of%20infrared%20and%20visible%20image%20fusion%20technology%20lies%20in%0Aapplying%20the%20fusion%20results%20to%20downstream%20tasks.%20However%2C%20existing%20methods%20face%0Achallenges%20such%20as%20increased%20training%20complexity%20and%20significantly%20compromised%0Aperformance%20of%20individual%20tasks%20when%20addressing%20multiple%20downstream%20tasks%0Asimultaneously.%20To%20tackle%20this%2C%20we%20propose%20Task-Oriented%20Adaptive%20Regulation%0A%28T-OAR%29%2C%20an%20adaptive%20mechanism%20specifically%20designed%20for%20multi-task%0Aenvironments.%20Additionally%2C%20we%20introduce%20the%20Task-related%20Dynamic%20Prompt%0AInjection%20%28T-DPI%29%20module%2C%20which%20generates%20task-specific%20dynamic%20prompts%20from%0Auser-input%20text%20instructions%20and%20integrates%20them%20into%20target%20representations.%0AThis%20guides%20the%20feature%20extraction%20module%20to%20produce%20representations%20that%20are%0Amore%20closely%20aligned%20with%20the%20specific%20requirements%20of%20downstream%20tasks.%20By%0Aincorporating%20the%20T-DPI%20module%20into%20the%20T-OAR%20framework%2C%20our%20approach%20generates%0Afusion%20images%20tailored%20to%20task-specific%20requirements%20without%20the%20need%20for%0Aseparate%20training%20or%20task-specific%20weights.%20This%20not%20only%20reduces%20computational%0Acosts%20but%20also%20enhances%20adaptability%20and%20performance%20across%20multiple%20tasks.%0AExperimental%20results%20show%20that%20our%20method%20excels%20in%20object%20detection%2C%20semantic%0Asegmentation%2C%20and%20salient%20object%20detection%2C%20demonstrating%20its%20strong%0Aadaptability%2C%20flexibility%2C%20and%20task%20specificity.%20This%20provides%20an%20efficient%0Asolution%20for%20image%20fusion%20in%20multi-task%20environments%2C%20highlighting%20the%0Atechnology%27s%20potential%20across%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09387v1&entry.124074799=Read"},
{"title": "One Homography is All You Need: IMM-based Joint Homography and Multiple\n  Object State Estimation", "author": "Paul Johannes Claasen and Johan Pieter de Villiers", "abstract": "  A novel online MOT algorithm, IMM Joint Homography State Estimation\n(IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the\nonly additional 3D information, whereas other 3D MOT methods use regular 3D\nmeasurements. By jointly modelling the homography matrix and its dynamics as\npart of track state vectors, IMM-JHSE removes the explicit influence of camera\nmotion compensation techniques on predicted track position states, which was\nprevalent in previous approaches. Expanding upon this, static and dynamic\ncamera motion models are combined using an IMM filter. A simple bounding box\nmotion model is used to predict bounding box positions to incorporate image\nplane information. In addition to applying an IMM to camera motion, a\nnon-standard IMM approach is applied where bounding-box-based BIoU scores are\nmixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to\nperform association only, making IMM-JHSE robust to motion away from the ground\nplane. Finally, IMM-JHSE makes use of dynamic process and measurement noise\nestimation techniques. IMM-JHSE improves upon related techniques, including\nUCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car\ndatasets, increasing HOTA by 2.64 and 2.11, respectively, while offering\ncompetitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets.\nUsing publicly available detections, IMM-JHSE outperforms almost all other 2D\nMOT methods and is outperformed only by 3D MOT methods -- some of which are\noffline -- on the KITTI-car dataset. Compared to tracking-by-attention methods,\nIMM-JHSE shows remarkably similar performance on the DanceTrack dataset and\noutperforms them on the MOT17 dataset. The code is publicly available:\n\\url{https://github.com/Paulkie99/imm-jhse}.\n", "link": "http://arxiv.org/abs/2409.02562v2", "date": "2024-11-14", "relevancy": 2.1967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation&body=Title%3A%20One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation%0AAuthor%3A%20Paul%20Johannes%20Claasen%20and%20Johan%20Pieter%20de%20Villiers%0AAbstract%3A%20%20%20A%20novel%20online%20MOT%20algorithm%2C%20IMM%20Joint%20Homography%20State%20Estimation%0A%28IMM-JHSE%29%2C%20is%20proposed.%20IMM-JHSE%20uses%20an%20initial%20homography%20estimate%20as%20the%0Aonly%20additional%203D%20information%2C%20whereas%20other%203D%20MOT%20methods%20use%20regular%203D%0Ameasurements.%20By%20jointly%20modelling%20the%20homography%20matrix%20and%20its%20dynamics%20as%0Apart%20of%20track%20state%20vectors%2C%20IMM-JHSE%20removes%20the%20explicit%20influence%20of%20camera%0Amotion%20compensation%20techniques%20on%20predicted%20track%20position%20states%2C%20which%20was%0Aprevalent%20in%20previous%20approaches.%20Expanding%20upon%20this%2C%20static%20and%20dynamic%0Acamera%20motion%20models%20are%20combined%20using%20an%20IMM%20filter.%20A%20simple%20bounding%20box%0Amotion%20model%20is%20used%20to%20predict%20bounding%20box%20positions%20to%20incorporate%20image%0Aplane%20information.%20In%20addition%20to%20applying%20an%20IMM%20to%20camera%20motion%2C%20a%0Anon-standard%20IMM%20approach%20is%20applied%20where%20bounding-box-based%20BIoU%20scores%20are%0Amixed%20with%20ground-plane-based%20Mahalanobis%20distances%20in%20an%20IMM-like%20fashion%20to%0Aperform%20association%20only%2C%20making%20IMM-JHSE%20robust%20to%20motion%20away%20from%20the%20ground%0Aplane.%20Finally%2C%20IMM-JHSE%20makes%20use%20of%20dynamic%20process%20and%20measurement%20noise%0Aestimation%20techniques.%20IMM-JHSE%20improves%20upon%20related%20techniques%2C%20including%0AUCMCTrack%2C%20OC-SORT%2C%20C-BIoU%20and%20ByteTrack%20on%20the%20DanceTrack%20and%20KITTI-car%0Adatasets%2C%20increasing%20HOTA%20by%202.64%20and%202.11%2C%20respectively%2C%20while%20offering%0Acompetitive%20performance%20on%20the%20MOT17%2C%20MOT20%20and%20KITTI-pedestrian%20datasets.%0AUsing%20publicly%20available%20detections%2C%20IMM-JHSE%20outperforms%20almost%20all%20other%202D%0AMOT%20methods%20and%20is%20outperformed%20only%20by%203D%20MOT%20methods%20--%20some%20of%20which%20are%0Aoffline%20--%20on%20the%20KITTI-car%20dataset.%20Compared%20to%20tracking-by-attention%20methods%2C%0AIMM-JHSE%20shows%20remarkably%20similar%20performance%20on%20the%20DanceTrack%20dataset%20and%0Aoutperforms%20them%20on%20the%20MOT17%20dataset.%20The%20code%20is%20publicly%20available%3A%0A%5Curl%7Bhttps%3A//github.com/Paulkie99/imm-jhse%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Homography%2520is%2520All%2520You%2520Need%253A%2520IMM-based%2520Joint%2520Homography%2520and%2520Multiple%250A%2520%2520Object%2520State%2520Estimation%26entry.906535625%3DPaul%2520Johannes%2520Claasen%2520and%2520Johan%2520Pieter%2520de%2520Villiers%26entry.1292438233%3D%2520%2520A%2520novel%2520online%2520MOT%2520algorithm%252C%2520IMM%2520Joint%2520Homography%2520State%2520Estimation%250A%2528IMM-JHSE%2529%252C%2520is%2520proposed.%2520IMM-JHSE%2520uses%2520an%2520initial%2520homography%2520estimate%2520as%2520the%250Aonly%2520additional%25203D%2520information%252C%2520whereas%2520other%25203D%2520MOT%2520methods%2520use%2520regular%25203D%250Ameasurements.%2520By%2520jointly%2520modelling%2520the%2520homography%2520matrix%2520and%2520its%2520dynamics%2520as%250Apart%2520of%2520track%2520state%2520vectors%252C%2520IMM-JHSE%2520removes%2520the%2520explicit%2520influence%2520of%2520camera%250Amotion%2520compensation%2520techniques%2520on%2520predicted%2520track%2520position%2520states%252C%2520which%2520was%250Aprevalent%2520in%2520previous%2520approaches.%2520Expanding%2520upon%2520this%252C%2520static%2520and%2520dynamic%250Acamera%2520motion%2520models%2520are%2520combined%2520using%2520an%2520IMM%2520filter.%2520A%2520simple%2520bounding%2520box%250Amotion%2520model%2520is%2520used%2520to%2520predict%2520bounding%2520box%2520positions%2520to%2520incorporate%2520image%250Aplane%2520information.%2520In%2520addition%2520to%2520applying%2520an%2520IMM%2520to%2520camera%2520motion%252C%2520a%250Anon-standard%2520IMM%2520approach%2520is%2520applied%2520where%2520bounding-box-based%2520BIoU%2520scores%2520are%250Amixed%2520with%2520ground-plane-based%2520Mahalanobis%2520distances%2520in%2520an%2520IMM-like%2520fashion%2520to%250Aperform%2520association%2520only%252C%2520making%2520IMM-JHSE%2520robust%2520to%2520motion%2520away%2520from%2520the%2520ground%250Aplane.%2520Finally%252C%2520IMM-JHSE%2520makes%2520use%2520of%2520dynamic%2520process%2520and%2520measurement%2520noise%250Aestimation%2520techniques.%2520IMM-JHSE%2520improves%2520upon%2520related%2520techniques%252C%2520including%250AUCMCTrack%252C%2520OC-SORT%252C%2520C-BIoU%2520and%2520ByteTrack%2520on%2520the%2520DanceTrack%2520and%2520KITTI-car%250Adatasets%252C%2520increasing%2520HOTA%2520by%25202.64%2520and%25202.11%252C%2520respectively%252C%2520while%2520offering%250Acompetitive%2520performance%2520on%2520the%2520MOT17%252C%2520MOT20%2520and%2520KITTI-pedestrian%2520datasets.%250AUsing%2520publicly%2520available%2520detections%252C%2520IMM-JHSE%2520outperforms%2520almost%2520all%2520other%25202D%250AMOT%2520methods%2520and%2520is%2520outperformed%2520only%2520by%25203D%2520MOT%2520methods%2520--%2520some%2520of%2520which%2520are%250Aoffline%2520--%2520on%2520the%2520KITTI-car%2520dataset.%2520Compared%2520to%2520tracking-by-attention%2520methods%252C%250AIMM-JHSE%2520shows%2520remarkably%2520similar%2520performance%2520on%2520the%2520DanceTrack%2520dataset%2520and%250Aoutperforms%2520them%2520on%2520the%2520MOT17%2520dataset.%2520The%2520code%2520is%2520publicly%2520available%253A%250A%255Curl%257Bhttps%253A//github.com/Paulkie99/imm-jhse%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation&entry.906535625=Paul%20Johannes%20Claasen%20and%20Johan%20Pieter%20de%20Villiers&entry.1292438233=%20%20A%20novel%20online%20MOT%20algorithm%2C%20IMM%20Joint%20Homography%20State%20Estimation%0A%28IMM-JHSE%29%2C%20is%20proposed.%20IMM-JHSE%20uses%20an%20initial%20homography%20estimate%20as%20the%0Aonly%20additional%203D%20information%2C%20whereas%20other%203D%20MOT%20methods%20use%20regular%203D%0Ameasurements.%20By%20jointly%20modelling%20the%20homography%20matrix%20and%20its%20dynamics%20as%0Apart%20of%20track%20state%20vectors%2C%20IMM-JHSE%20removes%20the%20explicit%20influence%20of%20camera%0Amotion%20compensation%20techniques%20on%20predicted%20track%20position%20states%2C%20which%20was%0Aprevalent%20in%20previous%20approaches.%20Expanding%20upon%20this%2C%20static%20and%20dynamic%0Acamera%20motion%20models%20are%20combined%20using%20an%20IMM%20filter.%20A%20simple%20bounding%20box%0Amotion%20model%20is%20used%20to%20predict%20bounding%20box%20positions%20to%20incorporate%20image%0Aplane%20information.%20In%20addition%20to%20applying%20an%20IMM%20to%20camera%20motion%2C%20a%0Anon-standard%20IMM%20approach%20is%20applied%20where%20bounding-box-based%20BIoU%20scores%20are%0Amixed%20with%20ground-plane-based%20Mahalanobis%20distances%20in%20an%20IMM-like%20fashion%20to%0Aperform%20association%20only%2C%20making%20IMM-JHSE%20robust%20to%20motion%20away%20from%20the%20ground%0Aplane.%20Finally%2C%20IMM-JHSE%20makes%20use%20of%20dynamic%20process%20and%20measurement%20noise%0Aestimation%20techniques.%20IMM-JHSE%20improves%20upon%20related%20techniques%2C%20including%0AUCMCTrack%2C%20OC-SORT%2C%20C-BIoU%20and%20ByteTrack%20on%20the%20DanceTrack%20and%20KITTI-car%0Adatasets%2C%20increasing%20HOTA%20by%202.64%20and%202.11%2C%20respectively%2C%20while%20offering%0Acompetitive%20performance%20on%20the%20MOT17%2C%20MOT20%20and%20KITTI-pedestrian%20datasets.%0AUsing%20publicly%20available%20detections%2C%20IMM-JHSE%20outperforms%20almost%20all%20other%202D%0AMOT%20methods%20and%20is%20outperformed%20only%20by%203D%20MOT%20methods%20--%20some%20of%20which%20are%0Aoffline%20--%20on%20the%20KITTI-car%20dataset.%20Compared%20to%20tracking-by-attention%20methods%2C%0AIMM-JHSE%20shows%20remarkably%20similar%20performance%20on%20the%20DanceTrack%20dataset%20and%0Aoutperforms%20them%20on%20the%20MOT17%20dataset.%20The%20code%20is%20publicly%20available%3A%0A%5Curl%7Bhttps%3A//github.com/Paulkie99/imm-jhse%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02562v2&entry.124074799=Read"},
{"title": "Assessing the Performance of the DINOv2 Self-supervised Learning Vision\n  Transformer Model for the Segmentation of the Left Atrium from MRI Images", "author": "Bipasha Kundu and Bidur Khanal and Richard Simon and Cristian A. Linte", "abstract": "  Accurate left atrium (LA) segmentation from pre-operative scans is crucial\nfor diagnosing atrial fibrillation, treatment planning, and supporting surgical\ninterventions. While deep learning models are key in medical image\nsegmentation, they often require extensive manually annotated data. Foundation\nmodels trained on larger datasets have reduced this dependency, enhancing\ngeneralizability and robustness through transfer learning. We explore DINOv2, a\nself-supervised learning vision transformer trained on natural images, for LA\nsegmentation using MRI. The challenges for LA's complex anatomy, thin\nboundaries, and limited annotated data make accurate segmentation difficult\nbefore & during the image-guided intervention. We demonstrate DINOv2's ability\nto provide accurate & consistent segmentation, achieving a mean Dice score of\n.871 & a Jaccard Index of .792 for end-to-end fine-tuning. Through few-shot\nlearning across various data sizes & patient counts, DINOv2 consistently\noutperforms baseline models. These results suggest that DINOv2 effectively\nadapts to MRI with limited data, highlighting its potential as a competitive\ntool for segmentation & encouraging broader use in medical imaging.\n", "link": "http://arxiv.org/abs/2411.09598v1", "date": "2024-11-14", "relevancy": 2.1952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Performance%20of%20the%20DINOv2%20Self-supervised%20Learning%20Vision%0A%20%20Transformer%20Model%20for%20the%20Segmentation%20of%20the%20Left%20Atrium%20from%20MRI%20Images&body=Title%3A%20Assessing%20the%20Performance%20of%20the%20DINOv2%20Self-supervised%20Learning%20Vision%0A%20%20Transformer%20Model%20for%20the%20Segmentation%20of%20the%20Left%20Atrium%20from%20MRI%20Images%0AAuthor%3A%20Bipasha%20Kundu%20and%20Bidur%20Khanal%20and%20Richard%20Simon%20and%20Cristian%20A.%20Linte%0AAbstract%3A%20%20%20Accurate%20left%20atrium%20%28LA%29%20segmentation%20from%20pre-operative%20scans%20is%20crucial%0Afor%20diagnosing%20atrial%20fibrillation%2C%20treatment%20planning%2C%20and%20supporting%20surgical%0Ainterventions.%20While%20deep%20learning%20models%20are%20key%20in%20medical%20image%0Asegmentation%2C%20they%20often%20require%20extensive%20manually%20annotated%20data.%20Foundation%0Amodels%20trained%20on%20larger%20datasets%20have%20reduced%20this%20dependency%2C%20enhancing%0Ageneralizability%20and%20robustness%20through%20transfer%20learning.%20We%20explore%20DINOv2%2C%20a%0Aself-supervised%20learning%20vision%20transformer%20trained%20on%20natural%20images%2C%20for%20LA%0Asegmentation%20using%20MRI.%20The%20challenges%20for%20LA%27s%20complex%20anatomy%2C%20thin%0Aboundaries%2C%20and%20limited%20annotated%20data%20make%20accurate%20segmentation%20difficult%0Abefore%20%26%20during%20the%20image-guided%20intervention.%20We%20demonstrate%20DINOv2%27s%20ability%0Ato%20provide%20accurate%20%26%20consistent%20segmentation%2C%20achieving%20a%20mean%20Dice%20score%20of%0A.871%20%26%20a%20Jaccard%20Index%20of%20.792%20for%20end-to-end%20fine-tuning.%20Through%20few-shot%0Alearning%20across%20various%20data%20sizes%20%26%20patient%20counts%2C%20DINOv2%20consistently%0Aoutperforms%20baseline%20models.%20These%20results%20suggest%20that%20DINOv2%20effectively%0Aadapts%20to%20MRI%20with%20limited%20data%2C%20highlighting%20its%20potential%20as%20a%20competitive%0Atool%20for%20segmentation%20%26%20encouraging%20broader%20use%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Performance%2520of%2520the%2520DINOv2%2520Self-supervised%2520Learning%2520Vision%250A%2520%2520Transformer%2520Model%2520for%2520the%2520Segmentation%2520of%2520the%2520Left%2520Atrium%2520from%2520MRI%2520Images%26entry.906535625%3DBipasha%2520Kundu%2520and%2520Bidur%2520Khanal%2520and%2520Richard%2520Simon%2520and%2520Cristian%2520A.%2520Linte%26entry.1292438233%3D%2520%2520Accurate%2520left%2520atrium%2520%2528LA%2529%2520segmentation%2520from%2520pre-operative%2520scans%2520is%2520crucial%250Afor%2520diagnosing%2520atrial%2520fibrillation%252C%2520treatment%2520planning%252C%2520and%2520supporting%2520surgical%250Ainterventions.%2520While%2520deep%2520learning%2520models%2520are%2520key%2520in%2520medical%2520image%250Asegmentation%252C%2520they%2520often%2520require%2520extensive%2520manually%2520annotated%2520data.%2520Foundation%250Amodels%2520trained%2520on%2520larger%2520datasets%2520have%2520reduced%2520this%2520dependency%252C%2520enhancing%250Ageneralizability%2520and%2520robustness%2520through%2520transfer%2520learning.%2520We%2520explore%2520DINOv2%252C%2520a%250Aself-supervised%2520learning%2520vision%2520transformer%2520trained%2520on%2520natural%2520images%252C%2520for%2520LA%250Asegmentation%2520using%2520MRI.%2520The%2520challenges%2520for%2520LA%2527s%2520complex%2520anatomy%252C%2520thin%250Aboundaries%252C%2520and%2520limited%2520annotated%2520data%2520make%2520accurate%2520segmentation%2520difficult%250Abefore%2520%2526%2520during%2520the%2520image-guided%2520intervention.%2520We%2520demonstrate%2520DINOv2%2527s%2520ability%250Ato%2520provide%2520accurate%2520%2526%2520consistent%2520segmentation%252C%2520achieving%2520a%2520mean%2520Dice%2520score%2520of%250A.871%2520%2526%2520a%2520Jaccard%2520Index%2520of%2520.792%2520for%2520end-to-end%2520fine-tuning.%2520Through%2520few-shot%250Alearning%2520across%2520various%2520data%2520sizes%2520%2526%2520patient%2520counts%252C%2520DINOv2%2520consistently%250Aoutperforms%2520baseline%2520models.%2520These%2520results%2520suggest%2520that%2520DINOv2%2520effectively%250Aadapts%2520to%2520MRI%2520with%2520limited%2520data%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520competitive%250Atool%2520for%2520segmentation%2520%2526%2520encouraging%2520broader%2520use%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Performance%20of%20the%20DINOv2%20Self-supervised%20Learning%20Vision%0A%20%20Transformer%20Model%20for%20the%20Segmentation%20of%20the%20Left%20Atrium%20from%20MRI%20Images&entry.906535625=Bipasha%20Kundu%20and%20Bidur%20Khanal%20and%20Richard%20Simon%20and%20Cristian%20A.%20Linte&entry.1292438233=%20%20Accurate%20left%20atrium%20%28LA%29%20segmentation%20from%20pre-operative%20scans%20is%20crucial%0Afor%20diagnosing%20atrial%20fibrillation%2C%20treatment%20planning%2C%20and%20supporting%20surgical%0Ainterventions.%20While%20deep%20learning%20models%20are%20key%20in%20medical%20image%0Asegmentation%2C%20they%20often%20require%20extensive%20manually%20annotated%20data.%20Foundation%0Amodels%20trained%20on%20larger%20datasets%20have%20reduced%20this%20dependency%2C%20enhancing%0Ageneralizability%20and%20robustness%20through%20transfer%20learning.%20We%20explore%20DINOv2%2C%20a%0Aself-supervised%20learning%20vision%20transformer%20trained%20on%20natural%20images%2C%20for%20LA%0Asegmentation%20using%20MRI.%20The%20challenges%20for%20LA%27s%20complex%20anatomy%2C%20thin%0Aboundaries%2C%20and%20limited%20annotated%20data%20make%20accurate%20segmentation%20difficult%0Abefore%20%26%20during%20the%20image-guided%20intervention.%20We%20demonstrate%20DINOv2%27s%20ability%0Ato%20provide%20accurate%20%26%20consistent%20segmentation%2C%20achieving%20a%20mean%20Dice%20score%20of%0A.871%20%26%20a%20Jaccard%20Index%20of%20.792%20for%20end-to-end%20fine-tuning.%20Through%20few-shot%0Alearning%20across%20various%20data%20sizes%20%26%20patient%20counts%2C%20DINOv2%20consistently%0Aoutperforms%20baseline%20models.%20These%20results%20suggest%20that%20DINOv2%20effectively%0Aadapts%20to%20MRI%20with%20limited%20data%2C%20highlighting%20its%20potential%20as%20a%20competitive%0Atool%20for%20segmentation%20%26%20encouraging%20broader%20use%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09598v1&entry.124074799=Read"},
{"title": "SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph\n  Attention for Vision Transformers", "author": "Shravan Venkatraman and Jaskaran Singh Walia and Joe Dhanith P R", "abstract": "  Image classification is a computer vision task where a model analyzes an\nimage to categorize it into a specific label. Vision Transformers (ViT) improve\nthis task by leveraging self-attention to capture complex patterns and long\nrange relationships between image patches. However, a key challenge for ViTs is\nefficiently incorporating multiscale feature representations, which is inherent\nin CNNs through their hierarchical structure. In this paper, we introduce the\nScale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework\nthat addresses this challenge by integrating multi-scale features. Using\nEfficientNet as a backbone, the model extracts multi-scale feature maps, which\nare divided into patches to preserve semantic information. These patches are\norganized into a graph based on spatial and feature similarities, with a Graph\nAttention Network (GAT) refining the node embeddings. Finally, a Transformer\nencoder captures long-range dependencies and complex interactions. The SAG-ViT\nis evaluated on benchmark datasets, demonstrating its effectiveness in\nenhancing image classification performance.\n", "link": "http://arxiv.org/abs/2411.09420v1", "date": "2024-11-14", "relevancy": 2.1879, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5738}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5445}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAG-ViT%3A%20A%20Scale-Aware%2C%20High-Fidelity%20Patching%20Approach%20with%20Graph%0A%20%20Attention%20for%20Vision%20Transformers&body=Title%3A%20SAG-ViT%3A%20A%20Scale-Aware%2C%20High-Fidelity%20Patching%20Approach%20with%20Graph%0A%20%20Attention%20for%20Vision%20Transformers%0AAuthor%3A%20Shravan%20Venkatraman%20and%20Jaskaran%20Singh%20Walia%20and%20Joe%20Dhanith%20P%20R%0AAbstract%3A%20%20%20Image%20classification%20is%20a%20computer%20vision%20task%20where%20a%20model%20analyzes%20an%0Aimage%20to%20categorize%20it%20into%20a%20specific%20label.%20Vision%20Transformers%20%28ViT%29%20improve%0Athis%20task%20by%20leveraging%20self-attention%20to%20capture%20complex%20patterns%20and%20long%0Arange%20relationships%20between%20image%20patches.%20However%2C%20a%20key%20challenge%20for%20ViTs%20is%0Aefficiently%20incorporating%20multiscale%20feature%20representations%2C%20which%20is%20inherent%0Ain%20CNNs%20through%20their%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20introduce%20the%0AScale-Aware%20Graph%20Attention%20Vision%20Transformer%20%28SAG-ViT%29%2C%20a%20novel%20framework%0Athat%20addresses%20this%20challenge%20by%20integrating%20multi-scale%20features.%20Using%0AEfficientNet%20as%20a%20backbone%2C%20the%20model%20extracts%20multi-scale%20feature%20maps%2C%20which%0Aare%20divided%20into%20patches%20to%20preserve%20semantic%20information.%20These%20patches%20are%0Aorganized%20into%20a%20graph%20based%20on%20spatial%20and%20feature%20similarities%2C%20with%20a%20Graph%0AAttention%20Network%20%28GAT%29%20refining%20the%20node%20embeddings.%20Finally%2C%20a%20Transformer%0Aencoder%20captures%20long-range%20dependencies%20and%20complex%20interactions.%20The%20SAG-ViT%0Ais%20evaluated%20on%20benchmark%20datasets%2C%20demonstrating%20its%20effectiveness%20in%0Aenhancing%20image%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAG-ViT%253A%2520A%2520Scale-Aware%252C%2520High-Fidelity%2520Patching%2520Approach%2520with%2520Graph%250A%2520%2520Attention%2520for%2520Vision%2520Transformers%26entry.906535625%3DShravan%2520Venkatraman%2520and%2520Jaskaran%2520Singh%2520Walia%2520and%2520Joe%2520Dhanith%2520P%2520R%26entry.1292438233%3D%2520%2520Image%2520classification%2520is%2520a%2520computer%2520vision%2520task%2520where%2520a%2520model%2520analyzes%2520an%250Aimage%2520to%2520categorize%2520it%2520into%2520a%2520specific%2520label.%2520Vision%2520Transformers%2520%2528ViT%2529%2520improve%250Athis%2520task%2520by%2520leveraging%2520self-attention%2520to%2520capture%2520complex%2520patterns%2520and%2520long%250Arange%2520relationships%2520between%2520image%2520patches.%2520However%252C%2520a%2520key%2520challenge%2520for%2520ViTs%2520is%250Aefficiently%2520incorporating%2520multiscale%2520feature%2520representations%252C%2520which%2520is%2520inherent%250Ain%2520CNNs%2520through%2520their%2520hierarchical%2520structure.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250AScale-Aware%2520Graph%2520Attention%2520Vision%2520Transformer%2520%2528SAG-ViT%2529%252C%2520a%2520novel%2520framework%250Athat%2520addresses%2520this%2520challenge%2520by%2520integrating%2520multi-scale%2520features.%2520Using%250AEfficientNet%2520as%2520a%2520backbone%252C%2520the%2520model%2520extracts%2520multi-scale%2520feature%2520maps%252C%2520which%250Aare%2520divided%2520into%2520patches%2520to%2520preserve%2520semantic%2520information.%2520These%2520patches%2520are%250Aorganized%2520into%2520a%2520graph%2520based%2520on%2520spatial%2520and%2520feature%2520similarities%252C%2520with%2520a%2520Graph%250AAttention%2520Network%2520%2528GAT%2529%2520refining%2520the%2520node%2520embeddings.%2520Finally%252C%2520a%2520Transformer%250Aencoder%2520captures%2520long-range%2520dependencies%2520and%2520complex%2520interactions.%2520The%2520SAG-ViT%250Ais%2520evaluated%2520on%2520benchmark%2520datasets%252C%2520demonstrating%2520its%2520effectiveness%2520in%250Aenhancing%2520image%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAG-ViT%3A%20A%20Scale-Aware%2C%20High-Fidelity%20Patching%20Approach%20with%20Graph%0A%20%20Attention%20for%20Vision%20Transformers&entry.906535625=Shravan%20Venkatraman%20and%20Jaskaran%20Singh%20Walia%20and%20Joe%20Dhanith%20P%20R&entry.1292438233=%20%20Image%20classification%20is%20a%20computer%20vision%20task%20where%20a%20model%20analyzes%20an%0Aimage%20to%20categorize%20it%20into%20a%20specific%20label.%20Vision%20Transformers%20%28ViT%29%20improve%0Athis%20task%20by%20leveraging%20self-attention%20to%20capture%20complex%20patterns%20and%20long%0Arange%20relationships%20between%20image%20patches.%20However%2C%20a%20key%20challenge%20for%20ViTs%20is%0Aefficiently%20incorporating%20multiscale%20feature%20representations%2C%20which%20is%20inherent%0Ain%20CNNs%20through%20their%20hierarchical%20structure.%20In%20this%20paper%2C%20we%20introduce%20the%0AScale-Aware%20Graph%20Attention%20Vision%20Transformer%20%28SAG-ViT%29%2C%20a%20novel%20framework%0Athat%20addresses%20this%20challenge%20by%20integrating%20multi-scale%20features.%20Using%0AEfficientNet%20as%20a%20backbone%2C%20the%20model%20extracts%20multi-scale%20feature%20maps%2C%20which%0Aare%20divided%20into%20patches%20to%20preserve%20semantic%20information.%20These%20patches%20are%0Aorganized%20into%20a%20graph%20based%20on%20spatial%20and%20feature%20similarities%2C%20with%20a%20Graph%0AAttention%20Network%20%28GAT%29%20refining%20the%20node%20embeddings.%20Finally%2C%20a%20Transformer%0Aencoder%20captures%20long-range%20dependencies%20and%20complex%20interactions.%20The%20SAG-ViT%0Ais%20evaluated%20on%20benchmark%20datasets%2C%20demonstrating%20its%20effectiveness%20in%0Aenhancing%20image%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09420v1&entry.124074799=Read"},
{"title": "I2I-Mamba: Multi-modal medical image synthesis via selective state space\n  modeling", "author": "Omer F. Atli and Bilal Kabas and Fuat Arslan and Mahmut Yurt and Onat Dalmaz and Tolga \u00c7ukur", "abstract": "  In recent years, deep learning models comprising transformer components have\npushed the performance envelope in medical image synthesis tasks. Contrary to\nconvolutional neural networks (CNNs) that use static, local filters,\ntransformers use self-attention mechanisms to permit adaptive, non-local\nfiltering to sensitively capture long-range context. However, this sensitivity\ncomes at the expense of substantial model complexity, which can compromise\nlearning efficacy particularly on relatively modest-sized imaging datasets.\nHere, we propose a novel adversarial model for multi-modal medical image\nsynthesis, I2I-Mamba, that leverages selective state space modeling (SSM) to\nefficiently capture long-range context while maintaining local precision. To do\nthis, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneck\nof a convolutional backbone. In cmMamba blocks, SSM layers are used to learn\ncontext across the spatial dimension and channel-mixing layers are used to\nlearn context across the channel dimension of feature maps. Comprehensive\ndemonstrations are reported for imputing missing images in multi-contrast MRI\nand MRI-CT protocols. Our results indicate that I2I-Mamba offers superior\nperformance against state-of-the-art CNN- and transformer-based methods in\nsynthesizing target-modality images.\n", "link": "http://arxiv.org/abs/2405.14022v3", "date": "2024-11-14", "relevancy": 2.176, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5519}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2I-Mamba%3A%20Multi-modal%20medical%20image%20synthesis%20via%20selective%20state%20space%0A%20%20modeling&body=Title%3A%20I2I-Mamba%3A%20Multi-modal%20medical%20image%20synthesis%20via%20selective%20state%20space%0A%20%20modeling%0AAuthor%3A%20Omer%20F.%20Atli%20and%20Bilal%20Kabas%20and%20Fuat%20Arslan%20and%20Mahmut%20Yurt%20and%20Onat%20Dalmaz%20and%20Tolga%20%C3%87ukur%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20models%20comprising%20transformer%20components%20have%0Apushed%20the%20performance%20envelope%20in%20medical%20image%20synthesis%20tasks.%20Contrary%20to%0Aconvolutional%20neural%20networks%20%28CNNs%29%20that%20use%20static%2C%20local%20filters%2C%0Atransformers%20use%20self-attention%20mechanisms%20to%20permit%20adaptive%2C%20non-local%0Afiltering%20to%20sensitively%20capture%20long-range%20context.%20However%2C%20this%20sensitivity%0Acomes%20at%20the%20expense%20of%20substantial%20model%20complexity%2C%20which%20can%20compromise%0Alearning%20efficacy%20particularly%20on%20relatively%20modest-sized%20imaging%20datasets.%0AHere%2C%20we%20propose%20a%20novel%20adversarial%20model%20for%20multi-modal%20medical%20image%0Asynthesis%2C%20I2I-Mamba%2C%20that%20leverages%20selective%20state%20space%20modeling%20%28SSM%29%20to%0Aefficiently%20capture%20long-range%20context%20while%20maintaining%20local%20precision.%20To%20do%0Athis%2C%20I2I-Mamba%20injects%20channel-mixed%20Mamba%20%28cmMamba%29%20blocks%20in%20the%20bottleneck%0Aof%20a%20convolutional%20backbone.%20In%20cmMamba%20blocks%2C%20SSM%20layers%20are%20used%20to%20learn%0Acontext%20across%20the%20spatial%20dimension%20and%20channel-mixing%20layers%20are%20used%20to%0Alearn%20context%20across%20the%20channel%20dimension%20of%20feature%20maps.%20Comprehensive%0Ademonstrations%20are%20reported%20for%20imputing%20missing%20images%20in%20multi-contrast%20MRI%0Aand%20MRI-CT%20protocols.%20Our%20results%20indicate%20that%20I2I-Mamba%20offers%20superior%0Aperformance%20against%20state-of-the-art%20CNN-%20and%20transformer-based%20methods%20in%0Asynthesizing%20target-modality%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14022v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2I-Mamba%253A%2520Multi-modal%2520medical%2520image%2520synthesis%2520via%2520selective%2520state%2520space%250A%2520%2520modeling%26entry.906535625%3DOmer%2520F.%2520Atli%2520and%2520Bilal%2520Kabas%2520and%2520Fuat%2520Arslan%2520and%2520Mahmut%2520Yurt%2520and%2520Onat%2520Dalmaz%2520and%2520Tolga%2520%25C3%2587ukur%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning%2520models%2520comprising%2520transformer%2520components%2520have%250Apushed%2520the%2520performance%2520envelope%2520in%2520medical%2520image%2520synthesis%2520tasks.%2520Contrary%2520to%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520that%2520use%2520static%252C%2520local%2520filters%252C%250Atransformers%2520use%2520self-attention%2520mechanisms%2520to%2520permit%2520adaptive%252C%2520non-local%250Afiltering%2520to%2520sensitively%2520capture%2520long-range%2520context.%2520However%252C%2520this%2520sensitivity%250Acomes%2520at%2520the%2520expense%2520of%2520substantial%2520model%2520complexity%252C%2520which%2520can%2520compromise%250Alearning%2520efficacy%2520particularly%2520on%2520relatively%2520modest-sized%2520imaging%2520datasets.%250AHere%252C%2520we%2520propose%2520a%2520novel%2520adversarial%2520model%2520for%2520multi-modal%2520medical%2520image%250Asynthesis%252C%2520I2I-Mamba%252C%2520that%2520leverages%2520selective%2520state%2520space%2520modeling%2520%2528SSM%2529%2520to%250Aefficiently%2520capture%2520long-range%2520context%2520while%2520maintaining%2520local%2520precision.%2520To%2520do%250Athis%252C%2520I2I-Mamba%2520injects%2520channel-mixed%2520Mamba%2520%2528cmMamba%2529%2520blocks%2520in%2520the%2520bottleneck%250Aof%2520a%2520convolutional%2520backbone.%2520In%2520cmMamba%2520blocks%252C%2520SSM%2520layers%2520are%2520used%2520to%2520learn%250Acontext%2520across%2520the%2520spatial%2520dimension%2520and%2520channel-mixing%2520layers%2520are%2520used%2520to%250Alearn%2520context%2520across%2520the%2520channel%2520dimension%2520of%2520feature%2520maps.%2520Comprehensive%250Ademonstrations%2520are%2520reported%2520for%2520imputing%2520missing%2520images%2520in%2520multi-contrast%2520MRI%250Aand%2520MRI-CT%2520protocols.%2520Our%2520results%2520indicate%2520that%2520I2I-Mamba%2520offers%2520superior%250Aperformance%2520against%2520state-of-the-art%2520CNN-%2520and%2520transformer-based%2520methods%2520in%250Asynthesizing%2520target-modality%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14022v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2I-Mamba%3A%20Multi-modal%20medical%20image%20synthesis%20via%20selective%20state%20space%0A%20%20modeling&entry.906535625=Omer%20F.%20Atli%20and%20Bilal%20Kabas%20and%20Fuat%20Arslan%20and%20Mahmut%20Yurt%20and%20Onat%20Dalmaz%20and%20Tolga%20%C3%87ukur&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20models%20comprising%20transformer%20components%20have%0Apushed%20the%20performance%20envelope%20in%20medical%20image%20synthesis%20tasks.%20Contrary%20to%0Aconvolutional%20neural%20networks%20%28CNNs%29%20that%20use%20static%2C%20local%20filters%2C%0Atransformers%20use%20self-attention%20mechanisms%20to%20permit%20adaptive%2C%20non-local%0Afiltering%20to%20sensitively%20capture%20long-range%20context.%20However%2C%20this%20sensitivity%0Acomes%20at%20the%20expense%20of%20substantial%20model%20complexity%2C%20which%20can%20compromise%0Alearning%20efficacy%20particularly%20on%20relatively%20modest-sized%20imaging%20datasets.%0AHere%2C%20we%20propose%20a%20novel%20adversarial%20model%20for%20multi-modal%20medical%20image%0Asynthesis%2C%20I2I-Mamba%2C%20that%20leverages%20selective%20state%20space%20modeling%20%28SSM%29%20to%0Aefficiently%20capture%20long-range%20context%20while%20maintaining%20local%20precision.%20To%20do%0Athis%2C%20I2I-Mamba%20injects%20channel-mixed%20Mamba%20%28cmMamba%29%20blocks%20in%20the%20bottleneck%0Aof%20a%20convolutional%20backbone.%20In%20cmMamba%20blocks%2C%20SSM%20layers%20are%20used%20to%20learn%0Acontext%20across%20the%20spatial%20dimension%20and%20channel-mixing%20layers%20are%20used%20to%0Alearn%20context%20across%20the%20channel%20dimension%20of%20feature%20maps.%20Comprehensive%0Ademonstrations%20are%20reported%20for%20imputing%20missing%20images%20in%20multi-contrast%20MRI%0Aand%20MRI-CT%20protocols.%20Our%20results%20indicate%20that%20I2I-Mamba%20offers%20superior%0Aperformance%20against%20state-of-the-art%20CNN-%20and%20transformer-based%20methods%20in%0Asynthesizing%20target-modality%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14022v3&entry.124074799=Read"},
{"title": "GAN-Based Architecture for Low-dose Computed Tomography Imaging\n  Denoising", "author": "Yunuo Wang and Ningning Yang and Jialin Li", "abstract": "  Generative Adversarial Networks (GANs) have surfaced as a revolutionary\nelement within the domain of low-dose computed tomography (LDCT) imaging,\nproviding an advanced resolution to the enduring issue of reconciling radiation\nexposure with image quality. This comprehensive review synthesizes the rapid\nadvancements in GAN-based LDCT denoising techniques, examining the evolution\nfrom foundational architectures to state-of-the-art models incorporating\nadvanced features such as anatomical priors, perceptual loss functions, and\ninnovative regularization strategies. We critically analyze various GAN\narchitectures, including conditional GANs (cGANs), CycleGANs, and\nSuper-Resolution GANs (SRGANs), elucidating their unique strengths and\nlimitations in the context of LDCT denoising. The evaluation provides both\nqualitative and quantitative results related to the improvements in performance\nin benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.\nAfter highlighting the positive results, we discuss some of the challenges\npreventing a wider clinical use, including the interpretability of the images\ngenerated by GANs, synthetic artifacts, and the need for clinically relevant\nmetrics. The review concludes by highlighting the essential significance of\nGAN-based methodologies in the progression of precision medicine via tailored\nLDCT denoising models, underlining the transformative possibilities presented\nby artificial intelligence within contemporary radiological practice.\n", "link": "http://arxiv.org/abs/2411.09512v1", "date": "2024-11-14", "relevancy": 2.169, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5381}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising&body=Title%3A%20GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising%0AAuthor%3A%20Yunuo%20Wang%20and%20Ningning%20Yang%20and%20Jialin%20Li%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20surfaced%20as%20a%20revolutionary%0Aelement%20within%20the%20domain%20of%20low-dose%20computed%20tomography%20%28LDCT%29%20imaging%2C%0Aproviding%20an%20advanced%20resolution%20to%20the%20enduring%20issue%20of%20reconciling%20radiation%0Aexposure%20with%20image%20quality.%20This%20comprehensive%20review%20synthesizes%20the%20rapid%0Aadvancements%20in%20GAN-based%20LDCT%20denoising%20techniques%2C%20examining%20the%20evolution%0Afrom%20foundational%20architectures%20to%20state-of-the-art%20models%20incorporating%0Aadvanced%20features%20such%20as%20anatomical%20priors%2C%20perceptual%20loss%20functions%2C%20and%0Ainnovative%20regularization%20strategies.%20We%20critically%20analyze%20various%20GAN%0Aarchitectures%2C%20including%20conditional%20GANs%20%28cGANs%29%2C%20CycleGANs%2C%20and%0ASuper-Resolution%20GANs%20%28SRGANs%29%2C%20elucidating%20their%20unique%20strengths%20and%0Alimitations%20in%20the%20context%20of%20LDCT%20denoising.%20The%20evaluation%20provides%20both%0Aqualitative%20and%20quantitative%20results%20related%20to%20the%20improvements%20in%20performance%0Ain%20benchmark%20and%20clinical%20datasets%20with%20metrics%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0AAfter%20highlighting%20the%20positive%20results%2C%20we%20discuss%20some%20of%20the%20challenges%0Apreventing%20a%20wider%20clinical%20use%2C%20including%20the%20interpretability%20of%20the%20images%0Agenerated%20by%20GANs%2C%20synthetic%20artifacts%2C%20and%20the%20need%20for%20clinically%20relevant%0Ametrics.%20The%20review%20concludes%20by%20highlighting%20the%20essential%20significance%20of%0AGAN-based%20methodologies%20in%20the%20progression%20of%20precision%20medicine%20via%20tailored%0ALDCT%20denoising%20models%2C%20underlining%20the%20transformative%20possibilities%20presented%0Aby%20artificial%20intelligence%20within%20contemporary%20radiological%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAN-Based%2520Architecture%2520for%2520Low-dose%2520Computed%2520Tomography%2520Imaging%250A%2520%2520Denoising%26entry.906535625%3DYunuo%2520Wang%2520and%2520Ningning%2520Yang%2520and%2520Jialin%2520Li%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520surfaced%2520as%2520a%2520revolutionary%250Aelement%2520within%2520the%2520domain%2520of%2520low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520imaging%252C%250Aproviding%2520an%2520advanced%2520resolution%2520to%2520the%2520enduring%2520issue%2520of%2520reconciling%2520radiation%250Aexposure%2520with%2520image%2520quality.%2520This%2520comprehensive%2520review%2520synthesizes%2520the%2520rapid%250Aadvancements%2520in%2520GAN-based%2520LDCT%2520denoising%2520techniques%252C%2520examining%2520the%2520evolution%250Afrom%2520foundational%2520architectures%2520to%2520state-of-the-art%2520models%2520incorporating%250Aadvanced%2520features%2520such%2520as%2520anatomical%2520priors%252C%2520perceptual%2520loss%2520functions%252C%2520and%250Ainnovative%2520regularization%2520strategies.%2520We%2520critically%2520analyze%2520various%2520GAN%250Aarchitectures%252C%2520including%2520conditional%2520GANs%2520%2528cGANs%2529%252C%2520CycleGANs%252C%2520and%250ASuper-Resolution%2520GANs%2520%2528SRGANs%2529%252C%2520elucidating%2520their%2520unique%2520strengths%2520and%250Alimitations%2520in%2520the%2520context%2520of%2520LDCT%2520denoising.%2520The%2520evaluation%2520provides%2520both%250Aqualitative%2520and%2520quantitative%2520results%2520related%2520to%2520the%2520improvements%2520in%2520performance%250Ain%2520benchmark%2520and%2520clinical%2520datasets%2520with%2520metrics%2520such%2520as%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS.%250AAfter%2520highlighting%2520the%2520positive%2520results%252C%2520we%2520discuss%2520some%2520of%2520the%2520challenges%250Apreventing%2520a%2520wider%2520clinical%2520use%252C%2520including%2520the%2520interpretability%2520of%2520the%2520images%250Agenerated%2520by%2520GANs%252C%2520synthetic%2520artifacts%252C%2520and%2520the%2520need%2520for%2520clinically%2520relevant%250Ametrics.%2520The%2520review%2520concludes%2520by%2520highlighting%2520the%2520essential%2520significance%2520of%250AGAN-based%2520methodologies%2520in%2520the%2520progression%2520of%2520precision%2520medicine%2520via%2520tailored%250ALDCT%2520denoising%2520models%252C%2520underlining%2520the%2520transformative%2520possibilities%2520presented%250Aby%2520artificial%2520intelligence%2520within%2520contemporary%2520radiological%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAN-Based%20Architecture%20for%20Low-dose%20Computed%20Tomography%20Imaging%0A%20%20Denoising&entry.906535625=Yunuo%20Wang%20and%20Ningning%20Yang%20and%20Jialin%20Li&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20surfaced%20as%20a%20revolutionary%0Aelement%20within%20the%20domain%20of%20low-dose%20computed%20tomography%20%28LDCT%29%20imaging%2C%0Aproviding%20an%20advanced%20resolution%20to%20the%20enduring%20issue%20of%20reconciling%20radiation%0Aexposure%20with%20image%20quality.%20This%20comprehensive%20review%20synthesizes%20the%20rapid%0Aadvancements%20in%20GAN-based%20LDCT%20denoising%20techniques%2C%20examining%20the%20evolution%0Afrom%20foundational%20architectures%20to%20state-of-the-art%20models%20incorporating%0Aadvanced%20features%20such%20as%20anatomical%20priors%2C%20perceptual%20loss%20functions%2C%20and%0Ainnovative%20regularization%20strategies.%20We%20critically%20analyze%20various%20GAN%0Aarchitectures%2C%20including%20conditional%20GANs%20%28cGANs%29%2C%20CycleGANs%2C%20and%0ASuper-Resolution%20GANs%20%28SRGANs%29%2C%20elucidating%20their%20unique%20strengths%20and%0Alimitations%20in%20the%20context%20of%20LDCT%20denoising.%20The%20evaluation%20provides%20both%0Aqualitative%20and%20quantitative%20results%20related%20to%20the%20improvements%20in%20performance%0Ain%20benchmark%20and%20clinical%20datasets%20with%20metrics%20such%20as%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0AAfter%20highlighting%20the%20positive%20results%2C%20we%20discuss%20some%20of%20the%20challenges%0Apreventing%20a%20wider%20clinical%20use%2C%20including%20the%20interpretability%20of%20the%20images%0Agenerated%20by%20GANs%2C%20synthetic%20artifacts%2C%20and%20the%20need%20for%20clinically%20relevant%0Ametrics.%20The%20review%20concludes%20by%20highlighting%20the%20essential%20significance%20of%0AGAN-based%20methodologies%20in%20the%20progression%20of%20precision%20medicine%20via%20tailored%0ALDCT%20denoising%20models%2C%20underlining%20the%20transformative%20possibilities%20presented%0Aby%20artificial%20intelligence%20within%20contemporary%20radiological%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09512v1&entry.124074799=Read"},
{"title": "Graph Neural Networks and Differential Equations: A hybrid approach for\n  data assimilation of fluid flows", "author": "M. Quattromini and M. A. Bucci and S. Cherubini and O. Semeraro", "abstract": "  This study presents a novel hybrid approach that combines Graph Neural\nNetworks (GNNs) with Reynolds-Averaged Navier Stokes (RANS) equations to\nenhance the accuracy of mean flow reconstruction across a range of fluid\ndynamics applications. Traditional purely data-driven Neural Networks (NNs)\nmodels, often struggle maintaining physical consistency. Moreover, they\ntypically require large datasets to achieve reliable performances. The GNN\nframework, which naturally handles unstructured data such as complex geometries\nin Computational Fluid Dynamics (CFD), is here integrated with RANS equations\nas a physical baseline model. The methodology leverages the adjoint method,\nenabling the use of RANS-derived gradients as optimization terms in the GNN\ntraining process. This ensures that the learned model adheres to the governing\nphysics, maintaining physical consistency while improving the prediction\naccuracy. We test our approach on multiple CFD scenarios, including cases\ninvolving generalization with respect to the Reynolds number, sparse\nmeasurements, denoising and inpainting of missing portions of the mean flow.\nThe results demonstrate significant improvements in the accuracy of the\nreconstructed mean flow compared to purely data-driven models, using limited\namounts of data in the training dataset. The key strengths of this study are\nthe integration of physical laws into the training process of the GNN, and the\nability to achieve high-accuracy predictions with a limited amount of data,\nmaking this approach particularly valuable for applications in fluid dynamics\nwhere data is often scarce.\n", "link": "http://arxiv.org/abs/2411.09476v1", "date": "2024-11-14", "relevancy": 2.1678, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5777}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5414}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20and%20Differential%20Equations%3A%20A%20hybrid%20approach%20for%0A%20%20data%20assimilation%20of%20fluid%20flows&body=Title%3A%20Graph%20Neural%20Networks%20and%20Differential%20Equations%3A%20A%20hybrid%20approach%20for%0A%20%20data%20assimilation%20of%20fluid%20flows%0AAuthor%3A%20M.%20Quattromini%20and%20M.%20A.%20Bucci%20and%20S.%20Cherubini%20and%20O.%20Semeraro%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20hybrid%20approach%20that%20combines%20Graph%20Neural%0ANetworks%20%28GNNs%29%20with%20Reynolds-Averaged%20Navier%20Stokes%20%28RANS%29%20equations%20to%0Aenhance%20the%20accuracy%20of%20mean%20flow%20reconstruction%20across%20a%20range%20of%20fluid%0Adynamics%20applications.%20Traditional%20purely%20data-driven%20Neural%20Networks%20%28NNs%29%0Amodels%2C%20often%20struggle%20maintaining%20physical%20consistency.%20Moreover%2C%20they%0Atypically%20require%20large%20datasets%20to%20achieve%20reliable%20performances.%20The%20GNN%0Aframework%2C%20which%20naturally%20handles%20unstructured%20data%20such%20as%20complex%20geometries%0Ain%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20is%20here%20integrated%20with%20RANS%20equations%0Aas%20a%20physical%20baseline%20model.%20The%20methodology%20leverages%20the%20adjoint%20method%2C%0Aenabling%20the%20use%20of%20RANS-derived%20gradients%20as%20optimization%20terms%20in%20the%20GNN%0Atraining%20process.%20This%20ensures%20that%20the%20learned%20model%20adheres%20to%20the%20governing%0Aphysics%2C%20maintaining%20physical%20consistency%20while%20improving%20the%20prediction%0Aaccuracy.%20We%20test%20our%20approach%20on%20multiple%20CFD%20scenarios%2C%20including%20cases%0Ainvolving%20generalization%20with%20respect%20to%20the%20Reynolds%20number%2C%20sparse%0Ameasurements%2C%20denoising%20and%20inpainting%20of%20missing%20portions%20of%20the%20mean%20flow.%0AThe%20results%20demonstrate%20significant%20improvements%20in%20the%20accuracy%20of%20the%0Areconstructed%20mean%20flow%20compared%20to%20purely%20data-driven%20models%2C%20using%20limited%0Aamounts%20of%20data%20in%20the%20training%20dataset.%20The%20key%20strengths%20of%20this%20study%20are%0Athe%20integration%20of%20physical%20laws%20into%20the%20training%20process%20of%20the%20GNN%2C%20and%20the%0Aability%20to%20achieve%20high-accuracy%20predictions%20with%20a%20limited%20amount%20of%20data%2C%0Amaking%20this%20approach%20particularly%20valuable%20for%20applications%20in%20fluid%20dynamics%0Awhere%20data%20is%20often%20scarce.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520and%2520Differential%2520Equations%253A%2520A%2520hybrid%2520approach%2520for%250A%2520%2520data%2520assimilation%2520of%2520fluid%2520flows%26entry.906535625%3DM.%2520Quattromini%2520and%2520M.%2520A.%2520Bucci%2520and%2520S.%2520Cherubini%2520and%2520O.%2520Semeraro%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520hybrid%2520approach%2520that%2520combines%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520with%2520Reynolds-Averaged%2520Navier%2520Stokes%2520%2528RANS%2529%2520equations%2520to%250Aenhance%2520the%2520accuracy%2520of%2520mean%2520flow%2520reconstruction%2520across%2520a%2520range%2520of%2520fluid%250Adynamics%2520applications.%2520Traditional%2520purely%2520data-driven%2520Neural%2520Networks%2520%2528NNs%2529%250Amodels%252C%2520often%2520struggle%2520maintaining%2520physical%2520consistency.%2520Moreover%252C%2520they%250Atypically%2520require%2520large%2520datasets%2520to%2520achieve%2520reliable%2520performances.%2520The%2520GNN%250Aframework%252C%2520which%2520naturally%2520handles%2520unstructured%2520data%2520such%2520as%2520complex%2520geometries%250Ain%2520Computational%2520Fluid%2520Dynamics%2520%2528CFD%2529%252C%2520is%2520here%2520integrated%2520with%2520RANS%2520equations%250Aas%2520a%2520physical%2520baseline%2520model.%2520The%2520methodology%2520leverages%2520the%2520adjoint%2520method%252C%250Aenabling%2520the%2520use%2520of%2520RANS-derived%2520gradients%2520as%2520optimization%2520terms%2520in%2520the%2520GNN%250Atraining%2520process.%2520This%2520ensures%2520that%2520the%2520learned%2520model%2520adheres%2520to%2520the%2520governing%250Aphysics%252C%2520maintaining%2520physical%2520consistency%2520while%2520improving%2520the%2520prediction%250Aaccuracy.%2520We%2520test%2520our%2520approach%2520on%2520multiple%2520CFD%2520scenarios%252C%2520including%2520cases%250Ainvolving%2520generalization%2520with%2520respect%2520to%2520the%2520Reynolds%2520number%252C%2520sparse%250Ameasurements%252C%2520denoising%2520and%2520inpainting%2520of%2520missing%2520portions%2520of%2520the%2520mean%2520flow.%250AThe%2520results%2520demonstrate%2520significant%2520improvements%2520in%2520the%2520accuracy%2520of%2520the%250Areconstructed%2520mean%2520flow%2520compared%2520to%2520purely%2520data-driven%2520models%252C%2520using%2520limited%250Aamounts%2520of%2520data%2520in%2520the%2520training%2520dataset.%2520The%2520key%2520strengths%2520of%2520this%2520study%2520are%250Athe%2520integration%2520of%2520physical%2520laws%2520into%2520the%2520training%2520process%2520of%2520the%2520GNN%252C%2520and%2520the%250Aability%2520to%2520achieve%2520high-accuracy%2520predictions%2520with%2520a%2520limited%2520amount%2520of%2520data%252C%250Amaking%2520this%2520approach%2520particularly%2520valuable%2520for%2520applications%2520in%2520fluid%2520dynamics%250Awhere%2520data%2520is%2520often%2520scarce.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20and%20Differential%20Equations%3A%20A%20hybrid%20approach%20for%0A%20%20data%20assimilation%20of%20fluid%20flows&entry.906535625=M.%20Quattromini%20and%20M.%20A.%20Bucci%20and%20S.%20Cherubini%20and%20O.%20Semeraro&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20hybrid%20approach%20that%20combines%20Graph%20Neural%0ANetworks%20%28GNNs%29%20with%20Reynolds-Averaged%20Navier%20Stokes%20%28RANS%29%20equations%20to%0Aenhance%20the%20accuracy%20of%20mean%20flow%20reconstruction%20across%20a%20range%20of%20fluid%0Adynamics%20applications.%20Traditional%20purely%20data-driven%20Neural%20Networks%20%28NNs%29%0Amodels%2C%20often%20struggle%20maintaining%20physical%20consistency.%20Moreover%2C%20they%0Atypically%20require%20large%20datasets%20to%20achieve%20reliable%20performances.%20The%20GNN%0Aframework%2C%20which%20naturally%20handles%20unstructured%20data%20such%20as%20complex%20geometries%0Ain%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20is%20here%20integrated%20with%20RANS%20equations%0Aas%20a%20physical%20baseline%20model.%20The%20methodology%20leverages%20the%20adjoint%20method%2C%0Aenabling%20the%20use%20of%20RANS-derived%20gradients%20as%20optimization%20terms%20in%20the%20GNN%0Atraining%20process.%20This%20ensures%20that%20the%20learned%20model%20adheres%20to%20the%20governing%0Aphysics%2C%20maintaining%20physical%20consistency%20while%20improving%20the%20prediction%0Aaccuracy.%20We%20test%20our%20approach%20on%20multiple%20CFD%20scenarios%2C%20including%20cases%0Ainvolving%20generalization%20with%20respect%20to%20the%20Reynolds%20number%2C%20sparse%0Ameasurements%2C%20denoising%20and%20inpainting%20of%20missing%20portions%20of%20the%20mean%20flow.%0AThe%20results%20demonstrate%20significant%20improvements%20in%20the%20accuracy%20of%20the%0Areconstructed%20mean%20flow%20compared%20to%20purely%20data-driven%20models%2C%20using%20limited%0Aamounts%20of%20data%20in%20the%20training%20dataset.%20The%20key%20strengths%20of%20this%20study%20are%0Athe%20integration%20of%20physical%20laws%20into%20the%20training%20process%20of%20the%20GNN%2C%20and%20the%0Aability%20to%20achieve%20high-accuracy%20predictions%20with%20a%20limited%20amount%20of%20data%2C%0Amaking%20this%20approach%20particularly%20valuable%20for%20applications%20in%20fluid%20dynamics%0Awhere%20data%20is%20often%20scarce.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09476v1&entry.124074799=Read"},
{"title": "MikuDance: Animating Character Art with Mixed Motion Dynamics", "author": "Jiaxu Zhang and Xianfang Zeng and Xin Chen and Wei Zuo and Gang Yu and Zhigang Tu", "abstract": "  We propose MikuDance, a diffusion-based pipeline incorporating mixed motion\ndynamics to animate stylized character art. MikuDance consists of two key\ntechniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the\nchallenges of high-dynamic motion and reference-guidance misalignment in\ncharacter art animation. Specifically, a Scene Motion Tracking strategy is\npresented to explicitly model the dynamic camera in pixel-wise space, enabling\nunified character-scene motion modeling. Building on this, the Mixed-Control\nDiffusion implicitly aligns the scale and body shape of diverse characters with\nmotion guidance, allowing flexible control of local character motion.\nSubsequently, a Motion-Adaptive Normalization module is incorporated to\neffectively inject global scene motion, paving the way for comprehensive\ncharacter art animation. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of MikuDance across various character art\nand motion guidance, consistently producing high-quality animations with\nremarkable motion dynamics.\n", "link": "http://arxiv.org/abs/2411.08656v2", "date": "2024-11-14", "relevancy": 2.1666, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.588}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5122}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics&body=Title%3A%20MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics%0AAuthor%3A%20Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Zhigang%20Tu%0AAbstract%3A%20%20%20We%20propose%20MikuDance%2C%20a%20diffusion-based%20pipeline%20incorporating%20mixed%20motion%0Adynamics%20to%20animate%20stylized%20character%20art.%20MikuDance%20consists%20of%20two%20key%0Atechniques%3A%20Mixed%20Motion%20Modeling%20and%20Mixed-Control%20Diffusion%2C%20to%20address%20the%0Achallenges%20of%20high-dynamic%20motion%20and%20reference-guidance%20misalignment%20in%0Acharacter%20art%20animation.%20Specifically%2C%20a%20Scene%20Motion%20Tracking%20strategy%20is%0Apresented%20to%20explicitly%20model%20the%20dynamic%20camera%20in%20pixel-wise%20space%2C%20enabling%0Aunified%20character-scene%20motion%20modeling.%20Building%20on%20this%2C%20the%20Mixed-Control%0ADiffusion%20implicitly%20aligns%20the%20scale%20and%20body%20shape%20of%20diverse%20characters%20with%0Amotion%20guidance%2C%20allowing%20flexible%20control%20of%20local%20character%20motion.%0ASubsequently%2C%20a%20Motion-Adaptive%20Normalization%20module%20is%20incorporated%20to%0Aeffectively%20inject%20global%20scene%20motion%2C%20paving%20the%20way%20for%20comprehensive%0Acharacter%20art%20animation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20MikuDance%20across%20various%20character%20art%0Aand%20motion%20guidance%2C%20consistently%20producing%20high-quality%20animations%20with%0Aremarkable%20motion%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMikuDance%253A%2520Animating%2520Character%2520Art%2520with%2520Mixed%2520Motion%2520Dynamics%26entry.906535625%3DJiaxu%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Xin%2520Chen%2520and%2520Wei%2520Zuo%2520and%2520Gang%2520Yu%2520and%2520Zhigang%2520Tu%26entry.1292438233%3D%2520%2520We%2520propose%2520MikuDance%252C%2520a%2520diffusion-based%2520pipeline%2520incorporating%2520mixed%2520motion%250Adynamics%2520to%2520animate%2520stylized%2520character%2520art.%2520MikuDance%2520consists%2520of%2520two%2520key%250Atechniques%253A%2520Mixed%2520Motion%2520Modeling%2520and%2520Mixed-Control%2520Diffusion%252C%2520to%2520address%2520the%250Achallenges%2520of%2520high-dynamic%2520motion%2520and%2520reference-guidance%2520misalignment%2520in%250Acharacter%2520art%2520animation.%2520Specifically%252C%2520a%2520Scene%2520Motion%2520Tracking%2520strategy%2520is%250Apresented%2520to%2520explicitly%2520model%2520the%2520dynamic%2520camera%2520in%2520pixel-wise%2520space%252C%2520enabling%250Aunified%2520character-scene%2520motion%2520modeling.%2520Building%2520on%2520this%252C%2520the%2520Mixed-Control%250ADiffusion%2520implicitly%2520aligns%2520the%2520scale%2520and%2520body%2520shape%2520of%2520diverse%2520characters%2520with%250Amotion%2520guidance%252C%2520allowing%2520flexible%2520control%2520of%2520local%2520character%2520motion.%250ASubsequently%252C%2520a%2520Motion-Adaptive%2520Normalization%2520module%2520is%2520incorporated%2520to%250Aeffectively%2520inject%2520global%2520scene%2520motion%252C%2520paving%2520the%2520way%2520for%2520comprehensive%250Acharacter%2520art%2520animation.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generalizability%2520of%2520MikuDance%2520across%2520various%2520character%2520art%250Aand%2520motion%2520guidance%252C%2520consistently%2520producing%2520high-quality%2520animations%2520with%250Aremarkable%2520motion%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics&entry.906535625=Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Zhigang%20Tu&entry.1292438233=%20%20We%20propose%20MikuDance%2C%20a%20diffusion-based%20pipeline%20incorporating%20mixed%20motion%0Adynamics%20to%20animate%20stylized%20character%20art.%20MikuDance%20consists%20of%20two%20key%0Atechniques%3A%20Mixed%20Motion%20Modeling%20and%20Mixed-Control%20Diffusion%2C%20to%20address%20the%0Achallenges%20of%20high-dynamic%20motion%20and%20reference-guidance%20misalignment%20in%0Acharacter%20art%20animation.%20Specifically%2C%20a%20Scene%20Motion%20Tracking%20strategy%20is%0Apresented%20to%20explicitly%20model%20the%20dynamic%20camera%20in%20pixel-wise%20space%2C%20enabling%0Aunified%20character-scene%20motion%20modeling.%20Building%20on%20this%2C%20the%20Mixed-Control%0ADiffusion%20implicitly%20aligns%20the%20scale%20and%20body%20shape%20of%20diverse%20characters%20with%0Amotion%20guidance%2C%20allowing%20flexible%20control%20of%20local%20character%20motion.%0ASubsequently%2C%20a%20Motion-Adaptive%20Normalization%20module%20is%20incorporated%20to%0Aeffectively%20inject%20global%20scene%20motion%2C%20paving%20the%20way%20for%20comprehensive%0Acharacter%20art%20animation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20MikuDance%20across%20various%20character%20art%0Aand%20motion%20guidance%2C%20consistently%20producing%20high-quality%20animations%20with%0Aremarkable%20motion%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08656v2&entry.124074799=Read"},
{"title": "Pie: Pooling CPU Memory for LLM Inference", "author": "Yi Xu and Ziming Mao and Xiangxi Mo and Shu Liu and Ion Stoica", "abstract": "  The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.\n", "link": "http://arxiv.org/abs/2411.09317v1", "date": "2024-11-14", "relevancy": 2.1625, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4397}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.432}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pie%3A%20Pooling%20CPU%20Memory%20for%20LLM%20Inference&body=Title%3A%20Pie%3A%20Pooling%20CPU%20Memory%20for%20LLM%20Inference%0AAuthor%3A%20Yi%20Xu%20and%20Ziming%20Mao%20and%20Xiangxi%20Mo%20and%20Shu%20Liu%20and%20Ion%20Stoica%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20LLMs%20has%20revolutionized%20natural%20language%20processing%20and%0AAI%20analysis%2C%20but%20their%20increasing%20size%20and%20memory%20demands%20present%20significant%0Achallenges.%20A%20common%20solution%20is%20to%20spill%20over%20to%20CPU%20memory%3B%20however%2C%0Atraditional%20GPU-CPU%20memory%20swapping%20often%20results%20in%20higher%20latency%20and%20lower%0Athroughput.%0A%20%20This%20paper%20introduces%20Pie%2C%20an%20LLM%20inference%20framework%20that%20addresses%20these%0Achallenges%20with%20performance-transparent%20swapping%20and%20adaptive%20expansion.%20By%0Aleveraging%20predictable%20memory%20access%20patterns%20and%20the%20high%20bandwidth%20of%20modern%0Ahardware%20like%20the%20NVIDIA%20GH200%20Grace%20Hopper%20Superchip%2C%20Pie%20enables%20concurrent%0Adata%20swapping%20without%20affecting%20foreground%20computation%2C%20expanding%20effective%0Amemory%20without%20added%20latency.%20Adaptive%20expansion%20dynamically%20adjusts%20CPU%20memory%0Aallocation%20based%20on%20real-time%20information%2C%20optimizing%20memory%20usage%20and%0Aperformance%20under%20varying%20conditions.%0A%20%20Pie%20maintains%20low%20computation%20latency%2C%20high%20throughput%2C%20and%20high%20elasticity.%0AOur%20experimental%20evaluation%20demonstrates%20that%20Pie%20achieves%20optimal%20swapping%0Apolicy%20during%20cache%20warmup%20and%20effectively%20balances%20increased%20memory%20capacity%0Awith%20negligible%20impact%20on%20computation.%20With%20its%20extended%20capacity%2C%20Pie%0Aoutperforms%20vLLM%20by%20up%20to%201.9X%20in%20throughput%20and%202X%20in%20latency.%20Additionally%2C%0APie%20can%20reduce%20GPU%20memory%20usage%20by%20up%20to%201.67X%20while%20maintaining%20the%20same%0Aperformance.%20Compared%20to%20FlexGen%2C%20an%20offline%20profiling-based%20swapping%20solution%2C%0APie%20achieves%20magnitudes%20lower%20latency%20and%209.4X%20higher%20throughput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPie%253A%2520Pooling%2520CPU%2520Memory%2520for%2520LLM%2520Inference%26entry.906535625%3DYi%2520Xu%2520and%2520Ziming%2520Mao%2520and%2520Xiangxi%2520Mo%2520and%2520Shu%2520Liu%2520and%2520Ion%2520Stoica%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520LLMs%2520has%2520revolutionized%2520natural%2520language%2520processing%2520and%250AAI%2520analysis%252C%2520but%2520their%2520increasing%2520size%2520and%2520memory%2520demands%2520present%2520significant%250Achallenges.%2520A%2520common%2520solution%2520is%2520to%2520spill%2520over%2520to%2520CPU%2520memory%253B%2520however%252C%250Atraditional%2520GPU-CPU%2520memory%2520swapping%2520often%2520results%2520in%2520higher%2520latency%2520and%2520lower%250Athroughput.%250A%2520%2520This%2520paper%2520introduces%2520Pie%252C%2520an%2520LLM%2520inference%2520framework%2520that%2520addresses%2520these%250Achallenges%2520with%2520performance-transparent%2520swapping%2520and%2520adaptive%2520expansion.%2520By%250Aleveraging%2520predictable%2520memory%2520access%2520patterns%2520and%2520the%2520high%2520bandwidth%2520of%2520modern%250Ahardware%2520like%2520the%2520NVIDIA%2520GH200%2520Grace%2520Hopper%2520Superchip%252C%2520Pie%2520enables%2520concurrent%250Adata%2520swapping%2520without%2520affecting%2520foreground%2520computation%252C%2520expanding%2520effective%250Amemory%2520without%2520added%2520latency.%2520Adaptive%2520expansion%2520dynamically%2520adjusts%2520CPU%2520memory%250Aallocation%2520based%2520on%2520real-time%2520information%252C%2520optimizing%2520memory%2520usage%2520and%250Aperformance%2520under%2520varying%2520conditions.%250A%2520%2520Pie%2520maintains%2520low%2520computation%2520latency%252C%2520high%2520throughput%252C%2520and%2520high%2520elasticity.%250AOur%2520experimental%2520evaluation%2520demonstrates%2520that%2520Pie%2520achieves%2520optimal%2520swapping%250Apolicy%2520during%2520cache%2520warmup%2520and%2520effectively%2520balances%2520increased%2520memory%2520capacity%250Awith%2520negligible%2520impact%2520on%2520computation.%2520With%2520its%2520extended%2520capacity%252C%2520Pie%250Aoutperforms%2520vLLM%2520by%2520up%2520to%25201.9X%2520in%2520throughput%2520and%25202X%2520in%2520latency.%2520Additionally%252C%250APie%2520can%2520reduce%2520GPU%2520memory%2520usage%2520by%2520up%2520to%25201.67X%2520while%2520maintaining%2520the%2520same%250Aperformance.%2520Compared%2520to%2520FlexGen%252C%2520an%2520offline%2520profiling-based%2520swapping%2520solution%252C%250APie%2520achieves%2520magnitudes%2520lower%2520latency%2520and%25209.4X%2520higher%2520throughput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pie%3A%20Pooling%20CPU%20Memory%20for%20LLM%20Inference&entry.906535625=Yi%20Xu%20and%20Ziming%20Mao%20and%20Xiangxi%20Mo%20and%20Shu%20Liu%20and%20Ion%20Stoica&entry.1292438233=%20%20The%20rapid%20growth%20of%20LLMs%20has%20revolutionized%20natural%20language%20processing%20and%0AAI%20analysis%2C%20but%20their%20increasing%20size%20and%20memory%20demands%20present%20significant%0Achallenges.%20A%20common%20solution%20is%20to%20spill%20over%20to%20CPU%20memory%3B%20however%2C%0Atraditional%20GPU-CPU%20memory%20swapping%20often%20results%20in%20higher%20latency%20and%20lower%0Athroughput.%0A%20%20This%20paper%20introduces%20Pie%2C%20an%20LLM%20inference%20framework%20that%20addresses%20these%0Achallenges%20with%20performance-transparent%20swapping%20and%20adaptive%20expansion.%20By%0Aleveraging%20predictable%20memory%20access%20patterns%20and%20the%20high%20bandwidth%20of%20modern%0Ahardware%20like%20the%20NVIDIA%20GH200%20Grace%20Hopper%20Superchip%2C%20Pie%20enables%20concurrent%0Adata%20swapping%20without%20affecting%20foreground%20computation%2C%20expanding%20effective%0Amemory%20without%20added%20latency.%20Adaptive%20expansion%20dynamically%20adjusts%20CPU%20memory%0Aallocation%20based%20on%20real-time%20information%2C%20optimizing%20memory%20usage%20and%0Aperformance%20under%20varying%20conditions.%0A%20%20Pie%20maintains%20low%20computation%20latency%2C%20high%20throughput%2C%20and%20high%20elasticity.%0AOur%20experimental%20evaluation%20demonstrates%20that%20Pie%20achieves%20optimal%20swapping%0Apolicy%20during%20cache%20warmup%20and%20effectively%20balances%20increased%20memory%20capacity%0Awith%20negligible%20impact%20on%20computation.%20With%20its%20extended%20capacity%2C%20Pie%0Aoutperforms%20vLLM%20by%20up%20to%201.9X%20in%20throughput%20and%202X%20in%20latency.%20Additionally%2C%0APie%20can%20reduce%20GPU%20memory%20usage%20by%20up%20to%201.67X%20while%20maintaining%20the%20same%0Aperformance.%20Compared%20to%20FlexGen%2C%20an%20offline%20profiling-based%20swapping%20solution%2C%0APie%20achieves%20magnitudes%20lower%20latency%20and%209.4X%20higher%20throughput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09317v1&entry.124074799=Read"},
{"title": "Marker-free Human Gait Analysis using a Smart Edge Sensor System", "author": "Eva Katharina Bauer and Simon Bultmann and Sven Behnke", "abstract": "  The human gait is a complex interplay between the neuronal and the muscular\nsystems, reflecting an individual's neurological and physiological condition.\nThis makes gait analysis a valuable tool for biomechanics and medical experts.\nTraditional observational gait analysis is cost-effective but lacks reliability\nand accuracy, while instrumented gait analysis, particularly using marker-based\noptical systems, provides accurate data but is expensive and time-consuming. In\nthis paper, we introduce a novel markerless approach for gait analysis using a\nmulti-camera setup with smart edge sensors to estimate 3D body poses without\nfiducial markers. We propose a Siamese embedding network with triplet loss\ncalculation to identify individuals by their gait pattern. This network\neffectively maps gait sequences to an embedding space that enables clustering\nsequences from the same individual or activity closely together while\nseparating those of different ones. Our results demonstrate the potential of\nthe proposed system for efficient automated gait analysis in diverse real-world\nenvironments, facilitating a wide range of applications.\n", "link": "http://arxiv.org/abs/2411.09538v1", "date": "2024-11-14", "relevancy": 2.1592, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5685}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5419}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Marker-free%20Human%20Gait%20Analysis%20using%20a%20Smart%20Edge%20Sensor%20System&body=Title%3A%20Marker-free%20Human%20Gait%20Analysis%20using%20a%20Smart%20Edge%20Sensor%20System%0AAuthor%3A%20Eva%20Katharina%20Bauer%20and%20Simon%20Bultmann%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20The%20human%20gait%20is%20a%20complex%20interplay%20between%20the%20neuronal%20and%20the%20muscular%0Asystems%2C%20reflecting%20an%20individual%27s%20neurological%20and%20physiological%20condition.%0AThis%20makes%20gait%20analysis%20a%20valuable%20tool%20for%20biomechanics%20and%20medical%20experts.%0ATraditional%20observational%20gait%20analysis%20is%20cost-effective%20but%20lacks%20reliability%0Aand%20accuracy%2C%20while%20instrumented%20gait%20analysis%2C%20particularly%20using%20marker-based%0Aoptical%20systems%2C%20provides%20accurate%20data%20but%20is%20expensive%20and%20time-consuming.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20markerless%20approach%20for%20gait%20analysis%20using%20a%0Amulti-camera%20setup%20with%20smart%20edge%20sensors%20to%20estimate%203D%20body%20poses%20without%0Afiducial%20markers.%20We%20propose%20a%20Siamese%20embedding%20network%20with%20triplet%20loss%0Acalculation%20to%20identify%20individuals%20by%20their%20gait%20pattern.%20This%20network%0Aeffectively%20maps%20gait%20sequences%20to%20an%20embedding%20space%20that%20enables%20clustering%0Asequences%20from%20the%20same%20individual%20or%20activity%20closely%20together%20while%0Aseparating%20those%20of%20different%20ones.%20Our%20results%20demonstrate%20the%20potential%20of%0Athe%20proposed%20system%20for%20efficient%20automated%20gait%20analysis%20in%20diverse%20real-world%0Aenvironments%2C%20facilitating%20a%20wide%20range%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarker-free%2520Human%2520Gait%2520Analysis%2520using%2520a%2520Smart%2520Edge%2520Sensor%2520System%26entry.906535625%3DEva%2520Katharina%2520Bauer%2520and%2520Simon%2520Bultmann%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520The%2520human%2520gait%2520is%2520a%2520complex%2520interplay%2520between%2520the%2520neuronal%2520and%2520the%2520muscular%250Asystems%252C%2520reflecting%2520an%2520individual%2527s%2520neurological%2520and%2520physiological%2520condition.%250AThis%2520makes%2520gait%2520analysis%2520a%2520valuable%2520tool%2520for%2520biomechanics%2520and%2520medical%2520experts.%250ATraditional%2520observational%2520gait%2520analysis%2520is%2520cost-effective%2520but%2520lacks%2520reliability%250Aand%2520accuracy%252C%2520while%2520instrumented%2520gait%2520analysis%252C%2520particularly%2520using%2520marker-based%250Aoptical%2520systems%252C%2520provides%2520accurate%2520data%2520but%2520is%2520expensive%2520and%2520time-consuming.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520markerless%2520approach%2520for%2520gait%2520analysis%2520using%2520a%250Amulti-camera%2520setup%2520with%2520smart%2520edge%2520sensors%2520to%2520estimate%25203D%2520body%2520poses%2520without%250Afiducial%2520markers.%2520We%2520propose%2520a%2520Siamese%2520embedding%2520network%2520with%2520triplet%2520loss%250Acalculation%2520to%2520identify%2520individuals%2520by%2520their%2520gait%2520pattern.%2520This%2520network%250Aeffectively%2520maps%2520gait%2520sequences%2520to%2520an%2520embedding%2520space%2520that%2520enables%2520clustering%250Asequences%2520from%2520the%2520same%2520individual%2520or%2520activity%2520closely%2520together%2520while%250Aseparating%2520those%2520of%2520different%2520ones.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%250Athe%2520proposed%2520system%2520for%2520efficient%2520automated%2520gait%2520analysis%2520in%2520diverse%2520real-world%250Aenvironments%252C%2520facilitating%2520a%2520wide%2520range%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Marker-free%20Human%20Gait%20Analysis%20using%20a%20Smart%20Edge%20Sensor%20System&entry.906535625=Eva%20Katharina%20Bauer%20and%20Simon%20Bultmann%20and%20Sven%20Behnke&entry.1292438233=%20%20The%20human%20gait%20is%20a%20complex%20interplay%20between%20the%20neuronal%20and%20the%20muscular%0Asystems%2C%20reflecting%20an%20individual%27s%20neurological%20and%20physiological%20condition.%0AThis%20makes%20gait%20analysis%20a%20valuable%20tool%20for%20biomechanics%20and%20medical%20experts.%0ATraditional%20observational%20gait%20analysis%20is%20cost-effective%20but%20lacks%20reliability%0Aand%20accuracy%2C%20while%20instrumented%20gait%20analysis%2C%20particularly%20using%20marker-based%0Aoptical%20systems%2C%20provides%20accurate%20data%20but%20is%20expensive%20and%20time-consuming.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20markerless%20approach%20for%20gait%20analysis%20using%20a%0Amulti-camera%20setup%20with%20smart%20edge%20sensors%20to%20estimate%203D%20body%20poses%20without%0Afiducial%20markers.%20We%20propose%20a%20Siamese%20embedding%20network%20with%20triplet%20loss%0Acalculation%20to%20identify%20individuals%20by%20their%20gait%20pattern.%20This%20network%0Aeffectively%20maps%20gait%20sequences%20to%20an%20embedding%20space%20that%20enables%20clustering%0Asequences%20from%20the%20same%20individual%20or%20activity%20closely%20together%20while%0Aseparating%20those%20of%20different%20ones.%20Our%20results%20demonstrate%20the%20potential%20of%0Athe%20proposed%20system%20for%20efficient%20automated%20gait%20analysis%20in%20diverse%20real-world%0Aenvironments%2C%20facilitating%20a%20wide%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09538v1&entry.124074799=Read"},
{"title": "Generative Adversarial Networks for Spatio-Spectral Compression of\n  Hyperspectral Images", "author": "Martin Hermann Paul Fuchs and Akshara Preethy Byju and Alisa Walda and Behnood Rasti and Beg\u00fcm Demir", "abstract": "  The development of deep learning-based models for the compression of\nhyperspectral images (HSIs) has recently attracted great attention in remote\nsensing due to the sharp growing of hyperspectral data archives. Most of the\nexisting models achieve either spectral or spatial compression, and do not\njointly consider the spatio-spectral redundancies present in HSIs. To address\nthis problem, in this paper we focus our attention on the High Fidelity\nCompression (HiFiC) model (which is proven to be highly effective for spatial\ncompression problems) and adapt it to perform spatio-spectral compression of\nHSIs. In detail, we introduce two new models: i) HiFiC using Squeeze and\nExcitation (SE) blocks (denoted as HiFiC$_{SE}$); and ii) HiFiC with 3D\nconvolutions (denoted as HiFiC$_{3D}$) in the framework of compression of HSIs.\nWe analyze the effectiveness of HiFiC$_{SE}$ and HiFiC$_{3D}$ in compressing\nthe spatio-spectral redundancies with channel attention and inter-dependency\nanalysis. Experimental results show the efficacy of the proposed models in\nperforming spatio-spectral compression, while reconstructing images at reduced\nbitrates with higher reconstruction quality. The code of the proposed models is\npublicly available at https://git.tu-berlin.de/rsim/HSI-SSC .\n", "link": "http://arxiv.org/abs/2305.08514v4", "date": "2024-11-14", "relevancy": 2.1543, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images&body=Title%3A%20Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images%0AAuthor%3A%20Martin%20Hermann%20Paul%20Fuchs%20and%20Akshara%20Preethy%20Byju%20and%20Alisa%20Walda%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20The%20development%20of%20deep%20learning-based%20models%20for%20the%20compression%20of%0Ahyperspectral%20images%20%28HSIs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%0Asensing%20due%20to%20the%20sharp%20growing%20of%20hyperspectral%20data%20archives.%20Most%20of%20the%0Aexisting%20models%20achieve%20either%20spectral%20or%20spatial%20compression%2C%20and%20do%20not%0Ajointly%20consider%20the%20spatio-spectral%20redundancies%20present%20in%20HSIs.%20To%20address%0Athis%20problem%2C%20in%20this%20paper%20we%20focus%20our%20attention%20on%20the%20High%20Fidelity%0ACompression%20%28HiFiC%29%20model%20%28which%20is%20proven%20to%20be%20highly%20effective%20for%20spatial%0Acompression%20problems%29%20and%20adapt%20it%20to%20perform%20spatio-spectral%20compression%20of%0AHSIs.%20In%20detail%2C%20we%20introduce%20two%20new%20models%3A%20i%29%20HiFiC%20using%20Squeeze%20and%0AExcitation%20%28SE%29%20blocks%20%28denoted%20as%20HiFiC%24_%7BSE%7D%24%29%3B%20and%20ii%29%20HiFiC%20with%203D%0Aconvolutions%20%28denoted%20as%20HiFiC%24_%7B3D%7D%24%29%20in%20the%20framework%20of%20compression%20of%20HSIs.%0AWe%20analyze%20the%20effectiveness%20of%20HiFiC%24_%7BSE%7D%24%20and%20HiFiC%24_%7B3D%7D%24%20in%20compressing%0Athe%20spatio-spectral%20redundancies%20with%20channel%20attention%20and%20inter-dependency%0Aanalysis.%20Experimental%20results%20show%20the%20efficacy%20of%20the%20proposed%20models%20in%0Aperforming%20spatio-spectral%20compression%2C%20while%20reconstructing%20images%20at%20reduced%0Abitrates%20with%20higher%20reconstruction%20quality.%20The%20code%20of%20the%20proposed%20models%20is%0Apublicly%20available%20at%20https%3A//git.tu-berlin.de/rsim/HSI-SSC%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08514v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520for%2520Spatio-Spectral%2520Compression%2520of%250A%2520%2520Hyperspectral%2520Images%26entry.906535625%3DMartin%2520Hermann%2520Paul%2520Fuchs%2520and%2520Akshara%2520Preethy%2520Byju%2520and%2520Alisa%2520Walda%2520and%2520Behnood%2520Rasti%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520deep%2520learning-based%2520models%2520for%2520the%2520compression%2520of%250Ahyperspectral%2520images%2520%2528HSIs%2529%2520has%2520recently%2520attracted%2520great%2520attention%2520in%2520remote%250Asensing%2520due%2520to%2520the%2520sharp%2520growing%2520of%2520hyperspectral%2520data%2520archives.%2520Most%2520of%2520the%250Aexisting%2520models%2520achieve%2520either%2520spectral%2520or%2520spatial%2520compression%252C%2520and%2520do%2520not%250Ajointly%2520consider%2520the%2520spatio-spectral%2520redundancies%2520present%2520in%2520HSIs.%2520To%2520address%250Athis%2520problem%252C%2520in%2520this%2520paper%2520we%2520focus%2520our%2520attention%2520on%2520the%2520High%2520Fidelity%250ACompression%2520%2528HiFiC%2529%2520model%2520%2528which%2520is%2520proven%2520to%2520be%2520highly%2520effective%2520for%2520spatial%250Acompression%2520problems%2529%2520and%2520adapt%2520it%2520to%2520perform%2520spatio-spectral%2520compression%2520of%250AHSIs.%2520In%2520detail%252C%2520we%2520introduce%2520two%2520new%2520models%253A%2520i%2529%2520HiFiC%2520using%2520Squeeze%2520and%250AExcitation%2520%2528SE%2529%2520blocks%2520%2528denoted%2520as%2520HiFiC%2524_%257BSE%257D%2524%2529%253B%2520and%2520ii%2529%2520HiFiC%2520with%25203D%250Aconvolutions%2520%2528denoted%2520as%2520HiFiC%2524_%257B3D%257D%2524%2529%2520in%2520the%2520framework%2520of%2520compression%2520of%2520HSIs.%250AWe%2520analyze%2520the%2520effectiveness%2520of%2520HiFiC%2524_%257BSE%257D%2524%2520and%2520HiFiC%2524_%257B3D%257D%2524%2520in%2520compressing%250Athe%2520spatio-spectral%2520redundancies%2520with%2520channel%2520attention%2520and%2520inter-dependency%250Aanalysis.%2520Experimental%2520results%2520show%2520the%2520efficacy%2520of%2520the%2520proposed%2520models%2520in%250Aperforming%2520spatio-spectral%2520compression%252C%2520while%2520reconstructing%2520images%2520at%2520reduced%250Abitrates%2520with%2520higher%2520reconstruction%2520quality.%2520The%2520code%2520of%2520the%2520proposed%2520models%2520is%250Apublicly%2520available%2520at%2520https%253A//git.tu-berlin.de/rsim/HSI-SSC%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08514v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images&entry.906535625=Martin%20Hermann%20Paul%20Fuchs%20and%20Akshara%20Preethy%20Byju%20and%20Alisa%20Walda%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20The%20development%20of%20deep%20learning-based%20models%20for%20the%20compression%20of%0Ahyperspectral%20images%20%28HSIs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%0Asensing%20due%20to%20the%20sharp%20growing%20of%20hyperspectral%20data%20archives.%20Most%20of%20the%0Aexisting%20models%20achieve%20either%20spectral%20or%20spatial%20compression%2C%20and%20do%20not%0Ajointly%20consider%20the%20spatio-spectral%20redundancies%20present%20in%20HSIs.%20To%20address%0Athis%20problem%2C%20in%20this%20paper%20we%20focus%20our%20attention%20on%20the%20High%20Fidelity%0ACompression%20%28HiFiC%29%20model%20%28which%20is%20proven%20to%20be%20highly%20effective%20for%20spatial%0Acompression%20problems%29%20and%20adapt%20it%20to%20perform%20spatio-spectral%20compression%20of%0AHSIs.%20In%20detail%2C%20we%20introduce%20two%20new%20models%3A%20i%29%20HiFiC%20using%20Squeeze%20and%0AExcitation%20%28SE%29%20blocks%20%28denoted%20as%20HiFiC%24_%7BSE%7D%24%29%3B%20and%20ii%29%20HiFiC%20with%203D%0Aconvolutions%20%28denoted%20as%20HiFiC%24_%7B3D%7D%24%29%20in%20the%20framework%20of%20compression%20of%20HSIs.%0AWe%20analyze%20the%20effectiveness%20of%20HiFiC%24_%7BSE%7D%24%20and%20HiFiC%24_%7B3D%7D%24%20in%20compressing%0Athe%20spatio-spectral%20redundancies%20with%20channel%20attention%20and%20inter-dependency%0Aanalysis.%20Experimental%20results%20show%20the%20efficacy%20of%20the%20proposed%20models%20in%0Aperforming%20spatio-spectral%20compression%2C%20while%20reconstructing%20images%20at%20reduced%0Abitrates%20with%20higher%20reconstruction%20quality.%20The%20code%20of%20the%20proposed%20models%20is%0Apublicly%20available%20at%20https%3A//git.tu-berlin.de/rsim/HSI-SSC%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08514v4&entry.124074799=Read"},
{"title": "MagicQuill: An Intelligent Interactive Image Editing System", "author": "Zichen Liu and Yue Yu and Hao Ouyang and Qiuyu Wang and Ka Leong Cheng and Wen Wang and Zhiheng Liu and Qifeng Chen and Yujun Shen", "abstract": "  Image editing involves a variety of complex tasks and requires efficient and\nprecise manipulation techniques. In this paper, we present MagicQuill, an\nintegrated image editing system that enables swift actualization of creative\nideas. Our system features a streamlined yet functionally robust interface,\nallowing for the articulation of editing operations (e.g., inserting elements,\nerasing objects, altering color) with minimal input. These interactions are\nmonitored by a multimodal large language model (MLLM) to anticipate editing\nintentions in real time, bypassing the need for explicit prompt entry. Finally,\nwe apply a powerful diffusion prior, enhanced by a carefully learned two-branch\nplug-in module, to process editing requests with precise control. Experimental\nresults demonstrate the effectiveness of MagicQuill in achieving high-quality\nimage edits. Please visit https://magic-quill.github.io to try out our system.\n", "link": "http://arxiv.org/abs/2411.09703v1", "date": "2024-11-14", "relevancy": 2.1482, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5485}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5343}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicQuill%3A%20An%20Intelligent%20Interactive%20Image%20Editing%20System&body=Title%3A%20MagicQuill%3A%20An%20Intelligent%20Interactive%20Image%20Editing%20System%0AAuthor%3A%20Zichen%20Liu%20and%20Yue%20Yu%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Wen%20Wang%20and%20Zhiheng%20Liu%20and%20Qifeng%20Chen%20and%20Yujun%20Shen%0AAbstract%3A%20%20%20Image%20editing%20involves%20a%20variety%20of%20complex%20tasks%20and%20requires%20efficient%20and%0Aprecise%20manipulation%20techniques.%20In%20this%20paper%2C%20we%20present%20MagicQuill%2C%20an%0Aintegrated%20image%20editing%20system%20that%20enables%20swift%20actualization%20of%20creative%0Aideas.%20Our%20system%20features%20a%20streamlined%20yet%20functionally%20robust%20interface%2C%0Aallowing%20for%20the%20articulation%20of%20editing%20operations%20%28e.g.%2C%20inserting%20elements%2C%0Aerasing%20objects%2C%20altering%20color%29%20with%20minimal%20input.%20These%20interactions%20are%0Amonitored%20by%20a%20multimodal%20large%20language%20model%20%28MLLM%29%20to%20anticipate%20editing%0Aintentions%20in%20real%20time%2C%20bypassing%20the%20need%20for%20explicit%20prompt%20entry.%20Finally%2C%0Awe%20apply%20a%20powerful%20diffusion%20prior%2C%20enhanced%20by%20a%20carefully%20learned%20two-branch%0Aplug-in%20module%2C%20to%20process%20editing%20requests%20with%20precise%20control.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20MagicQuill%20in%20achieving%20high-quality%0Aimage%20edits.%20Please%20visit%20https%3A//magic-quill.github.io%20to%20try%20out%20our%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicQuill%253A%2520An%2520Intelligent%2520Interactive%2520Image%2520Editing%2520System%26entry.906535625%3DZichen%2520Liu%2520and%2520Yue%2520Yu%2520and%2520Hao%2520Ouyang%2520and%2520Qiuyu%2520Wang%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Wen%2520Wang%2520and%2520Zhiheng%2520Liu%2520and%2520Qifeng%2520Chen%2520and%2520Yujun%2520Shen%26entry.1292438233%3D%2520%2520Image%2520editing%2520involves%2520a%2520variety%2520of%2520complex%2520tasks%2520and%2520requires%2520efficient%2520and%250Aprecise%2520manipulation%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520present%2520MagicQuill%252C%2520an%250Aintegrated%2520image%2520editing%2520system%2520that%2520enables%2520swift%2520actualization%2520of%2520creative%250Aideas.%2520Our%2520system%2520features%2520a%2520streamlined%2520yet%2520functionally%2520robust%2520interface%252C%250Aallowing%2520for%2520the%2520articulation%2520of%2520editing%2520operations%2520%2528e.g.%252C%2520inserting%2520elements%252C%250Aerasing%2520objects%252C%2520altering%2520color%2529%2520with%2520minimal%2520input.%2520These%2520interactions%2520are%250Amonitored%2520by%2520a%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520to%2520anticipate%2520editing%250Aintentions%2520in%2520real%2520time%252C%2520bypassing%2520the%2520need%2520for%2520explicit%2520prompt%2520entry.%2520Finally%252C%250Awe%2520apply%2520a%2520powerful%2520diffusion%2520prior%252C%2520enhanced%2520by%2520a%2520carefully%2520learned%2520two-branch%250Aplug-in%2520module%252C%2520to%2520process%2520editing%2520requests%2520with%2520precise%2520control.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520MagicQuill%2520in%2520achieving%2520high-quality%250Aimage%2520edits.%2520Please%2520visit%2520https%253A//magic-quill.github.io%2520to%2520try%2520out%2520our%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicQuill%3A%20An%20Intelligent%20Interactive%20Image%20Editing%20System&entry.906535625=Zichen%20Liu%20and%20Yue%20Yu%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Wen%20Wang%20and%20Zhiheng%20Liu%20and%20Qifeng%20Chen%20and%20Yujun%20Shen&entry.1292438233=%20%20Image%20editing%20involves%20a%20variety%20of%20complex%20tasks%20and%20requires%20efficient%20and%0Aprecise%20manipulation%20techniques.%20In%20this%20paper%2C%20we%20present%20MagicQuill%2C%20an%0Aintegrated%20image%20editing%20system%20that%20enables%20swift%20actualization%20of%20creative%0Aideas.%20Our%20system%20features%20a%20streamlined%20yet%20functionally%20robust%20interface%2C%0Aallowing%20for%20the%20articulation%20of%20editing%20operations%20%28e.g.%2C%20inserting%20elements%2C%0Aerasing%20objects%2C%20altering%20color%29%20with%20minimal%20input.%20These%20interactions%20are%0Amonitored%20by%20a%20multimodal%20large%20language%20model%20%28MLLM%29%20to%20anticipate%20editing%0Aintentions%20in%20real%20time%2C%20bypassing%20the%20need%20for%20explicit%20prompt%20entry.%20Finally%2C%0Awe%20apply%20a%20powerful%20diffusion%20prior%2C%20enhanced%20by%20a%20carefully%20learned%20two-branch%0Aplug-in%20module%2C%20to%20process%20editing%20requests%20with%20precise%20control.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20MagicQuill%20in%20achieving%20high-quality%0Aimage%20edits.%20Please%20visit%20https%3A//magic-quill.github.io%20to%20try%20out%20our%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09703v1&entry.124074799=Read"},
{"title": "Volume-Preserving Transformers for Learning Time Series Data with\n  Structure", "author": "Benedikt Brantner and Guillaume de Romemont and Michael Kraus and Zeyuan Li", "abstract": "  Two of the many trends in neural network research of the past few years have\nbeen (i) the learning of dynamical systems, especially with recurrent neural\nnetworks such as long short-term memory networks (LSTMs) and (ii) the\nintroduction of transformer neural networks for natural language processing\n(NLP) tasks.\n  While some work has been performed on the intersection of these two trends,\nthose efforts were largely limited to using the vanilla transformer directly\nwithout adjusting its architecture for the setting of a physical system.\n  In this work we develop a transformer-inspired neural network and use it to\nlearn a dynamical system. We (for the first time) change the activation\nfunction of the attention layer to imbue the transformer with\nstructure-preserving properties to improve long-term stability. This is shown\nto be of great advantage when applying the neural network to learning the\ntrajectory of a rigid body.\n", "link": "http://arxiv.org/abs/2312.11166v4", "date": "2024-11-14", "relevancy": 2.1452, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5302}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure&body=Title%3A%20Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure%0AAuthor%3A%20Benedikt%20Brantner%20and%20Guillaume%20de%20Romemont%20and%20Michael%20Kraus%20and%20Zeyuan%20Li%0AAbstract%3A%20%20%20Two%20of%20the%20many%20trends%20in%20neural%20network%20research%20of%20the%20past%20few%20years%20have%0Abeen%20%28i%29%20the%20learning%20of%20dynamical%20systems%2C%20especially%20with%20recurrent%20neural%0Anetworks%20such%20as%20long%20short-term%20memory%20networks%20%28LSTMs%29%20and%20%28ii%29%20the%0Aintroduction%20of%20transformer%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20tasks.%0A%20%20While%20some%20work%20has%20been%20performed%20on%20the%20intersection%20of%20these%20two%20trends%2C%0Athose%20efforts%20were%20largely%20limited%20to%20using%20the%20vanilla%20transformer%20directly%0Awithout%20adjusting%20its%20architecture%20for%20the%20setting%20of%20a%20physical%20system.%0A%20%20In%20this%20work%20we%20develop%20a%20transformer-inspired%20neural%20network%20and%20use%20it%20to%0Alearn%20a%20dynamical%20system.%20We%20%28for%20the%20first%20time%29%20change%20the%20activation%0Afunction%20of%20the%20attention%20layer%20to%20imbue%20the%20transformer%20with%0Astructure-preserving%20properties%20to%20improve%20long-term%20stability.%20This%20is%20shown%0Ato%20be%20of%20great%20advantage%20when%20applying%20the%20neural%20network%20to%20learning%20the%0Atrajectory%20of%20a%20rigid%20body.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11166v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolume-Preserving%2520Transformers%2520for%2520Learning%2520Time%2520Series%2520Data%2520with%250A%2520%2520Structure%26entry.906535625%3DBenedikt%2520Brantner%2520and%2520Guillaume%2520de%2520Romemont%2520and%2520Michael%2520Kraus%2520and%2520Zeyuan%2520Li%26entry.1292438233%3D%2520%2520Two%2520of%2520the%2520many%2520trends%2520in%2520neural%2520network%2520research%2520of%2520the%2520past%2520few%2520years%2520have%250Abeen%2520%2528i%2529%2520the%2520learning%2520of%2520dynamical%2520systems%252C%2520especially%2520with%2520recurrent%2520neural%250Anetworks%2520such%2520as%2520long%2520short-term%2520memory%2520networks%2520%2528LSTMs%2529%2520and%2520%2528ii%2529%2520the%250Aintroduction%2520of%2520transformer%2520neural%2520networks%2520for%2520natural%2520language%2520processing%250A%2528NLP%2529%2520tasks.%250A%2520%2520While%2520some%2520work%2520has%2520been%2520performed%2520on%2520the%2520intersection%2520of%2520these%2520two%2520trends%252C%250Athose%2520efforts%2520were%2520largely%2520limited%2520to%2520using%2520the%2520vanilla%2520transformer%2520directly%250Awithout%2520adjusting%2520its%2520architecture%2520for%2520the%2520setting%2520of%2520a%2520physical%2520system.%250A%2520%2520In%2520this%2520work%2520we%2520develop%2520a%2520transformer-inspired%2520neural%2520network%2520and%2520use%2520it%2520to%250Alearn%2520a%2520dynamical%2520system.%2520We%2520%2528for%2520the%2520first%2520time%2529%2520change%2520the%2520activation%250Afunction%2520of%2520the%2520attention%2520layer%2520to%2520imbue%2520the%2520transformer%2520with%250Astructure-preserving%2520properties%2520to%2520improve%2520long-term%2520stability.%2520This%2520is%2520shown%250Ato%2520be%2520of%2520great%2520advantage%2520when%2520applying%2520the%2520neural%2520network%2520to%2520learning%2520the%250Atrajectory%2520of%2520a%2520rigid%2520body.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11166v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure&entry.906535625=Benedikt%20Brantner%20and%20Guillaume%20de%20Romemont%20and%20Michael%20Kraus%20and%20Zeyuan%20Li&entry.1292438233=%20%20Two%20of%20the%20many%20trends%20in%20neural%20network%20research%20of%20the%20past%20few%20years%20have%0Abeen%20%28i%29%20the%20learning%20of%20dynamical%20systems%2C%20especially%20with%20recurrent%20neural%0Anetworks%20such%20as%20long%20short-term%20memory%20networks%20%28LSTMs%29%20and%20%28ii%29%20the%0Aintroduction%20of%20transformer%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20tasks.%0A%20%20While%20some%20work%20has%20been%20performed%20on%20the%20intersection%20of%20these%20two%20trends%2C%0Athose%20efforts%20were%20largely%20limited%20to%20using%20the%20vanilla%20transformer%20directly%0Awithout%20adjusting%20its%20architecture%20for%20the%20setting%20of%20a%20physical%20system.%0A%20%20In%20this%20work%20we%20develop%20a%20transformer-inspired%20neural%20network%20and%20use%20it%20to%0Alearn%20a%20dynamical%20system.%20We%20%28for%20the%20first%20time%29%20change%20the%20activation%0Afunction%20of%20the%20attention%20layer%20to%20imbue%20the%20transformer%20with%0Astructure-preserving%20properties%20to%20improve%20long-term%20stability.%20This%20is%20shown%0Ato%20be%20of%20great%20advantage%20when%20applying%20the%20neural%20network%20to%20learning%20the%0Atrajectory%20of%20a%20rigid%20body.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11166v4&entry.124074799=Read"},
{"title": "Grounding is All You Need? Dual Temporal Grounding for Video Dialog", "author": "You Qin and Wei Ji and Xinze Lan and Hao Fei and Xun Yang and Dan Guo and Roger Zimmermann and Lizi Liao", "abstract": "  In the realm of video dialog response generation, the understanding of video\ncontent and the temporal nuances of conversation history are paramount. While a\nsegment of current research leans heavily on large-scale pretrained\nvisual-language models and often overlooks temporal dynamics, another delves\ndeep into spatial-temporal relationships within videos but demands intricate\nobject trajectory pre-extractions and sidelines dialog temporal dynamics. This\npaper introduces the Dual Temporal Grounding-enhanced Video Dialog model\n(DTGVD), strategically designed to merge the strengths of both dominant\napproaches. It emphasizes dual temporal relationships by predicting dialog\nturn-specific temporal regions, filtering video content accordingly, and\ngrounding responses in both video and dialog contexts. One standout feature of\nDTGVD is its heightened attention to chronological interplay. By recognizing\nand acting upon the dependencies between different dialog turns, it captures\nmore nuanced conversational dynamics. To further bolster the alignment between\nvideo and dialog temporal dynamics, we've implemented a list-wise contrastive\nlearning strategy. Within this framework, accurately grounded turn-clip\npairings are designated as positive samples, while less precise pairings are\ncategorized as negative. This refined classification is then funneled into our\nholistic end-to-end response generation mechanism. Evaluations using\nAVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our\nmethodology.\n", "link": "http://arxiv.org/abs/2410.05767v2", "date": "2024-11-14", "relevancy": 2.1438, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5472}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20is%20All%20You%20Need%3F%20Dual%20Temporal%20Grounding%20for%20Video%20Dialog&body=Title%3A%20Grounding%20is%20All%20You%20Need%3F%20Dual%20Temporal%20Grounding%20for%20Video%20Dialog%0AAuthor%3A%20You%20Qin%20and%20Wei%20Ji%20and%20Xinze%20Lan%20and%20Hao%20Fei%20and%20Xun%20Yang%20and%20Dan%20Guo%20and%20Roger%20Zimmermann%20and%20Lizi%20Liao%0AAbstract%3A%20%20%20In%20the%20realm%20of%20video%20dialog%20response%20generation%2C%20the%20understanding%20of%20video%0Acontent%20and%20the%20temporal%20nuances%20of%20conversation%20history%20are%20paramount.%20While%20a%0Asegment%20of%20current%20research%20leans%20heavily%20on%20large-scale%20pretrained%0Avisual-language%20models%20and%20often%20overlooks%20temporal%20dynamics%2C%20another%20delves%0Adeep%20into%20spatial-temporal%20relationships%20within%20videos%20but%20demands%20intricate%0Aobject%20trajectory%20pre-extractions%20and%20sidelines%20dialog%20temporal%20dynamics.%20This%0Apaper%20introduces%20the%20Dual%20Temporal%20Grounding-enhanced%20Video%20Dialog%20model%0A%28DTGVD%29%2C%20strategically%20designed%20to%20merge%20the%20strengths%20of%20both%20dominant%0Aapproaches.%20It%20emphasizes%20dual%20temporal%20relationships%20by%20predicting%20dialog%0Aturn-specific%20temporal%20regions%2C%20filtering%20video%20content%20accordingly%2C%20and%0Agrounding%20responses%20in%20both%20video%20and%20dialog%20contexts.%20One%20standout%20feature%20of%0ADTGVD%20is%20its%20heightened%20attention%20to%20chronological%20interplay.%20By%20recognizing%0Aand%20acting%20upon%20the%20dependencies%20between%20different%20dialog%20turns%2C%20it%20captures%0Amore%20nuanced%20conversational%20dynamics.%20To%20further%20bolster%20the%20alignment%20between%0Avideo%20and%20dialog%20temporal%20dynamics%2C%20we%27ve%20implemented%20a%20list-wise%20contrastive%0Alearning%20strategy.%20Within%20this%20framework%2C%20accurately%20grounded%20turn-clip%0Apairings%20are%20designated%20as%20positive%20samples%2C%20while%20less%20precise%20pairings%20are%0Acategorized%20as%20negative.%20This%20refined%20classification%20is%20then%20funneled%20into%20our%0Aholistic%20end-to-end%20response%20generation%20mechanism.%20Evaluations%20using%0AAVSD%40DSTC-7%20and%20AVSD%40DSTC-8%20datasets%20underscore%20the%20superiority%20of%20our%0Amethodology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520is%2520All%2520You%2520Need%253F%2520Dual%2520Temporal%2520Grounding%2520for%2520Video%2520Dialog%26entry.906535625%3DYou%2520Qin%2520and%2520Wei%2520Ji%2520and%2520Xinze%2520Lan%2520and%2520Hao%2520Fei%2520and%2520Xun%2520Yang%2520and%2520Dan%2520Guo%2520and%2520Roger%2520Zimmermann%2520and%2520Lizi%2520Liao%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520video%2520dialog%2520response%2520generation%252C%2520the%2520understanding%2520of%2520video%250Acontent%2520and%2520the%2520temporal%2520nuances%2520of%2520conversation%2520history%2520are%2520paramount.%2520While%2520a%250Asegment%2520of%2520current%2520research%2520leans%2520heavily%2520on%2520large-scale%2520pretrained%250Avisual-language%2520models%2520and%2520often%2520overlooks%2520temporal%2520dynamics%252C%2520another%2520delves%250Adeep%2520into%2520spatial-temporal%2520relationships%2520within%2520videos%2520but%2520demands%2520intricate%250Aobject%2520trajectory%2520pre-extractions%2520and%2520sidelines%2520dialog%2520temporal%2520dynamics.%2520This%250Apaper%2520introduces%2520the%2520Dual%2520Temporal%2520Grounding-enhanced%2520Video%2520Dialog%2520model%250A%2528DTGVD%2529%252C%2520strategically%2520designed%2520to%2520merge%2520the%2520strengths%2520of%2520both%2520dominant%250Aapproaches.%2520It%2520emphasizes%2520dual%2520temporal%2520relationships%2520by%2520predicting%2520dialog%250Aturn-specific%2520temporal%2520regions%252C%2520filtering%2520video%2520content%2520accordingly%252C%2520and%250Agrounding%2520responses%2520in%2520both%2520video%2520and%2520dialog%2520contexts.%2520One%2520standout%2520feature%2520of%250ADTGVD%2520is%2520its%2520heightened%2520attention%2520to%2520chronological%2520interplay.%2520By%2520recognizing%250Aand%2520acting%2520upon%2520the%2520dependencies%2520between%2520different%2520dialog%2520turns%252C%2520it%2520captures%250Amore%2520nuanced%2520conversational%2520dynamics.%2520To%2520further%2520bolster%2520the%2520alignment%2520between%250Avideo%2520and%2520dialog%2520temporal%2520dynamics%252C%2520we%2527ve%2520implemented%2520a%2520list-wise%2520contrastive%250Alearning%2520strategy.%2520Within%2520this%2520framework%252C%2520accurately%2520grounded%2520turn-clip%250Apairings%2520are%2520designated%2520as%2520positive%2520samples%252C%2520while%2520less%2520precise%2520pairings%2520are%250Acategorized%2520as%2520negative.%2520This%2520refined%2520classification%2520is%2520then%2520funneled%2520into%2520our%250Aholistic%2520end-to-end%2520response%2520generation%2520mechanism.%2520Evaluations%2520using%250AAVSD%2540DSTC-7%2520and%2520AVSD%2540DSTC-8%2520datasets%2520underscore%2520the%2520superiority%2520of%2520our%250Amethodology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20is%20All%20You%20Need%3F%20Dual%20Temporal%20Grounding%20for%20Video%20Dialog&entry.906535625=You%20Qin%20and%20Wei%20Ji%20and%20Xinze%20Lan%20and%20Hao%20Fei%20and%20Xun%20Yang%20and%20Dan%20Guo%20and%20Roger%20Zimmermann%20and%20Lizi%20Liao&entry.1292438233=%20%20In%20the%20realm%20of%20video%20dialog%20response%20generation%2C%20the%20understanding%20of%20video%0Acontent%20and%20the%20temporal%20nuances%20of%20conversation%20history%20are%20paramount.%20While%20a%0Asegment%20of%20current%20research%20leans%20heavily%20on%20large-scale%20pretrained%0Avisual-language%20models%20and%20often%20overlooks%20temporal%20dynamics%2C%20another%20delves%0Adeep%20into%20spatial-temporal%20relationships%20within%20videos%20but%20demands%20intricate%0Aobject%20trajectory%20pre-extractions%20and%20sidelines%20dialog%20temporal%20dynamics.%20This%0Apaper%20introduces%20the%20Dual%20Temporal%20Grounding-enhanced%20Video%20Dialog%20model%0A%28DTGVD%29%2C%20strategically%20designed%20to%20merge%20the%20strengths%20of%20both%20dominant%0Aapproaches.%20It%20emphasizes%20dual%20temporal%20relationships%20by%20predicting%20dialog%0Aturn-specific%20temporal%20regions%2C%20filtering%20video%20content%20accordingly%2C%20and%0Agrounding%20responses%20in%20both%20video%20and%20dialog%20contexts.%20One%20standout%20feature%20of%0ADTGVD%20is%20its%20heightened%20attention%20to%20chronological%20interplay.%20By%20recognizing%0Aand%20acting%20upon%20the%20dependencies%20between%20different%20dialog%20turns%2C%20it%20captures%0Amore%20nuanced%20conversational%20dynamics.%20To%20further%20bolster%20the%20alignment%20between%0Avideo%20and%20dialog%20temporal%20dynamics%2C%20we%27ve%20implemented%20a%20list-wise%20contrastive%0Alearning%20strategy.%20Within%20this%20framework%2C%20accurately%20grounded%20turn-clip%0Apairings%20are%20designated%20as%20positive%20samples%2C%20while%20less%20precise%20pairings%20are%0Acategorized%20as%20negative.%20This%20refined%20classification%20is%20then%20funneled%20into%20our%0Aholistic%20end-to-end%20response%20generation%20mechanism.%20Evaluations%20using%0AAVSD%40DSTC-7%20and%20AVSD%40DSTC-8%20datasets%20underscore%20the%20superiority%20of%20our%0Amethodology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05767v2&entry.124074799=Read"},
{"title": "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised\n  Anomaly Detection", "author": "Jia Guo and Shuai Lu and Weihang Zhang and Fang Chen and Hongen Liao and Huiqi Li", "abstract": "  Recent studies highlighted a practical setting of unsupervised anomaly\ndetection (UAD) that builds a unified model for multi-class images. Despite\nvarious advancements addressing this challenging task, the detection\nperformance under the multi-class setting still lags far behind\nstate-of-the-art class-separated models. Our research aims to bridge this\nsubstantial performance gap. In this paper, we introduce a minimalistic\nreconstruction-based anomaly detection framework, namely Dinomaly, which\nleverages pure Transformer architectures without relying on complex designs,\nadditional modules, or specialized tricks. Given this powerful framework\nconsisted of only Attentions and MLPs, we found four simple components that are\nessential to multi-class anomaly detection: (1) Foundation Transformers that\nextracts universal and discriminative features, (2) Noisy Bottleneck where\npre-existing Dropouts do all the noise injection tricks, (3) Linear Attention\nthat naturally cannot focus, and (4) Loose Reconstruction that does not force\nlayer-to-layer and point-by-point reconstruction. Extensive experiments are\nconducted across popular anomaly detection benchmarks including MVTec-AD, VisA,\nand Real-IAD. Our proposed Dinomaly achieves impressive image-level AUROC of\n99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only\nsuperior to state-of-the-art multi-class UAD methods, but also achieves the\nmost advanced class-separated UAD records.\n", "link": "http://arxiv.org/abs/2405.14325v4", "date": "2024-11-14", "relevancy": 2.1249, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dinomaly%3A%20The%20Less%20Is%20More%20Philosophy%20in%20Multi-Class%20Unsupervised%0A%20%20Anomaly%20Detection&body=Title%3A%20Dinomaly%3A%20The%20Less%20Is%20More%20Philosophy%20in%20Multi-Class%20Unsupervised%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Jia%20Guo%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Fang%20Chen%20and%20Hongen%20Liao%20and%20Huiqi%20Li%0AAbstract%3A%20%20%20Recent%20studies%20highlighted%20a%20practical%20setting%20of%20unsupervised%20anomaly%0Adetection%20%28UAD%29%20that%20builds%20a%20unified%20model%20for%20multi-class%20images.%20Despite%0Avarious%20advancements%20addressing%20this%20challenging%20task%2C%20the%20detection%0Aperformance%20under%20the%20multi-class%20setting%20still%20lags%20far%20behind%0Astate-of-the-art%20class-separated%20models.%20Our%20research%20aims%20to%20bridge%20this%0Asubstantial%20performance%20gap.%20In%20this%20paper%2C%20we%20introduce%20a%20minimalistic%0Areconstruction-based%20anomaly%20detection%20framework%2C%20namely%20Dinomaly%2C%20which%0Aleverages%20pure%20Transformer%20architectures%20without%20relying%20on%20complex%20designs%2C%0Aadditional%20modules%2C%20or%20specialized%20tricks.%20Given%20this%20powerful%20framework%0Aconsisted%20of%20only%20Attentions%20and%20MLPs%2C%20we%20found%20four%20simple%20components%20that%20are%0Aessential%20to%20multi-class%20anomaly%20detection%3A%20%281%29%20Foundation%20Transformers%20that%0Aextracts%20universal%20and%20discriminative%20features%2C%20%282%29%20Noisy%20Bottleneck%20where%0Apre-existing%20Dropouts%20do%20all%20the%20noise%20injection%20tricks%2C%20%283%29%20Linear%20Attention%0Athat%20naturally%20cannot%20focus%2C%20and%20%284%29%20Loose%20Reconstruction%20that%20does%20not%20force%0Alayer-to-layer%20and%20point-by-point%20reconstruction.%20Extensive%20experiments%20are%0Aconducted%20across%20popular%20anomaly%20detection%20benchmarks%20including%20MVTec-AD%2C%20VisA%2C%0Aand%20Real-IAD.%20Our%20proposed%20Dinomaly%20achieves%20impressive%20image-level%20AUROC%20of%0A99.6%25%2C%2098.7%25%2C%20and%2089.3%25%20on%20the%20three%20datasets%20respectively%2C%20which%20is%20not%20only%0Asuperior%20to%20state-of-the-art%20multi-class%20UAD%20methods%2C%20but%20also%20achieves%20the%0Amost%20advanced%20class-separated%20UAD%20records.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14325v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDinomaly%253A%2520The%2520Less%2520Is%2520More%2520Philosophy%2520in%2520Multi-Class%2520Unsupervised%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DJia%2520Guo%2520and%2520Shuai%2520Lu%2520and%2520Weihang%2520Zhang%2520and%2520Fang%2520Chen%2520and%2520Hongen%2520Liao%2520and%2520Huiqi%2520Li%26entry.1292438233%3D%2520%2520Recent%2520studies%2520highlighted%2520a%2520practical%2520setting%2520of%2520unsupervised%2520anomaly%250Adetection%2520%2528UAD%2529%2520that%2520builds%2520a%2520unified%2520model%2520for%2520multi-class%2520images.%2520Despite%250Avarious%2520advancements%2520addressing%2520this%2520challenging%2520task%252C%2520the%2520detection%250Aperformance%2520under%2520the%2520multi-class%2520setting%2520still%2520lags%2520far%2520behind%250Astate-of-the-art%2520class-separated%2520models.%2520Our%2520research%2520aims%2520to%2520bridge%2520this%250Asubstantial%2520performance%2520gap.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520minimalistic%250Areconstruction-based%2520anomaly%2520detection%2520framework%252C%2520namely%2520Dinomaly%252C%2520which%250Aleverages%2520pure%2520Transformer%2520architectures%2520without%2520relying%2520on%2520complex%2520designs%252C%250Aadditional%2520modules%252C%2520or%2520specialized%2520tricks.%2520Given%2520this%2520powerful%2520framework%250Aconsisted%2520of%2520only%2520Attentions%2520and%2520MLPs%252C%2520we%2520found%2520four%2520simple%2520components%2520that%2520are%250Aessential%2520to%2520multi-class%2520anomaly%2520detection%253A%2520%25281%2529%2520Foundation%2520Transformers%2520that%250Aextracts%2520universal%2520and%2520discriminative%2520features%252C%2520%25282%2529%2520Noisy%2520Bottleneck%2520where%250Apre-existing%2520Dropouts%2520do%2520all%2520the%2520noise%2520injection%2520tricks%252C%2520%25283%2529%2520Linear%2520Attention%250Athat%2520naturally%2520cannot%2520focus%252C%2520and%2520%25284%2529%2520Loose%2520Reconstruction%2520that%2520does%2520not%2520force%250Alayer-to-layer%2520and%2520point-by-point%2520reconstruction.%2520Extensive%2520experiments%2520are%250Aconducted%2520across%2520popular%2520anomaly%2520detection%2520benchmarks%2520including%2520MVTec-AD%252C%2520VisA%252C%250Aand%2520Real-IAD.%2520Our%2520proposed%2520Dinomaly%2520achieves%2520impressive%2520image-level%2520AUROC%2520of%250A99.6%2525%252C%252098.7%2525%252C%2520and%252089.3%2525%2520on%2520the%2520three%2520datasets%2520respectively%252C%2520which%2520is%2520not%2520only%250Asuperior%2520to%2520state-of-the-art%2520multi-class%2520UAD%2520methods%252C%2520but%2520also%2520achieves%2520the%250Amost%2520advanced%2520class-separated%2520UAD%2520records.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14325v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dinomaly%3A%20The%20Less%20Is%20More%20Philosophy%20in%20Multi-Class%20Unsupervised%0A%20%20Anomaly%20Detection&entry.906535625=Jia%20Guo%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Fang%20Chen%20and%20Hongen%20Liao%20and%20Huiqi%20Li&entry.1292438233=%20%20Recent%20studies%20highlighted%20a%20practical%20setting%20of%20unsupervised%20anomaly%0Adetection%20%28UAD%29%20that%20builds%20a%20unified%20model%20for%20multi-class%20images.%20Despite%0Avarious%20advancements%20addressing%20this%20challenging%20task%2C%20the%20detection%0Aperformance%20under%20the%20multi-class%20setting%20still%20lags%20far%20behind%0Astate-of-the-art%20class-separated%20models.%20Our%20research%20aims%20to%20bridge%20this%0Asubstantial%20performance%20gap.%20In%20this%20paper%2C%20we%20introduce%20a%20minimalistic%0Areconstruction-based%20anomaly%20detection%20framework%2C%20namely%20Dinomaly%2C%20which%0Aleverages%20pure%20Transformer%20architectures%20without%20relying%20on%20complex%20designs%2C%0Aadditional%20modules%2C%20or%20specialized%20tricks.%20Given%20this%20powerful%20framework%0Aconsisted%20of%20only%20Attentions%20and%20MLPs%2C%20we%20found%20four%20simple%20components%20that%20are%0Aessential%20to%20multi-class%20anomaly%20detection%3A%20%281%29%20Foundation%20Transformers%20that%0Aextracts%20universal%20and%20discriminative%20features%2C%20%282%29%20Noisy%20Bottleneck%20where%0Apre-existing%20Dropouts%20do%20all%20the%20noise%20injection%20tricks%2C%20%283%29%20Linear%20Attention%0Athat%20naturally%20cannot%20focus%2C%20and%20%284%29%20Loose%20Reconstruction%20that%20does%20not%20force%0Alayer-to-layer%20and%20point-by-point%20reconstruction.%20Extensive%20experiments%20are%0Aconducted%20across%20popular%20anomaly%20detection%20benchmarks%20including%20MVTec-AD%2C%20VisA%2C%0Aand%20Real-IAD.%20Our%20proposed%20Dinomaly%20achieves%20impressive%20image-level%20AUROC%20of%0A99.6%25%2C%2098.7%25%2C%20and%2089.3%25%20on%20the%20three%20datasets%20respectively%2C%20which%20is%20not%20only%0Asuperior%20to%20state-of-the-art%20multi-class%20UAD%20methods%2C%20but%20also%20achieves%20the%0Amost%20advanced%20class-separated%20UAD%20records.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14325v4&entry.124074799=Read"},
{"title": "Super-resolution multi-contrast unbiased eye atlases with deep\n  probabilistic refinement", "author": "Ho Hin Lee and Adam M. Saunders and Michael E. Kim and Samuel W. Remedios and Lucas W. Remedios and Yucheng Tang and Qi Yang and Xin Yu and Shunxing Bao and Chloe Cho and Louise A. Mawn and Tonia S. Rex and Kevin L. Schey and Blake E. Dewey and Jeffrey M. Spraggins and Jerry L. Prince and Yuankai Huo and Bennett A. Landman", "abstract": "  Purpose: Eye morphology varies significantly across the population,\nespecially for the orbit and optic nerve. These variations limit the\nfeasibility and robustness of generalizing population-wise features of eye\norgans to an unbiased spatial reference.\n  Approach: To tackle these limitations, we propose a process for creating\nhigh-resolution unbiased eye atlases. First, to restore spatial details from\nscans with a low through-plane resolution compared to a high in-plane\nresolution, we apply a deep learning-based super-resolution algorithm. Then, we\ngenerate an initial unbiased reference with an iterative metric-based\nregistration using a small portion of subject scans. We register the remaining\nscans to this template and refine the template using an unsupervised deep\nprobabilistic approach that generates a more expansive deformation field to\nenhance the organ boundary alignment. We demonstrate this framework using\nmagnetic resonance images across four different tissue contrasts, generating\nfour atlases in separate spatial alignments.\n  Results: For each tissue contrast, we find a significant improvement using\nthe Wilcoxon signed-rank test in the average Dice score across four labeled\nregions compared to a standard registration framework consisting of rigid,\naffine, and deformable transformations. These results highlight the effective\nalignment of eye organs and boundaries using our proposed process.\n  Conclusions: By combining super-resolution preprocessing and deep\nprobabilistic models, we address the challenge of generating an eye atlas to\nserve as a standardized reference across a largely variable population.\n", "link": "http://arxiv.org/abs/2401.03060v3", "date": "2024-11-14", "relevancy": 2.1186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement&body=Title%3A%20Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement%0AAuthor%3A%20Ho%20Hin%20Lee%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Samuel%20W.%20Remedios%20and%20Lucas%20W.%20Remedios%20and%20Yucheng%20Tang%20and%20Qi%20Yang%20and%20Xin%20Yu%20and%20Shunxing%20Bao%20and%20Chloe%20Cho%20and%20Louise%20A.%20Mawn%20and%20Tonia%20S.%20Rex%20and%20Kevin%20L.%20Schey%20and%20Blake%20E.%20Dewey%20and%20Jeffrey%20M.%20Spraggins%20and%20Jerry%20L.%20Prince%20and%20Yuankai%20Huo%20and%20Bennett%20A.%20Landman%0AAbstract%3A%20%20%20Purpose%3A%20Eye%20morphology%20varies%20significantly%20across%20the%20population%2C%0Aespecially%20for%20the%20orbit%20and%20optic%20nerve.%20These%20variations%20limit%20the%0Afeasibility%20and%20robustness%20of%20generalizing%20population-wise%20features%20of%20eye%0Aorgans%20to%20an%20unbiased%20spatial%20reference.%0A%20%20Approach%3A%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20process%20for%20creating%0Ahigh-resolution%20unbiased%20eye%20atlases.%20First%2C%20to%20restore%20spatial%20details%20from%0Ascans%20with%20a%20low%20through-plane%20resolution%20compared%20to%20a%20high%20in-plane%0Aresolution%2C%20we%20apply%20a%20deep%20learning-based%20super-resolution%20algorithm.%20Then%2C%20we%0Agenerate%20an%20initial%20unbiased%20reference%20with%20an%20iterative%20metric-based%0Aregistration%20using%20a%20small%20portion%20of%20subject%20scans.%20We%20register%20the%20remaining%0Ascans%20to%20this%20template%20and%20refine%20the%20template%20using%20an%20unsupervised%20deep%0Aprobabilistic%20approach%20that%20generates%20a%20more%20expansive%20deformation%20field%20to%0Aenhance%20the%20organ%20boundary%20alignment.%20We%20demonstrate%20this%20framework%20using%0Amagnetic%20resonance%20images%20across%20four%20different%20tissue%20contrasts%2C%20generating%0Afour%20atlases%20in%20separate%20spatial%20alignments.%0A%20%20Results%3A%20For%20each%20tissue%20contrast%2C%20we%20find%20a%20significant%20improvement%20using%0Athe%20Wilcoxon%20signed-rank%20test%20in%20the%20average%20Dice%20score%20across%20four%20labeled%0Aregions%20compared%20to%20a%20standard%20registration%20framework%20consisting%20of%20rigid%2C%0Aaffine%2C%20and%20deformable%20transformations.%20These%20results%20highlight%20the%20effective%0Aalignment%20of%20eye%20organs%20and%20boundaries%20using%20our%20proposed%20process.%0A%20%20Conclusions%3A%20By%20combining%20super-resolution%20preprocessing%20and%20deep%0Aprobabilistic%20models%2C%20we%20address%20the%20challenge%20of%20generating%20an%20eye%20atlas%20to%0Aserve%20as%20a%20standardized%20reference%20across%20a%20largely%20variable%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03060v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-resolution%2520multi-contrast%2520unbiased%2520eye%2520atlases%2520with%2520deep%250A%2520%2520probabilistic%2520refinement%26entry.906535625%3DHo%2520Hin%2520Lee%2520and%2520Adam%2520M.%2520Saunders%2520and%2520Michael%2520E.%2520Kim%2520and%2520Samuel%2520W.%2520Remedios%2520and%2520Lucas%2520W.%2520Remedios%2520and%2520Yucheng%2520Tang%2520and%2520Qi%2520Yang%2520and%2520Xin%2520Yu%2520and%2520Shunxing%2520Bao%2520and%2520Chloe%2520Cho%2520and%2520Louise%2520A.%2520Mawn%2520and%2520Tonia%2520S.%2520Rex%2520and%2520Kevin%2520L.%2520Schey%2520and%2520Blake%2520E.%2520Dewey%2520and%2520Jeffrey%2520M.%2520Spraggins%2520and%2520Jerry%2520L.%2520Prince%2520and%2520Yuankai%2520Huo%2520and%2520Bennett%2520A.%2520Landman%26entry.1292438233%3D%2520%2520Purpose%253A%2520Eye%2520morphology%2520varies%2520significantly%2520across%2520the%2520population%252C%250Aespecially%2520for%2520the%2520orbit%2520and%2520optic%2520nerve.%2520These%2520variations%2520limit%2520the%250Afeasibility%2520and%2520robustness%2520of%2520generalizing%2520population-wise%2520features%2520of%2520eye%250Aorgans%2520to%2520an%2520unbiased%2520spatial%2520reference.%250A%2520%2520Approach%253A%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520propose%2520a%2520process%2520for%2520creating%250Ahigh-resolution%2520unbiased%2520eye%2520atlases.%2520First%252C%2520to%2520restore%2520spatial%2520details%2520from%250Ascans%2520with%2520a%2520low%2520through-plane%2520resolution%2520compared%2520to%2520a%2520high%2520in-plane%250Aresolution%252C%2520we%2520apply%2520a%2520deep%2520learning-based%2520super-resolution%2520algorithm.%2520Then%252C%2520we%250Agenerate%2520an%2520initial%2520unbiased%2520reference%2520with%2520an%2520iterative%2520metric-based%250Aregistration%2520using%2520a%2520small%2520portion%2520of%2520subject%2520scans.%2520We%2520register%2520the%2520remaining%250Ascans%2520to%2520this%2520template%2520and%2520refine%2520the%2520template%2520using%2520an%2520unsupervised%2520deep%250Aprobabilistic%2520approach%2520that%2520generates%2520a%2520more%2520expansive%2520deformation%2520field%2520to%250Aenhance%2520the%2520organ%2520boundary%2520alignment.%2520We%2520demonstrate%2520this%2520framework%2520using%250Amagnetic%2520resonance%2520images%2520across%2520four%2520different%2520tissue%2520contrasts%252C%2520generating%250Afour%2520atlases%2520in%2520separate%2520spatial%2520alignments.%250A%2520%2520Results%253A%2520For%2520each%2520tissue%2520contrast%252C%2520we%2520find%2520a%2520significant%2520improvement%2520using%250Athe%2520Wilcoxon%2520signed-rank%2520test%2520in%2520the%2520average%2520Dice%2520score%2520across%2520four%2520labeled%250Aregions%2520compared%2520to%2520a%2520standard%2520registration%2520framework%2520consisting%2520of%2520rigid%252C%250Aaffine%252C%2520and%2520deformable%2520transformations.%2520These%2520results%2520highlight%2520the%2520effective%250Aalignment%2520of%2520eye%2520organs%2520and%2520boundaries%2520using%2520our%2520proposed%2520process.%250A%2520%2520Conclusions%253A%2520By%2520combining%2520super-resolution%2520preprocessing%2520and%2520deep%250Aprobabilistic%2520models%252C%2520we%2520address%2520the%2520challenge%2520of%2520generating%2520an%2520eye%2520atlas%2520to%250Aserve%2520as%2520a%2520standardized%2520reference%2520across%2520a%2520largely%2520variable%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03060v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement&entry.906535625=Ho%20Hin%20Lee%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Samuel%20W.%20Remedios%20and%20Lucas%20W.%20Remedios%20and%20Yucheng%20Tang%20and%20Qi%20Yang%20and%20Xin%20Yu%20and%20Shunxing%20Bao%20and%20Chloe%20Cho%20and%20Louise%20A.%20Mawn%20and%20Tonia%20S.%20Rex%20and%20Kevin%20L.%20Schey%20and%20Blake%20E.%20Dewey%20and%20Jeffrey%20M.%20Spraggins%20and%20Jerry%20L.%20Prince%20and%20Yuankai%20Huo%20and%20Bennett%20A.%20Landman&entry.1292438233=%20%20Purpose%3A%20Eye%20morphology%20varies%20significantly%20across%20the%20population%2C%0Aespecially%20for%20the%20orbit%20and%20optic%20nerve.%20These%20variations%20limit%20the%0Afeasibility%20and%20robustness%20of%20generalizing%20population-wise%20features%20of%20eye%0Aorgans%20to%20an%20unbiased%20spatial%20reference.%0A%20%20Approach%3A%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20process%20for%20creating%0Ahigh-resolution%20unbiased%20eye%20atlases.%20First%2C%20to%20restore%20spatial%20details%20from%0Ascans%20with%20a%20low%20through-plane%20resolution%20compared%20to%20a%20high%20in-plane%0Aresolution%2C%20we%20apply%20a%20deep%20learning-based%20super-resolution%20algorithm.%20Then%2C%20we%0Agenerate%20an%20initial%20unbiased%20reference%20with%20an%20iterative%20metric-based%0Aregistration%20using%20a%20small%20portion%20of%20subject%20scans.%20We%20register%20the%20remaining%0Ascans%20to%20this%20template%20and%20refine%20the%20template%20using%20an%20unsupervised%20deep%0Aprobabilistic%20approach%20that%20generates%20a%20more%20expansive%20deformation%20field%20to%0Aenhance%20the%20organ%20boundary%20alignment.%20We%20demonstrate%20this%20framework%20using%0Amagnetic%20resonance%20images%20across%20four%20different%20tissue%20contrasts%2C%20generating%0Afour%20atlases%20in%20separate%20spatial%20alignments.%0A%20%20Results%3A%20For%20each%20tissue%20contrast%2C%20we%20find%20a%20significant%20improvement%20using%0Athe%20Wilcoxon%20signed-rank%20test%20in%20the%20average%20Dice%20score%20across%20four%20labeled%0Aregions%20compared%20to%20a%20standard%20registration%20framework%20consisting%20of%20rigid%2C%0Aaffine%2C%20and%20deformable%20transformations.%20These%20results%20highlight%20the%20effective%0Aalignment%20of%20eye%20organs%20and%20boundaries%20using%20our%20proposed%20process.%0A%20%20Conclusions%3A%20By%20combining%20super-resolution%20preprocessing%20and%20deep%0Aprobabilistic%20models%2C%20we%20address%20the%20challenge%20of%20generating%20an%20eye%20atlas%20to%0Aserve%20as%20a%20standardized%20reference%20across%20a%20largely%20variable%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03060v3&entry.124074799=Read"},
{"title": "Script-centric behavior understanding for assisted autism spectrum\n  disorder diagnosis", "author": "Wenxing Liu and Yueran Pan and Ming Li", "abstract": "  Observing and analyzing children's social behaviors is crucial for the early\ndiagnosis of Autism Spectrum Disorders (ASD). This work focuses on\nautomatically detecting ASD using computer vision techniques and large language\nmodels (LLMs). Existing methods typically rely on supervised learning. However,\nthe scarcity of ASD diagnostic datasets and the lack of interpretability in\ndiagnostic results significantly limits its clinical application. To address\nthese challenges, we introduce a novel unsupervised approach based on\nscript-centric behavior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a zero-shot or\nfew-shot manner. Specifically, we propose a scripts transcription module for\nmultimodal behavior data textualization and a domain prompts module to bridge\nLLMs. Our method achieves an accuracy of 92.00\\% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance of supervised\nlearning methods by 3.58\\% absolutely. Extensive experiments confirm the\neffectiveness of our approach and suggest its potential for advancing ASD\nresearch through LLMs.\n", "link": "http://arxiv.org/abs/2411.09413v1", "date": "2024-11-14", "relevancy": 2.1186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Script-centric%20behavior%20understanding%20for%20assisted%20autism%20spectrum%0A%20%20disorder%20diagnosis&body=Title%3A%20Script-centric%20behavior%20understanding%20for%20assisted%20autism%20spectrum%0A%20%20disorder%20diagnosis%0AAuthor%3A%20Wenxing%20Liu%20and%20Yueran%20Pan%20and%20Ming%20Li%0AAbstract%3A%20%20%20Observing%20and%20analyzing%20children%27s%20social%20behaviors%20is%20crucial%20for%20the%20early%0Adiagnosis%20of%20Autism%20Spectrum%20Disorders%20%28ASD%29.%20This%20work%20focuses%20on%0Aautomatically%20detecting%20ASD%20using%20computer%20vision%20techniques%20and%20large%20language%0Amodels%20%28LLMs%29.%20Existing%20methods%20typically%20rely%20on%20supervised%20learning.%20However%2C%0Athe%20scarcity%20of%20ASD%20diagnostic%20datasets%20and%20the%20lack%20of%20interpretability%20in%0Adiagnostic%20results%20significantly%20limits%20its%20clinical%20application.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20novel%20unsupervised%20approach%20based%20on%0Ascript-centric%20behavior%20understanding.%20Our%20pipeline%20converts%20video%20content%20into%0Ascripts%20that%20describe%20the%20behavior%20of%20characters%2C%20leveraging%20the%0Ageneralizability%20of%20large%20language%20models%20to%20detect%20ASD%20in%20a%20zero-shot%20or%0Afew-shot%20manner.%20Specifically%2C%20we%20propose%20a%20scripts%20transcription%20module%20for%0Amultimodal%20behavior%20data%20textualization%20and%20a%20domain%20prompts%20module%20to%20bridge%0ALLMs.%20Our%20method%20achieves%20an%20accuracy%20of%2092.00%5C%25%20in%20diagnosing%20ASD%20in%20children%0Awith%20an%20average%20age%20of%2024%20months%2C%20surpassing%20the%20performance%20of%20supervised%0Alearning%20methods%20by%203.58%5C%25%20absolutely.%20Extensive%20experiments%20confirm%20the%0Aeffectiveness%20of%20our%20approach%20and%20suggest%20its%20potential%20for%20advancing%20ASD%0Aresearch%20through%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScript-centric%2520behavior%2520understanding%2520for%2520assisted%2520autism%2520spectrum%250A%2520%2520disorder%2520diagnosis%26entry.906535625%3DWenxing%2520Liu%2520and%2520Yueran%2520Pan%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520Observing%2520and%2520analyzing%2520children%2527s%2520social%2520behaviors%2520is%2520crucial%2520for%2520the%2520early%250Adiagnosis%2520of%2520Autism%2520Spectrum%2520Disorders%2520%2528ASD%2529.%2520This%2520work%2520focuses%2520on%250Aautomatically%2520detecting%2520ASD%2520using%2520computer%2520vision%2520techniques%2520and%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520Existing%2520methods%2520typically%2520rely%2520on%2520supervised%2520learning.%2520However%252C%250Athe%2520scarcity%2520of%2520ASD%2520diagnostic%2520datasets%2520and%2520the%2520lack%2520of%2520interpretability%2520in%250Adiagnostic%2520results%2520significantly%2520limits%2520its%2520clinical%2520application.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520unsupervised%2520approach%2520based%2520on%250Ascript-centric%2520behavior%2520understanding.%2520Our%2520pipeline%2520converts%2520video%2520content%2520into%250Ascripts%2520that%2520describe%2520the%2520behavior%2520of%2520characters%252C%2520leveraging%2520the%250Ageneralizability%2520of%2520large%2520language%2520models%2520to%2520detect%2520ASD%2520in%2520a%2520zero-shot%2520or%250Afew-shot%2520manner.%2520Specifically%252C%2520we%2520propose%2520a%2520scripts%2520transcription%2520module%2520for%250Amultimodal%2520behavior%2520data%2520textualization%2520and%2520a%2520domain%2520prompts%2520module%2520to%2520bridge%250ALLMs.%2520Our%2520method%2520achieves%2520an%2520accuracy%2520of%252092.00%255C%2525%2520in%2520diagnosing%2520ASD%2520in%2520children%250Awith%2520an%2520average%2520age%2520of%252024%2520months%252C%2520surpassing%2520the%2520performance%2520of%2520supervised%250Alearning%2520methods%2520by%25203.58%255C%2525%2520absolutely.%2520Extensive%2520experiments%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520and%2520suggest%2520its%2520potential%2520for%2520advancing%2520ASD%250Aresearch%2520through%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Script-centric%20behavior%20understanding%20for%20assisted%20autism%20spectrum%0A%20%20disorder%20diagnosis&entry.906535625=Wenxing%20Liu%20and%20Yueran%20Pan%20and%20Ming%20Li&entry.1292438233=%20%20Observing%20and%20analyzing%20children%27s%20social%20behaviors%20is%20crucial%20for%20the%20early%0Adiagnosis%20of%20Autism%20Spectrum%20Disorders%20%28ASD%29.%20This%20work%20focuses%20on%0Aautomatically%20detecting%20ASD%20using%20computer%20vision%20techniques%20and%20large%20language%0Amodels%20%28LLMs%29.%20Existing%20methods%20typically%20rely%20on%20supervised%20learning.%20However%2C%0Athe%20scarcity%20of%20ASD%20diagnostic%20datasets%20and%20the%20lack%20of%20interpretability%20in%0Adiagnostic%20results%20significantly%20limits%20its%20clinical%20application.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20novel%20unsupervised%20approach%20based%20on%0Ascript-centric%20behavior%20understanding.%20Our%20pipeline%20converts%20video%20content%20into%0Ascripts%20that%20describe%20the%20behavior%20of%20characters%2C%20leveraging%20the%0Ageneralizability%20of%20large%20language%20models%20to%20detect%20ASD%20in%20a%20zero-shot%20or%0Afew-shot%20manner.%20Specifically%2C%20we%20propose%20a%20scripts%20transcription%20module%20for%0Amultimodal%20behavior%20data%20textualization%20and%20a%20domain%20prompts%20module%20to%20bridge%0ALLMs.%20Our%20method%20achieves%20an%20accuracy%20of%2092.00%5C%25%20in%20diagnosing%20ASD%20in%20children%0Awith%20an%20average%20age%20of%2024%20months%2C%20surpassing%20the%20performance%20of%20supervised%0Alearning%20methods%20by%203.58%5C%25%20absolutely.%20Extensive%20experiments%20confirm%20the%0Aeffectiveness%20of%20our%20approach%20and%20suggest%20its%20potential%20for%20advancing%20ASD%0Aresearch%20through%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09413v1&entry.124074799=Read"},
{"title": "ResidualDroppath: Enhancing Feature Reuse over Residual Connections", "author": "Sejik Park", "abstract": "  Residual connections are one of the most important components in neural\nnetwork architectures for mitigating the vanishing gradient problem and\nfacilitating the training of much deeper networks. One possible explanation for\nhow residual connections aid deeper network training is by promoting feature\nreuse. However, we identify and analyze the limitations of feature reuse with\nvanilla residual connections. To address these limitations, we propose\nmodifications in training methods. Specifically, we provide an additional\nopportunity for the model to learn feature reuse with residual connections\nthrough two types of iterations during training. The first type of iteration\ninvolves using droppath, which enforces feature reuse by randomly dropping a\nsubset of layers. The second type of iteration focuses on training the dropped\nparts of the model while freezing the undropped parts. As a result, the dropped\nparts learn in a way that encourages feature reuse, as the model relies on the\nundropped parts with feature reuse in mind. Overall, we demonstrated\nperformance improvements in models with residual connections for image\nclassification in certain cases.\n", "link": "http://arxiv.org/abs/2411.09475v1", "date": "2024-11-14", "relevancy": 2.1184, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5477}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5197}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResidualDroppath%3A%20Enhancing%20Feature%20Reuse%20over%20Residual%20Connections&body=Title%3A%20ResidualDroppath%3A%20Enhancing%20Feature%20Reuse%20over%20Residual%20Connections%0AAuthor%3A%20Sejik%20Park%0AAbstract%3A%20%20%20Residual%20connections%20are%20one%20of%20the%20most%20important%20components%20in%20neural%0Anetwork%20architectures%20for%20mitigating%20the%20vanishing%20gradient%20problem%20and%0Afacilitating%20the%20training%20of%20much%20deeper%20networks.%20One%20possible%20explanation%20for%0Ahow%20residual%20connections%20aid%20deeper%20network%20training%20is%20by%20promoting%20feature%0Areuse.%20However%2C%20we%20identify%20and%20analyze%20the%20limitations%20of%20feature%20reuse%20with%0Avanilla%20residual%20connections.%20To%20address%20these%20limitations%2C%20we%20propose%0Amodifications%20in%20training%20methods.%20Specifically%2C%20we%20provide%20an%20additional%0Aopportunity%20for%20the%20model%20to%20learn%20feature%20reuse%20with%20residual%20connections%0Athrough%20two%20types%20of%20iterations%20during%20training.%20The%20first%20type%20of%20iteration%0Ainvolves%20using%20droppath%2C%20which%20enforces%20feature%20reuse%20by%20randomly%20dropping%20a%0Asubset%20of%20layers.%20The%20second%20type%20of%20iteration%20focuses%20on%20training%20the%20dropped%0Aparts%20of%20the%20model%20while%20freezing%20the%20undropped%20parts.%20As%20a%20result%2C%20the%20dropped%0Aparts%20learn%20in%20a%20way%20that%20encourages%20feature%20reuse%2C%20as%20the%20model%20relies%20on%20the%0Aundropped%20parts%20with%20feature%20reuse%20in%20mind.%20Overall%2C%20we%20demonstrated%0Aperformance%20improvements%20in%20models%20with%20residual%20connections%20for%20image%0Aclassification%20in%20certain%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidualDroppath%253A%2520Enhancing%2520Feature%2520Reuse%2520over%2520Residual%2520Connections%26entry.906535625%3DSejik%2520Park%26entry.1292438233%3D%2520%2520Residual%2520connections%2520are%2520one%2520of%2520the%2520most%2520important%2520components%2520in%2520neural%250Anetwork%2520architectures%2520for%2520mitigating%2520the%2520vanishing%2520gradient%2520problem%2520and%250Afacilitating%2520the%2520training%2520of%2520much%2520deeper%2520networks.%2520One%2520possible%2520explanation%2520for%250Ahow%2520residual%2520connections%2520aid%2520deeper%2520network%2520training%2520is%2520by%2520promoting%2520feature%250Areuse.%2520However%252C%2520we%2520identify%2520and%2520analyze%2520the%2520limitations%2520of%2520feature%2520reuse%2520with%250Avanilla%2520residual%2520connections.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250Amodifications%2520in%2520training%2520methods.%2520Specifically%252C%2520we%2520provide%2520an%2520additional%250Aopportunity%2520for%2520the%2520model%2520to%2520learn%2520feature%2520reuse%2520with%2520residual%2520connections%250Athrough%2520two%2520types%2520of%2520iterations%2520during%2520training.%2520The%2520first%2520type%2520of%2520iteration%250Ainvolves%2520using%2520droppath%252C%2520which%2520enforces%2520feature%2520reuse%2520by%2520randomly%2520dropping%2520a%250Asubset%2520of%2520layers.%2520The%2520second%2520type%2520of%2520iteration%2520focuses%2520on%2520training%2520the%2520dropped%250Aparts%2520of%2520the%2520model%2520while%2520freezing%2520the%2520undropped%2520parts.%2520As%2520a%2520result%252C%2520the%2520dropped%250Aparts%2520learn%2520in%2520a%2520way%2520that%2520encourages%2520feature%2520reuse%252C%2520as%2520the%2520model%2520relies%2520on%2520the%250Aundropped%2520parts%2520with%2520feature%2520reuse%2520in%2520mind.%2520Overall%252C%2520we%2520demonstrated%250Aperformance%2520improvements%2520in%2520models%2520with%2520residual%2520connections%2520for%2520image%250Aclassification%2520in%2520certain%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResidualDroppath%3A%20Enhancing%20Feature%20Reuse%20over%20Residual%20Connections&entry.906535625=Sejik%20Park&entry.1292438233=%20%20Residual%20connections%20are%20one%20of%20the%20most%20important%20components%20in%20neural%0Anetwork%20architectures%20for%20mitigating%20the%20vanishing%20gradient%20problem%20and%0Afacilitating%20the%20training%20of%20much%20deeper%20networks.%20One%20possible%20explanation%20for%0Ahow%20residual%20connections%20aid%20deeper%20network%20training%20is%20by%20promoting%20feature%0Areuse.%20However%2C%20we%20identify%20and%20analyze%20the%20limitations%20of%20feature%20reuse%20with%0Avanilla%20residual%20connections.%20To%20address%20these%20limitations%2C%20we%20propose%0Amodifications%20in%20training%20methods.%20Specifically%2C%20we%20provide%20an%20additional%0Aopportunity%20for%20the%20model%20to%20learn%20feature%20reuse%20with%20residual%20connections%0Athrough%20two%20types%20of%20iterations%20during%20training.%20The%20first%20type%20of%20iteration%0Ainvolves%20using%20droppath%2C%20which%20enforces%20feature%20reuse%20by%20randomly%20dropping%20a%0Asubset%20of%20layers.%20The%20second%20type%20of%20iteration%20focuses%20on%20training%20the%20dropped%0Aparts%20of%20the%20model%20while%20freezing%20the%20undropped%20parts.%20As%20a%20result%2C%20the%20dropped%0Aparts%20learn%20in%20a%20way%20that%20encourages%20feature%20reuse%2C%20as%20the%20model%20relies%20on%20the%0Aundropped%20parts%20with%20feature%20reuse%20in%20mind.%20Overall%2C%20we%20demonstrated%0Aperformance%20improvements%20in%20models%20with%20residual%20connections%20for%20image%0Aclassification%20in%20certain%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09475v1&entry.124074799=Read"},
{"title": "Benchmarking SLAM Algorithms in the Cloud: The SLAM Hive Benchmarking\n  Suite", "author": "Xinzhe Liu and Yuanyuan Yang and Bowen Xu and Delin Feng and S\u00f6ren Schwertfeger", "abstract": "  Evaluating the performance of Simultaneous Localization and Mapping (SLAM)\nalgorithms is essential for scientists and users of robotic systems alike. But\nthere are a multitude of different permutations of possible options of hardware\nsetups and algorithm configurations, as well as different datasets and\nalgorithms, such that it was previously infeasible to thoroughly compare SLAM\nsystems against the full state of the art. To solve that we present the SLAM\nHive Benchmarking Suite, which is able to analyze SLAM algorithms in 1000's of\nmapping runs, through its utilization of container technology and deployment in\nthe cloud. This paper presents the architecture and open source implementation\nof SLAM Hive and compares it to existing efforts on SLAM evaluation. We perform\nmapping runs with popular visual, RGBD and LiDAR based SLAM algorithms against\ncommonly used datasets and show how SLAM Hive can be used to conveniently\nanalyze the results against various aspects. Through this we envision that SLAM\nHive can become an essential tool for proper comparisons and evaluations of\nSLAM algorithms and thus drive the scientific development in the research on\nSLAM. The open source software as well as a demo to show the live analysis of\n1000's of mapping runs can be found on our SLAM Hive website.\n", "link": "http://arxiv.org/abs/2406.17586v2", "date": "2024-11-14", "relevancy": 2.1143, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20Benchmarking%0A%20%20Suite&body=Title%3A%20Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20Benchmarking%0A%20%20Suite%0AAuthor%3A%20Xinzhe%20Liu%20and%20Yuanyuan%20Yang%20and%20Bowen%20Xu%20and%20Delin%20Feng%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20Evaluating%20the%20performance%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Aalgorithms%20is%20essential%20for%20scientists%20and%20users%20of%20robotic%20systems%20alike.%20But%0Athere%20are%20a%20multitude%20of%20different%20permutations%20of%20possible%20options%20of%20hardware%0Asetups%20and%20algorithm%20configurations%2C%20as%20well%20as%20different%20datasets%20and%0Aalgorithms%2C%20such%20that%20it%20was%20previously%20infeasible%20to%20thoroughly%20compare%20SLAM%0Asystems%20against%20the%20full%20state%20of%20the%20art.%20To%20solve%20that%20we%20present%20the%20SLAM%0AHive%20Benchmarking%20Suite%2C%20which%20is%20able%20to%20analyze%20SLAM%20algorithms%20in%201000%27s%20of%0Amapping%20runs%2C%20through%20its%20utilization%20of%20container%20technology%20and%20deployment%20in%0Athe%20cloud.%20This%20paper%20presents%20the%20architecture%20and%20open%20source%20implementation%0Aof%20SLAM%20Hive%20and%20compares%20it%20to%20existing%20efforts%20on%20SLAM%20evaluation.%20We%20perform%0Amapping%20runs%20with%20popular%20visual%2C%20RGBD%20and%20LiDAR%20based%20SLAM%20algorithms%20against%0Acommonly%20used%20datasets%20and%20show%20how%20SLAM%20Hive%20can%20be%20used%20to%20conveniently%0Aanalyze%20the%20results%20against%20various%20aspects.%20Through%20this%20we%20envision%20that%20SLAM%0AHive%20can%20become%20an%20essential%20tool%20for%20proper%20comparisons%20and%20evaluations%20of%0ASLAM%20algorithms%20and%20thus%20drive%20the%20scientific%20development%20in%20the%20research%20on%0ASLAM.%20The%20open%20source%20software%20as%20well%20as%20a%20demo%20to%20show%20the%20live%20analysis%20of%0A1000%27s%20of%20mapping%20runs%20can%20be%20found%20on%20our%20SLAM%20Hive%20website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520SLAM%2520Algorithms%2520in%2520the%2520Cloud%253A%2520The%2520SLAM%2520Hive%2520Benchmarking%250A%2520%2520Suite%26entry.906535625%3DXinzhe%2520Liu%2520and%2520Yuanyuan%2520Yang%2520and%2520Bowen%2520Xu%2520and%2520Delin%2520Feng%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520performance%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%250Aalgorithms%2520is%2520essential%2520for%2520scientists%2520and%2520users%2520of%2520robotic%2520systems%2520alike.%2520But%250Athere%2520are%2520a%2520multitude%2520of%2520different%2520permutations%2520of%2520possible%2520options%2520of%2520hardware%250Asetups%2520and%2520algorithm%2520configurations%252C%2520as%2520well%2520as%2520different%2520datasets%2520and%250Aalgorithms%252C%2520such%2520that%2520it%2520was%2520previously%2520infeasible%2520to%2520thoroughly%2520compare%2520SLAM%250Asystems%2520against%2520the%2520full%2520state%2520of%2520the%2520art.%2520To%2520solve%2520that%2520we%2520present%2520the%2520SLAM%250AHive%2520Benchmarking%2520Suite%252C%2520which%2520is%2520able%2520to%2520analyze%2520SLAM%2520algorithms%2520in%25201000%2527s%2520of%250Amapping%2520runs%252C%2520through%2520its%2520utilization%2520of%2520container%2520technology%2520and%2520deployment%2520in%250Athe%2520cloud.%2520This%2520paper%2520presents%2520the%2520architecture%2520and%2520open%2520source%2520implementation%250Aof%2520SLAM%2520Hive%2520and%2520compares%2520it%2520to%2520existing%2520efforts%2520on%2520SLAM%2520evaluation.%2520We%2520perform%250Amapping%2520runs%2520with%2520popular%2520visual%252C%2520RGBD%2520and%2520LiDAR%2520based%2520SLAM%2520algorithms%2520against%250Acommonly%2520used%2520datasets%2520and%2520show%2520how%2520SLAM%2520Hive%2520can%2520be%2520used%2520to%2520conveniently%250Aanalyze%2520the%2520results%2520against%2520various%2520aspects.%2520Through%2520this%2520we%2520envision%2520that%2520SLAM%250AHive%2520can%2520become%2520an%2520essential%2520tool%2520for%2520proper%2520comparisons%2520and%2520evaluations%2520of%250ASLAM%2520algorithms%2520and%2520thus%2520drive%2520the%2520scientific%2520development%2520in%2520the%2520research%2520on%250ASLAM.%2520The%2520open%2520source%2520software%2520as%2520well%2520as%2520a%2520demo%2520to%2520show%2520the%2520live%2520analysis%2520of%250A1000%2527s%2520of%2520mapping%2520runs%2520can%2520be%2520found%2520on%2520our%2520SLAM%2520Hive%2520website.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20Benchmarking%0A%20%20Suite&entry.906535625=Xinzhe%20Liu%20and%20Yuanyuan%20Yang%20and%20Bowen%20Xu%20and%20Delin%20Feng%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20Evaluating%20the%20performance%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Aalgorithms%20is%20essential%20for%20scientists%20and%20users%20of%20robotic%20systems%20alike.%20But%0Athere%20are%20a%20multitude%20of%20different%20permutations%20of%20possible%20options%20of%20hardware%0Asetups%20and%20algorithm%20configurations%2C%20as%20well%20as%20different%20datasets%20and%0Aalgorithms%2C%20such%20that%20it%20was%20previously%20infeasible%20to%20thoroughly%20compare%20SLAM%0Asystems%20against%20the%20full%20state%20of%20the%20art.%20To%20solve%20that%20we%20present%20the%20SLAM%0AHive%20Benchmarking%20Suite%2C%20which%20is%20able%20to%20analyze%20SLAM%20algorithms%20in%201000%27s%20of%0Amapping%20runs%2C%20through%20its%20utilization%20of%20container%20technology%20and%20deployment%20in%0Athe%20cloud.%20This%20paper%20presents%20the%20architecture%20and%20open%20source%20implementation%0Aof%20SLAM%20Hive%20and%20compares%20it%20to%20existing%20efforts%20on%20SLAM%20evaluation.%20We%20perform%0Amapping%20runs%20with%20popular%20visual%2C%20RGBD%20and%20LiDAR%20based%20SLAM%20algorithms%20against%0Acommonly%20used%20datasets%20and%20show%20how%20SLAM%20Hive%20can%20be%20used%20to%20conveniently%0Aanalyze%20the%20results%20against%20various%20aspects.%20Through%20this%20we%20envision%20that%20SLAM%0AHive%20can%20become%20an%20essential%20tool%20for%20proper%20comparisons%20and%20evaluations%20of%0ASLAM%20algorithms%20and%20thus%20drive%20the%20scientific%20development%20in%20the%20research%20on%0ASLAM.%20The%20open%20source%20software%20as%20well%20as%20a%20demo%20to%20show%20the%20live%20analysis%20of%0A1000%27s%20of%20mapping%20runs%20can%20be%20found%20on%20our%20SLAM%20Hive%20website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17586v2&entry.124074799=Read"},
{"title": "VPBSD:Vessel-Pattern-Based Semi-Supervised Distillation for Efficient 3D\n  Microscopic Cerebrovascular Segmentation", "author": "Xi Lin and Shixuan Zhao and Xinxu Wei and Amir Shmuel and Yongjie Li", "abstract": "  3D microscopic cerebrovascular images are characterized by their high\nresolution, presenting significant annotation challenges, large data volumes,\nand intricate variations in detail. Together, these factors make achieving\nhigh-quality, efficient whole-brain segmentation particularly demanding. In\nthis paper, we propose a novel Vessel-Pattern-Based Semi-Supervised\nDistillation pipeline (VpbSD) to address the challenges of 3D microscopic\ncerebrovascular segmentation. This pipeline initially constructs a\nvessel-pattern codebook that captures diverse vascular structures from\nunlabeled data during the teacher model's pretraining phase. In the knowledge\ndistillation stage, the codebook facilitates the transfer of rich knowledge\nfrom a heterogeneous teacher model to a student model, while the\nsemi-supervised approach further enhances the student model's exposure to\ndiverse learning samples. Experimental results on real-world data, including\ncomparisons with state-of-the-art methods and ablation studies, demonstrate\nthat our pipeline and its individual components effectively address the\nchallenges inherent in microscopic cerebrovascular segmentation.\n", "link": "http://arxiv.org/abs/2411.09567v1", "date": "2024-11-14", "relevancy": 2.1128, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5227}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VPBSD%3AVessel-Pattern-Based%20Semi-Supervised%20Distillation%20for%20Efficient%203D%0A%20%20Microscopic%20Cerebrovascular%20Segmentation&body=Title%3A%20VPBSD%3AVessel-Pattern-Based%20Semi-Supervised%20Distillation%20for%20Efficient%203D%0A%20%20Microscopic%20Cerebrovascular%20Segmentation%0AAuthor%3A%20Xi%20Lin%20and%20Shixuan%20Zhao%20and%20Xinxu%20Wei%20and%20Amir%20Shmuel%20and%20Yongjie%20Li%0AAbstract%3A%20%20%203D%20microscopic%20cerebrovascular%20images%20are%20characterized%20by%20their%20high%0Aresolution%2C%20presenting%20significant%20annotation%20challenges%2C%20large%20data%20volumes%2C%0Aand%20intricate%20variations%20in%20detail.%20Together%2C%20these%20factors%20make%20achieving%0Ahigh-quality%2C%20efficient%20whole-brain%20segmentation%20particularly%20demanding.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Vessel-Pattern-Based%20Semi-Supervised%0ADistillation%20pipeline%20%28VpbSD%29%20to%20address%20the%20challenges%20of%203D%20microscopic%0Acerebrovascular%20segmentation.%20This%20pipeline%20initially%20constructs%20a%0Avessel-pattern%20codebook%20that%20captures%20diverse%20vascular%20structures%20from%0Aunlabeled%20data%20during%20the%20teacher%20model%27s%20pretraining%20phase.%20In%20the%20knowledge%0Adistillation%20stage%2C%20the%20codebook%20facilitates%20the%20transfer%20of%20rich%20knowledge%0Afrom%20a%20heterogeneous%20teacher%20model%20to%20a%20student%20model%2C%20while%20the%0Asemi-supervised%20approach%20further%20enhances%20the%20student%20model%27s%20exposure%20to%0Adiverse%20learning%20samples.%20Experimental%20results%20on%20real-world%20data%2C%20including%0Acomparisons%20with%20state-of-the-art%20methods%20and%20ablation%20studies%2C%20demonstrate%0Athat%20our%20pipeline%20and%20its%20individual%20components%20effectively%20address%20the%0Achallenges%20inherent%20in%20microscopic%20cerebrovascular%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVPBSD%253AVessel-Pattern-Based%2520Semi-Supervised%2520Distillation%2520for%2520Efficient%25203D%250A%2520%2520Microscopic%2520Cerebrovascular%2520Segmentation%26entry.906535625%3DXi%2520Lin%2520and%2520Shixuan%2520Zhao%2520and%2520Xinxu%2520Wei%2520and%2520Amir%2520Shmuel%2520and%2520Yongjie%2520Li%26entry.1292438233%3D%2520%25203D%2520microscopic%2520cerebrovascular%2520images%2520are%2520characterized%2520by%2520their%2520high%250Aresolution%252C%2520presenting%2520significant%2520annotation%2520challenges%252C%2520large%2520data%2520volumes%252C%250Aand%2520intricate%2520variations%2520in%2520detail.%2520Together%252C%2520these%2520factors%2520make%2520achieving%250Ahigh-quality%252C%2520efficient%2520whole-brain%2520segmentation%2520particularly%2520demanding.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Vessel-Pattern-Based%2520Semi-Supervised%250ADistillation%2520pipeline%2520%2528VpbSD%2529%2520to%2520address%2520the%2520challenges%2520of%25203D%2520microscopic%250Acerebrovascular%2520segmentation.%2520This%2520pipeline%2520initially%2520constructs%2520a%250Avessel-pattern%2520codebook%2520that%2520captures%2520diverse%2520vascular%2520structures%2520from%250Aunlabeled%2520data%2520during%2520the%2520teacher%2520model%2527s%2520pretraining%2520phase.%2520In%2520the%2520knowledge%250Adistillation%2520stage%252C%2520the%2520codebook%2520facilitates%2520the%2520transfer%2520of%2520rich%2520knowledge%250Afrom%2520a%2520heterogeneous%2520teacher%2520model%2520to%2520a%2520student%2520model%252C%2520while%2520the%250Asemi-supervised%2520approach%2520further%2520enhances%2520the%2520student%2520model%2527s%2520exposure%2520to%250Adiverse%2520learning%2520samples.%2520Experimental%2520results%2520on%2520real-world%2520data%252C%2520including%250Acomparisons%2520with%2520state-of-the-art%2520methods%2520and%2520ablation%2520studies%252C%2520demonstrate%250Athat%2520our%2520pipeline%2520and%2520its%2520individual%2520components%2520effectively%2520address%2520the%250Achallenges%2520inherent%2520in%2520microscopic%2520cerebrovascular%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VPBSD%3AVessel-Pattern-Based%20Semi-Supervised%20Distillation%20for%20Efficient%203D%0A%20%20Microscopic%20Cerebrovascular%20Segmentation&entry.906535625=Xi%20Lin%20and%20Shixuan%20Zhao%20and%20Xinxu%20Wei%20and%20Amir%20Shmuel%20and%20Yongjie%20Li&entry.1292438233=%20%203D%20microscopic%20cerebrovascular%20images%20are%20characterized%20by%20their%20high%0Aresolution%2C%20presenting%20significant%20annotation%20challenges%2C%20large%20data%20volumes%2C%0Aand%20intricate%20variations%20in%20detail.%20Together%2C%20these%20factors%20make%20achieving%0Ahigh-quality%2C%20efficient%20whole-brain%20segmentation%20particularly%20demanding.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Vessel-Pattern-Based%20Semi-Supervised%0ADistillation%20pipeline%20%28VpbSD%29%20to%20address%20the%20challenges%20of%203D%20microscopic%0Acerebrovascular%20segmentation.%20This%20pipeline%20initially%20constructs%20a%0Avessel-pattern%20codebook%20that%20captures%20diverse%20vascular%20structures%20from%0Aunlabeled%20data%20during%20the%20teacher%20model%27s%20pretraining%20phase.%20In%20the%20knowledge%0Adistillation%20stage%2C%20the%20codebook%20facilitates%20the%20transfer%20of%20rich%20knowledge%0Afrom%20a%20heterogeneous%20teacher%20model%20to%20a%20student%20model%2C%20while%20the%0Asemi-supervised%20approach%20further%20enhances%20the%20student%20model%27s%20exposure%20to%0Adiverse%20learning%20samples.%20Experimental%20results%20on%20real-world%20data%2C%20including%0Acomparisons%20with%20state-of-the-art%20methods%20and%20ablation%20studies%2C%20demonstrate%0Athat%20our%20pipeline%20and%20its%20individual%20components%20effectively%20address%20the%0Achallenges%20inherent%20in%20microscopic%20cerebrovascular%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09567v1&entry.124074799=Read"},
{"title": "Knowledge Bases in Support of Large Language Models for Processing Web\n  News", "author": "Yihe Zhang and Nabin Pakka and Nian-Feng Tzeng", "abstract": "  Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results.\n", "link": "http://arxiv.org/abs/2411.08278v2", "date": "2024-11-14", "relevancy": 2.1018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Bases%20in%20Support%20of%20Large%20Language%20Models%20for%20Processing%20Web%0A%20%20News&body=Title%3A%20Knowledge%20Bases%20in%20Support%20of%20Large%20Language%20Models%20for%20Processing%20Web%0A%20%20News%0AAuthor%3A%20Yihe%20Zhang%20and%20Nabin%20Pakka%20and%20Nian-Feng%20Tzeng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20received%20considerable%20interest%20in%20wide%0Aapplications%20lately.%20During%20pre-training%20via%20massive%20datasets%2C%20such%20a%20model%0Aimplicitly%20memorizes%20the%20factual%20knowledge%20of%20trained%20datasets%20in%20its%20hidden%0Aparameters.%20However%2C%20knowledge%20held%20implicitly%20in%20parameters%20often%20makes%20its%0Ause%20by%20downstream%20applications%20ineffective%20due%20to%20the%20lack%20of%20common-sense%0Areasoning.%20In%20this%20article%2C%20we%20introduce%20a%20general%20framework%20that%20permits%20to%0Abuild%20knowledge%20bases%20with%20an%20aid%20of%20LLMs%2C%20tailored%20for%20processing%20Web%20news.%0AThe%20framework%20applies%20a%20rule-based%20News%20Information%20Extractor%20%28NewsIE%29%20to%20news%0Aitems%20for%20extracting%20their%20relational%20tuples%2C%20referred%20to%20as%20knowledge%20bases%2C%0Awhich%20are%20then%20graph-convoluted%20with%20the%20implicit%20knowledge%20facts%20of%20news%20items%0Aobtained%20by%20LLMs%2C%20for%20their%20classification.%20It%20involves%20two%20lightweight%0Acomponents%3A%201%29%20NewsIE%3A%20for%20extracting%20the%20structural%20information%20of%20every%20news%0Aitem%2C%20in%20the%20form%20of%20relational%20tuples%3B%202%29%20BERTGraph%3A%20for%20graph%20convoluting%20the%0Aimplicit%20knowledge%20facts%20with%20relational%20tuples%20extracted%20by%20NewsIE.%20We%20have%0Aevaluated%20our%20framework%20under%20different%20news-related%20datasets%20for%20news%20category%0Aclassification%2C%20with%20promising%20experimental%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Bases%2520in%2520Support%2520of%2520Large%2520Language%2520Models%2520for%2520Processing%2520Web%250A%2520%2520News%26entry.906535625%3DYihe%2520Zhang%2520and%2520Nabin%2520Pakka%2520and%2520Nian-Feng%2520Tzeng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520received%2520considerable%2520interest%2520in%2520wide%250Aapplications%2520lately.%2520During%2520pre-training%2520via%2520massive%2520datasets%252C%2520such%2520a%2520model%250Aimplicitly%2520memorizes%2520the%2520factual%2520knowledge%2520of%2520trained%2520datasets%2520in%2520its%2520hidden%250Aparameters.%2520However%252C%2520knowledge%2520held%2520implicitly%2520in%2520parameters%2520often%2520makes%2520its%250Ause%2520by%2520downstream%2520applications%2520ineffective%2520due%2520to%2520the%2520lack%2520of%2520common-sense%250Areasoning.%2520In%2520this%2520article%252C%2520we%2520introduce%2520a%2520general%2520framework%2520that%2520permits%2520to%250Abuild%2520knowledge%2520bases%2520with%2520an%2520aid%2520of%2520LLMs%252C%2520tailored%2520for%2520processing%2520Web%2520news.%250AThe%2520framework%2520applies%2520a%2520rule-based%2520News%2520Information%2520Extractor%2520%2528NewsIE%2529%2520to%2520news%250Aitems%2520for%2520extracting%2520their%2520relational%2520tuples%252C%2520referred%2520to%2520as%2520knowledge%2520bases%252C%250Awhich%2520are%2520then%2520graph-convoluted%2520with%2520the%2520implicit%2520knowledge%2520facts%2520of%2520news%2520items%250Aobtained%2520by%2520LLMs%252C%2520for%2520their%2520classification.%2520It%2520involves%2520two%2520lightweight%250Acomponents%253A%25201%2529%2520NewsIE%253A%2520for%2520extracting%2520the%2520structural%2520information%2520of%2520every%2520news%250Aitem%252C%2520in%2520the%2520form%2520of%2520relational%2520tuples%253B%25202%2529%2520BERTGraph%253A%2520for%2520graph%2520convoluting%2520the%250Aimplicit%2520knowledge%2520facts%2520with%2520relational%2520tuples%2520extracted%2520by%2520NewsIE.%2520We%2520have%250Aevaluated%2520our%2520framework%2520under%2520different%2520news-related%2520datasets%2520for%2520news%2520category%250Aclassification%252C%2520with%2520promising%2520experimental%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Bases%20in%20Support%20of%20Large%20Language%20Models%20for%20Processing%20Web%0A%20%20News&entry.906535625=Yihe%20Zhang%20and%20Nabin%20Pakka%20and%20Nian-Feng%20Tzeng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20received%20considerable%20interest%20in%20wide%0Aapplications%20lately.%20During%20pre-training%20via%20massive%20datasets%2C%20such%20a%20model%0Aimplicitly%20memorizes%20the%20factual%20knowledge%20of%20trained%20datasets%20in%20its%20hidden%0Aparameters.%20However%2C%20knowledge%20held%20implicitly%20in%20parameters%20often%20makes%20its%0Ause%20by%20downstream%20applications%20ineffective%20due%20to%20the%20lack%20of%20common-sense%0Areasoning.%20In%20this%20article%2C%20we%20introduce%20a%20general%20framework%20that%20permits%20to%0Abuild%20knowledge%20bases%20with%20an%20aid%20of%20LLMs%2C%20tailored%20for%20processing%20Web%20news.%0AThe%20framework%20applies%20a%20rule-based%20News%20Information%20Extractor%20%28NewsIE%29%20to%20news%0Aitems%20for%20extracting%20their%20relational%20tuples%2C%20referred%20to%20as%20knowledge%20bases%2C%0Awhich%20are%20then%20graph-convoluted%20with%20the%20implicit%20knowledge%20facts%20of%20news%20items%0Aobtained%20by%20LLMs%2C%20for%20their%20classification.%20It%20involves%20two%20lightweight%0Acomponents%3A%201%29%20NewsIE%3A%20for%20extracting%20the%20structural%20information%20of%20every%20news%0Aitem%2C%20in%20the%20form%20of%20relational%20tuples%3B%202%29%20BERTGraph%3A%20for%20graph%20convoluting%20the%0Aimplicit%20knowledge%20facts%20with%20relational%20tuples%20extracted%20by%20NewsIE.%20We%20have%0Aevaluated%20our%20framework%20under%20different%20news-related%20datasets%20for%20news%20category%0Aclassification%2C%20with%20promising%20experimental%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08278v2&entry.124074799=Read"},
{"title": "ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion\n  Models", "author": "Wei Pang and Masoumeh Shafieinejad and Lucy Liu and Stephanie Hazlewood and Xi He", "abstract": "  Recent research in tabular data synthesis has focused on single tables,\nwhereas real-world applications often involve complex data with tens or\nhundreds of interconnected tables. Previous approaches to synthesizing\nmulti-relational (multi-table) data fall short in two key aspects: scalability\nfor larger datasets and capturing long-range dependencies, such as correlations\nbetween attributes spread across different tables. Inspired by the success of\ndiffusion models in tabular data modeling, we introduce\n  $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$\n$\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$\n$\\textbf{M}odels$ (ClavaDDPM). This novel approach leverages clustering labels\nas intermediaries to model relationships between tables, specifically focusing\non foreign key constraints. ClavaDDPM leverages the robust generation\ncapabilities of diffusion models while incorporating efficient algorithms to\npropagate the learned latent variables across tables. This enables ClavaDDPM to\ncapture long-range dependencies effectively.\n  Extensive evaluations on multi-table datasets of varying sizes show that\nClavaDDPM significantly outperforms existing methods for these long-range\ndependencies while remaining competitive on utility metrics for single-table\ndata.\n", "link": "http://arxiv.org/abs/2405.17724v2", "date": "2024-11-14", "relevancy": 2.101, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5297}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClavaDDPM%3A%20Multi-relational%20Data%20Synthesis%20with%20Cluster-guided%20Diffusion%0A%20%20Models&body=Title%3A%20ClavaDDPM%3A%20Multi-relational%20Data%20Synthesis%20with%20Cluster-guided%20Diffusion%0A%20%20Models%0AAuthor%3A%20Wei%20Pang%20and%20Masoumeh%20Shafieinejad%20and%20Lucy%20Liu%20and%20Stephanie%20Hazlewood%20and%20Xi%20He%0AAbstract%3A%20%20%20Recent%20research%20in%20tabular%20data%20synthesis%20has%20focused%20on%20single%20tables%2C%0Awhereas%20real-world%20applications%20often%20involve%20complex%20data%20with%20tens%20or%0Ahundreds%20of%20interconnected%20tables.%20Previous%20approaches%20to%20synthesizing%0Amulti-relational%20%28multi-table%29%20data%20fall%20short%20in%20two%20key%20aspects%3A%20scalability%0Afor%20larger%20datasets%20and%20capturing%20long-range%20dependencies%2C%20such%20as%20correlations%0Abetween%20attributes%20spread%20across%20different%20tables.%20Inspired%20by%20the%20success%20of%0Adiffusion%20models%20in%20tabular%20data%20modeling%2C%20we%20introduce%0A%20%20%24%5Ctextbf%7BC%7Dluster%24%20%24%5Ctextbf%7BLa%7Dtent%24%20%24%5Ctextbf%7BVa%7Driable%24%20%24guided%24%0A%24%5Ctextbf%7BD%7Denoising%24%20%24%5Ctextbf%7BD%7Diffusion%24%20%24%5Ctextbf%7BP%7Drobabilistic%24%0A%24%5Ctextbf%7BM%7Dodels%24%20%28ClavaDDPM%29.%20This%20novel%20approach%20leverages%20clustering%20labels%0Aas%20intermediaries%20to%20model%20relationships%20between%20tables%2C%20specifically%20focusing%0Aon%20foreign%20key%20constraints.%20ClavaDDPM%20leverages%20the%20robust%20generation%0Acapabilities%20of%20diffusion%20models%20while%20incorporating%20efficient%20algorithms%20to%0Apropagate%20the%20learned%20latent%20variables%20across%20tables.%20This%20enables%20ClavaDDPM%20to%0Acapture%20long-range%20dependencies%20effectively.%0A%20%20Extensive%20evaluations%20on%20multi-table%20datasets%20of%20varying%20sizes%20show%20that%0AClavaDDPM%20significantly%20outperforms%20existing%20methods%20for%20these%20long-range%0Adependencies%20while%20remaining%20competitive%20on%20utility%20metrics%20for%20single-table%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClavaDDPM%253A%2520Multi-relational%2520Data%2520Synthesis%2520with%2520Cluster-guided%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DWei%2520Pang%2520and%2520Masoumeh%2520Shafieinejad%2520and%2520Lucy%2520Liu%2520and%2520Stephanie%2520Hazlewood%2520and%2520Xi%2520He%26entry.1292438233%3D%2520%2520Recent%2520research%2520in%2520tabular%2520data%2520synthesis%2520has%2520focused%2520on%2520single%2520tables%252C%250Awhereas%2520real-world%2520applications%2520often%2520involve%2520complex%2520data%2520with%2520tens%2520or%250Ahundreds%2520of%2520interconnected%2520tables.%2520Previous%2520approaches%2520to%2520synthesizing%250Amulti-relational%2520%2528multi-table%2529%2520data%2520fall%2520short%2520in%2520two%2520key%2520aspects%253A%2520scalability%250Afor%2520larger%2520datasets%2520and%2520capturing%2520long-range%2520dependencies%252C%2520such%2520as%2520correlations%250Abetween%2520attributes%2520spread%2520across%2520different%2520tables.%2520Inspired%2520by%2520the%2520success%2520of%250Adiffusion%2520models%2520in%2520tabular%2520data%2520modeling%252C%2520we%2520introduce%250A%2520%2520%2524%255Ctextbf%257BC%257Dluster%2524%2520%2524%255Ctextbf%257BLa%257Dtent%2524%2520%2524%255Ctextbf%257BVa%257Driable%2524%2520%2524guided%2524%250A%2524%255Ctextbf%257BD%257Denoising%2524%2520%2524%255Ctextbf%257BD%257Diffusion%2524%2520%2524%255Ctextbf%257BP%257Drobabilistic%2524%250A%2524%255Ctextbf%257BM%257Dodels%2524%2520%2528ClavaDDPM%2529.%2520This%2520novel%2520approach%2520leverages%2520clustering%2520labels%250Aas%2520intermediaries%2520to%2520model%2520relationships%2520between%2520tables%252C%2520specifically%2520focusing%250Aon%2520foreign%2520key%2520constraints.%2520ClavaDDPM%2520leverages%2520the%2520robust%2520generation%250Acapabilities%2520of%2520diffusion%2520models%2520while%2520incorporating%2520efficient%2520algorithms%2520to%250Apropagate%2520the%2520learned%2520latent%2520variables%2520across%2520tables.%2520This%2520enables%2520ClavaDDPM%2520to%250Acapture%2520long-range%2520dependencies%2520effectively.%250A%2520%2520Extensive%2520evaluations%2520on%2520multi-table%2520datasets%2520of%2520varying%2520sizes%2520show%2520that%250AClavaDDPM%2520significantly%2520outperforms%2520existing%2520methods%2520for%2520these%2520long-range%250Adependencies%2520while%2520remaining%2520competitive%2520on%2520utility%2520metrics%2520for%2520single-table%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClavaDDPM%3A%20Multi-relational%20Data%20Synthesis%20with%20Cluster-guided%20Diffusion%0A%20%20Models&entry.906535625=Wei%20Pang%20and%20Masoumeh%20Shafieinejad%20and%20Lucy%20Liu%20and%20Stephanie%20Hazlewood%20and%20Xi%20He&entry.1292438233=%20%20Recent%20research%20in%20tabular%20data%20synthesis%20has%20focused%20on%20single%20tables%2C%0Awhereas%20real-world%20applications%20often%20involve%20complex%20data%20with%20tens%20or%0Ahundreds%20of%20interconnected%20tables.%20Previous%20approaches%20to%20synthesizing%0Amulti-relational%20%28multi-table%29%20data%20fall%20short%20in%20two%20key%20aspects%3A%20scalability%0Afor%20larger%20datasets%20and%20capturing%20long-range%20dependencies%2C%20such%20as%20correlations%0Abetween%20attributes%20spread%20across%20different%20tables.%20Inspired%20by%20the%20success%20of%0Adiffusion%20models%20in%20tabular%20data%20modeling%2C%20we%20introduce%0A%20%20%24%5Ctextbf%7BC%7Dluster%24%20%24%5Ctextbf%7BLa%7Dtent%24%20%24%5Ctextbf%7BVa%7Driable%24%20%24guided%24%0A%24%5Ctextbf%7BD%7Denoising%24%20%24%5Ctextbf%7BD%7Diffusion%24%20%24%5Ctextbf%7BP%7Drobabilistic%24%0A%24%5Ctextbf%7BM%7Dodels%24%20%28ClavaDDPM%29.%20This%20novel%20approach%20leverages%20clustering%20labels%0Aas%20intermediaries%20to%20model%20relationships%20between%20tables%2C%20specifically%20focusing%0Aon%20foreign%20key%20constraints.%20ClavaDDPM%20leverages%20the%20robust%20generation%0Acapabilities%20of%20diffusion%20models%20while%20incorporating%20efficient%20algorithms%20to%0Apropagate%20the%20learned%20latent%20variables%20across%20tables.%20This%20enables%20ClavaDDPM%20to%0Acapture%20long-range%20dependencies%20effectively.%0A%20%20Extensive%20evaluations%20on%20multi-table%20datasets%20of%20varying%20sizes%20show%20that%0AClavaDDPM%20significantly%20outperforms%20existing%20methods%20for%20these%20long-range%0Adependencies%20while%20remaining%20competitive%20on%20utility%20metrics%20for%20single-table%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17724v2&entry.124074799=Read"},
{"title": "Image Processing for Motion Magnification", "author": "Nadaniela Egidi and Josephin Giacomini and Paolo Leonesi and Pierluigi Maponi and Federico Mearelli and Edin Trebovic", "abstract": "  Motion Magnification (MM) is a collection of relative recent techniques\nwithin the realm of Image Processing. The main motivation of introducing these\ntechniques in to support the human visual system to capture relevant\ndisplacements of an object of interest; these motions can be in object color\nand in object location. In fact, the goal is to opportunely process a video\nsequence to obtain as output a new video in which motions are magnified and\nvisible to the viewer. We propose a numerical technique using the Phase-Based\nMotion Magnification which analyses the video sequence in the Fourier Domain\nand rely on the Fourier Shifting Property. We describe the mathematical\nfoundation of this method and the corresponding implementation in a numerical\nalgorithm. We present preliminary experiments, focusing on some basic test made\nup using synthetic images.\n", "link": "http://arxiv.org/abs/2411.09555v1", "date": "2024-11-14", "relevancy": 2.092, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5299}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5239}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Processing%20for%20Motion%20Magnification&body=Title%3A%20Image%20Processing%20for%20Motion%20Magnification%0AAuthor%3A%20Nadaniela%20Egidi%20and%20Josephin%20Giacomini%20and%20Paolo%20Leonesi%20and%20Pierluigi%20Maponi%20and%20Federico%20Mearelli%20and%20Edin%20Trebovic%0AAbstract%3A%20%20%20Motion%20Magnification%20%28MM%29%20is%20a%20collection%20of%20relative%20recent%20techniques%0Awithin%20the%20realm%20of%20Image%20Processing.%20The%20main%20motivation%20of%20introducing%20these%0Atechniques%20in%20to%20support%20the%20human%20visual%20system%20to%20capture%20relevant%0Adisplacements%20of%20an%20object%20of%20interest%3B%20these%20motions%20can%20be%20in%20object%20color%0Aand%20in%20object%20location.%20In%20fact%2C%20the%20goal%20is%20to%20opportunely%20process%20a%20video%0Asequence%20to%20obtain%20as%20output%20a%20new%20video%20in%20which%20motions%20are%20magnified%20and%0Avisible%20to%20the%20viewer.%20We%20propose%20a%20numerical%20technique%20using%20the%20Phase-Based%0AMotion%20Magnification%20which%20analyses%20the%20video%20sequence%20in%20the%20Fourier%20Domain%0Aand%20rely%20on%20the%20Fourier%20Shifting%20Property.%20We%20describe%20the%20mathematical%0Afoundation%20of%20this%20method%20and%20the%20corresponding%20implementation%20in%20a%20numerical%0Aalgorithm.%20We%20present%20preliminary%20experiments%2C%20focusing%20on%20some%20basic%20test%20made%0Aup%20using%20synthetic%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Processing%2520for%2520Motion%2520Magnification%26entry.906535625%3DNadaniela%2520Egidi%2520and%2520Josephin%2520Giacomini%2520and%2520Paolo%2520Leonesi%2520and%2520Pierluigi%2520Maponi%2520and%2520Federico%2520Mearelli%2520and%2520Edin%2520Trebovic%26entry.1292438233%3D%2520%2520Motion%2520Magnification%2520%2528MM%2529%2520is%2520a%2520collection%2520of%2520relative%2520recent%2520techniques%250Awithin%2520the%2520realm%2520of%2520Image%2520Processing.%2520The%2520main%2520motivation%2520of%2520introducing%2520these%250Atechniques%2520in%2520to%2520support%2520the%2520human%2520visual%2520system%2520to%2520capture%2520relevant%250Adisplacements%2520of%2520an%2520object%2520of%2520interest%253B%2520these%2520motions%2520can%2520be%2520in%2520object%2520color%250Aand%2520in%2520object%2520location.%2520In%2520fact%252C%2520the%2520goal%2520is%2520to%2520opportunely%2520process%2520a%2520video%250Asequence%2520to%2520obtain%2520as%2520output%2520a%2520new%2520video%2520in%2520which%2520motions%2520are%2520magnified%2520and%250Avisible%2520to%2520the%2520viewer.%2520We%2520propose%2520a%2520numerical%2520technique%2520using%2520the%2520Phase-Based%250AMotion%2520Magnification%2520which%2520analyses%2520the%2520video%2520sequence%2520in%2520the%2520Fourier%2520Domain%250Aand%2520rely%2520on%2520the%2520Fourier%2520Shifting%2520Property.%2520We%2520describe%2520the%2520mathematical%250Afoundation%2520of%2520this%2520method%2520and%2520the%2520corresponding%2520implementation%2520in%2520a%2520numerical%250Aalgorithm.%2520We%2520present%2520preliminary%2520experiments%252C%2520focusing%2520on%2520some%2520basic%2520test%2520made%250Aup%2520using%2520synthetic%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Processing%20for%20Motion%20Magnification&entry.906535625=Nadaniela%20Egidi%20and%20Josephin%20Giacomini%20and%20Paolo%20Leonesi%20and%20Pierluigi%20Maponi%20and%20Federico%20Mearelli%20and%20Edin%20Trebovic&entry.1292438233=%20%20Motion%20Magnification%20%28MM%29%20is%20a%20collection%20of%20relative%20recent%20techniques%0Awithin%20the%20realm%20of%20Image%20Processing.%20The%20main%20motivation%20of%20introducing%20these%0Atechniques%20in%20to%20support%20the%20human%20visual%20system%20to%20capture%20relevant%0Adisplacements%20of%20an%20object%20of%20interest%3B%20these%20motions%20can%20be%20in%20object%20color%0Aand%20in%20object%20location.%20In%20fact%2C%20the%20goal%20is%20to%20opportunely%20process%20a%20video%0Asequence%20to%20obtain%20as%20output%20a%20new%20video%20in%20which%20motions%20are%20magnified%20and%0Avisible%20to%20the%20viewer.%20We%20propose%20a%20numerical%20technique%20using%20the%20Phase-Based%0AMotion%20Magnification%20which%20analyses%20the%20video%20sequence%20in%20the%20Fourier%20Domain%0Aand%20rely%20on%20the%20Fourier%20Shifting%20Property.%20We%20describe%20the%20mathematical%0Afoundation%20of%20this%20method%20and%20the%20corresponding%20implementation%20in%20a%20numerical%0Aalgorithm.%20We%20present%20preliminary%20experiments%2C%20focusing%20on%20some%20basic%20test%20made%0Aup%20using%20synthetic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09555v1&entry.124074799=Read"},
{"title": "Breaking the Low-Rank Dilemma of Linear Attention", "author": "Qihang Fan and Huaibo Huang and Ran He", "abstract": "  The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.\n", "link": "http://arxiv.org/abs/2411.07635v2", "date": "2024-11-14", "relevancy": 2.0871, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Low-Rank%20Dilemma%20of%20Linear%20Attention&body=Title%3A%20Breaking%20the%20Low-Rank%20Dilemma%20of%20Linear%20Attention%0AAuthor%3A%20Qihang%20Fan%20and%20Huaibo%20Huang%20and%20Ran%20He%0AAbstract%3A%20%20%20The%20Softmax%20attention%20mechanism%20in%20Transformer%20models%20is%20notoriously%0Acomputationally%20expensive%2C%20particularly%20due%20to%20its%20quadratic%20complexity%2C%20posing%0Asignificant%20challenges%20in%20vision%20applications.%20In%20contrast%2C%20linear%20attention%0Aprovides%20a%20far%20more%20efficient%20solution%20by%20reducing%20the%20complexity%20to%20linear%0Alevels.%20However%2C%20compared%20to%20Softmax%20attention%2C%20linear%20attention%20often%0Aexperiences%20significant%20performance%20degradation.%20Our%20experiments%20indicate%20that%0Athis%20performance%20drop%20is%20due%20to%20the%20low-rank%20nature%20of%20linear%20attention%27s%0Afeature%20map%2C%20which%20hinders%20its%20ability%20to%20adequately%20model%20complex%20spatial%0Ainformation.%20In%20this%20paper%2C%20to%20break%20the%20low-rank%20dilemma%20of%20linear%20attention%2C%0Awe%20conduct%20rank%20analysis%20from%20two%20perspectives%3A%20the%20KV%20buffer%20and%20the%20output%0Afeatures.%20Consequently%2C%20we%20introduce%20Rank-Augmented%20Linear%20Attention%20%28RALA%29%2C%0Awhich%20rivals%20the%20performance%20of%20Softmax%20attention%20while%20maintaining%20linear%0Acomplexity%20and%20high%20efficiency.%20Based%20on%20RALA%2C%20we%20construct%20the%20Rank-Augmented%0AVision%20Linear%20Transformer%20%28RAVLT%29.%20Extensive%20experiments%20demonstrate%20that%20RAVLT%0Aachieves%20excellent%20performance%20across%20various%20vision%20tasks.%20Specifically%2C%0Awithout%20using%20any%20additional%20labels%2C%20data%2C%20or%20supervision%20during%20training%2C%0ARAVLT%20achieves%20an%2084.4%25%20Top-1%20accuracy%20on%20ImageNet-1k%20with%20only%2026M%20parameters%0Aand%204.6G%20FLOPs.%20This%20result%20significantly%20surpasses%20previous%20linear%20attention%0Amechanisms%2C%20fully%20illustrating%20the%20potential%20of%20RALA.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/qhfan/RALA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Low-Rank%2520Dilemma%2520of%2520Linear%2520Attention%26entry.906535625%3DQihang%2520Fan%2520and%2520Huaibo%2520Huang%2520and%2520Ran%2520He%26entry.1292438233%3D%2520%2520The%2520Softmax%2520attention%2520mechanism%2520in%2520Transformer%2520models%2520is%2520notoriously%250Acomputationally%2520expensive%252C%2520particularly%2520due%2520to%2520its%2520quadratic%2520complexity%252C%2520posing%250Asignificant%2520challenges%2520in%2520vision%2520applications.%2520In%2520contrast%252C%2520linear%2520attention%250Aprovides%2520a%2520far%2520more%2520efficient%2520solution%2520by%2520reducing%2520the%2520complexity%2520to%2520linear%250Alevels.%2520However%252C%2520compared%2520to%2520Softmax%2520attention%252C%2520linear%2520attention%2520often%250Aexperiences%2520significant%2520performance%2520degradation.%2520Our%2520experiments%2520indicate%2520that%250Athis%2520performance%2520drop%2520is%2520due%2520to%2520the%2520low-rank%2520nature%2520of%2520linear%2520attention%2527s%250Afeature%2520map%252C%2520which%2520hinders%2520its%2520ability%2520to%2520adequately%2520model%2520complex%2520spatial%250Ainformation.%2520In%2520this%2520paper%252C%2520to%2520break%2520the%2520low-rank%2520dilemma%2520of%2520linear%2520attention%252C%250Awe%2520conduct%2520rank%2520analysis%2520from%2520two%2520perspectives%253A%2520the%2520KV%2520buffer%2520and%2520the%2520output%250Afeatures.%2520Consequently%252C%2520we%2520introduce%2520Rank-Augmented%2520Linear%2520Attention%2520%2528RALA%2529%252C%250Awhich%2520rivals%2520the%2520performance%2520of%2520Softmax%2520attention%2520while%2520maintaining%2520linear%250Acomplexity%2520and%2520high%2520efficiency.%2520Based%2520on%2520RALA%252C%2520we%2520construct%2520the%2520Rank-Augmented%250AVision%2520Linear%2520Transformer%2520%2528RAVLT%2529.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RAVLT%250Aachieves%2520excellent%2520performance%2520across%2520various%2520vision%2520tasks.%2520Specifically%252C%250Awithout%2520using%2520any%2520additional%2520labels%252C%2520data%252C%2520or%2520supervision%2520during%2520training%252C%250ARAVLT%2520achieves%2520an%252084.4%2525%2520Top-1%2520accuracy%2520on%2520ImageNet-1k%2520with%2520only%252026M%2520parameters%250Aand%25204.6G%2520FLOPs.%2520This%2520result%2520significantly%2520surpasses%2520previous%2520linear%2520attention%250Amechanisms%252C%2520fully%2520illustrating%2520the%2520potential%2520of%2520RALA.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/qhfan/RALA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Low-Rank%20Dilemma%20of%20Linear%20Attention&entry.906535625=Qihang%20Fan%20and%20Huaibo%20Huang%20and%20Ran%20He&entry.1292438233=%20%20The%20Softmax%20attention%20mechanism%20in%20Transformer%20models%20is%20notoriously%0Acomputationally%20expensive%2C%20particularly%20due%20to%20its%20quadratic%20complexity%2C%20posing%0Asignificant%20challenges%20in%20vision%20applications.%20In%20contrast%2C%20linear%20attention%0Aprovides%20a%20far%20more%20efficient%20solution%20by%20reducing%20the%20complexity%20to%20linear%0Alevels.%20However%2C%20compared%20to%20Softmax%20attention%2C%20linear%20attention%20often%0Aexperiences%20significant%20performance%20degradation.%20Our%20experiments%20indicate%20that%0Athis%20performance%20drop%20is%20due%20to%20the%20low-rank%20nature%20of%20linear%20attention%27s%0Afeature%20map%2C%20which%20hinders%20its%20ability%20to%20adequately%20model%20complex%20spatial%0Ainformation.%20In%20this%20paper%2C%20to%20break%20the%20low-rank%20dilemma%20of%20linear%20attention%2C%0Awe%20conduct%20rank%20analysis%20from%20two%20perspectives%3A%20the%20KV%20buffer%20and%20the%20output%0Afeatures.%20Consequently%2C%20we%20introduce%20Rank-Augmented%20Linear%20Attention%20%28RALA%29%2C%0Awhich%20rivals%20the%20performance%20of%20Softmax%20attention%20while%20maintaining%20linear%0Acomplexity%20and%20high%20efficiency.%20Based%20on%20RALA%2C%20we%20construct%20the%20Rank-Augmented%0AVision%20Linear%20Transformer%20%28RAVLT%29.%20Extensive%20experiments%20demonstrate%20that%20RAVLT%0Aachieves%20excellent%20performance%20across%20various%20vision%20tasks.%20Specifically%2C%0Awithout%20using%20any%20additional%20labels%2C%20data%2C%20or%20supervision%20during%20training%2C%0ARAVLT%20achieves%20an%2084.4%25%20Top-1%20accuracy%20on%20ImageNet-1k%20with%20only%2026M%20parameters%0Aand%204.6G%20FLOPs.%20This%20result%20significantly%20surpasses%20previous%20linear%20attention%0Amechanisms%2C%20fully%20illustrating%20the%20potential%20of%20RALA.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/qhfan/RALA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07635v2&entry.124074799=Read"},
{"title": "Imagined Speech and Visual Imagery as Intuitive Paradigms for\n  Brain-Computer Interfaces", "author": "Seo-Hyun Lee and Ji-Ha Park and Deok-Seon Kim", "abstract": "  Recent advancements in brain-computer interface (BCI) technology have\nemphasized the promise of imagined speech and visual imagery as effective\nparadigms for intuitive communication. This study investigates the\nclassification performance and brain connectivity patterns associated with\nthese paradigms, focusing on decoding accuracy across selected word classes.\nSixteen participants engaged in tasks involving thirteen imagined speech and\nvisual imagery classes, revealing above-chance classification accuracy for both\nparadigms. Variability in classification accuracy across individual classes\nhighlights the influence of sensory and motor associations in imagined speech\nand vivid visual associations in visual imagery. Connectivity analysis further\ndemonstrated increased functional connectivity in language-related and sensory\nregions for imagined speech, whereas visual imagery activated spatial and\nvisual processing networks. These findings suggest the potential of imagined\nspeech and visual imagery as an intuitive and scalable paradigm for BCI\ncommunication when selecting optimal word classes. Further exploration of the\ndecoding outcomes for these two paradigms could provide insights for practical\nBCI communication.\n", "link": "http://arxiv.org/abs/2411.09400v1", "date": "2024-11-14", "relevancy": 2.0627, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces&body=Title%3A%20Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces%0AAuthor%3A%20Seo-Hyun%20Lee%20and%20Ji-Ha%20Park%20and%20Deok-Seon%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20brain-computer%20interface%20%28BCI%29%20technology%20have%0Aemphasized%20the%20promise%20of%20imagined%20speech%20and%20visual%20imagery%20as%20effective%0Aparadigms%20for%20intuitive%20communication.%20This%20study%20investigates%20the%0Aclassification%20performance%20and%20brain%20connectivity%20patterns%20associated%20with%0Athese%20paradigms%2C%20focusing%20on%20decoding%20accuracy%20across%20selected%20word%20classes.%0ASixteen%20participants%20engaged%20in%20tasks%20involving%20thirteen%20imagined%20speech%20and%0Avisual%20imagery%20classes%2C%20revealing%20above-chance%20classification%20accuracy%20for%20both%0Aparadigms.%20Variability%20in%20classification%20accuracy%20across%20individual%20classes%0Ahighlights%20the%20influence%20of%20sensory%20and%20motor%20associations%20in%20imagined%20speech%0Aand%20vivid%20visual%20associations%20in%20visual%20imagery.%20Connectivity%20analysis%20further%0Ademonstrated%20increased%20functional%20connectivity%20in%20language-related%20and%20sensory%0Aregions%20for%20imagined%20speech%2C%20whereas%20visual%20imagery%20activated%20spatial%20and%0Avisual%20processing%20networks.%20These%20findings%20suggest%20the%20potential%20of%20imagined%0Aspeech%20and%20visual%20imagery%20as%20an%20intuitive%20and%20scalable%20paradigm%20for%20BCI%0Acommunication%20when%20selecting%20optimal%20word%20classes.%20Further%20exploration%20of%20the%0Adecoding%20outcomes%20for%20these%20two%20paradigms%20could%20provide%20insights%20for%20practical%0ABCI%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagined%2520Speech%2520and%2520Visual%2520Imagery%2520as%2520Intuitive%2520Paradigms%2520for%250A%2520%2520Brain-Computer%2520Interfaces%26entry.906535625%3DSeo-Hyun%2520Lee%2520and%2520Ji-Ha%2520Park%2520and%2520Deok-Seon%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520brain-computer%2520interface%2520%2528BCI%2529%2520technology%2520have%250Aemphasized%2520the%2520promise%2520of%2520imagined%2520speech%2520and%2520visual%2520imagery%2520as%2520effective%250Aparadigms%2520for%2520intuitive%2520communication.%2520This%2520study%2520investigates%2520the%250Aclassification%2520performance%2520and%2520brain%2520connectivity%2520patterns%2520associated%2520with%250Athese%2520paradigms%252C%2520focusing%2520on%2520decoding%2520accuracy%2520across%2520selected%2520word%2520classes.%250ASixteen%2520participants%2520engaged%2520in%2520tasks%2520involving%2520thirteen%2520imagined%2520speech%2520and%250Avisual%2520imagery%2520classes%252C%2520revealing%2520above-chance%2520classification%2520accuracy%2520for%2520both%250Aparadigms.%2520Variability%2520in%2520classification%2520accuracy%2520across%2520individual%2520classes%250Ahighlights%2520the%2520influence%2520of%2520sensory%2520and%2520motor%2520associations%2520in%2520imagined%2520speech%250Aand%2520vivid%2520visual%2520associations%2520in%2520visual%2520imagery.%2520Connectivity%2520analysis%2520further%250Ademonstrated%2520increased%2520functional%2520connectivity%2520in%2520language-related%2520and%2520sensory%250Aregions%2520for%2520imagined%2520speech%252C%2520whereas%2520visual%2520imagery%2520activated%2520spatial%2520and%250Avisual%2520processing%2520networks.%2520These%2520findings%2520suggest%2520the%2520potential%2520of%2520imagined%250Aspeech%2520and%2520visual%2520imagery%2520as%2520an%2520intuitive%2520and%2520scalable%2520paradigm%2520for%2520BCI%250Acommunication%2520when%2520selecting%2520optimal%2520word%2520classes.%2520Further%2520exploration%2520of%2520the%250Adecoding%2520outcomes%2520for%2520these%2520two%2520paradigms%2520could%2520provide%2520insights%2520for%2520practical%250ABCI%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces&entry.906535625=Seo-Hyun%20Lee%20and%20Ji-Ha%20Park%20and%20Deok-Seon%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20brain-computer%20interface%20%28BCI%29%20technology%20have%0Aemphasized%20the%20promise%20of%20imagined%20speech%20and%20visual%20imagery%20as%20effective%0Aparadigms%20for%20intuitive%20communication.%20This%20study%20investigates%20the%0Aclassification%20performance%20and%20brain%20connectivity%20patterns%20associated%20with%0Athese%20paradigms%2C%20focusing%20on%20decoding%20accuracy%20across%20selected%20word%20classes.%0ASixteen%20participants%20engaged%20in%20tasks%20involving%20thirteen%20imagined%20speech%20and%0Avisual%20imagery%20classes%2C%20revealing%20above-chance%20classification%20accuracy%20for%20both%0Aparadigms.%20Variability%20in%20classification%20accuracy%20across%20individual%20classes%0Ahighlights%20the%20influence%20of%20sensory%20and%20motor%20associations%20in%20imagined%20speech%0Aand%20vivid%20visual%20associations%20in%20visual%20imagery.%20Connectivity%20analysis%20further%0Ademonstrated%20increased%20functional%20connectivity%20in%20language-related%20and%20sensory%0Aregions%20for%20imagined%20speech%2C%20whereas%20visual%20imagery%20activated%20spatial%20and%0Avisual%20processing%20networks.%20These%20findings%20suggest%20the%20potential%20of%20imagined%0Aspeech%20and%20visual%20imagery%20as%20an%20intuitive%20and%20scalable%20paradigm%20for%20BCI%0Acommunication%20when%20selecting%20optimal%20word%20classes.%20Further%20exploration%20of%20the%0Adecoding%20outcomes%20for%20these%20two%20paradigms%20could%20provide%20insights%20for%20practical%0ABCI%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09400v1&entry.124074799=Read"},
{"title": "A Similarity-Based Oversampling Method for Multi-label Imbalanced Text\n  Data", "author": "Ismail Hakki Karaman and Gulser Koksal and Levent Eriskin and Salih Salihoglu", "abstract": "  In real-world applications, as data availability increases, obtaining labeled\ndata for machine learning (ML) projects remains challenging due to the high\ncosts and intensive efforts required for data annotation. Many ML projects,\nparticularly those focused on multi-label classification, also grapple with\ndata imbalance issues, where certain classes may lack sufficient data to train\neffective classifiers. This study introduces and examines a novel oversampling\nmethod for multi-label text classification, designed to address performance\nchallenges associated with data imbalance. The proposed method identifies\npotential new samples from unlabeled data by leveraging similarity measures\nbetween instances. By iteratively searching the unlabeled dataset, the method\nlocates instances similar to those in underrepresented classes and evaluates\ntheir contribution to classifier performance enhancement. Instances that\ndemonstrate performance improvement are then added to the labeled dataset.\nExperimental results indicate that the proposed approach effectively enhances\nclassifier performance post-oversampling.\n", "link": "http://arxiv.org/abs/2411.01013v2", "date": "2024-11-14", "relevancy": 2.0419, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4816}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Similarity-Based%20Oversampling%20Method%20for%20Multi-label%20Imbalanced%20Text%0A%20%20Data&body=Title%3A%20A%20Similarity-Based%20Oversampling%20Method%20for%20Multi-label%20Imbalanced%20Text%0A%20%20Data%0AAuthor%3A%20Ismail%20Hakki%20Karaman%20and%20Gulser%20Koksal%20and%20Levent%20Eriskin%20and%20Salih%20Salihoglu%0AAbstract%3A%20%20%20In%20real-world%20applications%2C%20as%20data%20availability%20increases%2C%20obtaining%20labeled%0Adata%20for%20machine%20learning%20%28ML%29%20projects%20remains%20challenging%20due%20to%20the%20high%0Acosts%20and%20intensive%20efforts%20required%20for%20data%20annotation.%20Many%20ML%20projects%2C%0Aparticularly%20those%20focused%20on%20multi-label%20classification%2C%20also%20grapple%20with%0Adata%20imbalance%20issues%2C%20where%20certain%20classes%20may%20lack%20sufficient%20data%20to%20train%0Aeffective%20classifiers.%20This%20study%20introduces%20and%20examines%20a%20novel%20oversampling%0Amethod%20for%20multi-label%20text%20classification%2C%20designed%20to%20address%20performance%0Achallenges%20associated%20with%20data%20imbalance.%20The%20proposed%20method%20identifies%0Apotential%20new%20samples%20from%20unlabeled%20data%20by%20leveraging%20similarity%20measures%0Abetween%20instances.%20By%20iteratively%20searching%20the%20unlabeled%20dataset%2C%20the%20method%0Alocates%20instances%20similar%20to%20those%20in%20underrepresented%20classes%20and%20evaluates%0Atheir%20contribution%20to%20classifier%20performance%20enhancement.%20Instances%20that%0Ademonstrate%20performance%20improvement%20are%20then%20added%20to%20the%20labeled%20dataset.%0AExperimental%20results%20indicate%20that%20the%20proposed%20approach%20effectively%20enhances%0Aclassifier%20performance%20post-oversampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Similarity-Based%2520Oversampling%2520Method%2520for%2520Multi-label%2520Imbalanced%2520Text%250A%2520%2520Data%26entry.906535625%3DIsmail%2520Hakki%2520Karaman%2520and%2520Gulser%2520Koksal%2520and%2520Levent%2520Eriskin%2520and%2520Salih%2520Salihoglu%26entry.1292438233%3D%2520%2520In%2520real-world%2520applications%252C%2520as%2520data%2520availability%2520increases%252C%2520obtaining%2520labeled%250Adata%2520for%2520machine%2520learning%2520%2528ML%2529%2520projects%2520remains%2520challenging%2520due%2520to%2520the%2520high%250Acosts%2520and%2520intensive%2520efforts%2520required%2520for%2520data%2520annotation.%2520Many%2520ML%2520projects%252C%250Aparticularly%2520those%2520focused%2520on%2520multi-label%2520classification%252C%2520also%2520grapple%2520with%250Adata%2520imbalance%2520issues%252C%2520where%2520certain%2520classes%2520may%2520lack%2520sufficient%2520data%2520to%2520train%250Aeffective%2520classifiers.%2520This%2520study%2520introduces%2520and%2520examines%2520a%2520novel%2520oversampling%250Amethod%2520for%2520multi-label%2520text%2520classification%252C%2520designed%2520to%2520address%2520performance%250Achallenges%2520associated%2520with%2520data%2520imbalance.%2520The%2520proposed%2520method%2520identifies%250Apotential%2520new%2520samples%2520from%2520unlabeled%2520data%2520by%2520leveraging%2520similarity%2520measures%250Abetween%2520instances.%2520By%2520iteratively%2520searching%2520the%2520unlabeled%2520dataset%252C%2520the%2520method%250Alocates%2520instances%2520similar%2520to%2520those%2520in%2520underrepresented%2520classes%2520and%2520evaluates%250Atheir%2520contribution%2520to%2520classifier%2520performance%2520enhancement.%2520Instances%2520that%250Ademonstrate%2520performance%2520improvement%2520are%2520then%2520added%2520to%2520the%2520labeled%2520dataset.%250AExperimental%2520results%2520indicate%2520that%2520the%2520proposed%2520approach%2520effectively%2520enhances%250Aclassifier%2520performance%2520post-oversampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Similarity-Based%20Oversampling%20Method%20for%20Multi-label%20Imbalanced%20Text%0A%20%20Data&entry.906535625=Ismail%20Hakki%20Karaman%20and%20Gulser%20Koksal%20and%20Levent%20Eriskin%20and%20Salih%20Salihoglu&entry.1292438233=%20%20In%20real-world%20applications%2C%20as%20data%20availability%20increases%2C%20obtaining%20labeled%0Adata%20for%20machine%20learning%20%28ML%29%20projects%20remains%20challenging%20due%20to%20the%20high%0Acosts%20and%20intensive%20efforts%20required%20for%20data%20annotation.%20Many%20ML%20projects%2C%0Aparticularly%20those%20focused%20on%20multi-label%20classification%2C%20also%20grapple%20with%0Adata%20imbalance%20issues%2C%20where%20certain%20classes%20may%20lack%20sufficient%20data%20to%20train%0Aeffective%20classifiers.%20This%20study%20introduces%20and%20examines%20a%20novel%20oversampling%0Amethod%20for%20multi-label%20text%20classification%2C%20designed%20to%20address%20performance%0Achallenges%20associated%20with%20data%20imbalance.%20The%20proposed%20method%20identifies%0Apotential%20new%20samples%20from%20unlabeled%20data%20by%20leveraging%20similarity%20measures%0Abetween%20instances.%20By%20iteratively%20searching%20the%20unlabeled%20dataset%2C%20the%20method%0Alocates%20instances%20similar%20to%20those%20in%20underrepresented%20classes%20and%20evaluates%0Atheir%20contribution%20to%20classifier%20performance%20enhancement.%20Instances%20that%0Ademonstrate%20performance%20improvement%20are%20then%20added%20to%20the%20labeled%20dataset.%0AExperimental%20results%20indicate%20that%20the%20proposed%20approach%20effectively%20enhances%0Aclassifier%20performance%20post-oversampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01013v2&entry.124074799=Read"},
{"title": "On the Limits of Language Generation: Trade-Offs Between Hallucination\n  and Mode Collapse", "author": "Alkis Kalavasis and Anay Mehrotra and Grigoris Velegkas", "abstract": "  Specifying all desirable properties of a language model is challenging, but\ncertain requirements seem essential. Given samples from an unknown language,\nthe trained model should produce valid strings not seen in training and be\nexpressive enough to capture the language's full richness. Otherwise,\noutputting invalid strings constitutes \"hallucination,\" and failing to capture\nthe full range leads to \"mode collapse.\" We ask if a language model can meet\nboth requirements.\n  We investigate this within a statistical language generation setting building\non Gold and Angluin. Here, the model receives random samples from a\ndistribution over an unknown language K, which belongs to a possibly infinite\ncollection of languages. The goal is to generate unseen strings from K. We say\nthe model generates from K with consistency and breadth if, as training size\nincreases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in\nlanguage generation are possible. We answer this negatively: for a large class\nof language models, including next-token prediction models, this is impossible\nfor most collections of candidate languages. This contrasts with [KM24]'s\nresult, showing consistent generation without breadth is possible for any\ncountable collection of languages. Our finding highlights that generation with\nbreadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples\nneeded for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is\nachievable for any countable collection of languages when negative examples\n(strings outside K) are available alongside positive ones. This suggests that\npost-training feedback, which encodes negative examples, can be crucial in\nreducing hallucinations while limiting mode collapse.\n", "link": "http://arxiv.org/abs/2411.09642v1", "date": "2024-11-14", "relevancy": 2.0213, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5279}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Limits%20of%20Language%20Generation%3A%20Trade-Offs%20Between%20Hallucination%0A%20%20and%20Mode%20Collapse&body=Title%3A%20On%20the%20Limits%20of%20Language%20Generation%3A%20Trade-Offs%20Between%20Hallucination%0A%20%20and%20Mode%20Collapse%0AAuthor%3A%20Alkis%20Kalavasis%20and%20Anay%20Mehrotra%20and%20Grigoris%20Velegkas%0AAbstract%3A%20%20%20Specifying%20all%20desirable%20properties%20of%20a%20language%20model%20is%20challenging%2C%20but%0Acertain%20requirements%20seem%20essential.%20Given%20samples%20from%20an%20unknown%20language%2C%0Athe%20trained%20model%20should%20produce%20valid%20strings%20not%20seen%20in%20training%20and%20be%0Aexpressive%20enough%20to%20capture%20the%20language%27s%20full%20richness.%20Otherwise%2C%0Aoutputting%20invalid%20strings%20constitutes%20%22hallucination%2C%22%20and%20failing%20to%20capture%0Athe%20full%20range%20leads%20to%20%22mode%20collapse.%22%20We%20ask%20if%20a%20language%20model%20can%20meet%0Aboth%20requirements.%0A%20%20We%20investigate%20this%20within%20a%20statistical%20language%20generation%20setting%20building%0Aon%20Gold%20and%20Angluin.%20Here%2C%20the%20model%20receives%20random%20samples%20from%20a%0Adistribution%20over%20an%20unknown%20language%20K%2C%20which%20belongs%20to%20a%20possibly%20infinite%0Acollection%20of%20languages.%20The%20goal%20is%20to%20generate%20unseen%20strings%20from%20K.%20We%20say%0Athe%20model%20generates%20from%20K%20with%20consistency%20and%20breadth%20if%2C%20as%20training%20size%0Aincreases%2C%20its%20output%20converges%20to%20all%20unseen%20strings%20in%20K.%0A%20%20Kleinberg%20and%20Mullainathan%20%5BKM24%5D%20asked%20if%20consistency%20and%20breadth%20in%0Alanguage%20generation%20are%20possible.%20We%20answer%20this%20negatively%3A%20for%20a%20large%20class%0Aof%20language%20models%2C%20including%20next-token%20prediction%20models%2C%20this%20is%20impossible%0Afor%20most%20collections%20of%20candidate%20languages.%20This%20contrasts%20with%20%5BKM24%5D%27s%0Aresult%2C%20showing%20consistent%20generation%20without%20breadth%20is%20possible%20for%20any%0Acountable%20collection%20of%20languages.%20Our%20finding%20highlights%20that%20generation%20with%0Abreadth%20fundamentally%20differs%20from%20generation%20without%20breadth.%0A%20%20As%20a%20byproduct%2C%20we%20establish%20near-tight%20bounds%20on%20the%20number%20of%20samples%0Aneeded%20for%20generation%20with%20or%20without%20breadth.%0A%20%20Finally%2C%20our%20results%20offer%20hope%3A%20consistent%20generation%20with%20breadth%20is%0Aachievable%20for%20any%20countable%20collection%20of%20languages%20when%20negative%20examples%0A%28strings%20outside%20K%29%20are%20available%20alongside%20positive%20ones.%20This%20suggests%20that%0Apost-training%20feedback%2C%20which%20encodes%20negative%20examples%2C%20can%20be%20crucial%20in%0Areducing%20hallucinations%20while%20limiting%20mode%20collapse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Limits%2520of%2520Language%2520Generation%253A%2520Trade-Offs%2520Between%2520Hallucination%250A%2520%2520and%2520Mode%2520Collapse%26entry.906535625%3DAlkis%2520Kalavasis%2520and%2520Anay%2520Mehrotra%2520and%2520Grigoris%2520Velegkas%26entry.1292438233%3D%2520%2520Specifying%2520all%2520desirable%2520properties%2520of%2520a%2520language%2520model%2520is%2520challenging%252C%2520but%250Acertain%2520requirements%2520seem%2520essential.%2520Given%2520samples%2520from%2520an%2520unknown%2520language%252C%250Athe%2520trained%2520model%2520should%2520produce%2520valid%2520strings%2520not%2520seen%2520in%2520training%2520and%2520be%250Aexpressive%2520enough%2520to%2520capture%2520the%2520language%2527s%2520full%2520richness.%2520Otherwise%252C%250Aoutputting%2520invalid%2520strings%2520constitutes%2520%2522hallucination%252C%2522%2520and%2520failing%2520to%2520capture%250Athe%2520full%2520range%2520leads%2520to%2520%2522mode%2520collapse.%2522%2520We%2520ask%2520if%2520a%2520language%2520model%2520can%2520meet%250Aboth%2520requirements.%250A%2520%2520We%2520investigate%2520this%2520within%2520a%2520statistical%2520language%2520generation%2520setting%2520building%250Aon%2520Gold%2520and%2520Angluin.%2520Here%252C%2520the%2520model%2520receives%2520random%2520samples%2520from%2520a%250Adistribution%2520over%2520an%2520unknown%2520language%2520K%252C%2520which%2520belongs%2520to%2520a%2520possibly%2520infinite%250Acollection%2520of%2520languages.%2520The%2520goal%2520is%2520to%2520generate%2520unseen%2520strings%2520from%2520K.%2520We%2520say%250Athe%2520model%2520generates%2520from%2520K%2520with%2520consistency%2520and%2520breadth%2520if%252C%2520as%2520training%2520size%250Aincreases%252C%2520its%2520output%2520converges%2520to%2520all%2520unseen%2520strings%2520in%2520K.%250A%2520%2520Kleinberg%2520and%2520Mullainathan%2520%255BKM24%255D%2520asked%2520if%2520consistency%2520and%2520breadth%2520in%250Alanguage%2520generation%2520are%2520possible.%2520We%2520answer%2520this%2520negatively%253A%2520for%2520a%2520large%2520class%250Aof%2520language%2520models%252C%2520including%2520next-token%2520prediction%2520models%252C%2520this%2520is%2520impossible%250Afor%2520most%2520collections%2520of%2520candidate%2520languages.%2520This%2520contrasts%2520with%2520%255BKM24%255D%2527s%250Aresult%252C%2520showing%2520consistent%2520generation%2520without%2520breadth%2520is%2520possible%2520for%2520any%250Acountable%2520collection%2520of%2520languages.%2520Our%2520finding%2520highlights%2520that%2520generation%2520with%250Abreadth%2520fundamentally%2520differs%2520from%2520generation%2520without%2520breadth.%250A%2520%2520As%2520a%2520byproduct%252C%2520we%2520establish%2520near-tight%2520bounds%2520on%2520the%2520number%2520of%2520samples%250Aneeded%2520for%2520generation%2520with%2520or%2520without%2520breadth.%250A%2520%2520Finally%252C%2520our%2520results%2520offer%2520hope%253A%2520consistent%2520generation%2520with%2520breadth%2520is%250Aachievable%2520for%2520any%2520countable%2520collection%2520of%2520languages%2520when%2520negative%2520examples%250A%2528strings%2520outside%2520K%2529%2520are%2520available%2520alongside%2520positive%2520ones.%2520This%2520suggests%2520that%250Apost-training%2520feedback%252C%2520which%2520encodes%2520negative%2520examples%252C%2520can%2520be%2520crucial%2520in%250Areducing%2520hallucinations%2520while%2520limiting%2520mode%2520collapse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Limits%20of%20Language%20Generation%3A%20Trade-Offs%20Between%20Hallucination%0A%20%20and%20Mode%20Collapse&entry.906535625=Alkis%20Kalavasis%20and%20Anay%20Mehrotra%20and%20Grigoris%20Velegkas&entry.1292438233=%20%20Specifying%20all%20desirable%20properties%20of%20a%20language%20model%20is%20challenging%2C%20but%0Acertain%20requirements%20seem%20essential.%20Given%20samples%20from%20an%20unknown%20language%2C%0Athe%20trained%20model%20should%20produce%20valid%20strings%20not%20seen%20in%20training%20and%20be%0Aexpressive%20enough%20to%20capture%20the%20language%27s%20full%20richness.%20Otherwise%2C%0Aoutputting%20invalid%20strings%20constitutes%20%22hallucination%2C%22%20and%20failing%20to%20capture%0Athe%20full%20range%20leads%20to%20%22mode%20collapse.%22%20We%20ask%20if%20a%20language%20model%20can%20meet%0Aboth%20requirements.%0A%20%20We%20investigate%20this%20within%20a%20statistical%20language%20generation%20setting%20building%0Aon%20Gold%20and%20Angluin.%20Here%2C%20the%20model%20receives%20random%20samples%20from%20a%0Adistribution%20over%20an%20unknown%20language%20K%2C%20which%20belongs%20to%20a%20possibly%20infinite%0Acollection%20of%20languages.%20The%20goal%20is%20to%20generate%20unseen%20strings%20from%20K.%20We%20say%0Athe%20model%20generates%20from%20K%20with%20consistency%20and%20breadth%20if%2C%20as%20training%20size%0Aincreases%2C%20its%20output%20converges%20to%20all%20unseen%20strings%20in%20K.%0A%20%20Kleinberg%20and%20Mullainathan%20%5BKM24%5D%20asked%20if%20consistency%20and%20breadth%20in%0Alanguage%20generation%20are%20possible.%20We%20answer%20this%20negatively%3A%20for%20a%20large%20class%0Aof%20language%20models%2C%20including%20next-token%20prediction%20models%2C%20this%20is%20impossible%0Afor%20most%20collections%20of%20candidate%20languages.%20This%20contrasts%20with%20%5BKM24%5D%27s%0Aresult%2C%20showing%20consistent%20generation%20without%20breadth%20is%20possible%20for%20any%0Acountable%20collection%20of%20languages.%20Our%20finding%20highlights%20that%20generation%20with%0Abreadth%20fundamentally%20differs%20from%20generation%20without%20breadth.%0A%20%20As%20a%20byproduct%2C%20we%20establish%20near-tight%20bounds%20on%20the%20number%20of%20samples%0Aneeded%20for%20generation%20with%20or%20without%20breadth.%0A%20%20Finally%2C%20our%20results%20offer%20hope%3A%20consistent%20generation%20with%20breadth%20is%0Aachievable%20for%20any%20countable%20collection%20of%20languages%20when%20negative%20examples%0A%28strings%20outside%20K%29%20are%20available%20alongside%20positive%20ones.%20This%20suggests%20that%0Apost-training%20feedback%2C%20which%20encodes%20negative%20examples%2C%20can%20be%20crucial%20in%0Areducing%20hallucinations%20while%20limiting%20mode%20collapse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09642v1&entry.124074799=Read"},
{"title": "InfiBench: Evaluating the Question-Answering Capabilities of Code Large\n  Language Models", "author": "Linyi Li and Shijie Geng and Zhenwen Li and Yibo He and Hao Yu and Ziyue Hua and Guanghan Ning and Siwei Wang and Tao Xie and Hongxia Yang", "abstract": "  Large Language Models for code (code LLMs) have witnessed tremendous progress\nin recent years. With the rapid development of code LLMs, many popular\nevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to\nmeasure the performance of code LLMs with a particular focus on code generation\ntasks. However, they are insufficient to cover the full range of expected\ncapabilities of code LLMs, which span beyond code generation to answering\ndiverse coding-related questions. To fill this gap, we propose InfiBench, the\nfirst large-scale freeform question-answering (QA) benchmark for code to our\nknowledge, comprising 234 carefully selected high-quality Stack Overflow\nquestions that span across 15 programming languages. InfiBench uses four types\nof model-free automatic metrics to evaluate response correctness where domain\nexperts carefully concretize the criterion for each question. We conduct a\nsystematic evaluation for over 100 latest code LLMs on InfiBench, leading to a\nseries of novel and insightful findings. Our detailed analyses showcase\npotential directions for further advancement of code LLMs. InfiBench is fully\nopen source at https://infi-coder.github.io/infibench and continuously\nexpanding to foster more scientific and systematic practices for code LLM\nevaluation.\n", "link": "http://arxiv.org/abs/2404.07940v3", "date": "2024-11-14", "relevancy": 2.0107, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiBench%3A%20Evaluating%20the%20Question-Answering%20Capabilities%20of%20Code%20Large%0A%20%20Language%20Models&body=Title%3A%20InfiBench%3A%20Evaluating%20the%20Question-Answering%20Capabilities%20of%20Code%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Linyi%20Li%20and%20Shijie%20Geng%20and%20Zhenwen%20Li%20and%20Yibo%20He%20and%20Hao%20Yu%20and%20Ziyue%20Hua%20and%20Guanghan%20Ning%20and%20Siwei%20Wang%20and%20Tao%20Xie%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20for%20code%20%28code%20LLMs%29%20have%20witnessed%20tremendous%20progress%0Ain%20recent%20years.%20With%20the%20rapid%20development%20of%20code%20LLMs%2C%20many%20popular%0Aevaluation%20benchmarks%2C%20such%20as%20HumanEval%2C%20DS-1000%2C%20and%20MBPP%2C%20have%20emerged%20to%0Ameasure%20the%20performance%20of%20code%20LLMs%20with%20a%20particular%20focus%20on%20code%20generation%0Atasks.%20However%2C%20they%20are%20insufficient%20to%20cover%20the%20full%20range%20of%20expected%0Acapabilities%20of%20code%20LLMs%2C%20which%20span%20beyond%20code%20generation%20to%20answering%0Adiverse%20coding-related%20questions.%20To%20fill%20this%20gap%2C%20we%20propose%20InfiBench%2C%20the%0Afirst%20large-scale%20freeform%20question-answering%20%28QA%29%20benchmark%20for%20code%20to%20our%0Aknowledge%2C%20comprising%20234%20carefully%20selected%20high-quality%20Stack%20Overflow%0Aquestions%20that%20span%20across%2015%20programming%20languages.%20InfiBench%20uses%20four%20types%0Aof%20model-free%20automatic%20metrics%20to%20evaluate%20response%20correctness%20where%20domain%0Aexperts%20carefully%20concretize%20the%20criterion%20for%20each%20question.%20We%20conduct%20a%0Asystematic%20evaluation%20for%20over%20100%20latest%20code%20LLMs%20on%20InfiBench%2C%20leading%20to%20a%0Aseries%20of%20novel%20and%20insightful%20findings.%20Our%20detailed%20analyses%20showcase%0Apotential%20directions%20for%20further%20advancement%20of%20code%20LLMs.%20InfiBench%20is%20fully%0Aopen%20source%20at%20https%3A//infi-coder.github.io/infibench%20and%20continuously%0Aexpanding%20to%20foster%20more%20scientific%20and%20systematic%20practices%20for%20code%20LLM%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiBench%253A%2520Evaluating%2520the%2520Question-Answering%2520Capabilities%2520of%2520Code%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DLinyi%2520Li%2520and%2520Shijie%2520Geng%2520and%2520Zhenwen%2520Li%2520and%2520Yibo%2520He%2520and%2520Hao%2520Yu%2520and%2520Ziyue%2520Hua%2520and%2520Guanghan%2520Ning%2520and%2520Siwei%2520Wang%2520and%2520Tao%2520Xie%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520for%2520code%2520%2528code%2520LLMs%2529%2520have%2520witnessed%2520tremendous%2520progress%250Ain%2520recent%2520years.%2520With%2520the%2520rapid%2520development%2520of%2520code%2520LLMs%252C%2520many%2520popular%250Aevaluation%2520benchmarks%252C%2520such%2520as%2520HumanEval%252C%2520DS-1000%252C%2520and%2520MBPP%252C%2520have%2520emerged%2520to%250Ameasure%2520the%2520performance%2520of%2520code%2520LLMs%2520with%2520a%2520particular%2520focus%2520on%2520code%2520generation%250Atasks.%2520However%252C%2520they%2520are%2520insufficient%2520to%2520cover%2520the%2520full%2520range%2520of%2520expected%250Acapabilities%2520of%2520code%2520LLMs%252C%2520which%2520span%2520beyond%2520code%2520generation%2520to%2520answering%250Adiverse%2520coding-related%2520questions.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520InfiBench%252C%2520the%250Afirst%2520large-scale%2520freeform%2520question-answering%2520%2528QA%2529%2520benchmark%2520for%2520code%2520to%2520our%250Aknowledge%252C%2520comprising%2520234%2520carefully%2520selected%2520high-quality%2520Stack%2520Overflow%250Aquestions%2520that%2520span%2520across%252015%2520programming%2520languages.%2520InfiBench%2520uses%2520four%2520types%250Aof%2520model-free%2520automatic%2520metrics%2520to%2520evaluate%2520response%2520correctness%2520where%2520domain%250Aexperts%2520carefully%2520concretize%2520the%2520criterion%2520for%2520each%2520question.%2520We%2520conduct%2520a%250Asystematic%2520evaluation%2520for%2520over%2520100%2520latest%2520code%2520LLMs%2520on%2520InfiBench%252C%2520leading%2520to%2520a%250Aseries%2520of%2520novel%2520and%2520insightful%2520findings.%2520Our%2520detailed%2520analyses%2520showcase%250Apotential%2520directions%2520for%2520further%2520advancement%2520of%2520code%2520LLMs.%2520InfiBench%2520is%2520fully%250Aopen%2520source%2520at%2520https%253A//infi-coder.github.io/infibench%2520and%2520continuously%250Aexpanding%2520to%2520foster%2520more%2520scientific%2520and%2520systematic%2520practices%2520for%2520code%2520LLM%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiBench%3A%20Evaluating%20the%20Question-Answering%20Capabilities%20of%20Code%20Large%0A%20%20Language%20Models&entry.906535625=Linyi%20Li%20and%20Shijie%20Geng%20and%20Zhenwen%20Li%20and%20Yibo%20He%20and%20Hao%20Yu%20and%20Ziyue%20Hua%20and%20Guanghan%20Ning%20and%20Siwei%20Wang%20and%20Tao%20Xie%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20for%20code%20%28code%20LLMs%29%20have%20witnessed%20tremendous%20progress%0Ain%20recent%20years.%20With%20the%20rapid%20development%20of%20code%20LLMs%2C%20many%20popular%0Aevaluation%20benchmarks%2C%20such%20as%20HumanEval%2C%20DS-1000%2C%20and%20MBPP%2C%20have%20emerged%20to%0Ameasure%20the%20performance%20of%20code%20LLMs%20with%20a%20particular%20focus%20on%20code%20generation%0Atasks.%20However%2C%20they%20are%20insufficient%20to%20cover%20the%20full%20range%20of%20expected%0Acapabilities%20of%20code%20LLMs%2C%20which%20span%20beyond%20code%20generation%20to%20answering%0Adiverse%20coding-related%20questions.%20To%20fill%20this%20gap%2C%20we%20propose%20InfiBench%2C%20the%0Afirst%20large-scale%20freeform%20question-answering%20%28QA%29%20benchmark%20for%20code%20to%20our%0Aknowledge%2C%20comprising%20234%20carefully%20selected%20high-quality%20Stack%20Overflow%0Aquestions%20that%20span%20across%2015%20programming%20languages.%20InfiBench%20uses%20four%20types%0Aof%20model-free%20automatic%20metrics%20to%20evaluate%20response%20correctness%20where%20domain%0Aexperts%20carefully%20concretize%20the%20criterion%20for%20each%20question.%20We%20conduct%20a%0Asystematic%20evaluation%20for%20over%20100%20latest%20code%20LLMs%20on%20InfiBench%2C%20leading%20to%20a%0Aseries%20of%20novel%20and%20insightful%20findings.%20Our%20detailed%20analyses%20showcase%0Apotential%20directions%20for%20further%20advancement%20of%20code%20LLMs.%20InfiBench%20is%20fully%0Aopen%20source%20at%20https%3A//infi-coder.github.io/infibench%20and%20continuously%0Aexpanding%20to%20foster%20more%20scientific%20and%20systematic%20practices%20for%20code%20LLM%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07940v3&entry.124074799=Read"},
{"title": "Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models", "author": "Zi-Xuan Huang and Jia-Wei Chen and Zhi-Peng Zhang and Chia-Mu Yu", "abstract": "  Visual prompting (VP) is a new technique that adapts well-trained frozen\nmodels for source domain tasks to target domain tasks. This study examines VP's\nbenefits for black-box model-level backdoor detection. The visual prompt in VP\nmaps class subspaces between source and target domains. We identify a\nmisalignment, termed class subspace inconsistency, between clean and poisoned\ndatasets. Based on this, we introduce \\textsc{BProm}, a black-box model-level\ndetection method to identify backdoors in suspicious models, if any.\n\\textsc{BProm} leverages the low classification accuracy of prompted models\nwhen backdoors are present. Extensive experiments confirm \\textsc{BProm}'s\neffectiveness.\n", "link": "http://arxiv.org/abs/2411.09540v1", "date": "2024-11-14", "relevancy": 1.977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20the%20Unseen%3A%20Detecting%20Hidden%20Backdoors%20in%20Black-Box%20Models&body=Title%3A%20Prompting%20the%20Unseen%3A%20Detecting%20Hidden%20Backdoors%20in%20Black-Box%20Models%0AAuthor%3A%20Zi-Xuan%20Huang%20and%20Jia-Wei%20Chen%20and%20Zhi-Peng%20Zhang%20and%20Chia-Mu%20Yu%0AAbstract%3A%20%20%20Visual%20prompting%20%28VP%29%20is%20a%20new%20technique%20that%20adapts%20well-trained%20frozen%0Amodels%20for%20source%20domain%20tasks%20to%20target%20domain%20tasks.%20This%20study%20examines%20VP%27s%0Abenefits%20for%20black-box%20model-level%20backdoor%20detection.%20The%20visual%20prompt%20in%20VP%0Amaps%20class%20subspaces%20between%20source%20and%20target%20domains.%20We%20identify%20a%0Amisalignment%2C%20termed%20class%20subspace%20inconsistency%2C%20between%20clean%20and%20poisoned%0Adatasets.%20Based%20on%20this%2C%20we%20introduce%20%5Ctextsc%7BBProm%7D%2C%20a%20black-box%20model-level%0Adetection%20method%20to%20identify%20backdoors%20in%20suspicious%20models%2C%20if%20any.%0A%5Ctextsc%7BBProm%7D%20leverages%20the%20low%20classification%20accuracy%20of%20prompted%20models%0Awhen%20backdoors%20are%20present.%20Extensive%20experiments%20confirm%20%5Ctextsc%7BBProm%7D%27s%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520the%2520Unseen%253A%2520Detecting%2520Hidden%2520Backdoors%2520in%2520Black-Box%2520Models%26entry.906535625%3DZi-Xuan%2520Huang%2520and%2520Jia-Wei%2520Chen%2520and%2520Zhi-Peng%2520Zhang%2520and%2520Chia-Mu%2520Yu%26entry.1292438233%3D%2520%2520Visual%2520prompting%2520%2528VP%2529%2520is%2520a%2520new%2520technique%2520that%2520adapts%2520well-trained%2520frozen%250Amodels%2520for%2520source%2520domain%2520tasks%2520to%2520target%2520domain%2520tasks.%2520This%2520study%2520examines%2520VP%2527s%250Abenefits%2520for%2520black-box%2520model-level%2520backdoor%2520detection.%2520The%2520visual%2520prompt%2520in%2520VP%250Amaps%2520class%2520subspaces%2520between%2520source%2520and%2520target%2520domains.%2520We%2520identify%2520a%250Amisalignment%252C%2520termed%2520class%2520subspace%2520inconsistency%252C%2520between%2520clean%2520and%2520poisoned%250Adatasets.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520%255Ctextsc%257BBProm%257D%252C%2520a%2520black-box%2520model-level%250Adetection%2520method%2520to%2520identify%2520backdoors%2520in%2520suspicious%2520models%252C%2520if%2520any.%250A%255Ctextsc%257BBProm%257D%2520leverages%2520the%2520low%2520classification%2520accuracy%2520of%2520prompted%2520models%250Awhen%2520backdoors%2520are%2520present.%2520Extensive%2520experiments%2520confirm%2520%255Ctextsc%257BBProm%257D%2527s%250Aeffectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20the%20Unseen%3A%20Detecting%20Hidden%20Backdoors%20in%20Black-Box%20Models&entry.906535625=Zi-Xuan%20Huang%20and%20Jia-Wei%20Chen%20and%20Zhi-Peng%20Zhang%20and%20Chia-Mu%20Yu&entry.1292438233=%20%20Visual%20prompting%20%28VP%29%20is%20a%20new%20technique%20that%20adapts%20well-trained%20frozen%0Amodels%20for%20source%20domain%20tasks%20to%20target%20domain%20tasks.%20This%20study%20examines%20VP%27s%0Abenefits%20for%20black-box%20model-level%20backdoor%20detection.%20The%20visual%20prompt%20in%20VP%0Amaps%20class%20subspaces%20between%20source%20and%20target%20domains.%20We%20identify%20a%0Amisalignment%2C%20termed%20class%20subspace%20inconsistency%2C%20between%20clean%20and%20poisoned%0Adatasets.%20Based%20on%20this%2C%20we%20introduce%20%5Ctextsc%7BBProm%7D%2C%20a%20black-box%20model-level%0Adetection%20method%20to%20identify%20backdoors%20in%20suspicious%20models%2C%20if%20any.%0A%5Ctextsc%7BBProm%7D%20leverages%20the%20low%20classification%20accuracy%20of%20prompted%20models%0Awhen%20backdoors%20are%20present.%20Extensive%20experiments%20confirm%20%5Ctextsc%7BBProm%7D%27s%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09540v1&entry.124074799=Read"},
{"title": "Stability and Generalization for Distributed SGDA", "author": "Miaoxi Zhu and Yan Sun and Li Shen and Bo Du and Dacheng Tao", "abstract": "  Minimax optimization is gaining increasing attention in modern machine\nlearning applications. Driven by large-scale models and massive volumes of data\ncollected from edge devices, as well as the concern to preserve client privacy,\ncommunication-efficient distributed minimax optimization algorithms become\npopular, such as Local Stochastic Gradient Descent Ascent (Local-SGDA), and\nLocal Decentralized SGDA (Local-DSGDA). While most existing research on\ndistributed minimax algorithms focuses on convergence rates, computation\ncomplexity, and communication efficiency, the generalization performance\nremains underdeveloped, whereas generalization ability is a pivotal indicator\nfor evaluating the holistic performance of a model when fed with unknown data.\nIn this paper, we propose the stability-based generalization analytical\nframework for Distributed-SGDA, which unifies two popular distributed minimax\nalgorithms including Local-SGDA and Local-DSGDA, and conduct a comprehensive\nanalysis of stability error, generalization gap, and population risk across\ndifferent metrics under various settings, e.g., (S)C-(S)C, PL-SC, and NC-NC\ncases. Our theoretical results reveal the trade-off between the generalization\ngap and optimization error and suggest hyperparameters choice to obtain the\noptimal population risk. Numerical experiments for Local-SGDA and Local-DSGDA\nvalidate the theoretical results.\n", "link": "http://arxiv.org/abs/2411.09365v1", "date": "2024-11-14", "relevancy": 1.9714, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4948}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4925}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20and%20Generalization%20for%20Distributed%20SGDA&body=Title%3A%20Stability%20and%20Generalization%20for%20Distributed%20SGDA%0AAuthor%3A%20Miaoxi%20Zhu%20and%20Yan%20Sun%20and%20Li%20Shen%20and%20Bo%20Du%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Minimax%20optimization%20is%20gaining%20increasing%20attention%20in%20modern%20machine%0Alearning%20applications.%20Driven%20by%20large-scale%20models%20and%20massive%20volumes%20of%20data%0Acollected%20from%20edge%20devices%2C%20as%20well%20as%20the%20concern%20to%20preserve%20client%20privacy%2C%0Acommunication-efficient%20distributed%20minimax%20optimization%20algorithms%20become%0Apopular%2C%20such%20as%20Local%20Stochastic%20Gradient%20Descent%20Ascent%20%28Local-SGDA%29%2C%20and%0ALocal%20Decentralized%20SGDA%20%28Local-DSGDA%29.%20While%20most%20existing%20research%20on%0Adistributed%20minimax%20algorithms%20focuses%20on%20convergence%20rates%2C%20computation%0Acomplexity%2C%20and%20communication%20efficiency%2C%20the%20generalization%20performance%0Aremains%20underdeveloped%2C%20whereas%20generalization%20ability%20is%20a%20pivotal%20indicator%0Afor%20evaluating%20the%20holistic%20performance%20of%20a%20model%20when%20fed%20with%20unknown%20data.%0AIn%20this%20paper%2C%20we%20propose%20the%20stability-based%20generalization%20analytical%0Aframework%20for%20Distributed-SGDA%2C%20which%20unifies%20two%20popular%20distributed%20minimax%0Aalgorithms%20including%20Local-SGDA%20and%20Local-DSGDA%2C%20and%20conduct%20a%20comprehensive%0Aanalysis%20of%20stability%20error%2C%20generalization%20gap%2C%20and%20population%20risk%20across%0Adifferent%20metrics%20under%20various%20settings%2C%20e.g.%2C%20%28S%29C-%28S%29C%2C%20PL-SC%2C%20and%20NC-NC%0Acases.%20Our%20theoretical%20results%20reveal%20the%20trade-off%20between%20the%20generalization%0Agap%20and%20optimization%20error%20and%20suggest%20hyperparameters%20choice%20to%20obtain%20the%0Aoptimal%20population%20risk.%20Numerical%20experiments%20for%20Local-SGDA%20and%20Local-DSGDA%0Avalidate%20the%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520and%2520Generalization%2520for%2520Distributed%2520SGDA%26entry.906535625%3DMiaoxi%2520Zhu%2520and%2520Yan%2520Sun%2520and%2520Li%2520Shen%2520and%2520Bo%2520Du%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Minimax%2520optimization%2520is%2520gaining%2520increasing%2520attention%2520in%2520modern%2520machine%250Alearning%2520applications.%2520Driven%2520by%2520large-scale%2520models%2520and%2520massive%2520volumes%2520of%2520data%250Acollected%2520from%2520edge%2520devices%252C%2520as%2520well%2520as%2520the%2520concern%2520to%2520preserve%2520client%2520privacy%252C%250Acommunication-efficient%2520distributed%2520minimax%2520optimization%2520algorithms%2520become%250Apopular%252C%2520such%2520as%2520Local%2520Stochastic%2520Gradient%2520Descent%2520Ascent%2520%2528Local-SGDA%2529%252C%2520and%250ALocal%2520Decentralized%2520SGDA%2520%2528Local-DSGDA%2529.%2520While%2520most%2520existing%2520research%2520on%250Adistributed%2520minimax%2520algorithms%2520focuses%2520on%2520convergence%2520rates%252C%2520computation%250Acomplexity%252C%2520and%2520communication%2520efficiency%252C%2520the%2520generalization%2520performance%250Aremains%2520underdeveloped%252C%2520whereas%2520generalization%2520ability%2520is%2520a%2520pivotal%2520indicator%250Afor%2520evaluating%2520the%2520holistic%2520performance%2520of%2520a%2520model%2520when%2520fed%2520with%2520unknown%2520data.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520the%2520stability-based%2520generalization%2520analytical%250Aframework%2520for%2520Distributed-SGDA%252C%2520which%2520unifies%2520two%2520popular%2520distributed%2520minimax%250Aalgorithms%2520including%2520Local-SGDA%2520and%2520Local-DSGDA%252C%2520and%2520conduct%2520a%2520comprehensive%250Aanalysis%2520of%2520stability%2520error%252C%2520generalization%2520gap%252C%2520and%2520population%2520risk%2520across%250Adifferent%2520metrics%2520under%2520various%2520settings%252C%2520e.g.%252C%2520%2528S%2529C-%2528S%2529C%252C%2520PL-SC%252C%2520and%2520NC-NC%250Acases.%2520Our%2520theoretical%2520results%2520reveal%2520the%2520trade-off%2520between%2520the%2520generalization%250Agap%2520and%2520optimization%2520error%2520and%2520suggest%2520hyperparameters%2520choice%2520to%2520obtain%2520the%250Aoptimal%2520population%2520risk.%2520Numerical%2520experiments%2520for%2520Local-SGDA%2520and%2520Local-DSGDA%250Avalidate%2520the%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20and%20Generalization%20for%20Distributed%20SGDA&entry.906535625=Miaoxi%20Zhu%20and%20Yan%20Sun%20and%20Li%20Shen%20and%20Bo%20Du%20and%20Dacheng%20Tao&entry.1292438233=%20%20Minimax%20optimization%20is%20gaining%20increasing%20attention%20in%20modern%20machine%0Alearning%20applications.%20Driven%20by%20large-scale%20models%20and%20massive%20volumes%20of%20data%0Acollected%20from%20edge%20devices%2C%20as%20well%20as%20the%20concern%20to%20preserve%20client%20privacy%2C%0Acommunication-efficient%20distributed%20minimax%20optimization%20algorithms%20become%0Apopular%2C%20such%20as%20Local%20Stochastic%20Gradient%20Descent%20Ascent%20%28Local-SGDA%29%2C%20and%0ALocal%20Decentralized%20SGDA%20%28Local-DSGDA%29.%20While%20most%20existing%20research%20on%0Adistributed%20minimax%20algorithms%20focuses%20on%20convergence%20rates%2C%20computation%0Acomplexity%2C%20and%20communication%20efficiency%2C%20the%20generalization%20performance%0Aremains%20underdeveloped%2C%20whereas%20generalization%20ability%20is%20a%20pivotal%20indicator%0Afor%20evaluating%20the%20holistic%20performance%20of%20a%20model%20when%20fed%20with%20unknown%20data.%0AIn%20this%20paper%2C%20we%20propose%20the%20stability-based%20generalization%20analytical%0Aframework%20for%20Distributed-SGDA%2C%20which%20unifies%20two%20popular%20distributed%20minimax%0Aalgorithms%20including%20Local-SGDA%20and%20Local-DSGDA%2C%20and%20conduct%20a%20comprehensive%0Aanalysis%20of%20stability%20error%2C%20generalization%20gap%2C%20and%20population%20risk%20across%0Adifferent%20metrics%20under%20various%20settings%2C%20e.g.%2C%20%28S%29C-%28S%29C%2C%20PL-SC%2C%20and%20NC-NC%0Acases.%20Our%20theoretical%20results%20reveal%20the%20trade-off%20between%20the%20generalization%0Agap%20and%20optimization%20error%20and%20suggest%20hyperparameters%20choice%20to%20obtain%20the%0Aoptimal%20population%20risk.%20Numerical%20experiments%20for%20Local-SGDA%20and%20Local-DSGDA%0Avalidate%20the%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09365v1&entry.124074799=Read"},
{"title": "A Practical Guide to Fine-tuning Language Models with Limited Data", "author": "M\u00e1rton Sz\u00e9p and Daniel Rueckert and R\u00fcdiger von Eisenhart-Rothe and Florian Hinterwimmer", "abstract": "  Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research.\n", "link": "http://arxiv.org/abs/2411.09539v1", "date": "2024-11-14", "relevancy": 1.9657, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practical%20Guide%20to%20Fine-tuning%20Language%20Models%20with%20Limited%20Data&body=Title%3A%20A%20Practical%20Guide%20to%20Fine-tuning%20Language%20Models%20with%20Limited%20Data%0AAuthor%3A%20M%C3%A1rton%20Sz%C3%A9p%20and%20Daniel%20Rueckert%20and%20R%C3%BCdiger%20von%20Eisenhart-Rothe%20and%20Florian%20Hinterwimmer%0AAbstract%3A%20%20%20Employing%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20the%20de%20facto%0Astandard%20in%20Natural%20Language%20Processing%20%28NLP%29%20despite%20their%20extensive%20data%0Arequirements.%20Motivated%20by%20the%20recent%20surge%20in%20research%20focused%20on%20training%0ALLMs%20with%20limited%20data%2C%20particularly%20in%20low-resource%20domains%20and%20languages%2C%0Athis%20paper%20surveys%20recent%20transfer%20learning%20approaches%20to%20optimize%20model%0Aperformance%20in%20downstream%20tasks%20where%20data%20is%20scarce.%20We%20first%20address%20initial%0Aand%20continued%20pre-training%20strategies%20to%20better%20leverage%20prior%20knowledge%20in%0Aunseen%20domains%20and%20languages.%20We%20then%20examine%20how%20to%20maximize%20the%20utility%20of%0Alimited%20data%20during%20fine-tuning%20and%20few-shot%20learning.%20The%20final%20section%20takes%0Aa%20task-specific%20perspective%2C%20reviewing%20models%20and%20methods%20suited%20for%20different%0Alevels%20of%20data%20scarcity.%20Our%20goal%20is%20to%20provide%20practitioners%20with%20practical%0Aguidelines%20for%20overcoming%20the%20challenges%20posed%20by%20constrained%20data%20while%20also%0Ahighlighting%20promising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practical%2520Guide%2520to%2520Fine-tuning%2520Language%2520Models%2520with%2520Limited%2520Data%26entry.906535625%3DM%25C3%25A1rton%2520Sz%25C3%25A9p%2520and%2520Daniel%2520Rueckert%2520and%2520R%25C3%25BCdiger%2520von%2520Eisenhart-Rothe%2520and%2520Florian%2520Hinterwimmer%26entry.1292438233%3D%2520%2520Employing%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520become%2520the%2520de%2520facto%250Astandard%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520despite%2520their%2520extensive%2520data%250Arequirements.%2520Motivated%2520by%2520the%2520recent%2520surge%2520in%2520research%2520focused%2520on%2520training%250ALLMs%2520with%2520limited%2520data%252C%2520particularly%2520in%2520low-resource%2520domains%2520and%2520languages%252C%250Athis%2520paper%2520surveys%2520recent%2520transfer%2520learning%2520approaches%2520to%2520optimize%2520model%250Aperformance%2520in%2520downstream%2520tasks%2520where%2520data%2520is%2520scarce.%2520We%2520first%2520address%2520initial%250Aand%2520continued%2520pre-training%2520strategies%2520to%2520better%2520leverage%2520prior%2520knowledge%2520in%250Aunseen%2520domains%2520and%2520languages.%2520We%2520then%2520examine%2520how%2520to%2520maximize%2520the%2520utility%2520of%250Alimited%2520data%2520during%2520fine-tuning%2520and%2520few-shot%2520learning.%2520The%2520final%2520section%2520takes%250Aa%2520task-specific%2520perspective%252C%2520reviewing%2520models%2520and%2520methods%2520suited%2520for%2520different%250Alevels%2520of%2520data%2520scarcity.%2520Our%2520goal%2520is%2520to%2520provide%2520practitioners%2520with%2520practical%250Aguidelines%2520for%2520overcoming%2520the%2520challenges%2520posed%2520by%2520constrained%2520data%2520while%2520also%250Ahighlighting%2520promising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practical%20Guide%20to%20Fine-tuning%20Language%20Models%20with%20Limited%20Data&entry.906535625=M%C3%A1rton%20Sz%C3%A9p%20and%20Daniel%20Rueckert%20and%20R%C3%BCdiger%20von%20Eisenhart-Rothe%20and%20Florian%20Hinterwimmer&entry.1292438233=%20%20Employing%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20the%20de%20facto%0Astandard%20in%20Natural%20Language%20Processing%20%28NLP%29%20despite%20their%20extensive%20data%0Arequirements.%20Motivated%20by%20the%20recent%20surge%20in%20research%20focused%20on%20training%0ALLMs%20with%20limited%20data%2C%20particularly%20in%20low-resource%20domains%20and%20languages%2C%0Athis%20paper%20surveys%20recent%20transfer%20learning%20approaches%20to%20optimize%20model%0Aperformance%20in%20downstream%20tasks%20where%20data%20is%20scarce.%20We%20first%20address%20initial%0Aand%20continued%20pre-training%20strategies%20to%20better%20leverage%20prior%20knowledge%20in%0Aunseen%20domains%20and%20languages.%20We%20then%20examine%20how%20to%20maximize%20the%20utility%20of%0Alimited%20data%20during%20fine-tuning%20and%20few-shot%20learning.%20The%20final%20section%20takes%0Aa%20task-specific%20perspective%2C%20reviewing%20models%20and%20methods%20suited%20for%20different%0Alevels%20of%20data%20scarcity.%20Our%20goal%20is%20to%20provide%20practitioners%20with%20practical%0Aguidelines%20for%20overcoming%20the%20challenges%20posed%20by%20constrained%20data%20while%20also%0Ahighlighting%20promising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09539v1&entry.124074799=Read"},
{"title": "SMILE-UHURA Challenge -- Small Vessel Segmentation at Mesoscopic Scale\n  from Ultra-High Resolution 7T Magnetic Resonance Angiograms", "author": "Soumick Chatterjee and Hendrik Mattern and Marc D\u00f6rner and Alessandro Sciarra and Florian Dubost and Hannes Schnurre and Rupali Khatun and Chun-Chih Yu and Tsung-Lin Hsieh and Yi-Shan Tsai and Yi-Zeng Fang and Yung-Ching Yang and Juinn-Dar Huang and Marshall Xu and Siyu Liu and Fernanda L. Ribeiro and Saskia Bollmann and Karthikesh Varma Chintalapati and Chethan Mysuru Radhakrishna and Sri Chandana Hudukula Ram Kumara and Raviteja Sutrave and Abdul Qayyum and Moona Mazher and Imran Razzak and Cristobal Rodero and Steven Niederren and Fengming Lin and Yan Xia and Jiacheng Wang and Riyu Qiu and Liansheng Wang and Arya Yazdan Panah and Rosana El Jurdi and Guanghui Fu and Janan Arslan and Ghislain Vaillant and Romain Valabregue and Didier Dormont and Bruno Stankoff and Olivier Colliot and Luisa Vargas and Isai Daniel Chac\u00f3n and Ioannis Pitsiorlas and Pablo Arbel\u00e1ez and Maria A. Zuluaga and Stefanie Schreiber and Oliver Speck and Andreas N\u00fcrnberger", "abstract": "  The human brain receives nutrients and oxygen through an intricate network of\nblood vessels. Pathology affecting small vessels, at the mesoscopic scale,\nrepresents a critical vulnerability within the cerebral blood supply and can\nlead to severe conditions, such as Cerebral Small Vessel Diseases. The advent\nof 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution\nimages, making it possible to visualise such vessels in the brain. However, the\nlack of publicly available annotated datasets has impeded the development of\nrobust, machine learning-driven segmentation algorithms. To address this, the\nSMILE-UHURA challenge was organised. This challenge, held in conjunction with\nthe ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform\nfor researchers working on related topics. The SMILE-UHURA challenge addresses\nthe gap in publicly available annotated datasets by providing an annotated\ndataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was\ncreated through a combination of automated pre-segmentation and extensive\nmanual refinement. In this manuscript, sixteen submitted methods and two\nbaseline methods are compared both quantitatively and qualitatively on two\ndifferent datasets: held-out test MRAs from the same dataset as the training\ndata (with labels kept secret) and a separate 7T ToF MRA dataset where both\ninput volumes and labels are kept secret. The results demonstrate that most of\nthe submitted deep learning methods, trained on the provided training dataset,\nachieved reliable segmentation performance. Dice scores reached up to 0.838\n$\\pm$ 0.066 and 0.716 $\\pm$ 0.125 on the respective datasets, with an average\nperformance of up to 0.804 $\\pm$ 0.15.\n", "link": "http://arxiv.org/abs/2411.09593v1", "date": "2024-11-14", "relevancy": 1.9523, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMILE-UHURA%20Challenge%20--%20Small%20Vessel%20Segmentation%20at%20Mesoscopic%20Scale%0A%20%20from%20Ultra-High%20Resolution%207T%20Magnetic%20Resonance%20Angiograms&body=Title%3A%20SMILE-UHURA%20Challenge%20--%20Small%20Vessel%20Segmentation%20at%20Mesoscopic%20Scale%0A%20%20from%20Ultra-High%20Resolution%207T%20Magnetic%20Resonance%20Angiograms%0AAuthor%3A%20Soumick%20Chatterjee%20and%20Hendrik%20Mattern%20and%20Marc%20D%C3%B6rner%20and%20Alessandro%20Sciarra%20and%20Florian%20Dubost%20and%20Hannes%20Schnurre%20and%20Rupali%20Khatun%20and%20Chun-Chih%20Yu%20and%20Tsung-Lin%20Hsieh%20and%20Yi-Shan%20Tsai%20and%20Yi-Zeng%20Fang%20and%20Yung-Ching%20Yang%20and%20Juinn-Dar%20Huang%20and%20Marshall%20Xu%20and%20Siyu%20Liu%20and%20Fernanda%20L.%20Ribeiro%20and%20Saskia%20Bollmann%20and%20Karthikesh%20Varma%20Chintalapati%20and%20Chethan%20Mysuru%20Radhakrishna%20and%20Sri%20Chandana%20Hudukula%20Ram%20Kumara%20and%20Raviteja%20Sutrave%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Imran%20Razzak%20and%20Cristobal%20Rodero%20and%20Steven%20Niederren%20and%20Fengming%20Lin%20and%20Yan%20Xia%20and%20Jiacheng%20Wang%20and%20Riyu%20Qiu%20and%20Liansheng%20Wang%20and%20Arya%20Yazdan%20Panah%20and%20Rosana%20El%20Jurdi%20and%20Guanghui%20Fu%20and%20Janan%20Arslan%20and%20Ghislain%20Vaillant%20and%20Romain%20Valabregue%20and%20Didier%20Dormont%20and%20Bruno%20Stankoff%20and%20Olivier%20Colliot%20and%20Luisa%20Vargas%20and%20Isai%20Daniel%20Chac%C3%B3n%20and%20Ioannis%20Pitsiorlas%20and%20Pablo%20Arbel%C3%A1ez%20and%20Maria%20A.%20Zuluaga%20and%20Stefanie%20Schreiber%20and%20Oliver%20Speck%20and%20Andreas%20N%C3%BCrnberger%0AAbstract%3A%20%20%20The%20human%20brain%20receives%20nutrients%20and%20oxygen%20through%20an%20intricate%20network%20of%0Ablood%20vessels.%20Pathology%20affecting%20small%20vessels%2C%20at%20the%20mesoscopic%20scale%2C%0Arepresents%20a%20critical%20vulnerability%20within%20the%20cerebral%20blood%20supply%20and%20can%0Alead%20to%20severe%20conditions%2C%20such%20as%20Cerebral%20Small%20Vessel%20Diseases.%20The%20advent%0Aof%207%20Tesla%20MRI%20systems%20has%20enabled%20the%20acquisition%20of%20higher%20spatial%20resolution%0Aimages%2C%20making%20it%20possible%20to%20visualise%20such%20vessels%20in%20the%20brain.%20However%2C%20the%0Alack%20of%20publicly%20available%20annotated%20datasets%20has%20impeded%20the%20development%20of%0Arobust%2C%20machine%20learning-driven%20segmentation%20algorithms.%20To%20address%20this%2C%20the%0ASMILE-UHURA%20challenge%20was%20organised.%20This%20challenge%2C%20held%20in%20conjunction%20with%0Athe%20ISBI%202023%2C%20in%20Cartagena%20de%20Indias%2C%20Colombia%2C%20aimed%20to%20provide%20a%20platform%0Afor%20researchers%20working%20on%20related%20topics.%20The%20SMILE-UHURA%20challenge%20addresses%0Athe%20gap%20in%20publicly%20available%20annotated%20datasets%20by%20providing%20an%20annotated%0Adataset%20of%20Time-of-Flight%20angiography%20acquired%20with%207T%20MRI.%20This%20dataset%20was%0Acreated%20through%20a%20combination%20of%20automated%20pre-segmentation%20and%20extensive%0Amanual%20refinement.%20In%20this%20manuscript%2C%20sixteen%20submitted%20methods%20and%20two%0Abaseline%20methods%20are%20compared%20both%20quantitatively%20and%20qualitatively%20on%20two%0Adifferent%20datasets%3A%20held-out%20test%20MRAs%20from%20the%20same%20dataset%20as%20the%20training%0Adata%20%28with%20labels%20kept%20secret%29%20and%20a%20separate%207T%20ToF%20MRA%20dataset%20where%20both%0Ainput%20volumes%20and%20labels%20are%20kept%20secret.%20The%20results%20demonstrate%20that%20most%20of%0Athe%20submitted%20deep%20learning%20methods%2C%20trained%20on%20the%20provided%20training%20dataset%2C%0Aachieved%20reliable%20segmentation%20performance.%20Dice%20scores%20reached%20up%20to%200.838%0A%24%5Cpm%24%200.066%20and%200.716%20%24%5Cpm%24%200.125%20on%20the%20respective%20datasets%2C%20with%20an%20average%0Aperformance%20of%20up%20to%200.804%20%24%5Cpm%24%200.15.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMILE-UHURA%2520Challenge%2520--%2520Small%2520Vessel%2520Segmentation%2520at%2520Mesoscopic%2520Scale%250A%2520%2520from%2520Ultra-High%2520Resolution%25207T%2520Magnetic%2520Resonance%2520Angiograms%26entry.906535625%3DSoumick%2520Chatterjee%2520and%2520Hendrik%2520Mattern%2520and%2520Marc%2520D%25C3%25B6rner%2520and%2520Alessandro%2520Sciarra%2520and%2520Florian%2520Dubost%2520and%2520Hannes%2520Schnurre%2520and%2520Rupali%2520Khatun%2520and%2520Chun-Chih%2520Yu%2520and%2520Tsung-Lin%2520Hsieh%2520and%2520Yi-Shan%2520Tsai%2520and%2520Yi-Zeng%2520Fang%2520and%2520Yung-Ching%2520Yang%2520and%2520Juinn-Dar%2520Huang%2520and%2520Marshall%2520Xu%2520and%2520Siyu%2520Liu%2520and%2520Fernanda%2520L.%2520Ribeiro%2520and%2520Saskia%2520Bollmann%2520and%2520Karthikesh%2520Varma%2520Chintalapati%2520and%2520Chethan%2520Mysuru%2520Radhakrishna%2520and%2520Sri%2520Chandana%2520Hudukula%2520Ram%2520Kumara%2520and%2520Raviteja%2520Sutrave%2520and%2520Abdul%2520Qayyum%2520and%2520Moona%2520Mazher%2520and%2520Imran%2520Razzak%2520and%2520Cristobal%2520Rodero%2520and%2520Steven%2520Niederren%2520and%2520Fengming%2520Lin%2520and%2520Yan%2520Xia%2520and%2520Jiacheng%2520Wang%2520and%2520Riyu%2520Qiu%2520and%2520Liansheng%2520Wang%2520and%2520Arya%2520Yazdan%2520Panah%2520and%2520Rosana%2520El%2520Jurdi%2520and%2520Guanghui%2520Fu%2520and%2520Janan%2520Arslan%2520and%2520Ghislain%2520Vaillant%2520and%2520Romain%2520Valabregue%2520and%2520Didier%2520Dormont%2520and%2520Bruno%2520Stankoff%2520and%2520Olivier%2520Colliot%2520and%2520Luisa%2520Vargas%2520and%2520Isai%2520Daniel%2520Chac%25C3%25B3n%2520and%2520Ioannis%2520Pitsiorlas%2520and%2520Pablo%2520Arbel%25C3%25A1ez%2520and%2520Maria%2520A.%2520Zuluaga%2520and%2520Stefanie%2520Schreiber%2520and%2520Oliver%2520Speck%2520and%2520Andreas%2520N%25C3%25BCrnberger%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520receives%2520nutrients%2520and%2520oxygen%2520through%2520an%2520intricate%2520network%2520of%250Ablood%2520vessels.%2520Pathology%2520affecting%2520small%2520vessels%252C%2520at%2520the%2520mesoscopic%2520scale%252C%250Arepresents%2520a%2520critical%2520vulnerability%2520within%2520the%2520cerebral%2520blood%2520supply%2520and%2520can%250Alead%2520to%2520severe%2520conditions%252C%2520such%2520as%2520Cerebral%2520Small%2520Vessel%2520Diseases.%2520The%2520advent%250Aof%25207%2520Tesla%2520MRI%2520systems%2520has%2520enabled%2520the%2520acquisition%2520of%2520higher%2520spatial%2520resolution%250Aimages%252C%2520making%2520it%2520possible%2520to%2520visualise%2520such%2520vessels%2520in%2520the%2520brain.%2520However%252C%2520the%250Alack%2520of%2520publicly%2520available%2520annotated%2520datasets%2520has%2520impeded%2520the%2520development%2520of%250Arobust%252C%2520machine%2520learning-driven%2520segmentation%2520algorithms.%2520To%2520address%2520this%252C%2520the%250ASMILE-UHURA%2520challenge%2520was%2520organised.%2520This%2520challenge%252C%2520held%2520in%2520conjunction%2520with%250Athe%2520ISBI%25202023%252C%2520in%2520Cartagena%2520de%2520Indias%252C%2520Colombia%252C%2520aimed%2520to%2520provide%2520a%2520platform%250Afor%2520researchers%2520working%2520on%2520related%2520topics.%2520The%2520SMILE-UHURA%2520challenge%2520addresses%250Athe%2520gap%2520in%2520publicly%2520available%2520annotated%2520datasets%2520by%2520providing%2520an%2520annotated%250Adataset%2520of%2520Time-of-Flight%2520angiography%2520acquired%2520with%25207T%2520MRI.%2520This%2520dataset%2520was%250Acreated%2520through%2520a%2520combination%2520of%2520automated%2520pre-segmentation%2520and%2520extensive%250Amanual%2520refinement.%2520In%2520this%2520manuscript%252C%2520sixteen%2520submitted%2520methods%2520and%2520two%250Abaseline%2520methods%2520are%2520compared%2520both%2520quantitatively%2520and%2520qualitatively%2520on%2520two%250Adifferent%2520datasets%253A%2520held-out%2520test%2520MRAs%2520from%2520the%2520same%2520dataset%2520as%2520the%2520training%250Adata%2520%2528with%2520labels%2520kept%2520secret%2529%2520and%2520a%2520separate%25207T%2520ToF%2520MRA%2520dataset%2520where%2520both%250Ainput%2520volumes%2520and%2520labels%2520are%2520kept%2520secret.%2520The%2520results%2520demonstrate%2520that%2520most%2520of%250Athe%2520submitted%2520deep%2520learning%2520methods%252C%2520trained%2520on%2520the%2520provided%2520training%2520dataset%252C%250Aachieved%2520reliable%2520segmentation%2520performance.%2520Dice%2520scores%2520reached%2520up%2520to%25200.838%250A%2524%255Cpm%2524%25200.066%2520and%25200.716%2520%2524%255Cpm%2524%25200.125%2520on%2520the%2520respective%2520datasets%252C%2520with%2520an%2520average%250Aperformance%2520of%2520up%2520to%25200.804%2520%2524%255Cpm%2524%25200.15.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMILE-UHURA%20Challenge%20--%20Small%20Vessel%20Segmentation%20at%20Mesoscopic%20Scale%0A%20%20from%20Ultra-High%20Resolution%207T%20Magnetic%20Resonance%20Angiograms&entry.906535625=Soumick%20Chatterjee%20and%20Hendrik%20Mattern%20and%20Marc%20D%C3%B6rner%20and%20Alessandro%20Sciarra%20and%20Florian%20Dubost%20and%20Hannes%20Schnurre%20and%20Rupali%20Khatun%20and%20Chun-Chih%20Yu%20and%20Tsung-Lin%20Hsieh%20and%20Yi-Shan%20Tsai%20and%20Yi-Zeng%20Fang%20and%20Yung-Ching%20Yang%20and%20Juinn-Dar%20Huang%20and%20Marshall%20Xu%20and%20Siyu%20Liu%20and%20Fernanda%20L.%20Ribeiro%20and%20Saskia%20Bollmann%20and%20Karthikesh%20Varma%20Chintalapati%20and%20Chethan%20Mysuru%20Radhakrishna%20and%20Sri%20Chandana%20Hudukula%20Ram%20Kumara%20and%20Raviteja%20Sutrave%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Imran%20Razzak%20and%20Cristobal%20Rodero%20and%20Steven%20Niederren%20and%20Fengming%20Lin%20and%20Yan%20Xia%20and%20Jiacheng%20Wang%20and%20Riyu%20Qiu%20and%20Liansheng%20Wang%20and%20Arya%20Yazdan%20Panah%20and%20Rosana%20El%20Jurdi%20and%20Guanghui%20Fu%20and%20Janan%20Arslan%20and%20Ghislain%20Vaillant%20and%20Romain%20Valabregue%20and%20Didier%20Dormont%20and%20Bruno%20Stankoff%20and%20Olivier%20Colliot%20and%20Luisa%20Vargas%20and%20Isai%20Daniel%20Chac%C3%B3n%20and%20Ioannis%20Pitsiorlas%20and%20Pablo%20Arbel%C3%A1ez%20and%20Maria%20A.%20Zuluaga%20and%20Stefanie%20Schreiber%20and%20Oliver%20Speck%20and%20Andreas%20N%C3%BCrnberger&entry.1292438233=%20%20The%20human%20brain%20receives%20nutrients%20and%20oxygen%20through%20an%20intricate%20network%20of%0Ablood%20vessels.%20Pathology%20affecting%20small%20vessels%2C%20at%20the%20mesoscopic%20scale%2C%0Arepresents%20a%20critical%20vulnerability%20within%20the%20cerebral%20blood%20supply%20and%20can%0Alead%20to%20severe%20conditions%2C%20such%20as%20Cerebral%20Small%20Vessel%20Diseases.%20The%20advent%0Aof%207%20Tesla%20MRI%20systems%20has%20enabled%20the%20acquisition%20of%20higher%20spatial%20resolution%0Aimages%2C%20making%20it%20possible%20to%20visualise%20such%20vessels%20in%20the%20brain.%20However%2C%20the%0Alack%20of%20publicly%20available%20annotated%20datasets%20has%20impeded%20the%20development%20of%0Arobust%2C%20machine%20learning-driven%20segmentation%20algorithms.%20To%20address%20this%2C%20the%0ASMILE-UHURA%20challenge%20was%20organised.%20This%20challenge%2C%20held%20in%20conjunction%20with%0Athe%20ISBI%202023%2C%20in%20Cartagena%20de%20Indias%2C%20Colombia%2C%20aimed%20to%20provide%20a%20platform%0Afor%20researchers%20working%20on%20related%20topics.%20The%20SMILE-UHURA%20challenge%20addresses%0Athe%20gap%20in%20publicly%20available%20annotated%20datasets%20by%20providing%20an%20annotated%0Adataset%20of%20Time-of-Flight%20angiography%20acquired%20with%207T%20MRI.%20This%20dataset%20was%0Acreated%20through%20a%20combination%20of%20automated%20pre-segmentation%20and%20extensive%0Amanual%20refinement.%20In%20this%20manuscript%2C%20sixteen%20submitted%20methods%20and%20two%0Abaseline%20methods%20are%20compared%20both%20quantitatively%20and%20qualitatively%20on%20two%0Adifferent%20datasets%3A%20held-out%20test%20MRAs%20from%20the%20same%20dataset%20as%20the%20training%0Adata%20%28with%20labels%20kept%20secret%29%20and%20a%20separate%207T%20ToF%20MRA%20dataset%20where%20both%0Ainput%20volumes%20and%20labels%20are%20kept%20secret.%20The%20results%20demonstrate%20that%20most%20of%0Athe%20submitted%20deep%20learning%20methods%2C%20trained%20on%20the%20provided%20training%20dataset%2C%0Aachieved%20reliable%20segmentation%20performance.%20Dice%20scores%20reached%20up%20to%200.838%0A%24%5Cpm%24%200.066%20and%200.716%20%24%5Cpm%24%200.125%20on%20the%20respective%20datasets%2C%20with%20an%20average%0Aperformance%20of%20up%20to%200.804%20%24%5Cpm%24%200.15.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09593v1&entry.124074799=Read"},
{"title": "SINETRA: a Versatile Framework for Evaluating Single Neuron Tracking in\n  Behaving Animals", "author": "Raphael Reme and Alasdair Newson and Elsa Angelini and Jean-Christophe Olivo-Marin and Thibault Lagach", "abstract": "  Accurately tracking neuronal activity in behaving animals presents\nsignificant challenges due to complex motions and background noise. The lack of\nannotated datasets limits the evaluation and improvement of such tracking\nalgorithms. To address this, we developed SINETRA, a versatile simulator that\ngenerates synthetic tracking data for particles on a deformable background,\nclosely mimicking live animal recordings. This simulator produces annotated 2D\nand 3D videos that reflect the intricate movements seen in behaving animals\nlike Hydra Vulgaris. We evaluated four state-of-the-art tracking algorithms\nhighlighting the current limitations of these methods in challenging scenarios\nand paving the way for improved cell tracking techniques in dynamic biological\nsystems.\n", "link": "http://arxiv.org/abs/2411.09462v1", "date": "2024-11-14", "relevancy": 1.9498, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4897}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4897}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINETRA%3A%20a%20Versatile%20Framework%20for%20Evaluating%20Single%20Neuron%20Tracking%20in%0A%20%20Behaving%20Animals&body=Title%3A%20SINETRA%3A%20a%20Versatile%20Framework%20for%20Evaluating%20Single%20Neuron%20Tracking%20in%0A%20%20Behaving%20Animals%0AAuthor%3A%20Raphael%20Reme%20and%20Alasdair%20Newson%20and%20Elsa%20Angelini%20and%20Jean-Christophe%20Olivo-Marin%20and%20Thibault%20Lagach%0AAbstract%3A%20%20%20Accurately%20tracking%20neuronal%20activity%20in%20behaving%20animals%20presents%0Asignificant%20challenges%20due%20to%20complex%20motions%20and%20background%20noise.%20The%20lack%20of%0Aannotated%20datasets%20limits%20the%20evaluation%20and%20improvement%20of%20such%20tracking%0Aalgorithms.%20To%20address%20this%2C%20we%20developed%20SINETRA%2C%20a%20versatile%20simulator%20that%0Agenerates%20synthetic%20tracking%20data%20for%20particles%20on%20a%20deformable%20background%2C%0Aclosely%20mimicking%20live%20animal%20recordings.%20This%20simulator%20produces%20annotated%202D%0Aand%203D%20videos%20that%20reflect%20the%20intricate%20movements%20seen%20in%20behaving%20animals%0Alike%20Hydra%20Vulgaris.%20We%20evaluated%20four%20state-of-the-art%20tracking%20algorithms%0Ahighlighting%20the%20current%20limitations%20of%20these%20methods%20in%20challenging%20scenarios%0Aand%20paving%20the%20way%20for%20improved%20cell%20tracking%20techniques%20in%20dynamic%20biological%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINETRA%253A%2520a%2520Versatile%2520Framework%2520for%2520Evaluating%2520Single%2520Neuron%2520Tracking%2520in%250A%2520%2520Behaving%2520Animals%26entry.906535625%3DRaphael%2520Reme%2520and%2520Alasdair%2520Newson%2520and%2520Elsa%2520Angelini%2520and%2520Jean-Christophe%2520Olivo-Marin%2520and%2520Thibault%2520Lagach%26entry.1292438233%3D%2520%2520Accurately%2520tracking%2520neuronal%2520activity%2520in%2520behaving%2520animals%2520presents%250Asignificant%2520challenges%2520due%2520to%2520complex%2520motions%2520and%2520background%2520noise.%2520The%2520lack%2520of%250Aannotated%2520datasets%2520limits%2520the%2520evaluation%2520and%2520improvement%2520of%2520such%2520tracking%250Aalgorithms.%2520To%2520address%2520this%252C%2520we%2520developed%2520SINETRA%252C%2520a%2520versatile%2520simulator%2520that%250Agenerates%2520synthetic%2520tracking%2520data%2520for%2520particles%2520on%2520a%2520deformable%2520background%252C%250Aclosely%2520mimicking%2520live%2520animal%2520recordings.%2520This%2520simulator%2520produces%2520annotated%25202D%250Aand%25203D%2520videos%2520that%2520reflect%2520the%2520intricate%2520movements%2520seen%2520in%2520behaving%2520animals%250Alike%2520Hydra%2520Vulgaris.%2520We%2520evaluated%2520four%2520state-of-the-art%2520tracking%2520algorithms%250Ahighlighting%2520the%2520current%2520limitations%2520of%2520these%2520methods%2520in%2520challenging%2520scenarios%250Aand%2520paving%2520the%2520way%2520for%2520improved%2520cell%2520tracking%2520techniques%2520in%2520dynamic%2520biological%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINETRA%3A%20a%20Versatile%20Framework%20for%20Evaluating%20Single%20Neuron%20Tracking%20in%0A%20%20Behaving%20Animals&entry.906535625=Raphael%20Reme%20and%20Alasdair%20Newson%20and%20Elsa%20Angelini%20and%20Jean-Christophe%20Olivo-Marin%20and%20Thibault%20Lagach&entry.1292438233=%20%20Accurately%20tracking%20neuronal%20activity%20in%20behaving%20animals%20presents%0Asignificant%20challenges%20due%20to%20complex%20motions%20and%20background%20noise.%20The%20lack%20of%0Aannotated%20datasets%20limits%20the%20evaluation%20and%20improvement%20of%20such%20tracking%0Aalgorithms.%20To%20address%20this%2C%20we%20developed%20SINETRA%2C%20a%20versatile%20simulator%20that%0Agenerates%20synthetic%20tracking%20data%20for%20particles%20on%20a%20deformable%20background%2C%0Aclosely%20mimicking%20live%20animal%20recordings.%20This%20simulator%20produces%20annotated%202D%0Aand%203D%20videos%20that%20reflect%20the%20intricate%20movements%20seen%20in%20behaving%20animals%0Alike%20Hydra%20Vulgaris.%20We%20evaluated%20four%20state-of-the-art%20tracking%20algorithms%0Ahighlighting%20the%20current%20limitations%20of%20these%20methods%20in%20challenging%20scenarios%0Aand%20paving%20the%20way%20for%20improved%20cell%20tracking%20techniques%20in%20dynamic%20biological%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09462v1&entry.124074799=Read"},
{"title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs", "author": "Mengyuan Zhang and Ruihui Wang and Bo Xia and Yuan Sun and Xiaobing Zhao", "abstract": "  Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.\n", "link": "http://arxiv.org/abs/2411.09492v1", "date": "2024-11-14", "relevancy": 1.9449, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Eval%3A%20A%20Hierarchical%20Benchmark%20for%20Modern%20Mongolian%20Evaluation%20in%0A%20%20LLMs&body=Title%3A%20MM-Eval%3A%20A%20Hierarchical%20Benchmark%20for%20Modern%20Mongolian%20Evaluation%20in%0A%20%20LLMs%0AAuthor%3A%20Mengyuan%20Zhang%20and%20Ruihui%20Wang%20and%20Bo%20Xia%20and%20Yuan%20Sun%20and%20Xiaobing%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20high-resource%20languages%20but%20face%0Anotable%20challenges%20in%20low-resource%20languages%20like%20Mongolian.%20This%20paper%0Aaddresses%20these%20challenges%20by%20categorizing%20capabilities%20into%20language%20abilities%0A%28syntax%20and%20semantics%29%20and%20cognitive%20abilities%20%28knowledge%20and%20reasoning%29.%20To%0Asystematically%20evaluate%20these%20areas%2C%20we%20developed%20MM-Eval%2C%20a%20specialized%0Adataset%20based%20on%20Modern%20Mongolian%20Language%20Textbook%20I%20and%20enriched%20with%20WebQSP%0Aand%20MGSM%20datasets.%0A%20%20Preliminary%20experiments%20on%20models%20including%20Qwen2-7B-Instruct%2C%20GLM4-9b-chat%2C%0ALlama3.1-8B-Instruct%2C%20GPT-4%2C%20and%20DeepseekV2.5%20revealed%20that%3A%201%29%20all%20models%0Aperformed%20better%20on%20syntactic%20tasks%20than%20semantic%20tasks%2C%20highlighting%20a%20gap%20in%0Adeeper%20language%20understanding%3B%20and%202%29%20knowledge%20tasks%20showed%20a%20moderate%0Adecline%2C%20suggesting%20that%20models%20can%20transfer%20general%20knowledge%20from%0Ahigh-resource%20to%20low-resource%20contexts.%0A%20%20The%20release%20of%20MM-Eval%2C%20comprising%20569%20syntax%2C%20677%20semantics%2C%20344%20knowledge%2C%0Aand%20250%20reasoning%20tasks%2C%20offers%20valuable%20insights%20for%20advancing%20NLP%20and%20LLMs%20in%0Alow-resource%20languages%20like%20Mongolian.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/joenahm/MM-Eval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Eval%253A%2520A%2520Hierarchical%2520Benchmark%2520for%2520Modern%2520Mongolian%2520Evaluation%2520in%250A%2520%2520LLMs%26entry.906535625%3DMengyuan%2520Zhang%2520and%2520Ruihui%2520Wang%2520and%2520Bo%2520Xia%2520and%2520Yuan%2520Sun%2520and%2520Xiaobing%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520high-resource%2520languages%2520but%2520face%250Anotable%2520challenges%2520in%2520low-resource%2520languages%2520like%2520Mongolian.%2520This%2520paper%250Aaddresses%2520these%2520challenges%2520by%2520categorizing%2520capabilities%2520into%2520language%2520abilities%250A%2528syntax%2520and%2520semantics%2529%2520and%2520cognitive%2520abilities%2520%2528knowledge%2520and%2520reasoning%2529.%2520To%250Asystematically%2520evaluate%2520these%2520areas%252C%2520we%2520developed%2520MM-Eval%252C%2520a%2520specialized%250Adataset%2520based%2520on%2520Modern%2520Mongolian%2520Language%2520Textbook%2520I%2520and%2520enriched%2520with%2520WebQSP%250Aand%2520MGSM%2520datasets.%250A%2520%2520Preliminary%2520experiments%2520on%2520models%2520including%2520Qwen2-7B-Instruct%252C%2520GLM4-9b-chat%252C%250ALlama3.1-8B-Instruct%252C%2520GPT-4%252C%2520and%2520DeepseekV2.5%2520revealed%2520that%253A%25201%2529%2520all%2520models%250Aperformed%2520better%2520on%2520syntactic%2520tasks%2520than%2520semantic%2520tasks%252C%2520highlighting%2520a%2520gap%2520in%250Adeeper%2520language%2520understanding%253B%2520and%25202%2529%2520knowledge%2520tasks%2520showed%2520a%2520moderate%250Adecline%252C%2520suggesting%2520that%2520models%2520can%2520transfer%2520general%2520knowledge%2520from%250Ahigh-resource%2520to%2520low-resource%2520contexts.%250A%2520%2520The%2520release%2520of%2520MM-Eval%252C%2520comprising%2520569%2520syntax%252C%2520677%2520semantics%252C%2520344%2520knowledge%252C%250Aand%2520250%2520reasoning%2520tasks%252C%2520offers%2520valuable%2520insights%2520for%2520advancing%2520NLP%2520and%2520LLMs%2520in%250Alow-resource%2520languages%2520like%2520Mongolian.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//github.com/joenahm/MM-Eval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Eval%3A%20A%20Hierarchical%20Benchmark%20for%20Modern%20Mongolian%20Evaluation%20in%0A%20%20LLMs&entry.906535625=Mengyuan%20Zhang%20and%20Ruihui%20Wang%20and%20Bo%20Xia%20and%20Yuan%20Sun%20and%20Xiaobing%20Zhao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20high-resource%20languages%20but%20face%0Anotable%20challenges%20in%20low-resource%20languages%20like%20Mongolian.%20This%20paper%0Aaddresses%20these%20challenges%20by%20categorizing%20capabilities%20into%20language%20abilities%0A%28syntax%20and%20semantics%29%20and%20cognitive%20abilities%20%28knowledge%20and%20reasoning%29.%20To%0Asystematically%20evaluate%20these%20areas%2C%20we%20developed%20MM-Eval%2C%20a%20specialized%0Adataset%20based%20on%20Modern%20Mongolian%20Language%20Textbook%20I%20and%20enriched%20with%20WebQSP%0Aand%20MGSM%20datasets.%0A%20%20Preliminary%20experiments%20on%20models%20including%20Qwen2-7B-Instruct%2C%20GLM4-9b-chat%2C%0ALlama3.1-8B-Instruct%2C%20GPT-4%2C%20and%20DeepseekV2.5%20revealed%20that%3A%201%29%20all%20models%0Aperformed%20better%20on%20syntactic%20tasks%20than%20semantic%20tasks%2C%20highlighting%20a%20gap%20in%0Adeeper%20language%20understanding%3B%20and%202%29%20knowledge%20tasks%20showed%20a%20moderate%0Adecline%2C%20suggesting%20that%20models%20can%20transfer%20general%20knowledge%20from%0Ahigh-resource%20to%20low-resource%20contexts.%0A%20%20The%20release%20of%20MM-Eval%2C%20comprising%20569%20syntax%2C%20677%20semantics%2C%20344%20knowledge%2C%0Aand%20250%20reasoning%20tasks%2C%20offers%20valuable%20insights%20for%20advancing%20NLP%20and%20LLMs%20in%0Alow-resource%20languages%20like%20Mongolian.%20The%20dataset%20is%20available%20at%0Ahttps%3A//github.com/joenahm/MM-Eval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09492v1&entry.124074799=Read"},
{"title": "OpenGeMM: A High-Utilization GeMM Accelerator Generator with Lightweight\n  RISC-V Control and Tight Memory Coupling", "author": "Xiaoling Yi and Ryan Antonio and Joren Dumoulin and Jiacong Sun and Josse Van Delm and Guilherme Paim and Marian Verhelst", "abstract": "  Deep neural networks (DNNs) face significant challenges when deployed on\nresource-constrained extreme edge devices due to their computational and\ndata-intensive nature. While standalone accelerators tailored for specific\napplication scenarios suffer from inflexible control and limited\nprogrammability, generic hardware acceleration platforms coupled with RISC-V\nCPUs can enable high reusability and flexibility, yet typically at the expense\nof system level efficiency and low utilization. To fill this gap, we propose\nOpenGeMM, an open-source acceleration platform, jointly demonstrating high\nefficiency and utilization, as well as ease of configurability and\nprogrammability. OpenGeMM encompasses a parameterized Chisel-coded GeMM\naccelerator, a lightweight RISC-V processor, and a tightly coupled multi-banked\nscratchpad memory. The GeMM core utilization and system efficiency are boosted\nthrough three mechanisms: configuration pre-loading, input pre-fetching with\noutput buffering, and programmable strided memory access. Experimental results\nshow that OpenGeMM can consistently achieve hardware utilization ranging from\n81.89% to 99.34% across diverse CNN and Transformer workloads. Compared to the\nSotA open-source Gemmini accelerator, OpenGeMM demonstrates a 3.58x to 16.40x\nspeedup on normalized throughput across a wide variety ofGeMM workloads, while\nachieving 4.68 TOPS/W system efficiency.\n", "link": "http://arxiv.org/abs/2411.09543v1", "date": "2024-11-14", "relevancy": 1.9432, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4955}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4917}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenGeMM%3A%20A%20High-Utilization%20GeMM%20Accelerator%20Generator%20with%20Lightweight%0A%20%20RISC-V%20Control%20and%20Tight%20Memory%20Coupling&body=Title%3A%20OpenGeMM%3A%20A%20High-Utilization%20GeMM%20Accelerator%20Generator%20with%20Lightweight%0A%20%20RISC-V%20Control%20and%20Tight%20Memory%20Coupling%0AAuthor%3A%20Xiaoling%20Yi%20and%20Ryan%20Antonio%20and%20Joren%20Dumoulin%20and%20Jiacong%20Sun%20and%20Josse%20Van%20Delm%20and%20Guilherme%20Paim%20and%20Marian%20Verhelst%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20face%20significant%20challenges%20when%20deployed%20on%0Aresource-constrained%20extreme%20edge%20devices%20due%20to%20their%20computational%20and%0Adata-intensive%20nature.%20While%20standalone%20accelerators%20tailored%20for%20specific%0Aapplication%20scenarios%20suffer%20from%20inflexible%20control%20and%20limited%0Aprogrammability%2C%20generic%20hardware%20acceleration%20platforms%20coupled%20with%20RISC-V%0ACPUs%20can%20enable%20high%20reusability%20and%20flexibility%2C%20yet%20typically%20at%20the%20expense%0Aof%20system%20level%20efficiency%20and%20low%20utilization.%20To%20fill%20this%20gap%2C%20we%20propose%0AOpenGeMM%2C%20an%20open-source%20acceleration%20platform%2C%20jointly%20demonstrating%20high%0Aefficiency%20and%20utilization%2C%20as%20well%20as%20ease%20of%20configurability%20and%0Aprogrammability.%20OpenGeMM%20encompasses%20a%20parameterized%20Chisel-coded%20GeMM%0Aaccelerator%2C%20a%20lightweight%20RISC-V%20processor%2C%20and%20a%20tightly%20coupled%20multi-banked%0Ascratchpad%20memory.%20The%20GeMM%20core%20utilization%20and%20system%20efficiency%20are%20boosted%0Athrough%20three%20mechanisms%3A%20configuration%20pre-loading%2C%20input%20pre-fetching%20with%0Aoutput%20buffering%2C%20and%20programmable%20strided%20memory%20access.%20Experimental%20results%0Ashow%20that%20OpenGeMM%20can%20consistently%20achieve%20hardware%20utilization%20ranging%20from%0A81.89%25%20to%2099.34%25%20across%20diverse%20CNN%20and%20Transformer%20workloads.%20Compared%20to%20the%0ASotA%20open-source%20Gemmini%20accelerator%2C%20OpenGeMM%20demonstrates%20a%203.58x%20to%2016.40x%0Aspeedup%20on%20normalized%20throughput%20across%20a%20wide%20variety%20ofGeMM%20workloads%2C%20while%0Aachieving%204.68%20TOPS/W%20system%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenGeMM%253A%2520A%2520High-Utilization%2520GeMM%2520Accelerator%2520Generator%2520with%2520Lightweight%250A%2520%2520RISC-V%2520Control%2520and%2520Tight%2520Memory%2520Coupling%26entry.906535625%3DXiaoling%2520Yi%2520and%2520Ryan%2520Antonio%2520and%2520Joren%2520Dumoulin%2520and%2520Jiacong%2520Sun%2520and%2520Josse%2520Van%2520Delm%2520and%2520Guilherme%2520Paim%2520and%2520Marian%2520Verhelst%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520face%2520significant%2520challenges%2520when%2520deployed%2520on%250Aresource-constrained%2520extreme%2520edge%2520devices%2520due%2520to%2520their%2520computational%2520and%250Adata-intensive%2520nature.%2520While%2520standalone%2520accelerators%2520tailored%2520for%2520specific%250Aapplication%2520scenarios%2520suffer%2520from%2520inflexible%2520control%2520and%2520limited%250Aprogrammability%252C%2520generic%2520hardware%2520acceleration%2520platforms%2520coupled%2520with%2520RISC-V%250ACPUs%2520can%2520enable%2520high%2520reusability%2520and%2520flexibility%252C%2520yet%2520typically%2520at%2520the%2520expense%250Aof%2520system%2520level%2520efficiency%2520and%2520low%2520utilization.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%250AOpenGeMM%252C%2520an%2520open-source%2520acceleration%2520platform%252C%2520jointly%2520demonstrating%2520high%250Aefficiency%2520and%2520utilization%252C%2520as%2520well%2520as%2520ease%2520of%2520configurability%2520and%250Aprogrammability.%2520OpenGeMM%2520encompasses%2520a%2520parameterized%2520Chisel-coded%2520GeMM%250Aaccelerator%252C%2520a%2520lightweight%2520RISC-V%2520processor%252C%2520and%2520a%2520tightly%2520coupled%2520multi-banked%250Ascratchpad%2520memory.%2520The%2520GeMM%2520core%2520utilization%2520and%2520system%2520efficiency%2520are%2520boosted%250Athrough%2520three%2520mechanisms%253A%2520configuration%2520pre-loading%252C%2520input%2520pre-fetching%2520with%250Aoutput%2520buffering%252C%2520and%2520programmable%2520strided%2520memory%2520access.%2520Experimental%2520results%250Ashow%2520that%2520OpenGeMM%2520can%2520consistently%2520achieve%2520hardware%2520utilization%2520ranging%2520from%250A81.89%2525%2520to%252099.34%2525%2520across%2520diverse%2520CNN%2520and%2520Transformer%2520workloads.%2520Compared%2520to%2520the%250ASotA%2520open-source%2520Gemmini%2520accelerator%252C%2520OpenGeMM%2520demonstrates%2520a%25203.58x%2520to%252016.40x%250Aspeedup%2520on%2520normalized%2520throughput%2520across%2520a%2520wide%2520variety%2520ofGeMM%2520workloads%252C%2520while%250Aachieving%25204.68%2520TOPS/W%2520system%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenGeMM%3A%20A%20High-Utilization%20GeMM%20Accelerator%20Generator%20with%20Lightweight%0A%20%20RISC-V%20Control%20and%20Tight%20Memory%20Coupling&entry.906535625=Xiaoling%20Yi%20and%20Ryan%20Antonio%20and%20Joren%20Dumoulin%20and%20Jiacong%20Sun%20and%20Josse%20Van%20Delm%20and%20Guilherme%20Paim%20and%20Marian%20Verhelst&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20face%20significant%20challenges%20when%20deployed%20on%0Aresource-constrained%20extreme%20edge%20devices%20due%20to%20their%20computational%20and%0Adata-intensive%20nature.%20While%20standalone%20accelerators%20tailored%20for%20specific%0Aapplication%20scenarios%20suffer%20from%20inflexible%20control%20and%20limited%0Aprogrammability%2C%20generic%20hardware%20acceleration%20platforms%20coupled%20with%20RISC-V%0ACPUs%20can%20enable%20high%20reusability%20and%20flexibility%2C%20yet%20typically%20at%20the%20expense%0Aof%20system%20level%20efficiency%20and%20low%20utilization.%20To%20fill%20this%20gap%2C%20we%20propose%0AOpenGeMM%2C%20an%20open-source%20acceleration%20platform%2C%20jointly%20demonstrating%20high%0Aefficiency%20and%20utilization%2C%20as%20well%20as%20ease%20of%20configurability%20and%0Aprogrammability.%20OpenGeMM%20encompasses%20a%20parameterized%20Chisel-coded%20GeMM%0Aaccelerator%2C%20a%20lightweight%20RISC-V%20processor%2C%20and%20a%20tightly%20coupled%20multi-banked%0Ascratchpad%20memory.%20The%20GeMM%20core%20utilization%20and%20system%20efficiency%20are%20boosted%0Athrough%20three%20mechanisms%3A%20configuration%20pre-loading%2C%20input%20pre-fetching%20with%0Aoutput%20buffering%2C%20and%20programmable%20strided%20memory%20access.%20Experimental%20results%0Ashow%20that%20OpenGeMM%20can%20consistently%20achieve%20hardware%20utilization%20ranging%20from%0A81.89%25%20to%2099.34%25%20across%20diverse%20CNN%20and%20Transformer%20workloads.%20Compared%20to%20the%0ASotA%20open-source%20Gemmini%20accelerator%2C%20OpenGeMM%20demonstrates%20a%203.58x%20to%2016.40x%0Aspeedup%20on%20normalized%20throughput%20across%20a%20wide%20variety%20ofGeMM%20workloads%2C%20while%0Aachieving%204.68%20TOPS/W%20system%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09543v1&entry.124074799=Read"},
{"title": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method", "author": "Guoqing Zhang and Keita Fukuyama and Kazumasa Kishimoto and Tomohiro Kuroda", "abstract": "  Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility.\n", "link": "http://arxiv.org/abs/2411.08586v2", "date": "2024-11-14", "relevancy": 1.9426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method&body=Title%3A%20Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method%0AAuthor%3A%20Guoqing%20Zhang%20and%20Keita%20Fukuyama%20and%20Kazumasa%20Kishimoto%20and%20Tomohiro%20Kuroda%0AAbstract%3A%20%20%20Summarizing%20patient%20clinical%20notes%20is%20vital%20for%20reducing%20documentation%0Aburdens.%20Current%20manual%20summarization%20makes%20medical%20staff%20struggle.%20We%20propose%0Aan%20automatic%20method%20using%20LLMs%2C%20but%20long%20inputs%20cause%20LLMs%20to%20lose%20context%2C%0Areducing%20output%20quality%20especially%20in%20small%20size%20model.%20We%20used%20a%207B%20model%2C%0Aopen-calm-7b%2C%20enhanced%20with%20Native%20Bayes%20Context%20Extend%20and%20a%20redesigned%0Adecoding%20mechanism%20to%20reference%20one%20sentence%20at%20a%20time%2C%20keeping%20inputs%20within%0Acontext%20windows%2C%202048%20tokens.%20Our%20improved%20model%20achieved%20near%20parity%20with%0AGoogle%27s%20over%20175B%20Gemini%20on%20ROUGE-L%20metrics%20with%20200%20samples%2C%20indicating%0Astrong%20performance%20using%20less%20resources%2C%20enhancing%20automated%20EMR%20summarization%0Afeasibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Automatic%2520Summarization%2520of%2520Long%2520Clinical%2520Records%2520Using%250A%2520%2520Dynamic%2520Context%2520Extension%253ATesting%2520and%2520Evaluation%2520of%2520the%2520NBCE%2520Method%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Keita%2520Fukuyama%2520and%2520Kazumasa%2520Kishimoto%2520and%2520Tomohiro%2520Kuroda%26entry.1292438233%3D%2520%2520Summarizing%2520patient%2520clinical%2520notes%2520is%2520vital%2520for%2520reducing%2520documentation%250Aburdens.%2520Current%2520manual%2520summarization%2520makes%2520medical%2520staff%2520struggle.%2520We%2520propose%250Aan%2520automatic%2520method%2520using%2520LLMs%252C%2520but%2520long%2520inputs%2520cause%2520LLMs%2520to%2520lose%2520context%252C%250Areducing%2520output%2520quality%2520especially%2520in%2520small%2520size%2520model.%2520We%2520used%2520a%25207B%2520model%252C%250Aopen-calm-7b%252C%2520enhanced%2520with%2520Native%2520Bayes%2520Context%2520Extend%2520and%2520a%2520redesigned%250Adecoding%2520mechanism%2520to%2520reference%2520one%2520sentence%2520at%2520a%2520time%252C%2520keeping%2520inputs%2520within%250Acontext%2520windows%252C%25202048%2520tokens.%2520Our%2520improved%2520model%2520achieved%2520near%2520parity%2520with%250AGoogle%2527s%2520over%2520175B%2520Gemini%2520on%2520ROUGE-L%2520metrics%2520with%2520200%2520samples%252C%2520indicating%250Astrong%2520performance%2520using%2520less%2520resources%252C%2520enhancing%2520automated%2520EMR%2520summarization%250Afeasibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method&entry.906535625=Guoqing%20Zhang%20and%20Keita%20Fukuyama%20and%20Kazumasa%20Kishimoto%20and%20Tomohiro%20Kuroda&entry.1292438233=%20%20Summarizing%20patient%20clinical%20notes%20is%20vital%20for%20reducing%20documentation%0Aburdens.%20Current%20manual%20summarization%20makes%20medical%20staff%20struggle.%20We%20propose%0Aan%20automatic%20method%20using%20LLMs%2C%20but%20long%20inputs%20cause%20LLMs%20to%20lose%20context%2C%0Areducing%20output%20quality%20especially%20in%20small%20size%20model.%20We%20used%20a%207B%20model%2C%0Aopen-calm-7b%2C%20enhanced%20with%20Native%20Bayes%20Context%20Extend%20and%20a%20redesigned%0Adecoding%20mechanism%20to%20reference%20one%20sentence%20at%20a%20time%2C%20keeping%20inputs%20within%0Acontext%20windows%2C%202048%20tokens.%20Our%20improved%20model%20achieved%20near%20parity%20with%0AGoogle%27s%20over%20175B%20Gemini%20on%20ROUGE-L%20metrics%20with%20200%20samples%2C%20indicating%0Astrong%20performance%20using%20less%20resources%2C%20enhancing%20automated%20EMR%20summarization%0Afeasibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08586v2&entry.124074799=Read"},
{"title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test", "author": "Seongmin Lee and Hsiang Hsu and Chun-Fu Chen", "abstract": "  LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.\n", "link": "http://arxiv.org/abs/2411.09689v1", "date": "2024-11-14", "relevancy": 1.9415, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5077}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4834}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Hallucination%20Reasoning%20with%20Zero-shot%20Knowledge%20Test&body=Title%3A%20LLM%20Hallucination%20Reasoning%20with%20Zero-shot%20Knowledge%20Test%0AAuthor%3A%20Seongmin%20Lee%20and%20Hsiang%20Hsu%20and%20Chun-Fu%20Chen%0AAbstract%3A%20%20%20LLM%20hallucination%2C%20where%20LLMs%20occasionally%20generate%20unfaithful%20text%2C%20poses%0Asignificant%20challenges%20for%20their%20practical%20applications.%20Most%20existing%0Adetection%20methods%20rely%20on%20external%20knowledge%2C%20LLM%20fine-tuning%2C%20or%0Ahallucination-labeled%20datasets%2C%20and%20they%20do%20not%20distinguish%20between%20different%0Atypes%20of%20hallucinations%2C%20which%20are%20crucial%20for%20improving%20detection%20performance.%0AWe%20introduce%20a%20new%20task%2C%20Hallucination%20Reasoning%2C%20which%20classifies%0ALLM-generated%20text%20into%20one%20of%20three%20categories%3A%20aligned%2C%20misaligned%2C%20and%0Afabricated.%20Our%20novel%20zero-shot%20method%20assesses%20whether%20LLM%20has%20enough%0Aknowledge%20about%20a%20given%20prompt%20and%20text.%20Our%20experiments%20conducted%20on%20new%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20hallucination%20reasoning%0Aand%20underscore%20its%20importance%20for%20enhancing%20detection%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Hallucination%2520Reasoning%2520with%2520Zero-shot%2520Knowledge%2520Test%26entry.906535625%3DSeongmin%2520Lee%2520and%2520Hsiang%2520Hsu%2520and%2520Chun-Fu%2520Chen%26entry.1292438233%3D%2520%2520LLM%2520hallucination%252C%2520where%2520LLMs%2520occasionally%2520generate%2520unfaithful%2520text%252C%2520poses%250Asignificant%2520challenges%2520for%2520their%2520practical%2520applications.%2520Most%2520existing%250Adetection%2520methods%2520rely%2520on%2520external%2520knowledge%252C%2520LLM%2520fine-tuning%252C%2520or%250Ahallucination-labeled%2520datasets%252C%2520and%2520they%2520do%2520not%2520distinguish%2520between%2520different%250Atypes%2520of%2520hallucinations%252C%2520which%2520are%2520crucial%2520for%2520improving%2520detection%2520performance.%250AWe%2520introduce%2520a%2520new%2520task%252C%2520Hallucination%2520Reasoning%252C%2520which%2520classifies%250ALLM-generated%2520text%2520into%2520one%2520of%2520three%2520categories%253A%2520aligned%252C%2520misaligned%252C%2520and%250Afabricated.%2520Our%2520novel%2520zero-shot%2520method%2520assesses%2520whether%2520LLM%2520has%2520enough%250Aknowledge%2520about%2520a%2520given%2520prompt%2520and%2520text.%2520Our%2520experiments%2520conducted%2520on%2520new%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520hallucination%2520reasoning%250Aand%2520underscore%2520its%2520importance%2520for%2520enhancing%2520detection%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Hallucination%20Reasoning%20with%20Zero-shot%20Knowledge%20Test&entry.906535625=Seongmin%20Lee%20and%20Hsiang%20Hsu%20and%20Chun-Fu%20Chen&entry.1292438233=%20%20LLM%20hallucination%2C%20where%20LLMs%20occasionally%20generate%20unfaithful%20text%2C%20poses%0Asignificant%20challenges%20for%20their%20practical%20applications.%20Most%20existing%0Adetection%20methods%20rely%20on%20external%20knowledge%2C%20LLM%20fine-tuning%2C%20or%0Ahallucination-labeled%20datasets%2C%20and%20they%20do%20not%20distinguish%20between%20different%0Atypes%20of%20hallucinations%2C%20which%20are%20crucial%20for%20improving%20detection%20performance.%0AWe%20introduce%20a%20new%20task%2C%20Hallucination%20Reasoning%2C%20which%20classifies%0ALLM-generated%20text%20into%20one%20of%20three%20categories%3A%20aligned%2C%20misaligned%2C%20and%0Afabricated.%20Our%20novel%20zero-shot%20method%20assesses%20whether%20LLM%20has%20enough%0Aknowledge%20about%20a%20given%20prompt%20and%20text.%20Our%20experiments%20conducted%20on%20new%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20hallucination%20reasoning%0Aand%20underscore%20its%20importance%20for%20enhancing%20detection%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09689v1&entry.124074799=Read"},
{"title": "Approximated Variational Bayesian Inverse Reinforcement Learning for\n  Large Language Model Alignment", "author": "Yuang Cai and Yuyu Yuan and Jinsheng Shi and Qinhong Lin", "abstract": "  The alignment of large language models (LLMs) is crucial for generating\nhelpful and harmless content. Existing approaches leverage preference-based\nhuman feedback data to learn the reward function and align the LLM with the\nfeedback data. However, these approaches focus on modeling the reward\ndifference between the chosen and rejected demonstrations, rather than directly\nmodeling the true reward from each demonstration. Moreover, these approaches\nassume that the reward is only obtained at the end of the sentence, which\noverlooks the modeling of intermediate rewards. These issues lead to\ninsufficient use of training signals in the feedback data, limiting the\nrepresentation and generalization ability of the reward and potentially\nresulting in reward hacking. In this paper, we formulate LLM alignment as a\nBayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel\ntraining objective, Approximated Variational Alignment (AVA), to perform LLM\nalignment through Approximated Variational Reward Imitation Learning (AVRIL).\nThe BIRL formulation facilitates intermediate reward modeling and direct reward\nmodeling on each single demonstration, which enhances the utilization of\ntraining signals in the feedback data. Experiments show that AVA outperforms\nexisting LLM alignment approaches in reward modeling, RL fine-tuning, and\ndirect optimization.\n", "link": "http://arxiv.org/abs/2411.09341v1", "date": "2024-11-14", "relevancy": 1.9323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximated%20Variational%20Bayesian%20Inverse%20Reinforcement%20Learning%20for%0A%20%20Large%20Language%20Model%20Alignment&body=Title%3A%20Approximated%20Variational%20Bayesian%20Inverse%20Reinforcement%20Learning%20for%0A%20%20Large%20Language%20Model%20Alignment%0AAuthor%3A%20Yuang%20Cai%20and%20Yuyu%20Yuan%20and%20Jinsheng%20Shi%20and%20Qinhong%20Lin%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%20generating%0Ahelpful%20and%20harmless%20content.%20Existing%20approaches%20leverage%20preference-based%0Ahuman%20feedback%20data%20to%20learn%20the%20reward%20function%20and%20align%20the%20LLM%20with%20the%0Afeedback%20data.%20However%2C%20these%20approaches%20focus%20on%20modeling%20the%20reward%0Adifference%20between%20the%20chosen%20and%20rejected%20demonstrations%2C%20rather%20than%20directly%0Amodeling%20the%20true%20reward%20from%20each%20demonstration.%20Moreover%2C%20these%20approaches%0Aassume%20that%20the%20reward%20is%20only%20obtained%20at%20the%20end%20of%20the%20sentence%2C%20which%0Aoverlooks%20the%20modeling%20of%20intermediate%20rewards.%20These%20issues%20lead%20to%0Ainsufficient%20use%20of%20training%20signals%20in%20the%20feedback%20data%2C%20limiting%20the%0Arepresentation%20and%20generalization%20ability%20of%20the%20reward%20and%20potentially%0Aresulting%20in%20reward%20hacking.%20In%20this%20paper%2C%20we%20formulate%20LLM%20alignment%20as%20a%0ABayesian%20Inverse%20Reinforcement%20Learning%20%28BIRL%29%20problem%20and%20propose%20a%20novel%0Atraining%20objective%2C%20Approximated%20Variational%20Alignment%20%28AVA%29%2C%20to%20perform%20LLM%0Aalignment%20through%20Approximated%20Variational%20Reward%20Imitation%20Learning%20%28AVRIL%29.%0AThe%20BIRL%20formulation%20facilitates%20intermediate%20reward%20modeling%20and%20direct%20reward%0Amodeling%20on%20each%20single%20demonstration%2C%20which%20enhances%20the%20utilization%20of%0Atraining%20signals%20in%20the%20feedback%20data.%20Experiments%20show%20that%20AVA%20outperforms%0Aexisting%20LLM%20alignment%20approaches%20in%20reward%20modeling%2C%20RL%20fine-tuning%2C%20and%0Adirect%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximated%2520Variational%2520Bayesian%2520Inverse%2520Reinforcement%2520Learning%2520for%250A%2520%2520Large%2520Language%2520Model%2520Alignment%26entry.906535625%3DYuang%2520Cai%2520and%2520Yuyu%2520Yuan%2520and%2520Jinsheng%2520Shi%2520and%2520Qinhong%2520Lin%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%2520for%2520generating%250Ahelpful%2520and%2520harmless%2520content.%2520Existing%2520approaches%2520leverage%2520preference-based%250Ahuman%2520feedback%2520data%2520to%2520learn%2520the%2520reward%2520function%2520and%2520align%2520the%2520LLM%2520with%2520the%250Afeedback%2520data.%2520However%252C%2520these%2520approaches%2520focus%2520on%2520modeling%2520the%2520reward%250Adifference%2520between%2520the%2520chosen%2520and%2520rejected%2520demonstrations%252C%2520rather%2520than%2520directly%250Amodeling%2520the%2520true%2520reward%2520from%2520each%2520demonstration.%2520Moreover%252C%2520these%2520approaches%250Aassume%2520that%2520the%2520reward%2520is%2520only%2520obtained%2520at%2520the%2520end%2520of%2520the%2520sentence%252C%2520which%250Aoverlooks%2520the%2520modeling%2520of%2520intermediate%2520rewards.%2520These%2520issues%2520lead%2520to%250Ainsufficient%2520use%2520of%2520training%2520signals%2520in%2520the%2520feedback%2520data%252C%2520limiting%2520the%250Arepresentation%2520and%2520generalization%2520ability%2520of%2520the%2520reward%2520and%2520potentially%250Aresulting%2520in%2520reward%2520hacking.%2520In%2520this%2520paper%252C%2520we%2520formulate%2520LLM%2520alignment%2520as%2520a%250ABayesian%2520Inverse%2520Reinforcement%2520Learning%2520%2528BIRL%2529%2520problem%2520and%2520propose%2520a%2520novel%250Atraining%2520objective%252C%2520Approximated%2520Variational%2520Alignment%2520%2528AVA%2529%252C%2520to%2520perform%2520LLM%250Aalignment%2520through%2520Approximated%2520Variational%2520Reward%2520Imitation%2520Learning%2520%2528AVRIL%2529.%250AThe%2520BIRL%2520formulation%2520facilitates%2520intermediate%2520reward%2520modeling%2520and%2520direct%2520reward%250Amodeling%2520on%2520each%2520single%2520demonstration%252C%2520which%2520enhances%2520the%2520utilization%2520of%250Atraining%2520signals%2520in%2520the%2520feedback%2520data.%2520Experiments%2520show%2520that%2520AVA%2520outperforms%250Aexisting%2520LLM%2520alignment%2520approaches%2520in%2520reward%2520modeling%252C%2520RL%2520fine-tuning%252C%2520and%250Adirect%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximated%20Variational%20Bayesian%20Inverse%20Reinforcement%20Learning%20for%0A%20%20Large%20Language%20Model%20Alignment&entry.906535625=Yuang%20Cai%20and%20Yuyu%20Yuan%20and%20Jinsheng%20Shi%20and%20Qinhong%20Lin&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%20generating%0Ahelpful%20and%20harmless%20content.%20Existing%20approaches%20leverage%20preference-based%0Ahuman%20feedback%20data%20to%20learn%20the%20reward%20function%20and%20align%20the%20LLM%20with%20the%0Afeedback%20data.%20However%2C%20these%20approaches%20focus%20on%20modeling%20the%20reward%0Adifference%20between%20the%20chosen%20and%20rejected%20demonstrations%2C%20rather%20than%20directly%0Amodeling%20the%20true%20reward%20from%20each%20demonstration.%20Moreover%2C%20these%20approaches%0Aassume%20that%20the%20reward%20is%20only%20obtained%20at%20the%20end%20of%20the%20sentence%2C%20which%0Aoverlooks%20the%20modeling%20of%20intermediate%20rewards.%20These%20issues%20lead%20to%0Ainsufficient%20use%20of%20training%20signals%20in%20the%20feedback%20data%2C%20limiting%20the%0Arepresentation%20and%20generalization%20ability%20of%20the%20reward%20and%20potentially%0Aresulting%20in%20reward%20hacking.%20In%20this%20paper%2C%20we%20formulate%20LLM%20alignment%20as%20a%0ABayesian%20Inverse%20Reinforcement%20Learning%20%28BIRL%29%20problem%20and%20propose%20a%20novel%0Atraining%20objective%2C%20Approximated%20Variational%20Alignment%20%28AVA%29%2C%20to%20perform%20LLM%0Aalignment%20through%20Approximated%20Variational%20Reward%20Imitation%20Learning%20%28AVRIL%29.%0AThe%20BIRL%20formulation%20facilitates%20intermediate%20reward%20modeling%20and%20direct%20reward%0Amodeling%20on%20each%20single%20demonstration%2C%20which%20enhances%20the%20utilization%20of%0Atraining%20signals%20in%20the%20feedback%20data.%20Experiments%20show%20that%20AVA%20outperforms%0Aexisting%20LLM%20alignment%20approaches%20in%20reward%20modeling%2C%20RL%20fine-tuning%2C%20and%0Adirect%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09341v1&entry.124074799=Read"},
{"title": "Caravan MultiMet: Extending Caravan with Multiple Weather Nowcasts and\n  Forecasts", "author": "Guy Shalev and Frederik Kratzert", "abstract": "  The Caravan large-sample hydrology dataset (Kratzert et al., 2023) was\ncreated to standardize and harmonize streamflow data from various regional\ndatasets, combined with globally available meteorological forcing and catchment\nattributes. This community-driven project also allows researchers to\nconveniently extend the dataset for additional basins, as done 6 times to date\n(see https://github.com/kratzert/Caravan/discussions/10). We present a novel\nextension to Caravan, focusing on enriching the meteorological forcing data.\nOur extension adds three precipitation nowcast products (CPC, IMERG v07 Early,\nand CHIRPS) and three weather forecast products (ECMWF IFS HRES, GraphCast, and\nCHIRPS-GEFS) to the existing ERA5-Land reanalysis data. The inclusion of\ndiverse data sources, particularly weather forecasts, enables more robust\nevaluation and benchmarking of hydrological models, especially for real-time\nforecasting scenarios. To the best of our knowledge, this extension makes\nCaravan the first large-sample hydrology dataset to incorporate weather\nforecast data, significantly enhancing its capabilities and fostering\nadvancements in hydrological research, benchmarking, and real-time hydrologic\nforecasting. The data is publicly available under a CC-BY-4.0 license on Zenodo\nin two parts (https://zenodo.org/records/14161235,\nhttps://zenodo.org/records/14161281) and on Google Cloud Platform (GCP) - see\nmore under the Data Availability chapter.\n", "link": "http://arxiv.org/abs/2411.09459v1", "date": "2024-11-14", "relevancy": 1.9227, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3969}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3784}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Caravan%20MultiMet%3A%20Extending%20Caravan%20with%20Multiple%20Weather%20Nowcasts%20and%0A%20%20Forecasts&body=Title%3A%20Caravan%20MultiMet%3A%20Extending%20Caravan%20with%20Multiple%20Weather%20Nowcasts%20and%0A%20%20Forecasts%0AAuthor%3A%20Guy%20Shalev%20and%20Frederik%20Kratzert%0AAbstract%3A%20%20%20The%20Caravan%20large-sample%20hydrology%20dataset%20%28Kratzert%20et%20al.%2C%202023%29%20was%0Acreated%20to%20standardize%20and%20harmonize%20streamflow%20data%20from%20various%20regional%0Adatasets%2C%20combined%20with%20globally%20available%20meteorological%20forcing%20and%20catchment%0Aattributes.%20This%20community-driven%20project%20also%20allows%20researchers%20to%0Aconveniently%20extend%20the%20dataset%20for%20additional%20basins%2C%20as%20done%206%20times%20to%20date%0A%28see%20https%3A//github.com/kratzert/Caravan/discussions/10%29.%20We%20present%20a%20novel%0Aextension%20to%20Caravan%2C%20focusing%20on%20enriching%20the%20meteorological%20forcing%20data.%0AOur%20extension%20adds%20three%20precipitation%20nowcast%20products%20%28CPC%2C%20IMERG%20v07%20Early%2C%0Aand%20CHIRPS%29%20and%20three%20weather%20forecast%20products%20%28ECMWF%20IFS%20HRES%2C%20GraphCast%2C%20and%0ACHIRPS-GEFS%29%20to%20the%20existing%20ERA5-Land%20reanalysis%20data.%20The%20inclusion%20of%0Adiverse%20data%20sources%2C%20particularly%20weather%20forecasts%2C%20enables%20more%20robust%0Aevaluation%20and%20benchmarking%20of%20hydrological%20models%2C%20especially%20for%20real-time%0Aforecasting%20scenarios.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20extension%20makes%0ACaravan%20the%20first%20large-sample%20hydrology%20dataset%20to%20incorporate%20weather%0Aforecast%20data%2C%20significantly%20enhancing%20its%20capabilities%20and%20fostering%0Aadvancements%20in%20hydrological%20research%2C%20benchmarking%2C%20and%20real-time%20hydrologic%0Aforecasting.%20The%20data%20is%20publicly%20available%20under%20a%20CC-BY-4.0%20license%20on%20Zenodo%0Ain%20two%20parts%20%28https%3A//zenodo.org/records/14161235%2C%0Ahttps%3A//zenodo.org/records/14161281%29%20and%20on%20Google%20Cloud%20Platform%20%28GCP%29%20-%20see%0Amore%20under%20the%20Data%20Availability%20chapter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaravan%2520MultiMet%253A%2520Extending%2520Caravan%2520with%2520Multiple%2520Weather%2520Nowcasts%2520and%250A%2520%2520Forecasts%26entry.906535625%3DGuy%2520Shalev%2520and%2520Frederik%2520Kratzert%26entry.1292438233%3D%2520%2520The%2520Caravan%2520large-sample%2520hydrology%2520dataset%2520%2528Kratzert%2520et%2520al.%252C%25202023%2529%2520was%250Acreated%2520to%2520standardize%2520and%2520harmonize%2520streamflow%2520data%2520from%2520various%2520regional%250Adatasets%252C%2520combined%2520with%2520globally%2520available%2520meteorological%2520forcing%2520and%2520catchment%250Aattributes.%2520This%2520community-driven%2520project%2520also%2520allows%2520researchers%2520to%250Aconveniently%2520extend%2520the%2520dataset%2520for%2520additional%2520basins%252C%2520as%2520done%25206%2520times%2520to%2520date%250A%2528see%2520https%253A//github.com/kratzert/Caravan/discussions/10%2529.%2520We%2520present%2520a%2520novel%250Aextension%2520to%2520Caravan%252C%2520focusing%2520on%2520enriching%2520the%2520meteorological%2520forcing%2520data.%250AOur%2520extension%2520adds%2520three%2520precipitation%2520nowcast%2520products%2520%2528CPC%252C%2520IMERG%2520v07%2520Early%252C%250Aand%2520CHIRPS%2529%2520and%2520three%2520weather%2520forecast%2520products%2520%2528ECMWF%2520IFS%2520HRES%252C%2520GraphCast%252C%2520and%250ACHIRPS-GEFS%2529%2520to%2520the%2520existing%2520ERA5-Land%2520reanalysis%2520data.%2520The%2520inclusion%2520of%250Adiverse%2520data%2520sources%252C%2520particularly%2520weather%2520forecasts%252C%2520enables%2520more%2520robust%250Aevaluation%2520and%2520benchmarking%2520of%2520hydrological%2520models%252C%2520especially%2520for%2520real-time%250Aforecasting%2520scenarios.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520extension%2520makes%250ACaravan%2520the%2520first%2520large-sample%2520hydrology%2520dataset%2520to%2520incorporate%2520weather%250Aforecast%2520data%252C%2520significantly%2520enhancing%2520its%2520capabilities%2520and%2520fostering%250Aadvancements%2520in%2520hydrological%2520research%252C%2520benchmarking%252C%2520and%2520real-time%2520hydrologic%250Aforecasting.%2520The%2520data%2520is%2520publicly%2520available%2520under%2520a%2520CC-BY-4.0%2520license%2520on%2520Zenodo%250Ain%2520two%2520parts%2520%2528https%253A//zenodo.org/records/14161235%252C%250Ahttps%253A//zenodo.org/records/14161281%2529%2520and%2520on%2520Google%2520Cloud%2520Platform%2520%2528GCP%2529%2520-%2520see%250Amore%2520under%2520the%2520Data%2520Availability%2520chapter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Caravan%20MultiMet%3A%20Extending%20Caravan%20with%20Multiple%20Weather%20Nowcasts%20and%0A%20%20Forecasts&entry.906535625=Guy%20Shalev%20and%20Frederik%20Kratzert&entry.1292438233=%20%20The%20Caravan%20large-sample%20hydrology%20dataset%20%28Kratzert%20et%20al.%2C%202023%29%20was%0Acreated%20to%20standardize%20and%20harmonize%20streamflow%20data%20from%20various%20regional%0Adatasets%2C%20combined%20with%20globally%20available%20meteorological%20forcing%20and%20catchment%0Aattributes.%20This%20community-driven%20project%20also%20allows%20researchers%20to%0Aconveniently%20extend%20the%20dataset%20for%20additional%20basins%2C%20as%20done%206%20times%20to%20date%0A%28see%20https%3A//github.com/kratzert/Caravan/discussions/10%29.%20We%20present%20a%20novel%0Aextension%20to%20Caravan%2C%20focusing%20on%20enriching%20the%20meteorological%20forcing%20data.%0AOur%20extension%20adds%20three%20precipitation%20nowcast%20products%20%28CPC%2C%20IMERG%20v07%20Early%2C%0Aand%20CHIRPS%29%20and%20three%20weather%20forecast%20products%20%28ECMWF%20IFS%20HRES%2C%20GraphCast%2C%20and%0ACHIRPS-GEFS%29%20to%20the%20existing%20ERA5-Land%20reanalysis%20data.%20The%20inclusion%20of%0Adiverse%20data%20sources%2C%20particularly%20weather%20forecasts%2C%20enables%20more%20robust%0Aevaluation%20and%20benchmarking%20of%20hydrological%20models%2C%20especially%20for%20real-time%0Aforecasting%20scenarios.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20extension%20makes%0ACaravan%20the%20first%20large-sample%20hydrology%20dataset%20to%20incorporate%20weather%0Aforecast%20data%2C%20significantly%20enhancing%20its%20capabilities%20and%20fostering%0Aadvancements%20in%20hydrological%20research%2C%20benchmarking%2C%20and%20real-time%20hydrologic%0Aforecasting.%20The%20data%20is%20publicly%20available%20under%20a%20CC-BY-4.0%20license%20on%20Zenodo%0Ain%20two%20parts%20%28https%3A//zenodo.org/records/14161235%2C%0Ahttps%3A//zenodo.org/records/14161281%29%20and%20on%20Google%20Cloud%20Platform%20%28GCP%29%20-%20see%0Amore%20under%20the%20Data%20Availability%20chapter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09459v1&entry.124074799=Read"},
{"title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal\n  Language Modelling (CLM)", "author": "Nicolas Drapier and Aladine Chetouani and Aur\u00e9lien Chateigner", "abstract": "  The prediction of ship trajectories is a growing field of study in artificial\nintelligence. Traditional methods rely on the use of LSTM, GRU networks, and\neven Transformer architectures for the prediction of spatio-temporal series.\nThis study proposes a viable alternative for predicting these trajectories\nusing only GNSS positions. It considers this spatio-temporal problem as a\nnatural language processing problem. The latitude/longitude coordinates of AIS\nmessages are transformed into cell identifiers using the H3 index. Thanks to\nthe pseudo-octal representation, it becomes easier for language models to learn\nthe spatial hierarchy of the H3 index. The method is compared with a classical\nKalman filter, widely used in the maritime domain, and introduces the Fr\\'echet\ndistance as the main evaluation metric. We show that it is possible to predict\nship trajectories quite precisely up to 8 hours ahead with 30 minutes of\ncontext, using solely GNSS positions, without relying on any additional\ninformation such as speed, course, or external conditions - unlike many\ntraditional methods. We demonstrate that this alternative works well enough to\npredict trajectories worldwide.\n", "link": "http://arxiv.org/abs/2405.09596v2", "date": "2024-11-14", "relevancy": 1.921, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4807}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Maritime%20Trajectory%20Forecasting%20via%20H3%20Index%20and%20Causal%0A%20%20Language%20Modelling%20%28CLM%29&body=Title%3A%20Enhancing%20Maritime%20Trajectory%20Forecasting%20via%20H3%20Index%20and%20Causal%0A%20%20Language%20Modelling%20%28CLM%29%0AAuthor%3A%20Nicolas%20Drapier%20and%20Aladine%20Chetouani%20and%20Aur%C3%A9lien%20Chateigner%0AAbstract%3A%20%20%20The%20prediction%20of%20ship%20trajectories%20is%20a%20growing%20field%20of%20study%20in%20artificial%0Aintelligence.%20Traditional%20methods%20rely%20on%20the%20use%20of%20LSTM%2C%20GRU%20networks%2C%20and%0Aeven%20Transformer%20architectures%20for%20the%20prediction%20of%20spatio-temporal%20series.%0AThis%20study%20proposes%20a%20viable%20alternative%20for%20predicting%20these%20trajectories%0Ausing%20only%20GNSS%20positions.%20It%20considers%20this%20spatio-temporal%20problem%20as%20a%0Anatural%20language%20processing%20problem.%20The%20latitude/longitude%20coordinates%20of%20AIS%0Amessages%20are%20transformed%20into%20cell%20identifiers%20using%20the%20H3%20index.%20Thanks%20to%0Athe%20pseudo-octal%20representation%2C%20it%20becomes%20easier%20for%20language%20models%20to%20learn%0Athe%20spatial%20hierarchy%20of%20the%20H3%20index.%20The%20method%20is%20compared%20with%20a%20classical%0AKalman%20filter%2C%20widely%20used%20in%20the%20maritime%20domain%2C%20and%20introduces%20the%20Fr%5C%27echet%0Adistance%20as%20the%20main%20evaluation%20metric.%20We%20show%20that%20it%20is%20possible%20to%20predict%0Aship%20trajectories%20quite%20precisely%20up%20to%208%20hours%20ahead%20with%2030%20minutes%20of%0Acontext%2C%20using%20solely%20GNSS%20positions%2C%20without%20relying%20on%20any%20additional%0Ainformation%20such%20as%20speed%2C%20course%2C%20or%20external%20conditions%20-%20unlike%20many%0Atraditional%20methods.%20We%20demonstrate%20that%20this%20alternative%20works%20well%20enough%20to%0Apredict%20trajectories%20worldwide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Maritime%2520Trajectory%2520Forecasting%2520via%2520H3%2520Index%2520and%2520Causal%250A%2520%2520Language%2520Modelling%2520%2528CLM%2529%26entry.906535625%3DNicolas%2520Drapier%2520and%2520Aladine%2520Chetouani%2520and%2520Aur%25C3%25A9lien%2520Chateigner%26entry.1292438233%3D%2520%2520The%2520prediction%2520of%2520ship%2520trajectories%2520is%2520a%2520growing%2520field%2520of%2520study%2520in%2520artificial%250Aintelligence.%2520Traditional%2520methods%2520rely%2520on%2520the%2520use%2520of%2520LSTM%252C%2520GRU%2520networks%252C%2520and%250Aeven%2520Transformer%2520architectures%2520for%2520the%2520prediction%2520of%2520spatio-temporal%2520series.%250AThis%2520study%2520proposes%2520a%2520viable%2520alternative%2520for%2520predicting%2520these%2520trajectories%250Ausing%2520only%2520GNSS%2520positions.%2520It%2520considers%2520this%2520spatio-temporal%2520problem%2520as%2520a%250Anatural%2520language%2520processing%2520problem.%2520The%2520latitude/longitude%2520coordinates%2520of%2520AIS%250Amessages%2520are%2520transformed%2520into%2520cell%2520identifiers%2520using%2520the%2520H3%2520index.%2520Thanks%2520to%250Athe%2520pseudo-octal%2520representation%252C%2520it%2520becomes%2520easier%2520for%2520language%2520models%2520to%2520learn%250Athe%2520spatial%2520hierarchy%2520of%2520the%2520H3%2520index.%2520The%2520method%2520is%2520compared%2520with%2520a%2520classical%250AKalman%2520filter%252C%2520widely%2520used%2520in%2520the%2520maritime%2520domain%252C%2520and%2520introduces%2520the%2520Fr%255C%2527echet%250Adistance%2520as%2520the%2520main%2520evaluation%2520metric.%2520We%2520show%2520that%2520it%2520is%2520possible%2520to%2520predict%250Aship%2520trajectories%2520quite%2520precisely%2520up%2520to%25208%2520hours%2520ahead%2520with%252030%2520minutes%2520of%250Acontext%252C%2520using%2520solely%2520GNSS%2520positions%252C%2520without%2520relying%2520on%2520any%2520additional%250Ainformation%2520such%2520as%2520speed%252C%2520course%252C%2520or%2520external%2520conditions%2520-%2520unlike%2520many%250Atraditional%2520methods.%2520We%2520demonstrate%2520that%2520this%2520alternative%2520works%2520well%2520enough%2520to%250Apredict%2520trajectories%2520worldwide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Maritime%20Trajectory%20Forecasting%20via%20H3%20Index%20and%20Causal%0A%20%20Language%20Modelling%20%28CLM%29&entry.906535625=Nicolas%20Drapier%20and%20Aladine%20Chetouani%20and%20Aur%C3%A9lien%20Chateigner&entry.1292438233=%20%20The%20prediction%20of%20ship%20trajectories%20is%20a%20growing%20field%20of%20study%20in%20artificial%0Aintelligence.%20Traditional%20methods%20rely%20on%20the%20use%20of%20LSTM%2C%20GRU%20networks%2C%20and%0Aeven%20Transformer%20architectures%20for%20the%20prediction%20of%20spatio-temporal%20series.%0AThis%20study%20proposes%20a%20viable%20alternative%20for%20predicting%20these%20trajectories%0Ausing%20only%20GNSS%20positions.%20It%20considers%20this%20spatio-temporal%20problem%20as%20a%0Anatural%20language%20processing%20problem.%20The%20latitude/longitude%20coordinates%20of%20AIS%0Amessages%20are%20transformed%20into%20cell%20identifiers%20using%20the%20H3%20index.%20Thanks%20to%0Athe%20pseudo-octal%20representation%2C%20it%20becomes%20easier%20for%20language%20models%20to%20learn%0Athe%20spatial%20hierarchy%20of%20the%20H3%20index.%20The%20method%20is%20compared%20with%20a%20classical%0AKalman%20filter%2C%20widely%20used%20in%20the%20maritime%20domain%2C%20and%20introduces%20the%20Fr%5C%27echet%0Adistance%20as%20the%20main%20evaluation%20metric.%20We%20show%20that%20it%20is%20possible%20to%20predict%0Aship%20trajectories%20quite%20precisely%20up%20to%208%20hours%20ahead%20with%2030%20minutes%20of%0Acontext%2C%20using%20solely%20GNSS%20positions%2C%20without%20relying%20on%20any%20additional%0Ainformation%20such%20as%20speed%2C%20course%2C%20or%20external%20conditions%20-%20unlike%20many%0Atraditional%20methods.%20We%20demonstrate%20that%20this%20alternative%20works%20well%20enough%20to%0Apredict%20trajectories%20worldwide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09596v2&entry.124074799=Read"},
{"title": "Tract-RLFormer: A Tract-Specific RL policy based Decoder-only\n  Transformer Network", "author": "Ankita Joshi and Ashutosh Sharma and Anoushkrit Goel and Ranjeet Ranjan Jha and Chirag Ahuja and Arnav Bhavsar and Aditya Nigam", "abstract": "  Fiber tractography is a cornerstone of neuroimaging, enabling the detailed\nmapping of the brain's white matter pathways through diffusion MRI. This is\ncrucial for understanding brain connectivity and function, making it a valuable\ntool in neurological applications. Despite its importance, tractography faces\nchallenges due to its complexity and susceptibility to false positives,\nmisrepresenting vital pathways. To address these issues, recent strategies have\nshifted towards deep learning, utilizing supervised learning, which depends on\nprecise ground truth, or reinforcement learning, which operates without it. In\nthis work, we propose Tract-RLFormer, a network utilizing both supervised and\nreinforcement learning, in a two-stage policy refinement process that markedly\nimproves the accuracy and generalizability across various data-sets. By\nemploying a tract-specific approach, our network directly delineates the tracts\nof interest, bypassing the traditional segmentation process. Through rigorous\nvalidation on datasets such as TractoInferno, HCP, and ISMRM-2015, our\nmethodology demonstrates a leap forward in tractography, showcasing its ability\nto accurately map the brain's white matter tracts.\n", "link": "http://arxiv.org/abs/2411.05757v2", "date": "2024-11-14", "relevancy": 1.9198, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4937}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network&body=Title%3A%20Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network%0AAuthor%3A%20Ankita%20Joshi%20and%20Ashutosh%20Sharma%20and%20Anoushkrit%20Goel%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Arnav%20Bhavsar%20and%20Aditya%20Nigam%0AAbstract%3A%20%20%20Fiber%20tractography%20is%20a%20cornerstone%20of%20neuroimaging%2C%20enabling%20the%20detailed%0Amapping%20of%20the%20brain%27s%20white%20matter%20pathways%20through%20diffusion%20MRI.%20This%20is%0Acrucial%20for%20understanding%20brain%20connectivity%20and%20function%2C%20making%20it%20a%20valuable%0Atool%20in%20neurological%20applications.%20Despite%20its%20importance%2C%20tractography%20faces%0Achallenges%20due%20to%20its%20complexity%20and%20susceptibility%20to%20false%20positives%2C%0Amisrepresenting%20vital%20pathways.%20To%20address%20these%20issues%2C%20recent%20strategies%20have%0Ashifted%20towards%20deep%20learning%2C%20utilizing%20supervised%20learning%2C%20which%20depends%20on%0Aprecise%20ground%20truth%2C%20or%20reinforcement%20learning%2C%20which%20operates%20without%20it.%20In%0Athis%20work%2C%20we%20propose%20Tract-RLFormer%2C%20a%20network%20utilizing%20both%20supervised%20and%0Areinforcement%20learning%2C%20in%20a%20two-stage%20policy%20refinement%20process%20that%20markedly%0Aimproves%20the%20accuracy%20and%20generalizability%20across%20various%20data-sets.%20By%0Aemploying%20a%20tract-specific%20approach%2C%20our%20network%20directly%20delineates%20the%20tracts%0Aof%20interest%2C%20bypassing%20the%20traditional%20segmentation%20process.%20Through%20rigorous%0Avalidation%20on%20datasets%20such%20as%20TractoInferno%2C%20HCP%2C%20and%20ISMRM-2015%2C%20our%0Amethodology%20demonstrates%20a%20leap%20forward%20in%20tractography%2C%20showcasing%20its%20ability%0Ato%20accurately%20map%20the%20brain%27s%20white%20matter%20tracts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05757v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTract-RLFormer%253A%2520A%2520Tract-Specific%2520RL%2520policy%2520based%2520Decoder-only%250A%2520%2520Transformer%2520Network%26entry.906535625%3DAnkita%2520Joshi%2520and%2520Ashutosh%2520Sharma%2520and%2520Anoushkrit%2520Goel%2520and%2520Ranjeet%2520Ranjan%2520Jha%2520and%2520Chirag%2520Ahuja%2520and%2520Arnav%2520Bhavsar%2520and%2520Aditya%2520Nigam%26entry.1292438233%3D%2520%2520Fiber%2520tractography%2520is%2520a%2520cornerstone%2520of%2520neuroimaging%252C%2520enabling%2520the%2520detailed%250Amapping%2520of%2520the%2520brain%2527s%2520white%2520matter%2520pathways%2520through%2520diffusion%2520MRI.%2520This%2520is%250Acrucial%2520for%2520understanding%2520brain%2520connectivity%2520and%2520function%252C%2520making%2520it%2520a%2520valuable%250Atool%2520in%2520neurological%2520applications.%2520Despite%2520its%2520importance%252C%2520tractography%2520faces%250Achallenges%2520due%2520to%2520its%2520complexity%2520and%2520susceptibility%2520to%2520false%2520positives%252C%250Amisrepresenting%2520vital%2520pathways.%2520To%2520address%2520these%2520issues%252C%2520recent%2520strategies%2520have%250Ashifted%2520towards%2520deep%2520learning%252C%2520utilizing%2520supervised%2520learning%252C%2520which%2520depends%2520on%250Aprecise%2520ground%2520truth%252C%2520or%2520reinforcement%2520learning%252C%2520which%2520operates%2520without%2520it.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Tract-RLFormer%252C%2520a%2520network%2520utilizing%2520both%2520supervised%2520and%250Areinforcement%2520learning%252C%2520in%2520a%2520two-stage%2520policy%2520refinement%2520process%2520that%2520markedly%250Aimproves%2520the%2520accuracy%2520and%2520generalizability%2520across%2520various%2520data-sets.%2520By%250Aemploying%2520a%2520tract-specific%2520approach%252C%2520our%2520network%2520directly%2520delineates%2520the%2520tracts%250Aof%2520interest%252C%2520bypassing%2520the%2520traditional%2520segmentation%2520process.%2520Through%2520rigorous%250Avalidation%2520on%2520datasets%2520such%2520as%2520TractoInferno%252C%2520HCP%252C%2520and%2520ISMRM-2015%252C%2520our%250Amethodology%2520demonstrates%2520a%2520leap%2520forward%2520in%2520tractography%252C%2520showcasing%2520its%2520ability%250Ato%2520accurately%2520map%2520the%2520brain%2527s%2520white%2520matter%2520tracts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05757v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network&entry.906535625=Ankita%20Joshi%20and%20Ashutosh%20Sharma%20and%20Anoushkrit%20Goel%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Arnav%20Bhavsar%20and%20Aditya%20Nigam&entry.1292438233=%20%20Fiber%20tractography%20is%20a%20cornerstone%20of%20neuroimaging%2C%20enabling%20the%20detailed%0Amapping%20of%20the%20brain%27s%20white%20matter%20pathways%20through%20diffusion%20MRI.%20This%20is%0Acrucial%20for%20understanding%20brain%20connectivity%20and%20function%2C%20making%20it%20a%20valuable%0Atool%20in%20neurological%20applications.%20Despite%20its%20importance%2C%20tractography%20faces%0Achallenges%20due%20to%20its%20complexity%20and%20susceptibility%20to%20false%20positives%2C%0Amisrepresenting%20vital%20pathways.%20To%20address%20these%20issues%2C%20recent%20strategies%20have%0Ashifted%20towards%20deep%20learning%2C%20utilizing%20supervised%20learning%2C%20which%20depends%20on%0Aprecise%20ground%20truth%2C%20or%20reinforcement%20learning%2C%20which%20operates%20without%20it.%20In%0Athis%20work%2C%20we%20propose%20Tract-RLFormer%2C%20a%20network%20utilizing%20both%20supervised%20and%0Areinforcement%20learning%2C%20in%20a%20two-stage%20policy%20refinement%20process%20that%20markedly%0Aimproves%20the%20accuracy%20and%20generalizability%20across%20various%20data-sets.%20By%0Aemploying%20a%20tract-specific%20approach%2C%20our%20network%20directly%20delineates%20the%20tracts%0Aof%20interest%2C%20bypassing%20the%20traditional%20segmentation%20process.%20Through%20rigorous%0Avalidation%20on%20datasets%20such%20as%20TractoInferno%2C%20HCP%2C%20and%20ISMRM-2015%2C%20our%0Amethodology%20demonstrates%20a%20leap%20forward%20in%20tractography%2C%20showcasing%20its%20ability%0Ato%20accurately%20map%20the%20brain%27s%20white%20matter%20tracts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05757v2&entry.124074799=Read"},
{"title": "Image Regeneration: Evaluating Text-to-Image Model via Generating\n  Identical Image with Multimodal Large Language Models", "author": "Chutian Meng and Fan Ma and Jiaxu Miao and Chi Zhang and Yi Yang and Yueting Zhuang", "abstract": "  Diffusion models have revitalized the image generation domain, playing\ncrucial roles in both academic research and artistic expression. With the\nemergence of new diffusion models, assessing the performance of text-to-image\nmodels has become increasingly important. Current metrics focus on directly\nmatching the input text with the generated image, but due to cross-modal\ninformation asymmetry, this leads to unreliable or incomplete assessment\nresults. Motivated by this, we introduce the Image Regeneration task in this\nstudy to assess text-to-image models by tasking the T2I model with generating\nan image according to the reference image. We use GPT4V to bridge the gap\nbetween the reference image and the text input for the T2I model, allowing T2I\nmodels to understand image content. This evaluation process is simplified as\ncomparisons between the generated image and the reference image are\nstraightforward. Two regeneration datasets spanning content-diverse and\nstyle-diverse evaluation dataset are introduced to evaluate the leading\ndiffusion models currently available. Additionally, we present ImageRepainter\nframework to enhance the quality of generated images by improving content\ncomprehension via MLLM guided iterative generation and revision. Our\ncomprehensive experiments have showcased the effectiveness of this framework in\nassessing the generative capabilities of models. By leveraging MLLM, we have\ndemonstrated that a robust T2M can produce images more closely resembling the\nreference image.\n", "link": "http://arxiv.org/abs/2411.09449v1", "date": "2024-11-14", "relevancy": 1.8986, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6699}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6326}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Regeneration%3A%20Evaluating%20Text-to-Image%20Model%20via%20Generating%0A%20%20Identical%20Image%20with%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Image%20Regeneration%3A%20Evaluating%20Text-to-Image%20Model%20via%20Generating%0A%20%20Identical%20Image%20with%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Chutian%20Meng%20and%20Fan%20Ma%20and%20Jiaxu%20Miao%20and%20Chi%20Zhang%20and%20Yi%20Yang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20revitalized%20the%20image%20generation%20domain%2C%20playing%0Acrucial%20roles%20in%20both%20academic%20research%20and%20artistic%20expression.%20With%20the%0Aemergence%20of%20new%20diffusion%20models%2C%20assessing%20the%20performance%20of%20text-to-image%0Amodels%20has%20become%20increasingly%20important.%20Current%20metrics%20focus%20on%20directly%0Amatching%20the%20input%20text%20with%20the%20generated%20image%2C%20but%20due%20to%20cross-modal%0Ainformation%20asymmetry%2C%20this%20leads%20to%20unreliable%20or%20incomplete%20assessment%0Aresults.%20Motivated%20by%20this%2C%20we%20introduce%20the%20Image%20Regeneration%20task%20in%20this%0Astudy%20to%20assess%20text-to-image%20models%20by%20tasking%20the%20T2I%20model%20with%20generating%0Aan%20image%20according%20to%20the%20reference%20image.%20We%20use%20GPT4V%20to%20bridge%20the%20gap%0Abetween%20the%20reference%20image%20and%20the%20text%20input%20for%20the%20T2I%20model%2C%20allowing%20T2I%0Amodels%20to%20understand%20image%20content.%20This%20evaluation%20process%20is%20simplified%20as%0Acomparisons%20between%20the%20generated%20image%20and%20the%20reference%20image%20are%0Astraightforward.%20Two%20regeneration%20datasets%20spanning%20content-diverse%20and%0Astyle-diverse%20evaluation%20dataset%20are%20introduced%20to%20evaluate%20the%20leading%0Adiffusion%20models%20currently%20available.%20Additionally%2C%20we%20present%20ImageRepainter%0Aframework%20to%20enhance%20the%20quality%20of%20generated%20images%20by%20improving%20content%0Acomprehension%20via%20MLLM%20guided%20iterative%20generation%20and%20revision.%20Our%0Acomprehensive%20experiments%20have%20showcased%20the%20effectiveness%20of%20this%20framework%20in%0Aassessing%20the%20generative%20capabilities%20of%20models.%20By%20leveraging%20MLLM%2C%20we%20have%0Ademonstrated%20that%20a%20robust%20T2M%20can%20produce%20images%20more%20closely%20resembling%20the%0Areference%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Regeneration%253A%2520Evaluating%2520Text-to-Image%2520Model%2520via%2520Generating%250A%2520%2520Identical%2520Image%2520with%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DChutian%2520Meng%2520and%2520Fan%2520Ma%2520and%2520Jiaxu%2520Miao%2520and%2520Chi%2520Zhang%2520and%2520Yi%2520Yang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520revitalized%2520the%2520image%2520generation%2520domain%252C%2520playing%250Acrucial%2520roles%2520in%2520both%2520academic%2520research%2520and%2520artistic%2520expression.%2520With%2520the%250Aemergence%2520of%2520new%2520diffusion%2520models%252C%2520assessing%2520the%2520performance%2520of%2520text-to-image%250Amodels%2520has%2520become%2520increasingly%2520important.%2520Current%2520metrics%2520focus%2520on%2520directly%250Amatching%2520the%2520input%2520text%2520with%2520the%2520generated%2520image%252C%2520but%2520due%2520to%2520cross-modal%250Ainformation%2520asymmetry%252C%2520this%2520leads%2520to%2520unreliable%2520or%2520incomplete%2520assessment%250Aresults.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520the%2520Image%2520Regeneration%2520task%2520in%2520this%250Astudy%2520to%2520assess%2520text-to-image%2520models%2520by%2520tasking%2520the%2520T2I%2520model%2520with%2520generating%250Aan%2520image%2520according%2520to%2520the%2520reference%2520image.%2520We%2520use%2520GPT4V%2520to%2520bridge%2520the%2520gap%250Abetween%2520the%2520reference%2520image%2520and%2520the%2520text%2520input%2520for%2520the%2520T2I%2520model%252C%2520allowing%2520T2I%250Amodels%2520to%2520understand%2520image%2520content.%2520This%2520evaluation%2520process%2520is%2520simplified%2520as%250Acomparisons%2520between%2520the%2520generated%2520image%2520and%2520the%2520reference%2520image%2520are%250Astraightforward.%2520Two%2520regeneration%2520datasets%2520spanning%2520content-diverse%2520and%250Astyle-diverse%2520evaluation%2520dataset%2520are%2520introduced%2520to%2520evaluate%2520the%2520leading%250Adiffusion%2520models%2520currently%2520available.%2520Additionally%252C%2520we%2520present%2520ImageRepainter%250Aframework%2520to%2520enhance%2520the%2520quality%2520of%2520generated%2520images%2520by%2520improving%2520content%250Acomprehension%2520via%2520MLLM%2520guided%2520iterative%2520generation%2520and%2520revision.%2520Our%250Acomprehensive%2520experiments%2520have%2520showcased%2520the%2520effectiveness%2520of%2520this%2520framework%2520in%250Aassessing%2520the%2520generative%2520capabilities%2520of%2520models.%2520By%2520leveraging%2520MLLM%252C%2520we%2520have%250Ademonstrated%2520that%2520a%2520robust%2520T2M%2520can%2520produce%2520images%2520more%2520closely%2520resembling%2520the%250Areference%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Regeneration%3A%20Evaluating%20Text-to-Image%20Model%20via%20Generating%0A%20%20Identical%20Image%20with%20Multimodal%20Large%20Language%20Models&entry.906535625=Chutian%20Meng%20and%20Fan%20Ma%20and%20Jiaxu%20Miao%20and%20Chi%20Zhang%20and%20Yi%20Yang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Diffusion%20models%20have%20revitalized%20the%20image%20generation%20domain%2C%20playing%0Acrucial%20roles%20in%20both%20academic%20research%20and%20artistic%20expression.%20With%20the%0Aemergence%20of%20new%20diffusion%20models%2C%20assessing%20the%20performance%20of%20text-to-image%0Amodels%20has%20become%20increasingly%20important.%20Current%20metrics%20focus%20on%20directly%0Amatching%20the%20input%20text%20with%20the%20generated%20image%2C%20but%20due%20to%20cross-modal%0Ainformation%20asymmetry%2C%20this%20leads%20to%20unreliable%20or%20incomplete%20assessment%0Aresults.%20Motivated%20by%20this%2C%20we%20introduce%20the%20Image%20Regeneration%20task%20in%20this%0Astudy%20to%20assess%20text-to-image%20models%20by%20tasking%20the%20T2I%20model%20with%20generating%0Aan%20image%20according%20to%20the%20reference%20image.%20We%20use%20GPT4V%20to%20bridge%20the%20gap%0Abetween%20the%20reference%20image%20and%20the%20text%20input%20for%20the%20T2I%20model%2C%20allowing%20T2I%0Amodels%20to%20understand%20image%20content.%20This%20evaluation%20process%20is%20simplified%20as%0Acomparisons%20between%20the%20generated%20image%20and%20the%20reference%20image%20are%0Astraightforward.%20Two%20regeneration%20datasets%20spanning%20content-diverse%20and%0Astyle-diverse%20evaluation%20dataset%20are%20introduced%20to%20evaluate%20the%20leading%0Adiffusion%20models%20currently%20available.%20Additionally%2C%20we%20present%20ImageRepainter%0Aframework%20to%20enhance%20the%20quality%20of%20generated%20images%20by%20improving%20content%0Acomprehension%20via%20MLLM%20guided%20iterative%20generation%20and%20revision.%20Our%0Acomprehensive%20experiments%20have%20showcased%20the%20effectiveness%20of%20this%20framework%20in%0Aassessing%20the%20generative%20capabilities%20of%20models.%20By%20leveraging%20MLLM%2C%20we%20have%0Ademonstrated%20that%20a%20robust%20T2M%20can%20produce%20images%20more%20closely%20resembling%20the%0Areference%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09449v1&entry.124074799=Read"},
{"title": "Are nuclear masks all you need for improved out-of-domain\n  generalisation? A closer look at cancer classification in histopathology", "author": "Dhananjay Tomar and Alexander Binder and Andreas Kleppe", "abstract": "  Domain generalisation in computational histopathology is challenging because\nthe images are substantially affected by differences among hospitals due to\nfactors like fixation and staining of tissue and imaging equipment. We\nhypothesise that focusing on nuclei can improve the out-of-domain (OOD)\ngeneralisation in cancer detection. We propose a simple approach to improve OOD\ngeneralisation for cancer detection by focusing on nuclear morphology and\norganisation, as these are domain-invariant features critical in cancer\ndetection. Our approach integrates original images with nuclear segmentation\nmasks during training, encouraging the model to prioritise nuclei and their\nspatial arrangement. Going beyond mere data augmentation, we introduce a\nregularisation technique that aligns the representations of masks and original\nimages. We show, using multiple datasets, that our method improves OOD\ngeneralisation and also leads to increased robustness to image corruptions and\nadversarial attacks. The source code is available at\nhttps://github.com/undercutspiky/SFL/\n", "link": "http://arxiv.org/abs/2411.09373v1", "date": "2024-11-14", "relevancy": 1.8981, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4823}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20nuclear%20masks%20all%20you%20need%20for%20improved%20out-of-domain%0A%20%20generalisation%3F%20A%20closer%20look%20at%20cancer%20classification%20in%20histopathology&body=Title%3A%20Are%20nuclear%20masks%20all%20you%20need%20for%20improved%20out-of-domain%0A%20%20generalisation%3F%20A%20closer%20look%20at%20cancer%20classification%20in%20histopathology%0AAuthor%3A%20Dhananjay%20Tomar%20and%20Alexander%20Binder%20and%20Andreas%20Kleppe%0AAbstract%3A%20%20%20Domain%20generalisation%20in%20computational%20histopathology%20is%20challenging%20because%0Athe%20images%20are%20substantially%20affected%20by%20differences%20among%20hospitals%20due%20to%0Afactors%20like%20fixation%20and%20staining%20of%20tissue%20and%20imaging%20equipment.%20We%0Ahypothesise%20that%20focusing%20on%20nuclei%20can%20improve%20the%20out-of-domain%20%28OOD%29%0Ageneralisation%20in%20cancer%20detection.%20We%20propose%20a%20simple%20approach%20to%20improve%20OOD%0Ageneralisation%20for%20cancer%20detection%20by%20focusing%20on%20nuclear%20morphology%20and%0Aorganisation%2C%20as%20these%20are%20domain-invariant%20features%20critical%20in%20cancer%0Adetection.%20Our%20approach%20integrates%20original%20images%20with%20nuclear%20segmentation%0Amasks%20during%20training%2C%20encouraging%20the%20model%20to%20prioritise%20nuclei%20and%20their%0Aspatial%20arrangement.%20Going%20beyond%20mere%20data%20augmentation%2C%20we%20introduce%20a%0Aregularisation%20technique%20that%20aligns%20the%20representations%20of%20masks%20and%20original%0Aimages.%20We%20show%2C%20using%20multiple%20datasets%2C%20that%20our%20method%20improves%20OOD%0Ageneralisation%20and%20also%20leads%20to%20increased%20robustness%20to%20image%20corruptions%20and%0Aadversarial%20attacks.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/undercutspiky/SFL/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520nuclear%2520masks%2520all%2520you%2520need%2520for%2520improved%2520out-of-domain%250A%2520%2520generalisation%253F%2520A%2520closer%2520look%2520at%2520cancer%2520classification%2520in%2520histopathology%26entry.906535625%3DDhananjay%2520Tomar%2520and%2520Alexander%2520Binder%2520and%2520Andreas%2520Kleppe%26entry.1292438233%3D%2520%2520Domain%2520generalisation%2520in%2520computational%2520histopathology%2520is%2520challenging%2520because%250Athe%2520images%2520are%2520substantially%2520affected%2520by%2520differences%2520among%2520hospitals%2520due%2520to%250Afactors%2520like%2520fixation%2520and%2520staining%2520of%2520tissue%2520and%2520imaging%2520equipment.%2520We%250Ahypothesise%2520that%2520focusing%2520on%2520nuclei%2520can%2520improve%2520the%2520out-of-domain%2520%2528OOD%2529%250Ageneralisation%2520in%2520cancer%2520detection.%2520We%2520propose%2520a%2520simple%2520approach%2520to%2520improve%2520OOD%250Ageneralisation%2520for%2520cancer%2520detection%2520by%2520focusing%2520on%2520nuclear%2520morphology%2520and%250Aorganisation%252C%2520as%2520these%2520are%2520domain-invariant%2520features%2520critical%2520in%2520cancer%250Adetection.%2520Our%2520approach%2520integrates%2520original%2520images%2520with%2520nuclear%2520segmentation%250Amasks%2520during%2520training%252C%2520encouraging%2520the%2520model%2520to%2520prioritise%2520nuclei%2520and%2520their%250Aspatial%2520arrangement.%2520Going%2520beyond%2520mere%2520data%2520augmentation%252C%2520we%2520introduce%2520a%250Aregularisation%2520technique%2520that%2520aligns%2520the%2520representations%2520of%2520masks%2520and%2520original%250Aimages.%2520We%2520show%252C%2520using%2520multiple%2520datasets%252C%2520that%2520our%2520method%2520improves%2520OOD%250Ageneralisation%2520and%2520also%2520leads%2520to%2520increased%2520robustness%2520to%2520image%2520corruptions%2520and%250Aadversarial%2520attacks.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/undercutspiky/SFL/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20nuclear%20masks%20all%20you%20need%20for%20improved%20out-of-domain%0A%20%20generalisation%3F%20A%20closer%20look%20at%20cancer%20classification%20in%20histopathology&entry.906535625=Dhananjay%20Tomar%20and%20Alexander%20Binder%20and%20Andreas%20Kleppe&entry.1292438233=%20%20Domain%20generalisation%20in%20computational%20histopathology%20is%20challenging%20because%0Athe%20images%20are%20substantially%20affected%20by%20differences%20among%20hospitals%20due%20to%0Afactors%20like%20fixation%20and%20staining%20of%20tissue%20and%20imaging%20equipment.%20We%0Ahypothesise%20that%20focusing%20on%20nuclei%20can%20improve%20the%20out-of-domain%20%28OOD%29%0Ageneralisation%20in%20cancer%20detection.%20We%20propose%20a%20simple%20approach%20to%20improve%20OOD%0Ageneralisation%20for%20cancer%20detection%20by%20focusing%20on%20nuclear%20morphology%20and%0Aorganisation%2C%20as%20these%20are%20domain-invariant%20features%20critical%20in%20cancer%0Adetection.%20Our%20approach%20integrates%20original%20images%20with%20nuclear%20segmentation%0Amasks%20during%20training%2C%20encouraging%20the%20model%20to%20prioritise%20nuclei%20and%20their%0Aspatial%20arrangement.%20Going%20beyond%20mere%20data%20augmentation%2C%20we%20introduce%20a%0Aregularisation%20technique%20that%20aligns%20the%20representations%20of%20masks%20and%20original%0Aimages.%20We%20show%2C%20using%20multiple%20datasets%2C%20that%20our%20method%20improves%20OOD%0Ageneralisation%20and%20also%20leads%20to%20increased%20robustness%20to%20image%20corruptions%20and%0Aadversarial%20attacks.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/undercutspiky/SFL/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09373v1&entry.124074799=Read"},
{"title": "Expert Study on Interpretable Machine Learning Models with Missing Data", "author": "Lena Stempfle and Arthur James and Julie Josse and Tobias Gauss and Fredrik D. Johansson", "abstract": "  Inherently interpretable machine learning (IML) models provide valuable\ninsights for clinical decision-making but face challenges when features have\nmissing values. Classical solutions like imputation or excluding incomplete\nrecords are often unsuitable in applications where values are missing at test\ntime. In this work, we conducted a survey with 71 clinicians from 29 trauma\ncenters across France, including 20 complete responses to study the interaction\nbetween medical professionals and IML applied to data with missing values. This\nprovided valuable insights into how missing data is interpreted in clinical\nmachine learning. We used the prediction of hemorrhagic shock as a concrete\nexample to gauge the willingness and readiness of the participants to adopt IML\nmodels from three classes of methods. Our findings show that, while clinicians\nvalue interpretability and are familiar with common IML methods, classical\nimputation techniques often misalign with their intuition, and that models that\nnatively handle missing values are preferred. These results emphasize the need\nto integrate clinical intuition into future IML models for better\nhuman-computer interaction.\n", "link": "http://arxiv.org/abs/2411.09591v1", "date": "2024-11-14", "relevancy": 1.891, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert%20Study%20on%20Interpretable%20Machine%20Learning%20Models%20with%20Missing%20Data&body=Title%3A%20Expert%20Study%20on%20Interpretable%20Machine%20Learning%20Models%20with%20Missing%20Data%0AAuthor%3A%20Lena%20Stempfle%20and%20Arthur%20James%20and%20Julie%20Josse%20and%20Tobias%20Gauss%20and%20Fredrik%20D.%20Johansson%0AAbstract%3A%20%20%20Inherently%20interpretable%20machine%20learning%20%28IML%29%20models%20provide%20valuable%0Ainsights%20for%20clinical%20decision-making%20but%20face%20challenges%20when%20features%20have%0Amissing%20values.%20Classical%20solutions%20like%20imputation%20or%20excluding%20incomplete%0Arecords%20are%20often%20unsuitable%20in%20applications%20where%20values%20are%20missing%20at%20test%0Atime.%20In%20this%20work%2C%20we%20conducted%20a%20survey%20with%2071%20clinicians%20from%2029%20trauma%0Acenters%20across%20France%2C%20including%2020%20complete%20responses%20to%20study%20the%20interaction%0Abetween%20medical%20professionals%20and%20IML%20applied%20to%20data%20with%20missing%20values.%20This%0Aprovided%20valuable%20insights%20into%20how%20missing%20data%20is%20interpreted%20in%20clinical%0Amachine%20learning.%20We%20used%20the%20prediction%20of%20hemorrhagic%20shock%20as%20a%20concrete%0Aexample%20to%20gauge%20the%20willingness%20and%20readiness%20of%20the%20participants%20to%20adopt%20IML%0Amodels%20from%20three%20classes%20of%20methods.%20Our%20findings%20show%20that%2C%20while%20clinicians%0Avalue%20interpretability%20and%20are%20familiar%20with%20common%20IML%20methods%2C%20classical%0Aimputation%20techniques%20often%20misalign%20with%20their%20intuition%2C%20and%20that%20models%20that%0Anatively%20handle%20missing%20values%20are%20preferred.%20These%20results%20emphasize%20the%20need%0Ato%20integrate%20clinical%20intuition%20into%20future%20IML%20models%20for%20better%0Ahuman-computer%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert%2520Study%2520on%2520Interpretable%2520Machine%2520Learning%2520Models%2520with%2520Missing%2520Data%26entry.906535625%3DLena%2520Stempfle%2520and%2520Arthur%2520James%2520and%2520Julie%2520Josse%2520and%2520Tobias%2520Gauss%2520and%2520Fredrik%2520D.%2520Johansson%26entry.1292438233%3D%2520%2520Inherently%2520interpretable%2520machine%2520learning%2520%2528IML%2529%2520models%2520provide%2520valuable%250Ainsights%2520for%2520clinical%2520decision-making%2520but%2520face%2520challenges%2520when%2520features%2520have%250Amissing%2520values.%2520Classical%2520solutions%2520like%2520imputation%2520or%2520excluding%2520incomplete%250Arecords%2520are%2520often%2520unsuitable%2520in%2520applications%2520where%2520values%2520are%2520missing%2520at%2520test%250Atime.%2520In%2520this%2520work%252C%2520we%2520conducted%2520a%2520survey%2520with%252071%2520clinicians%2520from%252029%2520trauma%250Acenters%2520across%2520France%252C%2520including%252020%2520complete%2520responses%2520to%2520study%2520the%2520interaction%250Abetween%2520medical%2520professionals%2520and%2520IML%2520applied%2520to%2520data%2520with%2520missing%2520values.%2520This%250Aprovided%2520valuable%2520insights%2520into%2520how%2520missing%2520data%2520is%2520interpreted%2520in%2520clinical%250Amachine%2520learning.%2520We%2520used%2520the%2520prediction%2520of%2520hemorrhagic%2520shock%2520as%2520a%2520concrete%250Aexample%2520to%2520gauge%2520the%2520willingness%2520and%2520readiness%2520of%2520the%2520participants%2520to%2520adopt%2520IML%250Amodels%2520from%2520three%2520classes%2520of%2520methods.%2520Our%2520findings%2520show%2520that%252C%2520while%2520clinicians%250Avalue%2520interpretability%2520and%2520are%2520familiar%2520with%2520common%2520IML%2520methods%252C%2520classical%250Aimputation%2520techniques%2520often%2520misalign%2520with%2520their%2520intuition%252C%2520and%2520that%2520models%2520that%250Anatively%2520handle%2520missing%2520values%2520are%2520preferred.%2520These%2520results%2520emphasize%2520the%2520need%250Ato%2520integrate%2520clinical%2520intuition%2520into%2520future%2520IML%2520models%2520for%2520better%250Ahuman-computer%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert%20Study%20on%20Interpretable%20Machine%20Learning%20Models%20with%20Missing%20Data&entry.906535625=Lena%20Stempfle%20and%20Arthur%20James%20and%20Julie%20Josse%20and%20Tobias%20Gauss%20and%20Fredrik%20D.%20Johansson&entry.1292438233=%20%20Inherently%20interpretable%20machine%20learning%20%28IML%29%20models%20provide%20valuable%0Ainsights%20for%20clinical%20decision-making%20but%20face%20challenges%20when%20features%20have%0Amissing%20values.%20Classical%20solutions%20like%20imputation%20or%20excluding%20incomplete%0Arecords%20are%20often%20unsuitable%20in%20applications%20where%20values%20are%20missing%20at%20test%0Atime.%20In%20this%20work%2C%20we%20conducted%20a%20survey%20with%2071%20clinicians%20from%2029%20trauma%0Acenters%20across%20France%2C%20including%2020%20complete%20responses%20to%20study%20the%20interaction%0Abetween%20medical%20professionals%20and%20IML%20applied%20to%20data%20with%20missing%20values.%20This%0Aprovided%20valuable%20insights%20into%20how%20missing%20data%20is%20interpreted%20in%20clinical%0Amachine%20learning.%20We%20used%20the%20prediction%20of%20hemorrhagic%20shock%20as%20a%20concrete%0Aexample%20to%20gauge%20the%20willingness%20and%20readiness%20of%20the%20participants%20to%20adopt%20IML%0Amodels%20from%20three%20classes%20of%20methods.%20Our%20findings%20show%20that%2C%20while%20clinicians%0Avalue%20interpretability%20and%20are%20familiar%20with%20common%20IML%20methods%2C%20classical%0Aimputation%20techniques%20often%20misalign%20with%20their%20intuition%2C%20and%20that%20models%20that%0Anatively%20handle%20missing%20values%20are%20preferred.%20These%20results%20emphasize%20the%20need%0Ato%20integrate%20clinical%20intuition%20into%20future%20IML%20models%20for%20better%0Ahuman-computer%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09591v1&entry.124074799=Read"},
{"title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding", "author": "Vojtech Formanek and Ondrej Sotolar", "abstract": "  A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.\n", "link": "http://arxiv.org/abs/2411.05777v2", "date": "2024-11-14", "relevancy": 1.877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding&body=Title%3A%20Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding%0AAuthor%3A%20Vojtech%20Formanek%20and%20Ondrej%20Sotolar%0AAbstract%3A%20%20%20A%20growing%20amount%20of%20literature%20critiques%20the%20current%20operationalizations%20of%0Aempathy%20based%20on%20loose%20definitions%20of%20the%20construct.%20Such%20definitions%0Anegatively%20affect%20dataset%20quality%2C%20model%20robustness%2C%20and%20evaluation%0Areliability.%20We%20propose%20an%20empathy%20evaluation%20framework%20that%20operationalizes%0Aempathy%20close%20to%20its%20psychological%20origins.%20The%20framework%20measures%20the%20variance%0Ain%20responses%20of%20LLMs%20to%20prompts%20using%20existing%20metrics%20for%20empathy%20and%0Aemotional%20valence.%20The%20variance%20is%20introduced%20through%20the%20controlled%20generation%0Aof%20the%20prompts%20by%20varying%20social%20biases%20affecting%20context%20understanding%2C%20thus%0Aimpacting%20empathetic%20understanding.%20The%20control%20over%20generation%20ensures%20high%0Atheoretical%20validity%20of%20the%20constructs%20in%20the%20prompt%20dataset.%20Also%2C%20it%20makes%0Ahigh-quality%20translation%2C%20especially%20into%20languages%20that%20currently%20have%0Alittle-to-no%20way%20of%20evaluating%20empathy%20or%20bias%2C%20such%20as%20the%20Slavonic%20family%2C%0Amore%20manageable.%20Using%20chosen%20LLMs%20and%20various%20prompt%20types%2C%20we%20demonstrate%20the%0Aempathy%20evaluation%20with%20the%20framework%2C%20including%20multiple-choice%20answers%20and%0Afree%20generation.%20The%20variance%20in%20our%20initial%20evaluation%20sample%20is%20small%20and%20we%0Awere%20unable%20to%20measure%20convincing%20differences%20between%20the%20empathetic%0Aunderstanding%20in%20contexts%20given%20by%20different%20social%20groups.%20However%2C%20the%0Aresults%20are%20promising%20because%20the%20models%20showed%20significant%20alterations%20their%0Areasoning%20chains%20needed%20to%20capture%20the%20relatively%20subtle%20changes%20in%20the%0Aprompts.%20This%20provides%20the%20basis%20for%20future%20research%20into%20the%20construction%20of%0Athe%20evaluation%20sample%20and%20statistical%20methods%20for%20measuring%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantitative%2520Assessment%2520of%2520Intersectional%2520Empathetic%2520Bias%2520and%250A%2520%2520Understanding%26entry.906535625%3DVojtech%2520Formanek%2520and%2520Ondrej%2520Sotolar%26entry.1292438233%3D%2520%2520A%2520growing%2520amount%2520of%2520literature%2520critiques%2520the%2520current%2520operationalizations%2520of%250Aempathy%2520based%2520on%2520loose%2520definitions%2520of%2520the%2520construct.%2520Such%2520definitions%250Anegatively%2520affect%2520dataset%2520quality%252C%2520model%2520robustness%252C%2520and%2520evaluation%250Areliability.%2520We%2520propose%2520an%2520empathy%2520evaluation%2520framework%2520that%2520operationalizes%250Aempathy%2520close%2520to%2520its%2520psychological%2520origins.%2520The%2520framework%2520measures%2520the%2520variance%250Ain%2520responses%2520of%2520LLMs%2520to%2520prompts%2520using%2520existing%2520metrics%2520for%2520empathy%2520and%250Aemotional%2520valence.%2520The%2520variance%2520is%2520introduced%2520through%2520the%2520controlled%2520generation%250Aof%2520the%2520prompts%2520by%2520varying%2520social%2520biases%2520affecting%2520context%2520understanding%252C%2520thus%250Aimpacting%2520empathetic%2520understanding.%2520The%2520control%2520over%2520generation%2520ensures%2520high%250Atheoretical%2520validity%2520of%2520the%2520constructs%2520in%2520the%2520prompt%2520dataset.%2520Also%252C%2520it%2520makes%250Ahigh-quality%2520translation%252C%2520especially%2520into%2520languages%2520that%2520currently%2520have%250Alittle-to-no%2520way%2520of%2520evaluating%2520empathy%2520or%2520bias%252C%2520such%2520as%2520the%2520Slavonic%2520family%252C%250Amore%2520manageable.%2520Using%2520chosen%2520LLMs%2520and%2520various%2520prompt%2520types%252C%2520we%2520demonstrate%2520the%250Aempathy%2520evaluation%2520with%2520the%2520framework%252C%2520including%2520multiple-choice%2520answers%2520and%250Afree%2520generation.%2520The%2520variance%2520in%2520our%2520initial%2520evaluation%2520sample%2520is%2520small%2520and%2520we%250Awere%2520unable%2520to%2520measure%2520convincing%2520differences%2520between%2520the%2520empathetic%250Aunderstanding%2520in%2520contexts%2520given%2520by%2520different%2520social%2520groups.%2520However%252C%2520the%250Aresults%2520are%2520promising%2520because%2520the%2520models%2520showed%2520significant%2520alterations%2520their%250Areasoning%2520chains%2520needed%2520to%2520capture%2520the%2520relatively%2520subtle%2520changes%2520in%2520the%250Aprompts.%2520This%2520provides%2520the%2520basis%2520for%2520future%2520research%2520into%2520the%2520construction%2520of%250Athe%2520evaluation%2520sample%2520and%2520statistical%2520methods%2520for%2520measuring%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding&entry.906535625=Vojtech%20Formanek%20and%20Ondrej%20Sotolar&entry.1292438233=%20%20A%20growing%20amount%20of%20literature%20critiques%20the%20current%20operationalizations%20of%0Aempathy%20based%20on%20loose%20definitions%20of%20the%20construct.%20Such%20definitions%0Anegatively%20affect%20dataset%20quality%2C%20model%20robustness%2C%20and%20evaluation%0Areliability.%20We%20propose%20an%20empathy%20evaluation%20framework%20that%20operationalizes%0Aempathy%20close%20to%20its%20psychological%20origins.%20The%20framework%20measures%20the%20variance%0Ain%20responses%20of%20LLMs%20to%20prompts%20using%20existing%20metrics%20for%20empathy%20and%0Aemotional%20valence.%20The%20variance%20is%20introduced%20through%20the%20controlled%20generation%0Aof%20the%20prompts%20by%20varying%20social%20biases%20affecting%20context%20understanding%2C%20thus%0Aimpacting%20empathetic%20understanding.%20The%20control%20over%20generation%20ensures%20high%0Atheoretical%20validity%20of%20the%20constructs%20in%20the%20prompt%20dataset.%20Also%2C%20it%20makes%0Ahigh-quality%20translation%2C%20especially%20into%20languages%20that%20currently%20have%0Alittle-to-no%20way%20of%20evaluating%20empathy%20or%20bias%2C%20such%20as%20the%20Slavonic%20family%2C%0Amore%20manageable.%20Using%20chosen%20LLMs%20and%20various%20prompt%20types%2C%20we%20demonstrate%20the%0Aempathy%20evaluation%20with%20the%20framework%2C%20including%20multiple-choice%20answers%20and%0Afree%20generation.%20The%20variance%20in%20our%20initial%20evaluation%20sample%20is%20small%20and%20we%0Awere%20unable%20to%20measure%20convincing%20differences%20between%20the%20empathetic%0Aunderstanding%20in%20contexts%20given%20by%20different%20social%20groups.%20However%2C%20the%0Aresults%20are%20promising%20because%20the%20models%20showed%20significant%20alterations%20their%0Areasoning%20chains%20needed%20to%20capture%20the%20relatively%20subtle%20changes%20in%20the%0Aprompts.%20This%20provides%20the%20basis%20for%20future%20research%20into%20the%20construction%20of%0Athe%20evaluation%20sample%20and%20statistical%20methods%20for%20measuring%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05777v2&entry.124074799=Read"},
{"title": "MICCAI-CDMRI 2023 QuantConn Challenge Findings on Achieving Robust\n  Quantitative Connectivity through Harmonized Preprocessing of Diffusion MRI", "author": "Nancy R. Newlin and Kurt Schilling and Serge Koudoro and Bramsh Qamar Chandio and Praitayini Kanakaraj and Daniel Moyer and Claire E. Kelly and Sila Genc and Jian Chen and Joseph Yuan-Mou Yang and Ye Wu and Yifei He and Jiawei Zhang and Qingrun Zeng and Fan Zhang and Nagesh Adluru and Vishwesh Nath and Sudhir Pathak and Walter Schneider and Anurag Gade and Yogesh Rathi and Tom Hendriks and Anna Vilanova and Maxime Chamberland and Tomasz Pieciak and Dominika Ciupek and Antonio Trist\u00e1n Vega and Santiago Aja-Fern\u00e1ndez and Maciej Malawski and Gani Ouedraogo and Julia Machnio and Christian Ewert and Paul M. Thompson and Neda Jahanshad and Eleftherios Garyfallidis and Bennett A. Landman", "abstract": "  White matter alterations are increasingly implicated in neurological diseases\nand their progression. International-scale studies use diffusion-weighted\nmagnetic resonance imaging (DW-MRI) to qualitatively identify changes in white\nmatter microstructure and connectivity. Yet, quantitative analysis of DW-MRI\ndata is hindered by inconsistencies stemming from varying acquisition\nprotocols. There is a pressing need to harmonize the preprocessing of DW-MRI\ndatasets to ensure the derivation of robust quantitative diffusion metrics\nacross acquisitions. In the MICCAI-CDMRI 2023 QuantConn challenge, participants\nwere provided raw data from the same individuals collected on the same scanner\nbut with two different acquisitions and tasked with preprocessing the DW-MRI to\nminimize acquisition differences while retaining biological variation.\nSubmissions are evaluated on the reproducibility and comparability of\ncross-acquisition bundle-wise microstructure measures, bundle shape features,\nand connectomics. The key innovations of the QuantConn challenge are that (1)\nwe assess bundles and tractography in the context of harmonization for the\nfirst time, (2) we assess connectomics in the context of harmonization for the\nfirst time, and (3) we have 10x additional subjects over prior harmonization\nchallenge, MUSHAC and 100x over SuperMUDI. We find that bundle surface area,\nfractional anisotropy, connectome assortativity, betweenness centrality, edge\ncount, modularity, nodal strength, and participation coefficient measures are\nmost biased by acquisition and that machine learning voxel-wise correction,\nRISH mapping, and NeSH methods effectively reduce these biases. In addition,\nmicrostructure measures AD, MD, RD, bundle length, connectome density,\nefficiency, and path length are least biased by these acquisition differences.\n", "link": "http://arxiv.org/abs/2411.09618v1", "date": "2024-11-14", "relevancy": 1.8762, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MICCAI-CDMRI%202023%20QuantConn%20Challenge%20Findings%20on%20Achieving%20Robust%0A%20%20Quantitative%20Connectivity%20through%20Harmonized%20Preprocessing%20of%20Diffusion%20MRI&body=Title%3A%20MICCAI-CDMRI%202023%20QuantConn%20Challenge%20Findings%20on%20Achieving%20Robust%0A%20%20Quantitative%20Connectivity%20through%20Harmonized%20Preprocessing%20of%20Diffusion%20MRI%0AAuthor%3A%20Nancy%20R.%20Newlin%20and%20Kurt%20Schilling%20and%20Serge%20Koudoro%20and%20Bramsh%20Qamar%20Chandio%20and%20Praitayini%20Kanakaraj%20and%20Daniel%20Moyer%20and%20Claire%20E.%20Kelly%20and%20Sila%20Genc%20and%20Jian%20Chen%20and%20Joseph%20Yuan-Mou%20Yang%20and%20Ye%20Wu%20and%20Yifei%20He%20and%20Jiawei%20Zhang%20and%20Qingrun%20Zeng%20and%20Fan%20Zhang%20and%20Nagesh%20Adluru%20and%20Vishwesh%20Nath%20and%20Sudhir%20Pathak%20and%20Walter%20Schneider%20and%20Anurag%20Gade%20and%20Yogesh%20Rathi%20and%20Tom%20Hendriks%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland%20and%20Tomasz%20Pieciak%20and%20Dominika%20Ciupek%20and%20Antonio%20Trist%C3%A1n%20Vega%20and%20Santiago%20Aja-Fern%C3%A1ndez%20and%20Maciej%20Malawski%20and%20Gani%20Ouedraogo%20and%20Julia%20Machnio%20and%20Christian%20Ewert%20and%20Paul%20M.%20Thompson%20and%20Neda%20Jahanshad%20and%20Eleftherios%20Garyfallidis%20and%20Bennett%20A.%20Landman%0AAbstract%3A%20%20%20White%20matter%20alterations%20are%20increasingly%20implicated%20in%20neurological%20diseases%0Aand%20their%20progression.%20International-scale%20studies%20use%20diffusion-weighted%0Amagnetic%20resonance%20imaging%20%28DW-MRI%29%20to%20qualitatively%20identify%20changes%20in%20white%0Amatter%20microstructure%20and%20connectivity.%20Yet%2C%20quantitative%20analysis%20of%20DW-MRI%0Adata%20is%20hindered%20by%20inconsistencies%20stemming%20from%20varying%20acquisition%0Aprotocols.%20There%20is%20a%20pressing%20need%20to%20harmonize%20the%20preprocessing%20of%20DW-MRI%0Adatasets%20to%20ensure%20the%20derivation%20of%20robust%20quantitative%20diffusion%20metrics%0Aacross%20acquisitions.%20In%20the%20MICCAI-CDMRI%202023%20QuantConn%20challenge%2C%20participants%0Awere%20provided%20raw%20data%20from%20the%20same%20individuals%20collected%20on%20the%20same%20scanner%0Abut%20with%20two%20different%20acquisitions%20and%20tasked%20with%20preprocessing%20the%20DW-MRI%20to%0Aminimize%20acquisition%20differences%20while%20retaining%20biological%20variation.%0ASubmissions%20are%20evaluated%20on%20the%20reproducibility%20and%20comparability%20of%0Across-acquisition%20bundle-wise%20microstructure%20measures%2C%20bundle%20shape%20features%2C%0Aand%20connectomics.%20The%20key%20innovations%20of%20the%20QuantConn%20challenge%20are%20that%20%281%29%0Awe%20assess%20bundles%20and%20tractography%20in%20the%20context%20of%20harmonization%20for%20the%0Afirst%20time%2C%20%282%29%20we%20assess%20connectomics%20in%20the%20context%20of%20harmonization%20for%20the%0Afirst%20time%2C%20and%20%283%29%20we%20have%2010x%20additional%20subjects%20over%20prior%20harmonization%0Achallenge%2C%20MUSHAC%20and%20100x%20over%20SuperMUDI.%20We%20find%20that%20bundle%20surface%20area%2C%0Afractional%20anisotropy%2C%20connectome%20assortativity%2C%20betweenness%20centrality%2C%20edge%0Acount%2C%20modularity%2C%20nodal%20strength%2C%20and%20participation%20coefficient%20measures%20are%0Amost%20biased%20by%20acquisition%20and%20that%20machine%20learning%20voxel-wise%20correction%2C%0ARISH%20mapping%2C%20and%20NeSH%20methods%20effectively%20reduce%20these%20biases.%20In%20addition%2C%0Amicrostructure%20measures%20AD%2C%20MD%2C%20RD%2C%20bundle%20length%2C%20connectome%20density%2C%0Aefficiency%2C%20and%20path%20length%20are%20least%20biased%20by%20these%20acquisition%20differences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMICCAI-CDMRI%25202023%2520QuantConn%2520Challenge%2520Findings%2520on%2520Achieving%2520Robust%250A%2520%2520Quantitative%2520Connectivity%2520through%2520Harmonized%2520Preprocessing%2520of%2520Diffusion%2520MRI%26entry.906535625%3DNancy%2520R.%2520Newlin%2520and%2520Kurt%2520Schilling%2520and%2520Serge%2520Koudoro%2520and%2520Bramsh%2520Qamar%2520Chandio%2520and%2520Praitayini%2520Kanakaraj%2520and%2520Daniel%2520Moyer%2520and%2520Claire%2520E.%2520Kelly%2520and%2520Sila%2520Genc%2520and%2520Jian%2520Chen%2520and%2520Joseph%2520Yuan-Mou%2520Yang%2520and%2520Ye%2520Wu%2520and%2520Yifei%2520He%2520and%2520Jiawei%2520Zhang%2520and%2520Qingrun%2520Zeng%2520and%2520Fan%2520Zhang%2520and%2520Nagesh%2520Adluru%2520and%2520Vishwesh%2520Nath%2520and%2520Sudhir%2520Pathak%2520and%2520Walter%2520Schneider%2520and%2520Anurag%2520Gade%2520and%2520Yogesh%2520Rathi%2520and%2520Tom%2520Hendriks%2520and%2520Anna%2520Vilanova%2520and%2520Maxime%2520Chamberland%2520and%2520Tomasz%2520Pieciak%2520and%2520Dominika%2520Ciupek%2520and%2520Antonio%2520Trist%25C3%25A1n%2520Vega%2520and%2520Santiago%2520Aja-Fern%25C3%25A1ndez%2520and%2520Maciej%2520Malawski%2520and%2520Gani%2520Ouedraogo%2520and%2520Julia%2520Machnio%2520and%2520Christian%2520Ewert%2520and%2520Paul%2520M.%2520Thompson%2520and%2520Neda%2520Jahanshad%2520and%2520Eleftherios%2520Garyfallidis%2520and%2520Bennett%2520A.%2520Landman%26entry.1292438233%3D%2520%2520White%2520matter%2520alterations%2520are%2520increasingly%2520implicated%2520in%2520neurological%2520diseases%250Aand%2520their%2520progression.%2520International-scale%2520studies%2520use%2520diffusion-weighted%250Amagnetic%2520resonance%2520imaging%2520%2528DW-MRI%2529%2520to%2520qualitatively%2520identify%2520changes%2520in%2520white%250Amatter%2520microstructure%2520and%2520connectivity.%2520Yet%252C%2520quantitative%2520analysis%2520of%2520DW-MRI%250Adata%2520is%2520hindered%2520by%2520inconsistencies%2520stemming%2520from%2520varying%2520acquisition%250Aprotocols.%2520There%2520is%2520a%2520pressing%2520need%2520to%2520harmonize%2520the%2520preprocessing%2520of%2520DW-MRI%250Adatasets%2520to%2520ensure%2520the%2520derivation%2520of%2520robust%2520quantitative%2520diffusion%2520metrics%250Aacross%2520acquisitions.%2520In%2520the%2520MICCAI-CDMRI%25202023%2520QuantConn%2520challenge%252C%2520participants%250Awere%2520provided%2520raw%2520data%2520from%2520the%2520same%2520individuals%2520collected%2520on%2520the%2520same%2520scanner%250Abut%2520with%2520two%2520different%2520acquisitions%2520and%2520tasked%2520with%2520preprocessing%2520the%2520DW-MRI%2520to%250Aminimize%2520acquisition%2520differences%2520while%2520retaining%2520biological%2520variation.%250ASubmissions%2520are%2520evaluated%2520on%2520the%2520reproducibility%2520and%2520comparability%2520of%250Across-acquisition%2520bundle-wise%2520microstructure%2520measures%252C%2520bundle%2520shape%2520features%252C%250Aand%2520connectomics.%2520The%2520key%2520innovations%2520of%2520the%2520QuantConn%2520challenge%2520are%2520that%2520%25281%2529%250Awe%2520assess%2520bundles%2520and%2520tractography%2520in%2520the%2520context%2520of%2520harmonization%2520for%2520the%250Afirst%2520time%252C%2520%25282%2529%2520we%2520assess%2520connectomics%2520in%2520the%2520context%2520of%2520harmonization%2520for%2520the%250Afirst%2520time%252C%2520and%2520%25283%2529%2520we%2520have%252010x%2520additional%2520subjects%2520over%2520prior%2520harmonization%250Achallenge%252C%2520MUSHAC%2520and%2520100x%2520over%2520SuperMUDI.%2520We%2520find%2520that%2520bundle%2520surface%2520area%252C%250Afractional%2520anisotropy%252C%2520connectome%2520assortativity%252C%2520betweenness%2520centrality%252C%2520edge%250Acount%252C%2520modularity%252C%2520nodal%2520strength%252C%2520and%2520participation%2520coefficient%2520measures%2520are%250Amost%2520biased%2520by%2520acquisition%2520and%2520that%2520machine%2520learning%2520voxel-wise%2520correction%252C%250ARISH%2520mapping%252C%2520and%2520NeSH%2520methods%2520effectively%2520reduce%2520these%2520biases.%2520In%2520addition%252C%250Amicrostructure%2520measures%2520AD%252C%2520MD%252C%2520RD%252C%2520bundle%2520length%252C%2520connectome%2520density%252C%250Aefficiency%252C%2520and%2520path%2520length%2520are%2520least%2520biased%2520by%2520these%2520acquisition%2520differences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MICCAI-CDMRI%202023%20QuantConn%20Challenge%20Findings%20on%20Achieving%20Robust%0A%20%20Quantitative%20Connectivity%20through%20Harmonized%20Preprocessing%20of%20Diffusion%20MRI&entry.906535625=Nancy%20R.%20Newlin%20and%20Kurt%20Schilling%20and%20Serge%20Koudoro%20and%20Bramsh%20Qamar%20Chandio%20and%20Praitayini%20Kanakaraj%20and%20Daniel%20Moyer%20and%20Claire%20E.%20Kelly%20and%20Sila%20Genc%20and%20Jian%20Chen%20and%20Joseph%20Yuan-Mou%20Yang%20and%20Ye%20Wu%20and%20Yifei%20He%20and%20Jiawei%20Zhang%20and%20Qingrun%20Zeng%20and%20Fan%20Zhang%20and%20Nagesh%20Adluru%20and%20Vishwesh%20Nath%20and%20Sudhir%20Pathak%20and%20Walter%20Schneider%20and%20Anurag%20Gade%20and%20Yogesh%20Rathi%20and%20Tom%20Hendriks%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland%20and%20Tomasz%20Pieciak%20and%20Dominika%20Ciupek%20and%20Antonio%20Trist%C3%A1n%20Vega%20and%20Santiago%20Aja-Fern%C3%A1ndez%20and%20Maciej%20Malawski%20and%20Gani%20Ouedraogo%20and%20Julia%20Machnio%20and%20Christian%20Ewert%20and%20Paul%20M.%20Thompson%20and%20Neda%20Jahanshad%20and%20Eleftherios%20Garyfallidis%20and%20Bennett%20A.%20Landman&entry.1292438233=%20%20White%20matter%20alterations%20are%20increasingly%20implicated%20in%20neurological%20diseases%0Aand%20their%20progression.%20International-scale%20studies%20use%20diffusion-weighted%0Amagnetic%20resonance%20imaging%20%28DW-MRI%29%20to%20qualitatively%20identify%20changes%20in%20white%0Amatter%20microstructure%20and%20connectivity.%20Yet%2C%20quantitative%20analysis%20of%20DW-MRI%0Adata%20is%20hindered%20by%20inconsistencies%20stemming%20from%20varying%20acquisition%0Aprotocols.%20There%20is%20a%20pressing%20need%20to%20harmonize%20the%20preprocessing%20of%20DW-MRI%0Adatasets%20to%20ensure%20the%20derivation%20of%20robust%20quantitative%20diffusion%20metrics%0Aacross%20acquisitions.%20In%20the%20MICCAI-CDMRI%202023%20QuantConn%20challenge%2C%20participants%0Awere%20provided%20raw%20data%20from%20the%20same%20individuals%20collected%20on%20the%20same%20scanner%0Abut%20with%20two%20different%20acquisitions%20and%20tasked%20with%20preprocessing%20the%20DW-MRI%20to%0Aminimize%20acquisition%20differences%20while%20retaining%20biological%20variation.%0ASubmissions%20are%20evaluated%20on%20the%20reproducibility%20and%20comparability%20of%0Across-acquisition%20bundle-wise%20microstructure%20measures%2C%20bundle%20shape%20features%2C%0Aand%20connectomics.%20The%20key%20innovations%20of%20the%20QuantConn%20challenge%20are%20that%20%281%29%0Awe%20assess%20bundles%20and%20tractography%20in%20the%20context%20of%20harmonization%20for%20the%0Afirst%20time%2C%20%282%29%20we%20assess%20connectomics%20in%20the%20context%20of%20harmonization%20for%20the%0Afirst%20time%2C%20and%20%283%29%20we%20have%2010x%20additional%20subjects%20over%20prior%20harmonization%0Achallenge%2C%20MUSHAC%20and%20100x%20over%20SuperMUDI.%20We%20find%20that%20bundle%20surface%20area%2C%0Afractional%20anisotropy%2C%20connectome%20assortativity%2C%20betweenness%20centrality%2C%20edge%0Acount%2C%20modularity%2C%20nodal%20strength%2C%20and%20participation%20coefficient%20measures%20are%0Amost%20biased%20by%20acquisition%20and%20that%20machine%20learning%20voxel-wise%20correction%2C%0ARISH%20mapping%2C%20and%20NeSH%20methods%20effectively%20reduce%20these%20biases.%20In%20addition%2C%0Amicrostructure%20measures%20AD%2C%20MD%2C%20RD%2C%20bundle%20length%2C%20connectome%20density%2C%0Aefficiency%2C%20and%20path%20length%20are%20least%20biased%20by%20these%20acquisition%20differences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09618v1&entry.124074799=Read"},
{"title": "AI-driven inverse design of materials: Past, present and future", "author": "Xiao-Qi Han and Xin-De Wang and Meng-Yuan Xu and Zhen Feng and Bo-Wen Yao and Peng-Jie Guo and Ze-Feng Gao and Zhong-Yi Lu", "abstract": "  The discovery of advanced materials is the cornerstone of human technological\ndevelopment and progress. The structures of materials and their corresponding\nproperties are essentially the result of a complex interplay of multiple\ndegrees of freedom such as lattice, charge, spin, symmetry, and topology. This\nposes significant challenges for the inverse design methods of materials.\nHumans have long explored new materials through a large number of experiments\nand proposed corresponding theoretical systems to predict new material\nproperties and structures. With the improvement of computational power,\nresearchers have gradually developed various electronic structure calculation\nmethods, particularly such as the one based density functional theory, as well\nas high-throughput computational methods. Recently, the rapid development of\nartificial intelligence technology in the field of computer science has enabled\nthe effective characterization of the implicit association between material\nproperties and structures, thus opening up an efficient paradigm for the\ninverse design of functional materials. A significant progress has been made in\ninverse design of materials based on generative and discriminative models,\nattracting widespread attention from researchers. Considering this rapid\ntechnological progress, in this survey, we look back on the latest advancements\nin AI-driven inverse design of materials by introducing the background, key\nfindings, and mainstream technological development routes. In addition, we\nsummarize the remaining issues for future directions. This survey provides the\nlatest overview of AI-driven inverse design of materials, which can serve as a\nuseful resource for researchers.\n", "link": "http://arxiv.org/abs/2411.09429v1", "date": "2024-11-14", "relevancy": 1.8674, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4818}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.457}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-driven%20inverse%20design%20of%20materials%3A%20Past%2C%20present%20and%20future&body=Title%3A%20AI-driven%20inverse%20design%20of%20materials%3A%20Past%2C%20present%20and%20future%0AAuthor%3A%20Xiao-Qi%20Han%20and%20Xin-De%20Wang%20and%20Meng-Yuan%20Xu%20and%20Zhen%20Feng%20and%20Bo-Wen%20Yao%20and%20Peng-Jie%20Guo%20and%20Ze-Feng%20Gao%20and%20Zhong-Yi%20Lu%0AAbstract%3A%20%20%20The%20discovery%20of%20advanced%20materials%20is%20the%20cornerstone%20of%20human%20technological%0Adevelopment%20and%20progress.%20The%20structures%20of%20materials%20and%20their%20corresponding%0Aproperties%20are%20essentially%20the%20result%20of%20a%20complex%20interplay%20of%20multiple%0Adegrees%20of%20freedom%20such%20as%20lattice%2C%20charge%2C%20spin%2C%20symmetry%2C%20and%20topology.%20This%0Aposes%20significant%20challenges%20for%20the%20inverse%20design%20methods%20of%20materials.%0AHumans%20have%20long%20explored%20new%20materials%20through%20a%20large%20number%20of%20experiments%0Aand%20proposed%20corresponding%20theoretical%20systems%20to%20predict%20new%20material%0Aproperties%20and%20structures.%20With%20the%20improvement%20of%20computational%20power%2C%0Aresearchers%20have%20gradually%20developed%20various%20electronic%20structure%20calculation%0Amethods%2C%20particularly%20such%20as%20the%20one%20based%20density%20functional%20theory%2C%20as%20well%0Aas%20high-throughput%20computational%20methods.%20Recently%2C%20the%20rapid%20development%20of%0Aartificial%20intelligence%20technology%20in%20the%20field%20of%20computer%20science%20has%20enabled%0Athe%20effective%20characterization%20of%20the%20implicit%20association%20between%20material%0Aproperties%20and%20structures%2C%20thus%20opening%20up%20an%20efficient%20paradigm%20for%20the%0Ainverse%20design%20of%20functional%20materials.%20A%20significant%20progress%20has%20been%20made%20in%0Ainverse%20design%20of%20materials%20based%20on%20generative%20and%20discriminative%20models%2C%0Aattracting%20widespread%20attention%20from%20researchers.%20Considering%20this%20rapid%0Atechnological%20progress%2C%20in%20this%20survey%2C%20we%20look%20back%20on%20the%20latest%20advancements%0Ain%20AI-driven%20inverse%20design%20of%20materials%20by%20introducing%20the%20background%2C%20key%0Afindings%2C%20and%20mainstream%20technological%20development%20routes.%20In%20addition%2C%20we%0Asummarize%20the%20remaining%20issues%20for%20future%20directions.%20This%20survey%20provides%20the%0Alatest%20overview%20of%20AI-driven%20inverse%20design%20of%20materials%2C%20which%20can%20serve%20as%20a%0Auseful%20resource%20for%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-driven%2520inverse%2520design%2520of%2520materials%253A%2520Past%252C%2520present%2520and%2520future%26entry.906535625%3DXiao-Qi%2520Han%2520and%2520Xin-De%2520Wang%2520and%2520Meng-Yuan%2520Xu%2520and%2520Zhen%2520Feng%2520and%2520Bo-Wen%2520Yao%2520and%2520Peng-Jie%2520Guo%2520and%2520Ze-Feng%2520Gao%2520and%2520Zhong-Yi%2520Lu%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520advanced%2520materials%2520is%2520the%2520cornerstone%2520of%2520human%2520technological%250Adevelopment%2520and%2520progress.%2520The%2520structures%2520of%2520materials%2520and%2520their%2520corresponding%250Aproperties%2520are%2520essentially%2520the%2520result%2520of%2520a%2520complex%2520interplay%2520of%2520multiple%250Adegrees%2520of%2520freedom%2520such%2520as%2520lattice%252C%2520charge%252C%2520spin%252C%2520symmetry%252C%2520and%2520topology.%2520This%250Aposes%2520significant%2520challenges%2520for%2520the%2520inverse%2520design%2520methods%2520of%2520materials.%250AHumans%2520have%2520long%2520explored%2520new%2520materials%2520through%2520a%2520large%2520number%2520of%2520experiments%250Aand%2520proposed%2520corresponding%2520theoretical%2520systems%2520to%2520predict%2520new%2520material%250Aproperties%2520and%2520structures.%2520With%2520the%2520improvement%2520of%2520computational%2520power%252C%250Aresearchers%2520have%2520gradually%2520developed%2520various%2520electronic%2520structure%2520calculation%250Amethods%252C%2520particularly%2520such%2520as%2520the%2520one%2520based%2520density%2520functional%2520theory%252C%2520as%2520well%250Aas%2520high-throughput%2520computational%2520methods.%2520Recently%252C%2520the%2520rapid%2520development%2520of%250Aartificial%2520intelligence%2520technology%2520in%2520the%2520field%2520of%2520computer%2520science%2520has%2520enabled%250Athe%2520effective%2520characterization%2520of%2520the%2520implicit%2520association%2520between%2520material%250Aproperties%2520and%2520structures%252C%2520thus%2520opening%2520up%2520an%2520efficient%2520paradigm%2520for%2520the%250Ainverse%2520design%2520of%2520functional%2520materials.%2520A%2520significant%2520progress%2520has%2520been%2520made%2520in%250Ainverse%2520design%2520of%2520materials%2520based%2520on%2520generative%2520and%2520discriminative%2520models%252C%250Aattracting%2520widespread%2520attention%2520from%2520researchers.%2520Considering%2520this%2520rapid%250Atechnological%2520progress%252C%2520in%2520this%2520survey%252C%2520we%2520look%2520back%2520on%2520the%2520latest%2520advancements%250Ain%2520AI-driven%2520inverse%2520design%2520of%2520materials%2520by%2520introducing%2520the%2520background%252C%2520key%250Afindings%252C%2520and%2520mainstream%2520technological%2520development%2520routes.%2520In%2520addition%252C%2520we%250Asummarize%2520the%2520remaining%2520issues%2520for%2520future%2520directions.%2520This%2520survey%2520provides%2520the%250Alatest%2520overview%2520of%2520AI-driven%2520inverse%2520design%2520of%2520materials%252C%2520which%2520can%2520serve%2520as%2520a%250Auseful%2520resource%2520for%2520researchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-driven%20inverse%20design%20of%20materials%3A%20Past%2C%20present%20and%20future&entry.906535625=Xiao-Qi%20Han%20and%20Xin-De%20Wang%20and%20Meng-Yuan%20Xu%20and%20Zhen%20Feng%20and%20Bo-Wen%20Yao%20and%20Peng-Jie%20Guo%20and%20Ze-Feng%20Gao%20and%20Zhong-Yi%20Lu&entry.1292438233=%20%20The%20discovery%20of%20advanced%20materials%20is%20the%20cornerstone%20of%20human%20technological%0Adevelopment%20and%20progress.%20The%20structures%20of%20materials%20and%20their%20corresponding%0Aproperties%20are%20essentially%20the%20result%20of%20a%20complex%20interplay%20of%20multiple%0Adegrees%20of%20freedom%20such%20as%20lattice%2C%20charge%2C%20spin%2C%20symmetry%2C%20and%20topology.%20This%0Aposes%20significant%20challenges%20for%20the%20inverse%20design%20methods%20of%20materials.%0AHumans%20have%20long%20explored%20new%20materials%20through%20a%20large%20number%20of%20experiments%0Aand%20proposed%20corresponding%20theoretical%20systems%20to%20predict%20new%20material%0Aproperties%20and%20structures.%20With%20the%20improvement%20of%20computational%20power%2C%0Aresearchers%20have%20gradually%20developed%20various%20electronic%20structure%20calculation%0Amethods%2C%20particularly%20such%20as%20the%20one%20based%20density%20functional%20theory%2C%20as%20well%0Aas%20high-throughput%20computational%20methods.%20Recently%2C%20the%20rapid%20development%20of%0Aartificial%20intelligence%20technology%20in%20the%20field%20of%20computer%20science%20has%20enabled%0Athe%20effective%20characterization%20of%20the%20implicit%20association%20between%20material%0Aproperties%20and%20structures%2C%20thus%20opening%20up%20an%20efficient%20paradigm%20for%20the%0Ainverse%20design%20of%20functional%20materials.%20A%20significant%20progress%20has%20been%20made%20in%0Ainverse%20design%20of%20materials%20based%20on%20generative%20and%20discriminative%20models%2C%0Aattracting%20widespread%20attention%20from%20researchers.%20Considering%20this%20rapid%0Atechnological%20progress%2C%20in%20this%20survey%2C%20we%20look%20back%20on%20the%20latest%20advancements%0Ain%20AI-driven%20inverse%20design%20of%20materials%20by%20introducing%20the%20background%2C%20key%0Afindings%2C%20and%20mainstream%20technological%20development%20routes.%20In%20addition%2C%20we%0Asummarize%20the%20remaining%20issues%20for%20future%20directions.%20This%20survey%20provides%20the%0Alatest%20overview%20of%20AI-driven%20inverse%20design%20of%20materials%2C%20which%20can%20serve%20as%20a%0Auseful%20resource%20for%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09429v1&entry.124074799=Read"},
{"title": "Automated Segmentation of Ischemic Stroke Lesions in Non-Contrast\n  Computed Tomography Images for Enhanced Treatment and Prognosis", "author": "Toufiq Musah and Prince Ebenezer Adjei and Kojo Obed Otoo", "abstract": "  Stroke is the second leading cause of death worldwide, and is increasingly\nprevalent in low- and middle-income countries (LMICs). Timely interventions can\nsignificantly influence stroke survivability and the quality of life after\ntreatment. However, the standard and most widely available imaging method for\nconfirming strokes and their sub-types, the NCCT, is more challenging and\ntime-consuming to employ in cases of ischemic stroke. For this reason, we\ndeveloped an automated method for ischemic stroke lesion segmentation in NCCTs\nusing the nnU-Net frame work, aimed at enhancing early treatment and improving\nthe prognosis of ischemic stroke patients. We achieved Dice scores of 0.596 and\nIntersection over Union (IoU) scores of 0.501 on the sampled dataset. After\nadjusting for outliers, these scores improved to 0.752 for the Dice score and\n0.643 for the IoU. Proper delineation of the region of infarction can help\nclinicians better assess the potential impact of the infarction, and guide\ntreatment procedures.\n", "link": "http://arxiv.org/abs/2411.09402v1", "date": "2024-11-14", "relevancy": 1.8662, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4672}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Segmentation%20of%20Ischemic%20Stroke%20Lesions%20in%20Non-Contrast%0A%20%20Computed%20Tomography%20Images%20for%20Enhanced%20Treatment%20and%20Prognosis&body=Title%3A%20Automated%20Segmentation%20of%20Ischemic%20Stroke%20Lesions%20in%20Non-Contrast%0A%20%20Computed%20Tomography%20Images%20for%20Enhanced%20Treatment%20and%20Prognosis%0AAuthor%3A%20Toufiq%20Musah%20and%20Prince%20Ebenezer%20Adjei%20and%20Kojo%20Obed%20Otoo%0AAbstract%3A%20%20%20Stroke%20is%20the%20second%20leading%20cause%20of%20death%20worldwide%2C%20and%20is%20increasingly%0Aprevalent%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20Timely%20interventions%20can%0Asignificantly%20influence%20stroke%20survivability%20and%20the%20quality%20of%20life%20after%0Atreatment.%20However%2C%20the%20standard%20and%20most%20widely%20available%20imaging%20method%20for%0Aconfirming%20strokes%20and%20their%20sub-types%2C%20the%20NCCT%2C%20is%20more%20challenging%20and%0Atime-consuming%20to%20employ%20in%20cases%20of%20ischemic%20stroke.%20For%20this%20reason%2C%20we%0Adeveloped%20an%20automated%20method%20for%20ischemic%20stroke%20lesion%20segmentation%20in%20NCCTs%0Ausing%20the%20nnU-Net%20frame%20work%2C%20aimed%20at%20enhancing%20early%20treatment%20and%20improving%0Athe%20prognosis%20of%20ischemic%20stroke%20patients.%20We%20achieved%20Dice%20scores%20of%200.596%20and%0AIntersection%20over%20Union%20%28IoU%29%20scores%20of%200.501%20on%20the%20sampled%20dataset.%20After%0Aadjusting%20for%20outliers%2C%20these%20scores%20improved%20to%200.752%20for%20the%20Dice%20score%20and%0A0.643%20for%20the%20IoU.%20Proper%20delineation%20of%20the%20region%20of%20infarction%20can%20help%0Aclinicians%20better%20assess%20the%20potential%20impact%20of%20the%20infarction%2C%20and%20guide%0Atreatment%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Segmentation%2520of%2520Ischemic%2520Stroke%2520Lesions%2520in%2520Non-Contrast%250A%2520%2520Computed%2520Tomography%2520Images%2520for%2520Enhanced%2520Treatment%2520and%2520Prognosis%26entry.906535625%3DToufiq%2520Musah%2520and%2520Prince%2520Ebenezer%2520Adjei%2520and%2520Kojo%2520Obed%2520Otoo%26entry.1292438233%3D%2520%2520Stroke%2520is%2520the%2520second%2520leading%2520cause%2520of%2520death%2520worldwide%252C%2520and%2520is%2520increasingly%250Aprevalent%2520in%2520low-%2520and%2520middle-income%2520countries%2520%2528LMICs%2529.%2520Timely%2520interventions%2520can%250Asignificantly%2520influence%2520stroke%2520survivability%2520and%2520the%2520quality%2520of%2520life%2520after%250Atreatment.%2520However%252C%2520the%2520standard%2520and%2520most%2520widely%2520available%2520imaging%2520method%2520for%250Aconfirming%2520strokes%2520and%2520their%2520sub-types%252C%2520the%2520NCCT%252C%2520is%2520more%2520challenging%2520and%250Atime-consuming%2520to%2520employ%2520in%2520cases%2520of%2520ischemic%2520stroke.%2520For%2520this%2520reason%252C%2520we%250Adeveloped%2520an%2520automated%2520method%2520for%2520ischemic%2520stroke%2520lesion%2520segmentation%2520in%2520NCCTs%250Ausing%2520the%2520nnU-Net%2520frame%2520work%252C%2520aimed%2520at%2520enhancing%2520early%2520treatment%2520and%2520improving%250Athe%2520prognosis%2520of%2520ischemic%2520stroke%2520patients.%2520We%2520achieved%2520Dice%2520scores%2520of%25200.596%2520and%250AIntersection%2520over%2520Union%2520%2528IoU%2529%2520scores%2520of%25200.501%2520on%2520the%2520sampled%2520dataset.%2520After%250Aadjusting%2520for%2520outliers%252C%2520these%2520scores%2520improved%2520to%25200.752%2520for%2520the%2520Dice%2520score%2520and%250A0.643%2520for%2520the%2520IoU.%2520Proper%2520delineation%2520of%2520the%2520region%2520of%2520infarction%2520can%2520help%250Aclinicians%2520better%2520assess%2520the%2520potential%2520impact%2520of%2520the%2520infarction%252C%2520and%2520guide%250Atreatment%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Segmentation%20of%20Ischemic%20Stroke%20Lesions%20in%20Non-Contrast%0A%20%20Computed%20Tomography%20Images%20for%20Enhanced%20Treatment%20and%20Prognosis&entry.906535625=Toufiq%20Musah%20and%20Prince%20Ebenezer%20Adjei%20and%20Kojo%20Obed%20Otoo&entry.1292438233=%20%20Stroke%20is%20the%20second%20leading%20cause%20of%20death%20worldwide%2C%20and%20is%20increasingly%0Aprevalent%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20Timely%20interventions%20can%0Asignificantly%20influence%20stroke%20survivability%20and%20the%20quality%20of%20life%20after%0Atreatment.%20However%2C%20the%20standard%20and%20most%20widely%20available%20imaging%20method%20for%0Aconfirming%20strokes%20and%20their%20sub-types%2C%20the%20NCCT%2C%20is%20more%20challenging%20and%0Atime-consuming%20to%20employ%20in%20cases%20of%20ischemic%20stroke.%20For%20this%20reason%2C%20we%0Adeveloped%20an%20automated%20method%20for%20ischemic%20stroke%20lesion%20segmentation%20in%20NCCTs%0Ausing%20the%20nnU-Net%20frame%20work%2C%20aimed%20at%20enhancing%20early%20treatment%20and%20improving%0Athe%20prognosis%20of%20ischemic%20stroke%20patients.%20We%20achieved%20Dice%20scores%20of%200.596%20and%0AIntersection%20over%20Union%20%28IoU%29%20scores%20of%200.501%20on%20the%20sampled%20dataset.%20After%0Aadjusting%20for%20outliers%2C%20these%20scores%20improved%20to%200.752%20for%20the%20Dice%20score%20and%0A0.643%20for%20the%20IoU.%20Proper%20delineation%20of%20the%20region%20of%20infarction%20can%20help%0Aclinicians%20better%20assess%20the%20potential%20impact%20of%20the%20infarction%2C%20and%20guide%0Atreatment%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09402v1&entry.124074799=Read"},
{"title": "Improving hp-Variational Physics-Informed Neural Networks for\n  Steady-State Convection-Dominated Problems", "author": "Thivin Anandh and Divij Ghose and Himanshu Jain and Pratham Sunkad and Sashikumaar Ganesan and Volker John", "abstract": "  This paper proposes and studies two extensions of applying hp-variational\nphysics-informed neural networks, more precisely the FastVPINNs framework, to\nconvection-dominated convection-diffusion-reaction problems. First, a term in\nthe spirit of a SUPG stabilization is included in the loss functional and a\nnetwork architecture is proposed that predicts spatially varying stabilization\nparameters. Having observed that the selection of the indicator function in\nhard-constrained Dirichlet boundary conditions has a big impact on the accuracy\nof the computed solutions, the second novelty is the proposal of a network\narchitecture that learns good parameters for a class of indicator functions.\nNumerical studies show that both proposals lead to noticeably more accurate\nresults than approaches that can be found in the literature.\n", "link": "http://arxiv.org/abs/2411.09329v1", "date": "2024-11-14", "relevancy": 1.861, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4767}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20hp-Variational%20Physics-Informed%20Neural%20Networks%20for%0A%20%20Steady-State%20Convection-Dominated%20Problems&body=Title%3A%20Improving%20hp-Variational%20Physics-Informed%20Neural%20Networks%20for%0A%20%20Steady-State%20Convection-Dominated%20Problems%0AAuthor%3A%20Thivin%20Anandh%20and%20Divij%20Ghose%20and%20Himanshu%20Jain%20and%20Pratham%20Sunkad%20and%20Sashikumaar%20Ganesan%20and%20Volker%20John%0AAbstract%3A%20%20%20This%20paper%20proposes%20and%20studies%20two%20extensions%20of%20applying%20hp-variational%0Aphysics-informed%20neural%20networks%2C%20more%20precisely%20the%20FastVPINNs%20framework%2C%20to%0Aconvection-dominated%20convection-diffusion-reaction%20problems.%20First%2C%20a%20term%20in%0Athe%20spirit%20of%20a%20SUPG%20stabilization%20is%20included%20in%20the%20loss%20functional%20and%20a%0Anetwork%20architecture%20is%20proposed%20that%20predicts%20spatially%20varying%20stabilization%0Aparameters.%20Having%20observed%20that%20the%20selection%20of%20the%20indicator%20function%20in%0Ahard-constrained%20Dirichlet%20boundary%20conditions%20has%20a%20big%20impact%20on%20the%20accuracy%0Aof%20the%20computed%20solutions%2C%20the%20second%20novelty%20is%20the%20proposal%20of%20a%20network%0Aarchitecture%20that%20learns%20good%20parameters%20for%20a%20class%20of%20indicator%20functions.%0ANumerical%20studies%20show%20that%20both%20proposals%20lead%20to%20noticeably%20more%20accurate%0Aresults%20than%20approaches%20that%20can%20be%20found%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520hp-Variational%2520Physics-Informed%2520Neural%2520Networks%2520for%250A%2520%2520Steady-State%2520Convection-Dominated%2520Problems%26entry.906535625%3DThivin%2520Anandh%2520and%2520Divij%2520Ghose%2520and%2520Himanshu%2520Jain%2520and%2520Pratham%2520Sunkad%2520and%2520Sashikumaar%2520Ganesan%2520and%2520Volker%2520John%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520and%2520studies%2520two%2520extensions%2520of%2520applying%2520hp-variational%250Aphysics-informed%2520neural%2520networks%252C%2520more%2520precisely%2520the%2520FastVPINNs%2520framework%252C%2520to%250Aconvection-dominated%2520convection-diffusion-reaction%2520problems.%2520First%252C%2520a%2520term%2520in%250Athe%2520spirit%2520of%2520a%2520SUPG%2520stabilization%2520is%2520included%2520in%2520the%2520loss%2520functional%2520and%2520a%250Anetwork%2520architecture%2520is%2520proposed%2520that%2520predicts%2520spatially%2520varying%2520stabilization%250Aparameters.%2520Having%2520observed%2520that%2520the%2520selection%2520of%2520the%2520indicator%2520function%2520in%250Ahard-constrained%2520Dirichlet%2520boundary%2520conditions%2520has%2520a%2520big%2520impact%2520on%2520the%2520accuracy%250Aof%2520the%2520computed%2520solutions%252C%2520the%2520second%2520novelty%2520is%2520the%2520proposal%2520of%2520a%2520network%250Aarchitecture%2520that%2520learns%2520good%2520parameters%2520for%2520a%2520class%2520of%2520indicator%2520functions.%250ANumerical%2520studies%2520show%2520that%2520both%2520proposals%2520lead%2520to%2520noticeably%2520more%2520accurate%250Aresults%2520than%2520approaches%2520that%2520can%2520be%2520found%2520in%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20hp-Variational%20Physics-Informed%20Neural%20Networks%20for%0A%20%20Steady-State%20Convection-Dominated%20Problems&entry.906535625=Thivin%20Anandh%20and%20Divij%20Ghose%20and%20Himanshu%20Jain%20and%20Pratham%20Sunkad%20and%20Sashikumaar%20Ganesan%20and%20Volker%20John&entry.1292438233=%20%20This%20paper%20proposes%20and%20studies%20two%20extensions%20of%20applying%20hp-variational%0Aphysics-informed%20neural%20networks%2C%20more%20precisely%20the%20FastVPINNs%20framework%2C%20to%0Aconvection-dominated%20convection-diffusion-reaction%20problems.%20First%2C%20a%20term%20in%0Athe%20spirit%20of%20a%20SUPG%20stabilization%20is%20included%20in%20the%20loss%20functional%20and%20a%0Anetwork%20architecture%20is%20proposed%20that%20predicts%20spatially%20varying%20stabilization%0Aparameters.%20Having%20observed%20that%20the%20selection%20of%20the%20indicator%20function%20in%0Ahard-constrained%20Dirichlet%20boundary%20conditions%20has%20a%20big%20impact%20on%20the%20accuracy%0Aof%20the%20computed%20solutions%2C%20the%20second%20novelty%20is%20the%20proposal%20of%20a%20network%0Aarchitecture%20that%20learns%20good%20parameters%20for%20a%20class%20of%20indicator%20functions.%0ANumerical%20studies%20show%20that%20both%20proposals%20lead%20to%20noticeably%20more%20accurate%0Aresults%20than%20approaches%20that%20can%20be%20found%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09329v1&entry.124074799=Read"},
{"title": "PTR: Precision-Driven Tool Recommendation for Large Language Models", "author": "Hang Gao and Yongfeng Zhang", "abstract": "  By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.\n", "link": "http://arxiv.org/abs/2411.09613v1", "date": "2024-11-14", "relevancy": 1.8574, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTR%3A%20Precision-Driven%20Tool%20Recommendation%20for%20Large%20Language%20Models&body=Title%3A%20PTR%3A%20Precision-Driven%20Tool%20Recommendation%20for%20Large%20Language%20Models%0AAuthor%3A%20Hang%20Gao%20and%20Yongfeng%20Zhang%0AAbstract%3A%20%20%20By%20augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20tools%2C%20their%0Acapacity%20to%20solve%20complex%20problems%20has%20been%20significantly%20enhanced.%20However%2C%0Adespite%20ongoing%20advancements%20in%20the%20parsing%20capabilities%20of%20LLMs%2C%20incorporating%0Aall%20available%20tools%20simultaneously%20in%20the%20prompt%20remains%20impractical%20due%20to%20the%0Avast%20number%20of%20external%20tools.%20Consequently%2C%20it%20is%20essential%20to%20provide%20LLMs%0Awith%20a%20precise%20set%20of%20tools%20tailored%20to%20the%20specific%20task%2C%20considering%20both%0Aquantity%20and%20quality.%20Current%20tool%20retrieval%20methods%20primarily%20focus%20on%0Arefining%20the%20ranking%20list%20of%20tools%20and%20directly%20packaging%20a%20fixed%20number%20of%0Atop-ranked%20tools%20as%20the%20tool%20set.%20However%2C%20these%20approaches%20often%20fail%20to%20equip%0ALLMs%20with%20the%20optimal%20set%20of%20tools%20prior%20to%20execution%2C%20since%20the%20optimal%20number%0Aof%20tools%20for%20different%20tasks%20could%20be%20different%2C%20resulting%20in%20inefficiencies%0Asuch%20as%20redundant%20or%20unsuitable%20tools%2C%20which%20impede%20immediate%20access%20to%20the%0Amost%20relevant%20tools.%20This%20paper%20addresses%20the%20challenge%20of%20recommending%20precise%0Atoolsets%20for%20LLMs.%20We%20introduce%20the%20problem%20of%20tool%20recommendation%2C%20define%20its%0Ascope%2C%20and%20propose%20a%20novel%20Precision-driven%20Tool%20Recommendation%20%28PTR%29%20approach.%0APTR%20captures%20an%20initial%2C%20concise%20set%20of%20tools%20by%20leveraging%20historical%20tool%0Abundle%20usage%20and%20dynamically%20adjusts%20the%20tool%20set%20by%20performing%20tool%20matching%2C%0Aculminating%20in%20a%20multi-view-based%20tool%20addition.%20Additionally%2C%20we%20present%20a%20new%0Adataset%2C%20RecTools%2C%20and%20a%20metric%2C%20TRACC%2C%20designed%20to%20evaluate%20the%20effectiveness%0Aof%20tool%20recommendation%20for%20LLMs.%20We%20further%20validate%20our%20design%20choices%20through%0Acomprehensive%20experiments%2C%20demonstrating%20promising%20accuracy%20across%20two%20open%0Abenchmarks%20and%20our%20RecTools%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTR%253A%2520Precision-Driven%2520Tool%2520Recommendation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DHang%2520Gao%2520and%2520Yongfeng%2520Zhang%26entry.1292438233%3D%2520%2520By%2520augmenting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520external%2520tools%252C%2520their%250Acapacity%2520to%2520solve%2520complex%2520problems%2520has%2520been%2520significantly%2520enhanced.%2520However%252C%250Adespite%2520ongoing%2520advancements%2520in%2520the%2520parsing%2520capabilities%2520of%2520LLMs%252C%2520incorporating%250Aall%2520available%2520tools%2520simultaneously%2520in%2520the%2520prompt%2520remains%2520impractical%2520due%2520to%2520the%250Avast%2520number%2520of%2520external%2520tools.%2520Consequently%252C%2520it%2520is%2520essential%2520to%2520provide%2520LLMs%250Awith%2520a%2520precise%2520set%2520of%2520tools%2520tailored%2520to%2520the%2520specific%2520task%252C%2520considering%2520both%250Aquantity%2520and%2520quality.%2520Current%2520tool%2520retrieval%2520methods%2520primarily%2520focus%2520on%250Arefining%2520the%2520ranking%2520list%2520of%2520tools%2520and%2520directly%2520packaging%2520a%2520fixed%2520number%2520of%250Atop-ranked%2520tools%2520as%2520the%2520tool%2520set.%2520However%252C%2520these%2520approaches%2520often%2520fail%2520to%2520equip%250ALLMs%2520with%2520the%2520optimal%2520set%2520of%2520tools%2520prior%2520to%2520execution%252C%2520since%2520the%2520optimal%2520number%250Aof%2520tools%2520for%2520different%2520tasks%2520could%2520be%2520different%252C%2520resulting%2520in%2520inefficiencies%250Asuch%2520as%2520redundant%2520or%2520unsuitable%2520tools%252C%2520which%2520impede%2520immediate%2520access%2520to%2520the%250Amost%2520relevant%2520tools.%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520recommending%2520precise%250Atoolsets%2520for%2520LLMs.%2520We%2520introduce%2520the%2520problem%2520of%2520tool%2520recommendation%252C%2520define%2520its%250Ascope%252C%2520and%2520propose%2520a%2520novel%2520Precision-driven%2520Tool%2520Recommendation%2520%2528PTR%2529%2520approach.%250APTR%2520captures%2520an%2520initial%252C%2520concise%2520set%2520of%2520tools%2520by%2520leveraging%2520historical%2520tool%250Abundle%2520usage%2520and%2520dynamically%2520adjusts%2520the%2520tool%2520set%2520by%2520performing%2520tool%2520matching%252C%250Aculminating%2520in%2520a%2520multi-view-based%2520tool%2520addition.%2520Additionally%252C%2520we%2520present%2520a%2520new%250Adataset%252C%2520RecTools%252C%2520and%2520a%2520metric%252C%2520TRACC%252C%2520designed%2520to%2520evaluate%2520the%2520effectiveness%250Aof%2520tool%2520recommendation%2520for%2520LLMs.%2520We%2520further%2520validate%2520our%2520design%2520choices%2520through%250Acomprehensive%2520experiments%252C%2520demonstrating%2520promising%2520accuracy%2520across%2520two%2520open%250Abenchmarks%2520and%2520our%2520RecTools%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTR%3A%20Precision-Driven%20Tool%20Recommendation%20for%20Large%20Language%20Models&entry.906535625=Hang%20Gao%20and%20Yongfeng%20Zhang&entry.1292438233=%20%20By%20augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20tools%2C%20their%0Acapacity%20to%20solve%20complex%20problems%20has%20been%20significantly%20enhanced.%20However%2C%0Adespite%20ongoing%20advancements%20in%20the%20parsing%20capabilities%20of%20LLMs%2C%20incorporating%0Aall%20available%20tools%20simultaneously%20in%20the%20prompt%20remains%20impractical%20due%20to%20the%0Avast%20number%20of%20external%20tools.%20Consequently%2C%20it%20is%20essential%20to%20provide%20LLMs%0Awith%20a%20precise%20set%20of%20tools%20tailored%20to%20the%20specific%20task%2C%20considering%20both%0Aquantity%20and%20quality.%20Current%20tool%20retrieval%20methods%20primarily%20focus%20on%0Arefining%20the%20ranking%20list%20of%20tools%20and%20directly%20packaging%20a%20fixed%20number%20of%0Atop-ranked%20tools%20as%20the%20tool%20set.%20However%2C%20these%20approaches%20often%20fail%20to%20equip%0ALLMs%20with%20the%20optimal%20set%20of%20tools%20prior%20to%20execution%2C%20since%20the%20optimal%20number%0Aof%20tools%20for%20different%20tasks%20could%20be%20different%2C%20resulting%20in%20inefficiencies%0Asuch%20as%20redundant%20or%20unsuitable%20tools%2C%20which%20impede%20immediate%20access%20to%20the%0Amost%20relevant%20tools.%20This%20paper%20addresses%20the%20challenge%20of%20recommending%20precise%0Atoolsets%20for%20LLMs.%20We%20introduce%20the%20problem%20of%20tool%20recommendation%2C%20define%20its%0Ascope%2C%20and%20propose%20a%20novel%20Precision-driven%20Tool%20Recommendation%20%28PTR%29%20approach.%0APTR%20captures%20an%20initial%2C%20concise%20set%20of%20tools%20by%20leveraging%20historical%20tool%0Abundle%20usage%20and%20dynamically%20adjusts%20the%20tool%20set%20by%20performing%20tool%20matching%2C%0Aculminating%20in%20a%20multi-view-based%20tool%20addition.%20Additionally%2C%20we%20present%20a%20new%0Adataset%2C%20RecTools%2C%20and%20a%20metric%2C%20TRACC%2C%20designed%20to%20evaluate%20the%20effectiveness%0Aof%20tool%20recommendation%20for%20LLMs.%20We%20further%20validate%20our%20design%20choices%20through%0Acomprehensive%20experiments%2C%20demonstrating%20promising%20accuracy%20across%20two%20open%0Abenchmarks%20and%20our%20RecTools%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09613v1&entry.124074799=Read"},
{"title": "How to Boost Any Loss Function", "author": "Richard Nock and Yishay Mansour", "abstract": "  Boosting is a highly successful ML-born optimization setting in which one is\nrequired to computationally efficiently learn arbitrarily good models based on\nthe access to a weak learner oracle, providing classifiers performing at least\nslightly differently from random guessing. A key difference with gradient-based\noptimization is that boosting's original model does not requires access to\nfirst order information about a loss, yet the decades long history of boosting\nhas quickly evolved it into a first order optimization setting -- sometimes\neven wrongfully defining it as such. Owing to recent progress extending\ngradient-based optimization to use only a loss' zeroth ($0^{th}$) order\ninformation to learn, this begs the question: what loss functions can be\nefficiently optimized with boosting and what is the information really needed\nfor boosting to meet the original boosting blueprint's requirements?\n  We provide a constructive formal answer essentially showing that any loss\nfunction can be optimized with boosting and thus boosting can achieve a feat\nnot yet known to be possible in the classical $0^{th}$ order setting, since\nloss functions are not required to be be convex, nor differentiable or\nLipschitz -- and in fact not required to be continuous either. Some tools we\nuse are rooted in quantum calculus, the mathematical field -- not to be\nconfounded with quantum computation -- that studies calculus without passing to\nthe limit, and thus without using first order information.\n", "link": "http://arxiv.org/abs/2407.02279v2", "date": "2024-11-14", "relevancy": 1.8442, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5107}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Boost%20Any%20Loss%20Function&body=Title%3A%20How%20to%20Boost%20Any%20Loss%20Function%0AAuthor%3A%20Richard%20Nock%20and%20Yishay%20Mansour%0AAbstract%3A%20%20%20Boosting%20is%20a%20highly%20successful%20ML-born%20optimization%20setting%20in%20which%20one%20is%0Arequired%20to%20computationally%20efficiently%20learn%20arbitrarily%20good%20models%20based%20on%0Athe%20access%20to%20a%20weak%20learner%20oracle%2C%20providing%20classifiers%20performing%20at%20least%0Aslightly%20differently%20from%20random%20guessing.%20A%20key%20difference%20with%20gradient-based%0Aoptimization%20is%20that%20boosting%27s%20original%20model%20does%20not%20requires%20access%20to%0Afirst%20order%20information%20about%20a%20loss%2C%20yet%20the%20decades%20long%20history%20of%20boosting%0Ahas%20quickly%20evolved%20it%20into%20a%20first%20order%20optimization%20setting%20--%20sometimes%0Aeven%20wrongfully%20defining%20it%20as%20such.%20Owing%20to%20recent%20progress%20extending%0Agradient-based%20optimization%20to%20use%20only%20a%20loss%27%20zeroth%20%28%240%5E%7Bth%7D%24%29%20order%0Ainformation%20to%20learn%2C%20this%20begs%20the%20question%3A%20what%20loss%20functions%20can%20be%0Aefficiently%20optimized%20with%20boosting%20and%20what%20is%20the%20information%20really%20needed%0Afor%20boosting%20to%20meet%20the%20original%20boosting%20blueprint%27s%20requirements%3F%0A%20%20We%20provide%20a%20constructive%20formal%20answer%20essentially%20showing%20that%20any%20loss%0Afunction%20can%20be%20optimized%20with%20boosting%20and%20thus%20boosting%20can%20achieve%20a%20feat%0Anot%20yet%20known%20to%20be%20possible%20in%20the%20classical%20%240%5E%7Bth%7D%24%20order%20setting%2C%20since%0Aloss%20functions%20are%20not%20required%20to%20be%20be%20convex%2C%20nor%20differentiable%20or%0ALipschitz%20--%20and%20in%20fact%20not%20required%20to%20be%20continuous%20either.%20Some%20tools%20we%0Ause%20are%20rooted%20in%20quantum%20calculus%2C%20the%20mathematical%20field%20--%20not%20to%20be%0Aconfounded%20with%20quantum%20computation%20--%20that%20studies%20calculus%20without%20passing%20to%0Athe%20limit%2C%20and%20thus%20without%20using%20first%20order%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Boost%2520Any%2520Loss%2520Function%26entry.906535625%3DRichard%2520Nock%2520and%2520Yishay%2520Mansour%26entry.1292438233%3D%2520%2520Boosting%2520is%2520a%2520highly%2520successful%2520ML-born%2520optimization%2520setting%2520in%2520which%2520one%2520is%250Arequired%2520to%2520computationally%2520efficiently%2520learn%2520arbitrarily%2520good%2520models%2520based%2520on%250Athe%2520access%2520to%2520a%2520weak%2520learner%2520oracle%252C%2520providing%2520classifiers%2520performing%2520at%2520least%250Aslightly%2520differently%2520from%2520random%2520guessing.%2520A%2520key%2520difference%2520with%2520gradient-based%250Aoptimization%2520is%2520that%2520boosting%2527s%2520original%2520model%2520does%2520not%2520requires%2520access%2520to%250Afirst%2520order%2520information%2520about%2520a%2520loss%252C%2520yet%2520the%2520decades%2520long%2520history%2520of%2520boosting%250Ahas%2520quickly%2520evolved%2520it%2520into%2520a%2520first%2520order%2520optimization%2520setting%2520--%2520sometimes%250Aeven%2520wrongfully%2520defining%2520it%2520as%2520such.%2520Owing%2520to%2520recent%2520progress%2520extending%250Agradient-based%2520optimization%2520to%2520use%2520only%2520a%2520loss%2527%2520zeroth%2520%2528%25240%255E%257Bth%257D%2524%2529%2520order%250Ainformation%2520to%2520learn%252C%2520this%2520begs%2520the%2520question%253A%2520what%2520loss%2520functions%2520can%2520be%250Aefficiently%2520optimized%2520with%2520boosting%2520and%2520what%2520is%2520the%2520information%2520really%2520needed%250Afor%2520boosting%2520to%2520meet%2520the%2520original%2520boosting%2520blueprint%2527s%2520requirements%253F%250A%2520%2520We%2520provide%2520a%2520constructive%2520formal%2520answer%2520essentially%2520showing%2520that%2520any%2520loss%250Afunction%2520can%2520be%2520optimized%2520with%2520boosting%2520and%2520thus%2520boosting%2520can%2520achieve%2520a%2520feat%250Anot%2520yet%2520known%2520to%2520be%2520possible%2520in%2520the%2520classical%2520%25240%255E%257Bth%257D%2524%2520order%2520setting%252C%2520since%250Aloss%2520functions%2520are%2520not%2520required%2520to%2520be%2520be%2520convex%252C%2520nor%2520differentiable%2520or%250ALipschitz%2520--%2520and%2520in%2520fact%2520not%2520required%2520to%2520be%2520continuous%2520either.%2520Some%2520tools%2520we%250Ause%2520are%2520rooted%2520in%2520quantum%2520calculus%252C%2520the%2520mathematical%2520field%2520--%2520not%2520to%2520be%250Aconfounded%2520with%2520quantum%2520computation%2520--%2520that%2520studies%2520calculus%2520without%2520passing%2520to%250Athe%2520limit%252C%2520and%2520thus%2520without%2520using%2520first%2520order%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Boost%20Any%20Loss%20Function&entry.906535625=Richard%20Nock%20and%20Yishay%20Mansour&entry.1292438233=%20%20Boosting%20is%20a%20highly%20successful%20ML-born%20optimization%20setting%20in%20which%20one%20is%0Arequired%20to%20computationally%20efficiently%20learn%20arbitrarily%20good%20models%20based%20on%0Athe%20access%20to%20a%20weak%20learner%20oracle%2C%20providing%20classifiers%20performing%20at%20least%0Aslightly%20differently%20from%20random%20guessing.%20A%20key%20difference%20with%20gradient-based%0Aoptimization%20is%20that%20boosting%27s%20original%20model%20does%20not%20requires%20access%20to%0Afirst%20order%20information%20about%20a%20loss%2C%20yet%20the%20decades%20long%20history%20of%20boosting%0Ahas%20quickly%20evolved%20it%20into%20a%20first%20order%20optimization%20setting%20--%20sometimes%0Aeven%20wrongfully%20defining%20it%20as%20such.%20Owing%20to%20recent%20progress%20extending%0Agradient-based%20optimization%20to%20use%20only%20a%20loss%27%20zeroth%20%28%240%5E%7Bth%7D%24%29%20order%0Ainformation%20to%20learn%2C%20this%20begs%20the%20question%3A%20what%20loss%20functions%20can%20be%0Aefficiently%20optimized%20with%20boosting%20and%20what%20is%20the%20information%20really%20needed%0Afor%20boosting%20to%20meet%20the%20original%20boosting%20blueprint%27s%20requirements%3F%0A%20%20We%20provide%20a%20constructive%20formal%20answer%20essentially%20showing%20that%20any%20loss%0Afunction%20can%20be%20optimized%20with%20boosting%20and%20thus%20boosting%20can%20achieve%20a%20feat%0Anot%20yet%20known%20to%20be%20possible%20in%20the%20classical%20%240%5E%7Bth%7D%24%20order%20setting%2C%20since%0Aloss%20functions%20are%20not%20required%20to%20be%20be%20convex%2C%20nor%20differentiable%20or%0ALipschitz%20--%20and%20in%20fact%20not%20required%20to%20be%20continuous%20either.%20Some%20tools%20we%0Ause%20are%20rooted%20in%20quantum%20calculus%2C%20the%20mathematical%20field%20--%20not%20to%20be%0Aconfounded%20with%20quantum%20computation%20--%20that%20studies%20calculus%20without%20passing%20to%0Athe%20limit%2C%20and%20thus%20without%20using%20first%20order%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02279v2&entry.124074799=Read"},
{"title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks", "author": "Yifan Zeng and Yiran Wu and Xiao Zhang and Huazheng Wang and Qingyun Wu", "abstract": "  Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.\n", "link": "http://arxiv.org/abs/2403.04783v2", "date": "2024-11-14", "relevancy": 1.837, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4627}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoDefense%3A%20Multi-Agent%20LLM%20Defense%20against%20Jailbreak%20Attacks&body=Title%3A%20AutoDefense%3A%20Multi-Agent%20LLM%20Defense%20against%20Jailbreak%20Attacks%0AAuthor%3A%20Yifan%20Zeng%20and%20Yiran%20Wu%20and%20Xiao%20Zhang%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%0AAbstract%3A%20%20%20Despite%20extensive%20pre-training%20in%20moral%20alignment%20to%20prevent%20generating%0Aharmful%20information%2C%20large%20language%20models%20%28LLMs%29%20remain%20vulnerable%20to%0Ajailbreak%20attacks.%20In%20this%20paper%2C%20we%20propose%20AutoDefense%2C%20a%20multi-agent%20defense%0Aframework%20that%20filters%20harmful%20responses%20from%20LLMs.%20With%20the%20response-filtering%0Amechanism%2C%20our%20framework%20is%20robust%20against%20different%20jailbreak%20attack%20prompts%2C%0Aand%20can%20be%20used%20to%20defend%20different%20victim%20models.%20AutoDefense%20assigns%0Adifferent%20roles%20to%20LLM%20agents%20and%20employs%20them%20to%20complete%20the%20defense%20task%0Acollaboratively.%20The%20division%20in%20tasks%20enhances%20the%20overall%0Ainstruction-following%20of%20LLMs%20and%20enables%20the%20integration%20of%20other%20defense%0Acomponents%20as%20tools.%20With%20AutoDefense%2C%20small%20open-source%20LMs%20can%20serve%20as%0Aagents%20and%20defend%20larger%20models%20against%20jailbreak%20attacks.%20Our%20experiments%20show%0Athat%20AutoDefense%20can%20effectively%20defense%20against%20different%20jailbreak%20attacks%2C%0Awhile%20maintaining%20the%20performance%20at%20normal%20user%20request.%20For%20example%2C%20we%0Areduce%20the%20attack%20success%20rate%20on%20GPT-3.5%20from%2055.74%25%20to%207.95%25%20using%0ALLaMA-2-13b%20with%20a%203-agent%20system.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/XHMY/AutoDefense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoDefense%253A%2520Multi-Agent%2520LLM%2520Defense%2520against%2520Jailbreak%2520Attacks%26entry.906535625%3DYifan%2520Zeng%2520and%2520Yiran%2520Wu%2520and%2520Xiao%2520Zhang%2520and%2520Huazheng%2520Wang%2520and%2520Qingyun%2520Wu%26entry.1292438233%3D%2520%2520Despite%2520extensive%2520pre-training%2520in%2520moral%2520alignment%2520to%2520prevent%2520generating%250Aharmful%2520information%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520remain%2520vulnerable%2520to%250Ajailbreak%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AutoDefense%252C%2520a%2520multi-agent%2520defense%250Aframework%2520that%2520filters%2520harmful%2520responses%2520from%2520LLMs.%2520With%2520the%2520response-filtering%250Amechanism%252C%2520our%2520framework%2520is%2520robust%2520against%2520different%2520jailbreak%2520attack%2520prompts%252C%250Aand%2520can%2520be%2520used%2520to%2520defend%2520different%2520victim%2520models.%2520AutoDefense%2520assigns%250Adifferent%2520roles%2520to%2520LLM%2520agents%2520and%2520employs%2520them%2520to%2520complete%2520the%2520defense%2520task%250Acollaboratively.%2520The%2520division%2520in%2520tasks%2520enhances%2520the%2520overall%250Ainstruction-following%2520of%2520LLMs%2520and%2520enables%2520the%2520integration%2520of%2520other%2520defense%250Acomponents%2520as%2520tools.%2520With%2520AutoDefense%252C%2520small%2520open-source%2520LMs%2520can%2520serve%2520as%250Aagents%2520and%2520defend%2520larger%2520models%2520against%2520jailbreak%2520attacks.%2520Our%2520experiments%2520show%250Athat%2520AutoDefense%2520can%2520effectively%2520defense%2520against%2520different%2520jailbreak%2520attacks%252C%250Awhile%2520maintaining%2520the%2520performance%2520at%2520normal%2520user%2520request.%2520For%2520example%252C%2520we%250Areduce%2520the%2520attack%2520success%2520rate%2520on%2520GPT-3.5%2520from%252055.74%2525%2520to%25207.95%2525%2520using%250ALLaMA-2-13b%2520with%2520a%25203-agent%2520system.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/XHMY/AutoDefense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoDefense%3A%20Multi-Agent%20LLM%20Defense%20against%20Jailbreak%20Attacks&entry.906535625=Yifan%20Zeng%20and%20Yiran%20Wu%20and%20Xiao%20Zhang%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu&entry.1292438233=%20%20Despite%20extensive%20pre-training%20in%20moral%20alignment%20to%20prevent%20generating%0Aharmful%20information%2C%20large%20language%20models%20%28LLMs%29%20remain%20vulnerable%20to%0Ajailbreak%20attacks.%20In%20this%20paper%2C%20we%20propose%20AutoDefense%2C%20a%20multi-agent%20defense%0Aframework%20that%20filters%20harmful%20responses%20from%20LLMs.%20With%20the%20response-filtering%0Amechanism%2C%20our%20framework%20is%20robust%20against%20different%20jailbreak%20attack%20prompts%2C%0Aand%20can%20be%20used%20to%20defend%20different%20victim%20models.%20AutoDefense%20assigns%0Adifferent%20roles%20to%20LLM%20agents%20and%20employs%20them%20to%20complete%20the%20defense%20task%0Acollaboratively.%20The%20division%20in%20tasks%20enhances%20the%20overall%0Ainstruction-following%20of%20LLMs%20and%20enables%20the%20integration%20of%20other%20defense%0Acomponents%20as%20tools.%20With%20AutoDefense%2C%20small%20open-source%20LMs%20can%20serve%20as%0Aagents%20and%20defend%20larger%20models%20against%20jailbreak%20attacks.%20Our%20experiments%20show%0Athat%20AutoDefense%20can%20effectively%20defense%20against%20different%20jailbreak%20attacks%2C%0Awhile%20maintaining%20the%20performance%20at%20normal%20user%20request.%20For%20example%2C%20we%0Areduce%20the%20attack%20success%20rate%20on%20GPT-3.5%20from%2055.74%25%20to%207.95%25%20using%0ALLaMA-2-13b%20with%20a%203-agent%20system.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/XHMY/AutoDefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04783v2&entry.124074799=Read"},
{"title": "Modular Fault Diagnosis Framework for Complex Autonomous Driving Systems", "author": "Stefan Orf and Sven Ochs and Jens Doll and Albert Schotschneider and Marc Heinrich and Marc Ren\u00e9 Zofka and J. Marius Z\u00f6llner", "abstract": "  Fault diagnosis is crucial for complex autonomous mobile systems, especially\nfor modern-day autonomous driving (AD). Different actors, numerous use cases,\nand complex heterogeneous components motivate a fault diagnosis of the system\nand overall system integrity. AD systems are composed of many heterogeneous\ncomponents, each with different functionality and possibly using a different\nalgorithm (e.g., rule-based vs. AI components). In addition, these components\nare subject to the vehicle's driving state and are highly dependent. This\npaper, therefore, faces this problem by presenting the concept of a modular\nfault diagnosis framework for AD systems. The concept suggests modular state\nmonitoring and diagnosis elements, together with a state- and dependency-aware\naggregation method. Our proposed classification scheme allows for the\ncategorization of the fault diagnosis modules. The concept is implemented on AD\nshuttle buses and evaluated to demonstrate its capabilities.\n", "link": "http://arxiv.org/abs/2411.09643v1", "date": "2024-11-14", "relevancy": 1.8267, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4813}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Fault%20Diagnosis%20Framework%20for%20Complex%20Autonomous%20Driving%20Systems&body=Title%3A%20Modular%20Fault%20Diagnosis%20Framework%20for%20Complex%20Autonomous%20Driving%20Systems%0AAuthor%3A%20Stefan%20Orf%20and%20Sven%20Ochs%20and%20Jens%20Doll%20and%20Albert%20Schotschneider%20and%20Marc%20Heinrich%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Fault%20diagnosis%20is%20crucial%20for%20complex%20autonomous%20mobile%20systems%2C%20especially%0Afor%20modern-day%20autonomous%20driving%20%28AD%29.%20Different%20actors%2C%20numerous%20use%20cases%2C%0Aand%20complex%20heterogeneous%20components%20motivate%20a%20fault%20diagnosis%20of%20the%20system%0Aand%20overall%20system%20integrity.%20AD%20systems%20are%20composed%20of%20many%20heterogeneous%0Acomponents%2C%20each%20with%20different%20functionality%20and%20possibly%20using%20a%20different%0Aalgorithm%20%28e.g.%2C%20rule-based%20vs.%20AI%20components%29.%20In%20addition%2C%20these%20components%0Aare%20subject%20to%20the%20vehicle%27s%20driving%20state%20and%20are%20highly%20dependent.%20This%0Apaper%2C%20therefore%2C%20faces%20this%20problem%20by%20presenting%20the%20concept%20of%20a%20modular%0Afault%20diagnosis%20framework%20for%20AD%20systems.%20The%20concept%20suggests%20modular%20state%0Amonitoring%20and%20diagnosis%20elements%2C%20together%20with%20a%20state-%20and%20dependency-aware%0Aaggregation%20method.%20Our%20proposed%20classification%20scheme%20allows%20for%20the%0Acategorization%20of%20the%20fault%20diagnosis%20modules.%20The%20concept%20is%20implemented%20on%20AD%0Ashuttle%20buses%20and%20evaluated%20to%20demonstrate%20its%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Fault%2520Diagnosis%2520Framework%2520for%2520Complex%2520Autonomous%2520Driving%2520Systems%26entry.906535625%3DStefan%2520Orf%2520and%2520Sven%2520Ochs%2520and%2520Jens%2520Doll%2520and%2520Albert%2520Schotschneider%2520and%2520Marc%2520Heinrich%2520and%2520Marc%2520Ren%25C3%25A9%2520Zofka%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Fault%2520diagnosis%2520is%2520crucial%2520for%2520complex%2520autonomous%2520mobile%2520systems%252C%2520especially%250Afor%2520modern-day%2520autonomous%2520driving%2520%2528AD%2529.%2520Different%2520actors%252C%2520numerous%2520use%2520cases%252C%250Aand%2520complex%2520heterogeneous%2520components%2520motivate%2520a%2520fault%2520diagnosis%2520of%2520the%2520system%250Aand%2520overall%2520system%2520integrity.%2520AD%2520systems%2520are%2520composed%2520of%2520many%2520heterogeneous%250Acomponents%252C%2520each%2520with%2520different%2520functionality%2520and%2520possibly%2520using%2520a%2520different%250Aalgorithm%2520%2528e.g.%252C%2520rule-based%2520vs.%2520AI%2520components%2529.%2520In%2520addition%252C%2520these%2520components%250Aare%2520subject%2520to%2520the%2520vehicle%2527s%2520driving%2520state%2520and%2520are%2520highly%2520dependent.%2520This%250Apaper%252C%2520therefore%252C%2520faces%2520this%2520problem%2520by%2520presenting%2520the%2520concept%2520of%2520a%2520modular%250Afault%2520diagnosis%2520framework%2520for%2520AD%2520systems.%2520The%2520concept%2520suggests%2520modular%2520state%250Amonitoring%2520and%2520diagnosis%2520elements%252C%2520together%2520with%2520a%2520state-%2520and%2520dependency-aware%250Aaggregation%2520method.%2520Our%2520proposed%2520classification%2520scheme%2520allows%2520for%2520the%250Acategorization%2520of%2520the%2520fault%2520diagnosis%2520modules.%2520The%2520concept%2520is%2520implemented%2520on%2520AD%250Ashuttle%2520buses%2520and%2520evaluated%2520to%2520demonstrate%2520its%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Fault%20Diagnosis%20Framework%20for%20Complex%20Autonomous%20Driving%20Systems&entry.906535625=Stefan%20Orf%20and%20Sven%20Ochs%20and%20Jens%20Doll%20and%20Albert%20Schotschneider%20and%20Marc%20Heinrich%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Fault%20diagnosis%20is%20crucial%20for%20complex%20autonomous%20mobile%20systems%2C%20especially%0Afor%20modern-day%20autonomous%20driving%20%28AD%29.%20Different%20actors%2C%20numerous%20use%20cases%2C%0Aand%20complex%20heterogeneous%20components%20motivate%20a%20fault%20diagnosis%20of%20the%20system%0Aand%20overall%20system%20integrity.%20AD%20systems%20are%20composed%20of%20many%20heterogeneous%0Acomponents%2C%20each%20with%20different%20functionality%20and%20possibly%20using%20a%20different%0Aalgorithm%20%28e.g.%2C%20rule-based%20vs.%20AI%20components%29.%20In%20addition%2C%20these%20components%0Aare%20subject%20to%20the%20vehicle%27s%20driving%20state%20and%20are%20highly%20dependent.%20This%0Apaper%2C%20therefore%2C%20faces%20this%20problem%20by%20presenting%20the%20concept%20of%20a%20modular%0Afault%20diagnosis%20framework%20for%20AD%20systems.%20The%20concept%20suggests%20modular%20state%0Amonitoring%20and%20diagnosis%20elements%2C%20together%20with%20a%20state-%20and%20dependency-aware%0Aaggregation%20method.%20Our%20proposed%20classification%20scheme%20allows%20for%20the%0Acategorization%20of%20the%20fault%20diagnosis%20modules.%20The%20concept%20is%20implemented%20on%20AD%0Ashuttle%20buses%20and%20evaluated%20to%20demonstrate%20its%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09643v1&entry.124074799=Read"},
{"title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric", "author": "Hyukhun Koh and Dohyung Kim and Minwoo Lee and Kyomin Jung", "abstract": "  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n", "link": "http://arxiv.org/abs/2402.06900v5", "date": "2024-11-14", "relevancy": 1.8194, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Recognize%20Toxicity%3F%20A%20Structured%20Investigation%20Framework%20and%0A%20%20Toxicity%20Metric&body=Title%3A%20Can%20LLMs%20Recognize%20Toxicity%3F%20A%20Structured%20Investigation%20Framework%20and%0A%20%20Toxicity%20Metric%0AAuthor%3A%20Hyukhun%20Koh%20and%20Dohyung%20Kim%20and%20Minwoo%20Lee%20and%20Kyomin%20Jung%0AAbstract%3A%20%20%20In%20the%20pursuit%20of%20developing%20Large%20Language%20Models%20%28LLMs%29%20that%20adhere%20to%0Asocietal%20standards%2C%20it%20is%20imperative%20to%20detect%20the%20toxicity%20in%20the%20generated%0Atext.%20The%20majority%20of%20existing%20toxicity%20metrics%20rely%20on%20encoder%20models%20trained%0Aon%20specific%20toxicity%20datasets%2C%20which%20are%20susceptible%20to%20out-of-distribution%0A%28OOD%29%20problems%20and%20depend%20on%20the%20dataset%27s%20definition%20of%20toxicity.%20In%20this%0Apaper%2C%20we%20introduce%20a%20robust%20metric%20grounded%20on%20LLMs%20to%20flexibly%20measure%0Atoxicity%20according%20to%20the%20given%20definition.%20We%20first%20analyze%20the%20toxicity%0Afactors%2C%20followed%20by%20an%20examination%20of%20the%20intrinsic%20toxic%20attributes%20of%20LLMs%0Ato%20ascertain%20their%20suitability%20as%20evaluators.%20Finally%2C%20we%20evaluate%20the%0Aperformance%20of%20our%20metric%20with%20detailed%20analysis.%20Our%20empirical%20results%0Ademonstrate%20outstanding%20performance%20in%20measuring%20toxicity%20within%20verified%0Afactors%2C%20improving%20on%20conventional%20metrics%20by%2012%20points%20in%20the%20F1%20score.%20Our%0Afindings%20also%20indicate%20that%20upstream%20toxicity%20significantly%20influences%0Adownstream%20metrics%2C%20suggesting%20that%20LLMs%20are%20unsuitable%20for%20toxicity%0Aevaluations%20within%20unverified%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06900v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Recognize%2520Toxicity%253F%2520A%2520Structured%2520Investigation%2520Framework%2520and%250A%2520%2520Toxicity%2520Metric%26entry.906535625%3DHyukhun%2520Koh%2520and%2520Dohyung%2520Kim%2520and%2520Minwoo%2520Lee%2520and%2520Kyomin%2520Jung%26entry.1292438233%3D%2520%2520In%2520the%2520pursuit%2520of%2520developing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520adhere%2520to%250Asocietal%2520standards%252C%2520it%2520is%2520imperative%2520to%2520detect%2520the%2520toxicity%2520in%2520the%2520generated%250Atext.%2520The%2520majority%2520of%2520existing%2520toxicity%2520metrics%2520rely%2520on%2520encoder%2520models%2520trained%250Aon%2520specific%2520toxicity%2520datasets%252C%2520which%2520are%2520susceptible%2520to%2520out-of-distribution%250A%2528OOD%2529%2520problems%2520and%2520depend%2520on%2520the%2520dataset%2527s%2520definition%2520of%2520toxicity.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520robust%2520metric%2520grounded%2520on%2520LLMs%2520to%2520flexibly%2520measure%250Atoxicity%2520according%2520to%2520the%2520given%2520definition.%2520We%2520first%2520analyze%2520the%2520toxicity%250Afactors%252C%2520followed%2520by%2520an%2520examination%2520of%2520the%2520intrinsic%2520toxic%2520attributes%2520of%2520LLMs%250Ato%2520ascertain%2520their%2520suitability%2520as%2520evaluators.%2520Finally%252C%2520we%2520evaluate%2520the%250Aperformance%2520of%2520our%2520metric%2520with%2520detailed%2520analysis.%2520Our%2520empirical%2520results%250Ademonstrate%2520outstanding%2520performance%2520in%2520measuring%2520toxicity%2520within%2520verified%250Afactors%252C%2520improving%2520on%2520conventional%2520metrics%2520by%252012%2520points%2520in%2520the%2520F1%2520score.%2520Our%250Afindings%2520also%2520indicate%2520that%2520upstream%2520toxicity%2520significantly%2520influences%250Adownstream%2520metrics%252C%2520suggesting%2520that%2520LLMs%2520are%2520unsuitable%2520for%2520toxicity%250Aevaluations%2520within%2520unverified%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06900v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Recognize%20Toxicity%3F%20A%20Structured%20Investigation%20Framework%20and%0A%20%20Toxicity%20Metric&entry.906535625=Hyukhun%20Koh%20and%20Dohyung%20Kim%20and%20Minwoo%20Lee%20and%20Kyomin%20Jung&entry.1292438233=%20%20In%20the%20pursuit%20of%20developing%20Large%20Language%20Models%20%28LLMs%29%20that%20adhere%20to%0Asocietal%20standards%2C%20it%20is%20imperative%20to%20detect%20the%20toxicity%20in%20the%20generated%0Atext.%20The%20majority%20of%20existing%20toxicity%20metrics%20rely%20on%20encoder%20models%20trained%0Aon%20specific%20toxicity%20datasets%2C%20which%20are%20susceptible%20to%20out-of-distribution%0A%28OOD%29%20problems%20and%20depend%20on%20the%20dataset%27s%20definition%20of%20toxicity.%20In%20this%0Apaper%2C%20we%20introduce%20a%20robust%20metric%20grounded%20on%20LLMs%20to%20flexibly%20measure%0Atoxicity%20according%20to%20the%20given%20definition.%20We%20first%20analyze%20the%20toxicity%0Afactors%2C%20followed%20by%20an%20examination%20of%20the%20intrinsic%20toxic%20attributes%20of%20LLMs%0Ato%20ascertain%20their%20suitability%20as%20evaluators.%20Finally%2C%20we%20evaluate%20the%0Aperformance%20of%20our%20metric%20with%20detailed%20analysis.%20Our%20empirical%20results%0Ademonstrate%20outstanding%20performance%20in%20measuring%20toxicity%20within%20verified%0Afactors%2C%20improving%20on%20conventional%20metrics%20by%2012%20points%20in%20the%20F1%20score.%20Our%0Afindings%20also%20indicate%20that%20upstream%20toxicity%20significantly%20influences%0Adownstream%20metrics%2C%20suggesting%20that%20LLMs%20are%20unsuitable%20for%20toxicity%0Aevaluations%20within%20unverified%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06900v5&entry.124074799=Read"},
{"title": "Towards a Classification of Open-Source ML Models and Datasets for\n  Software Engineering", "author": "Alexandra Gonz\u00e1lez and Xavier Franch and David Lo and Silverio Mart\u00ednez-Fern\u00e1ndez", "abstract": "  Background: Open-Source Pre-Trained Models (PTMs) and datasets provide\nextensive resources for various Machine Learning (ML) tasks, yet these\nresources lack a classification tailored to Software Engineering (SE) needs.\nAims: We apply an SE-oriented classification to PTMs and datasets on a popular\nopen-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs\nover time. Method: We conducted a repository mining study. We started with a\nsystematically gathered database of PTMs and datasets from the HF API. Our\nselection was refined by analyzing model and dataset cards and metadata, such\nas tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are\nreplicable, with a publicly accessible replication package. Results: The most\ncommon SE task among PTMs and datasets is code generation, with a primary focus\non software development and limited attention to software management. Popular\nPTMs and datasets mainly target software development. Among ML tasks, text\ngeneration is the most common in SE PTMs and datasets. There has been a marked\nincrease in PTMs for SE since 2023 Q2. Conclusions: This study underscores the\nneed for broader task coverage to enhance the integration of ML within SE\npractices.\n", "link": "http://arxiv.org/abs/2411.09683v1", "date": "2024-11-14", "relevancy": 1.8133, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Classification%20of%20Open-Source%20ML%20Models%20and%20Datasets%20for%0A%20%20Software%20Engineering&body=Title%3A%20Towards%20a%20Classification%20of%20Open-Source%20ML%20Models%20and%20Datasets%20for%0A%20%20Software%20Engineering%0AAuthor%3A%20Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez%0AAbstract%3A%20%20%20Background%3A%20Open-Source%20Pre-Trained%20Models%20%28PTMs%29%20and%20datasets%20provide%0Aextensive%20resources%20for%20various%20Machine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%0Aresources%20lack%20a%20classification%20tailored%20to%20Software%20Engineering%20%28SE%29%20needs.%0AAims%3A%20We%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20and%20datasets%20on%20a%20popular%0Aopen-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29%2C%20and%20analyze%20the%20evolution%20of%20PTMs%0Aover%20time.%20Method%3A%20We%20conducted%20a%20repository%20mining%20study.%20We%20started%20with%20a%0Asystematically%20gathered%20database%20of%20PTMs%20and%20datasets%20from%20the%20HF%20API.%20Our%0Aselection%20was%20refined%20by%20analyzing%20model%20and%20dataset%20cards%20and%20metadata%2C%20such%0Aas%20tags%2C%20and%20confirming%20SE%20relevance%20using%20Gemini%201.5%20Pro.%20All%20analyses%20are%0Areplicable%2C%20with%20a%20publicly%20accessible%20replication%20package.%20Results%3A%20The%20most%0Acommon%20SE%20task%20among%20PTMs%20and%20datasets%20is%20code%20generation%2C%20with%20a%20primary%20focus%0Aon%20software%20development%20and%20limited%20attention%20to%20software%20management.%20Popular%0APTMs%20and%20datasets%20mainly%20target%20software%20development.%20Among%20ML%20tasks%2C%20text%0Ageneration%20is%20the%20most%20common%20in%20SE%20PTMs%20and%20datasets.%20There%20has%20been%20a%20marked%0Aincrease%20in%20PTMs%20for%20SE%20since%202023%20Q2.%20Conclusions%3A%20This%20study%20underscores%20the%0Aneed%20for%20broader%20task%20coverage%20to%20enhance%20the%20integration%20of%20ML%20within%20SE%0Apractices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Classification%2520of%2520Open-Source%2520ML%2520Models%2520and%2520Datasets%2520for%250A%2520%2520Software%2520Engineering%26entry.906535625%3DAlexandra%2520Gonz%25C3%25A1lez%2520and%2520Xavier%2520Franch%2520and%2520David%2520Lo%2520and%2520Silverio%2520Mart%25C3%25ADnez-Fern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520Background%253A%2520Open-Source%2520Pre-Trained%2520Models%2520%2528PTMs%2529%2520and%2520datasets%2520provide%250Aextensive%2520resources%2520for%2520various%2520Machine%2520Learning%2520%2528ML%2529%2520tasks%252C%2520yet%2520these%250Aresources%2520lack%2520a%2520classification%2520tailored%2520to%2520Software%2520Engineering%2520%2528SE%2529%2520needs.%250AAims%253A%2520We%2520apply%2520an%2520SE-oriented%2520classification%2520to%2520PTMs%2520and%2520datasets%2520on%2520a%2520popular%250Aopen-source%2520ML%2520repository%252C%2520Hugging%2520Face%2520%2528HF%2529%252C%2520and%2520analyze%2520the%2520evolution%2520of%2520PTMs%250Aover%2520time.%2520Method%253A%2520We%2520conducted%2520a%2520repository%2520mining%2520study.%2520We%2520started%2520with%2520a%250Asystematically%2520gathered%2520database%2520of%2520PTMs%2520and%2520datasets%2520from%2520the%2520HF%2520API.%2520Our%250Aselection%2520was%2520refined%2520by%2520analyzing%2520model%2520and%2520dataset%2520cards%2520and%2520metadata%252C%2520such%250Aas%2520tags%252C%2520and%2520confirming%2520SE%2520relevance%2520using%2520Gemini%25201.5%2520Pro.%2520All%2520analyses%2520are%250Areplicable%252C%2520with%2520a%2520publicly%2520accessible%2520replication%2520package.%2520Results%253A%2520The%2520most%250Acommon%2520SE%2520task%2520among%2520PTMs%2520and%2520datasets%2520is%2520code%2520generation%252C%2520with%2520a%2520primary%2520focus%250Aon%2520software%2520development%2520and%2520limited%2520attention%2520to%2520software%2520management.%2520Popular%250APTMs%2520and%2520datasets%2520mainly%2520target%2520software%2520development.%2520Among%2520ML%2520tasks%252C%2520text%250Ageneration%2520is%2520the%2520most%2520common%2520in%2520SE%2520PTMs%2520and%2520datasets.%2520There%2520has%2520been%2520a%2520marked%250Aincrease%2520in%2520PTMs%2520for%2520SE%2520since%25202023%2520Q2.%2520Conclusions%253A%2520This%2520study%2520underscores%2520the%250Aneed%2520for%2520broader%2520task%2520coverage%2520to%2520enhance%2520the%2520integration%2520of%2520ML%2520within%2520SE%250Apractices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Classification%20of%20Open-Source%20ML%20Models%20and%20Datasets%20for%0A%20%20Software%20Engineering&entry.906535625=Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez&entry.1292438233=%20%20Background%3A%20Open-Source%20Pre-Trained%20Models%20%28PTMs%29%20and%20datasets%20provide%0Aextensive%20resources%20for%20various%20Machine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%0Aresources%20lack%20a%20classification%20tailored%20to%20Software%20Engineering%20%28SE%29%20needs.%0AAims%3A%20We%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20and%20datasets%20on%20a%20popular%0Aopen-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29%2C%20and%20analyze%20the%20evolution%20of%20PTMs%0Aover%20time.%20Method%3A%20We%20conducted%20a%20repository%20mining%20study.%20We%20started%20with%20a%0Asystematically%20gathered%20database%20of%20PTMs%20and%20datasets%20from%20the%20HF%20API.%20Our%0Aselection%20was%20refined%20by%20analyzing%20model%20and%20dataset%20cards%20and%20metadata%2C%20such%0Aas%20tags%2C%20and%20confirming%20SE%20relevance%20using%20Gemini%201.5%20Pro.%20All%20analyses%20are%0Areplicable%2C%20with%20a%20publicly%20accessible%20replication%20package.%20Results%3A%20The%20most%0Acommon%20SE%20task%20among%20PTMs%20and%20datasets%20is%20code%20generation%2C%20with%20a%20primary%20focus%0Aon%20software%20development%20and%20limited%20attention%20to%20software%20management.%20Popular%0APTMs%20and%20datasets%20mainly%20target%20software%20development.%20Among%20ML%20tasks%2C%20text%0Ageneration%20is%20the%20most%20common%20in%20SE%20PTMs%20and%20datasets.%20There%20has%20been%20a%20marked%0Aincrease%20in%20PTMs%20for%20SE%20since%202023%20Q2.%20Conclusions%3A%20This%20study%20underscores%20the%0Aneed%20for%20broader%20task%20coverage%20to%20enhance%20the%20integration%20of%20ML%20within%20SE%0Apractices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09683v1&entry.124074799=Read"},
{"title": "MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation", "author": "Jonas Serych and Michal Neoral and Jiri Matas", "abstract": "  In this work, we present MFTIQ, a novel dense long-term tracking model that\nadvances the Multi-Flow Tracker (MFT) framework to address challenges in\npoint-level visual tracking in video sequences. MFTIQ builds upon the\nflow-chaining concepts of MFT, integrating an Independent Quality (IQ) module\nthat separates correspondence quality estimation from optical flow\ncomputations. This decoupling significantly enhances the accuracy and\nflexibility of the tracking process, allowing MFTIQ to maintain reliable\ntrajectory predictions even in scenarios of prolonged occlusions and complex\ndynamics. Designed to be \"plug-and-play\", MFTIQ can be employed with any\noff-the-shelf optical flow method without the need for fine-tuning or\narchitectural modifications. Experimental validations on the TAP-Vid Davis\ndataset show that MFTIQ with RoMa optical flow not only surpasses MFT but also\nperforms comparably to state-of-the-art trackers while having substantially\nfaster processing speed. Code and models available at\nhttps://github.com/serycjon/MFTIQ .\n", "link": "http://arxiv.org/abs/2411.09551v1", "date": "2024-11-14", "relevancy": 1.5057, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5051}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5049}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFTIQ%3A%20Multi-Flow%20Tracker%20with%20Independent%20Matching%20Quality%20Estimation&body=Title%3A%20MFTIQ%3A%20Multi-Flow%20Tracker%20with%20Independent%20Matching%20Quality%20Estimation%0AAuthor%3A%20Jonas%20Serych%20and%20Michal%20Neoral%20and%20Jiri%20Matas%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20MFTIQ%2C%20a%20novel%20dense%20long-term%20tracking%20model%20that%0Aadvances%20the%20Multi-Flow%20Tracker%20%28MFT%29%20framework%20to%20address%20challenges%20in%0Apoint-level%20visual%20tracking%20in%20video%20sequences.%20MFTIQ%20builds%20upon%20the%0Aflow-chaining%20concepts%20of%20MFT%2C%20integrating%20an%20Independent%20Quality%20%28IQ%29%20module%0Athat%20separates%20correspondence%20quality%20estimation%20from%20optical%20flow%0Acomputations.%20This%20decoupling%20significantly%20enhances%20the%20accuracy%20and%0Aflexibility%20of%20the%20tracking%20process%2C%20allowing%20MFTIQ%20to%20maintain%20reliable%0Atrajectory%20predictions%20even%20in%20scenarios%20of%20prolonged%20occlusions%20and%20complex%0Adynamics.%20Designed%20to%20be%20%22plug-and-play%22%2C%20MFTIQ%20can%20be%20employed%20with%20any%0Aoff-the-shelf%20optical%20flow%20method%20without%20the%20need%20for%20fine-tuning%20or%0Aarchitectural%20modifications.%20Experimental%20validations%20on%20the%20TAP-Vid%20Davis%0Adataset%20show%20that%20MFTIQ%20with%20RoMa%20optical%20flow%20not%20only%20surpasses%20MFT%20but%20also%0Aperforms%20comparably%20to%20state-of-the-art%20trackers%20while%20having%20substantially%0Afaster%20processing%20speed.%20Code%20and%20models%20available%20at%0Ahttps%3A//github.com/serycjon/MFTIQ%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFTIQ%253A%2520Multi-Flow%2520Tracker%2520with%2520Independent%2520Matching%2520Quality%2520Estimation%26entry.906535625%3DJonas%2520Serych%2520and%2520Michal%2520Neoral%2520and%2520Jiri%2520Matas%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520MFTIQ%252C%2520a%2520novel%2520dense%2520long-term%2520tracking%2520model%2520that%250Aadvances%2520the%2520Multi-Flow%2520Tracker%2520%2528MFT%2529%2520framework%2520to%2520address%2520challenges%2520in%250Apoint-level%2520visual%2520tracking%2520in%2520video%2520sequences.%2520MFTIQ%2520builds%2520upon%2520the%250Aflow-chaining%2520concepts%2520of%2520MFT%252C%2520integrating%2520an%2520Independent%2520Quality%2520%2528IQ%2529%2520module%250Athat%2520separates%2520correspondence%2520quality%2520estimation%2520from%2520optical%2520flow%250Acomputations.%2520This%2520decoupling%2520significantly%2520enhances%2520the%2520accuracy%2520and%250Aflexibility%2520of%2520the%2520tracking%2520process%252C%2520allowing%2520MFTIQ%2520to%2520maintain%2520reliable%250Atrajectory%2520predictions%2520even%2520in%2520scenarios%2520of%2520prolonged%2520occlusions%2520and%2520complex%250Adynamics.%2520Designed%2520to%2520be%2520%2522plug-and-play%2522%252C%2520MFTIQ%2520can%2520be%2520employed%2520with%2520any%250Aoff-the-shelf%2520optical%2520flow%2520method%2520without%2520the%2520need%2520for%2520fine-tuning%2520or%250Aarchitectural%2520modifications.%2520Experimental%2520validations%2520on%2520the%2520TAP-Vid%2520Davis%250Adataset%2520show%2520that%2520MFTIQ%2520with%2520RoMa%2520optical%2520flow%2520not%2520only%2520surpasses%2520MFT%2520but%2520also%250Aperforms%2520comparably%2520to%2520state-of-the-art%2520trackers%2520while%2520having%2520substantially%250Afaster%2520processing%2520speed.%2520Code%2520and%2520models%2520available%2520at%250Ahttps%253A//github.com/serycjon/MFTIQ%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFTIQ%3A%20Multi-Flow%20Tracker%20with%20Independent%20Matching%20Quality%20Estimation&entry.906535625=Jonas%20Serych%20and%20Michal%20Neoral%20and%20Jiri%20Matas&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20MFTIQ%2C%20a%20novel%20dense%20long-term%20tracking%20model%20that%0Aadvances%20the%20Multi-Flow%20Tracker%20%28MFT%29%20framework%20to%20address%20challenges%20in%0Apoint-level%20visual%20tracking%20in%20video%20sequences.%20MFTIQ%20builds%20upon%20the%0Aflow-chaining%20concepts%20of%20MFT%2C%20integrating%20an%20Independent%20Quality%20%28IQ%29%20module%0Athat%20separates%20correspondence%20quality%20estimation%20from%20optical%20flow%0Acomputations.%20This%20decoupling%20significantly%20enhances%20the%20accuracy%20and%0Aflexibility%20of%20the%20tracking%20process%2C%20allowing%20MFTIQ%20to%20maintain%20reliable%0Atrajectory%20predictions%20even%20in%20scenarios%20of%20prolonged%20occlusions%20and%20complex%0Adynamics.%20Designed%20to%20be%20%22plug-and-play%22%2C%20MFTIQ%20can%20be%20employed%20with%20any%0Aoff-the-shelf%20optical%20flow%20method%20without%20the%20need%20for%20fine-tuning%20or%0Aarchitectural%20modifications.%20Experimental%20validations%20on%20the%20TAP-Vid%20Davis%0Adataset%20show%20that%20MFTIQ%20with%20RoMa%20optical%20flow%20not%20only%20surpasses%20MFT%20but%20also%0Aperforms%20comparably%20to%20state-of-the-art%20trackers%20while%20having%20substantially%0Afaster%20processing%20speed.%20Code%20and%20models%20available%20at%0Ahttps%3A//github.com/serycjon/MFTIQ%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09551v1&entry.124074799=Read"},
{"title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims", "author": "Haoran Wang and Aman Rangapur and Xiongxiao Xu and Yueqing Liang and Haroon Gharwi and Carl Yang and Kai Shu", "abstract": "  Existing claim verification datasets often do not require systems to perform\ncomplex reasoning or effectively interpret multimodal evidence. To address\nthis, we introduce a new task: multi-hop multimodal claim verification. This\ntask challenges models to reason over multiple pieces of evidence from diverse\nsources, including text, images, and tables, and determine whether the combined\nmultimodal evidence supports or refutes a given claim. To study this task, we\nconstruct MMCV, a large-scale dataset comprising 16k multi-hop claims paired\nwith multimodal evidence, generated and refined using large language models,\nwith additional input from human feedback. We show that MMCV is challenging\neven for the latest state-of-the-art multimodal large language models,\nespecially as the number of reasoning hops increases. Additionally, we\nestablish a human performance benchmark on a subset of MMCV. We hope this\ndataset and its evaluation task will encourage future research in multimodal\nmulti-hop claim verification.\n", "link": "http://arxiv.org/abs/2411.09547v1", "date": "2024-11-14", "relevancy": 1.6281, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Piecing%20It%20All%20Together%3A%20Verifying%20Multi-Hop%20Multimodal%20Claims&body=Title%3A%20Piecing%20It%20All%20Together%3A%20Verifying%20Multi-Hop%20Multimodal%20Claims%0AAuthor%3A%20Haoran%20Wang%20and%20Aman%20Rangapur%20and%20Xiongxiao%20Xu%20and%20Yueqing%20Liang%20and%20Haroon%20Gharwi%20and%20Carl%20Yang%20and%20Kai%20Shu%0AAbstract%3A%20%20%20Existing%20claim%20verification%20datasets%20often%20do%20not%20require%20systems%20to%20perform%0Acomplex%20reasoning%20or%20effectively%20interpret%20multimodal%20evidence.%20To%20address%0Athis%2C%20we%20introduce%20a%20new%20task%3A%20multi-hop%20multimodal%20claim%20verification.%20This%0Atask%20challenges%20models%20to%20reason%20over%20multiple%20pieces%20of%20evidence%20from%20diverse%0Asources%2C%20including%20text%2C%20images%2C%20and%20tables%2C%20and%20determine%20whether%20the%20combined%0Amultimodal%20evidence%20supports%20or%20refutes%20a%20given%20claim.%20To%20study%20this%20task%2C%20we%0Aconstruct%20MMCV%2C%20a%20large-scale%20dataset%20comprising%2016k%20multi-hop%20claims%20paired%0Awith%20multimodal%20evidence%2C%20generated%20and%20refined%20using%20large%20language%20models%2C%0Awith%20additional%20input%20from%20human%20feedback.%20We%20show%20that%20MMCV%20is%20challenging%0Aeven%20for%20the%20latest%20state-of-the-art%20multimodal%20large%20language%20models%2C%0Aespecially%20as%20the%20number%20of%20reasoning%20hops%20increases.%20Additionally%2C%20we%0Aestablish%20a%20human%20performance%20benchmark%20on%20a%20subset%20of%20MMCV.%20We%20hope%20this%0Adataset%20and%20its%20evaluation%20task%20will%20encourage%20future%20research%20in%20multimodal%0Amulti-hop%20claim%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiecing%2520It%2520All%2520Together%253A%2520Verifying%2520Multi-Hop%2520Multimodal%2520Claims%26entry.906535625%3DHaoran%2520Wang%2520and%2520Aman%2520Rangapur%2520and%2520Xiongxiao%2520Xu%2520and%2520Yueqing%2520Liang%2520and%2520Haroon%2520Gharwi%2520and%2520Carl%2520Yang%2520and%2520Kai%2520Shu%26entry.1292438233%3D%2520%2520Existing%2520claim%2520verification%2520datasets%2520often%2520do%2520not%2520require%2520systems%2520to%2520perform%250Acomplex%2520reasoning%2520or%2520effectively%2520interpret%2520multimodal%2520evidence.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520a%2520new%2520task%253A%2520multi-hop%2520multimodal%2520claim%2520verification.%2520This%250Atask%2520challenges%2520models%2520to%2520reason%2520over%2520multiple%2520pieces%2520of%2520evidence%2520from%2520diverse%250Asources%252C%2520including%2520text%252C%2520images%252C%2520and%2520tables%252C%2520and%2520determine%2520whether%2520the%2520combined%250Amultimodal%2520evidence%2520supports%2520or%2520refutes%2520a%2520given%2520claim.%2520To%2520study%2520this%2520task%252C%2520we%250Aconstruct%2520MMCV%252C%2520a%2520large-scale%2520dataset%2520comprising%252016k%2520multi-hop%2520claims%2520paired%250Awith%2520multimodal%2520evidence%252C%2520generated%2520and%2520refined%2520using%2520large%2520language%2520models%252C%250Awith%2520additional%2520input%2520from%2520human%2520feedback.%2520We%2520show%2520that%2520MMCV%2520is%2520challenging%250Aeven%2520for%2520the%2520latest%2520state-of-the-art%2520multimodal%2520large%2520language%2520models%252C%250Aespecially%2520as%2520the%2520number%2520of%2520reasoning%2520hops%2520increases.%2520Additionally%252C%2520we%250Aestablish%2520a%2520human%2520performance%2520benchmark%2520on%2520a%2520subset%2520of%2520MMCV.%2520We%2520hope%2520this%250Adataset%2520and%2520its%2520evaluation%2520task%2520will%2520encourage%2520future%2520research%2520in%2520multimodal%250Amulti-hop%2520claim%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Piecing%20It%20All%20Together%3A%20Verifying%20Multi-Hop%20Multimodal%20Claims&entry.906535625=Haoran%20Wang%20and%20Aman%20Rangapur%20and%20Xiongxiao%20Xu%20and%20Yueqing%20Liang%20and%20Haroon%20Gharwi%20and%20Carl%20Yang%20and%20Kai%20Shu&entry.1292438233=%20%20Existing%20claim%20verification%20datasets%20often%20do%20not%20require%20systems%20to%20perform%0Acomplex%20reasoning%20or%20effectively%20interpret%20multimodal%20evidence.%20To%20address%0Athis%2C%20we%20introduce%20a%20new%20task%3A%20multi-hop%20multimodal%20claim%20verification.%20This%0Atask%20challenges%20models%20to%20reason%20over%20multiple%20pieces%20of%20evidence%20from%20diverse%0Asources%2C%20including%20text%2C%20images%2C%20and%20tables%2C%20and%20determine%20whether%20the%20combined%0Amultimodal%20evidence%20supports%20or%20refutes%20a%20given%20claim.%20To%20study%20this%20task%2C%20we%0Aconstruct%20MMCV%2C%20a%20large-scale%20dataset%20comprising%2016k%20multi-hop%20claims%20paired%0Awith%20multimodal%20evidence%2C%20generated%20and%20refined%20using%20large%20language%20models%2C%0Awith%20additional%20input%20from%20human%20feedback.%20We%20show%20that%20MMCV%20is%20challenging%0Aeven%20for%20the%20latest%20state-of-the-art%20multimodal%20large%20language%20models%2C%0Aespecially%20as%20the%20number%20of%20reasoning%20hops%20increases.%20Additionally%2C%20we%0Aestablish%20a%20human%20performance%20benchmark%20on%20a%20subset%20of%20MMCV.%20We%20hope%20this%0Adataset%20and%20its%20evaluation%20task%20will%20encourage%20future%20research%20in%20multimodal%0Amulti-hop%20claim%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09547v1&entry.124074799=Read"},
{"title": "LTLf+ and PPLTL+: Extending LTLf and PPLTL to Infinite Traces", "author": "Benjamin Aminof and Giuseppe De Giacomo and Sasha Rubin and Moshe Y. Vardi", "abstract": "  We introduce LTLf+ and PPLTL+, two logics to express properties of infinite\ntraces, that are based on the linear-time temporal logics LTLf and PPLTL on\nfinite traces. LTLf+/PPLTL+ use levels of Manna and Pnueli's LTL\nsafety-progress hierarchy, and thus have the same expressive power as LTL.\nHowever, they also retain a crucial characteristic of the reactive synthesis\nproblem for the base logics: the game arena for strategy extraction can be\nderived from deterministic finite automata (DFA). Consequently, these logics\ncircumvent the notorious difficulties associated with determinizing infinite\ntrace automata, typical of LTL reactive synthesis. We present DFA-based\nsynthesis techniques for LTLf+/PPLTL+, and show that synthesis is\n2EXPTIME-complete for LTLf+ (matching LTLf) and EXPTIME-complete for PPLTL+\n(matching PPLTL). Notably, while PPLTL+ retains the full expressive power of\nLTL, reactive synthesis is EXPTIME-complete instead of 2EXPTIME-complete. The\ntechniques are also adapted to optimally solve satisfiability, validity, and\nmodel-checking, to get EXPSPACE-complete for LTLf+ (extending a recent result\nfor the guarantee level using LTLf), and PSPACE-complete for PPLTL+.\n", "link": "http://arxiv.org/abs/2411.09366v1", "date": "2024-11-14", "relevancy": 1.1584, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3917}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.38}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTLf%2B%20and%20PPLTL%2B%3A%20Extending%20LTLf%20and%20PPLTL%20to%20Infinite%20Traces&body=Title%3A%20LTLf%2B%20and%20PPLTL%2B%3A%20Extending%20LTLf%20and%20PPLTL%20to%20Infinite%20Traces%0AAuthor%3A%20Benjamin%20Aminof%20and%20Giuseppe%20De%20Giacomo%20and%20Sasha%20Rubin%20and%20Moshe%20Y.%20Vardi%0AAbstract%3A%20%20%20We%20introduce%20LTLf%2B%20and%20PPLTL%2B%2C%20two%20logics%20to%20express%20properties%20of%20infinite%0Atraces%2C%20that%20are%20based%20on%20the%20linear-time%20temporal%20logics%20LTLf%20and%20PPLTL%20on%0Afinite%20traces.%20LTLf%2B/PPLTL%2B%20use%20levels%20of%20Manna%20and%20Pnueli%27s%20LTL%0Asafety-progress%20hierarchy%2C%20and%20thus%20have%20the%20same%20expressive%20power%20as%20LTL.%0AHowever%2C%20they%20also%20retain%20a%20crucial%20characteristic%20of%20the%20reactive%20synthesis%0Aproblem%20for%20the%20base%20logics%3A%20the%20game%20arena%20for%20strategy%20extraction%20can%20be%0Aderived%20from%20deterministic%20finite%20automata%20%28DFA%29.%20Consequently%2C%20these%20logics%0Acircumvent%20the%20notorious%20difficulties%20associated%20with%20determinizing%20infinite%0Atrace%20automata%2C%20typical%20of%20LTL%20reactive%20synthesis.%20We%20present%20DFA-based%0Asynthesis%20techniques%20for%20LTLf%2B/PPLTL%2B%2C%20and%20show%20that%20synthesis%20is%0A2EXPTIME-complete%20for%20LTLf%2B%20%28matching%20LTLf%29%20and%20EXPTIME-complete%20for%20PPLTL%2B%0A%28matching%20PPLTL%29.%20Notably%2C%20while%20PPLTL%2B%20retains%20the%20full%20expressive%20power%20of%0ALTL%2C%20reactive%20synthesis%20is%20EXPTIME-complete%20instead%20of%202EXPTIME-complete.%20The%0Atechniques%20are%20also%20adapted%20to%20optimally%20solve%20satisfiability%2C%20validity%2C%20and%0Amodel-checking%2C%20to%20get%20EXPSPACE-complete%20for%20LTLf%2B%20%28extending%20a%20recent%20result%0Afor%20the%20guarantee%20level%20using%20LTLf%29%2C%20and%20PSPACE-complete%20for%20PPLTL%2B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTLf%252B%2520and%2520PPLTL%252B%253A%2520Extending%2520LTLf%2520and%2520PPLTL%2520to%2520Infinite%2520Traces%26entry.906535625%3DBenjamin%2520Aminof%2520and%2520Giuseppe%2520De%2520Giacomo%2520and%2520Sasha%2520Rubin%2520and%2520Moshe%2520Y.%2520Vardi%26entry.1292438233%3D%2520%2520We%2520introduce%2520LTLf%252B%2520and%2520PPLTL%252B%252C%2520two%2520logics%2520to%2520express%2520properties%2520of%2520infinite%250Atraces%252C%2520that%2520are%2520based%2520on%2520the%2520linear-time%2520temporal%2520logics%2520LTLf%2520and%2520PPLTL%2520on%250Afinite%2520traces.%2520LTLf%252B/PPLTL%252B%2520use%2520levels%2520of%2520Manna%2520and%2520Pnueli%2527s%2520LTL%250Asafety-progress%2520hierarchy%252C%2520and%2520thus%2520have%2520the%2520same%2520expressive%2520power%2520as%2520LTL.%250AHowever%252C%2520they%2520also%2520retain%2520a%2520crucial%2520characteristic%2520of%2520the%2520reactive%2520synthesis%250Aproblem%2520for%2520the%2520base%2520logics%253A%2520the%2520game%2520arena%2520for%2520strategy%2520extraction%2520can%2520be%250Aderived%2520from%2520deterministic%2520finite%2520automata%2520%2528DFA%2529.%2520Consequently%252C%2520these%2520logics%250Acircumvent%2520the%2520notorious%2520difficulties%2520associated%2520with%2520determinizing%2520infinite%250Atrace%2520automata%252C%2520typical%2520of%2520LTL%2520reactive%2520synthesis.%2520We%2520present%2520DFA-based%250Asynthesis%2520techniques%2520for%2520LTLf%252B/PPLTL%252B%252C%2520and%2520show%2520that%2520synthesis%2520is%250A2EXPTIME-complete%2520for%2520LTLf%252B%2520%2528matching%2520LTLf%2529%2520and%2520EXPTIME-complete%2520for%2520PPLTL%252B%250A%2528matching%2520PPLTL%2529.%2520Notably%252C%2520while%2520PPLTL%252B%2520retains%2520the%2520full%2520expressive%2520power%2520of%250ALTL%252C%2520reactive%2520synthesis%2520is%2520EXPTIME-complete%2520instead%2520of%25202EXPTIME-complete.%2520The%250Atechniques%2520are%2520also%2520adapted%2520to%2520optimally%2520solve%2520satisfiability%252C%2520validity%252C%2520and%250Amodel-checking%252C%2520to%2520get%2520EXPSPACE-complete%2520for%2520LTLf%252B%2520%2528extending%2520a%2520recent%2520result%250Afor%2520the%2520guarantee%2520level%2520using%2520LTLf%2529%252C%2520and%2520PSPACE-complete%2520for%2520PPLTL%252B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTLf%2B%20and%20PPLTL%2B%3A%20Extending%20LTLf%20and%20PPLTL%20to%20Infinite%20Traces&entry.906535625=Benjamin%20Aminof%20and%20Giuseppe%20De%20Giacomo%20and%20Sasha%20Rubin%20and%20Moshe%20Y.%20Vardi&entry.1292438233=%20%20We%20introduce%20LTLf%2B%20and%20PPLTL%2B%2C%20two%20logics%20to%20express%20properties%20of%20infinite%0Atraces%2C%20that%20are%20based%20on%20the%20linear-time%20temporal%20logics%20LTLf%20and%20PPLTL%20on%0Afinite%20traces.%20LTLf%2B/PPLTL%2B%20use%20levels%20of%20Manna%20and%20Pnueli%27s%20LTL%0Asafety-progress%20hierarchy%2C%20and%20thus%20have%20the%20same%20expressive%20power%20as%20LTL.%0AHowever%2C%20they%20also%20retain%20a%20crucial%20characteristic%20of%20the%20reactive%20synthesis%0Aproblem%20for%20the%20base%20logics%3A%20the%20game%20arena%20for%20strategy%20extraction%20can%20be%0Aderived%20from%20deterministic%20finite%20automata%20%28DFA%29.%20Consequently%2C%20these%20logics%0Acircumvent%20the%20notorious%20difficulties%20associated%20with%20determinizing%20infinite%0Atrace%20automata%2C%20typical%20of%20LTL%20reactive%20synthesis.%20We%20present%20DFA-based%0Asynthesis%20techniques%20for%20LTLf%2B/PPLTL%2B%2C%20and%20show%20that%20synthesis%20is%0A2EXPTIME-complete%20for%20LTLf%2B%20%28matching%20LTLf%29%20and%20EXPTIME-complete%20for%20PPLTL%2B%0A%28matching%20PPLTL%29.%20Notably%2C%20while%20PPLTL%2B%20retains%20the%20full%20expressive%20power%20of%0ALTL%2C%20reactive%20synthesis%20is%20EXPTIME-complete%20instead%20of%202EXPTIME-complete.%20The%0Atechniques%20are%20also%20adapted%20to%20optimally%20solve%20satisfiability%2C%20validity%2C%20and%0Amodel-checking%2C%20to%20get%20EXPSPACE-complete%20for%20LTLf%2B%20%28extending%20a%20recent%20result%0Afor%20the%20guarantee%20level%20using%20LTLf%29%2C%20and%20PSPACE-complete%20for%20PPLTL%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09366v1&entry.124074799=Read"},
{"title": "Communication Compression for Tensor Parallel LLM Inference", "author": "Jan Hansen-Palmus and Michael Truong-Le and Oliver Hausd\u00f6rfer and Alok Verma", "abstract": "  Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.\n", "link": "http://arxiv.org/abs/2411.09510v1", "date": "2024-11-14", "relevancy": 1.5115, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5382}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4968}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication%20Compression%20for%20Tensor%20Parallel%20LLM%20Inference&body=Title%3A%20Communication%20Compression%20for%20Tensor%20Parallel%20LLM%20Inference%0AAuthor%3A%20Jan%20Hansen-Palmus%20and%20Michael%20Truong-Le%20and%20Oliver%20Hausd%C3%B6rfer%20and%20Alok%20Verma%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20pushed%20the%20frontier%20of%20artificial%0Aintelligence%20but%20are%20comprised%20of%20hundreds%20of%20billions%20of%20parameters%20and%0Aoperations.%20For%20faster%20inference%20latency%2C%20LLMs%20are%20deployed%20on%20multiple%0Ahardware%20accelerators%20through%20various%20Model%20Parallelism%20strategies.%20Our%20paper%0Alooks%20into%20the%20details%20on%20one%20such%20strategy%20-%20Tensor%20Parallel%20-%20and%20proposes%20to%0Areduce%20latency%20by%20compressing%20inter-accelerator%20communication.%20We%20leverage%20fine%0Agrained%20quantization%20techniques%20to%20compress%20selected%20activations%20by%203.5%20-%204.5x.%0AOur%20proposed%20method%20leads%20up%20to%202x%20reduction%20of%20time-to-first-token%20%28TTFT%29%20with%0Anegligible%20model%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication%2520Compression%2520for%2520Tensor%2520Parallel%2520LLM%2520Inference%26entry.906535625%3DJan%2520Hansen-Palmus%2520and%2520Michael%2520Truong-Le%2520and%2520Oliver%2520Hausd%25C3%25B6rfer%2520and%2520Alok%2520Verma%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520pushed%2520the%2520frontier%2520of%2520artificial%250Aintelligence%2520but%2520are%2520comprised%2520of%2520hundreds%2520of%2520billions%2520of%2520parameters%2520and%250Aoperations.%2520For%2520faster%2520inference%2520latency%252C%2520LLMs%2520are%2520deployed%2520on%2520multiple%250Ahardware%2520accelerators%2520through%2520various%2520Model%2520Parallelism%2520strategies.%2520Our%2520paper%250Alooks%2520into%2520the%2520details%2520on%2520one%2520such%2520strategy%2520-%2520Tensor%2520Parallel%2520-%2520and%2520proposes%2520to%250Areduce%2520latency%2520by%2520compressing%2520inter-accelerator%2520communication.%2520We%2520leverage%2520fine%250Agrained%2520quantization%2520techniques%2520to%2520compress%2520selected%2520activations%2520by%25203.5%2520-%25204.5x.%250AOur%2520proposed%2520method%2520leads%2520up%2520to%25202x%2520reduction%2520of%2520time-to-first-token%2520%2528TTFT%2529%2520with%250Anegligible%2520model%2520performance%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication%20Compression%20for%20Tensor%20Parallel%20LLM%20Inference&entry.906535625=Jan%20Hansen-Palmus%20and%20Michael%20Truong-Le%20and%20Oliver%20Hausd%C3%B6rfer%20and%20Alok%20Verma&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20pushed%20the%20frontier%20of%20artificial%0Aintelligence%20but%20are%20comprised%20of%20hundreds%20of%20billions%20of%20parameters%20and%0Aoperations.%20For%20faster%20inference%20latency%2C%20LLMs%20are%20deployed%20on%20multiple%0Ahardware%20accelerators%20through%20various%20Model%20Parallelism%20strategies.%20Our%20paper%0Alooks%20into%20the%20details%20on%20one%20such%20strategy%20-%20Tensor%20Parallel%20-%20and%20proposes%20to%0Areduce%20latency%20by%20compressing%20inter-accelerator%20communication.%20We%20leverage%20fine%0Agrained%20quantization%20techniques%20to%20compress%20selected%20activations%20by%203.5%20-%204.5x.%0AOur%20proposed%20method%20leads%20up%20to%202x%20reduction%20of%20time-to-first-token%20%28TTFT%29%20with%0Anegligible%20model%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09510v1&entry.124074799=Read"},
{"title": "Quantum Machine Learning: An Interplay Between Quantum Computing and\n  Machine Learning", "author": "Jun Qi and Chao-Han Yang and Samuel Yen-Chi Chen and Pin-Yu Chen", "abstract": "  Quantum machine learning (QML) is a rapidly growing field that combines\nquantum computing principles with traditional machine learning. It seeks to\nrevolutionize machine learning by harnessing the unique capabilities of quantum\nmechanics and employs machine learning techniques to advance quantum computing\nresearch. This paper introduces quantum computing for the machine learning\nparadigm, where variational quantum circuits (VQC) are used to develop QML\narchitectures on noisy intermediate-scale quantum (NISQ) devices. We discuss\nmachine learning for the quantum computing paradigm, showcasing our recent\ntheoretical and empirical findings. In particular, we delve into future\ndirections for studying QML, exploring the potential industrial impacts of QML\nresearch.\n", "link": "http://arxiv.org/abs/2411.09403v1", "date": "2024-11-14", "relevancy": 1.6449, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Machine%20Learning%3A%20An%20Interplay%20Between%20Quantum%20Computing%20and%0A%20%20Machine%20Learning&body=Title%3A%20Quantum%20Machine%20Learning%3A%20An%20Interplay%20Between%20Quantum%20Computing%20and%0A%20%20Machine%20Learning%0AAuthor%3A%20Jun%20Qi%20and%20Chao-Han%20Yang%20and%20Samuel%20Yen-Chi%20Chen%20and%20Pin-Yu%20Chen%0AAbstract%3A%20%20%20Quantum%20machine%20learning%20%28QML%29%20is%20a%20rapidly%20growing%20field%20that%20combines%0Aquantum%20computing%20principles%20with%20traditional%20machine%20learning.%20It%20seeks%20to%0Arevolutionize%20machine%20learning%20by%20harnessing%20the%20unique%20capabilities%20of%20quantum%0Amechanics%20and%20employs%20machine%20learning%20techniques%20to%20advance%20quantum%20computing%0Aresearch.%20This%20paper%20introduces%20quantum%20computing%20for%20the%20machine%20learning%0Aparadigm%2C%20where%20variational%20quantum%20circuits%20%28VQC%29%20are%20used%20to%20develop%20QML%0Aarchitectures%20on%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20devices.%20We%20discuss%0Amachine%20learning%20for%20the%20quantum%20computing%20paradigm%2C%20showcasing%20our%20recent%0Atheoretical%20and%20empirical%20findings.%20In%20particular%2C%20we%20delve%20into%20future%0Adirections%20for%20studying%20QML%2C%20exploring%20the%20potential%20industrial%20impacts%20of%20QML%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Machine%2520Learning%253A%2520An%2520Interplay%2520Between%2520Quantum%2520Computing%2520and%250A%2520%2520Machine%2520Learning%26entry.906535625%3DJun%2520Qi%2520and%2520Chao-Han%2520Yang%2520and%2520Samuel%2520Yen-Chi%2520Chen%2520and%2520Pin-Yu%2520Chen%26entry.1292438233%3D%2520%2520Quantum%2520machine%2520learning%2520%2528QML%2529%2520is%2520a%2520rapidly%2520growing%2520field%2520that%2520combines%250Aquantum%2520computing%2520principles%2520with%2520traditional%2520machine%2520learning.%2520It%2520seeks%2520to%250Arevolutionize%2520machine%2520learning%2520by%2520harnessing%2520the%2520unique%2520capabilities%2520of%2520quantum%250Amechanics%2520and%2520employs%2520machine%2520learning%2520techniques%2520to%2520advance%2520quantum%2520computing%250Aresearch.%2520This%2520paper%2520introduces%2520quantum%2520computing%2520for%2520the%2520machine%2520learning%250Aparadigm%252C%2520where%2520variational%2520quantum%2520circuits%2520%2528VQC%2529%2520are%2520used%2520to%2520develop%2520QML%250Aarchitectures%2520on%2520noisy%2520intermediate-scale%2520quantum%2520%2528NISQ%2529%2520devices.%2520We%2520discuss%250Amachine%2520learning%2520for%2520the%2520quantum%2520computing%2520paradigm%252C%2520showcasing%2520our%2520recent%250Atheoretical%2520and%2520empirical%2520findings.%2520In%2520particular%252C%2520we%2520delve%2520into%2520future%250Adirections%2520for%2520studying%2520QML%252C%2520exploring%2520the%2520potential%2520industrial%2520impacts%2520of%2520QML%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Machine%20Learning%3A%20An%20Interplay%20Between%20Quantum%20Computing%20and%0A%20%20Machine%20Learning&entry.906535625=Jun%20Qi%20and%20Chao-Han%20Yang%20and%20Samuel%20Yen-Chi%20Chen%20and%20Pin-Yu%20Chen&entry.1292438233=%20%20Quantum%20machine%20learning%20%28QML%29%20is%20a%20rapidly%20growing%20field%20that%20combines%0Aquantum%20computing%20principles%20with%20traditional%20machine%20learning.%20It%20seeks%20to%0Arevolutionize%20machine%20learning%20by%20harnessing%20the%20unique%20capabilities%20of%20quantum%0Amechanics%20and%20employs%20machine%20learning%20techniques%20to%20advance%20quantum%20computing%0Aresearch.%20This%20paper%20introduces%20quantum%20computing%20for%20the%20machine%20learning%0Aparadigm%2C%20where%20variational%20quantum%20circuits%20%28VQC%29%20are%20used%20to%20develop%20QML%0Aarchitectures%20on%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20devices.%20We%20discuss%0Amachine%20learning%20for%20the%20quantum%20computing%20paradigm%2C%20showcasing%20our%20recent%0Atheoretical%20and%20empirical%20findings.%20In%20particular%2C%20we%20delve%20into%20future%0Adirections%20for%20studying%20QML%2C%20exploring%20the%20potential%20industrial%20impacts%20of%20QML%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09403v1&entry.124074799=Read"},
{"title": "Counterfactual Uncertainty Quantification of Factual Estimand of\n  Efficacy from Before-and-After Treatment Repeated Measures Randomized\n  Controlled Trials", "author": "Xingya Wang and Yang Han and Yushi Liu and Szu-Yu Tang and Jason C. Hsu", "abstract": "  The ideal estimand for comparing a new treatment $Rx$ with a control $C$ is\nthe $\\textit{counterfactual}$ efficacy $Rx:C$, the expected differential\noutcome between $Rx$ and $C$ if each patient were given $\\textit{both}$. While\ncounterfactual $\\textit{point estimation}$ from $\\textit{factual}$ Randomized\nControlled Trials (RCTs) has been available, this article shows\n$\\textit{counterfactual}$ uncertainty quantification (CUQ), quantifying\nuncertainty for factual point estimates but in a counterfactual setting, is\nsurprisingly achievable. We achieve CUQ whose variability is typically smaller\nthan factual UQ, by creating a new statistical modeling principle called ETZ\nwhich is applicable to RCTs with $\\textit{Before-and-After}$ treatment Repeated\nMeasures, common in many therapeutic areas.\n  We urge caution when estimate of the unobservable true condition of a patient\nbefore treatment has measurement error, because that violation of standard\nregression assumption can cause attenuation in estimating treatment effects.\nFortunately, we prove that, for traditional medicine in general, and for\ntargeted therapy with efficacy defined as averaged over the population,\ncounterfactual point estimation is unbiased. However, for targeted therapy,\nboth Real Human and Digital Twins approaches should respect this limitation,\nlest predicted treatment effect in $\\textit{subgroups}$ will have bias.\n", "link": "http://arxiv.org/abs/2411.09635v1", "date": "2024-11-14", "relevancy": 1.1409, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3665}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Uncertainty%20Quantification%20of%20Factual%20Estimand%20of%0A%20%20Efficacy%20from%20Before-and-After%20Treatment%20Repeated%20Measures%20Randomized%0A%20%20Controlled%20Trials&body=Title%3A%20Counterfactual%20Uncertainty%20Quantification%20of%20Factual%20Estimand%20of%0A%20%20Efficacy%20from%20Before-and-After%20Treatment%20Repeated%20Measures%20Randomized%0A%20%20Controlled%20Trials%0AAuthor%3A%20Xingya%20Wang%20and%20Yang%20Han%20and%20Yushi%20Liu%20and%20Szu-Yu%20Tang%20and%20Jason%20C.%20Hsu%0AAbstract%3A%20%20%20The%20ideal%20estimand%20for%20comparing%20a%20new%20treatment%20%24Rx%24%20with%20a%20control%20%24C%24%20is%0Athe%20%24%5Ctextit%7Bcounterfactual%7D%24%20efficacy%20%24Rx%3AC%24%2C%20the%20expected%20differential%0Aoutcome%20between%20%24Rx%24%20and%20%24C%24%20if%20each%20patient%20were%20given%20%24%5Ctextit%7Bboth%7D%24.%20While%0Acounterfactual%20%24%5Ctextit%7Bpoint%20estimation%7D%24%20from%20%24%5Ctextit%7Bfactual%7D%24%20Randomized%0AControlled%20Trials%20%28RCTs%29%20has%20been%20available%2C%20this%20article%20shows%0A%24%5Ctextit%7Bcounterfactual%7D%24%20uncertainty%20quantification%20%28CUQ%29%2C%20quantifying%0Auncertainty%20for%20factual%20point%20estimates%20but%20in%20a%20counterfactual%20setting%2C%20is%0Asurprisingly%20achievable.%20We%20achieve%20CUQ%20whose%20variability%20is%20typically%20smaller%0Athan%20factual%20UQ%2C%20by%20creating%20a%20new%20statistical%20modeling%20principle%20called%20ETZ%0Awhich%20is%20applicable%20to%20RCTs%20with%20%24%5Ctextit%7BBefore-and-After%7D%24%20treatment%20Repeated%0AMeasures%2C%20common%20in%20many%20therapeutic%20areas.%0A%20%20We%20urge%20caution%20when%20estimate%20of%20the%20unobservable%20true%20condition%20of%20a%20patient%0Abefore%20treatment%20has%20measurement%20error%2C%20because%20that%20violation%20of%20standard%0Aregression%20assumption%20can%20cause%20attenuation%20in%20estimating%20treatment%20effects.%0AFortunately%2C%20we%20prove%20that%2C%20for%20traditional%20medicine%20in%20general%2C%20and%20for%0Atargeted%20therapy%20with%20efficacy%20defined%20as%20averaged%20over%20the%20population%2C%0Acounterfactual%20point%20estimation%20is%20unbiased.%20However%2C%20for%20targeted%20therapy%2C%0Aboth%20Real%20Human%20and%20Digital%20Twins%20approaches%20should%20respect%20this%20limitation%2C%0Alest%20predicted%20treatment%20effect%20in%20%24%5Ctextit%7Bsubgroups%7D%24%20will%20have%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Uncertainty%2520Quantification%2520of%2520Factual%2520Estimand%2520of%250A%2520%2520Efficacy%2520from%2520Before-and-After%2520Treatment%2520Repeated%2520Measures%2520Randomized%250A%2520%2520Controlled%2520Trials%26entry.906535625%3DXingya%2520Wang%2520and%2520Yang%2520Han%2520and%2520Yushi%2520Liu%2520and%2520Szu-Yu%2520Tang%2520and%2520Jason%2520C.%2520Hsu%26entry.1292438233%3D%2520%2520The%2520ideal%2520estimand%2520for%2520comparing%2520a%2520new%2520treatment%2520%2524Rx%2524%2520with%2520a%2520control%2520%2524C%2524%2520is%250Athe%2520%2524%255Ctextit%257Bcounterfactual%257D%2524%2520efficacy%2520%2524Rx%253AC%2524%252C%2520the%2520expected%2520differential%250Aoutcome%2520between%2520%2524Rx%2524%2520and%2520%2524C%2524%2520if%2520each%2520patient%2520were%2520given%2520%2524%255Ctextit%257Bboth%257D%2524.%2520While%250Acounterfactual%2520%2524%255Ctextit%257Bpoint%2520estimation%257D%2524%2520from%2520%2524%255Ctextit%257Bfactual%257D%2524%2520Randomized%250AControlled%2520Trials%2520%2528RCTs%2529%2520has%2520been%2520available%252C%2520this%2520article%2520shows%250A%2524%255Ctextit%257Bcounterfactual%257D%2524%2520uncertainty%2520quantification%2520%2528CUQ%2529%252C%2520quantifying%250Auncertainty%2520for%2520factual%2520point%2520estimates%2520but%2520in%2520a%2520counterfactual%2520setting%252C%2520is%250Asurprisingly%2520achievable.%2520We%2520achieve%2520CUQ%2520whose%2520variability%2520is%2520typically%2520smaller%250Athan%2520factual%2520UQ%252C%2520by%2520creating%2520a%2520new%2520statistical%2520modeling%2520principle%2520called%2520ETZ%250Awhich%2520is%2520applicable%2520to%2520RCTs%2520with%2520%2524%255Ctextit%257BBefore-and-After%257D%2524%2520treatment%2520Repeated%250AMeasures%252C%2520common%2520in%2520many%2520therapeutic%2520areas.%250A%2520%2520We%2520urge%2520caution%2520when%2520estimate%2520of%2520the%2520unobservable%2520true%2520condition%2520of%2520a%2520patient%250Abefore%2520treatment%2520has%2520measurement%2520error%252C%2520because%2520that%2520violation%2520of%2520standard%250Aregression%2520assumption%2520can%2520cause%2520attenuation%2520in%2520estimating%2520treatment%2520effects.%250AFortunately%252C%2520we%2520prove%2520that%252C%2520for%2520traditional%2520medicine%2520in%2520general%252C%2520and%2520for%250Atargeted%2520therapy%2520with%2520efficacy%2520defined%2520as%2520averaged%2520over%2520the%2520population%252C%250Acounterfactual%2520point%2520estimation%2520is%2520unbiased.%2520However%252C%2520for%2520targeted%2520therapy%252C%250Aboth%2520Real%2520Human%2520and%2520Digital%2520Twins%2520approaches%2520should%2520respect%2520this%2520limitation%252C%250Alest%2520predicted%2520treatment%2520effect%2520in%2520%2524%255Ctextit%257Bsubgroups%257D%2524%2520will%2520have%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Uncertainty%20Quantification%20of%20Factual%20Estimand%20of%0A%20%20Efficacy%20from%20Before-and-After%20Treatment%20Repeated%20Measures%20Randomized%0A%20%20Controlled%20Trials&entry.906535625=Xingya%20Wang%20and%20Yang%20Han%20and%20Yushi%20Liu%20and%20Szu-Yu%20Tang%20and%20Jason%20C.%20Hsu&entry.1292438233=%20%20The%20ideal%20estimand%20for%20comparing%20a%20new%20treatment%20%24Rx%24%20with%20a%20control%20%24C%24%20is%0Athe%20%24%5Ctextit%7Bcounterfactual%7D%24%20efficacy%20%24Rx%3AC%24%2C%20the%20expected%20differential%0Aoutcome%20between%20%24Rx%24%20and%20%24C%24%20if%20each%20patient%20were%20given%20%24%5Ctextit%7Bboth%7D%24.%20While%0Acounterfactual%20%24%5Ctextit%7Bpoint%20estimation%7D%24%20from%20%24%5Ctextit%7Bfactual%7D%24%20Randomized%0AControlled%20Trials%20%28RCTs%29%20has%20been%20available%2C%20this%20article%20shows%0A%24%5Ctextit%7Bcounterfactual%7D%24%20uncertainty%20quantification%20%28CUQ%29%2C%20quantifying%0Auncertainty%20for%20factual%20point%20estimates%20but%20in%20a%20counterfactual%20setting%2C%20is%0Asurprisingly%20achievable.%20We%20achieve%20CUQ%20whose%20variability%20is%20typically%20smaller%0Athan%20factual%20UQ%2C%20by%20creating%20a%20new%20statistical%20modeling%20principle%20called%20ETZ%0Awhich%20is%20applicable%20to%20RCTs%20with%20%24%5Ctextit%7BBefore-and-After%7D%24%20treatment%20Repeated%0AMeasures%2C%20common%20in%20many%20therapeutic%20areas.%0A%20%20We%20urge%20caution%20when%20estimate%20of%20the%20unobservable%20true%20condition%20of%20a%20patient%0Abefore%20treatment%20has%20measurement%20error%2C%20because%20that%20violation%20of%20standard%0Aregression%20assumption%20can%20cause%20attenuation%20in%20estimating%20treatment%20effects.%0AFortunately%2C%20we%20prove%20that%2C%20for%20traditional%20medicine%20in%20general%2C%20and%20for%0Atargeted%20therapy%20with%20efficacy%20defined%20as%20averaged%20over%20the%20population%2C%0Acounterfactual%20point%20estimation%20is%20unbiased.%20However%2C%20for%20targeted%20therapy%2C%0Aboth%20Real%20Human%20and%20Digital%20Twins%20approaches%20should%20respect%20this%20limitation%2C%0Alest%20predicted%20treatment%20effect%20in%20%24%5Ctextit%7Bsubgroups%7D%24%20will%20have%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09635v1&entry.124074799=Read"},
{"title": "DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination", "author": "Jia Fu and Xiao Zhang and Sepideh Pashami and Fatemeh Rahimian and Anders Holst", "abstract": "  In the ever-evolving adversarial machine learning landscape, developing\neffective defenses against patch attacks has become a critical challenge,\nnecessitating reliable solutions to safeguard real-world AI systems. Although\ndiffusion models have shown remarkable capacity in image synthesis and have\nbeen recently utilized to counter $\\ell_p$-norm bounded attacks, their\npotential in mitigating localized patch attacks remains largely underexplored.\nIn this work, we propose DiffPAD, a novel framework that harnesses the power of\ndiffusion models for adversarial patch decontamination. DiffPAD first performs\nsuper-resolution restoration on downsampled input images, then adopts\nbinarization, dynamic thresholding scheme and sliding window for effective\nlocalization of adversarial patches. Such a design is inspired by the\ntheoretically derived correlation between patch size and diffusion restoration\nerror that is generalized across diverse patch attack scenarios. Finally,\nDiffPAD applies inpainting techniques to the original input images with the\nestimated patch region being masked. By integrating closed-form solutions for\nsuper-resolution restoration and image inpainting into the conditional reverse\nsampling process of a pre-trained diffusion model, DiffPAD obviates the need\nfor text guidance or fine-tuning. Through comprehensive experiments, we\ndemonstrate that DiffPAD not only achieves state-of-the-art adversarial\nrobustness against patch attacks but also excels in recovering naturalistic\nimages without patch remnants. The source code is available at\nhttps://github.com/JasonFu1998/DiffPAD.\n", "link": "http://arxiv.org/abs/2410.24006v2", "date": "2024-11-14", "relevancy": 1.7333, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5986}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5735}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPAD%3A%20Denoising%20Diffusion-based%20Adversarial%20Patch%20Decontamination&body=Title%3A%20DiffPAD%3A%20Denoising%20Diffusion-based%20Adversarial%20Patch%20Decontamination%0AAuthor%3A%20Jia%20Fu%20and%20Xiao%20Zhang%20and%20Sepideh%20Pashami%20and%20Fatemeh%20Rahimian%20and%20Anders%20Holst%0AAbstract%3A%20%20%20In%20the%20ever-evolving%20adversarial%20machine%20learning%20landscape%2C%20developing%0Aeffective%20defenses%20against%20patch%20attacks%20has%20become%20a%20critical%20challenge%2C%0Anecessitating%20reliable%20solutions%20to%20safeguard%20real-world%20AI%20systems.%20Although%0Adiffusion%20models%20have%20shown%20remarkable%20capacity%20in%20image%20synthesis%20and%20have%0Abeen%20recently%20utilized%20to%20counter%20%24%5Cell_p%24-norm%20bounded%20attacks%2C%20their%0Apotential%20in%20mitigating%20localized%20patch%20attacks%20remains%20largely%20underexplored.%0AIn%20this%20work%2C%20we%20propose%20DiffPAD%2C%20a%20novel%20framework%20that%20harnesses%20the%20power%20of%0Adiffusion%20models%20for%20adversarial%20patch%20decontamination.%20DiffPAD%20first%20performs%0Asuper-resolution%20restoration%20on%20downsampled%20input%20images%2C%20then%20adopts%0Abinarization%2C%20dynamic%20thresholding%20scheme%20and%20sliding%20window%20for%20effective%0Alocalization%20of%20adversarial%20patches.%20Such%20a%20design%20is%20inspired%20by%20the%0Atheoretically%20derived%20correlation%20between%20patch%20size%20and%20diffusion%20restoration%0Aerror%20that%20is%20generalized%20across%20diverse%20patch%20attack%20scenarios.%20Finally%2C%0ADiffPAD%20applies%20inpainting%20techniques%20to%20the%20original%20input%20images%20with%20the%0Aestimated%20patch%20region%20being%20masked.%20By%20integrating%20closed-form%20solutions%20for%0Asuper-resolution%20restoration%20and%20image%20inpainting%20into%20the%20conditional%20reverse%0Asampling%20process%20of%20a%20pre-trained%20diffusion%20model%2C%20DiffPAD%20obviates%20the%20need%0Afor%20text%20guidance%20or%20fine-tuning.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20DiffPAD%20not%20only%20achieves%20state-of-the-art%20adversarial%0Arobustness%20against%20patch%20attacks%20but%20also%20excels%20in%20recovering%20naturalistic%0Aimages%20without%20patch%20remnants.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JasonFu1998/DiffPAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPAD%253A%2520Denoising%2520Diffusion-based%2520Adversarial%2520Patch%2520Decontamination%26entry.906535625%3DJia%2520Fu%2520and%2520Xiao%2520Zhang%2520and%2520Sepideh%2520Pashami%2520and%2520Fatemeh%2520Rahimian%2520and%2520Anders%2520Holst%26entry.1292438233%3D%2520%2520In%2520the%2520ever-evolving%2520adversarial%2520machine%2520learning%2520landscape%252C%2520developing%250Aeffective%2520defenses%2520against%2520patch%2520attacks%2520has%2520become%2520a%2520critical%2520challenge%252C%250Anecessitating%2520reliable%2520solutions%2520to%2520safeguard%2520real-world%2520AI%2520systems.%2520Although%250Adiffusion%2520models%2520have%2520shown%2520remarkable%2520capacity%2520in%2520image%2520synthesis%2520and%2520have%250Abeen%2520recently%2520utilized%2520to%2520counter%2520%2524%255Cell_p%2524-norm%2520bounded%2520attacks%252C%2520their%250Apotential%2520in%2520mitigating%2520localized%2520patch%2520attacks%2520remains%2520largely%2520underexplored.%250AIn%2520this%2520work%252C%2520we%2520propose%2520DiffPAD%252C%2520a%2520novel%2520framework%2520that%2520harnesses%2520the%2520power%2520of%250Adiffusion%2520models%2520for%2520adversarial%2520patch%2520decontamination.%2520DiffPAD%2520first%2520performs%250Asuper-resolution%2520restoration%2520on%2520downsampled%2520input%2520images%252C%2520then%2520adopts%250Abinarization%252C%2520dynamic%2520thresholding%2520scheme%2520and%2520sliding%2520window%2520for%2520effective%250Alocalization%2520of%2520adversarial%2520patches.%2520Such%2520a%2520design%2520is%2520inspired%2520by%2520the%250Atheoretically%2520derived%2520correlation%2520between%2520patch%2520size%2520and%2520diffusion%2520restoration%250Aerror%2520that%2520is%2520generalized%2520across%2520diverse%2520patch%2520attack%2520scenarios.%2520Finally%252C%250ADiffPAD%2520applies%2520inpainting%2520techniques%2520to%2520the%2520original%2520input%2520images%2520with%2520the%250Aestimated%2520patch%2520region%2520being%2520masked.%2520By%2520integrating%2520closed-form%2520solutions%2520for%250Asuper-resolution%2520restoration%2520and%2520image%2520inpainting%2520into%2520the%2520conditional%2520reverse%250Asampling%2520process%2520of%2520a%2520pre-trained%2520diffusion%2520model%252C%2520DiffPAD%2520obviates%2520the%2520need%250Afor%2520text%2520guidance%2520or%2520fine-tuning.%2520Through%2520comprehensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520DiffPAD%2520not%2520only%2520achieves%2520state-of-the-art%2520adversarial%250Arobustness%2520against%2520patch%2520attacks%2520but%2520also%2520excels%2520in%2520recovering%2520naturalistic%250Aimages%2520without%2520patch%2520remnants.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JasonFu1998/DiffPAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPAD%3A%20Denoising%20Diffusion-based%20Adversarial%20Patch%20Decontamination&entry.906535625=Jia%20Fu%20and%20Xiao%20Zhang%20and%20Sepideh%20Pashami%20and%20Fatemeh%20Rahimian%20and%20Anders%20Holst&entry.1292438233=%20%20In%20the%20ever-evolving%20adversarial%20machine%20learning%20landscape%2C%20developing%0Aeffective%20defenses%20against%20patch%20attacks%20has%20become%20a%20critical%20challenge%2C%0Anecessitating%20reliable%20solutions%20to%20safeguard%20real-world%20AI%20systems.%20Although%0Adiffusion%20models%20have%20shown%20remarkable%20capacity%20in%20image%20synthesis%20and%20have%0Abeen%20recently%20utilized%20to%20counter%20%24%5Cell_p%24-norm%20bounded%20attacks%2C%20their%0Apotential%20in%20mitigating%20localized%20patch%20attacks%20remains%20largely%20underexplored.%0AIn%20this%20work%2C%20we%20propose%20DiffPAD%2C%20a%20novel%20framework%20that%20harnesses%20the%20power%20of%0Adiffusion%20models%20for%20adversarial%20patch%20decontamination.%20DiffPAD%20first%20performs%0Asuper-resolution%20restoration%20on%20downsampled%20input%20images%2C%20then%20adopts%0Abinarization%2C%20dynamic%20thresholding%20scheme%20and%20sliding%20window%20for%20effective%0Alocalization%20of%20adversarial%20patches.%20Such%20a%20design%20is%20inspired%20by%20the%0Atheoretically%20derived%20correlation%20between%20patch%20size%20and%20diffusion%20restoration%0Aerror%20that%20is%20generalized%20across%20diverse%20patch%20attack%20scenarios.%20Finally%2C%0ADiffPAD%20applies%20inpainting%20techniques%20to%20the%20original%20input%20images%20with%20the%0Aestimated%20patch%20region%20being%20masked.%20By%20integrating%20closed-form%20solutions%20for%0Asuper-resolution%20restoration%20and%20image%20inpainting%20into%20the%20conditional%20reverse%0Asampling%20process%20of%20a%20pre-trained%20diffusion%20model%2C%20DiffPAD%20obviates%20the%20need%0Afor%20text%20guidance%20or%20fine-tuning.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20DiffPAD%20not%20only%20achieves%20state-of-the-art%20adversarial%0Arobustness%20against%20patch%20attacks%20but%20also%20excels%20in%20recovering%20naturalistic%0Aimages%20without%20patch%20remnants.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JasonFu1998/DiffPAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24006v2&entry.124074799=Read"},
{"title": "Adaptive Deviation Learning for Visual Anomaly Detection with Data\n  Contamination", "author": "Anindya Sundar Das and Guansong Pang and Monowar Bhuyan", "abstract": "  Visual anomaly detection targets to detect images that notably differ from\nnormal pattern, and it has found extensive application in identifying defective\nparts within the manufacturing industry. These anomaly detection paradigms\npredominantly focus on training detection models using only clean, unlabeled\nnormal samples, assuming an absence of contamination; a condition often unmet\nin real-world scenarios. The performance of these methods significantly depends\non the quality of the data and usually decreases when exposed to noise. We\nintroduce a systematic adaptive method that employs deviation learning to\ncompute anomaly scores end-to-end while addressing data contamination by\nassigning relative importance to the weights of individual instances. In this\napproach, the anomaly scores for normal instances are designed to approximate\nscalar scores obtained from the known prior distribution. Meanwhile, anomaly\nscores for anomaly examples are adjusted to exhibit statistically significant\ndeviations from these reference scores. Our approach incorporates a constrained\noptimization problem within the deviation learning framework to update instance\nweights, resolving this problem for each mini-batch. Comprehensive experiments\non the MVTec and VisA benchmark datasets indicate that our proposed method\nsurpasses competing techniques and exhibits both stability and robustness in\nthe presence of data contamination.\n", "link": "http://arxiv.org/abs/2411.09558v1", "date": "2024-11-14", "relevancy": 1.57, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.534}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.526}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Deviation%20Learning%20for%20Visual%20Anomaly%20Detection%20with%20Data%0A%20%20Contamination&body=Title%3A%20Adaptive%20Deviation%20Learning%20for%20Visual%20Anomaly%20Detection%20with%20Data%0A%20%20Contamination%0AAuthor%3A%20Anindya%20Sundar%20Das%20and%20Guansong%20Pang%20and%20Monowar%20Bhuyan%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20targets%20to%20detect%20images%20that%20notably%20differ%20from%0Anormal%20pattern%2C%20and%20it%20has%20found%20extensive%20application%20in%20identifying%20defective%0Aparts%20within%20the%20manufacturing%20industry.%20These%20anomaly%20detection%20paradigms%0Apredominantly%20focus%20on%20training%20detection%20models%20using%20only%20clean%2C%20unlabeled%0Anormal%20samples%2C%20assuming%20an%20absence%20of%20contamination%3B%20a%20condition%20often%20unmet%0Ain%20real-world%20scenarios.%20The%20performance%20of%20these%20methods%20significantly%20depends%0Aon%20the%20quality%20of%20the%20data%20and%20usually%20decreases%20when%20exposed%20to%20noise.%20We%0Aintroduce%20a%20systematic%20adaptive%20method%20that%20employs%20deviation%20learning%20to%0Acompute%20anomaly%20scores%20end-to-end%20while%20addressing%20data%20contamination%20by%0Aassigning%20relative%20importance%20to%20the%20weights%20of%20individual%20instances.%20In%20this%0Aapproach%2C%20the%20anomaly%20scores%20for%20normal%20instances%20are%20designed%20to%20approximate%0Ascalar%20scores%20obtained%20from%20the%20known%20prior%20distribution.%20Meanwhile%2C%20anomaly%0Ascores%20for%20anomaly%20examples%20are%20adjusted%20to%20exhibit%20statistically%20significant%0Adeviations%20from%20these%20reference%20scores.%20Our%20approach%20incorporates%20a%20constrained%0Aoptimization%20problem%20within%20the%20deviation%20learning%20framework%20to%20update%20instance%0Aweights%2C%20resolving%20this%20problem%20for%20each%20mini-batch.%20Comprehensive%20experiments%0Aon%20the%20MVTec%20and%20VisA%20benchmark%20datasets%20indicate%20that%20our%20proposed%20method%0Asurpasses%20competing%20techniques%20and%20exhibits%20both%20stability%20and%20robustness%20in%0Athe%20presence%20of%20data%20contamination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Deviation%2520Learning%2520for%2520Visual%2520Anomaly%2520Detection%2520with%2520Data%250A%2520%2520Contamination%26entry.906535625%3DAnindya%2520Sundar%2520Das%2520and%2520Guansong%2520Pang%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520targets%2520to%2520detect%2520images%2520that%2520notably%2520differ%2520from%250Anormal%2520pattern%252C%2520and%2520it%2520has%2520found%2520extensive%2520application%2520in%2520identifying%2520defective%250Aparts%2520within%2520the%2520manufacturing%2520industry.%2520These%2520anomaly%2520detection%2520paradigms%250Apredominantly%2520focus%2520on%2520training%2520detection%2520models%2520using%2520only%2520clean%252C%2520unlabeled%250Anormal%2520samples%252C%2520assuming%2520an%2520absence%2520of%2520contamination%253B%2520a%2520condition%2520often%2520unmet%250Ain%2520real-world%2520scenarios.%2520The%2520performance%2520of%2520these%2520methods%2520significantly%2520depends%250Aon%2520the%2520quality%2520of%2520the%2520data%2520and%2520usually%2520decreases%2520when%2520exposed%2520to%2520noise.%2520We%250Aintroduce%2520a%2520systematic%2520adaptive%2520method%2520that%2520employs%2520deviation%2520learning%2520to%250Acompute%2520anomaly%2520scores%2520end-to-end%2520while%2520addressing%2520data%2520contamination%2520by%250Aassigning%2520relative%2520importance%2520to%2520the%2520weights%2520of%2520individual%2520instances.%2520In%2520this%250Aapproach%252C%2520the%2520anomaly%2520scores%2520for%2520normal%2520instances%2520are%2520designed%2520to%2520approximate%250Ascalar%2520scores%2520obtained%2520from%2520the%2520known%2520prior%2520distribution.%2520Meanwhile%252C%2520anomaly%250Ascores%2520for%2520anomaly%2520examples%2520are%2520adjusted%2520to%2520exhibit%2520statistically%2520significant%250Adeviations%2520from%2520these%2520reference%2520scores.%2520Our%2520approach%2520incorporates%2520a%2520constrained%250Aoptimization%2520problem%2520within%2520the%2520deviation%2520learning%2520framework%2520to%2520update%2520instance%250Aweights%252C%2520resolving%2520this%2520problem%2520for%2520each%2520mini-batch.%2520Comprehensive%2520experiments%250Aon%2520the%2520MVTec%2520and%2520VisA%2520benchmark%2520datasets%2520indicate%2520that%2520our%2520proposed%2520method%250Asurpasses%2520competing%2520techniques%2520and%2520exhibits%2520both%2520stability%2520and%2520robustness%2520in%250Athe%2520presence%2520of%2520data%2520contamination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Deviation%20Learning%20for%20Visual%20Anomaly%20Detection%20with%20Data%0A%20%20Contamination&entry.906535625=Anindya%20Sundar%20Das%20and%20Guansong%20Pang%20and%20Monowar%20Bhuyan&entry.1292438233=%20%20Visual%20anomaly%20detection%20targets%20to%20detect%20images%20that%20notably%20differ%20from%0Anormal%20pattern%2C%20and%20it%20has%20found%20extensive%20application%20in%20identifying%20defective%0Aparts%20within%20the%20manufacturing%20industry.%20These%20anomaly%20detection%20paradigms%0Apredominantly%20focus%20on%20training%20detection%20models%20using%20only%20clean%2C%20unlabeled%0Anormal%20samples%2C%20assuming%20an%20absence%20of%20contamination%3B%20a%20condition%20often%20unmet%0Ain%20real-world%20scenarios.%20The%20performance%20of%20these%20methods%20significantly%20depends%0Aon%20the%20quality%20of%20the%20data%20and%20usually%20decreases%20when%20exposed%20to%20noise.%20We%0Aintroduce%20a%20systematic%20adaptive%20method%20that%20employs%20deviation%20learning%20to%0Acompute%20anomaly%20scores%20end-to-end%20while%20addressing%20data%20contamination%20by%0Aassigning%20relative%20importance%20to%20the%20weights%20of%20individual%20instances.%20In%20this%0Aapproach%2C%20the%20anomaly%20scores%20for%20normal%20instances%20are%20designed%20to%20approximate%0Ascalar%20scores%20obtained%20from%20the%20known%20prior%20distribution.%20Meanwhile%2C%20anomaly%0Ascores%20for%20anomaly%20examples%20are%20adjusted%20to%20exhibit%20statistically%20significant%0Adeviations%20from%20these%20reference%20scores.%20Our%20approach%20incorporates%20a%20constrained%0Aoptimization%20problem%20within%20the%20deviation%20learning%20framework%20to%20update%20instance%0Aweights%2C%20resolving%20this%20problem%20for%20each%20mini-batch.%20Comprehensive%20experiments%0Aon%20the%20MVTec%20and%20VisA%20benchmark%20datasets%20indicate%20that%20our%20proposed%20method%0Asurpasses%20competing%20techniques%20and%20exhibits%20both%20stability%20and%20robustness%20in%0Athe%20presence%20of%20data%20contamination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09558v1&entry.124074799=Read"},
{"title": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations", "author": "Junwen Wang and Zhonghao Wang and Oscar MacCormac and Jonathan Shapey and Tom Vercauteren", "abstract": "  Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. %but\n\\emph{no background class} annotation. These multi-class positive annotations\nnaturally fall within the in-distribution (ID) set. Unlabelled pixels may\ncontain positive classes but also negative ones, including what is typically\nreferred to as \\emph{background} in standard segmentation formulations. Here,\nwe forgo the need for background annotation and consider these together with\nany other unseen classes as part of the OOD set. Our framework can integrate,\nat a pixel-level, any OOD detection approaches designed for classification\ntasks. To address the lack of existing OOD datasets and established evaluation\nmetric for medical image segmentation, we propose a cross-validation strategy\nthat treats held-out labelled classes as OOD. Extensive experiments on both\nmulti-class hyperspectral and RGB surgical imaging datasets demonstrate the\nrobustness and generalisation capability of our proposed framework.\n", "link": "http://arxiv.org/abs/2411.09553v1", "date": "2024-11-14", "relevancy": 1.5846, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5564}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5224}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OOD-SEG%3A%20Out-Of-Distribution%20detection%20for%20image%20SEGmentation%20with%0A%20%20sparse%20multi-class%20positive-only%20annotations&body=Title%3A%20OOD-SEG%3A%20Out-Of-Distribution%20detection%20for%20image%20SEGmentation%20with%0A%20%20sparse%20multi-class%20positive-only%20annotations%0AAuthor%3A%20Junwen%20Wang%20and%20Zhonghao%20Wang%20and%20Oscar%20MacCormac%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Despite%20significant%20advancements%2C%20segmentation%20based%20on%20deep%20neural%20networks%0Ain%20medical%20and%20surgical%20imaging%20faces%20several%20challenges%2C%20two%20of%20which%20we%20aim%0Ato%20address%20in%20this%20work.%20First%2C%20acquiring%20complete%20pixel-level%20segmentation%0Alabels%20for%20medical%20images%20is%20time-consuming%20and%20requires%20domain%20expertise.%0ASecond%2C%20typical%20segmentation%20pipelines%20cannot%20detect%20out-of-distribution%20%28OOD%29%0Apixels%2C%20leaving%20them%20prone%20to%20spurious%20outputs%20during%20deployment.%20In%20this%20work%2C%0Awe%20propose%20a%20novel%20segmentation%20approach%20exploiting%20OOD%20detection%20that%20learns%0Aonly%20from%20sparsely%20annotated%20pixels%20from%20multiple%20positive-only%20classes.%20%25but%0A%5Cemph%7Bno%20background%20class%7D%20annotation.%20These%20multi-class%20positive%20annotations%0Anaturally%20fall%20within%20the%20in-distribution%20%28ID%29%20set.%20Unlabelled%20pixels%20may%0Acontain%20positive%20classes%20but%20also%20negative%20ones%2C%20including%20what%20is%20typically%0Areferred%20to%20as%20%5Cemph%7Bbackground%7D%20in%20standard%20segmentation%20formulations.%20Here%2C%0Awe%20forgo%20the%20need%20for%20background%20annotation%20and%20consider%20these%20together%20with%0Aany%20other%20unseen%20classes%20as%20part%20of%20the%20OOD%20set.%20Our%20framework%20can%20integrate%2C%0Aat%20a%20pixel-level%2C%20any%20OOD%20detection%20approaches%20designed%20for%20classification%0Atasks.%20To%20address%20the%20lack%20of%20existing%20OOD%20datasets%20and%20established%20evaluation%0Ametric%20for%20medical%20image%20segmentation%2C%20we%20propose%20a%20cross-validation%20strategy%0Athat%20treats%20held-out%20labelled%20classes%20as%20OOD.%20Extensive%20experiments%20on%20both%0Amulti-class%20hyperspectral%20and%20RGB%20surgical%20imaging%20datasets%20demonstrate%20the%0Arobustness%20and%20generalisation%20capability%20of%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOOD-SEG%253A%2520Out-Of-Distribution%2520detection%2520for%2520image%2520SEGmentation%2520with%250A%2520%2520sparse%2520multi-class%2520positive-only%2520annotations%26entry.906535625%3DJunwen%2520Wang%2520and%2520Zhonghao%2520Wang%2520and%2520Oscar%2520MacCormac%2520and%2520Jonathan%2520Shapey%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%252C%2520segmentation%2520based%2520on%2520deep%2520neural%2520networks%250Ain%2520medical%2520and%2520surgical%2520imaging%2520faces%2520several%2520challenges%252C%2520two%2520of%2520which%2520we%2520aim%250Ato%2520address%2520in%2520this%2520work.%2520First%252C%2520acquiring%2520complete%2520pixel-level%2520segmentation%250Alabels%2520for%2520medical%2520images%2520is%2520time-consuming%2520and%2520requires%2520domain%2520expertise.%250ASecond%252C%2520typical%2520segmentation%2520pipelines%2520cannot%2520detect%2520out-of-distribution%2520%2528OOD%2529%250Apixels%252C%2520leaving%2520them%2520prone%2520to%2520spurious%2520outputs%2520during%2520deployment.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520novel%2520segmentation%2520approach%2520exploiting%2520OOD%2520detection%2520that%2520learns%250Aonly%2520from%2520sparsely%2520annotated%2520pixels%2520from%2520multiple%2520positive-only%2520classes.%2520%2525but%250A%255Cemph%257Bno%2520background%2520class%257D%2520annotation.%2520These%2520multi-class%2520positive%2520annotations%250Anaturally%2520fall%2520within%2520the%2520in-distribution%2520%2528ID%2529%2520set.%2520Unlabelled%2520pixels%2520may%250Acontain%2520positive%2520classes%2520but%2520also%2520negative%2520ones%252C%2520including%2520what%2520is%2520typically%250Areferred%2520to%2520as%2520%255Cemph%257Bbackground%257D%2520in%2520standard%2520segmentation%2520formulations.%2520Here%252C%250Awe%2520forgo%2520the%2520need%2520for%2520background%2520annotation%2520and%2520consider%2520these%2520together%2520with%250Aany%2520other%2520unseen%2520classes%2520as%2520part%2520of%2520the%2520OOD%2520set.%2520Our%2520framework%2520can%2520integrate%252C%250Aat%2520a%2520pixel-level%252C%2520any%2520OOD%2520detection%2520approaches%2520designed%2520for%2520classification%250Atasks.%2520To%2520address%2520the%2520lack%2520of%2520existing%2520OOD%2520datasets%2520and%2520established%2520evaluation%250Ametric%2520for%2520medical%2520image%2520segmentation%252C%2520we%2520propose%2520a%2520cross-validation%2520strategy%250Athat%2520treats%2520held-out%2520labelled%2520classes%2520as%2520OOD.%2520Extensive%2520experiments%2520on%2520both%250Amulti-class%2520hyperspectral%2520and%2520RGB%2520surgical%2520imaging%2520datasets%2520demonstrate%2520the%250Arobustness%2520and%2520generalisation%2520capability%2520of%2520our%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OOD-SEG%3A%20Out-Of-Distribution%20detection%20for%20image%20SEGmentation%20with%0A%20%20sparse%20multi-class%20positive-only%20annotations&entry.906535625=Junwen%20Wang%20and%20Zhonghao%20Wang%20and%20Oscar%20MacCormac%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Despite%20significant%20advancements%2C%20segmentation%20based%20on%20deep%20neural%20networks%0Ain%20medical%20and%20surgical%20imaging%20faces%20several%20challenges%2C%20two%20of%20which%20we%20aim%0Ato%20address%20in%20this%20work.%20First%2C%20acquiring%20complete%20pixel-level%20segmentation%0Alabels%20for%20medical%20images%20is%20time-consuming%20and%20requires%20domain%20expertise.%0ASecond%2C%20typical%20segmentation%20pipelines%20cannot%20detect%20out-of-distribution%20%28OOD%29%0Apixels%2C%20leaving%20them%20prone%20to%20spurious%20outputs%20during%20deployment.%20In%20this%20work%2C%0Awe%20propose%20a%20novel%20segmentation%20approach%20exploiting%20OOD%20detection%20that%20learns%0Aonly%20from%20sparsely%20annotated%20pixels%20from%20multiple%20positive-only%20classes.%20%25but%0A%5Cemph%7Bno%20background%20class%7D%20annotation.%20These%20multi-class%20positive%20annotations%0Anaturally%20fall%20within%20the%20in-distribution%20%28ID%29%20set.%20Unlabelled%20pixels%20may%0Acontain%20positive%20classes%20but%20also%20negative%20ones%2C%20including%20what%20is%20typically%0Areferred%20to%20as%20%5Cemph%7Bbackground%7D%20in%20standard%20segmentation%20formulations.%20Here%2C%0Awe%20forgo%20the%20need%20for%20background%20annotation%20and%20consider%20these%20together%20with%0Aany%20other%20unseen%20classes%20as%20part%20of%20the%20OOD%20set.%20Our%20framework%20can%20integrate%2C%0Aat%20a%20pixel-level%2C%20any%20OOD%20detection%20approaches%20designed%20for%20classification%0Atasks.%20To%20address%20the%20lack%20of%20existing%20OOD%20datasets%20and%20established%20evaluation%0Ametric%20for%20medical%20image%20segmentation%2C%20we%20propose%20a%20cross-validation%20strategy%0Athat%20treats%20held-out%20labelled%20classes%20as%20OOD.%20Extensive%20experiments%20on%20both%0Amulti-class%20hyperspectral%20and%20RGB%20surgical%20imaging%20datasets%20demonstrate%20the%0Arobustness%20and%20generalisation%20capability%20of%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09553v1&entry.124074799=Read"},
{"title": "Golden Noise for Diffusion Models: A Learning Framework", "author": "Zikai Zhou and Shitong Shao and Lichen Bai and Zhiqiang Xu and Bo Han and Zeke Xie", "abstract": "  Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.\n", "link": "http://arxiv.org/abs/2411.09502v1", "date": "2024-11-14", "relevancy": 1.2591, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6338}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6337}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Golden%20Noise%20for%20Diffusion%20Models%3A%20A%20Learning%20Framework&body=Title%3A%20Golden%20Noise%20for%20Diffusion%20Models%3A%20A%20Learning%20Framework%0AAuthor%3A%20Zikai%20Zhou%20and%20Shitong%20Shao%20and%20Lichen%20Bai%20and%20Zhiqiang%20Xu%20and%20Bo%20Han%20and%20Zeke%20Xie%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20model%20is%20a%20popular%20paradigm%20that%20synthesizes%0Apersonalized%20images%20by%20providing%20a%20text%20prompt%20and%20a%20random%20Gaussian%20noise.%0AWhile%20people%20observe%20that%20some%20noises%20are%20%60%60golden%20noises%27%27%20that%20can%20achieve%0Abetter%20text-image%20alignment%20and%20higher%20human%20preference%20than%20others%2C%20we%20still%0Alack%20a%20machine%20learning%20framework%20to%20obtain%20those%20golden%20noises.%20To%20learn%0Agolden%20noises%20for%20diffusion%20sampling%2C%20we%20mainly%20make%20three%20contributions%20in%0Athis%20paper.%20First%2C%20we%20identify%20a%20new%20concept%20termed%20the%20%5Ctextit%7Bnoise%20prompt%7D%2C%0Awhich%20aims%20at%20turning%20a%20random%20Gaussian%20noise%20into%20a%20golden%20noise%20by%20adding%20a%0Asmall%20desirable%20perturbation%20derived%20from%20the%20text%20prompt.%20Following%20the%0Aconcept%2C%20we%20first%20formulate%20the%20%5Ctextit%7Bnoise%20prompt%20learning%7D%20framework%20that%0Asystematically%20learns%20%60%60prompted%27%27%20golden%20noise%20associated%20with%20a%20text%20prompt%0Afor%20diffusion%20models.%20Second%2C%20we%20design%20a%20noise%20prompt%20data%20collection%20pipeline%0Aand%20collect%20a%20large-scale%20%5Ctextit%7Bnoise%20prompt%20dataset%7D~%28NPD%29%20that%20contains%0A100k%20pairs%20of%20random%20noises%20and%20golden%20noises%20with%20the%20associated%20text%20prompts.%0AWith%20the%20prepared%20NPD%20as%20the%20training%20dataset%2C%20we%20trained%20a%20small%20%5Ctextit%7Bnoise%0Aprompt%20network%7D~%28NPNet%29%20that%20can%20directly%20learn%20to%20transform%20a%20random%20noise%0Ainto%20a%20golden%20noise.%20The%20learned%20golden%20noise%20perturbation%20can%20be%20considered%20as%0Aa%20kind%20of%20prompt%20for%20noise%2C%20as%20it%20is%20rich%20in%20semantic%20information%20and%20tailored%0Ato%20the%20given%20text%20prompt.%20Third%2C%20our%20extensive%20experiments%20demonstrate%20the%0Aimpressive%20effectiveness%20and%20generalization%20of%20NPNet%20on%20improving%20the%20quality%0Aof%20synthesized%20images%20across%20various%20diffusion%20models%2C%20including%20SDXL%2C%0ADreamShaper-xl-v2-turbo%2C%20and%20Hunyuan-DiT.%20Moreover%2C%20NPNet%20is%20a%20small%20and%0Aefficient%20controller%20that%20acts%20as%20a%20plug-and-play%20module%20with%20very%20limited%0Aadditional%20inference%20and%20computational%20costs%2C%20as%20it%20just%20provides%20a%20golden%0Anoise%20instead%20of%20a%20random%20noise%20without%20accessing%20the%20original%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGolden%2520Noise%2520for%2520Diffusion%2520Models%253A%2520A%2520Learning%2520Framework%26entry.906535625%3DZikai%2520Zhou%2520and%2520Shitong%2520Shao%2520and%2520Lichen%2520Bai%2520and%2520Zhiqiang%2520Xu%2520and%2520Bo%2520Han%2520and%2520Zeke%2520Xie%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520model%2520is%2520a%2520popular%2520paradigm%2520that%2520synthesizes%250Apersonalized%2520images%2520by%2520providing%2520a%2520text%2520prompt%2520and%2520a%2520random%2520Gaussian%2520noise.%250AWhile%2520people%2520observe%2520that%2520some%2520noises%2520are%2520%2560%2560golden%2520noises%2527%2527%2520that%2520can%2520achieve%250Abetter%2520text-image%2520alignment%2520and%2520higher%2520human%2520preference%2520than%2520others%252C%2520we%2520still%250Alack%2520a%2520machine%2520learning%2520framework%2520to%2520obtain%2520those%2520golden%2520noises.%2520To%2520learn%250Agolden%2520noises%2520for%2520diffusion%2520sampling%252C%2520we%2520mainly%2520make%2520three%2520contributions%2520in%250Athis%2520paper.%2520First%252C%2520we%2520identify%2520a%2520new%2520concept%2520termed%2520the%2520%255Ctextit%257Bnoise%2520prompt%257D%252C%250Awhich%2520aims%2520at%2520turning%2520a%2520random%2520Gaussian%2520noise%2520into%2520a%2520golden%2520noise%2520by%2520adding%2520a%250Asmall%2520desirable%2520perturbation%2520derived%2520from%2520the%2520text%2520prompt.%2520Following%2520the%250Aconcept%252C%2520we%2520first%2520formulate%2520the%2520%255Ctextit%257Bnoise%2520prompt%2520learning%257D%2520framework%2520that%250Asystematically%2520learns%2520%2560%2560prompted%2527%2527%2520golden%2520noise%2520associated%2520with%2520a%2520text%2520prompt%250Afor%2520diffusion%2520models.%2520Second%252C%2520we%2520design%2520a%2520noise%2520prompt%2520data%2520collection%2520pipeline%250Aand%2520collect%2520a%2520large-scale%2520%255Ctextit%257Bnoise%2520prompt%2520dataset%257D~%2528NPD%2529%2520that%2520contains%250A100k%2520pairs%2520of%2520random%2520noises%2520and%2520golden%2520noises%2520with%2520the%2520associated%2520text%2520prompts.%250AWith%2520the%2520prepared%2520NPD%2520as%2520the%2520training%2520dataset%252C%2520we%2520trained%2520a%2520small%2520%255Ctextit%257Bnoise%250Aprompt%2520network%257D~%2528NPNet%2529%2520that%2520can%2520directly%2520learn%2520to%2520transform%2520a%2520random%2520noise%250Ainto%2520a%2520golden%2520noise.%2520The%2520learned%2520golden%2520noise%2520perturbation%2520can%2520be%2520considered%2520as%250Aa%2520kind%2520of%2520prompt%2520for%2520noise%252C%2520as%2520it%2520is%2520rich%2520in%2520semantic%2520information%2520and%2520tailored%250Ato%2520the%2520given%2520text%2520prompt.%2520Third%252C%2520our%2520extensive%2520experiments%2520demonstrate%2520the%250Aimpressive%2520effectiveness%2520and%2520generalization%2520of%2520NPNet%2520on%2520improving%2520the%2520quality%250Aof%2520synthesized%2520images%2520across%2520various%2520diffusion%2520models%252C%2520including%2520SDXL%252C%250ADreamShaper-xl-v2-turbo%252C%2520and%2520Hunyuan-DiT.%2520Moreover%252C%2520NPNet%2520is%2520a%2520small%2520and%250Aefficient%2520controller%2520that%2520acts%2520as%2520a%2520plug-and-play%2520module%2520with%2520very%2520limited%250Aadditional%2520inference%2520and%2520computational%2520costs%252C%2520as%2520it%2520just%2520provides%2520a%2520golden%250Anoise%2520instead%2520of%2520a%2520random%2520noise%2520without%2520accessing%2520the%2520original%2520pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Golden%20Noise%20for%20Diffusion%20Models%3A%20A%20Learning%20Framework&entry.906535625=Zikai%20Zhou%20and%20Shitong%20Shao%20and%20Lichen%20Bai%20and%20Zhiqiang%20Xu%20and%20Bo%20Han%20and%20Zeke%20Xie&entry.1292438233=%20%20Text-to-image%20diffusion%20model%20is%20a%20popular%20paradigm%20that%20synthesizes%0Apersonalized%20images%20by%20providing%20a%20text%20prompt%20and%20a%20random%20Gaussian%20noise.%0AWhile%20people%20observe%20that%20some%20noises%20are%20%60%60golden%20noises%27%27%20that%20can%20achieve%0Abetter%20text-image%20alignment%20and%20higher%20human%20preference%20than%20others%2C%20we%20still%0Alack%20a%20machine%20learning%20framework%20to%20obtain%20those%20golden%20noises.%20To%20learn%0Agolden%20noises%20for%20diffusion%20sampling%2C%20we%20mainly%20make%20three%20contributions%20in%0Athis%20paper.%20First%2C%20we%20identify%20a%20new%20concept%20termed%20the%20%5Ctextit%7Bnoise%20prompt%7D%2C%0Awhich%20aims%20at%20turning%20a%20random%20Gaussian%20noise%20into%20a%20golden%20noise%20by%20adding%20a%0Asmall%20desirable%20perturbation%20derived%20from%20the%20text%20prompt.%20Following%20the%0Aconcept%2C%20we%20first%20formulate%20the%20%5Ctextit%7Bnoise%20prompt%20learning%7D%20framework%20that%0Asystematically%20learns%20%60%60prompted%27%27%20golden%20noise%20associated%20with%20a%20text%20prompt%0Afor%20diffusion%20models.%20Second%2C%20we%20design%20a%20noise%20prompt%20data%20collection%20pipeline%0Aand%20collect%20a%20large-scale%20%5Ctextit%7Bnoise%20prompt%20dataset%7D~%28NPD%29%20that%20contains%0A100k%20pairs%20of%20random%20noises%20and%20golden%20noises%20with%20the%20associated%20text%20prompts.%0AWith%20the%20prepared%20NPD%20as%20the%20training%20dataset%2C%20we%20trained%20a%20small%20%5Ctextit%7Bnoise%0Aprompt%20network%7D~%28NPNet%29%20that%20can%20directly%20learn%20to%20transform%20a%20random%20noise%0Ainto%20a%20golden%20noise.%20The%20learned%20golden%20noise%20perturbation%20can%20be%20considered%20as%0Aa%20kind%20of%20prompt%20for%20noise%2C%20as%20it%20is%20rich%20in%20semantic%20information%20and%20tailored%0Ato%20the%20given%20text%20prompt.%20Third%2C%20our%20extensive%20experiments%20demonstrate%20the%0Aimpressive%20effectiveness%20and%20generalization%20of%20NPNet%20on%20improving%20the%20quality%0Aof%20synthesized%20images%20across%20various%20diffusion%20models%2C%20including%20SDXL%2C%0ADreamShaper-xl-v2-turbo%2C%20and%20Hunyuan-DiT.%20Moreover%2C%20NPNet%20is%20a%20small%20and%0Aefficient%20controller%20that%20acts%20as%20a%20plug-and-play%20module%20with%20very%20limited%0Aadditional%20inference%20and%20computational%20costs%2C%20as%20it%20just%20provides%20a%20golden%0Anoise%20instead%20of%20a%20random%20noise%20without%20accessing%20the%20original%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09502v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


