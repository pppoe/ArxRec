<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250511.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models", "author": "Radu Alexandru Rosu and Keyu Wu and Yao Feng and Youyi Zheng and Michael J. Black", "abstract": "  We address the task of generating 3D hair geometry from a single image, which\nis challenging due to the diversity of hairstyles and the lack of paired\nimage-to-3D hair data. Previous methods are primarily trained on synthetic data\nand cope with the limited amount of such data by using low-dimensional\nintermediate representations, such as guide strands and scalp-level embeddings,\nthat require post-processing to decode, upsample, and add realism. These\napproaches fail to reconstruct detailed hair, struggle with curly hair, or are\nlimited to handling only a few hairstyles. To overcome these limitations, we\npropose DiffLocks, a novel framework that enables detailed reconstruction of a\nwide variety of hairstyles directly from a single image. First, we address the\nlack of 3D hair data by automating the creation of the largest synthetic hair\ndataset to date, containing 40K hairstyles. Second, we leverage the synthetic\nhair dataset to learn an image-conditioned diffusion-transfomer model that\ngenerates accurate 3D strands from a single frontal image. By using a\npretrained image backbone, our method generalizes to in-the-wild images despite\nbeing trained only on synthetic data. Our diffusion model predicts a scalp\ntexture map in which any point in the map contains the latent code for an\nindividual hair strand. These codes are directly decoded to 3D strands without\npost-processing techniques. Representing individual strands, instead of guide\nstrands, enables the transformer to model the detailed spatial structure of\ncomplex hairstyles. With this, DiffLocks can recover highly curled hair, like\nafro hairstyles, from a single image for the first time. Data and code is\navailable at https://radualexandru.github.io/difflocks/\n", "link": "http://arxiv.org/abs/2505.06166v1", "date": "2025-05-09", "relevancy": 2.9903, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6163}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6162}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffLocks%3A%20Generating%203D%20Hair%20from%20a%20Single%20Image%20using%20Diffusion%20Models&body=Title%3A%20DiffLocks%3A%20Generating%203D%20Hair%20from%20a%20Single%20Image%20using%20Diffusion%20Models%0AAuthor%3A%20Radu%20Alexandru%20Rosu%20and%20Keyu%20Wu%20and%20Yao%20Feng%20and%20Youyi%20Zheng%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20We%20address%20the%20task%20of%20generating%203D%20hair%20geometry%20from%20a%20single%20image%2C%20which%0Ais%20challenging%20due%20to%20the%20diversity%20of%20hairstyles%20and%20the%20lack%20of%20paired%0Aimage-to-3D%20hair%20data.%20Previous%20methods%20are%20primarily%20trained%20on%20synthetic%20data%0Aand%20cope%20with%20the%20limited%20amount%20of%20such%20data%20by%20using%20low-dimensional%0Aintermediate%20representations%2C%20such%20as%20guide%20strands%20and%20scalp-level%20embeddings%2C%0Athat%20require%20post-processing%20to%20decode%2C%20upsample%2C%20and%20add%20realism.%20These%0Aapproaches%20fail%20to%20reconstruct%20detailed%20hair%2C%20struggle%20with%20curly%20hair%2C%20or%20are%0Alimited%20to%20handling%20only%20a%20few%20hairstyles.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20DiffLocks%2C%20a%20novel%20framework%20that%20enables%20detailed%20reconstruction%20of%20a%0Awide%20variety%20of%20hairstyles%20directly%20from%20a%20single%20image.%20First%2C%20we%20address%20the%0Alack%20of%203D%20hair%20data%20by%20automating%20the%20creation%20of%20the%20largest%20synthetic%20hair%0Adataset%20to%20date%2C%20containing%2040K%20hairstyles.%20Second%2C%20we%20leverage%20the%20synthetic%0Ahair%20dataset%20to%20learn%20an%20image-conditioned%20diffusion-transfomer%20model%20that%0Agenerates%20accurate%203D%20strands%20from%20a%20single%20frontal%20image.%20By%20using%20a%0Apretrained%20image%20backbone%2C%20our%20method%20generalizes%20to%20in-the-wild%20images%20despite%0Abeing%20trained%20only%20on%20synthetic%20data.%20Our%20diffusion%20model%20predicts%20a%20scalp%0Atexture%20map%20in%20which%20any%20point%20in%20the%20map%20contains%20the%20latent%20code%20for%20an%0Aindividual%20hair%20strand.%20These%20codes%20are%20directly%20decoded%20to%203D%20strands%20without%0Apost-processing%20techniques.%20Representing%20individual%20strands%2C%20instead%20of%20guide%0Astrands%2C%20enables%20the%20transformer%20to%20model%20the%20detailed%20spatial%20structure%20of%0Acomplex%20hairstyles.%20With%20this%2C%20DiffLocks%20can%20recover%20highly%20curled%20hair%2C%20like%0Aafro%20hairstyles%2C%20from%20a%20single%20image%20for%20the%20first%20time.%20Data%20and%20code%20is%0Aavailable%20at%20https%3A//radualexandru.github.io/difflocks/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffLocks%253A%2520Generating%25203D%2520Hair%2520from%2520a%2520Single%2520Image%2520using%2520Diffusion%2520Models%26entry.906535625%3DRadu%2520Alexandru%2520Rosu%2520and%2520Keyu%2520Wu%2520and%2520Yao%2520Feng%2520and%2520Youyi%2520Zheng%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520task%2520of%2520generating%25203D%2520hair%2520geometry%2520from%2520a%2520single%2520image%252C%2520which%250Ais%2520challenging%2520due%2520to%2520the%2520diversity%2520of%2520hairstyles%2520and%2520the%2520lack%2520of%2520paired%250Aimage-to-3D%2520hair%2520data.%2520Previous%2520methods%2520are%2520primarily%2520trained%2520on%2520synthetic%2520data%250Aand%2520cope%2520with%2520the%2520limited%2520amount%2520of%2520such%2520data%2520by%2520using%2520low-dimensional%250Aintermediate%2520representations%252C%2520such%2520as%2520guide%2520strands%2520and%2520scalp-level%2520embeddings%252C%250Athat%2520require%2520post-processing%2520to%2520decode%252C%2520upsample%252C%2520and%2520add%2520realism.%2520These%250Aapproaches%2520fail%2520to%2520reconstruct%2520detailed%2520hair%252C%2520struggle%2520with%2520curly%2520hair%252C%2520or%2520are%250Alimited%2520to%2520handling%2520only%2520a%2520few%2520hairstyles.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520DiffLocks%252C%2520a%2520novel%2520framework%2520that%2520enables%2520detailed%2520reconstruction%2520of%2520a%250Awide%2520variety%2520of%2520hairstyles%2520directly%2520from%2520a%2520single%2520image.%2520First%252C%2520we%2520address%2520the%250Alack%2520of%25203D%2520hair%2520data%2520by%2520automating%2520the%2520creation%2520of%2520the%2520largest%2520synthetic%2520hair%250Adataset%2520to%2520date%252C%2520containing%252040K%2520hairstyles.%2520Second%252C%2520we%2520leverage%2520the%2520synthetic%250Ahair%2520dataset%2520to%2520learn%2520an%2520image-conditioned%2520diffusion-transfomer%2520model%2520that%250Agenerates%2520accurate%25203D%2520strands%2520from%2520a%2520single%2520frontal%2520image.%2520By%2520using%2520a%250Apretrained%2520image%2520backbone%252C%2520our%2520method%2520generalizes%2520to%2520in-the-wild%2520images%2520despite%250Abeing%2520trained%2520only%2520on%2520synthetic%2520data.%2520Our%2520diffusion%2520model%2520predicts%2520a%2520scalp%250Atexture%2520map%2520in%2520which%2520any%2520point%2520in%2520the%2520map%2520contains%2520the%2520latent%2520code%2520for%2520an%250Aindividual%2520hair%2520strand.%2520These%2520codes%2520are%2520directly%2520decoded%2520to%25203D%2520strands%2520without%250Apost-processing%2520techniques.%2520Representing%2520individual%2520strands%252C%2520instead%2520of%2520guide%250Astrands%252C%2520enables%2520the%2520transformer%2520to%2520model%2520the%2520detailed%2520spatial%2520structure%2520of%250Acomplex%2520hairstyles.%2520With%2520this%252C%2520DiffLocks%2520can%2520recover%2520highly%2520curled%2520hair%252C%2520like%250Aafro%2520hairstyles%252C%2520from%2520a%2520single%2520image%2520for%2520the%2520first%2520time.%2520Data%2520and%2520code%2520is%250Aavailable%2520at%2520https%253A//radualexandru.github.io/difflocks/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffLocks%3A%20Generating%203D%20Hair%20from%20a%20Single%20Image%20using%20Diffusion%20Models&entry.906535625=Radu%20Alexandru%20Rosu%20and%20Keyu%20Wu%20and%20Yao%20Feng%20and%20Youyi%20Zheng%20and%20Michael%20J.%20Black&entry.1292438233=%20%20We%20address%20the%20task%20of%20generating%203D%20hair%20geometry%20from%20a%20single%20image%2C%20which%0Ais%20challenging%20due%20to%20the%20diversity%20of%20hairstyles%20and%20the%20lack%20of%20paired%0Aimage-to-3D%20hair%20data.%20Previous%20methods%20are%20primarily%20trained%20on%20synthetic%20data%0Aand%20cope%20with%20the%20limited%20amount%20of%20such%20data%20by%20using%20low-dimensional%0Aintermediate%20representations%2C%20such%20as%20guide%20strands%20and%20scalp-level%20embeddings%2C%0Athat%20require%20post-processing%20to%20decode%2C%20upsample%2C%20and%20add%20realism.%20These%0Aapproaches%20fail%20to%20reconstruct%20detailed%20hair%2C%20struggle%20with%20curly%20hair%2C%20or%20are%0Alimited%20to%20handling%20only%20a%20few%20hairstyles.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20DiffLocks%2C%20a%20novel%20framework%20that%20enables%20detailed%20reconstruction%20of%20a%0Awide%20variety%20of%20hairstyles%20directly%20from%20a%20single%20image.%20First%2C%20we%20address%20the%0Alack%20of%203D%20hair%20data%20by%20automating%20the%20creation%20of%20the%20largest%20synthetic%20hair%0Adataset%20to%20date%2C%20containing%2040K%20hairstyles.%20Second%2C%20we%20leverage%20the%20synthetic%0Ahair%20dataset%20to%20learn%20an%20image-conditioned%20diffusion-transfomer%20model%20that%0Agenerates%20accurate%203D%20strands%20from%20a%20single%20frontal%20image.%20By%20using%20a%0Apretrained%20image%20backbone%2C%20our%20method%20generalizes%20to%20in-the-wild%20images%20despite%0Abeing%20trained%20only%20on%20synthetic%20data.%20Our%20diffusion%20model%20predicts%20a%20scalp%0Atexture%20map%20in%20which%20any%20point%20in%20the%20map%20contains%20the%20latent%20code%20for%20an%0Aindividual%20hair%20strand.%20These%20codes%20are%20directly%20decoded%20to%203D%20strands%20without%0Apost-processing%20techniques.%20Representing%20individual%20strands%2C%20instead%20of%20guide%0Astrands%2C%20enables%20the%20transformer%20to%20model%20the%20detailed%20spatial%20structure%20of%0Acomplex%20hairstyles.%20With%20this%2C%20DiffLocks%20can%20recover%20highly%20curled%20hair%2C%20like%0Aafro%20hairstyles%2C%20from%20a%20single%20image%20for%20the%20first%20time.%20Data%20and%20code%20is%0Aavailable%20at%20https%3A//radualexandru.github.io/difflocks/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06166v1&entry.124074799=Read"},
{"title": "MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion", "author": "Saron Samuel and Dan DeGenaro and Jimena Guallar-Blasco and Kate Sanders and Oluwaseun Eisape and Tanner Spendlove and Arun Reddy and Alexander Martin and Andrew Yates and Eugene Yang and Cameron Carpenter and David Etter and Efsun Kayi and Matthew Wiesner and Kenton Murray and Reno Kriz", "abstract": "  Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities.\n", "link": "http://arxiv.org/abs/2503.20698v4", "date": "2025-05-09", "relevancy": 2.8398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion&body=Title%3A%20MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion%0AAuthor%3A%20Saron%20Samuel%20and%20Dan%20DeGenaro%20and%20Jimena%20Guallar-Blasco%20and%20Kate%20Sanders%20and%20Oluwaseun%20Eisape%20and%20Tanner%20Spendlove%20and%20Arun%20Reddy%20and%20Alexander%20Martin%20and%20Andrew%20Yates%20and%20Eugene%20Yang%20and%20Cameron%20Carpenter%20and%20David%20Etter%20and%20Efsun%20Kayi%20and%20Matthew%20Wiesner%20and%20Kenton%20Murray%20and%20Reno%20Kriz%0AAbstract%3A%20%20%20Videos%20inherently%20contain%20multiple%20modalities%2C%20including%20visual%20events%2C%20text%0Aoverlays%2C%20sounds%2C%20and%20speech%2C%20all%20of%20which%20are%20important%20for%20retrieval.%0AHowever%2C%20state-of-the-art%20multimodal%20language%20models%20like%20VAST%20and%20LanguageBind%0Aare%20built%20on%20vision-language%20models%20%28VLMs%29%2C%20and%20thus%20overly%20prioritize%20visual%0Asignals.%20Retrieval%20benchmarks%20further%20reinforce%20this%20bias%20by%20focusing%20on%20visual%0Aqueries%20and%20neglecting%20other%20modalities.%20We%20create%20a%20search%20system%20MMMORRF%20that%0Aextracts%20text%20and%20features%20from%20both%20visual%20and%20audio%20modalities%20and%20integrates%0Athem%20with%20a%20novel%20modality-aware%20weighted%20reciprocal%20rank%20fusion.%20MMMORRF%20is%0Aboth%20effective%20and%20efficient%2C%20demonstrating%20practicality%20in%20searching%20videos%0Abased%20on%20users%27%20information%20needs%20instead%20of%20visual%20descriptive%20queries.%20We%0Aevaluate%20MMMORRF%20on%20MultiVENT%202.0%20and%20TVR%2C%20two%20multimodal%20benchmarks%20designed%0Afor%20more%20targeted%20information%20needs%2C%20and%20find%20that%20it%20improves%20nDCG%4020%20by%2081%25%0Aover%20leading%20multimodal%20encoders%20and%2037%25%20over%20single-modality%20retrieval%2C%0Ademonstrating%20the%20value%20of%20integrating%20diverse%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20698v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMMORRF%253A%2520Multimodal%2520Multilingual%2520Modularized%2520Reciprocal%2520Rank%2520Fusion%26entry.906535625%3DSaron%2520Samuel%2520and%2520Dan%2520DeGenaro%2520and%2520Jimena%2520Guallar-Blasco%2520and%2520Kate%2520Sanders%2520and%2520Oluwaseun%2520Eisape%2520and%2520Tanner%2520Spendlove%2520and%2520Arun%2520Reddy%2520and%2520Alexander%2520Martin%2520and%2520Andrew%2520Yates%2520and%2520Eugene%2520Yang%2520and%2520Cameron%2520Carpenter%2520and%2520David%2520Etter%2520and%2520Efsun%2520Kayi%2520and%2520Matthew%2520Wiesner%2520and%2520Kenton%2520Murray%2520and%2520Reno%2520Kriz%26entry.1292438233%3D%2520%2520Videos%2520inherently%2520contain%2520multiple%2520modalities%252C%2520including%2520visual%2520events%252C%2520text%250Aoverlays%252C%2520sounds%252C%2520and%2520speech%252C%2520all%2520of%2520which%2520are%2520important%2520for%2520retrieval.%250AHowever%252C%2520state-of-the-art%2520multimodal%2520language%2520models%2520like%2520VAST%2520and%2520LanguageBind%250Aare%2520built%2520on%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520and%2520thus%2520overly%2520prioritize%2520visual%250Asignals.%2520Retrieval%2520benchmarks%2520further%2520reinforce%2520this%2520bias%2520by%2520focusing%2520on%2520visual%250Aqueries%2520and%2520neglecting%2520other%2520modalities.%2520We%2520create%2520a%2520search%2520system%2520MMMORRF%2520that%250Aextracts%2520text%2520and%2520features%2520from%2520both%2520visual%2520and%2520audio%2520modalities%2520and%2520integrates%250Athem%2520with%2520a%2520novel%2520modality-aware%2520weighted%2520reciprocal%2520rank%2520fusion.%2520MMMORRF%2520is%250Aboth%2520effective%2520and%2520efficient%252C%2520demonstrating%2520practicality%2520in%2520searching%2520videos%250Abased%2520on%2520users%2527%2520information%2520needs%2520instead%2520of%2520visual%2520descriptive%2520queries.%2520We%250Aevaluate%2520MMMORRF%2520on%2520MultiVENT%25202.0%2520and%2520TVR%252C%2520two%2520multimodal%2520benchmarks%2520designed%250Afor%2520more%2520targeted%2520information%2520needs%252C%2520and%2520find%2520that%2520it%2520improves%2520nDCG%254020%2520by%252081%2525%250Aover%2520leading%2520multimodal%2520encoders%2520and%252037%2525%2520over%2520single-modality%2520retrieval%252C%250Ademonstrating%2520the%2520value%2520of%2520integrating%2520diverse%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20698v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion&entry.906535625=Saron%20Samuel%20and%20Dan%20DeGenaro%20and%20Jimena%20Guallar-Blasco%20and%20Kate%20Sanders%20and%20Oluwaseun%20Eisape%20and%20Tanner%20Spendlove%20and%20Arun%20Reddy%20and%20Alexander%20Martin%20and%20Andrew%20Yates%20and%20Eugene%20Yang%20and%20Cameron%20Carpenter%20and%20David%20Etter%20and%20Efsun%20Kayi%20and%20Matthew%20Wiesner%20and%20Kenton%20Murray%20and%20Reno%20Kriz&entry.1292438233=%20%20Videos%20inherently%20contain%20multiple%20modalities%2C%20including%20visual%20events%2C%20text%0Aoverlays%2C%20sounds%2C%20and%20speech%2C%20all%20of%20which%20are%20important%20for%20retrieval.%0AHowever%2C%20state-of-the-art%20multimodal%20language%20models%20like%20VAST%20and%20LanguageBind%0Aare%20built%20on%20vision-language%20models%20%28VLMs%29%2C%20and%20thus%20overly%20prioritize%20visual%0Asignals.%20Retrieval%20benchmarks%20further%20reinforce%20this%20bias%20by%20focusing%20on%20visual%0Aqueries%20and%20neglecting%20other%20modalities.%20We%20create%20a%20search%20system%20MMMORRF%20that%0Aextracts%20text%20and%20features%20from%20both%20visual%20and%20audio%20modalities%20and%20integrates%0Athem%20with%20a%20novel%20modality-aware%20weighted%20reciprocal%20rank%20fusion.%20MMMORRF%20is%0Aboth%20effective%20and%20efficient%2C%20demonstrating%20practicality%20in%20searching%20videos%0Abased%20on%20users%27%20information%20needs%20instead%20of%20visual%20descriptive%20queries.%20We%0Aevaluate%20MMMORRF%20on%20MultiVENT%202.0%20and%20TVR%2C%20two%20multimodal%20benchmarks%20designed%0Afor%20more%20targeted%20information%20needs%2C%20and%20find%20that%20it%20improves%20nDCG%4020%20by%2081%25%0Aover%20leading%20multimodal%20encoders%20and%2037%25%20over%20single-modality%20retrieval%2C%0Ademonstrating%20the%20value%20of%20integrating%20diverse%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20698v4&entry.124074799=Read"},
{"title": "S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology\n  Reconstruction via Echocardiogram", "author": "Xilin Gong and Yongkai Chen and Shushan Wu and Fang Wang and Ping Ma and Wenxuan Zhong", "abstract": "  Echocardiogram is the most commonly used imaging modality in cardiac\nassessment duo to its non-invasive nature, real-time capability, and\ncost-effectiveness. Despite its advantages, most clinical echocardiograms\nprovide only two-dimensional views, limiting the ability to fully assess\ncardiac anatomy and function in three dimensions. While three-dimensional\nechocardiography exists, it often suffers from reduced resolution, limited\navailability, and higher acquisition costs. To overcome these challenges, we\npropose a deep learning framework S2MNet that reconstructs continuous and\nhigh-fidelity 3D heart models by integrating six slices of routinely acquired\n2D echocardiogram views. Our method has three advantages. First, our method\navoid the difficulties on training data acquasition by simulate six of 2D\nechocardiogram images from corresponding slices of a given 3D heart mesh.\nSecond, we introduce a deformation field-based method, which avoid spatial\ndiscontinuities or structural artifacts in 3D echocardiogram reconstructions.\nWe validate our method using clinically collected echocardiogram and\ndemonstrate that our estimated left ventricular volume, a key clinical\nindicator of cardiac function, is strongly correlated with the doctor measured\nGLPS, a clinical measurement that should demonstrate a negative correlation\nwith LVE in medical theory. This association confirms the reliability of our\nproposed 3D construction method.\n", "link": "http://arxiv.org/abs/2505.06105v1", "date": "2025-05-09", "relevancy": 2.8018, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5788}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5553}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2MNet%3A%20Speckle-To-Mesh%20Net%20for%20Three-Dimensional%20Cardiac%20Morphology%0A%20%20Reconstruction%20via%20Echocardiogram&body=Title%3A%20S2MNet%3A%20Speckle-To-Mesh%20Net%20for%20Three-Dimensional%20Cardiac%20Morphology%0A%20%20Reconstruction%20via%20Echocardiogram%0AAuthor%3A%20Xilin%20Gong%20and%20Yongkai%20Chen%20and%20Shushan%20Wu%20and%20Fang%20Wang%20and%20Ping%20Ma%20and%20Wenxuan%20Zhong%0AAbstract%3A%20%20%20Echocardiogram%20is%20the%20most%20commonly%20used%20imaging%20modality%20in%20cardiac%0Aassessment%20duo%20to%20its%20non-invasive%20nature%2C%20real-time%20capability%2C%20and%0Acost-effectiveness.%20Despite%20its%20advantages%2C%20most%20clinical%20echocardiograms%0Aprovide%20only%20two-dimensional%20views%2C%20limiting%20the%20ability%20to%20fully%20assess%0Acardiac%20anatomy%20and%20function%20in%20three%20dimensions.%20While%20three-dimensional%0Aechocardiography%20exists%2C%20it%20often%20suffers%20from%20reduced%20resolution%2C%20limited%0Aavailability%2C%20and%20higher%20acquisition%20costs.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20a%20deep%20learning%20framework%20S2MNet%20that%20reconstructs%20continuous%20and%0Ahigh-fidelity%203D%20heart%20models%20by%20integrating%20six%20slices%20of%20routinely%20acquired%0A2D%20echocardiogram%20views.%20Our%20method%20has%20three%20advantages.%20First%2C%20our%20method%0Aavoid%20the%20difficulties%20on%20training%20data%20acquasition%20by%20simulate%20six%20of%202D%0Aechocardiogram%20images%20from%20corresponding%20slices%20of%20a%20given%203D%20heart%20mesh.%0ASecond%2C%20we%20introduce%20a%20deformation%20field-based%20method%2C%20which%20avoid%20spatial%0Adiscontinuities%20or%20structural%20artifacts%20in%203D%20echocardiogram%20reconstructions.%0AWe%20validate%20our%20method%20using%20clinically%20collected%20echocardiogram%20and%0Ademonstrate%20that%20our%20estimated%20left%20ventricular%20volume%2C%20a%20key%20clinical%0Aindicator%20of%20cardiac%20function%2C%20is%20strongly%20correlated%20with%20the%20doctor%20measured%0AGLPS%2C%20a%20clinical%20measurement%20that%20should%20demonstrate%20a%20negative%20correlation%0Awith%20LVE%20in%20medical%20theory.%20This%20association%20confirms%20the%20reliability%20of%20our%0Aproposed%203D%20construction%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2MNet%253A%2520Speckle-To-Mesh%2520Net%2520for%2520Three-Dimensional%2520Cardiac%2520Morphology%250A%2520%2520Reconstruction%2520via%2520Echocardiogram%26entry.906535625%3DXilin%2520Gong%2520and%2520Yongkai%2520Chen%2520and%2520Shushan%2520Wu%2520and%2520Fang%2520Wang%2520and%2520Ping%2520Ma%2520and%2520Wenxuan%2520Zhong%26entry.1292438233%3D%2520%2520Echocardiogram%2520is%2520the%2520most%2520commonly%2520used%2520imaging%2520modality%2520in%2520cardiac%250Aassessment%2520duo%2520to%2520its%2520non-invasive%2520nature%252C%2520real-time%2520capability%252C%2520and%250Acost-effectiveness.%2520Despite%2520its%2520advantages%252C%2520most%2520clinical%2520echocardiograms%250Aprovide%2520only%2520two-dimensional%2520views%252C%2520limiting%2520the%2520ability%2520to%2520fully%2520assess%250Acardiac%2520anatomy%2520and%2520function%2520in%2520three%2520dimensions.%2520While%2520three-dimensional%250Aechocardiography%2520exists%252C%2520it%2520often%2520suffers%2520from%2520reduced%2520resolution%252C%2520limited%250Aavailability%252C%2520and%2520higher%2520acquisition%2520costs.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520deep%2520learning%2520framework%2520S2MNet%2520that%2520reconstructs%2520continuous%2520and%250Ahigh-fidelity%25203D%2520heart%2520models%2520by%2520integrating%2520six%2520slices%2520of%2520routinely%2520acquired%250A2D%2520echocardiogram%2520views.%2520Our%2520method%2520has%2520three%2520advantages.%2520First%252C%2520our%2520method%250Aavoid%2520the%2520difficulties%2520on%2520training%2520data%2520acquasition%2520by%2520simulate%2520six%2520of%25202D%250Aechocardiogram%2520images%2520from%2520corresponding%2520slices%2520of%2520a%2520given%25203D%2520heart%2520mesh.%250ASecond%252C%2520we%2520introduce%2520a%2520deformation%2520field-based%2520method%252C%2520which%2520avoid%2520spatial%250Adiscontinuities%2520or%2520structural%2520artifacts%2520in%25203D%2520echocardiogram%2520reconstructions.%250AWe%2520validate%2520our%2520method%2520using%2520clinically%2520collected%2520echocardiogram%2520and%250Ademonstrate%2520that%2520our%2520estimated%2520left%2520ventricular%2520volume%252C%2520a%2520key%2520clinical%250Aindicator%2520of%2520cardiac%2520function%252C%2520is%2520strongly%2520correlated%2520with%2520the%2520doctor%2520measured%250AGLPS%252C%2520a%2520clinical%2520measurement%2520that%2520should%2520demonstrate%2520a%2520negative%2520correlation%250Awith%2520LVE%2520in%2520medical%2520theory.%2520This%2520association%2520confirms%2520the%2520reliability%2520of%2520our%250Aproposed%25203D%2520construction%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2MNet%3A%20Speckle-To-Mesh%20Net%20for%20Three-Dimensional%20Cardiac%20Morphology%0A%20%20Reconstruction%20via%20Echocardiogram&entry.906535625=Xilin%20Gong%20and%20Yongkai%20Chen%20and%20Shushan%20Wu%20and%20Fang%20Wang%20and%20Ping%20Ma%20and%20Wenxuan%20Zhong&entry.1292438233=%20%20Echocardiogram%20is%20the%20most%20commonly%20used%20imaging%20modality%20in%20cardiac%0Aassessment%20duo%20to%20its%20non-invasive%20nature%2C%20real-time%20capability%2C%20and%0Acost-effectiveness.%20Despite%20its%20advantages%2C%20most%20clinical%20echocardiograms%0Aprovide%20only%20two-dimensional%20views%2C%20limiting%20the%20ability%20to%20fully%20assess%0Acardiac%20anatomy%20and%20function%20in%20three%20dimensions.%20While%20three-dimensional%0Aechocardiography%20exists%2C%20it%20often%20suffers%20from%20reduced%20resolution%2C%20limited%0Aavailability%2C%20and%20higher%20acquisition%20costs.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20a%20deep%20learning%20framework%20S2MNet%20that%20reconstructs%20continuous%20and%0Ahigh-fidelity%203D%20heart%20models%20by%20integrating%20six%20slices%20of%20routinely%20acquired%0A2D%20echocardiogram%20views.%20Our%20method%20has%20three%20advantages.%20First%2C%20our%20method%0Aavoid%20the%20difficulties%20on%20training%20data%20acquasition%20by%20simulate%20six%20of%202D%0Aechocardiogram%20images%20from%20corresponding%20slices%20of%20a%20given%203D%20heart%20mesh.%0ASecond%2C%20we%20introduce%20a%20deformation%20field-based%20method%2C%20which%20avoid%20spatial%0Adiscontinuities%20or%20structural%20artifacts%20in%203D%20echocardiogram%20reconstructions.%0AWe%20validate%20our%20method%20using%20clinically%20collected%20echocardiogram%20and%0Ademonstrate%20that%20our%20estimated%20left%20ventricular%20volume%2C%20a%20key%20clinical%0Aindicator%20of%20cardiac%20function%2C%20is%20strongly%20correlated%20with%20the%20doctor%20measured%0AGLPS%2C%20a%20clinical%20measurement%20that%20should%20demonstrate%20a%20negative%20correlation%0Awith%20LVE%20in%20medical%20theory.%20This%20association%20confirms%20the%20reliability%20of%20our%0Aproposed%203D%20construction%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06105v1&entry.124074799=Read"},
{"title": "Locality-aware Cross-modal Correspondence Learning for Dense\n  Audio-Visual Events Localization", "author": "Ling Xing and Hongyu Qu and Rui Yan and Xiangbo Shu and Jinhui Tang", "abstract": "  Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that are both audible and\nvisible in a long video, where events may co-occur and exhibit varying\ndurations. However, complex audio-visual scenes often involve asynchronization\nbetween modalities, making accurate localization challenging. Existing DAVE\nsolutions extract audio and visual features through unimodal encoders, and fuse\nthem via dense cross-modal interaction. However, independent unimodal encoding\nstruggles to emphasize shared semantics between modalities without cross-modal\nguidance, while dense cross-modal attention may over-attend to semantically\nunrelated audio-visual features. To address these problems, we present LoCo, a\nLocality-aware cross-modal Correspondence learning framework for DAVE. LoCo\nleverages the local temporal continuity of audio-visual events as important\nguidance to filter irrelevant cross-modal signals and enhance cross-modal\nalignment throughout both unimodal and cross-modal encoding stages. i)\nSpecifically, LoCo applies Local Correspondence Feature (LCF) Modulation to\nenforce unimodal encoders to focus on modality-shared semantics by modulating\nagreement between audio and visual features based on local cross-modal\ncoherence. ii) To better aggregate cross-modal relevant features, we further\ncustomize Local Adaptive Cross-modal (LAC) Interaction, which dynamically\nadjusts attention regions in a data-driven manner. This adaptive mechanism\nfocuses attention on local event boundaries and accommodates varying event\ndurations. By incorporating LCF and LAC, LoCo provides solid performance gains\nand outperforms existing DAVE methods.\n", "link": "http://arxiv.org/abs/2409.07967v4", "date": "2025-05-09", "relevancy": 2.7701, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&body=Title%3A%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization%0AAuthor%3A%20Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20are%20both%20audible%20and%0Avisible%20in%20a%20long%20video%2C%20where%20events%20may%20co-occur%20and%20exhibit%20varying%0Adurations.%20However%2C%20complex%20audio-visual%20scenes%20often%20involve%20asynchronization%0Abetween%20modalities%2C%20making%20accurate%20localization%20challenging.%20Existing%20DAVE%0Asolutions%20extract%20audio%20and%20visual%20features%20through%20unimodal%20encoders%2C%20and%20fuse%0Athem%20via%20dense%20cross-modal%20interaction.%20However%2C%20independent%20unimodal%20encoding%0Astruggles%20to%20emphasize%20shared%20semantics%20between%20modalities%20without%20cross-modal%0Aguidance%2C%20while%20dense%20cross-modal%20attention%20may%20over-attend%20to%20semantically%0Aunrelated%20audio-visual%20features.%20To%20address%20these%20problems%2C%20we%20present%20LoCo%2C%20a%0ALocality-aware%20cross-modal%20Correspondence%20learning%20framework%20for%20DAVE.%20LoCo%0Aleverages%20the%20local%20temporal%20continuity%20of%20audio-visual%20events%20as%20important%0Aguidance%20to%20filter%20irrelevant%20cross-modal%20signals%20and%20enhance%20cross-modal%0Aalignment%20throughout%20both%20unimodal%20and%20cross-modal%20encoding%20stages.%20i%29%0ASpecifically%2C%20LoCo%20applies%20Local%20Correspondence%20Feature%20%28LCF%29%20Modulation%20to%0Aenforce%20unimodal%20encoders%20to%20focus%20on%20modality-shared%20semantics%20by%20modulating%0Aagreement%20between%20audio%20and%20visual%20features%20based%20on%20local%20cross-modal%0Acoherence.%20ii%29%20To%20better%20aggregate%20cross-modal%20relevant%20features%2C%20we%20further%0Acustomize%20Local%20Adaptive%20Cross-modal%20%28LAC%29%20Interaction%2C%20which%20dynamically%0Aadjusts%20attention%20regions%20in%20a%20data-driven%20manner.%20This%20adaptive%20mechanism%0Afocuses%20attention%20on%20local%20event%20boundaries%20and%20accommodates%20varying%20event%0Adurations.%20By%20incorporating%20LCF%20and%20LAC%2C%20LoCo%20provides%20solid%20performance%20gains%0Aand%20outperforms%20existing%20DAVE%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07967v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality-aware%2520Cross-modal%2520Correspondence%2520Learning%2520for%2520Dense%250A%2520%2520Audio-Visual%2520Events%2520Localization%26entry.906535625%3DLing%2520Xing%2520and%2520Hongyu%2520Qu%2520and%2520Rui%2520Yan%2520and%2520Xiangbo%2520Shu%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Dense-localization%2520Audio-Visual%2520Events%2520%2528DAVE%2529%2520aims%2520to%2520identify%2520time%250Aboundaries%2520and%2520corresponding%2520categories%2520for%2520events%2520that%2520are%2520both%2520audible%2520and%250Avisible%2520in%2520a%2520long%2520video%252C%2520where%2520events%2520may%2520co-occur%2520and%2520exhibit%2520varying%250Adurations.%2520However%252C%2520complex%2520audio-visual%2520scenes%2520often%2520involve%2520asynchronization%250Abetween%2520modalities%252C%2520making%2520accurate%2520localization%2520challenging.%2520Existing%2520DAVE%250Asolutions%2520extract%2520audio%2520and%2520visual%2520features%2520through%2520unimodal%2520encoders%252C%2520and%2520fuse%250Athem%2520via%2520dense%2520cross-modal%2520interaction.%2520However%252C%2520independent%2520unimodal%2520encoding%250Astruggles%2520to%2520emphasize%2520shared%2520semantics%2520between%2520modalities%2520without%2520cross-modal%250Aguidance%252C%2520while%2520dense%2520cross-modal%2520attention%2520may%2520over-attend%2520to%2520semantically%250Aunrelated%2520audio-visual%2520features.%2520To%2520address%2520these%2520problems%252C%2520we%2520present%2520LoCo%252C%2520a%250ALocality-aware%2520cross-modal%2520Correspondence%2520learning%2520framework%2520for%2520DAVE.%2520LoCo%250Aleverages%2520the%2520local%2520temporal%2520continuity%2520of%2520audio-visual%2520events%2520as%2520important%250Aguidance%2520to%2520filter%2520irrelevant%2520cross-modal%2520signals%2520and%2520enhance%2520cross-modal%250Aalignment%2520throughout%2520both%2520unimodal%2520and%2520cross-modal%2520encoding%2520stages.%2520i%2529%250ASpecifically%252C%2520LoCo%2520applies%2520Local%2520Correspondence%2520Feature%2520%2528LCF%2529%2520Modulation%2520to%250Aenforce%2520unimodal%2520encoders%2520to%2520focus%2520on%2520modality-shared%2520semantics%2520by%2520modulating%250Aagreement%2520between%2520audio%2520and%2520visual%2520features%2520based%2520on%2520local%2520cross-modal%250Acoherence.%2520ii%2529%2520To%2520better%2520aggregate%2520cross-modal%2520relevant%2520features%252C%2520we%2520further%250Acustomize%2520Local%2520Adaptive%2520Cross-modal%2520%2528LAC%2529%2520Interaction%252C%2520which%2520dynamically%250Aadjusts%2520attention%2520regions%2520in%2520a%2520data-driven%2520manner.%2520This%2520adaptive%2520mechanism%250Afocuses%2520attention%2520on%2520local%2520event%2520boundaries%2520and%2520accommodates%2520varying%2520event%250Adurations.%2520By%2520incorporating%2520LCF%2520and%2520LAC%252C%2520LoCo%2520provides%2520solid%2520performance%2520gains%250Aand%2520outperforms%2520existing%2520DAVE%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07967v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&entry.906535625=Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang&entry.1292438233=%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20are%20both%20audible%20and%0Avisible%20in%20a%20long%20video%2C%20where%20events%20may%20co-occur%20and%20exhibit%20varying%0Adurations.%20However%2C%20complex%20audio-visual%20scenes%20often%20involve%20asynchronization%0Abetween%20modalities%2C%20making%20accurate%20localization%20challenging.%20Existing%20DAVE%0Asolutions%20extract%20audio%20and%20visual%20features%20through%20unimodal%20encoders%2C%20and%20fuse%0Athem%20via%20dense%20cross-modal%20interaction.%20However%2C%20independent%20unimodal%20encoding%0Astruggles%20to%20emphasize%20shared%20semantics%20between%20modalities%20without%20cross-modal%0Aguidance%2C%20while%20dense%20cross-modal%20attention%20may%20over-attend%20to%20semantically%0Aunrelated%20audio-visual%20features.%20To%20address%20these%20problems%2C%20we%20present%20LoCo%2C%20a%0ALocality-aware%20cross-modal%20Correspondence%20learning%20framework%20for%20DAVE.%20LoCo%0Aleverages%20the%20local%20temporal%20continuity%20of%20audio-visual%20events%20as%20important%0Aguidance%20to%20filter%20irrelevant%20cross-modal%20signals%20and%20enhance%20cross-modal%0Aalignment%20throughout%20both%20unimodal%20and%20cross-modal%20encoding%20stages.%20i%29%0ASpecifically%2C%20LoCo%20applies%20Local%20Correspondence%20Feature%20%28LCF%29%20Modulation%20to%0Aenforce%20unimodal%20encoders%20to%20focus%20on%20modality-shared%20semantics%20by%20modulating%0Aagreement%20between%20audio%20and%20visual%20features%20based%20on%20local%20cross-modal%0Acoherence.%20ii%29%20To%20better%20aggregate%20cross-modal%20relevant%20features%2C%20we%20further%0Acustomize%20Local%20Adaptive%20Cross-modal%20%28LAC%29%20Interaction%2C%20which%20dynamically%0Aadjusts%20attention%20regions%20in%20a%20data-driven%20manner.%20This%20adaptive%20mechanism%0Afocuses%20attention%20on%20local%20event%20boundaries%20and%20accommodates%20varying%20event%0Adurations.%20By%20incorporating%20LCF%20and%20LAC%2C%20LoCo%20provides%20solid%20performance%20gains%0Aand%20outperforms%20existing%20DAVE%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07967v4&entry.124074799=Read"},
{"title": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation", "author": "Julio Silva-Rodr\u00edguez and Jose Dolz and Ismail Ben Ayed", "abstract": "  The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and\nparameter-efficiency during adaptation. Building on a foundation model\npre-trained on open-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios.\n", "link": "http://arxiv.org/abs/2303.17051v4", "date": "2025-05-09", "relevancy": 2.7622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5598}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20and%20Few-Shot%20Parameter-Efficient%20Fine-Tuning%0A%20%20for%20Volumetric%20Organ%20Segmentation&body=Title%3A%20Towards%20Foundation%20Models%20and%20Few-Shot%20Parameter-Efficient%20Fine-Tuning%0A%20%20for%20Volumetric%20Organ%20Segmentation%0AAuthor%3A%20Julio%20Silva-Rodr%C3%ADguez%20and%20Jose%20Dolz%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20The%20recent%20popularity%20of%20foundation%20models%20and%20the%20pre-train-and-adapt%0Aparadigm%2C%20where%20a%20large-scale%20model%20is%20transferred%20to%20downstream%20tasks%2C%20is%0Againing%20attention%20for%20volumetric%20medical%20image%20segmentation.%20However%2C%20current%0Atransfer%20learning%20strategies%20devoted%20to%20full%20fine-tuning%20for%20transfer%20learning%0Amay%20require%20significant%20resources%20and%20yield%20sub-optimal%20results%20when%20the%0Alabeled%20data%20of%20the%20target%20task%20is%20scarce.%20This%20makes%20its%20applicability%20in%20real%0Aclinical%20settings%20challenging%20since%20these%20institutions%20are%20usually%20constrained%0Aon%20data%20and%20computational%20resources%20to%20develop%20proprietary%20solutions.%20To%0Aaddress%20this%20challenge%2C%20we%20formalize%20Few-Shot%20Efficient%20Fine-Tuning%20%28FSEFT%29%2C%20a%0Anovel%20and%20realistic%20scenario%20for%20adapting%20medical%20image%20segmentation%20foundation%0Amodels.%20This%20setting%20considers%20the%20key%20role%20of%20both%20data-%20and%0Aparameter-efficiency%20during%20adaptation.%20Building%20on%20a%20foundation%20model%0Apre-trained%20on%20open-access%20CT%20organ%20segmentation%20sources%2C%20we%20propose%20leveraging%0AParameter-Efficient%20Fine-Tuning%20and%20black-box%20Adapters%20to%20address%20such%0Achallenges.%20Furthermore%2C%20novel%20efficient%20adaptation%20methodologies%20are%0Aintroduced%20in%20this%20work%2C%20which%20include%20Spatial%20black-box%20Adapters%20that%20are%20more%0Aappropriate%20for%20dense%20prediction%20tasks%20and%20constrained%20transductive%20inference%2C%0Aleveraging%20task-specific%20prior%20knowledge.%20Our%20comprehensive%20transfer%20learning%0Aexperiments%20confirm%20the%20suitability%20of%20foundation%20models%20in%20medical%20image%0Asegmentation%20and%20unveil%20the%20limitations%20of%20popular%20fine-tuning%20strategies%20in%0Afew-shot%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17051v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520and%2520Few-Shot%2520Parameter-Efficient%2520Fine-Tuning%250A%2520%2520for%2520Volumetric%2520Organ%2520Segmentation%26entry.906535625%3DJulio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Jose%2520Dolz%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520The%2520recent%2520popularity%2520of%2520foundation%2520models%2520and%2520the%2520pre-train-and-adapt%250Aparadigm%252C%2520where%2520a%2520large-scale%2520model%2520is%2520transferred%2520to%2520downstream%2520tasks%252C%2520is%250Againing%2520attention%2520for%2520volumetric%2520medical%2520image%2520segmentation.%2520However%252C%2520current%250Atransfer%2520learning%2520strategies%2520devoted%2520to%2520full%2520fine-tuning%2520for%2520transfer%2520learning%250Amay%2520require%2520significant%2520resources%2520and%2520yield%2520sub-optimal%2520results%2520when%2520the%250Alabeled%2520data%2520of%2520the%2520target%2520task%2520is%2520scarce.%2520This%2520makes%2520its%2520applicability%2520in%2520real%250Aclinical%2520settings%2520challenging%2520since%2520these%2520institutions%2520are%2520usually%2520constrained%250Aon%2520data%2520and%2520computational%2520resources%2520to%2520develop%2520proprietary%2520solutions.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520formalize%2520Few-Shot%2520Efficient%2520Fine-Tuning%2520%2528FSEFT%2529%252C%2520a%250Anovel%2520and%2520realistic%2520scenario%2520for%2520adapting%2520medical%2520image%2520segmentation%2520foundation%250Amodels.%2520This%2520setting%2520considers%2520the%2520key%2520role%2520of%2520both%2520data-%2520and%250Aparameter-efficiency%2520during%2520adaptation.%2520Building%2520on%2520a%2520foundation%2520model%250Apre-trained%2520on%2520open-access%2520CT%2520organ%2520segmentation%2520sources%252C%2520we%2520propose%2520leveraging%250AParameter-Efficient%2520Fine-Tuning%2520and%2520black-box%2520Adapters%2520to%2520address%2520such%250Achallenges.%2520Furthermore%252C%2520novel%2520efficient%2520adaptation%2520methodologies%2520are%250Aintroduced%2520in%2520this%2520work%252C%2520which%2520include%2520Spatial%2520black-box%2520Adapters%2520that%2520are%2520more%250Aappropriate%2520for%2520dense%2520prediction%2520tasks%2520and%2520constrained%2520transductive%2520inference%252C%250Aleveraging%2520task-specific%2520prior%2520knowledge.%2520Our%2520comprehensive%2520transfer%2520learning%250Aexperiments%2520confirm%2520the%2520suitability%2520of%2520foundation%2520models%2520in%2520medical%2520image%250Asegmentation%2520and%2520unveil%2520the%2520limitations%2520of%2520popular%2520fine-tuning%2520strategies%2520in%250Afew-shot%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17051v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20and%20Few-Shot%20Parameter-Efficient%20Fine-Tuning%0A%20%20for%20Volumetric%20Organ%20Segmentation&entry.906535625=Julio%20Silva-Rodr%C3%ADguez%20and%20Jose%20Dolz%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20The%20recent%20popularity%20of%20foundation%20models%20and%20the%20pre-train-and-adapt%0Aparadigm%2C%20where%20a%20large-scale%20model%20is%20transferred%20to%20downstream%20tasks%2C%20is%0Againing%20attention%20for%20volumetric%20medical%20image%20segmentation.%20However%2C%20current%0Atransfer%20learning%20strategies%20devoted%20to%20full%20fine-tuning%20for%20transfer%20learning%0Amay%20require%20significant%20resources%20and%20yield%20sub-optimal%20results%20when%20the%0Alabeled%20data%20of%20the%20target%20task%20is%20scarce.%20This%20makes%20its%20applicability%20in%20real%0Aclinical%20settings%20challenging%20since%20these%20institutions%20are%20usually%20constrained%0Aon%20data%20and%20computational%20resources%20to%20develop%20proprietary%20solutions.%20To%0Aaddress%20this%20challenge%2C%20we%20formalize%20Few-Shot%20Efficient%20Fine-Tuning%20%28FSEFT%29%2C%20a%0Anovel%20and%20realistic%20scenario%20for%20adapting%20medical%20image%20segmentation%20foundation%0Amodels.%20This%20setting%20considers%20the%20key%20role%20of%20both%20data-%20and%0Aparameter-efficiency%20during%20adaptation.%20Building%20on%20a%20foundation%20model%0Apre-trained%20on%20open-access%20CT%20organ%20segmentation%20sources%2C%20we%20propose%20leveraging%0AParameter-Efficient%20Fine-Tuning%20and%20black-box%20Adapters%20to%20address%20such%0Achallenges.%20Furthermore%2C%20novel%20efficient%20adaptation%20methodologies%20are%0Aintroduced%20in%20this%20work%2C%20which%20include%20Spatial%20black-box%20Adapters%20that%20are%20more%0Aappropriate%20for%20dense%20prediction%20tasks%20and%20constrained%20transductive%20inference%2C%0Aleveraging%20task-specific%20prior%20knowledge.%20Our%20comprehensive%20transfer%20learning%0Aexperiments%20confirm%20the%20suitability%20of%20foundation%20models%20in%20medical%20image%0Asegmentation%20and%20unveil%20the%20limitations%20of%20popular%20fine-tuning%20strategies%20in%0Afew-shot%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17051v4&entry.124074799=Read"},
{"title": "Adapting a Segmentation Foundation Model for Medical Image\n  Classification", "author": "Pengfei Gu and Haoteng Tang and Islam A. Ebeid and Jose A. Nunez and Fabian Vazquez and Diego Adame and Marcus Zhan and Huimin Li and Bin Fu and Danny Z. Chen", "abstract": "  Recent advancements in foundation models, such as the Segment Anything Model\n(SAM), have shown strong performance in various vision tasks, particularly\nimage segmentation, due to their impressive zero-shot segmentation\ncapabilities. However, effectively adapting such models for medical image\nclassification is still a less explored topic. In this paper, we introduce a\nnew framework to adapt SAM for medical image classification. First, we utilize\nthe SAM image encoder as a feature extractor to capture segmentation-based\nfeatures that convey important spatial and contextual details of the image,\nwhile freezing its weights to avoid unnecessary overhead during training. Next,\nwe propose a novel Spatially Localized Channel Attention (SLCA) mechanism to\ncompute spatially localized attention weights for the feature maps. The\nfeatures extracted from SAM's image encoder are processed through SLCA to\ncompute attention weights, which are then integrated into deep learning\nclassification models to enhance their focus on spatially relevant or\nmeaningful regions of the image, thus improving classification performance.\nExperimental results on three public medical image classification datasets\ndemonstrate the effectiveness and data-efficiency of our approach.\n", "link": "http://arxiv.org/abs/2505.06217v1", "date": "2025-05-09", "relevancy": 2.7601, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20a%20Segmentation%20Foundation%20Model%20for%20Medical%20Image%0A%20%20Classification&body=Title%3A%20Adapting%20a%20Segmentation%20Foundation%20Model%20for%20Medical%20Image%0A%20%20Classification%0AAuthor%3A%20Pengfei%20Gu%20and%20Haoteng%20Tang%20and%20Islam%20A.%20Ebeid%20and%20Jose%20A.%20Nunez%20and%20Fabian%20Vazquez%20and%20Diego%20Adame%20and%20Marcus%20Zhan%20and%20Huimin%20Li%20and%20Bin%20Fu%20and%20Danny%20Z.%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20shown%20strong%20performance%20in%20various%20vision%20tasks%2C%20particularly%0Aimage%20segmentation%2C%20due%20to%20their%20impressive%20zero-shot%20segmentation%0Acapabilities.%20However%2C%20effectively%20adapting%20such%20models%20for%20medical%20image%0Aclassification%20is%20still%20a%20less%20explored%20topic.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20framework%20to%20adapt%20SAM%20for%20medical%20image%20classification.%20First%2C%20we%20utilize%0Athe%20SAM%20image%20encoder%20as%20a%20feature%20extractor%20to%20capture%20segmentation-based%0Afeatures%20that%20convey%20important%20spatial%20and%20contextual%20details%20of%20the%20image%2C%0Awhile%20freezing%20its%20weights%20to%20avoid%20unnecessary%20overhead%20during%20training.%20Next%2C%0Awe%20propose%20a%20novel%20Spatially%20Localized%20Channel%20Attention%20%28SLCA%29%20mechanism%20to%0Acompute%20spatially%20localized%20attention%20weights%20for%20the%20feature%20maps.%20The%0Afeatures%20extracted%20from%20SAM%27s%20image%20encoder%20are%20processed%20through%20SLCA%20to%0Acompute%20attention%20weights%2C%20which%20are%20then%20integrated%20into%20deep%20learning%0Aclassification%20models%20to%20enhance%20their%20focus%20on%20spatially%20relevant%20or%0Ameaningful%20regions%20of%20the%20image%2C%20thus%20improving%20classification%20performance.%0AExperimental%20results%20on%20three%20public%20medical%20image%20classification%20datasets%0Ademonstrate%20the%20effectiveness%20and%20data-efficiency%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520a%2520Segmentation%2520Foundation%2520Model%2520for%2520Medical%2520Image%250A%2520%2520Classification%26entry.906535625%3DPengfei%2520Gu%2520and%2520Haoteng%2520Tang%2520and%2520Islam%2520A.%2520Ebeid%2520and%2520Jose%2520A.%2520Nunez%2520and%2520Fabian%2520Vazquez%2520and%2520Diego%2520Adame%2520and%2520Marcus%2520Zhan%2520and%2520Huimin%2520Li%2520and%2520Bin%2520Fu%2520and%2520Danny%2520Z.%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%252C%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520have%2520shown%2520strong%2520performance%2520in%2520various%2520vision%2520tasks%252C%2520particularly%250Aimage%2520segmentation%252C%2520due%2520to%2520their%2520impressive%2520zero-shot%2520segmentation%250Acapabilities.%2520However%252C%2520effectively%2520adapting%2520such%2520models%2520for%2520medical%2520image%250Aclassification%2520is%2520still%2520a%2520less%2520explored%2520topic.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anew%2520framework%2520to%2520adapt%2520SAM%2520for%2520medical%2520image%2520classification.%2520First%252C%2520we%2520utilize%250Athe%2520SAM%2520image%2520encoder%2520as%2520a%2520feature%2520extractor%2520to%2520capture%2520segmentation-based%250Afeatures%2520that%2520convey%2520important%2520spatial%2520and%2520contextual%2520details%2520of%2520the%2520image%252C%250Awhile%2520freezing%2520its%2520weights%2520to%2520avoid%2520unnecessary%2520overhead%2520during%2520training.%2520Next%252C%250Awe%2520propose%2520a%2520novel%2520Spatially%2520Localized%2520Channel%2520Attention%2520%2528SLCA%2529%2520mechanism%2520to%250Acompute%2520spatially%2520localized%2520attention%2520weights%2520for%2520the%2520feature%2520maps.%2520The%250Afeatures%2520extracted%2520from%2520SAM%2527s%2520image%2520encoder%2520are%2520processed%2520through%2520SLCA%2520to%250Acompute%2520attention%2520weights%252C%2520which%2520are%2520then%2520integrated%2520into%2520deep%2520learning%250Aclassification%2520models%2520to%2520enhance%2520their%2520focus%2520on%2520spatially%2520relevant%2520or%250Ameaningful%2520regions%2520of%2520the%2520image%252C%2520thus%2520improving%2520classification%2520performance.%250AExperimental%2520results%2520on%2520three%2520public%2520medical%2520image%2520classification%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520and%2520data-efficiency%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20a%20Segmentation%20Foundation%20Model%20for%20Medical%20Image%0A%20%20Classification&entry.906535625=Pengfei%20Gu%20and%20Haoteng%20Tang%20and%20Islam%20A.%20Ebeid%20and%20Jose%20A.%20Nunez%20and%20Fabian%20Vazquez%20and%20Diego%20Adame%20and%20Marcus%20Zhan%20and%20Huimin%20Li%20and%20Bin%20Fu%20and%20Danny%20Z.%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20shown%20strong%20performance%20in%20various%20vision%20tasks%2C%20particularly%0Aimage%20segmentation%2C%20due%20to%20their%20impressive%20zero-shot%20segmentation%0Acapabilities.%20However%2C%20effectively%20adapting%20such%20models%20for%20medical%20image%0Aclassification%20is%20still%20a%20less%20explored%20topic.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20framework%20to%20adapt%20SAM%20for%20medical%20image%20classification.%20First%2C%20we%20utilize%0Athe%20SAM%20image%20encoder%20as%20a%20feature%20extractor%20to%20capture%20segmentation-based%0Afeatures%20that%20convey%20important%20spatial%20and%20contextual%20details%20of%20the%20image%2C%0Awhile%20freezing%20its%20weights%20to%20avoid%20unnecessary%20overhead%20during%20training.%20Next%2C%0Awe%20propose%20a%20novel%20Spatially%20Localized%20Channel%20Attention%20%28SLCA%29%20mechanism%20to%0Acompute%20spatially%20localized%20attention%20weights%20for%20the%20feature%20maps.%20The%0Afeatures%20extracted%20from%20SAM%27s%20image%20encoder%20are%20processed%20through%20SLCA%20to%0Acompute%20attention%20weights%2C%20which%20are%20then%20integrated%20into%20deep%20learning%0Aclassification%20models%20to%20enhance%20their%20focus%20on%20spatially%20relevant%20or%0Ameaningful%20regions%20of%20the%20image%2C%20thus%20improving%20classification%20performance.%0AExperimental%20results%20on%20three%20public%20medical%20image%20classification%20datasets%0Ademonstrate%20the%20effectiveness%20and%20data-efficiency%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06217v1&entry.124074799=Read"},
{"title": "Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp\n  Segmentation", "author": "Diego Adame and Jose A. Nunez and Fabian Vazquez and Nayeli Gurrola and Huimin Li and Haoteng Tang and Bin Fu and Pengfei Gu", "abstract": "  Convolutional neural network (CNN) and Transformer-based architectures are\ntwo dominant deep learning models for polyp segmentation. However, CNNs have\nlimited capability for modeling long-range dependencies, while Transformers\nincur quadratic computational complexity. Recently, State Space Models such as\nMamba have been recognized as a promising approach for polyp segmentation\nbecause they not only model long-range interactions effectively but also\nmaintain linear computational complexity. However, Mamba-based architectures\nstill struggle to capture topological features (e.g., connected components,\nloops, voids), leading to inaccurate boundary delineation and polyp\nsegmentation. To address these limitations, we propose a new approach called\nTopo-VM-UNetV2, which encodes topological features into the Mamba-based\nstate-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of\ntwo stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for\nthe training and test images, which are then used to compute topology attention\nmaps. Specifically, we first compute persistence diagrams of the PMs, then we\ngenerate persistence score maps by assigning persistence values (i.e., the\ndifference between death and birth times) of each topological feature to its\nbirth location, finally we transform persistence scores into attention weights\nusing the sigmoid function. Stage 2: These topology attention maps are\nintegrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to\nform a topology-guided semantics and detail infusion (Topo-SDI) module for\nenhancing the segmentation results. Extensive experiments on five public polyp\nsegmentation datasets demonstrate the effectiveness of our proposed method. The\ncode will be made publicly available.\n", "link": "http://arxiv.org/abs/2505.06210v1", "date": "2025-05-09", "relevancy": 2.7176, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5693}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topo-VM-UNetV2%3A%20Encoding%20Topology%20into%20Vision%20Mamba%20UNet%20for%20Polyp%0A%20%20Segmentation&body=Title%3A%20Topo-VM-UNetV2%3A%20Encoding%20Topology%20into%20Vision%20Mamba%20UNet%20for%20Polyp%0A%20%20Segmentation%0AAuthor%3A%20Diego%20Adame%20and%20Jose%20A.%20Nunez%20and%20Fabian%20Vazquez%20and%20Nayeli%20Gurrola%20and%20Huimin%20Li%20and%20Haoteng%20Tang%20and%20Bin%20Fu%20and%20Pengfei%20Gu%0AAbstract%3A%20%20%20Convolutional%20neural%20network%20%28CNN%29%20and%20Transformer-based%20architectures%20are%0Atwo%20dominant%20deep%20learning%20models%20for%20polyp%20segmentation.%20However%2C%20CNNs%20have%0Alimited%20capability%20for%20modeling%20long-range%20dependencies%2C%20while%20Transformers%0Aincur%20quadratic%20computational%20complexity.%20Recently%2C%20State%20Space%20Models%20such%20as%0AMamba%20have%20been%20recognized%20as%20a%20promising%20approach%20for%20polyp%20segmentation%0Abecause%20they%20not%20only%20model%20long-range%20interactions%20effectively%20but%20also%0Amaintain%20linear%20computational%20complexity.%20However%2C%20Mamba-based%20architectures%0Astill%20struggle%20to%20capture%20topological%20features%20%28e.g.%2C%20connected%20components%2C%0Aloops%2C%20voids%29%2C%20leading%20to%20inaccurate%20boundary%20delineation%20and%20polyp%0Asegmentation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%20approach%20called%0ATopo-VM-UNetV2%2C%20which%20encodes%20topological%20features%20into%20the%20Mamba-based%0Astate-of-the-art%20polyp%20segmentation%20model%2C%20VM-UNetV2.%20Our%20method%20consists%20of%0Atwo%20stages%3A%20Stage%201%3A%20VM-UNetV2%20is%20used%20to%20generate%20probability%20maps%20%28PMs%29%20for%0Athe%20training%20and%20test%20images%2C%20which%20are%20then%20used%20to%20compute%20topology%20attention%0Amaps.%20Specifically%2C%20we%20first%20compute%20persistence%20diagrams%20of%20the%20PMs%2C%20then%20we%0Agenerate%20persistence%20score%20maps%20by%20assigning%20persistence%20values%20%28i.e.%2C%20the%0Adifference%20between%20death%20and%20birth%20times%29%20of%20each%20topological%20feature%20to%20its%0Abirth%20location%2C%20finally%20we%20transform%20persistence%20scores%20into%20attention%20weights%0Ausing%20the%20sigmoid%20function.%20Stage%202%3A%20These%20topology%20attention%20maps%20are%0Aintegrated%20into%20the%20semantics%20and%20detail%20infusion%20%28SDI%29%20module%20of%20VM-UNetV2%20to%0Aform%20a%20topology-guided%20semantics%20and%20detail%20infusion%20%28Topo-SDI%29%20module%20for%0Aenhancing%20the%20segmentation%20results.%20Extensive%20experiments%20on%20five%20public%20polyp%0Asegmentation%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopo-VM-UNetV2%253A%2520Encoding%2520Topology%2520into%2520Vision%2520Mamba%2520UNet%2520for%2520Polyp%250A%2520%2520Segmentation%26entry.906535625%3DDiego%2520Adame%2520and%2520Jose%2520A.%2520Nunez%2520and%2520Fabian%2520Vazquez%2520and%2520Nayeli%2520Gurrola%2520and%2520Huimin%2520Li%2520and%2520Haoteng%2520Tang%2520and%2520Bin%2520Fu%2520and%2520Pengfei%2520Gu%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520network%2520%2528CNN%2529%2520and%2520Transformer-based%2520architectures%2520are%250Atwo%2520dominant%2520deep%2520learning%2520models%2520for%2520polyp%2520segmentation.%2520However%252C%2520CNNs%2520have%250Alimited%2520capability%2520for%2520modeling%2520long-range%2520dependencies%252C%2520while%2520Transformers%250Aincur%2520quadratic%2520computational%2520complexity.%2520Recently%252C%2520State%2520Space%2520Models%2520such%2520as%250AMamba%2520have%2520been%2520recognized%2520as%2520a%2520promising%2520approach%2520for%2520polyp%2520segmentation%250Abecause%2520they%2520not%2520only%2520model%2520long-range%2520interactions%2520effectively%2520but%2520also%250Amaintain%2520linear%2520computational%2520complexity.%2520However%252C%2520Mamba-based%2520architectures%250Astill%2520struggle%2520to%2520capture%2520topological%2520features%2520%2528e.g.%252C%2520connected%2520components%252C%250Aloops%252C%2520voids%2529%252C%2520leading%2520to%2520inaccurate%2520boundary%2520delineation%2520and%2520polyp%250Asegmentation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520new%2520approach%2520called%250ATopo-VM-UNetV2%252C%2520which%2520encodes%2520topological%2520features%2520into%2520the%2520Mamba-based%250Astate-of-the-art%2520polyp%2520segmentation%2520model%252C%2520VM-UNetV2.%2520Our%2520method%2520consists%2520of%250Atwo%2520stages%253A%2520Stage%25201%253A%2520VM-UNetV2%2520is%2520used%2520to%2520generate%2520probability%2520maps%2520%2528PMs%2529%2520for%250Athe%2520training%2520and%2520test%2520images%252C%2520which%2520are%2520then%2520used%2520to%2520compute%2520topology%2520attention%250Amaps.%2520Specifically%252C%2520we%2520first%2520compute%2520persistence%2520diagrams%2520of%2520the%2520PMs%252C%2520then%2520we%250Agenerate%2520persistence%2520score%2520maps%2520by%2520assigning%2520persistence%2520values%2520%2528i.e.%252C%2520the%250Adifference%2520between%2520death%2520and%2520birth%2520times%2529%2520of%2520each%2520topological%2520feature%2520to%2520its%250Abirth%2520location%252C%2520finally%2520we%2520transform%2520persistence%2520scores%2520into%2520attention%2520weights%250Ausing%2520the%2520sigmoid%2520function.%2520Stage%25202%253A%2520These%2520topology%2520attention%2520maps%2520are%250Aintegrated%2520into%2520the%2520semantics%2520and%2520detail%2520infusion%2520%2528SDI%2529%2520module%2520of%2520VM-UNetV2%2520to%250Aform%2520a%2520topology-guided%2520semantics%2520and%2520detail%2520infusion%2520%2528Topo-SDI%2529%2520module%2520for%250Aenhancing%2520the%2520segmentation%2520results.%2520Extensive%2520experiments%2520on%2520five%2520public%2520polyp%250Asegmentation%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520The%250Acode%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topo-VM-UNetV2%3A%20Encoding%20Topology%20into%20Vision%20Mamba%20UNet%20for%20Polyp%0A%20%20Segmentation&entry.906535625=Diego%20Adame%20and%20Jose%20A.%20Nunez%20and%20Fabian%20Vazquez%20and%20Nayeli%20Gurrola%20and%20Huimin%20Li%20and%20Haoteng%20Tang%20and%20Bin%20Fu%20and%20Pengfei%20Gu&entry.1292438233=%20%20Convolutional%20neural%20network%20%28CNN%29%20and%20Transformer-based%20architectures%20are%0Atwo%20dominant%20deep%20learning%20models%20for%20polyp%20segmentation.%20However%2C%20CNNs%20have%0Alimited%20capability%20for%20modeling%20long-range%20dependencies%2C%20while%20Transformers%0Aincur%20quadratic%20computational%20complexity.%20Recently%2C%20State%20Space%20Models%20such%20as%0AMamba%20have%20been%20recognized%20as%20a%20promising%20approach%20for%20polyp%20segmentation%0Abecause%20they%20not%20only%20model%20long-range%20interactions%20effectively%20but%20also%0Amaintain%20linear%20computational%20complexity.%20However%2C%20Mamba-based%20architectures%0Astill%20struggle%20to%20capture%20topological%20features%20%28e.g.%2C%20connected%20components%2C%0Aloops%2C%20voids%29%2C%20leading%20to%20inaccurate%20boundary%20delineation%20and%20polyp%0Asegmentation.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%20approach%20called%0ATopo-VM-UNetV2%2C%20which%20encodes%20topological%20features%20into%20the%20Mamba-based%0Astate-of-the-art%20polyp%20segmentation%20model%2C%20VM-UNetV2.%20Our%20method%20consists%20of%0Atwo%20stages%3A%20Stage%201%3A%20VM-UNetV2%20is%20used%20to%20generate%20probability%20maps%20%28PMs%29%20for%0Athe%20training%20and%20test%20images%2C%20which%20are%20then%20used%20to%20compute%20topology%20attention%0Amaps.%20Specifically%2C%20we%20first%20compute%20persistence%20diagrams%20of%20the%20PMs%2C%20then%20we%0Agenerate%20persistence%20score%20maps%20by%20assigning%20persistence%20values%20%28i.e.%2C%20the%0Adifference%20between%20death%20and%20birth%20times%29%20of%20each%20topological%20feature%20to%20its%0Abirth%20location%2C%20finally%20we%20transform%20persistence%20scores%20into%20attention%20weights%0Ausing%20the%20sigmoid%20function.%20Stage%202%3A%20These%20topology%20attention%20maps%20are%0Aintegrated%20into%20the%20semantics%20and%20detail%20infusion%20%28SDI%29%20module%20of%20VM-UNetV2%20to%0Aform%20a%20topology-guided%20semantics%20and%20detail%20infusion%20%28Topo-SDI%29%20module%20for%0Aenhancing%20the%20segmentation%20results.%20Extensive%20experiments%20on%20five%20public%20polyp%0Asegmentation%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06210v1&entry.124074799=Read"},
{"title": "Structure-preserving contrastive learning for spatial time series", "author": "Yiru Jiao and Sander van Cranenburgh and Simeon Calvert and Hans van Lint", "abstract": "  The effectiveness of neural network models largely relies on learning\nmeaningful latent patterns from data, where self-supervised learning of\ninformative representations can enhance model performance and generalisability.\nHowever, self-supervised representation learning for spatially characterised\ntime series, which are ubiquitous in transportation domain, poses unique\nchallenges due to the necessity of maintaining fine-grained spatio-temporal\nsimilarities in the latent space. In this study, we introduce two\nstructure-preserving regularisers for the contrastive learning of spatial time\nseries: one regulariser preserves the topology of similarities between\ninstances, and the other preserves the graph geometry of similarities across\nspatial and temporal dimensions. To balance the contrastive learning objective\nand the need for structure preservation, we propose a dynamic weighting\nmechanism that adaptively manages this trade-off and stabilises training. We\nvalidate the proposed method through extensive experiments, including\nmultivariate time series classification to demonstrate its general\napplicability, as well as macroscopic and microscopic traffic prediction to\nhighlight its particular usefulness in encoding traffic interactions. Across\nall tasks, our method preserves the similarity structures more effectively and\nimproves state-of-the-art task performances. This method can be integrated with\nan arbitrary neural network model and is particularly beneficial for time\nseries data with spatial or geographical features. Furthermore, our findings\nsuggest that well-preserved similarity structures in the latent space indicate\nmore informative and useful representations. This provides insights to design\nmore effective neural networks for data-driven transportation research. Our\ncode is made openly accessible with all resulting data at\nhttps://github.com/yiru-jiao/spclt\n", "link": "http://arxiv.org/abs/2502.06380v3", "date": "2025-05-09", "relevancy": 2.6787, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&body=Title%3A%20Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series%0AAuthor%3A%20Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint%0AAbstract%3A%20%20%20The%20effectiveness%20of%20neural%20network%20models%20largely%20relies%20on%20learning%0Ameaningful%20latent%20patterns%20from%20data%2C%20where%20self-supervised%20learning%20of%0Ainformative%20representations%20can%20enhance%20model%20performance%20and%20generalisability.%0AHowever%2C%20self-supervised%20representation%20learning%20for%20spatially%20characterised%0Atime%20series%2C%20which%20are%20ubiquitous%20in%20transportation%20domain%2C%20poses%20unique%0Achallenges%20due%20to%20the%20necessity%20of%20maintaining%20fine-grained%20spatio-temporal%0Asimilarities%20in%20the%20latent%20space.%20In%20this%20study%2C%20we%20introduce%20two%0Astructure-preserving%20regularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%0Aseries%3A%20one%20regulariser%20preserves%20the%20topology%20of%20similarities%20between%0Ainstances%2C%20and%20the%20other%20preserves%20the%20graph%20geometry%20of%20similarities%20across%0Aspatial%20and%20temporal%20dimensions.%20To%20balance%20the%20contrastive%20learning%20objective%0Aand%20the%20need%20for%20structure%20preservation%2C%20we%20propose%20a%20dynamic%20weighting%0Amechanism%20that%20adaptively%20manages%20this%20trade-off%20and%20stabilises%20training.%20We%0Avalidate%20the%20proposed%20method%20through%20extensive%20experiments%2C%20including%0Amultivariate%20time%20series%20classification%20to%20demonstrate%20its%20general%0Aapplicability%2C%20as%20well%20as%20macroscopic%20and%20microscopic%20traffic%20prediction%20to%0Ahighlight%20its%20particular%20usefulness%20in%20encoding%20traffic%20interactions.%20Across%0Aall%20tasks%2C%20our%20method%20preserves%20the%20similarity%20structures%20more%20effectively%20and%0Aimproves%20state-of-the-art%20task%20performances.%20This%20method%20can%20be%20integrated%20with%0Aan%20arbitrary%20neural%20network%20model%20and%20is%20particularly%20beneficial%20for%20time%0Aseries%20data%20with%20spatial%20or%20geographical%20features.%20Furthermore%2C%20our%20findings%0Asuggest%20that%20well-preserved%20similarity%20structures%20in%20the%20latent%20space%20indicate%0Amore%20informative%20and%20useful%20representations.%20This%20provides%20insights%20to%20design%0Amore%20effective%20neural%20networks%20for%20data-driven%20transportation%20research.%20Our%0Acode%20is%20made%20openly%20accessible%20with%20all%20resulting%20data%20at%0Ahttps%3A//github.com/yiru-jiao/spclt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520contrastive%2520learning%2520for%2520spatial%2520time%2520series%26entry.906535625%3DYiru%2520Jiao%2520and%2520Sander%2520van%2520Cranenburgh%2520and%2520Simeon%2520Calvert%2520and%2520Hans%2520van%2520Lint%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520neural%2520network%2520models%2520largely%2520relies%2520on%2520learning%250Ameaningful%2520latent%2520patterns%2520from%2520data%252C%2520where%2520self-supervised%2520learning%2520of%250Ainformative%2520representations%2520can%2520enhance%2520model%2520performance%2520and%2520generalisability.%250AHowever%252C%2520self-supervised%2520representation%2520learning%2520for%2520spatially%2520characterised%250Atime%2520series%252C%2520which%2520are%2520ubiquitous%2520in%2520transportation%2520domain%252C%2520poses%2520unique%250Achallenges%2520due%2520to%2520the%2520necessity%2520of%2520maintaining%2520fine-grained%2520spatio-temporal%250Asimilarities%2520in%2520the%2520latent%2520space.%2520In%2520this%2520study%252C%2520we%2520introduce%2520two%250Astructure-preserving%2520regularisers%2520for%2520the%2520contrastive%2520learning%2520of%2520spatial%2520time%250Aseries%253A%2520one%2520regulariser%2520preserves%2520the%2520topology%2520of%2520similarities%2520between%250Ainstances%252C%2520and%2520the%2520other%2520preserves%2520the%2520graph%2520geometry%2520of%2520similarities%2520across%250Aspatial%2520and%2520temporal%2520dimensions.%2520To%2520balance%2520the%2520contrastive%2520learning%2520objective%250Aand%2520the%2520need%2520for%2520structure%2520preservation%252C%2520we%2520propose%2520a%2520dynamic%2520weighting%250Amechanism%2520that%2520adaptively%2520manages%2520this%2520trade-off%2520and%2520stabilises%2520training.%2520We%250Avalidate%2520the%2520proposed%2520method%2520through%2520extensive%2520experiments%252C%2520including%250Amultivariate%2520time%2520series%2520classification%2520to%2520demonstrate%2520its%2520general%250Aapplicability%252C%2520as%2520well%2520as%2520macroscopic%2520and%2520microscopic%2520traffic%2520prediction%2520to%250Ahighlight%2520its%2520particular%2520usefulness%2520in%2520encoding%2520traffic%2520interactions.%2520Across%250Aall%2520tasks%252C%2520our%2520method%2520preserves%2520the%2520similarity%2520structures%2520more%2520effectively%2520and%250Aimproves%2520state-of-the-art%2520task%2520performances.%2520This%2520method%2520can%2520be%2520integrated%2520with%250Aan%2520arbitrary%2520neural%2520network%2520model%2520and%2520is%2520particularly%2520beneficial%2520for%2520time%250Aseries%2520data%2520with%2520spatial%2520or%2520geographical%2520features.%2520Furthermore%252C%2520our%2520findings%250Asuggest%2520that%2520well-preserved%2520similarity%2520structures%2520in%2520the%2520latent%2520space%2520indicate%250Amore%2520informative%2520and%2520useful%2520representations.%2520This%2520provides%2520insights%2520to%2520design%250Amore%2520effective%2520neural%2520networks%2520for%2520data-driven%2520transportation%2520research.%2520Our%250Acode%2520is%2520made%2520openly%2520accessible%2520with%2520all%2520resulting%2520data%2520at%250Ahttps%253A//github.com/yiru-jiao/spclt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20contrastive%20learning%20for%20spatial%20time%20series&entry.906535625=Yiru%20Jiao%20and%20Sander%20van%20Cranenburgh%20and%20Simeon%20Calvert%20and%20Hans%20van%20Lint&entry.1292438233=%20%20The%20effectiveness%20of%20neural%20network%20models%20largely%20relies%20on%20learning%0Ameaningful%20latent%20patterns%20from%20data%2C%20where%20self-supervised%20learning%20of%0Ainformative%20representations%20can%20enhance%20model%20performance%20and%20generalisability.%0AHowever%2C%20self-supervised%20representation%20learning%20for%20spatially%20characterised%0Atime%20series%2C%20which%20are%20ubiquitous%20in%20transportation%20domain%2C%20poses%20unique%0Achallenges%20due%20to%20the%20necessity%20of%20maintaining%20fine-grained%20spatio-temporal%0Asimilarities%20in%20the%20latent%20space.%20In%20this%20study%2C%20we%20introduce%20two%0Astructure-preserving%20regularisers%20for%20the%20contrastive%20learning%20of%20spatial%20time%0Aseries%3A%20one%20regulariser%20preserves%20the%20topology%20of%20similarities%20between%0Ainstances%2C%20and%20the%20other%20preserves%20the%20graph%20geometry%20of%20similarities%20across%0Aspatial%20and%20temporal%20dimensions.%20To%20balance%20the%20contrastive%20learning%20objective%0Aand%20the%20need%20for%20structure%20preservation%2C%20we%20propose%20a%20dynamic%20weighting%0Amechanism%20that%20adaptively%20manages%20this%20trade-off%20and%20stabilises%20training.%20We%0Avalidate%20the%20proposed%20method%20through%20extensive%20experiments%2C%20including%0Amultivariate%20time%20series%20classification%20to%20demonstrate%20its%20general%0Aapplicability%2C%20as%20well%20as%20macroscopic%20and%20microscopic%20traffic%20prediction%20to%0Ahighlight%20its%20particular%20usefulness%20in%20encoding%20traffic%20interactions.%20Across%0Aall%20tasks%2C%20our%20method%20preserves%20the%20similarity%20structures%20more%20effectively%20and%0Aimproves%20state-of-the-art%20task%20performances.%20This%20method%20can%20be%20integrated%20with%0Aan%20arbitrary%20neural%20network%20model%20and%20is%20particularly%20beneficial%20for%20time%0Aseries%20data%20with%20spatial%20or%20geographical%20features.%20Furthermore%2C%20our%20findings%0Asuggest%20that%20well-preserved%20similarity%20structures%20in%20the%20latent%20space%20indicate%0Amore%20informative%20and%20useful%20representations.%20This%20provides%20insights%20to%20design%0Amore%20effective%20neural%20networks%20for%20data-driven%20transportation%20research.%20Our%0Acode%20is%20made%20openly%20accessible%20with%20all%20resulting%20data%20at%0Ahttps%3A//github.com/yiru-jiao/spclt%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06380v3&entry.124074799=Read"},
{"title": "Anymate: A Dataset and Baselines for Learning 3D Object Rigging", "author": "Yufan Deng and Yuhao Zhang and Chen Geng and Shangzhe Wu and Jiajun Wu", "abstract": "  Rigging and skinning are essential steps to create realistic 3D animations,\noften requiring significant expertise and manual effort. Traditional attempts\nat automating these processes rely heavily on geometric heuristics and often\nstruggle with objects of complex geometry. Recent data-driven approaches show\npotential for better generality, but are often constrained by limited training\ndata. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets\npaired with expert-crafted rigging and skinning information -- 70 times larger\nthan existing datasets. Using this dataset, we propose a learning-based\nauto-rigging framework with three sequential modules for joint, connectivity,\nand skinning weight prediction. We systematically design and experiment with\nvarious architectures as baselines for each module and conduct comprehensive\nevaluations on our dataset to compare their performance. Our models\nsignificantly outperform existing methods, providing a foundation for comparing\nfuture methods in automated rigging and skinning. Code and dataset can be found\nat https://anymate3d.github.io/.\n", "link": "http://arxiv.org/abs/2505.06227v1", "date": "2025-05-09", "relevancy": 2.6786, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anymate%3A%20A%20Dataset%20and%20Baselines%20for%20Learning%203D%20Object%20Rigging&body=Title%3A%20Anymate%3A%20A%20Dataset%20and%20Baselines%20for%20Learning%203D%20Object%20Rigging%0AAuthor%3A%20Yufan%20Deng%20and%20Yuhao%20Zhang%20and%20Chen%20Geng%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20Rigging%20and%20skinning%20are%20essential%20steps%20to%20create%20realistic%203D%20animations%2C%0Aoften%20requiring%20significant%20expertise%20and%20manual%20effort.%20Traditional%20attempts%0Aat%20automating%20these%20processes%20rely%20heavily%20on%20geometric%20heuristics%20and%20often%0Astruggle%20with%20objects%20of%20complex%20geometry.%20Recent%20data-driven%20approaches%20show%0Apotential%20for%20better%20generality%2C%20but%20are%20often%20constrained%20by%20limited%20training%0Adata.%20We%20present%20the%20Anymate%20Dataset%2C%20a%20large-scale%20dataset%20of%20230K%203D%20assets%0Apaired%20with%20expert-crafted%20rigging%20and%20skinning%20information%20--%2070%20times%20larger%0Athan%20existing%20datasets.%20Using%20this%20dataset%2C%20we%20propose%20a%20learning-based%0Aauto-rigging%20framework%20with%20three%20sequential%20modules%20for%20joint%2C%20connectivity%2C%0Aand%20skinning%20weight%20prediction.%20We%20systematically%20design%20and%20experiment%20with%0Avarious%20architectures%20as%20baselines%20for%20each%20module%20and%20conduct%20comprehensive%0Aevaluations%20on%20our%20dataset%20to%20compare%20their%20performance.%20Our%20models%0Asignificantly%20outperform%20existing%20methods%2C%20providing%20a%20foundation%20for%20comparing%0Afuture%20methods%20in%20automated%20rigging%20and%20skinning.%20Code%20and%20dataset%20can%20be%20found%0Aat%20https%3A//anymate3d.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnymate%253A%2520A%2520Dataset%2520and%2520Baselines%2520for%2520Learning%25203D%2520Object%2520Rigging%26entry.906535625%3DYufan%2520Deng%2520and%2520Yuhao%2520Zhang%2520and%2520Chen%2520Geng%2520and%2520Shangzhe%2520Wu%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520Rigging%2520and%2520skinning%2520are%2520essential%2520steps%2520to%2520create%2520realistic%25203D%2520animations%252C%250Aoften%2520requiring%2520significant%2520expertise%2520and%2520manual%2520effort.%2520Traditional%2520attempts%250Aat%2520automating%2520these%2520processes%2520rely%2520heavily%2520on%2520geometric%2520heuristics%2520and%2520often%250Astruggle%2520with%2520objects%2520of%2520complex%2520geometry.%2520Recent%2520data-driven%2520approaches%2520show%250Apotential%2520for%2520better%2520generality%252C%2520but%2520are%2520often%2520constrained%2520by%2520limited%2520training%250Adata.%2520We%2520present%2520the%2520Anymate%2520Dataset%252C%2520a%2520large-scale%2520dataset%2520of%2520230K%25203D%2520assets%250Apaired%2520with%2520expert-crafted%2520rigging%2520and%2520skinning%2520information%2520--%252070%2520times%2520larger%250Athan%2520existing%2520datasets.%2520Using%2520this%2520dataset%252C%2520we%2520propose%2520a%2520learning-based%250Aauto-rigging%2520framework%2520with%2520three%2520sequential%2520modules%2520for%2520joint%252C%2520connectivity%252C%250Aand%2520skinning%2520weight%2520prediction.%2520We%2520systematically%2520design%2520and%2520experiment%2520with%250Avarious%2520architectures%2520as%2520baselines%2520for%2520each%2520module%2520and%2520conduct%2520comprehensive%250Aevaluations%2520on%2520our%2520dataset%2520to%2520compare%2520their%2520performance.%2520Our%2520models%250Asignificantly%2520outperform%2520existing%2520methods%252C%2520providing%2520a%2520foundation%2520for%2520comparing%250Afuture%2520methods%2520in%2520automated%2520rigging%2520and%2520skinning.%2520Code%2520and%2520dataset%2520can%2520be%2520found%250Aat%2520https%253A//anymate3d.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anymate%3A%20A%20Dataset%20and%20Baselines%20for%20Learning%203D%20Object%20Rigging&entry.906535625=Yufan%20Deng%20and%20Yuhao%20Zhang%20and%20Chen%20Geng%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu&entry.1292438233=%20%20Rigging%20and%20skinning%20are%20essential%20steps%20to%20create%20realistic%203D%20animations%2C%0Aoften%20requiring%20significant%20expertise%20and%20manual%20effort.%20Traditional%20attempts%0Aat%20automating%20these%20processes%20rely%20heavily%20on%20geometric%20heuristics%20and%20often%0Astruggle%20with%20objects%20of%20complex%20geometry.%20Recent%20data-driven%20approaches%20show%0Apotential%20for%20better%20generality%2C%20but%20are%20often%20constrained%20by%20limited%20training%0Adata.%20We%20present%20the%20Anymate%20Dataset%2C%20a%20large-scale%20dataset%20of%20230K%203D%20assets%0Apaired%20with%20expert-crafted%20rigging%20and%20skinning%20information%20--%2070%20times%20larger%0Athan%20existing%20datasets.%20Using%20this%20dataset%2C%20we%20propose%20a%20learning-based%0Aauto-rigging%20framework%20with%20three%20sequential%20modules%20for%20joint%2C%20connectivity%2C%0Aand%20skinning%20weight%20prediction.%20We%20systematically%20design%20and%20experiment%20with%0Avarious%20architectures%20as%20baselines%20for%20each%20module%20and%20conduct%20comprehensive%0Aevaluations%20on%20our%20dataset%20to%20compare%20their%20performance.%20Our%20models%0Asignificantly%20outperform%20existing%20methods%2C%20providing%20a%20foundation%20for%20comparing%0Afuture%20methods%20in%20automated%20rigging%20and%20skinning.%20Code%20and%20dataset%20can%20be%20found%0Aat%20https%3A//anymate3d.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06227v1&entry.124074799=Read"},
{"title": "Autoencoder-Based Hybrid Replay for Class-Incremental Learning", "author": "Milad Khademi Nori and Il-Min Kim and Guanghui Wang", "abstract": "  In class-incremental learning (CIL), effective incremental learning\nstrategies are essential to mitigate task confusion and catastrophic\nforgetting, especially as the number of tasks $t$ increases. Current exemplar\nreplay strategies impose $\\mathcal{O}(t)$ memory/compute complexities. We\npropose an autoencoder-based hybrid replay (AHR) strategy that leverages our\nnew hybrid autoencoder (HAE) to function as a compressor to alleviate the\nrequirement for large memory, achieving $\\mathcal{O}(0.1 t)$ at the worst case\nwith the computing complexity of $\\mathcal{O}(t)$ while accomplishing\nstate-of-the-art performance. The decoder later recovers the exemplar data\nstored in the latent space, rather than in raw format. Additionally, HAE is\ndesigned for both discriminative and generative modeling, enabling\nclassification and replay capabilities, respectively. HAE adopts the charged\nparticle system energy minimization equations and repulsive force algorithm for\nthe incremental embedding and distribution of new class centroids in its latent\nspace. Our results demonstrate that AHR consistently outperforms recent\nbaselines across multiple benchmarks while operating with the same\nmemory/compute budgets. The source code is included in the supplementary\nmaterial and will be open-sourced upon publication.\n", "link": "http://arxiv.org/abs/2505.05926v1", "date": "2025-05-09", "relevancy": 2.6485, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5836}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning&body=Title%3A%20Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning%0AAuthor%3A%20Milad%20Khademi%20Nori%20and%20Il-Min%20Kim%20and%20Guanghui%20Wang%0AAbstract%3A%20%20%20In%20class-incremental%20learning%20%28CIL%29%2C%20effective%20incremental%20learning%0Astrategies%20are%20essential%20to%20mitigate%20task%20confusion%20and%20catastrophic%0Aforgetting%2C%20especially%20as%20the%20number%20of%20tasks%20%24t%24%20increases.%20Current%20exemplar%0Areplay%20strategies%20impose%20%24%5Cmathcal%7BO%7D%28t%29%24%20memory/compute%20complexities.%20We%0Apropose%20an%20autoencoder-based%20hybrid%20replay%20%28AHR%29%20strategy%20that%20leverages%20our%0Anew%20hybrid%20autoencoder%20%28HAE%29%20to%20function%20as%20a%20compressor%20to%20alleviate%20the%0Arequirement%20for%20large%20memory%2C%20achieving%20%24%5Cmathcal%7BO%7D%280.1%20t%29%24%20at%20the%20worst%20case%0Awith%20the%20computing%20complexity%20of%20%24%5Cmathcal%7BO%7D%28t%29%24%20while%20accomplishing%0Astate-of-the-art%20performance.%20The%20decoder%20later%20recovers%20the%20exemplar%20data%0Astored%20in%20the%20latent%20space%2C%20rather%20than%20in%20raw%20format.%20Additionally%2C%20HAE%20is%0Adesigned%20for%20both%20discriminative%20and%20generative%20modeling%2C%20enabling%0Aclassification%20and%20replay%20capabilities%2C%20respectively.%20HAE%20adopts%20the%20charged%0Aparticle%20system%20energy%20minimization%20equations%20and%20repulsive%20force%20algorithm%20for%0Athe%20incremental%20embedding%20and%20distribution%20of%20new%20class%20centroids%20in%20its%20latent%0Aspace.%20Our%20results%20demonstrate%20that%20AHR%20consistently%20outperforms%20recent%0Abaselines%20across%20multiple%20benchmarks%20while%20operating%20with%20the%20same%0Amemory/compute%20budgets.%20The%20source%20code%20is%20included%20in%20the%20supplementary%0Amaterial%20and%20will%20be%20open-sourced%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoder-Based%2520Hybrid%2520Replay%2520for%2520Class-Incremental%2520Learning%26entry.906535625%3DMilad%2520Khademi%2520Nori%2520and%2520Il-Min%2520Kim%2520and%2520Guanghui%2520Wang%26entry.1292438233%3D%2520%2520In%2520class-incremental%2520learning%2520%2528CIL%2529%252C%2520effective%2520incremental%2520learning%250Astrategies%2520are%2520essential%2520to%2520mitigate%2520task%2520confusion%2520and%2520catastrophic%250Aforgetting%252C%2520especially%2520as%2520the%2520number%2520of%2520tasks%2520%2524t%2524%2520increases.%2520Current%2520exemplar%250Areplay%2520strategies%2520impose%2520%2524%255Cmathcal%257BO%257D%2528t%2529%2524%2520memory/compute%2520complexities.%2520We%250Apropose%2520an%2520autoencoder-based%2520hybrid%2520replay%2520%2528AHR%2529%2520strategy%2520that%2520leverages%2520our%250Anew%2520hybrid%2520autoencoder%2520%2528HAE%2529%2520to%2520function%2520as%2520a%2520compressor%2520to%2520alleviate%2520the%250Arequirement%2520for%2520large%2520memory%252C%2520achieving%2520%2524%255Cmathcal%257BO%257D%25280.1%2520t%2529%2524%2520at%2520the%2520worst%2520case%250Awith%2520the%2520computing%2520complexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528t%2529%2524%2520while%2520accomplishing%250Astate-of-the-art%2520performance.%2520The%2520decoder%2520later%2520recovers%2520the%2520exemplar%2520data%250Astored%2520in%2520the%2520latent%2520space%252C%2520rather%2520than%2520in%2520raw%2520format.%2520Additionally%252C%2520HAE%2520is%250Adesigned%2520for%2520both%2520discriminative%2520and%2520generative%2520modeling%252C%2520enabling%250Aclassification%2520and%2520replay%2520capabilities%252C%2520respectively.%2520HAE%2520adopts%2520the%2520charged%250Aparticle%2520system%2520energy%2520minimization%2520equations%2520and%2520repulsive%2520force%2520algorithm%2520for%250Athe%2520incremental%2520embedding%2520and%2520distribution%2520of%2520new%2520class%2520centroids%2520in%2520its%2520latent%250Aspace.%2520Our%2520results%2520demonstrate%2520that%2520AHR%2520consistently%2520outperforms%2520recent%250Abaselines%2520across%2520multiple%2520benchmarks%2520while%2520operating%2520with%2520the%2520same%250Amemory/compute%2520budgets.%2520The%2520source%2520code%2520is%2520included%2520in%2520the%2520supplementary%250Amaterial%2520and%2520will%2520be%2520open-sourced%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoder-Based%20Hybrid%20Replay%20for%20Class-Incremental%20Learning&entry.906535625=Milad%20Khademi%20Nori%20and%20Il-Min%20Kim%20and%20Guanghui%20Wang&entry.1292438233=%20%20In%20class-incremental%20learning%20%28CIL%29%2C%20effective%20incremental%20learning%0Astrategies%20are%20essential%20to%20mitigate%20task%20confusion%20and%20catastrophic%0Aforgetting%2C%20especially%20as%20the%20number%20of%20tasks%20%24t%24%20increases.%20Current%20exemplar%0Areplay%20strategies%20impose%20%24%5Cmathcal%7BO%7D%28t%29%24%20memory/compute%20complexities.%20We%0Apropose%20an%20autoencoder-based%20hybrid%20replay%20%28AHR%29%20strategy%20that%20leverages%20our%0Anew%20hybrid%20autoencoder%20%28HAE%29%20to%20function%20as%20a%20compressor%20to%20alleviate%20the%0Arequirement%20for%20large%20memory%2C%20achieving%20%24%5Cmathcal%7BO%7D%280.1%20t%29%24%20at%20the%20worst%20case%0Awith%20the%20computing%20complexity%20of%20%24%5Cmathcal%7BO%7D%28t%29%24%20while%20accomplishing%0Astate-of-the-art%20performance.%20The%20decoder%20later%20recovers%20the%20exemplar%20data%0Astored%20in%20the%20latent%20space%2C%20rather%20than%20in%20raw%20format.%20Additionally%2C%20HAE%20is%0Adesigned%20for%20both%20discriminative%20and%20generative%20modeling%2C%20enabling%0Aclassification%20and%20replay%20capabilities%2C%20respectively.%20HAE%20adopts%20the%20charged%0Aparticle%20system%20energy%20minimization%20equations%20and%20repulsive%20force%20algorithm%20for%0Athe%20incremental%20embedding%20and%20distribution%20of%20new%20class%20centroids%20in%20its%20latent%0Aspace.%20Our%20results%20demonstrate%20that%20AHR%20consistently%20outperforms%20recent%0Abaselines%20across%20multiple%20benchmarks%20while%20operating%20with%20the%20same%0Amemory/compute%20budgets.%20The%20source%20code%20is%20included%20in%20the%20supplementary%0Amaterial%20and%20will%20be%20open-sourced%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05926v1&entry.124074799=Read"},
{"title": "Image space formalism of convolutional neural networks for k-space\n  interpolation", "author": "Peter Dawood and Felix Breuer and Istvan Homolya and Maximilian Gram and Peter M. Jakob and Moritz Zaiss and Martin Blaimer", "abstract": "  Purpose: Noise resilience in image reconstructions by scan-specific robust\nartificial neural networks for k-space interpolation (RAKI) is linked to\nnonlinear activations in k-space. To gain a deeper understanding of this\nrelationship, an image space formalism of RAKI is introduced for analyzing\nnoise propagation analytically, identifying and characterizing image\nreconstruction features and to describe the role of nonlinear activations in a\nhuman readable manner. Methods: The image space formalism for RAKI inference is\nemployed by expressing nonlinear activations in k-space as element-wise\nmultiplications with activation masks, which transform into convolutions in\nimage space. Jacobians of the de-aliased, coil-combined image relative to the\naliased coil images can be expressed algebraically, and thus, the noise\namplification is quantified analytically (g-factor maps). We analyze the role\nof nonlinearity for noise resilience by controlling the degree of nonlinearity\nin the reconstruction model via the negative slope parameter in leaky ReLU.\nResults: The analytical g-factor maps correspond with those obtained from Monte\nCarlo simulations and from an auto differentiation approach for in vivo brain\nimages. Apparent blurring and contrast loss artifacts are identified as\nimplications of enhanced noise resilience. These residual artifacts can be\ntraded against noise resilience by adjusting the degree of nonlinearity in the\nmodel (Tikhonov-like regularization) in case of limited training data. The\ninspection of image space activations reveals an autocorrelation pattern\nleading to a potential center artifact. Conclusion: The image space formalism\nof RAKI provides the means for analytical quantitative noisepropagation\nanalysis and human-readable visualization of the effects of the nonlinear\nactivation functions in k-space.\n", "link": "http://arxiv.org/abs/2402.17410v2", "date": "2025-05-09", "relevancy": 2.5673, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5229}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5125}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20space%20formalism%20of%20convolutional%20neural%20networks%20for%20k-space%0A%20%20interpolation&body=Title%3A%20Image%20space%20formalism%20of%20convolutional%20neural%20networks%20for%20k-space%0A%20%20interpolation%0AAuthor%3A%20Peter%20Dawood%20and%20Felix%20Breuer%20and%20Istvan%20Homolya%20and%20Maximilian%20Gram%20and%20Peter%20M.%20Jakob%20and%20Moritz%20Zaiss%20and%20Martin%20Blaimer%0AAbstract%3A%20%20%20Purpose%3A%20Noise%20resilience%20in%20image%20reconstructions%20by%20scan-specific%20robust%0Aartificial%20neural%20networks%20for%20k-space%20interpolation%20%28RAKI%29%20is%20linked%20to%0Anonlinear%20activations%20in%20k-space.%20To%20gain%20a%20deeper%20understanding%20of%20this%0Arelationship%2C%20an%20image%20space%20formalism%20of%20RAKI%20is%20introduced%20for%20analyzing%0Anoise%20propagation%20analytically%2C%20identifying%20and%20characterizing%20image%0Areconstruction%20features%20and%20to%20describe%20the%20role%20of%20nonlinear%20activations%20in%20a%0Ahuman%20readable%20manner.%20Methods%3A%20The%20image%20space%20formalism%20for%20RAKI%20inference%20is%0Aemployed%20by%20expressing%20nonlinear%20activations%20in%20k-space%20as%20element-wise%0Amultiplications%20with%20activation%20masks%2C%20which%20transform%20into%20convolutions%20in%0Aimage%20space.%20Jacobians%20of%20the%20de-aliased%2C%20coil-combined%20image%20relative%20to%20the%0Aaliased%20coil%20images%20can%20be%20expressed%20algebraically%2C%20and%20thus%2C%20the%20noise%0Aamplification%20is%20quantified%20analytically%20%28g-factor%20maps%29.%20We%20analyze%20the%20role%0Aof%20nonlinearity%20for%20noise%20resilience%20by%20controlling%20the%20degree%20of%20nonlinearity%0Ain%20the%20reconstruction%20model%20via%20the%20negative%20slope%20parameter%20in%20leaky%20ReLU.%0AResults%3A%20The%20analytical%20g-factor%20maps%20correspond%20with%20those%20obtained%20from%20Monte%0ACarlo%20simulations%20and%20from%20an%20auto%20differentiation%20approach%20for%20in%20vivo%20brain%0Aimages.%20Apparent%20blurring%20and%20contrast%20loss%20artifacts%20are%20identified%20as%0Aimplications%20of%20enhanced%20noise%20resilience.%20These%20residual%20artifacts%20can%20be%0Atraded%20against%20noise%20resilience%20by%20adjusting%20the%20degree%20of%20nonlinearity%20in%20the%0Amodel%20%28Tikhonov-like%20regularization%29%20in%20case%20of%20limited%20training%20data.%20The%0Ainspection%20of%20image%20space%20activations%20reveals%20an%20autocorrelation%20pattern%0Aleading%20to%20a%20potential%20center%20artifact.%20Conclusion%3A%20The%20image%20space%20formalism%0Aof%20RAKI%20provides%20the%20means%20for%20analytical%20quantitative%20noisepropagation%0Aanalysis%20and%20human-readable%20visualization%20of%20the%20effects%20of%20the%20nonlinear%0Aactivation%20functions%20in%20k-space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520space%2520formalism%2520of%2520convolutional%2520neural%2520networks%2520for%2520k-space%250A%2520%2520interpolation%26entry.906535625%3DPeter%2520Dawood%2520and%2520Felix%2520Breuer%2520and%2520Istvan%2520Homolya%2520and%2520Maximilian%2520Gram%2520and%2520Peter%2520M.%2520Jakob%2520and%2520Moritz%2520Zaiss%2520and%2520Martin%2520Blaimer%26entry.1292438233%3D%2520%2520Purpose%253A%2520Noise%2520resilience%2520in%2520image%2520reconstructions%2520by%2520scan-specific%2520robust%250Aartificial%2520neural%2520networks%2520for%2520k-space%2520interpolation%2520%2528RAKI%2529%2520is%2520linked%2520to%250Anonlinear%2520activations%2520in%2520k-space.%2520To%2520gain%2520a%2520deeper%2520understanding%2520of%2520this%250Arelationship%252C%2520an%2520image%2520space%2520formalism%2520of%2520RAKI%2520is%2520introduced%2520for%2520analyzing%250Anoise%2520propagation%2520analytically%252C%2520identifying%2520and%2520characterizing%2520image%250Areconstruction%2520features%2520and%2520to%2520describe%2520the%2520role%2520of%2520nonlinear%2520activations%2520in%2520a%250Ahuman%2520readable%2520manner.%2520Methods%253A%2520The%2520image%2520space%2520formalism%2520for%2520RAKI%2520inference%2520is%250Aemployed%2520by%2520expressing%2520nonlinear%2520activations%2520in%2520k-space%2520as%2520element-wise%250Amultiplications%2520with%2520activation%2520masks%252C%2520which%2520transform%2520into%2520convolutions%2520in%250Aimage%2520space.%2520Jacobians%2520of%2520the%2520de-aliased%252C%2520coil-combined%2520image%2520relative%2520to%2520the%250Aaliased%2520coil%2520images%2520can%2520be%2520expressed%2520algebraically%252C%2520and%2520thus%252C%2520the%2520noise%250Aamplification%2520is%2520quantified%2520analytically%2520%2528g-factor%2520maps%2529.%2520We%2520analyze%2520the%2520role%250Aof%2520nonlinearity%2520for%2520noise%2520resilience%2520by%2520controlling%2520the%2520degree%2520of%2520nonlinearity%250Ain%2520the%2520reconstruction%2520model%2520via%2520the%2520negative%2520slope%2520parameter%2520in%2520leaky%2520ReLU.%250AResults%253A%2520The%2520analytical%2520g-factor%2520maps%2520correspond%2520with%2520those%2520obtained%2520from%2520Monte%250ACarlo%2520simulations%2520and%2520from%2520an%2520auto%2520differentiation%2520approach%2520for%2520in%2520vivo%2520brain%250Aimages.%2520Apparent%2520blurring%2520and%2520contrast%2520loss%2520artifacts%2520are%2520identified%2520as%250Aimplications%2520of%2520enhanced%2520noise%2520resilience.%2520These%2520residual%2520artifacts%2520can%2520be%250Atraded%2520against%2520noise%2520resilience%2520by%2520adjusting%2520the%2520degree%2520of%2520nonlinearity%2520in%2520the%250Amodel%2520%2528Tikhonov-like%2520regularization%2529%2520in%2520case%2520of%2520limited%2520training%2520data.%2520The%250Ainspection%2520of%2520image%2520space%2520activations%2520reveals%2520an%2520autocorrelation%2520pattern%250Aleading%2520to%2520a%2520potential%2520center%2520artifact.%2520Conclusion%253A%2520The%2520image%2520space%2520formalism%250Aof%2520RAKI%2520provides%2520the%2520means%2520for%2520analytical%2520quantitative%2520noisepropagation%250Aanalysis%2520and%2520human-readable%2520visualization%2520of%2520the%2520effects%2520of%2520the%2520nonlinear%250Aactivation%2520functions%2520in%2520k-space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20space%20formalism%20of%20convolutional%20neural%20networks%20for%20k-space%0A%20%20interpolation&entry.906535625=Peter%20Dawood%20and%20Felix%20Breuer%20and%20Istvan%20Homolya%20and%20Maximilian%20Gram%20and%20Peter%20M.%20Jakob%20and%20Moritz%20Zaiss%20and%20Martin%20Blaimer&entry.1292438233=%20%20Purpose%3A%20Noise%20resilience%20in%20image%20reconstructions%20by%20scan-specific%20robust%0Aartificial%20neural%20networks%20for%20k-space%20interpolation%20%28RAKI%29%20is%20linked%20to%0Anonlinear%20activations%20in%20k-space.%20To%20gain%20a%20deeper%20understanding%20of%20this%0Arelationship%2C%20an%20image%20space%20formalism%20of%20RAKI%20is%20introduced%20for%20analyzing%0Anoise%20propagation%20analytically%2C%20identifying%20and%20characterizing%20image%0Areconstruction%20features%20and%20to%20describe%20the%20role%20of%20nonlinear%20activations%20in%20a%0Ahuman%20readable%20manner.%20Methods%3A%20The%20image%20space%20formalism%20for%20RAKI%20inference%20is%0Aemployed%20by%20expressing%20nonlinear%20activations%20in%20k-space%20as%20element-wise%0Amultiplications%20with%20activation%20masks%2C%20which%20transform%20into%20convolutions%20in%0Aimage%20space.%20Jacobians%20of%20the%20de-aliased%2C%20coil-combined%20image%20relative%20to%20the%0Aaliased%20coil%20images%20can%20be%20expressed%20algebraically%2C%20and%20thus%2C%20the%20noise%0Aamplification%20is%20quantified%20analytically%20%28g-factor%20maps%29.%20We%20analyze%20the%20role%0Aof%20nonlinearity%20for%20noise%20resilience%20by%20controlling%20the%20degree%20of%20nonlinearity%0Ain%20the%20reconstruction%20model%20via%20the%20negative%20slope%20parameter%20in%20leaky%20ReLU.%0AResults%3A%20The%20analytical%20g-factor%20maps%20correspond%20with%20those%20obtained%20from%20Monte%0ACarlo%20simulations%20and%20from%20an%20auto%20differentiation%20approach%20for%20in%20vivo%20brain%0Aimages.%20Apparent%20blurring%20and%20contrast%20loss%20artifacts%20are%20identified%20as%0Aimplications%20of%20enhanced%20noise%20resilience.%20These%20residual%20artifacts%20can%20be%0Atraded%20against%20noise%20resilience%20by%20adjusting%20the%20degree%20of%20nonlinearity%20in%20the%0Amodel%20%28Tikhonov-like%20regularization%29%20in%20case%20of%20limited%20training%20data.%20The%0Ainspection%20of%20image%20space%20activations%20reveals%20an%20autocorrelation%20pattern%0Aleading%20to%20a%20potential%20center%20artifact.%20Conclusion%3A%20The%20image%20space%20formalism%0Aof%20RAKI%20provides%20the%20means%20for%20analytical%20quantitative%20noisepropagation%0Aanalysis%20and%20human-readable%20visualization%20of%20the%20effects%20of%20the%20nonlinear%0Aactivation%20functions%20in%20k-space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17410v2&entry.124074799=Read"},
{"title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene\n  Understanding", "author": "Yuchen Rao and Stefan Ainetter and Sinisa Stekovic and Vincent Lepetit and Friedrich Fraundorfer", "abstract": "  High-level 3D scene understanding is essential in many applications. However,\nthe challenges of generating accurate 3D annotations make development of deep\nlearning models difficult. We turn to recent advancements in automatic\nretrieval of synthetic CAD models, and show that data generated by such methods\ncan be used as high-quality ground truth for training supervised deep learning\nmodels. More exactly, we employ a pipeline akin to the one previously used to\nautomatically annotate objects in ScanNet scenes with their 9D poses and CAD\nmodels. This time, we apply it to the recent ScanNet++ v1 dataset, which\npreviously lacked such annotations. Our findings demonstrate that it is not\nonly possible to train deep learning models on these automatically-obtained\nannotations but that the resulting models outperform those trained on manually\nannotated data. We validate this on two distinct tasks: point cloud completion\nand single-view CAD model retrieval and alignment. Our results underscore the\npotential of automatic 3D annotations to enhance model performance while\nsignificantly reducing annotation costs. To support future research in 3D scene\nunderstanding, we will release our annotations, which we call SCANnotate++,\nalong with our trained models.\n", "link": "http://arxiv.org/abs/2504.13580v2", "date": "2025-05-09", "relevancy": 2.564, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6457}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&body=Title%3A%20Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer%0AAbstract%3A%20%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Automatic%2520CAD%2520Annotations%2520for%2520Supervised%2520Learning%2520in%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYuchen%2520Rao%2520and%2520Stefan%2520Ainetter%2520and%2520Sinisa%2520Stekovic%2520and%2520Vincent%2520Lepetit%2520and%2520Friedrich%2520Fraundorfer%26entry.1292438233%3D%2520%2520High-level%25203D%2520scene%2520understanding%2520is%2520essential%2520in%2520many%2520applications.%2520However%252C%250Athe%2520challenges%2520of%2520generating%2520accurate%25203D%2520annotations%2520make%2520development%2520of%2520deep%250Alearning%2520models%2520difficult.%2520We%2520turn%2520to%2520recent%2520advancements%2520in%2520automatic%250Aretrieval%2520of%2520synthetic%2520CAD%2520models%252C%2520and%2520show%2520that%2520data%2520generated%2520by%2520such%2520methods%250Acan%2520be%2520used%2520as%2520high-quality%2520ground%2520truth%2520for%2520training%2520supervised%2520deep%2520learning%250Amodels.%2520More%2520exactly%252C%2520we%2520employ%2520a%2520pipeline%2520akin%2520to%2520the%2520one%2520previously%2520used%2520to%250Aautomatically%2520annotate%2520objects%2520in%2520ScanNet%2520scenes%2520with%2520their%25209D%2520poses%2520and%2520CAD%250Amodels.%2520This%2520time%252C%2520we%2520apply%2520it%2520to%2520the%2520recent%2520ScanNet%252B%252B%2520v1%2520dataset%252C%2520which%250Apreviously%2520lacked%2520such%2520annotations.%2520Our%2520findings%2520demonstrate%2520that%2520it%2520is%2520not%250Aonly%2520possible%2520to%2520train%2520deep%2520learning%2520models%2520on%2520these%2520automatically-obtained%250Aannotations%2520but%2520that%2520the%2520resulting%2520models%2520outperform%2520those%2520trained%2520on%2520manually%250Aannotated%2520data.%2520We%2520validate%2520this%2520on%2520two%2520distinct%2520tasks%253A%2520point%2520cloud%2520completion%250Aand%2520single-view%2520CAD%2520model%2520retrieval%2520and%2520alignment.%2520Our%2520results%2520underscore%2520the%250Apotential%2520of%2520automatic%25203D%2520annotations%2520to%2520enhance%2520model%2520performance%2520while%250Asignificantly%2520reducing%2520annotation%2520costs.%2520To%2520support%2520future%2520research%2520in%25203D%2520scene%250Aunderstanding%252C%2520we%2520will%2520release%2520our%2520annotations%252C%2520which%2520we%2520call%2520SCANnotate%252B%252B%252C%250Aalong%2520with%2520our%2520trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Automatic%20CAD%20Annotations%20for%20Supervised%20Learning%20in%203D%20Scene%0A%20%20Understanding&entry.906535625=Yuchen%20Rao%20and%20Stefan%20Ainetter%20and%20Sinisa%20Stekovic%20and%20Vincent%20Lepetit%20and%20Friedrich%20Fraundorfer&entry.1292438233=%20%20High-level%203D%20scene%20understanding%20is%20essential%20in%20many%20applications.%20However%2C%0Athe%20challenges%20of%20generating%20accurate%203D%20annotations%20make%20development%20of%20deep%0Alearning%20models%20difficult.%20We%20turn%20to%20recent%20advancements%20in%20automatic%0Aretrieval%20of%20synthetic%20CAD%20models%2C%20and%20show%20that%20data%20generated%20by%20such%20methods%0Acan%20be%20used%20as%20high-quality%20ground%20truth%20for%20training%20supervised%20deep%20learning%0Amodels.%20More%20exactly%2C%20we%20employ%20a%20pipeline%20akin%20to%20the%20one%20previously%20used%20to%0Aautomatically%20annotate%20objects%20in%20ScanNet%20scenes%20with%20their%209D%20poses%20and%20CAD%0Amodels.%20This%20time%2C%20we%20apply%20it%20to%20the%20recent%20ScanNet%2B%2B%20v1%20dataset%2C%20which%0Apreviously%20lacked%20such%20annotations.%20Our%20findings%20demonstrate%20that%20it%20is%20not%0Aonly%20possible%20to%20train%20deep%20learning%20models%20on%20these%20automatically-obtained%0Aannotations%20but%20that%20the%20resulting%20models%20outperform%20those%20trained%20on%20manually%0Aannotated%20data.%20We%20validate%20this%20on%20two%20distinct%20tasks%3A%20point%20cloud%20completion%0Aand%20single-view%20CAD%20model%20retrieval%20and%20alignment.%20Our%20results%20underscore%20the%0Apotential%20of%20automatic%203D%20annotations%20to%20enhance%20model%20performance%20while%0Asignificantly%20reducing%20annotation%20costs.%20To%20support%20future%20research%20in%203D%20scene%0Aunderstanding%2C%20we%20will%20release%20our%20annotations%2C%20which%20we%20call%20SCANnotate%2B%2B%2C%0Aalong%20with%20our%20trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13580v2&entry.124074799=Read"},
{"title": "Gradient-Guided Annealing for Domain Generalization", "author": "Aristotelis Ballas and Christos Diou", "abstract": "  Domain Generalization (DG) research has gained considerable traction as of\nlate, since the ability to generalize to unseen data distributions is a\nrequirement that eludes even state-of-the-art training algorithms. In this\npaper we observe that the initial iterations of model training play a key role\nin domain generalization effectiveness, since the loss landscape may be\nsignificantly different across the training and test distributions, contrary to\nthe case of i.i.d. data. Conflicts between gradients of the loss components of\neach domain lead the optimization procedure to undesirable local minima that do\nnot capture the domain-invariant features of the target classes. We propose\nalleviating domain conflicts in model optimization, by iteratively annealing\nthe parameters of a model in the early stages of training and searching for\npoints where gradients align between domains. By discovering a set of parameter\nvalues where gradients are updated towards the same direction for each data\ndistribution present in the training set, the proposed Gradient-Guided\nAnnealing (GGA) algorithm encourages models to seek out minima that exhibit\nimproved robustness against domain shifts. The efficacy of GGA is evaluated on\nfive widely accepted and challenging image classification domain generalization\nbenchmarks, where its use alone is able to establish highly competitive or even\nstate-of-the-art performance. Moreover, when combined with previously proposed\ndomain-generalization algorithms it is able to consistently improve their\neffectiveness by significant margins.\n", "link": "http://arxiv.org/abs/2502.20162v5", "date": "2025-05-09", "relevancy": 2.5626, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Guided%20Annealing%20for%20Domain%20Generalization&body=Title%3A%20Gradient-Guided%20Annealing%20for%20Domain%20Generalization%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%20research%20has%20gained%20considerable%20traction%20as%20of%0Alate%2C%20since%20the%20ability%20to%20generalize%20to%20unseen%20data%20distributions%20is%20a%0Arequirement%20that%20eludes%20even%20state-of-the-art%20training%20algorithms.%20In%20this%0Apaper%20we%20observe%20that%20the%20initial%20iterations%20of%20model%20training%20play%20a%20key%20role%0Ain%20domain%20generalization%20effectiveness%2C%20since%20the%20loss%20landscape%20may%20be%0Asignificantly%20different%20across%20the%20training%20and%20test%20distributions%2C%20contrary%20to%0Athe%20case%20of%20i.i.d.%20data.%20Conflicts%20between%20gradients%20of%20the%20loss%20components%20of%0Aeach%20domain%20lead%20the%20optimization%20procedure%20to%20undesirable%20local%20minima%20that%20do%0Anot%20capture%20the%20domain-invariant%20features%20of%20the%20target%20classes.%20We%20propose%0Aalleviating%20domain%20conflicts%20in%20model%20optimization%2C%20by%20iteratively%20annealing%0Athe%20parameters%20of%20a%20model%20in%20the%20early%20stages%20of%20training%20and%20searching%20for%0Apoints%20where%20gradients%20align%20between%20domains.%20By%20discovering%20a%20set%20of%20parameter%0Avalues%20where%20gradients%20are%20updated%20towards%20the%20same%20direction%20for%20each%20data%0Adistribution%20present%20in%20the%20training%20set%2C%20the%20proposed%20Gradient-Guided%0AAnnealing%20%28GGA%29%20algorithm%20encourages%20models%20to%20seek%20out%20minima%20that%20exhibit%0Aimproved%20robustness%20against%20domain%20shifts.%20The%20efficacy%20of%20GGA%20is%20evaluated%20on%0Afive%20widely%20accepted%20and%20challenging%20image%20classification%20domain%20generalization%0Abenchmarks%2C%20where%20its%20use%20alone%20is%20able%20to%20establish%20highly%20competitive%20or%20even%0Astate-of-the-art%20performance.%20Moreover%2C%20when%20combined%20with%20previously%20proposed%0Adomain-generalization%20algorithms%20it%20is%20able%20to%20consistently%20improve%20their%0Aeffectiveness%20by%20significant%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20162v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Guided%2520Annealing%2520for%2520Domain%2520Generalization%26entry.906535625%3DAristotelis%2520Ballas%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%2520research%2520has%2520gained%2520considerable%2520traction%2520as%2520of%250Alate%252C%2520since%2520the%2520ability%2520to%2520generalize%2520to%2520unseen%2520data%2520distributions%2520is%2520a%250Arequirement%2520that%2520eludes%2520even%2520state-of-the-art%2520training%2520algorithms.%2520In%2520this%250Apaper%2520we%2520observe%2520that%2520the%2520initial%2520iterations%2520of%2520model%2520training%2520play%2520a%2520key%2520role%250Ain%2520domain%2520generalization%2520effectiveness%252C%2520since%2520the%2520loss%2520landscape%2520may%2520be%250Asignificantly%2520different%2520across%2520the%2520training%2520and%2520test%2520distributions%252C%2520contrary%2520to%250Athe%2520case%2520of%2520i.i.d.%2520data.%2520Conflicts%2520between%2520gradients%2520of%2520the%2520loss%2520components%2520of%250Aeach%2520domain%2520lead%2520the%2520optimization%2520procedure%2520to%2520undesirable%2520local%2520minima%2520that%2520do%250Anot%2520capture%2520the%2520domain-invariant%2520features%2520of%2520the%2520target%2520classes.%2520We%2520propose%250Aalleviating%2520domain%2520conflicts%2520in%2520model%2520optimization%252C%2520by%2520iteratively%2520annealing%250Athe%2520parameters%2520of%2520a%2520model%2520in%2520the%2520early%2520stages%2520of%2520training%2520and%2520searching%2520for%250Apoints%2520where%2520gradients%2520align%2520between%2520domains.%2520By%2520discovering%2520a%2520set%2520of%2520parameter%250Avalues%2520where%2520gradients%2520are%2520updated%2520towards%2520the%2520same%2520direction%2520for%2520each%2520data%250Adistribution%2520present%2520in%2520the%2520training%2520set%252C%2520the%2520proposed%2520Gradient-Guided%250AAnnealing%2520%2528GGA%2529%2520algorithm%2520encourages%2520models%2520to%2520seek%2520out%2520minima%2520that%2520exhibit%250Aimproved%2520robustness%2520against%2520domain%2520shifts.%2520The%2520efficacy%2520of%2520GGA%2520is%2520evaluated%2520on%250Afive%2520widely%2520accepted%2520and%2520challenging%2520image%2520classification%2520domain%2520generalization%250Abenchmarks%252C%2520where%2520its%2520use%2520alone%2520is%2520able%2520to%2520establish%2520highly%2520competitive%2520or%2520even%250Astate-of-the-art%2520performance.%2520Moreover%252C%2520when%2520combined%2520with%2520previously%2520proposed%250Adomain-generalization%2520algorithms%2520it%2520is%2520able%2520to%2520consistently%2520improve%2520their%250Aeffectiveness%2520by%2520significant%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20162v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Guided%20Annealing%20for%20Domain%20Generalization&entry.906535625=Aristotelis%20Ballas%20and%20Christos%20Diou&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%20research%20has%20gained%20considerable%20traction%20as%20of%0Alate%2C%20since%20the%20ability%20to%20generalize%20to%20unseen%20data%20distributions%20is%20a%0Arequirement%20that%20eludes%20even%20state-of-the-art%20training%20algorithms.%20In%20this%0Apaper%20we%20observe%20that%20the%20initial%20iterations%20of%20model%20training%20play%20a%20key%20role%0Ain%20domain%20generalization%20effectiveness%2C%20since%20the%20loss%20landscape%20may%20be%0Asignificantly%20different%20across%20the%20training%20and%20test%20distributions%2C%20contrary%20to%0Athe%20case%20of%20i.i.d.%20data.%20Conflicts%20between%20gradients%20of%20the%20loss%20components%20of%0Aeach%20domain%20lead%20the%20optimization%20procedure%20to%20undesirable%20local%20minima%20that%20do%0Anot%20capture%20the%20domain-invariant%20features%20of%20the%20target%20classes.%20We%20propose%0Aalleviating%20domain%20conflicts%20in%20model%20optimization%2C%20by%20iteratively%20annealing%0Athe%20parameters%20of%20a%20model%20in%20the%20early%20stages%20of%20training%20and%20searching%20for%0Apoints%20where%20gradients%20align%20between%20domains.%20By%20discovering%20a%20set%20of%20parameter%0Avalues%20where%20gradients%20are%20updated%20towards%20the%20same%20direction%20for%20each%20data%0Adistribution%20present%20in%20the%20training%20set%2C%20the%20proposed%20Gradient-Guided%0AAnnealing%20%28GGA%29%20algorithm%20encourages%20models%20to%20seek%20out%20minima%20that%20exhibit%0Aimproved%20robustness%20against%20domain%20shifts.%20The%20efficacy%20of%20GGA%20is%20evaluated%20on%0Afive%20widely%20accepted%20and%20challenging%20image%20classification%20domain%20generalization%0Abenchmarks%2C%20where%20its%20use%20alone%20is%20able%20to%20establish%20highly%20competitive%20or%20even%0Astate-of-the-art%20performance.%20Moreover%2C%20when%20combined%20with%20previously%20proposed%0Adomain-generalization%20algorithms%20it%20is%20able%20to%20consistently%20improve%20their%0Aeffectiveness%20by%20significant%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20162v5&entry.124074799=Read"},
{"title": "An Invitation to Deep Reinforcement Learning", "author": "Bernhard Jaeger and Andreas Geiger", "abstract": "  Training a deep neural network to maximize a target objective has become the\nstandard recipe for successful machine learning over the last decade. These\nnetworks can be optimized with supervised learning, if the target objective is\ndifferentiable. For many interesting problems, this is however not the case.\nCommon objectives like intersection over union (IoU), bilingual evaluation\nunderstudy (BLEU) score or rewards cannot be optimized with supervised\nlearning. A common workaround is to define differentiable surrogate losses,\nleading to suboptimal solutions with respect to the actual objective.\nReinforcement learning (RL) has emerged as a promising alternative for\noptimizing deep neural networks to maximize non-differentiable objectives in\nrecent years. Examples include aligning large language models via human\nfeedback, code generation, object detection or control problems. This makes RL\ntechniques relevant to the larger machine learning audience. The subject is,\nhowever, time intensive to approach due to the large range of methods, as well\nas the often very theoretical presentation. In this introduction, we take an\nalternative approach, different from classic reinforcement learning textbooks.\nRather than focusing on tabular problems, we introduce reinforcement learning\nas a generalization of supervised learning, which we first apply to\nnon-differentiable objectives and later to temporal problems. Assuming only\nbasic knowledge of supervised learning, the reader will be able to understand\nstate-of-the-art deep RL algorithms like proximal policy optimization (PPO)\nafter reading this tutorial.\n", "link": "http://arxiv.org/abs/2312.08365v3", "date": "2025-05-09", "relevancy": 2.5102, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5157}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Invitation%20to%20Deep%20Reinforcement%20Learning&body=Title%3A%20An%20Invitation%20to%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20Training%20a%20deep%20neural%20network%20to%20maximize%20a%20target%20objective%20has%20become%20the%0Astandard%20recipe%20for%20successful%20machine%20learning%20over%20the%20last%20decade.%20These%0Anetworks%20can%20be%20optimized%20with%20supervised%20learning%2C%20if%20the%20target%20objective%20is%0Adifferentiable.%20For%20many%20interesting%20problems%2C%20this%20is%20however%20not%20the%20case.%0ACommon%20objectives%20like%20intersection%20over%20union%20%28IoU%29%2C%20bilingual%20evaluation%0Aunderstudy%20%28BLEU%29%20score%20or%20rewards%20cannot%20be%20optimized%20with%20supervised%0Alearning.%20A%20common%20workaround%20is%20to%20define%20differentiable%20surrogate%20losses%2C%0Aleading%20to%20suboptimal%20solutions%20with%20respect%20to%20the%20actual%20objective.%0AReinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20alternative%20for%0Aoptimizing%20deep%20neural%20networks%20to%20maximize%20non-differentiable%20objectives%20in%0Arecent%20years.%20Examples%20include%20aligning%20large%20language%20models%20via%20human%0Afeedback%2C%20code%20generation%2C%20object%20detection%20or%20control%20problems.%20This%20makes%20RL%0Atechniques%20relevant%20to%20the%20larger%20machine%20learning%20audience.%20The%20subject%20is%2C%0Ahowever%2C%20time%20intensive%20to%20approach%20due%20to%20the%20large%20range%20of%20methods%2C%20as%20well%0Aas%20the%20often%20very%20theoretical%20presentation.%20In%20this%20introduction%2C%20we%20take%20an%0Aalternative%20approach%2C%20different%20from%20classic%20reinforcement%20learning%20textbooks.%0ARather%20than%20focusing%20on%20tabular%20problems%2C%20we%20introduce%20reinforcement%20learning%0Aas%20a%20generalization%20of%20supervised%20learning%2C%20which%20we%20first%20apply%20to%0Anon-differentiable%20objectives%20and%20later%20to%20temporal%20problems.%20Assuming%20only%0Abasic%20knowledge%20of%20supervised%20learning%2C%20the%20reader%20will%20be%20able%20to%20understand%0Astate-of-the-art%20deep%20RL%20algorithms%20like%20proximal%20policy%20optimization%20%28PPO%29%0Aafter%20reading%20this%20tutorial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08365v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Invitation%2520to%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DBernhard%2520Jaeger%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520Training%2520a%2520deep%2520neural%2520network%2520to%2520maximize%2520a%2520target%2520objective%2520has%2520become%2520the%250Astandard%2520recipe%2520for%2520successful%2520machine%2520learning%2520over%2520the%2520last%2520decade.%2520These%250Anetworks%2520can%2520be%2520optimized%2520with%2520supervised%2520learning%252C%2520if%2520the%2520target%2520objective%2520is%250Adifferentiable.%2520For%2520many%2520interesting%2520problems%252C%2520this%2520is%2520however%2520not%2520the%2520case.%250ACommon%2520objectives%2520like%2520intersection%2520over%2520union%2520%2528IoU%2529%252C%2520bilingual%2520evaluation%250Aunderstudy%2520%2528BLEU%2529%2520score%2520or%2520rewards%2520cannot%2520be%2520optimized%2520with%2520supervised%250Alearning.%2520A%2520common%2520workaround%2520is%2520to%2520define%2520differentiable%2520surrogate%2520losses%252C%250Aleading%2520to%2520suboptimal%2520solutions%2520with%2520respect%2520to%2520the%2520actual%2520objective.%250AReinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520for%250Aoptimizing%2520deep%2520neural%2520networks%2520to%2520maximize%2520non-differentiable%2520objectives%2520in%250Arecent%2520years.%2520Examples%2520include%2520aligning%2520large%2520language%2520models%2520via%2520human%250Afeedback%252C%2520code%2520generation%252C%2520object%2520detection%2520or%2520control%2520problems.%2520This%2520makes%2520RL%250Atechniques%2520relevant%2520to%2520the%2520larger%2520machine%2520learning%2520audience.%2520The%2520subject%2520is%252C%250Ahowever%252C%2520time%2520intensive%2520to%2520approach%2520due%2520to%2520the%2520large%2520range%2520of%2520methods%252C%2520as%2520well%250Aas%2520the%2520often%2520very%2520theoretical%2520presentation.%2520In%2520this%2520introduction%252C%2520we%2520take%2520an%250Aalternative%2520approach%252C%2520different%2520from%2520classic%2520reinforcement%2520learning%2520textbooks.%250ARather%2520than%2520focusing%2520on%2520tabular%2520problems%252C%2520we%2520introduce%2520reinforcement%2520learning%250Aas%2520a%2520generalization%2520of%2520supervised%2520learning%252C%2520which%2520we%2520first%2520apply%2520to%250Anon-differentiable%2520objectives%2520and%2520later%2520to%2520temporal%2520problems.%2520Assuming%2520only%250Abasic%2520knowledge%2520of%2520supervised%2520learning%252C%2520the%2520reader%2520will%2520be%2520able%2520to%2520understand%250Astate-of-the-art%2520deep%2520RL%2520algorithms%2520like%2520proximal%2520policy%2520optimization%2520%2528PPO%2529%250Aafter%2520reading%2520this%2520tutorial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08365v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Invitation%20to%20Deep%20Reinforcement%20Learning&entry.906535625=Bernhard%20Jaeger%20and%20Andreas%20Geiger&entry.1292438233=%20%20Training%20a%20deep%20neural%20network%20to%20maximize%20a%20target%20objective%20has%20become%20the%0Astandard%20recipe%20for%20successful%20machine%20learning%20over%20the%20last%20decade.%20These%0Anetworks%20can%20be%20optimized%20with%20supervised%20learning%2C%20if%20the%20target%20objective%20is%0Adifferentiable.%20For%20many%20interesting%20problems%2C%20this%20is%20however%20not%20the%20case.%0ACommon%20objectives%20like%20intersection%20over%20union%20%28IoU%29%2C%20bilingual%20evaluation%0Aunderstudy%20%28BLEU%29%20score%20or%20rewards%20cannot%20be%20optimized%20with%20supervised%0Alearning.%20A%20common%20workaround%20is%20to%20define%20differentiable%20surrogate%20losses%2C%0Aleading%20to%20suboptimal%20solutions%20with%20respect%20to%20the%20actual%20objective.%0AReinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20alternative%20for%0Aoptimizing%20deep%20neural%20networks%20to%20maximize%20non-differentiable%20objectives%20in%0Arecent%20years.%20Examples%20include%20aligning%20large%20language%20models%20via%20human%0Afeedback%2C%20code%20generation%2C%20object%20detection%20or%20control%20problems.%20This%20makes%20RL%0Atechniques%20relevant%20to%20the%20larger%20machine%20learning%20audience.%20The%20subject%20is%2C%0Ahowever%2C%20time%20intensive%20to%20approach%20due%20to%20the%20large%20range%20of%20methods%2C%20as%20well%0Aas%20the%20often%20very%20theoretical%20presentation.%20In%20this%20introduction%2C%20we%20take%20an%0Aalternative%20approach%2C%20different%20from%20classic%20reinforcement%20learning%20textbooks.%0ARather%20than%20focusing%20on%20tabular%20problems%2C%20we%20introduce%20reinforcement%20learning%0Aas%20a%20generalization%20of%20supervised%20learning%2C%20which%20we%20first%20apply%20to%0Anon-differentiable%20objectives%20and%20later%20to%20temporal%20problems.%20Assuming%20only%0Abasic%20knowledge%20of%20supervised%20learning%2C%20the%20reader%20will%20be%20able%20to%20understand%0Astate-of-the-art%20deep%20RL%20algorithms%20like%20proximal%20policy%20optimization%20%28PPO%29%0Aafter%20reading%20this%20tutorial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08365v3&entry.124074799=Read"},
{"title": "A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping\n  Semantic Community Detection", "author": "Abdelfateh Bekkair and Slimane Bellaouar and Slimane Oulad-Naoui", "abstract": "  Community detection in networks with overlapping structures remains a\nsignificant challenge, particularly in noisy real-world environments where\nintegrating topology, node attributes, and prior information is critical. To\naddress this, we propose a semi-supervised graph autoencoder that combines\ngraph multi-head attention and modularity maximization to robustly detect\noverlapping communities. The model learns semantic representations by fusing\nstructural, attribute, and prior knowledge while explicitly addressing noise in\nnode features. Key innovations include a noise-resistant architecture and a\nsemantic semi-supervised design optimized for community quality through\nmodularity constraints. Experiments demonstrate superior performance the model\noutperforms state-of-the-art methods in overlapping community detection\n(improvements in NMI and F1-score) and exhibits exceptional robustness to\nattribute noise, maintaining stable performance under 60\\% feature corruption.\nThese results highlight the importance of integrating attribute semantics and\nstructural patterns for accurate community discovery in complex networks.\n", "link": "http://arxiv.org/abs/2505.05965v1", "date": "2025-05-09", "relevancy": 2.4694, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5138}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Noise-Resilient%20Semi-Supervised%20Graph%20Autoencoder%20for%20Overlapping%0A%20%20Semantic%20Community%20Detection&body=Title%3A%20A%20Noise-Resilient%20Semi-Supervised%20Graph%20Autoencoder%20for%20Overlapping%0A%20%20Semantic%20Community%20Detection%0AAuthor%3A%20Abdelfateh%20Bekkair%20and%20Slimane%20Bellaouar%20and%20Slimane%20Oulad-Naoui%0AAbstract%3A%20%20%20Community%20detection%20in%20networks%20with%20overlapping%20structures%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20noisy%20real-world%20environments%20where%0Aintegrating%20topology%2C%20node%20attributes%2C%20and%20prior%20information%20is%20critical.%20To%0Aaddress%20this%2C%20we%20propose%20a%20semi-supervised%20graph%20autoencoder%20that%20combines%0Agraph%20multi-head%20attention%20and%20modularity%20maximization%20to%20robustly%20detect%0Aoverlapping%20communities.%20The%20model%20learns%20semantic%20representations%20by%20fusing%0Astructural%2C%20attribute%2C%20and%20prior%20knowledge%20while%20explicitly%20addressing%20noise%20in%0Anode%20features.%20Key%20innovations%20include%20a%20noise-resistant%20architecture%20and%20a%0Asemantic%20semi-supervised%20design%20optimized%20for%20community%20quality%20through%0Amodularity%20constraints.%20Experiments%20demonstrate%20superior%20performance%20the%20model%0Aoutperforms%20state-of-the-art%20methods%20in%20overlapping%20community%20detection%0A%28improvements%20in%20NMI%20and%20F1-score%29%20and%20exhibits%20exceptional%20robustness%20to%0Aattribute%20noise%2C%20maintaining%20stable%20performance%20under%2060%5C%25%20feature%20corruption.%0AThese%20results%20highlight%20the%20importance%20of%20integrating%20attribute%20semantics%20and%0Astructural%20patterns%20for%20accurate%20community%20discovery%20in%20complex%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Noise-Resilient%2520Semi-Supervised%2520Graph%2520Autoencoder%2520for%2520Overlapping%250A%2520%2520Semantic%2520Community%2520Detection%26entry.906535625%3DAbdelfateh%2520Bekkair%2520and%2520Slimane%2520Bellaouar%2520and%2520Slimane%2520Oulad-Naoui%26entry.1292438233%3D%2520%2520Community%2520detection%2520in%2520networks%2520with%2520overlapping%2520structures%2520remains%2520a%250Asignificant%2520challenge%252C%2520particularly%2520in%2520noisy%2520real-world%2520environments%2520where%250Aintegrating%2520topology%252C%2520node%2520attributes%252C%2520and%2520prior%2520information%2520is%2520critical.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520semi-supervised%2520graph%2520autoencoder%2520that%2520combines%250Agraph%2520multi-head%2520attention%2520and%2520modularity%2520maximization%2520to%2520robustly%2520detect%250Aoverlapping%2520communities.%2520The%2520model%2520learns%2520semantic%2520representations%2520by%2520fusing%250Astructural%252C%2520attribute%252C%2520and%2520prior%2520knowledge%2520while%2520explicitly%2520addressing%2520noise%2520in%250Anode%2520features.%2520Key%2520innovations%2520include%2520a%2520noise-resistant%2520architecture%2520and%2520a%250Asemantic%2520semi-supervised%2520design%2520optimized%2520for%2520community%2520quality%2520through%250Amodularity%2520constraints.%2520Experiments%2520demonstrate%2520superior%2520performance%2520the%2520model%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520overlapping%2520community%2520detection%250A%2528improvements%2520in%2520NMI%2520and%2520F1-score%2529%2520and%2520exhibits%2520exceptional%2520robustness%2520to%250Aattribute%2520noise%252C%2520maintaining%2520stable%2520performance%2520under%252060%255C%2525%2520feature%2520corruption.%250AThese%2520results%2520highlight%2520the%2520importance%2520of%2520integrating%2520attribute%2520semantics%2520and%250Astructural%2520patterns%2520for%2520accurate%2520community%2520discovery%2520in%2520complex%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Noise-Resilient%20Semi-Supervised%20Graph%20Autoencoder%20for%20Overlapping%0A%20%20Semantic%20Community%20Detection&entry.906535625=Abdelfateh%20Bekkair%20and%20Slimane%20Bellaouar%20and%20Slimane%20Oulad-Naoui&entry.1292438233=%20%20Community%20detection%20in%20networks%20with%20overlapping%20structures%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20noisy%20real-world%20environments%20where%0Aintegrating%20topology%2C%20node%20attributes%2C%20and%20prior%20information%20is%20critical.%20To%0Aaddress%20this%2C%20we%20propose%20a%20semi-supervised%20graph%20autoencoder%20that%20combines%0Agraph%20multi-head%20attention%20and%20modularity%20maximization%20to%20robustly%20detect%0Aoverlapping%20communities.%20The%20model%20learns%20semantic%20representations%20by%20fusing%0Astructural%2C%20attribute%2C%20and%20prior%20knowledge%20while%20explicitly%20addressing%20noise%20in%0Anode%20features.%20Key%20innovations%20include%20a%20noise-resistant%20architecture%20and%20a%0Asemantic%20semi-supervised%20design%20optimized%20for%20community%20quality%20through%0Amodularity%20constraints.%20Experiments%20demonstrate%20superior%20performance%20the%20model%0Aoutperforms%20state-of-the-art%20methods%20in%20overlapping%20community%20detection%0A%28improvements%20in%20NMI%20and%20F1-score%29%20and%20exhibits%20exceptional%20robustness%20to%0Aattribute%20noise%2C%20maintaining%20stable%20performance%20under%2060%5C%25%20feature%20corruption.%0AThese%20results%20highlight%20the%20importance%20of%20integrating%20attribute%20semantics%20and%0Astructural%20patterns%20for%20accurate%20community%20discovery%20in%20complex%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05965v1&entry.124074799=Read"},
{"title": "FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image\n  Registration", "author": "Ying Zhang and Shuai Guo and Chenxi Sun and Yuchen Zhu and Jinhai Xiang", "abstract": "  In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient.\n", "link": "http://arxiv.org/abs/2505.04938v2", "date": "2025-05-09", "relevancy": 2.4622, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FF-PNet%3A%20A%20Pyramid%20Network%20Based%20on%20Feature%20and%20Field%20for%20Brain%20Image%0A%20%20Registration&body=Title%3A%20FF-PNet%3A%20A%20Pyramid%20Network%20Based%20on%20Feature%20and%20Field%20for%20Brain%20Image%0A%20%20Registration%0AAuthor%3A%20Ying%20Zhang%20and%20Shuai%20Guo%20and%20Chenxi%20Sun%20and%20Yuchen%20Zhu%20and%20Jinhai%20Xiang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deformable%20medical%20image%20registration%20techniques%20have%20made%0Asignificant%20progress.%20However%2C%20existing%20models%20still%20lack%20efficiency%20in%0Aparallel%20extraction%20of%20coarse%20and%20fine-grained%20features.%20To%20address%20this%2C%20we%0Aconstruct%20a%20new%20pyramid%20registration%20network%20based%20on%20feature%20and%20deformation%0Afield%20%28FF-PNet%29.%20For%20coarse-grained%20feature%20extraction%2C%20we%20design%20a%20Residual%0AFeature%20Fusion%20Module%20%28RFFM%29%2C%20for%20fine-grained%20image%20deformation%2C%20we%20propose%20a%0AResidual%20Deformation%20Field%20Fusion%20Module%20%28RDFFM%29.%20Through%20the%20parallel%0Aoperation%20of%20these%20two%20modules%2C%20the%20model%20can%20effectively%20handle%20complex%20image%0Adeformations.%20It%20is%20worth%20emphasizing%20that%20the%20encoding%20stage%20of%20FF-PNet%20only%0Aemploys%20traditional%20convolutional%20neural%20networks%20without%20any%20attention%0Amechanisms%20or%20multilayer%20perceptrons%2C%20yet%20it%20still%20achieves%20remarkable%0Aimprovements%20in%20registration%20accuracy%2C%20fully%20demonstrating%20the%20superior%20feature%0Adecoding%20capabilities%20of%20RFFM%20and%20RDFFM.%20We%20conducted%20extensive%20experiments%20on%0Athe%20LPBA%20and%20OASIS%20datasets.%20The%20results%20show%20our%20network%20consistently%0Aoutperforms%20popular%20methods%20in%20metrics%20like%20the%20Dice%20Similarity%20Coefficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04938v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFF-PNet%253A%2520A%2520Pyramid%2520Network%2520Based%2520on%2520Feature%2520and%2520Field%2520for%2520Brain%2520Image%250A%2520%2520Registration%26entry.906535625%3DYing%2520Zhang%2520and%2520Shuai%2520Guo%2520and%2520Chenxi%2520Sun%2520and%2520Yuchen%2520Zhu%2520and%2520Jinhai%2520Xiang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deformable%2520medical%2520image%2520registration%2520techniques%2520have%2520made%250Asignificant%2520progress.%2520However%252C%2520existing%2520models%2520still%2520lack%2520efficiency%2520in%250Aparallel%2520extraction%2520of%2520coarse%2520and%2520fine-grained%2520features.%2520To%2520address%2520this%252C%2520we%250Aconstruct%2520a%2520new%2520pyramid%2520registration%2520network%2520based%2520on%2520feature%2520and%2520deformation%250Afield%2520%2528FF-PNet%2529.%2520For%2520coarse-grained%2520feature%2520extraction%252C%2520we%2520design%2520a%2520Residual%250AFeature%2520Fusion%2520Module%2520%2528RFFM%2529%252C%2520for%2520fine-grained%2520image%2520deformation%252C%2520we%2520propose%2520a%250AResidual%2520Deformation%2520Field%2520Fusion%2520Module%2520%2528RDFFM%2529.%2520Through%2520the%2520parallel%250Aoperation%2520of%2520these%2520two%2520modules%252C%2520the%2520model%2520can%2520effectively%2520handle%2520complex%2520image%250Adeformations.%2520It%2520is%2520worth%2520emphasizing%2520that%2520the%2520encoding%2520stage%2520of%2520FF-PNet%2520only%250Aemploys%2520traditional%2520convolutional%2520neural%2520networks%2520without%2520any%2520attention%250Amechanisms%2520or%2520multilayer%2520perceptrons%252C%2520yet%2520it%2520still%2520achieves%2520remarkable%250Aimprovements%2520in%2520registration%2520accuracy%252C%2520fully%2520demonstrating%2520the%2520superior%2520feature%250Adecoding%2520capabilities%2520of%2520RFFM%2520and%2520RDFFM.%2520We%2520conducted%2520extensive%2520experiments%2520on%250Athe%2520LPBA%2520and%2520OASIS%2520datasets.%2520The%2520results%2520show%2520our%2520network%2520consistently%250Aoutperforms%2520popular%2520methods%2520in%2520metrics%2520like%2520the%2520Dice%2520Similarity%2520Coefficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04938v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FF-PNet%3A%20A%20Pyramid%20Network%20Based%20on%20Feature%20and%20Field%20for%20Brain%20Image%0A%20%20Registration&entry.906535625=Ying%20Zhang%20and%20Shuai%20Guo%20and%20Chenxi%20Sun%20and%20Yuchen%20Zhu%20and%20Jinhai%20Xiang&entry.1292438233=%20%20In%20recent%20years%2C%20deformable%20medical%20image%20registration%20techniques%20have%20made%0Asignificant%20progress.%20However%2C%20existing%20models%20still%20lack%20efficiency%20in%0Aparallel%20extraction%20of%20coarse%20and%20fine-grained%20features.%20To%20address%20this%2C%20we%0Aconstruct%20a%20new%20pyramid%20registration%20network%20based%20on%20feature%20and%20deformation%0Afield%20%28FF-PNet%29.%20For%20coarse-grained%20feature%20extraction%2C%20we%20design%20a%20Residual%0AFeature%20Fusion%20Module%20%28RFFM%29%2C%20for%20fine-grained%20image%20deformation%2C%20we%20propose%20a%0AResidual%20Deformation%20Field%20Fusion%20Module%20%28RDFFM%29.%20Through%20the%20parallel%0Aoperation%20of%20these%20two%20modules%2C%20the%20model%20can%20effectively%20handle%20complex%20image%0Adeformations.%20It%20is%20worth%20emphasizing%20that%20the%20encoding%20stage%20of%20FF-PNet%20only%0Aemploys%20traditional%20convolutional%20neural%20networks%20without%20any%20attention%0Amechanisms%20or%20multilayer%20perceptrons%2C%20yet%20it%20still%20achieves%20remarkable%0Aimprovements%20in%20registration%20accuracy%2C%20fully%20demonstrating%20the%20superior%20feature%0Adecoding%20capabilities%20of%20RFFM%20and%20RDFFM.%20We%20conducted%20extensive%20experiments%20on%0Athe%20LPBA%20and%20OASIS%20datasets.%20The%20results%20show%20our%20network%20consistently%0Aoutperforms%20popular%20methods%20in%20metrics%20like%20the%20Dice%20Similarity%20Coefficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04938v2&entry.124074799=Read"},
{"title": "The Application of Deep Learning for Lymph Node Segmentation: A\n  Systematic Review", "author": "Jingguo Qu and Xinyang Han and Man-Lik Chui and Yao Pu and Simon Takadiyi Gunda and Ziman Chen and Jing Qin and Ann Dorothy King and Winnie Chiu-Wing Chu and Jing Cai and Michael Tin-Cheung Ying", "abstract": "  Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies.\n", "link": "http://arxiv.org/abs/2505.06118v1", "date": "2025-05-09", "relevancy": 2.451, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Application%20of%20Deep%20Learning%20for%20Lymph%20Node%20Segmentation%3A%20A%0A%20%20Systematic%20Review&body=Title%3A%20The%20Application%20of%20Deep%20Learning%20for%20Lymph%20Node%20Segmentation%3A%20A%0A%20%20Systematic%20Review%0AAuthor%3A%20Jingguo%20Qu%20and%20Xinyang%20Han%20and%20Man-Lik%20Chui%20and%20Yao%20Pu%20and%20Simon%20Takadiyi%20Gunda%20and%20Ziman%20Chen%20and%20Jing%20Qin%20and%20Ann%20Dorothy%20King%20and%20Winnie%20Chiu-Wing%20Chu%20and%20Jing%20Cai%20and%20Michael%20Tin-Cheung%20Ying%0AAbstract%3A%20%20%20Automatic%20lymph%20node%20segmentation%20is%20the%20cornerstone%20for%20advances%20in%20computer%0Avision%20tasks%20for%20early%20detection%20and%20staging%20of%20cancer.%20Traditional%0Asegmentation%20methods%20are%20constrained%20by%20manual%20delineation%20and%20variability%20in%0Aoperator%20proficiency%2C%20limiting%20their%20ability%20to%20achieve%20high%20accuracy.%20The%0Aintroduction%20of%20deep%20learning%20technologies%20offers%20new%20possibilities%20for%0Aimproving%20the%20accuracy%20of%20lymph%20node%20image%20analysis.%20This%20study%20evaluates%20the%0Aapplication%20of%20deep%20learning%20in%20lymph%20node%20segmentation%20and%20discusses%20the%0Amethodologies%20of%20various%20deep%20learning%20architectures%20such%20as%20convolutional%0Aneural%20networks%2C%20encoder-decoder%20networks%2C%20and%20transformers%20in%20analyzing%0Amedical%20imaging%20data%20across%20different%20modalities.%20Despite%20the%20advancements%2C%20it%0Astill%20confronts%20challenges%20like%20the%20shape%20diversity%20of%20lymph%20nodes%2C%20the%0Ascarcity%20of%20accurately%20labeled%20datasets%2C%20and%20the%20inadequate%20development%20of%0Amethods%20that%20are%20robust%20and%20generalizable%20across%20different%20imaging%20modalities.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20that%20provides%20a%0Acomprehensive%20overview%20of%20the%20application%20of%20deep%20learning%20techniques%20in%20lymph%0Anode%20segmentation%20task.%20Furthermore%2C%20this%20study%20also%20explores%20potential%20future%0Aresearch%20directions%2C%20including%20multimodal%20fusion%20techniques%2C%20transfer%20learning%2C%0Aand%20the%20use%20of%20large-scale%20pre-trained%20models%20to%20overcome%20current%20limitations%0Awhile%20enhancing%20cancer%20diagnosis%20and%20treatment%20planning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Application%2520of%2520Deep%2520Learning%2520for%2520Lymph%2520Node%2520Segmentation%253A%2520A%250A%2520%2520Systematic%2520Review%26entry.906535625%3DJingguo%2520Qu%2520and%2520Xinyang%2520Han%2520and%2520Man-Lik%2520Chui%2520and%2520Yao%2520Pu%2520and%2520Simon%2520Takadiyi%2520Gunda%2520and%2520Ziman%2520Chen%2520and%2520Jing%2520Qin%2520and%2520Ann%2520Dorothy%2520King%2520and%2520Winnie%2520Chiu-Wing%2520Chu%2520and%2520Jing%2520Cai%2520and%2520Michael%2520Tin-Cheung%2520Ying%26entry.1292438233%3D%2520%2520Automatic%2520lymph%2520node%2520segmentation%2520is%2520the%2520cornerstone%2520for%2520advances%2520in%2520computer%250Avision%2520tasks%2520for%2520early%2520detection%2520and%2520staging%2520of%2520cancer.%2520Traditional%250Asegmentation%2520methods%2520are%2520constrained%2520by%2520manual%2520delineation%2520and%2520variability%2520in%250Aoperator%2520proficiency%252C%2520limiting%2520their%2520ability%2520to%2520achieve%2520high%2520accuracy.%2520The%250Aintroduction%2520of%2520deep%2520learning%2520technologies%2520offers%2520new%2520possibilities%2520for%250Aimproving%2520the%2520accuracy%2520of%2520lymph%2520node%2520image%2520analysis.%2520This%2520study%2520evaluates%2520the%250Aapplication%2520of%2520deep%2520learning%2520in%2520lymph%2520node%2520segmentation%2520and%2520discusses%2520the%250Amethodologies%2520of%2520various%2520deep%2520learning%2520architectures%2520such%2520as%2520convolutional%250Aneural%2520networks%252C%2520encoder-decoder%2520networks%252C%2520and%2520transformers%2520in%2520analyzing%250Amedical%2520imaging%2520data%2520across%2520different%2520modalities.%2520Despite%2520the%2520advancements%252C%2520it%250Astill%2520confronts%2520challenges%2520like%2520the%2520shape%2520diversity%2520of%2520lymph%2520nodes%252C%2520the%250Ascarcity%2520of%2520accurately%2520labeled%2520datasets%252C%2520and%2520the%2520inadequate%2520development%2520of%250Amethods%2520that%2520are%2520robust%2520and%2520generalizable%2520across%2520different%2520imaging%2520modalities.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520that%2520provides%2520a%250Acomprehensive%2520overview%2520of%2520the%2520application%2520of%2520deep%2520learning%2520techniques%2520in%2520lymph%250Anode%2520segmentation%2520task.%2520Furthermore%252C%2520this%2520study%2520also%2520explores%2520potential%2520future%250Aresearch%2520directions%252C%2520including%2520multimodal%2520fusion%2520techniques%252C%2520transfer%2520learning%252C%250Aand%2520the%2520use%2520of%2520large-scale%2520pre-trained%2520models%2520to%2520overcome%2520current%2520limitations%250Awhile%2520enhancing%2520cancer%2520diagnosis%2520and%2520treatment%2520planning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Application%20of%20Deep%20Learning%20for%20Lymph%20Node%20Segmentation%3A%20A%0A%20%20Systematic%20Review&entry.906535625=Jingguo%20Qu%20and%20Xinyang%20Han%20and%20Man-Lik%20Chui%20and%20Yao%20Pu%20and%20Simon%20Takadiyi%20Gunda%20and%20Ziman%20Chen%20and%20Jing%20Qin%20and%20Ann%20Dorothy%20King%20and%20Winnie%20Chiu-Wing%20Chu%20and%20Jing%20Cai%20and%20Michael%20Tin-Cheung%20Ying&entry.1292438233=%20%20Automatic%20lymph%20node%20segmentation%20is%20the%20cornerstone%20for%20advances%20in%20computer%0Avision%20tasks%20for%20early%20detection%20and%20staging%20of%20cancer.%20Traditional%0Asegmentation%20methods%20are%20constrained%20by%20manual%20delineation%20and%20variability%20in%0Aoperator%20proficiency%2C%20limiting%20their%20ability%20to%20achieve%20high%20accuracy.%20The%0Aintroduction%20of%20deep%20learning%20technologies%20offers%20new%20possibilities%20for%0Aimproving%20the%20accuracy%20of%20lymph%20node%20image%20analysis.%20This%20study%20evaluates%20the%0Aapplication%20of%20deep%20learning%20in%20lymph%20node%20segmentation%20and%20discusses%20the%0Amethodologies%20of%20various%20deep%20learning%20architectures%20such%20as%20convolutional%0Aneural%20networks%2C%20encoder-decoder%20networks%2C%20and%20transformers%20in%20analyzing%0Amedical%20imaging%20data%20across%20different%20modalities.%20Despite%20the%20advancements%2C%20it%0Astill%20confronts%20challenges%20like%20the%20shape%20diversity%20of%20lymph%20nodes%2C%20the%0Ascarcity%20of%20accurately%20labeled%20datasets%2C%20and%20the%20inadequate%20development%20of%0Amethods%20that%20are%20robust%20and%20generalizable%20across%20different%20imaging%20modalities.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20that%20provides%20a%0Acomprehensive%20overview%20of%20the%20application%20of%20deep%20learning%20techniques%20in%20lymph%0Anode%20segmentation%20task.%20Furthermore%2C%20this%20study%20also%20explores%20potential%20future%0Aresearch%20directions%2C%20including%20multimodal%20fusion%20techniques%2C%20transfer%20learning%2C%0Aand%20the%20use%20of%20large-scale%20pre-trained%20models%20to%20overcome%20current%20limitations%0Awhile%20enhancing%20cancer%20diagnosis%20and%20treatment%20planning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06118v1&entry.124074799=Read"},
{"title": "Learning Music Audio Representations With Limited Data", "author": "Christos Plachouras and Emmanouil Benetos and Johan Pauwels", "abstract": "  Large deep-learning models for music, including those focused on learning\ngeneral-purpose music audio representations, are often assumed to require\nsubstantial training data to achieve high performance. If true, this would pose\nchallenges in scenarios where audio data or annotations are scarce, such as for\nunderrepresented music traditions, non-popular genres, and personalized music\ncreation and listening. Understanding how these models behave in limited-data\nscenarios could be crucial for developing techniques to tackle them.\n  In this work, we investigate the behavior of several music audio\nrepresentation models under limited-data learning regimes. We consider music\nmodels with various architectures, training paradigms, and input durations, and\ntrain them on data collections ranging from 5 to 8,000 minutes long. We\nevaluate the learned representations on various music information retrieval\ntasks and analyze their robustness to noise. We show that, under certain\nconditions, representations from limited-data and even random models perform\ncomparably to ones from large-dataset models, though handcrafted features\noutperform all learned representations in some tasks.\n", "link": "http://arxiv.org/abs/2505.06042v1", "date": "2025-05-09", "relevancy": 2.4289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Music%20Audio%20Representations%20With%20Limited%20Data&body=Title%3A%20Learning%20Music%20Audio%20Representations%20With%20Limited%20Data%0AAuthor%3A%20Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Johan%20Pauwels%0AAbstract%3A%20%20%20Large%20deep-learning%20models%20for%20music%2C%20including%20those%20focused%20on%20learning%0Ageneral-purpose%20music%20audio%20representations%2C%20are%20often%20assumed%20to%20require%0Asubstantial%20training%20data%20to%20achieve%20high%20performance.%20If%20true%2C%20this%20would%20pose%0Achallenges%20in%20scenarios%20where%20audio%20data%20or%20annotations%20are%20scarce%2C%20such%20as%20for%0Aunderrepresented%20music%20traditions%2C%20non-popular%20genres%2C%20and%20personalized%20music%0Acreation%20and%20listening.%20Understanding%20how%20these%20models%20behave%20in%20limited-data%0Ascenarios%20could%20be%20crucial%20for%20developing%20techniques%20to%20tackle%20them.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20behavior%20of%20several%20music%20audio%0Arepresentation%20models%20under%20limited-data%20learning%20regimes.%20We%20consider%20music%0Amodels%20with%20various%20architectures%2C%20training%20paradigms%2C%20and%20input%20durations%2C%20and%0Atrain%20them%20on%20data%20collections%20ranging%20from%205%20to%208%2C000%20minutes%20long.%20We%0Aevaluate%20the%20learned%20representations%20on%20various%20music%20information%20retrieval%0Atasks%20and%20analyze%20their%20robustness%20to%20noise.%20We%20show%20that%2C%20under%20certain%0Aconditions%2C%20representations%20from%20limited-data%20and%20even%20random%20models%20perform%0Acomparably%20to%20ones%20from%20large-dataset%20models%2C%20though%20handcrafted%20features%0Aoutperform%20all%20learned%20representations%20in%20some%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Music%2520Audio%2520Representations%2520With%2520Limited%2520Data%26entry.906535625%3DChristos%2520Plachouras%2520and%2520Emmanouil%2520Benetos%2520and%2520Johan%2520Pauwels%26entry.1292438233%3D%2520%2520Large%2520deep-learning%2520models%2520for%2520music%252C%2520including%2520those%2520focused%2520on%2520learning%250Ageneral-purpose%2520music%2520audio%2520representations%252C%2520are%2520often%2520assumed%2520to%2520require%250Asubstantial%2520training%2520data%2520to%2520achieve%2520high%2520performance.%2520If%2520true%252C%2520this%2520would%2520pose%250Achallenges%2520in%2520scenarios%2520where%2520audio%2520data%2520or%2520annotations%2520are%2520scarce%252C%2520such%2520as%2520for%250Aunderrepresented%2520music%2520traditions%252C%2520non-popular%2520genres%252C%2520and%2520personalized%2520music%250Acreation%2520and%2520listening.%2520Understanding%2520how%2520these%2520models%2520behave%2520in%2520limited-data%250Ascenarios%2520could%2520be%2520crucial%2520for%2520developing%2520techniques%2520to%2520tackle%2520them.%250A%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520behavior%2520of%2520several%2520music%2520audio%250Arepresentation%2520models%2520under%2520limited-data%2520learning%2520regimes.%2520We%2520consider%2520music%250Amodels%2520with%2520various%2520architectures%252C%2520training%2520paradigms%252C%2520and%2520input%2520durations%252C%2520and%250Atrain%2520them%2520on%2520data%2520collections%2520ranging%2520from%25205%2520to%25208%252C000%2520minutes%2520long.%2520We%250Aevaluate%2520the%2520learned%2520representations%2520on%2520various%2520music%2520information%2520retrieval%250Atasks%2520and%2520analyze%2520their%2520robustness%2520to%2520noise.%2520We%2520show%2520that%252C%2520under%2520certain%250Aconditions%252C%2520representations%2520from%2520limited-data%2520and%2520even%2520random%2520models%2520perform%250Acomparably%2520to%2520ones%2520from%2520large-dataset%2520models%252C%2520though%2520handcrafted%2520features%250Aoutperform%2520all%2520learned%2520representations%2520in%2520some%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Music%20Audio%20Representations%20With%20Limited%20Data&entry.906535625=Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Johan%20Pauwels&entry.1292438233=%20%20Large%20deep-learning%20models%20for%20music%2C%20including%20those%20focused%20on%20learning%0Ageneral-purpose%20music%20audio%20representations%2C%20are%20often%20assumed%20to%20require%0Asubstantial%20training%20data%20to%20achieve%20high%20performance.%20If%20true%2C%20this%20would%20pose%0Achallenges%20in%20scenarios%20where%20audio%20data%20or%20annotations%20are%20scarce%2C%20such%20as%20for%0Aunderrepresented%20music%20traditions%2C%20non-popular%20genres%2C%20and%20personalized%20music%0Acreation%20and%20listening.%20Understanding%20how%20these%20models%20behave%20in%20limited-data%0Ascenarios%20could%20be%20crucial%20for%20developing%20techniques%20to%20tackle%20them.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20behavior%20of%20several%20music%20audio%0Arepresentation%20models%20under%20limited-data%20learning%20regimes.%20We%20consider%20music%0Amodels%20with%20various%20architectures%2C%20training%20paradigms%2C%20and%20input%20durations%2C%20and%0Atrain%20them%20on%20data%20collections%20ranging%20from%205%20to%208%2C000%20minutes%20long.%20We%0Aevaluate%20the%20learned%20representations%20on%20various%20music%20information%20retrieval%0Atasks%20and%20analyze%20their%20robustness%20to%20noise.%20We%20show%20that%2C%20under%20certain%0Aconditions%2C%20representations%20from%20limited-data%20and%20even%20random%20models%20perform%0Acomparably%20to%20ones%20from%20large-dataset%20models%2C%20though%20handcrafted%20features%0Aoutperform%20all%20learned%20representations%20in%20some%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06042v1&entry.124074799=Read"},
{"title": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for\n  Few-shot Action Recognition", "author": "Congqi Cao and Peiheng Han and Yueran zhang and Yating Yu and Qinyi Lv and Lingtong Min and Yanning zhang", "abstract": "  Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp.\n", "link": "http://arxiv.org/abs/2505.06002v1", "date": "2025-05-09", "relevancy": 2.4187, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6602}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5666}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Adapter%2B%2B%3A%20Task-specific%20Adaptation%20with%20Order-aware%20Alignment%20for%0A%20%20Few-shot%20Action%20Recognition&body=Title%3A%20Task-Adapter%2B%2B%3A%20Task-specific%20Adaptation%20with%20Order-aware%20Alignment%20for%0A%20%20Few-shot%20Action%20Recognition%0AAuthor%3A%20Congqi%20Cao%20and%20Peiheng%20Han%20and%20Yueran%20zhang%20and%20Yating%20Yu%20and%20Qinyi%20Lv%20and%20Lingtong%20Min%20and%20Yanning%20zhang%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20models%20have%20achieved%20remarkable%20success%20in%20language%0Aand%20image%20tasks%2C%20leading%20an%20increasing%20number%20of%20studies%20to%20explore%20the%0Aapplication%20of%20pre-trained%20image%20models%2C%20such%20as%20CLIP%2C%20in%20the%20domain%20of%0Afew-shot%20action%20recognition%20%28FSAR%29.%20However%2C%20current%20methods%20generally%20suffer%0Afrom%20several%20problems%3A%201%29%20Direct%20fine-tuning%20often%20undermines%20the%0Ageneralization%20capability%20of%20the%20pre-trained%20model%3B%202%29%20The%20exploration%20of%0Atask-specific%20information%20is%20insufficient%20in%20the%20visual%20tasks%3B%203%29%20The%20semantic%0Aorder%20information%20is%20typically%20overlooked%20during%20text%20modeling%3B%204%29%20Existing%0Across-modal%20alignment%20techniques%20ignore%20the%20temporal%20coupling%20of%20multimodal%0Ainformation.%20To%20address%20these%2C%20we%20propose%20Task-Adapter%2B%2B%2C%20a%20parameter-efficient%0Adual%20adaptation%20method%20for%20both%20image%20and%20text%20encoders.%20Specifically%2C%20to%20make%0Afull%20use%20of%20the%20variations%20across%20different%20few-shot%20learning%20tasks%2C%20we%20design%0Aa%20task-specific%20adaptation%20for%20the%20image%20encoder%20so%20that%20the%20most%0Adiscriminative%20information%20can%20be%20well%20noticed%20during%20feature%20extraction.%0AFurthermore%2C%20we%20leverage%20large%20language%20models%20%28LLMs%29%20to%20generate%20detailed%0Asequential%20sub-action%20descriptions%20for%20each%20action%20class%2C%20and%20introduce%0Asemantic%20order%20adapters%20into%20the%20text%20encoder%20to%20effectively%20model%20the%0Asequential%20relationships%20between%20these%20sub-actions.%20Finally%2C%20we%20develop%20an%0Ainnovative%20fine-grained%20cross-modal%20alignment%20strategy%20that%20actively%20maps%0Avisual%20features%20to%20reside%20in%20the%20same%20temporal%20stage%20as%20semantic%20descriptions.%0AExtensive%20experiments%20fully%20demonstrate%20the%20effectiveness%20and%20superiority%20of%0Athe%20proposed%20method%2C%20which%20achieves%20state-of-the-art%20performance%20on%205%0Abenchmarks%20consistently.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Jaulin-Bage/Task-Adapter-pp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Adapter%252B%252B%253A%2520Task-specific%2520Adaptation%2520with%2520Order-aware%2520Alignment%2520for%250A%2520%2520Few-shot%2520Action%2520Recognition%26entry.906535625%3DCongqi%2520Cao%2520and%2520Peiheng%2520Han%2520and%2520Yueran%2520zhang%2520and%2520Yating%2520Yu%2520and%2520Qinyi%2520Lv%2520and%2520Lingtong%2520Min%2520and%2520Yanning%2520zhang%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520language%250Aand%2520image%2520tasks%252C%2520leading%2520an%2520increasing%2520number%2520of%2520studies%2520to%2520explore%2520the%250Aapplication%2520of%2520pre-trained%2520image%2520models%252C%2520such%2520as%2520CLIP%252C%2520in%2520the%2520domain%2520of%250Afew-shot%2520action%2520recognition%2520%2528FSAR%2529.%2520However%252C%2520current%2520methods%2520generally%2520suffer%250Afrom%2520several%2520problems%253A%25201%2529%2520Direct%2520fine-tuning%2520often%2520undermines%2520the%250Ageneralization%2520capability%2520of%2520the%2520pre-trained%2520model%253B%25202%2529%2520The%2520exploration%2520of%250Atask-specific%2520information%2520is%2520insufficient%2520in%2520the%2520visual%2520tasks%253B%25203%2529%2520The%2520semantic%250Aorder%2520information%2520is%2520typically%2520overlooked%2520during%2520text%2520modeling%253B%25204%2529%2520Existing%250Across-modal%2520alignment%2520techniques%2520ignore%2520the%2520temporal%2520coupling%2520of%2520multimodal%250Ainformation.%2520To%2520address%2520these%252C%2520we%2520propose%2520Task-Adapter%252B%252B%252C%2520a%2520parameter-efficient%250Adual%2520adaptation%2520method%2520for%2520both%2520image%2520and%2520text%2520encoders.%2520Specifically%252C%2520to%2520make%250Afull%2520use%2520of%2520the%2520variations%2520across%2520different%2520few-shot%2520learning%2520tasks%252C%2520we%2520design%250Aa%2520task-specific%2520adaptation%2520for%2520the%2520image%2520encoder%2520so%2520that%2520the%2520most%250Adiscriminative%2520information%2520can%2520be%2520well%2520noticed%2520during%2520feature%2520extraction.%250AFurthermore%252C%2520we%2520leverage%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520detailed%250Asequential%2520sub-action%2520descriptions%2520for%2520each%2520action%2520class%252C%2520and%2520introduce%250Asemantic%2520order%2520adapters%2520into%2520the%2520text%2520encoder%2520to%2520effectively%2520model%2520the%250Asequential%2520relationships%2520between%2520these%2520sub-actions.%2520Finally%252C%2520we%2520develop%2520an%250Ainnovative%2520fine-grained%2520cross-modal%2520alignment%2520strategy%2520that%2520actively%2520maps%250Avisual%2520features%2520to%2520reside%2520in%2520the%2520same%2520temporal%2520stage%2520as%2520semantic%2520descriptions.%250AExtensive%2520experiments%2520fully%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%250Athe%2520proposed%2520method%252C%2520which%2520achieves%2520state-of-the-art%2520performance%2520on%25205%250Abenchmarks%2520consistently.%2520The%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/Jaulin-Bage/Task-Adapter-pp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Adapter%2B%2B%3A%20Task-specific%20Adaptation%20with%20Order-aware%20Alignment%20for%0A%20%20Few-shot%20Action%20Recognition&entry.906535625=Congqi%20Cao%20and%20Peiheng%20Han%20and%20Yueran%20zhang%20and%20Yating%20Yu%20and%20Qinyi%20Lv%20and%20Lingtong%20Min%20and%20Yanning%20zhang&entry.1292438233=%20%20Large-scale%20pre-trained%20models%20have%20achieved%20remarkable%20success%20in%20language%0Aand%20image%20tasks%2C%20leading%20an%20increasing%20number%20of%20studies%20to%20explore%20the%0Aapplication%20of%20pre-trained%20image%20models%2C%20such%20as%20CLIP%2C%20in%20the%20domain%20of%0Afew-shot%20action%20recognition%20%28FSAR%29.%20However%2C%20current%20methods%20generally%20suffer%0Afrom%20several%20problems%3A%201%29%20Direct%20fine-tuning%20often%20undermines%20the%0Ageneralization%20capability%20of%20the%20pre-trained%20model%3B%202%29%20The%20exploration%20of%0Atask-specific%20information%20is%20insufficient%20in%20the%20visual%20tasks%3B%203%29%20The%20semantic%0Aorder%20information%20is%20typically%20overlooked%20during%20text%20modeling%3B%204%29%20Existing%0Across-modal%20alignment%20techniques%20ignore%20the%20temporal%20coupling%20of%20multimodal%0Ainformation.%20To%20address%20these%2C%20we%20propose%20Task-Adapter%2B%2B%2C%20a%20parameter-efficient%0Adual%20adaptation%20method%20for%20both%20image%20and%20text%20encoders.%20Specifically%2C%20to%20make%0Afull%20use%20of%20the%20variations%20across%20different%20few-shot%20learning%20tasks%2C%20we%20design%0Aa%20task-specific%20adaptation%20for%20the%20image%20encoder%20so%20that%20the%20most%0Adiscriminative%20information%20can%20be%20well%20noticed%20during%20feature%20extraction.%0AFurthermore%2C%20we%20leverage%20large%20language%20models%20%28LLMs%29%20to%20generate%20detailed%0Asequential%20sub-action%20descriptions%20for%20each%20action%20class%2C%20and%20introduce%0Asemantic%20order%20adapters%20into%20the%20text%20encoder%20to%20effectively%20model%20the%0Asequential%20relationships%20between%20these%20sub-actions.%20Finally%2C%20we%20develop%20an%0Ainnovative%20fine-grained%20cross-modal%20alignment%20strategy%20that%20actively%20maps%0Avisual%20features%20to%20reside%20in%20the%20same%20temporal%20stage%20as%20semantic%20descriptions.%0AExtensive%20experiments%20fully%20demonstrate%20the%20effectiveness%20and%20superiority%20of%0Athe%20proposed%20method%2C%20which%20achieves%20state-of-the-art%20performance%20on%205%0Abenchmarks%20consistently.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/Jaulin-Bage/Task-Adapter-pp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06002v1&entry.124074799=Read"},
{"title": "Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner\n  Structure of Networks", "author": "Gouki Minegishi and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Grokking is an intriguing phenomenon of delayed generalization, where neural\nnetworks initially memorize training data with perfect accuracy but exhibit\npoor generalization, subsequently transitioning to a generalizing solution with\ncontinued training. While factors such as weight norms and sparsity have been\nproposed to explain this delayed generalization, the influence of network\nstructure remains underexplored. In this work, we link the grokking phenomenon\nto the lottery ticket hypothesis to investigate the impact of internal network\nstructures. We demonstrate that utilizing lottery tickets obtained during the\ngeneralizing phase (termed grokked tickets) significantly reduces delayed\ngeneralization across various tasks, including multiple modular arithmetic\noperations, polynomial regression, sparse parity, and MNIST classification.\nThrough controlled experiments, we show that the mitigation of delayed\ngeneralization is not due solely to reduced weight norms or increased sparsity,\nbut rather to the discovery of good subnetworks. Furthermore, we find that\ngrokked tickets exhibit periodic weight patterns, beneficial graph properties\nsuch as increased average path lengths and reduced clustering coefficients, and\nundergo rapid structural changes that coincide with improvements in\ngeneralization. Additionally, pruning techniques like the edge-popup algorithm\ncan identify these effective structures without modifying the weights, thereby\ntransforming memorizing networks into generalizing ones. These results\nunderscore the novel insight that structural exploration plays a pivotal role\nin understanding grokking. The implementation code can be accessed via this\nlink: https://github.com/gouki510/Grokking-Tickets.\n", "link": "http://arxiv.org/abs/2310.19470v3", "date": "2025-05-09", "relevancy": 2.38, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4803}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4782}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Lottery%20Ticket%20and%20Grokking%3A%20Understanding%20Grokking%20from%20Inner%0A%20%20Structure%20of%20Networks&body=Title%3A%20Bridging%20Lottery%20Ticket%20and%20Grokking%3A%20Understanding%20Grokking%20from%20Inner%0A%20%20Structure%20of%20Networks%0AAuthor%3A%20Gouki%20Minegishi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Grokking%20is%20an%20intriguing%20phenomenon%20of%20delayed%20generalization%2C%20where%20neural%0Anetworks%20initially%20memorize%20training%20data%20with%20perfect%20accuracy%20but%20exhibit%0Apoor%20generalization%2C%20subsequently%20transitioning%20to%20a%20generalizing%20solution%20with%0Acontinued%20training.%20While%20factors%20such%20as%20weight%20norms%20and%20sparsity%20have%20been%0Aproposed%20to%20explain%20this%20delayed%20generalization%2C%20the%20influence%20of%20network%0Astructure%20remains%20underexplored.%20In%20this%20work%2C%20we%20link%20the%20grokking%20phenomenon%0Ato%20the%20lottery%20ticket%20hypothesis%20to%20investigate%20the%20impact%20of%20internal%20network%0Astructures.%20We%20demonstrate%20that%20utilizing%20lottery%20tickets%20obtained%20during%20the%0Ageneralizing%20phase%20%28termed%20grokked%20tickets%29%20significantly%20reduces%20delayed%0Ageneralization%20across%20various%20tasks%2C%20including%20multiple%20modular%20arithmetic%0Aoperations%2C%20polynomial%20regression%2C%20sparse%20parity%2C%20and%20MNIST%20classification.%0AThrough%20controlled%20experiments%2C%20we%20show%20that%20the%20mitigation%20of%20delayed%0Ageneralization%20is%20not%20due%20solely%20to%20reduced%20weight%20norms%20or%20increased%20sparsity%2C%0Abut%20rather%20to%20the%20discovery%20of%20good%20subnetworks.%20Furthermore%2C%20we%20find%20that%0Agrokked%20tickets%20exhibit%20periodic%20weight%20patterns%2C%20beneficial%20graph%20properties%0Asuch%20as%20increased%20average%20path%20lengths%20and%20reduced%20clustering%20coefficients%2C%20and%0Aundergo%20rapid%20structural%20changes%20that%20coincide%20with%20improvements%20in%0Ageneralization.%20Additionally%2C%20pruning%20techniques%20like%20the%20edge-popup%20algorithm%0Acan%20identify%20these%20effective%20structures%20without%20modifying%20the%20weights%2C%20thereby%0Atransforming%20memorizing%20networks%20into%20generalizing%20ones.%20These%20results%0Aunderscore%20the%20novel%20insight%20that%20structural%20exploration%20plays%20a%20pivotal%20role%0Ain%20understanding%20grokking.%20The%20implementation%20code%20can%20be%20accessed%20via%20this%0Alink%3A%20https%3A//github.com/gouki510/Grokking-Tickets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19470v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Lottery%2520Ticket%2520and%2520Grokking%253A%2520Understanding%2520Grokking%2520from%2520Inner%250A%2520%2520Structure%2520of%2520Networks%26entry.906535625%3DGouki%2520Minegishi%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Grokking%2520is%2520an%2520intriguing%2520phenomenon%2520of%2520delayed%2520generalization%252C%2520where%2520neural%250Anetworks%2520initially%2520memorize%2520training%2520data%2520with%2520perfect%2520accuracy%2520but%2520exhibit%250Apoor%2520generalization%252C%2520subsequently%2520transitioning%2520to%2520a%2520generalizing%2520solution%2520with%250Acontinued%2520training.%2520While%2520factors%2520such%2520as%2520weight%2520norms%2520and%2520sparsity%2520have%2520been%250Aproposed%2520to%2520explain%2520this%2520delayed%2520generalization%252C%2520the%2520influence%2520of%2520network%250Astructure%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520link%2520the%2520grokking%2520phenomenon%250Ato%2520the%2520lottery%2520ticket%2520hypothesis%2520to%2520investigate%2520the%2520impact%2520of%2520internal%2520network%250Astructures.%2520We%2520demonstrate%2520that%2520utilizing%2520lottery%2520tickets%2520obtained%2520during%2520the%250Ageneralizing%2520phase%2520%2528termed%2520grokked%2520tickets%2529%2520significantly%2520reduces%2520delayed%250Ageneralization%2520across%2520various%2520tasks%252C%2520including%2520multiple%2520modular%2520arithmetic%250Aoperations%252C%2520polynomial%2520regression%252C%2520sparse%2520parity%252C%2520and%2520MNIST%2520classification.%250AThrough%2520controlled%2520experiments%252C%2520we%2520show%2520that%2520the%2520mitigation%2520of%2520delayed%250Ageneralization%2520is%2520not%2520due%2520solely%2520to%2520reduced%2520weight%2520norms%2520or%2520increased%2520sparsity%252C%250Abut%2520rather%2520to%2520the%2520discovery%2520of%2520good%2520subnetworks.%2520Furthermore%252C%2520we%2520find%2520that%250Agrokked%2520tickets%2520exhibit%2520periodic%2520weight%2520patterns%252C%2520beneficial%2520graph%2520properties%250Asuch%2520as%2520increased%2520average%2520path%2520lengths%2520and%2520reduced%2520clustering%2520coefficients%252C%2520and%250Aundergo%2520rapid%2520structural%2520changes%2520that%2520coincide%2520with%2520improvements%2520in%250Ageneralization.%2520Additionally%252C%2520pruning%2520techniques%2520like%2520the%2520edge-popup%2520algorithm%250Acan%2520identify%2520these%2520effective%2520structures%2520without%2520modifying%2520the%2520weights%252C%2520thereby%250Atransforming%2520memorizing%2520networks%2520into%2520generalizing%2520ones.%2520These%2520results%250Aunderscore%2520the%2520novel%2520insight%2520that%2520structural%2520exploration%2520plays%2520a%2520pivotal%2520role%250Ain%2520understanding%2520grokking.%2520The%2520implementation%2520code%2520can%2520be%2520accessed%2520via%2520this%250Alink%253A%2520https%253A//github.com/gouki510/Grokking-Tickets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19470v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Lottery%20Ticket%20and%20Grokking%3A%20Understanding%20Grokking%20from%20Inner%0A%20%20Structure%20of%20Networks&entry.906535625=Gouki%20Minegishi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Grokking%20is%20an%20intriguing%20phenomenon%20of%20delayed%20generalization%2C%20where%20neural%0Anetworks%20initially%20memorize%20training%20data%20with%20perfect%20accuracy%20but%20exhibit%0Apoor%20generalization%2C%20subsequently%20transitioning%20to%20a%20generalizing%20solution%20with%0Acontinued%20training.%20While%20factors%20such%20as%20weight%20norms%20and%20sparsity%20have%20been%0Aproposed%20to%20explain%20this%20delayed%20generalization%2C%20the%20influence%20of%20network%0Astructure%20remains%20underexplored.%20In%20this%20work%2C%20we%20link%20the%20grokking%20phenomenon%0Ato%20the%20lottery%20ticket%20hypothesis%20to%20investigate%20the%20impact%20of%20internal%20network%0Astructures.%20We%20demonstrate%20that%20utilizing%20lottery%20tickets%20obtained%20during%20the%0Ageneralizing%20phase%20%28termed%20grokked%20tickets%29%20significantly%20reduces%20delayed%0Ageneralization%20across%20various%20tasks%2C%20including%20multiple%20modular%20arithmetic%0Aoperations%2C%20polynomial%20regression%2C%20sparse%20parity%2C%20and%20MNIST%20classification.%0AThrough%20controlled%20experiments%2C%20we%20show%20that%20the%20mitigation%20of%20delayed%0Ageneralization%20is%20not%20due%20solely%20to%20reduced%20weight%20norms%20or%20increased%20sparsity%2C%0Abut%20rather%20to%20the%20discovery%20of%20good%20subnetworks.%20Furthermore%2C%20we%20find%20that%0Agrokked%20tickets%20exhibit%20periodic%20weight%20patterns%2C%20beneficial%20graph%20properties%0Asuch%20as%20increased%20average%20path%20lengths%20and%20reduced%20clustering%20coefficients%2C%20and%0Aundergo%20rapid%20structural%20changes%20that%20coincide%20with%20improvements%20in%0Ageneralization.%20Additionally%2C%20pruning%20techniques%20like%20the%20edge-popup%20algorithm%0Acan%20identify%20these%20effective%20structures%20without%20modifying%20the%20weights%2C%20thereby%0Atransforming%20memorizing%20networks%20into%20generalizing%20ones.%20These%20results%0Aunderscore%20the%20novel%20insight%20that%20structural%20exploration%20plays%20a%20pivotal%20role%0Ain%20understanding%20grokking.%20The%20implementation%20code%20can%20be%20accessed%20via%20this%0Alink%3A%20https%3A//github.com/gouki510/Grokking-Tickets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19470v3&entry.124074799=Read"},
{"title": "Towards a Unified Representation Evaluation Framework Beyond Downstream\n  Tasks", "author": "Christos Plachouras and Julien Guinot and George Fazekas and Elio Quinton and Emmanouil Benetos and Johan Pauwels", "abstract": "  Downstream probing has been the dominant method for evaluating model\nrepresentations, an important process given the increasing prominence of\nself-supervised learning and foundation models. However, downstream probing\nprimarily assesses the availability of task-relevant information in the model's\nlatent space, overlooking attributes such as equivariance, invariance, and\ndisentanglement, which contribute to the interpretability, adaptability, and\nutility of representations in real-world applications. While some attempts have\nbeen made to measure these qualities in representations, no unified evaluation\nframework with modular, generalizable, and interpretable metrics exists.\n  In this paper, we argue for the importance of representation evaluation\nbeyond downstream probing. We introduce a standardized protocol to quantify\ninformativeness, equivariance, invariance, and disentanglement of factors of\nvariation in model representations. We use it to evaluate representations from\na variety of models in the image and speech domains using different\narchitectures and pretraining approaches on identified controllable factors of\nvariation. We find that representations from models with similar downstream\nperformance can behave substantially differently with regard to these\nattributes. This hints that the respective mechanisms underlying their\ndownstream performance are functionally different, prompting new research\ndirections to understand and improve representations.\n", "link": "http://arxiv.org/abs/2505.06224v1", "date": "2025-05-09", "relevancy": 2.3798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6085}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Unified%20Representation%20Evaluation%20Framework%20Beyond%20Downstream%0A%20%20Tasks&body=Title%3A%20Towards%20a%20Unified%20Representation%20Evaluation%20Framework%20Beyond%20Downstream%0A%20%20Tasks%0AAuthor%3A%20Christos%20Plachouras%20and%20Julien%20Guinot%20and%20George%20Fazekas%20and%20Elio%20Quinton%20and%20Emmanouil%20Benetos%20and%20Johan%20Pauwels%0AAbstract%3A%20%20%20Downstream%20probing%20has%20been%20the%20dominant%20method%20for%20evaluating%20model%0Arepresentations%2C%20an%20important%20process%20given%20the%20increasing%20prominence%20of%0Aself-supervised%20learning%20and%20foundation%20models.%20However%2C%20downstream%20probing%0Aprimarily%20assesses%20the%20availability%20of%20task-relevant%20information%20in%20the%20model%27s%0Alatent%20space%2C%20overlooking%20attributes%20such%20as%20equivariance%2C%20invariance%2C%20and%0Adisentanglement%2C%20which%20contribute%20to%20the%20interpretability%2C%20adaptability%2C%20and%0Autility%20of%20representations%20in%20real-world%20applications.%20While%20some%20attempts%20have%0Abeen%20made%20to%20measure%20these%20qualities%20in%20representations%2C%20no%20unified%20evaluation%0Aframework%20with%20modular%2C%20generalizable%2C%20and%20interpretable%20metrics%20exists.%0A%20%20In%20this%20paper%2C%20we%20argue%20for%20the%20importance%20of%20representation%20evaluation%0Abeyond%20downstream%20probing.%20We%20introduce%20a%20standardized%20protocol%20to%20quantify%0Ainformativeness%2C%20equivariance%2C%20invariance%2C%20and%20disentanglement%20of%20factors%20of%0Avariation%20in%20model%20representations.%20We%20use%20it%20to%20evaluate%20representations%20from%0Aa%20variety%20of%20models%20in%20the%20image%20and%20speech%20domains%20using%20different%0Aarchitectures%20and%20pretraining%20approaches%20on%20identified%20controllable%20factors%20of%0Avariation.%20We%20find%20that%20representations%20from%20models%20with%20similar%20downstream%0Aperformance%20can%20behave%20substantially%20differently%20with%20regard%20to%20these%0Aattributes.%20This%20hints%20that%20the%20respective%20mechanisms%20underlying%20their%0Adownstream%20performance%20are%20functionally%20different%2C%20prompting%20new%20research%0Adirections%20to%20understand%20and%20improve%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Unified%2520Representation%2520Evaluation%2520Framework%2520Beyond%2520Downstream%250A%2520%2520Tasks%26entry.906535625%3DChristos%2520Plachouras%2520and%2520Julien%2520Guinot%2520and%2520George%2520Fazekas%2520and%2520Elio%2520Quinton%2520and%2520Emmanouil%2520Benetos%2520and%2520Johan%2520Pauwels%26entry.1292438233%3D%2520%2520Downstream%2520probing%2520has%2520been%2520the%2520dominant%2520method%2520for%2520evaluating%2520model%250Arepresentations%252C%2520an%2520important%2520process%2520given%2520the%2520increasing%2520prominence%2520of%250Aself-supervised%2520learning%2520and%2520foundation%2520models.%2520However%252C%2520downstream%2520probing%250Aprimarily%2520assesses%2520the%2520availability%2520of%2520task-relevant%2520information%2520in%2520the%2520model%2527s%250Alatent%2520space%252C%2520overlooking%2520attributes%2520such%2520as%2520equivariance%252C%2520invariance%252C%2520and%250Adisentanglement%252C%2520which%2520contribute%2520to%2520the%2520interpretability%252C%2520adaptability%252C%2520and%250Autility%2520of%2520representations%2520in%2520real-world%2520applications.%2520While%2520some%2520attempts%2520have%250Abeen%2520made%2520to%2520measure%2520these%2520qualities%2520in%2520representations%252C%2520no%2520unified%2520evaluation%250Aframework%2520with%2520modular%252C%2520generalizable%252C%2520and%2520interpretable%2520metrics%2520exists.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520argue%2520for%2520the%2520importance%2520of%2520representation%2520evaluation%250Abeyond%2520downstream%2520probing.%2520We%2520introduce%2520a%2520standardized%2520protocol%2520to%2520quantify%250Ainformativeness%252C%2520equivariance%252C%2520invariance%252C%2520and%2520disentanglement%2520of%2520factors%2520of%250Avariation%2520in%2520model%2520representations.%2520We%2520use%2520it%2520to%2520evaluate%2520representations%2520from%250Aa%2520variety%2520of%2520models%2520in%2520the%2520image%2520and%2520speech%2520domains%2520using%2520different%250Aarchitectures%2520and%2520pretraining%2520approaches%2520on%2520identified%2520controllable%2520factors%2520of%250Avariation.%2520We%2520find%2520that%2520representations%2520from%2520models%2520with%2520similar%2520downstream%250Aperformance%2520can%2520behave%2520substantially%2520differently%2520with%2520regard%2520to%2520these%250Aattributes.%2520This%2520hints%2520that%2520the%2520respective%2520mechanisms%2520underlying%2520their%250Adownstream%2520performance%2520are%2520functionally%2520different%252C%2520prompting%2520new%2520research%250Adirections%2520to%2520understand%2520and%2520improve%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Unified%20Representation%20Evaluation%20Framework%20Beyond%20Downstream%0A%20%20Tasks&entry.906535625=Christos%20Plachouras%20and%20Julien%20Guinot%20and%20George%20Fazekas%20and%20Elio%20Quinton%20and%20Emmanouil%20Benetos%20and%20Johan%20Pauwels&entry.1292438233=%20%20Downstream%20probing%20has%20been%20the%20dominant%20method%20for%20evaluating%20model%0Arepresentations%2C%20an%20important%20process%20given%20the%20increasing%20prominence%20of%0Aself-supervised%20learning%20and%20foundation%20models.%20However%2C%20downstream%20probing%0Aprimarily%20assesses%20the%20availability%20of%20task-relevant%20information%20in%20the%20model%27s%0Alatent%20space%2C%20overlooking%20attributes%20such%20as%20equivariance%2C%20invariance%2C%20and%0Adisentanglement%2C%20which%20contribute%20to%20the%20interpretability%2C%20adaptability%2C%20and%0Autility%20of%20representations%20in%20real-world%20applications.%20While%20some%20attempts%20have%0Abeen%20made%20to%20measure%20these%20qualities%20in%20representations%2C%20no%20unified%20evaluation%0Aframework%20with%20modular%2C%20generalizable%2C%20and%20interpretable%20metrics%20exists.%0A%20%20In%20this%20paper%2C%20we%20argue%20for%20the%20importance%20of%20representation%20evaluation%0Abeyond%20downstream%20probing.%20We%20introduce%20a%20standardized%20protocol%20to%20quantify%0Ainformativeness%2C%20equivariance%2C%20invariance%2C%20and%20disentanglement%20of%20factors%20of%0Avariation%20in%20model%20representations.%20We%20use%20it%20to%20evaluate%20representations%20from%0Aa%20variety%20of%20models%20in%20the%20image%20and%20speech%20domains%20using%20different%0Aarchitectures%20and%20pretraining%20approaches%20on%20identified%20controllable%20factors%20of%0Avariation.%20We%20find%20that%20representations%20from%20models%20with%20similar%20downstream%0Aperformance%20can%20behave%20substantially%20differently%20with%20regard%20to%20these%0Aattributes.%20This%20hints%20that%20the%20respective%20mechanisms%20underlying%20their%0Adownstream%20performance%20are%20functionally%20different%2C%20prompting%20new%20research%0Adirections%20to%20understand%20and%20improve%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06224v1&entry.124074799=Read"},
{"title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation", "author": "Yukang Cao and Xinying Guo and Mingyuan Zhang and Haozhe Xie and Chenyang Gu and Ziwei Liu", "abstract": "  While recent advances in text-to-motion generation have shown promising\nresults, they typically assume all individuals are grouped as a single unit.\nScaling these methods to handle larger crowds and ensuring that individuals\nrespond appropriately to specific events remains a significant challenge. This\nis primarily due to the complexities of scene planning, which involves\norganizing groups, planning their activities, and coordinating interactions,\nand controllable motion generation. In this paper, we present CrowdMoGen, the\nfirst zero-shot framework for collective motion generation, which effectively\ngroups individuals and generates event-aligned motion sequences from text\nprompts. 1) Being limited by the available datasets for training an effective\nscene planning module in a supervised manner, we instead propose a crowd scene\nplanner that leverages pre-trained large language models (LLMs) to organize\nindividuals into distinct groups. While LLMs offer high-level guidance for\ngroup divisions, they lack the low-level understanding of human motion. To\naddress this, we further propose integrating an SMPL-based joint prior to\ngenerate context-appropriate activities, which consists of both joint\ntrajectories and textual descriptions. 2) Secondly, to incorporate the assigned\nactivities into the generative network, we introduce a collective motion\ngenerator that integrates the activities into a transformer-based network in a\njoint-wise manner, maintaining the spatial constraints during the multi-step\ndenoising process. Extensive experiments demonstrate that CrowdMoGen\nsignificantly outperforms previous approaches, delivering realistic,\nevent-driven motion sequences that are spatially coherent. As the first\nframework of collective motion generation, CrowdMoGen has the potential to\nadvance applications in urban simulation, crowd planning, and other large-scale\ninteractive environments.\n", "link": "http://arxiv.org/abs/2407.06188v2", "date": "2025-05-09", "relevancy": 2.3718, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6028}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5904}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation&body=Title%3A%20CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation%0AAuthor%3A%20Yukang%20Cao%20and%20Xinying%20Guo%20and%20Mingyuan%20Zhang%20and%20Haozhe%20Xie%20and%20Chenyang%20Gu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20While%20recent%20advances%20in%20text-to-motion%20generation%20have%20shown%20promising%0Aresults%2C%20they%20typically%20assume%20all%20individuals%20are%20grouped%20as%20a%20single%20unit.%0AScaling%20these%20methods%20to%20handle%20larger%20crowds%20and%20ensuring%20that%20individuals%0Arespond%20appropriately%20to%20specific%20events%20remains%20a%20significant%20challenge.%20This%0Ais%20primarily%20due%20to%20the%20complexities%20of%20scene%20planning%2C%20which%20involves%0Aorganizing%20groups%2C%20planning%20their%20activities%2C%20and%20coordinating%20interactions%2C%0Aand%20controllable%20motion%20generation.%20In%20this%20paper%2C%20we%20present%20CrowdMoGen%2C%20the%0Afirst%20zero-shot%20framework%20for%20collective%20motion%20generation%2C%20which%20effectively%0Agroups%20individuals%20and%20generates%20event-aligned%20motion%20sequences%20from%20text%0Aprompts.%201%29%20Being%20limited%20by%20the%20available%20datasets%20for%20training%20an%20effective%0Ascene%20planning%20module%20in%20a%20supervised%20manner%2C%20we%20instead%20propose%20a%20crowd%20scene%0Aplanner%20that%20leverages%20pre-trained%20large%20language%20models%20%28LLMs%29%20to%20organize%0Aindividuals%20into%20distinct%20groups.%20While%20LLMs%20offer%20high-level%20guidance%20for%0Agroup%20divisions%2C%20they%20lack%20the%20low-level%20understanding%20of%20human%20motion.%20To%0Aaddress%20this%2C%20we%20further%20propose%20integrating%20an%20SMPL-based%20joint%20prior%20to%0Agenerate%20context-appropriate%20activities%2C%20which%20consists%20of%20both%20joint%0Atrajectories%20and%20textual%20descriptions.%202%29%20Secondly%2C%20to%20incorporate%20the%20assigned%0Aactivities%20into%20the%20generative%20network%2C%20we%20introduce%20a%20collective%20motion%0Agenerator%20that%20integrates%20the%20activities%20into%20a%20transformer-based%20network%20in%20a%0Ajoint-wise%20manner%2C%20maintaining%20the%20spatial%20constraints%20during%20the%20multi-step%0Adenoising%20process.%20Extensive%20experiments%20demonstrate%20that%20CrowdMoGen%0Asignificantly%20outperforms%20previous%20approaches%2C%20delivering%20realistic%2C%0Aevent-driven%20motion%20sequences%20that%20are%20spatially%20coherent.%20As%20the%20first%0Aframework%20of%20collective%20motion%20generation%2C%20CrowdMoGen%20has%20the%20potential%20to%0Aadvance%20applications%20in%20urban%20simulation%2C%20crowd%20planning%2C%20and%20other%20large-scale%0Ainteractive%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrowdMoGen%253A%2520Zero-Shot%2520Text-Driven%2520Collective%2520Motion%2520Generation%26entry.906535625%3DYukang%2520Cao%2520and%2520Xinying%2520Guo%2520and%2520Mingyuan%2520Zhang%2520and%2520Haozhe%2520Xie%2520and%2520Chenyang%2520Gu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520While%2520recent%2520advances%2520in%2520text-to-motion%2520generation%2520have%2520shown%2520promising%250Aresults%252C%2520they%2520typically%2520assume%2520all%2520individuals%2520are%2520grouped%2520as%2520a%2520single%2520unit.%250AScaling%2520these%2520methods%2520to%2520handle%2520larger%2520crowds%2520and%2520ensuring%2520that%2520individuals%250Arespond%2520appropriately%2520to%2520specific%2520events%2520remains%2520a%2520significant%2520challenge.%2520This%250Ais%2520primarily%2520due%2520to%2520the%2520complexities%2520of%2520scene%2520planning%252C%2520which%2520involves%250Aorganizing%2520groups%252C%2520planning%2520their%2520activities%252C%2520and%2520coordinating%2520interactions%252C%250Aand%2520controllable%2520motion%2520generation.%2520In%2520this%2520paper%252C%2520we%2520present%2520CrowdMoGen%252C%2520the%250Afirst%2520zero-shot%2520framework%2520for%2520collective%2520motion%2520generation%252C%2520which%2520effectively%250Agroups%2520individuals%2520and%2520generates%2520event-aligned%2520motion%2520sequences%2520from%2520text%250Aprompts.%25201%2529%2520Being%2520limited%2520by%2520the%2520available%2520datasets%2520for%2520training%2520an%2520effective%250Ascene%2520planning%2520module%2520in%2520a%2520supervised%2520manner%252C%2520we%2520instead%2520propose%2520a%2520crowd%2520scene%250Aplanner%2520that%2520leverages%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520organize%250Aindividuals%2520into%2520distinct%2520groups.%2520While%2520LLMs%2520offer%2520high-level%2520guidance%2520for%250Agroup%2520divisions%252C%2520they%2520lack%2520the%2520low-level%2520understanding%2520of%2520human%2520motion.%2520To%250Aaddress%2520this%252C%2520we%2520further%2520propose%2520integrating%2520an%2520SMPL-based%2520joint%2520prior%2520to%250Agenerate%2520context-appropriate%2520activities%252C%2520which%2520consists%2520of%2520both%2520joint%250Atrajectories%2520and%2520textual%2520descriptions.%25202%2529%2520Secondly%252C%2520to%2520incorporate%2520the%2520assigned%250Aactivities%2520into%2520the%2520generative%2520network%252C%2520we%2520introduce%2520a%2520collective%2520motion%250Agenerator%2520that%2520integrates%2520the%2520activities%2520into%2520a%2520transformer-based%2520network%2520in%2520a%250Ajoint-wise%2520manner%252C%2520maintaining%2520the%2520spatial%2520constraints%2520during%2520the%2520multi-step%250Adenoising%2520process.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CrowdMoGen%250Asignificantly%2520outperforms%2520previous%2520approaches%252C%2520delivering%2520realistic%252C%250Aevent-driven%2520motion%2520sequences%2520that%2520are%2520spatially%2520coherent.%2520As%2520the%2520first%250Aframework%2520of%2520collective%2520motion%2520generation%252C%2520CrowdMoGen%2520has%2520the%2520potential%2520to%250Aadvance%2520applications%2520in%2520urban%2520simulation%252C%2520crowd%2520planning%252C%2520and%2520other%2520large-scale%250Ainteractive%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation&entry.906535625=Yukang%20Cao%20and%20Xinying%20Guo%20and%20Mingyuan%20Zhang%20and%20Haozhe%20Xie%20and%20Chenyang%20Gu%20and%20Ziwei%20Liu&entry.1292438233=%20%20While%20recent%20advances%20in%20text-to-motion%20generation%20have%20shown%20promising%0Aresults%2C%20they%20typically%20assume%20all%20individuals%20are%20grouped%20as%20a%20single%20unit.%0AScaling%20these%20methods%20to%20handle%20larger%20crowds%20and%20ensuring%20that%20individuals%0Arespond%20appropriately%20to%20specific%20events%20remains%20a%20significant%20challenge.%20This%0Ais%20primarily%20due%20to%20the%20complexities%20of%20scene%20planning%2C%20which%20involves%0Aorganizing%20groups%2C%20planning%20their%20activities%2C%20and%20coordinating%20interactions%2C%0Aand%20controllable%20motion%20generation.%20In%20this%20paper%2C%20we%20present%20CrowdMoGen%2C%20the%0Afirst%20zero-shot%20framework%20for%20collective%20motion%20generation%2C%20which%20effectively%0Agroups%20individuals%20and%20generates%20event-aligned%20motion%20sequences%20from%20text%0Aprompts.%201%29%20Being%20limited%20by%20the%20available%20datasets%20for%20training%20an%20effective%0Ascene%20planning%20module%20in%20a%20supervised%20manner%2C%20we%20instead%20propose%20a%20crowd%20scene%0Aplanner%20that%20leverages%20pre-trained%20large%20language%20models%20%28LLMs%29%20to%20organize%0Aindividuals%20into%20distinct%20groups.%20While%20LLMs%20offer%20high-level%20guidance%20for%0Agroup%20divisions%2C%20they%20lack%20the%20low-level%20understanding%20of%20human%20motion.%20To%0Aaddress%20this%2C%20we%20further%20propose%20integrating%20an%20SMPL-based%20joint%20prior%20to%0Agenerate%20context-appropriate%20activities%2C%20which%20consists%20of%20both%20joint%0Atrajectories%20and%20textual%20descriptions.%202%29%20Secondly%2C%20to%20incorporate%20the%20assigned%0Aactivities%20into%20the%20generative%20network%2C%20we%20introduce%20a%20collective%20motion%0Agenerator%20that%20integrates%20the%20activities%20into%20a%20transformer-based%20network%20in%20a%0Ajoint-wise%20manner%2C%20maintaining%20the%20spatial%20constraints%20during%20the%20multi-step%0Adenoising%20process.%20Extensive%20experiments%20demonstrate%20that%20CrowdMoGen%0Asignificantly%20outperforms%20previous%20approaches%2C%20delivering%20realistic%2C%0Aevent-driven%20motion%20sequences%20that%20are%20spatially%20coherent.%20As%20the%20first%0Aframework%20of%20collective%20motion%20generation%2C%20CrowdMoGen%20has%20the%20potential%20to%0Aadvance%20applications%20in%20urban%20simulation%2C%20crowd%20planning%2C%20and%20other%20large-scale%0Ainteractive%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06188v2&entry.124074799=Read"},
{"title": "Generalizable Sleep Staging via Multi-Level Domain Alignment", "author": "Jiquan Wang and Sha Zhao and Haiteng Jiang and Shijian Li and Tao Li and Gang Pan", "abstract": "  Automatic sleep staging is essential for sleep assessment and disorder\ndiagnosis. Most existing methods depend on one specific dataset and are limited\nto be generalized to other unseen datasets, for which the training data and\ntesting data are from the same dataset. In this paper, we introduce domain\ngeneralization into automatic sleep staging and propose the task of\ngeneralizable sleep staging which aims to improve the model generalization\nability to unseen datasets. Inspired by existing domain generalization methods,\nwe adopt the feature alignment idea and propose a framework called SleepDG to\nsolve it. Considering both of local salient features and sequential features\nare important for sleep staging, we propose a Multi-level Feature Alignment\ncombining epoch-level and sequence-level feature alignment to learn\ndomain-invariant feature representations. Specifically, we design an\nEpoch-level Feature Alignment to align the feature distribution of each single\nsleep epoch among different domains, and a Sequence-level Feature Alignment to\nminimize the discrepancy of sequential features among different domains.\nSleepDG is validated on five public datasets, achieving the state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2401.05363v5", "date": "2025-05-09", "relevancy": 2.3358, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.477}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Sleep%20Staging%20via%20Multi-Level%20Domain%20Alignment&body=Title%3A%20Generalizable%20Sleep%20Staging%20via%20Multi-Level%20Domain%20Alignment%0AAuthor%3A%20Jiquan%20Wang%20and%20Sha%20Zhao%20and%20Haiteng%20Jiang%20and%20Shijian%20Li%20and%20Tao%20Li%20and%20Gang%20Pan%0AAbstract%3A%20%20%20Automatic%20sleep%20staging%20is%20essential%20for%20sleep%20assessment%20and%20disorder%0Adiagnosis.%20Most%20existing%20methods%20depend%20on%20one%20specific%20dataset%20and%20are%20limited%0Ato%20be%20generalized%20to%20other%20unseen%20datasets%2C%20for%20which%20the%20training%20data%20and%0Atesting%20data%20are%20from%20the%20same%20dataset.%20In%20this%20paper%2C%20we%20introduce%20domain%0Ageneralization%20into%20automatic%20sleep%20staging%20and%20propose%20the%20task%20of%0Ageneralizable%20sleep%20staging%20which%20aims%20to%20improve%20the%20model%20generalization%0Aability%20to%20unseen%20datasets.%20Inspired%20by%20existing%20domain%20generalization%20methods%2C%0Awe%20adopt%20the%20feature%20alignment%20idea%20and%20propose%20a%20framework%20called%20SleepDG%20to%0Asolve%20it.%20Considering%20both%20of%20local%20salient%20features%20and%20sequential%20features%0Aare%20important%20for%20sleep%20staging%2C%20we%20propose%20a%20Multi-level%20Feature%20Alignment%0Acombining%20epoch-level%20and%20sequence-level%20feature%20alignment%20to%20learn%0Adomain-invariant%20feature%20representations.%20Specifically%2C%20we%20design%20an%0AEpoch-level%20Feature%20Alignment%20to%20align%20the%20feature%20distribution%20of%20each%20single%0Asleep%20epoch%20among%20different%20domains%2C%20and%20a%20Sequence-level%20Feature%20Alignment%20to%0Aminimize%20the%20discrepancy%20of%20sequential%20features%20among%20different%20domains.%0ASleepDG%20is%20validated%20on%20five%20public%20datasets%2C%20achieving%20the%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05363v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Sleep%2520Staging%2520via%2520Multi-Level%2520Domain%2520Alignment%26entry.906535625%3DJiquan%2520Wang%2520and%2520Sha%2520Zhao%2520and%2520Haiteng%2520Jiang%2520and%2520Shijian%2520Li%2520and%2520Tao%2520Li%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520Automatic%2520sleep%2520staging%2520is%2520essential%2520for%2520sleep%2520assessment%2520and%2520disorder%250Adiagnosis.%2520Most%2520existing%2520methods%2520depend%2520on%2520one%2520specific%2520dataset%2520and%2520are%2520limited%250Ato%2520be%2520generalized%2520to%2520other%2520unseen%2520datasets%252C%2520for%2520which%2520the%2520training%2520data%2520and%250Atesting%2520data%2520are%2520from%2520the%2520same%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520domain%250Ageneralization%2520into%2520automatic%2520sleep%2520staging%2520and%2520propose%2520the%2520task%2520of%250Ageneralizable%2520sleep%2520staging%2520which%2520aims%2520to%2520improve%2520the%2520model%2520generalization%250Aability%2520to%2520unseen%2520datasets.%2520Inspired%2520by%2520existing%2520domain%2520generalization%2520methods%252C%250Awe%2520adopt%2520the%2520feature%2520alignment%2520idea%2520and%2520propose%2520a%2520framework%2520called%2520SleepDG%2520to%250Asolve%2520it.%2520Considering%2520both%2520of%2520local%2520salient%2520features%2520and%2520sequential%2520features%250Aare%2520important%2520for%2520sleep%2520staging%252C%2520we%2520propose%2520a%2520Multi-level%2520Feature%2520Alignment%250Acombining%2520epoch-level%2520and%2520sequence-level%2520feature%2520alignment%2520to%2520learn%250Adomain-invariant%2520feature%2520representations.%2520Specifically%252C%2520we%2520design%2520an%250AEpoch-level%2520Feature%2520Alignment%2520to%2520align%2520the%2520feature%2520distribution%2520of%2520each%2520single%250Asleep%2520epoch%2520among%2520different%2520domains%252C%2520and%2520a%2520Sequence-level%2520Feature%2520Alignment%2520to%250Aminimize%2520the%2520discrepancy%2520of%2520sequential%2520features%2520among%2520different%2520domains.%250ASleepDG%2520is%2520validated%2520on%2520five%2520public%2520datasets%252C%2520achieving%2520the%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05363v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Sleep%20Staging%20via%20Multi-Level%20Domain%20Alignment&entry.906535625=Jiquan%20Wang%20and%20Sha%20Zhao%20and%20Haiteng%20Jiang%20and%20Shijian%20Li%20and%20Tao%20Li%20and%20Gang%20Pan&entry.1292438233=%20%20Automatic%20sleep%20staging%20is%20essential%20for%20sleep%20assessment%20and%20disorder%0Adiagnosis.%20Most%20existing%20methods%20depend%20on%20one%20specific%20dataset%20and%20are%20limited%0Ato%20be%20generalized%20to%20other%20unseen%20datasets%2C%20for%20which%20the%20training%20data%20and%0Atesting%20data%20are%20from%20the%20same%20dataset.%20In%20this%20paper%2C%20we%20introduce%20domain%0Ageneralization%20into%20automatic%20sleep%20staging%20and%20propose%20the%20task%20of%0Ageneralizable%20sleep%20staging%20which%20aims%20to%20improve%20the%20model%20generalization%0Aability%20to%20unseen%20datasets.%20Inspired%20by%20existing%20domain%20generalization%20methods%2C%0Awe%20adopt%20the%20feature%20alignment%20idea%20and%20propose%20a%20framework%20called%20SleepDG%20to%0Asolve%20it.%20Considering%20both%20of%20local%20salient%20features%20and%20sequential%20features%0Aare%20important%20for%20sleep%20staging%2C%20we%20propose%20a%20Multi-level%20Feature%20Alignment%0Acombining%20epoch-level%20and%20sequence-level%20feature%20alignment%20to%20learn%0Adomain-invariant%20feature%20representations.%20Specifically%2C%20we%20design%20an%0AEpoch-level%20Feature%20Alignment%20to%20align%20the%20feature%20distribution%20of%20each%20single%0Asleep%20epoch%20among%20different%20domains%2C%20and%20a%20Sequence-level%20Feature%20Alignment%20to%0Aminimize%20the%20discrepancy%20of%20sequential%20features%20among%20different%20domains.%0ASleepDG%20is%20validated%20on%20five%20public%20datasets%2C%20achieving%20the%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05363v5&entry.124074799=Read"},
{"title": "Towards Better Cephalometric Landmark Detection with Diffusion Data\n  Generation", "author": "Dongqian Guo and Wencheng Han and Pang Lyu and Yuxi Zhou and Jianbing Shen", "abstract": "  Cephalometric landmark detection is essential for orthodontic diagnostics and\ntreatment planning. Nevertheless, the scarcity of samples in data collection\nand the extensive effort required for manual annotation have significantly\nimpeded the availability of diverse datasets. This limitation has restricted\nthe effectiveness of deep learning-based detection methods, particularly those\nbased on large-scale vision models. To address these challenges, we have\ndeveloped an innovative data generation method capable of producing diverse\ncephalometric X-ray images along with corresponding annotations without human\nintervention. To achieve this, our approach initiates by constructing new\ncephalometric landmark annotations using anatomical priors. Then, we employ a\ndiffusion-based generator to create realistic X-ray images that correspond\nclosely with these annotations. To achieve precise control in producing samples\nwith different attributes, we introduce a novel prompt cephalometric X-ray\nimage dataset. This dataset includes real cephalometric X-ray images and\ndetailed medical text prompts describing the images. By leveraging these\ndetailed prompts, our method improves the generation process to control\ndifferent styles and attributes. Facilitated by the large, diverse generated\ndata, we introduce large-scale vision detection models into the cephalometric\nlandmark detection task to improve accuracy. Experimental results demonstrate\nthat training with the generated data substantially enhances the performance.\nCompared to methods without using the generated data, our approach improves the\nSuccess Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and\ndata are available at: https://um-lab.github.io/cepha-generation\n", "link": "http://arxiv.org/abs/2505.06055v1", "date": "2025-05-09", "relevancy": 2.3294, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5842}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5842}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Better%20Cephalometric%20Landmark%20Detection%20with%20Diffusion%20Data%0A%20%20Generation&body=Title%3A%20Towards%20Better%20Cephalometric%20Landmark%20Detection%20with%20Diffusion%20Data%0A%20%20Generation%0AAuthor%3A%20Dongqian%20Guo%20and%20Wencheng%20Han%20and%20Pang%20Lyu%20and%20Yuxi%20Zhou%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Cephalometric%20landmark%20detection%20is%20essential%20for%20orthodontic%20diagnostics%20and%0Atreatment%20planning.%20Nevertheless%2C%20the%20scarcity%20of%20samples%20in%20data%20collection%0Aand%20the%20extensive%20effort%20required%20for%20manual%20annotation%20have%20significantly%0Aimpeded%20the%20availability%20of%20diverse%20datasets.%20This%20limitation%20has%20restricted%0Athe%20effectiveness%20of%20deep%20learning-based%20detection%20methods%2C%20particularly%20those%0Abased%20on%20large-scale%20vision%20models.%20To%20address%20these%20challenges%2C%20we%20have%0Adeveloped%20an%20innovative%20data%20generation%20method%20capable%20of%20producing%20diverse%0Acephalometric%20X-ray%20images%20along%20with%20corresponding%20annotations%20without%20human%0Aintervention.%20To%20achieve%20this%2C%20our%20approach%20initiates%20by%20constructing%20new%0Acephalometric%20landmark%20annotations%20using%20anatomical%20priors.%20Then%2C%20we%20employ%20a%0Adiffusion-based%20generator%20to%20create%20realistic%20X-ray%20images%20that%20correspond%0Aclosely%20with%20these%20annotations.%20To%20achieve%20precise%20control%20in%20producing%20samples%0Awith%20different%20attributes%2C%20we%20introduce%20a%20novel%20prompt%20cephalometric%20X-ray%0Aimage%20dataset.%20This%20dataset%20includes%20real%20cephalometric%20X-ray%20images%20and%0Adetailed%20medical%20text%20prompts%20describing%20the%20images.%20By%20leveraging%20these%0Adetailed%20prompts%2C%20our%20method%20improves%20the%20generation%20process%20to%20control%0Adifferent%20styles%20and%20attributes.%20Facilitated%20by%20the%20large%2C%20diverse%20generated%0Adata%2C%20we%20introduce%20large-scale%20vision%20detection%20models%20into%20the%20cephalometric%0Alandmark%20detection%20task%20to%20improve%20accuracy.%20Experimental%20results%20demonstrate%0Athat%20training%20with%20the%20generated%20data%20substantially%20enhances%20the%20performance.%0ACompared%20to%20methods%20without%20using%20the%20generated%20data%2C%20our%20approach%20improves%20the%0ASuccess%20Detection%20Rate%20%28SDR%29%20by%206.5%25%2C%20attaining%20a%20notable%2082.2%25.%20All%20code%20and%0Adata%20are%20available%20at%3A%20https%3A//um-lab.github.io/cepha-generation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Better%2520Cephalometric%2520Landmark%2520Detection%2520with%2520Diffusion%2520Data%250A%2520%2520Generation%26entry.906535625%3DDongqian%2520Guo%2520and%2520Wencheng%2520Han%2520and%2520Pang%2520Lyu%2520and%2520Yuxi%2520Zhou%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Cephalometric%2520landmark%2520detection%2520is%2520essential%2520for%2520orthodontic%2520diagnostics%2520and%250Atreatment%2520planning.%2520Nevertheless%252C%2520the%2520scarcity%2520of%2520samples%2520in%2520data%2520collection%250Aand%2520the%2520extensive%2520effort%2520required%2520for%2520manual%2520annotation%2520have%2520significantly%250Aimpeded%2520the%2520availability%2520of%2520diverse%2520datasets.%2520This%2520limitation%2520has%2520restricted%250Athe%2520effectiveness%2520of%2520deep%2520learning-based%2520detection%2520methods%252C%2520particularly%2520those%250Abased%2520on%2520large-scale%2520vision%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520have%250Adeveloped%2520an%2520innovative%2520data%2520generation%2520method%2520capable%2520of%2520producing%2520diverse%250Acephalometric%2520X-ray%2520images%2520along%2520with%2520corresponding%2520annotations%2520without%2520human%250Aintervention.%2520To%2520achieve%2520this%252C%2520our%2520approach%2520initiates%2520by%2520constructing%2520new%250Acephalometric%2520landmark%2520annotations%2520using%2520anatomical%2520priors.%2520Then%252C%2520we%2520employ%2520a%250Adiffusion-based%2520generator%2520to%2520create%2520realistic%2520X-ray%2520images%2520that%2520correspond%250Aclosely%2520with%2520these%2520annotations.%2520To%2520achieve%2520precise%2520control%2520in%2520producing%2520samples%250Awith%2520different%2520attributes%252C%2520we%2520introduce%2520a%2520novel%2520prompt%2520cephalometric%2520X-ray%250Aimage%2520dataset.%2520This%2520dataset%2520includes%2520real%2520cephalometric%2520X-ray%2520images%2520and%250Adetailed%2520medical%2520text%2520prompts%2520describing%2520the%2520images.%2520By%2520leveraging%2520these%250Adetailed%2520prompts%252C%2520our%2520method%2520improves%2520the%2520generation%2520process%2520to%2520control%250Adifferent%2520styles%2520and%2520attributes.%2520Facilitated%2520by%2520the%2520large%252C%2520diverse%2520generated%250Adata%252C%2520we%2520introduce%2520large-scale%2520vision%2520detection%2520models%2520into%2520the%2520cephalometric%250Alandmark%2520detection%2520task%2520to%2520improve%2520accuracy.%2520Experimental%2520results%2520demonstrate%250Athat%2520training%2520with%2520the%2520generated%2520data%2520substantially%2520enhances%2520the%2520performance.%250ACompared%2520to%2520methods%2520without%2520using%2520the%2520generated%2520data%252C%2520our%2520approach%2520improves%2520the%250ASuccess%2520Detection%2520Rate%2520%2528SDR%2529%2520by%25206.5%2525%252C%2520attaining%2520a%2520notable%252082.2%2525.%2520All%2520code%2520and%250Adata%2520are%2520available%2520at%253A%2520https%253A//um-lab.github.io/cepha-generation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Better%20Cephalometric%20Landmark%20Detection%20with%20Diffusion%20Data%0A%20%20Generation&entry.906535625=Dongqian%20Guo%20and%20Wencheng%20Han%20and%20Pang%20Lyu%20and%20Yuxi%20Zhou%20and%20Jianbing%20Shen&entry.1292438233=%20%20Cephalometric%20landmark%20detection%20is%20essential%20for%20orthodontic%20diagnostics%20and%0Atreatment%20planning.%20Nevertheless%2C%20the%20scarcity%20of%20samples%20in%20data%20collection%0Aand%20the%20extensive%20effort%20required%20for%20manual%20annotation%20have%20significantly%0Aimpeded%20the%20availability%20of%20diverse%20datasets.%20This%20limitation%20has%20restricted%0Athe%20effectiveness%20of%20deep%20learning-based%20detection%20methods%2C%20particularly%20those%0Abased%20on%20large-scale%20vision%20models.%20To%20address%20these%20challenges%2C%20we%20have%0Adeveloped%20an%20innovative%20data%20generation%20method%20capable%20of%20producing%20diverse%0Acephalometric%20X-ray%20images%20along%20with%20corresponding%20annotations%20without%20human%0Aintervention.%20To%20achieve%20this%2C%20our%20approach%20initiates%20by%20constructing%20new%0Acephalometric%20landmark%20annotations%20using%20anatomical%20priors.%20Then%2C%20we%20employ%20a%0Adiffusion-based%20generator%20to%20create%20realistic%20X-ray%20images%20that%20correspond%0Aclosely%20with%20these%20annotations.%20To%20achieve%20precise%20control%20in%20producing%20samples%0Awith%20different%20attributes%2C%20we%20introduce%20a%20novel%20prompt%20cephalometric%20X-ray%0Aimage%20dataset.%20This%20dataset%20includes%20real%20cephalometric%20X-ray%20images%20and%0Adetailed%20medical%20text%20prompts%20describing%20the%20images.%20By%20leveraging%20these%0Adetailed%20prompts%2C%20our%20method%20improves%20the%20generation%20process%20to%20control%0Adifferent%20styles%20and%20attributes.%20Facilitated%20by%20the%20large%2C%20diverse%20generated%0Adata%2C%20we%20introduce%20large-scale%20vision%20detection%20models%20into%20the%20cephalometric%0Alandmark%20detection%20task%20to%20improve%20accuracy.%20Experimental%20results%20demonstrate%0Athat%20training%20with%20the%20generated%20data%20substantially%20enhances%20the%20performance.%0ACompared%20to%20methods%20without%20using%20the%20generated%20data%2C%20our%20approach%20improves%20the%0ASuccess%20Detection%20Rate%20%28SDR%29%20by%206.5%25%2C%20attaining%20a%20notable%2082.2%25.%20All%20code%20and%0Adata%20are%20available%20at%3A%20https%3A//um-lab.github.io/cepha-generation%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06055v1&entry.124074799=Read"},
{"title": "Foundation Models For Seismic Data Processing: An Extensive Review", "author": "Fabian Fuchs and Mario Ruben Fernandez and Norman Ettrich and Janis Keuper", "abstract": "  Seismic processing plays a crucial role in transforming raw data into\nhigh-quality subsurface images, pivotal for various geoscience applications.\nDespite its importance, traditional seismic processing techniques face\nchallenges such as noisy and damaged data and the reliance on manual,\ntime-consuming workflows. The emergence of deep learning approaches has\nintroduced effective and user-friendly alternatives, yet many of these deep\nlearning approaches rely on synthetic datasets and specialized neural networks.\nRecently, foundation models have gained traction in the seismic domain, due to\ntheir success in the natural image domain. Therefore, we investigate the\napplication of natural image foundation models on the three seismic processing\ntasks: demultiple, interpolation, and denoising. We evaluate the impact of\ndifferent model characteristics, such as pre-training technique and neural\nnetwork architecture, on performance and efficiency. Rather than proposing a\nsingle seismic foundation model, we critically examine various natural image\nfoundation models and suggest some promising candidates for future exploration.\n", "link": "http://arxiv.org/abs/2503.24166v2", "date": "2025-05-09", "relevancy": 2.3112, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20For%20Seismic%20Data%20Processing%3A%20An%20Extensive%20Review&body=Title%3A%20Foundation%20Models%20For%20Seismic%20Data%20Processing%3A%20An%20Extensive%20Review%0AAuthor%3A%20Fabian%20Fuchs%20and%20Mario%20Ruben%20Fernandez%20and%20Norman%20Ettrich%20and%20Janis%20Keuper%0AAbstract%3A%20%20%20Seismic%20processing%20plays%20a%20crucial%20role%20in%20transforming%20raw%20data%20into%0Ahigh-quality%20subsurface%20images%2C%20pivotal%20for%20various%20geoscience%20applications.%0ADespite%20its%20importance%2C%20traditional%20seismic%20processing%20techniques%20face%0Achallenges%20such%20as%20noisy%20and%20damaged%20data%20and%20the%20reliance%20on%20manual%2C%0Atime-consuming%20workflows.%20The%20emergence%20of%20deep%20learning%20approaches%20has%0Aintroduced%20effective%20and%20user-friendly%20alternatives%2C%20yet%20many%20of%20these%20deep%0Alearning%20approaches%20rely%20on%20synthetic%20datasets%20and%20specialized%20neural%20networks.%0ARecently%2C%20foundation%20models%20have%20gained%20traction%20in%20the%20seismic%20domain%2C%20due%20to%0Atheir%20success%20in%20the%20natural%20image%20domain.%20Therefore%2C%20we%20investigate%20the%0Aapplication%20of%20natural%20image%20foundation%20models%20on%20the%20three%20seismic%20processing%0Atasks%3A%20demultiple%2C%20interpolation%2C%20and%20denoising.%20We%20evaluate%20the%20impact%20of%0Adifferent%20model%20characteristics%2C%20such%20as%20pre-training%20technique%20and%20neural%0Anetwork%20architecture%2C%20on%20performance%20and%20efficiency.%20Rather%20than%20proposing%20a%0Asingle%20seismic%20foundation%20model%2C%20we%20critically%20examine%20various%20natural%20image%0Afoundation%20models%20and%20suggest%20some%20promising%20candidates%20for%20future%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24166v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520For%2520Seismic%2520Data%2520Processing%253A%2520An%2520Extensive%2520Review%26entry.906535625%3DFabian%2520Fuchs%2520and%2520Mario%2520Ruben%2520Fernandez%2520and%2520Norman%2520Ettrich%2520and%2520Janis%2520Keuper%26entry.1292438233%3D%2520%2520Seismic%2520processing%2520plays%2520a%2520crucial%2520role%2520in%2520transforming%2520raw%2520data%2520into%250Ahigh-quality%2520subsurface%2520images%252C%2520pivotal%2520for%2520various%2520geoscience%2520applications.%250ADespite%2520its%2520importance%252C%2520traditional%2520seismic%2520processing%2520techniques%2520face%250Achallenges%2520such%2520as%2520noisy%2520and%2520damaged%2520data%2520and%2520the%2520reliance%2520on%2520manual%252C%250Atime-consuming%2520workflows.%2520The%2520emergence%2520of%2520deep%2520learning%2520approaches%2520has%250Aintroduced%2520effective%2520and%2520user-friendly%2520alternatives%252C%2520yet%2520many%2520of%2520these%2520deep%250Alearning%2520approaches%2520rely%2520on%2520synthetic%2520datasets%2520and%2520specialized%2520neural%2520networks.%250ARecently%252C%2520foundation%2520models%2520have%2520gained%2520traction%2520in%2520the%2520seismic%2520domain%252C%2520due%2520to%250Atheir%2520success%2520in%2520the%2520natural%2520image%2520domain.%2520Therefore%252C%2520we%2520investigate%2520the%250Aapplication%2520of%2520natural%2520image%2520foundation%2520models%2520on%2520the%2520three%2520seismic%2520processing%250Atasks%253A%2520demultiple%252C%2520interpolation%252C%2520and%2520denoising.%2520We%2520evaluate%2520the%2520impact%2520of%250Adifferent%2520model%2520characteristics%252C%2520such%2520as%2520pre-training%2520technique%2520and%2520neural%250Anetwork%2520architecture%252C%2520on%2520performance%2520and%2520efficiency.%2520Rather%2520than%2520proposing%2520a%250Asingle%2520seismic%2520foundation%2520model%252C%2520we%2520critically%2520examine%2520various%2520natural%2520image%250Afoundation%2520models%2520and%2520suggest%2520some%2520promising%2520candidates%2520for%2520future%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24166v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20For%20Seismic%20Data%20Processing%3A%20An%20Extensive%20Review&entry.906535625=Fabian%20Fuchs%20and%20Mario%20Ruben%20Fernandez%20and%20Norman%20Ettrich%20and%20Janis%20Keuper&entry.1292438233=%20%20Seismic%20processing%20plays%20a%20crucial%20role%20in%20transforming%20raw%20data%20into%0Ahigh-quality%20subsurface%20images%2C%20pivotal%20for%20various%20geoscience%20applications.%0ADespite%20its%20importance%2C%20traditional%20seismic%20processing%20techniques%20face%0Achallenges%20such%20as%20noisy%20and%20damaged%20data%20and%20the%20reliance%20on%20manual%2C%0Atime-consuming%20workflows.%20The%20emergence%20of%20deep%20learning%20approaches%20has%0Aintroduced%20effective%20and%20user-friendly%20alternatives%2C%20yet%20many%20of%20these%20deep%0Alearning%20approaches%20rely%20on%20synthetic%20datasets%20and%20specialized%20neural%20networks.%0ARecently%2C%20foundation%20models%20have%20gained%20traction%20in%20the%20seismic%20domain%2C%20due%20to%0Atheir%20success%20in%20the%20natural%20image%20domain.%20Therefore%2C%20we%20investigate%20the%0Aapplication%20of%20natural%20image%20foundation%20models%20on%20the%20three%20seismic%20processing%0Atasks%3A%20demultiple%2C%20interpolation%2C%20and%20denoising.%20We%20evaluate%20the%20impact%20of%0Adifferent%20model%20characteristics%2C%20such%20as%20pre-training%20technique%20and%20neural%0Anetwork%20architecture%2C%20on%20performance%20and%20efficiency.%20Rather%20than%20proposing%20a%0Asingle%20seismic%20foundation%20model%2C%20we%20critically%20examine%20various%20natural%20image%0Afoundation%20models%20and%20suggest%20some%20promising%20candidates%20for%20future%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24166v2&entry.124074799=Read"},
{"title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding\n  with 3D Objects", "author": "Tobias Preintner and Weixuan Yuan and Qi Huang and Adrian K\u00f6nig and Thomas B\u00e4ck and Elena Raponi and Niki van Stein", "abstract": "  Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels.\n", "link": "http://arxiv.org/abs/2505.06030v1", "date": "2025-05-09", "relevancy": 2.307, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Are%20You%20Wrong%3F%20Counterfactual%20Explanations%20for%20Language%20Grounding%0A%20%20with%203D%20Objects&body=Title%3A%20Why%20Are%20You%20Wrong%3F%20Counterfactual%20Explanations%20for%20Language%20Grounding%0A%20%20with%203D%20Objects%0AAuthor%3A%20Tobias%20Preintner%20and%20Weixuan%20Yuan%20and%20Qi%20Huang%20and%20Adrian%20K%C3%B6nig%20and%20Thomas%20B%C3%A4ck%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%0AAbstract%3A%20%20%20Combining%20natural%20language%20and%20geometric%20shapes%20is%20an%20emerging%20research%20area%0Awith%20multiple%20applications%20in%20robotics%20and%20language-assisted%20design.%20A%20crucial%0Atask%20in%20this%20domain%20is%20object%20referent%20identification%2C%20which%20involves%20selecting%0Aa%203D%20object%20given%20a%20textual%20description%20of%20the%20target.%20Variability%20in%20language%0Adescriptions%20and%20spatial%20relationships%20of%203D%20objects%20makes%20this%20a%20complex%20task%2C%0Aincreasing%20the%20need%20to%20better%20understand%20the%20behavior%20of%20neural%20network%20models%0Ain%20this%20domain.%20However%2C%20limited%20research%20has%20been%20conducted%20in%20this%20area.%0ASpecifically%2C%20when%20a%20model%20makes%20an%20incorrect%20prediction%20despite%20being%20provided%0Awith%20a%20seemingly%20correct%20object%20description%2C%20practitioners%20are%20left%20wondering%3A%0A%22Why%20is%20the%20model%20wrong%3F%22.%20In%20this%20work%2C%20we%20present%20a%20method%20answering%20this%0Aquestion%20by%20generating%20counterfactual%20examples.%20Our%20method%20takes%20a%0Amisclassified%20sample%2C%20which%20includes%20two%20objects%20and%20a%20text%20description%2C%20and%0Agenerates%20an%20alternative%20yet%20similar%20formulation%20that%20would%20have%20resulted%20in%20a%0Acorrect%20prediction%20by%20the%20model.%20We%20have%20evaluated%20our%20approach%20with%20data%20from%0Athe%20ShapeTalk%20dataset%20along%20with%20three%20distinct%20models.%20Our%20counterfactual%0Aexamples%20maintain%20the%20structure%20of%20the%20original%20description%2C%20are%20semantically%0Asimilar%20and%20meaningful.%20They%20reveal%20weaknesses%20in%20the%20description%2C%20model%20bias%0Aand%20enhance%20the%20understanding%20of%20the%20models%20behavior.%20Theses%20insights%20help%0Apractitioners%20to%20better%20interact%20with%20systems%20as%20well%20as%20engineers%20to%20improve%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Are%2520You%2520Wrong%253F%2520Counterfactual%2520Explanations%2520for%2520Language%2520Grounding%250A%2520%2520with%25203D%2520Objects%26entry.906535625%3DTobias%2520Preintner%2520and%2520Weixuan%2520Yuan%2520and%2520Qi%2520Huang%2520and%2520Adrian%2520K%25C3%25B6nig%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Elena%2520Raponi%2520and%2520Niki%2520van%2520Stein%26entry.1292438233%3D%2520%2520Combining%2520natural%2520language%2520and%2520geometric%2520shapes%2520is%2520an%2520emerging%2520research%2520area%250Awith%2520multiple%2520applications%2520in%2520robotics%2520and%2520language-assisted%2520design.%2520A%2520crucial%250Atask%2520in%2520this%2520domain%2520is%2520object%2520referent%2520identification%252C%2520which%2520involves%2520selecting%250Aa%25203D%2520object%2520given%2520a%2520textual%2520description%2520of%2520the%2520target.%2520Variability%2520in%2520language%250Adescriptions%2520and%2520spatial%2520relationships%2520of%25203D%2520objects%2520makes%2520this%2520a%2520complex%2520task%252C%250Aincreasing%2520the%2520need%2520to%2520better%2520understand%2520the%2520behavior%2520of%2520neural%2520network%2520models%250Ain%2520this%2520domain.%2520However%252C%2520limited%2520research%2520has%2520been%2520conducted%2520in%2520this%2520area.%250ASpecifically%252C%2520when%2520a%2520model%2520makes%2520an%2520incorrect%2520prediction%2520despite%2520being%2520provided%250Awith%2520a%2520seemingly%2520correct%2520object%2520description%252C%2520practitioners%2520are%2520left%2520wondering%253A%250A%2522Why%2520is%2520the%2520model%2520wrong%253F%2522.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520method%2520answering%2520this%250Aquestion%2520by%2520generating%2520counterfactual%2520examples.%2520Our%2520method%2520takes%2520a%250Amisclassified%2520sample%252C%2520which%2520includes%2520two%2520objects%2520and%2520a%2520text%2520description%252C%2520and%250Agenerates%2520an%2520alternative%2520yet%2520similar%2520formulation%2520that%2520would%2520have%2520resulted%2520in%2520a%250Acorrect%2520prediction%2520by%2520the%2520model.%2520We%2520have%2520evaluated%2520our%2520approach%2520with%2520data%2520from%250Athe%2520ShapeTalk%2520dataset%2520along%2520with%2520three%2520distinct%2520models.%2520Our%2520counterfactual%250Aexamples%2520maintain%2520the%2520structure%2520of%2520the%2520original%2520description%252C%2520are%2520semantically%250Asimilar%2520and%2520meaningful.%2520They%2520reveal%2520weaknesses%2520in%2520the%2520description%252C%2520model%2520bias%250Aand%2520enhance%2520the%2520understanding%2520of%2520the%2520models%2520behavior.%2520Theses%2520insights%2520help%250Apractitioners%2520to%2520better%2520interact%2520with%2520systems%2520as%2520well%2520as%2520engineers%2520to%2520improve%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Are%20You%20Wrong%3F%20Counterfactual%20Explanations%20for%20Language%20Grounding%0A%20%20with%203D%20Objects&entry.906535625=Tobias%20Preintner%20and%20Weixuan%20Yuan%20and%20Qi%20Huang%20and%20Adrian%20K%C3%B6nig%20and%20Thomas%20B%C3%A4ck%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein&entry.1292438233=%20%20Combining%20natural%20language%20and%20geometric%20shapes%20is%20an%20emerging%20research%20area%0Awith%20multiple%20applications%20in%20robotics%20and%20language-assisted%20design.%20A%20crucial%0Atask%20in%20this%20domain%20is%20object%20referent%20identification%2C%20which%20involves%20selecting%0Aa%203D%20object%20given%20a%20textual%20description%20of%20the%20target.%20Variability%20in%20language%0Adescriptions%20and%20spatial%20relationships%20of%203D%20objects%20makes%20this%20a%20complex%20task%2C%0Aincreasing%20the%20need%20to%20better%20understand%20the%20behavior%20of%20neural%20network%20models%0Ain%20this%20domain.%20However%2C%20limited%20research%20has%20been%20conducted%20in%20this%20area.%0ASpecifically%2C%20when%20a%20model%20makes%20an%20incorrect%20prediction%20despite%20being%20provided%0Awith%20a%20seemingly%20correct%20object%20description%2C%20practitioners%20are%20left%20wondering%3A%0A%22Why%20is%20the%20model%20wrong%3F%22.%20In%20this%20work%2C%20we%20present%20a%20method%20answering%20this%0Aquestion%20by%20generating%20counterfactual%20examples.%20Our%20method%20takes%20a%0Amisclassified%20sample%2C%20which%20includes%20two%20objects%20and%20a%20text%20description%2C%20and%0Agenerates%20an%20alternative%20yet%20similar%20formulation%20that%20would%20have%20resulted%20in%20a%0Acorrect%20prediction%20by%20the%20model.%20We%20have%20evaluated%20our%20approach%20with%20data%20from%0Athe%20ShapeTalk%20dataset%20along%20with%20three%20distinct%20models.%20Our%20counterfactual%0Aexamples%20maintain%20the%20structure%20of%20the%20original%20description%2C%20are%20semantically%0Asimilar%20and%20meaningful.%20They%20reveal%20weaknesses%20in%20the%20description%2C%20model%20bias%0Aand%20enhance%20the%20understanding%20of%20the%20models%20behavior.%20Theses%20insights%20help%0Apractitioners%20to%20better%20interact%20with%20systems%20as%20well%20as%20engineers%20to%20improve%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06030v1&entry.124074799=Read"},
{"title": "How to build the best medical image segmentation algorithm using\n  foundation models: a comprehensive empirical study with Segment Anything\n  Model", "author": "Hanxue Gu and Haoyu Dong and Jichen Yang and Maciej A. Mazurowski", "abstract": "  Automated segmentation is a fundamental medical image analysis task, which\nenjoys significant advances due to the advent of deep learning. While\nfoundation models have been useful in natural language processing and some\nvision tasks for some time, the foundation model developed with image\nsegmentation in mind - Segment Anything Model (SAM) - has been developed only\nrecently and has shown similar promise. However, there are still no systematic\nanalyses or \"best-practice\" guidelines for optimal fine-tuning of SAM for\nmedical image segmentation. This work summarizes existing fine-tuning\nstrategies with various backbone architectures, model components, and\nfine-tuning algorithms across 18 combinations, and evaluates them on 17\ndatasets covering all common radiology modalities. Our study reveals that (1)\nfine-tuning SAM leads to slightly better performance than previous segmentation\nmethods, (2) fine-tuning strategies that use parameter-efficient learning in\nboth the encoder and decoder are superior to other strategies, (3) network\narchitecture has a small impact on final performance, (4) further training SAM\nwith self-supervised learning can improve final model performance. We also\ndemonstrate the ineffectiveness of some methods popular in the literature and\nfurther expand our experiments into few-shot and prompt-based settings. Lastly,\nwe released our code and MRI-specific fine-tuned weights, which consistently\nobtained superior performance over the original SAM, at\nhttps://github.com/mazurowski-lab/finetune-SAM.\n", "link": "http://arxiv.org/abs/2404.09957v3", "date": "2025-05-09", "relevancy": 2.251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20build%20the%20best%20medical%20image%20segmentation%20algorithm%20using%0A%20%20foundation%20models%3A%20a%20comprehensive%20empirical%20study%20with%20Segment%20Anything%0A%20%20Model&body=Title%3A%20How%20to%20build%20the%20best%20medical%20image%20segmentation%20algorithm%20using%0A%20%20foundation%20models%3A%20a%20comprehensive%20empirical%20study%20with%20Segment%20Anything%0A%20%20Model%0AAuthor%3A%20Hanxue%20Gu%20and%20Haoyu%20Dong%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Automated%20segmentation%20is%20a%20fundamental%20medical%20image%20analysis%20task%2C%20which%0Aenjoys%20significant%20advances%20due%20to%20the%20advent%20of%20deep%20learning.%20While%0Afoundation%20models%20have%20been%20useful%20in%20natural%20language%20processing%20and%20some%0Avision%20tasks%20for%20some%20time%2C%20the%20foundation%20model%20developed%20with%20image%0Asegmentation%20in%20mind%20-%20Segment%20Anything%20Model%20%28SAM%29%20-%20has%20been%20developed%20only%0Arecently%20and%20has%20shown%20similar%20promise.%20However%2C%20there%20are%20still%20no%20systematic%0Aanalyses%20or%20%22best-practice%22%20guidelines%20for%20optimal%20fine-tuning%20of%20SAM%20for%0Amedical%20image%20segmentation.%20This%20work%20summarizes%20existing%20fine-tuning%0Astrategies%20with%20various%20backbone%20architectures%2C%20model%20components%2C%20and%0Afine-tuning%20algorithms%20across%2018%20combinations%2C%20and%20evaluates%20them%20on%2017%0Adatasets%20covering%20all%20common%20radiology%20modalities.%20Our%20study%20reveals%20that%20%281%29%0Afine-tuning%20SAM%20leads%20to%20slightly%20better%20performance%20than%20previous%20segmentation%0Amethods%2C%20%282%29%20fine-tuning%20strategies%20that%20use%20parameter-efficient%20learning%20in%0Aboth%20the%20encoder%20and%20decoder%20are%20superior%20to%20other%20strategies%2C%20%283%29%20network%0Aarchitecture%20has%20a%20small%20impact%20on%20final%20performance%2C%20%284%29%20further%20training%20SAM%0Awith%20self-supervised%20learning%20can%20improve%20final%20model%20performance.%20We%20also%0Ademonstrate%20the%20ineffectiveness%20of%20some%20methods%20popular%20in%20the%20literature%20and%0Afurther%20expand%20our%20experiments%20into%20few-shot%20and%20prompt-based%20settings.%20Lastly%2C%0Awe%20released%20our%20code%20and%20MRI-specific%20fine-tuned%20weights%2C%20which%20consistently%0Aobtained%20superior%20performance%20over%20the%20original%20SAM%2C%20at%0Ahttps%3A//github.com/mazurowski-lab/finetune-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520build%2520the%2520best%2520medical%2520image%2520segmentation%2520algorithm%2520using%250A%2520%2520foundation%2520models%253A%2520a%2520comprehensive%2520empirical%2520study%2520with%2520Segment%2520Anything%250A%2520%2520Model%26entry.906535625%3DHanxue%2520Gu%2520and%2520Haoyu%2520Dong%2520and%2520Jichen%2520Yang%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Automated%2520segmentation%2520is%2520a%2520fundamental%2520medical%2520image%2520analysis%2520task%252C%2520which%250Aenjoys%2520significant%2520advances%2520due%2520to%2520the%2520advent%2520of%2520deep%2520learning.%2520While%250Afoundation%2520models%2520have%2520been%2520useful%2520in%2520natural%2520language%2520processing%2520and%2520some%250Avision%2520tasks%2520for%2520some%2520time%252C%2520the%2520foundation%2520model%2520developed%2520with%2520image%250Asegmentation%2520in%2520mind%2520-%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520-%2520has%2520been%2520developed%2520only%250Arecently%2520and%2520has%2520shown%2520similar%2520promise.%2520However%252C%2520there%2520are%2520still%2520no%2520systematic%250Aanalyses%2520or%2520%2522best-practice%2522%2520guidelines%2520for%2520optimal%2520fine-tuning%2520of%2520SAM%2520for%250Amedical%2520image%2520segmentation.%2520This%2520work%2520summarizes%2520existing%2520fine-tuning%250Astrategies%2520with%2520various%2520backbone%2520architectures%252C%2520model%2520components%252C%2520and%250Afine-tuning%2520algorithms%2520across%252018%2520combinations%252C%2520and%2520evaluates%2520them%2520on%252017%250Adatasets%2520covering%2520all%2520common%2520radiology%2520modalities.%2520Our%2520study%2520reveals%2520that%2520%25281%2529%250Afine-tuning%2520SAM%2520leads%2520to%2520slightly%2520better%2520performance%2520than%2520previous%2520segmentation%250Amethods%252C%2520%25282%2529%2520fine-tuning%2520strategies%2520that%2520use%2520parameter-efficient%2520learning%2520in%250Aboth%2520the%2520encoder%2520and%2520decoder%2520are%2520superior%2520to%2520other%2520strategies%252C%2520%25283%2529%2520network%250Aarchitecture%2520has%2520a%2520small%2520impact%2520on%2520final%2520performance%252C%2520%25284%2529%2520further%2520training%2520SAM%250Awith%2520self-supervised%2520learning%2520can%2520improve%2520final%2520model%2520performance.%2520We%2520also%250Ademonstrate%2520the%2520ineffectiveness%2520of%2520some%2520methods%2520popular%2520in%2520the%2520literature%2520and%250Afurther%2520expand%2520our%2520experiments%2520into%2520few-shot%2520and%2520prompt-based%2520settings.%2520Lastly%252C%250Awe%2520released%2520our%2520code%2520and%2520MRI-specific%2520fine-tuned%2520weights%252C%2520which%2520consistently%250Aobtained%2520superior%2520performance%2520over%2520the%2520original%2520SAM%252C%2520at%250Ahttps%253A//github.com/mazurowski-lab/finetune-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20build%20the%20best%20medical%20image%20segmentation%20algorithm%20using%0A%20%20foundation%20models%3A%20a%20comprehensive%20empirical%20study%20with%20Segment%20Anything%0A%20%20Model&entry.906535625=Hanxue%20Gu%20and%20Haoyu%20Dong%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Automated%20segmentation%20is%20a%20fundamental%20medical%20image%20analysis%20task%2C%20which%0Aenjoys%20significant%20advances%20due%20to%20the%20advent%20of%20deep%20learning.%20While%0Afoundation%20models%20have%20been%20useful%20in%20natural%20language%20processing%20and%20some%0Avision%20tasks%20for%20some%20time%2C%20the%20foundation%20model%20developed%20with%20image%0Asegmentation%20in%20mind%20-%20Segment%20Anything%20Model%20%28SAM%29%20-%20has%20been%20developed%20only%0Arecently%20and%20has%20shown%20similar%20promise.%20However%2C%20there%20are%20still%20no%20systematic%0Aanalyses%20or%20%22best-practice%22%20guidelines%20for%20optimal%20fine-tuning%20of%20SAM%20for%0Amedical%20image%20segmentation.%20This%20work%20summarizes%20existing%20fine-tuning%0Astrategies%20with%20various%20backbone%20architectures%2C%20model%20components%2C%20and%0Afine-tuning%20algorithms%20across%2018%20combinations%2C%20and%20evaluates%20them%20on%2017%0Adatasets%20covering%20all%20common%20radiology%20modalities.%20Our%20study%20reveals%20that%20%281%29%0Afine-tuning%20SAM%20leads%20to%20slightly%20better%20performance%20than%20previous%20segmentation%0Amethods%2C%20%282%29%20fine-tuning%20strategies%20that%20use%20parameter-efficient%20learning%20in%0Aboth%20the%20encoder%20and%20decoder%20are%20superior%20to%20other%20strategies%2C%20%283%29%20network%0Aarchitecture%20has%20a%20small%20impact%20on%20final%20performance%2C%20%284%29%20further%20training%20SAM%0Awith%20self-supervised%20learning%20can%20improve%20final%20model%20performance.%20We%20also%0Ademonstrate%20the%20ineffectiveness%20of%20some%20methods%20popular%20in%20the%20literature%20and%0Afurther%20expand%20our%20experiments%20into%20few-shot%20and%20prompt-based%20settings.%20Lastly%2C%0Awe%20released%20our%20code%20and%20MRI-specific%20fine-tuned%20weights%2C%20which%20consistently%0Aobtained%20superior%20performance%20over%20the%20original%20SAM%2C%20at%0Ahttps%3A//github.com/mazurowski-lab/finetune-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09957v3&entry.124074799=Read"},
{"title": "ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with\n  Hierarchical Planning", "author": "Jiawei Hou and Yuting Xiao and Xiangyang Xue and Taiping Zeng", "abstract": "  We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation\n(ZSON) approach designed for complex multi-room indoor environments.\n  By planning hierarchically leveraging a global topologigal map with layout\ninformation and local imperative approach with detailed scene representation\nmemory, ELA-ZSON achieves both efficient and effective navigation.\n  The process is managed by an LLM-powered agent, ensuring seamless effective\nplanning and navigation, without the need for human interaction, complex\nrewards, or costly training.\n  Our experimental results on the MP3D benchmark achieves 85\\% object\nnavigation success rate (SR) and 79\\% success rate weighted by path length\n(SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared\nto exsisting methods). Furthermore, we validate the robustness of our approach\nthrough virtual agent and real-world robotic deployment, showcasing its\ncapability in practical scenarios. See\nhttps://anonymous.4open.science/r/ELA-ZSON-C67E/ for details.\n", "link": "http://arxiv.org/abs/2505.06131v1", "date": "2025-05-09", "relevancy": 2.2415, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.558}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELA-ZSON%3A%20Efficient%20Layout-Aware%20Zero-Shot%20Object%20Navigation%20Agent%20with%0A%20%20Hierarchical%20Planning&body=Title%3A%20ELA-ZSON%3A%20Efficient%20Layout-Aware%20Zero-Shot%20Object%20Navigation%20Agent%20with%0A%20%20Hierarchical%20Planning%0AAuthor%3A%20Jiawei%20Hou%20and%20Yuting%20Xiao%20and%20Xiangyang%20Xue%20and%20Taiping%20Zeng%0AAbstract%3A%20%20%20We%20introduce%20ELA-ZSON%2C%20an%20efficient%20layout-aware%20zero-shot%20object%20navigation%0A%28ZSON%29%20approach%20designed%20for%20complex%20multi-room%20indoor%20environments.%0A%20%20By%20planning%20hierarchically%20leveraging%20a%20global%20topologigal%20map%20with%20layout%0Ainformation%20and%20local%20imperative%20approach%20with%20detailed%20scene%20representation%0Amemory%2C%20ELA-ZSON%20achieves%20both%20efficient%20and%20effective%20navigation.%0A%20%20The%20process%20is%20managed%20by%20an%20LLM-powered%20agent%2C%20ensuring%20seamless%20effective%0Aplanning%20and%20navigation%2C%20without%20the%20need%20for%20human%20interaction%2C%20complex%0Arewards%2C%20or%20costly%20training.%0A%20%20Our%20experimental%20results%20on%20the%20MP3D%20benchmark%20achieves%2085%5C%25%20object%0Anavigation%20success%20rate%20%28SR%29%20and%2079%5C%25%20success%20rate%20weighted%20by%20path%20length%0A%28SPL%29%20%28over%2040%5C%25%20point%20improvement%20in%20SR%20and%2060%5C%25%20improvement%20in%20SPL%20compared%0Ato%20exsisting%20methods%29.%20Furthermore%2C%20we%20validate%20the%20robustness%20of%20our%20approach%0Athrough%20virtual%20agent%20and%20real-world%20robotic%20deployment%2C%20showcasing%20its%0Acapability%20in%20practical%20scenarios.%20See%0Ahttps%3A//anonymous.4open.science/r/ELA-ZSON-C67E/%20for%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELA-ZSON%253A%2520Efficient%2520Layout-Aware%2520Zero-Shot%2520Object%2520Navigation%2520Agent%2520with%250A%2520%2520Hierarchical%2520Planning%26entry.906535625%3DJiawei%2520Hou%2520and%2520Yuting%2520Xiao%2520and%2520Xiangyang%2520Xue%2520and%2520Taiping%2520Zeng%26entry.1292438233%3D%2520%2520We%2520introduce%2520ELA-ZSON%252C%2520an%2520efficient%2520layout-aware%2520zero-shot%2520object%2520navigation%250A%2528ZSON%2529%2520approach%2520designed%2520for%2520complex%2520multi-room%2520indoor%2520environments.%250A%2520%2520By%2520planning%2520hierarchically%2520leveraging%2520a%2520global%2520topologigal%2520map%2520with%2520layout%250Ainformation%2520and%2520local%2520imperative%2520approach%2520with%2520detailed%2520scene%2520representation%250Amemory%252C%2520ELA-ZSON%2520achieves%2520both%2520efficient%2520and%2520effective%2520navigation.%250A%2520%2520The%2520process%2520is%2520managed%2520by%2520an%2520LLM-powered%2520agent%252C%2520ensuring%2520seamless%2520effective%250Aplanning%2520and%2520navigation%252C%2520without%2520the%2520need%2520for%2520human%2520interaction%252C%2520complex%250Arewards%252C%2520or%2520costly%2520training.%250A%2520%2520Our%2520experimental%2520results%2520on%2520the%2520MP3D%2520benchmark%2520achieves%252085%255C%2525%2520object%250Anavigation%2520success%2520rate%2520%2528SR%2529%2520and%252079%255C%2525%2520success%2520rate%2520weighted%2520by%2520path%2520length%250A%2528SPL%2529%2520%2528over%252040%255C%2525%2520point%2520improvement%2520in%2520SR%2520and%252060%255C%2525%2520improvement%2520in%2520SPL%2520compared%250Ato%2520exsisting%2520methods%2529.%2520Furthermore%252C%2520we%2520validate%2520the%2520robustness%2520of%2520our%2520approach%250Athrough%2520virtual%2520agent%2520and%2520real-world%2520robotic%2520deployment%252C%2520showcasing%2520its%250Acapability%2520in%2520practical%2520scenarios.%2520See%250Ahttps%253A//anonymous.4open.science/r/ELA-ZSON-C67E/%2520for%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELA-ZSON%3A%20Efficient%20Layout-Aware%20Zero-Shot%20Object%20Navigation%20Agent%20with%0A%20%20Hierarchical%20Planning&entry.906535625=Jiawei%20Hou%20and%20Yuting%20Xiao%20and%20Xiangyang%20Xue%20and%20Taiping%20Zeng&entry.1292438233=%20%20We%20introduce%20ELA-ZSON%2C%20an%20efficient%20layout-aware%20zero-shot%20object%20navigation%0A%28ZSON%29%20approach%20designed%20for%20complex%20multi-room%20indoor%20environments.%0A%20%20By%20planning%20hierarchically%20leveraging%20a%20global%20topologigal%20map%20with%20layout%0Ainformation%20and%20local%20imperative%20approach%20with%20detailed%20scene%20representation%0Amemory%2C%20ELA-ZSON%20achieves%20both%20efficient%20and%20effective%20navigation.%0A%20%20The%20process%20is%20managed%20by%20an%20LLM-powered%20agent%2C%20ensuring%20seamless%20effective%0Aplanning%20and%20navigation%2C%20without%20the%20need%20for%20human%20interaction%2C%20complex%0Arewards%2C%20or%20costly%20training.%0A%20%20Our%20experimental%20results%20on%20the%20MP3D%20benchmark%20achieves%2085%5C%25%20object%0Anavigation%20success%20rate%20%28SR%29%20and%2079%5C%25%20success%20rate%20weighted%20by%20path%20length%0A%28SPL%29%20%28over%2040%5C%25%20point%20improvement%20in%20SR%20and%2060%5C%25%20improvement%20in%20SPL%20compared%0Ato%20exsisting%20methods%29.%20Furthermore%2C%20we%20validate%20the%20robustness%20of%20our%20approach%0Athrough%20virtual%20agent%20and%20real-world%20robotic%20deployment%2C%20showcasing%20its%0Acapability%20in%20practical%20scenarios.%20See%0Ahttps%3A//anonymous.4open.science/r/ELA-ZSON-C67E/%20for%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06131v1&entry.124074799=Read"},
{"title": "Sparsification Under Siege: Defending Against Poisoning Attacks in\n  Communication-Efficient Federated Learning", "author": "Zhiyong Jin and Runhua Xu and Chao Li and Yizhong Liu and Jianxin Li", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency.\n", "link": "http://arxiv.org/abs/2505.01454v2", "date": "2025-05-09", "relevancy": 2.2352, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&body=Title%3A%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning%0AAuthor%3A%20Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsification%2520Under%2520Siege%253A%2520Defending%2520Against%2520Poisoning%2520Attacks%2520in%250A%2520%2520Communication-Efficient%2520Federated%2520Learning%26entry.906535625%3DZhiyong%2520Jin%2520and%2520Runhua%2520Xu%2520and%2520Chao%2520Li%2520and%2520Yizhong%2520Liu%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy%252C%2520yet%2520it%2520faces%2520significant%250Achallenges%2520in%2520communication%2520efficiency%2520and%2520vulnerability%2520to%2520poisoning%2520attacks.%250AWhile%2520sparsification%2520techniques%2520mitigate%2520communication%2520overhead%2520by%2520transmitting%250Aonly%2520critical%2520model%2520parameters%252C%2520they%2520inadvertently%2520amplify%2520security%2520risks%253A%250Aadversarial%2520clients%2520can%2520exploit%2520sparse%2520updates%2520to%2520evade%2520detection%2520and%2520degrade%250Amodel%2520performance.%2520Existing%2520defense%2520mechanisms%252C%2520designed%2520for%2520standard%2520FL%250Acommunication%2520scenarios%252C%2520are%2520ineffective%2520in%2520addressing%2520these%2520vulnerabilities%250Awithin%2520sparsified%2520FL.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520FLARE%252C%2520a%2520novel%2520federated%250Alearning%2520framework%2520that%2520integrates%2520sparse%2520index%2520mask%2520inspection%2520and%2520model%250Aupdate%2520sign%2520similarity%2520analysis%2520to%2520detect%2520and%2520mitigate%2520poisoning%2520attacks%2520in%250Asparsified%2520FL.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520adversarial%250Ascenarios%2520demonstrate%2520that%2520FLARE%2520significantly%2520outperforms%2520existing%2520defense%250Astrategies%252C%2520effectively%2520securing%2520sparsified%2520FL%2520against%2520poisoning%2520attacks%2520while%250Amaintaining%2520communication%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&entry.906535625=Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01454v2&entry.124074799=Read"},
{"title": "Embedded Hierarchical MPC for Autonomous Navigation", "author": "Dennis Benders and Johannes K\u00f6hler and Thijs Niesten and Robert Babu\u0161ka and Javier Alonso-Mora and Laura Ferranti", "abstract": "  To efficiently deploy robotic systems in society, mobile robots must move\nautonomously and safely through complex environments. Nonlinear model\npredictive control (MPC) methods provide a natural way to find a dynamically\nfeasible trajectory through the environment without colliding with nearby\nobstacles. However, the limited computation power available on typical embedded\nrobotic systems, such as quadrotors, poses a challenge to running MPC in real\ntime, including its most expensive tasks: constraints generation and\noptimization. To address this problem, we propose a novel hierarchical MPC\nscheme that consists of a planning and a tracking layer. The planner constructs\na trajectory with a long prediction horizon at a slow rate, while the tracker\nensures trajectory tracking at a relatively fast rate. We prove that the\nproposed framework avoids collisions and is recursively feasible. Furthermore,\nwe demonstrate its effectiveness in simulations and lab experiments with a\nquadrotor that needs to reach a goal position in a complex static environment.\nThe code is efficiently implemented on the quadrotor's embedded computer to\nensure real-time feasibility. Compared to a state-of-the-art single-layer MPC\nformulation, this allows us to increase the planning horizon by a factor of 5,\nwhich results in significantly better performance.\n", "link": "http://arxiv.org/abs/2406.11506v4", "date": "2025-05-09", "relevancy": 2.2303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5775}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&body=Title%3A%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation%0AAuthor%3A%20Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti%0AAbstract%3A%20%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20must%20move%0Aautonomously%20and%20safely%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%20real%0Atime%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20consists%20of%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%20constructs%0Aa%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%20the%20tracker%0Aensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%20that%20the%0Aproposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%20Furthermore%2C%0Awe%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%20experiments%20with%20a%0Aquadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%20static%20environment.%0AThe%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%20embedded%20computer%20to%0Aensure%20real-time%20feasibility.%20Compared%20to%20a%20state-of-the-art%20single-layer%20MPC%0Aformulation%2C%20this%20allows%20us%20to%20increase%20the%20planning%20horizon%20by%20a%20factor%20of%205%2C%0Awhich%20results%20in%20significantly%20better%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11506v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520Hierarchical%2520MPC%2520for%2520Autonomous%2520Navigation%26entry.906535625%3DDennis%2520Benders%2520and%2520Johannes%2520K%25C3%25B6hler%2520and%2520Thijs%2520Niesten%2520and%2520Robert%2520Babu%25C5%25A1ka%2520and%2520Javier%2520Alonso-Mora%2520and%2520Laura%2520Ferranti%26entry.1292438233%3D%2520%2520To%2520efficiently%2520deploy%2520robotic%2520systems%2520in%2520society%252C%2520mobile%2520robots%2520must%2520move%250Aautonomously%2520and%2520safely%2520through%2520complex%2520environments.%2520Nonlinear%2520model%250Apredictive%2520control%2520%2528MPC%2529%2520methods%2520provide%2520a%2520natural%2520way%2520to%2520find%2520a%2520dynamically%250Afeasible%2520trajectory%2520through%2520the%2520environment%2520without%2520colliding%2520with%2520nearby%250Aobstacles.%2520However%252C%2520the%2520limited%2520computation%2520power%2520available%2520on%2520typical%2520embedded%250Arobotic%2520systems%252C%2520such%2520as%2520quadrotors%252C%2520poses%2520a%2520challenge%2520to%2520running%2520MPC%2520in%2520real%250Atime%252C%2520including%2520its%2520most%2520expensive%2520tasks%253A%2520constraints%2520generation%2520and%250Aoptimization.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520MPC%250Ascheme%2520that%2520consists%2520of%2520a%2520planning%2520and%2520a%2520tracking%2520layer.%2520The%2520planner%2520constructs%250Aa%2520trajectory%2520with%2520a%2520long%2520prediction%2520horizon%2520at%2520a%2520slow%2520rate%252C%2520while%2520the%2520tracker%250Aensures%2520trajectory%2520tracking%2520at%2520a%2520relatively%2520fast%2520rate.%2520We%2520prove%2520that%2520the%250Aproposed%2520framework%2520avoids%2520collisions%2520and%2520is%2520recursively%2520feasible.%2520Furthermore%252C%250Awe%2520demonstrate%2520its%2520effectiveness%2520in%2520simulations%2520and%2520lab%2520experiments%2520with%2520a%250Aquadrotor%2520that%2520needs%2520to%2520reach%2520a%2520goal%2520position%2520in%2520a%2520complex%2520static%2520environment.%250AThe%2520code%2520is%2520efficiently%2520implemented%2520on%2520the%2520quadrotor%2527s%2520embedded%2520computer%2520to%250Aensure%2520real-time%2520feasibility.%2520Compared%2520to%2520a%2520state-of-the-art%2520single-layer%2520MPC%250Aformulation%252C%2520this%2520allows%2520us%2520to%2520increase%2520the%2520planning%2520horizon%2520by%2520a%2520factor%2520of%25205%252C%250Awhich%2520results%2520in%2520significantly%2520better%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11506v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&entry.906535625=Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti&entry.1292438233=%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20must%20move%0Aautonomously%20and%20safely%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%20real%0Atime%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20consists%20of%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%20constructs%0Aa%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%20the%20tracker%0Aensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%20that%20the%0Aproposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%20Furthermore%2C%0Awe%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%20experiments%20with%20a%0Aquadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%20static%20environment.%0AThe%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%20embedded%20computer%20to%0Aensure%20real-time%20feasibility.%20Compared%20to%20a%20state-of-the-art%20single-layer%20MPC%0Aformulation%2C%20this%20allows%20us%20to%20increase%20the%20planning%20horizon%20by%20a%20factor%20of%205%2C%0Awhich%20results%20in%20significantly%20better%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11506v4&entry.124074799=Read"},
{"title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block", "author": "Maan Alhazmi and Abdulrahman Altahhan", "abstract": "  The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset.\n", "link": "http://arxiv.org/abs/2505.05943v1", "date": "2025-05-09", "relevancy": 2.2207, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%203D%20Attention%20via%20Triplet%20Squeeze%20and%20Excitation%20Block&body=Title%3A%20Achieving%203D%20Attention%20via%20Triplet%20Squeeze%20and%20Excitation%20Block%0AAuthor%3A%20Maan%20Alhazmi%20and%20Abdulrahman%20Altahhan%0AAbstract%3A%20%20%20The%20emergence%20of%20ConvNeXt%20and%20its%20variants%20has%20reaffirmed%20the%20conceptual%20and%0Astructural%20suitability%20of%20CNN-based%20models%20for%20vision%20tasks%2C%20re-establishing%0Athem%20as%20key%20players%20in%20image%20classification%20in%20general%2C%20and%20in%20facial%0Aexpression%20recognition%20%28FER%29%20in%20particular.%20In%20this%20paper%2C%20we%20propose%20a%20new%20set%0Aof%20models%20that%20build%20on%20these%20advancements%20by%20incorporating%20a%20new%20set%20of%0Aattention%20mechanisms%20that%20combines%20Triplet%20attention%20with%0ASqueeze-and-Excitation%20%28TripSE%29%20in%20four%20different%20variants.%20We%20demonstrate%20the%0Aeffectiveness%20of%20these%20variants%20by%20applying%20them%20to%20the%20ResNet18%2C%20DenseNet%20and%0AConvNext%20architectures%20to%20validate%20their%20versatility%20and%20impact.%20Our%20study%0Ashows%20that%20incorporating%20a%20TripSE%20block%20in%20these%20CNN%20models%20boosts%20their%0Aperformances%2C%20particularly%20for%20the%20ConvNeXt%20architecture%2C%20indicating%20its%0Autility.%20We%20evaluate%20the%20proposed%20mechanisms%20and%20associated%20models%20across%20four%0Adatasets%2C%20namely%20CIFAR100%2C%20ImageNet%2C%20FER2013%20and%20AffectNet%20datasets%2C%20where%0AConvNext%20with%20TripSE%20achieves%20state-of-the-art%20results%20with%20an%20accuracy%20of%0A%5Ctextbf%7B78.27%5C%25%7D%20on%20the%20popular%20FER2013%20dataset%2C%20a%20new%20feat%20for%20this%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%25203D%2520Attention%2520via%2520Triplet%2520Squeeze%2520and%2520Excitation%2520Block%26entry.906535625%3DMaan%2520Alhazmi%2520and%2520Abdulrahman%2520Altahhan%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520ConvNeXt%2520and%2520its%2520variants%2520has%2520reaffirmed%2520the%2520conceptual%2520and%250Astructural%2520suitability%2520of%2520CNN-based%2520models%2520for%2520vision%2520tasks%252C%2520re-establishing%250Athem%2520as%2520key%2520players%2520in%2520image%2520classification%2520in%2520general%252C%2520and%2520in%2520facial%250Aexpression%2520recognition%2520%2528FER%2529%2520in%2520particular.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520set%250Aof%2520models%2520that%2520build%2520on%2520these%2520advancements%2520by%2520incorporating%2520a%2520new%2520set%2520of%250Aattention%2520mechanisms%2520that%2520combines%2520Triplet%2520attention%2520with%250ASqueeze-and-Excitation%2520%2528TripSE%2529%2520in%2520four%2520different%2520variants.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520these%2520variants%2520by%2520applying%2520them%2520to%2520the%2520ResNet18%252C%2520DenseNet%2520and%250AConvNext%2520architectures%2520to%2520validate%2520their%2520versatility%2520and%2520impact.%2520Our%2520study%250Ashows%2520that%2520incorporating%2520a%2520TripSE%2520block%2520in%2520these%2520CNN%2520models%2520boosts%2520their%250Aperformances%252C%2520particularly%2520for%2520the%2520ConvNeXt%2520architecture%252C%2520indicating%2520its%250Autility.%2520We%2520evaluate%2520the%2520proposed%2520mechanisms%2520and%2520associated%2520models%2520across%2520four%250Adatasets%252C%2520namely%2520CIFAR100%252C%2520ImageNet%252C%2520FER2013%2520and%2520AffectNet%2520datasets%252C%2520where%250AConvNext%2520with%2520TripSE%2520achieves%2520state-of-the-art%2520results%2520with%2520an%2520accuracy%2520of%250A%255Ctextbf%257B78.27%255C%2525%257D%2520on%2520the%2520popular%2520FER2013%2520dataset%252C%2520a%2520new%2520feat%2520for%2520this%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%203D%20Attention%20via%20Triplet%20Squeeze%20and%20Excitation%20Block&entry.906535625=Maan%20Alhazmi%20and%20Abdulrahman%20Altahhan&entry.1292438233=%20%20The%20emergence%20of%20ConvNeXt%20and%20its%20variants%20has%20reaffirmed%20the%20conceptual%20and%0Astructural%20suitability%20of%20CNN-based%20models%20for%20vision%20tasks%2C%20re-establishing%0Athem%20as%20key%20players%20in%20image%20classification%20in%20general%2C%20and%20in%20facial%0Aexpression%20recognition%20%28FER%29%20in%20particular.%20In%20this%20paper%2C%20we%20propose%20a%20new%20set%0Aof%20models%20that%20build%20on%20these%20advancements%20by%20incorporating%20a%20new%20set%20of%0Aattention%20mechanisms%20that%20combines%20Triplet%20attention%20with%0ASqueeze-and-Excitation%20%28TripSE%29%20in%20four%20different%20variants.%20We%20demonstrate%20the%0Aeffectiveness%20of%20these%20variants%20by%20applying%20them%20to%20the%20ResNet18%2C%20DenseNet%20and%0AConvNext%20architectures%20to%20validate%20their%20versatility%20and%20impact.%20Our%20study%0Ashows%20that%20incorporating%20a%20TripSE%20block%20in%20these%20CNN%20models%20boosts%20their%0Aperformances%2C%20particularly%20for%20the%20ConvNeXt%20architecture%2C%20indicating%20its%0Autility.%20We%20evaluate%20the%20proposed%20mechanisms%20and%20associated%20models%20across%20four%0Adatasets%2C%20namely%20CIFAR100%2C%20ImageNet%2C%20FER2013%20and%20AffectNet%20datasets%2C%20where%0AConvNext%20with%20TripSE%20achieves%20state-of-the-art%20results%20with%20an%20accuracy%20of%0A%5Ctextbf%7B78.27%5C%25%7D%20on%20the%20popular%20FER2013%20dataset%2C%20a%20new%20feat%20for%20this%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05943v1&entry.124074799=Read"},
{"title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for\n  Resource-Efficient 3D Reconstruction", "author": "Noah Frahm and Dongxu Zhao and Andrea Dunn Beltran and Ron Alterovitz and Jan-Michael Frahm and Junier Oliva and Roni Sengupta", "abstract": "  Next Best View (NBV) algorithms aim to acquire an optimal set of images using\nminimal resources, time, or number of captures to enable efficient 3D\nreconstruction of a scene. Existing approaches often rely on prior scene\nknowledge or additional image captures and often develop policies that maximize\ncoverage. Yet, for many real scenes with complex geometry and self-occlusions,\ncoverage maximization does not lead to better reconstruction quality directly.\nIn this paper, we propose the View Introspection Network (VIN), which is\ntrained to predict the reconstruction quality improvement of views directly,\nand the VIN-NBV policy. A greedy sequential sampling-based policy, where at\neach acquisition step, we sample multiple query views and choose the one with\nthe highest VIN predicted improvement score. We design the VIN to perform\n3D-aware featurization of the reconstruction built from prior acquisitions, and\nfor each query view create a feature that can be decoded into an improvement\nscore. We then train the VIN using imitation learning to predict the\nreconstruction improvement score. We show that VIN-NBV improves reconstruction\nquality by ~30% over a coverage maximization baseline when operating with\nconstraints on the number of acquisitions or the time in motion.\n", "link": "http://arxiv.org/abs/2505.06219v1", "date": "2025-05-09", "relevancy": 2.1964, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction&body=Title%3A%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction%0AAuthor%3A%20Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20acquire%20an%20optimal%20set%20of%20images%20using%0Aminimal%20resources%2C%20time%2C%20or%20number%20of%20captures%20to%20enable%20efficient%203D%0Areconstruction%20of%20a%20scene.%20Existing%20approaches%20often%20rely%20on%20prior%20scene%0Aknowledge%20or%20additional%20image%20captures%20and%20often%20develop%20policies%20that%20maximize%0Acoverage.%20Yet%2C%20for%20many%20real%20scenes%20with%20complex%20geometry%20and%20self-occlusions%2C%0Acoverage%20maximization%20does%20not%20lead%20to%20better%20reconstruction%20quality%20directly.%0AIn%20this%20paper%2C%20we%20propose%20the%20View%20Introspection%20Network%20%28VIN%29%2C%20which%20is%0Atrained%20to%20predict%20the%20reconstruction%20quality%20improvement%20of%20views%20directly%2C%0Aand%20the%20VIN-NBV%20policy.%20A%20greedy%20sequential%20sampling-based%20policy%2C%20where%20at%0Aeach%20acquisition%20step%2C%20we%20sample%20multiple%20query%20views%20and%20choose%20the%20one%20with%0Athe%20highest%20VIN%20predicted%20improvement%20score.%20We%20design%20the%20VIN%20to%20perform%0A3D-aware%20featurization%20of%20the%20reconstruction%20built%20from%20prior%20acquisitions%2C%20and%0Afor%20each%20query%20view%20create%20a%20feature%20that%20can%20be%20decoded%20into%20an%20improvement%0Ascore.%20We%20then%20train%20the%20VIN%20using%20imitation%20learning%20to%20predict%20the%0Areconstruction%20improvement%20score.%20We%20show%20that%20VIN-NBV%20improves%20reconstruction%0Aquality%20by%20~30%25%20over%20a%20coverage%20maximization%20baseline%20when%20operating%20with%0Aconstraints%20on%20the%20number%20of%20acquisitions%20or%20the%20time%20in%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIN-NBV%253A%2520A%2520View%2520Introspection%2520Network%2520for%2520Next-Best-View%2520Selection%2520for%250A%2520%2520Resource-Efficient%25203D%2520Reconstruction%26entry.906535625%3DNoah%2520Frahm%2520and%2520Dongxu%2520Zhao%2520and%2520Andrea%2520Dunn%2520Beltran%2520and%2520Ron%2520Alterovitz%2520and%2520Jan-Michael%2520Frahm%2520and%2520Junier%2520Oliva%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520Next%2520Best%2520View%2520%2528NBV%2529%2520algorithms%2520aim%2520to%2520acquire%2520an%2520optimal%2520set%2520of%2520images%2520using%250Aminimal%2520resources%252C%2520time%252C%2520or%2520number%2520of%2520captures%2520to%2520enable%2520efficient%25203D%250Areconstruction%2520of%2520a%2520scene.%2520Existing%2520approaches%2520often%2520rely%2520on%2520prior%2520scene%250Aknowledge%2520or%2520additional%2520image%2520captures%2520and%2520often%2520develop%2520policies%2520that%2520maximize%250Acoverage.%2520Yet%252C%2520for%2520many%2520real%2520scenes%2520with%2520complex%2520geometry%2520and%2520self-occlusions%252C%250Acoverage%2520maximization%2520does%2520not%2520lead%2520to%2520better%2520reconstruction%2520quality%2520directly.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520the%2520View%2520Introspection%2520Network%2520%2528VIN%2529%252C%2520which%2520is%250Atrained%2520to%2520predict%2520the%2520reconstruction%2520quality%2520improvement%2520of%2520views%2520directly%252C%250Aand%2520the%2520VIN-NBV%2520policy.%2520A%2520greedy%2520sequential%2520sampling-based%2520policy%252C%2520where%2520at%250Aeach%2520acquisition%2520step%252C%2520we%2520sample%2520multiple%2520query%2520views%2520and%2520choose%2520the%2520one%2520with%250Athe%2520highest%2520VIN%2520predicted%2520improvement%2520score.%2520We%2520design%2520the%2520VIN%2520to%2520perform%250A3D-aware%2520featurization%2520of%2520the%2520reconstruction%2520built%2520from%2520prior%2520acquisitions%252C%2520and%250Afor%2520each%2520query%2520view%2520create%2520a%2520feature%2520that%2520can%2520be%2520decoded%2520into%2520an%2520improvement%250Ascore.%2520We%2520then%2520train%2520the%2520VIN%2520using%2520imitation%2520learning%2520to%2520predict%2520the%250Areconstruction%2520improvement%2520score.%2520We%2520show%2520that%2520VIN-NBV%2520improves%2520reconstruction%250Aquality%2520by%2520~30%2525%2520over%2520a%2520coverage%2520maximization%2520baseline%2520when%2520operating%2520with%250Aconstraints%2520on%2520the%2520number%2520of%2520acquisitions%2520or%2520the%2520time%2520in%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%20for%0A%20%20Resource-Efficient%203D%20Reconstruction&entry.906535625=Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta&entry.1292438233=%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20acquire%20an%20optimal%20set%20of%20images%20using%0Aminimal%20resources%2C%20time%2C%20or%20number%20of%20captures%20to%20enable%20efficient%203D%0Areconstruction%20of%20a%20scene.%20Existing%20approaches%20often%20rely%20on%20prior%20scene%0Aknowledge%20or%20additional%20image%20captures%20and%20often%20develop%20policies%20that%20maximize%0Acoverage.%20Yet%2C%20for%20many%20real%20scenes%20with%20complex%20geometry%20and%20self-occlusions%2C%0Acoverage%20maximization%20does%20not%20lead%20to%20better%20reconstruction%20quality%20directly.%0AIn%20this%20paper%2C%20we%20propose%20the%20View%20Introspection%20Network%20%28VIN%29%2C%20which%20is%0Atrained%20to%20predict%20the%20reconstruction%20quality%20improvement%20of%20views%20directly%2C%0Aand%20the%20VIN-NBV%20policy.%20A%20greedy%20sequential%20sampling-based%20policy%2C%20where%20at%0Aeach%20acquisition%20step%2C%20we%20sample%20multiple%20query%20views%20and%20choose%20the%20one%20with%0Athe%20highest%20VIN%20predicted%20improvement%20score.%20We%20design%20the%20VIN%20to%20perform%0A3D-aware%20featurization%20of%20the%20reconstruction%20built%20from%20prior%20acquisitions%2C%20and%0Afor%20each%20query%20view%20create%20a%20feature%20that%20can%20be%20decoded%20into%20an%20improvement%0Ascore.%20We%20then%20train%20the%20VIN%20using%20imitation%20learning%20to%20predict%20the%0Areconstruction%20improvement%20score.%20We%20show%20that%20VIN-NBV%20improves%20reconstruction%0Aquality%20by%20~30%25%20over%20a%20coverage%20maximization%20baseline%20when%20operating%20with%0Aconstraints%20on%20the%20number%20of%20acquisitions%20or%20the%20time%20in%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06219v1&entry.124074799=Read"},
{"title": "CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation\n  for UAV Tracking", "author": "Weihong Li and Xiaoqiong Liu and Heng Fan and Libo Zhang", "abstract": "  Recent advancements in visual object tracking have markedly improved the\ncapabilities of unmanned aerial vehicle (UAV) tracking, which is a critical\ncomponent in real-world robotics applications. While the integration of\nhierarchical lightweight networks has become a prevalent strategy for enhancing\nefficiency in UAV tracking, it often results in a significant drop in network\ncapacity, which further exacerbates challenges in UAV scenarios, such as\nfrequent occlusions and extreme changes in viewing angles. To address these\nissues, we introduce a novel family of UAV trackers, termed CGTrack, which\ncombines explicit and implicit techniques to expand network capacity within a\ncoarse-to-fine framework. Specifically, we first introduce a Hierarchical\nFeature Cascade (HFC) module that leverages the spirit of feature reuse to\nincrease network capacity by integrating the deep semantic cues with the rich\nspatial information, incurring minimal computational costs while enhancing\nfeature representation. Based on this, we design a novel Lightweight Gated\nCenter Head (LGCH) that utilizes gating mechanisms to decouple target-oriented\ncoordinates from previously expanded features, which contain dense local\ndiscriminative information. Extensive experiments on three challenging UAV\ntracking benchmarks demonstrate that CGTrack achieves state-of-the-art\nperformance while running fast. Code will be available at\nhttps://github.com/Nightwatch-Fox11/CGTrack.\n", "link": "http://arxiv.org/abs/2505.05936v1", "date": "2025-05-09", "relevancy": 2.1657, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5589}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5399}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGTrack%3A%20Cascade%20Gating%20Network%20with%20Hierarchical%20Feature%20Aggregation%0A%20%20for%20UAV%20Tracking&body=Title%3A%20CGTrack%3A%20Cascade%20Gating%20Network%20with%20Hierarchical%20Feature%20Aggregation%0A%20%20for%20UAV%20Tracking%0AAuthor%3A%20Weihong%20Li%20and%20Xiaoqiong%20Liu%20and%20Heng%20Fan%20and%20Libo%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20visual%20object%20tracking%20have%20markedly%20improved%20the%0Acapabilities%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20tracking%2C%20which%20is%20a%20critical%0Acomponent%20in%20real-world%20robotics%20applications.%20While%20the%20integration%20of%0Ahierarchical%20lightweight%20networks%20has%20become%20a%20prevalent%20strategy%20for%20enhancing%0Aefficiency%20in%20UAV%20tracking%2C%20it%20often%20results%20in%20a%20significant%20drop%20in%20network%0Acapacity%2C%20which%20further%20exacerbates%20challenges%20in%20UAV%20scenarios%2C%20such%20as%0Afrequent%20occlusions%20and%20extreme%20changes%20in%20viewing%20angles.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20novel%20family%20of%20UAV%20trackers%2C%20termed%20CGTrack%2C%20which%0Acombines%20explicit%20and%20implicit%20techniques%20to%20expand%20network%20capacity%20within%20a%0Acoarse-to-fine%20framework.%20Specifically%2C%20we%20first%20introduce%20a%20Hierarchical%0AFeature%20Cascade%20%28HFC%29%20module%20that%20leverages%20the%20spirit%20of%20feature%20reuse%20to%0Aincrease%20network%20capacity%20by%20integrating%20the%20deep%20semantic%20cues%20with%20the%20rich%0Aspatial%20information%2C%20incurring%20minimal%20computational%20costs%20while%20enhancing%0Afeature%20representation.%20Based%20on%20this%2C%20we%20design%20a%20novel%20Lightweight%20Gated%0ACenter%20Head%20%28LGCH%29%20that%20utilizes%20gating%20mechanisms%20to%20decouple%20target-oriented%0Acoordinates%20from%20previously%20expanded%20features%2C%20which%20contain%20dense%20local%0Adiscriminative%20information.%20Extensive%20experiments%20on%20three%20challenging%20UAV%0Atracking%20benchmarks%20demonstrate%20that%20CGTrack%20achieves%20state-of-the-art%0Aperformance%20while%20running%20fast.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/Nightwatch-Fox11/CGTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGTrack%253A%2520Cascade%2520Gating%2520Network%2520with%2520Hierarchical%2520Feature%2520Aggregation%250A%2520%2520for%2520UAV%2520Tracking%26entry.906535625%3DWeihong%2520Li%2520and%2520Xiaoqiong%2520Liu%2520and%2520Heng%2520Fan%2520and%2520Libo%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520visual%2520object%2520tracking%2520have%2520markedly%2520improved%2520the%250Acapabilities%2520of%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520tracking%252C%2520which%2520is%2520a%2520critical%250Acomponent%2520in%2520real-world%2520robotics%2520applications.%2520While%2520the%2520integration%2520of%250Ahierarchical%2520lightweight%2520networks%2520has%2520become%2520a%2520prevalent%2520strategy%2520for%2520enhancing%250Aefficiency%2520in%2520UAV%2520tracking%252C%2520it%2520often%2520results%2520in%2520a%2520significant%2520drop%2520in%2520network%250Acapacity%252C%2520which%2520further%2520exacerbates%2520challenges%2520in%2520UAV%2520scenarios%252C%2520such%2520as%250Afrequent%2520occlusions%2520and%2520extreme%2520changes%2520in%2520viewing%2520angles.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520a%2520novel%2520family%2520of%2520UAV%2520trackers%252C%2520termed%2520CGTrack%252C%2520which%250Acombines%2520explicit%2520and%2520implicit%2520techniques%2520to%2520expand%2520network%2520capacity%2520within%2520a%250Acoarse-to-fine%2520framework.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520Hierarchical%250AFeature%2520Cascade%2520%2528HFC%2529%2520module%2520that%2520leverages%2520the%2520spirit%2520of%2520feature%2520reuse%2520to%250Aincrease%2520network%2520capacity%2520by%2520integrating%2520the%2520deep%2520semantic%2520cues%2520with%2520the%2520rich%250Aspatial%2520information%252C%2520incurring%2520minimal%2520computational%2520costs%2520while%2520enhancing%250Afeature%2520representation.%2520Based%2520on%2520this%252C%2520we%2520design%2520a%2520novel%2520Lightweight%2520Gated%250ACenter%2520Head%2520%2528LGCH%2529%2520that%2520utilizes%2520gating%2520mechanisms%2520to%2520decouple%2520target-oriented%250Acoordinates%2520from%2520previously%2520expanded%2520features%252C%2520which%2520contain%2520dense%2520local%250Adiscriminative%2520information.%2520Extensive%2520experiments%2520on%2520three%2520challenging%2520UAV%250Atracking%2520benchmarks%2520demonstrate%2520that%2520CGTrack%2520achieves%2520state-of-the-art%250Aperformance%2520while%2520running%2520fast.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Nightwatch-Fox11/CGTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGTrack%3A%20Cascade%20Gating%20Network%20with%20Hierarchical%20Feature%20Aggregation%0A%20%20for%20UAV%20Tracking&entry.906535625=Weihong%20Li%20and%20Xiaoqiong%20Liu%20and%20Heng%20Fan%20and%20Libo%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20visual%20object%20tracking%20have%20markedly%20improved%20the%0Acapabilities%20of%20unmanned%20aerial%20vehicle%20%28UAV%29%20tracking%2C%20which%20is%20a%20critical%0Acomponent%20in%20real-world%20robotics%20applications.%20While%20the%20integration%20of%0Ahierarchical%20lightweight%20networks%20has%20become%20a%20prevalent%20strategy%20for%20enhancing%0Aefficiency%20in%20UAV%20tracking%2C%20it%20often%20results%20in%20a%20significant%20drop%20in%20network%0Acapacity%2C%20which%20further%20exacerbates%20challenges%20in%20UAV%20scenarios%2C%20such%20as%0Afrequent%20occlusions%20and%20extreme%20changes%20in%20viewing%20angles.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20novel%20family%20of%20UAV%20trackers%2C%20termed%20CGTrack%2C%20which%0Acombines%20explicit%20and%20implicit%20techniques%20to%20expand%20network%20capacity%20within%20a%0Acoarse-to-fine%20framework.%20Specifically%2C%20we%20first%20introduce%20a%20Hierarchical%0AFeature%20Cascade%20%28HFC%29%20module%20that%20leverages%20the%20spirit%20of%20feature%20reuse%20to%0Aincrease%20network%20capacity%20by%20integrating%20the%20deep%20semantic%20cues%20with%20the%20rich%0Aspatial%20information%2C%20incurring%20minimal%20computational%20costs%20while%20enhancing%0Afeature%20representation.%20Based%20on%20this%2C%20we%20design%20a%20novel%20Lightweight%20Gated%0ACenter%20Head%20%28LGCH%29%20that%20utilizes%20gating%20mechanisms%20to%20decouple%20target-oriented%0Acoordinates%20from%20previously%20expanded%20features%2C%20which%20contain%20dense%20local%0Adiscriminative%20information.%20Extensive%20experiments%20on%20three%20challenging%20UAV%0Atracking%20benchmarks%20demonstrate%20that%20CGTrack%20achieves%20state-of-the-art%0Aperformance%20while%20running%20fast.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/Nightwatch-Fox11/CGTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05936v1&entry.124074799=Read"},
{"title": "Credal Wrapper of Model Averaging for Uncertainty Estimation in\n  Classification", "author": "Kaizheng Wang and Fabio Cuzzolin and Keivan Shariatmadar and David Moens and Hans Hallez", "abstract": "  This paper presents an innovative approach, called credal wrapper, to\nformulating a credal set representation of model averaging for Bayesian neural\nnetworks (BNNs) and deep ensembles (DEs), capable of improving uncertainty\nestimation in classification tasks. Given a finite collection of single\npredictive distributions derived from BNNs or DEs, the proposed credal wrapper\napproach extracts an upper and a lower probability bound per class,\nacknowledging the epistemic uncertainty due to the availability of a limited\namount of distributions. Such probability intervals over classes can be mapped\non a convex set of probabilities (a credal set) from which, in turn, a unique\nprediction can be obtained using a transformation called intersection\nprobability transformation. In this article, we conduct extensive experiments\non several out-of-distribution (OOD) detection benchmarks, encompassing various\ndataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C,\nCIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network\narchitectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base).\nCompared to the BNN and DE baselines, the proposed credal wrapper method\nexhibits superior performance in uncertainty estimation and achieves a lower\nexpected calibration error on corrupted data.\n", "link": "http://arxiv.org/abs/2405.15047v2", "date": "2025-05-09", "relevancy": 2.1601, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5798}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5339}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Credal%20Wrapper%20of%20Model%20Averaging%20for%20Uncertainty%20Estimation%20in%0A%20%20Classification&body=Title%3A%20Credal%20Wrapper%20of%20Model%20Averaging%20for%20Uncertainty%20Estimation%20in%0A%20%20Classification%0AAuthor%3A%20Kaizheng%20Wang%20and%20Fabio%20Cuzzolin%20and%20Keivan%20Shariatmadar%20and%20David%20Moens%20and%20Hans%20Hallez%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20innovative%20approach%2C%20called%20credal%20wrapper%2C%20to%0Aformulating%20a%20credal%20set%20representation%20of%20model%20averaging%20for%20Bayesian%20neural%0Anetworks%20%28BNNs%29%20and%20deep%20ensembles%20%28DEs%29%2C%20capable%20of%20improving%20uncertainty%0Aestimation%20in%20classification%20tasks.%20Given%20a%20finite%20collection%20of%20single%0Apredictive%20distributions%20derived%20from%20BNNs%20or%20DEs%2C%20the%20proposed%20credal%20wrapper%0Aapproach%20extracts%20an%20upper%20and%20a%20lower%20probability%20bound%20per%20class%2C%0Aacknowledging%20the%20epistemic%20uncertainty%20due%20to%20the%20availability%20of%20a%20limited%0Aamount%20of%20distributions.%20Such%20probability%20intervals%20over%20classes%20can%20be%20mapped%0Aon%20a%20convex%20set%20of%20probabilities%20%28a%20credal%20set%29%20from%20which%2C%20in%20turn%2C%20a%20unique%0Aprediction%20can%20be%20obtained%20using%20a%20transformation%20called%20intersection%0Aprobability%20transformation.%20In%20this%20article%2C%20we%20conduct%20extensive%20experiments%0Aon%20several%20out-of-distribution%20%28OOD%29%20detection%20benchmarks%2C%20encompassing%20various%0Adataset%20pairs%20%28CIFAR10/100%20vs%20SVHN/Tiny-ImageNet%2C%20CIFAR10%20vs%20CIFAR10-C%2C%0ACIFAR100%20vs%20CIFAR100-C%20and%20ImageNet%20vs%20ImageNet-O%29%20and%20using%20different%20network%0Aarchitectures%20%28such%20as%20VGG16%2C%20ResNet-18/50%2C%20EfficientNet%20B2%2C%20and%20ViT%20Base%29.%0ACompared%20to%20the%20BNN%20and%20DE%20baselines%2C%20the%20proposed%20credal%20wrapper%20method%0Aexhibits%20superior%20performance%20in%20uncertainty%20estimation%20and%20achieves%20a%20lower%0Aexpected%20calibration%20error%20on%20corrupted%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCredal%2520Wrapper%2520of%2520Model%2520Averaging%2520for%2520Uncertainty%2520Estimation%2520in%250A%2520%2520Classification%26entry.906535625%3DKaizheng%2520Wang%2520and%2520Fabio%2520Cuzzolin%2520and%2520Keivan%2520Shariatmadar%2520and%2520David%2520Moens%2520and%2520Hans%2520Hallez%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520innovative%2520approach%252C%2520called%2520credal%2520wrapper%252C%2520to%250Aformulating%2520a%2520credal%2520set%2520representation%2520of%2520model%2520averaging%2520for%2520Bayesian%2520neural%250Anetworks%2520%2528BNNs%2529%2520and%2520deep%2520ensembles%2520%2528DEs%2529%252C%2520capable%2520of%2520improving%2520uncertainty%250Aestimation%2520in%2520classification%2520tasks.%2520Given%2520a%2520finite%2520collection%2520of%2520single%250Apredictive%2520distributions%2520derived%2520from%2520BNNs%2520or%2520DEs%252C%2520the%2520proposed%2520credal%2520wrapper%250Aapproach%2520extracts%2520an%2520upper%2520and%2520a%2520lower%2520probability%2520bound%2520per%2520class%252C%250Aacknowledging%2520the%2520epistemic%2520uncertainty%2520due%2520to%2520the%2520availability%2520of%2520a%2520limited%250Aamount%2520of%2520distributions.%2520Such%2520probability%2520intervals%2520over%2520classes%2520can%2520be%2520mapped%250Aon%2520a%2520convex%2520set%2520of%2520probabilities%2520%2528a%2520credal%2520set%2529%2520from%2520which%252C%2520in%2520turn%252C%2520a%2520unique%250Aprediction%2520can%2520be%2520obtained%2520using%2520a%2520transformation%2520called%2520intersection%250Aprobability%2520transformation.%2520In%2520this%2520article%252C%2520we%2520conduct%2520extensive%2520experiments%250Aon%2520several%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520benchmarks%252C%2520encompassing%2520various%250Adataset%2520pairs%2520%2528CIFAR10/100%2520vs%2520SVHN/Tiny-ImageNet%252C%2520CIFAR10%2520vs%2520CIFAR10-C%252C%250ACIFAR100%2520vs%2520CIFAR100-C%2520and%2520ImageNet%2520vs%2520ImageNet-O%2529%2520and%2520using%2520different%2520network%250Aarchitectures%2520%2528such%2520as%2520VGG16%252C%2520ResNet-18/50%252C%2520EfficientNet%2520B2%252C%2520and%2520ViT%2520Base%2529.%250ACompared%2520to%2520the%2520BNN%2520and%2520DE%2520baselines%252C%2520the%2520proposed%2520credal%2520wrapper%2520method%250Aexhibits%2520superior%2520performance%2520in%2520uncertainty%2520estimation%2520and%2520achieves%2520a%2520lower%250Aexpected%2520calibration%2520error%2520on%2520corrupted%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Credal%20Wrapper%20of%20Model%20Averaging%20for%20Uncertainty%20Estimation%20in%0A%20%20Classification&entry.906535625=Kaizheng%20Wang%20and%20Fabio%20Cuzzolin%20and%20Keivan%20Shariatmadar%20and%20David%20Moens%20and%20Hans%20Hallez&entry.1292438233=%20%20This%20paper%20presents%20an%20innovative%20approach%2C%20called%20credal%20wrapper%2C%20to%0Aformulating%20a%20credal%20set%20representation%20of%20model%20averaging%20for%20Bayesian%20neural%0Anetworks%20%28BNNs%29%20and%20deep%20ensembles%20%28DEs%29%2C%20capable%20of%20improving%20uncertainty%0Aestimation%20in%20classification%20tasks.%20Given%20a%20finite%20collection%20of%20single%0Apredictive%20distributions%20derived%20from%20BNNs%20or%20DEs%2C%20the%20proposed%20credal%20wrapper%0Aapproach%20extracts%20an%20upper%20and%20a%20lower%20probability%20bound%20per%20class%2C%0Aacknowledging%20the%20epistemic%20uncertainty%20due%20to%20the%20availability%20of%20a%20limited%0Aamount%20of%20distributions.%20Such%20probability%20intervals%20over%20classes%20can%20be%20mapped%0Aon%20a%20convex%20set%20of%20probabilities%20%28a%20credal%20set%29%20from%20which%2C%20in%20turn%2C%20a%20unique%0Aprediction%20can%20be%20obtained%20using%20a%20transformation%20called%20intersection%0Aprobability%20transformation.%20In%20this%20article%2C%20we%20conduct%20extensive%20experiments%0Aon%20several%20out-of-distribution%20%28OOD%29%20detection%20benchmarks%2C%20encompassing%20various%0Adataset%20pairs%20%28CIFAR10/100%20vs%20SVHN/Tiny-ImageNet%2C%20CIFAR10%20vs%20CIFAR10-C%2C%0ACIFAR100%20vs%20CIFAR100-C%20and%20ImageNet%20vs%20ImageNet-O%29%20and%20using%20different%20network%0Aarchitectures%20%28such%20as%20VGG16%2C%20ResNet-18/50%2C%20EfficientNet%20B2%2C%20and%20ViT%20Base%29.%0ACompared%20to%20the%20BNN%20and%20DE%20baselines%2C%20the%20proposed%20credal%20wrapper%20method%0Aexhibits%20superior%20performance%20in%20uncertainty%20estimation%20and%20achieves%20a%20lower%0Aexpected%20calibration%20error%20on%20corrupted%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15047v2&entry.124074799=Read"},
{"title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text\n  Dataset Derived from Textbooks", "author": "Wenqi Zeng and Yuqi Sun and Chenxi Ma and Weimin Tan and Bo Yan", "abstract": "  Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin\n", "link": "http://arxiv.org/abs/2505.06152v1", "date": "2025-05-09", "relevancy": 2.1473, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Skin%3A%20Enhancing%20Dermatology%20Vision-Language%20Model%20with%20an%20Image-Text%0A%20%20Dataset%20Derived%20from%20Textbooks&body=Title%3A%20MM-Skin%3A%20Enhancing%20Dermatology%20Vision-Language%20Model%20with%20an%20Image-Text%0A%20%20Dataset%20Derived%20from%20Textbooks%0AAuthor%3A%20Wenqi%20Zeng%20and%20Yuqi%20Sun%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan%0AAbstract%3A%20%20%20Medical%20vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20as%20clinical%0Aassistants%20across%20various%20medical%20fields.%20However%2C%20specialized%20dermatology%20VLM%0Acapable%20of%20delivering%20professional%20and%20detailed%20diagnostic%20analysis%20remains%0Aunderdeveloped%2C%20primarily%20due%20to%20less%20specialized%20text%20descriptions%20in%20current%0Adermatology%20multimodal%20datasets.%20To%20address%20this%20issue%2C%20we%20propose%20MM-Skin%2C%20the%0Afirst%20large-scale%20multimodal%20dermatology%20dataset%20that%20encompasses%203%20imaging%0Amodalities%2C%20including%20clinical%2C%20dermoscopic%2C%20and%20pathological%20and%20nearly%2010k%0Ahigh-quality%20image-text%20pairs%20collected%20from%20professional%20textbooks.%20In%0Aaddition%2C%20we%20generate%20over%2027k%20diverse%2C%20instruction-following%20vision%20question%0Aanswering%20%28VQA%29%20samples%20%289%20times%20the%20size%20of%20current%20largest%20dermatology%20VQA%0Adataset%29.%20Leveraging%20public%20datasets%20and%20MM-Skin%2C%20we%20developed%20SkinVL%2C%20a%0Adermatology-specific%20VLM%20designed%20for%20precise%20and%20nuanced%20skin%20disease%0Ainterpretation.%20Comprehensive%20benchmark%20evaluations%20of%20SkinVL%20on%20VQA%2C%0Asupervised%20fine-tuning%20%28SFT%29%20and%20zero-shot%20classification%20tasks%20across%208%0Adatasets%2C%20reveal%20its%20exceptional%20performance%20for%20skin%20diseases%20in%20comparison%20to%0Aboth%20general%20and%20medical%20VLM%20models.%20The%20introduction%20of%20MM-Skin%20and%20SkinVL%0Aoffers%20a%20meaningful%20contribution%20to%20advancing%20the%20development%20of%20clinical%0Adermatology%20VLM%20assistants.%20MM-Skin%20is%20available%20at%0Ahttps%3A//github.com/ZwQ803/MM-Skin%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Skin%253A%2520Enhancing%2520Dermatology%2520Vision-Language%2520Model%2520with%2520an%2520Image-Text%250A%2520%2520Dataset%2520Derived%2520from%2520Textbooks%26entry.906535625%3DWenqi%2520Zeng%2520and%2520Yuqi%2520Sun%2520and%2520Chenxi%2520Ma%2520and%2520Weimin%2520Tan%2520and%2520Bo%2520Yan%26entry.1292438233%3D%2520%2520Medical%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520promise%2520as%2520clinical%250Aassistants%2520across%2520various%2520medical%2520fields.%2520However%252C%2520specialized%2520dermatology%2520VLM%250Acapable%2520of%2520delivering%2520professional%2520and%2520detailed%2520diagnostic%2520analysis%2520remains%250Aunderdeveloped%252C%2520primarily%2520due%2520to%2520less%2520specialized%2520text%2520descriptions%2520in%2520current%250Adermatology%2520multimodal%2520datasets.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520MM-Skin%252C%2520the%250Afirst%2520large-scale%2520multimodal%2520dermatology%2520dataset%2520that%2520encompasses%25203%2520imaging%250Amodalities%252C%2520including%2520clinical%252C%2520dermoscopic%252C%2520and%2520pathological%2520and%2520nearly%252010k%250Ahigh-quality%2520image-text%2520pairs%2520collected%2520from%2520professional%2520textbooks.%2520In%250Aaddition%252C%2520we%2520generate%2520over%252027k%2520diverse%252C%2520instruction-following%2520vision%2520question%250Aanswering%2520%2528VQA%2529%2520samples%2520%25289%2520times%2520the%2520size%2520of%2520current%2520largest%2520dermatology%2520VQA%250Adataset%2529.%2520Leveraging%2520public%2520datasets%2520and%2520MM-Skin%252C%2520we%2520developed%2520SkinVL%252C%2520a%250Adermatology-specific%2520VLM%2520designed%2520for%2520precise%2520and%2520nuanced%2520skin%2520disease%250Ainterpretation.%2520Comprehensive%2520benchmark%2520evaluations%2520of%2520SkinVL%2520on%2520VQA%252C%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520zero-shot%2520classification%2520tasks%2520across%25208%250Adatasets%252C%2520reveal%2520its%2520exceptional%2520performance%2520for%2520skin%2520diseases%2520in%2520comparison%2520to%250Aboth%2520general%2520and%2520medical%2520VLM%2520models.%2520The%2520introduction%2520of%2520MM-Skin%2520and%2520SkinVL%250Aoffers%2520a%2520meaningful%2520contribution%2520to%2520advancing%2520the%2520development%2520of%2520clinical%250Adermatology%2520VLM%2520assistants.%2520MM-Skin%2520is%2520available%2520at%250Ahttps%253A//github.com/ZwQ803/MM-Skin%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Skin%3A%20Enhancing%20Dermatology%20Vision-Language%20Model%20with%20an%20Image-Text%0A%20%20Dataset%20Derived%20from%20Textbooks&entry.906535625=Wenqi%20Zeng%20and%20Yuqi%20Sun%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan&entry.1292438233=%20%20Medical%20vision-language%20models%20%28VLMs%29%20have%20shown%20promise%20as%20clinical%0Aassistants%20across%20various%20medical%20fields.%20However%2C%20specialized%20dermatology%20VLM%0Acapable%20of%20delivering%20professional%20and%20detailed%20diagnostic%20analysis%20remains%0Aunderdeveloped%2C%20primarily%20due%20to%20less%20specialized%20text%20descriptions%20in%20current%0Adermatology%20multimodal%20datasets.%20To%20address%20this%20issue%2C%20we%20propose%20MM-Skin%2C%20the%0Afirst%20large-scale%20multimodal%20dermatology%20dataset%20that%20encompasses%203%20imaging%0Amodalities%2C%20including%20clinical%2C%20dermoscopic%2C%20and%20pathological%20and%20nearly%2010k%0Ahigh-quality%20image-text%20pairs%20collected%20from%20professional%20textbooks.%20In%0Aaddition%2C%20we%20generate%20over%2027k%20diverse%2C%20instruction-following%20vision%20question%0Aanswering%20%28VQA%29%20samples%20%289%20times%20the%20size%20of%20current%20largest%20dermatology%20VQA%0Adataset%29.%20Leveraging%20public%20datasets%20and%20MM-Skin%2C%20we%20developed%20SkinVL%2C%20a%0Adermatology-specific%20VLM%20designed%20for%20precise%20and%20nuanced%20skin%20disease%0Ainterpretation.%20Comprehensive%20benchmark%20evaluations%20of%20SkinVL%20on%20VQA%2C%0Asupervised%20fine-tuning%20%28SFT%29%20and%20zero-shot%20classification%20tasks%20across%208%0Adatasets%2C%20reveal%20its%20exceptional%20performance%20for%20skin%20diseases%20in%20comparison%20to%0Aboth%20general%20and%20medical%20VLM%20models.%20The%20introduction%20of%20MM-Skin%20and%20SkinVL%0Aoffers%20a%20meaningful%20contribution%20to%20advancing%20the%20development%20of%20clinical%0Adermatology%20VLM%20assistants.%20MM-Skin%20is%20available%20at%0Ahttps%3A//github.com/ZwQ803/MM-Skin%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06152v1&entry.124074799=Read"},
{"title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding", "author": "Shuai Wang and Ivona Najdenkoska and Hongyi Zhu and Stevan Rudinac and Monika Kackovic and Nachoem Wijnberg and Marcel Worring", "abstract": "  Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.\n", "link": "http://arxiv.org/abs/2505.06020v1", "date": "2025-05-09", "relevancy": 2.1443, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding&body=Title%3A%20ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding%0AAuthor%3A%20Shuai%20Wang%20and%20Ivona%20Najdenkoska%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Monika%20Kackovic%20and%20Nachoem%20Wijnberg%20and%20Marcel%20Worring%0AAbstract%3A%20%20%20Understanding%20visual%20art%20requires%20reasoning%20across%20multiple%20perspectives%20--%0Acultural%2C%20historical%2C%20and%20stylistic%20--%20beyond%20mere%20object%20recognition.%20While%0Arecent%20multimodal%20large%20language%20models%20%28MLLMs%29%20perform%20well%20on%20general%20image%0Acaptioning%2C%20they%20often%20fail%20to%20capture%20the%20nuanced%20interpretations%20that%20fine%0Aart%20demands.%20We%20propose%20ArtRAG%2C%20a%20novel%2C%20training-free%20framework%20that%20combines%0Astructured%20knowledge%20with%20retrieval-augmented%20generation%20%28RAG%29%20for%0Amulti-perspective%20artwork%20explanation.%20ArtRAG%20automatically%20constructs%20an%20Art%0AContext%20Knowledge%20Graph%20%28ACKG%29%20from%20domain-specific%20textual%20sources%2C%20organizing%0Aentities%20such%20as%20artists%2C%20movements%2C%20themes%2C%20and%20historical%20events%20into%20a%20rich%2C%0Ainterpretable%20graph.%20At%20inference%20time%2C%20a%20multi-granular%20structured%20retriever%0Aselects%20semantically%20and%20topologically%20relevant%20subgraphs%20to%20guide%20generation.%0AThis%20enables%20MLLMs%20to%20produce%20contextually%20grounded%2C%20culturally%20informed%20art%0Adescriptions.%20Experiments%20on%20the%20SemArt%20and%20Artpedia%20datasets%20show%20that%20ArtRAG%0Aoutperforms%20several%20heavily%20trained%20baselines.%20Human%20evaluations%20further%0Aconfirm%20that%20ArtRAG%20generates%20coherent%2C%20insightful%2C%20and%20culturally%20enriched%0Ainterpretations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtRAG%253A%2520Retrieval-Augmented%2520Generation%2520with%2520Structured%2520Context%2520for%250A%2520%2520Visual%2520Art%2520Understanding%26entry.906535625%3DShuai%2520Wang%2520and%2520Ivona%2520Najdenkoska%2520and%2520Hongyi%2520Zhu%2520and%2520Stevan%2520Rudinac%2520and%2520Monika%2520Kackovic%2520and%2520Nachoem%2520Wijnberg%2520and%2520Marcel%2520Worring%26entry.1292438233%3D%2520%2520Understanding%2520visual%2520art%2520requires%2520reasoning%2520across%2520multiple%2520perspectives%2520--%250Acultural%252C%2520historical%252C%2520and%2520stylistic%2520--%2520beyond%2520mere%2520object%2520recognition.%2520While%250Arecent%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520perform%2520well%2520on%2520general%2520image%250Acaptioning%252C%2520they%2520often%2520fail%2520to%2520capture%2520the%2520nuanced%2520interpretations%2520that%2520fine%250Aart%2520demands.%2520We%2520propose%2520ArtRAG%252C%2520a%2520novel%252C%2520training-free%2520framework%2520that%2520combines%250Astructured%2520knowledge%2520with%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520for%250Amulti-perspective%2520artwork%2520explanation.%2520ArtRAG%2520automatically%2520constructs%2520an%2520Art%250AContext%2520Knowledge%2520Graph%2520%2528ACKG%2529%2520from%2520domain-specific%2520textual%2520sources%252C%2520organizing%250Aentities%2520such%2520as%2520artists%252C%2520movements%252C%2520themes%252C%2520and%2520historical%2520events%2520into%2520a%2520rich%252C%250Ainterpretable%2520graph.%2520At%2520inference%2520time%252C%2520a%2520multi-granular%2520structured%2520retriever%250Aselects%2520semantically%2520and%2520topologically%2520relevant%2520subgraphs%2520to%2520guide%2520generation.%250AThis%2520enables%2520MLLMs%2520to%2520produce%2520contextually%2520grounded%252C%2520culturally%2520informed%2520art%250Adescriptions.%2520Experiments%2520on%2520the%2520SemArt%2520and%2520Artpedia%2520datasets%2520show%2520that%2520ArtRAG%250Aoutperforms%2520several%2520heavily%2520trained%2520baselines.%2520Human%2520evaluations%2520further%250Aconfirm%2520that%2520ArtRAG%2520generates%2520coherent%252C%2520insightful%252C%2520and%2520culturally%2520enriched%250Ainterpretations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtRAG%3A%20Retrieval-Augmented%20Generation%20with%20Structured%20Context%20for%0A%20%20Visual%20Art%20Understanding&entry.906535625=Shuai%20Wang%20and%20Ivona%20Najdenkoska%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Monika%20Kackovic%20and%20Nachoem%20Wijnberg%20and%20Marcel%20Worring&entry.1292438233=%20%20Understanding%20visual%20art%20requires%20reasoning%20across%20multiple%20perspectives%20--%0Acultural%2C%20historical%2C%20and%20stylistic%20--%20beyond%20mere%20object%20recognition.%20While%0Arecent%20multimodal%20large%20language%20models%20%28MLLMs%29%20perform%20well%20on%20general%20image%0Acaptioning%2C%20they%20often%20fail%20to%20capture%20the%20nuanced%20interpretations%20that%20fine%0Aart%20demands.%20We%20propose%20ArtRAG%2C%20a%20novel%2C%20training-free%20framework%20that%20combines%0Astructured%20knowledge%20with%20retrieval-augmented%20generation%20%28RAG%29%20for%0Amulti-perspective%20artwork%20explanation.%20ArtRAG%20automatically%20constructs%20an%20Art%0AContext%20Knowledge%20Graph%20%28ACKG%29%20from%20domain-specific%20textual%20sources%2C%20organizing%0Aentities%20such%20as%20artists%2C%20movements%2C%20themes%2C%20and%20historical%20events%20into%20a%20rich%2C%0Ainterpretable%20graph.%20At%20inference%20time%2C%20a%20multi-granular%20structured%20retriever%0Aselects%20semantically%20and%20topologically%20relevant%20subgraphs%20to%20guide%20generation.%0AThis%20enables%20MLLMs%20to%20produce%20contextually%20grounded%2C%20culturally%20informed%20art%0Adescriptions.%20Experiments%20on%20the%20SemArt%20and%20Artpedia%20datasets%20show%20that%20ArtRAG%0Aoutperforms%20several%20heavily%20trained%20baselines.%20Human%20evaluations%20further%0Aconfirm%20that%20ArtRAG%20generates%20coherent%2C%20insightful%2C%20and%20culturally%20enriched%0Ainterpretations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06020v1&entry.124074799=Read"},
{"title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning\n  with Demonstrations", "author": "Shuaiyi Huang and Mara Levy and Anubhav Gupta and Daniel Ekpo and Ruijie Zheng and Abhinav Shrivastava", "abstract": "  Preference feedback collected by human or VLM annotators is often noisy,\npresenting a significant challenge for preference-based reinforcement learning\nthat relies on accurate preference labels. To address this challenge, we\npropose TREND, a novel framework that integrates few-shot expert demonstrations\nwith a tri-teaching strategy for effective noise mitigation. Our method trains\nthree reward models simultaneously, where each model views its small-loss\npreference pairs as useful knowledge and teaches such useful pairs to its peer\nnetwork for updating the parameters. Remarkably, our approach requires as few\nas one to three expert demonstrations to achieve high performance. We evaluate\nTREND on various robotic manipulation tasks, achieving up to 90% success rates\neven with noise levels as high as 40%, highlighting its effective robustness in\nhandling noisy preference feedback. Project page:\nhttps://shuaiyihuang.github.io/publications/TREND.\n", "link": "http://arxiv.org/abs/2505.06079v1", "date": "2025-05-09", "relevancy": 2.139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5424}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TREND%3A%20Tri-teaching%20for%20Robust%20Preference-based%20Reinforcement%20Learning%0A%20%20with%20Demonstrations&body=Title%3A%20TREND%3A%20Tri-teaching%20for%20Robust%20Preference-based%20Reinforcement%20Learning%0A%20%20with%20Demonstrations%0AAuthor%3A%20Shuaiyi%20Huang%20and%20Mara%20Levy%20and%20Anubhav%20Gupta%20and%20Daniel%20Ekpo%20and%20Ruijie%20Zheng%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Preference%20feedback%20collected%20by%20human%20or%20VLM%20annotators%20is%20often%20noisy%2C%0Apresenting%20a%20significant%20challenge%20for%20preference-based%20reinforcement%20learning%0Athat%20relies%20on%20accurate%20preference%20labels.%20To%20address%20this%20challenge%2C%20we%0Apropose%20TREND%2C%20a%20novel%20framework%20that%20integrates%20few-shot%20expert%20demonstrations%0Awith%20a%20tri-teaching%20strategy%20for%20effective%20noise%20mitigation.%20Our%20method%20trains%0Athree%20reward%20models%20simultaneously%2C%20where%20each%20model%20views%20its%20small-loss%0Apreference%20pairs%20as%20useful%20knowledge%20and%20teaches%20such%20useful%20pairs%20to%20its%20peer%0Anetwork%20for%20updating%20the%20parameters.%20Remarkably%2C%20our%20approach%20requires%20as%20few%0Aas%20one%20to%20three%20expert%20demonstrations%20to%20achieve%20high%20performance.%20We%20evaluate%0ATREND%20on%20various%20robotic%20manipulation%20tasks%2C%20achieving%20up%20to%2090%25%20success%20rates%0Aeven%20with%20noise%20levels%20as%20high%20as%2040%25%2C%20highlighting%20its%20effective%20robustness%20in%0Ahandling%20noisy%20preference%20feedback.%20Project%20page%3A%0Ahttps%3A//shuaiyihuang.github.io/publications/TREND.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTREND%253A%2520Tri-teaching%2520for%2520Robust%2520Preference-based%2520Reinforcement%2520Learning%250A%2520%2520with%2520Demonstrations%26entry.906535625%3DShuaiyi%2520Huang%2520and%2520Mara%2520Levy%2520and%2520Anubhav%2520Gupta%2520and%2520Daniel%2520Ekpo%2520and%2520Ruijie%2520Zheng%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Preference%2520feedback%2520collected%2520by%2520human%2520or%2520VLM%2520annotators%2520is%2520often%2520noisy%252C%250Apresenting%2520a%2520significant%2520challenge%2520for%2520preference-based%2520reinforcement%2520learning%250Athat%2520relies%2520on%2520accurate%2520preference%2520labels.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520TREND%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520few-shot%2520expert%2520demonstrations%250Awith%2520a%2520tri-teaching%2520strategy%2520for%2520effective%2520noise%2520mitigation.%2520Our%2520method%2520trains%250Athree%2520reward%2520models%2520simultaneously%252C%2520where%2520each%2520model%2520views%2520its%2520small-loss%250Apreference%2520pairs%2520as%2520useful%2520knowledge%2520and%2520teaches%2520such%2520useful%2520pairs%2520to%2520its%2520peer%250Anetwork%2520for%2520updating%2520the%2520parameters.%2520Remarkably%252C%2520our%2520approach%2520requires%2520as%2520few%250Aas%2520one%2520to%2520three%2520expert%2520demonstrations%2520to%2520achieve%2520high%2520performance.%2520We%2520evaluate%250ATREND%2520on%2520various%2520robotic%2520manipulation%2520tasks%252C%2520achieving%2520up%2520to%252090%2525%2520success%2520rates%250Aeven%2520with%2520noise%2520levels%2520as%2520high%2520as%252040%2525%252C%2520highlighting%2520its%2520effective%2520robustness%2520in%250Ahandling%2520noisy%2520preference%2520feedback.%2520Project%2520page%253A%250Ahttps%253A//shuaiyihuang.github.io/publications/TREND.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TREND%3A%20Tri-teaching%20for%20Robust%20Preference-based%20Reinforcement%20Learning%0A%20%20with%20Demonstrations&entry.906535625=Shuaiyi%20Huang%20and%20Mara%20Levy%20and%20Anubhav%20Gupta%20and%20Daniel%20Ekpo%20and%20Ruijie%20Zheng%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Preference%20feedback%20collected%20by%20human%20or%20VLM%20annotators%20is%20often%20noisy%2C%0Apresenting%20a%20significant%20challenge%20for%20preference-based%20reinforcement%20learning%0Athat%20relies%20on%20accurate%20preference%20labels.%20To%20address%20this%20challenge%2C%20we%0Apropose%20TREND%2C%20a%20novel%20framework%20that%20integrates%20few-shot%20expert%20demonstrations%0Awith%20a%20tri-teaching%20strategy%20for%20effective%20noise%20mitigation.%20Our%20method%20trains%0Athree%20reward%20models%20simultaneously%2C%20where%20each%20model%20views%20its%20small-loss%0Apreference%20pairs%20as%20useful%20knowledge%20and%20teaches%20such%20useful%20pairs%20to%20its%20peer%0Anetwork%20for%20updating%20the%20parameters.%20Remarkably%2C%20our%20approach%20requires%20as%20few%0Aas%20one%20to%20three%20expert%20demonstrations%20to%20achieve%20high%20performance.%20We%20evaluate%0ATREND%20on%20various%20robotic%20manipulation%20tasks%2C%20achieving%20up%20to%2090%25%20success%20rates%0Aeven%20with%20noise%20levels%20as%20high%20as%2040%25%2C%20highlighting%20its%20effective%20robustness%20in%0Ahandling%20noisy%20preference%20feedback.%20Project%20page%3A%0Ahttps%3A//shuaiyihuang.github.io/publications/TREND.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06079v1&entry.124074799=Read"},
{"title": "NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh\n  Generation", "author": "Qiujie Dong and Huibiao Wen and Rui Xu and Shuangmin Chen and Jiaran Zhou and Shiqing Xin and Changhe Tu and Taku Komura and Wenping Wang", "abstract": "  Quadrilateral mesh generation plays a crucial role in numerical simulations\nwithin Computer-Aided Design and Engineering (CAD/E). Producing high-quality\nquadrangulation typically requires satisfying four key criteria. First, the\nquadrilateral mesh should closely align with principal curvature directions.\nSecond, singular points should be strategically placed and effectively\nminimized. Third, the mesh should accurately conform to sharp feature edges.\nLastly, quadrangulation results should exhibit robustness against noise and\nminor geometric variations. Existing methods generally involve first computing\na regular cross field to represent quad element orientations across the\nsurface, followed by extracting a quadrilateral mesh aligned closely with this\ncross field. A primary challenge with this approach is balancing the smoothness\nof the cross field with its alignment to pre-computed principal curvature\ndirections, which are sensitive to small surface perturbations and often\nill-defined in spherical or planar regions.\n  To tackle this challenge, we propose NeurCross, a novel framework that\nsimultaneously optimizes a cross field and a neural signed distance function\n(SDF), whose zero-level set serves as a proxy of the input shape. Our joint\noptimization is guided by three factors: faithful approximation of the\noptimized SDF surface to the input surface, alignment between the cross field\nand the principal curvature field derived from the SDF surface, and smoothness\nof the cross field. Acting as an intermediary, the neural SDF contributes in\ntwo essential ways. First, it provides an alternative, optimizable base surface\nexhibiting more regular principal curvature directions for guiding the cross\nfield. Second, we leverage the Hessian matrix of the neural SDF to implicitly\nenforce cross field alignment with principal curvature directions...\n", "link": "http://arxiv.org/abs/2405.13745v3", "date": "2025-05-09", "relevancy": 2.1386, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5681}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5132}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeurCross%3A%20A%20Neural%20Approach%20to%20Computing%20Cross%20Fields%20for%20Quad%20Mesh%0A%20%20Generation&body=Title%3A%20NeurCross%3A%20A%20Neural%20Approach%20to%20Computing%20Cross%20Fields%20for%20Quad%20Mesh%0A%20%20Generation%0AAuthor%3A%20Qiujie%20Dong%20and%20Huibiao%20Wen%20and%20Rui%20Xu%20and%20Shuangmin%20Chen%20and%20Jiaran%20Zhou%20and%20Shiqing%20Xin%20and%20Changhe%20Tu%20and%20Taku%20Komura%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20Quadrilateral%20mesh%20generation%20plays%20a%20crucial%20role%20in%20numerical%20simulations%0Awithin%20Computer-Aided%20Design%20and%20Engineering%20%28CAD/E%29.%20Producing%20high-quality%0Aquadrangulation%20typically%20requires%20satisfying%20four%20key%20criteria.%20First%2C%20the%0Aquadrilateral%20mesh%20should%20closely%20align%20with%20principal%20curvature%20directions.%0ASecond%2C%20singular%20points%20should%20be%20strategically%20placed%20and%20effectively%0Aminimized.%20Third%2C%20the%20mesh%20should%20accurately%20conform%20to%20sharp%20feature%20edges.%0ALastly%2C%20quadrangulation%20results%20should%20exhibit%20robustness%20against%20noise%20and%0Aminor%20geometric%20variations.%20Existing%20methods%20generally%20involve%20first%20computing%0Aa%20regular%20cross%20field%20to%20represent%20quad%20element%20orientations%20across%20the%0Asurface%2C%20followed%20by%20extracting%20a%20quadrilateral%20mesh%20aligned%20closely%20with%20this%0Across%20field.%20A%20primary%20challenge%20with%20this%20approach%20is%20balancing%20the%20smoothness%0Aof%20the%20cross%20field%20with%20its%20alignment%20to%20pre-computed%20principal%20curvature%0Adirections%2C%20which%20are%20sensitive%20to%20small%20surface%20perturbations%20and%20often%0Aill-defined%20in%20spherical%20or%20planar%20regions.%0A%20%20To%20tackle%20this%20challenge%2C%20we%20propose%20NeurCross%2C%20a%20novel%20framework%20that%0Asimultaneously%20optimizes%20a%20cross%20field%20and%20a%20neural%20signed%20distance%20function%0A%28SDF%29%2C%20whose%20zero-level%20set%20serves%20as%20a%20proxy%20of%20the%20input%20shape.%20Our%20joint%0Aoptimization%20is%20guided%20by%20three%20factors%3A%20faithful%20approximation%20of%20the%0Aoptimized%20SDF%20surface%20to%20the%20input%20surface%2C%20alignment%20between%20the%20cross%20field%0Aand%20the%20principal%20curvature%20field%20derived%20from%20the%20SDF%20surface%2C%20and%20smoothness%0Aof%20the%20cross%20field.%20Acting%20as%20an%20intermediary%2C%20the%20neural%20SDF%20contributes%20in%0Atwo%20essential%20ways.%20First%2C%20it%20provides%20an%20alternative%2C%20optimizable%20base%20surface%0Aexhibiting%20more%20regular%20principal%20curvature%20directions%20for%20guiding%20the%20cross%0Afield.%20Second%2C%20we%20leverage%20the%20Hessian%20matrix%20of%20the%20neural%20SDF%20to%20implicitly%0Aenforce%20cross%20field%20alignment%20with%20principal%20curvature%20directions...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurCross%253A%2520A%2520Neural%2520Approach%2520to%2520Computing%2520Cross%2520Fields%2520for%2520Quad%2520Mesh%250A%2520%2520Generation%26entry.906535625%3DQiujie%2520Dong%2520and%2520Huibiao%2520Wen%2520and%2520Rui%2520Xu%2520and%2520Shuangmin%2520Chen%2520and%2520Jiaran%2520Zhou%2520and%2520Shiqing%2520Xin%2520and%2520Changhe%2520Tu%2520and%2520Taku%2520Komura%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520Quadrilateral%2520mesh%2520generation%2520plays%2520a%2520crucial%2520role%2520in%2520numerical%2520simulations%250Awithin%2520Computer-Aided%2520Design%2520and%2520Engineering%2520%2528CAD/E%2529.%2520Producing%2520high-quality%250Aquadrangulation%2520typically%2520requires%2520satisfying%2520four%2520key%2520criteria.%2520First%252C%2520the%250Aquadrilateral%2520mesh%2520should%2520closely%2520align%2520with%2520principal%2520curvature%2520directions.%250ASecond%252C%2520singular%2520points%2520should%2520be%2520strategically%2520placed%2520and%2520effectively%250Aminimized.%2520Third%252C%2520the%2520mesh%2520should%2520accurately%2520conform%2520to%2520sharp%2520feature%2520edges.%250ALastly%252C%2520quadrangulation%2520results%2520should%2520exhibit%2520robustness%2520against%2520noise%2520and%250Aminor%2520geometric%2520variations.%2520Existing%2520methods%2520generally%2520involve%2520first%2520computing%250Aa%2520regular%2520cross%2520field%2520to%2520represent%2520quad%2520element%2520orientations%2520across%2520the%250Asurface%252C%2520followed%2520by%2520extracting%2520a%2520quadrilateral%2520mesh%2520aligned%2520closely%2520with%2520this%250Across%2520field.%2520A%2520primary%2520challenge%2520with%2520this%2520approach%2520is%2520balancing%2520the%2520smoothness%250Aof%2520the%2520cross%2520field%2520with%2520its%2520alignment%2520to%2520pre-computed%2520principal%2520curvature%250Adirections%252C%2520which%2520are%2520sensitive%2520to%2520small%2520surface%2520perturbations%2520and%2520often%250Aill-defined%2520in%2520spherical%2520or%2520planar%2520regions.%250A%2520%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520NeurCross%252C%2520a%2520novel%2520framework%2520that%250Asimultaneously%2520optimizes%2520a%2520cross%2520field%2520and%2520a%2520neural%2520signed%2520distance%2520function%250A%2528SDF%2529%252C%2520whose%2520zero-level%2520set%2520serves%2520as%2520a%2520proxy%2520of%2520the%2520input%2520shape.%2520Our%2520joint%250Aoptimization%2520is%2520guided%2520by%2520three%2520factors%253A%2520faithful%2520approximation%2520of%2520the%250Aoptimized%2520SDF%2520surface%2520to%2520the%2520input%2520surface%252C%2520alignment%2520between%2520the%2520cross%2520field%250Aand%2520the%2520principal%2520curvature%2520field%2520derived%2520from%2520the%2520SDF%2520surface%252C%2520and%2520smoothness%250Aof%2520the%2520cross%2520field.%2520Acting%2520as%2520an%2520intermediary%252C%2520the%2520neural%2520SDF%2520contributes%2520in%250Atwo%2520essential%2520ways.%2520First%252C%2520it%2520provides%2520an%2520alternative%252C%2520optimizable%2520base%2520surface%250Aexhibiting%2520more%2520regular%2520principal%2520curvature%2520directions%2520for%2520guiding%2520the%2520cross%250Afield.%2520Second%252C%2520we%2520leverage%2520the%2520Hessian%2520matrix%2520of%2520the%2520neural%2520SDF%2520to%2520implicitly%250Aenforce%2520cross%2520field%2520alignment%2520with%2520principal%2520curvature%2520directions...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeurCross%3A%20A%20Neural%20Approach%20to%20Computing%20Cross%20Fields%20for%20Quad%20Mesh%0A%20%20Generation&entry.906535625=Qiujie%20Dong%20and%20Huibiao%20Wen%20and%20Rui%20Xu%20and%20Shuangmin%20Chen%20and%20Jiaran%20Zhou%20and%20Shiqing%20Xin%20and%20Changhe%20Tu%20and%20Taku%20Komura%20and%20Wenping%20Wang&entry.1292438233=%20%20Quadrilateral%20mesh%20generation%20plays%20a%20crucial%20role%20in%20numerical%20simulations%0Awithin%20Computer-Aided%20Design%20and%20Engineering%20%28CAD/E%29.%20Producing%20high-quality%0Aquadrangulation%20typically%20requires%20satisfying%20four%20key%20criteria.%20First%2C%20the%0Aquadrilateral%20mesh%20should%20closely%20align%20with%20principal%20curvature%20directions.%0ASecond%2C%20singular%20points%20should%20be%20strategically%20placed%20and%20effectively%0Aminimized.%20Third%2C%20the%20mesh%20should%20accurately%20conform%20to%20sharp%20feature%20edges.%0ALastly%2C%20quadrangulation%20results%20should%20exhibit%20robustness%20against%20noise%20and%0Aminor%20geometric%20variations.%20Existing%20methods%20generally%20involve%20first%20computing%0Aa%20regular%20cross%20field%20to%20represent%20quad%20element%20orientations%20across%20the%0Asurface%2C%20followed%20by%20extracting%20a%20quadrilateral%20mesh%20aligned%20closely%20with%20this%0Across%20field.%20A%20primary%20challenge%20with%20this%20approach%20is%20balancing%20the%20smoothness%0Aof%20the%20cross%20field%20with%20its%20alignment%20to%20pre-computed%20principal%20curvature%0Adirections%2C%20which%20are%20sensitive%20to%20small%20surface%20perturbations%20and%20often%0Aill-defined%20in%20spherical%20or%20planar%20regions.%0A%20%20To%20tackle%20this%20challenge%2C%20we%20propose%20NeurCross%2C%20a%20novel%20framework%20that%0Asimultaneously%20optimizes%20a%20cross%20field%20and%20a%20neural%20signed%20distance%20function%0A%28SDF%29%2C%20whose%20zero-level%20set%20serves%20as%20a%20proxy%20of%20the%20input%20shape.%20Our%20joint%0Aoptimization%20is%20guided%20by%20three%20factors%3A%20faithful%20approximation%20of%20the%0Aoptimized%20SDF%20surface%20to%20the%20input%20surface%2C%20alignment%20between%20the%20cross%20field%0Aand%20the%20principal%20curvature%20field%20derived%20from%20the%20SDF%20surface%2C%20and%20smoothness%0Aof%20the%20cross%20field.%20Acting%20as%20an%20intermediary%2C%20the%20neural%20SDF%20contributes%20in%0Atwo%20essential%20ways.%20First%2C%20it%20provides%20an%20alternative%2C%20optimizable%20base%20surface%0Aexhibiting%20more%20regular%20principal%20curvature%20directions%20for%20guiding%20the%20cross%0Afield.%20Second%2C%20we%20leverage%20the%20Hessian%20matrix%20of%20the%20neural%20SDF%20to%20implicitly%0Aenforce%20cross%20field%20alignment%20with%20principal%20curvature%20directions...%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13745v3&entry.124074799=Read"},
{"title": "BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion\n  Segmentation", "author": "Hongming Wang and Yifeng Wu and Huimin Huang and Hongtao Wu and Jia-Xuan Jiang and Xiaodong Zhang and Hao Zheng and Xian Wu and Yefeng Zheng and Jinping Xu and Jing Cheng", "abstract": "  The segmentation of substantial brain lesions is a significant and\nchallenging task in the field of medical image segmentation. Substantial brain\nlesions in brain imaging exhibit high heterogeneity, with indistinct boundaries\nbetween lesion regions and normal brain tissue. Small lesions in single slices\nare difficult to identify, making the accurate and reproducible segmentation of\nabnormal regions, as well as their feature description, highly complex.\nExisting methods have the following limitations: 1) They rely solely on\nsingle-modal information for learning, neglecting the multi-modal information\ncommonly used in diagnosis. This hampers the ability to comprehensively acquire\nbrain lesion information from multiple perspectives and prevents the effective\nintegration and utilization of multi-modal data inputs, thereby limiting a\nholistic understanding of lesions. 2) They are constrained by the amount of\ndata available, leading to low sensitivity to small lesions and difficulty in\ndetecting subtle pathological changes. 3) Current SAM-based models rely on\nexternal prompts, which cannot achieve automatic segmentation and, to some\nextent, affect diagnostic efficiency.To address these issues, we have developed\na large-scale fully automated segmentation model specifically designed for\nbrain lesion segmentation, named BrainSegDMLF. This model has the following\nfeatures: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and\nintegrates multi-modal data during the encoding process, providing the SAM\nencoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling\nDecoder, enabling the model to extract rich low-level and high-level features\neven with limited data, thereby detecting the presence of small lesions. 3)\nAutomatic segmentation masks, allowing the model to generate lesion masks\nautomatically without requiring manual prompts.\n", "link": "http://arxiv.org/abs/2505.06133v1", "date": "2025-05-09", "relevancy": 2.13, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainSegDMlF%3A%20A%20Dynamic%20Fusion-enhanced%20SAM%20for%20Brain%20Lesion%0A%20%20Segmentation&body=Title%3A%20BrainSegDMlF%3A%20A%20Dynamic%20Fusion-enhanced%20SAM%20for%20Brain%20Lesion%0A%20%20Segmentation%0AAuthor%3A%20Hongming%20Wang%20and%20Yifeng%20Wu%20and%20Huimin%20Huang%20and%20Hongtao%20Wu%20and%20Jia-Xuan%20Jiang%20and%20Xiaodong%20Zhang%20and%20Hao%20Zheng%20and%20Xian%20Wu%20and%20Yefeng%20Zheng%20and%20Jinping%20Xu%20and%20Jing%20Cheng%0AAbstract%3A%20%20%20The%20segmentation%20of%20substantial%20brain%20lesions%20is%20a%20significant%20and%0Achallenging%20task%20in%20the%20field%20of%20medical%20image%20segmentation.%20Substantial%20brain%0Alesions%20in%20brain%20imaging%20exhibit%20high%20heterogeneity%2C%20with%20indistinct%20boundaries%0Abetween%20lesion%20regions%20and%20normal%20brain%20tissue.%20Small%20lesions%20in%20single%20slices%0Aare%20difficult%20to%20identify%2C%20making%20the%20accurate%20and%20reproducible%20segmentation%20of%0Aabnormal%20regions%2C%20as%20well%20as%20their%20feature%20description%2C%20highly%20complex.%0AExisting%20methods%20have%20the%20following%20limitations%3A%201%29%20They%20rely%20solely%20on%0Asingle-modal%20information%20for%20learning%2C%20neglecting%20the%20multi-modal%20information%0Acommonly%20used%20in%20diagnosis.%20This%20hampers%20the%20ability%20to%20comprehensively%20acquire%0Abrain%20lesion%20information%20from%20multiple%20perspectives%20and%20prevents%20the%20effective%0Aintegration%20and%20utilization%20of%20multi-modal%20data%20inputs%2C%20thereby%20limiting%20a%0Aholistic%20understanding%20of%20lesions.%202%29%20They%20are%20constrained%20by%20the%20amount%20of%0Adata%20available%2C%20leading%20to%20low%20sensitivity%20to%20small%20lesions%20and%20difficulty%20in%0Adetecting%20subtle%20pathological%20changes.%203%29%20Current%20SAM-based%20models%20rely%20on%0Aexternal%20prompts%2C%20which%20cannot%20achieve%20automatic%20segmentation%20and%2C%20to%20some%0Aextent%2C%20affect%20diagnostic%20efficiency.To%20address%20these%20issues%2C%20we%20have%20developed%0Aa%20large-scale%20fully%20automated%20segmentation%20model%20specifically%20designed%20for%0Abrain%20lesion%20segmentation%2C%20named%20BrainSegDMLF.%20This%20model%20has%20the%20following%0Afeatures%3A%201%29%20Dynamic%20Modal%20Interactive%20Fusion%20%28DMIF%29%20module%20that%20processes%20and%0Aintegrates%20multi-modal%20data%20during%20the%20encoding%20process%2C%20providing%20the%20SAM%0Aencoder%20with%20more%20comprehensive%20modal%20information.%202%29%20Layer-by-Layer%20Upsampling%0ADecoder%2C%20enabling%20the%20model%20to%20extract%20rich%20low-level%20and%20high-level%20features%0Aeven%20with%20limited%20data%2C%20thereby%20detecting%20the%20presence%20of%20small%20lesions.%203%29%0AAutomatic%20segmentation%20masks%2C%20allowing%20the%20model%20to%20generate%20lesion%20masks%0Aautomatically%20without%20requiring%20manual%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainSegDMlF%253A%2520A%2520Dynamic%2520Fusion-enhanced%2520SAM%2520for%2520Brain%2520Lesion%250A%2520%2520Segmentation%26entry.906535625%3DHongming%2520Wang%2520and%2520Yifeng%2520Wu%2520and%2520Huimin%2520Huang%2520and%2520Hongtao%2520Wu%2520and%2520Jia-Xuan%2520Jiang%2520and%2520Xiaodong%2520Zhang%2520and%2520Hao%2520Zheng%2520and%2520Xian%2520Wu%2520and%2520Yefeng%2520Zheng%2520and%2520Jinping%2520Xu%2520and%2520Jing%2520Cheng%26entry.1292438233%3D%2520%2520The%2520segmentation%2520of%2520substantial%2520brain%2520lesions%2520is%2520a%2520significant%2520and%250Achallenging%2520task%2520in%2520the%2520field%2520of%2520medical%2520image%2520segmentation.%2520Substantial%2520brain%250Alesions%2520in%2520brain%2520imaging%2520exhibit%2520high%2520heterogeneity%252C%2520with%2520indistinct%2520boundaries%250Abetween%2520lesion%2520regions%2520and%2520normal%2520brain%2520tissue.%2520Small%2520lesions%2520in%2520single%2520slices%250Aare%2520difficult%2520to%2520identify%252C%2520making%2520the%2520accurate%2520and%2520reproducible%2520segmentation%2520of%250Aabnormal%2520regions%252C%2520as%2520well%2520as%2520their%2520feature%2520description%252C%2520highly%2520complex.%250AExisting%2520methods%2520have%2520the%2520following%2520limitations%253A%25201%2529%2520They%2520rely%2520solely%2520on%250Asingle-modal%2520information%2520for%2520learning%252C%2520neglecting%2520the%2520multi-modal%2520information%250Acommonly%2520used%2520in%2520diagnosis.%2520This%2520hampers%2520the%2520ability%2520to%2520comprehensively%2520acquire%250Abrain%2520lesion%2520information%2520from%2520multiple%2520perspectives%2520and%2520prevents%2520the%2520effective%250Aintegration%2520and%2520utilization%2520of%2520multi-modal%2520data%2520inputs%252C%2520thereby%2520limiting%2520a%250Aholistic%2520understanding%2520of%2520lesions.%25202%2529%2520They%2520are%2520constrained%2520by%2520the%2520amount%2520of%250Adata%2520available%252C%2520leading%2520to%2520low%2520sensitivity%2520to%2520small%2520lesions%2520and%2520difficulty%2520in%250Adetecting%2520subtle%2520pathological%2520changes.%25203%2529%2520Current%2520SAM-based%2520models%2520rely%2520on%250Aexternal%2520prompts%252C%2520which%2520cannot%2520achieve%2520automatic%2520segmentation%2520and%252C%2520to%2520some%250Aextent%252C%2520affect%2520diagnostic%2520efficiency.To%2520address%2520these%2520issues%252C%2520we%2520have%2520developed%250Aa%2520large-scale%2520fully%2520automated%2520segmentation%2520model%2520specifically%2520designed%2520for%250Abrain%2520lesion%2520segmentation%252C%2520named%2520BrainSegDMLF.%2520This%2520model%2520has%2520the%2520following%250Afeatures%253A%25201%2529%2520Dynamic%2520Modal%2520Interactive%2520Fusion%2520%2528DMIF%2529%2520module%2520that%2520processes%2520and%250Aintegrates%2520multi-modal%2520data%2520during%2520the%2520encoding%2520process%252C%2520providing%2520the%2520SAM%250Aencoder%2520with%2520more%2520comprehensive%2520modal%2520information.%25202%2529%2520Layer-by-Layer%2520Upsampling%250ADecoder%252C%2520enabling%2520the%2520model%2520to%2520extract%2520rich%2520low-level%2520and%2520high-level%2520features%250Aeven%2520with%2520limited%2520data%252C%2520thereby%2520detecting%2520the%2520presence%2520of%2520small%2520lesions.%25203%2529%250AAutomatic%2520segmentation%2520masks%252C%2520allowing%2520the%2520model%2520to%2520generate%2520lesion%2520masks%250Aautomatically%2520without%2520requiring%2520manual%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainSegDMlF%3A%20A%20Dynamic%20Fusion-enhanced%20SAM%20for%20Brain%20Lesion%0A%20%20Segmentation&entry.906535625=Hongming%20Wang%20and%20Yifeng%20Wu%20and%20Huimin%20Huang%20and%20Hongtao%20Wu%20and%20Jia-Xuan%20Jiang%20and%20Xiaodong%20Zhang%20and%20Hao%20Zheng%20and%20Xian%20Wu%20and%20Yefeng%20Zheng%20and%20Jinping%20Xu%20and%20Jing%20Cheng&entry.1292438233=%20%20The%20segmentation%20of%20substantial%20brain%20lesions%20is%20a%20significant%20and%0Achallenging%20task%20in%20the%20field%20of%20medical%20image%20segmentation.%20Substantial%20brain%0Alesions%20in%20brain%20imaging%20exhibit%20high%20heterogeneity%2C%20with%20indistinct%20boundaries%0Abetween%20lesion%20regions%20and%20normal%20brain%20tissue.%20Small%20lesions%20in%20single%20slices%0Aare%20difficult%20to%20identify%2C%20making%20the%20accurate%20and%20reproducible%20segmentation%20of%0Aabnormal%20regions%2C%20as%20well%20as%20their%20feature%20description%2C%20highly%20complex.%0AExisting%20methods%20have%20the%20following%20limitations%3A%201%29%20They%20rely%20solely%20on%0Asingle-modal%20information%20for%20learning%2C%20neglecting%20the%20multi-modal%20information%0Acommonly%20used%20in%20diagnosis.%20This%20hampers%20the%20ability%20to%20comprehensively%20acquire%0Abrain%20lesion%20information%20from%20multiple%20perspectives%20and%20prevents%20the%20effective%0Aintegration%20and%20utilization%20of%20multi-modal%20data%20inputs%2C%20thereby%20limiting%20a%0Aholistic%20understanding%20of%20lesions.%202%29%20They%20are%20constrained%20by%20the%20amount%20of%0Adata%20available%2C%20leading%20to%20low%20sensitivity%20to%20small%20lesions%20and%20difficulty%20in%0Adetecting%20subtle%20pathological%20changes.%203%29%20Current%20SAM-based%20models%20rely%20on%0Aexternal%20prompts%2C%20which%20cannot%20achieve%20automatic%20segmentation%20and%2C%20to%20some%0Aextent%2C%20affect%20diagnostic%20efficiency.To%20address%20these%20issues%2C%20we%20have%20developed%0Aa%20large-scale%20fully%20automated%20segmentation%20model%20specifically%20designed%20for%0Abrain%20lesion%20segmentation%2C%20named%20BrainSegDMLF.%20This%20model%20has%20the%20following%0Afeatures%3A%201%29%20Dynamic%20Modal%20Interactive%20Fusion%20%28DMIF%29%20module%20that%20processes%20and%0Aintegrates%20multi-modal%20data%20during%20the%20encoding%20process%2C%20providing%20the%20SAM%0Aencoder%20with%20more%20comprehensive%20modal%20information.%202%29%20Layer-by-Layer%20Upsampling%0ADecoder%2C%20enabling%20the%20model%20to%20extract%20rich%20low-level%20and%20high-level%20features%0Aeven%20with%20limited%20data%2C%20thereby%20detecting%20the%20presence%20of%20small%20lesions.%203%29%0AAutomatic%20segmentation%20masks%2C%20allowing%20the%20model%20to%20generate%20lesion%20masks%0Aautomatically%20without%20requiring%20manual%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06133v1&entry.124074799=Read"},
{"title": "AnySat: One Earth Observation Model for Many Resolutions, Scales, and\n  Modalities", "author": "Guillaume Astruc and Nicolas Gonthier and Clement Mallet and Loic Landrieu", "abstract": "  Geospatial models must adapt to the diversity of Earth observation data in\nterms of resolutions, scales, and modalities. However, existing approaches\nexpect fixed input configurations, which limits their practical applicability.\nWe propose AnySat, a multimodal model based on joint embedding predictive\narchitecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a\nsingle model on highly heterogeneous data in a self-supervised manner. To\ndemonstrate the advantages of this unified approach, we compile GeoPlex, a\ncollection of 5 multimodal datasets with varying characteristics and $11$\ndistinct sensors. We then train a single powerful model on these diverse\ndatasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art\nresults on the test sets of GeoPlex and for 6 external datasets across various\nenvironment monitoring tasks: land cover mapping, tree species identification,\ncrop type classification, change detection, climate type classification, and\nsegmentation of flood, burn scar, and deforestation. The code and models are\navailable at https://github.com/gastruc/AnySat.\n", "link": "http://arxiv.org/abs/2412.14123v3", "date": "2025-05-09", "relevancy": 2.1159, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnySat%3A%20One%20Earth%20Observation%20Model%20for%20Many%20Resolutions%2C%20Scales%2C%20and%0A%20%20Modalities&body=Title%3A%20AnySat%3A%20One%20Earth%20Observation%20Model%20for%20Many%20Resolutions%2C%20Scales%2C%20and%0A%20%20Modalities%0AAuthor%3A%20Guillaume%20Astruc%20and%20Nicolas%20Gonthier%20and%20Clement%20Mallet%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20Geospatial%20models%20must%20adapt%20to%20the%20diversity%20of%20Earth%20observation%20data%20in%0Aterms%20of%20resolutions%2C%20scales%2C%20and%20modalities.%20However%2C%20existing%20approaches%0Aexpect%20fixed%20input%20configurations%2C%20which%20limits%20their%20practical%20applicability.%0AWe%20propose%20AnySat%2C%20a%20multimodal%20model%20based%20on%20joint%20embedding%20predictive%0Aarchitecture%20%28JEPA%29%20and%20scale-adaptive%20spatial%20encoders%2C%20allowing%20us%20to%20train%20a%0Asingle%20model%20on%20highly%20heterogeneous%20data%20in%20a%20self-supervised%20manner.%20To%0Ademonstrate%20the%20advantages%20of%20this%20unified%20approach%2C%20we%20compile%20GeoPlex%2C%20a%0Acollection%20of%205%20multimodal%20datasets%20with%20varying%20characteristics%20and%20%2411%24%0Adistinct%20sensors.%20We%20then%20train%20a%20single%20powerful%20model%20on%20these%20diverse%0Adatasets%20simultaneously.%20Once%20fine-tuned%20or%20probed%2C%20we%20reach%20state-of-the-art%0Aresults%20on%20the%20test%20sets%20of%20GeoPlex%20and%20for%206%20external%20datasets%20across%20various%0Aenvironment%20monitoring%20tasks%3A%20land%20cover%20mapping%2C%20tree%20species%20identification%2C%0Acrop%20type%20classification%2C%20change%20detection%2C%20climate%20type%20classification%2C%20and%0Asegmentation%20of%20flood%2C%20burn%20scar%2C%20and%20deforestation.%20The%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/gastruc/AnySat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14123v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnySat%253A%2520One%2520Earth%2520Observation%2520Model%2520for%2520Many%2520Resolutions%252C%2520Scales%252C%2520and%250A%2520%2520Modalities%26entry.906535625%3DGuillaume%2520Astruc%2520and%2520Nicolas%2520Gonthier%2520and%2520Clement%2520Mallet%2520and%2520Loic%2520Landrieu%26entry.1292438233%3D%2520%2520Geospatial%2520models%2520must%2520adapt%2520to%2520the%2520diversity%2520of%2520Earth%2520observation%2520data%2520in%250Aterms%2520of%2520resolutions%252C%2520scales%252C%2520and%2520modalities.%2520However%252C%2520existing%2520approaches%250Aexpect%2520fixed%2520input%2520configurations%252C%2520which%2520limits%2520their%2520practical%2520applicability.%250AWe%2520propose%2520AnySat%252C%2520a%2520multimodal%2520model%2520based%2520on%2520joint%2520embedding%2520predictive%250Aarchitecture%2520%2528JEPA%2529%2520and%2520scale-adaptive%2520spatial%2520encoders%252C%2520allowing%2520us%2520to%2520train%2520a%250Asingle%2520model%2520on%2520highly%2520heterogeneous%2520data%2520in%2520a%2520self-supervised%2520manner.%2520To%250Ademonstrate%2520the%2520advantages%2520of%2520this%2520unified%2520approach%252C%2520we%2520compile%2520GeoPlex%252C%2520a%250Acollection%2520of%25205%2520multimodal%2520datasets%2520with%2520varying%2520characteristics%2520and%2520%252411%2524%250Adistinct%2520sensors.%2520We%2520then%2520train%2520a%2520single%2520powerful%2520model%2520on%2520these%2520diverse%250Adatasets%2520simultaneously.%2520Once%2520fine-tuned%2520or%2520probed%252C%2520we%2520reach%2520state-of-the-art%250Aresults%2520on%2520the%2520test%2520sets%2520of%2520GeoPlex%2520and%2520for%25206%2520external%2520datasets%2520across%2520various%250Aenvironment%2520monitoring%2520tasks%253A%2520land%2520cover%2520mapping%252C%2520tree%2520species%2520identification%252C%250Acrop%2520type%2520classification%252C%2520change%2520detection%252C%2520climate%2520type%2520classification%252C%2520and%250Asegmentation%2520of%2520flood%252C%2520burn%2520scar%252C%2520and%2520deforestation.%2520The%2520code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/gastruc/AnySat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14123v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnySat%3A%20One%20Earth%20Observation%20Model%20for%20Many%20Resolutions%2C%20Scales%2C%20and%0A%20%20Modalities&entry.906535625=Guillaume%20Astruc%20and%20Nicolas%20Gonthier%20and%20Clement%20Mallet%20and%20Loic%20Landrieu&entry.1292438233=%20%20Geospatial%20models%20must%20adapt%20to%20the%20diversity%20of%20Earth%20observation%20data%20in%0Aterms%20of%20resolutions%2C%20scales%2C%20and%20modalities.%20However%2C%20existing%20approaches%0Aexpect%20fixed%20input%20configurations%2C%20which%20limits%20their%20practical%20applicability.%0AWe%20propose%20AnySat%2C%20a%20multimodal%20model%20based%20on%20joint%20embedding%20predictive%0Aarchitecture%20%28JEPA%29%20and%20scale-adaptive%20spatial%20encoders%2C%20allowing%20us%20to%20train%20a%0Asingle%20model%20on%20highly%20heterogeneous%20data%20in%20a%20self-supervised%20manner.%20To%0Ademonstrate%20the%20advantages%20of%20this%20unified%20approach%2C%20we%20compile%20GeoPlex%2C%20a%0Acollection%20of%205%20multimodal%20datasets%20with%20varying%20characteristics%20and%20%2411%24%0Adistinct%20sensors.%20We%20then%20train%20a%20single%20powerful%20model%20on%20these%20diverse%0Adatasets%20simultaneously.%20Once%20fine-tuned%20or%20probed%2C%20we%20reach%20state-of-the-art%0Aresults%20on%20the%20test%20sets%20of%20GeoPlex%20and%20for%206%20external%20datasets%20across%20various%0Aenvironment%20monitoring%20tasks%3A%20land%20cover%20mapping%2C%20tree%20species%20identification%2C%0Acrop%20type%20classification%2C%20change%20detection%2C%20climate%20type%20classification%2C%20and%0Asegmentation%20of%20flood%2C%20burn%20scar%2C%20and%20deforestation.%20The%20code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/gastruc/AnySat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14123v3&entry.124074799=Read"},
{"title": "Document Image Rectification Bases on Self-Adaptive Multitask Fusion", "author": "Heng Li and Xiangping Wu and Qingcai Chen", "abstract": "  Deformed document image rectification is essential for real-world document\nunderstanding tasks, such as layout analysis and text recognition. However,\ncurrent multi-task methods -- such as background removal, 3D coordinate\nprediction, and text line segmentation -- often overlook the complementary\nfeatures between tasks and their interactions. To address this gap, we propose\na self-adaptive learnable multi-task fusion rectification network named\nSalmRec. This network incorporates an inter-task feature aggregation module\nthat adaptively improves the perception of geometric distortions, enhances\nfeature complementarity, and reduces negative interference. We also introduce a\ngating mechanism to balance features both within global tasks and between local\ntasks effectively. Experimental results on two English benchmarks (DIR300 and\nDocUNet) and one Chinese benchmark (DocReal) demonstrate that our method\nsignificantly improves rectification performance. Ablation studies further\nhighlight the positive impact of different tasks on dewarping and the\neffectiveness of our proposed module.\n", "link": "http://arxiv.org/abs/2505.06038v1", "date": "2025-05-09", "relevancy": 2.108, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Document%20Image%20Rectification%20Bases%20on%20Self-Adaptive%20Multitask%20Fusion&body=Title%3A%20Document%20Image%20Rectification%20Bases%20on%20Self-Adaptive%20Multitask%20Fusion%0AAuthor%3A%20Heng%20Li%20and%20Xiangping%20Wu%20and%20Qingcai%20Chen%0AAbstract%3A%20%20%20Deformed%20document%20image%20rectification%20is%20essential%20for%20real-world%20document%0Aunderstanding%20tasks%2C%20such%20as%20layout%20analysis%20and%20text%20recognition.%20However%2C%0Acurrent%20multi-task%20methods%20--%20such%20as%20background%20removal%2C%203D%20coordinate%0Aprediction%2C%20and%20text%20line%20segmentation%20--%20often%20overlook%20the%20complementary%0Afeatures%20between%20tasks%20and%20their%20interactions.%20To%20address%20this%20gap%2C%20we%20propose%0Aa%20self-adaptive%20learnable%20multi-task%20fusion%20rectification%20network%20named%0ASalmRec.%20This%20network%20incorporates%20an%20inter-task%20feature%20aggregation%20module%0Athat%20adaptively%20improves%20the%20perception%20of%20geometric%20distortions%2C%20enhances%0Afeature%20complementarity%2C%20and%20reduces%20negative%20interference.%20We%20also%20introduce%20a%0Agating%20mechanism%20to%20balance%20features%20both%20within%20global%20tasks%20and%20between%20local%0Atasks%20effectively.%20Experimental%20results%20on%20two%20English%20benchmarks%20%28DIR300%20and%0ADocUNet%29%20and%20one%20Chinese%20benchmark%20%28DocReal%29%20demonstrate%20that%20our%20method%0Asignificantly%20improves%20rectification%20performance.%20Ablation%20studies%20further%0Ahighlight%20the%20positive%20impact%20of%20different%20tasks%20on%20dewarping%20and%20the%0Aeffectiveness%20of%20our%20proposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocument%2520Image%2520Rectification%2520Bases%2520on%2520Self-Adaptive%2520Multitask%2520Fusion%26entry.906535625%3DHeng%2520Li%2520and%2520Xiangping%2520Wu%2520and%2520Qingcai%2520Chen%26entry.1292438233%3D%2520%2520Deformed%2520document%2520image%2520rectification%2520is%2520essential%2520for%2520real-world%2520document%250Aunderstanding%2520tasks%252C%2520such%2520as%2520layout%2520analysis%2520and%2520text%2520recognition.%2520However%252C%250Acurrent%2520multi-task%2520methods%2520--%2520such%2520as%2520background%2520removal%252C%25203D%2520coordinate%250Aprediction%252C%2520and%2520text%2520line%2520segmentation%2520--%2520often%2520overlook%2520the%2520complementary%250Afeatures%2520between%2520tasks%2520and%2520their%2520interactions.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%250Aa%2520self-adaptive%2520learnable%2520multi-task%2520fusion%2520rectification%2520network%2520named%250ASalmRec.%2520This%2520network%2520incorporates%2520an%2520inter-task%2520feature%2520aggregation%2520module%250Athat%2520adaptively%2520improves%2520the%2520perception%2520of%2520geometric%2520distortions%252C%2520enhances%250Afeature%2520complementarity%252C%2520and%2520reduces%2520negative%2520interference.%2520We%2520also%2520introduce%2520a%250Agating%2520mechanism%2520to%2520balance%2520features%2520both%2520within%2520global%2520tasks%2520and%2520between%2520local%250Atasks%2520effectively.%2520Experimental%2520results%2520on%2520two%2520English%2520benchmarks%2520%2528DIR300%2520and%250ADocUNet%2529%2520and%2520one%2520Chinese%2520benchmark%2520%2528DocReal%2529%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520improves%2520rectification%2520performance.%2520Ablation%2520studies%2520further%250Ahighlight%2520the%2520positive%2520impact%2520of%2520different%2520tasks%2520on%2520dewarping%2520and%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Document%20Image%20Rectification%20Bases%20on%20Self-Adaptive%20Multitask%20Fusion&entry.906535625=Heng%20Li%20and%20Xiangping%20Wu%20and%20Qingcai%20Chen&entry.1292438233=%20%20Deformed%20document%20image%20rectification%20is%20essential%20for%20real-world%20document%0Aunderstanding%20tasks%2C%20such%20as%20layout%20analysis%20and%20text%20recognition.%20However%2C%0Acurrent%20multi-task%20methods%20--%20such%20as%20background%20removal%2C%203D%20coordinate%0Aprediction%2C%20and%20text%20line%20segmentation%20--%20often%20overlook%20the%20complementary%0Afeatures%20between%20tasks%20and%20their%20interactions.%20To%20address%20this%20gap%2C%20we%20propose%0Aa%20self-adaptive%20learnable%20multi-task%20fusion%20rectification%20network%20named%0ASalmRec.%20This%20network%20incorporates%20an%20inter-task%20feature%20aggregation%20module%0Athat%20adaptively%20improves%20the%20perception%20of%20geometric%20distortions%2C%20enhances%0Afeature%20complementarity%2C%20and%20reduces%20negative%20interference.%20We%20also%20introduce%20a%0Agating%20mechanism%20to%20balance%20features%20both%20within%20global%20tasks%20and%20between%20local%0Atasks%20effectively.%20Experimental%20results%20on%20two%20English%20benchmarks%20%28DIR300%20and%0ADocUNet%29%20and%20one%20Chinese%20benchmark%20%28DocReal%29%20demonstrate%20that%20our%20method%0Asignificantly%20improves%20rectification%20performance.%20Ablation%20studies%20further%0Ahighlight%20the%20positive%20impact%20of%20different%20tasks%20on%20dewarping%20and%20the%0Aeffectiveness%20of%20our%20proposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06038v1&entry.124074799=Read"},
{"title": "On the Depth of Monotone ReLU Neural Networks and ICNNs", "author": "Egor Bakaev and Florestan Brunck and Christoph Hertrich and Daniel Reichman and Amir Yehudayoff", "abstract": "  We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and\ninput convex neural networks (ICNN). Our focus is on expressivity, mostly in\nterms of depth, and we prove the following lower bounds. For the maximum\nfunction MAX$_n$ computing the maximum of $n$ real numbers, we show that\nReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a\nsharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove\ndepth separations between ReLU networks and ICNNs; for every $k$, there is a\ndepth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$\nICNN. The proofs are based on deep connections between neural networks and\npolyhedral geometry, and also use isoperimetric properties of triangulations.\n", "link": "http://arxiv.org/abs/2505.06169v1", "date": "2025-05-09", "relevancy": 2.1032, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Depth%20of%20Monotone%20ReLU%20Neural%20Networks%20and%20ICNNs&body=Title%3A%20On%20the%20Depth%20of%20Monotone%20ReLU%20Neural%20Networks%20and%20ICNNs%0AAuthor%3A%20Egor%20Bakaev%20and%20Florestan%20Brunck%20and%20Christoph%20Hertrich%20and%20Daniel%20Reichman%20and%20Amir%20Yehudayoff%0AAbstract%3A%20%20%20We%20study%20two%20models%20of%20ReLU%20neural%20networks%3A%20monotone%20networks%20%28ReLU%24%5E%2B%24%29%20and%0Ainput%20convex%20neural%20networks%20%28ICNN%29.%20Our%20focus%20is%20on%20expressivity%2C%20mostly%20in%0Aterms%20of%20depth%2C%20and%20we%20prove%20the%20following%20lower%20bounds.%20For%20the%20maximum%0Afunction%20MAX%24_n%24%20computing%20the%20maximum%20of%20%24n%24%20real%20numbers%2C%20we%20show%20that%0AReLU%24%5E%2B%24%20networks%20cannot%20compute%20MAX%24_n%24%2C%20or%20even%20approximate%20it.%20We%20prove%20a%0Asharp%20%24n%24%20lower%20bound%20on%20the%20ICNN%20depth%20complexity%20of%20MAX%24_n%24.%20We%20also%20prove%0Adepth%20separations%20between%20ReLU%20networks%20and%20ICNNs%3B%20for%20every%20%24k%24%2C%20there%20is%20a%0Adepth-2%20ReLU%20network%20of%20size%20%24O%28k%5E2%29%24%20that%20cannot%20be%20simulated%20by%20a%20depth-%24k%24%0AICNN.%20The%20proofs%20are%20based%20on%20deep%20connections%20between%20neural%20networks%20and%0Apolyhedral%20geometry%2C%20and%20also%20use%20isoperimetric%20properties%20of%20triangulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Depth%2520of%2520Monotone%2520ReLU%2520Neural%2520Networks%2520and%2520ICNNs%26entry.906535625%3DEgor%2520Bakaev%2520and%2520Florestan%2520Brunck%2520and%2520Christoph%2520Hertrich%2520and%2520Daniel%2520Reichman%2520and%2520Amir%2520Yehudayoff%26entry.1292438233%3D%2520%2520We%2520study%2520two%2520models%2520of%2520ReLU%2520neural%2520networks%253A%2520monotone%2520networks%2520%2528ReLU%2524%255E%252B%2524%2529%2520and%250Ainput%2520convex%2520neural%2520networks%2520%2528ICNN%2529.%2520Our%2520focus%2520is%2520on%2520expressivity%252C%2520mostly%2520in%250Aterms%2520of%2520depth%252C%2520and%2520we%2520prove%2520the%2520following%2520lower%2520bounds.%2520For%2520the%2520maximum%250Afunction%2520MAX%2524_n%2524%2520computing%2520the%2520maximum%2520of%2520%2524n%2524%2520real%2520numbers%252C%2520we%2520show%2520that%250AReLU%2524%255E%252B%2524%2520networks%2520cannot%2520compute%2520MAX%2524_n%2524%252C%2520or%2520even%2520approximate%2520it.%2520We%2520prove%2520a%250Asharp%2520%2524n%2524%2520lower%2520bound%2520on%2520the%2520ICNN%2520depth%2520complexity%2520of%2520MAX%2524_n%2524.%2520We%2520also%2520prove%250Adepth%2520separations%2520between%2520ReLU%2520networks%2520and%2520ICNNs%253B%2520for%2520every%2520%2524k%2524%252C%2520there%2520is%2520a%250Adepth-2%2520ReLU%2520network%2520of%2520size%2520%2524O%2528k%255E2%2529%2524%2520that%2520cannot%2520be%2520simulated%2520by%2520a%2520depth-%2524k%2524%250AICNN.%2520The%2520proofs%2520are%2520based%2520on%2520deep%2520connections%2520between%2520neural%2520networks%2520and%250Apolyhedral%2520geometry%252C%2520and%2520also%2520use%2520isoperimetric%2520properties%2520of%2520triangulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Depth%20of%20Monotone%20ReLU%20Neural%20Networks%20and%20ICNNs&entry.906535625=Egor%20Bakaev%20and%20Florestan%20Brunck%20and%20Christoph%20Hertrich%20and%20Daniel%20Reichman%20and%20Amir%20Yehudayoff&entry.1292438233=%20%20We%20study%20two%20models%20of%20ReLU%20neural%20networks%3A%20monotone%20networks%20%28ReLU%24%5E%2B%24%29%20and%0Ainput%20convex%20neural%20networks%20%28ICNN%29.%20Our%20focus%20is%20on%20expressivity%2C%20mostly%20in%0Aterms%20of%20depth%2C%20and%20we%20prove%20the%20following%20lower%20bounds.%20For%20the%20maximum%0Afunction%20MAX%24_n%24%20computing%20the%20maximum%20of%20%24n%24%20real%20numbers%2C%20we%20show%20that%0AReLU%24%5E%2B%24%20networks%20cannot%20compute%20MAX%24_n%24%2C%20or%20even%20approximate%20it.%20We%20prove%20a%0Asharp%20%24n%24%20lower%20bound%20on%20the%20ICNN%20depth%20complexity%20of%20MAX%24_n%24.%20We%20also%20prove%0Adepth%20separations%20between%20ReLU%20networks%20and%20ICNNs%3B%20for%20every%20%24k%24%2C%20there%20is%20a%0Adepth-2%20ReLU%20network%20of%20size%20%24O%28k%5E2%29%24%20that%20cannot%20be%20simulated%20by%20a%20depth-%24k%24%0AICNN.%20The%20proofs%20are%20based%20on%20deep%20connections%20between%20neural%20networks%20and%0Apolyhedral%20geometry%2C%20and%20also%20use%20isoperimetric%20properties%20of%20triangulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06169v1&entry.124074799=Read"},
{"title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "author": "Lennart Justen", "abstract": "  This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.\n", "link": "http://arxiv.org/abs/2505.06108v1", "date": "2025-05-09", "relevancy": 2.0701, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks&body=Title%3A%20LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks%0AAuthor%3A%20Lennart%20Justen%0AAbstract%3A%20%20%20This%20study%20systematically%20evaluates%2027%20frontier%20Large%20Language%20Models%20on%0Aeight%20diverse%20biology%20benchmarks%20spanning%20molecular%20biology%2C%20genetics%2C%20cloning%2C%0Avirology%2C%20and%20biosecurity.%20Models%20from%20major%20AI%20developers%20released%20between%0ANovember%202022%20and%20April%202025%20were%20assessed%20through%20ten%20independent%20runs%20per%0Abenchmark.%20The%20findings%20reveal%20dramatic%20improvements%20in%20biological%0Acapabilities.%20Top%20model%20performance%20increased%20more%20than%204-fold%20on%20the%0Achallenging%20text-only%20subset%20of%20the%20Virology%20Capabilities%20Test%20over%20the%20study%0Aperiod%2C%20with%20the%20top%20model%20now%20performing%20twice%20as%20well%20as%20expert%20virologists.%0ASeveral%20models%20now%20match%20or%20exceed%20expert-level%20performance%20on%20other%0Achallenging%20benchmarks%2C%20including%20LAB-Bench%20CloningScenarios%20and%20the%20biology%0Asubsets%20of%20GPQA%20and%20WMDP.%20Contrary%20to%20expectations%2C%20chain-of-thought%20did%20not%0Asubstantially%20improve%20performance%20over%20zero-shot%20evaluation%2C%20while%20extended%0Areasoning%20features%20in%20o3-mini%20and%20Claude%203.7%20Sonnet%20typically%20improved%0Aperformance%20as%20predicted%20by%20inference%20scaling.%20Benchmarks%20such%20as%20PubMedQA%20and%0Athe%20MMLU%20and%20WMDP%20biology%20subsets%20exhibited%20performance%20plateaus%20well%20below%0A100%25%2C%20suggesting%20benchmark%20saturation%20and%20errors%20in%20the%20underlying%20benchmark%0Adata.%20The%20analysis%20highlights%20the%20need%20for%20more%20sophisticated%20evaluation%0Amethodologies%20as%20AI%20systems%20continue%20to%20advance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Outperform%2520Experts%2520on%2520Challenging%2520Biology%2520Benchmarks%26entry.906535625%3DLennart%2520Justen%26entry.1292438233%3D%2520%2520This%2520study%2520systematically%2520evaluates%252027%2520frontier%2520Large%2520Language%2520Models%2520on%250Aeight%2520diverse%2520biology%2520benchmarks%2520spanning%2520molecular%2520biology%252C%2520genetics%252C%2520cloning%252C%250Avirology%252C%2520and%2520biosecurity.%2520Models%2520from%2520major%2520AI%2520developers%2520released%2520between%250ANovember%25202022%2520and%2520April%25202025%2520were%2520assessed%2520through%2520ten%2520independent%2520runs%2520per%250Abenchmark.%2520The%2520findings%2520reveal%2520dramatic%2520improvements%2520in%2520biological%250Acapabilities.%2520Top%2520model%2520performance%2520increased%2520more%2520than%25204-fold%2520on%2520the%250Achallenging%2520text-only%2520subset%2520of%2520the%2520Virology%2520Capabilities%2520Test%2520over%2520the%2520study%250Aperiod%252C%2520with%2520the%2520top%2520model%2520now%2520performing%2520twice%2520as%2520well%2520as%2520expert%2520virologists.%250ASeveral%2520models%2520now%2520match%2520or%2520exceed%2520expert-level%2520performance%2520on%2520other%250Achallenging%2520benchmarks%252C%2520including%2520LAB-Bench%2520CloningScenarios%2520and%2520the%2520biology%250Asubsets%2520of%2520GPQA%2520and%2520WMDP.%2520Contrary%2520to%2520expectations%252C%2520chain-of-thought%2520did%2520not%250Asubstantially%2520improve%2520performance%2520over%2520zero-shot%2520evaluation%252C%2520while%2520extended%250Areasoning%2520features%2520in%2520o3-mini%2520and%2520Claude%25203.7%2520Sonnet%2520typically%2520improved%250Aperformance%2520as%2520predicted%2520by%2520inference%2520scaling.%2520Benchmarks%2520such%2520as%2520PubMedQA%2520and%250Athe%2520MMLU%2520and%2520WMDP%2520biology%2520subsets%2520exhibited%2520performance%2520plateaus%2520well%2520below%250A100%2525%252C%2520suggesting%2520benchmark%2520saturation%2520and%2520errors%2520in%2520the%2520underlying%2520benchmark%250Adata.%2520The%2520analysis%2520highlights%2520the%2520need%2520for%2520more%2520sophisticated%2520evaluation%250Amethodologies%2520as%2520AI%2520systems%2520continue%2520to%2520advance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Outperform%20Experts%20on%20Challenging%20Biology%20Benchmarks&entry.906535625=Lennart%20Justen&entry.1292438233=%20%20This%20study%20systematically%20evaluates%2027%20frontier%20Large%20Language%20Models%20on%0Aeight%20diverse%20biology%20benchmarks%20spanning%20molecular%20biology%2C%20genetics%2C%20cloning%2C%0Avirology%2C%20and%20biosecurity.%20Models%20from%20major%20AI%20developers%20released%20between%0ANovember%202022%20and%20April%202025%20were%20assessed%20through%20ten%20independent%20runs%20per%0Abenchmark.%20The%20findings%20reveal%20dramatic%20improvements%20in%20biological%0Acapabilities.%20Top%20model%20performance%20increased%20more%20than%204-fold%20on%20the%0Achallenging%20text-only%20subset%20of%20the%20Virology%20Capabilities%20Test%20over%20the%20study%0Aperiod%2C%20with%20the%20top%20model%20now%20performing%20twice%20as%20well%20as%20expert%20virologists.%0ASeveral%20models%20now%20match%20or%20exceed%20expert-level%20performance%20on%20other%0Achallenging%20benchmarks%2C%20including%20LAB-Bench%20CloningScenarios%20and%20the%20biology%0Asubsets%20of%20GPQA%20and%20WMDP.%20Contrary%20to%20expectations%2C%20chain-of-thought%20did%20not%0Asubstantially%20improve%20performance%20over%20zero-shot%20evaluation%2C%20while%20extended%0Areasoning%20features%20in%20o3-mini%20and%20Claude%203.7%20Sonnet%20typically%20improved%0Aperformance%20as%20predicted%20by%20inference%20scaling.%20Benchmarks%20such%20as%20PubMedQA%20and%0Athe%20MMLU%20and%20WMDP%20biology%20subsets%20exhibited%20performance%20plateaus%20well%20below%0A100%25%2C%20suggesting%20benchmark%20saturation%20and%20errors%20in%20the%20underlying%20benchmark%0Adata.%20The%20analysis%20highlights%20the%20need%20for%20more%20sophisticated%20evaluation%0Amethodologies%20as%20AI%20systems%20continue%20to%20advance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06108v1&entry.124074799=Read"},
{"title": "Uncovering Model Processing Strategies with Non-Negative Per-Example\n  Fisher Factorization", "author": "Michael Matena and Colin Raffel", "abstract": "  We introduce NPEFF (Non-Negative Per-Example Fisher Factorization), an\ninterpretability method that aims to uncover strategies used by a model to\ngenerate its predictions. NPEFF decomposes per-example Fisher matrices using a\nnovel decomposition algorithm that learns a set of components represented by\nlearned rank-1 positive semi-definite matrices. Through a combination of human\nevaluation and automated analysis, we demonstrate that these NPEFF components\ncorrespond to model processing strategies for a variety of language models and\ntext processing tasks. We further show how to construct parameter perturbations\nfrom NPEFF components to selectively disrupt a given component's role in the\nmodel's processing. Along with conducting extensive ablation studies, we\ninclude experiments to show how NPEFF can be used to analyze and mitigate\ncollateral effects of unlearning and use NPEFF to study in-context learning.\nFurthermore, we demonstrate the advantages of NPEFF over baselines such as\ngradient clustering and using sparse autoencoders for dictionary learning over\nmodel activations.\n", "link": "http://arxiv.org/abs/2310.04649v2", "date": "2025-05-09", "relevancy": 2.069, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Model%20Processing%20Strategies%20with%20Non-Negative%20Per-Example%0A%20%20Fisher%20Factorization&body=Title%3A%20Uncovering%20Model%20Processing%20Strategies%20with%20Non-Negative%20Per-Example%0A%20%20Fisher%20Factorization%0AAuthor%3A%20Michael%20Matena%20and%20Colin%20Raffel%0AAbstract%3A%20%20%20We%20introduce%20NPEFF%20%28Non-Negative%20Per-Example%20Fisher%20Factorization%29%2C%20an%0Ainterpretability%20method%20that%20aims%20to%20uncover%20strategies%20used%20by%20a%20model%20to%0Agenerate%20its%20predictions.%20NPEFF%20decomposes%20per-example%20Fisher%20matrices%20using%20a%0Anovel%20decomposition%20algorithm%20that%20learns%20a%20set%20of%20components%20represented%20by%0Alearned%20rank-1%20positive%20semi-definite%20matrices.%20Through%20a%20combination%20of%20human%0Aevaluation%20and%20automated%20analysis%2C%20we%20demonstrate%20that%20these%20NPEFF%20components%0Acorrespond%20to%20model%20processing%20strategies%20for%20a%20variety%20of%20language%20models%20and%0Atext%20processing%20tasks.%20We%20further%20show%20how%20to%20construct%20parameter%20perturbations%0Afrom%20NPEFF%20components%20to%20selectively%20disrupt%20a%20given%20component%27s%20role%20in%20the%0Amodel%27s%20processing.%20Along%20with%20conducting%20extensive%20ablation%20studies%2C%20we%0Ainclude%20experiments%20to%20show%20how%20NPEFF%20can%20be%20used%20to%20analyze%20and%20mitigate%0Acollateral%20effects%20of%20unlearning%20and%20use%20NPEFF%20to%20study%20in-context%20learning.%0AFurthermore%2C%20we%20demonstrate%20the%20advantages%20of%20NPEFF%20over%20baselines%20such%20as%0Agradient%20clustering%20and%20using%20sparse%20autoencoders%20for%20dictionary%20learning%20over%0Amodel%20activations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Model%2520Processing%2520Strategies%2520with%2520Non-Negative%2520Per-Example%250A%2520%2520Fisher%2520Factorization%26entry.906535625%3DMichael%2520Matena%2520and%2520Colin%2520Raffel%26entry.1292438233%3D%2520%2520We%2520introduce%2520NPEFF%2520%2528Non-Negative%2520Per-Example%2520Fisher%2520Factorization%2529%252C%2520an%250Ainterpretability%2520method%2520that%2520aims%2520to%2520uncover%2520strategies%2520used%2520by%2520a%2520model%2520to%250Agenerate%2520its%2520predictions.%2520NPEFF%2520decomposes%2520per-example%2520Fisher%2520matrices%2520using%2520a%250Anovel%2520decomposition%2520algorithm%2520that%2520learns%2520a%2520set%2520of%2520components%2520represented%2520by%250Alearned%2520rank-1%2520positive%2520semi-definite%2520matrices.%2520Through%2520a%2520combination%2520of%2520human%250Aevaluation%2520and%2520automated%2520analysis%252C%2520we%2520demonstrate%2520that%2520these%2520NPEFF%2520components%250Acorrespond%2520to%2520model%2520processing%2520strategies%2520for%2520a%2520variety%2520of%2520language%2520models%2520and%250Atext%2520processing%2520tasks.%2520We%2520further%2520show%2520how%2520to%2520construct%2520parameter%2520perturbations%250Afrom%2520NPEFF%2520components%2520to%2520selectively%2520disrupt%2520a%2520given%2520component%2527s%2520role%2520in%2520the%250Amodel%2527s%2520processing.%2520Along%2520with%2520conducting%2520extensive%2520ablation%2520studies%252C%2520we%250Ainclude%2520experiments%2520to%2520show%2520how%2520NPEFF%2520can%2520be%2520used%2520to%2520analyze%2520and%2520mitigate%250Acollateral%2520effects%2520of%2520unlearning%2520and%2520use%2520NPEFF%2520to%2520study%2520in-context%2520learning.%250AFurthermore%252C%2520we%2520demonstrate%2520the%2520advantages%2520of%2520NPEFF%2520over%2520baselines%2520such%2520as%250Agradient%2520clustering%2520and%2520using%2520sparse%2520autoencoders%2520for%2520dictionary%2520learning%2520over%250Amodel%2520activations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Model%20Processing%20Strategies%20with%20Non-Negative%20Per-Example%0A%20%20Fisher%20Factorization&entry.906535625=Michael%20Matena%20and%20Colin%20Raffel&entry.1292438233=%20%20We%20introduce%20NPEFF%20%28Non-Negative%20Per-Example%20Fisher%20Factorization%29%2C%20an%0Ainterpretability%20method%20that%20aims%20to%20uncover%20strategies%20used%20by%20a%20model%20to%0Agenerate%20its%20predictions.%20NPEFF%20decomposes%20per-example%20Fisher%20matrices%20using%20a%0Anovel%20decomposition%20algorithm%20that%20learns%20a%20set%20of%20components%20represented%20by%0Alearned%20rank-1%20positive%20semi-definite%20matrices.%20Through%20a%20combination%20of%20human%0Aevaluation%20and%20automated%20analysis%2C%20we%20demonstrate%20that%20these%20NPEFF%20components%0Acorrespond%20to%20model%20processing%20strategies%20for%20a%20variety%20of%20language%20models%20and%0Atext%20processing%20tasks.%20We%20further%20show%20how%20to%20construct%20parameter%20perturbations%0Afrom%20NPEFF%20components%20to%20selectively%20disrupt%20a%20given%20component%27s%20role%20in%20the%0Amodel%27s%20processing.%20Along%20with%20conducting%20extensive%20ablation%20studies%2C%20we%0Ainclude%20experiments%20to%20show%20how%20NPEFF%20can%20be%20used%20to%20analyze%20and%20mitigate%0Acollateral%20effects%20of%20unlearning%20and%20use%20NPEFF%20to%20study%20in-context%20learning.%0AFurthermore%2C%20we%20demonstrate%20the%20advantages%20of%20NPEFF%20over%20baselines%20such%20as%0Agradient%20clustering%20and%20using%20sparse%20autoencoders%20for%20dictionary%20learning%20over%0Amodel%20activations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04649v2&entry.124074799=Read"},
{"title": "Patch distribution modeling framework adaptive cosine estimator\n  (PaDiM-ACE) for anomaly detection and localization in synthetic aperture\n  radar imagery", "author": "Angelina Ibarra and Joshua Peeples", "abstract": "  This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.\n", "link": "http://arxiv.org/abs/2504.08049v2", "date": "2025-05-09", "relevancy": 2.0595, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch%20distribution%20modeling%20framework%20adaptive%20cosine%20estimator%0A%20%20%28PaDiM-ACE%29%20for%20anomaly%20detection%20and%20localization%20in%20synthetic%20aperture%0A%20%20radar%20imagery&body=Title%3A%20Patch%20distribution%20modeling%20framework%20adaptive%20cosine%20estimator%0A%20%20%28PaDiM-ACE%29%20for%20anomaly%20detection%20and%20localization%20in%20synthetic%20aperture%0A%20%20radar%20imagery%0AAuthor%3A%20Angelina%20Ibarra%20and%20Joshua%20Peeples%0AAbstract%3A%20%20%20This%20work%20presents%20a%20new%20approach%20to%20anomaly%20detection%20and%20localization%20in%0Asynthetic%20aperture%20radar%20imagery%20%28SAR%29%2C%20expanding%20upon%20the%20existing%20patch%0Adistribution%20modeling%20framework%20%28PaDiM%29.%20We%20introduce%20the%20adaptive%20cosine%0Aestimator%20%28ACE%29%20detection%20statistic.%20PaDiM%20uses%20the%20Mahalanobis%20distance%20at%0Ainference%2C%20an%20unbounded%20metric.%20ACE%20instead%20uses%20the%20cosine%20similarity%20metric%2C%0Aproviding%20bounded%20anomaly%20detection%20scores.%20The%20proposed%20method%20is%20evaluated%0Aacross%20multiple%20SAR%20datasets%2C%20with%20performance%20metrics%20including%20the%20area%20under%0Athe%20receiver%20operating%20curve%20%28AUROC%29%20at%20the%20image%20and%20pixel%20level%2C%20aiming%20for%0Aincreased%20performance%20in%20anomaly%20detection%20and%20localization%20of%20SAR%20imagery.%20The%0Acode%20is%20publicly%20available%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch%2520distribution%2520modeling%2520framework%2520adaptive%2520cosine%2520estimator%250A%2520%2520%2528PaDiM-ACE%2529%2520for%2520anomaly%2520detection%2520and%2520localization%2520in%2520synthetic%2520aperture%250A%2520%2520radar%2520imagery%26entry.906535625%3DAngelina%2520Ibarra%2520and%2520Joshua%2520Peeples%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520new%2520approach%2520to%2520anomaly%2520detection%2520and%2520localization%2520in%250Asynthetic%2520aperture%2520radar%2520imagery%2520%2528SAR%2529%252C%2520expanding%2520upon%2520the%2520existing%2520patch%250Adistribution%2520modeling%2520framework%2520%2528PaDiM%2529.%2520We%2520introduce%2520the%2520adaptive%2520cosine%250Aestimator%2520%2528ACE%2529%2520detection%2520statistic.%2520PaDiM%2520uses%2520the%2520Mahalanobis%2520distance%2520at%250Ainference%252C%2520an%2520unbounded%2520metric.%2520ACE%2520instead%2520uses%2520the%2520cosine%2520similarity%2520metric%252C%250Aproviding%2520bounded%2520anomaly%2520detection%2520scores.%2520The%2520proposed%2520method%2520is%2520evaluated%250Aacross%2520multiple%2520SAR%2520datasets%252C%2520with%2520performance%2520metrics%2520including%2520the%2520area%2520under%250Athe%2520receiver%2520operating%2520curve%2520%2528AUROC%2529%2520at%2520the%2520image%2520and%2520pixel%2520level%252C%2520aiming%2520for%250Aincreased%2520performance%2520in%2520anomaly%2520detection%2520and%2520localization%2520of%2520SAR%2520imagery.%2520The%250Acode%2520is%2520publicly%2520available%253A%250Ahttps%253A//github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch%20distribution%20modeling%20framework%20adaptive%20cosine%20estimator%0A%20%20%28PaDiM-ACE%29%20for%20anomaly%20detection%20and%20localization%20in%20synthetic%20aperture%0A%20%20radar%20imagery&entry.906535625=Angelina%20Ibarra%20and%20Joshua%20Peeples&entry.1292438233=%20%20This%20work%20presents%20a%20new%20approach%20to%20anomaly%20detection%20and%20localization%20in%0Asynthetic%20aperture%20radar%20imagery%20%28SAR%29%2C%20expanding%20upon%20the%20existing%20patch%0Adistribution%20modeling%20framework%20%28PaDiM%29.%20We%20introduce%20the%20adaptive%20cosine%0Aestimator%20%28ACE%29%20detection%20statistic.%20PaDiM%20uses%20the%20Mahalanobis%20distance%20at%0Ainference%2C%20an%20unbounded%20metric.%20ACE%20instead%20uses%20the%20cosine%20similarity%20metric%2C%0Aproviding%20bounded%20anomaly%20detection%20scores.%20The%20proposed%20method%20is%20evaluated%0Aacross%20multiple%20SAR%20datasets%2C%20with%20performance%20metrics%20including%20the%20area%20under%0Athe%20receiver%20operating%20curve%20%28AUROC%29%20at%20the%20image%20and%20pixel%20level%2C%20aiming%20for%0Aincreased%20performance%20in%20anomaly%20detection%20and%20localization%20of%20SAR%20imagery.%20The%0Acode%20is%20publicly%20available%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08049v2&entry.124074799=Read"},
{"title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks", "author": "Kejie Zhao and Wenjia Hua and Aiersi Tuerhong and Luziwei Leng and Yuxin Ma and Qinghai Guo", "abstract": "  Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.\n", "link": "http://arxiv.org/abs/2505.05375v2", "date": "2025-05-09", "relevancy": 2.0541, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Kejie%20Zhao%20and%20Wenjia%20Hua%20and%20Aiersi%20Tuerhong%20and%20Luziwei%20Leng%20and%20Yuxin%20Ma%20and%20Qinghai%20Guo%0AAbstract%3A%20%20%20Recently%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20deployed%20on%20neuromorphic%20chips%2C%0Aprovide%20highly%20efficient%20solutions%20on%20edge%20devices%20in%20different%20scenarios.%0AHowever%2C%20their%20ability%20to%20adapt%20to%20distribution%20shifts%20after%20deployment%20has%0Abecome%20a%20crucial%20challenge.%20Online%20test-time%20adaptation%20%28OTTA%29%20offers%20a%0Apromising%20solution%20by%20enabling%20models%20to%20dynamically%20adjust%20to%20new%20data%0Adistributions%20without%20requiring%20source%20data%20or%20labeled%20target%20samples.%0ANevertheless%2C%20existing%20OTTA%20methods%20are%20largely%20designed%20for%20traditional%0Aartificial%20neural%20networks%20and%20are%20not%20well-suited%20for%20SNNs.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20low-power%2C%20neuromorphic%20chip-friendly%20online%20test-time%0Aadaptation%20framework%2C%20aiming%20to%20enhance%20model%20generalization%20under%20distribution%0Ashifts.%20The%20proposed%20approach%20is%20called%20Threshold%20Modulation%20%28TM%29%2C%20which%0Adynamically%20adjusts%20the%20firing%20threshold%20through%20neuronal%20dynamics-inspired%0Anormalization%2C%20being%20more%20compatible%20with%20neuromorphic%20hardware.%20Experimental%0Aresults%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%20method%20in%0Aimproving%20the%20robustness%20of%20SNNs%20against%20distribution%20shifts%20while%20maintaining%0Alow%20computational%20cost.%20The%20proposed%20method%20offers%20a%20practical%20solution%20for%0Aonline%20test-time%20adaptation%20of%20SNNs%2C%20providing%20inspiration%20for%20the%20design%20of%0Afuture%20neuromorphic%20chips.%20The%20demo%20code%20is%20available%20at%0Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThreshold%2520Modulation%2520for%2520Online%2520Test-Time%2520Adaptation%2520of%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DKejie%2520Zhao%2520and%2520Wenjia%2520Hua%2520and%2520Aiersi%2520Tuerhong%2520and%2520Luziwei%2520Leng%2520and%2520Yuxin%2520Ma%2520and%2520Qinghai%2520Guo%26entry.1292438233%3D%2520%2520Recently%252C%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520deployed%2520on%2520neuromorphic%2520chips%252C%250Aprovide%2520highly%2520efficient%2520solutions%2520on%2520edge%2520devices%2520in%2520different%2520scenarios.%250AHowever%252C%2520their%2520ability%2520to%2520adapt%2520to%2520distribution%2520shifts%2520after%2520deployment%2520has%250Abecome%2520a%2520crucial%2520challenge.%2520Online%2520test-time%2520adaptation%2520%2528OTTA%2529%2520offers%2520a%250Apromising%2520solution%2520by%2520enabling%2520models%2520to%2520dynamically%2520adjust%2520to%2520new%2520data%250Adistributions%2520without%2520requiring%2520source%2520data%2520or%2520labeled%2520target%2520samples.%250ANevertheless%252C%2520existing%2520OTTA%2520methods%2520are%2520largely%2520designed%2520for%2520traditional%250Aartificial%2520neural%2520networks%2520and%2520are%2520not%2520well-suited%2520for%2520SNNs.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520a%2520low-power%252C%2520neuromorphic%2520chip-friendly%2520online%2520test-time%250Aadaptation%2520framework%252C%2520aiming%2520to%2520enhance%2520model%2520generalization%2520under%2520distribution%250Ashifts.%2520The%2520proposed%2520approach%2520is%2520called%2520Threshold%2520Modulation%2520%2528TM%2529%252C%2520which%250Adynamically%2520adjusts%2520the%2520firing%2520threshold%2520through%2520neuronal%2520dynamics-inspired%250Anormalization%252C%2520being%2520more%2520compatible%2520with%2520neuromorphic%2520hardware.%2520Experimental%250Aresults%2520on%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520method%2520in%250Aimproving%2520the%2520robustness%2520of%2520SNNs%2520against%2520distribution%2520shifts%2520while%2520maintaining%250Alow%2520computational%2520cost.%2520The%2520proposed%2520method%2520offers%2520a%2520practical%2520solution%2520for%250Aonline%2520test-time%2520adaptation%2520of%2520SNNs%252C%2520providing%2520inspiration%2520for%2520the%2520design%2520of%250Afuture%2520neuromorphic%2520chips.%2520The%2520demo%2520code%2520is%2520available%2520at%250Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Threshold%20Modulation%20for%20Online%20Test-Time%20Adaptation%20of%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Kejie%20Zhao%20and%20Wenjia%20Hua%20and%20Aiersi%20Tuerhong%20and%20Luziwei%20Leng%20and%20Yuxin%20Ma%20and%20Qinghai%20Guo&entry.1292438233=%20%20Recently%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20deployed%20on%20neuromorphic%20chips%2C%0Aprovide%20highly%20efficient%20solutions%20on%20edge%20devices%20in%20different%20scenarios.%0AHowever%2C%20their%20ability%20to%20adapt%20to%20distribution%20shifts%20after%20deployment%20has%0Abecome%20a%20crucial%20challenge.%20Online%20test-time%20adaptation%20%28OTTA%29%20offers%20a%0Apromising%20solution%20by%20enabling%20models%20to%20dynamically%20adjust%20to%20new%20data%0Adistributions%20without%20requiring%20source%20data%20or%20labeled%20target%20samples.%0ANevertheless%2C%20existing%20OTTA%20methods%20are%20largely%20designed%20for%20traditional%0Aartificial%20neural%20networks%20and%20are%20not%20well-suited%20for%20SNNs.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20low-power%2C%20neuromorphic%20chip-friendly%20online%20test-time%0Aadaptation%20framework%2C%20aiming%20to%20enhance%20model%20generalization%20under%20distribution%0Ashifts.%20The%20proposed%20approach%20is%20called%20Threshold%20Modulation%20%28TM%29%2C%20which%0Adynamically%20adjusts%20the%20firing%20threshold%20through%20neuronal%20dynamics-inspired%0Anormalization%2C%20being%20more%20compatible%20with%20neuromorphic%20hardware.%20Experimental%0Aresults%20on%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%20method%20in%0Aimproving%20the%20robustness%20of%20SNNs%20against%20distribution%20shifts%20while%20maintaining%0Alow%20computational%20cost.%20The%20proposed%20method%20offers%20a%20practical%20solution%20for%0Aonline%20test-time%20adaptation%20of%20SNNs%2C%20providing%20inspiration%20for%20the%20design%20of%0Afuture%20neuromorphic%20chips.%20The%20demo%20code%20is%20available%20at%0Agithub.com/NneurotransmitterR/TM-OTTA-SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05375v2&entry.124074799=Read"},
{"title": "Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free\n  Framework for Tensor Denoising", "author": "Hiroki Hasegawa and Yukihiko Okada", "abstract": "  In modern data-driven tasks such as classification, optimization, and\nforecasting, mitigating the effects of intrinsic noise is crucial for improving\npredictive accuracy. While numerous denoising techniques have been developed,\nthe rising dimensionality of real-world datasets limits conventional\nmatrix-based methods in preserving data structure and accuracy. This challenge\nhas led to increasing interest in tensor-based approaches, which naturally\ncapture multi-way data relationships. However, classical tensor decomposition\nmethods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative\noptimization, making them computationally expensive and less practical. In this\nwork, we propose a novel low-rank approximation method for tensor data that\navoids these limitations. Our approach applies statistically grounded singular\nvalue thresholding to mode-wise matricizations, enabling automatic extraction\nof significant components without requiring prior rank specification or\niterative refinement. Experiments on synthetic and real-world tensors show that\nour method consistently outperforms existing techniques in terms of estimation\naccuracy and computational efficiency, especially in noisy high-dimensional\nsettings.\n", "link": "http://arxiv.org/abs/2505.06203v1", "date": "2025-05-09", "relevancy": 2.0473, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5267}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto%20Tensor%20Singular%20Value%20Thresholding%3A%20A%20Non-Iterative%20and%20Rank-Free%0A%20%20Framework%20for%20Tensor%20Denoising&body=Title%3A%20Auto%20Tensor%20Singular%20Value%20Thresholding%3A%20A%20Non-Iterative%20and%20Rank-Free%0A%20%20Framework%20for%20Tensor%20Denoising%0AAuthor%3A%20Hiroki%20Hasegawa%20and%20Yukihiko%20Okada%0AAbstract%3A%20%20%20In%20modern%20data-driven%20tasks%20such%20as%20classification%2C%20optimization%2C%20and%0Aforecasting%2C%20mitigating%20the%20effects%20of%20intrinsic%20noise%20is%20crucial%20for%20improving%0Apredictive%20accuracy.%20While%20numerous%20denoising%20techniques%20have%20been%20developed%2C%0Athe%20rising%20dimensionality%20of%20real-world%20datasets%20limits%20conventional%0Amatrix-based%20methods%20in%20preserving%20data%20structure%20and%20accuracy.%20This%20challenge%0Ahas%20led%20to%20increasing%20interest%20in%20tensor-based%20approaches%2C%20which%20naturally%0Acapture%20multi-way%20data%20relationships.%20However%2C%20classical%20tensor%20decomposition%0Amethods%20%28e.g.%2C%20HOSVD%2C%20HOOI%29%20typically%20require%20pre-specified%20ranks%20and%20iterative%0Aoptimization%2C%20making%20them%20computationally%20expensive%20and%20less%20practical.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20low-rank%20approximation%20method%20for%20tensor%20data%20that%0Aavoids%20these%20limitations.%20Our%20approach%20applies%20statistically%20grounded%20singular%0Avalue%20thresholding%20to%20mode-wise%20matricizations%2C%20enabling%20automatic%20extraction%0Aof%20significant%20components%20without%20requiring%20prior%20rank%20specification%20or%0Aiterative%20refinement.%20Experiments%20on%20synthetic%20and%20real-world%20tensors%20show%20that%0Aour%20method%20consistently%20outperforms%20existing%20techniques%20in%20terms%20of%20estimation%0Aaccuracy%20and%20computational%20efficiency%2C%20especially%20in%20noisy%20high-dimensional%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto%2520Tensor%2520Singular%2520Value%2520Thresholding%253A%2520A%2520Non-Iterative%2520and%2520Rank-Free%250A%2520%2520Framework%2520for%2520Tensor%2520Denoising%26entry.906535625%3DHiroki%2520Hasegawa%2520and%2520Yukihiko%2520Okada%26entry.1292438233%3D%2520%2520In%2520modern%2520data-driven%2520tasks%2520such%2520as%2520classification%252C%2520optimization%252C%2520and%250Aforecasting%252C%2520mitigating%2520the%2520effects%2520of%2520intrinsic%2520noise%2520is%2520crucial%2520for%2520improving%250Apredictive%2520accuracy.%2520While%2520numerous%2520denoising%2520techniques%2520have%2520been%2520developed%252C%250Athe%2520rising%2520dimensionality%2520of%2520real-world%2520datasets%2520limits%2520conventional%250Amatrix-based%2520methods%2520in%2520preserving%2520data%2520structure%2520and%2520accuracy.%2520This%2520challenge%250Ahas%2520led%2520to%2520increasing%2520interest%2520in%2520tensor-based%2520approaches%252C%2520which%2520naturally%250Acapture%2520multi-way%2520data%2520relationships.%2520However%252C%2520classical%2520tensor%2520decomposition%250Amethods%2520%2528e.g.%252C%2520HOSVD%252C%2520HOOI%2529%2520typically%2520require%2520pre-specified%2520ranks%2520and%2520iterative%250Aoptimization%252C%2520making%2520them%2520computationally%2520expensive%2520and%2520less%2520practical.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520low-rank%2520approximation%2520method%2520for%2520tensor%2520data%2520that%250Aavoids%2520these%2520limitations.%2520Our%2520approach%2520applies%2520statistically%2520grounded%2520singular%250Avalue%2520thresholding%2520to%2520mode-wise%2520matricizations%252C%2520enabling%2520automatic%2520extraction%250Aof%2520significant%2520components%2520without%2520requiring%2520prior%2520rank%2520specification%2520or%250Aiterative%2520refinement.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520tensors%2520show%2520that%250Aour%2520method%2520consistently%2520outperforms%2520existing%2520techniques%2520in%2520terms%2520of%2520estimation%250Aaccuracy%2520and%2520computational%2520efficiency%252C%2520especially%2520in%2520noisy%2520high-dimensional%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto%20Tensor%20Singular%20Value%20Thresholding%3A%20A%20Non-Iterative%20and%20Rank-Free%0A%20%20Framework%20for%20Tensor%20Denoising&entry.906535625=Hiroki%20Hasegawa%20and%20Yukihiko%20Okada&entry.1292438233=%20%20In%20modern%20data-driven%20tasks%20such%20as%20classification%2C%20optimization%2C%20and%0Aforecasting%2C%20mitigating%20the%20effects%20of%20intrinsic%20noise%20is%20crucial%20for%20improving%0Apredictive%20accuracy.%20While%20numerous%20denoising%20techniques%20have%20been%20developed%2C%0Athe%20rising%20dimensionality%20of%20real-world%20datasets%20limits%20conventional%0Amatrix-based%20methods%20in%20preserving%20data%20structure%20and%20accuracy.%20This%20challenge%0Ahas%20led%20to%20increasing%20interest%20in%20tensor-based%20approaches%2C%20which%20naturally%0Acapture%20multi-way%20data%20relationships.%20However%2C%20classical%20tensor%20decomposition%0Amethods%20%28e.g.%2C%20HOSVD%2C%20HOOI%29%20typically%20require%20pre-specified%20ranks%20and%20iterative%0Aoptimization%2C%20making%20them%20computationally%20expensive%20and%20less%20practical.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20low-rank%20approximation%20method%20for%20tensor%20data%20that%0Aavoids%20these%20limitations.%20Our%20approach%20applies%20statistically%20grounded%20singular%0Avalue%20thresholding%20to%20mode-wise%20matricizations%2C%20enabling%20automatic%20extraction%0Aof%20significant%20components%20without%20requiring%20prior%20rank%20specification%20or%0Aiterative%20refinement.%20Experiments%20on%20synthetic%20and%20real-world%20tensors%20show%20that%0Aour%20method%20consistently%20outperforms%20existing%20techniques%20in%20terms%20of%20estimation%0Aaccuracy%20and%20computational%20efficiency%2C%20especially%20in%20noisy%20high-dimensional%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06203v1&entry.124074799=Read"},
{"title": "MotherNet: Fast Training and Inference via Hyper-Network Transformers", "author": "Andreas M\u00fcller and Carlo Curino and Raghu Ramakrishnan", "abstract": "  Foundation models are transforming machine learning across many modalities,\nwith in-context learning replacing classical model training. Recent work on\ntabular data hints at a similar opportunity to build foundation models for\nclassification for numerical data. However, existing meta-learning approaches\ncan not compete with tree-based methods in terms of inference time. In this\npaper, we propose MotherNet, a hypernetwork architecture trained on synthetic\nclassification tasks that, once prompted with a never-seen-before training set\ngenerates the weights of a trained ``child'' neural-network by in-context\nlearning using a single forward pass. In contrast to most existing\nhypernetworks that are usually trained for relatively constrained multi-task\nsettings, MotherNet can create models for multiclass classification on\narbitrary tabular datasets without any dataset specific gradient descent. The\nchild network generated by MotherNet outperforms neural networks trained using\ngradient descent on small datasets, and is comparable to predictions by TabPFN\nand standard ML methods like Gradient Boosting. Unlike a direct application of\nTabPFN, MotherNet generated networks are highly efficient at inference time. We\nalso demonstrate that HyperFast is unable to perform effective in-context\nlearning on small datasets, and heavily relies on dataset specific fine-tuning\nand hyper-parameter tuning, while MotherNet requires no fine-tuning or\nper-dataset hyper-parameters.\n", "link": "http://arxiv.org/abs/2312.08598v2", "date": "2025-05-09", "relevancy": 2.0398, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5478}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotherNet%3A%20Fast%20Training%20and%20Inference%20via%20Hyper-Network%20Transformers&body=Title%3A%20MotherNet%3A%20Fast%20Training%20and%20Inference%20via%20Hyper-Network%20Transformers%0AAuthor%3A%20Andreas%20M%C3%BCller%20and%20Carlo%20Curino%20and%20Raghu%20Ramakrishnan%0AAbstract%3A%20%20%20Foundation%20models%20are%20transforming%20machine%20learning%20across%20many%20modalities%2C%0Awith%20in-context%20learning%20replacing%20classical%20model%20training.%20Recent%20work%20on%0Atabular%20data%20hints%20at%20a%20similar%20opportunity%20to%20build%20foundation%20models%20for%0Aclassification%20for%20numerical%20data.%20However%2C%20existing%20meta-learning%20approaches%0Acan%20not%20compete%20with%20tree-based%20methods%20in%20terms%20of%20inference%20time.%20In%20this%0Apaper%2C%20we%20propose%20MotherNet%2C%20a%20hypernetwork%20architecture%20trained%20on%20synthetic%0Aclassification%20tasks%20that%2C%20once%20prompted%20with%20a%20never-seen-before%20training%20set%0Agenerates%20the%20weights%20of%20a%20trained%20%60%60child%27%27%20neural-network%20by%20in-context%0Alearning%20using%20a%20single%20forward%20pass.%20In%20contrast%20to%20most%20existing%0Ahypernetworks%20that%20are%20usually%20trained%20for%20relatively%20constrained%20multi-task%0Asettings%2C%20MotherNet%20can%20create%20models%20for%20multiclass%20classification%20on%0Aarbitrary%20tabular%20datasets%20without%20any%20dataset%20specific%20gradient%20descent.%20The%0Achild%20network%20generated%20by%20MotherNet%20outperforms%20neural%20networks%20trained%20using%0Agradient%20descent%20on%20small%20datasets%2C%20and%20is%20comparable%20to%20predictions%20by%20TabPFN%0Aand%20standard%20ML%20methods%20like%20Gradient%20Boosting.%20Unlike%20a%20direct%20application%20of%0ATabPFN%2C%20MotherNet%20generated%20networks%20are%20highly%20efficient%20at%20inference%20time.%20We%0Aalso%20demonstrate%20that%20HyperFast%20is%20unable%20to%20perform%20effective%20in-context%0Alearning%20on%20small%20datasets%2C%20and%20heavily%20relies%20on%20dataset%20specific%20fine-tuning%0Aand%20hyper-parameter%20tuning%2C%20while%20MotherNet%20requires%20no%20fine-tuning%20or%0Aper-dataset%20hyper-parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotherNet%253A%2520Fast%2520Training%2520and%2520Inference%2520via%2520Hyper-Network%2520Transformers%26entry.906535625%3DAndreas%2520M%25C3%25BCller%2520and%2520Carlo%2520Curino%2520and%2520Raghu%2520Ramakrishnan%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520transforming%2520machine%2520learning%2520across%2520many%2520modalities%252C%250Awith%2520in-context%2520learning%2520replacing%2520classical%2520model%2520training.%2520Recent%2520work%2520on%250Atabular%2520data%2520hints%2520at%2520a%2520similar%2520opportunity%2520to%2520build%2520foundation%2520models%2520for%250Aclassification%2520for%2520numerical%2520data.%2520However%252C%2520existing%2520meta-learning%2520approaches%250Acan%2520not%2520compete%2520with%2520tree-based%2520methods%2520in%2520terms%2520of%2520inference%2520time.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520MotherNet%252C%2520a%2520hypernetwork%2520architecture%2520trained%2520on%2520synthetic%250Aclassification%2520tasks%2520that%252C%2520once%2520prompted%2520with%2520a%2520never-seen-before%2520training%2520set%250Agenerates%2520the%2520weights%2520of%2520a%2520trained%2520%2560%2560child%2527%2527%2520neural-network%2520by%2520in-context%250Alearning%2520using%2520a%2520single%2520forward%2520pass.%2520In%2520contrast%2520to%2520most%2520existing%250Ahypernetworks%2520that%2520are%2520usually%2520trained%2520for%2520relatively%2520constrained%2520multi-task%250Asettings%252C%2520MotherNet%2520can%2520create%2520models%2520for%2520multiclass%2520classification%2520on%250Aarbitrary%2520tabular%2520datasets%2520without%2520any%2520dataset%2520specific%2520gradient%2520descent.%2520The%250Achild%2520network%2520generated%2520by%2520MotherNet%2520outperforms%2520neural%2520networks%2520trained%2520using%250Agradient%2520descent%2520on%2520small%2520datasets%252C%2520and%2520is%2520comparable%2520to%2520predictions%2520by%2520TabPFN%250Aand%2520standard%2520ML%2520methods%2520like%2520Gradient%2520Boosting.%2520Unlike%2520a%2520direct%2520application%2520of%250ATabPFN%252C%2520MotherNet%2520generated%2520networks%2520are%2520highly%2520efficient%2520at%2520inference%2520time.%2520We%250Aalso%2520demonstrate%2520that%2520HyperFast%2520is%2520unable%2520to%2520perform%2520effective%2520in-context%250Alearning%2520on%2520small%2520datasets%252C%2520and%2520heavily%2520relies%2520on%2520dataset%2520specific%2520fine-tuning%250Aand%2520hyper-parameter%2520tuning%252C%2520while%2520MotherNet%2520requires%2520no%2520fine-tuning%2520or%250Aper-dataset%2520hyper-parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotherNet%3A%20Fast%20Training%20and%20Inference%20via%20Hyper-Network%20Transformers&entry.906535625=Andreas%20M%C3%BCller%20and%20Carlo%20Curino%20and%20Raghu%20Ramakrishnan&entry.1292438233=%20%20Foundation%20models%20are%20transforming%20machine%20learning%20across%20many%20modalities%2C%0Awith%20in-context%20learning%20replacing%20classical%20model%20training.%20Recent%20work%20on%0Atabular%20data%20hints%20at%20a%20similar%20opportunity%20to%20build%20foundation%20models%20for%0Aclassification%20for%20numerical%20data.%20However%2C%20existing%20meta-learning%20approaches%0Acan%20not%20compete%20with%20tree-based%20methods%20in%20terms%20of%20inference%20time.%20In%20this%0Apaper%2C%20we%20propose%20MotherNet%2C%20a%20hypernetwork%20architecture%20trained%20on%20synthetic%0Aclassification%20tasks%20that%2C%20once%20prompted%20with%20a%20never-seen-before%20training%20set%0Agenerates%20the%20weights%20of%20a%20trained%20%60%60child%27%27%20neural-network%20by%20in-context%0Alearning%20using%20a%20single%20forward%20pass.%20In%20contrast%20to%20most%20existing%0Ahypernetworks%20that%20are%20usually%20trained%20for%20relatively%20constrained%20multi-task%0Asettings%2C%20MotherNet%20can%20create%20models%20for%20multiclass%20classification%20on%0Aarbitrary%20tabular%20datasets%20without%20any%20dataset%20specific%20gradient%20descent.%20The%0Achild%20network%20generated%20by%20MotherNet%20outperforms%20neural%20networks%20trained%20using%0Agradient%20descent%20on%20small%20datasets%2C%20and%20is%20comparable%20to%20predictions%20by%20TabPFN%0Aand%20standard%20ML%20methods%20like%20Gradient%20Boosting.%20Unlike%20a%20direct%20application%20of%0ATabPFN%2C%20MotherNet%20generated%20networks%20are%20highly%20efficient%20at%20inference%20time.%20We%0Aalso%20demonstrate%20that%20HyperFast%20is%20unable%20to%20perform%20effective%20in-context%0Alearning%20on%20small%20datasets%2C%20and%20heavily%20relies%20on%20dataset%20specific%20fine-tuning%0Aand%20hyper-parameter%20tuning%2C%20while%20MotherNet%20requires%20no%20fine-tuning%20or%0Aper-dataset%20hyper-parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08598v2&entry.124074799=Read"},
{"title": "Brain Hematoma Marker Recognition Using Multitask Learning:\n  SwinTransformer and Swin-Unet", "author": "Kodai Hirata and Tsuyoshi Okita", "abstract": "  This paper proposes a method MTL-Swin-Unet which is multi-task learning using\ntransformers for classification and semantic segmentation. For\nspurious-correlation problems, this method allows us to enhance the image\nrepresentation with two other image representations: representation obtained by\nsemantic segmentation and representation obtained by image reconstruction. In\nour experiments, the proposed method outperformed in F-value measure than other\nclassifiers when the test data included slices from the same patient (no\ncovariate shift). Similarly, when the test data did not include slices from the\nsame patient (covariate shift setting), the proposed method outperformed in AUC\nmeasure.\n", "link": "http://arxiv.org/abs/2505.06185v1", "date": "2025-05-09", "relevancy": 2.0289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5316}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Hematoma%20Marker%20Recognition%20Using%20Multitask%20Learning%3A%0A%20%20SwinTransformer%20and%20Swin-Unet&body=Title%3A%20Brain%20Hematoma%20Marker%20Recognition%20Using%20Multitask%20Learning%3A%0A%20%20SwinTransformer%20and%20Swin-Unet%0AAuthor%3A%20Kodai%20Hirata%20and%20Tsuyoshi%20Okita%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20method%20MTL-Swin-Unet%20which%20is%20multi-task%20learning%20using%0Atransformers%20for%20classification%20and%20semantic%20segmentation.%20For%0Aspurious-correlation%20problems%2C%20this%20method%20allows%20us%20to%20enhance%20the%20image%0Arepresentation%20with%20two%20other%20image%20representations%3A%20representation%20obtained%20by%0Asemantic%20segmentation%20and%20representation%20obtained%20by%20image%20reconstruction.%20In%0Aour%20experiments%2C%20the%20proposed%20method%20outperformed%20in%20F-value%20measure%20than%20other%0Aclassifiers%20when%20the%20test%20data%20included%20slices%20from%20the%20same%20patient%20%28no%0Acovariate%20shift%29.%20Similarly%2C%20when%20the%20test%20data%20did%20not%20include%20slices%20from%20the%0Asame%20patient%20%28covariate%20shift%20setting%29%2C%20the%20proposed%20method%20outperformed%20in%20AUC%0Ameasure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Hematoma%2520Marker%2520Recognition%2520Using%2520Multitask%2520Learning%253A%250A%2520%2520SwinTransformer%2520and%2520Swin-Unet%26entry.906535625%3DKodai%2520Hirata%2520and%2520Tsuyoshi%2520Okita%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520method%2520MTL-Swin-Unet%2520which%2520is%2520multi-task%2520learning%2520using%250Atransformers%2520for%2520classification%2520and%2520semantic%2520segmentation.%2520For%250Aspurious-correlation%2520problems%252C%2520this%2520method%2520allows%2520us%2520to%2520enhance%2520the%2520image%250Arepresentation%2520with%2520two%2520other%2520image%2520representations%253A%2520representation%2520obtained%2520by%250Asemantic%2520segmentation%2520and%2520representation%2520obtained%2520by%2520image%2520reconstruction.%2520In%250Aour%2520experiments%252C%2520the%2520proposed%2520method%2520outperformed%2520in%2520F-value%2520measure%2520than%2520other%250Aclassifiers%2520when%2520the%2520test%2520data%2520included%2520slices%2520from%2520the%2520same%2520patient%2520%2528no%250Acovariate%2520shift%2529.%2520Similarly%252C%2520when%2520the%2520test%2520data%2520did%2520not%2520include%2520slices%2520from%2520the%250Asame%2520patient%2520%2528covariate%2520shift%2520setting%2529%252C%2520the%2520proposed%2520method%2520outperformed%2520in%2520AUC%250Ameasure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Hematoma%20Marker%20Recognition%20Using%20Multitask%20Learning%3A%0A%20%20SwinTransformer%20and%20Swin-Unet&entry.906535625=Kodai%20Hirata%20and%20Tsuyoshi%20Okita&entry.1292438233=%20%20This%20paper%20proposes%20a%20method%20MTL-Swin-Unet%20which%20is%20multi-task%20learning%20using%0Atransformers%20for%20classification%20and%20semantic%20segmentation.%20For%0Aspurious-correlation%20problems%2C%20this%20method%20allows%20us%20to%20enhance%20the%20image%0Arepresentation%20with%20two%20other%20image%20representations%3A%20representation%20obtained%20by%0Asemantic%20segmentation%20and%20representation%20obtained%20by%20image%20reconstruction.%20In%0Aour%20experiments%2C%20the%20proposed%20method%20outperformed%20in%20F-value%20measure%20than%20other%0Aclassifiers%20when%20the%20test%20data%20included%20slices%20from%20the%20same%20patient%20%28no%0Acovariate%20shift%29.%20Similarly%2C%20when%20the%20test%20data%20did%20not%20include%20slices%20from%20the%0Asame%20patient%20%28covariate%20shift%20setting%29%2C%20the%20proposed%20method%20outperformed%20in%20AUC%0Ameasure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06185v1&entry.124074799=Read"},
{"title": "Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling\n  Constructs", "author": "Chico Sundermann and Stefan Vill and Elias Kuiter and Sebastian Krieter and Thomas Th\u00fcm and Matthias Tichy", "abstract": "  Configurable systems typically consist of reusable assets that have\ndependencies between each other. To specify such dependencies, feature models\nare commonly used. As feature models in practice are often complex, automated\nreasoning is typically employed to analyze the dependencies. Here, the de facto\nstandard is translating the feature model to conjunctive normal form (CNF) to\nenable employing off-the-shelf tools, such as SAT or #SAT solvers. However,\nmodern feature-modeling dialects often contain constructs, such as cardinality\nconstraints, that are ill-suited for conversion to CNF. This mismatch between\nthe input of reasoning engines and the available feature-modeling dialects\nlimits the applicability of the more expressive constructs. In this work, we\nshorten this gap between expressive constructs and scalable automated\nreasoning. Our contribution is twofold: First, we provide a pseudo-Boolean\nencoding for feature models, which facilitates smaller representations of\ncommonly employed constructs compared to Boolean encoding. Second, we propose a\nnovel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the\ncompiled d-DNNFs, we can resort to a plethora of efficient analyses already\nused in feature modeling. Our empirical evaluation shows that our proposal\nsubstantially outperforms the state-of-the-art based on CNF inputs for\nexpressive constructs. For every considered dataset representing different\nfeature models and feature-modeling constructs, the feature models can be\nsignificantly faster translated to pseudo-Boolean than to CNF. Overall,\nderiving d-DNNFs from a feature model with the targeted expressive constraints\ncan be substantially accelerated using our pseudo-Boolean approach.\nFurthermore, our approach is competitive on feature models with only basic\nconstructs.\n", "link": "http://arxiv.org/abs/2505.05976v1", "date": "2025-05-09", "relevancy": 2.0283, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Boolean%20d-DNNF%20Compilation%20for%20Expressive%20Feature%20Modeling%0A%20%20Constructs&body=Title%3A%20Pseudo-Boolean%20d-DNNF%20Compilation%20for%20Expressive%20Feature%20Modeling%0A%20%20Constructs%0AAuthor%3A%20Chico%20Sundermann%20and%20Stefan%20Vill%20and%20Elias%20Kuiter%20and%20Sebastian%20Krieter%20and%20Thomas%20Th%C3%BCm%20and%20Matthias%20Tichy%0AAbstract%3A%20%20%20Configurable%20systems%20typically%20consist%20of%20reusable%20assets%20that%20have%0Adependencies%20between%20each%20other.%20To%20specify%20such%20dependencies%2C%20feature%20models%0Aare%20commonly%20used.%20As%20feature%20models%20in%20practice%20are%20often%20complex%2C%20automated%0Areasoning%20is%20typically%20employed%20to%20analyze%20the%20dependencies.%20Here%2C%20the%20de%20facto%0Astandard%20is%20translating%20the%20feature%20model%20to%20conjunctive%20normal%20form%20%28CNF%29%20to%0Aenable%20employing%20off-the-shelf%20tools%2C%20such%20as%20SAT%20or%20%23SAT%20solvers.%20However%2C%0Amodern%20feature-modeling%20dialects%20often%20contain%20constructs%2C%20such%20as%20cardinality%0Aconstraints%2C%20that%20are%20ill-suited%20for%20conversion%20to%20CNF.%20This%20mismatch%20between%0Athe%20input%20of%20reasoning%20engines%20and%20the%20available%20feature-modeling%20dialects%0Alimits%20the%20applicability%20of%20the%20more%20expressive%20constructs.%20In%20this%20work%2C%20we%0Ashorten%20this%20gap%20between%20expressive%20constructs%20and%20scalable%20automated%0Areasoning.%20Our%20contribution%20is%20twofold%3A%20First%2C%20we%20provide%20a%20pseudo-Boolean%0Aencoding%20for%20feature%20models%2C%20which%20facilitates%20smaller%20representations%20of%0Acommonly%20employed%20constructs%20compared%20to%20Boolean%20encoding.%20Second%2C%20we%20propose%20a%0Anovel%20method%20to%20compile%20pseudo-Boolean%20formulas%20to%20Boolean%20d-DNNF.%20With%20the%0Acompiled%20d-DNNFs%2C%20we%20can%20resort%20to%20a%20plethora%20of%20efficient%20analyses%20already%0Aused%20in%20feature%20modeling.%20Our%20empirical%20evaluation%20shows%20that%20our%20proposal%0Asubstantially%20outperforms%20the%20state-of-the-art%20based%20on%20CNF%20inputs%20for%0Aexpressive%20constructs.%20For%20every%20considered%20dataset%20representing%20different%0Afeature%20models%20and%20feature-modeling%20constructs%2C%20the%20feature%20models%20can%20be%0Asignificantly%20faster%20translated%20to%20pseudo-Boolean%20than%20to%20CNF.%20Overall%2C%0Aderiving%20d-DNNFs%20from%20a%20feature%20model%20with%20the%20targeted%20expressive%20constraints%0Acan%20be%20substantially%20accelerated%20using%20our%20pseudo-Boolean%20approach.%0AFurthermore%2C%20our%20approach%20is%20competitive%20on%20feature%20models%20with%20only%20basic%0Aconstructs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Boolean%2520d-DNNF%2520Compilation%2520for%2520Expressive%2520Feature%2520Modeling%250A%2520%2520Constructs%26entry.906535625%3DChico%2520Sundermann%2520and%2520Stefan%2520Vill%2520and%2520Elias%2520Kuiter%2520and%2520Sebastian%2520Krieter%2520and%2520Thomas%2520Th%25C3%25BCm%2520and%2520Matthias%2520Tichy%26entry.1292438233%3D%2520%2520Configurable%2520systems%2520typically%2520consist%2520of%2520reusable%2520assets%2520that%2520have%250Adependencies%2520between%2520each%2520other.%2520To%2520specify%2520such%2520dependencies%252C%2520feature%2520models%250Aare%2520commonly%2520used.%2520As%2520feature%2520models%2520in%2520practice%2520are%2520often%2520complex%252C%2520automated%250Areasoning%2520is%2520typically%2520employed%2520to%2520analyze%2520the%2520dependencies.%2520Here%252C%2520the%2520de%2520facto%250Astandard%2520is%2520translating%2520the%2520feature%2520model%2520to%2520conjunctive%2520normal%2520form%2520%2528CNF%2529%2520to%250Aenable%2520employing%2520off-the-shelf%2520tools%252C%2520such%2520as%2520SAT%2520or%2520%2523SAT%2520solvers.%2520However%252C%250Amodern%2520feature-modeling%2520dialects%2520often%2520contain%2520constructs%252C%2520such%2520as%2520cardinality%250Aconstraints%252C%2520that%2520are%2520ill-suited%2520for%2520conversion%2520to%2520CNF.%2520This%2520mismatch%2520between%250Athe%2520input%2520of%2520reasoning%2520engines%2520and%2520the%2520available%2520feature-modeling%2520dialects%250Alimits%2520the%2520applicability%2520of%2520the%2520more%2520expressive%2520constructs.%2520In%2520this%2520work%252C%2520we%250Ashorten%2520this%2520gap%2520between%2520expressive%2520constructs%2520and%2520scalable%2520automated%250Areasoning.%2520Our%2520contribution%2520is%2520twofold%253A%2520First%252C%2520we%2520provide%2520a%2520pseudo-Boolean%250Aencoding%2520for%2520feature%2520models%252C%2520which%2520facilitates%2520smaller%2520representations%2520of%250Acommonly%2520employed%2520constructs%2520compared%2520to%2520Boolean%2520encoding.%2520Second%252C%2520we%2520propose%2520a%250Anovel%2520method%2520to%2520compile%2520pseudo-Boolean%2520formulas%2520to%2520Boolean%2520d-DNNF.%2520With%2520the%250Acompiled%2520d-DNNFs%252C%2520we%2520can%2520resort%2520to%2520a%2520plethora%2520of%2520efficient%2520analyses%2520already%250Aused%2520in%2520feature%2520modeling.%2520Our%2520empirical%2520evaluation%2520shows%2520that%2520our%2520proposal%250Asubstantially%2520outperforms%2520the%2520state-of-the-art%2520based%2520on%2520CNF%2520inputs%2520for%250Aexpressive%2520constructs.%2520For%2520every%2520considered%2520dataset%2520representing%2520different%250Afeature%2520models%2520and%2520feature-modeling%2520constructs%252C%2520the%2520feature%2520models%2520can%2520be%250Asignificantly%2520faster%2520translated%2520to%2520pseudo-Boolean%2520than%2520to%2520CNF.%2520Overall%252C%250Aderiving%2520d-DNNFs%2520from%2520a%2520feature%2520model%2520with%2520the%2520targeted%2520expressive%2520constraints%250Acan%2520be%2520substantially%2520accelerated%2520using%2520our%2520pseudo-Boolean%2520approach.%250AFurthermore%252C%2520our%2520approach%2520is%2520competitive%2520on%2520feature%2520models%2520with%2520only%2520basic%250Aconstructs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Boolean%20d-DNNF%20Compilation%20for%20Expressive%20Feature%20Modeling%0A%20%20Constructs&entry.906535625=Chico%20Sundermann%20and%20Stefan%20Vill%20and%20Elias%20Kuiter%20and%20Sebastian%20Krieter%20and%20Thomas%20Th%C3%BCm%20and%20Matthias%20Tichy&entry.1292438233=%20%20Configurable%20systems%20typically%20consist%20of%20reusable%20assets%20that%20have%0Adependencies%20between%20each%20other.%20To%20specify%20such%20dependencies%2C%20feature%20models%0Aare%20commonly%20used.%20As%20feature%20models%20in%20practice%20are%20often%20complex%2C%20automated%0Areasoning%20is%20typically%20employed%20to%20analyze%20the%20dependencies.%20Here%2C%20the%20de%20facto%0Astandard%20is%20translating%20the%20feature%20model%20to%20conjunctive%20normal%20form%20%28CNF%29%20to%0Aenable%20employing%20off-the-shelf%20tools%2C%20such%20as%20SAT%20or%20%23SAT%20solvers.%20However%2C%0Amodern%20feature-modeling%20dialects%20often%20contain%20constructs%2C%20such%20as%20cardinality%0Aconstraints%2C%20that%20are%20ill-suited%20for%20conversion%20to%20CNF.%20This%20mismatch%20between%0Athe%20input%20of%20reasoning%20engines%20and%20the%20available%20feature-modeling%20dialects%0Alimits%20the%20applicability%20of%20the%20more%20expressive%20constructs.%20In%20this%20work%2C%20we%0Ashorten%20this%20gap%20between%20expressive%20constructs%20and%20scalable%20automated%0Areasoning.%20Our%20contribution%20is%20twofold%3A%20First%2C%20we%20provide%20a%20pseudo-Boolean%0Aencoding%20for%20feature%20models%2C%20which%20facilitates%20smaller%20representations%20of%0Acommonly%20employed%20constructs%20compared%20to%20Boolean%20encoding.%20Second%2C%20we%20propose%20a%0Anovel%20method%20to%20compile%20pseudo-Boolean%20formulas%20to%20Boolean%20d-DNNF.%20With%20the%0Acompiled%20d-DNNFs%2C%20we%20can%20resort%20to%20a%20plethora%20of%20efficient%20analyses%20already%0Aused%20in%20feature%20modeling.%20Our%20empirical%20evaluation%20shows%20that%20our%20proposal%0Asubstantially%20outperforms%20the%20state-of-the-art%20based%20on%20CNF%20inputs%20for%0Aexpressive%20constructs.%20For%20every%20considered%20dataset%20representing%20different%0Afeature%20models%20and%20feature-modeling%20constructs%2C%20the%20feature%20models%20can%20be%0Asignificantly%20faster%20translated%20to%20pseudo-Boolean%20than%20to%20CNF.%20Overall%2C%0Aderiving%20d-DNNFs%20from%20a%20feature%20model%20with%20the%20targeted%20expressive%20constraints%0Acan%20be%20substantially%20accelerated%20using%20our%20pseudo-Boolean%20approach.%0AFurthermore%2C%20our%20approach%20is%20competitive%20on%20feature%20models%20with%20only%20basic%0Aconstructs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05976v1&entry.124074799=Read"},
{"title": "Towards Robust Few-Shot Text Classification Using Transformer\n  Architectures and Dual Loss Strategies", "author": "Xu Han and Yumeng Sun and Weiqiang Huang and Hongye Zheng and Junliang Du", "abstract": "  Few-shot text classification has important application value in low-resource\nenvironments. This paper proposes a strategy that combines adaptive\nfine-tuning, contrastive learning, and regularization optimization to improve\nthe classification performance of Transformer-based models. Experiments on the\nFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform\nwell in few-shot tasks, especially in the 5-shot setting, which can more\neffectively capture text features and improve classification accuracy. The\nexperiment also found that there are significant differences in the\nclassification difficulty of different relationship categories. Some categories\nhave fuzzy semantic boundaries or complex feature distributions, making it\ndifficult for the standard cross entropy loss to learn the discriminative\ninformation required to distinguish categories. By introducing contrastive loss\nand regularization loss, the generalization ability of the model is enhanced,\neffectively alleviating the overfitting problem in few-shot environments. In\naddition, the research results show that the use of Transformer models or\ngenerative architectures with stronger self-attention mechanisms can help\nimprove the stability and accuracy of few-shot classification.\n", "link": "http://arxiv.org/abs/2505.06145v1", "date": "2025-05-09", "relevancy": 2.0272, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Few-Shot%20Text%20Classification%20Using%20Transformer%0A%20%20Architectures%20and%20Dual%20Loss%20Strategies&body=Title%3A%20Towards%20Robust%20Few-Shot%20Text%20Classification%20Using%20Transformer%0A%20%20Architectures%20and%20Dual%20Loss%20Strategies%0AAuthor%3A%20Xu%20Han%20and%20Yumeng%20Sun%20and%20Weiqiang%20Huang%20and%20Hongye%20Zheng%20and%20Junliang%20Du%0AAbstract%3A%20%20%20Few-shot%20text%20classification%20has%20important%20application%20value%20in%20low-resource%0Aenvironments.%20This%20paper%20proposes%20a%20strategy%20that%20combines%20adaptive%0Afine-tuning%2C%20contrastive%20learning%2C%20and%20regularization%20optimization%20to%20improve%0Athe%20classification%20performance%20of%20Transformer-based%20models.%20Experiments%20on%20the%0AFewRel%202.0%20dataset%20show%20that%20T5-small%2C%20DeBERTa-v3%2C%20and%20RoBERTa-base%20perform%0Awell%20in%20few-shot%20tasks%2C%20especially%20in%20the%205-shot%20setting%2C%20which%20can%20more%0Aeffectively%20capture%20text%20features%20and%20improve%20classification%20accuracy.%20The%0Aexperiment%20also%20found%20that%20there%20are%20significant%20differences%20in%20the%0Aclassification%20difficulty%20of%20different%20relationship%20categories.%20Some%20categories%0Ahave%20fuzzy%20semantic%20boundaries%20or%20complex%20feature%20distributions%2C%20making%20it%0Adifficult%20for%20the%20standard%20cross%20entropy%20loss%20to%20learn%20the%20discriminative%0Ainformation%20required%20to%20distinguish%20categories.%20By%20introducing%20contrastive%20loss%0Aand%20regularization%20loss%2C%20the%20generalization%20ability%20of%20the%20model%20is%20enhanced%2C%0Aeffectively%20alleviating%20the%20overfitting%20problem%20in%20few-shot%20environments.%20In%0Aaddition%2C%20the%20research%20results%20show%20that%20the%20use%20of%20Transformer%20models%20or%0Agenerative%20architectures%20with%20stronger%20self-attention%20mechanisms%20can%20help%0Aimprove%20the%20stability%20and%20accuracy%20of%20few-shot%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Few-Shot%2520Text%2520Classification%2520Using%2520Transformer%250A%2520%2520Architectures%2520and%2520Dual%2520Loss%2520Strategies%26entry.906535625%3DXu%2520Han%2520and%2520Yumeng%2520Sun%2520and%2520Weiqiang%2520Huang%2520and%2520Hongye%2520Zheng%2520and%2520Junliang%2520Du%26entry.1292438233%3D%2520%2520Few-shot%2520text%2520classification%2520has%2520important%2520application%2520value%2520in%2520low-resource%250Aenvironments.%2520This%2520paper%2520proposes%2520a%2520strategy%2520that%2520combines%2520adaptive%250Afine-tuning%252C%2520contrastive%2520learning%252C%2520and%2520regularization%2520optimization%2520to%2520improve%250Athe%2520classification%2520performance%2520of%2520Transformer-based%2520models.%2520Experiments%2520on%2520the%250AFewRel%25202.0%2520dataset%2520show%2520that%2520T5-small%252C%2520DeBERTa-v3%252C%2520and%2520RoBERTa-base%2520perform%250Awell%2520in%2520few-shot%2520tasks%252C%2520especially%2520in%2520the%25205-shot%2520setting%252C%2520which%2520can%2520more%250Aeffectively%2520capture%2520text%2520features%2520and%2520improve%2520classification%2520accuracy.%2520The%250Aexperiment%2520also%2520found%2520that%2520there%2520are%2520significant%2520differences%2520in%2520the%250Aclassification%2520difficulty%2520of%2520different%2520relationship%2520categories.%2520Some%2520categories%250Ahave%2520fuzzy%2520semantic%2520boundaries%2520or%2520complex%2520feature%2520distributions%252C%2520making%2520it%250Adifficult%2520for%2520the%2520standard%2520cross%2520entropy%2520loss%2520to%2520learn%2520the%2520discriminative%250Ainformation%2520required%2520to%2520distinguish%2520categories.%2520By%2520introducing%2520contrastive%2520loss%250Aand%2520regularization%2520loss%252C%2520the%2520generalization%2520ability%2520of%2520the%2520model%2520is%2520enhanced%252C%250Aeffectively%2520alleviating%2520the%2520overfitting%2520problem%2520in%2520few-shot%2520environments.%2520In%250Aaddition%252C%2520the%2520research%2520results%2520show%2520that%2520the%2520use%2520of%2520Transformer%2520models%2520or%250Agenerative%2520architectures%2520with%2520stronger%2520self-attention%2520mechanisms%2520can%2520help%250Aimprove%2520the%2520stability%2520and%2520accuracy%2520of%2520few-shot%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Few-Shot%20Text%20Classification%20Using%20Transformer%0A%20%20Architectures%20and%20Dual%20Loss%20Strategies&entry.906535625=Xu%20Han%20and%20Yumeng%20Sun%20and%20Weiqiang%20Huang%20and%20Hongye%20Zheng%20and%20Junliang%20Du&entry.1292438233=%20%20Few-shot%20text%20classification%20has%20important%20application%20value%20in%20low-resource%0Aenvironments.%20This%20paper%20proposes%20a%20strategy%20that%20combines%20adaptive%0Afine-tuning%2C%20contrastive%20learning%2C%20and%20regularization%20optimization%20to%20improve%0Athe%20classification%20performance%20of%20Transformer-based%20models.%20Experiments%20on%20the%0AFewRel%202.0%20dataset%20show%20that%20T5-small%2C%20DeBERTa-v3%2C%20and%20RoBERTa-base%20perform%0Awell%20in%20few-shot%20tasks%2C%20especially%20in%20the%205-shot%20setting%2C%20which%20can%20more%0Aeffectively%20capture%20text%20features%20and%20improve%20classification%20accuracy.%20The%0Aexperiment%20also%20found%20that%20there%20are%20significant%20differences%20in%20the%0Aclassification%20difficulty%20of%20different%20relationship%20categories.%20Some%20categories%0Ahave%20fuzzy%20semantic%20boundaries%20or%20complex%20feature%20distributions%2C%20making%20it%0Adifficult%20for%20the%20standard%20cross%20entropy%20loss%20to%20learn%20the%20discriminative%0Ainformation%20required%20to%20distinguish%20categories.%20By%20introducing%20contrastive%20loss%0Aand%20regularization%20loss%2C%20the%20generalization%20ability%20of%20the%20model%20is%20enhanced%2C%0Aeffectively%20alleviating%20the%20overfitting%20problem%20in%20few-shot%20environments.%20In%0Aaddition%2C%20the%20research%20results%20show%20that%20the%20use%20of%20Transformer%20models%20or%0Agenerative%20architectures%20with%20stronger%20self-attention%20mechanisms%20can%20help%0Aimprove%20the%20stability%20and%20accuracy%20of%20few-shot%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06145v1&entry.124074799=Read"},
{"title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models", "author": "Bang Zhang and Ruotian Ma and Qingxuan Jiang and Peisong Wang and Jiaqi Chen and Zheng Xie and Xingyu Chen and Yue Wang and Fanghua Ye and Jian Li and Yifan Yang and Zhaopeng Tu and Xiaolong Li", "abstract": "  Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.\n", "link": "http://arxiv.org/abs/2505.02847v2", "date": "2025-05-09", "relevancy": 2.0234, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sentient%20Agent%20as%20a%20Judge%3A%20Evaluating%20Higher-Order%20Social%20Cognition%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Sentient%20Agent%20as%20a%20Judge%3A%20Evaluating%20Higher-Order%20Social%20Cognition%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Bang%20Zhang%20and%20Ruotian%20Ma%20and%20Qingxuan%20Jiang%20and%20Peisong%20Wang%20and%20Jiaqi%20Chen%20and%20Zheng%20Xie%20and%20Xingyu%20Chen%20and%20Yue%20Wang%20and%20Fanghua%20Ye%20and%20Jian%20Li%20and%20Yifan%20Yang%20and%20Zhaopeng%20Tu%20and%20Xiaolong%20Li%0AAbstract%3A%20%20%20Assessing%20how%20well%20a%20large%20language%20model%20%28LLM%29%20understands%20human%2C%20rather%0Athan%20merely%20text%2C%20remains%20an%20open%20challenge.%20To%20bridge%20the%20gap%2C%20we%20introduce%0ASentient%20Agent%20as%20a%20Judge%20%28SAGE%29%2C%20an%20automated%20evaluation%20framework%20that%0Ameasures%20an%20LLM%27s%20higher-order%20social%20cognition.%20SAGE%20instantiates%20a%20Sentient%0AAgent%20that%20simulates%20human-like%20emotional%20changes%20and%20inner%20thoughts%20during%0Ainteraction%2C%20providing%20a%20more%20realistic%20evaluation%20of%20the%20tested%20model%20in%0Amulti-turn%20conversations.%20At%20every%20turn%2C%20the%20agent%20reasons%20about%20%28i%29%20how%20its%0Aemotion%20changes%2C%20%28ii%29%20how%20it%20feels%2C%20and%20%28iii%29%20how%20it%20should%20reply%2C%20yielding%20a%0Anumerical%20emotion%20trajectory%20and%20interpretable%20inner%20thoughts.%20Experiments%20on%0A100%20supportive-dialogue%20scenarios%20show%20that%20the%20final%20Sentient%20emotion%20score%0Acorrelates%20strongly%20with%20Barrett-Lennard%20Relationship%20Inventory%20%28BLRI%29%20ratings%0Aand%20utterance-level%20empathy%20metrics%2C%20validating%20psychological%20fidelity.%20We%20also%0Abuild%20a%20public%20Sentient%20Leaderboard%20covering%2018%20commercial%20and%20open-source%0Amodels%20that%20uncovers%20substantial%20gaps%20%28up%20to%204x%29%20between%20frontier%20systems%0A%28GPT-4o-Latest%2C%20Gemini2.5-Pro%29%20and%20earlier%20baselines%2C%20gaps%20not%20reflected%20in%0Aconventional%20leaderboards%20%28e.g.%2C%20Arena%29.%20SAGE%20thus%20provides%20a%20principled%2C%0Ascalable%20and%20interpretable%20tool%20for%20tracking%20progress%20toward%20genuinely%0Aempathetic%20and%20socially%20adept%20language%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSentient%2520Agent%2520as%2520a%2520Judge%253A%2520Evaluating%2520Higher-Order%2520Social%2520Cognition%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DBang%2520Zhang%2520and%2520Ruotian%2520Ma%2520and%2520Qingxuan%2520Jiang%2520and%2520Peisong%2520Wang%2520and%2520Jiaqi%2520Chen%2520and%2520Zheng%2520Xie%2520and%2520Xingyu%2520Chen%2520and%2520Yue%2520Wang%2520and%2520Fanghua%2520Ye%2520and%2520Jian%2520Li%2520and%2520Yifan%2520Yang%2520and%2520Zhaopeng%2520Tu%2520and%2520Xiaolong%2520Li%26entry.1292438233%3D%2520%2520Assessing%2520how%2520well%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520understands%2520human%252C%2520rather%250Athan%2520merely%2520text%252C%2520remains%2520an%2520open%2520challenge.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520introduce%250ASentient%2520Agent%2520as%2520a%2520Judge%2520%2528SAGE%2529%252C%2520an%2520automated%2520evaluation%2520framework%2520that%250Ameasures%2520an%2520LLM%2527s%2520higher-order%2520social%2520cognition.%2520SAGE%2520instantiates%2520a%2520Sentient%250AAgent%2520that%2520simulates%2520human-like%2520emotional%2520changes%2520and%2520inner%2520thoughts%2520during%250Ainteraction%252C%2520providing%2520a%2520more%2520realistic%2520evaluation%2520of%2520the%2520tested%2520model%2520in%250Amulti-turn%2520conversations.%2520At%2520every%2520turn%252C%2520the%2520agent%2520reasons%2520about%2520%2528i%2529%2520how%2520its%250Aemotion%2520changes%252C%2520%2528ii%2529%2520how%2520it%2520feels%252C%2520and%2520%2528iii%2529%2520how%2520it%2520should%2520reply%252C%2520yielding%2520a%250Anumerical%2520emotion%2520trajectory%2520and%2520interpretable%2520inner%2520thoughts.%2520Experiments%2520on%250A100%2520supportive-dialogue%2520scenarios%2520show%2520that%2520the%2520final%2520Sentient%2520emotion%2520score%250Acorrelates%2520strongly%2520with%2520Barrett-Lennard%2520Relationship%2520Inventory%2520%2528BLRI%2529%2520ratings%250Aand%2520utterance-level%2520empathy%2520metrics%252C%2520validating%2520psychological%2520fidelity.%2520We%2520also%250Abuild%2520a%2520public%2520Sentient%2520Leaderboard%2520covering%252018%2520commercial%2520and%2520open-source%250Amodels%2520that%2520uncovers%2520substantial%2520gaps%2520%2528up%2520to%25204x%2529%2520between%2520frontier%2520systems%250A%2528GPT-4o-Latest%252C%2520Gemini2.5-Pro%2529%2520and%2520earlier%2520baselines%252C%2520gaps%2520not%2520reflected%2520in%250Aconventional%2520leaderboards%2520%2528e.g.%252C%2520Arena%2529.%2520SAGE%2520thus%2520provides%2520a%2520principled%252C%250Ascalable%2520and%2520interpretable%2520tool%2520for%2520tracking%2520progress%2520toward%2520genuinely%250Aempathetic%2520and%2520socially%2520adept%2520language%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sentient%20Agent%20as%20a%20Judge%3A%20Evaluating%20Higher-Order%20Social%20Cognition%20in%0A%20%20Large%20Language%20Models&entry.906535625=Bang%20Zhang%20and%20Ruotian%20Ma%20and%20Qingxuan%20Jiang%20and%20Peisong%20Wang%20and%20Jiaqi%20Chen%20and%20Zheng%20Xie%20and%20Xingyu%20Chen%20and%20Yue%20Wang%20and%20Fanghua%20Ye%20and%20Jian%20Li%20and%20Yifan%20Yang%20and%20Zhaopeng%20Tu%20and%20Xiaolong%20Li&entry.1292438233=%20%20Assessing%20how%20well%20a%20large%20language%20model%20%28LLM%29%20understands%20human%2C%20rather%0Athan%20merely%20text%2C%20remains%20an%20open%20challenge.%20To%20bridge%20the%20gap%2C%20we%20introduce%0ASentient%20Agent%20as%20a%20Judge%20%28SAGE%29%2C%20an%20automated%20evaluation%20framework%20that%0Ameasures%20an%20LLM%27s%20higher-order%20social%20cognition.%20SAGE%20instantiates%20a%20Sentient%0AAgent%20that%20simulates%20human-like%20emotional%20changes%20and%20inner%20thoughts%20during%0Ainteraction%2C%20providing%20a%20more%20realistic%20evaluation%20of%20the%20tested%20model%20in%0Amulti-turn%20conversations.%20At%20every%20turn%2C%20the%20agent%20reasons%20about%20%28i%29%20how%20its%0Aemotion%20changes%2C%20%28ii%29%20how%20it%20feels%2C%20and%20%28iii%29%20how%20it%20should%20reply%2C%20yielding%20a%0Anumerical%20emotion%20trajectory%20and%20interpretable%20inner%20thoughts.%20Experiments%20on%0A100%20supportive-dialogue%20scenarios%20show%20that%20the%20final%20Sentient%20emotion%20score%0Acorrelates%20strongly%20with%20Barrett-Lennard%20Relationship%20Inventory%20%28BLRI%29%20ratings%0Aand%20utterance-level%20empathy%20metrics%2C%20validating%20psychological%20fidelity.%20We%20also%0Abuild%20a%20public%20Sentient%20Leaderboard%20covering%2018%20commercial%20and%20open-source%0Amodels%20that%20uncovers%20substantial%20gaps%20%28up%20to%204x%29%20between%20frontier%20systems%0A%28GPT-4o-Latest%2C%20Gemini2.5-Pro%29%20and%20earlier%20baselines%2C%20gaps%20not%20reflected%20in%0Aconventional%20leaderboards%20%28e.g.%2C%20Arena%29.%20SAGE%20thus%20provides%20a%20principled%2C%0Ascalable%20and%20interpretable%20tool%20for%20tracking%20progress%20toward%20genuinely%0Aempathetic%20and%20socially%20adept%20language%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02847v2&entry.124074799=Read"},
{"title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation", "author": "Stefan Vasilev and Christian Herold and Baohao Liao and Seyyed Hadi Hashemi and Shahram Khadivi and Christof Monz", "abstract": "  This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.\n", "link": "http://arxiv.org/abs/2505.06027v1", "date": "2025-05-09", "relevancy": 2.0088, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5429}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5053}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unilogit%3A%20Robust%20Machine%20Unlearning%20for%20LLMs%20Using%20Uniform-Target%0A%20%20Self-Distillation&body=Title%3A%20Unilogit%3A%20Robust%20Machine%20Unlearning%20for%20LLMs%20Using%20Uniform-Target%0A%20%20Self-Distillation%0AAuthor%3A%20Stefan%20Vasilev%20and%20Christian%20Herold%20and%20Baohao%20Liao%20and%20Seyyed%20Hadi%20Hashemi%20and%20Shahram%20Khadivi%20and%20Christof%20Monz%0AAbstract%3A%20%20%20This%20paper%20introduces%20Unilogit%2C%20a%20novel%20self-distillation%20method%20for%20machine%0Aunlearning%20in%20Large%20Language%20Models.%20Unilogit%20addresses%20the%20challenge%20of%0Aselectively%20forgetting%20specific%20information%20while%20maintaining%20overall%20model%0Autility%2C%20a%20critical%20task%20in%20compliance%20with%20data%20privacy%20regulations%20like%20GDPR.%0AUnlike%20prior%20methods%20that%20rely%20on%20static%20hyperparameters%20or%20starting%20model%0Aoutputs%2C%20Unilogit%20dynamically%20adjusts%20target%20logits%20to%20achieve%20a%20uniform%0Aprobability%20for%20the%20target%20token%2C%20leveraging%20the%20current%20model%27s%20outputs%20for%0Amore%20accurate%20self-distillation%20targets.%20This%20approach%20not%20only%20eliminates%20the%0Aneed%20for%20additional%20hyperparameters%20but%20also%20enhances%20the%20model%27s%20ability%20to%0Aapproximate%20the%20golden%20targets.%20Extensive%20experiments%20on%20public%20benchmarks%20and%0Aan%20in-house%20e-commerce%20dataset%20demonstrate%20Unilogit%27s%20superior%20performance%20in%0Abalancing%20forget%20and%20retain%20objectives%2C%20outperforming%20state-of-the-art%20methods%0Asuch%20as%20NPO%20and%20UnDIAL.%20Our%20analysis%20further%20reveals%20Unilogit%27s%20robustness%0Aacross%20various%20scenarios%2C%20highlighting%20its%20practical%20applicability%20and%0Aeffectiveness%20in%20achieving%20efficacious%20machine%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnilogit%253A%2520Robust%2520Machine%2520Unlearning%2520for%2520LLMs%2520Using%2520Uniform-Target%250A%2520%2520Self-Distillation%26entry.906535625%3DStefan%2520Vasilev%2520and%2520Christian%2520Herold%2520and%2520Baohao%2520Liao%2520and%2520Seyyed%2520Hadi%2520Hashemi%2520and%2520Shahram%2520Khadivi%2520and%2520Christof%2520Monz%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Unilogit%252C%2520a%2520novel%2520self-distillation%2520method%2520for%2520machine%250Aunlearning%2520in%2520Large%2520Language%2520Models.%2520Unilogit%2520addresses%2520the%2520challenge%2520of%250Aselectively%2520forgetting%2520specific%2520information%2520while%2520maintaining%2520overall%2520model%250Autility%252C%2520a%2520critical%2520task%2520in%2520compliance%2520with%2520data%2520privacy%2520regulations%2520like%2520GDPR.%250AUnlike%2520prior%2520methods%2520that%2520rely%2520on%2520static%2520hyperparameters%2520or%2520starting%2520model%250Aoutputs%252C%2520Unilogit%2520dynamically%2520adjusts%2520target%2520logits%2520to%2520achieve%2520a%2520uniform%250Aprobability%2520for%2520the%2520target%2520token%252C%2520leveraging%2520the%2520current%2520model%2527s%2520outputs%2520for%250Amore%2520accurate%2520self-distillation%2520targets.%2520This%2520approach%2520not%2520only%2520eliminates%2520the%250Aneed%2520for%2520additional%2520hyperparameters%2520but%2520also%2520enhances%2520the%2520model%2527s%2520ability%2520to%250Aapproximate%2520the%2520golden%2520targets.%2520Extensive%2520experiments%2520on%2520public%2520benchmarks%2520and%250Aan%2520in-house%2520e-commerce%2520dataset%2520demonstrate%2520Unilogit%2527s%2520superior%2520performance%2520in%250Abalancing%2520forget%2520and%2520retain%2520objectives%252C%2520outperforming%2520state-of-the-art%2520methods%250Asuch%2520as%2520NPO%2520and%2520UnDIAL.%2520Our%2520analysis%2520further%2520reveals%2520Unilogit%2527s%2520robustness%250Aacross%2520various%2520scenarios%252C%2520highlighting%2520its%2520practical%2520applicability%2520and%250Aeffectiveness%2520in%2520achieving%2520efficacious%2520machine%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unilogit%3A%20Robust%20Machine%20Unlearning%20for%20LLMs%20Using%20Uniform-Target%0A%20%20Self-Distillation&entry.906535625=Stefan%20Vasilev%20and%20Christian%20Herold%20and%20Baohao%20Liao%20and%20Seyyed%20Hadi%20Hashemi%20and%20Shahram%20Khadivi%20and%20Christof%20Monz&entry.1292438233=%20%20This%20paper%20introduces%20Unilogit%2C%20a%20novel%20self-distillation%20method%20for%20machine%0Aunlearning%20in%20Large%20Language%20Models.%20Unilogit%20addresses%20the%20challenge%20of%0Aselectively%20forgetting%20specific%20information%20while%20maintaining%20overall%20model%0Autility%2C%20a%20critical%20task%20in%20compliance%20with%20data%20privacy%20regulations%20like%20GDPR.%0AUnlike%20prior%20methods%20that%20rely%20on%20static%20hyperparameters%20or%20starting%20model%0Aoutputs%2C%20Unilogit%20dynamically%20adjusts%20target%20logits%20to%20achieve%20a%20uniform%0Aprobability%20for%20the%20target%20token%2C%20leveraging%20the%20current%20model%27s%20outputs%20for%0Amore%20accurate%20self-distillation%20targets.%20This%20approach%20not%20only%20eliminates%20the%0Aneed%20for%20additional%20hyperparameters%20but%20also%20enhances%20the%20model%27s%20ability%20to%0Aapproximate%20the%20golden%20targets.%20Extensive%20experiments%20on%20public%20benchmarks%20and%0Aan%20in-house%20e-commerce%20dataset%20demonstrate%20Unilogit%27s%20superior%20performance%20in%0Abalancing%20forget%20and%20retain%20objectives%2C%20outperforming%20state-of-the-art%20methods%0Asuch%20as%20NPO%20and%20UnDIAL.%20Our%20analysis%20further%20reveals%20Unilogit%27s%20robustness%0Aacross%20various%20scenarios%2C%20highlighting%20its%20practical%20applicability%20and%0Aeffectiveness%20in%20achieving%20efficacious%20machine%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06027v1&entry.124074799=Read"},
{"title": "Exact Imposition of Safety Boundary Conditions in Neural Reachable Tubes", "author": "Aditya Singh and Zeyuan Feng and Somil Bansal", "abstract": "  Hamilton-Jacobi (HJ) reachability analysis is a widely adopted verification\ntool to provide safety and performance guarantees for autonomous systems.\nHowever, it involves solving a partial differential equation (PDE) to compute a\nsafety value function, whose computational and memory complexity scales\nexponentially with the state dimension, making its direct application to\nlarge-scale systems intractable. To overcome these challenges, DeepReach, a\nrecently proposed learning-based approach, approximates high-dimensional\nreachable tubes using neural networks (NNs). While shown to be effective, the\naccuracy of the learned solution decreases with system complexity. One of the\nreasons for this degradation is a soft imposition of safety constraints during\nthe learning process, which corresponds to the boundary conditions of the PDE,\nresulting in inaccurate value functions. In this work, we propose ExactBC, a\nvariant of DeepReach that imposes safety constraints exactly during the\nlearning process by restructuring the overall value function as a weighted sum\nof the boundary condition and the NN output. Moreover, the proposed variant no\nlonger needs a boundary loss term during the training process, thus eliminating\nthe need to balance different loss terms. We demonstrate the efficacy of the\nproposed approach in significantly improving the accuracy of the learned value\nfunction for four challenging reachability tasks: a rimless wheel system with\nstate resets, collision avoidance in a cluttered environment, autonomous rocket\nlanding, and multi-aircraft collision avoidance.\n", "link": "http://arxiv.org/abs/2404.00814v3", "date": "2025-05-09", "relevancy": 2.0064, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4999}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Imposition%20of%20Safety%20Boundary%20Conditions%20in%20Neural%20Reachable%20Tubes&body=Title%3A%20Exact%20Imposition%20of%20Safety%20Boundary%20Conditions%20in%20Neural%20Reachable%20Tubes%0AAuthor%3A%20Aditya%20Singh%20and%20Zeyuan%20Feng%20and%20Somil%20Bansal%0AAbstract%3A%20%20%20Hamilton-Jacobi%20%28HJ%29%20reachability%20analysis%20is%20a%20widely%20adopted%20verification%0Atool%20to%20provide%20safety%20and%20performance%20guarantees%20for%20autonomous%20systems.%0AHowever%2C%20it%20involves%20solving%20a%20partial%20differential%20equation%20%28PDE%29%20to%20compute%20a%0Asafety%20value%20function%2C%20whose%20computational%20and%20memory%20complexity%20scales%0Aexponentially%20with%20the%20state%20dimension%2C%20making%20its%20direct%20application%20to%0Alarge-scale%20systems%20intractable.%20To%20overcome%20these%20challenges%2C%20DeepReach%2C%20a%0Arecently%20proposed%20learning-based%20approach%2C%20approximates%20high-dimensional%0Areachable%20tubes%20using%20neural%20networks%20%28NNs%29.%20While%20shown%20to%20be%20effective%2C%20the%0Aaccuracy%20of%20the%20learned%20solution%20decreases%20with%20system%20complexity.%20One%20of%20the%0Areasons%20for%20this%20degradation%20is%20a%20soft%20imposition%20of%20safety%20constraints%20during%0Athe%20learning%20process%2C%20which%20corresponds%20to%20the%20boundary%20conditions%20of%20the%20PDE%2C%0Aresulting%20in%20inaccurate%20value%20functions.%20In%20this%20work%2C%20we%20propose%20ExactBC%2C%20a%0Avariant%20of%20DeepReach%20that%20imposes%20safety%20constraints%20exactly%20during%20the%0Alearning%20process%20by%20restructuring%20the%20overall%20value%20function%20as%20a%20weighted%20sum%0Aof%20the%20boundary%20condition%20and%20the%20NN%20output.%20Moreover%2C%20the%20proposed%20variant%20no%0Alonger%20needs%20a%20boundary%20loss%20term%20during%20the%20training%20process%2C%20thus%20eliminating%0Athe%20need%20to%20balance%20different%20loss%20terms.%20We%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20approach%20in%20significantly%20improving%20the%20accuracy%20of%20the%20learned%20value%0Afunction%20for%20four%20challenging%20reachability%20tasks%3A%20a%20rimless%20wheel%20system%20with%0Astate%20resets%2C%20collision%20avoidance%20in%20a%20cluttered%20environment%2C%20autonomous%20rocket%0Alanding%2C%20and%20multi-aircraft%20collision%20avoidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00814v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Imposition%2520of%2520Safety%2520Boundary%2520Conditions%2520in%2520Neural%2520Reachable%2520Tubes%26entry.906535625%3DAditya%2520Singh%2520and%2520Zeyuan%2520Feng%2520and%2520Somil%2520Bansal%26entry.1292438233%3D%2520%2520Hamilton-Jacobi%2520%2528HJ%2529%2520reachability%2520analysis%2520is%2520a%2520widely%2520adopted%2520verification%250Atool%2520to%2520provide%2520safety%2520and%2520performance%2520guarantees%2520for%2520autonomous%2520systems.%250AHowever%252C%2520it%2520involves%2520solving%2520a%2520partial%2520differential%2520equation%2520%2528PDE%2529%2520to%2520compute%2520a%250Asafety%2520value%2520function%252C%2520whose%2520computational%2520and%2520memory%2520complexity%2520scales%250Aexponentially%2520with%2520the%2520state%2520dimension%252C%2520making%2520its%2520direct%2520application%2520to%250Alarge-scale%2520systems%2520intractable.%2520To%2520overcome%2520these%2520challenges%252C%2520DeepReach%252C%2520a%250Arecently%2520proposed%2520learning-based%2520approach%252C%2520approximates%2520high-dimensional%250Areachable%2520tubes%2520using%2520neural%2520networks%2520%2528NNs%2529.%2520While%2520shown%2520to%2520be%2520effective%252C%2520the%250Aaccuracy%2520of%2520the%2520learned%2520solution%2520decreases%2520with%2520system%2520complexity.%2520One%2520of%2520the%250Areasons%2520for%2520this%2520degradation%2520is%2520a%2520soft%2520imposition%2520of%2520safety%2520constraints%2520during%250Athe%2520learning%2520process%252C%2520which%2520corresponds%2520to%2520the%2520boundary%2520conditions%2520of%2520the%2520PDE%252C%250Aresulting%2520in%2520inaccurate%2520value%2520functions.%2520In%2520this%2520work%252C%2520we%2520propose%2520ExactBC%252C%2520a%250Avariant%2520of%2520DeepReach%2520that%2520imposes%2520safety%2520constraints%2520exactly%2520during%2520the%250Alearning%2520process%2520by%2520restructuring%2520the%2520overall%2520value%2520function%2520as%2520a%2520weighted%2520sum%250Aof%2520the%2520boundary%2520condition%2520and%2520the%2520NN%2520output.%2520Moreover%252C%2520the%2520proposed%2520variant%2520no%250Alonger%2520needs%2520a%2520boundary%2520loss%2520term%2520during%2520the%2520training%2520process%252C%2520thus%2520eliminating%250Athe%2520need%2520to%2520balance%2520different%2520loss%2520terms.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520the%250Aproposed%2520approach%2520in%2520significantly%2520improving%2520the%2520accuracy%2520of%2520the%2520learned%2520value%250Afunction%2520for%2520four%2520challenging%2520reachability%2520tasks%253A%2520a%2520rimless%2520wheel%2520system%2520with%250Astate%2520resets%252C%2520collision%2520avoidance%2520in%2520a%2520cluttered%2520environment%252C%2520autonomous%2520rocket%250Alanding%252C%2520and%2520multi-aircraft%2520collision%2520avoidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00814v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Imposition%20of%20Safety%20Boundary%20Conditions%20in%20Neural%20Reachable%20Tubes&entry.906535625=Aditya%20Singh%20and%20Zeyuan%20Feng%20and%20Somil%20Bansal&entry.1292438233=%20%20Hamilton-Jacobi%20%28HJ%29%20reachability%20analysis%20is%20a%20widely%20adopted%20verification%0Atool%20to%20provide%20safety%20and%20performance%20guarantees%20for%20autonomous%20systems.%0AHowever%2C%20it%20involves%20solving%20a%20partial%20differential%20equation%20%28PDE%29%20to%20compute%20a%0Asafety%20value%20function%2C%20whose%20computational%20and%20memory%20complexity%20scales%0Aexponentially%20with%20the%20state%20dimension%2C%20making%20its%20direct%20application%20to%0Alarge-scale%20systems%20intractable.%20To%20overcome%20these%20challenges%2C%20DeepReach%2C%20a%0Arecently%20proposed%20learning-based%20approach%2C%20approximates%20high-dimensional%0Areachable%20tubes%20using%20neural%20networks%20%28NNs%29.%20While%20shown%20to%20be%20effective%2C%20the%0Aaccuracy%20of%20the%20learned%20solution%20decreases%20with%20system%20complexity.%20One%20of%20the%0Areasons%20for%20this%20degradation%20is%20a%20soft%20imposition%20of%20safety%20constraints%20during%0Athe%20learning%20process%2C%20which%20corresponds%20to%20the%20boundary%20conditions%20of%20the%20PDE%2C%0Aresulting%20in%20inaccurate%20value%20functions.%20In%20this%20work%2C%20we%20propose%20ExactBC%2C%20a%0Avariant%20of%20DeepReach%20that%20imposes%20safety%20constraints%20exactly%20during%20the%0Alearning%20process%20by%20restructuring%20the%20overall%20value%20function%20as%20a%20weighted%20sum%0Aof%20the%20boundary%20condition%20and%20the%20NN%20output.%20Moreover%2C%20the%20proposed%20variant%20no%0Alonger%20needs%20a%20boundary%20loss%20term%20during%20the%20training%20process%2C%20thus%20eliminating%0Athe%20need%20to%20balance%20different%20loss%20terms.%20We%20demonstrate%20the%20efficacy%20of%20the%0Aproposed%20approach%20in%20significantly%20improving%20the%20accuracy%20of%20the%20learned%20value%0Afunction%20for%20four%20challenging%20reachability%20tasks%3A%20a%20rimless%20wheel%20system%20with%0Astate%20resets%2C%20collision%20avoidance%20in%20a%20cluttered%20environment%2C%20autonomous%20rocket%0Alanding%2C%20and%20multi-aircraft%20collision%20avoidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00814v3&entry.124074799=Read"},
{"title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes", "author": "Stefan Lenz and Arsenij Ustjanzew and Marco Jeray and Meike Ressing and Torsten Panholzer", "abstract": "  Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.\n", "link": "http://arxiv.org/abs/2501.12106v3", "date": "2025-05-09", "relevancy": 2.0051, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes&body=Title%3A%20Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes%0AAuthor%3A%20Stefan%20Lenz%20and%20Arsenij%20Ustjanzew%20and%20Marco%20Jeray%20and%20Meike%20Ressing%20and%20Torsten%20Panholzer%0AAbstract%3A%20%20%20Tumor%20documentation%20in%20Germany%20is%20largely%20done%20manually%2C%20requiring%20reading%0Apatient%20records%20and%20entering%20data%20into%20structured%20databases.%20Large%20language%0Amodels%20%28LLMs%29%20could%20potentially%20enhance%20this%20process%20by%20improving%20efficiency%0Aand%20reliability.%20This%20evaluation%20tests%20eleven%20different%20open%20source%20LLMs%20with%0Asizes%20ranging%20from%201-70%20billion%20model%20parameters%20on%20three%20basic%20tasks%20of%20the%0Atumor%20documentation%20process%3A%20identifying%20tumor%20diagnoses%2C%20assigning%20ICD-10%0Acodes%2C%20and%20extracting%20the%20date%20of%20first%20diagnosis.%20For%20evaluating%20the%20LLMs%20on%0Athese%20tasks%2C%20a%20dataset%20of%20annotated%20text%20snippets%20based%20on%20anonymized%20doctors%27%0Anotes%20from%20urology%20was%20prepared.%20Different%20prompting%20strategies%20were%20used%20to%0Ainvestigate%20the%20effect%20of%20the%20number%20of%20examples%20in%20few-shot%20prompting%20and%20to%0Aexplore%20the%20capabilities%20of%20the%20LLMs%20in%20general.%20The%20models%20Llama%203.1%208B%2C%0AMistral%207B%2C%20and%20Mistral%20NeMo%2012%20B%20performed%20comparably%20well%20in%20the%20tasks.%0AModels%20with%20less%20extensive%20training%20data%20or%20having%20fewer%20than%207%20billion%0Aparameters%20showed%20notably%20lower%20performance%2C%20while%20larger%20models%20did%20not%0Adisplay%20performance%20gains.%20Examples%20from%20a%20different%20medical%20domain%20than%0Aurology%20could%20also%20improve%20the%20outcome%20in%20few-shot%20prompting%2C%20which%0Ademonstrates%20the%20ability%20of%20LLMs%20to%20handle%20tasks%20needed%20for%20tumor%0Adocumentation.%20Open%20source%20LLMs%20show%20a%20strong%20potential%20for%20automating%20tumor%0Adocumentation.%20Models%20from%207-12%20billion%20parameters%20could%20offer%20an%20optimal%0Abalance%20between%20performance%20and%20resource%20efficiency.%20With%20tailored%20fine-tuning%0Aand%20well-designed%20prompting%2C%20these%20models%20might%20become%20important%20tools%20for%0Aclinical%20documentation%20in%20the%20future.%20The%20code%20for%20the%20evaluation%20is%20available%0Afrom%20https%3A//github.com/stefan-m-lenz/UroLlmEval.%20We%20also%20release%20the%20dataset%0Aas%20a%20new%20valuable%20resource%20that%20addresses%20the%20shortage%20of%20authentic%20and%20easily%0Aaccessible%20benchmarks%20in%20German-language%20medical%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12106v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520open%2520source%2520large%2520language%2520models%2520be%2520used%2520for%2520tumor%2520documentation%2520in%250A%2520%2520Germany%253F%2520--%2520An%2520evaluation%2520on%2520urological%2520doctors%2527%2520notes%26entry.906535625%3DStefan%2520Lenz%2520and%2520Arsenij%2520Ustjanzew%2520and%2520Marco%2520Jeray%2520and%2520Meike%2520Ressing%2520and%2520Torsten%2520Panholzer%26entry.1292438233%3D%2520%2520Tumor%2520documentation%2520in%2520Germany%2520is%2520largely%2520done%2520manually%252C%2520requiring%2520reading%250Apatient%2520records%2520and%2520entering%2520data%2520into%2520structured%2520databases.%2520Large%2520language%250Amodels%2520%2528LLMs%2529%2520could%2520potentially%2520enhance%2520this%2520process%2520by%2520improving%2520efficiency%250Aand%2520reliability.%2520This%2520evaluation%2520tests%2520eleven%2520different%2520open%2520source%2520LLMs%2520with%250Asizes%2520ranging%2520from%25201-70%2520billion%2520model%2520parameters%2520on%2520three%2520basic%2520tasks%2520of%2520the%250Atumor%2520documentation%2520process%253A%2520identifying%2520tumor%2520diagnoses%252C%2520assigning%2520ICD-10%250Acodes%252C%2520and%2520extracting%2520the%2520date%2520of%2520first%2520diagnosis.%2520For%2520evaluating%2520the%2520LLMs%2520on%250Athese%2520tasks%252C%2520a%2520dataset%2520of%2520annotated%2520text%2520snippets%2520based%2520on%2520anonymized%2520doctors%2527%250Anotes%2520from%2520urology%2520was%2520prepared.%2520Different%2520prompting%2520strategies%2520were%2520used%2520to%250Ainvestigate%2520the%2520effect%2520of%2520the%2520number%2520of%2520examples%2520in%2520few-shot%2520prompting%2520and%2520to%250Aexplore%2520the%2520capabilities%2520of%2520the%2520LLMs%2520in%2520general.%2520The%2520models%2520Llama%25203.1%25208B%252C%250AMistral%25207B%252C%2520and%2520Mistral%2520NeMo%252012%2520B%2520performed%2520comparably%2520well%2520in%2520the%2520tasks.%250AModels%2520with%2520less%2520extensive%2520training%2520data%2520or%2520having%2520fewer%2520than%25207%2520billion%250Aparameters%2520showed%2520notably%2520lower%2520performance%252C%2520while%2520larger%2520models%2520did%2520not%250Adisplay%2520performance%2520gains.%2520Examples%2520from%2520a%2520different%2520medical%2520domain%2520than%250Aurology%2520could%2520also%2520improve%2520the%2520outcome%2520in%2520few-shot%2520prompting%252C%2520which%250Ademonstrates%2520the%2520ability%2520of%2520LLMs%2520to%2520handle%2520tasks%2520needed%2520for%2520tumor%250Adocumentation.%2520Open%2520source%2520LLMs%2520show%2520a%2520strong%2520potential%2520for%2520automating%2520tumor%250Adocumentation.%2520Models%2520from%25207-12%2520billion%2520parameters%2520could%2520offer%2520an%2520optimal%250Abalance%2520between%2520performance%2520and%2520resource%2520efficiency.%2520With%2520tailored%2520fine-tuning%250Aand%2520well-designed%2520prompting%252C%2520these%2520models%2520might%2520become%2520important%2520tools%2520for%250Aclinical%2520documentation%2520in%2520the%2520future.%2520The%2520code%2520for%2520the%2520evaluation%2520is%2520available%250Afrom%2520https%253A//github.com/stefan-m-lenz/UroLlmEval.%2520We%2520also%2520release%2520the%2520dataset%250Aas%2520a%2520new%2520valuable%2520resource%2520that%2520addresses%2520the%2520shortage%2520of%2520authentic%2520and%2520easily%250Aaccessible%2520benchmarks%2520in%2520German-language%2520medical%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12106v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20open%20source%20large%20language%20models%20be%20used%20for%20tumor%20documentation%20in%0A%20%20Germany%3F%20--%20An%20evaluation%20on%20urological%20doctors%27%20notes&entry.906535625=Stefan%20Lenz%20and%20Arsenij%20Ustjanzew%20and%20Marco%20Jeray%20and%20Meike%20Ressing%20and%20Torsten%20Panholzer&entry.1292438233=%20%20Tumor%20documentation%20in%20Germany%20is%20largely%20done%20manually%2C%20requiring%20reading%0Apatient%20records%20and%20entering%20data%20into%20structured%20databases.%20Large%20language%0Amodels%20%28LLMs%29%20could%20potentially%20enhance%20this%20process%20by%20improving%20efficiency%0Aand%20reliability.%20This%20evaluation%20tests%20eleven%20different%20open%20source%20LLMs%20with%0Asizes%20ranging%20from%201-70%20billion%20model%20parameters%20on%20three%20basic%20tasks%20of%20the%0Atumor%20documentation%20process%3A%20identifying%20tumor%20diagnoses%2C%20assigning%20ICD-10%0Acodes%2C%20and%20extracting%20the%20date%20of%20first%20diagnosis.%20For%20evaluating%20the%20LLMs%20on%0Athese%20tasks%2C%20a%20dataset%20of%20annotated%20text%20snippets%20based%20on%20anonymized%20doctors%27%0Anotes%20from%20urology%20was%20prepared.%20Different%20prompting%20strategies%20were%20used%20to%0Ainvestigate%20the%20effect%20of%20the%20number%20of%20examples%20in%20few-shot%20prompting%20and%20to%0Aexplore%20the%20capabilities%20of%20the%20LLMs%20in%20general.%20The%20models%20Llama%203.1%208B%2C%0AMistral%207B%2C%20and%20Mistral%20NeMo%2012%20B%20performed%20comparably%20well%20in%20the%20tasks.%0AModels%20with%20less%20extensive%20training%20data%20or%20having%20fewer%20than%207%20billion%0Aparameters%20showed%20notably%20lower%20performance%2C%20while%20larger%20models%20did%20not%0Adisplay%20performance%20gains.%20Examples%20from%20a%20different%20medical%20domain%20than%0Aurology%20could%20also%20improve%20the%20outcome%20in%20few-shot%20prompting%2C%20which%0Ademonstrates%20the%20ability%20of%20LLMs%20to%20handle%20tasks%20needed%20for%20tumor%0Adocumentation.%20Open%20source%20LLMs%20show%20a%20strong%20potential%20for%20automating%20tumor%0Adocumentation.%20Models%20from%207-12%20billion%20parameters%20could%20offer%20an%20optimal%0Abalance%20between%20performance%20and%20resource%20efficiency.%20With%20tailored%20fine-tuning%0Aand%20well-designed%20prompting%2C%20these%20models%20might%20become%20important%20tools%20for%0Aclinical%20documentation%20in%20the%20future.%20The%20code%20for%20the%20evaluation%20is%20available%0Afrom%20https%3A//github.com/stefan-m-lenz/UroLlmEval.%20We%20also%20release%20the%20dataset%0Aas%20a%20new%20valuable%20resource%20that%20addresses%20the%20shortage%20of%20authentic%20and%20easily%0Aaccessible%20benchmarks%20in%20German-language%20medical%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12106v3&entry.124074799=Read"},
{"title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning", "author": "Yi-Fan Zhang and Xingyu Lu and Xiao Hu and Chaoyou Fu and Bin Wen and Tianke Zhang and Changyi Liu and Kaiyu Jiang and Kaibing Chen and Kaiyu Tang and Haojie Ding and Jiankang Chen and Fan Yang and Zhang Zhang and Tingting Gao and Liang Wang", "abstract": "  Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.\n", "link": "http://arxiv.org/abs/2505.02835v2", "date": "2025-05-09", "relevancy": 2.0035, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning&body=Title%3A%20R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yi-Fan%20Zhang%20and%20Xingyu%20Lu%20and%20Xiao%20Hu%20and%20Chaoyou%20Fu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Kaiyu%20Jiang%20and%20Kaibing%20Chen%20and%20Kaiyu%20Tang%20and%20Haojie%20Ding%20and%20Jiankang%20Chen%20and%20Fan%20Yang%20and%20Zhang%20Zhang%20and%20Tingting%20Gao%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Multimodal%20Reward%20Models%20%28MRMs%29%20play%20a%20crucial%20role%20in%20enhancing%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20improving%20the%20model%20structure%20and%0Atraining%20data%20of%20MRMs%2C%20there%20has%20been%20limited%20exploration%20into%20the%0Aeffectiveness%20of%20long-term%20reasoning%20capabilities%20for%20reward%20modeling%20and%20how%0Ato%20activate%20these%20capabilities%20in%20MRMs.%20In%20this%20paper%2C%20we%20explore%20how%0AReinforcement%20Learning%20%28RL%29%20can%20be%20used%20to%20improve%20reward%20modeling.%0ASpecifically%2C%20we%20reformulate%20the%20reward%20modeling%20problem%20as%20a%20rule-based%20RL%0Atask.%20However%2C%20we%20observe%20that%20directly%20applying%20existing%20RL%20algorithms%2C%20such%0Aas%20Reinforce%2B%2B%2C%20to%20reward%20modeling%20often%20leads%20to%20training%20instability%20or%20even%0Acollapse%20due%20to%20the%20inherent%20limitations%20of%20these%20algorithms.%20To%20address%20this%0Aissue%2C%20we%20propose%20the%20StableReinforce%20algorithm%2C%20which%20refines%20the%20training%0Aloss%2C%20advantage%20estimation%20strategy%2C%20and%20reward%20design%20of%20existing%20RL%20methods.%0AThese%20refinements%20result%20in%20more%20stable%20training%20dynamics%20and%20superior%0Aperformance.%20To%20facilitate%20MRM%20training%2C%20we%20collect%20200K%20preference%20data%20from%0Adiverse%20datasets.%20Our%20reward%20model%2C%20R1-Reward%2C%20trained%20using%20the%0AStableReinforce%20algorithm%20on%20this%20dataset%2C%20significantly%20improves%20performance%0Aon%20multimodal%20reward%20modeling%20benchmarks.%20Compared%20to%20previous%20SOTA%20models%2C%0AR1-Reward%20achieves%20a%20%248.4%5C%25%24%20improvement%20on%20the%20VL%20Reward-Bench%20and%20a%20%2414.3%5C%25%24%0Aimprovement%20on%20the%20Multimodal%20Reward%20Bench.%20Moreover%2C%20with%20more%20inference%0Acompute%2C%20R1-Reward%27s%20performance%20is%20further%20enhanced%2C%20highlighting%20the%0Apotential%20of%20RL%20algorithms%20in%20optimizing%20MRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR1-Reward%253A%2520Training%2520Multimodal%2520Reward%2520Model%2520Through%2520Stable%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYi-Fan%2520Zhang%2520and%2520Xingyu%2520Lu%2520and%2520Xiao%2520Hu%2520and%2520Chaoyou%2520Fu%2520and%2520Bin%2520Wen%2520and%2520Tianke%2520Zhang%2520and%2520Changyi%2520Liu%2520and%2520Kaiyu%2520Jiang%2520and%2520Kaibing%2520Chen%2520and%2520Kaiyu%2520Tang%2520and%2520Haojie%2520Ding%2520and%2520Jiankang%2520Chen%2520and%2520Fan%2520Yang%2520and%2520Zhang%2520Zhang%2520and%2520Tingting%2520Gao%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Reward%2520Models%2520%2528MRMs%2529%2520play%2520a%2520crucial%2520role%2520in%2520enhancing%2520the%250Aperformance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520While%2520recent%250Aadvancements%2520have%2520primarily%2520focused%2520on%2520improving%2520the%2520model%2520structure%2520and%250Atraining%2520data%2520of%2520MRMs%252C%2520there%2520has%2520been%2520limited%2520exploration%2520into%2520the%250Aeffectiveness%2520of%2520long-term%2520reasoning%2520capabilities%2520for%2520reward%2520modeling%2520and%2520how%250Ato%2520activate%2520these%2520capabilities%2520in%2520MRMs.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%250AReinforcement%2520Learning%2520%2528RL%2529%2520can%2520be%2520used%2520to%2520improve%2520reward%2520modeling.%250ASpecifically%252C%2520we%2520reformulate%2520the%2520reward%2520modeling%2520problem%2520as%2520a%2520rule-based%2520RL%250Atask.%2520However%252C%2520we%2520observe%2520that%2520directly%2520applying%2520existing%2520RL%2520algorithms%252C%2520such%250Aas%2520Reinforce%252B%252B%252C%2520to%2520reward%2520modeling%2520often%2520leads%2520to%2520training%2520instability%2520or%2520even%250Acollapse%2520due%2520to%2520the%2520inherent%2520limitations%2520of%2520these%2520algorithms.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520the%2520StableReinforce%2520algorithm%252C%2520which%2520refines%2520the%2520training%250Aloss%252C%2520advantage%2520estimation%2520strategy%252C%2520and%2520reward%2520design%2520of%2520existing%2520RL%2520methods.%250AThese%2520refinements%2520result%2520in%2520more%2520stable%2520training%2520dynamics%2520and%2520superior%250Aperformance.%2520To%2520facilitate%2520MRM%2520training%252C%2520we%2520collect%2520200K%2520preference%2520data%2520from%250Adiverse%2520datasets.%2520Our%2520reward%2520model%252C%2520R1-Reward%252C%2520trained%2520using%2520the%250AStableReinforce%2520algorithm%2520on%2520this%2520dataset%252C%2520significantly%2520improves%2520performance%250Aon%2520multimodal%2520reward%2520modeling%2520benchmarks.%2520Compared%2520to%2520previous%2520SOTA%2520models%252C%250AR1-Reward%2520achieves%2520a%2520%25248.4%255C%2525%2524%2520improvement%2520on%2520the%2520VL%2520Reward-Bench%2520and%2520a%2520%252414.3%255C%2525%2524%250Aimprovement%2520on%2520the%2520Multimodal%2520Reward%2520Bench.%2520Moreover%252C%2520with%2520more%2520inference%250Acompute%252C%2520R1-Reward%2527s%2520performance%2520is%2520further%2520enhanced%252C%2520highlighting%2520the%250Apotential%2520of%2520RL%2520algorithms%2520in%2520optimizing%2520MRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R1-Reward%3A%20Training%20Multimodal%20Reward%20Model%20Through%20Stable%20Reinforcement%0A%20%20Learning&entry.906535625=Yi-Fan%20Zhang%20and%20Xingyu%20Lu%20and%20Xiao%20Hu%20and%20Chaoyou%20Fu%20and%20Bin%20Wen%20and%20Tianke%20Zhang%20and%20Changyi%20Liu%20and%20Kaiyu%20Jiang%20and%20Kaibing%20Chen%20and%20Kaiyu%20Tang%20and%20Haojie%20Ding%20and%20Jiankang%20Chen%20and%20Fan%20Yang%20and%20Zhang%20Zhang%20and%20Tingting%20Gao%20and%20Liang%20Wang&entry.1292438233=%20%20Multimodal%20Reward%20Models%20%28MRMs%29%20play%20a%20crucial%20role%20in%20enhancing%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20recent%0Aadvancements%20have%20primarily%20focused%20on%20improving%20the%20model%20structure%20and%0Atraining%20data%20of%20MRMs%2C%20there%20has%20been%20limited%20exploration%20into%20the%0Aeffectiveness%20of%20long-term%20reasoning%20capabilities%20for%20reward%20modeling%20and%20how%0Ato%20activate%20these%20capabilities%20in%20MRMs.%20In%20this%20paper%2C%20we%20explore%20how%0AReinforcement%20Learning%20%28RL%29%20can%20be%20used%20to%20improve%20reward%20modeling.%0ASpecifically%2C%20we%20reformulate%20the%20reward%20modeling%20problem%20as%20a%20rule-based%20RL%0Atask.%20However%2C%20we%20observe%20that%20directly%20applying%20existing%20RL%20algorithms%2C%20such%0Aas%20Reinforce%2B%2B%2C%20to%20reward%20modeling%20often%20leads%20to%20training%20instability%20or%20even%0Acollapse%20due%20to%20the%20inherent%20limitations%20of%20these%20algorithms.%20To%20address%20this%0Aissue%2C%20we%20propose%20the%20StableReinforce%20algorithm%2C%20which%20refines%20the%20training%0Aloss%2C%20advantage%20estimation%20strategy%2C%20and%20reward%20design%20of%20existing%20RL%20methods.%0AThese%20refinements%20result%20in%20more%20stable%20training%20dynamics%20and%20superior%0Aperformance.%20To%20facilitate%20MRM%20training%2C%20we%20collect%20200K%20preference%20data%20from%0Adiverse%20datasets.%20Our%20reward%20model%2C%20R1-Reward%2C%20trained%20using%20the%0AStableReinforce%20algorithm%20on%20this%20dataset%2C%20significantly%20improves%20performance%0Aon%20multimodal%20reward%20modeling%20benchmarks.%20Compared%20to%20previous%20SOTA%20models%2C%0AR1-Reward%20achieves%20a%20%248.4%5C%25%24%20improvement%20on%20the%20VL%20Reward-Bench%20and%20a%20%2414.3%5C%25%24%0Aimprovement%20on%20the%20Multimodal%20Reward%20Bench.%20Moreover%2C%20with%20more%20inference%0Acompute%2C%20R1-Reward%27s%20performance%20is%20further%20enhanced%2C%20highlighting%20the%0Apotential%20of%20RL%20algorithms%20in%20optimizing%20MRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02835v2&entry.124074799=Read"},
{"title": "Priority-Driven Safe Model Predictive Control Approach to Autonomous\n  Driving Applications", "author": "Francesco Prignoli and Ying Shuai Quan and Mohammad Jeddi and Jonas Sj\u00f6berg and Paolo Falcone", "abstract": "  This paper demonstrates the applicability of the safe model predictive\ncontrol (SMPC) framework to autonomous driving scenarios, focusing on the\ndesign of adaptive cruise control (ACC) and automated lane-change systems.\nBuilding on the SMPC approach with priority-driven constraint softening --\nwhich ensures the satisfaction of \\emph{hard} constraints under external\ndisturbances by selectively softening a predefined subset of adjustable\nconstraints -- we show how the algorithm dynamically relaxes lower-priority,\ncomfort-related constraints in response to unexpected disturbances while\npreserving critical safety requirements such as collision avoidance and\nlane-keeping. A learning-based algorithm approximating the time consuming SMPC\nis introduced to enable real-time execution. Simulations in real-world driving\nscenarios subject to unpredicted disturbances confirm that this prioritized\nsoftening mechanism consistently upholds stringent safety constraints,\nunderscoring the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2505.05933v1", "date": "2025-05-09", "relevancy": 2.0015, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5044}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Priority-Driven%20Safe%20Model%20Predictive%20Control%20Approach%20to%20Autonomous%0A%20%20Driving%20Applications&body=Title%3A%20Priority-Driven%20Safe%20Model%20Predictive%20Control%20Approach%20to%20Autonomous%0A%20%20Driving%20Applications%0AAuthor%3A%20Francesco%20Prignoli%20and%20Ying%20Shuai%20Quan%20and%20Mohammad%20Jeddi%20and%20Jonas%20Sj%C3%B6berg%20and%20Paolo%20Falcone%0AAbstract%3A%20%20%20This%20paper%20demonstrates%20the%20applicability%20of%20the%20safe%20model%20predictive%0Acontrol%20%28SMPC%29%20framework%20to%20autonomous%20driving%20scenarios%2C%20focusing%20on%20the%0Adesign%20of%20adaptive%20cruise%20control%20%28ACC%29%20and%20automated%20lane-change%20systems.%0ABuilding%20on%20the%20SMPC%20approach%20with%20priority-driven%20constraint%20softening%20--%0Awhich%20ensures%20the%20satisfaction%20of%20%5Cemph%7Bhard%7D%20constraints%20under%20external%0Adisturbances%20by%20selectively%20softening%20a%20predefined%20subset%20of%20adjustable%0Aconstraints%20--%20we%20show%20how%20the%20algorithm%20dynamically%20relaxes%20lower-priority%2C%0Acomfort-related%20constraints%20in%20response%20to%20unexpected%20disturbances%20while%0Apreserving%20critical%20safety%20requirements%20such%20as%20collision%20avoidance%20and%0Alane-keeping.%20A%20learning-based%20algorithm%20approximating%20the%20time%20consuming%20SMPC%0Ais%20introduced%20to%20enable%20real-time%20execution.%20Simulations%20in%20real-world%20driving%0Ascenarios%20subject%20to%20unpredicted%20disturbances%20confirm%20that%20this%20prioritized%0Asoftening%20mechanism%20consistently%20upholds%20stringent%20safety%20constraints%2C%0Aunderscoring%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPriority-Driven%2520Safe%2520Model%2520Predictive%2520Control%2520Approach%2520to%2520Autonomous%250A%2520%2520Driving%2520Applications%26entry.906535625%3DFrancesco%2520Prignoli%2520and%2520Ying%2520Shuai%2520Quan%2520and%2520Mohammad%2520Jeddi%2520and%2520Jonas%2520Sj%25C3%25B6berg%2520and%2520Paolo%2520Falcone%26entry.1292438233%3D%2520%2520This%2520paper%2520demonstrates%2520the%2520applicability%2520of%2520the%2520safe%2520model%2520predictive%250Acontrol%2520%2528SMPC%2529%2520framework%2520to%2520autonomous%2520driving%2520scenarios%252C%2520focusing%2520on%2520the%250Adesign%2520of%2520adaptive%2520cruise%2520control%2520%2528ACC%2529%2520and%2520automated%2520lane-change%2520systems.%250ABuilding%2520on%2520the%2520SMPC%2520approach%2520with%2520priority-driven%2520constraint%2520softening%2520--%250Awhich%2520ensures%2520the%2520satisfaction%2520of%2520%255Cemph%257Bhard%257D%2520constraints%2520under%2520external%250Adisturbances%2520by%2520selectively%2520softening%2520a%2520predefined%2520subset%2520of%2520adjustable%250Aconstraints%2520--%2520we%2520show%2520how%2520the%2520algorithm%2520dynamically%2520relaxes%2520lower-priority%252C%250Acomfort-related%2520constraints%2520in%2520response%2520to%2520unexpected%2520disturbances%2520while%250Apreserving%2520critical%2520safety%2520requirements%2520such%2520as%2520collision%2520avoidance%2520and%250Alane-keeping.%2520A%2520learning-based%2520algorithm%2520approximating%2520the%2520time%2520consuming%2520SMPC%250Ais%2520introduced%2520to%2520enable%2520real-time%2520execution.%2520Simulations%2520in%2520real-world%2520driving%250Ascenarios%2520subject%2520to%2520unpredicted%2520disturbances%2520confirm%2520that%2520this%2520prioritized%250Asoftening%2520mechanism%2520consistently%2520upholds%2520stringent%2520safety%2520constraints%252C%250Aunderscoring%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Priority-Driven%20Safe%20Model%20Predictive%20Control%20Approach%20to%20Autonomous%0A%20%20Driving%20Applications&entry.906535625=Francesco%20Prignoli%20and%20Ying%20Shuai%20Quan%20and%20Mohammad%20Jeddi%20and%20Jonas%20Sj%C3%B6berg%20and%20Paolo%20Falcone&entry.1292438233=%20%20This%20paper%20demonstrates%20the%20applicability%20of%20the%20safe%20model%20predictive%0Acontrol%20%28SMPC%29%20framework%20to%20autonomous%20driving%20scenarios%2C%20focusing%20on%20the%0Adesign%20of%20adaptive%20cruise%20control%20%28ACC%29%20and%20automated%20lane-change%20systems.%0ABuilding%20on%20the%20SMPC%20approach%20with%20priority-driven%20constraint%20softening%20--%0Awhich%20ensures%20the%20satisfaction%20of%20%5Cemph%7Bhard%7D%20constraints%20under%20external%0Adisturbances%20by%20selectively%20softening%20a%20predefined%20subset%20of%20adjustable%0Aconstraints%20--%20we%20show%20how%20the%20algorithm%20dynamically%20relaxes%20lower-priority%2C%0Acomfort-related%20constraints%20in%20response%20to%20unexpected%20disturbances%20while%0Apreserving%20critical%20safety%20requirements%20such%20as%20collision%20avoidance%20and%0Alane-keeping.%20A%20learning-based%20algorithm%20approximating%20the%20time%20consuming%20SMPC%0Ais%20introduced%20to%20enable%20real-time%20execution.%20Simulations%20in%20real-world%20driving%0Ascenarios%20subject%20to%20unpredicted%20disturbances%20confirm%20that%20this%20prioritized%0Asoftening%20mechanism%20consistently%20upholds%20stringent%20safety%20constraints%2C%0Aunderscoring%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05933v1&entry.124074799=Read"},
{"title": "GreenLight-Gym: Reinforcement learning benchmark environment for control\n  of greenhouse production systems", "author": "Bart van Laatum and Eldert J. van Henten and Sjoerd Boersma", "abstract": "  This study presents GreenLight-Gym, a new, fast, open-source benchmark\nenvironment for developing reinforcement learning (RL) methods in greenhouse\ncrop production control. Built on the state-of-the-art GreenLight model, it\nfeatures a differentiable C++ implementation leveraging the CasADi framework\nfor efficient numerical integration. GreenLight-Gym improves simulation speed\nby a factor of 17 over the original GreenLight implementation. A modular Python\nenvironment wrapper enables flexible configuration of control tasks and\nRL-based controllers. This flexibility is demonstrated by learning controllers\nunder parametric uncertainty using two well-known RL algorithms. GreenLight-Gym\nprovides a standardized benchmark for advancing RL methodologies and evaluating\ngreenhouse control solutions under diverse conditions. The greenhouse control\ncommunity is encouraged to use and extend this benchmark to accelerate\ninnovation in greenhouse crop production.\n", "link": "http://arxiv.org/abs/2410.05336v2", "date": "2025-05-09", "relevancy": 1.9923, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4997}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GreenLight-Gym%3A%20Reinforcement%20learning%20benchmark%20environment%20for%20control%0A%20%20of%20greenhouse%20production%20systems&body=Title%3A%20GreenLight-Gym%3A%20Reinforcement%20learning%20benchmark%20environment%20for%20control%0A%20%20of%20greenhouse%20production%20systems%0AAuthor%3A%20Bart%20van%20Laatum%20and%20Eldert%20J.%20van%20Henten%20and%20Sjoerd%20Boersma%0AAbstract%3A%20%20%20This%20study%20presents%20GreenLight-Gym%2C%20a%20new%2C%20fast%2C%20open-source%20benchmark%0Aenvironment%20for%20developing%20reinforcement%20learning%20%28RL%29%20methods%20in%20greenhouse%0Acrop%20production%20control.%20Built%20on%20the%20state-of-the-art%20GreenLight%20model%2C%20it%0Afeatures%20a%20differentiable%20C%2B%2B%20implementation%20leveraging%20the%20CasADi%20framework%0Afor%20efficient%20numerical%20integration.%20GreenLight-Gym%20improves%20simulation%20speed%0Aby%20a%20factor%20of%2017%20over%20the%20original%20GreenLight%20implementation.%20A%20modular%20Python%0Aenvironment%20wrapper%20enables%20flexible%20configuration%20of%20control%20tasks%20and%0ARL-based%20controllers.%20This%20flexibility%20is%20demonstrated%20by%20learning%20controllers%0Aunder%20parametric%20uncertainty%20using%20two%20well-known%20RL%20algorithms.%20GreenLight-Gym%0Aprovides%20a%20standardized%20benchmark%20for%20advancing%20RL%20methodologies%20and%20evaluating%0Agreenhouse%20control%20solutions%20under%20diverse%20conditions.%20The%20greenhouse%20control%0Acommunity%20is%20encouraged%20to%20use%20and%20extend%20this%20benchmark%20to%20accelerate%0Ainnovation%20in%20greenhouse%20crop%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreenLight-Gym%253A%2520Reinforcement%2520learning%2520benchmark%2520environment%2520for%2520control%250A%2520%2520of%2520greenhouse%2520production%2520systems%26entry.906535625%3DBart%2520van%2520Laatum%2520and%2520Eldert%2520J.%2520van%2520Henten%2520and%2520Sjoerd%2520Boersma%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520GreenLight-Gym%252C%2520a%2520new%252C%2520fast%252C%2520open-source%2520benchmark%250Aenvironment%2520for%2520developing%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520in%2520greenhouse%250Acrop%2520production%2520control.%2520Built%2520on%2520the%2520state-of-the-art%2520GreenLight%2520model%252C%2520it%250Afeatures%2520a%2520differentiable%2520C%252B%252B%2520implementation%2520leveraging%2520the%2520CasADi%2520framework%250Afor%2520efficient%2520numerical%2520integration.%2520GreenLight-Gym%2520improves%2520simulation%2520speed%250Aby%2520a%2520factor%2520of%252017%2520over%2520the%2520original%2520GreenLight%2520implementation.%2520A%2520modular%2520Python%250Aenvironment%2520wrapper%2520enables%2520flexible%2520configuration%2520of%2520control%2520tasks%2520and%250ARL-based%2520controllers.%2520This%2520flexibility%2520is%2520demonstrated%2520by%2520learning%2520controllers%250Aunder%2520parametric%2520uncertainty%2520using%2520two%2520well-known%2520RL%2520algorithms.%2520GreenLight-Gym%250Aprovides%2520a%2520standardized%2520benchmark%2520for%2520advancing%2520RL%2520methodologies%2520and%2520evaluating%250Agreenhouse%2520control%2520solutions%2520under%2520diverse%2520conditions.%2520The%2520greenhouse%2520control%250Acommunity%2520is%2520encouraged%2520to%2520use%2520and%2520extend%2520this%2520benchmark%2520to%2520accelerate%250Ainnovation%2520in%2520greenhouse%2520crop%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GreenLight-Gym%3A%20Reinforcement%20learning%20benchmark%20environment%20for%20control%0A%20%20of%20greenhouse%20production%20systems&entry.906535625=Bart%20van%20Laatum%20and%20Eldert%20J.%20van%20Henten%20and%20Sjoerd%20Boersma&entry.1292438233=%20%20This%20study%20presents%20GreenLight-Gym%2C%20a%20new%2C%20fast%2C%20open-source%20benchmark%0Aenvironment%20for%20developing%20reinforcement%20learning%20%28RL%29%20methods%20in%20greenhouse%0Acrop%20production%20control.%20Built%20on%20the%20state-of-the-art%20GreenLight%20model%2C%20it%0Afeatures%20a%20differentiable%20C%2B%2B%20implementation%20leveraging%20the%20CasADi%20framework%0Afor%20efficient%20numerical%20integration.%20GreenLight-Gym%20improves%20simulation%20speed%0Aby%20a%20factor%20of%2017%20over%20the%20original%20GreenLight%20implementation.%20A%20modular%20Python%0Aenvironment%20wrapper%20enables%20flexible%20configuration%20of%20control%20tasks%20and%0ARL-based%20controllers.%20This%20flexibility%20is%20demonstrated%20by%20learning%20controllers%0Aunder%20parametric%20uncertainty%20using%20two%20well-known%20RL%20algorithms.%20GreenLight-Gym%0Aprovides%20a%20standardized%20benchmark%20for%20advancing%20RL%20methodologies%20and%20evaluating%0Agreenhouse%20control%20solutions%20under%20diverse%20conditions.%20The%20greenhouse%20control%0Acommunity%20is%20encouraged%20to%20use%20and%20extend%20this%20benchmark%20to%20accelerate%0Ainnovation%20in%20greenhouse%20crop%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05336v2&entry.124074799=Read"},
{"title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering", "author": "Ran Zhang and Wei Zhao and Lieve Macken and Steffen Eger", "abstract": "  The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed specifically for literary\ntranslation evaluation. LiTransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LiTransProQA approaches human-level evaluation performance\ncomparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.\n", "link": "http://arxiv.org/abs/2505.05423v2", "date": "2025-05-09", "relevancy": 1.9778, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiTransProQA%3A%20an%20LLM-based%20Literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering&body=Title%3A%20LiTransProQA%3A%20an%20LLM-based%20Literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering%0AAuthor%3A%20Ran%20Zhang%20and%20Wei%20Zhao%20and%20Lieve%20Macken%20and%20Steffen%20Eger%0AAbstract%3A%20%20%20The%20impact%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20into%20literary%0Adomains.%20However%2C%20existing%20evaluation%20metrics%20prioritize%20mechanical%20accuracy%0Aover%20artistic%20expression%20and%20tend%20to%20overrate%20machine%20translation%20%28MT%29%20as%20being%0Asuperior%20to%20experienced%20professional%20human%20translation.%20In%20the%20long%20run%2C%20this%0Abias%20could%20result%20in%20a%20permanent%20decline%20in%20translation%20quality%20and%20cultural%0Aauthenticity.%20In%20response%20to%20the%20urgent%20need%20for%20a%20specialized%20literary%0Aevaluation%20metric%2C%20we%20introduce%20LiTransProQA%2C%20a%20novel%2C%20reference-free%2C%0ALLM-based%20question-answering%20framework%20designed%20specifically%20for%20literary%0Atranslation%20evaluation.%20LiTransProQA%20uniquely%20integrates%20insights%20from%0Aprofessional%20literary%20translators%20and%20researchers%2C%20focusing%20on%20critical%0Aelements%20in%20literary%20quality%20assessment%20such%20as%20literary%20devices%2C%20cultural%0Aunderstanding%2C%20and%20authorial%20voice.%20Our%20extensive%20evaluation%20shows%20that%20while%0Aliterary-finetuned%20XCOMET-XL%20yields%20marginal%20gains%2C%20LiTransProQA%20substantially%0Aoutperforms%20current%20metrics%2C%20achieving%20up%20to%200.07%20gain%20in%20correlation%20%28ACC-EQ%0Aand%20Kendall%27s%20tau%29%20and%20surpassing%20the%20best%20state-of-the-art%20metrics%20by%20over%2015%0Apoints%20in%20adequacy%20assessments.%20Incorporating%20professional%20translator%20insights%0Aas%20weights%20further%20improves%20performance%2C%20highlighting%20the%20value%20of%20translator%0Ainputs.%20Notably%2C%20LiTransProQA%20approaches%20human-level%20evaluation%20performance%0Acomparable%20to%20trained%20linguistic%20annotators.%20It%20demonstrates%20broad%0Aapplicability%20to%20open-source%20models%20such%20as%20LLaMA3.3-70b%20and%20Qwen2.5-32b%2C%0Aindicating%20its%20potential%20as%20an%20accessible%20and%20training-free%20literary%20evaluation%0Ametric%20and%20a%20valuable%20tool%20for%20evaluating%20texts%20that%20require%20local%20processing%0Adue%20to%20copyright%20or%20ethical%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiTransProQA%253A%2520an%2520LLM-based%2520Literary%2520Translation%2520evaluation%2520metric%2520with%250A%2520%2520Professional%2520Question%2520Answering%26entry.906535625%3DRan%2520Zhang%2520and%2520Wei%2520Zhao%2520and%2520Lieve%2520Macken%2520and%2520Steffen%2520Eger%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520extended%2520into%2520literary%250Adomains.%2520However%252C%2520existing%2520evaluation%2520metrics%2520prioritize%2520mechanical%2520accuracy%250Aover%2520artistic%2520expression%2520and%2520tend%2520to%2520overrate%2520machine%2520translation%2520%2528MT%2529%2520as%2520being%250Asuperior%2520to%2520experienced%2520professional%2520human%2520translation.%2520In%2520the%2520long%2520run%252C%2520this%250Abias%2520could%2520result%2520in%2520a%2520permanent%2520decline%2520in%2520translation%2520quality%2520and%2520cultural%250Aauthenticity.%2520In%2520response%2520to%2520the%2520urgent%2520need%2520for%2520a%2520specialized%2520literary%250Aevaluation%2520metric%252C%2520we%2520introduce%2520LiTransProQA%252C%2520a%2520novel%252C%2520reference-free%252C%250ALLM-based%2520question-answering%2520framework%2520designed%2520specifically%2520for%2520literary%250Atranslation%2520evaluation.%2520LiTransProQA%2520uniquely%2520integrates%2520insights%2520from%250Aprofessional%2520literary%2520translators%2520and%2520researchers%252C%2520focusing%2520on%2520critical%250Aelements%2520in%2520literary%2520quality%2520assessment%2520such%2520as%2520literary%2520devices%252C%2520cultural%250Aunderstanding%252C%2520and%2520authorial%2520voice.%2520Our%2520extensive%2520evaluation%2520shows%2520that%2520while%250Aliterary-finetuned%2520XCOMET-XL%2520yields%2520marginal%2520gains%252C%2520LiTransProQA%2520substantially%250Aoutperforms%2520current%2520metrics%252C%2520achieving%2520up%2520to%25200.07%2520gain%2520in%2520correlation%2520%2528ACC-EQ%250Aand%2520Kendall%2527s%2520tau%2529%2520and%2520surpassing%2520the%2520best%2520state-of-the-art%2520metrics%2520by%2520over%252015%250Apoints%2520in%2520adequacy%2520assessments.%2520Incorporating%2520professional%2520translator%2520insights%250Aas%2520weights%2520further%2520improves%2520performance%252C%2520highlighting%2520the%2520value%2520of%2520translator%250Ainputs.%2520Notably%252C%2520LiTransProQA%2520approaches%2520human-level%2520evaluation%2520performance%250Acomparable%2520to%2520trained%2520linguistic%2520annotators.%2520It%2520demonstrates%2520broad%250Aapplicability%2520to%2520open-source%2520models%2520such%2520as%2520LLaMA3.3-70b%2520and%2520Qwen2.5-32b%252C%250Aindicating%2520its%2520potential%2520as%2520an%2520accessible%2520and%2520training-free%2520literary%2520evaluation%250Ametric%2520and%2520a%2520valuable%2520tool%2520for%2520evaluating%2520texts%2520that%2520require%2520local%2520processing%250Adue%2520to%2520copyright%2520or%2520ethical%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiTransProQA%3A%20an%20LLM-based%20Literary%20Translation%20evaluation%20metric%20with%0A%20%20Professional%20Question%20Answering&entry.906535625=Ran%20Zhang%20and%20Wei%20Zhao%20and%20Lieve%20Macken%20and%20Steffen%20Eger&entry.1292438233=%20%20The%20impact%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20extended%20into%20literary%0Adomains.%20However%2C%20existing%20evaluation%20metrics%20prioritize%20mechanical%20accuracy%0Aover%20artistic%20expression%20and%20tend%20to%20overrate%20machine%20translation%20%28MT%29%20as%20being%0Asuperior%20to%20experienced%20professional%20human%20translation.%20In%20the%20long%20run%2C%20this%0Abias%20could%20result%20in%20a%20permanent%20decline%20in%20translation%20quality%20and%20cultural%0Aauthenticity.%20In%20response%20to%20the%20urgent%20need%20for%20a%20specialized%20literary%0Aevaluation%20metric%2C%20we%20introduce%20LiTransProQA%2C%20a%20novel%2C%20reference-free%2C%0ALLM-based%20question-answering%20framework%20designed%20specifically%20for%20literary%0Atranslation%20evaluation.%20LiTransProQA%20uniquely%20integrates%20insights%20from%0Aprofessional%20literary%20translators%20and%20researchers%2C%20focusing%20on%20critical%0Aelements%20in%20literary%20quality%20assessment%20such%20as%20literary%20devices%2C%20cultural%0Aunderstanding%2C%20and%20authorial%20voice.%20Our%20extensive%20evaluation%20shows%20that%20while%0Aliterary-finetuned%20XCOMET-XL%20yields%20marginal%20gains%2C%20LiTransProQA%20substantially%0Aoutperforms%20current%20metrics%2C%20achieving%20up%20to%200.07%20gain%20in%20correlation%20%28ACC-EQ%0Aand%20Kendall%27s%20tau%29%20and%20surpassing%20the%20best%20state-of-the-art%20metrics%20by%20over%2015%0Apoints%20in%20adequacy%20assessments.%20Incorporating%20professional%20translator%20insights%0Aas%20weights%20further%20improves%20performance%2C%20highlighting%20the%20value%20of%20translator%0Ainputs.%20Notably%2C%20LiTransProQA%20approaches%20human-level%20evaluation%20performance%0Acomparable%20to%20trained%20linguistic%20annotators.%20It%20demonstrates%20broad%0Aapplicability%20to%20open-source%20models%20such%20as%20LLaMA3.3-70b%20and%20Qwen2.5-32b%2C%0Aindicating%20its%20potential%20as%20an%20accessible%20and%20training-free%20literary%20evaluation%0Ametric%20and%20a%20valuable%20tool%20for%20evaluating%20texts%20that%20require%20local%20processing%0Adue%20to%20copyright%20or%20ethical%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05423v2&entry.124074799=Read"},
{"title": "Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous\n  Information Networks", "author": "Hongye Zheng and Yue Xing and Lipeng Zhu and Xu Han and Junliang Du and Wanyu Cui", "abstract": "  This study focuses on the problem of path modeling in heterogeneous\ninformation networks and proposes a multi-hop path-aware recommendation\nframework. The method centers on multi-hop paths composed of various types of\nentities and relations. It models user preferences through three stages: path\nselection, semantic representation, and attention-based fusion. In the path\nselection stage, a path filtering mechanism is introduced to remove redundant\nand noisy information. In the representation learning stage, a sequential\nmodeling structure is used to jointly encode entities and relations, preserving\nthe semantic dependencies within paths. In the fusion stage, an attention\nmechanism assigns different weights to each path to generate a global user\ninterest representation. Experiments conducted on real-world datasets such as\nAmazon-Book show that the proposed method significantly outperforms existing\nrecommendation models across multiple evaluation metrics, including HR@10,\nRecall@10, and Precision@10. The results confirm the effectiveness of multi-hop\npaths in capturing high-order interaction semantics and demonstrate the\nexpressive modeling capabilities of the framework in heterogeneous\nrecommendation scenarios. This method provides both theoretical and practical\nvalue by integrating structural information modeling in heterogeneous networks\nwith recommendation algorithm design. It offers a more expressive and flexible\nparadigm for learning user preferences in complex data environments.\n", "link": "http://arxiv.org/abs/2505.05989v1", "date": "2025-05-09", "relevancy": 1.9733, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Multi-Hop%20Semantic%20Paths%20for%20Recommendation%20in%20Heterogeneous%0A%20%20Information%20Networks&body=Title%3A%20Modeling%20Multi-Hop%20Semantic%20Paths%20for%20Recommendation%20in%20Heterogeneous%0A%20%20Information%20Networks%0AAuthor%3A%20Hongye%20Zheng%20and%20Yue%20Xing%20and%20Lipeng%20Zhu%20and%20Xu%20Han%20and%20Junliang%20Du%20and%20Wanyu%20Cui%0AAbstract%3A%20%20%20This%20study%20focuses%20on%20the%20problem%20of%20path%20modeling%20in%20heterogeneous%0Ainformation%20networks%20and%20proposes%20a%20multi-hop%20path-aware%20recommendation%0Aframework.%20The%20method%20centers%20on%20multi-hop%20paths%20composed%20of%20various%20types%20of%0Aentities%20and%20relations.%20It%20models%20user%20preferences%20through%20three%20stages%3A%20path%0Aselection%2C%20semantic%20representation%2C%20and%20attention-based%20fusion.%20In%20the%20path%0Aselection%20stage%2C%20a%20path%20filtering%20mechanism%20is%20introduced%20to%20remove%20redundant%0Aand%20noisy%20information.%20In%20the%20representation%20learning%20stage%2C%20a%20sequential%0Amodeling%20structure%20is%20used%20to%20jointly%20encode%20entities%20and%20relations%2C%20preserving%0Athe%20semantic%20dependencies%20within%20paths.%20In%20the%20fusion%20stage%2C%20an%20attention%0Amechanism%20assigns%20different%20weights%20to%20each%20path%20to%20generate%20a%20global%20user%0Ainterest%20representation.%20Experiments%20conducted%20on%20real-world%20datasets%20such%20as%0AAmazon-Book%20show%20that%20the%20proposed%20method%20significantly%20outperforms%20existing%0Arecommendation%20models%20across%20multiple%20evaluation%20metrics%2C%20including%20HR%4010%2C%0ARecall%4010%2C%20and%20Precision%4010.%20The%20results%20confirm%20the%20effectiveness%20of%20multi-hop%0Apaths%20in%20capturing%20high-order%20interaction%20semantics%20and%20demonstrate%20the%0Aexpressive%20modeling%20capabilities%20of%20the%20framework%20in%20heterogeneous%0Arecommendation%20scenarios.%20This%20method%20provides%20both%20theoretical%20and%20practical%0Avalue%20by%20integrating%20structural%20information%20modeling%20in%20heterogeneous%20networks%0Awith%20recommendation%20algorithm%20design.%20It%20offers%20a%20more%20expressive%20and%20flexible%0Aparadigm%20for%20learning%20user%20preferences%20in%20complex%20data%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Multi-Hop%2520Semantic%2520Paths%2520for%2520Recommendation%2520in%2520Heterogeneous%250A%2520%2520Information%2520Networks%26entry.906535625%3DHongye%2520Zheng%2520and%2520Yue%2520Xing%2520and%2520Lipeng%2520Zhu%2520and%2520Xu%2520Han%2520and%2520Junliang%2520Du%2520and%2520Wanyu%2520Cui%26entry.1292438233%3D%2520%2520This%2520study%2520focuses%2520on%2520the%2520problem%2520of%2520path%2520modeling%2520in%2520heterogeneous%250Ainformation%2520networks%2520and%2520proposes%2520a%2520multi-hop%2520path-aware%2520recommendation%250Aframework.%2520The%2520method%2520centers%2520on%2520multi-hop%2520paths%2520composed%2520of%2520various%2520types%2520of%250Aentities%2520and%2520relations.%2520It%2520models%2520user%2520preferences%2520through%2520three%2520stages%253A%2520path%250Aselection%252C%2520semantic%2520representation%252C%2520and%2520attention-based%2520fusion.%2520In%2520the%2520path%250Aselection%2520stage%252C%2520a%2520path%2520filtering%2520mechanism%2520is%2520introduced%2520to%2520remove%2520redundant%250Aand%2520noisy%2520information.%2520In%2520the%2520representation%2520learning%2520stage%252C%2520a%2520sequential%250Amodeling%2520structure%2520is%2520used%2520to%2520jointly%2520encode%2520entities%2520and%2520relations%252C%2520preserving%250Athe%2520semantic%2520dependencies%2520within%2520paths.%2520In%2520the%2520fusion%2520stage%252C%2520an%2520attention%250Amechanism%2520assigns%2520different%2520weights%2520to%2520each%2520path%2520to%2520generate%2520a%2520global%2520user%250Ainterest%2520representation.%2520Experiments%2520conducted%2520on%2520real-world%2520datasets%2520such%2520as%250AAmazon-Book%2520show%2520that%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520existing%250Arecommendation%2520models%2520across%2520multiple%2520evaluation%2520metrics%252C%2520including%2520HR%254010%252C%250ARecall%254010%252C%2520and%2520Precision%254010.%2520The%2520results%2520confirm%2520the%2520effectiveness%2520of%2520multi-hop%250Apaths%2520in%2520capturing%2520high-order%2520interaction%2520semantics%2520and%2520demonstrate%2520the%250Aexpressive%2520modeling%2520capabilities%2520of%2520the%2520framework%2520in%2520heterogeneous%250Arecommendation%2520scenarios.%2520This%2520method%2520provides%2520both%2520theoretical%2520and%2520practical%250Avalue%2520by%2520integrating%2520structural%2520information%2520modeling%2520in%2520heterogeneous%2520networks%250Awith%2520recommendation%2520algorithm%2520design.%2520It%2520offers%2520a%2520more%2520expressive%2520and%2520flexible%250Aparadigm%2520for%2520learning%2520user%2520preferences%2520in%2520complex%2520data%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Multi-Hop%20Semantic%20Paths%20for%20Recommendation%20in%20Heterogeneous%0A%20%20Information%20Networks&entry.906535625=Hongye%20Zheng%20and%20Yue%20Xing%20and%20Lipeng%20Zhu%20and%20Xu%20Han%20and%20Junliang%20Du%20and%20Wanyu%20Cui&entry.1292438233=%20%20This%20study%20focuses%20on%20the%20problem%20of%20path%20modeling%20in%20heterogeneous%0Ainformation%20networks%20and%20proposes%20a%20multi-hop%20path-aware%20recommendation%0Aframework.%20The%20method%20centers%20on%20multi-hop%20paths%20composed%20of%20various%20types%20of%0Aentities%20and%20relations.%20It%20models%20user%20preferences%20through%20three%20stages%3A%20path%0Aselection%2C%20semantic%20representation%2C%20and%20attention-based%20fusion.%20In%20the%20path%0Aselection%20stage%2C%20a%20path%20filtering%20mechanism%20is%20introduced%20to%20remove%20redundant%0Aand%20noisy%20information.%20In%20the%20representation%20learning%20stage%2C%20a%20sequential%0Amodeling%20structure%20is%20used%20to%20jointly%20encode%20entities%20and%20relations%2C%20preserving%0Athe%20semantic%20dependencies%20within%20paths.%20In%20the%20fusion%20stage%2C%20an%20attention%0Amechanism%20assigns%20different%20weights%20to%20each%20path%20to%20generate%20a%20global%20user%0Ainterest%20representation.%20Experiments%20conducted%20on%20real-world%20datasets%20such%20as%0AAmazon-Book%20show%20that%20the%20proposed%20method%20significantly%20outperforms%20existing%0Arecommendation%20models%20across%20multiple%20evaluation%20metrics%2C%20including%20HR%4010%2C%0ARecall%4010%2C%20and%20Precision%4010.%20The%20results%20confirm%20the%20effectiveness%20of%20multi-hop%0Apaths%20in%20capturing%20high-order%20interaction%20semantics%20and%20demonstrate%20the%0Aexpressive%20modeling%20capabilities%20of%20the%20framework%20in%20heterogeneous%0Arecommendation%20scenarios.%20This%20method%20provides%20both%20theoretical%20and%20practical%0Avalue%20by%20integrating%20structural%20information%20modeling%20in%20heterogeneous%20networks%0Awith%20recommendation%20algorithm%20design.%20It%20offers%20a%20more%20expressive%20and%20flexible%0Aparadigm%20for%20learning%20user%20preferences%20in%20complex%20data%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05989v1&entry.124074799=Read"},
{"title": "Reliable and Efficient Inverse Analysis using Physics-Informed Neural\n  Networks with Distance Functions and Adaptive Weight Tuning", "author": "Shota Deguchi and Mitsuteru Asai", "abstract": "  Physics-informed neural networks have attracted significant attention in\nscientific machine learning for their capability to solve forward and inverse\nproblems governed by partial differential equations. However, the accuracy of\nPINN solutions is often limited by the treatment of boundary conditions.\nConventional penalty-based methods, which incorporate boundary conditions as\npenalty terms in the loss function, cannot guarantee exact satisfaction of the\ngiven boundary conditions and are highly sensitive to the choice of penalty\nparameters. This paper demonstrates that distance functions, specifically\nR-functions, can be leveraged to enforce boundary conditions, overcoming these\nlimitations. R-functions provide normalized distance fields, enabling accurate\nrepresentation of boundary geometries, including non-convex domains, and\nfacilitating various types of boundary conditions. We extend this distance\nfunction-based boundary condition imposition method to inverse problems using\nPINNs and introduce an adaptive weight tuning technique to ensure reliable and\nefficient inverse analysis. We demonstrate the efficacy of the method through\nseveral numerical experiments. Numerical results show that the proposed method\nsolves inverse problems more accurately and efficiently than penalty-based\nmethods, even in the presence of complex non-convex geometries. This approach\noffers a reliable and efficient framework for inverse analysis using PINNs,\nwith potential applications across a wide range of engineering problems.\n", "link": "http://arxiv.org/abs/2504.18091v2", "date": "2025-05-09", "relevancy": 1.9701, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4887}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20and%20Efficient%20Inverse%20Analysis%20using%20Physics-Informed%20Neural%0A%20%20Networks%20with%20Distance%20Functions%20and%20Adaptive%20Weight%20Tuning&body=Title%3A%20Reliable%20and%20Efficient%20Inverse%20Analysis%20using%20Physics-Informed%20Neural%0A%20%20Networks%20with%20Distance%20Functions%20and%20Adaptive%20Weight%20Tuning%0AAuthor%3A%20Shota%20Deguchi%20and%20Mitsuteru%20Asai%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20have%20attracted%20significant%20attention%20in%0Ascientific%20machine%20learning%20for%20their%20capability%20to%20solve%20forward%20and%20inverse%0Aproblems%20governed%20by%20partial%20differential%20equations.%20However%2C%20the%20accuracy%20of%0APINN%20solutions%20is%20often%20limited%20by%20the%20treatment%20of%20boundary%20conditions.%0AConventional%20penalty-based%20methods%2C%20which%20incorporate%20boundary%20conditions%20as%0Apenalty%20terms%20in%20the%20loss%20function%2C%20cannot%20guarantee%20exact%20satisfaction%20of%20the%0Agiven%20boundary%20conditions%20and%20are%20highly%20sensitive%20to%20the%20choice%20of%20penalty%0Aparameters.%20This%20paper%20demonstrates%20that%20distance%20functions%2C%20specifically%0AR-functions%2C%20can%20be%20leveraged%20to%20enforce%20boundary%20conditions%2C%20overcoming%20these%0Alimitations.%20R-functions%20provide%20normalized%20distance%20fields%2C%20enabling%20accurate%0Arepresentation%20of%20boundary%20geometries%2C%20including%20non-convex%20domains%2C%20and%0Afacilitating%20various%20types%20of%20boundary%20conditions.%20We%20extend%20this%20distance%0Afunction-based%20boundary%20condition%20imposition%20method%20to%20inverse%20problems%20using%0APINNs%20and%20introduce%20an%20adaptive%20weight%20tuning%20technique%20to%20ensure%20reliable%20and%0Aefficient%20inverse%20analysis.%20We%20demonstrate%20the%20efficacy%20of%20the%20method%20through%0Aseveral%20numerical%20experiments.%20Numerical%20results%20show%20that%20the%20proposed%20method%0Asolves%20inverse%20problems%20more%20accurately%20and%20efficiently%20than%20penalty-based%0Amethods%2C%20even%20in%20the%20presence%20of%20complex%20non-convex%20geometries.%20This%20approach%0Aoffers%20a%20reliable%20and%20efficient%20framework%20for%20inverse%20analysis%20using%20PINNs%2C%0Awith%20potential%20applications%20across%20a%20wide%20range%20of%20engineering%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18091v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520and%2520Efficient%2520Inverse%2520Analysis%2520using%2520Physics-Informed%2520Neural%250A%2520%2520Networks%2520with%2520Distance%2520Functions%2520and%2520Adaptive%2520Weight%2520Tuning%26entry.906535625%3DShota%2520Deguchi%2520and%2520Mitsuteru%2520Asai%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520have%2520attracted%2520significant%2520attention%2520in%250Ascientific%2520machine%2520learning%2520for%2520their%2520capability%2520to%2520solve%2520forward%2520and%2520inverse%250Aproblems%2520governed%2520by%2520partial%2520differential%2520equations.%2520However%252C%2520the%2520accuracy%2520of%250APINN%2520solutions%2520is%2520often%2520limited%2520by%2520the%2520treatment%2520of%2520boundary%2520conditions.%250AConventional%2520penalty-based%2520methods%252C%2520which%2520incorporate%2520boundary%2520conditions%2520as%250Apenalty%2520terms%2520in%2520the%2520loss%2520function%252C%2520cannot%2520guarantee%2520exact%2520satisfaction%2520of%2520the%250Agiven%2520boundary%2520conditions%2520and%2520are%2520highly%2520sensitive%2520to%2520the%2520choice%2520of%2520penalty%250Aparameters.%2520This%2520paper%2520demonstrates%2520that%2520distance%2520functions%252C%2520specifically%250AR-functions%252C%2520can%2520be%2520leveraged%2520to%2520enforce%2520boundary%2520conditions%252C%2520overcoming%2520these%250Alimitations.%2520R-functions%2520provide%2520normalized%2520distance%2520fields%252C%2520enabling%2520accurate%250Arepresentation%2520of%2520boundary%2520geometries%252C%2520including%2520non-convex%2520domains%252C%2520and%250Afacilitating%2520various%2520types%2520of%2520boundary%2520conditions.%2520We%2520extend%2520this%2520distance%250Afunction-based%2520boundary%2520condition%2520imposition%2520method%2520to%2520inverse%2520problems%2520using%250APINNs%2520and%2520introduce%2520an%2520adaptive%2520weight%2520tuning%2520technique%2520to%2520ensure%2520reliable%2520and%250Aefficient%2520inverse%2520analysis.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520method%2520through%250Aseveral%2520numerical%2520experiments.%2520Numerical%2520results%2520show%2520that%2520the%2520proposed%2520method%250Asolves%2520inverse%2520problems%2520more%2520accurately%2520and%2520efficiently%2520than%2520penalty-based%250Amethods%252C%2520even%2520in%2520the%2520presence%2520of%2520complex%2520non-convex%2520geometries.%2520This%2520approach%250Aoffers%2520a%2520reliable%2520and%2520efficient%2520framework%2520for%2520inverse%2520analysis%2520using%2520PINNs%252C%250Awith%2520potential%2520applications%2520across%2520a%2520wide%2520range%2520of%2520engineering%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18091v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20and%20Efficient%20Inverse%20Analysis%20using%20Physics-Informed%20Neural%0A%20%20Networks%20with%20Distance%20Functions%20and%20Adaptive%20Weight%20Tuning&entry.906535625=Shota%20Deguchi%20and%20Mitsuteru%20Asai&entry.1292438233=%20%20Physics-informed%20neural%20networks%20have%20attracted%20significant%20attention%20in%0Ascientific%20machine%20learning%20for%20their%20capability%20to%20solve%20forward%20and%20inverse%0Aproblems%20governed%20by%20partial%20differential%20equations.%20However%2C%20the%20accuracy%20of%0APINN%20solutions%20is%20often%20limited%20by%20the%20treatment%20of%20boundary%20conditions.%0AConventional%20penalty-based%20methods%2C%20which%20incorporate%20boundary%20conditions%20as%0Apenalty%20terms%20in%20the%20loss%20function%2C%20cannot%20guarantee%20exact%20satisfaction%20of%20the%0Agiven%20boundary%20conditions%20and%20are%20highly%20sensitive%20to%20the%20choice%20of%20penalty%0Aparameters.%20This%20paper%20demonstrates%20that%20distance%20functions%2C%20specifically%0AR-functions%2C%20can%20be%20leveraged%20to%20enforce%20boundary%20conditions%2C%20overcoming%20these%0Alimitations.%20R-functions%20provide%20normalized%20distance%20fields%2C%20enabling%20accurate%0Arepresentation%20of%20boundary%20geometries%2C%20including%20non-convex%20domains%2C%20and%0Afacilitating%20various%20types%20of%20boundary%20conditions.%20We%20extend%20this%20distance%0Afunction-based%20boundary%20condition%20imposition%20method%20to%20inverse%20problems%20using%0APINNs%20and%20introduce%20an%20adaptive%20weight%20tuning%20technique%20to%20ensure%20reliable%20and%0Aefficient%20inverse%20analysis.%20We%20demonstrate%20the%20efficacy%20of%20the%20method%20through%0Aseveral%20numerical%20experiments.%20Numerical%20results%20show%20that%20the%20proposed%20method%0Asolves%20inverse%20problems%20more%20accurately%20and%20efficiently%20than%20penalty-based%0Amethods%2C%20even%20in%20the%20presence%20of%20complex%20non-convex%20geometries.%20This%20approach%0Aoffers%20a%20reliable%20and%20efficient%20framework%20for%20inverse%20analysis%20using%20PINNs%2C%0Awith%20potential%20applications%20across%20a%20wide%20range%20of%20engineering%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18091v2&entry.124074799=Read"},
{"title": "Fast Differentiable Modal Simulation of Non-linear Strings, Membranes,\n  and Plates", "author": "Rodrigo Diaz and Mark Sandler", "abstract": "  Modal methods for simulating vibrations of strings, membranes, and plates are\nwidely used in acoustics and physically informed audio synthesis. However,\ntraditional implementations, particularly for non-linear models like the von\nK\\'arm\\'an plate, are computationally demanding and lack differentiability,\nlimiting inverse modelling and real-time applications. We introduce a fast,\ndifferentiable, GPU-accelerated modal framework built with the JAX library,\nproviding efficient simulations and enabling gradient-based inverse modelling.\nBenchmarks show that our approach significantly outperforms CPU and GPU-based\nimplementations, particularly for simulations with many modes. Inverse\nmodelling experiments demonstrate that our approach can recover physical\nparameters, including tension, stiffness, and geometry, from both synthetic and\nexperimental data. Although fitting physical parameters is more sensitive to\ninitialisation compared to other methods, it provides greater interpretability\nand more compact parameterisation. The code is released as open source to\nsupport future research and applications in differentiable physical modelling\nand sound synthesis.\n", "link": "http://arxiv.org/abs/2505.05940v1", "date": "2025-05-09", "relevancy": 1.9688, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4983}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates&body=Title%3A%20Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates%0AAuthor%3A%20Rodrigo%20Diaz%20and%20Mark%20Sandler%0AAbstract%3A%20%20%20Modal%20methods%20for%20simulating%20vibrations%20of%20strings%2C%20membranes%2C%20and%20plates%20are%0Awidely%20used%20in%20acoustics%20and%20physically%20informed%20audio%20synthesis.%20However%2C%0Atraditional%20implementations%2C%20particularly%20for%20non-linear%20models%20like%20the%20von%0AK%5C%27arm%5C%27an%20plate%2C%20are%20computationally%20demanding%20and%20lack%20differentiability%2C%0Alimiting%20inverse%20modelling%20and%20real-time%20applications.%20We%20introduce%20a%20fast%2C%0Adifferentiable%2C%20GPU-accelerated%20modal%20framework%20built%20with%20the%20JAX%20library%2C%0Aproviding%20efficient%20simulations%20and%20enabling%20gradient-based%20inverse%20modelling.%0ABenchmarks%20show%20that%20our%20approach%20significantly%20outperforms%20CPU%20and%20GPU-based%0Aimplementations%2C%20particularly%20for%20simulations%20with%20many%20modes.%20Inverse%0Amodelling%20experiments%20demonstrate%20that%20our%20approach%20can%20recover%20physical%0Aparameters%2C%20including%20tension%2C%20stiffness%2C%20and%20geometry%2C%20from%20both%20synthetic%20and%0Aexperimental%20data.%20Although%20fitting%20physical%20parameters%20is%20more%20sensitive%20to%0Ainitialisation%20compared%20to%20other%20methods%2C%20it%20provides%20greater%20interpretability%0Aand%20more%20compact%20parameterisation.%20The%20code%20is%20released%20as%20open%20source%20to%0Asupport%20future%20research%20and%20applications%20in%20differentiable%20physical%20modelling%0Aand%20sound%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Differentiable%2520Modal%2520Simulation%2520of%2520Non-linear%2520Strings%252C%2520Membranes%252C%250A%2520%2520and%2520Plates%26entry.906535625%3DRodrigo%2520Diaz%2520and%2520Mark%2520Sandler%26entry.1292438233%3D%2520%2520Modal%2520methods%2520for%2520simulating%2520vibrations%2520of%2520strings%252C%2520membranes%252C%2520and%2520plates%2520are%250Awidely%2520used%2520in%2520acoustics%2520and%2520physically%2520informed%2520audio%2520synthesis.%2520However%252C%250Atraditional%2520implementations%252C%2520particularly%2520for%2520non-linear%2520models%2520like%2520the%2520von%250AK%255C%2527arm%255C%2527an%2520plate%252C%2520are%2520computationally%2520demanding%2520and%2520lack%2520differentiability%252C%250Alimiting%2520inverse%2520modelling%2520and%2520real-time%2520applications.%2520We%2520introduce%2520a%2520fast%252C%250Adifferentiable%252C%2520GPU-accelerated%2520modal%2520framework%2520built%2520with%2520the%2520JAX%2520library%252C%250Aproviding%2520efficient%2520simulations%2520and%2520enabling%2520gradient-based%2520inverse%2520modelling.%250ABenchmarks%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520CPU%2520and%2520GPU-based%250Aimplementations%252C%2520particularly%2520for%2520simulations%2520with%2520many%2520modes.%2520Inverse%250Amodelling%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520recover%2520physical%250Aparameters%252C%2520including%2520tension%252C%2520stiffness%252C%2520and%2520geometry%252C%2520from%2520both%2520synthetic%2520and%250Aexperimental%2520data.%2520Although%2520fitting%2520physical%2520parameters%2520is%2520more%2520sensitive%2520to%250Ainitialisation%2520compared%2520to%2520other%2520methods%252C%2520it%2520provides%2520greater%2520interpretability%250Aand%2520more%2520compact%2520parameterisation.%2520The%2520code%2520is%2520released%2520as%2520open%2520source%2520to%250Asupport%2520future%2520research%2520and%2520applications%2520in%2520differentiable%2520physical%2520modelling%250Aand%2520sound%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates&entry.906535625=Rodrigo%20Diaz%20and%20Mark%20Sandler&entry.1292438233=%20%20Modal%20methods%20for%20simulating%20vibrations%20of%20strings%2C%20membranes%2C%20and%20plates%20are%0Awidely%20used%20in%20acoustics%20and%20physically%20informed%20audio%20synthesis.%20However%2C%0Atraditional%20implementations%2C%20particularly%20for%20non-linear%20models%20like%20the%20von%0AK%5C%27arm%5C%27an%20plate%2C%20are%20computationally%20demanding%20and%20lack%20differentiability%2C%0Alimiting%20inverse%20modelling%20and%20real-time%20applications.%20We%20introduce%20a%20fast%2C%0Adifferentiable%2C%20GPU-accelerated%20modal%20framework%20built%20with%20the%20JAX%20library%2C%0Aproviding%20efficient%20simulations%20and%20enabling%20gradient-based%20inverse%20modelling.%0ABenchmarks%20show%20that%20our%20approach%20significantly%20outperforms%20CPU%20and%20GPU-based%0Aimplementations%2C%20particularly%20for%20simulations%20with%20many%20modes.%20Inverse%0Amodelling%20experiments%20demonstrate%20that%20our%20approach%20can%20recover%20physical%0Aparameters%2C%20including%20tension%2C%20stiffness%2C%20and%20geometry%2C%20from%20both%20synthetic%20and%0Aexperimental%20data.%20Although%20fitting%20physical%20parameters%20is%20more%20sensitive%20to%0Ainitialisation%20compared%20to%20other%20methods%2C%20it%20provides%20greater%20interpretability%0Aand%20more%20compact%20parameterisation.%20The%20code%20is%20released%20as%20open%20source%20to%0Asupport%20future%20research%20and%20applications%20in%20differentiable%20physical%20modelling%0Aand%20sound%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05940v1&entry.124074799=Read"},
{"title": "Differentiable Fuzzy Neural Networks for Recommender Systems", "author": "Stephan Bartl and Kevin Innerebner and Elisabeth Lex", "abstract": "  As recommender systems become increasingly complex, transparency is essential\nto increase user trust, accountability, and regulatory compliance.\nNeuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic\nlearning offer a promising approach toward transparent and user-centric\nsystems. In this work-in-progress, we investigate using fuzzy neural networks\n(FNNs) as a neuro-symbolic approach for recommendations that learn logic-based\nrules over predefined, human-readable atoms. Each rule corresponds to a fuzzy\nlogic expression, making the recommender's decision process inherently\ntransparent. In contrast to black-box machine learning methods, our approach\nreveals the reasoning behind a recommendation while maintaining competitive\nperformance. We evaluate our method on a synthetic and MovieLens 1M datasets\nand compare it to state-of-the-art recommendation algorithms. Our results\ndemonstrate that our approach accurately captures user behavior while providing\na transparent decision-making process. Finally, the differentiable nature of\nthis approach facilitates an integration with other neural models, enabling the\ndevelopment of hybrid, transparent recommender systems.\n", "link": "http://arxiv.org/abs/2505.06000v1", "date": "2025-05-09", "relevancy": 1.96, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5165}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Fuzzy%20Neural%20Networks%20for%20Recommender%20Systems&body=Title%3A%20Differentiable%20Fuzzy%20Neural%20Networks%20for%20Recommender%20Systems%0AAuthor%3A%20Stephan%20Bartl%20and%20Kevin%20Innerebner%20and%20Elisabeth%20Lex%0AAbstract%3A%20%20%20As%20recommender%20systems%20become%20increasingly%20complex%2C%20transparency%20is%20essential%0Ato%20increase%20user%20trust%2C%20accountability%2C%20and%20regulatory%20compliance.%0ANeuro-symbolic%20approaches%20that%20integrate%20symbolic%20reasoning%20with%20sub-symbolic%0Alearning%20offer%20a%20promising%20approach%20toward%20transparent%20and%20user-centric%0Asystems.%20In%20this%20work-in-progress%2C%20we%20investigate%20using%20fuzzy%20neural%20networks%0A%28FNNs%29%20as%20a%20neuro-symbolic%20approach%20for%20recommendations%20that%20learn%20logic-based%0Arules%20over%20predefined%2C%20human-readable%20atoms.%20Each%20rule%20corresponds%20to%20a%20fuzzy%0Alogic%20expression%2C%20making%20the%20recommender%27s%20decision%20process%20inherently%0Atransparent.%20In%20contrast%20to%20black-box%20machine%20learning%20methods%2C%20our%20approach%0Areveals%20the%20reasoning%20behind%20a%20recommendation%20while%20maintaining%20competitive%0Aperformance.%20We%20evaluate%20our%20method%20on%20a%20synthetic%20and%20MovieLens%201M%20datasets%0Aand%20compare%20it%20to%20state-of-the-art%20recommendation%20algorithms.%20Our%20results%0Ademonstrate%20that%20our%20approach%20accurately%20captures%20user%20behavior%20while%20providing%0Aa%20transparent%20decision-making%20process.%20Finally%2C%20the%20differentiable%20nature%20of%0Athis%20approach%20facilitates%20an%20integration%20with%20other%20neural%20models%2C%20enabling%20the%0Adevelopment%20of%20hybrid%2C%20transparent%20recommender%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Fuzzy%2520Neural%2520Networks%2520for%2520Recommender%2520Systems%26entry.906535625%3DStephan%2520Bartl%2520and%2520Kevin%2520Innerebner%2520and%2520Elisabeth%2520Lex%26entry.1292438233%3D%2520%2520As%2520recommender%2520systems%2520become%2520increasingly%2520complex%252C%2520transparency%2520is%2520essential%250Ato%2520increase%2520user%2520trust%252C%2520accountability%252C%2520and%2520regulatory%2520compliance.%250ANeuro-symbolic%2520approaches%2520that%2520integrate%2520symbolic%2520reasoning%2520with%2520sub-symbolic%250Alearning%2520offer%2520a%2520promising%2520approach%2520toward%2520transparent%2520and%2520user-centric%250Asystems.%2520In%2520this%2520work-in-progress%252C%2520we%2520investigate%2520using%2520fuzzy%2520neural%2520networks%250A%2528FNNs%2529%2520as%2520a%2520neuro-symbolic%2520approach%2520for%2520recommendations%2520that%2520learn%2520logic-based%250Arules%2520over%2520predefined%252C%2520human-readable%2520atoms.%2520Each%2520rule%2520corresponds%2520to%2520a%2520fuzzy%250Alogic%2520expression%252C%2520making%2520the%2520recommender%2527s%2520decision%2520process%2520inherently%250Atransparent.%2520In%2520contrast%2520to%2520black-box%2520machine%2520learning%2520methods%252C%2520our%2520approach%250Areveals%2520the%2520reasoning%2520behind%2520a%2520recommendation%2520while%2520maintaining%2520competitive%250Aperformance.%2520We%2520evaluate%2520our%2520method%2520on%2520a%2520synthetic%2520and%2520MovieLens%25201M%2520datasets%250Aand%2520compare%2520it%2520to%2520state-of-the-art%2520recommendation%2520algorithms.%2520Our%2520results%250Ademonstrate%2520that%2520our%2520approach%2520accurately%2520captures%2520user%2520behavior%2520while%2520providing%250Aa%2520transparent%2520decision-making%2520process.%2520Finally%252C%2520the%2520differentiable%2520nature%2520of%250Athis%2520approach%2520facilitates%2520an%2520integration%2520with%2520other%2520neural%2520models%252C%2520enabling%2520the%250Adevelopment%2520of%2520hybrid%252C%2520transparent%2520recommender%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Fuzzy%20Neural%20Networks%20for%20Recommender%20Systems&entry.906535625=Stephan%20Bartl%20and%20Kevin%20Innerebner%20and%20Elisabeth%20Lex&entry.1292438233=%20%20As%20recommender%20systems%20become%20increasingly%20complex%2C%20transparency%20is%20essential%0Ato%20increase%20user%20trust%2C%20accountability%2C%20and%20regulatory%20compliance.%0ANeuro-symbolic%20approaches%20that%20integrate%20symbolic%20reasoning%20with%20sub-symbolic%0Alearning%20offer%20a%20promising%20approach%20toward%20transparent%20and%20user-centric%0Asystems.%20In%20this%20work-in-progress%2C%20we%20investigate%20using%20fuzzy%20neural%20networks%0A%28FNNs%29%20as%20a%20neuro-symbolic%20approach%20for%20recommendations%20that%20learn%20logic-based%0Arules%20over%20predefined%2C%20human-readable%20atoms.%20Each%20rule%20corresponds%20to%20a%20fuzzy%0Alogic%20expression%2C%20making%20the%20recommender%27s%20decision%20process%20inherently%0Atransparent.%20In%20contrast%20to%20black-box%20machine%20learning%20methods%2C%20our%20approach%0Areveals%20the%20reasoning%20behind%20a%20recommendation%20while%20maintaining%20competitive%0Aperformance.%20We%20evaluate%20our%20method%20on%20a%20synthetic%20and%20MovieLens%201M%20datasets%0Aand%20compare%20it%20to%20state-of-the-art%20recommendation%20algorithms.%20Our%20results%0Ademonstrate%20that%20our%20approach%20accurately%20captures%20user%20behavior%20while%20providing%0Aa%20transparent%20decision-making%20process.%20Finally%2C%20the%20differentiable%20nature%20of%0Athis%20approach%20facilitates%20an%20integration%20with%20other%20neural%20models%2C%20enabling%20the%0Adevelopment%20of%20hybrid%2C%20transparent%20recommender%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06000v1&entry.124074799=Read"},
{"title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed\n  Compute Budgets", "author": "Ryan Lagasse and Aidan Kiernans and Avijit Ghosh and Shiri Dori-Hacohen", "abstract": "  We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings.\n", "link": "http://arxiv.org/abs/2505.06150v1", "date": "2025-05-09", "relevancy": 1.9584, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.514}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scaling%20Law%20for%20Token%20Efficiency%20in%20LLM%20Fine-Tuning%20Under%20Fixed%0A%20%20Compute%20Budgets&body=Title%3A%20A%20Scaling%20Law%20for%20Token%20Efficiency%20in%20LLM%20Fine-Tuning%20Under%20Fixed%0A%20%20Compute%20Budgets%0AAuthor%3A%20Ryan%20Lagasse%20and%20Aidan%20Kiernans%20and%20Avijit%20Ghosh%20and%20Shiri%20Dori-Hacohen%0AAbstract%3A%20%20%20We%20introduce%20a%20scaling%20law%20for%20fine-tuning%20large%20language%20models%20%28LLMs%29%20under%0Afixed%20compute%20budgets%20that%20explicitly%20accounts%20for%20data%20composition.%0AConventional%20approaches%20measure%20training%20data%20solely%20by%20total%20tokens%2C%20yet%20the%0Anumber%20of%20examples%20and%20their%20average%20token%20length%20--%20what%20we%20term%20%5Cemph%7Bdataset%0Avolume%7D%20--%20play%20a%20decisive%20role%20in%20model%20performance.%20Our%20formulation%20is%20tuned%0Afollowing%20established%20procedures.%20Experiments%20on%20the%20BRICC%20dataset%0A%5Ccite%7Bsalavati2024reducing%7D%20and%20subsets%20of%20the%20MMLU%20dataset%0A%5Ccite%7Bhendrycks2021measuringmassivemultitasklanguage%7D%2C%20evaluated%20under%20multiple%0Asubsampling%20strategies%2C%20reveal%20that%20data%20composition%20significantly%20affects%0Atoken%20efficiency.%20These%20results%20motivate%20refined%20scaling%20laws%20for%20practical%20LLM%0Afine-tuning%20in%20resource-constrained%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scaling%2520Law%2520for%2520Token%2520Efficiency%2520in%2520LLM%2520Fine-Tuning%2520Under%2520Fixed%250A%2520%2520Compute%2520Budgets%26entry.906535625%3DRyan%2520Lagasse%2520and%2520Aidan%2520Kiernans%2520and%2520Avijit%2520Ghosh%2520and%2520Shiri%2520Dori-Hacohen%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520scaling%2520law%2520for%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520under%250Afixed%2520compute%2520budgets%2520that%2520explicitly%2520accounts%2520for%2520data%2520composition.%250AConventional%2520approaches%2520measure%2520training%2520data%2520solely%2520by%2520total%2520tokens%252C%2520yet%2520the%250Anumber%2520of%2520examples%2520and%2520their%2520average%2520token%2520length%2520--%2520what%2520we%2520term%2520%255Cemph%257Bdataset%250Avolume%257D%2520--%2520play%2520a%2520decisive%2520role%2520in%2520model%2520performance.%2520Our%2520formulation%2520is%2520tuned%250Afollowing%2520established%2520procedures.%2520Experiments%2520on%2520the%2520BRICC%2520dataset%250A%255Ccite%257Bsalavati2024reducing%257D%2520and%2520subsets%2520of%2520the%2520MMLU%2520dataset%250A%255Ccite%257Bhendrycks2021measuringmassivemultitasklanguage%257D%252C%2520evaluated%2520under%2520multiple%250Asubsampling%2520strategies%252C%2520reveal%2520that%2520data%2520composition%2520significantly%2520affects%250Atoken%2520efficiency.%2520These%2520results%2520motivate%2520refined%2520scaling%2520laws%2520for%2520practical%2520LLM%250Afine-tuning%2520in%2520resource-constrained%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scaling%20Law%20for%20Token%20Efficiency%20in%20LLM%20Fine-Tuning%20Under%20Fixed%0A%20%20Compute%20Budgets&entry.906535625=Ryan%20Lagasse%20and%20Aidan%20Kiernans%20and%20Avijit%20Ghosh%20and%20Shiri%20Dori-Hacohen&entry.1292438233=%20%20We%20introduce%20a%20scaling%20law%20for%20fine-tuning%20large%20language%20models%20%28LLMs%29%20under%0Afixed%20compute%20budgets%20that%20explicitly%20accounts%20for%20data%20composition.%0AConventional%20approaches%20measure%20training%20data%20solely%20by%20total%20tokens%2C%20yet%20the%0Anumber%20of%20examples%20and%20their%20average%20token%20length%20--%20what%20we%20term%20%5Cemph%7Bdataset%0Avolume%7D%20--%20play%20a%20decisive%20role%20in%20model%20performance.%20Our%20formulation%20is%20tuned%0Afollowing%20established%20procedures.%20Experiments%20on%20the%20BRICC%20dataset%0A%5Ccite%7Bsalavati2024reducing%7D%20and%20subsets%20of%20the%20MMLU%20dataset%0A%5Ccite%7Bhendrycks2021measuringmassivemultitasklanguage%7D%2C%20evaluated%20under%20multiple%0Asubsampling%20strategies%2C%20reveal%20that%20data%20composition%20significantly%20affects%0Atoken%20efficiency.%20These%20results%20motivate%20refined%20scaling%20laws%20for%20practical%20LLM%0Afine-tuning%20in%20resource-constrained%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06150v1&entry.124074799=Read"},
{"title": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle\n  Routing Problem with Time Windows", "author": "Linjiang Cao and Maonan Wang and Xi Xiong", "abstract": "  The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence.\n", "link": "http://arxiv.org/abs/2505.06178v1", "date": "2025-05-09", "relevancy": 1.9532, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large%20Language%20Model-Enhanced%20Q-learning%20for%20Capacitated%20Vehicle%0A%20%20Routing%20Problem%20with%20Time%20Windows&body=Title%3A%20A%20Large%20Language%20Model-Enhanced%20Q-learning%20for%20Capacitated%20Vehicle%0A%20%20Routing%20Problem%20with%20Time%20Windows%0AAuthor%3A%20Linjiang%20Cao%20and%20Maonan%20Wang%20and%20Xi%20Xiong%0AAbstract%3A%20%20%20The%20Capacitated%20Vehicle%20Routing%20Problem%20with%20Time%20Windows%20%28CVRPTW%29%20is%20a%0Aclassic%20NP-hard%20combinatorial%20optimization%20problem%20widely%20applied%20in%20logistics%0Adistribution%20and%20transportation%20management.%20Its%20complexity%20stems%20from%20the%0Aconstraints%20of%20vehicle%20capacity%20and%20time%20windows%2C%20which%20pose%20significant%0Achallenges%20to%20traditional%20approaches.%20Advances%20in%20Large%20Language%20Models%20%28LLMs%29%0Aprovide%20new%20possibilities%20for%20finding%20approximate%20solutions%20to%20CVRPTW.%20This%0Apaper%20proposes%20a%20novel%20LLM-enhanced%20Q-learning%20framework%20to%20address%20the%20CVRPTW%0Awith%20real-time%20emergency%20constraints.%20Our%20solution%20introduces%20an%20adaptive%0Atwo-phase%20training%20mechanism%20that%20transitions%20from%20the%20LLM-guided%20exploration%0Aphase%20to%20the%20autonomous%20optimization%20phase%20of%20Q-network.%20To%20ensure%20reliability%2C%0Awe%20design%20a%20three-tier%20self-correction%20mechanism%20based%20on%20the%20Chain-of-Thought%0A%28CoT%29%20for%20LLMs%3A%20syntactic%20validation%2C%20semantic%20verification%2C%20and%20physical%0Aconstraint%20enforcement.%20In%20addition%2C%20we%20also%20prioritized%20replay%20of%20the%0Aexperience%20generated%20by%20LLMs%20to%20amplify%20the%20regulatory%20role%20of%20LLMs%20in%20the%0Aarchitecture.%20Experimental%20results%20demonstrate%20that%20our%20framework%20achieves%20a%0A7.3%5C%25%20average%20reduction%20in%20cost%20compared%20to%20traditional%20Q-learning%2C%20with%20fewer%0Atraining%20steps%20required%20for%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large%2520Language%2520Model-Enhanced%2520Q-learning%2520for%2520Capacitated%2520Vehicle%250A%2520%2520Routing%2520Problem%2520with%2520Time%2520Windows%26entry.906535625%3DLinjiang%2520Cao%2520and%2520Maonan%2520Wang%2520and%2520Xi%2520Xiong%26entry.1292438233%3D%2520%2520The%2520Capacitated%2520Vehicle%2520Routing%2520Problem%2520with%2520Time%2520Windows%2520%2528CVRPTW%2529%2520is%2520a%250Aclassic%2520NP-hard%2520combinatorial%2520optimization%2520problem%2520widely%2520applied%2520in%2520logistics%250Adistribution%2520and%2520transportation%2520management.%2520Its%2520complexity%2520stems%2520from%2520the%250Aconstraints%2520of%2520vehicle%2520capacity%2520and%2520time%2520windows%252C%2520which%2520pose%2520significant%250Achallenges%2520to%2520traditional%2520approaches.%2520Advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aprovide%2520new%2520possibilities%2520for%2520finding%2520approximate%2520solutions%2520to%2520CVRPTW.%2520This%250Apaper%2520proposes%2520a%2520novel%2520LLM-enhanced%2520Q-learning%2520framework%2520to%2520address%2520the%2520CVRPTW%250Awith%2520real-time%2520emergency%2520constraints.%2520Our%2520solution%2520introduces%2520an%2520adaptive%250Atwo-phase%2520training%2520mechanism%2520that%2520transitions%2520from%2520the%2520LLM-guided%2520exploration%250Aphase%2520to%2520the%2520autonomous%2520optimization%2520phase%2520of%2520Q-network.%2520To%2520ensure%2520reliability%252C%250Awe%2520design%2520a%2520three-tier%2520self-correction%2520mechanism%2520based%2520on%2520the%2520Chain-of-Thought%250A%2528CoT%2529%2520for%2520LLMs%253A%2520syntactic%2520validation%252C%2520semantic%2520verification%252C%2520and%2520physical%250Aconstraint%2520enforcement.%2520In%2520addition%252C%2520we%2520also%2520prioritized%2520replay%2520of%2520the%250Aexperience%2520generated%2520by%2520LLMs%2520to%2520amplify%2520the%2520regulatory%2520role%2520of%2520LLMs%2520in%2520the%250Aarchitecture.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520a%250A7.3%255C%2525%2520average%2520reduction%2520in%2520cost%2520compared%2520to%2520traditional%2520Q-learning%252C%2520with%2520fewer%250Atraining%2520steps%2520required%2520for%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large%20Language%20Model-Enhanced%20Q-learning%20for%20Capacitated%20Vehicle%0A%20%20Routing%20Problem%20with%20Time%20Windows&entry.906535625=Linjiang%20Cao%20and%20Maonan%20Wang%20and%20Xi%20Xiong&entry.1292438233=%20%20The%20Capacitated%20Vehicle%20Routing%20Problem%20with%20Time%20Windows%20%28CVRPTW%29%20is%20a%0Aclassic%20NP-hard%20combinatorial%20optimization%20problem%20widely%20applied%20in%20logistics%0Adistribution%20and%20transportation%20management.%20Its%20complexity%20stems%20from%20the%0Aconstraints%20of%20vehicle%20capacity%20and%20time%20windows%2C%20which%20pose%20significant%0Achallenges%20to%20traditional%20approaches.%20Advances%20in%20Large%20Language%20Models%20%28LLMs%29%0Aprovide%20new%20possibilities%20for%20finding%20approximate%20solutions%20to%20CVRPTW.%20This%0Apaper%20proposes%20a%20novel%20LLM-enhanced%20Q-learning%20framework%20to%20address%20the%20CVRPTW%0Awith%20real-time%20emergency%20constraints.%20Our%20solution%20introduces%20an%20adaptive%0Atwo-phase%20training%20mechanism%20that%20transitions%20from%20the%20LLM-guided%20exploration%0Aphase%20to%20the%20autonomous%20optimization%20phase%20of%20Q-network.%20To%20ensure%20reliability%2C%0Awe%20design%20a%20three-tier%20self-correction%20mechanism%20based%20on%20the%20Chain-of-Thought%0A%28CoT%29%20for%20LLMs%3A%20syntactic%20validation%2C%20semantic%20verification%2C%20and%20physical%0Aconstraint%20enforcement.%20In%20addition%2C%20we%20also%20prioritized%20replay%20of%20the%0Aexperience%20generated%20by%20LLMs%20to%20amplify%20the%20regulatory%20role%20of%20LLMs%20in%20the%0Aarchitecture.%20Experimental%20results%20demonstrate%20that%20our%20framework%20achieves%20a%0A7.3%5C%25%20average%20reduction%20in%20cost%20compared%20to%20traditional%20Q-learning%2C%20with%20fewer%0Atraining%20steps%20required%20for%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06178v1&entry.124074799=Read"},
{"title": "Efficient Quantum Convolutional Neural Networks for Image\n  Classification: Overcoming Hardware Constraints", "author": "Peter R\u00f6seler and Oliver Schaudt and Helmut Berg and Christian Bauckhage and Matthias Koch", "abstract": "  While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.\n", "link": "http://arxiv.org/abs/2505.05957v1", "date": "2025-05-09", "relevancy": 1.9493, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5048}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Quantum%20Convolutional%20Neural%20Networks%20for%20Image%0A%20%20Classification%3A%20Overcoming%20Hardware%20Constraints&body=Title%3A%20Efficient%20Quantum%20Convolutional%20Neural%20Networks%20for%20Image%0A%20%20Classification%3A%20Overcoming%20Hardware%20Constraints%0AAuthor%3A%20Peter%20R%C3%B6seler%20and%20Oliver%20Schaudt%20and%20Helmut%20Berg%20and%20Christian%20Bauckhage%20and%20Matthias%20Koch%0AAbstract%3A%20%20%20While%20classical%20convolutional%20neural%20networks%20%28CNNs%29%20have%20revolutionized%0Aimage%20classification%2C%20the%20emergence%20of%20quantum%20computing%20presents%20new%0Aopportunities%20for%20enhancing%20neural%20network%20architectures.%20Quantum%20CNNs%20%28QCNNs%29%0Aleverage%20quantum%20mechanical%20properties%20and%20hold%20potential%20to%20outperform%0Aclassical%20approaches.%20However%2C%20their%20implementation%20on%20current%20noisy%0Aintermediate-scale%20quantum%20%28NISQ%29%20devices%20remains%20challenging%20due%20to%20hardware%0Alimitations.%20In%20our%20research%2C%20we%20address%20this%20challenge%20by%20introducing%20an%0Aencoding%20scheme%20that%20significantly%20reduces%20the%20input%20dimensionality.%20We%0Ademonstrate%20that%20a%20primitive%20QCNN%20architecture%20with%2049%20qubits%20is%20sufficient%20to%0Adirectly%20process%20%2428%5Ctimes%2028%24%20pixel%20MNIST%20images%2C%20eliminating%20the%20need%20for%0Aclassical%20dimensionality%20reduction%20pre-processing.%20Additionally%2C%20we%20propose%20an%0Aautomated%20framework%20based%20on%20expressibility%2C%20entanglement%2C%20and%20complexity%0Acharacteristics%20to%20identify%20the%20building%20blocks%20of%20QCNNs%2C%20parameterized%20quantum%0Acircuits%20%28PQCs%29.%20Our%20approach%20demonstrates%20advantages%20in%20accuracy%20and%0Aconvergence%20speed%20with%20a%20similar%20parameter%20count%20compared%20to%20both%20hybrid%20QCNNs%0Aand%20classical%20CNNs.%20We%20validated%20our%20experiments%20on%20IBM%27s%20Heron%20r2%20quantum%0Aprocessor%2C%20achieving%20%2496.08%5C%25%24%20classification%20accuracy%2C%20surpassing%20the%0A%2471.74%5C%25%24%20benchmark%20of%20traditional%20approaches%20under%20identical%20training%0Aconditions.%20These%20results%20represent%20one%20of%20the%20first%20implementations%20of%20image%0Aclassifications%20on%20real%20quantum%20hardware%20and%20validate%20the%20potential%20of%20quantum%0Acomputing%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Quantum%2520Convolutional%2520Neural%2520Networks%2520for%2520Image%250A%2520%2520Classification%253A%2520Overcoming%2520Hardware%2520Constraints%26entry.906535625%3DPeter%2520R%25C3%25B6seler%2520and%2520Oliver%2520Schaudt%2520and%2520Helmut%2520Berg%2520and%2520Christian%2520Bauckhage%2520and%2520Matthias%2520Koch%26entry.1292438233%3D%2520%2520While%2520classical%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520revolutionized%250Aimage%2520classification%252C%2520the%2520emergence%2520of%2520quantum%2520computing%2520presents%2520new%250Aopportunities%2520for%2520enhancing%2520neural%2520network%2520architectures.%2520Quantum%2520CNNs%2520%2528QCNNs%2529%250Aleverage%2520quantum%2520mechanical%2520properties%2520and%2520hold%2520potential%2520to%2520outperform%250Aclassical%2520approaches.%2520However%252C%2520their%2520implementation%2520on%2520current%2520noisy%250Aintermediate-scale%2520quantum%2520%2528NISQ%2529%2520devices%2520remains%2520challenging%2520due%2520to%2520hardware%250Alimitations.%2520In%2520our%2520research%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%2520an%250Aencoding%2520scheme%2520that%2520significantly%2520reduces%2520the%2520input%2520dimensionality.%2520We%250Ademonstrate%2520that%2520a%2520primitive%2520QCNN%2520architecture%2520with%252049%2520qubits%2520is%2520sufficient%2520to%250Adirectly%2520process%2520%252428%255Ctimes%252028%2524%2520pixel%2520MNIST%2520images%252C%2520eliminating%2520the%2520need%2520for%250Aclassical%2520dimensionality%2520reduction%2520pre-processing.%2520Additionally%252C%2520we%2520propose%2520an%250Aautomated%2520framework%2520based%2520on%2520expressibility%252C%2520entanglement%252C%2520and%2520complexity%250Acharacteristics%2520to%2520identify%2520the%2520building%2520blocks%2520of%2520QCNNs%252C%2520parameterized%2520quantum%250Acircuits%2520%2528PQCs%2529.%2520Our%2520approach%2520demonstrates%2520advantages%2520in%2520accuracy%2520and%250Aconvergence%2520speed%2520with%2520a%2520similar%2520parameter%2520count%2520compared%2520to%2520both%2520hybrid%2520QCNNs%250Aand%2520classical%2520CNNs.%2520We%2520validated%2520our%2520experiments%2520on%2520IBM%2527s%2520Heron%2520r2%2520quantum%250Aprocessor%252C%2520achieving%2520%252496.08%255C%2525%2524%2520classification%2520accuracy%252C%2520surpassing%2520the%250A%252471.74%255C%2525%2524%2520benchmark%2520of%2520traditional%2520approaches%2520under%2520identical%2520training%250Aconditions.%2520These%2520results%2520represent%2520one%2520of%2520the%2520first%2520implementations%2520of%2520image%250Aclassifications%2520on%2520real%2520quantum%2520hardware%2520and%2520validate%2520the%2520potential%2520of%2520quantum%250Acomputing%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Quantum%20Convolutional%20Neural%20Networks%20for%20Image%0A%20%20Classification%3A%20Overcoming%20Hardware%20Constraints&entry.906535625=Peter%20R%C3%B6seler%20and%20Oliver%20Schaudt%20and%20Helmut%20Berg%20and%20Christian%20Bauckhage%20and%20Matthias%20Koch&entry.1292438233=%20%20While%20classical%20convolutional%20neural%20networks%20%28CNNs%29%20have%20revolutionized%0Aimage%20classification%2C%20the%20emergence%20of%20quantum%20computing%20presents%20new%0Aopportunities%20for%20enhancing%20neural%20network%20architectures.%20Quantum%20CNNs%20%28QCNNs%29%0Aleverage%20quantum%20mechanical%20properties%20and%20hold%20potential%20to%20outperform%0Aclassical%20approaches.%20However%2C%20their%20implementation%20on%20current%20noisy%0Aintermediate-scale%20quantum%20%28NISQ%29%20devices%20remains%20challenging%20due%20to%20hardware%0Alimitations.%20In%20our%20research%2C%20we%20address%20this%20challenge%20by%20introducing%20an%0Aencoding%20scheme%20that%20significantly%20reduces%20the%20input%20dimensionality.%20We%0Ademonstrate%20that%20a%20primitive%20QCNN%20architecture%20with%2049%20qubits%20is%20sufficient%20to%0Adirectly%20process%20%2428%5Ctimes%2028%24%20pixel%20MNIST%20images%2C%20eliminating%20the%20need%20for%0Aclassical%20dimensionality%20reduction%20pre-processing.%20Additionally%2C%20we%20propose%20an%0Aautomated%20framework%20based%20on%20expressibility%2C%20entanglement%2C%20and%20complexity%0Acharacteristics%20to%20identify%20the%20building%20blocks%20of%20QCNNs%2C%20parameterized%20quantum%0Acircuits%20%28PQCs%29.%20Our%20approach%20demonstrates%20advantages%20in%20accuracy%20and%0Aconvergence%20speed%20with%20a%20similar%20parameter%20count%20compared%20to%20both%20hybrid%20QCNNs%0Aand%20classical%20CNNs.%20We%20validated%20our%20experiments%20on%20IBM%27s%20Heron%20r2%20quantum%0Aprocessor%2C%20achieving%20%2496.08%5C%25%24%20classification%20accuracy%2C%20surpassing%20the%0A%2471.74%5C%25%24%20benchmark%20of%20traditional%20approaches%20under%20identical%20training%0Aconditions.%20These%20results%20represent%20one%20of%20the%20first%20implementations%20of%20image%0Aclassifications%20on%20real%20quantum%20hardware%20and%20validate%20the%20potential%20of%20quantum%0Acomputing%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05957v1&entry.124074799=Read"},
{"title": "Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free\n  Environmental Mapping for Autonomous Vehicles", "author": "Anupkumar Bochare", "abstract": "  Autonomous vehicle perception systems have traditionally relied on costly\nLiDAR sensors to generate precise environmental representations. In this paper,\nwe propose a camera-only perception framework that produces Bird's Eye View\n(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines\nYOLOv11-based object detection with DepthAnythingV2 monocular depth estimation\nacross multi-camera inputs to achieve comprehensive 360-degree scene\nunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenes\ndatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle\ndetection rates when compared against LiDAR ground truth, with average\npositional errors limited to 1.2 meters. These results highlight the potential\nof deep learning to extract rich spatial information using only camera inputs,\nenabling cost-efficient autonomous navigation without sacrificing accuracy.\n", "link": "http://arxiv.org/abs/2505.06113v1", "date": "2025-05-09", "relevancy": 1.9055, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6552}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6363}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera-Only%20Bird%27s%20Eye%20View%20Perception%3A%20A%20Neural%20Approach%20to%20LiDAR-Free%0A%20%20Environmental%20Mapping%20for%20Autonomous%20Vehicles&body=Title%3A%20Camera-Only%20Bird%27s%20Eye%20View%20Perception%3A%20A%20Neural%20Approach%20to%20LiDAR-Free%0A%20%20Environmental%20Mapping%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Anupkumar%20Bochare%0AAbstract%3A%20%20%20Autonomous%20vehicle%20perception%20systems%20have%20traditionally%20relied%20on%20costly%0ALiDAR%20sensors%20to%20generate%20precise%20environmental%20representations.%20In%20this%20paper%2C%0Awe%20propose%20a%20camera-only%20perception%20framework%20that%20produces%20Bird%27s%20Eye%20View%0A%28BEV%29%20maps%20by%20extending%20the%20Lift-Splat-Shoot%20architecture.%20Our%20method%20combines%0AYOLOv11-based%20object%20detection%20with%20DepthAnythingV2%20monocular%20depth%20estimation%0Aacross%20multi-camera%20inputs%20to%20achieve%20comprehensive%20360-degree%20scene%0Aunderstanding.%20We%20evaluate%20our%20approach%20on%20the%20OpenLane-V2%20and%20NuScenes%0Adatasets%2C%20achieving%20up%20to%2085%25%20road%20segmentation%20accuracy%20and%2085-90%25%20vehicle%0Adetection%20rates%20when%20compared%20against%20LiDAR%20ground%20truth%2C%20with%20average%0Apositional%20errors%20limited%20to%201.2%20meters.%20These%20results%20highlight%20the%20potential%0Aof%20deep%20learning%20to%20extract%20rich%20spatial%20information%20using%20only%20camera%20inputs%2C%0Aenabling%20cost-efficient%20autonomous%20navigation%20without%20sacrificing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera-Only%2520Bird%2527s%2520Eye%2520View%2520Perception%253A%2520A%2520Neural%2520Approach%2520to%2520LiDAR-Free%250A%2520%2520Environmental%2520Mapping%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DAnupkumar%2520Bochare%26entry.1292438233%3D%2520%2520Autonomous%2520vehicle%2520perception%2520systems%2520have%2520traditionally%2520relied%2520on%2520costly%250ALiDAR%2520sensors%2520to%2520generate%2520precise%2520environmental%2520representations.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520camera-only%2520perception%2520framework%2520that%2520produces%2520Bird%2527s%2520Eye%2520View%250A%2528BEV%2529%2520maps%2520by%2520extending%2520the%2520Lift-Splat-Shoot%2520architecture.%2520Our%2520method%2520combines%250AYOLOv11-based%2520object%2520detection%2520with%2520DepthAnythingV2%2520monocular%2520depth%2520estimation%250Aacross%2520multi-camera%2520inputs%2520to%2520achieve%2520comprehensive%2520360-degree%2520scene%250Aunderstanding.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520OpenLane-V2%2520and%2520NuScenes%250Adatasets%252C%2520achieving%2520up%2520to%252085%2525%2520road%2520segmentation%2520accuracy%2520and%252085-90%2525%2520vehicle%250Adetection%2520rates%2520when%2520compared%2520against%2520LiDAR%2520ground%2520truth%252C%2520with%2520average%250Apositional%2520errors%2520limited%2520to%25201.2%2520meters.%2520These%2520results%2520highlight%2520the%2520potential%250Aof%2520deep%2520learning%2520to%2520extract%2520rich%2520spatial%2520information%2520using%2520only%2520camera%2520inputs%252C%250Aenabling%2520cost-efficient%2520autonomous%2520navigation%2520without%2520sacrificing%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera-Only%20Bird%27s%20Eye%20View%20Perception%3A%20A%20Neural%20Approach%20to%20LiDAR-Free%0A%20%20Environmental%20Mapping%20for%20Autonomous%20Vehicles&entry.906535625=Anupkumar%20Bochare&entry.1292438233=%20%20Autonomous%20vehicle%20perception%20systems%20have%20traditionally%20relied%20on%20costly%0ALiDAR%20sensors%20to%20generate%20precise%20environmental%20representations.%20In%20this%20paper%2C%0Awe%20propose%20a%20camera-only%20perception%20framework%20that%20produces%20Bird%27s%20Eye%20View%0A%28BEV%29%20maps%20by%20extending%20the%20Lift-Splat-Shoot%20architecture.%20Our%20method%20combines%0AYOLOv11-based%20object%20detection%20with%20DepthAnythingV2%20monocular%20depth%20estimation%0Aacross%20multi-camera%20inputs%20to%20achieve%20comprehensive%20360-degree%20scene%0Aunderstanding.%20We%20evaluate%20our%20approach%20on%20the%20OpenLane-V2%20and%20NuScenes%0Adatasets%2C%20achieving%20up%20to%2085%25%20road%20segmentation%20accuracy%20and%2085-90%25%20vehicle%0Adetection%20rates%20when%20compared%20against%20LiDAR%20ground%20truth%2C%20with%20average%0Apositional%20errors%20limited%20to%201.2%20meters.%20These%20results%20highlight%20the%20potential%0Aof%20deep%20learning%20to%20extract%20rich%20spatial%20information%20using%20only%20camera%20inputs%2C%0Aenabling%20cost-efficient%20autonomous%20navigation%20without%20sacrificing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06113v1&entry.124074799=Read"},
{"title": "Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in\n  Michigan-Style Learning Fuzzy-Classifier Systems", "author": "Hiroki Shiraishi and Yohei Hayamizu and Tomonori Hashiyama", "abstract": "  This paper focuses on the impact of rule representation in Michigan-style\nLearning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A\nwell-representation of the rules in an LFCS is crucial for improving its\nperformance. However, conventional rule representations frequently need help\naddressing problems with unknown data characteristics. To address this issue,\nthis paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive\nrule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates\na fuzzy indicator as a new rule parameter that sets the membership function of\na rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes.\nThe fuzzy indicator is optimized with evolutionary operators, allowing the\nsystem to search for an optimal rule representation. Results from extensive\nexperiments conducted on continuous space problems demonstrate that\nAdaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular\nand fuzzy-hypertrapezoidal rule representations in classification accuracy.\nAdditionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and\nreal-world problems with inherent uncertainty, such as missing values, leading\nto stable classification performance.\n", "link": "http://arxiv.org/abs/2505.06017v1", "date": "2025-05-09", "relevancy": 1.8964, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.494}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fuzzy-UCS%20Revisited%3A%20Self-Adaptation%20of%20Rule%20Representations%20in%0A%20%20Michigan-Style%20Learning%20Fuzzy-Classifier%20Systems&body=Title%3A%20Fuzzy-UCS%20Revisited%3A%20Self-Adaptation%20of%20Rule%20Representations%20in%0A%20%20Michigan-Style%20Learning%20Fuzzy-Classifier%20Systems%0AAuthor%3A%20Hiroki%20Shiraishi%20and%20Yohei%20Hayamizu%20and%20Tomonori%20Hashiyama%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20impact%20of%20rule%20representation%20in%20Michigan-style%0ALearning%20Fuzzy-Classifier%20Systems%20%28LFCSs%29%20on%20its%20classification%20performance.%20A%0Awell-representation%20of%20the%20rules%20in%20an%20LFCS%20is%20crucial%20for%20improving%20its%0Aperformance.%20However%2C%20conventional%20rule%20representations%20frequently%20need%20help%0Aaddressing%20problems%20with%20unknown%20data%20characteristics.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20supervised%20LFCS%20%28i.e.%2C%20Fuzzy-UCS%29%20with%20a%20self-adaptive%0Arule%20representation%20mechanism%2C%20entitled%20Adaptive-UCS.%20Adaptive-UCS%20incorporates%0Aa%20fuzzy%20indicator%20as%20a%20new%20rule%20parameter%20that%20sets%20the%20membership%20function%20of%0Aa%20rule%20as%20either%20rectangular%20%28i.e.%2C%20crisp%29%20or%20triangular%20%28i.e.%2C%20fuzzy%29%20shapes.%0AThe%20fuzzy%20indicator%20is%20optimized%20with%20evolutionary%20operators%2C%20allowing%20the%0Asystem%20to%20search%20for%20an%20optimal%20rule%20representation.%20Results%20from%20extensive%0Aexperiments%20conducted%20on%20continuous%20space%20problems%20demonstrate%20that%0AAdaptive-UCS%20outperforms%20other%20UCSs%20with%20conventional%20crisp-hyperrectangular%0Aand%20fuzzy-hypertrapezoidal%20rule%20representations%20in%20classification%20accuracy.%0AAdditionally%2C%20Adaptive-UCS%20exhibits%20robustness%20in%20the%20case%20of%20noisy%20inputs%20and%0Areal-world%20problems%20with%20inherent%20uncertainty%2C%20such%20as%20missing%20values%2C%20leading%0Ato%20stable%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuzzy-UCS%2520Revisited%253A%2520Self-Adaptation%2520of%2520Rule%2520Representations%2520in%250A%2520%2520Michigan-Style%2520Learning%2520Fuzzy-Classifier%2520Systems%26entry.906535625%3DHiroki%2520Shiraishi%2520and%2520Yohei%2520Hayamizu%2520and%2520Tomonori%2520Hashiyama%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520impact%2520of%2520rule%2520representation%2520in%2520Michigan-style%250ALearning%2520Fuzzy-Classifier%2520Systems%2520%2528LFCSs%2529%2520on%2520its%2520classification%2520performance.%2520A%250Awell-representation%2520of%2520the%2520rules%2520in%2520an%2520LFCS%2520is%2520crucial%2520for%2520improving%2520its%250Aperformance.%2520However%252C%2520conventional%2520rule%2520representations%2520frequently%2520need%2520help%250Aaddressing%2520problems%2520with%2520unknown%2520data%2520characteristics.%2520To%2520address%2520this%2520issue%252C%250Athis%2520paper%2520proposes%2520a%2520supervised%2520LFCS%2520%2528i.e.%252C%2520Fuzzy-UCS%2529%2520with%2520a%2520self-adaptive%250Arule%2520representation%2520mechanism%252C%2520entitled%2520Adaptive-UCS.%2520Adaptive-UCS%2520incorporates%250Aa%2520fuzzy%2520indicator%2520as%2520a%2520new%2520rule%2520parameter%2520that%2520sets%2520the%2520membership%2520function%2520of%250Aa%2520rule%2520as%2520either%2520rectangular%2520%2528i.e.%252C%2520crisp%2529%2520or%2520triangular%2520%2528i.e.%252C%2520fuzzy%2529%2520shapes.%250AThe%2520fuzzy%2520indicator%2520is%2520optimized%2520with%2520evolutionary%2520operators%252C%2520allowing%2520the%250Asystem%2520to%2520search%2520for%2520an%2520optimal%2520rule%2520representation.%2520Results%2520from%2520extensive%250Aexperiments%2520conducted%2520on%2520continuous%2520space%2520problems%2520demonstrate%2520that%250AAdaptive-UCS%2520outperforms%2520other%2520UCSs%2520with%2520conventional%2520crisp-hyperrectangular%250Aand%2520fuzzy-hypertrapezoidal%2520rule%2520representations%2520in%2520classification%2520accuracy.%250AAdditionally%252C%2520Adaptive-UCS%2520exhibits%2520robustness%2520in%2520the%2520case%2520of%2520noisy%2520inputs%2520and%250Areal-world%2520problems%2520with%2520inherent%2520uncertainty%252C%2520such%2520as%2520missing%2520values%252C%2520leading%250Ato%2520stable%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuzzy-UCS%20Revisited%3A%20Self-Adaptation%20of%20Rule%20Representations%20in%0A%20%20Michigan-Style%20Learning%20Fuzzy-Classifier%20Systems&entry.906535625=Hiroki%20Shiraishi%20and%20Yohei%20Hayamizu%20and%20Tomonori%20Hashiyama&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20impact%20of%20rule%20representation%20in%20Michigan-style%0ALearning%20Fuzzy-Classifier%20Systems%20%28LFCSs%29%20on%20its%20classification%20performance.%20A%0Awell-representation%20of%20the%20rules%20in%20an%20LFCS%20is%20crucial%20for%20improving%20its%0Aperformance.%20However%2C%20conventional%20rule%20representations%20frequently%20need%20help%0Aaddressing%20problems%20with%20unknown%20data%20characteristics.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20supervised%20LFCS%20%28i.e.%2C%20Fuzzy-UCS%29%20with%20a%20self-adaptive%0Arule%20representation%20mechanism%2C%20entitled%20Adaptive-UCS.%20Adaptive-UCS%20incorporates%0Aa%20fuzzy%20indicator%20as%20a%20new%20rule%20parameter%20that%20sets%20the%20membership%20function%20of%0Aa%20rule%20as%20either%20rectangular%20%28i.e.%2C%20crisp%29%20or%20triangular%20%28i.e.%2C%20fuzzy%29%20shapes.%0AThe%20fuzzy%20indicator%20is%20optimized%20with%20evolutionary%20operators%2C%20allowing%20the%0Asystem%20to%20search%20for%20an%20optimal%20rule%20representation.%20Results%20from%20extensive%0Aexperiments%20conducted%20on%20continuous%20space%20problems%20demonstrate%20that%0AAdaptive-UCS%20outperforms%20other%20UCSs%20with%20conventional%20crisp-hyperrectangular%0Aand%20fuzzy-hypertrapezoidal%20rule%20representations%20in%20classification%20accuracy.%0AAdditionally%2C%20Adaptive-UCS%20exhibits%20robustness%20in%20the%20case%20of%20noisy%20inputs%20and%0Areal-world%20problems%20with%20inherent%20uncertainty%2C%20such%20as%20missing%20values%2C%20leading%0Ato%20stable%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06017v1&entry.124074799=Read"},
{"title": "Universal Approximation Theorem for Deep Q-Learning via FBSDE System", "author": "Qian Qi", "abstract": "  The approximation capabilities of Deep Q-Networks (DQNs) are commonly\njustified by general Universal Approximation Theorems (UATs) that do not\nleverage the intrinsic structural properties of the optimal Q-function, the\nsolution to a Bellman equation. This paper establishes a UAT for a class of\nDQNs whose architecture is designed to emulate the iterative refinement process\ninherent in Bellman updates. A central element of our analysis is the\npropagation of regularity: while the transformation induced by a single Bellman\noperator application exhibits regularity, for which Backward Stochastic\nDifferential Equations (BSDEs) theory provides analytical tools, the uniform\nregularity of the entire sequence of value iteration iterates--specifically,\ntheir uniform Lipschitz continuity on compact domains under standard Lipschitz\nassumptions on the problem data--is derived from finite-horizon dynamic\nprogramming principles. We demonstrate that layers of a deep residual network,\nconceived as neural operators acting on function spaces, can approximate the\naction of the Bellman operator. The resulting approximation theorem is thus\nintrinsically linked to the control problem's structure, offering a proof\ntechnique wherein network depth directly corresponds to iterations of value\nfunction refinement, accompanied by controlled error propagation. This\nperspective reveals a dynamic systems view of the network's operation on a\nspace of value functions.\n", "link": "http://arxiv.org/abs/2505.06023v1", "date": "2025-05-09", "relevancy": 1.8893, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4754}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4735}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Approximation%20Theorem%20for%20Deep%20Q-Learning%20via%20FBSDE%20System&body=Title%3A%20Universal%20Approximation%20Theorem%20for%20Deep%20Q-Learning%20via%20FBSDE%20System%0AAuthor%3A%20Qian%20Qi%0AAbstract%3A%20%20%20The%20approximation%20capabilities%20of%20Deep%20Q-Networks%20%28DQNs%29%20are%20commonly%0Ajustified%20by%20general%20Universal%20Approximation%20Theorems%20%28UATs%29%20that%20do%20not%0Aleverage%20the%20intrinsic%20structural%20properties%20of%20the%20optimal%20Q-function%2C%20the%0Asolution%20to%20a%20Bellman%20equation.%20This%20paper%20establishes%20a%20UAT%20for%20a%20class%20of%0ADQNs%20whose%20architecture%20is%20designed%20to%20emulate%20the%20iterative%20refinement%20process%0Ainherent%20in%20Bellman%20updates.%20A%20central%20element%20of%20our%20analysis%20is%20the%0Apropagation%20of%20regularity%3A%20while%20the%20transformation%20induced%20by%20a%20single%20Bellman%0Aoperator%20application%20exhibits%20regularity%2C%20for%20which%20Backward%20Stochastic%0ADifferential%20Equations%20%28BSDEs%29%20theory%20provides%20analytical%20tools%2C%20the%20uniform%0Aregularity%20of%20the%20entire%20sequence%20of%20value%20iteration%20iterates--specifically%2C%0Atheir%20uniform%20Lipschitz%20continuity%20on%20compact%20domains%20under%20standard%20Lipschitz%0Aassumptions%20on%20the%20problem%20data--is%20derived%20from%20finite-horizon%20dynamic%0Aprogramming%20principles.%20We%20demonstrate%20that%20layers%20of%20a%20deep%20residual%20network%2C%0Aconceived%20as%20neural%20operators%20acting%20on%20function%20spaces%2C%20can%20approximate%20the%0Aaction%20of%20the%20Bellman%20operator.%20The%20resulting%20approximation%20theorem%20is%20thus%0Aintrinsically%20linked%20to%20the%20control%20problem%27s%20structure%2C%20offering%20a%20proof%0Atechnique%20wherein%20network%20depth%20directly%20corresponds%20to%20iterations%20of%20value%0Afunction%20refinement%2C%20accompanied%20by%20controlled%20error%20propagation.%20This%0Aperspective%20reveals%20a%20dynamic%20systems%20view%20of%20the%20network%27s%20operation%20on%20a%0Aspace%20of%20value%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Approximation%2520Theorem%2520for%2520Deep%2520Q-Learning%2520via%2520FBSDE%2520System%26entry.906535625%3DQian%2520Qi%26entry.1292438233%3D%2520%2520The%2520approximation%2520capabilities%2520of%2520Deep%2520Q-Networks%2520%2528DQNs%2529%2520are%2520commonly%250Ajustified%2520by%2520general%2520Universal%2520Approximation%2520Theorems%2520%2528UATs%2529%2520that%2520do%2520not%250Aleverage%2520the%2520intrinsic%2520structural%2520properties%2520of%2520the%2520optimal%2520Q-function%252C%2520the%250Asolution%2520to%2520a%2520Bellman%2520equation.%2520This%2520paper%2520establishes%2520a%2520UAT%2520for%2520a%2520class%2520of%250ADQNs%2520whose%2520architecture%2520is%2520designed%2520to%2520emulate%2520the%2520iterative%2520refinement%2520process%250Ainherent%2520in%2520Bellman%2520updates.%2520A%2520central%2520element%2520of%2520our%2520analysis%2520is%2520the%250Apropagation%2520of%2520regularity%253A%2520while%2520the%2520transformation%2520induced%2520by%2520a%2520single%2520Bellman%250Aoperator%2520application%2520exhibits%2520regularity%252C%2520for%2520which%2520Backward%2520Stochastic%250ADifferential%2520Equations%2520%2528BSDEs%2529%2520theory%2520provides%2520analytical%2520tools%252C%2520the%2520uniform%250Aregularity%2520of%2520the%2520entire%2520sequence%2520of%2520value%2520iteration%2520iterates--specifically%252C%250Atheir%2520uniform%2520Lipschitz%2520continuity%2520on%2520compact%2520domains%2520under%2520standard%2520Lipschitz%250Aassumptions%2520on%2520the%2520problem%2520data--is%2520derived%2520from%2520finite-horizon%2520dynamic%250Aprogramming%2520principles.%2520We%2520demonstrate%2520that%2520layers%2520of%2520a%2520deep%2520residual%2520network%252C%250Aconceived%2520as%2520neural%2520operators%2520acting%2520on%2520function%2520spaces%252C%2520can%2520approximate%2520the%250Aaction%2520of%2520the%2520Bellman%2520operator.%2520The%2520resulting%2520approximation%2520theorem%2520is%2520thus%250Aintrinsically%2520linked%2520to%2520the%2520control%2520problem%2527s%2520structure%252C%2520offering%2520a%2520proof%250Atechnique%2520wherein%2520network%2520depth%2520directly%2520corresponds%2520to%2520iterations%2520of%2520value%250Afunction%2520refinement%252C%2520accompanied%2520by%2520controlled%2520error%2520propagation.%2520This%250Aperspective%2520reveals%2520a%2520dynamic%2520systems%2520view%2520of%2520the%2520network%2527s%2520operation%2520on%2520a%250Aspace%2520of%2520value%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Approximation%20Theorem%20for%20Deep%20Q-Learning%20via%20FBSDE%20System&entry.906535625=Qian%20Qi&entry.1292438233=%20%20The%20approximation%20capabilities%20of%20Deep%20Q-Networks%20%28DQNs%29%20are%20commonly%0Ajustified%20by%20general%20Universal%20Approximation%20Theorems%20%28UATs%29%20that%20do%20not%0Aleverage%20the%20intrinsic%20structural%20properties%20of%20the%20optimal%20Q-function%2C%20the%0Asolution%20to%20a%20Bellman%20equation.%20This%20paper%20establishes%20a%20UAT%20for%20a%20class%20of%0ADQNs%20whose%20architecture%20is%20designed%20to%20emulate%20the%20iterative%20refinement%20process%0Ainherent%20in%20Bellman%20updates.%20A%20central%20element%20of%20our%20analysis%20is%20the%0Apropagation%20of%20regularity%3A%20while%20the%20transformation%20induced%20by%20a%20single%20Bellman%0Aoperator%20application%20exhibits%20regularity%2C%20for%20which%20Backward%20Stochastic%0ADifferential%20Equations%20%28BSDEs%29%20theory%20provides%20analytical%20tools%2C%20the%20uniform%0Aregularity%20of%20the%20entire%20sequence%20of%20value%20iteration%20iterates--specifically%2C%0Atheir%20uniform%20Lipschitz%20continuity%20on%20compact%20domains%20under%20standard%20Lipschitz%0Aassumptions%20on%20the%20problem%20data--is%20derived%20from%20finite-horizon%20dynamic%0Aprogramming%20principles.%20We%20demonstrate%20that%20layers%20of%20a%20deep%20residual%20network%2C%0Aconceived%20as%20neural%20operators%20acting%20on%20function%20spaces%2C%20can%20approximate%20the%0Aaction%20of%20the%20Bellman%20operator.%20The%20resulting%20approximation%20theorem%20is%20thus%0Aintrinsically%20linked%20to%20the%20control%20problem%27s%20structure%2C%20offering%20a%20proof%0Atechnique%20wherein%20network%20depth%20directly%20corresponds%20to%20iterations%20of%20value%0Afunction%20refinement%2C%20accompanied%20by%20controlled%20error%20propagation.%20This%0Aperspective%20reveals%20a%20dynamic%20systems%20view%20of%20the%20network%27s%20operation%20on%20a%0Aspace%20of%20value%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06023v1&entry.124074799=Read"},
{"title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in\n  Text Classification", "author": "Leon Eshuijs and Shihan Wang and Antske Fokkens", "abstract": "  Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads.\n", "link": "http://arxiv.org/abs/2505.06032v1", "date": "2025-05-09", "relevancy": 1.8816, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4763}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short-circuiting%20Shortcuts%3A%20Mechanistic%20Investigation%20of%20Shortcuts%20in%0A%20%20Text%20Classification&body=Title%3A%20Short-circuiting%20Shortcuts%3A%20Mechanistic%20Investigation%20of%20Shortcuts%20in%0A%20%20Text%20Classification%0AAuthor%3A%20Leon%20Eshuijs%20and%20Shihan%20Wang%20and%20Antske%20Fokkens%0AAbstract%3A%20%20%20Reliance%20on%20spurious%20correlations%20%28shortcuts%29%20has%20been%20shown%20to%20underlie%20many%0Aof%20the%20successes%20of%20language%20models.%20Previous%20work%20focused%20on%20identifying%20the%0Ainput%20elements%20that%20impact%20prediction.%20We%20investigate%20how%20shortcuts%20are%0Aactually%20processed%20within%20the%20model%27s%20decision-making%20mechanism.%20We%20use%20actor%0Anames%20in%20movie%20reviews%20as%20controllable%20shortcuts%20with%20known%20impact%20on%20the%0Aoutcome.%20We%20use%20mechanistic%20interpretability%20methods%20and%20identify%20specific%0Aattention%20heads%20that%20focus%20on%20shortcuts.%20These%20heads%20gear%20the%20model%20towards%20a%0Alabel%20before%20processing%20the%20complete%20input%2C%20effectively%20making%20premature%0Adecisions%20that%20bypass%20contextual%20analysis.%20Based%20on%20these%20findings%2C%20we%0Aintroduce%20Head-based%20Token%20Attribution%20%28HTA%29%2C%20which%20traces%20intermediate%0Adecisions%20back%20to%20input%20tokens.%20We%20show%20that%20HTA%20is%20effective%20in%20detecting%0Ashortcuts%20in%20LLMs%20and%20enables%20targeted%20mitigation%20by%20selectively%20deactivating%0Ashortcut-related%20attention%20heads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort-circuiting%2520Shortcuts%253A%2520Mechanistic%2520Investigation%2520of%2520Shortcuts%2520in%250A%2520%2520Text%2520Classification%26entry.906535625%3DLeon%2520Eshuijs%2520and%2520Shihan%2520Wang%2520and%2520Antske%2520Fokkens%26entry.1292438233%3D%2520%2520Reliance%2520on%2520spurious%2520correlations%2520%2528shortcuts%2529%2520has%2520been%2520shown%2520to%2520underlie%2520many%250Aof%2520the%2520successes%2520of%2520language%2520models.%2520Previous%2520work%2520focused%2520on%2520identifying%2520the%250Ainput%2520elements%2520that%2520impact%2520prediction.%2520We%2520investigate%2520how%2520shortcuts%2520are%250Aactually%2520processed%2520within%2520the%2520model%2527s%2520decision-making%2520mechanism.%2520We%2520use%2520actor%250Anames%2520in%2520movie%2520reviews%2520as%2520controllable%2520shortcuts%2520with%2520known%2520impact%2520on%2520the%250Aoutcome.%2520We%2520use%2520mechanistic%2520interpretability%2520methods%2520and%2520identify%2520specific%250Aattention%2520heads%2520that%2520focus%2520on%2520shortcuts.%2520These%2520heads%2520gear%2520the%2520model%2520towards%2520a%250Alabel%2520before%2520processing%2520the%2520complete%2520input%252C%2520effectively%2520making%2520premature%250Adecisions%2520that%2520bypass%2520contextual%2520analysis.%2520Based%2520on%2520these%2520findings%252C%2520we%250Aintroduce%2520Head-based%2520Token%2520Attribution%2520%2528HTA%2529%252C%2520which%2520traces%2520intermediate%250Adecisions%2520back%2520to%2520input%2520tokens.%2520We%2520show%2520that%2520HTA%2520is%2520effective%2520in%2520detecting%250Ashortcuts%2520in%2520LLMs%2520and%2520enables%2520targeted%2520mitigation%2520by%2520selectively%2520deactivating%250Ashortcut-related%2520attention%2520heads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short-circuiting%20Shortcuts%3A%20Mechanistic%20Investigation%20of%20Shortcuts%20in%0A%20%20Text%20Classification&entry.906535625=Leon%20Eshuijs%20and%20Shihan%20Wang%20and%20Antske%20Fokkens&entry.1292438233=%20%20Reliance%20on%20spurious%20correlations%20%28shortcuts%29%20has%20been%20shown%20to%20underlie%20many%0Aof%20the%20successes%20of%20language%20models.%20Previous%20work%20focused%20on%20identifying%20the%0Ainput%20elements%20that%20impact%20prediction.%20We%20investigate%20how%20shortcuts%20are%0Aactually%20processed%20within%20the%20model%27s%20decision-making%20mechanism.%20We%20use%20actor%0Anames%20in%20movie%20reviews%20as%20controllable%20shortcuts%20with%20known%20impact%20on%20the%0Aoutcome.%20We%20use%20mechanistic%20interpretability%20methods%20and%20identify%20specific%0Aattention%20heads%20that%20focus%20on%20shortcuts.%20These%20heads%20gear%20the%20model%20towards%20a%0Alabel%20before%20processing%20the%20complete%20input%2C%20effectively%20making%20premature%0Adecisions%20that%20bypass%20contextual%20analysis.%20Based%20on%20these%20findings%2C%20we%0Aintroduce%20Head-based%20Token%20Attribution%20%28HTA%29%2C%20which%20traces%20intermediate%0Adecisions%20back%20to%20input%20tokens.%20We%20show%20that%20HTA%20is%20effective%20in%20detecting%0Ashortcuts%20in%20LLMs%20and%20enables%20targeted%20mitigation%20by%20selectively%20deactivating%0Ashortcut-related%20attention%20heads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06032v1&entry.124074799=Read"},
{"title": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy", "author": "Haoqi Wu and Wei Dai and Li Wang and Qiang Yan", "abstract": "  Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.\n", "link": "http://arxiv.org/abs/2505.05922v1", "date": "2025-05-09", "relevancy": 1.8607, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPE%3A%20Context-Aware%20Prompt%20Perturbation%20Mechanism%20with%20Differential%0A%20%20Privacy&body=Title%3A%20CAPE%3A%20Context-Aware%20Prompt%20Perturbation%20Mechanism%20with%20Differential%0A%20%20Privacy%0AAuthor%3A%20Haoqi%20Wu%20and%20Wei%20Dai%20and%20Li%20Wang%20and%20Qiang%20Yan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20significant%20popularity%20due%20to%20their%0Aremarkable%20capabilities%20in%20text%20understanding%20and%20generation.%20However%2C%20despite%0Atheir%20widespread%20deployment%20in%20inference%20services%20such%20as%20ChatGPT%2C%20concerns%0Aabout%20the%20potential%20leakage%20of%20sensitive%20user%20data%20have%20arisen.%20Existing%0Asolutions%20primarily%20rely%20on%20privacy-enhancing%20technologies%20to%20mitigate%20such%0Arisks%2C%20facing%20the%20trade-off%20among%20efficiency%2C%20privacy%2C%20and%20utility.%20To%20narrow%0Athis%20gap%2C%20we%20propose%20Cape%2C%20a%20context-aware%20prompt%20perturbation%20mechanism%20based%0Aon%20differential%20privacy%2C%20to%20enable%20efficient%20inference%20with%20an%20improved%0Aprivacy-utility%20trade-off.%20Concretely%2C%20we%20introduce%20a%20hybrid%20utility%20function%0Athat%20better%20captures%20the%20token%20similarity.%20Additionally%2C%20we%20propose%20a%0Abucketized%20sampling%20mechanism%20to%20handle%20large%20sampling%20space%2C%20which%20might%20lead%0Ato%20long-tail%20phenomenons.%20Extensive%20experiments%20across%20multiple%20datasets%2C%20along%0Awith%20ablation%20studies%2C%20demonstrate%20that%20Cape%20achieves%20a%20better%20privacy-utility%0Atrade-off%20compared%20to%20prior%20state-of-the-art%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPE%253A%2520Context-Aware%2520Prompt%2520Perturbation%2520Mechanism%2520with%2520Differential%250A%2520%2520Privacy%26entry.906535625%3DHaoqi%2520Wu%2520and%2520Wei%2520Dai%2520and%2520Li%2520Wang%2520and%2520Qiang%2520Yan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520gained%2520significant%2520popularity%2520due%2520to%2520their%250Aremarkable%2520capabilities%2520in%2520text%2520understanding%2520and%2520generation.%2520However%252C%2520despite%250Atheir%2520widespread%2520deployment%2520in%2520inference%2520services%2520such%2520as%2520ChatGPT%252C%2520concerns%250Aabout%2520the%2520potential%2520leakage%2520of%2520sensitive%2520user%2520data%2520have%2520arisen.%2520Existing%250Asolutions%2520primarily%2520rely%2520on%2520privacy-enhancing%2520technologies%2520to%2520mitigate%2520such%250Arisks%252C%2520facing%2520the%2520trade-off%2520among%2520efficiency%252C%2520privacy%252C%2520and%2520utility.%2520To%2520narrow%250Athis%2520gap%252C%2520we%2520propose%2520Cape%252C%2520a%2520context-aware%2520prompt%2520perturbation%2520mechanism%2520based%250Aon%2520differential%2520privacy%252C%2520to%2520enable%2520efficient%2520inference%2520with%2520an%2520improved%250Aprivacy-utility%2520trade-off.%2520Concretely%252C%2520we%2520introduce%2520a%2520hybrid%2520utility%2520function%250Athat%2520better%2520captures%2520the%2520token%2520similarity.%2520Additionally%252C%2520we%2520propose%2520a%250Abucketized%2520sampling%2520mechanism%2520to%2520handle%2520large%2520sampling%2520space%252C%2520which%2520might%2520lead%250Ato%2520long-tail%2520phenomenons.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%252C%2520along%250Awith%2520ablation%2520studies%252C%2520demonstrate%2520that%2520Cape%2520achieves%2520a%2520better%2520privacy-utility%250Atrade-off%2520compared%2520to%2520prior%2520state-of-the-art%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPE%3A%20Context-Aware%20Prompt%20Perturbation%20Mechanism%20with%20Differential%0A%20%20Privacy&entry.906535625=Haoqi%20Wu%20and%20Wei%20Dai%20and%20Li%20Wang%20and%20Qiang%20Yan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20significant%20popularity%20due%20to%20their%0Aremarkable%20capabilities%20in%20text%20understanding%20and%20generation.%20However%2C%20despite%0Atheir%20widespread%20deployment%20in%20inference%20services%20such%20as%20ChatGPT%2C%20concerns%0Aabout%20the%20potential%20leakage%20of%20sensitive%20user%20data%20have%20arisen.%20Existing%0Asolutions%20primarily%20rely%20on%20privacy-enhancing%20technologies%20to%20mitigate%20such%0Arisks%2C%20facing%20the%20trade-off%20among%20efficiency%2C%20privacy%2C%20and%20utility.%20To%20narrow%0Athis%20gap%2C%20we%20propose%20Cape%2C%20a%20context-aware%20prompt%20perturbation%20mechanism%20based%0Aon%20differential%20privacy%2C%20to%20enable%20efficient%20inference%20with%20an%20improved%0Aprivacy-utility%20trade-off.%20Concretely%2C%20we%20introduce%20a%20hybrid%20utility%20function%0Athat%20better%20captures%20the%20token%20similarity.%20Additionally%2C%20we%20propose%20a%0Abucketized%20sampling%20mechanism%20to%20handle%20large%20sampling%20space%2C%20which%20might%20lead%0Ato%20long-tail%20phenomenons.%20Extensive%20experiments%20across%20multiple%20datasets%2C%20along%0Awith%20ablation%20studies%2C%20demonstrate%20that%20Cape%20achieves%20a%20better%20privacy-utility%0Atrade-off%20compared%20to%20prior%20state-of-the-art%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05922v1&entry.124074799=Read"},
{"title": "k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via\n  LLM-based Centroids", "author": "Jairo Diaz-Rodriguez", "abstract": "  We introduce k-LLMmeans, a novel modification of the k-means algorithm for\ntext clustering that leverages LLM-generated summaries as cluster centroids,\ncapturing semantic nuances often missed by purely numerical averages. This\ndesign preserves the core optimization properties of k-means while enhancing\nsemantic interpretability and avoiding the scalability and instability issues\ntypical of modern LLM-based clustering. Unlike existing methods, our approach\ndoes not increase LLM usage with dataset size and produces transparent\nintermediate outputs. We further extend it with a mini-batch variant for\nefficient, real-time clustering of streaming text. Extensive experiments across\nmultiple datasets, embeddings, and LLMs show that k-LLMmeans consistently\noutperforms k-means and other traditional baselines and achieves results\ncomparable to state-of-the-art LLM-based clustering, with a fraction of the LLM\ncalls. Finally, we present a case study on sequential text streams and\nintroduce a new benchmark dataset constructed from StackExchange to evaluate\ntext-stream clustering methods.\n", "link": "http://arxiv.org/abs/2502.09667v2", "date": "2025-05-09", "relevancy": 1.8547, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4558}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20k-LLMmeans%3A%20Scalable%2C%20Stable%2C%20and%20Interpretable%20Text%20Clustering%20via%0A%20%20LLM-based%20Centroids&body=Title%3A%20k-LLMmeans%3A%20Scalable%2C%20Stable%2C%20and%20Interpretable%20Text%20Clustering%20via%0A%20%20LLM-based%20Centroids%0AAuthor%3A%20Jairo%20Diaz-Rodriguez%0AAbstract%3A%20%20%20We%20introduce%20k-LLMmeans%2C%20a%20novel%20modification%20of%20the%20k-means%20algorithm%20for%0Atext%20clustering%20that%20leverages%20LLM-generated%20summaries%20as%20cluster%20centroids%2C%0Acapturing%20semantic%20nuances%20often%20missed%20by%20purely%20numerical%20averages.%20This%0Adesign%20preserves%20the%20core%20optimization%20properties%20of%20k-means%20while%20enhancing%0Asemantic%20interpretability%20and%20avoiding%20the%20scalability%20and%20instability%20issues%0Atypical%20of%20modern%20LLM-based%20clustering.%20Unlike%20existing%20methods%2C%20our%20approach%0Adoes%20not%20increase%20LLM%20usage%20with%20dataset%20size%20and%20produces%20transparent%0Aintermediate%20outputs.%20We%20further%20extend%20it%20with%20a%20mini-batch%20variant%20for%0Aefficient%2C%20real-time%20clustering%20of%20streaming%20text.%20Extensive%20experiments%20across%0Amultiple%20datasets%2C%20embeddings%2C%20and%20LLMs%20show%20that%20k-LLMmeans%20consistently%0Aoutperforms%20k-means%20and%20other%20traditional%20baselines%20and%20achieves%20results%0Acomparable%20to%20state-of-the-art%20LLM-based%20clustering%2C%20with%20a%20fraction%20of%20the%20LLM%0Acalls.%20Finally%2C%20we%20present%20a%20case%20study%20on%20sequential%20text%20streams%20and%0Aintroduce%20a%20new%20benchmark%20dataset%20constructed%20from%20StackExchange%20to%20evaluate%0Atext-stream%20clustering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dk-LLMmeans%253A%2520Scalable%252C%2520Stable%252C%2520and%2520Interpretable%2520Text%2520Clustering%2520via%250A%2520%2520LLM-based%2520Centroids%26entry.906535625%3DJairo%2520Diaz-Rodriguez%26entry.1292438233%3D%2520%2520We%2520introduce%2520k-LLMmeans%252C%2520a%2520novel%2520modification%2520of%2520the%2520k-means%2520algorithm%2520for%250Atext%2520clustering%2520that%2520leverages%2520LLM-generated%2520summaries%2520as%2520cluster%2520centroids%252C%250Acapturing%2520semantic%2520nuances%2520often%2520missed%2520by%2520purely%2520numerical%2520averages.%2520This%250Adesign%2520preserves%2520the%2520core%2520optimization%2520properties%2520of%2520k-means%2520while%2520enhancing%250Asemantic%2520interpretability%2520and%2520avoiding%2520the%2520scalability%2520and%2520instability%2520issues%250Atypical%2520of%2520modern%2520LLM-based%2520clustering.%2520Unlike%2520existing%2520methods%252C%2520our%2520approach%250Adoes%2520not%2520increase%2520LLM%2520usage%2520with%2520dataset%2520size%2520and%2520produces%2520transparent%250Aintermediate%2520outputs.%2520We%2520further%2520extend%2520it%2520with%2520a%2520mini-batch%2520variant%2520for%250Aefficient%252C%2520real-time%2520clustering%2520of%2520streaming%2520text.%2520Extensive%2520experiments%2520across%250Amultiple%2520datasets%252C%2520embeddings%252C%2520and%2520LLMs%2520show%2520that%2520k-LLMmeans%2520consistently%250Aoutperforms%2520k-means%2520and%2520other%2520traditional%2520baselines%2520and%2520achieves%2520results%250Acomparable%2520to%2520state-of-the-art%2520LLM-based%2520clustering%252C%2520with%2520a%2520fraction%2520of%2520the%2520LLM%250Acalls.%2520Finally%252C%2520we%2520present%2520a%2520case%2520study%2520on%2520sequential%2520text%2520streams%2520and%250Aintroduce%2520a%2520new%2520benchmark%2520dataset%2520constructed%2520from%2520StackExchange%2520to%2520evaluate%250Atext-stream%2520clustering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=k-LLMmeans%3A%20Scalable%2C%20Stable%2C%20and%20Interpretable%20Text%20Clustering%20via%0A%20%20LLM-based%20Centroids&entry.906535625=Jairo%20Diaz-Rodriguez&entry.1292438233=%20%20We%20introduce%20k-LLMmeans%2C%20a%20novel%20modification%20of%20the%20k-means%20algorithm%20for%0Atext%20clustering%20that%20leverages%20LLM-generated%20summaries%20as%20cluster%20centroids%2C%0Acapturing%20semantic%20nuances%20often%20missed%20by%20purely%20numerical%20averages.%20This%0Adesign%20preserves%20the%20core%20optimization%20properties%20of%20k-means%20while%20enhancing%0Asemantic%20interpretability%20and%20avoiding%20the%20scalability%20and%20instability%20issues%0Atypical%20of%20modern%20LLM-based%20clustering.%20Unlike%20existing%20methods%2C%20our%20approach%0Adoes%20not%20increase%20LLM%20usage%20with%20dataset%20size%20and%20produces%20transparent%0Aintermediate%20outputs.%20We%20further%20extend%20it%20with%20a%20mini-batch%20variant%20for%0Aefficient%2C%20real-time%20clustering%20of%20streaming%20text.%20Extensive%20experiments%20across%0Amultiple%20datasets%2C%20embeddings%2C%20and%20LLMs%20show%20that%20k-LLMmeans%20consistently%0Aoutperforms%20k-means%20and%20other%20traditional%20baselines%20and%20achieves%20results%0Acomparable%20to%20state-of-the-art%20LLM-based%20clustering%2C%20with%20a%20fraction%20of%20the%20LLM%0Acalls.%20Finally%2C%20we%20present%20a%20case%20study%20on%20sequential%20text%20streams%20and%0Aintroduce%20a%20new%20benchmark%20dataset%20constructed%20from%20StackExchange%20to%20evaluate%0Atext-stream%20clustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09667v2&entry.124074799=Read"},
{"title": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models", "author": "Yiqing Xu and Jiayuan Mao and Yilun Du and Tomas Loz\u00e1no-P\u00e9rez and Leslie Pack Kaelbling and David Hsu", "abstract": "  This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.\n", "link": "http://arxiv.org/abs/2405.11928v3", "date": "2025-05-09", "relevancy": 1.8543, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6465}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6329}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models&body=Title%3A%20%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models%0AAuthor%3A%20Yiqing%20Xu%20and%20Jiayuan%20Mao%20and%20Yilun%20Du%20and%20Tomas%20Loz%C3%A1no-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling%20and%20David%20Hsu%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20challenge%20of%20developing%20robots%20capable%20of%0Aunderstanding%20under-specified%20instructions%20for%20creating%20functional%20object%0Aarrangements%2C%20such%20as%20%22set%20up%20a%20dining%20table%20for%20two%22%3B%20previous%20arrangement%0Aapproaches%20have%20focused%20on%20much%20more%20explicit%20instructions%2C%20such%20as%20%22put%20object%0AA%20on%20the%20table.%22%20We%20introduce%20a%20framework%2C%20SetItUp%2C%20for%20learning%20to%20interpret%0Aunder-specified%20instructions.%20SetItUp%20takes%20a%20small%20number%20of%20training%20examples%0Aand%20a%20human-crafted%20program%20sketch%20to%20uncover%20arrangement%20rules%20for%20specific%0Ascene%20types.%20By%20leveraging%20an%20intermediate%20graph-like%20representation%20of%0Aabstract%20spatial%20relationships%20among%20objects%2C%20SetItUp%20decomposes%20the%0Aarrangement%20problem%20into%20two%20subproblems%3A%20i%29%20learning%20the%20arrangement%20patterns%0Afrom%20limited%20data%20and%20ii%29%20grounding%20these%20abstract%20relationships%20into%20object%0Aposes.%20SetItUp%20leverages%20large%20language%20models%20%28LLMs%29%20to%20propose%20the%20abstract%0Aspatial%20relationships%20among%20objects%20in%20novel%20scenes%20as%20the%20constraints%20to%20be%0Asatisfied%3B%20then%2C%20it%20composes%20a%20library%20of%20diffusion%20models%20associated%20with%0Athese%20abstract%20relationships%20to%20find%20object%20poses%20that%20satisfy%20the%20constraints.%0AWe%20validate%20our%20framework%20on%20a%20dataset%20comprising%20study%20desks%2C%20dining%20tables%2C%0Aand%20coffee%20tables%2C%20with%20the%20results%20showing%20superior%20performance%20in%20generating%0Aphysically%20plausible%2C%20functional%2C%20and%20aesthetically%20pleasing%20object%0Aarrangements%20compared%20to%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Set%2520It%2520Up%2521%2522%253A%2520Functional%2520Object%2520Arrangement%2520with%2520Compositional%250A%2520%2520Generative%2520Models%26entry.906535625%3DYiqing%2520Xu%2520and%2520Jiayuan%2520Mao%2520and%2520Yilun%2520Du%2520and%2520Tomas%2520Loz%25C3%25A1no-P%25C3%25A9rez%2520and%2520Leslie%2520Pack%2520Kaelbling%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520challenge%2520of%2520developing%2520robots%2520capable%2520of%250Aunderstanding%2520under-specified%2520instructions%2520for%2520creating%2520functional%2520object%250Aarrangements%252C%2520such%2520as%2520%2522set%2520up%2520a%2520dining%2520table%2520for%2520two%2522%253B%2520previous%2520arrangement%250Aapproaches%2520have%2520focused%2520on%2520much%2520more%2520explicit%2520instructions%252C%2520such%2520as%2520%2522put%2520object%250AA%2520on%2520the%2520table.%2522%2520We%2520introduce%2520a%2520framework%252C%2520SetItUp%252C%2520for%2520learning%2520to%2520interpret%250Aunder-specified%2520instructions.%2520SetItUp%2520takes%2520a%2520small%2520number%2520of%2520training%2520examples%250Aand%2520a%2520human-crafted%2520program%2520sketch%2520to%2520uncover%2520arrangement%2520rules%2520for%2520specific%250Ascene%2520types.%2520By%2520leveraging%2520an%2520intermediate%2520graph-like%2520representation%2520of%250Aabstract%2520spatial%2520relationships%2520among%2520objects%252C%2520SetItUp%2520decomposes%2520the%250Aarrangement%2520problem%2520into%2520two%2520subproblems%253A%2520i%2529%2520learning%2520the%2520arrangement%2520patterns%250Afrom%2520limited%2520data%2520and%2520ii%2529%2520grounding%2520these%2520abstract%2520relationships%2520into%2520object%250Aposes.%2520SetItUp%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520propose%2520the%2520abstract%250Aspatial%2520relationships%2520among%2520objects%2520in%2520novel%2520scenes%2520as%2520the%2520constraints%2520to%2520be%250Asatisfied%253B%2520then%252C%2520it%2520composes%2520a%2520library%2520of%2520diffusion%2520models%2520associated%2520with%250Athese%2520abstract%2520relationships%2520to%2520find%2520object%2520poses%2520that%2520satisfy%2520the%2520constraints.%250AWe%2520validate%2520our%2520framework%2520on%2520a%2520dataset%2520comprising%2520study%2520desks%252C%2520dining%2520tables%252C%250Aand%2520coffee%2520tables%252C%2520with%2520the%2520results%2520showing%2520superior%2520performance%2520in%2520generating%250Aphysically%2520plausible%252C%2520functional%252C%2520and%2520aesthetically%2520pleasing%2520object%250Aarrangements%2520compared%2520to%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Set%20It%20Up%21%22%3A%20Functional%20Object%20Arrangement%20with%20Compositional%0A%20%20Generative%20Models&entry.906535625=Yiqing%20Xu%20and%20Jiayuan%20Mao%20and%20Yilun%20Du%20and%20Tomas%20Loz%C3%A1no-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling%20and%20David%20Hsu&entry.1292438233=%20%20This%20paper%20studies%20the%20challenge%20of%20developing%20robots%20capable%20of%0Aunderstanding%20under-specified%20instructions%20for%20creating%20functional%20object%0Aarrangements%2C%20such%20as%20%22set%20up%20a%20dining%20table%20for%20two%22%3B%20previous%20arrangement%0Aapproaches%20have%20focused%20on%20much%20more%20explicit%20instructions%2C%20such%20as%20%22put%20object%0AA%20on%20the%20table.%22%20We%20introduce%20a%20framework%2C%20SetItUp%2C%20for%20learning%20to%20interpret%0Aunder-specified%20instructions.%20SetItUp%20takes%20a%20small%20number%20of%20training%20examples%0Aand%20a%20human-crafted%20program%20sketch%20to%20uncover%20arrangement%20rules%20for%20specific%0Ascene%20types.%20By%20leveraging%20an%20intermediate%20graph-like%20representation%20of%0Aabstract%20spatial%20relationships%20among%20objects%2C%20SetItUp%20decomposes%20the%0Aarrangement%20problem%20into%20two%20subproblems%3A%20i%29%20learning%20the%20arrangement%20patterns%0Afrom%20limited%20data%20and%20ii%29%20grounding%20these%20abstract%20relationships%20into%20object%0Aposes.%20SetItUp%20leverages%20large%20language%20models%20%28LLMs%29%20to%20propose%20the%20abstract%0Aspatial%20relationships%20among%20objects%20in%20novel%20scenes%20as%20the%20constraints%20to%20be%0Asatisfied%3B%20then%2C%20it%20composes%20a%20library%20of%20diffusion%20models%20associated%20with%0Athese%20abstract%20relationships%20to%20find%20object%20poses%20that%20satisfy%20the%20constraints.%0AWe%20validate%20our%20framework%20on%20a%20dataset%20comprising%20study%20desks%2C%20dining%20tables%2C%0Aand%20coffee%20tables%2C%20with%20the%20results%20showing%20superior%20performance%20in%20generating%0Aphysically%20plausible%2C%20functional%2C%20and%20aesthetically%20pleasing%20object%0Aarrangements%20compared%20to%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11928v3&entry.124074799=Read"},
{"title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server", "author": "Xiang Zhang and Yongfeng Zhang", "abstract": "  The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and is integrated into the AIOS main\nbranch at https://github.com/agiresearch/AIOS.\n", "link": "http://arxiv.org/abs/2504.14411v3", "date": "2025-05-09", "relevancy": 1.8444, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4679}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4595}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planet%20as%20a%20Brain%3A%20Towards%20Internet%20of%20AgentSites%20based%20on%20AIOS%20Server&body=Title%3A%20Planet%20as%20a%20Brain%3A%20Towards%20Internet%20of%20AgentSites%20based%20on%20AIOS%20Server%0AAuthor%3A%20Xiang%20Zhang%20and%20Yongfeng%20Zhang%0AAbstract%3A%20%20%20The%20internet%20is%20undergoing%20a%20historical%20transformation%20from%20the%20%22Internet%20of%0AWebsites%22%20to%20the%20%22Internet%20of%20AgentSites.%22%20While%20traditional%20Websites%20served%20as%0Athe%20foundation%20for%20information%20hosting%20and%20dissemination%2C%20a%20new%20frontier%20is%0Aemerging%20where%20AgentSites%20serve%20as%20the%20hubs%20of%20the%20internet%2C%20where%20each%0AAgentSite%20hosts%20one%20or%20more%20AI%20agents%20that%20receive%20tasks%2C%20address%20them%2C%20and%0Adeliver%20actionable%20solutions%2C%20marking%20a%20significant%20shift%20in%20the%20digital%0Alandscape%20and%20representing%20the%20next%20generation%20of%20online%20ecosystems.%20Under%20this%0Avision%2C%20AIOS%2C%20the%20AI%20Agent%20Operating%20System%2C%20serves%20as%20the%20server%20for%20the%0Adevelopment%2C%20deployment%20and%20execution%20of%20AI%20agents%2C%20which%20is%20a%20fundamental%0Ainfrastructure%20for%20the%20Internet%20of%20Agentsites.%0A%20%20In%20this%20paper%2C%20we%20introduce%20AIOS%20Server%2C%20a%20runtime%20framework%20to%20host%20agents%0Aand%20enable%20global-scale%20collaboration%20among%20decentralized%20agents.%20AIOS%20Server%0Aprovides%20a%20communication%20protocol%20leveraging%20the%20Model%20Context%20Protocol%20%28MCP%29%0Aand%20JSON-RPC%20to%20enable%20agent-agent%20or%20human-agent%20interactions.%20Each%20AIOS%20node%0Aoperates%20as%20a%20server%20to%20host%20and%20execute%20agents%2C%20while%20supporting%20peer-to-peer%0Acoordination%20without%20reliance%20on%20centralized%20orchestration.%20Based%20on%20AIOS%0AServer%2C%20we%20further%20present%20the%20world%27s%20first%20practically%20deployed%20Internet%20of%0AAgentsites%20%28AIOS-IoA%29%2C%20including%20AgentHub%20for%20agent%20registration%20and%20discovery%0Aand%20AgentChat%20for%20interactive%20communication%2C%20at%20https%3A//planet.aios.foundation.%0AThe%20agent%20discovery%20mechanism%20based%20on%20Distributed%20Hash%20Tables%20%28DHT%29%20and%20a%0AGossip%20protocol%20serves%20as%20the%20search%20engine%20for%20the%20internet%20of%20agentsites.%0AThis%20work%20provides%20a%20practical%20foundation%20for%20building%20the%20Internet%20of%0AAgentsites-a%20new%20paradigm%20where%20autonomous%20agents%20become%20first-class%20citizens%0Aof%20the%20web.%20The%20implementation%20is%20available%20at%0Ahttps%3A//github.com/agiresearch/AIOS.Server%20and%20is%20integrated%20into%20the%20AIOS%20main%0Abranch%20at%20https%3A//github.com/agiresearch/AIOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14411v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanet%2520as%2520a%2520Brain%253A%2520Towards%2520Internet%2520of%2520AgentSites%2520based%2520on%2520AIOS%2520Server%26entry.906535625%3DXiang%2520Zhang%2520and%2520Yongfeng%2520Zhang%26entry.1292438233%3D%2520%2520The%2520internet%2520is%2520undergoing%2520a%2520historical%2520transformation%2520from%2520the%2520%2522Internet%2520of%250AWebsites%2522%2520to%2520the%2520%2522Internet%2520of%2520AgentSites.%2522%2520While%2520traditional%2520Websites%2520served%2520as%250Athe%2520foundation%2520for%2520information%2520hosting%2520and%2520dissemination%252C%2520a%2520new%2520frontier%2520is%250Aemerging%2520where%2520AgentSites%2520serve%2520as%2520the%2520hubs%2520of%2520the%2520internet%252C%2520where%2520each%250AAgentSite%2520hosts%2520one%2520or%2520more%2520AI%2520agents%2520that%2520receive%2520tasks%252C%2520address%2520them%252C%2520and%250Adeliver%2520actionable%2520solutions%252C%2520marking%2520a%2520significant%2520shift%2520in%2520the%2520digital%250Alandscape%2520and%2520representing%2520the%2520next%2520generation%2520of%2520online%2520ecosystems.%2520Under%2520this%250Avision%252C%2520AIOS%252C%2520the%2520AI%2520Agent%2520Operating%2520System%252C%2520serves%2520as%2520the%2520server%2520for%2520the%250Adevelopment%252C%2520deployment%2520and%2520execution%2520of%2520AI%2520agents%252C%2520which%2520is%2520a%2520fundamental%250Ainfrastructure%2520for%2520the%2520Internet%2520of%2520Agentsites.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AIOS%2520Server%252C%2520a%2520runtime%2520framework%2520to%2520host%2520agents%250Aand%2520enable%2520global-scale%2520collaboration%2520among%2520decentralized%2520agents.%2520AIOS%2520Server%250Aprovides%2520a%2520communication%2520protocol%2520leveraging%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%250Aand%2520JSON-RPC%2520to%2520enable%2520agent-agent%2520or%2520human-agent%2520interactions.%2520Each%2520AIOS%2520node%250Aoperates%2520as%2520a%2520server%2520to%2520host%2520and%2520execute%2520agents%252C%2520while%2520supporting%2520peer-to-peer%250Acoordination%2520without%2520reliance%2520on%2520centralized%2520orchestration.%2520Based%2520on%2520AIOS%250AServer%252C%2520we%2520further%2520present%2520the%2520world%2527s%2520first%2520practically%2520deployed%2520Internet%2520of%250AAgentsites%2520%2528AIOS-IoA%2529%252C%2520including%2520AgentHub%2520for%2520agent%2520registration%2520and%2520discovery%250Aand%2520AgentChat%2520for%2520interactive%2520communication%252C%2520at%2520https%253A//planet.aios.foundation.%250AThe%2520agent%2520discovery%2520mechanism%2520based%2520on%2520Distributed%2520Hash%2520Tables%2520%2528DHT%2529%2520and%2520a%250AGossip%2520protocol%2520serves%2520as%2520the%2520search%2520engine%2520for%2520the%2520internet%2520of%2520agentsites.%250AThis%2520work%2520provides%2520a%2520practical%2520foundation%2520for%2520building%2520the%2520Internet%2520of%250AAgentsites-a%2520new%2520paradigm%2520where%2520autonomous%2520agents%2520become%2520first-class%2520citizens%250Aof%2520the%2520web.%2520The%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/agiresearch/AIOS.Server%2520and%2520is%2520integrated%2520into%2520the%2520AIOS%2520main%250Abranch%2520at%2520https%253A//github.com/agiresearch/AIOS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14411v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planet%20as%20a%20Brain%3A%20Towards%20Internet%20of%20AgentSites%20based%20on%20AIOS%20Server&entry.906535625=Xiang%20Zhang%20and%20Yongfeng%20Zhang&entry.1292438233=%20%20The%20internet%20is%20undergoing%20a%20historical%20transformation%20from%20the%20%22Internet%20of%0AWebsites%22%20to%20the%20%22Internet%20of%20AgentSites.%22%20While%20traditional%20Websites%20served%20as%0Athe%20foundation%20for%20information%20hosting%20and%20dissemination%2C%20a%20new%20frontier%20is%0Aemerging%20where%20AgentSites%20serve%20as%20the%20hubs%20of%20the%20internet%2C%20where%20each%0AAgentSite%20hosts%20one%20or%20more%20AI%20agents%20that%20receive%20tasks%2C%20address%20them%2C%20and%0Adeliver%20actionable%20solutions%2C%20marking%20a%20significant%20shift%20in%20the%20digital%0Alandscape%20and%20representing%20the%20next%20generation%20of%20online%20ecosystems.%20Under%20this%0Avision%2C%20AIOS%2C%20the%20AI%20Agent%20Operating%20System%2C%20serves%20as%20the%20server%20for%20the%0Adevelopment%2C%20deployment%20and%20execution%20of%20AI%20agents%2C%20which%20is%20a%20fundamental%0Ainfrastructure%20for%20the%20Internet%20of%20Agentsites.%0A%20%20In%20this%20paper%2C%20we%20introduce%20AIOS%20Server%2C%20a%20runtime%20framework%20to%20host%20agents%0Aand%20enable%20global-scale%20collaboration%20among%20decentralized%20agents.%20AIOS%20Server%0Aprovides%20a%20communication%20protocol%20leveraging%20the%20Model%20Context%20Protocol%20%28MCP%29%0Aand%20JSON-RPC%20to%20enable%20agent-agent%20or%20human-agent%20interactions.%20Each%20AIOS%20node%0Aoperates%20as%20a%20server%20to%20host%20and%20execute%20agents%2C%20while%20supporting%20peer-to-peer%0Acoordination%20without%20reliance%20on%20centralized%20orchestration.%20Based%20on%20AIOS%0AServer%2C%20we%20further%20present%20the%20world%27s%20first%20practically%20deployed%20Internet%20of%0AAgentsites%20%28AIOS-IoA%29%2C%20including%20AgentHub%20for%20agent%20registration%20and%20discovery%0Aand%20AgentChat%20for%20interactive%20communication%2C%20at%20https%3A//planet.aios.foundation.%0AThe%20agent%20discovery%20mechanism%20based%20on%20Distributed%20Hash%20Tables%20%28DHT%29%20and%20a%0AGossip%20protocol%20serves%20as%20the%20search%20engine%20for%20the%20internet%20of%20agentsites.%0AThis%20work%20provides%20a%20practical%20foundation%20for%20building%20the%20Internet%20of%0AAgentsites-a%20new%20paradigm%20where%20autonomous%20agents%20become%20first-class%20citizens%0Aof%20the%20web.%20The%20implementation%20is%20available%20at%0Ahttps%3A//github.com/agiresearch/AIOS.Server%20and%20is%20integrated%20into%20the%20AIOS%20main%0Abranch%20at%20https%3A//github.com/agiresearch/AIOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14411v3&entry.124074799=Read"},
{"title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization", "author": "Zihang Song and Matteo Zecchin and Bipin Rajendran and Osvaldo Simeone", "abstract": "  This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.\n", "link": "http://arxiv.org/abs/2505.06175v1", "date": "2025-05-09", "relevancy": 1.8264, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4758}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4434}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turbo-ICL%3A%20In-Context%20Learning-Based%20Turbo%20Equalization&body=Title%3A%20Turbo-ICL%3A%20In-Context%20Learning-Based%20Turbo%20Equalization%0AAuthor%3A%20Zihang%20Song%20and%20Matteo%20Zecchin%20and%20Bipin%20Rajendran%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20in-context%20learning%20%28ICL%29%20framework%2C%20inspired%0Aby%20large%20language%20models%20%28LLMs%29%2C%20for%20soft-input%20soft-output%20channel%0Aequalization%20in%20coded%20multiple-input%20multiple-output%20%28MIMO%29%20systems.%20The%0Aproposed%20approach%20learns%20to%20infer%20posterior%20symbol%20distributions%20directly%20from%0Aa%20prompt%20of%20pilot%20signals%20and%20decoder%20feedback.%20A%20key%20innovation%20is%20the%20use%20of%0Aprompt%20augmentation%20to%20incorporate%20extrinsic%20information%20from%20the%20decoder%0Aoutput%20as%20additional%20context%2C%20enabling%20the%20ICL%20model%20to%20refine%20its%20symbol%0Aestimates%20iteratively%20across%20turbo%20decoding%20iterations.%20Two%20model%20variants%2C%0Abased%20on%20Transformer%20and%20state-space%20architectures%2C%20are%20developed%20and%0Aevaluated.%20Extensive%20simulations%20demonstrate%20that%2C%20when%20traditional%20linear%0Aassumptions%20break%20down%2C%20e.g.%2C%20in%20the%20presence%20of%20low-resolution%20quantization%2C%0AICL%20equalizers%20consistently%20outperform%20conventional%20model-based%20baselines%2C%20even%0Awhen%20the%20latter%20are%20provided%20with%20perfect%20channel%20state%20information.%20Results%0Aalso%20highlight%20the%20advantage%20of%20Transformer-based%20models%20under%20limited%20training%0Adiversity%2C%20as%20well%20as%20the%20efficiency%20of%20state-space%20models%20in%0Aresource-constrained%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurbo-ICL%253A%2520In-Context%2520Learning-Based%2520Turbo%2520Equalization%26entry.906535625%3DZihang%2520Song%2520and%2520Matteo%2520Zecchin%2520and%2520Bipin%2520Rajendran%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520in-context%2520learning%2520%2528ICL%2529%2520framework%252C%2520inspired%250Aby%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520for%2520soft-input%2520soft-output%2520channel%250Aequalization%2520in%2520coded%2520multiple-input%2520multiple-output%2520%2528MIMO%2529%2520systems.%2520The%250Aproposed%2520approach%2520learns%2520to%2520infer%2520posterior%2520symbol%2520distributions%2520directly%2520from%250Aa%2520prompt%2520of%2520pilot%2520signals%2520and%2520decoder%2520feedback.%2520A%2520key%2520innovation%2520is%2520the%2520use%2520of%250Aprompt%2520augmentation%2520to%2520incorporate%2520extrinsic%2520information%2520from%2520the%2520decoder%250Aoutput%2520as%2520additional%2520context%252C%2520enabling%2520the%2520ICL%2520model%2520to%2520refine%2520its%2520symbol%250Aestimates%2520iteratively%2520across%2520turbo%2520decoding%2520iterations.%2520Two%2520model%2520variants%252C%250Abased%2520on%2520Transformer%2520and%2520state-space%2520architectures%252C%2520are%2520developed%2520and%250Aevaluated.%2520Extensive%2520simulations%2520demonstrate%2520that%252C%2520when%2520traditional%2520linear%250Aassumptions%2520break%2520down%252C%2520e.g.%252C%2520in%2520the%2520presence%2520of%2520low-resolution%2520quantization%252C%250AICL%2520equalizers%2520consistently%2520outperform%2520conventional%2520model-based%2520baselines%252C%2520even%250Awhen%2520the%2520latter%2520are%2520provided%2520with%2520perfect%2520channel%2520state%2520information.%2520Results%250Aalso%2520highlight%2520the%2520advantage%2520of%2520Transformer-based%2520models%2520under%2520limited%2520training%250Adiversity%252C%2520as%2520well%2520as%2520the%2520efficiency%2520of%2520state-space%2520models%2520in%250Aresource-constrained%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turbo-ICL%3A%20In-Context%20Learning-Based%20Turbo%20Equalization&entry.906535625=Zihang%20Song%20and%20Matteo%20Zecchin%20and%20Bipin%20Rajendran%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20in-context%20learning%20%28ICL%29%20framework%2C%20inspired%0Aby%20large%20language%20models%20%28LLMs%29%2C%20for%20soft-input%20soft-output%20channel%0Aequalization%20in%20coded%20multiple-input%20multiple-output%20%28MIMO%29%20systems.%20The%0Aproposed%20approach%20learns%20to%20infer%20posterior%20symbol%20distributions%20directly%20from%0Aa%20prompt%20of%20pilot%20signals%20and%20decoder%20feedback.%20A%20key%20innovation%20is%20the%20use%20of%0Aprompt%20augmentation%20to%20incorporate%20extrinsic%20information%20from%20the%20decoder%0Aoutput%20as%20additional%20context%2C%20enabling%20the%20ICL%20model%20to%20refine%20its%20symbol%0Aestimates%20iteratively%20across%20turbo%20decoding%20iterations.%20Two%20model%20variants%2C%0Abased%20on%20Transformer%20and%20state-space%20architectures%2C%20are%20developed%20and%0Aevaluated.%20Extensive%20simulations%20demonstrate%20that%2C%20when%20traditional%20linear%0Aassumptions%20break%20down%2C%20e.g.%2C%20in%20the%20presence%20of%20low-resolution%20quantization%2C%0AICL%20equalizers%20consistently%20outperform%20conventional%20model-based%20baselines%2C%20even%0Awhen%20the%20latter%20are%20provided%20with%20perfect%20channel%20state%20information.%20Results%0Aalso%20highlight%20the%20advantage%20of%20Transformer-based%20models%20under%20limited%20training%0Adiversity%2C%20as%20well%20as%20the%20efficiency%20of%20state-space%20models%20in%0Aresource-constrained%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06175v1&entry.124074799=Read"},
{"title": "Centralized Decision-Making for Platooning By Using SPaT-Driven\n  Reference Speeds", "author": "Melih Yazgan and S\u00fcleyman Tatar and J. Marius Z\u00f6llner", "abstract": "  This paper introduces a centralized approach for fuel-efficient urban\nplatooning by leveraging real-time Vehicle- to-Everything (V2X) communication\nand Signal Phase and Timing (SPaT) data. A nonlinear Model Predictive Control\n(MPC) algorithm optimizes the trajectories of platoon leader vehicles,\nemploying an asymmetric cost function to minimize fuel-intensive acceleration.\nFollowing vehicles utilize a gap- and velocity-based control strategy,\ncomplemented by dynamic platoon splitting logic communicated through Platoon\nControl Messages (PCM) and Platoon Awareness Messages (PAM). Simulation results\nobtained from the CARLA environment demonstrate substantial fuel savings of up\nto 41.2%, along with smoother traffic flows, fewer vehicle stops, and improved\nintersection throughput.\n", "link": "http://arxiv.org/abs/2505.06071v1", "date": "2025-05-09", "relevancy": 1.8182, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4615}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centralized%20Decision-Making%20for%20Platooning%20By%20Using%20SPaT-Driven%0A%20%20Reference%20Speeds&body=Title%3A%20Centralized%20Decision-Making%20for%20Platooning%20By%20Using%20SPaT-Driven%0A%20%20Reference%20Speeds%0AAuthor%3A%20Melih%20Yazgan%20and%20S%C3%BCleyman%20Tatar%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20centralized%20approach%20for%20fuel-efficient%20urban%0Aplatooning%20by%20leveraging%20real-time%20Vehicle-%20to-Everything%20%28V2X%29%20communication%0Aand%20Signal%20Phase%20and%20Timing%20%28SPaT%29%20data.%20A%20nonlinear%20Model%20Predictive%20Control%0A%28MPC%29%20algorithm%20optimizes%20the%20trajectories%20of%20platoon%20leader%20vehicles%2C%0Aemploying%20an%20asymmetric%20cost%20function%20to%20minimize%20fuel-intensive%20acceleration.%0AFollowing%20vehicles%20utilize%20a%20gap-%20and%20velocity-based%20control%20strategy%2C%0Acomplemented%20by%20dynamic%20platoon%20splitting%20logic%20communicated%20through%20Platoon%0AControl%20Messages%20%28PCM%29%20and%20Platoon%20Awareness%20Messages%20%28PAM%29.%20Simulation%20results%0Aobtained%20from%20the%20CARLA%20environment%20demonstrate%20substantial%20fuel%20savings%20of%20up%0Ato%2041.2%25%2C%20along%20with%20smoother%20traffic%20flows%2C%20fewer%20vehicle%20stops%2C%20and%20improved%0Aintersection%20throughput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentralized%2520Decision-Making%2520for%2520Platooning%2520By%2520Using%2520SPaT-Driven%250A%2520%2520Reference%2520Speeds%26entry.906535625%3DMelih%2520Yazgan%2520and%2520S%25C3%25BCleyman%2520Tatar%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520centralized%2520approach%2520for%2520fuel-efficient%2520urban%250Aplatooning%2520by%2520leveraging%2520real-time%2520Vehicle-%2520to-Everything%2520%2528V2X%2529%2520communication%250Aand%2520Signal%2520Phase%2520and%2520Timing%2520%2528SPaT%2529%2520data.%2520A%2520nonlinear%2520Model%2520Predictive%2520Control%250A%2528MPC%2529%2520algorithm%2520optimizes%2520the%2520trajectories%2520of%2520platoon%2520leader%2520vehicles%252C%250Aemploying%2520an%2520asymmetric%2520cost%2520function%2520to%2520minimize%2520fuel-intensive%2520acceleration.%250AFollowing%2520vehicles%2520utilize%2520a%2520gap-%2520and%2520velocity-based%2520control%2520strategy%252C%250Acomplemented%2520by%2520dynamic%2520platoon%2520splitting%2520logic%2520communicated%2520through%2520Platoon%250AControl%2520Messages%2520%2528PCM%2529%2520and%2520Platoon%2520Awareness%2520Messages%2520%2528PAM%2529.%2520Simulation%2520results%250Aobtained%2520from%2520the%2520CARLA%2520environment%2520demonstrate%2520substantial%2520fuel%2520savings%2520of%2520up%250Ato%252041.2%2525%252C%2520along%2520with%2520smoother%2520traffic%2520flows%252C%2520fewer%2520vehicle%2520stops%252C%2520and%2520improved%250Aintersection%2520throughput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centralized%20Decision-Making%20for%20Platooning%20By%20Using%20SPaT-Driven%0A%20%20Reference%20Speeds&entry.906535625=Melih%20Yazgan%20and%20S%C3%BCleyman%20Tatar%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20This%20paper%20introduces%20a%20centralized%20approach%20for%20fuel-efficient%20urban%0Aplatooning%20by%20leveraging%20real-time%20Vehicle-%20to-Everything%20%28V2X%29%20communication%0Aand%20Signal%20Phase%20and%20Timing%20%28SPaT%29%20data.%20A%20nonlinear%20Model%20Predictive%20Control%0A%28MPC%29%20algorithm%20optimizes%20the%20trajectories%20of%20platoon%20leader%20vehicles%2C%0Aemploying%20an%20asymmetric%20cost%20function%20to%20minimize%20fuel-intensive%20acceleration.%0AFollowing%20vehicles%20utilize%20a%20gap-%20and%20velocity-based%20control%20strategy%2C%0Acomplemented%20by%20dynamic%20platoon%20splitting%20logic%20communicated%20through%20Platoon%0AControl%20Messages%20%28PCM%29%20and%20Platoon%20Awareness%20Messages%20%28PAM%29.%20Simulation%20results%0Aobtained%20from%20the%20CARLA%20environment%20demonstrate%20substantial%20fuel%20savings%20of%20up%0Ato%2041.2%25%2C%20along%20with%20smoother%20traffic%20flows%2C%20fewer%20vehicle%20stops%2C%20and%20improved%0Aintersection%20throughput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06071v1&entry.124074799=Read"},
{"title": "Subspace Aggregation Query and Index Generation for Multidimensional\n  Resource Space Model", "author": "Xiaoping Sun and Hai Zhuge", "abstract": "  Organizing resources in a multidimensional classification space is an\napproach to efficiently managing and querying large-scale resources. This paper\ndefines an aggregation query on subspace defined by a range on the partial\norder on coordinate tree at each dimension, where each point contains resources\naggregated along the paths of partial order relations on the points so that\naggregated resources at each point within the subspace can be measured, ranked\nand selected. To efficiently locate non-empty points in a large subspace, an\napproach to generating graph index is proposed to build inclusion links with\npartial order relations on coordinates of dimensions to enable a subspace query\nto reach non-empty points by following indexing links and aggregate resources\nalong indexing paths back to their super points. Generating such an index is\ncostly as the number of children of an index node can be very large so that the\ntotal number of indexing nodes is unbounded. The proposed approach adopts the\nfollowing strategies to reduce the cost: (1) adding intersection links between\ntwo indexing nodes, which can better reduce query processing costs while\ncontrolling the number of nodes of the graph index; (2) intersection links are\nadded between two nodes according to the probabilistic distribution calculated\nfor estimating the costs of adding intersection between two nodes; (3)\ncoordinates at one dimension having more resources are split by coordinates at\nanother dimension to balance the number of resources hold by indexing nodes;\nand, (4) short-cut links are added between sibling coordinates of coordinate\ntrees to make an efficient query on linear order coordinates. Analysis and\nexperiments verified the effectiveness of the generated index in supporting\nsubspace aggregation query. This work makes significant contributions to the\ndevelopment of data model based on multi-dimensional classification.\n", "link": "http://arxiv.org/abs/2505.02129v2", "date": "2025-05-09", "relevancy": 1.7977, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4546}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subspace%20Aggregation%20Query%20and%20Index%20Generation%20for%20Multidimensional%0A%20%20Resource%20Space%20Model&body=Title%3A%20Subspace%20Aggregation%20Query%20and%20Index%20Generation%20for%20Multidimensional%0A%20%20Resource%20Space%20Model%0AAuthor%3A%20Xiaoping%20Sun%20and%20Hai%20Zhuge%0AAbstract%3A%20%20%20Organizing%20resources%20in%20a%20multidimensional%20classification%20space%20is%20an%0Aapproach%20to%20efficiently%20managing%20and%20querying%20large-scale%20resources.%20This%20paper%0Adefines%20an%20aggregation%20query%20on%20subspace%20defined%20by%20a%20range%20on%20the%20partial%0Aorder%20on%20coordinate%20tree%20at%20each%20dimension%2C%20where%20each%20point%20contains%20resources%0Aaggregated%20along%20the%20paths%20of%20partial%20order%20relations%20on%20the%20points%20so%20that%0Aaggregated%20resources%20at%20each%20point%20within%20the%20subspace%20can%20be%20measured%2C%20ranked%0Aand%20selected.%20To%20efficiently%20locate%20non-empty%20points%20in%20a%20large%20subspace%2C%20an%0Aapproach%20to%20generating%20graph%20index%20is%20proposed%20to%20build%20inclusion%20links%20with%0Apartial%20order%20relations%20on%20coordinates%20of%20dimensions%20to%20enable%20a%20subspace%20query%0Ato%20reach%20non-empty%20points%20by%20following%20indexing%20links%20and%20aggregate%20resources%0Aalong%20indexing%20paths%20back%20to%20their%20super%20points.%20Generating%20such%20an%20index%20is%0Acostly%20as%20the%20number%20of%20children%20of%20an%20index%20node%20can%20be%20very%20large%20so%20that%20the%0Atotal%20number%20of%20indexing%20nodes%20is%20unbounded.%20The%20proposed%20approach%20adopts%20the%0Afollowing%20strategies%20to%20reduce%20the%20cost%3A%20%281%29%20adding%20intersection%20links%20between%0Atwo%20indexing%20nodes%2C%20which%20can%20better%20reduce%20query%20processing%20costs%20while%0Acontrolling%20the%20number%20of%20nodes%20of%20the%20graph%20index%3B%20%282%29%20intersection%20links%20are%0Aadded%20between%20two%20nodes%20according%20to%20the%20probabilistic%20distribution%20calculated%0Afor%20estimating%20the%20costs%20of%20adding%20intersection%20between%20two%20nodes%3B%20%283%29%0Acoordinates%20at%20one%20dimension%20having%20more%20resources%20are%20split%20by%20coordinates%20at%0Aanother%20dimension%20to%20balance%20the%20number%20of%20resources%20hold%20by%20indexing%20nodes%3B%0Aand%2C%20%284%29%20short-cut%20links%20are%20added%20between%20sibling%20coordinates%20of%20coordinate%0Atrees%20to%20make%20an%20efficient%20query%20on%20linear%20order%20coordinates.%20Analysis%20and%0Aexperiments%20verified%20the%20effectiveness%20of%20the%20generated%20index%20in%20supporting%0Asubspace%20aggregation%20query.%20This%20work%20makes%20significant%20contributions%20to%20the%0Adevelopment%20of%20data%20model%20based%20on%20multi-dimensional%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubspace%2520Aggregation%2520Query%2520and%2520Index%2520Generation%2520for%2520Multidimensional%250A%2520%2520Resource%2520Space%2520Model%26entry.906535625%3DXiaoping%2520Sun%2520and%2520Hai%2520Zhuge%26entry.1292438233%3D%2520%2520Organizing%2520resources%2520in%2520a%2520multidimensional%2520classification%2520space%2520is%2520an%250Aapproach%2520to%2520efficiently%2520managing%2520and%2520querying%2520large-scale%2520resources.%2520This%2520paper%250Adefines%2520an%2520aggregation%2520query%2520on%2520subspace%2520defined%2520by%2520a%2520range%2520on%2520the%2520partial%250Aorder%2520on%2520coordinate%2520tree%2520at%2520each%2520dimension%252C%2520where%2520each%2520point%2520contains%2520resources%250Aaggregated%2520along%2520the%2520paths%2520of%2520partial%2520order%2520relations%2520on%2520the%2520points%2520so%2520that%250Aaggregated%2520resources%2520at%2520each%2520point%2520within%2520the%2520subspace%2520can%2520be%2520measured%252C%2520ranked%250Aand%2520selected.%2520To%2520efficiently%2520locate%2520non-empty%2520points%2520in%2520a%2520large%2520subspace%252C%2520an%250Aapproach%2520to%2520generating%2520graph%2520index%2520is%2520proposed%2520to%2520build%2520inclusion%2520links%2520with%250Apartial%2520order%2520relations%2520on%2520coordinates%2520of%2520dimensions%2520to%2520enable%2520a%2520subspace%2520query%250Ato%2520reach%2520non-empty%2520points%2520by%2520following%2520indexing%2520links%2520and%2520aggregate%2520resources%250Aalong%2520indexing%2520paths%2520back%2520to%2520their%2520super%2520points.%2520Generating%2520such%2520an%2520index%2520is%250Acostly%2520as%2520the%2520number%2520of%2520children%2520of%2520an%2520index%2520node%2520can%2520be%2520very%2520large%2520so%2520that%2520the%250Atotal%2520number%2520of%2520indexing%2520nodes%2520is%2520unbounded.%2520The%2520proposed%2520approach%2520adopts%2520the%250Afollowing%2520strategies%2520to%2520reduce%2520the%2520cost%253A%2520%25281%2529%2520adding%2520intersection%2520links%2520between%250Atwo%2520indexing%2520nodes%252C%2520which%2520can%2520better%2520reduce%2520query%2520processing%2520costs%2520while%250Acontrolling%2520the%2520number%2520of%2520nodes%2520of%2520the%2520graph%2520index%253B%2520%25282%2529%2520intersection%2520links%2520are%250Aadded%2520between%2520two%2520nodes%2520according%2520to%2520the%2520probabilistic%2520distribution%2520calculated%250Afor%2520estimating%2520the%2520costs%2520of%2520adding%2520intersection%2520between%2520two%2520nodes%253B%2520%25283%2529%250Acoordinates%2520at%2520one%2520dimension%2520having%2520more%2520resources%2520are%2520split%2520by%2520coordinates%2520at%250Aanother%2520dimension%2520to%2520balance%2520the%2520number%2520of%2520resources%2520hold%2520by%2520indexing%2520nodes%253B%250Aand%252C%2520%25284%2529%2520short-cut%2520links%2520are%2520added%2520between%2520sibling%2520coordinates%2520of%2520coordinate%250Atrees%2520to%2520make%2520an%2520efficient%2520query%2520on%2520linear%2520order%2520coordinates.%2520Analysis%2520and%250Aexperiments%2520verified%2520the%2520effectiveness%2520of%2520the%2520generated%2520index%2520in%2520supporting%250Asubspace%2520aggregation%2520query.%2520This%2520work%2520makes%2520significant%2520contributions%2520to%2520the%250Adevelopment%2520of%2520data%2520model%2520based%2520on%2520multi-dimensional%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subspace%20Aggregation%20Query%20and%20Index%20Generation%20for%20Multidimensional%0A%20%20Resource%20Space%20Model&entry.906535625=Xiaoping%20Sun%20and%20Hai%20Zhuge&entry.1292438233=%20%20Organizing%20resources%20in%20a%20multidimensional%20classification%20space%20is%20an%0Aapproach%20to%20efficiently%20managing%20and%20querying%20large-scale%20resources.%20This%20paper%0Adefines%20an%20aggregation%20query%20on%20subspace%20defined%20by%20a%20range%20on%20the%20partial%0Aorder%20on%20coordinate%20tree%20at%20each%20dimension%2C%20where%20each%20point%20contains%20resources%0Aaggregated%20along%20the%20paths%20of%20partial%20order%20relations%20on%20the%20points%20so%20that%0Aaggregated%20resources%20at%20each%20point%20within%20the%20subspace%20can%20be%20measured%2C%20ranked%0Aand%20selected.%20To%20efficiently%20locate%20non-empty%20points%20in%20a%20large%20subspace%2C%20an%0Aapproach%20to%20generating%20graph%20index%20is%20proposed%20to%20build%20inclusion%20links%20with%0Apartial%20order%20relations%20on%20coordinates%20of%20dimensions%20to%20enable%20a%20subspace%20query%0Ato%20reach%20non-empty%20points%20by%20following%20indexing%20links%20and%20aggregate%20resources%0Aalong%20indexing%20paths%20back%20to%20their%20super%20points.%20Generating%20such%20an%20index%20is%0Acostly%20as%20the%20number%20of%20children%20of%20an%20index%20node%20can%20be%20very%20large%20so%20that%20the%0Atotal%20number%20of%20indexing%20nodes%20is%20unbounded.%20The%20proposed%20approach%20adopts%20the%0Afollowing%20strategies%20to%20reduce%20the%20cost%3A%20%281%29%20adding%20intersection%20links%20between%0Atwo%20indexing%20nodes%2C%20which%20can%20better%20reduce%20query%20processing%20costs%20while%0Acontrolling%20the%20number%20of%20nodes%20of%20the%20graph%20index%3B%20%282%29%20intersection%20links%20are%0Aadded%20between%20two%20nodes%20according%20to%20the%20probabilistic%20distribution%20calculated%0Afor%20estimating%20the%20costs%20of%20adding%20intersection%20between%20two%20nodes%3B%20%283%29%0Acoordinates%20at%20one%20dimension%20having%20more%20resources%20are%20split%20by%20coordinates%20at%0Aanother%20dimension%20to%20balance%20the%20number%20of%20resources%20hold%20by%20indexing%20nodes%3B%0Aand%2C%20%284%29%20short-cut%20links%20are%20added%20between%20sibling%20coordinates%20of%20coordinate%0Atrees%20to%20make%20an%20efficient%20query%20on%20linear%20order%20coordinates.%20Analysis%20and%0Aexperiments%20verified%20the%20effectiveness%20of%20the%20generated%20index%20in%20supporting%0Asubspace%20aggregation%20query.%20This%20work%20makes%20significant%20contributions%20to%20the%0Adevelopment%20of%20data%20model%20based%20on%20multi-dimensional%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02129v2&entry.124074799=Read"},
{"title": "Deep hybrid models: infer and plan in a dynamic world", "author": "Matteo Priorelli and Ivilin Peev Stoianov", "abstract": "  To determine an optimal plan for complex tasks, one often deals with dynamic\nand hierarchical relationships between several entities. Traditionally, such\nproblems are tackled with optimal control, which relies on the optimization of\ncost functions; instead, a recent biologically-motivated proposal casts\nplanning and control as an inference process. Active inference assumes that\naction and perception are two complementary aspects of life whereby the role of\nthe former is to fulfill the predictions inferred by the latter. Here, we\npresent an active inference approach that exploits discrete and continuous\nprocessing, based on three features: the representation of potential body\nconfigurations in relation to the objects of interest; the use of hierarchical\nrelationships that enable the agent to easily interpret and flexibly expand its\nbody schema for tool use; the definition of potential trajectories related to\nthe agent's intentions, used to infer and plan with dynamic elements at\ndifferent temporal scales. We evaluate this deep hybrid model on a habitual\ntask: reaching a moving object after having picked a moving tool. We show that\nthe model can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control.\n", "link": "http://arxiv.org/abs/2402.10088v4", "date": "2025-05-09", "relevancy": 1.7967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.638}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5942}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20a%20dynamic%20world&body=Title%3A%20Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20a%20dynamic%20world%0AAuthor%3A%20Matteo%20Priorelli%20and%20Ivilin%20Peev%20Stoianov%0AAbstract%3A%20%20%20To%20determine%20an%20optimal%20plan%20for%20complex%20tasks%2C%20one%20often%20deals%20with%20dynamic%0Aand%20hierarchical%20relationships%20between%20several%20entities.%20Traditionally%2C%20such%0Aproblems%20are%20tackled%20with%20optimal%20control%2C%20which%20relies%20on%20the%20optimization%20of%0Acost%20functions%3B%20instead%2C%20a%20recent%20biologically-motivated%20proposal%20casts%0Aplanning%20and%20control%20as%20an%20inference%20process.%20Active%20inference%20assumes%20that%0Aaction%20and%20perception%20are%20two%20complementary%20aspects%20of%20life%20whereby%20the%20role%20of%0Athe%20former%20is%20to%20fulfill%20the%20predictions%20inferred%20by%20the%20latter.%20Here%2C%20we%0Apresent%20an%20active%20inference%20approach%20that%20exploits%20discrete%20and%20continuous%0Aprocessing%2C%20based%20on%20three%20features%3A%20the%20representation%20of%20potential%20body%0Aconfigurations%20in%20relation%20to%20the%20objects%20of%20interest%3B%20the%20use%20of%20hierarchical%0Arelationships%20that%20enable%20the%20agent%20to%20easily%20interpret%20and%20flexibly%20expand%20its%0Abody%20schema%20for%20tool%20use%3B%20the%20definition%20of%20potential%20trajectories%20related%20to%0Athe%20agent%27s%20intentions%2C%20used%20to%20infer%20and%20plan%20with%20dynamic%20elements%20at%0Adifferent%20temporal%20scales.%20We%20evaluate%20this%20deep%20hybrid%20model%20on%20a%20habitual%0Atask%3A%20reaching%20a%20moving%20object%20after%20having%20picked%20a%20moving%20tool.%20We%20show%20that%0Athe%20model%20can%20tackle%20the%20presented%20task%20under%20different%20conditions.%20This%20study%0Aextends%20past%20work%20on%20planning%20as%20inference%20and%20advances%20an%20alternative%0Adirection%20to%20optimal%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10088v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520hybrid%2520models%253A%2520infer%2520and%2520plan%2520in%2520a%2520dynamic%2520world%26entry.906535625%3DMatteo%2520Priorelli%2520and%2520Ivilin%2520Peev%2520Stoianov%26entry.1292438233%3D%2520%2520To%2520determine%2520an%2520optimal%2520plan%2520for%2520complex%2520tasks%252C%2520one%2520often%2520deals%2520with%2520dynamic%250Aand%2520hierarchical%2520relationships%2520between%2520several%2520entities.%2520Traditionally%252C%2520such%250Aproblems%2520are%2520tackled%2520with%2520optimal%2520control%252C%2520which%2520relies%2520on%2520the%2520optimization%2520of%250Acost%2520functions%253B%2520instead%252C%2520a%2520recent%2520biologically-motivated%2520proposal%2520casts%250Aplanning%2520and%2520control%2520as%2520an%2520inference%2520process.%2520Active%2520inference%2520assumes%2520that%250Aaction%2520and%2520perception%2520are%2520two%2520complementary%2520aspects%2520of%2520life%2520whereby%2520the%2520role%2520of%250Athe%2520former%2520is%2520to%2520fulfill%2520the%2520predictions%2520inferred%2520by%2520the%2520latter.%2520Here%252C%2520we%250Apresent%2520an%2520active%2520inference%2520approach%2520that%2520exploits%2520discrete%2520and%2520continuous%250Aprocessing%252C%2520based%2520on%2520three%2520features%253A%2520the%2520representation%2520of%2520potential%2520body%250Aconfigurations%2520in%2520relation%2520to%2520the%2520objects%2520of%2520interest%253B%2520the%2520use%2520of%2520hierarchical%250Arelationships%2520that%2520enable%2520the%2520agent%2520to%2520easily%2520interpret%2520and%2520flexibly%2520expand%2520its%250Abody%2520schema%2520for%2520tool%2520use%253B%2520the%2520definition%2520of%2520potential%2520trajectories%2520related%2520to%250Athe%2520agent%2527s%2520intentions%252C%2520used%2520to%2520infer%2520and%2520plan%2520with%2520dynamic%2520elements%2520at%250Adifferent%2520temporal%2520scales.%2520We%2520evaluate%2520this%2520deep%2520hybrid%2520model%2520on%2520a%2520habitual%250Atask%253A%2520reaching%2520a%2520moving%2520object%2520after%2520having%2520picked%2520a%2520moving%2520tool.%2520We%2520show%2520that%250Athe%2520model%2520can%2520tackle%2520the%2520presented%2520task%2520under%2520different%2520conditions.%2520This%2520study%250Aextends%2520past%2520work%2520on%2520planning%2520as%2520inference%2520and%2520advances%2520an%2520alternative%250Adirection%2520to%2520optimal%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10088v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20hybrid%20models%3A%20infer%20and%20plan%20in%20a%20dynamic%20world&entry.906535625=Matteo%20Priorelli%20and%20Ivilin%20Peev%20Stoianov&entry.1292438233=%20%20To%20determine%20an%20optimal%20plan%20for%20complex%20tasks%2C%20one%20often%20deals%20with%20dynamic%0Aand%20hierarchical%20relationships%20between%20several%20entities.%20Traditionally%2C%20such%0Aproblems%20are%20tackled%20with%20optimal%20control%2C%20which%20relies%20on%20the%20optimization%20of%0Acost%20functions%3B%20instead%2C%20a%20recent%20biologically-motivated%20proposal%20casts%0Aplanning%20and%20control%20as%20an%20inference%20process.%20Active%20inference%20assumes%20that%0Aaction%20and%20perception%20are%20two%20complementary%20aspects%20of%20life%20whereby%20the%20role%20of%0Athe%20former%20is%20to%20fulfill%20the%20predictions%20inferred%20by%20the%20latter.%20Here%2C%20we%0Apresent%20an%20active%20inference%20approach%20that%20exploits%20discrete%20and%20continuous%0Aprocessing%2C%20based%20on%20three%20features%3A%20the%20representation%20of%20potential%20body%0Aconfigurations%20in%20relation%20to%20the%20objects%20of%20interest%3B%20the%20use%20of%20hierarchical%0Arelationships%20that%20enable%20the%20agent%20to%20easily%20interpret%20and%20flexibly%20expand%20its%0Abody%20schema%20for%20tool%20use%3B%20the%20definition%20of%20potential%20trajectories%20related%20to%0Athe%20agent%27s%20intentions%2C%20used%20to%20infer%20and%20plan%20with%20dynamic%20elements%20at%0Adifferent%20temporal%20scales.%20We%20evaluate%20this%20deep%20hybrid%20model%20on%20a%20habitual%0Atask%3A%20reaching%20a%20moving%20object%20after%20having%20picked%20a%20moving%20tool.%20We%20show%20that%0Athe%20model%20can%20tackle%20the%20presented%20task%20under%20different%20conditions.%20This%20study%0Aextends%20past%20work%20on%20planning%20as%20inference%20and%20advances%20an%20alternative%0Adirection%20to%20optimal%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10088v4&entry.124074799=Read"},
{"title": "Learning-Augmented Algorithms for Boolean Satisfiability", "author": "Idan Attias and Xing Gao and Lev Reyzin", "abstract": "  Learning-augmented algorithms are a prominent recent development in beyond\nworst-case analysis. In this framework, a problem instance is provided with a\nprediction (``advice'') from a machine-learning oracle, which provides partial\ninformation about an optimal solution, and the goal is to design algorithms\nthat leverage this advice to improve worst-case performance. We study the\nclassic Boolean satisfiability (SAT) decision and optimization problems within\nthis framework using two forms of advice. ``Subset advice\" provides a random\n$\\epsilon$ fraction of the variables from an optimal assignment, whereas\n``label advice\" provides noisy predictions for all variables in an optimal\nassignment.\n  For the decision problem $k$-SAT, by using the subset advice we accelerate\nthe exponential running time of the PPSZ family of algorithms due to Paturi,\nPudlak, Saks and Zane, which currently represent the state of the art in the\nworst case. We accelerate the running time by a multiplicative factor of\n$2^{-c}$ in the base of the exponent, where $c$ is a function of $\\epsilon$ and\n$k$. For the optimization problem, we show how to incorporate subset advice in\na black-box fashion with any $\\alpha$-approximation algorithm, improving the\napproximation ratio to $\\alpha + (1 - \\alpha)\\epsilon$. Specifically, we\nachieve approximations of $0.94 + \\Omega(\\epsilon)$ for MAX-$2$-SAT, $7/8 +\n\\Omega(\\epsilon)$ for MAX-$3$-SAT, and $0.79 + \\Omega(\\epsilon)$ for MAX-SAT.\nMoreover, for label advice, we obtain near-optimal approximation for instances\nwith large average degree, thereby generalizing recent results on MAX-CUT and\nMAX-$2$-LIN.\n", "link": "http://arxiv.org/abs/2505.06146v1", "date": "2025-05-09", "relevancy": 1.7964, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4671}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-Augmented%20Algorithms%20for%20Boolean%20Satisfiability&body=Title%3A%20Learning-Augmented%20Algorithms%20for%20Boolean%20Satisfiability%0AAuthor%3A%20Idan%20Attias%20and%20Xing%20Gao%20and%20Lev%20Reyzin%0AAbstract%3A%20%20%20Learning-augmented%20algorithms%20are%20a%20prominent%20recent%20development%20in%20beyond%0Aworst-case%20analysis.%20In%20this%20framework%2C%20a%20problem%20instance%20is%20provided%20with%20a%0Aprediction%20%28%60%60advice%27%27%29%20from%20a%20machine-learning%20oracle%2C%20which%20provides%20partial%0Ainformation%20about%20an%20optimal%20solution%2C%20and%20the%20goal%20is%20to%20design%20algorithms%0Athat%20leverage%20this%20advice%20to%20improve%20worst-case%20performance.%20We%20study%20the%0Aclassic%20Boolean%20satisfiability%20%28SAT%29%20decision%20and%20optimization%20problems%20within%0Athis%20framework%20using%20two%20forms%20of%20advice.%20%60%60Subset%20advice%22%20provides%20a%20random%0A%24%5Cepsilon%24%20fraction%20of%20the%20variables%20from%20an%20optimal%20assignment%2C%20whereas%0A%60%60label%20advice%22%20provides%20noisy%20predictions%20for%20all%20variables%20in%20an%20optimal%0Aassignment.%0A%20%20For%20the%20decision%20problem%20%24k%24-SAT%2C%20by%20using%20the%20subset%20advice%20we%20accelerate%0Athe%20exponential%20running%20time%20of%20the%20PPSZ%20family%20of%20algorithms%20due%20to%20Paturi%2C%0APudlak%2C%20Saks%20and%20Zane%2C%20which%20currently%20represent%20the%20state%20of%20the%20art%20in%20the%0Aworst%20case.%20We%20accelerate%20the%20running%20time%20by%20a%20multiplicative%20factor%20of%0A%242%5E%7B-c%7D%24%20in%20the%20base%20of%20the%20exponent%2C%20where%20%24c%24%20is%20a%20function%20of%20%24%5Cepsilon%24%20and%0A%24k%24.%20For%20the%20optimization%20problem%2C%20we%20show%20how%20to%20incorporate%20subset%20advice%20in%0Aa%20black-box%20fashion%20with%20any%20%24%5Calpha%24-approximation%20algorithm%2C%20improving%20the%0Aapproximation%20ratio%20to%20%24%5Calpha%20%2B%20%281%20-%20%5Calpha%29%5Cepsilon%24.%20Specifically%2C%20we%0Aachieve%20approximations%20of%20%240.94%20%2B%20%5COmega%28%5Cepsilon%29%24%20for%20MAX-%242%24-SAT%2C%20%247/8%20%2B%0A%5COmega%28%5Cepsilon%29%24%20for%20MAX-%243%24-SAT%2C%20and%20%240.79%20%2B%20%5COmega%28%5Cepsilon%29%24%20for%20MAX-SAT.%0AMoreover%2C%20for%20label%20advice%2C%20we%20obtain%20near-optimal%20approximation%20for%20instances%0Awith%20large%20average%20degree%2C%20thereby%20generalizing%20recent%20results%20on%20MAX-CUT%20and%0AMAX-%242%24-LIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-Augmented%2520Algorithms%2520for%2520Boolean%2520Satisfiability%26entry.906535625%3DIdan%2520Attias%2520and%2520Xing%2520Gao%2520and%2520Lev%2520Reyzin%26entry.1292438233%3D%2520%2520Learning-augmented%2520algorithms%2520are%2520a%2520prominent%2520recent%2520development%2520in%2520beyond%250Aworst-case%2520analysis.%2520In%2520this%2520framework%252C%2520a%2520problem%2520instance%2520is%2520provided%2520with%2520a%250Aprediction%2520%2528%2560%2560advice%2527%2527%2529%2520from%2520a%2520machine-learning%2520oracle%252C%2520which%2520provides%2520partial%250Ainformation%2520about%2520an%2520optimal%2520solution%252C%2520and%2520the%2520goal%2520is%2520to%2520design%2520algorithms%250Athat%2520leverage%2520this%2520advice%2520to%2520improve%2520worst-case%2520performance.%2520We%2520study%2520the%250Aclassic%2520Boolean%2520satisfiability%2520%2528SAT%2529%2520decision%2520and%2520optimization%2520problems%2520within%250Athis%2520framework%2520using%2520two%2520forms%2520of%2520advice.%2520%2560%2560Subset%2520advice%2522%2520provides%2520a%2520random%250A%2524%255Cepsilon%2524%2520fraction%2520of%2520the%2520variables%2520from%2520an%2520optimal%2520assignment%252C%2520whereas%250A%2560%2560label%2520advice%2522%2520provides%2520noisy%2520predictions%2520for%2520all%2520variables%2520in%2520an%2520optimal%250Aassignment.%250A%2520%2520For%2520the%2520decision%2520problem%2520%2524k%2524-SAT%252C%2520by%2520using%2520the%2520subset%2520advice%2520we%2520accelerate%250Athe%2520exponential%2520running%2520time%2520of%2520the%2520PPSZ%2520family%2520of%2520algorithms%2520due%2520to%2520Paturi%252C%250APudlak%252C%2520Saks%2520and%2520Zane%252C%2520which%2520currently%2520represent%2520the%2520state%2520of%2520the%2520art%2520in%2520the%250Aworst%2520case.%2520We%2520accelerate%2520the%2520running%2520time%2520by%2520a%2520multiplicative%2520factor%2520of%250A%25242%255E%257B-c%257D%2524%2520in%2520the%2520base%2520of%2520the%2520exponent%252C%2520where%2520%2524c%2524%2520is%2520a%2520function%2520of%2520%2524%255Cepsilon%2524%2520and%250A%2524k%2524.%2520For%2520the%2520optimization%2520problem%252C%2520we%2520show%2520how%2520to%2520incorporate%2520subset%2520advice%2520in%250Aa%2520black-box%2520fashion%2520with%2520any%2520%2524%255Calpha%2524-approximation%2520algorithm%252C%2520improving%2520the%250Aapproximation%2520ratio%2520to%2520%2524%255Calpha%2520%252B%2520%25281%2520-%2520%255Calpha%2529%255Cepsilon%2524.%2520Specifically%252C%2520we%250Aachieve%2520approximations%2520of%2520%25240.94%2520%252B%2520%255COmega%2528%255Cepsilon%2529%2524%2520for%2520MAX-%25242%2524-SAT%252C%2520%25247/8%2520%252B%250A%255COmega%2528%255Cepsilon%2529%2524%2520for%2520MAX-%25243%2524-SAT%252C%2520and%2520%25240.79%2520%252B%2520%255COmega%2528%255Cepsilon%2529%2524%2520for%2520MAX-SAT.%250AMoreover%252C%2520for%2520label%2520advice%252C%2520we%2520obtain%2520near-optimal%2520approximation%2520for%2520instances%250Awith%2520large%2520average%2520degree%252C%2520thereby%2520generalizing%2520recent%2520results%2520on%2520MAX-CUT%2520and%250AMAX-%25242%2524-LIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-Augmented%20Algorithms%20for%20Boolean%20Satisfiability&entry.906535625=Idan%20Attias%20and%20Xing%20Gao%20and%20Lev%20Reyzin&entry.1292438233=%20%20Learning-augmented%20algorithms%20are%20a%20prominent%20recent%20development%20in%20beyond%0Aworst-case%20analysis.%20In%20this%20framework%2C%20a%20problem%20instance%20is%20provided%20with%20a%0Aprediction%20%28%60%60advice%27%27%29%20from%20a%20machine-learning%20oracle%2C%20which%20provides%20partial%0Ainformation%20about%20an%20optimal%20solution%2C%20and%20the%20goal%20is%20to%20design%20algorithms%0Athat%20leverage%20this%20advice%20to%20improve%20worst-case%20performance.%20We%20study%20the%0Aclassic%20Boolean%20satisfiability%20%28SAT%29%20decision%20and%20optimization%20problems%20within%0Athis%20framework%20using%20two%20forms%20of%20advice.%20%60%60Subset%20advice%22%20provides%20a%20random%0A%24%5Cepsilon%24%20fraction%20of%20the%20variables%20from%20an%20optimal%20assignment%2C%20whereas%0A%60%60label%20advice%22%20provides%20noisy%20predictions%20for%20all%20variables%20in%20an%20optimal%0Aassignment.%0A%20%20For%20the%20decision%20problem%20%24k%24-SAT%2C%20by%20using%20the%20subset%20advice%20we%20accelerate%0Athe%20exponential%20running%20time%20of%20the%20PPSZ%20family%20of%20algorithms%20due%20to%20Paturi%2C%0APudlak%2C%20Saks%20and%20Zane%2C%20which%20currently%20represent%20the%20state%20of%20the%20art%20in%20the%0Aworst%20case.%20We%20accelerate%20the%20running%20time%20by%20a%20multiplicative%20factor%20of%0A%242%5E%7B-c%7D%24%20in%20the%20base%20of%20the%20exponent%2C%20where%20%24c%24%20is%20a%20function%20of%20%24%5Cepsilon%24%20and%0A%24k%24.%20For%20the%20optimization%20problem%2C%20we%20show%20how%20to%20incorporate%20subset%20advice%20in%0Aa%20black-box%20fashion%20with%20any%20%24%5Calpha%24-approximation%20algorithm%2C%20improving%20the%0Aapproximation%20ratio%20to%20%24%5Calpha%20%2B%20%281%20-%20%5Calpha%29%5Cepsilon%24.%20Specifically%2C%20we%0Aachieve%20approximations%20of%20%240.94%20%2B%20%5COmega%28%5Cepsilon%29%24%20for%20MAX-%242%24-SAT%2C%20%247/8%20%2B%0A%5COmega%28%5Cepsilon%29%24%20for%20MAX-%243%24-SAT%2C%20and%20%240.79%20%2B%20%5COmega%28%5Cepsilon%29%24%20for%20MAX-SAT.%0AMoreover%2C%20for%20label%20advice%2C%20we%20obtain%20near-optimal%20approximation%20for%20instances%0Awith%20large%20average%20degree%2C%20thereby%20generalizing%20recent%20results%20on%20MAX-CUT%20and%0AMAX-%242%24-LIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06146v1&entry.124074799=Read"},
{"title": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds", "author": "Navin Sriram Ravie and Keerthi Vasan M and Asokan Thondiyath and Bijo Sebastian", "abstract": "  Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator.\n", "link": "http://arxiv.org/abs/2504.19716v2", "date": "2025-05-09", "relevancy": 1.7916, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6243}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuickGrasp%3A%20Lightweight%20Antipodal%20Grasp%20Planning%20with%20Point%20Clouds&body=Title%3A%20QuickGrasp%3A%20Lightweight%20Antipodal%20Grasp%20Planning%20with%20Point%20Clouds%0AAuthor%3A%20Navin%20Sriram%20Ravie%20and%20Keerthi%20Vasan%20M%20and%20Asokan%20Thondiyath%20and%20Bijo%20Sebastian%0AAbstract%3A%20%20%20Grasping%20has%20been%20a%20long-standing%20challenge%20in%20facilitating%20the%20final%0Ainterface%20between%20a%20robot%20and%20the%20environment.%20As%20environments%20and%20tasks%20become%0Acomplicated%2C%20the%20need%20to%20embed%20higher%20intelligence%20to%20infer%20from%20the%0Asurroundings%20and%20act%20on%20them%20has%20become%20necessary.%20Although%20most%20methods%0Autilize%20techniques%20to%20estimate%20grasp%20pose%20by%20treating%20the%20problem%20via%20pure%0Asampling-based%20approaches%20in%20the%20six-degree-of-freedom%20space%20or%20as%20a%20learning%0Aproblem%2C%20they%20usually%20fail%20in%20real-life%20settings%20owing%20to%20poor%20generalization%0Aacross%20domains.%20In%20addition%2C%20the%20time%20taken%20to%20generate%20the%20grasp%20plan%20and%20the%0Alack%20of%20repeatability%2C%20owing%20to%20sampling%20inefficiency%20and%20the%20probabilistic%0Anature%20of%20existing%20grasp%20planning%20approaches%2C%20severely%20limits%20their%20application%0Ain%20real-world%20tasks.%20This%20paper%20presents%20a%20lightweight%20analytical%20approach%0Atowards%20robotic%20grasp%20planning%2C%20particularly%20antipodal%20grasps%2C%20with%20little%20to%0Ano%20sampling%20in%20the%20six-degree-of-freedom%20space.%20The%20proposed%20grasp%20planning%0Aalgorithm%20is%20formulated%20as%20an%20optimization%20problem%20towards%20estimating%20grasp%0Apoints%20on%20the%20object%20surface%20instead%20of%20directly%20estimating%20the%20end-effector%0Apose.%20To%20this%20extent%2C%20a%20soft-region-growing%20algorithm%20is%20presented%20for%0Aeffective%20plane%20segmentation%2C%20even%20in%20the%20case%20of%20curved%20surfaces.%20An%0Aoptimization-based%20quality%20metric%20is%20then%20used%20for%20the%20evaluation%20of%20grasp%0Apoints%20to%20ensure%20indirect%20force%20closure.%20The%20proposed%20grasp%20framework%20is%0Acompared%20with%20the%20existing%20state-of-the-art%20grasp%20planning%20approach%2C%20Grasp%20pose%0Adetection%20%28GPD%29%2C%20as%20a%20baseline%20over%20multiple%20simulated%20objects.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20in%20comparison%20to%20GPD%20is%20also%20evaluated%0Ain%20a%20real-world%20setting%20using%20image%20and%20point-cloud%20data%2C%20with%20the%20planned%0Agrasps%20being%20executed%20using%20a%20ROBOTIQ%20gripper%20and%20UR5%20manipulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuickGrasp%253A%2520Lightweight%2520Antipodal%2520Grasp%2520Planning%2520with%2520Point%2520Clouds%26entry.906535625%3DNavin%2520Sriram%2520Ravie%2520and%2520Keerthi%2520Vasan%2520M%2520and%2520Asokan%2520Thondiyath%2520and%2520Bijo%2520Sebastian%26entry.1292438233%3D%2520%2520Grasping%2520has%2520been%2520a%2520long-standing%2520challenge%2520in%2520facilitating%2520the%2520final%250Ainterface%2520between%2520a%2520robot%2520and%2520the%2520environment.%2520As%2520environments%2520and%2520tasks%2520become%250Acomplicated%252C%2520the%2520need%2520to%2520embed%2520higher%2520intelligence%2520to%2520infer%2520from%2520the%250Asurroundings%2520and%2520act%2520on%2520them%2520has%2520become%2520necessary.%2520Although%2520most%2520methods%250Autilize%2520techniques%2520to%2520estimate%2520grasp%2520pose%2520by%2520treating%2520the%2520problem%2520via%2520pure%250Asampling-based%2520approaches%2520in%2520the%2520six-degree-of-freedom%2520space%2520or%2520as%2520a%2520learning%250Aproblem%252C%2520they%2520usually%2520fail%2520in%2520real-life%2520settings%2520owing%2520to%2520poor%2520generalization%250Aacross%2520domains.%2520In%2520addition%252C%2520the%2520time%2520taken%2520to%2520generate%2520the%2520grasp%2520plan%2520and%2520the%250Alack%2520of%2520repeatability%252C%2520owing%2520to%2520sampling%2520inefficiency%2520and%2520the%2520probabilistic%250Anature%2520of%2520existing%2520grasp%2520planning%2520approaches%252C%2520severely%2520limits%2520their%2520application%250Ain%2520real-world%2520tasks.%2520This%2520paper%2520presents%2520a%2520lightweight%2520analytical%2520approach%250Atowards%2520robotic%2520grasp%2520planning%252C%2520particularly%2520antipodal%2520grasps%252C%2520with%2520little%2520to%250Ano%2520sampling%2520in%2520the%2520six-degree-of-freedom%2520space.%2520The%2520proposed%2520grasp%2520planning%250Aalgorithm%2520is%2520formulated%2520as%2520an%2520optimization%2520problem%2520towards%2520estimating%2520grasp%250Apoints%2520on%2520the%2520object%2520surface%2520instead%2520of%2520directly%2520estimating%2520the%2520end-effector%250Apose.%2520To%2520this%2520extent%252C%2520a%2520soft-region-growing%2520algorithm%2520is%2520presented%2520for%250Aeffective%2520plane%2520segmentation%252C%2520even%2520in%2520the%2520case%2520of%2520curved%2520surfaces.%2520An%250Aoptimization-based%2520quality%2520metric%2520is%2520then%2520used%2520for%2520the%2520evaluation%2520of%2520grasp%250Apoints%2520to%2520ensure%2520indirect%2520force%2520closure.%2520The%2520proposed%2520grasp%2520framework%2520is%250Acompared%2520with%2520the%2520existing%2520state-of-the-art%2520grasp%2520planning%2520approach%252C%2520Grasp%2520pose%250Adetection%2520%2528GPD%2529%252C%2520as%2520a%2520baseline%2520over%2520multiple%2520simulated%2520objects.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520in%2520comparison%2520to%2520GPD%2520is%2520also%2520evaluated%250Ain%2520a%2520real-world%2520setting%2520using%2520image%2520and%2520point-cloud%2520data%252C%2520with%2520the%2520planned%250Agrasps%2520being%2520executed%2520using%2520a%2520ROBOTIQ%2520gripper%2520and%2520UR5%2520manipulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuickGrasp%3A%20Lightweight%20Antipodal%20Grasp%20Planning%20with%20Point%20Clouds&entry.906535625=Navin%20Sriram%20Ravie%20and%20Keerthi%20Vasan%20M%20and%20Asokan%20Thondiyath%20and%20Bijo%20Sebastian&entry.1292438233=%20%20Grasping%20has%20been%20a%20long-standing%20challenge%20in%20facilitating%20the%20final%0Ainterface%20between%20a%20robot%20and%20the%20environment.%20As%20environments%20and%20tasks%20become%0Acomplicated%2C%20the%20need%20to%20embed%20higher%20intelligence%20to%20infer%20from%20the%0Asurroundings%20and%20act%20on%20them%20has%20become%20necessary.%20Although%20most%20methods%0Autilize%20techniques%20to%20estimate%20grasp%20pose%20by%20treating%20the%20problem%20via%20pure%0Asampling-based%20approaches%20in%20the%20six-degree-of-freedom%20space%20or%20as%20a%20learning%0Aproblem%2C%20they%20usually%20fail%20in%20real-life%20settings%20owing%20to%20poor%20generalization%0Aacross%20domains.%20In%20addition%2C%20the%20time%20taken%20to%20generate%20the%20grasp%20plan%20and%20the%0Alack%20of%20repeatability%2C%20owing%20to%20sampling%20inefficiency%20and%20the%20probabilistic%0Anature%20of%20existing%20grasp%20planning%20approaches%2C%20severely%20limits%20their%20application%0Ain%20real-world%20tasks.%20This%20paper%20presents%20a%20lightweight%20analytical%20approach%0Atowards%20robotic%20grasp%20planning%2C%20particularly%20antipodal%20grasps%2C%20with%20little%20to%0Ano%20sampling%20in%20the%20six-degree-of-freedom%20space.%20The%20proposed%20grasp%20planning%0Aalgorithm%20is%20formulated%20as%20an%20optimization%20problem%20towards%20estimating%20grasp%0Apoints%20on%20the%20object%20surface%20instead%20of%20directly%20estimating%20the%20end-effector%0Apose.%20To%20this%20extent%2C%20a%20soft-region-growing%20algorithm%20is%20presented%20for%0Aeffective%20plane%20segmentation%2C%20even%20in%20the%20case%20of%20curved%20surfaces.%20An%0Aoptimization-based%20quality%20metric%20is%20then%20used%20for%20the%20evaluation%20of%20grasp%0Apoints%20to%20ensure%20indirect%20force%20closure.%20The%20proposed%20grasp%20framework%20is%0Acompared%20with%20the%20existing%20state-of-the-art%20grasp%20planning%20approach%2C%20Grasp%20pose%0Adetection%20%28GPD%29%2C%20as%20a%20baseline%20over%20multiple%20simulated%20objects.%20The%0Aeffectiveness%20of%20the%20proposed%20approach%20in%20comparison%20to%20GPD%20is%20also%20evaluated%0Ain%20a%20real-world%20setting%20using%20image%20and%20point-cloud%20data%2C%20with%20the%20planned%0Agrasps%20being%20executed%20using%20a%20ROBOTIQ%20gripper%20and%20UR5%20manipulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19716v2&entry.124074799=Read"},
{"title": "FIC-TSC: Learning Time Series Classification with Fisher Information\n  Constraint", "author": "Xiwen Chen and Wenhui Zhu and Peijie Qiu and Hao Wang and Huayu Li and Zihan Li and Yalin Wang and Aristeidis Sotiras and Abolfazl Razi", "abstract": "  Analyzing time series data is crucial to a wide spectrum of applications,\nincluding economics, online marketplaces, and human healthcare. In particular,\ntime series classification plays an indispensable role in segmenting different\nphases in stock markets, predicting customer behavior, and classifying worker\nactions and engagement levels. These aspects contribute significantly to the\nadvancement of automated decision-making and system optimization in real-world\napplications. However, there is a large consensus that time series data often\nsuffers from domain shifts between training and test sets, which dramatically\ndegrades the classification performance. Despite the success of (reversible)\ninstance normalization in handling the domain shifts for time series regression\ntasks, its performance in classification is unsatisfactory. In this paper, we\npropose \\textit{FIC-TSC}, a training framework for time series classification\nthat leverages Fisher information as the constraint. We theoretically and\nempirically show this is an efficient and effective solution to guide the model\nconverge toward flatter minima, which enhances its generalizability to\ndistribution shifts. We rigorously evaluate our method on 30 UEA multivariate\nand 85 UCR univariate datasets. Our empirical results demonstrate the\nsuperiority of the proposed method over 14 recent state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.06114v1", "date": "2025-05-09", "relevancy": 1.7892, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4533}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4511}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIC-TSC%3A%20Learning%20Time%20Series%20Classification%20with%20Fisher%20Information%0A%20%20Constraint&body=Title%3A%20FIC-TSC%3A%20Learning%20Time%20Series%20Classification%20with%20Fisher%20Information%0A%20%20Constraint%0AAuthor%3A%20Xiwen%20Chen%20and%20Wenhui%20Zhu%20and%20Peijie%20Qiu%20and%20Hao%20Wang%20and%20Huayu%20Li%20and%20Zihan%20Li%20and%20Yalin%20Wang%20and%20Aristeidis%20Sotiras%20and%20Abolfazl%20Razi%0AAbstract%3A%20%20%20Analyzing%20time%20series%20data%20is%20crucial%20to%20a%20wide%20spectrum%20of%20applications%2C%0Aincluding%20economics%2C%20online%20marketplaces%2C%20and%20human%20healthcare.%20In%20particular%2C%0Atime%20series%20classification%20plays%20an%20indispensable%20role%20in%20segmenting%20different%0Aphases%20in%20stock%20markets%2C%20predicting%20customer%20behavior%2C%20and%20classifying%20worker%0Aactions%20and%20engagement%20levels.%20These%20aspects%20contribute%20significantly%20to%20the%0Aadvancement%20of%20automated%20decision-making%20and%20system%20optimization%20in%20real-world%0Aapplications.%20However%2C%20there%20is%20a%20large%20consensus%20that%20time%20series%20data%20often%0Asuffers%20from%20domain%20shifts%20between%20training%20and%20test%20sets%2C%20which%20dramatically%0Adegrades%20the%20classification%20performance.%20Despite%20the%20success%20of%20%28reversible%29%0Ainstance%20normalization%20in%20handling%20the%20domain%20shifts%20for%20time%20series%20regression%0Atasks%2C%20its%20performance%20in%20classification%20is%20unsatisfactory.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextit%7BFIC-TSC%7D%2C%20a%20training%20framework%20for%20time%20series%20classification%0Athat%20leverages%20Fisher%20information%20as%20the%20constraint.%20We%20theoretically%20and%0Aempirically%20show%20this%20is%20an%20efficient%20and%20effective%20solution%20to%20guide%20the%20model%0Aconverge%20toward%20flatter%20minima%2C%20which%20enhances%20its%20generalizability%20to%0Adistribution%20shifts.%20We%20rigorously%20evaluate%20our%20method%20on%2030%20UEA%20multivariate%0Aand%2085%20UCR%20univariate%20datasets.%20Our%20empirical%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%2014%20recent%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIC-TSC%253A%2520Learning%2520Time%2520Series%2520Classification%2520with%2520Fisher%2520Information%250A%2520%2520Constraint%26entry.906535625%3DXiwen%2520Chen%2520and%2520Wenhui%2520Zhu%2520and%2520Peijie%2520Qiu%2520and%2520Hao%2520Wang%2520and%2520Huayu%2520Li%2520and%2520Zihan%2520Li%2520and%2520Yalin%2520Wang%2520and%2520Aristeidis%2520Sotiras%2520and%2520Abolfazl%2520Razi%26entry.1292438233%3D%2520%2520Analyzing%2520time%2520series%2520data%2520is%2520crucial%2520to%2520a%2520wide%2520spectrum%2520of%2520applications%252C%250Aincluding%2520economics%252C%2520online%2520marketplaces%252C%2520and%2520human%2520healthcare.%2520In%2520particular%252C%250Atime%2520series%2520classification%2520plays%2520an%2520indispensable%2520role%2520in%2520segmenting%2520different%250Aphases%2520in%2520stock%2520markets%252C%2520predicting%2520customer%2520behavior%252C%2520and%2520classifying%2520worker%250Aactions%2520and%2520engagement%2520levels.%2520These%2520aspects%2520contribute%2520significantly%2520to%2520the%250Aadvancement%2520of%2520automated%2520decision-making%2520and%2520system%2520optimization%2520in%2520real-world%250Aapplications.%2520However%252C%2520there%2520is%2520a%2520large%2520consensus%2520that%2520time%2520series%2520data%2520often%250Asuffers%2520from%2520domain%2520shifts%2520between%2520training%2520and%2520test%2520sets%252C%2520which%2520dramatically%250Adegrades%2520the%2520classification%2520performance.%2520Despite%2520the%2520success%2520of%2520%2528reversible%2529%250Ainstance%2520normalization%2520in%2520handling%2520the%2520domain%2520shifts%2520for%2520time%2520series%2520regression%250Atasks%252C%2520its%2520performance%2520in%2520classification%2520is%2520unsatisfactory.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520%255Ctextit%257BFIC-TSC%257D%252C%2520a%2520training%2520framework%2520for%2520time%2520series%2520classification%250Athat%2520leverages%2520Fisher%2520information%2520as%2520the%2520constraint.%2520We%2520theoretically%2520and%250Aempirically%2520show%2520this%2520is%2520an%2520efficient%2520and%2520effective%2520solution%2520to%2520guide%2520the%2520model%250Aconverge%2520toward%2520flatter%2520minima%252C%2520which%2520enhances%2520its%2520generalizability%2520to%250Adistribution%2520shifts.%2520We%2520rigorously%2520evaluate%2520our%2520method%2520on%252030%2520UEA%2520multivariate%250Aand%252085%2520UCR%2520univariate%2520datasets.%2520Our%2520empirical%2520results%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method%2520over%252014%2520recent%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIC-TSC%3A%20Learning%20Time%20Series%20Classification%20with%20Fisher%20Information%0A%20%20Constraint&entry.906535625=Xiwen%20Chen%20and%20Wenhui%20Zhu%20and%20Peijie%20Qiu%20and%20Hao%20Wang%20and%20Huayu%20Li%20and%20Zihan%20Li%20and%20Yalin%20Wang%20and%20Aristeidis%20Sotiras%20and%20Abolfazl%20Razi&entry.1292438233=%20%20Analyzing%20time%20series%20data%20is%20crucial%20to%20a%20wide%20spectrum%20of%20applications%2C%0Aincluding%20economics%2C%20online%20marketplaces%2C%20and%20human%20healthcare.%20In%20particular%2C%0Atime%20series%20classification%20plays%20an%20indispensable%20role%20in%20segmenting%20different%0Aphases%20in%20stock%20markets%2C%20predicting%20customer%20behavior%2C%20and%20classifying%20worker%0Aactions%20and%20engagement%20levels.%20These%20aspects%20contribute%20significantly%20to%20the%0Aadvancement%20of%20automated%20decision-making%20and%20system%20optimization%20in%20real-world%0Aapplications.%20However%2C%20there%20is%20a%20large%20consensus%20that%20time%20series%20data%20often%0Asuffers%20from%20domain%20shifts%20between%20training%20and%20test%20sets%2C%20which%20dramatically%0Adegrades%20the%20classification%20performance.%20Despite%20the%20success%20of%20%28reversible%29%0Ainstance%20normalization%20in%20handling%20the%20domain%20shifts%20for%20time%20series%20regression%0Atasks%2C%20its%20performance%20in%20classification%20is%20unsatisfactory.%20In%20this%20paper%2C%20we%0Apropose%20%5Ctextit%7BFIC-TSC%7D%2C%20a%20training%20framework%20for%20time%20series%20classification%0Athat%20leverages%20Fisher%20information%20as%20the%20constraint.%20We%20theoretically%20and%0Aempirically%20show%20this%20is%20an%20efficient%20and%20effective%20solution%20to%20guide%20the%20model%0Aconverge%20toward%20flatter%20minima%2C%20which%20enhances%20its%20generalizability%20to%0Adistribution%20shifts.%20We%20rigorously%20evaluate%20our%20method%20on%2030%20UEA%20multivariate%0Aand%2085%20UCR%20univariate%20datasets.%20Our%20empirical%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%2014%20recent%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06114v1&entry.124074799=Read"},
{"title": "Higher-Order Graphon Neural Networks: Approximation and Cut Distance", "author": "Daniel Herbst and Stefanie Jegelka", "abstract": "  Graph limit models, like graphons for limits of dense graphs, have recently\nbeen used to study size transferability of graph neural networks (GNNs). While\nmost literature focuses on message passing GNNs (MPNNs), in this work we attend\nto the more powerful higher-order GNNs. First, we extend the $k$-WL test for\ngraphons (B\\\"oker, 2023) to the graphon-signal space and introduce\nsignal-weighted homomorphism densities as a key tool. As an exemplary focus, we\ngeneralize Invariant Graph Networks (IGNs) to graphons, proposing Invariant\nGraphon Networks (IWNs) defined via a subset of the IGN basis corresponding to\nbounded linear operators. Even with this restricted basis, we show that IWNs of\norder $k$ are at least as powerful as the $k$-WL test, and we establish\nuniversal approximation results for graphon-signals in $L^p$ distances. This\nsignificantly extends the prior work of Cai & Wang (2022), showing that IWNs--a\nsubset of their IGN-small--retain effectively the same expressivity as the full\nIGN basis in the limit. In contrast to their approach, our blueprint of IWNs\nalso aligns better with the geometry of graphon space, for example facilitating\ncomparability to MPNNs. We highlight that, while typical higher-order GNNs are\ndiscontinuous w.r.t. cut distance--which causes their lack of convergence and\nis inherently tied to the definition of $k$-WL--transferability remains\nachievable.\n", "link": "http://arxiv.org/abs/2503.14338v2", "date": "2025-05-09", "relevancy": 1.7874, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4532}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4434}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Graphon%20Neural%20Networks%3A%20Approximation%20and%20Cut%20Distance&body=Title%3A%20Higher-Order%20Graphon%20Neural%20Networks%3A%20Approximation%20and%20Cut%20Distance%0AAuthor%3A%20Daniel%20Herbst%20and%20Stefanie%20Jegelka%0AAbstract%3A%20%20%20Graph%20limit%20models%2C%20like%20graphons%20for%20limits%20of%20dense%20graphs%2C%20have%20recently%0Abeen%20used%20to%20study%20size%20transferability%20of%20graph%20neural%20networks%20%28GNNs%29.%20While%0Amost%20literature%20focuses%20on%20message%20passing%20GNNs%20%28MPNNs%29%2C%20in%20this%20work%20we%20attend%0Ato%20the%20more%20powerful%20higher-order%20GNNs.%20First%2C%20we%20extend%20the%20%24k%24-WL%20test%20for%0Agraphons%20%28B%5C%22oker%2C%202023%29%20to%20the%20graphon-signal%20space%20and%20introduce%0Asignal-weighted%20homomorphism%20densities%20as%20a%20key%20tool.%20As%20an%20exemplary%20focus%2C%20we%0Ageneralize%20Invariant%20Graph%20Networks%20%28IGNs%29%20to%20graphons%2C%20proposing%20Invariant%0AGraphon%20Networks%20%28IWNs%29%20defined%20via%20a%20subset%20of%20the%20IGN%20basis%20corresponding%20to%0Abounded%20linear%20operators.%20Even%20with%20this%20restricted%20basis%2C%20we%20show%20that%20IWNs%20of%0Aorder%20%24k%24%20are%20at%20least%20as%20powerful%20as%20the%20%24k%24-WL%20test%2C%20and%20we%20establish%0Auniversal%20approximation%20results%20for%20graphon-signals%20in%20%24L%5Ep%24%20distances.%20This%0Asignificantly%20extends%20the%20prior%20work%20of%20Cai%20%26%20Wang%20%282022%29%2C%20showing%20that%20IWNs--a%0Asubset%20of%20their%20IGN-small--retain%20effectively%20the%20same%20expressivity%20as%20the%20full%0AIGN%20basis%20in%20the%20limit.%20In%20contrast%20to%20their%20approach%2C%20our%20blueprint%20of%20IWNs%0Aalso%20aligns%20better%20with%20the%20geometry%20of%20graphon%20space%2C%20for%20example%20facilitating%0Acomparability%20to%20MPNNs.%20We%20highlight%20that%2C%20while%20typical%20higher-order%20GNNs%20are%0Adiscontinuous%20w.r.t.%20cut%20distance--which%20causes%20their%20lack%20of%20convergence%20and%0Ais%20inherently%20tied%20to%20the%20definition%20of%20%24k%24-WL--transferability%20remains%0Aachievable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Graphon%2520Neural%2520Networks%253A%2520Approximation%2520and%2520Cut%2520Distance%26entry.906535625%3DDaniel%2520Herbst%2520and%2520Stefanie%2520Jegelka%26entry.1292438233%3D%2520%2520Graph%2520limit%2520models%252C%2520like%2520graphons%2520for%2520limits%2520of%2520dense%2520graphs%252C%2520have%2520recently%250Abeen%2520used%2520to%2520study%2520size%2520transferability%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520While%250Amost%2520literature%2520focuses%2520on%2520message%2520passing%2520GNNs%2520%2528MPNNs%2529%252C%2520in%2520this%2520work%2520we%2520attend%250Ato%2520the%2520more%2520powerful%2520higher-order%2520GNNs.%2520First%252C%2520we%2520extend%2520the%2520%2524k%2524-WL%2520test%2520for%250Agraphons%2520%2528B%255C%2522oker%252C%25202023%2529%2520to%2520the%2520graphon-signal%2520space%2520and%2520introduce%250Asignal-weighted%2520homomorphism%2520densities%2520as%2520a%2520key%2520tool.%2520As%2520an%2520exemplary%2520focus%252C%2520we%250Ageneralize%2520Invariant%2520Graph%2520Networks%2520%2528IGNs%2529%2520to%2520graphons%252C%2520proposing%2520Invariant%250AGraphon%2520Networks%2520%2528IWNs%2529%2520defined%2520via%2520a%2520subset%2520of%2520the%2520IGN%2520basis%2520corresponding%2520to%250Abounded%2520linear%2520operators.%2520Even%2520with%2520this%2520restricted%2520basis%252C%2520we%2520show%2520that%2520IWNs%2520of%250Aorder%2520%2524k%2524%2520are%2520at%2520least%2520as%2520powerful%2520as%2520the%2520%2524k%2524-WL%2520test%252C%2520and%2520we%2520establish%250Auniversal%2520approximation%2520results%2520for%2520graphon-signals%2520in%2520%2524L%255Ep%2524%2520distances.%2520This%250Asignificantly%2520extends%2520the%2520prior%2520work%2520of%2520Cai%2520%2526%2520Wang%2520%25282022%2529%252C%2520showing%2520that%2520IWNs--a%250Asubset%2520of%2520their%2520IGN-small--retain%2520effectively%2520the%2520same%2520expressivity%2520as%2520the%2520full%250AIGN%2520basis%2520in%2520the%2520limit.%2520In%2520contrast%2520to%2520their%2520approach%252C%2520our%2520blueprint%2520of%2520IWNs%250Aalso%2520aligns%2520better%2520with%2520the%2520geometry%2520of%2520graphon%2520space%252C%2520for%2520example%2520facilitating%250Acomparability%2520to%2520MPNNs.%2520We%2520highlight%2520that%252C%2520while%2520typical%2520higher-order%2520GNNs%2520are%250Adiscontinuous%2520w.r.t.%2520cut%2520distance--which%2520causes%2520their%2520lack%2520of%2520convergence%2520and%250Ais%2520inherently%2520tied%2520to%2520the%2520definition%2520of%2520%2524k%2524-WL--transferability%2520remains%250Aachievable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Graphon%20Neural%20Networks%3A%20Approximation%20and%20Cut%20Distance&entry.906535625=Daniel%20Herbst%20and%20Stefanie%20Jegelka&entry.1292438233=%20%20Graph%20limit%20models%2C%20like%20graphons%20for%20limits%20of%20dense%20graphs%2C%20have%20recently%0Abeen%20used%20to%20study%20size%20transferability%20of%20graph%20neural%20networks%20%28GNNs%29.%20While%0Amost%20literature%20focuses%20on%20message%20passing%20GNNs%20%28MPNNs%29%2C%20in%20this%20work%20we%20attend%0Ato%20the%20more%20powerful%20higher-order%20GNNs.%20First%2C%20we%20extend%20the%20%24k%24-WL%20test%20for%0Agraphons%20%28B%5C%22oker%2C%202023%29%20to%20the%20graphon-signal%20space%20and%20introduce%0Asignal-weighted%20homomorphism%20densities%20as%20a%20key%20tool.%20As%20an%20exemplary%20focus%2C%20we%0Ageneralize%20Invariant%20Graph%20Networks%20%28IGNs%29%20to%20graphons%2C%20proposing%20Invariant%0AGraphon%20Networks%20%28IWNs%29%20defined%20via%20a%20subset%20of%20the%20IGN%20basis%20corresponding%20to%0Abounded%20linear%20operators.%20Even%20with%20this%20restricted%20basis%2C%20we%20show%20that%20IWNs%20of%0Aorder%20%24k%24%20are%20at%20least%20as%20powerful%20as%20the%20%24k%24-WL%20test%2C%20and%20we%20establish%0Auniversal%20approximation%20results%20for%20graphon-signals%20in%20%24L%5Ep%24%20distances.%20This%0Asignificantly%20extends%20the%20prior%20work%20of%20Cai%20%26%20Wang%20%282022%29%2C%20showing%20that%20IWNs--a%0Asubset%20of%20their%20IGN-small--retain%20effectively%20the%20same%20expressivity%20as%20the%20full%0AIGN%20basis%20in%20the%20limit.%20In%20contrast%20to%20their%20approach%2C%20our%20blueprint%20of%20IWNs%0Aalso%20aligns%20better%20with%20the%20geometry%20of%20graphon%20space%2C%20for%20example%20facilitating%0Acomparability%20to%20MPNNs.%20We%20highlight%20that%2C%20while%20typical%20higher-order%20GNNs%20are%0Adiscontinuous%20w.r.t.%20cut%20distance--which%20causes%20their%20lack%20of%20convergence%20and%0Ais%20inherently%20tied%20to%20the%20definition%20of%20%24k%24-WL--transferability%20remains%0Aachievable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14338v2&entry.124074799=Read"},
{"title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs", "author": "Sam Bush and Matthew DeLorenzo and Phat Tieu and Jeyavijayan Rajendran", "abstract": "  Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.\n", "link": "http://arxiv.org/abs/2505.06096v1", "date": "2025-05-09", "relevancy": 1.7748, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4726}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4251}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20and%20Fair%20Hardware%3A%20A%20Pathway%20to%20Copyright%20Infringement-Free%20Verilog%0A%20%20Generation%20using%20LLMs&body=Title%3A%20Free%20and%20Fair%20Hardware%3A%20A%20Pathway%20to%20Copyright%20Infringement-Free%20Verilog%0A%20%20Generation%20using%20LLMs%0AAuthor%3A%20Sam%20Bush%20and%20Matthew%20DeLorenzo%20and%20Phat%20Tieu%20and%20Jeyavijayan%20Rajendran%0AAbstract%3A%20%20%20Limitations%20in%20Large%20Language%20Model%20%28LLM%29%20capabilities%20for%20hardware%20design%0Atasks%2C%20such%20as%20generating%20functional%20Verilog%20codes%2C%20have%20motivated%20various%0Afine-tuning%20optimizations%20utilizing%20curated%20hardware%20datasets%20from%20open-source%0Arepositories.%20However%2C%20these%20datasets%20remain%20limited%20in%20size%20and%20contain%0Aminimal%20checks%20on%20licensing%20for%20reuse%2C%20resulting%20in%20potential%20copyright%0Aviolations%20by%20fine-tuned%20LLMs.%20Therefore%2C%20we%20propose%20an%20evaluation%20benchmark%20to%0Aestimate%20the%20risk%20of%20Verilog-trained%20LLMs%20to%20generate%20copyright-protected%0Acodes.%20To%20minimize%20this%20risk%2C%20we%20present%20an%20open-source%20Verilog%20dataset%2C%0AFreeSet%2C%20containing%20over%20220k%20files%2C%20along%20with%20the%20automated%20dataset%20curation%0Aframework%20utilized%20to%20provide%20additional%20guarantees%20of%20fair-use%20Verilog%20data.%0AWe%20then%20execute%20an%20LLM%20fine-tuning%20framework%20consisting%20of%20continual%0Apre-training%2C%20resulting%20in%20a%20fine-tuned%20Llama%20model%20for%20Verilog%2C%20FreeV.%20Our%0Aresults%20indicate%20that%20FreeV%20demonstrates%20the%20smallest%20risk%20of%0Acopyright-infringement%20among%20prior%20works%2C%20with%20only%20a%203%25%20violation%20rate.%0AFurthermore%2C%20experimental%20results%20demonstrate%20improvements%20in%20Verilog%0Ageneration%20functionality%20over%20its%20baseline%20model%2C%20improving%20VerilogEval%20pass%4010%0Arates%20by%20over%2010%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520and%2520Fair%2520Hardware%253A%2520A%2520Pathway%2520to%2520Copyright%2520Infringement-Free%2520Verilog%250A%2520%2520Generation%2520using%2520LLMs%26entry.906535625%3DSam%2520Bush%2520and%2520Matthew%2520DeLorenzo%2520and%2520Phat%2520Tieu%2520and%2520Jeyavijayan%2520Rajendran%26entry.1292438233%3D%2520%2520Limitations%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520capabilities%2520for%2520hardware%2520design%250Atasks%252C%2520such%2520as%2520generating%2520functional%2520Verilog%2520codes%252C%2520have%2520motivated%2520various%250Afine-tuning%2520optimizations%2520utilizing%2520curated%2520hardware%2520datasets%2520from%2520open-source%250Arepositories.%2520However%252C%2520these%2520datasets%2520remain%2520limited%2520in%2520size%2520and%2520contain%250Aminimal%2520checks%2520on%2520licensing%2520for%2520reuse%252C%2520resulting%2520in%2520potential%2520copyright%250Aviolations%2520by%2520fine-tuned%2520LLMs.%2520Therefore%252C%2520we%2520propose%2520an%2520evaluation%2520benchmark%2520to%250Aestimate%2520the%2520risk%2520of%2520Verilog-trained%2520LLMs%2520to%2520generate%2520copyright-protected%250Acodes.%2520To%2520minimize%2520this%2520risk%252C%2520we%2520present%2520an%2520open-source%2520Verilog%2520dataset%252C%250AFreeSet%252C%2520containing%2520over%2520220k%2520files%252C%2520along%2520with%2520the%2520automated%2520dataset%2520curation%250Aframework%2520utilized%2520to%2520provide%2520additional%2520guarantees%2520of%2520fair-use%2520Verilog%2520data.%250AWe%2520then%2520execute%2520an%2520LLM%2520fine-tuning%2520framework%2520consisting%2520of%2520continual%250Apre-training%252C%2520resulting%2520in%2520a%2520fine-tuned%2520Llama%2520model%2520for%2520Verilog%252C%2520FreeV.%2520Our%250Aresults%2520indicate%2520that%2520FreeV%2520demonstrates%2520the%2520smallest%2520risk%2520of%250Acopyright-infringement%2520among%2520prior%2520works%252C%2520with%2520only%2520a%25203%2525%2520violation%2520rate.%250AFurthermore%252C%2520experimental%2520results%2520demonstrate%2520improvements%2520in%2520Verilog%250Ageneration%2520functionality%2520over%2520its%2520baseline%2520model%252C%2520improving%2520VerilogEval%2520pass%254010%250Arates%2520by%2520over%252010%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20and%20Fair%20Hardware%3A%20A%20Pathway%20to%20Copyright%20Infringement-Free%20Verilog%0A%20%20Generation%20using%20LLMs&entry.906535625=Sam%20Bush%20and%20Matthew%20DeLorenzo%20and%20Phat%20Tieu%20and%20Jeyavijayan%20Rajendran&entry.1292438233=%20%20Limitations%20in%20Large%20Language%20Model%20%28LLM%29%20capabilities%20for%20hardware%20design%0Atasks%2C%20such%20as%20generating%20functional%20Verilog%20codes%2C%20have%20motivated%20various%0Afine-tuning%20optimizations%20utilizing%20curated%20hardware%20datasets%20from%20open-source%0Arepositories.%20However%2C%20these%20datasets%20remain%20limited%20in%20size%20and%20contain%0Aminimal%20checks%20on%20licensing%20for%20reuse%2C%20resulting%20in%20potential%20copyright%0Aviolations%20by%20fine-tuned%20LLMs.%20Therefore%2C%20we%20propose%20an%20evaluation%20benchmark%20to%0Aestimate%20the%20risk%20of%20Verilog-trained%20LLMs%20to%20generate%20copyright-protected%0Acodes.%20To%20minimize%20this%20risk%2C%20we%20present%20an%20open-source%20Verilog%20dataset%2C%0AFreeSet%2C%20containing%20over%20220k%20files%2C%20along%20with%20the%20automated%20dataset%20curation%0Aframework%20utilized%20to%20provide%20additional%20guarantees%20of%20fair-use%20Verilog%20data.%0AWe%20then%20execute%20an%20LLM%20fine-tuning%20framework%20consisting%20of%20continual%0Apre-training%2C%20resulting%20in%20a%20fine-tuned%20Llama%20model%20for%20Verilog%2C%20FreeV.%20Our%0Aresults%20indicate%20that%20FreeV%20demonstrates%20the%20smallest%20risk%20of%0Acopyright-infringement%20among%20prior%20works%2C%20with%20only%20a%203%25%20violation%20rate.%0AFurthermore%2C%20experimental%20results%20demonstrate%20improvements%20in%20Verilog%0Ageneration%20functionality%20over%20its%20baseline%20model%2C%20improving%20VerilogEval%20pass%4010%0Arates%20by%20over%2010%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06096v1&entry.124074799=Read"},
{"title": "CoverUp: Effective High Coverage Test Generation for Python", "author": "Juan Altmayer Pizzorno and Emery D. Berger", "abstract": "  Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.\n", "link": "http://arxiv.org/abs/2403.16218v4", "date": "2025-05-09", "relevancy": 1.7739, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4674}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4458}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoverUp%3A%20Effective%20High%20Coverage%20Test%20Generation%20for%20Python&body=Title%3A%20CoverUp%3A%20Effective%20High%20Coverage%20Test%20Generation%20for%20Python%0AAuthor%3A%20Juan%20Altmayer%20Pizzorno%20and%20Emery%20D.%20Berger%0AAbstract%3A%20%20%20Testing%20is%20an%20essential%20part%20of%20software%20development.%20Test%20generation%20tools%0Aattempt%20to%20automate%20the%20otherwise%20labor-intensive%20task%20of%20test%20creation%2C%20but%0Agenerating%20high-coverage%20tests%20remains%20challenging.%20This%20paper%20proposes%0ACoverUp%2C%20a%20novel%20approach%20to%20driving%20the%20generation%20of%20high-coverage%20Python%0Aregression%20tests.%20CoverUp%20combines%20coverage%20analysis%2C%20code%20context%2C%20and%0Afeedback%20in%20prompts%20that%20iteratively%20guide%20the%20LLM%20to%20generate%20tests%20that%0Aimprove%20line%20and%20branch%20coverage.%20We%20evaluate%20our%20prototype%20CoverUp%0Aimplementation%20across%20a%20benchmark%20of%20challenging%20code%20derived%20from%20open-source%0APython%20projects%20and%20show%20that%20CoverUp%20substantially%20improves%20on%20the%20state%20of%0Athe%20art.%20Compared%20to%20CodaMosa%2C%20a%20hybrid%20search/LLM-based%20test%20generator%2C%0ACoverUp%20achieves%20a%20per-module%20median%20line%2Bbranch%20coverage%20of%2080%25%20%28vs.%2047%25%29.%0ACompared%20to%20MuTAP%2C%20a%20mutation-%20and%20LLM-based%20test%20generator%2C%20CoverUp%20achieves%0Aan%20overall%20line%2Bbranch%20coverage%20of%2089%25%20%28vs.%2077%25%29.%20We%20also%20demonstrate%20that%0ACoverUp%27s%20performance%20stems%20not%20only%20from%20the%20LLM%20used%20but%20from%20the%20combined%0Aeffectiveness%20of%20its%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16218v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoverUp%253A%2520Effective%2520High%2520Coverage%2520Test%2520Generation%2520for%2520Python%26entry.906535625%3DJuan%2520Altmayer%2520Pizzorno%2520and%2520Emery%2520D.%2520Berger%26entry.1292438233%3D%2520%2520Testing%2520is%2520an%2520essential%2520part%2520of%2520software%2520development.%2520Test%2520generation%2520tools%250Aattempt%2520to%2520automate%2520the%2520otherwise%2520labor-intensive%2520task%2520of%2520test%2520creation%252C%2520but%250Agenerating%2520high-coverage%2520tests%2520remains%2520challenging.%2520This%2520paper%2520proposes%250ACoverUp%252C%2520a%2520novel%2520approach%2520to%2520driving%2520the%2520generation%2520of%2520high-coverage%2520Python%250Aregression%2520tests.%2520CoverUp%2520combines%2520coverage%2520analysis%252C%2520code%2520context%252C%2520and%250Afeedback%2520in%2520prompts%2520that%2520iteratively%2520guide%2520the%2520LLM%2520to%2520generate%2520tests%2520that%250Aimprove%2520line%2520and%2520branch%2520coverage.%2520We%2520evaluate%2520our%2520prototype%2520CoverUp%250Aimplementation%2520across%2520a%2520benchmark%2520of%2520challenging%2520code%2520derived%2520from%2520open-source%250APython%2520projects%2520and%2520show%2520that%2520CoverUp%2520substantially%2520improves%2520on%2520the%2520state%2520of%250Athe%2520art.%2520Compared%2520to%2520CodaMosa%252C%2520a%2520hybrid%2520search/LLM-based%2520test%2520generator%252C%250ACoverUp%2520achieves%2520a%2520per-module%2520median%2520line%252Bbranch%2520coverage%2520of%252080%2525%2520%2528vs.%252047%2525%2529.%250ACompared%2520to%2520MuTAP%252C%2520a%2520mutation-%2520and%2520LLM-based%2520test%2520generator%252C%2520CoverUp%2520achieves%250Aan%2520overall%2520line%252Bbranch%2520coverage%2520of%252089%2525%2520%2528vs.%252077%2525%2529.%2520We%2520also%2520demonstrate%2520that%250ACoverUp%2527s%2520performance%2520stems%2520not%2520only%2520from%2520the%2520LLM%2520used%2520but%2520from%2520the%2520combined%250Aeffectiveness%2520of%2520its%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16218v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoverUp%3A%20Effective%20High%20Coverage%20Test%20Generation%20for%20Python&entry.906535625=Juan%20Altmayer%20Pizzorno%20and%20Emery%20D.%20Berger&entry.1292438233=%20%20Testing%20is%20an%20essential%20part%20of%20software%20development.%20Test%20generation%20tools%0Aattempt%20to%20automate%20the%20otherwise%20labor-intensive%20task%20of%20test%20creation%2C%20but%0Agenerating%20high-coverage%20tests%20remains%20challenging.%20This%20paper%20proposes%0ACoverUp%2C%20a%20novel%20approach%20to%20driving%20the%20generation%20of%20high-coverage%20Python%0Aregression%20tests.%20CoverUp%20combines%20coverage%20analysis%2C%20code%20context%2C%20and%0Afeedback%20in%20prompts%20that%20iteratively%20guide%20the%20LLM%20to%20generate%20tests%20that%0Aimprove%20line%20and%20branch%20coverage.%20We%20evaluate%20our%20prototype%20CoverUp%0Aimplementation%20across%20a%20benchmark%20of%20challenging%20code%20derived%20from%20open-source%0APython%20projects%20and%20show%20that%20CoverUp%20substantially%20improves%20on%20the%20state%20of%0Athe%20art.%20Compared%20to%20CodaMosa%2C%20a%20hybrid%20search/LLM-based%20test%20generator%2C%0ACoverUp%20achieves%20a%20per-module%20median%20line%2Bbranch%20coverage%20of%2080%25%20%28vs.%2047%25%29.%0ACompared%20to%20MuTAP%2C%20a%20mutation-%20and%20LLM-based%20test%20generator%2C%20CoverUp%20achieves%0Aan%20overall%20line%2Bbranch%20coverage%20of%2089%25%20%28vs.%2077%25%29.%20We%20also%20demonstrate%20that%0ACoverUp%27s%20performance%20stems%20not%20only%20from%20the%20LLM%20used%20but%20from%20the%20combined%0Aeffectiveness%20of%20its%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16218v4&entry.124074799=Read"},
{"title": "Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based\n  Approach", "author": "Tim Schneider and Cristiana de Farias and Roberto Calandra and Liming Chen and Jan Peters", "abstract": "  Humans make extensive use of haptic exploration to map and identify the\nproperties of the objects that we touch. In robotics, active tactile perception\nhas emerged as an important research domain that complements vision for tasks\nsuch as object classification, shape reconstruction, and manipulation. This\nwork introduces TAP (Task-agnostic Active Perception) -- a novel framework that\nleverages reinforcement learning (RL) and transformer-based architectures to\naddress the challenges posed by partially observable environments. TAP\nintegrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified\noptimization objective, jointly training a perception module and\ndecision-making policy. By design, TAP is completely task-agnostic and can, in\nprinciple, generalize to any active perception problem. We evaluate TAP across\ndiverse tasks, including toy examples and realistic applications involving\nhaptic exploration of 3D models from the Tactile MNIST benchmark. Experiments\ndemonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST\nhaptic digit recognition task and a tactile pose estimation task. These\nfindings underscore the potential of TAP as a versatile and generalizable\nframework for advancing active tactile perception in robotics.\n", "link": "http://arxiv.org/abs/2505.06182v1", "date": "2025-05-09", "relevancy": 1.7639, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5932}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Perception%20for%20Tactile%20Sensing%3A%20A%20Task-Agnostic%20Attention-Based%0A%20%20Approach&body=Title%3A%20Active%20Perception%20for%20Tactile%20Sensing%3A%20A%20Task-Agnostic%20Attention-Based%0A%20%20Approach%0AAuthor%3A%20Tim%20Schneider%20and%20Cristiana%20de%20Farias%20and%20Roberto%20Calandra%20and%20Liming%20Chen%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Humans%20make%20extensive%20use%20of%20haptic%20exploration%20to%20map%20and%20identify%20the%0Aproperties%20of%20the%20objects%20that%20we%20touch.%20In%20robotics%2C%20active%20tactile%20perception%0Ahas%20emerged%20as%20an%20important%20research%20domain%20that%20complements%20vision%20for%20tasks%0Asuch%20as%20object%20classification%2C%20shape%20reconstruction%2C%20and%20manipulation.%20This%0Awork%20introduces%20TAP%20%28Task-agnostic%20Active%20Perception%29%20--%20a%20novel%20framework%20that%0Aleverages%20reinforcement%20learning%20%28RL%29%20and%20transformer-based%20architectures%20to%0Aaddress%20the%20challenges%20posed%20by%20partially%20observable%20environments.%20TAP%0Aintegrates%20Soft%20Actor-Critic%20%28SAC%29%20and%20CrossQ%20algorithms%20within%20a%20unified%0Aoptimization%20objective%2C%20jointly%20training%20a%20perception%20module%20and%0Adecision-making%20policy.%20By%20design%2C%20TAP%20is%20completely%20task-agnostic%20and%20can%2C%20in%0Aprinciple%2C%20generalize%20to%20any%20active%20perception%20problem.%20We%20evaluate%20TAP%20across%0Adiverse%20tasks%2C%20including%20toy%20examples%20and%20realistic%20applications%20involving%0Ahaptic%20exploration%20of%203D%20models%20from%20the%20Tactile%20MNIST%20benchmark.%20Experiments%0Ademonstrate%20the%20efficacy%20of%20TAP%2C%20achieving%20high%20accuracies%20on%20the%20Tactile%20MNIST%0Ahaptic%20digit%20recognition%20task%20and%20a%20tactile%20pose%20estimation%20task.%20These%0Afindings%20underscore%20the%20potential%20of%20TAP%20as%20a%20versatile%20and%20generalizable%0Aframework%20for%20advancing%20active%20tactile%20perception%20in%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Perception%2520for%2520Tactile%2520Sensing%253A%2520A%2520Task-Agnostic%2520Attention-Based%250A%2520%2520Approach%26entry.906535625%3DTim%2520Schneider%2520and%2520Cristiana%2520de%2520Farias%2520and%2520Roberto%2520Calandra%2520and%2520Liming%2520Chen%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Humans%2520make%2520extensive%2520use%2520of%2520haptic%2520exploration%2520to%2520map%2520and%2520identify%2520the%250Aproperties%2520of%2520the%2520objects%2520that%2520we%2520touch.%2520In%2520robotics%252C%2520active%2520tactile%2520perception%250Ahas%2520emerged%2520as%2520an%2520important%2520research%2520domain%2520that%2520complements%2520vision%2520for%2520tasks%250Asuch%2520as%2520object%2520classification%252C%2520shape%2520reconstruction%252C%2520and%2520manipulation.%2520This%250Awork%2520introduces%2520TAP%2520%2528Task-agnostic%2520Active%2520Perception%2529%2520--%2520a%2520novel%2520framework%2520that%250Aleverages%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520transformer-based%2520architectures%2520to%250Aaddress%2520the%2520challenges%2520posed%2520by%2520partially%2520observable%2520environments.%2520TAP%250Aintegrates%2520Soft%2520Actor-Critic%2520%2528SAC%2529%2520and%2520CrossQ%2520algorithms%2520within%2520a%2520unified%250Aoptimization%2520objective%252C%2520jointly%2520training%2520a%2520perception%2520module%2520and%250Adecision-making%2520policy.%2520By%2520design%252C%2520TAP%2520is%2520completely%2520task-agnostic%2520and%2520can%252C%2520in%250Aprinciple%252C%2520generalize%2520to%2520any%2520active%2520perception%2520problem.%2520We%2520evaluate%2520TAP%2520across%250Adiverse%2520tasks%252C%2520including%2520toy%2520examples%2520and%2520realistic%2520applications%2520involving%250Ahaptic%2520exploration%2520of%25203D%2520models%2520from%2520the%2520Tactile%2520MNIST%2520benchmark.%2520Experiments%250Ademonstrate%2520the%2520efficacy%2520of%2520TAP%252C%2520achieving%2520high%2520accuracies%2520on%2520the%2520Tactile%2520MNIST%250Ahaptic%2520digit%2520recognition%2520task%2520and%2520a%2520tactile%2520pose%2520estimation%2520task.%2520These%250Afindings%2520underscore%2520the%2520potential%2520of%2520TAP%2520as%2520a%2520versatile%2520and%2520generalizable%250Aframework%2520for%2520advancing%2520active%2520tactile%2520perception%2520in%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Perception%20for%20Tactile%20Sensing%3A%20A%20Task-Agnostic%20Attention-Based%0A%20%20Approach&entry.906535625=Tim%20Schneider%20and%20Cristiana%20de%20Farias%20and%20Roberto%20Calandra%20and%20Liming%20Chen%20and%20Jan%20Peters&entry.1292438233=%20%20Humans%20make%20extensive%20use%20of%20haptic%20exploration%20to%20map%20and%20identify%20the%0Aproperties%20of%20the%20objects%20that%20we%20touch.%20In%20robotics%2C%20active%20tactile%20perception%0Ahas%20emerged%20as%20an%20important%20research%20domain%20that%20complements%20vision%20for%20tasks%0Asuch%20as%20object%20classification%2C%20shape%20reconstruction%2C%20and%20manipulation.%20This%0Awork%20introduces%20TAP%20%28Task-agnostic%20Active%20Perception%29%20--%20a%20novel%20framework%20that%0Aleverages%20reinforcement%20learning%20%28RL%29%20and%20transformer-based%20architectures%20to%0Aaddress%20the%20challenges%20posed%20by%20partially%20observable%20environments.%20TAP%0Aintegrates%20Soft%20Actor-Critic%20%28SAC%29%20and%20CrossQ%20algorithms%20within%20a%20unified%0Aoptimization%20objective%2C%20jointly%20training%20a%20perception%20module%20and%0Adecision-making%20policy.%20By%20design%2C%20TAP%20is%20completely%20task-agnostic%20and%20can%2C%20in%0Aprinciple%2C%20generalize%20to%20any%20active%20perception%20problem.%20We%20evaluate%20TAP%20across%0Adiverse%20tasks%2C%20including%20toy%20examples%20and%20realistic%20applications%20involving%0Ahaptic%20exploration%20of%203D%20models%20from%20the%20Tactile%20MNIST%20benchmark.%20Experiments%0Ademonstrate%20the%20efficacy%20of%20TAP%2C%20achieving%20high%20accuracies%20on%20the%20Tactile%20MNIST%0Ahaptic%20digit%20recognition%20task%20and%20a%20tactile%20pose%20estimation%20task.%20These%0Afindings%20underscore%20the%20potential%20of%20TAP%20as%20a%20versatile%20and%20generalizable%0Aframework%20for%20advancing%20active%20tactile%20perception%20in%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06182v1&entry.124074799=Read"},
{"title": "Scaling Laws For Scalable Oversight", "author": "Joshua Engels and David D. Baek and Subhash Kantamneni and Max Tegmark", "abstract": "  Scalable oversight, the process by which weaker AI systems supervise stronger\nones, has been proposed as a key strategy to control future superintelligent\nsystems. However, it is still unclear how scalable oversight itself scales. To\naddress this gap, we propose a framework that quantifies the probability of\nsuccessful oversight as a function of the capabilities of the overseer and the\nsystem being overseen. Specifically, our framework models oversight as a game\nbetween capability-mismatched players; the players have oversight-specific Elo\nscores that are a piecewise-linear function of their general intelligence, with\ntwo plateaus corresponding to task incompetence and task saturation. We\nvalidate our framework with a modified version of the game Nim and then apply\nit to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each\ngame, we find scaling laws that approximate how domain performance depends on\ngeneral AI system capability. We then build on our findings in a theoretical\nstudy of Nested Scalable Oversight (NSO), a process in which trusted models\noversee untrusted stronger models, which then become the trusted models in the\nnext step. We identify conditions under which NSO succeeds and derive\nnumerically (and in some cases analytically) the optimal number of oversight\nlevels to maximize the probability of oversight success. We also apply our\ntheory to our four oversight games, where we find that NSO success rates at a\ngeneral Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for\nBackdoor Code, and 9.4% for Wargames; these rates decline further when\noverseeing stronger systems.\n", "link": "http://arxiv.org/abs/2504.18530v2", "date": "2025-05-09", "relevancy": 1.7574, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.448}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Laws%20For%20Scalable%20Oversight&body=Title%3A%20Scaling%20Laws%20For%20Scalable%20Oversight%0AAuthor%3A%20Joshua%20Engels%20and%20David%20D.%20Baek%20and%20Subhash%20Kantamneni%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Scalable%20oversight%2C%20the%20process%20by%20which%20weaker%20AI%20systems%20supervise%20stronger%0Aones%2C%20has%20been%20proposed%20as%20a%20key%20strategy%20to%20control%20future%20superintelligent%0Asystems.%20However%2C%20it%20is%20still%20unclear%20how%20scalable%20oversight%20itself%20scales.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20a%20framework%20that%20quantifies%20the%20probability%20of%0Asuccessful%20oversight%20as%20a%20function%20of%20the%20capabilities%20of%20the%20overseer%20and%20the%0Asystem%20being%20overseen.%20Specifically%2C%20our%20framework%20models%20oversight%20as%20a%20game%0Abetween%20capability-mismatched%20players%3B%20the%20players%20have%20oversight-specific%20Elo%0Ascores%20that%20are%20a%20piecewise-linear%20function%20of%20their%20general%20intelligence%2C%20with%0Atwo%20plateaus%20corresponding%20to%20task%20incompetence%20and%20task%20saturation.%20We%0Avalidate%20our%20framework%20with%20a%20modified%20version%20of%20the%20game%20Nim%20and%20then%20apply%0Ait%20to%20four%20oversight%20games%3A%20Mafia%2C%20Debate%2C%20Backdoor%20Code%20and%20Wargames.%20For%20each%0Agame%2C%20we%20find%20scaling%20laws%20that%20approximate%20how%20domain%20performance%20depends%20on%0Ageneral%20AI%20system%20capability.%20We%20then%20build%20on%20our%20findings%20in%20a%20theoretical%0Astudy%20of%20Nested%20Scalable%20Oversight%20%28NSO%29%2C%20a%20process%20in%20which%20trusted%20models%0Aoversee%20untrusted%20stronger%20models%2C%20which%20then%20become%20the%20trusted%20models%20in%20the%0Anext%20step.%20We%20identify%20conditions%20under%20which%20NSO%20succeeds%20and%20derive%0Anumerically%20%28and%20in%20some%20cases%20analytically%29%20the%20optimal%20number%20of%20oversight%0Alevels%20to%20maximize%20the%20probability%20of%20oversight%20success.%20We%20also%20apply%20our%0Atheory%20to%20our%20four%20oversight%20games%2C%20where%20we%20find%20that%20NSO%20success%20rates%20at%20a%0Ageneral%20Elo%20gap%20of%20400%20are%2013.5%25%20for%20Mafia%2C%2051.7%25%20for%20Debate%2C%2010.0%25%20for%0ABackdoor%20Code%2C%20and%209.4%25%20for%20Wargames%3B%20these%20rates%20decline%20further%20when%0Aoverseeing%20stronger%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Laws%2520For%2520Scalable%2520Oversight%26entry.906535625%3DJoshua%2520Engels%2520and%2520David%2520D.%2520Baek%2520and%2520Subhash%2520Kantamneni%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Scalable%2520oversight%252C%2520the%2520process%2520by%2520which%2520weaker%2520AI%2520systems%2520supervise%2520stronger%250Aones%252C%2520has%2520been%2520proposed%2520as%2520a%2520key%2520strategy%2520to%2520control%2520future%2520superintelligent%250Asystems.%2520However%252C%2520it%2520is%2520still%2520unclear%2520how%2520scalable%2520oversight%2520itself%2520scales.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520a%2520framework%2520that%2520quantifies%2520the%2520probability%2520of%250Asuccessful%2520oversight%2520as%2520a%2520function%2520of%2520the%2520capabilities%2520of%2520the%2520overseer%2520and%2520the%250Asystem%2520being%2520overseen.%2520Specifically%252C%2520our%2520framework%2520models%2520oversight%2520as%2520a%2520game%250Abetween%2520capability-mismatched%2520players%253B%2520the%2520players%2520have%2520oversight-specific%2520Elo%250Ascores%2520that%2520are%2520a%2520piecewise-linear%2520function%2520of%2520their%2520general%2520intelligence%252C%2520with%250Atwo%2520plateaus%2520corresponding%2520to%2520task%2520incompetence%2520and%2520task%2520saturation.%2520We%250Avalidate%2520our%2520framework%2520with%2520a%2520modified%2520version%2520of%2520the%2520game%2520Nim%2520and%2520then%2520apply%250Ait%2520to%2520four%2520oversight%2520games%253A%2520Mafia%252C%2520Debate%252C%2520Backdoor%2520Code%2520and%2520Wargames.%2520For%2520each%250Agame%252C%2520we%2520find%2520scaling%2520laws%2520that%2520approximate%2520how%2520domain%2520performance%2520depends%2520on%250Ageneral%2520AI%2520system%2520capability.%2520We%2520then%2520build%2520on%2520our%2520findings%2520in%2520a%2520theoretical%250Astudy%2520of%2520Nested%2520Scalable%2520Oversight%2520%2528NSO%2529%252C%2520a%2520process%2520in%2520which%2520trusted%2520models%250Aoversee%2520untrusted%2520stronger%2520models%252C%2520which%2520then%2520become%2520the%2520trusted%2520models%2520in%2520the%250Anext%2520step.%2520We%2520identify%2520conditions%2520under%2520which%2520NSO%2520succeeds%2520and%2520derive%250Anumerically%2520%2528and%2520in%2520some%2520cases%2520analytically%2529%2520the%2520optimal%2520number%2520of%2520oversight%250Alevels%2520to%2520maximize%2520the%2520probability%2520of%2520oversight%2520success.%2520We%2520also%2520apply%2520our%250Atheory%2520to%2520our%2520four%2520oversight%2520games%252C%2520where%2520we%2520find%2520that%2520NSO%2520success%2520rates%2520at%2520a%250Ageneral%2520Elo%2520gap%2520of%2520400%2520are%252013.5%2525%2520for%2520Mafia%252C%252051.7%2525%2520for%2520Debate%252C%252010.0%2525%2520for%250ABackdoor%2520Code%252C%2520and%25209.4%2525%2520for%2520Wargames%253B%2520these%2520rates%2520decline%2520further%2520when%250Aoverseeing%2520stronger%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Laws%20For%20Scalable%20Oversight&entry.906535625=Joshua%20Engels%20and%20David%20D.%20Baek%20and%20Subhash%20Kantamneni%20and%20Max%20Tegmark&entry.1292438233=%20%20Scalable%20oversight%2C%20the%20process%20by%20which%20weaker%20AI%20systems%20supervise%20stronger%0Aones%2C%20has%20been%20proposed%20as%20a%20key%20strategy%20to%20control%20future%20superintelligent%0Asystems.%20However%2C%20it%20is%20still%20unclear%20how%20scalable%20oversight%20itself%20scales.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20a%20framework%20that%20quantifies%20the%20probability%20of%0Asuccessful%20oversight%20as%20a%20function%20of%20the%20capabilities%20of%20the%20overseer%20and%20the%0Asystem%20being%20overseen.%20Specifically%2C%20our%20framework%20models%20oversight%20as%20a%20game%0Abetween%20capability-mismatched%20players%3B%20the%20players%20have%20oversight-specific%20Elo%0Ascores%20that%20are%20a%20piecewise-linear%20function%20of%20their%20general%20intelligence%2C%20with%0Atwo%20plateaus%20corresponding%20to%20task%20incompetence%20and%20task%20saturation.%20We%0Avalidate%20our%20framework%20with%20a%20modified%20version%20of%20the%20game%20Nim%20and%20then%20apply%0Ait%20to%20four%20oversight%20games%3A%20Mafia%2C%20Debate%2C%20Backdoor%20Code%20and%20Wargames.%20For%20each%0Agame%2C%20we%20find%20scaling%20laws%20that%20approximate%20how%20domain%20performance%20depends%20on%0Ageneral%20AI%20system%20capability.%20We%20then%20build%20on%20our%20findings%20in%20a%20theoretical%0Astudy%20of%20Nested%20Scalable%20Oversight%20%28NSO%29%2C%20a%20process%20in%20which%20trusted%20models%0Aoversee%20untrusted%20stronger%20models%2C%20which%20then%20become%20the%20trusted%20models%20in%20the%0Anext%20step.%20We%20identify%20conditions%20under%20which%20NSO%20succeeds%20and%20derive%0Anumerically%20%28and%20in%20some%20cases%20analytically%29%20the%20optimal%20number%20of%20oversight%0Alevels%20to%20maximize%20the%20probability%20of%20oversight%20success.%20We%20also%20apply%20our%0Atheory%20to%20our%20four%20oversight%20games%2C%20where%20we%20find%20that%20NSO%20success%20rates%20at%20a%0Ageneral%20Elo%20gap%20of%20400%20are%2013.5%25%20for%20Mafia%2C%2051.7%25%20for%20Debate%2C%2010.0%25%20for%0ABackdoor%20Code%2C%20and%209.4%25%20for%20Wargames%3B%20these%20rates%20decline%20further%20when%0Aoverseeing%20stronger%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18530v2&entry.124074799=Read"},
{"title": "PYRREGULAR: A Unified Framework for Irregular Time Series, with\n  Classification Benchmarks", "author": "Francesco Spinnato and Cristiano Landi", "abstract": "  Irregular temporal data, characterized by varying recording frequencies,\ndiffering observation durations, and missing values, presents significant\nchallenges across fields like mobility, healthcare, and environmental science.\nExisting research communities often overlook or address these challenges in\nisolation, leading to fragmented tools and methods. To bridge this gap, we\nintroduce a unified framework, and the first standardized dataset repository\nfor irregular time series classification, built on a common array format to\nenhance interoperability. This repository comprises 34 datasets on which we\nbenchmark 12 classifier models from diverse domains and communities. This work\naims to centralize research efforts and enable a more robust evaluation of\nirregular temporal data analysis methods.\n", "link": "http://arxiv.org/abs/2505.06047v1", "date": "2025-05-09", "relevancy": 1.7452, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4282}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PYRREGULAR%3A%20A%20Unified%20Framework%20for%20Irregular%20Time%20Series%2C%20with%0A%20%20Classification%20Benchmarks&body=Title%3A%20PYRREGULAR%3A%20A%20Unified%20Framework%20for%20Irregular%20Time%20Series%2C%20with%0A%20%20Classification%20Benchmarks%0AAuthor%3A%20Francesco%20Spinnato%20and%20Cristiano%20Landi%0AAbstract%3A%20%20%20Irregular%20temporal%20data%2C%20characterized%20by%20varying%20recording%20frequencies%2C%0Adiffering%20observation%20durations%2C%20and%20missing%20values%2C%20presents%20significant%0Achallenges%20across%20fields%20like%20mobility%2C%20healthcare%2C%20and%20environmental%20science.%0AExisting%20research%20communities%20often%20overlook%20or%20address%20these%20challenges%20in%0Aisolation%2C%20leading%20to%20fragmented%20tools%20and%20methods.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20a%20unified%20framework%2C%20and%20the%20first%20standardized%20dataset%20repository%0Afor%20irregular%20time%20series%20classification%2C%20built%20on%20a%20common%20array%20format%20to%0Aenhance%20interoperability.%20This%20repository%20comprises%2034%20datasets%20on%20which%20we%0Abenchmark%2012%20classifier%20models%20from%20diverse%20domains%20and%20communities.%20This%20work%0Aaims%20to%20centralize%20research%20efforts%20and%20enable%20a%20more%20robust%20evaluation%20of%0Airregular%20temporal%20data%20analysis%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPYRREGULAR%253A%2520A%2520Unified%2520Framework%2520for%2520Irregular%2520Time%2520Series%252C%2520with%250A%2520%2520Classification%2520Benchmarks%26entry.906535625%3DFrancesco%2520Spinnato%2520and%2520Cristiano%2520Landi%26entry.1292438233%3D%2520%2520Irregular%2520temporal%2520data%252C%2520characterized%2520by%2520varying%2520recording%2520frequencies%252C%250Adiffering%2520observation%2520durations%252C%2520and%2520missing%2520values%252C%2520presents%2520significant%250Achallenges%2520across%2520fields%2520like%2520mobility%252C%2520healthcare%252C%2520and%2520environmental%2520science.%250AExisting%2520research%2520communities%2520often%2520overlook%2520or%2520address%2520these%2520challenges%2520in%250Aisolation%252C%2520leading%2520to%2520fragmented%2520tools%2520and%2520methods.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520unified%2520framework%252C%2520and%2520the%2520first%2520standardized%2520dataset%2520repository%250Afor%2520irregular%2520time%2520series%2520classification%252C%2520built%2520on%2520a%2520common%2520array%2520format%2520to%250Aenhance%2520interoperability.%2520This%2520repository%2520comprises%252034%2520datasets%2520on%2520which%2520we%250Abenchmark%252012%2520classifier%2520models%2520from%2520diverse%2520domains%2520and%2520communities.%2520This%2520work%250Aaims%2520to%2520centralize%2520research%2520efforts%2520and%2520enable%2520a%2520more%2520robust%2520evaluation%2520of%250Airregular%2520temporal%2520data%2520analysis%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PYRREGULAR%3A%20A%20Unified%20Framework%20for%20Irregular%20Time%20Series%2C%20with%0A%20%20Classification%20Benchmarks&entry.906535625=Francesco%20Spinnato%20and%20Cristiano%20Landi&entry.1292438233=%20%20Irregular%20temporal%20data%2C%20characterized%20by%20varying%20recording%20frequencies%2C%0Adiffering%20observation%20durations%2C%20and%20missing%20values%2C%20presents%20significant%0Achallenges%20across%20fields%20like%20mobility%2C%20healthcare%2C%20and%20environmental%20science.%0AExisting%20research%20communities%20often%20overlook%20or%20address%20these%20challenges%20in%0Aisolation%2C%20leading%20to%20fragmented%20tools%20and%20methods.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20a%20unified%20framework%2C%20and%20the%20first%20standardized%20dataset%20repository%0Afor%20irregular%20time%20series%20classification%2C%20built%20on%20a%20common%20array%20format%20to%0Aenhance%20interoperability.%20This%20repository%20comprises%2034%20datasets%20on%20which%20we%0Abenchmark%2012%20classifier%20models%20from%20diverse%20domains%20and%20communities.%20This%20work%0Aaims%20to%20centralize%20research%20efforts%20and%20enable%20a%20more%20robust%20evaluation%20of%0Airregular%20temporal%20data%20analysis%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06047v1&entry.124074799=Read"},
{"title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and\n  Segmentation", "author": "Kunpeng Qiu and Zhiqiang Gao and Zhiying Zhou and Mingjie Sun and Yongxin Guo", "abstract": "  Deep learning has revolutionized medical image segmentation, yet its full\npotential remains constrained by the paucity of annotated datasets. While\ndiffusion models have emerged as a promising approach for generating synthetic\nimage-mask pairs to augment these datasets, they paradoxically suffer from the\nsame data scarcity challenges they aim to mitigate. Traditional mask-only\nmodels frequently yield low-fidelity images due to their inability to\nadequately capture morphological intricacies, which can critically compromise\nthe robustness and reliability of segmentation models. To alleviate this\nlimitation, we introduce Siamese-Diffusion, a novel dual-component model\ncomprising Mask-Diffusion and Image-Diffusion. During training, a Noise\nConsistency Loss is introduced between these components to enhance the\nmorphological fidelity of Mask-Diffusion in the parameter space. During\nsampling, only Mask-Diffusion is used, ensuring diversity and scalability.\nComprehensive experiments demonstrate the superiority of our method.\nSiamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,\nwhile UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at\nGitHub.\n", "link": "http://arxiv.org/abs/2505.06068v1", "date": "2025-05-09", "relevancy": 1.7295, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6298}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5687}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise-Consistent%20Siamese-Diffusion%20for%20Medical%20Image%20Synthesis%20and%0A%20%20Segmentation&body=Title%3A%20Noise-Consistent%20Siamese-Diffusion%20for%20Medical%20Image%20Synthesis%20and%0A%20%20Segmentation%0AAuthor%3A%20Kunpeng%20Qiu%20and%20Zhiqiang%20Gao%20and%20Zhiying%20Zhou%20and%20Mingjie%20Sun%20and%20Yongxin%20Guo%0AAbstract%3A%20%20%20Deep%20learning%20has%20revolutionized%20medical%20image%20segmentation%2C%20yet%20its%20full%0Apotential%20remains%20constrained%20by%20the%20paucity%20of%20annotated%20datasets.%20While%0Adiffusion%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20generating%20synthetic%0Aimage-mask%20pairs%20to%20augment%20these%20datasets%2C%20they%20paradoxically%20suffer%20from%20the%0Asame%20data%20scarcity%20challenges%20they%20aim%20to%20mitigate.%20Traditional%20mask-only%0Amodels%20frequently%20yield%20low-fidelity%20images%20due%20to%20their%20inability%20to%0Aadequately%20capture%20morphological%20intricacies%2C%20which%20can%20critically%20compromise%0Athe%20robustness%20and%20reliability%20of%20segmentation%20models.%20To%20alleviate%20this%0Alimitation%2C%20we%20introduce%20Siamese-Diffusion%2C%20a%20novel%20dual-component%20model%0Acomprising%20Mask-Diffusion%20and%20Image-Diffusion.%20During%20training%2C%20a%20Noise%0AConsistency%20Loss%20is%20introduced%20between%20these%20components%20to%20enhance%20the%0Amorphological%20fidelity%20of%20Mask-Diffusion%20in%20the%20parameter%20space.%20During%0Asampling%2C%20only%20Mask-Diffusion%20is%20used%2C%20ensuring%20diversity%20and%20scalability.%0AComprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method.%0ASiamese-Diffusion%20boosts%20SANet%27s%20mDice%20and%20mIoU%20by%203.6%25%20and%204.4%25%20on%20the%20Polyps%2C%0Awhile%20UNet%20improves%20by%201.52%25%20and%201.64%25%20on%20the%20ISIC2018.%20Code%20is%20available%20at%0AGitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise-Consistent%2520Siamese-Diffusion%2520for%2520Medical%2520Image%2520Synthesis%2520and%250A%2520%2520Segmentation%26entry.906535625%3DKunpeng%2520Qiu%2520and%2520Zhiqiang%2520Gao%2520and%2520Zhiying%2520Zhou%2520and%2520Mingjie%2520Sun%2520and%2520Yongxin%2520Guo%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520revolutionized%2520medical%2520image%2520segmentation%252C%2520yet%2520its%2520full%250Apotential%2520remains%2520constrained%2520by%2520the%2520paucity%2520of%2520annotated%2520datasets.%2520While%250Adiffusion%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520generating%2520synthetic%250Aimage-mask%2520pairs%2520to%2520augment%2520these%2520datasets%252C%2520they%2520paradoxically%2520suffer%2520from%2520the%250Asame%2520data%2520scarcity%2520challenges%2520they%2520aim%2520to%2520mitigate.%2520Traditional%2520mask-only%250Amodels%2520frequently%2520yield%2520low-fidelity%2520images%2520due%2520to%2520their%2520inability%2520to%250Aadequately%2520capture%2520morphological%2520intricacies%252C%2520which%2520can%2520critically%2520compromise%250Athe%2520robustness%2520and%2520reliability%2520of%2520segmentation%2520models.%2520To%2520alleviate%2520this%250Alimitation%252C%2520we%2520introduce%2520Siamese-Diffusion%252C%2520a%2520novel%2520dual-component%2520model%250Acomprising%2520Mask-Diffusion%2520and%2520Image-Diffusion.%2520During%2520training%252C%2520a%2520Noise%250AConsistency%2520Loss%2520is%2520introduced%2520between%2520these%2520components%2520to%2520enhance%2520the%250Amorphological%2520fidelity%2520of%2520Mask-Diffusion%2520in%2520the%2520parameter%2520space.%2520During%250Asampling%252C%2520only%2520Mask-Diffusion%2520is%2520used%252C%2520ensuring%2520diversity%2520and%2520scalability.%250AComprehensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method.%250ASiamese-Diffusion%2520boosts%2520SANet%2527s%2520mDice%2520and%2520mIoU%2520by%25203.6%2525%2520and%25204.4%2525%2520on%2520the%2520Polyps%252C%250Awhile%2520UNet%2520improves%2520by%25201.52%2525%2520and%25201.64%2525%2520on%2520the%2520ISIC2018.%2520Code%2520is%2520available%2520at%250AGitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-Consistent%20Siamese-Diffusion%20for%20Medical%20Image%20Synthesis%20and%0A%20%20Segmentation&entry.906535625=Kunpeng%20Qiu%20and%20Zhiqiang%20Gao%20and%20Zhiying%20Zhou%20and%20Mingjie%20Sun%20and%20Yongxin%20Guo&entry.1292438233=%20%20Deep%20learning%20has%20revolutionized%20medical%20image%20segmentation%2C%20yet%20its%20full%0Apotential%20remains%20constrained%20by%20the%20paucity%20of%20annotated%20datasets.%20While%0Adiffusion%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20generating%20synthetic%0Aimage-mask%20pairs%20to%20augment%20these%20datasets%2C%20they%20paradoxically%20suffer%20from%20the%0Asame%20data%20scarcity%20challenges%20they%20aim%20to%20mitigate.%20Traditional%20mask-only%0Amodels%20frequently%20yield%20low-fidelity%20images%20due%20to%20their%20inability%20to%0Aadequately%20capture%20morphological%20intricacies%2C%20which%20can%20critically%20compromise%0Athe%20robustness%20and%20reliability%20of%20segmentation%20models.%20To%20alleviate%20this%0Alimitation%2C%20we%20introduce%20Siamese-Diffusion%2C%20a%20novel%20dual-component%20model%0Acomprising%20Mask-Diffusion%20and%20Image-Diffusion.%20During%20training%2C%20a%20Noise%0AConsistency%20Loss%20is%20introduced%20between%20these%20components%20to%20enhance%20the%0Amorphological%20fidelity%20of%20Mask-Diffusion%20in%20the%20parameter%20space.%20During%0Asampling%2C%20only%20Mask-Diffusion%20is%20used%2C%20ensuring%20diversity%20and%20scalability.%0AComprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method.%0ASiamese-Diffusion%20boosts%20SANet%27s%20mDice%20and%20mIoU%20by%203.6%25%20and%204.4%25%20on%20the%20Polyps%2C%0Awhile%20UNet%20improves%20by%201.52%25%20and%201.64%25%20on%20the%20ISIC2018.%20Code%20is%20available%20at%0AGitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06068v1&entry.124074799=Read"},
{"title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails", "author": "Kwan-Yee Lin and Stella X. Yu", "abstract": "  Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development.\n", "link": "http://arxiv.org/abs/2505.06218v1", "date": "2025-05-09", "relevancy": 1.719, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5765}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Humanoids%20Hike%21%20Integrative%20Skill%20Development%20on%20Complex%20Trails&body=Title%3A%20Let%20Humanoids%20Hike%21%20Integrative%20Skill%20Development%20on%20Complex%20Trails%0AAuthor%3A%20Kwan-Yee%20Lin%20and%20Stella%20X.%20Yu%0AAbstract%3A%20%20%20Hiking%20on%20complex%20trails%20demands%20balance%2C%20agility%2C%20and%20adaptive%0Adecision-making%20over%20unpredictable%20terrain.%20Current%20humanoid%20research%20remains%0Afragmented%20and%20inadequate%20for%20hiking%3A%20locomotion%20focuses%20on%20motor%20skills%0Awithout%20long-term%20goals%20or%20situational%20awareness%2C%20while%20semantic%20navigation%0Aoverlooks%20real-world%20embodiment%20and%20local%20terrain%20variability.%20We%20propose%0Atraining%20humanoids%20to%20hike%20on%20complex%20trails%2C%20driving%20integrative%20skill%0Adevelopment%20across%20visual%20perception%2C%20decision%20making%2C%20and%20motor%20execution.%20We%0Adevelop%20a%20learning%20framework%2C%20LEGO-H%2C%20that%20enables%20a%20vision-equipped%20humanoid%0Arobot%20to%20hike%20complex%20trails%20autonomously.%20We%20introduce%20two%20technical%0Ainnovations%3A%201%29%20A%20temporal%20vision%20transformer%20variant%20-%20tailored%20into%0AHierarchical%20Reinforcement%20Learning%20framework%20-%20anticipates%20future%20local%20goals%0Ato%20guide%20movement%2C%20seamlessly%20integrating%20locomotion%20with%20goal-directed%0Anavigation.%202%29%20Latent%20representations%20of%20joint%20movement%20patterns%2C%20combined%20with%0Ahierarchical%20metric%20learning%20-%20enhance%20Privileged%20Learning%20scheme%20-%20enable%0Asmooth%20policy%20transfer%20from%20privileged%20training%20to%20onboard%20execution.%20These%0Acomponents%20allow%20LEGO-H%20to%20handle%20diverse%20physical%20and%20environmental%20challenges%0Awithout%20relying%20on%20predefined%20motion%20patterns.%20Experiments%20across%20varied%0Asimulated%20trails%20and%20robot%20morphologies%20highlight%20LEGO-H%27s%20versatility%20and%0Arobustness%2C%20positioning%20hiking%20as%20a%20compelling%20testbed%20for%20embodied%20autonomy%0Aand%20LEGO-H%20as%20a%20baseline%20for%20future%20humanoid%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Humanoids%2520Hike%2521%2520Integrative%2520Skill%2520Development%2520on%2520Complex%2520Trails%26entry.906535625%3DKwan-Yee%2520Lin%2520and%2520Stella%2520X.%2520Yu%26entry.1292438233%3D%2520%2520Hiking%2520on%2520complex%2520trails%2520demands%2520balance%252C%2520agility%252C%2520and%2520adaptive%250Adecision-making%2520over%2520unpredictable%2520terrain.%2520Current%2520humanoid%2520research%2520remains%250Afragmented%2520and%2520inadequate%2520for%2520hiking%253A%2520locomotion%2520focuses%2520on%2520motor%2520skills%250Awithout%2520long-term%2520goals%2520or%2520situational%2520awareness%252C%2520while%2520semantic%2520navigation%250Aoverlooks%2520real-world%2520embodiment%2520and%2520local%2520terrain%2520variability.%2520We%2520propose%250Atraining%2520humanoids%2520to%2520hike%2520on%2520complex%2520trails%252C%2520driving%2520integrative%2520skill%250Adevelopment%2520across%2520visual%2520perception%252C%2520decision%2520making%252C%2520and%2520motor%2520execution.%2520We%250Adevelop%2520a%2520learning%2520framework%252C%2520LEGO-H%252C%2520that%2520enables%2520a%2520vision-equipped%2520humanoid%250Arobot%2520to%2520hike%2520complex%2520trails%2520autonomously.%2520We%2520introduce%2520two%2520technical%250Ainnovations%253A%25201%2529%2520A%2520temporal%2520vision%2520transformer%2520variant%2520-%2520tailored%2520into%250AHierarchical%2520Reinforcement%2520Learning%2520framework%2520-%2520anticipates%2520future%2520local%2520goals%250Ato%2520guide%2520movement%252C%2520seamlessly%2520integrating%2520locomotion%2520with%2520goal-directed%250Anavigation.%25202%2529%2520Latent%2520representations%2520of%2520joint%2520movement%2520patterns%252C%2520combined%2520with%250Ahierarchical%2520metric%2520learning%2520-%2520enhance%2520Privileged%2520Learning%2520scheme%2520-%2520enable%250Asmooth%2520policy%2520transfer%2520from%2520privileged%2520training%2520to%2520onboard%2520execution.%2520These%250Acomponents%2520allow%2520LEGO-H%2520to%2520handle%2520diverse%2520physical%2520and%2520environmental%2520challenges%250Awithout%2520relying%2520on%2520predefined%2520motion%2520patterns.%2520Experiments%2520across%2520varied%250Asimulated%2520trails%2520and%2520robot%2520morphologies%2520highlight%2520LEGO-H%2527s%2520versatility%2520and%250Arobustness%252C%2520positioning%2520hiking%2520as%2520a%2520compelling%2520testbed%2520for%2520embodied%2520autonomy%250Aand%2520LEGO-H%2520as%2520a%2520baseline%2520for%2520future%2520humanoid%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Humanoids%20Hike%21%20Integrative%20Skill%20Development%20on%20Complex%20Trails&entry.906535625=Kwan-Yee%20Lin%20and%20Stella%20X.%20Yu&entry.1292438233=%20%20Hiking%20on%20complex%20trails%20demands%20balance%2C%20agility%2C%20and%20adaptive%0Adecision-making%20over%20unpredictable%20terrain.%20Current%20humanoid%20research%20remains%0Afragmented%20and%20inadequate%20for%20hiking%3A%20locomotion%20focuses%20on%20motor%20skills%0Awithout%20long-term%20goals%20or%20situational%20awareness%2C%20while%20semantic%20navigation%0Aoverlooks%20real-world%20embodiment%20and%20local%20terrain%20variability.%20We%20propose%0Atraining%20humanoids%20to%20hike%20on%20complex%20trails%2C%20driving%20integrative%20skill%0Adevelopment%20across%20visual%20perception%2C%20decision%20making%2C%20and%20motor%20execution.%20We%0Adevelop%20a%20learning%20framework%2C%20LEGO-H%2C%20that%20enables%20a%20vision-equipped%20humanoid%0Arobot%20to%20hike%20complex%20trails%20autonomously.%20We%20introduce%20two%20technical%0Ainnovations%3A%201%29%20A%20temporal%20vision%20transformer%20variant%20-%20tailored%20into%0AHierarchical%20Reinforcement%20Learning%20framework%20-%20anticipates%20future%20local%20goals%0Ato%20guide%20movement%2C%20seamlessly%20integrating%20locomotion%20with%20goal-directed%0Anavigation.%202%29%20Latent%20representations%20of%20joint%20movement%20patterns%2C%20combined%20with%0Ahierarchical%20metric%20learning%20-%20enhance%20Privileged%20Learning%20scheme%20-%20enable%0Asmooth%20policy%20transfer%20from%20privileged%20training%20to%20onboard%20execution.%20These%0Acomponents%20allow%20LEGO-H%20to%20handle%20diverse%20physical%20and%20environmental%20challenges%0Awithout%20relying%20on%20predefined%20motion%20patterns.%20Experiments%20across%20varied%0Asimulated%20trails%20and%20robot%20morphologies%20highlight%20LEGO-H%27s%20versatility%20and%0Arobustness%2C%20positioning%20hiking%20as%20a%20compelling%20testbed%20for%20embodied%20autonomy%0Aand%20LEGO-H%20as%20a%20baseline%20for%20future%20humanoid%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06218v1&entry.124074799=Read"},
{"title": "GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient\n  Optimization in Dynamic Environments", "author": "Stavros Orfanoudakis and Nanda Kishor Panda and Peter Palensky and Pedro P. Vergara", "abstract": "  Reinforcement Learning (RL) methods used for solving real-world optimization\nproblems often involve dynamic state-action spaces, larger scale, and sparse\nrewards, leading to significant challenges in convergence, scalability, and\nefficient exploration of the solution space. This study introduces GNN-DT, a\nnovel Decision Transformer (DT) architecture that integrates Graph Neural\nNetwork (GNN) embedders with a novel residual connection between input and\noutput tokens crucial for handling dynamic environments. By learning from\npreviously collected trajectories, GNN-DT tackles the sparse rewards\nlimitations of online RL algorithms and delivers high-quality solutions in\nreal-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging\noptimization problem and prove that its performance is superior and requires\nsignificantly fewer training trajectories, thus improving sample efficiency\ncompared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits\nrobust generalization to unseen environments and larger action spaces,\naddressing a critical gap in prior offline and online RL approaches.\n", "link": "http://arxiv.org/abs/2502.01778v2", "date": "2025-05-09", "relevancy": 1.7144, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6112}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-DT%3A%20Graph%20Neural%20Network%20Enhanced%20Decision%20Transformer%20for%20Efficient%0A%20%20Optimization%20in%20Dynamic%20Environments&body=Title%3A%20GNN-DT%3A%20Graph%20Neural%20Network%20Enhanced%20Decision%20Transformer%20for%20Efficient%0A%20%20Optimization%20in%20Dynamic%20Environments%0AAuthor%3A%20Stavros%20Orfanoudakis%20and%20Nanda%20Kishor%20Panda%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20methods%20used%20for%20solving%20real-world%20optimization%0Aproblems%20often%20involve%20dynamic%20state-action%20spaces%2C%20larger%20scale%2C%20and%20sparse%0Arewards%2C%20leading%20to%20significant%20challenges%20in%20convergence%2C%20scalability%2C%20and%0Aefficient%20exploration%20of%20the%20solution%20space.%20This%20study%20introduces%20GNN-DT%2C%20a%0Anovel%20Decision%20Transformer%20%28DT%29%20architecture%20that%20integrates%20Graph%20Neural%0ANetwork%20%28GNN%29%20embedders%20with%20a%20novel%20residual%20connection%20between%20input%20and%0Aoutput%20tokens%20crucial%20for%20handling%20dynamic%20environments.%20By%20learning%20from%0Apreviously%20collected%20trajectories%2C%20GNN-DT%20tackles%20the%20sparse%20rewards%0Alimitations%20of%20online%20RL%20algorithms%20and%20delivers%20high-quality%20solutions%20in%0Areal-time.%20We%20evaluate%20GNN-DT%20on%20the%20complex%20electric%20vehicle%20%28EV%29%20charging%0Aoptimization%20problem%20and%20prove%20that%20its%20performance%20is%20superior%20and%20requires%0Asignificantly%20fewer%20training%20trajectories%2C%20thus%20improving%20sample%20efficiency%0Acompared%20to%20existing%20DT%20and%20offline%20RL%20baselines.%20Furthermore%2C%20GNN-DT%20exhibits%0Arobust%20generalization%20to%20unseen%20environments%20and%20larger%20action%20spaces%2C%0Aaddressing%20a%20critical%20gap%20in%20prior%20offline%20and%20online%20RL%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-DT%253A%2520Graph%2520Neural%2520Network%2520Enhanced%2520Decision%2520Transformer%2520for%2520Efficient%250A%2520%2520Optimization%2520in%2520Dynamic%2520Environments%26entry.906535625%3DStavros%2520Orfanoudakis%2520and%2520Nanda%2520Kishor%2520Panda%2520and%2520Peter%2520Palensky%2520and%2520Pedro%2520P.%2520Vergara%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520methods%2520used%2520for%2520solving%2520real-world%2520optimization%250Aproblems%2520often%2520involve%2520dynamic%2520state-action%2520spaces%252C%2520larger%2520scale%252C%2520and%2520sparse%250Arewards%252C%2520leading%2520to%2520significant%2520challenges%2520in%2520convergence%252C%2520scalability%252C%2520and%250Aefficient%2520exploration%2520of%2520the%2520solution%2520space.%2520This%2520study%2520introduces%2520GNN-DT%252C%2520a%250Anovel%2520Decision%2520Transformer%2520%2528DT%2529%2520architecture%2520that%2520integrates%2520Graph%2520Neural%250ANetwork%2520%2528GNN%2529%2520embedders%2520with%2520a%2520novel%2520residual%2520connection%2520between%2520input%2520and%250Aoutput%2520tokens%2520crucial%2520for%2520handling%2520dynamic%2520environments.%2520By%2520learning%2520from%250Apreviously%2520collected%2520trajectories%252C%2520GNN-DT%2520tackles%2520the%2520sparse%2520rewards%250Alimitations%2520of%2520online%2520RL%2520algorithms%2520and%2520delivers%2520high-quality%2520solutions%2520in%250Areal-time.%2520We%2520evaluate%2520GNN-DT%2520on%2520the%2520complex%2520electric%2520vehicle%2520%2528EV%2529%2520charging%250Aoptimization%2520problem%2520and%2520prove%2520that%2520its%2520performance%2520is%2520superior%2520and%2520requires%250Asignificantly%2520fewer%2520training%2520trajectories%252C%2520thus%2520improving%2520sample%2520efficiency%250Acompared%2520to%2520existing%2520DT%2520and%2520offline%2520RL%2520baselines.%2520Furthermore%252C%2520GNN-DT%2520exhibits%250Arobust%2520generalization%2520to%2520unseen%2520environments%2520and%2520larger%2520action%2520spaces%252C%250Aaddressing%2520a%2520critical%2520gap%2520in%2520prior%2520offline%2520and%2520online%2520RL%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-DT%3A%20Graph%20Neural%20Network%20Enhanced%20Decision%20Transformer%20for%20Efficient%0A%20%20Optimization%20in%20Dynamic%20Environments&entry.906535625=Stavros%20Orfanoudakis%20and%20Nanda%20Kishor%20Panda%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20methods%20used%20for%20solving%20real-world%20optimization%0Aproblems%20often%20involve%20dynamic%20state-action%20spaces%2C%20larger%20scale%2C%20and%20sparse%0Arewards%2C%20leading%20to%20significant%20challenges%20in%20convergence%2C%20scalability%2C%20and%0Aefficient%20exploration%20of%20the%20solution%20space.%20This%20study%20introduces%20GNN-DT%2C%20a%0Anovel%20Decision%20Transformer%20%28DT%29%20architecture%20that%20integrates%20Graph%20Neural%0ANetwork%20%28GNN%29%20embedders%20with%20a%20novel%20residual%20connection%20between%20input%20and%0Aoutput%20tokens%20crucial%20for%20handling%20dynamic%20environments.%20By%20learning%20from%0Apreviously%20collected%20trajectories%2C%20GNN-DT%20tackles%20the%20sparse%20rewards%0Alimitations%20of%20online%20RL%20algorithms%20and%20delivers%20high-quality%20solutions%20in%0Areal-time.%20We%20evaluate%20GNN-DT%20on%20the%20complex%20electric%20vehicle%20%28EV%29%20charging%0Aoptimization%20problem%20and%20prove%20that%20its%20performance%20is%20superior%20and%20requires%0Asignificantly%20fewer%20training%20trajectories%2C%20thus%20improving%20sample%20efficiency%0Acompared%20to%20existing%20DT%20and%20offline%20RL%20baselines.%20Furthermore%2C%20GNN-DT%20exhibits%0Arobust%20generalization%20to%20unseen%20environments%20and%20larger%20action%20spaces%2C%0Aaddressing%20a%20critical%20gap%20in%20prior%20offline%20and%20online%20RL%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01778v2&entry.124074799=Read"},
{"title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training\n  of Gemma2", "author": "Vytenis \u0160liogeris and Povilas Daniu\u0161is and Art\u016bras Nakvosas", "abstract": "  This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs.\n", "link": "http://arxiv.org/abs/2505.05946v1", "date": "2025-05-09", "relevancy": 1.6978, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4228}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elastic%20Weight%20Consolidation%20for%20Full-Parameter%20Continual%20Pre-Training%0A%20%20of%20Gemma2&body=Title%3A%20Elastic%20Weight%20Consolidation%20for%20Full-Parameter%20Continual%20Pre-Training%0A%20%20of%20Gemma2%0AAuthor%3A%20Vytenis%20%C5%A0liogeris%20and%20Povilas%20Daniu%C5%A1is%20and%20Art%C5%ABras%20Nakvosas%0AAbstract%3A%20%20%20This%20technical%20report%20describes%20an%20experiment%20on%20autoregressive%20pre-training%0Aof%20Gemma2%202%20billion%20parameter%20large%20language%20model%20%28LLM%29%20with%2010%5C%25%20on%20the%0ALithuanian%20language%20component%20of%20CulturaX%20from%20the%20point%20of%20view%20of%20continual%0Alearning.%20We%20apply%20elastic%20weight%20consolidation%20%28EWC%29%20to%20the%20full%20set%20of%20the%0Amodel%27s%20parameters%20and%20investigate%20language%20understanding%20benchmarks%2C%0Aconsisting%20of%20Arc%2C%20Belebele%2C%20Gsm8K%2C%20Hellaswag%2C%20MMLU%2C%20TruthfulQA%2C%20and%20Winogrande%0Asets%20%28both%20in%20English%20and%20Lithuanian%20versions%29%2C%20and%20perplexity%20benchmarks.%20We%0Aempirically%20demonstrate%20that%20EWC%20regularisation%20allows%20us%20not%20only%20to%20mitigate%0Acatastrophic%20forgetting%20effects%20but%20also%20that%20it%20is%20potentially%20beneficial%20for%0Alearning%20of%20the%20new%20task%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElastic%2520Weight%2520Consolidation%2520for%2520Full-Parameter%2520Continual%2520Pre-Training%250A%2520%2520of%2520Gemma2%26entry.906535625%3DVytenis%2520%25C5%25A0liogeris%2520and%2520Povilas%2520Daniu%25C5%25A1is%2520and%2520Art%25C5%25ABras%2520Nakvosas%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520describes%2520an%2520experiment%2520on%2520autoregressive%2520pre-training%250Aof%2520Gemma2%25202%2520billion%2520parameter%2520large%2520language%2520model%2520%2528LLM%2529%2520with%252010%255C%2525%2520on%2520the%250ALithuanian%2520language%2520component%2520of%2520CulturaX%2520from%2520the%2520point%2520of%2520view%2520of%2520continual%250Alearning.%2520We%2520apply%2520elastic%2520weight%2520consolidation%2520%2528EWC%2529%2520to%2520the%2520full%2520set%2520of%2520the%250Amodel%2527s%2520parameters%2520and%2520investigate%2520language%2520understanding%2520benchmarks%252C%250Aconsisting%2520of%2520Arc%252C%2520Belebele%252C%2520Gsm8K%252C%2520Hellaswag%252C%2520MMLU%252C%2520TruthfulQA%252C%2520and%2520Winogrande%250Asets%2520%2528both%2520in%2520English%2520and%2520Lithuanian%2520versions%2529%252C%2520and%2520perplexity%2520benchmarks.%2520We%250Aempirically%2520demonstrate%2520that%2520EWC%2520regularisation%2520allows%2520us%2520not%2520only%2520to%2520mitigate%250Acatastrophic%2520forgetting%2520effects%2520but%2520also%2520that%2520it%2520is%2520potentially%2520beneficial%2520for%250Alearning%2520of%2520the%2520new%2520task%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elastic%20Weight%20Consolidation%20for%20Full-Parameter%20Continual%20Pre-Training%0A%20%20of%20Gemma2&entry.906535625=Vytenis%20%C5%A0liogeris%20and%20Povilas%20Daniu%C5%A1is%20and%20Art%C5%ABras%20Nakvosas&entry.1292438233=%20%20This%20technical%20report%20describes%20an%20experiment%20on%20autoregressive%20pre-training%0Aof%20Gemma2%202%20billion%20parameter%20large%20language%20model%20%28LLM%29%20with%2010%5C%25%20on%20the%0ALithuanian%20language%20component%20of%20CulturaX%20from%20the%20point%20of%20view%20of%20continual%0Alearning.%20We%20apply%20elastic%20weight%20consolidation%20%28EWC%29%20to%20the%20full%20set%20of%20the%0Amodel%27s%20parameters%20and%20investigate%20language%20understanding%20benchmarks%2C%0Aconsisting%20of%20Arc%2C%20Belebele%2C%20Gsm8K%2C%20Hellaswag%2C%20MMLU%2C%20TruthfulQA%2C%20and%20Winogrande%0Asets%20%28both%20in%20English%20and%20Lithuanian%20versions%29%2C%20and%20perplexity%20benchmarks.%20We%0Aempirically%20demonstrate%20that%20EWC%20regularisation%20allows%20us%20not%20only%20to%20mitigate%0Acatastrophic%20forgetting%20effects%20but%20also%20that%20it%20is%20potentially%20beneficial%20for%0Alearning%20of%20the%20new%20task%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05946v1&entry.124074799=Read"},
{"title": "Learning-based adaption of robotic friction models", "author": "Philipp Scholl and Maged Iskandar and Sebastian Wolf and Jinoh Lee and Aras Bacho and Alexander Dietrich and Alin Albu-Sch\u00e4ffer and Gitta Kutyniok", "abstract": "  In the Fourth Industrial Revolution, wherein artificial intelligence and the\nautomation of machines occupy a central role, the deployment of robots is\nindispensable. However, the manufacturing process using robots, especially in\ncollaboration with humans, is highly intricate. In particular, modeling the\nfriction torque in robotic joints is a longstanding problem due to the lack of\na good mathematical description. This motivates the usage of data-driven\nmethods in recent works. However, model-based and data-driven models often\nexhibit limitations in their ability to generalize beyond the specific dynamics\nthey were trained on, as we demonstrate in this paper. To address this\nchallenge, we introduce a novel approach based on residual learning, which aims\nto adapt an existing friction model to new dynamics using as little data as\npossible. We validate our approach by training a base neural network on a\nsymmetric friction data set to learn an accurate relation between the velocity\nand the friction torque. Subsequently, to adapt to more complex asymmetric\nsettings, we train a second network on a small dataset, focusing on predicting\nthe residual of the initial network's output. By combining the output of both\nnetworks in a suitable manner, our proposed estimator outperforms the\nconventional model-based approach, an extended LuGre model, and the base neural\nnetwork significantly. Furthermore, we evaluate our method on trajectories\ninvolving external loads and still observe a substantial improvement,\napproximately 60-70%, over the conventional approach. Our method does not rely\non data with external load during training, eliminating the need for external\ntorque sensors. This demonstrates the generalization capability of our\napproach, even with a small amount of data--less than a minute--enabling\nadaptation to diverse scenarios based on prior knowledge about friction in\ndifferent settings.\n", "link": "http://arxiv.org/abs/2310.16688v2", "date": "2025-05-09", "relevancy": 1.5713, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6008}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5095}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20adaption%20of%20robotic%20friction%20models&body=Title%3A%20Learning-based%20adaption%20of%20robotic%20friction%20models%0AAuthor%3A%20Philipp%20Scholl%20and%20Maged%20Iskandar%20and%20Sebastian%20Wolf%20and%20Jinoh%20Lee%20and%20Aras%20Bacho%20and%20Alexander%20Dietrich%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Gitta%20Kutyniok%0AAbstract%3A%20%20%20In%20the%20Fourth%20Industrial%20Revolution%2C%20wherein%20artificial%20intelligence%20and%20the%0Aautomation%20of%20machines%20occupy%20a%20central%20role%2C%20the%20deployment%20of%20robots%20is%0Aindispensable.%20However%2C%20the%20manufacturing%20process%20using%20robots%2C%20especially%20in%0Acollaboration%20with%20humans%2C%20is%20highly%20intricate.%20In%20particular%2C%20modeling%20the%0Afriction%20torque%20in%20robotic%20joints%20is%20a%20longstanding%20problem%20due%20to%20the%20lack%20of%0Aa%20good%20mathematical%20description.%20This%20motivates%20the%20usage%20of%20data-driven%0Amethods%20in%20recent%20works.%20However%2C%20model-based%20and%20data-driven%20models%20often%0Aexhibit%20limitations%20in%20their%20ability%20to%20generalize%20beyond%20the%20specific%20dynamics%0Athey%20were%20trained%20on%2C%20as%20we%20demonstrate%20in%20this%20paper.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20approach%20based%20on%20residual%20learning%2C%20which%20aims%0Ato%20adapt%20an%20existing%20friction%20model%20to%20new%20dynamics%20using%20as%20little%20data%20as%0Apossible.%20We%20validate%20our%20approach%20by%20training%20a%20base%20neural%20network%20on%20a%0Asymmetric%20friction%20data%20set%20to%20learn%20an%20accurate%20relation%20between%20the%20velocity%0Aand%20the%20friction%20torque.%20Subsequently%2C%20to%20adapt%20to%20more%20complex%20asymmetric%0Asettings%2C%20we%20train%20a%20second%20network%20on%20a%20small%20dataset%2C%20focusing%20on%20predicting%0Athe%20residual%20of%20the%20initial%20network%27s%20output.%20By%20combining%20the%20output%20of%20both%0Anetworks%20in%20a%20suitable%20manner%2C%20our%20proposed%20estimator%20outperforms%20the%0Aconventional%20model-based%20approach%2C%20an%20extended%20LuGre%20model%2C%20and%20the%20base%20neural%0Anetwork%20significantly.%20Furthermore%2C%20we%20evaluate%20our%20method%20on%20trajectories%0Ainvolving%20external%20loads%20and%20still%20observe%20a%20substantial%20improvement%2C%0Aapproximately%2060-70%25%2C%20over%20the%20conventional%20approach.%20Our%20method%20does%20not%20rely%0Aon%20data%20with%20external%20load%20during%20training%2C%20eliminating%20the%20need%20for%20external%0Atorque%20sensors.%20This%20demonstrates%20the%20generalization%20capability%20of%20our%0Aapproach%2C%20even%20with%20a%20small%20amount%20of%20data--less%20than%20a%20minute--enabling%0Aadaptation%20to%20diverse%20scenarios%20based%20on%20prior%20knowledge%20about%20friction%20in%0Adifferent%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520adaption%2520of%2520robotic%2520friction%2520models%26entry.906535625%3DPhilipp%2520Scholl%2520and%2520Maged%2520Iskandar%2520and%2520Sebastian%2520Wolf%2520and%2520Jinoh%2520Lee%2520and%2520Aras%2520Bacho%2520and%2520Alexander%2520Dietrich%2520and%2520Alin%2520Albu-Sch%25C3%25A4ffer%2520and%2520Gitta%2520Kutyniok%26entry.1292438233%3D%2520%2520In%2520the%2520Fourth%2520Industrial%2520Revolution%252C%2520wherein%2520artificial%2520intelligence%2520and%2520the%250Aautomation%2520of%2520machines%2520occupy%2520a%2520central%2520role%252C%2520the%2520deployment%2520of%2520robots%2520is%250Aindispensable.%2520However%252C%2520the%2520manufacturing%2520process%2520using%2520robots%252C%2520especially%2520in%250Acollaboration%2520with%2520humans%252C%2520is%2520highly%2520intricate.%2520In%2520particular%252C%2520modeling%2520the%250Afriction%2520torque%2520in%2520robotic%2520joints%2520is%2520a%2520longstanding%2520problem%2520due%2520to%2520the%2520lack%2520of%250Aa%2520good%2520mathematical%2520description.%2520This%2520motivates%2520the%2520usage%2520of%2520data-driven%250Amethods%2520in%2520recent%2520works.%2520However%252C%2520model-based%2520and%2520data-driven%2520models%2520often%250Aexhibit%2520limitations%2520in%2520their%2520ability%2520to%2520generalize%2520beyond%2520the%2520specific%2520dynamics%250Athey%2520were%2520trained%2520on%252C%2520as%2520we%2520demonstrate%2520in%2520this%2520paper.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520based%2520on%2520residual%2520learning%252C%2520which%2520aims%250Ato%2520adapt%2520an%2520existing%2520friction%2520model%2520to%2520new%2520dynamics%2520using%2520as%2520little%2520data%2520as%250Apossible.%2520We%2520validate%2520our%2520approach%2520by%2520training%2520a%2520base%2520neural%2520network%2520on%2520a%250Asymmetric%2520friction%2520data%2520set%2520to%2520learn%2520an%2520accurate%2520relation%2520between%2520the%2520velocity%250Aand%2520the%2520friction%2520torque.%2520Subsequently%252C%2520to%2520adapt%2520to%2520more%2520complex%2520asymmetric%250Asettings%252C%2520we%2520train%2520a%2520second%2520network%2520on%2520a%2520small%2520dataset%252C%2520focusing%2520on%2520predicting%250Athe%2520residual%2520of%2520the%2520initial%2520network%2527s%2520output.%2520By%2520combining%2520the%2520output%2520of%2520both%250Anetworks%2520in%2520a%2520suitable%2520manner%252C%2520our%2520proposed%2520estimator%2520outperforms%2520the%250Aconventional%2520model-based%2520approach%252C%2520an%2520extended%2520LuGre%2520model%252C%2520and%2520the%2520base%2520neural%250Anetwork%2520significantly.%2520Furthermore%252C%2520we%2520evaluate%2520our%2520method%2520on%2520trajectories%250Ainvolving%2520external%2520loads%2520and%2520still%2520observe%2520a%2520substantial%2520improvement%252C%250Aapproximately%252060-70%2525%252C%2520over%2520the%2520conventional%2520approach.%2520Our%2520method%2520does%2520not%2520rely%250Aon%2520data%2520with%2520external%2520load%2520during%2520training%252C%2520eliminating%2520the%2520need%2520for%2520external%250Atorque%2520sensors.%2520This%2520demonstrates%2520the%2520generalization%2520capability%2520of%2520our%250Aapproach%252C%2520even%2520with%2520a%2520small%2520amount%2520of%2520data--less%2520than%2520a%2520minute--enabling%250Aadaptation%2520to%2520diverse%2520scenarios%2520based%2520on%2520prior%2520knowledge%2520about%2520friction%2520in%250Adifferent%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20adaption%20of%20robotic%20friction%20models&entry.906535625=Philipp%20Scholl%20and%20Maged%20Iskandar%20and%20Sebastian%20Wolf%20and%20Jinoh%20Lee%20and%20Aras%20Bacho%20and%20Alexander%20Dietrich%20and%20Alin%20Albu-Sch%C3%A4ffer%20and%20Gitta%20Kutyniok&entry.1292438233=%20%20In%20the%20Fourth%20Industrial%20Revolution%2C%20wherein%20artificial%20intelligence%20and%20the%0Aautomation%20of%20machines%20occupy%20a%20central%20role%2C%20the%20deployment%20of%20robots%20is%0Aindispensable.%20However%2C%20the%20manufacturing%20process%20using%20robots%2C%20especially%20in%0Acollaboration%20with%20humans%2C%20is%20highly%20intricate.%20In%20particular%2C%20modeling%20the%0Afriction%20torque%20in%20robotic%20joints%20is%20a%20longstanding%20problem%20due%20to%20the%20lack%20of%0Aa%20good%20mathematical%20description.%20This%20motivates%20the%20usage%20of%20data-driven%0Amethods%20in%20recent%20works.%20However%2C%20model-based%20and%20data-driven%20models%20often%0Aexhibit%20limitations%20in%20their%20ability%20to%20generalize%20beyond%20the%20specific%20dynamics%0Athey%20were%20trained%20on%2C%20as%20we%20demonstrate%20in%20this%20paper.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20novel%20approach%20based%20on%20residual%20learning%2C%20which%20aims%0Ato%20adapt%20an%20existing%20friction%20model%20to%20new%20dynamics%20using%20as%20little%20data%20as%0Apossible.%20We%20validate%20our%20approach%20by%20training%20a%20base%20neural%20network%20on%20a%0Asymmetric%20friction%20data%20set%20to%20learn%20an%20accurate%20relation%20between%20the%20velocity%0Aand%20the%20friction%20torque.%20Subsequently%2C%20to%20adapt%20to%20more%20complex%20asymmetric%0Asettings%2C%20we%20train%20a%20second%20network%20on%20a%20small%20dataset%2C%20focusing%20on%20predicting%0Athe%20residual%20of%20the%20initial%20network%27s%20output.%20By%20combining%20the%20output%20of%20both%0Anetworks%20in%20a%20suitable%20manner%2C%20our%20proposed%20estimator%20outperforms%20the%0Aconventional%20model-based%20approach%2C%20an%20extended%20LuGre%20model%2C%20and%20the%20base%20neural%0Anetwork%20significantly.%20Furthermore%2C%20we%20evaluate%20our%20method%20on%20trajectories%0Ainvolving%20external%20loads%20and%20still%20observe%20a%20substantial%20improvement%2C%0Aapproximately%2060-70%25%2C%20over%20the%20conventional%20approach.%20Our%20method%20does%20not%20rely%0Aon%20data%20with%20external%20load%20during%20training%2C%20eliminating%20the%20need%20for%20external%0Atorque%20sensors.%20This%20demonstrates%20the%20generalization%20capability%20of%20our%0Aapproach%2C%20even%20with%20a%20small%20amount%20of%20data--less%20than%20a%20minute--enabling%0Aadaptation%20to%20diverse%20scenarios%20based%20on%20prior%20knowledge%20about%20friction%20in%0Adifferent%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16688v2&entry.124074799=Read"},
{"title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities", "author": "Hiari Pizzini Cavagna and Daniele Cesarini and Andrea Bartolini", "abstract": "  The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.\n", "link": "http://arxiv.org/abs/2505.06085v1", "date": "2025-05-09", "relevancy": 1.3714, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4781}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Tenstorrent%27s%20RISC-V%20MatMul%20Acceleration%20Capabilities&body=Title%3A%20Assessing%20Tenstorrent%27s%20RISC-V%20MatMul%20Acceleration%20Capabilities%0AAuthor%3A%20Hiari%20Pizzini%20Cavagna%20and%20Daniele%20Cesarini%20and%20Andrea%20Bartolini%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20generative%20AI%20as%20Large%20Language%20Models%20%28LLMs%29%0Aservices%20has%20driven%20the%20need%20for%20specialized%20hardware%20architectures%20that%0Aoptimize%20computational%20efficiency%20and%20energy%20consumption.%20This%20paper%20evaluates%0Athe%20performance%20of%20the%20Tenstorrent%20Grayskull%20e75%20RISC-V%20accelerator%20for%20basic%0Alinear%20algebra%20kernels%20at%20reduced%20numerical%20precision%2C%20a%20fundamental%20operation%0Ain%20LLM%20computations.%20We%20present%20a%20detailed%20characterization%20of%20Grayskull%27s%0Aexecution%20model%2C%20gridsize%2C%20matrix%20dimensions%2C%20data%20formats%2C%20and%20numerical%0Aprecision%20impact%20computational%20efficiency.%20Furthermore%2C%20we%20compare%20Grayskull%27s%0Aperformance%20against%20state-of-the-art%20architectures%20with%20tensor%20acceleration%2C%0Aincluding%20Intel%20Sapphire%20Rapids%20processors%20and%20two%20NVIDIA%20GPUs%20%28V100%20and%20A100%29.%0AWhilst%20NVIDIA%20GPUs%20dominate%20raw%20performance%2C%20Grayskull%20demonstrates%20a%0Acompetitive%20trade-off%20between%20power%20consumption%20and%20computational%20throughput%2C%0Areaching%20a%20peak%20of%201.55%20TFLOPs/Watt%20with%20BF16.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Tenstorrent%2527s%2520RISC-V%2520MatMul%2520Acceleration%2520Capabilities%26entry.906535625%3DHiari%2520Pizzini%2520Cavagna%2520and%2520Daniele%2520Cesarini%2520and%2520Andrea%2520Bartolini%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520generative%2520AI%2520as%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aservices%2520has%2520driven%2520the%2520need%2520for%2520specialized%2520hardware%2520architectures%2520that%250Aoptimize%2520computational%2520efficiency%2520and%2520energy%2520consumption.%2520This%2520paper%2520evaluates%250Athe%2520performance%2520of%2520the%2520Tenstorrent%2520Grayskull%2520e75%2520RISC-V%2520accelerator%2520for%2520basic%250Alinear%2520algebra%2520kernels%2520at%2520reduced%2520numerical%2520precision%252C%2520a%2520fundamental%2520operation%250Ain%2520LLM%2520computations.%2520We%2520present%2520a%2520detailed%2520characterization%2520of%2520Grayskull%2527s%250Aexecution%2520model%252C%2520gridsize%252C%2520matrix%2520dimensions%252C%2520data%2520formats%252C%2520and%2520numerical%250Aprecision%2520impact%2520computational%2520efficiency.%2520Furthermore%252C%2520we%2520compare%2520Grayskull%2527s%250Aperformance%2520against%2520state-of-the-art%2520architectures%2520with%2520tensor%2520acceleration%252C%250Aincluding%2520Intel%2520Sapphire%2520Rapids%2520processors%2520and%2520two%2520NVIDIA%2520GPUs%2520%2528V100%2520and%2520A100%2529.%250AWhilst%2520NVIDIA%2520GPUs%2520dominate%2520raw%2520performance%252C%2520Grayskull%2520demonstrates%2520a%250Acompetitive%2520trade-off%2520between%2520power%2520consumption%2520and%2520computational%2520throughput%252C%250Areaching%2520a%2520peak%2520of%25201.55%2520TFLOPs/Watt%2520with%2520BF16.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Tenstorrent%27s%20RISC-V%20MatMul%20Acceleration%20Capabilities&entry.906535625=Hiari%20Pizzini%20Cavagna%20and%20Daniele%20Cesarini%20and%20Andrea%20Bartolini&entry.1292438233=%20%20The%20increasing%20demand%20for%20generative%20AI%20as%20Large%20Language%20Models%20%28LLMs%29%0Aservices%20has%20driven%20the%20need%20for%20specialized%20hardware%20architectures%20that%0Aoptimize%20computational%20efficiency%20and%20energy%20consumption.%20This%20paper%20evaluates%0Athe%20performance%20of%20the%20Tenstorrent%20Grayskull%20e75%20RISC-V%20accelerator%20for%20basic%0Alinear%20algebra%20kernels%20at%20reduced%20numerical%20precision%2C%20a%20fundamental%20operation%0Ain%20LLM%20computations.%20We%20present%20a%20detailed%20characterization%20of%20Grayskull%27s%0Aexecution%20model%2C%20gridsize%2C%20matrix%20dimensions%2C%20data%20formats%2C%20and%20numerical%0Aprecision%20impact%20computational%20efficiency.%20Furthermore%2C%20we%20compare%20Grayskull%27s%0Aperformance%20against%20state-of-the-art%20architectures%20with%20tensor%20acceleration%2C%0Aincluding%20Intel%20Sapphire%20Rapids%20processors%20and%20two%20NVIDIA%20GPUs%20%28V100%20and%20A100%29.%0AWhilst%20NVIDIA%20GPUs%20dominate%20raw%20performance%2C%20Grayskull%20demonstrates%20a%0Acompetitive%20trade-off%20between%20power%20consumption%20and%20computational%20throughput%2C%0Areaching%20a%20peak%20of%201.55%20TFLOPs/Watt%20with%20BF16.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06085v1&entry.124074799=Read"},
{"title": "Human Perception-Inspired Grain Segmentation Refinement Using\n  Conditional Random Fields", "author": "Doruk Aksoy and Huolin L. Xin and Timothy J. Rupert and William J. Bowman", "abstract": "  Automated detection of grain boundaries in electron microscope images of\npolycrystalline materials could help accelerate the nanoscale characterization\nof myriad engineering materials and novel materials under scientific research.\nAccurate segmentation of interconnected line networks, such as grain boundaries\nin polycrystalline material microstructures, poses a significant challenge due\nto the fragmented masks produced by conventional computer vision algorithms,\nincluding convolutional neural networks. These algorithms struggle with thin\nmasks, often necessitating post-processing for effective contour closure and\ncontinuity. Previous approaches in this domain have typically relied on custom\npost-processing techniques that are problem-specific and heavily dependent on\nthe quality of the mask obtained from a computer vision algorithm. Addressing\nthis issue, this paper introduces a fast, high-fidelity post-processing\ntechnique that is universally applicable to segmentation masks of\ninterconnected line networks. Leveraging domain knowledge about grain boundary\nconnectivity, this method employs conditional random fields and perceptual\ngrouping rules to refine segmentation masks of any image with a discernible\ngrain structure. This approach significantly enhances segmentation mask\naccuracy, achieving a 79% segment identification accuracy in validation with a\nU-Net model on electron microscopy images of a polycrystalline oxide.\nAdditionally, a novel grain alignment metric is introduced, showing a 51%\nimprovement in grain alignment. This method not only enables rapid and accurate\nsegmentation but also facilitates an unprecedented level of data analysis,\nsignificantly improving the statistical representation of grain boundary\nnetworks, making it suitable for a range of disciplines where precise\nsegmentation of interconnected line networks is essential.\n", "link": "http://arxiv.org/abs/2312.09968v2", "date": "2025-05-09", "relevancy": 1.5466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5185}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Perception-Inspired%20Grain%20Segmentation%20Refinement%20Using%0A%20%20Conditional%20Random%20Fields&body=Title%3A%20Human%20Perception-Inspired%20Grain%20Segmentation%20Refinement%20Using%0A%20%20Conditional%20Random%20Fields%0AAuthor%3A%20Doruk%20Aksoy%20and%20Huolin%20L.%20Xin%20and%20Timothy%20J.%20Rupert%20and%20William%20J.%20Bowman%0AAbstract%3A%20%20%20Automated%20detection%20of%20grain%20boundaries%20in%20electron%20microscope%20images%20of%0Apolycrystalline%20materials%20could%20help%20accelerate%20the%20nanoscale%20characterization%0Aof%20myriad%20engineering%20materials%20and%20novel%20materials%20under%20scientific%20research.%0AAccurate%20segmentation%20of%20interconnected%20line%20networks%2C%20such%20as%20grain%20boundaries%0Ain%20polycrystalline%20material%20microstructures%2C%20poses%20a%20significant%20challenge%20due%0Ato%20the%20fragmented%20masks%20produced%20by%20conventional%20computer%20vision%20algorithms%2C%0Aincluding%20convolutional%20neural%20networks.%20These%20algorithms%20struggle%20with%20thin%0Amasks%2C%20often%20necessitating%20post-processing%20for%20effective%20contour%20closure%20and%0Acontinuity.%20Previous%20approaches%20in%20this%20domain%20have%20typically%20relied%20on%20custom%0Apost-processing%20techniques%20that%20are%20problem-specific%20and%20heavily%20dependent%20on%0Athe%20quality%20of%20the%20mask%20obtained%20from%20a%20computer%20vision%20algorithm.%20Addressing%0Athis%20issue%2C%20this%20paper%20introduces%20a%20fast%2C%20high-fidelity%20post-processing%0Atechnique%20that%20is%20universally%20applicable%20to%20segmentation%20masks%20of%0Ainterconnected%20line%20networks.%20Leveraging%20domain%20knowledge%20about%20grain%20boundary%0Aconnectivity%2C%20this%20method%20employs%20conditional%20random%20fields%20and%20perceptual%0Agrouping%20rules%20to%20refine%20segmentation%20masks%20of%20any%20image%20with%20a%20discernible%0Agrain%20structure.%20This%20approach%20significantly%20enhances%20segmentation%20mask%0Aaccuracy%2C%20achieving%20a%2079%25%20segment%20identification%20accuracy%20in%20validation%20with%20a%0AU-Net%20model%20on%20electron%20microscopy%20images%20of%20a%20polycrystalline%20oxide.%0AAdditionally%2C%20a%20novel%20grain%20alignment%20metric%20is%20introduced%2C%20showing%20a%2051%25%0Aimprovement%20in%20grain%20alignment.%20This%20method%20not%20only%20enables%20rapid%20and%20accurate%0Asegmentation%20but%20also%20facilitates%20an%20unprecedented%20level%20of%20data%20analysis%2C%0Asignificantly%20improving%20the%20statistical%20representation%20of%20grain%20boundary%0Anetworks%2C%20making%20it%20suitable%20for%20a%20range%20of%20disciplines%20where%20precise%0Asegmentation%20of%20interconnected%20line%20networks%20is%20essential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Perception-Inspired%2520Grain%2520Segmentation%2520Refinement%2520Using%250A%2520%2520Conditional%2520Random%2520Fields%26entry.906535625%3DDoruk%2520Aksoy%2520and%2520Huolin%2520L.%2520Xin%2520and%2520Timothy%2520J.%2520Rupert%2520and%2520William%2520J.%2520Bowman%26entry.1292438233%3D%2520%2520Automated%2520detection%2520of%2520grain%2520boundaries%2520in%2520electron%2520microscope%2520images%2520of%250Apolycrystalline%2520materials%2520could%2520help%2520accelerate%2520the%2520nanoscale%2520characterization%250Aof%2520myriad%2520engineering%2520materials%2520and%2520novel%2520materials%2520under%2520scientific%2520research.%250AAccurate%2520segmentation%2520of%2520interconnected%2520line%2520networks%252C%2520such%2520as%2520grain%2520boundaries%250Ain%2520polycrystalline%2520material%2520microstructures%252C%2520poses%2520a%2520significant%2520challenge%2520due%250Ato%2520the%2520fragmented%2520masks%2520produced%2520by%2520conventional%2520computer%2520vision%2520algorithms%252C%250Aincluding%2520convolutional%2520neural%2520networks.%2520These%2520algorithms%2520struggle%2520with%2520thin%250Amasks%252C%2520often%2520necessitating%2520post-processing%2520for%2520effective%2520contour%2520closure%2520and%250Acontinuity.%2520Previous%2520approaches%2520in%2520this%2520domain%2520have%2520typically%2520relied%2520on%2520custom%250Apost-processing%2520techniques%2520that%2520are%2520problem-specific%2520and%2520heavily%2520dependent%2520on%250Athe%2520quality%2520of%2520the%2520mask%2520obtained%2520from%2520a%2520computer%2520vision%2520algorithm.%2520Addressing%250Athis%2520issue%252C%2520this%2520paper%2520introduces%2520a%2520fast%252C%2520high-fidelity%2520post-processing%250Atechnique%2520that%2520is%2520universally%2520applicable%2520to%2520segmentation%2520masks%2520of%250Ainterconnected%2520line%2520networks.%2520Leveraging%2520domain%2520knowledge%2520about%2520grain%2520boundary%250Aconnectivity%252C%2520this%2520method%2520employs%2520conditional%2520random%2520fields%2520and%2520perceptual%250Agrouping%2520rules%2520to%2520refine%2520segmentation%2520masks%2520of%2520any%2520image%2520with%2520a%2520discernible%250Agrain%2520structure.%2520This%2520approach%2520significantly%2520enhances%2520segmentation%2520mask%250Aaccuracy%252C%2520achieving%2520a%252079%2525%2520segment%2520identification%2520accuracy%2520in%2520validation%2520with%2520a%250AU-Net%2520model%2520on%2520electron%2520microscopy%2520images%2520of%2520a%2520polycrystalline%2520oxide.%250AAdditionally%252C%2520a%2520novel%2520grain%2520alignment%2520metric%2520is%2520introduced%252C%2520showing%2520a%252051%2525%250Aimprovement%2520in%2520grain%2520alignment.%2520This%2520method%2520not%2520only%2520enables%2520rapid%2520and%2520accurate%250Asegmentation%2520but%2520also%2520facilitates%2520an%2520unprecedented%2520level%2520of%2520data%2520analysis%252C%250Asignificantly%2520improving%2520the%2520statistical%2520representation%2520of%2520grain%2520boundary%250Anetworks%252C%2520making%2520it%2520suitable%2520for%2520a%2520range%2520of%2520disciplines%2520where%2520precise%250Asegmentation%2520of%2520interconnected%2520line%2520networks%2520is%2520essential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Perception-Inspired%20Grain%20Segmentation%20Refinement%20Using%0A%20%20Conditional%20Random%20Fields&entry.906535625=Doruk%20Aksoy%20and%20Huolin%20L.%20Xin%20and%20Timothy%20J.%20Rupert%20and%20William%20J.%20Bowman&entry.1292438233=%20%20Automated%20detection%20of%20grain%20boundaries%20in%20electron%20microscope%20images%20of%0Apolycrystalline%20materials%20could%20help%20accelerate%20the%20nanoscale%20characterization%0Aof%20myriad%20engineering%20materials%20and%20novel%20materials%20under%20scientific%20research.%0AAccurate%20segmentation%20of%20interconnected%20line%20networks%2C%20such%20as%20grain%20boundaries%0Ain%20polycrystalline%20material%20microstructures%2C%20poses%20a%20significant%20challenge%20due%0Ato%20the%20fragmented%20masks%20produced%20by%20conventional%20computer%20vision%20algorithms%2C%0Aincluding%20convolutional%20neural%20networks.%20These%20algorithms%20struggle%20with%20thin%0Amasks%2C%20often%20necessitating%20post-processing%20for%20effective%20contour%20closure%20and%0Acontinuity.%20Previous%20approaches%20in%20this%20domain%20have%20typically%20relied%20on%20custom%0Apost-processing%20techniques%20that%20are%20problem-specific%20and%20heavily%20dependent%20on%0Athe%20quality%20of%20the%20mask%20obtained%20from%20a%20computer%20vision%20algorithm.%20Addressing%0Athis%20issue%2C%20this%20paper%20introduces%20a%20fast%2C%20high-fidelity%20post-processing%0Atechnique%20that%20is%20universally%20applicable%20to%20segmentation%20masks%20of%0Ainterconnected%20line%20networks.%20Leveraging%20domain%20knowledge%20about%20grain%20boundary%0Aconnectivity%2C%20this%20method%20employs%20conditional%20random%20fields%20and%20perceptual%0Agrouping%20rules%20to%20refine%20segmentation%20masks%20of%20any%20image%20with%20a%20discernible%0Agrain%20structure.%20This%20approach%20significantly%20enhances%20segmentation%20mask%0Aaccuracy%2C%20achieving%20a%2079%25%20segment%20identification%20accuracy%20in%20validation%20with%20a%0AU-Net%20model%20on%20electron%20microscopy%20images%20of%20a%20polycrystalline%20oxide.%0AAdditionally%2C%20a%20novel%20grain%20alignment%20metric%20is%20introduced%2C%20showing%20a%2051%25%0Aimprovement%20in%20grain%20alignment.%20This%20method%20not%20only%20enables%20rapid%20and%20accurate%0Asegmentation%20but%20also%20facilitates%20an%20unprecedented%20level%20of%20data%20analysis%2C%0Asignificantly%20improving%20the%20statistical%20representation%20of%20grain%20boundary%0Anetworks%2C%20making%20it%20suitable%20for%20a%20range%20of%20disciplines%20where%20precise%0Asegmentation%20of%20interconnected%20line%20networks%20is%20essential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09968v2&entry.124074799=Read"},
{"title": "Neuro-Symbolic Concepts", "author": "Jiayuan Mao and Joshua B. Tenenbaum and Jiajun Wu", "abstract": "  This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer.\n", "link": "http://arxiv.org/abs/2505.06191v1", "date": "2025-05-09", "relevancy": 1.6094, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5397}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-Symbolic%20Concepts&body=Title%3A%20Neuro-Symbolic%20Concepts%0AAuthor%3A%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20This%20article%20presents%20a%20concept-centric%20paradigm%20for%20building%20agents%20that%20can%0Alearn%20continually%20and%20reason%20flexibly.%20The%20concept-centric%20agent%20utilizes%20a%0Avocabulary%20of%20neuro-symbolic%20concepts.%20These%20concepts%2C%20such%20as%20object%2C%0Arelation%2C%20and%20action%20concepts%2C%20are%20grounded%20on%20sensory%20inputs%20and%20actuation%0Aoutputs.%20They%20are%20also%20compositional%2C%20allowing%20for%20the%20creation%20of%20novel%0Aconcepts%20through%20their%20structural%20combination.%20To%20facilitate%20learning%20and%0Areasoning%2C%20the%20concepts%20are%20typed%20and%20represented%20using%20a%20combination%20of%0Asymbolic%20programs%20and%20neural%20network%20representations.%20Leveraging%20such%0Aneuro-symbolic%20concepts%2C%20the%20agent%20can%20efficiently%20learn%20and%20recombine%20them%20to%0Asolve%20various%20tasks%20across%20different%20domains%2C%20ranging%20from%202D%20images%2C%20videos%2C%0A3D%20scenes%2C%20and%20robotic%20manipulation%20tasks.%20This%20concept-centric%20framework%0Aoffers%20several%20advantages%2C%20including%20data%20efficiency%2C%20compositional%0Ageneralization%2C%20continual%20learning%2C%20and%20zero-shot%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-Symbolic%2520Concepts%26entry.906535625%3DJiayuan%2520Mao%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520concept-centric%2520paradigm%2520for%2520building%2520agents%2520that%2520can%250Alearn%2520continually%2520and%2520reason%2520flexibly.%2520The%2520concept-centric%2520agent%2520utilizes%2520a%250Avocabulary%2520of%2520neuro-symbolic%2520concepts.%2520These%2520concepts%252C%2520such%2520as%2520object%252C%250Arelation%252C%2520and%2520action%2520concepts%252C%2520are%2520grounded%2520on%2520sensory%2520inputs%2520and%2520actuation%250Aoutputs.%2520They%2520are%2520also%2520compositional%252C%2520allowing%2520for%2520the%2520creation%2520of%2520novel%250Aconcepts%2520through%2520their%2520structural%2520combination.%2520To%2520facilitate%2520learning%2520and%250Areasoning%252C%2520the%2520concepts%2520are%2520typed%2520and%2520represented%2520using%2520a%2520combination%2520of%250Asymbolic%2520programs%2520and%2520neural%2520network%2520representations.%2520Leveraging%2520such%250Aneuro-symbolic%2520concepts%252C%2520the%2520agent%2520can%2520efficiently%2520learn%2520and%2520recombine%2520them%2520to%250Asolve%2520various%2520tasks%2520across%2520different%2520domains%252C%2520ranging%2520from%25202D%2520images%252C%2520videos%252C%250A3D%2520scenes%252C%2520and%2520robotic%2520manipulation%2520tasks.%2520This%2520concept-centric%2520framework%250Aoffers%2520several%2520advantages%252C%2520including%2520data%2520efficiency%252C%2520compositional%250Ageneralization%252C%2520continual%2520learning%252C%2520and%2520zero-shot%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-Symbolic%20Concepts&entry.906535625=Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Jiajun%20Wu&entry.1292438233=%20%20This%20article%20presents%20a%20concept-centric%20paradigm%20for%20building%20agents%20that%20can%0Alearn%20continually%20and%20reason%20flexibly.%20The%20concept-centric%20agent%20utilizes%20a%0Avocabulary%20of%20neuro-symbolic%20concepts.%20These%20concepts%2C%20such%20as%20object%2C%0Arelation%2C%20and%20action%20concepts%2C%20are%20grounded%20on%20sensory%20inputs%20and%20actuation%0Aoutputs.%20They%20are%20also%20compositional%2C%20allowing%20for%20the%20creation%20of%20novel%0Aconcepts%20through%20their%20structural%20combination.%20To%20facilitate%20learning%20and%0Areasoning%2C%20the%20concepts%20are%20typed%20and%20represented%20using%20a%20combination%20of%0Asymbolic%20programs%20and%20neural%20network%20representations.%20Leveraging%20such%0Aneuro-symbolic%20concepts%2C%20the%20agent%20can%20efficiently%20learn%20and%20recombine%20them%20to%0Asolve%20various%20tasks%20across%20different%20domains%2C%20ranging%20from%202D%20images%2C%20videos%2C%0A3D%20scenes%2C%20and%20robotic%20manipulation%20tasks.%20This%20concept-centric%20framework%0Aoffers%20several%20advantages%2C%20including%20data%20efficiency%2C%20compositional%0Ageneralization%2C%20continual%20learning%2C%20and%20zero-shot%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06191v1&entry.124074799=Read"},
{"title": "Realistic Adversarial Attacks for Robustness Evaluation of Trajectory\n  Prediction Models via Future State Perturbation", "author": "Julian F. Schumann and Jeroen Hagenus and Frederik Baymler Mathiesen and Arkady Zgonnikov", "abstract": "  Trajectory prediction is a key element of autonomous vehicle systems,\nenabling them to anticipate and react to the movements of other road users.\nEvaluating the robustness of prediction models against adversarial attacks is\nessential to ensure their reliability in real-world traffic. However, current\napproaches tend to focus on perturbing the past positions of surrounding\nagents, which can generate unrealistic scenarios and overlook critical\nvulnerabilities. This limitation may result in overly optimistic assessments of\nmodel performance in real-world conditions.\n  In this work, we demonstrate that perturbing not just past but also future\nstates of adversarial agents can uncover previously undetected weaknesses and\nthereby provide a more rigorous evaluation of model robustness. Our novel\napproach incorporates dynamic constraints and preserves tactical behaviors,\nenabling more effective and realistic adversarial attacks. We introduce new\nperformance measures to assess the realism and impact of these adversarial\ntrajectories. Testing our method on a state-of-the-art prediction model\nrevealed significant increases in prediction errors and collision rates under\nadversarial conditions. Qualitative analysis further showed that our attacks\ncan expose critical weaknesses, such as the inability of the model to detect\npotential collisions in what appear to be safe predictions. These results\nunderscore the need for more comprehensive adversarial testing to better\nevaluate and improve the reliability of trajectory prediction models for\nautonomous vehicles.\n", "link": "http://arxiv.org/abs/2505.06134v1", "date": "2025-05-09", "relevancy": 1.5356, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5106}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Adversarial%20Attacks%20for%20Robustness%20Evaluation%20of%20Trajectory%0A%20%20Prediction%20Models%20via%20Future%20State%20Perturbation&body=Title%3A%20Realistic%20Adversarial%20Attacks%20for%20Robustness%20Evaluation%20of%20Trajectory%0A%20%20Prediction%20Models%20via%20Future%20State%20Perturbation%0AAuthor%3A%20Julian%20F.%20Schumann%20and%20Jeroen%20Hagenus%20and%20Frederik%20Baymler%20Mathiesen%20and%20Arkady%20Zgonnikov%0AAbstract%3A%20%20%20Trajectory%20prediction%20is%20a%20key%20element%20of%20autonomous%20vehicle%20systems%2C%0Aenabling%20them%20to%20anticipate%20and%20react%20to%20the%20movements%20of%20other%20road%20users.%0AEvaluating%20the%20robustness%20of%20prediction%20models%20against%20adversarial%20attacks%20is%0Aessential%20to%20ensure%20their%20reliability%20in%20real-world%20traffic.%20However%2C%20current%0Aapproaches%20tend%20to%20focus%20on%20perturbing%20the%20past%20positions%20of%20surrounding%0Aagents%2C%20which%20can%20generate%20unrealistic%20scenarios%20and%20overlook%20critical%0Avulnerabilities.%20This%20limitation%20may%20result%20in%20overly%20optimistic%20assessments%20of%0Amodel%20performance%20in%20real-world%20conditions.%0A%20%20In%20this%20work%2C%20we%20demonstrate%20that%20perturbing%20not%20just%20past%20but%20also%20future%0Astates%20of%20adversarial%20agents%20can%20uncover%20previously%20undetected%20weaknesses%20and%0Athereby%20provide%20a%20more%20rigorous%20evaluation%20of%20model%20robustness.%20Our%20novel%0Aapproach%20incorporates%20dynamic%20constraints%20and%20preserves%20tactical%20behaviors%2C%0Aenabling%20more%20effective%20and%20realistic%20adversarial%20attacks.%20We%20introduce%20new%0Aperformance%20measures%20to%20assess%20the%20realism%20and%20impact%20of%20these%20adversarial%0Atrajectories.%20Testing%20our%20method%20on%20a%20state-of-the-art%20prediction%20model%0Arevealed%20significant%20increases%20in%20prediction%20errors%20and%20collision%20rates%20under%0Aadversarial%20conditions.%20Qualitative%20analysis%20further%20showed%20that%20our%20attacks%0Acan%20expose%20critical%20weaknesses%2C%20such%20as%20the%20inability%20of%20the%20model%20to%20detect%0Apotential%20collisions%20in%20what%20appear%20to%20be%20safe%20predictions.%20These%20results%0Aunderscore%20the%20need%20for%20more%20comprehensive%20adversarial%20testing%20to%20better%0Aevaluate%20and%20improve%20the%20reliability%20of%20trajectory%20prediction%20models%20for%0Aautonomous%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Adversarial%2520Attacks%2520for%2520Robustness%2520Evaluation%2520of%2520Trajectory%250A%2520%2520Prediction%2520Models%2520via%2520Future%2520State%2520Perturbation%26entry.906535625%3DJulian%2520F.%2520Schumann%2520and%2520Jeroen%2520Hagenus%2520and%2520Frederik%2520Baymler%2520Mathiesen%2520and%2520Arkady%2520Zgonnikov%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520is%2520a%2520key%2520element%2520of%2520autonomous%2520vehicle%2520systems%252C%250Aenabling%2520them%2520to%2520anticipate%2520and%2520react%2520to%2520the%2520movements%2520of%2520other%2520road%2520users.%250AEvaluating%2520the%2520robustness%2520of%2520prediction%2520models%2520against%2520adversarial%2520attacks%2520is%250Aessential%2520to%2520ensure%2520their%2520reliability%2520in%2520real-world%2520traffic.%2520However%252C%2520current%250Aapproaches%2520tend%2520to%2520focus%2520on%2520perturbing%2520the%2520past%2520positions%2520of%2520surrounding%250Aagents%252C%2520which%2520can%2520generate%2520unrealistic%2520scenarios%2520and%2520overlook%2520critical%250Avulnerabilities.%2520This%2520limitation%2520may%2520result%2520in%2520overly%2520optimistic%2520assessments%2520of%250Amodel%2520performance%2520in%2520real-world%2520conditions.%250A%2520%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520perturbing%2520not%2520just%2520past%2520but%2520also%2520future%250Astates%2520of%2520adversarial%2520agents%2520can%2520uncover%2520previously%2520undetected%2520weaknesses%2520and%250Athereby%2520provide%2520a%2520more%2520rigorous%2520evaluation%2520of%2520model%2520robustness.%2520Our%2520novel%250Aapproach%2520incorporates%2520dynamic%2520constraints%2520and%2520preserves%2520tactical%2520behaviors%252C%250Aenabling%2520more%2520effective%2520and%2520realistic%2520adversarial%2520attacks.%2520We%2520introduce%2520new%250Aperformance%2520measures%2520to%2520assess%2520the%2520realism%2520and%2520impact%2520of%2520these%2520adversarial%250Atrajectories.%2520Testing%2520our%2520method%2520on%2520a%2520state-of-the-art%2520prediction%2520model%250Arevealed%2520significant%2520increases%2520in%2520prediction%2520errors%2520and%2520collision%2520rates%2520under%250Aadversarial%2520conditions.%2520Qualitative%2520analysis%2520further%2520showed%2520that%2520our%2520attacks%250Acan%2520expose%2520critical%2520weaknesses%252C%2520such%2520as%2520the%2520inability%2520of%2520the%2520model%2520to%2520detect%250Apotential%2520collisions%2520in%2520what%2520appear%2520to%2520be%2520safe%2520predictions.%2520These%2520results%250Aunderscore%2520the%2520need%2520for%2520more%2520comprehensive%2520adversarial%2520testing%2520to%2520better%250Aevaluate%2520and%2520improve%2520the%2520reliability%2520of%2520trajectory%2520prediction%2520models%2520for%250Aautonomous%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Adversarial%20Attacks%20for%20Robustness%20Evaluation%20of%20Trajectory%0A%20%20Prediction%20Models%20via%20Future%20State%20Perturbation&entry.906535625=Julian%20F.%20Schumann%20and%20Jeroen%20Hagenus%20and%20Frederik%20Baymler%20Mathiesen%20and%20Arkady%20Zgonnikov&entry.1292438233=%20%20Trajectory%20prediction%20is%20a%20key%20element%20of%20autonomous%20vehicle%20systems%2C%0Aenabling%20them%20to%20anticipate%20and%20react%20to%20the%20movements%20of%20other%20road%20users.%0AEvaluating%20the%20robustness%20of%20prediction%20models%20against%20adversarial%20attacks%20is%0Aessential%20to%20ensure%20their%20reliability%20in%20real-world%20traffic.%20However%2C%20current%0Aapproaches%20tend%20to%20focus%20on%20perturbing%20the%20past%20positions%20of%20surrounding%0Aagents%2C%20which%20can%20generate%20unrealistic%20scenarios%20and%20overlook%20critical%0Avulnerabilities.%20This%20limitation%20may%20result%20in%20overly%20optimistic%20assessments%20of%0Amodel%20performance%20in%20real-world%20conditions.%0A%20%20In%20this%20work%2C%20we%20demonstrate%20that%20perturbing%20not%20just%20past%20but%20also%20future%0Astates%20of%20adversarial%20agents%20can%20uncover%20previously%20undetected%20weaknesses%20and%0Athereby%20provide%20a%20more%20rigorous%20evaluation%20of%20model%20robustness.%20Our%20novel%0Aapproach%20incorporates%20dynamic%20constraints%20and%20preserves%20tactical%20behaviors%2C%0Aenabling%20more%20effective%20and%20realistic%20adversarial%20attacks.%20We%20introduce%20new%0Aperformance%20measures%20to%20assess%20the%20realism%20and%20impact%20of%20these%20adversarial%0Atrajectories.%20Testing%20our%20method%20on%20a%20state-of-the-art%20prediction%20model%0Arevealed%20significant%20increases%20in%20prediction%20errors%20and%20collision%20rates%20under%0Aadversarial%20conditions.%20Qualitative%20analysis%20further%20showed%20that%20our%20attacks%0Acan%20expose%20critical%20weaknesses%2C%20such%20as%20the%20inability%20of%20the%20model%20to%20detect%0Apotential%20collisions%20in%20what%20appear%20to%20be%20safe%20predictions.%20These%20results%0Aunderscore%20the%20need%20for%20more%20comprehensive%20adversarial%20testing%20to%20better%0Aevaluate%20and%20improve%20the%20reliability%20of%20trajectory%20prediction%20models%20for%0Aautonomous%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06134v1&entry.124074799=Read"},
{"title": "Seqret: Mining Rule Sets from Event Sequences", "author": "Aleena Siji and Joscha C\u00fcppers and Osman Ali Mian and Jilles Vreeken", "abstract": "  Summarizing event sequences is a key aspect of data mining. Most existing\nmethods neglect conditional dependencies and focus on discovering sequential\npatterns only. In this paper, we study the problem of discovering both\nconditional and unconditional dependencies from event sequence data. We do so\nby discovering rules of the form $X \\rightarrow Y$ where $X$ and $Y$ are\nsequential patterns. Rules like these are simple to understand and provide a\nclear description of the relation between the antecedent and the consequent. To\ndiscover succinct and non-redundant sets of rules we formalize the problem in\nterms of the Minimum Description Length principle. As the search space is\nenormous and does not exhibit helpful structure, we propose the Seqret method\nto discover high-quality rule sets in practice. Through extensive empirical\nevaluation we show that unlike the state of the art, Seqret ably recovers the\nground truth on synthetic datasets and finds useful rules from real datasets.\n", "link": "http://arxiv.org/abs/2505.06049v1", "date": "2025-05-09", "relevancy": 1.6642, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seqret%3A%20Mining%20Rule%20Sets%20from%20Event%20Sequences&body=Title%3A%20Seqret%3A%20Mining%20Rule%20Sets%20from%20Event%20Sequences%0AAuthor%3A%20Aleena%20Siji%20and%20Joscha%20C%C3%BCppers%20and%20Osman%20Ali%20Mian%20and%20Jilles%20Vreeken%0AAbstract%3A%20%20%20Summarizing%20event%20sequences%20is%20a%20key%20aspect%20of%20data%20mining.%20Most%20existing%0Amethods%20neglect%20conditional%20dependencies%20and%20focus%20on%20discovering%20sequential%0Apatterns%20only.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20discovering%20both%0Aconditional%20and%20unconditional%20dependencies%20from%20event%20sequence%20data.%20We%20do%20so%0Aby%20discovering%20rules%20of%20the%20form%20%24X%20%5Crightarrow%20Y%24%20where%20%24X%24%20and%20%24Y%24%20are%0Asequential%20patterns.%20Rules%20like%20these%20are%20simple%20to%20understand%20and%20provide%20a%0Aclear%20description%20of%20the%20relation%20between%20the%20antecedent%20and%20the%20consequent.%20To%0Adiscover%20succinct%20and%20non-redundant%20sets%20of%20rules%20we%20formalize%20the%20problem%20in%0Aterms%20of%20the%20Minimum%20Description%20Length%20principle.%20As%20the%20search%20space%20is%0Aenormous%20and%20does%20not%20exhibit%20helpful%20structure%2C%20we%20propose%20the%20Seqret%20method%0Ato%20discover%20high-quality%20rule%20sets%20in%20practice.%20Through%20extensive%20empirical%0Aevaluation%20we%20show%20that%20unlike%20the%20state%20of%20the%20art%2C%20Seqret%20ably%20recovers%20the%0Aground%20truth%20on%20synthetic%20datasets%20and%20finds%20useful%20rules%20from%20real%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeqret%253A%2520Mining%2520Rule%2520Sets%2520from%2520Event%2520Sequences%26entry.906535625%3DAleena%2520Siji%2520and%2520Joscha%2520C%25C3%25BCppers%2520and%2520Osman%2520Ali%2520Mian%2520and%2520Jilles%2520Vreeken%26entry.1292438233%3D%2520%2520Summarizing%2520event%2520sequences%2520is%2520a%2520key%2520aspect%2520of%2520data%2520mining.%2520Most%2520existing%250Amethods%2520neglect%2520conditional%2520dependencies%2520and%2520focus%2520on%2520discovering%2520sequential%250Apatterns%2520only.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520problem%2520of%2520discovering%2520both%250Aconditional%2520and%2520unconditional%2520dependencies%2520from%2520event%2520sequence%2520data.%2520We%2520do%2520so%250Aby%2520discovering%2520rules%2520of%2520the%2520form%2520%2524X%2520%255Crightarrow%2520Y%2524%2520where%2520%2524X%2524%2520and%2520%2524Y%2524%2520are%250Asequential%2520patterns.%2520Rules%2520like%2520these%2520are%2520simple%2520to%2520understand%2520and%2520provide%2520a%250Aclear%2520description%2520of%2520the%2520relation%2520between%2520the%2520antecedent%2520and%2520the%2520consequent.%2520To%250Adiscover%2520succinct%2520and%2520non-redundant%2520sets%2520of%2520rules%2520we%2520formalize%2520the%2520problem%2520in%250Aterms%2520of%2520the%2520Minimum%2520Description%2520Length%2520principle.%2520As%2520the%2520search%2520space%2520is%250Aenormous%2520and%2520does%2520not%2520exhibit%2520helpful%2520structure%252C%2520we%2520propose%2520the%2520Seqret%2520method%250Ato%2520discover%2520high-quality%2520rule%2520sets%2520in%2520practice.%2520Through%2520extensive%2520empirical%250Aevaluation%2520we%2520show%2520that%2520unlike%2520the%2520state%2520of%2520the%2520art%252C%2520Seqret%2520ably%2520recovers%2520the%250Aground%2520truth%2520on%2520synthetic%2520datasets%2520and%2520finds%2520useful%2520rules%2520from%2520real%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seqret%3A%20Mining%20Rule%20Sets%20from%20Event%20Sequences&entry.906535625=Aleena%20Siji%20and%20Joscha%20C%C3%BCppers%20and%20Osman%20Ali%20Mian%20and%20Jilles%20Vreeken&entry.1292438233=%20%20Summarizing%20event%20sequences%20is%20a%20key%20aspect%20of%20data%20mining.%20Most%20existing%0Amethods%20neglect%20conditional%20dependencies%20and%20focus%20on%20discovering%20sequential%0Apatterns%20only.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20discovering%20both%0Aconditional%20and%20unconditional%20dependencies%20from%20event%20sequence%20data.%20We%20do%20so%0Aby%20discovering%20rules%20of%20the%20form%20%24X%20%5Crightarrow%20Y%24%20where%20%24X%24%20and%20%24Y%24%20are%0Asequential%20patterns.%20Rules%20like%20these%20are%20simple%20to%20understand%20and%20provide%20a%0Aclear%20description%20of%20the%20relation%20between%20the%20antecedent%20and%20the%20consequent.%20To%0Adiscover%20succinct%20and%20non-redundant%20sets%20of%20rules%20we%20formalize%20the%20problem%20in%0Aterms%20of%20the%20Minimum%20Description%20Length%20principle.%20As%20the%20search%20space%20is%0Aenormous%20and%20does%20not%20exhibit%20helpful%20structure%2C%20we%20propose%20the%20Seqret%20method%0Ato%20discover%20high-quality%20rule%20sets%20in%20practice.%20Through%20extensive%20empirical%0Aevaluation%20we%20show%20that%20unlike%20the%20state%20of%20the%20art%2C%20Seqret%20ably%20recovers%20the%0Aground%20truth%20on%20synthetic%20datasets%20and%20finds%20useful%20rules%20from%20real%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06049v1&entry.124074799=Read"},
{"title": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations", "author": "Yu-Hsiang Lan and Eric K. Oermann", "abstract": "  There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.\n", "link": "http://arxiv.org/abs/2505.00307v2", "date": "2025-05-09", "relevancy": 1.5071, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.565}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4874}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gateformer%3A%20Advancing%20Multivariate%20Time%20Series%20Forecasting%20through%0A%20%20Temporal%20and%20Variate-Wise%20Attention%20with%20Gated%20Representations&body=Title%3A%20Gateformer%3A%20Advancing%20Multivariate%20Time%20Series%20Forecasting%20through%0A%20%20Temporal%20and%20Variate-Wise%20Attention%20with%20Gated%20Representations%0AAuthor%3A%20Yu-Hsiang%20Lan%20and%20Eric%20K.%20Oermann%0AAbstract%3A%20%20%20There%20has%20been%20a%20recent%20surge%20of%20interest%20in%20time%20series%20modeling%20using%20the%0ATransformer%20architecture.%20However%2C%20forecasting%20multivariate%20time%20series%20with%0ATransformer%20presents%20a%20unique%20challenge%20as%20it%20requires%20modeling%20both%20temporal%0A%28cross-time%29%20and%20variate%20%28cross-variate%29%20dependencies.%20While%20Transformer-based%0Amodels%20have%20gained%20popularity%20for%20their%20flexibility%20in%20capturing%20both%0Asequential%20and%20cross-variate%20relationships%2C%20it%20is%20unclear%20how%20to%20best%20integrate%0Athese%20two%20sources%20of%20information%20in%20the%20context%20of%20the%20Transformer%20architecture%0Awhile%20optimizing%20for%20both%20performance%20and%20efficiency.%20We%20re-purpose%20the%0ATransformer%20architecture%20to%20effectively%20model%20both%20cross-time%20and%20cross-variate%0Adependencies.%20Our%20approach%20begins%20by%20embedding%20each%20variate%20independently%20into%0Aa%20variate-wise%20representation%20that%20captures%20its%20cross-time%20dynamics%2C%20and%20then%0Amodels%20cross-variate%20dependencies%20through%20attention%20mechanisms%20on%20these%20learned%0Aembeddings.%20Gating%20operations%20in%20both%20cross-time%20and%20cross-variate%20modeling%0Aphases%20regulate%20information%20flow%2C%20allowing%20the%20model%20to%20focus%20on%20the%20most%0Arelevant%20features%20for%20accurate%20predictions.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20across%2013%20real-world%20datasets%20and%20can%20be%0Aseamlessly%20integrated%20into%20other%20Transformer-based%20and%20LLM-based%20forecasters%2C%0Adelivering%20performance%20improvements%20up%20to%2020.7%5C%25%20over%20original%20models.%20Code%20is%0Aavailable%20at%20this%20repository%3A%20https%3A//github.com/nyuolab/Gateformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGateformer%253A%2520Advancing%2520Multivariate%2520Time%2520Series%2520Forecasting%2520through%250A%2520%2520Temporal%2520and%2520Variate-Wise%2520Attention%2520with%2520Gated%2520Representations%26entry.906535625%3DYu-Hsiang%2520Lan%2520and%2520Eric%2520K.%2520Oermann%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520recent%2520surge%2520of%2520interest%2520in%2520time%2520series%2520modeling%2520using%2520the%250ATransformer%2520architecture.%2520However%252C%2520forecasting%2520multivariate%2520time%2520series%2520with%250ATransformer%2520presents%2520a%2520unique%2520challenge%2520as%2520it%2520requires%2520modeling%2520both%2520temporal%250A%2528cross-time%2529%2520and%2520variate%2520%2528cross-variate%2529%2520dependencies.%2520While%2520Transformer-based%250Amodels%2520have%2520gained%2520popularity%2520for%2520their%2520flexibility%2520in%2520capturing%2520both%250Asequential%2520and%2520cross-variate%2520relationships%252C%2520it%2520is%2520unclear%2520how%2520to%2520best%2520integrate%250Athese%2520two%2520sources%2520of%2520information%2520in%2520the%2520context%2520of%2520the%2520Transformer%2520architecture%250Awhile%2520optimizing%2520for%2520both%2520performance%2520and%2520efficiency.%2520We%2520re-purpose%2520the%250ATransformer%2520architecture%2520to%2520effectively%2520model%2520both%2520cross-time%2520and%2520cross-variate%250Adependencies.%2520Our%2520approach%2520begins%2520by%2520embedding%2520each%2520variate%2520independently%2520into%250Aa%2520variate-wise%2520representation%2520that%2520captures%2520its%2520cross-time%2520dynamics%252C%2520and%2520then%250Amodels%2520cross-variate%2520dependencies%2520through%2520attention%2520mechanisms%2520on%2520these%2520learned%250Aembeddings.%2520Gating%2520operations%2520in%2520both%2520cross-time%2520and%2520cross-variate%2520modeling%250Aphases%2520regulate%2520information%2520flow%252C%2520allowing%2520the%2520model%2520to%2520focus%2520on%2520the%2520most%250Arelevant%2520features%2520for%2520accurate%2520predictions.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520across%252013%2520real-world%2520datasets%2520and%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520other%2520Transformer-based%2520and%2520LLM-based%2520forecasters%252C%250Adelivering%2520performance%2520improvements%2520up%2520to%252020.7%255C%2525%2520over%2520original%2520models.%2520Code%2520is%250Aavailable%2520at%2520this%2520repository%253A%2520https%253A//github.com/nyuolab/Gateformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gateformer%3A%20Advancing%20Multivariate%20Time%20Series%20Forecasting%20through%0A%20%20Temporal%20and%20Variate-Wise%20Attention%20with%20Gated%20Representations&entry.906535625=Yu-Hsiang%20Lan%20and%20Eric%20K.%20Oermann&entry.1292438233=%20%20There%20has%20been%20a%20recent%20surge%20of%20interest%20in%20time%20series%20modeling%20using%20the%0ATransformer%20architecture.%20However%2C%20forecasting%20multivariate%20time%20series%20with%0ATransformer%20presents%20a%20unique%20challenge%20as%20it%20requires%20modeling%20both%20temporal%0A%28cross-time%29%20and%20variate%20%28cross-variate%29%20dependencies.%20While%20Transformer-based%0Amodels%20have%20gained%20popularity%20for%20their%20flexibility%20in%20capturing%20both%0Asequential%20and%20cross-variate%20relationships%2C%20it%20is%20unclear%20how%20to%20best%20integrate%0Athese%20two%20sources%20of%20information%20in%20the%20context%20of%20the%20Transformer%20architecture%0Awhile%20optimizing%20for%20both%20performance%20and%20efficiency.%20We%20re-purpose%20the%0ATransformer%20architecture%20to%20effectively%20model%20both%20cross-time%20and%20cross-variate%0Adependencies.%20Our%20approach%20begins%20by%20embedding%20each%20variate%20independently%20into%0Aa%20variate-wise%20representation%20that%20captures%20its%20cross-time%20dynamics%2C%20and%20then%0Amodels%20cross-variate%20dependencies%20through%20attention%20mechanisms%20on%20these%20learned%0Aembeddings.%20Gating%20operations%20in%20both%20cross-time%20and%20cross-variate%20modeling%0Aphases%20regulate%20information%20flow%2C%20allowing%20the%20model%20to%20focus%20on%20the%20most%0Arelevant%20features%20for%20accurate%20predictions.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20across%2013%20real-world%20datasets%20and%20can%20be%0Aseamlessly%20integrated%20into%20other%20Transformer-based%20and%20LLM-based%20forecasters%2C%0Adelivering%20performance%20improvements%20up%20to%2020.7%5C%25%20over%20original%20models.%20Code%20is%0Aavailable%20at%20this%20repository%3A%20https%3A//github.com/nyuolab/Gateformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00307v2&entry.124074799=Read"},
{"title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing\n  Constraint for Domain Shift Mitigation", "author": "Dongying Li and Binyi Su and Hua Zhang and Yong Li and Haiyong Chen", "abstract": "  Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks.\n", "link": "http://arxiv.org/abs/2505.06117v1", "date": "2025-05-09", "relevancy": 1.1707, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6086}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5796}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photovoltaic%20Defect%20Image%20Generator%20with%20Boundary%20Alignment%20Smoothing%0A%20%20Constraint%20for%20Domain%20Shift%20Mitigation&body=Title%3A%20Photovoltaic%20Defect%20Image%20Generator%20with%20Boundary%20Alignment%20Smoothing%0A%20%20Constraint%20for%20Domain%20Shift%20Mitigation%0AAuthor%3A%20Dongying%20Li%20and%20Binyi%20Su%20and%20Hua%20Zhang%20and%20Yong%20Li%20and%20Haiyong%20Chen%0AAbstract%3A%20%20%20Accurate%20defect%20detection%20of%20photovoltaic%20%28PV%29%20cells%20is%20critical%20for%20ensuring%0Aquality%20and%20efficiency%20in%20intelligent%20PV%20manufacturing%20systems.%20However%2C%20the%0Ascarcity%20of%20rich%20defect%20data%20poses%20substantial%20challenges%20for%20effective%20model%0Atraining.%20While%20existing%20methods%20have%20explored%20generative%20models%20to%20augment%0Adatasets%2C%20they%20often%20suffer%20from%20instability%2C%20limited%20diversity%2C%20and%20domain%0Ashifts.%20To%20address%20these%20issues%2C%20we%20propose%20PDIG%2C%20a%20Photovoltaic%20Defect%20Image%0AGenerator%20based%20on%20Stable%20Diffusion%20%28SD%29.%20PDIG%20leverages%20the%20strong%20priors%0Alearned%20from%20large-scale%20datasets%20to%20enhance%20generation%20quality%20under%20limited%0Adata.%20Specifically%2C%20we%20introduce%20a%20Semantic%20Concept%20Embedding%20%28SCE%29%20module%20that%0Aincorporates%20text-conditioned%20priors%20to%20capture%20the%20relational%20concepts%20between%0Adefect%20types%20and%20their%20appearances.%20To%20further%20enrich%20the%20domain%20distribution%2C%0Awe%20design%20a%20Lightweight%20Industrial%20Style%20Adaptor%20%28LISA%29%2C%20which%20injects%0Aindustrial%20defect%20characteristics%20into%20the%20SD%20model%20through%20cross-disentangled%0Aattention.%20At%20inference%2C%20we%20propose%20a%20Text-Image%20Dual-Space%20Constraints%20%28TIDSC%29%0Amodule%2C%20enforcing%20the%20quality%20of%20generated%20images%20via%20positional%20consistency%0Aand%20spatial%20smoothing%20alignment.%20Extensive%20experiments%20demonstrate%20that%20PDIG%0Aachieves%20superior%20realism%20and%20diversity%20compared%20to%20state-of-the-art%20methods.%0ASpecifically%2C%20our%20approach%20improves%20Frechet%20Inception%20Distance%20%28FID%29%20by%2019.16%0Apoints%20over%20the%20second-best%20method%20and%20significantly%20enhances%20the%20performance%0Aof%20downstream%20defect%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhotovoltaic%2520Defect%2520Image%2520Generator%2520with%2520Boundary%2520Alignment%2520Smoothing%250A%2520%2520Constraint%2520for%2520Domain%2520Shift%2520Mitigation%26entry.906535625%3DDongying%2520Li%2520and%2520Binyi%2520Su%2520and%2520Hua%2520Zhang%2520and%2520Yong%2520Li%2520and%2520Haiyong%2520Chen%26entry.1292438233%3D%2520%2520Accurate%2520defect%2520detection%2520of%2520photovoltaic%2520%2528PV%2529%2520cells%2520is%2520critical%2520for%2520ensuring%250Aquality%2520and%2520efficiency%2520in%2520intelligent%2520PV%2520manufacturing%2520systems.%2520However%252C%2520the%250Ascarcity%2520of%2520rich%2520defect%2520data%2520poses%2520substantial%2520challenges%2520for%2520effective%2520model%250Atraining.%2520While%2520existing%2520methods%2520have%2520explored%2520generative%2520models%2520to%2520augment%250Adatasets%252C%2520they%2520often%2520suffer%2520from%2520instability%252C%2520limited%2520diversity%252C%2520and%2520domain%250Ashifts.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520PDIG%252C%2520a%2520Photovoltaic%2520Defect%2520Image%250AGenerator%2520based%2520on%2520Stable%2520Diffusion%2520%2528SD%2529.%2520PDIG%2520leverages%2520the%2520strong%2520priors%250Alearned%2520from%2520large-scale%2520datasets%2520to%2520enhance%2520generation%2520quality%2520under%2520limited%250Adata.%2520Specifically%252C%2520we%2520introduce%2520a%2520Semantic%2520Concept%2520Embedding%2520%2528SCE%2529%2520module%2520that%250Aincorporates%2520text-conditioned%2520priors%2520to%2520capture%2520the%2520relational%2520concepts%2520between%250Adefect%2520types%2520and%2520their%2520appearances.%2520To%2520further%2520enrich%2520the%2520domain%2520distribution%252C%250Awe%2520design%2520a%2520Lightweight%2520Industrial%2520Style%2520Adaptor%2520%2528LISA%2529%252C%2520which%2520injects%250Aindustrial%2520defect%2520characteristics%2520into%2520the%2520SD%2520model%2520through%2520cross-disentangled%250Aattention.%2520At%2520inference%252C%2520we%2520propose%2520a%2520Text-Image%2520Dual-Space%2520Constraints%2520%2528TIDSC%2529%250Amodule%252C%2520enforcing%2520the%2520quality%2520of%2520generated%2520images%2520via%2520positional%2520consistency%250Aand%2520spatial%2520smoothing%2520alignment.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PDIG%250Aachieves%2520superior%2520realism%2520and%2520diversity%2520compared%2520to%2520state-of-the-art%2520methods.%250ASpecifically%252C%2520our%2520approach%2520improves%2520Frechet%2520Inception%2520Distance%2520%2528FID%2529%2520by%252019.16%250Apoints%2520over%2520the%2520second-best%2520method%2520and%2520significantly%2520enhances%2520the%2520performance%250Aof%2520downstream%2520defect%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photovoltaic%20Defect%20Image%20Generator%20with%20Boundary%20Alignment%20Smoothing%0A%20%20Constraint%20for%20Domain%20Shift%20Mitigation&entry.906535625=Dongying%20Li%20and%20Binyi%20Su%20and%20Hua%20Zhang%20and%20Yong%20Li%20and%20Haiyong%20Chen&entry.1292438233=%20%20Accurate%20defect%20detection%20of%20photovoltaic%20%28PV%29%20cells%20is%20critical%20for%20ensuring%0Aquality%20and%20efficiency%20in%20intelligent%20PV%20manufacturing%20systems.%20However%2C%20the%0Ascarcity%20of%20rich%20defect%20data%20poses%20substantial%20challenges%20for%20effective%20model%0Atraining.%20While%20existing%20methods%20have%20explored%20generative%20models%20to%20augment%0Adatasets%2C%20they%20often%20suffer%20from%20instability%2C%20limited%20diversity%2C%20and%20domain%0Ashifts.%20To%20address%20these%20issues%2C%20we%20propose%20PDIG%2C%20a%20Photovoltaic%20Defect%20Image%0AGenerator%20based%20on%20Stable%20Diffusion%20%28SD%29.%20PDIG%20leverages%20the%20strong%20priors%0Alearned%20from%20large-scale%20datasets%20to%20enhance%20generation%20quality%20under%20limited%0Adata.%20Specifically%2C%20we%20introduce%20a%20Semantic%20Concept%20Embedding%20%28SCE%29%20module%20that%0Aincorporates%20text-conditioned%20priors%20to%20capture%20the%20relational%20concepts%20between%0Adefect%20types%20and%20their%20appearances.%20To%20further%20enrich%20the%20domain%20distribution%2C%0Awe%20design%20a%20Lightweight%20Industrial%20Style%20Adaptor%20%28LISA%29%2C%20which%20injects%0Aindustrial%20defect%20characteristics%20into%20the%20SD%20model%20through%20cross-disentangled%0Aattention.%20At%20inference%2C%20we%20propose%20a%20Text-Image%20Dual-Space%20Constraints%20%28TIDSC%29%0Amodule%2C%20enforcing%20the%20quality%20of%20generated%20images%20via%20positional%20consistency%0Aand%20spatial%20smoothing%20alignment.%20Extensive%20experiments%20demonstrate%20that%20PDIG%0Aachieves%20superior%20realism%20and%20diversity%20compared%20to%20state-of-the-art%20methods.%0ASpecifically%2C%20our%20approach%20improves%20Frechet%20Inception%20Distance%20%28FID%29%20by%2019.16%0Apoints%20over%20the%20second-best%20method%20and%20significantly%20enhances%20the%20performance%0Aof%20downstream%20defect%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06117v1&entry.124074799=Read"},
{"title": "Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles", "author": "Jiesong Lian and Yucong Huang and Chengdong Ma and Mingzhi Wang and Ying Wen and Long Hu and Yixue Hao", "abstract": "  For solving zero-sum games involving non-transitivity, a useful approach is\nto maintain a policy population to approximate the Nash Equilibrium (NE).\nPrevious studies have shown that the Policy Space Response Oracles (PSRO)\nalgorithm is an effective framework for solving such games. However, current\nmethods initialize a new policy from scratch or inherit a single historical\npolicy in Best Response (BR), missing the opportunity to leverage past policies\nto generate a better BR. In this paper, we propose Fusion-PSRO, which employs\nNash Policy Fusion to initialize a new policy for BR training. Nash Policy\nFusion serves as an implicit guiding policy that starts exploration on the\ncurrent Meta-NE, thus providing a closer approximation to BR. Moreover, it\ninsightfully captures a weighted moving average of past policies, dynamically\nadjusting these weights based on the Meta-NE in each iteration. This cumulative\nprocess further enhances the policy population. Empirical results on classic\nbenchmarks show that Fusion-PSRO achieves lower exploitability, thereby\nmitigating the shortcomings of previous research on policy initialization in\nBR.\n", "link": "http://arxiv.org/abs/2405.21027v5", "date": "2025-05-09", "relevancy": 0.824, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4142}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4113}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion-PSRO%3A%20Nash%20Policy%20Fusion%20for%20Policy%20Space%20Response%20Oracles&body=Title%3A%20Fusion-PSRO%3A%20Nash%20Policy%20Fusion%20for%20Policy%20Space%20Response%20Oracles%0AAuthor%3A%20Jiesong%20Lian%20and%20Yucong%20Huang%20and%20Chengdong%20Ma%20and%20Mingzhi%20Wang%20and%20Ying%20Wen%20and%20Long%20Hu%20and%20Yixue%20Hao%0AAbstract%3A%20%20%20For%20solving%20zero-sum%20games%20involving%20non-transitivity%2C%20a%20useful%20approach%20is%0Ato%20maintain%20a%20policy%20population%20to%20approximate%20the%20Nash%20Equilibrium%20%28NE%29.%0APrevious%20studies%20have%20shown%20that%20the%20Policy%20Space%20Response%20Oracles%20%28PSRO%29%0Aalgorithm%20is%20an%20effective%20framework%20for%20solving%20such%20games.%20However%2C%20current%0Amethods%20initialize%20a%20new%20policy%20from%20scratch%20or%20inherit%20a%20single%20historical%0Apolicy%20in%20Best%20Response%20%28BR%29%2C%20missing%20the%20opportunity%20to%20leverage%20past%20policies%0Ato%20generate%20a%20better%20BR.%20In%20this%20paper%2C%20we%20propose%20Fusion-PSRO%2C%20which%20employs%0ANash%20Policy%20Fusion%20to%20initialize%20a%20new%20policy%20for%20BR%20training.%20Nash%20Policy%0AFusion%20serves%20as%20an%20implicit%20guiding%20policy%20that%20starts%20exploration%20on%20the%0Acurrent%20Meta-NE%2C%20thus%20providing%20a%20closer%20approximation%20to%20BR.%20Moreover%2C%20it%0Ainsightfully%20captures%20a%20weighted%20moving%20average%20of%20past%20policies%2C%20dynamically%0Aadjusting%20these%20weights%20based%20on%20the%20Meta-NE%20in%20each%20iteration.%20This%20cumulative%0Aprocess%20further%20enhances%20the%20policy%20population.%20Empirical%20results%20on%20classic%0Abenchmarks%20show%20that%20Fusion-PSRO%20achieves%20lower%20exploitability%2C%20thereby%0Amitigating%20the%20shortcomings%20of%20previous%20research%20on%20policy%20initialization%20in%0ABR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21027v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion-PSRO%253A%2520Nash%2520Policy%2520Fusion%2520for%2520Policy%2520Space%2520Response%2520Oracles%26entry.906535625%3DJiesong%2520Lian%2520and%2520Yucong%2520Huang%2520and%2520Chengdong%2520Ma%2520and%2520Mingzhi%2520Wang%2520and%2520Ying%2520Wen%2520and%2520Long%2520Hu%2520and%2520Yixue%2520Hao%26entry.1292438233%3D%2520%2520For%2520solving%2520zero-sum%2520games%2520involving%2520non-transitivity%252C%2520a%2520useful%2520approach%2520is%250Ato%2520maintain%2520a%2520policy%2520population%2520to%2520approximate%2520the%2520Nash%2520Equilibrium%2520%2528NE%2529.%250APrevious%2520studies%2520have%2520shown%2520that%2520the%2520Policy%2520Space%2520Response%2520Oracles%2520%2528PSRO%2529%250Aalgorithm%2520is%2520an%2520effective%2520framework%2520for%2520solving%2520such%2520games.%2520However%252C%2520current%250Amethods%2520initialize%2520a%2520new%2520policy%2520from%2520scratch%2520or%2520inherit%2520a%2520single%2520historical%250Apolicy%2520in%2520Best%2520Response%2520%2528BR%2529%252C%2520missing%2520the%2520opportunity%2520to%2520leverage%2520past%2520policies%250Ato%2520generate%2520a%2520better%2520BR.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Fusion-PSRO%252C%2520which%2520employs%250ANash%2520Policy%2520Fusion%2520to%2520initialize%2520a%2520new%2520policy%2520for%2520BR%2520training.%2520Nash%2520Policy%250AFusion%2520serves%2520as%2520an%2520implicit%2520guiding%2520policy%2520that%2520starts%2520exploration%2520on%2520the%250Acurrent%2520Meta-NE%252C%2520thus%2520providing%2520a%2520closer%2520approximation%2520to%2520BR.%2520Moreover%252C%2520it%250Ainsightfully%2520captures%2520a%2520weighted%2520moving%2520average%2520of%2520past%2520policies%252C%2520dynamically%250Aadjusting%2520these%2520weights%2520based%2520on%2520the%2520Meta-NE%2520in%2520each%2520iteration.%2520This%2520cumulative%250Aprocess%2520further%2520enhances%2520the%2520policy%2520population.%2520Empirical%2520results%2520on%2520classic%250Abenchmarks%2520show%2520that%2520Fusion-PSRO%2520achieves%2520lower%2520exploitability%252C%2520thereby%250Amitigating%2520the%2520shortcomings%2520of%2520previous%2520research%2520on%2520policy%2520initialization%2520in%250ABR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21027v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion-PSRO%3A%20Nash%20Policy%20Fusion%20for%20Policy%20Space%20Response%20Oracles&entry.906535625=Jiesong%20Lian%20and%20Yucong%20Huang%20and%20Chengdong%20Ma%20and%20Mingzhi%20Wang%20and%20Ying%20Wen%20and%20Long%20Hu%20and%20Yixue%20Hao&entry.1292438233=%20%20For%20solving%20zero-sum%20games%20involving%20non-transitivity%2C%20a%20useful%20approach%20is%0Ato%20maintain%20a%20policy%20population%20to%20approximate%20the%20Nash%20Equilibrium%20%28NE%29.%0APrevious%20studies%20have%20shown%20that%20the%20Policy%20Space%20Response%20Oracles%20%28PSRO%29%0Aalgorithm%20is%20an%20effective%20framework%20for%20solving%20such%20games.%20However%2C%20current%0Amethods%20initialize%20a%20new%20policy%20from%20scratch%20or%20inherit%20a%20single%20historical%0Apolicy%20in%20Best%20Response%20%28BR%29%2C%20missing%20the%20opportunity%20to%20leverage%20past%20policies%0Ato%20generate%20a%20better%20BR.%20In%20this%20paper%2C%20we%20propose%20Fusion-PSRO%2C%20which%20employs%0ANash%20Policy%20Fusion%20to%20initialize%20a%20new%20policy%20for%20BR%20training.%20Nash%20Policy%0AFusion%20serves%20as%20an%20implicit%20guiding%20policy%20that%20starts%20exploration%20on%20the%0Acurrent%20Meta-NE%2C%20thus%20providing%20a%20closer%20approximation%20to%20BR.%20Moreover%2C%20it%0Ainsightfully%20captures%20a%20weighted%20moving%20average%20of%20past%20policies%2C%20dynamically%0Aadjusting%20these%20weights%20based%20on%20the%20Meta-NE%20in%20each%20iteration.%20This%20cumulative%0Aprocess%20further%20enhances%20the%20policy%20population.%20Empirical%20results%20on%20classic%0Abenchmarks%20show%20that%20Fusion-PSRO%20achieves%20lower%20exploitability%2C%20thereby%0Amitigating%20the%20shortcomings%20of%20previous%20research%20on%20policy%20initialization%20in%0ABR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21027v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


