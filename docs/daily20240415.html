<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240414.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Unsupervised Learning of Group Invariant and Equivariant Representations", "author": "Robin Winter and Marco Bertolini and Tuan Le and Frank No\u00e9 and Djork-Arn\u00e9 Clevert", "abstract": "  Equivariant neural networks, whose hidden features transform according to\nrepresentations of a group G acting on the data, exhibit training efficiency\nand an improved generalisation performance. In this work, we extend group\ninvariant and equivariant representation learning to the field of unsupervised\ndeep learning. We propose a general learning strategy based on an\nencoder-decoder framework in which the latent representation is separated in an\ninvariant term and an equivariant group action component. The key idea is that\nthe network learns to encode and decode data to and from a group-invariant\nrepresentation by additionally learning to predict the appropriate group action\nto align input and output pose to solve the reconstruction task. We derive the\nnecessary conditions on the equivariant encoder, and we present a construction\nvalid for any G, both discrete and continuous. We describe explicitly our\nconstruction for rotations, translations and permutations. We test the validity\nand the robustness of our approach in a variety of experiments with diverse\ndata types employing different network architectures.\n", "link": "http://arxiv.org/abs/2202.07559v3", "date": "2024-04-12", "relevancy": 2.679, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5624}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20of%20Group%20Invariant%20and%20Equivariant%20Representations&body=Title%3A%20Unsupervised%20Learning%20of%20Group%20Invariant%20and%20Equivariant%20Representations%0AAuthor%3A%20Robin%20Winter%20and%20Marco%20Bertolini%20and%20Tuan%20Le%20and%20Frank%20No%C3%A9%20and%20Djork-Arn%C3%A9%20Clevert%0AAbstract%3A%20%20%20Equivariant%20neural%20networks%2C%20whose%20hidden%20features%20transform%20according%20to%0Arepresentations%20of%20a%20group%20G%20acting%20on%20the%20data%2C%20exhibit%20training%20efficiency%0Aand%20an%20improved%20generalisation%20performance.%20In%20this%20work%2C%20we%20extend%20group%0Ainvariant%20and%20equivariant%20representation%20learning%20to%20the%20field%20of%20unsupervised%0Adeep%20learning.%20We%20propose%20a%20general%20learning%20strategy%20based%20on%20an%0Aencoder-decoder%20framework%20in%20which%20the%20latent%20representation%20is%20separated%20in%20an%0Ainvariant%20term%20and%20an%20equivariant%20group%20action%20component.%20The%20key%20idea%20is%20that%0Athe%20network%20learns%20to%20encode%20and%20decode%20data%20to%20and%20from%20a%20group-invariant%0Arepresentation%20by%20additionally%20learning%20to%20predict%20the%20appropriate%20group%20action%0Ato%20align%20input%20and%20output%20pose%20to%20solve%20the%20reconstruction%20task.%20We%20derive%20the%0Anecessary%20conditions%20on%20the%20equivariant%20encoder%2C%20and%20we%20present%20a%20construction%0Avalid%20for%20any%20G%2C%20both%20discrete%20and%20continuous.%20We%20describe%20explicitly%20our%0Aconstruction%20for%20rotations%2C%20translations%20and%20permutations.%20We%20test%20the%20validity%0Aand%20the%20robustness%20of%20our%20approach%20in%20a%20variety%20of%20experiments%20with%20diverse%0Adata%20types%20employing%20different%20network%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.07559v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20of%20Group%20Invariant%20and%20Equivariant%20Representations&entry.906535625=Robin%20Winter%20and%20Marco%20Bertolini%20and%20Tuan%20Le%20and%20Frank%20No%C3%A9%20and%20Djork-Arn%C3%A9%20Clevert&entry.1292438233=%20%20Equivariant%20neural%20networks%2C%20whose%20hidden%20features%20transform%20according%20to%0Arepresentations%20of%20a%20group%20G%20acting%20on%20the%20data%2C%20exhibit%20training%20efficiency%0Aand%20an%20improved%20generalisation%20performance.%20In%20this%20work%2C%20we%20extend%20group%0Ainvariant%20and%20equivariant%20representation%20learning%20to%20the%20field%20of%20unsupervised%0Adeep%20learning.%20We%20propose%20a%20general%20learning%20strategy%20based%20on%20an%0Aencoder-decoder%20framework%20in%20which%20the%20latent%20representation%20is%20separated%20in%20an%0Ainvariant%20term%20and%20an%20equivariant%20group%20action%20component.%20The%20key%20idea%20is%20that%0Athe%20network%20learns%20to%20encode%20and%20decode%20data%20to%20and%20from%20a%20group-invariant%0Arepresentation%20by%20additionally%20learning%20to%20predict%20the%20appropriate%20group%20action%0Ato%20align%20input%20and%20output%20pose%20to%20solve%20the%20reconstruction%20task.%20We%20derive%20the%0Anecessary%20conditions%20on%20the%20equivariant%20encoder%2C%20and%20we%20present%20a%20construction%0Avalid%20for%20any%20G%2C%20both%20discrete%20and%20continuous.%20We%20describe%20explicitly%20our%0Aconstruction%20for%20rotations%2C%20translations%20and%20permutations.%20We%20test%20the%20validity%0Aand%20the%20robustness%20of%20our%20approach%20in%20a%20variety%20of%20experiments%20with%20diverse%0Adata%20types%20employing%20different%20network%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.07559v3&entry.124074799=Read"},
{"title": "No Bells, Just Whistles: Sports Field Registration by Leveraging\n  Geometric Properties", "author": "Marc Guti\u00e9rrez-P\u00e9rez and Antonio Agudo", "abstract": "  Broadcast sports field registration is traditionally addressed as a\nhomography estimation task, mapping the visible image area to a planar field\nmodel, predominantly focusing on the main camera shot. Addressing the\nshortcomings of previous approaches, we propose a novel calibration pipeline\nenabling camera calibration using a 3D soccer field model and extending the\nprocess to assess the multiple-view nature of broadcast videos. Our approach\nbegins with a keypoint generation pipeline derived from SoccerNet dataset\nannotations, leveraging the geometric properties of the court. Subsequently, we\nexecute classical camera calibration through DLT algorithm in a minimalist\nfashion, without further refinement. Through extensive experimentation on\nreal-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup\n2014 and TS- WorldCup, our method demonstrates superior performance in both\nmultiple- and single-view 3D camera calibration while maintaining competitive\nresults in homography estimation compared to state-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2404.08401v1", "date": "2024-04-12", "relevancy": 2.6491, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5072}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20No%20Bells%2C%20Just%20Whistles%3A%20Sports%20Field%20Registration%20by%20Leveraging%0A%20%20Geometric%20Properties&body=Title%3A%20No%20Bells%2C%20Just%20Whistles%3A%20Sports%20Field%20Registration%20by%20Leveraging%0A%20%20Geometric%20Properties%0AAuthor%3A%20Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20Broadcast%20sports%20field%20registration%20is%20traditionally%20addressed%20as%20a%0Ahomography%20estimation%20task%2C%20mapping%20the%20visible%20image%20area%20to%20a%20planar%20field%0Amodel%2C%20predominantly%20focusing%20on%20the%20main%20camera%20shot.%20Addressing%20the%0Ashortcomings%20of%20previous%20approaches%2C%20we%20propose%20a%20novel%20calibration%20pipeline%0Aenabling%20camera%20calibration%20using%20a%203D%20soccer%20field%20model%20and%20extending%20the%0Aprocess%20to%20assess%20the%20multiple-view%20nature%20of%20broadcast%20videos.%20Our%20approach%0Abegins%20with%20a%20keypoint%20generation%20pipeline%20derived%20from%20SoccerNet%20dataset%0Aannotations%2C%20leveraging%20the%20geometric%20properties%20of%20the%20court.%20Subsequently%2C%20we%0Aexecute%20classical%20camera%20calibration%20through%20DLT%20algorithm%20in%20a%20minimalist%0Afashion%2C%20without%20further%20refinement.%20Through%20extensive%20experimentation%20on%0Areal-world%20soccer%20broadcast%20datasets%20such%20as%20SoccerNet-Calibration%2C%20WorldCup%0A2014%20and%20TS-%20WorldCup%2C%20our%20method%20demonstrates%20superior%20performance%20in%20both%0Amultiple-%20and%20single-view%203D%20camera%20calibration%20while%20maintaining%20competitive%0Aresults%20in%20homography%20estimation%20compared%20to%20state-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08401v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Bells%2C%20Just%20Whistles%3A%20Sports%20Field%20Registration%20by%20Leveraging%0A%20%20Geometric%20Properties&entry.906535625=Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo&entry.1292438233=%20%20Broadcast%20sports%20field%20registration%20is%20traditionally%20addressed%20as%20a%0Ahomography%20estimation%20task%2C%20mapping%20the%20visible%20image%20area%20to%20a%20planar%20field%0Amodel%2C%20predominantly%20focusing%20on%20the%20main%20camera%20shot.%20Addressing%20the%0Ashortcomings%20of%20previous%20approaches%2C%20we%20propose%20a%20novel%20calibration%20pipeline%0Aenabling%20camera%20calibration%20using%20a%203D%20soccer%20field%20model%20and%20extending%20the%0Aprocess%20to%20assess%20the%20multiple-view%20nature%20of%20broadcast%20videos.%20Our%20approach%0Abegins%20with%20a%20keypoint%20generation%20pipeline%20derived%20from%20SoccerNet%20dataset%0Aannotations%2C%20leveraging%20the%20geometric%20properties%20of%20the%20court.%20Subsequently%2C%20we%0Aexecute%20classical%20camera%20calibration%20through%20DLT%20algorithm%20in%20a%20minimalist%0Afashion%2C%20without%20further%20refinement.%20Through%20extensive%20experimentation%20on%0Areal-world%20soccer%20broadcast%20datasets%20such%20as%20SoccerNet-Calibration%2C%20WorldCup%0A2014%20and%20TS-%20WorldCup%2C%20our%20method%20demonstrates%20superior%20performance%20in%20both%0Amultiple-%20and%20single-view%203D%20camera%20calibration%20while%20maintaining%20competitive%0Aresults%20in%20homography%20estimation%20compared%20to%20state-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08401v1&entry.124074799=Read"},
{"title": "NC-TTT: A Noise Contrastive Approach for Test-Time Training", "author": "David Osowiechi and Gustavo A. Vargas Hakim and Mehrdad Noori and Milad Cheraghalikhani and Ali Bahri and Moslem Yazdanpanah and Ismail Ben Ayed and Christian Desrosiers", "abstract": "  Despite their exceptional performance in vision tasks, deep learning models\noften struggle when faced with domain shifts during testing. Test-Time Training\n(TTT) methods have recently gained popularity by their ability to enhance the\nrobustness of models through the addition of an auxiliary objective that is\njointly optimized with the main task. Being strictly unsupervised, this\nauxiliary objective is used at test time to adapt the model without any access\nto labels. In this work, we propose Noise-Contrastive Test-Time Training\n(NC-TTT), a novel unsupervised TTT technique based on the discrimination of\nnoisy feature maps. By learning to classify noisy views of projected feature\nmaps, and then adapting the model accordingly on new domains, classification\nperformance can be recovered by an important margin. Experiments on several\npopular test-time adaptation baselines demonstrate the advantages of our method\ncompared to recent approaches for this task. The code can be found\nat:https://github.com/GustavoVargasHakim/NCTTT.git\n", "link": "http://arxiv.org/abs/2404.08392v1", "date": "2024-04-12", "relevancy": 2.6413, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5486}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NC-TTT%3A%20A%20Noise%20Contrastive%20Approach%20for%20Test-Time%20Training&body=Title%3A%20NC-TTT%3A%20A%20Noise%20Contrastive%20Approach%20for%20Test-Time%20Training%0AAuthor%3A%20David%20Osowiechi%20and%20Gustavo%20A.%20Vargas%20Hakim%20and%20Mehrdad%20Noori%20and%20Milad%20Cheraghalikhani%20and%20Ali%20Bahri%20and%20Moslem%20Yazdanpanah%20and%20Ismail%20Ben%20Ayed%20and%20Christian%20Desrosiers%0AAbstract%3A%20%20%20Despite%20their%20exceptional%20performance%20in%20vision%20tasks%2C%20deep%20learning%20models%0Aoften%20struggle%20when%20faced%20with%20domain%20shifts%20during%20testing.%20Test-Time%20Training%0A%28TTT%29%20methods%20have%20recently%20gained%20popularity%20by%20their%20ability%20to%20enhance%20the%0Arobustness%20of%20models%20through%20the%20addition%20of%20an%20auxiliary%20objective%20that%20is%0Ajointly%20optimized%20with%20the%20main%20task.%20Being%20strictly%20unsupervised%2C%20this%0Aauxiliary%20objective%20is%20used%20at%20test%20time%20to%20adapt%20the%20model%20without%20any%20access%0Ato%20labels.%20In%20this%20work%2C%20we%20propose%20Noise-Contrastive%20Test-Time%20Training%0A%28NC-TTT%29%2C%20a%20novel%20unsupervised%20TTT%20technique%20based%20on%20the%20discrimination%20of%0Anoisy%20feature%20maps.%20By%20learning%20to%20classify%20noisy%20views%20of%20projected%20feature%0Amaps%2C%20and%20then%20adapting%20the%20model%20accordingly%20on%20new%20domains%2C%20classification%0Aperformance%20can%20be%20recovered%20by%20an%20important%20margin.%20Experiments%20on%20several%0Apopular%20test-time%20adaptation%20baselines%20demonstrate%20the%20advantages%20of%20our%20method%0Acompared%20to%20recent%20approaches%20for%20this%20task.%20The%20code%20can%20be%20found%0Aat%3Ahttps%3A//github.com/GustavoVargasHakim/NCTTT.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08392v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NC-TTT%3A%20A%20Noise%20Contrastive%20Approach%20for%20Test-Time%20Training&entry.906535625=David%20Osowiechi%20and%20Gustavo%20A.%20Vargas%20Hakim%20and%20Mehrdad%20Noori%20and%20Milad%20Cheraghalikhani%20and%20Ali%20Bahri%20and%20Moslem%20Yazdanpanah%20and%20Ismail%20Ben%20Ayed%20and%20Christian%20Desrosiers&entry.1292438233=%20%20Despite%20their%20exceptional%20performance%20in%20vision%20tasks%2C%20deep%20learning%20models%0Aoften%20struggle%20when%20faced%20with%20domain%20shifts%20during%20testing.%20Test-Time%20Training%0A%28TTT%29%20methods%20have%20recently%20gained%20popularity%20by%20their%20ability%20to%20enhance%20the%0Arobustness%20of%20models%20through%20the%20addition%20of%20an%20auxiliary%20objective%20that%20is%0Ajointly%20optimized%20with%20the%20main%20task.%20Being%20strictly%20unsupervised%2C%20this%0Aauxiliary%20objective%20is%20used%20at%20test%20time%20to%20adapt%20the%20model%20without%20any%20access%0Ato%20labels.%20In%20this%20work%2C%20we%20propose%20Noise-Contrastive%20Test-Time%20Training%0A%28NC-TTT%29%2C%20a%20novel%20unsupervised%20TTT%20technique%20based%20on%20the%20discrimination%20of%0Anoisy%20feature%20maps.%20By%20learning%20to%20classify%20noisy%20views%20of%20projected%20feature%0Amaps%2C%20and%20then%20adapting%20the%20model%20accordingly%20on%20new%20domains%2C%20classification%0Aperformance%20can%20be%20recovered%20by%20an%20important%20margin.%20Experiments%20on%20several%0Apopular%20test-time%20adaptation%20baselines%20demonstrate%20the%20advantages%20of%20our%20method%0Acompared%20to%20recent%20approaches%20for%20this%20task.%20The%20code%20can%20be%20found%0Aat%3Ahttps%3A//github.com/GustavoVargasHakim/NCTTT.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08392v1&entry.124074799=Read"},
{"title": "Adversarial Imitation Learning via Boosting", "author": "Jonathan D. Chang and Dhruv Sreenivas and Yingbing Huang and Kiant\u00e9 Brantley and Wen Sun", "abstract": "  Adversarial imitation learning (AIL) has stood out as a dominant framework\nacross various imitation learning (IL) applications, with Discriminator Actor\nCritic (DAC) (Kostrikov et al.,, 2019) demonstrating the effectiveness of\noff-policy learning algorithms in improving sample efficiency and scalability\nto higher-dimensional observations. Despite DAC's empirical success, the\noriginal AIL objective is on-policy and DAC's ad-hoc application of off-policy\ntraining does not guarantee successful imitation (Kostrikov et al., 2019;\n2020). Follow-up work such as ValueDICE (Kostrikov et al., 2020) tackles this\nissue by deriving a fully off-policy AIL objective. Instead in this work, we\ndevelop a novel and principled AIL algorithm via the framework of boosting.\nLike boosting, our new algorithm, AILBoost, maintains an ensemble of properly\nweighted weak learners (i.e., policies) and trains a discriminator that\nwitnesses the maximum discrepancy between the distributions of the ensemble and\nthe expert policy. We maintain a weighted replay buffer to represent the\nstate-action distribution induced by the ensemble, allowing us to train\ndiscriminators using the entire data collected so far. In the weighted replay\nbuffer, the contribution of the data from older policies are properly\ndiscounted with the weight computed based on the boosting framework.\nEmpirically, we evaluate our algorithm on both controller state-based and\npixel-based environments from the DeepMind Control Suite. AILBoost outperforms\nDAC on both types of environments, demonstrating the benefit of properly\nweighting replay buffer data for off-policy training. On state-based\nenvironments, DAC outperforms ValueDICE and IQ-Learn (Gary et al., 2021),\nachieving competitive performance with as little as one expert trajectory.\n", "link": "http://arxiv.org/abs/2404.08513v1", "date": "2024-04-12", "relevancy": 2.6013, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5313}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5197}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5097}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Imitation%20Learning%20via%20Boosting&body=Title%3A%20Adversarial%20Imitation%20Learning%20via%20Boosting%0AAuthor%3A%20Jonathan%20D.%20Chang%20and%20Dhruv%20Sreenivas%20and%20Yingbing%20Huang%20and%20Kiant%C3%A9%20Brantley%20and%20Wen%20Sun%0AAbstract%3A%20%20%20Adversarial%20imitation%20learning%20%28AIL%29%20has%20stood%20out%20as%20a%20dominant%20framework%0Aacross%20various%20imitation%20learning%20%28IL%29%20applications%2C%20with%20Discriminator%20Actor%0ACritic%20%28DAC%29%20%28Kostrikov%20et%20al.%2C%2C%202019%29%20demonstrating%20the%20effectiveness%20of%0Aoff-policy%20learning%20algorithms%20in%20improving%20sample%20efficiency%20and%20scalability%0Ato%20higher-dimensional%20observations.%20Despite%20DAC%27s%20empirical%20success%2C%20the%0Aoriginal%20AIL%20objective%20is%20on-policy%20and%20DAC%27s%20ad-hoc%20application%20of%20off-policy%0Atraining%20does%20not%20guarantee%20successful%20imitation%20%28Kostrikov%20et%20al.%2C%202019%3B%0A2020%29.%20Follow-up%20work%20such%20as%20ValueDICE%20%28Kostrikov%20et%20al.%2C%202020%29%20tackles%20this%0Aissue%20by%20deriving%20a%20fully%20off-policy%20AIL%20objective.%20Instead%20in%20this%20work%2C%20we%0Adevelop%20a%20novel%20and%20principled%20AIL%20algorithm%20via%20the%20framework%20of%20boosting.%0ALike%20boosting%2C%20our%20new%20algorithm%2C%20AILBoost%2C%20maintains%20an%20ensemble%20of%20properly%0Aweighted%20weak%20learners%20%28i.e.%2C%20policies%29%20and%20trains%20a%20discriminator%20that%0Awitnesses%20the%20maximum%20discrepancy%20between%20the%20distributions%20of%20the%20ensemble%20and%0Athe%20expert%20policy.%20We%20maintain%20a%20weighted%20replay%20buffer%20to%20represent%20the%0Astate-action%20distribution%20induced%20by%20the%20ensemble%2C%20allowing%20us%20to%20train%0Adiscriminators%20using%20the%20entire%20data%20collected%20so%20far.%20In%20the%20weighted%20replay%0Abuffer%2C%20the%20contribution%20of%20the%20data%20from%20older%20policies%20are%20properly%0Adiscounted%20with%20the%20weight%20computed%20based%20on%20the%20boosting%20framework.%0AEmpirically%2C%20we%20evaluate%20our%20algorithm%20on%20both%20controller%20state-based%20and%0Apixel-based%20environments%20from%20the%20DeepMind%20Control%20Suite.%20AILBoost%20outperforms%0ADAC%20on%20both%20types%20of%20environments%2C%20demonstrating%20the%20benefit%20of%20properly%0Aweighting%20replay%20buffer%20data%20for%20off-policy%20training.%20On%20state-based%0Aenvironments%2C%20DAC%20outperforms%20ValueDICE%20and%20IQ-Learn%20%28Gary%20et%20al.%2C%202021%29%2C%0Aachieving%20competitive%20performance%20with%20as%20little%20as%20one%20expert%20trajectory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08513v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Imitation%20Learning%20via%20Boosting&entry.906535625=Jonathan%20D.%20Chang%20and%20Dhruv%20Sreenivas%20and%20Yingbing%20Huang%20and%20Kiant%C3%A9%20Brantley%20and%20Wen%20Sun&entry.1292438233=%20%20Adversarial%20imitation%20learning%20%28AIL%29%20has%20stood%20out%20as%20a%20dominant%20framework%0Aacross%20various%20imitation%20learning%20%28IL%29%20applications%2C%20with%20Discriminator%20Actor%0ACritic%20%28DAC%29%20%28Kostrikov%20et%20al.%2C%2C%202019%29%20demonstrating%20the%20effectiveness%20of%0Aoff-policy%20learning%20algorithms%20in%20improving%20sample%20efficiency%20and%20scalability%0Ato%20higher-dimensional%20observations.%20Despite%20DAC%27s%20empirical%20success%2C%20the%0Aoriginal%20AIL%20objective%20is%20on-policy%20and%20DAC%27s%20ad-hoc%20application%20of%20off-policy%0Atraining%20does%20not%20guarantee%20successful%20imitation%20%28Kostrikov%20et%20al.%2C%202019%3B%0A2020%29.%20Follow-up%20work%20such%20as%20ValueDICE%20%28Kostrikov%20et%20al.%2C%202020%29%20tackles%20this%0Aissue%20by%20deriving%20a%20fully%20off-policy%20AIL%20objective.%20Instead%20in%20this%20work%2C%20we%0Adevelop%20a%20novel%20and%20principled%20AIL%20algorithm%20via%20the%20framework%20of%20boosting.%0ALike%20boosting%2C%20our%20new%20algorithm%2C%20AILBoost%2C%20maintains%20an%20ensemble%20of%20properly%0Aweighted%20weak%20learners%20%28i.e.%2C%20policies%29%20and%20trains%20a%20discriminator%20that%0Awitnesses%20the%20maximum%20discrepancy%20between%20the%20distributions%20of%20the%20ensemble%20and%0Athe%20expert%20policy.%20We%20maintain%20a%20weighted%20replay%20buffer%20to%20represent%20the%0Astate-action%20distribution%20induced%20by%20the%20ensemble%2C%20allowing%20us%20to%20train%0Adiscriminators%20using%20the%20entire%20data%20collected%20so%20far.%20In%20the%20weighted%20replay%0Abuffer%2C%20the%20contribution%20of%20the%20data%20from%20older%20policies%20are%20properly%0Adiscounted%20with%20the%20weight%20computed%20based%20on%20the%20boosting%20framework.%0AEmpirically%2C%20we%20evaluate%20our%20algorithm%20on%20both%20controller%20state-based%20and%0Apixel-based%20environments%20from%20the%20DeepMind%20Control%20Suite.%20AILBoost%20outperforms%0ADAC%20on%20both%20types%20of%20environments%2C%20demonstrating%20the%20benefit%20of%20properly%0Aweighting%20replay%20buffer%20data%20for%20off-policy%20training.%20On%20state-based%0Aenvironments%2C%20DAC%20outperforms%20ValueDICE%20and%20IQ-Learn%20%28Gary%20et%20al.%2C%202021%29%2C%0Aachieving%20competitive%20performance%20with%20as%20little%20as%20one%20expert%20trajectory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08513v1&entry.124074799=Read"},
{"title": "AdapterSwap: Continuous Training of LLMs with Data Removal and\n  Access-Control Guarantees", "author": "William Fleshman and Aleem Khan and Marc Marone and Benjamin Van Durme", "abstract": "  Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.\n", "link": "http://arxiv.org/abs/2404.08417v1", "date": "2024-04-12", "relevancy": 2.5528, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5779}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4822}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4716}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AdapterSwap%3A%20Continuous%20Training%20of%20LLMs%20with%20Data%20Removal%20and%0A%20%20Access-Control%20Guarantees&body=Title%3A%20AdapterSwap%3A%20Continuous%20Training%20of%20LLMs%20with%20Data%20Removal%20and%0A%20%20Access-Control%20Guarantees%0AAuthor%3A%20William%20Fleshman%20and%20Aleem%20Khan%20and%20Marc%20Marone%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20capable%20of%20completing%20knowledge%0Aintensive%20tasks%20by%20recalling%20information%20from%20a%20static%20pretraining%20corpus.%20Here%0Awe%20are%20concerned%20with%20LLMs%20in%20the%20context%20of%20evolving%20data%20requirements.%20For%0Ainstance%3A%20batches%20of%20new%20data%20that%20are%20introduced%20periodically%3B%20subsets%20of%20data%0Awith%20user-based%20access%20controls%3B%20or%20requirements%20on%20dynamic%20removal%20of%0Adocuments%20with%20guarantees%20that%20associated%20knowledge%20cannot%20be%20recalled.%20We%20wish%0Ato%20satisfy%20these%20requirements%20while%20at%20the%20same%20time%20ensuring%20a%20model%20does%20not%0Aforget%20old%20information%20when%20new%20data%20becomes%20available.%20To%20address%20these%0Aissues%2C%20we%20introduce%20AdapterSwap%2C%20a%20training%20and%20inference%20scheme%20that%0Aorganizes%20knowledge%20from%20a%20data%20collection%20into%20a%20set%20of%20low-rank%20adapters%2C%0Awhich%20are%20dynamically%20composed%20during%20inference.%20Our%20experiments%20demonstrate%0AAdapterSwap%27s%20ability%20to%20support%20efficient%20continual%20learning%2C%20while%20also%0Aenabling%20organizations%20to%20have%20fine-grained%20control%20over%20data%20access%20and%0Adeletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08417v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdapterSwap%3A%20Continuous%20Training%20of%20LLMs%20with%20Data%20Removal%20and%0A%20%20Access-Control%20Guarantees&entry.906535625=William%20Fleshman%20and%20Aleem%20Khan%20and%20Marc%20Marone%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20capable%20of%20completing%20knowledge%0Aintensive%20tasks%20by%20recalling%20information%20from%20a%20static%20pretraining%20corpus.%20Here%0Awe%20are%20concerned%20with%20LLMs%20in%20the%20context%20of%20evolving%20data%20requirements.%20For%0Ainstance%3A%20batches%20of%20new%20data%20that%20are%20introduced%20periodically%3B%20subsets%20of%20data%0Awith%20user-based%20access%20controls%3B%20or%20requirements%20on%20dynamic%20removal%20of%0Adocuments%20with%20guarantees%20that%20associated%20knowledge%20cannot%20be%20recalled.%20We%20wish%0Ato%20satisfy%20these%20requirements%20while%20at%20the%20same%20time%20ensuring%20a%20model%20does%20not%0Aforget%20old%20information%20when%20new%20data%20becomes%20available.%20To%20address%20these%0Aissues%2C%20we%20introduce%20AdapterSwap%2C%20a%20training%20and%20inference%20scheme%20that%0Aorganizes%20knowledge%20from%20a%20data%20collection%20into%20a%20set%20of%20low-rank%20adapters%2C%0Awhich%20are%20dynamically%20composed%20during%20inference.%20Our%20experiments%20demonstrate%0AAdapterSwap%27s%20ability%20to%20support%20efficient%20continual%20learning%2C%20while%20also%0Aenabling%20organizations%20to%20have%20fine-grained%20control%20over%20data%20access%20and%0Adeletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08417v1&entry.124074799=Read"},
{"title": "Enhancing Visual Question Answering through Question-Driven Image\n  Captions as Prompts", "author": "\u00d6vg\u00fc \u00d6zdemir and Erdem Akag\u00fcnd\u00fcz", "abstract": "  Visual question answering (VQA) is known as an AI-complete task as it\nrequires understanding, reasoning, and inferring about the vision and the\nlanguage content. Over the past few years, numerous neural architectures have\nbeen suggested for the VQA problem. However, achieving success in zero-shot VQA\nremains a challenge due to its requirement for advanced generalization and\nreasoning skills. This study explores the impact of incorporating image\ncaptioning as an intermediary process within the VQA pipeline. Specifically, we\nexplore the efficacy of utilizing image captions instead of images and\nleveraging large language models (LLMs) to establish a zero-shot setting. Since\nimage captioning is the most crucial step in this process, we compare the\nimpact of state-of-the-art image captioning models on VQA performance across\nvarious question types in terms of structure and semantics. We propose a\nstraightforward and efficient question-driven image captioning approach within\nthis pipeline to transfer contextual information into the question-answering\n(QA) model. This method involves extracting keywords from the question,\ngenerating a caption for each image-question pair using the keywords, and\nincorporating the question-driven caption into the LLM prompt. We evaluate the\nefficacy of using general-purpose and question-driven image captions in the VQA\npipeline. Our study highlights the potential of employing image captions and\nharnessing the capabilities of LLMs to achieve competitive performance on GQA\nunder the zero-shot setting. Our code is available at\n\\url{https://github.com/ovguyo/captions-in-VQA}.\n", "link": "http://arxiv.org/abs/2404.08589v1", "date": "2024-04-12", "relevancy": 2.5337, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Visual%20Question%20Answering%20through%20Question-Driven%20Image%0A%20%20Captions%20as%20Prompts&body=Title%3A%20Enhancing%20Visual%20Question%20Answering%20through%20Question-Driven%20Image%0A%20%20Captions%20as%20Prompts%0AAuthor%3A%20%C3%96vg%C3%BC%20%C3%96zdemir%20and%20Erdem%20Akag%C3%BCnd%C3%BCz%0AAbstract%3A%20%20%20Visual%20question%20answering%20%28VQA%29%20is%20known%20as%20an%20AI-complete%20task%20as%20it%0Arequires%20understanding%2C%20reasoning%2C%20and%20inferring%20about%20the%20vision%20and%20the%0Alanguage%20content.%20Over%20the%20past%20few%20years%2C%20numerous%20neural%20architectures%20have%0Abeen%20suggested%20for%20the%20VQA%20problem.%20However%2C%20achieving%20success%20in%20zero-shot%20VQA%0Aremains%20a%20challenge%20due%20to%20its%20requirement%20for%20advanced%20generalization%20and%0Areasoning%20skills.%20This%20study%20explores%20the%20impact%20of%20incorporating%20image%0Acaptioning%20as%20an%20intermediary%20process%20within%20the%20VQA%20pipeline.%20Specifically%2C%20we%0Aexplore%20the%20efficacy%20of%20utilizing%20image%20captions%20instead%20of%20images%20and%0Aleveraging%20large%20language%20models%20%28LLMs%29%20to%20establish%20a%20zero-shot%20setting.%20Since%0Aimage%20captioning%20is%20the%20most%20crucial%20step%20in%20this%20process%2C%20we%20compare%20the%0Aimpact%20of%20state-of-the-art%20image%20captioning%20models%20on%20VQA%20performance%20across%0Avarious%20question%20types%20in%20terms%20of%20structure%20and%20semantics.%20We%20propose%20a%0Astraightforward%20and%20efficient%20question-driven%20image%20captioning%20approach%20within%0Athis%20pipeline%20to%20transfer%20contextual%20information%20into%20the%20question-answering%0A%28QA%29%20model.%20This%20method%20involves%20extracting%20keywords%20from%20the%20question%2C%0Agenerating%20a%20caption%20for%20each%20image-question%20pair%20using%20the%20keywords%2C%20and%0Aincorporating%20the%20question-driven%20caption%20into%20the%20LLM%20prompt.%20We%20evaluate%20the%0Aefficacy%20of%20using%20general-purpose%20and%20question-driven%20image%20captions%20in%20the%20VQA%0Apipeline.%20Our%20study%20highlights%20the%20potential%20of%20employing%20image%20captions%20and%0Aharnessing%20the%20capabilities%20of%20LLMs%20to%20achieve%20competitive%20performance%20on%20GQA%0Aunder%20the%20zero-shot%20setting.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ovguyo/captions-in-VQA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Visual%20Question%20Answering%20through%20Question-Driven%20Image%0A%20%20Captions%20as%20Prompts&entry.906535625=%C3%96vg%C3%BC%20%C3%96zdemir%20and%20Erdem%20Akag%C3%BCnd%C3%BCz&entry.1292438233=%20%20Visual%20question%20answering%20%28VQA%29%20is%20known%20as%20an%20AI-complete%20task%20as%20it%0Arequires%20understanding%2C%20reasoning%2C%20and%20inferring%20about%20the%20vision%20and%20the%0Alanguage%20content.%20Over%20the%20past%20few%20years%2C%20numerous%20neural%20architectures%20have%0Abeen%20suggested%20for%20the%20VQA%20problem.%20However%2C%20achieving%20success%20in%20zero-shot%20VQA%0Aremains%20a%20challenge%20due%20to%20its%20requirement%20for%20advanced%20generalization%20and%0Areasoning%20skills.%20This%20study%20explores%20the%20impact%20of%20incorporating%20image%0Acaptioning%20as%20an%20intermediary%20process%20within%20the%20VQA%20pipeline.%20Specifically%2C%20we%0Aexplore%20the%20efficacy%20of%20utilizing%20image%20captions%20instead%20of%20images%20and%0Aleveraging%20large%20language%20models%20%28LLMs%29%20to%20establish%20a%20zero-shot%20setting.%20Since%0Aimage%20captioning%20is%20the%20most%20crucial%20step%20in%20this%20process%2C%20we%20compare%20the%0Aimpact%20of%20state-of-the-art%20image%20captioning%20models%20on%20VQA%20performance%20across%0Avarious%20question%20types%20in%20terms%20of%20structure%20and%20semantics.%20We%20propose%20a%0Astraightforward%20and%20efficient%20question-driven%20image%20captioning%20approach%20within%0Athis%20pipeline%20to%20transfer%20contextual%20information%20into%20the%20question-answering%0A%28QA%29%20model.%20This%20method%20involves%20extracting%20keywords%20from%20the%20question%2C%0Agenerating%20a%20caption%20for%20each%20image-question%20pair%20using%20the%20keywords%2C%20and%0Aincorporating%20the%20question-driven%20caption%20into%20the%20LLM%20prompt.%20We%20evaluate%20the%0Aefficacy%20of%20using%20general-purpose%20and%20question-driven%20image%20captions%20in%20the%20VQA%0Apipeline.%20Our%20study%20highlights%20the%20potential%20of%20employing%20image%20captions%20and%0Aharnessing%20the%20capabilities%20of%20LLMs%20to%20achieve%20competitive%20performance%20on%20GQA%0Aunder%20the%20zero-shot%20setting.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ovguyo/captions-in-VQA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08589v1&entry.124074799=Read"},
{"title": "Seismic First Break Picking in a Higher Dimension Using Deep Graph\n  Learning", "author": "Hongtao Wang and Li Long and Jiangshe Zhang and Xiaoli Wei and Chunxia Zhang and Zhenbo Guo", "abstract": "  Contemporary automatic first break (FB) picking methods typically analyze 1D\nsignals, 2D source gathers, or 3D source-receiver gathers. Utilizing\nhigher-dimensional data, such as 2D or 3D, incorporates global features,\nimproving the stability of local picking. Despite the benefits,\nhigh-dimensional data requires structured input and increases computational\ndemands. Addressing this, we propose a novel approach using deep graph learning\ncalled DGL-FB, constructing a large graph to efficiently extract information.\nIn this graph, each seismic trace is represented as a node, connected by edges\nthat reflect similarities. To manage the size of the graph, we develop a\nsubgraph sampling technique to streamline model training and inference. Our\nproposed framework, DGL-FB, leverages deep graph learning for FB picking. It\nencodes subgraphs into global features using a deep graph encoder.\nSubsequently, the encoded global features are combined with local node signals\nand fed into a ResUNet-based 1D segmentation network for FB detection. Field\nsurvey evaluations of DGL-FB show superior accuracy and stability compared to a\n2D U-Net-based benchmark method.\n", "link": "http://arxiv.org/abs/2404.08408v1", "date": "2024-04-12", "relevancy": 2.4923, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5094}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.49}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Seismic%20First%20Break%20Picking%20in%20a%20Higher%20Dimension%20Using%20Deep%20Graph%0A%20%20Learning&body=Title%3A%20Seismic%20First%20Break%20Picking%20in%20a%20Higher%20Dimension%20Using%20Deep%20Graph%0A%20%20Learning%0AAuthor%3A%20Hongtao%20Wang%20and%20Li%20Long%20and%20Jiangshe%20Zhang%20and%20Xiaoli%20Wei%20and%20Chunxia%20Zhang%20and%20Zhenbo%20Guo%0AAbstract%3A%20%20%20Contemporary%20automatic%20first%20break%20%28FB%29%20picking%20methods%20typically%20analyze%201D%0Asignals%2C%202D%20source%20gathers%2C%20or%203D%20source-receiver%20gathers.%20Utilizing%0Ahigher-dimensional%20data%2C%20such%20as%202D%20or%203D%2C%20incorporates%20global%20features%2C%0Aimproving%20the%20stability%20of%20local%20picking.%20Despite%20the%20benefits%2C%0Ahigh-dimensional%20data%20requires%20structured%20input%20and%20increases%20computational%0Ademands.%20Addressing%20this%2C%20we%20propose%20a%20novel%20approach%20using%20deep%20graph%20learning%0Acalled%20DGL-FB%2C%20constructing%20a%20large%20graph%20to%20efficiently%20extract%20information.%0AIn%20this%20graph%2C%20each%20seismic%20trace%20is%20represented%20as%20a%20node%2C%20connected%20by%20edges%0Athat%20reflect%20similarities.%20To%20manage%20the%20size%20of%20the%20graph%2C%20we%20develop%20a%0Asubgraph%20sampling%20technique%20to%20streamline%20model%20training%20and%20inference.%20Our%0Aproposed%20framework%2C%20DGL-FB%2C%20leverages%20deep%20graph%20learning%20for%20FB%20picking.%20It%0Aencodes%20subgraphs%20into%20global%20features%20using%20a%20deep%20graph%20encoder.%0ASubsequently%2C%20the%20encoded%20global%20features%20are%20combined%20with%20local%20node%20signals%0Aand%20fed%20into%20a%20ResUNet-based%201D%20segmentation%20network%20for%20FB%20detection.%20Field%0Asurvey%20evaluations%20of%20DGL-FB%20show%20superior%20accuracy%20and%20stability%20compared%20to%20a%0A2D%20U-Net-based%20benchmark%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08408v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seismic%20First%20Break%20Picking%20in%20a%20Higher%20Dimension%20Using%20Deep%20Graph%0A%20%20Learning&entry.906535625=Hongtao%20Wang%20and%20Li%20Long%20and%20Jiangshe%20Zhang%20and%20Xiaoli%20Wei%20and%20Chunxia%20Zhang%20and%20Zhenbo%20Guo&entry.1292438233=%20%20Contemporary%20automatic%20first%20break%20%28FB%29%20picking%20methods%20typically%20analyze%201D%0Asignals%2C%202D%20source%20gathers%2C%20or%203D%20source-receiver%20gathers.%20Utilizing%0Ahigher-dimensional%20data%2C%20such%20as%202D%20or%203D%2C%20incorporates%20global%20features%2C%0Aimproving%20the%20stability%20of%20local%20picking.%20Despite%20the%20benefits%2C%0Ahigh-dimensional%20data%20requires%20structured%20input%20and%20increases%20computational%0Ademands.%20Addressing%20this%2C%20we%20propose%20a%20novel%20approach%20using%20deep%20graph%20learning%0Acalled%20DGL-FB%2C%20constructing%20a%20large%20graph%20to%20efficiently%20extract%20information.%0AIn%20this%20graph%2C%20each%20seismic%20trace%20is%20represented%20as%20a%20node%2C%20connected%20by%20edges%0Athat%20reflect%20similarities.%20To%20manage%20the%20size%20of%20the%20graph%2C%20we%20develop%20a%0Asubgraph%20sampling%20technique%20to%20streamline%20model%20training%20and%20inference.%20Our%0Aproposed%20framework%2C%20DGL-FB%2C%20leverages%20deep%20graph%20learning%20for%20FB%20picking.%20It%0Aencodes%20subgraphs%20into%20global%20features%20using%20a%20deep%20graph%20encoder.%0ASubsequently%2C%20the%20encoded%20global%20features%20are%20combined%20with%20local%20node%20signals%0Aand%20fed%20into%20a%20ResUNet-based%201D%20segmentation%20network%20for%20FB%20detection.%20Field%0Asurvey%20evaluations%20of%20DGL-FB%20show%20superior%20accuracy%20and%20stability%20compared%20to%20a%0A2D%20U-Net-based%20benchmark%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08408v1&entry.124074799=Read"},
{"title": "Deep Classifier Mimicry without Data Access", "author": "Steven Braun and Martin Mundt and Kristian Kersting", "abstract": "  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n", "link": "http://arxiv.org/abs/2306.02090v4", "date": "2024-04-12", "relevancy": 2.4892, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5062}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Classifier%20Mimicry%20without%20Data%20Access&body=Title%3A%20Deep%20Classifier%20Mimicry%20without%20Data%20Access%0AAuthor%3A%20Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02090v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Classifier%20Mimicry%20without%20Data%20Access&entry.906535625=Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting&entry.1292438233=%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02090v4&entry.124074799=Read"},
{"title": "An improved tabular data generator with VAE-GMM integration", "author": "Patricia A. Apell\u00e1niz and Juan Parras and Santiago Zazo", "abstract": "  The rising use of machine learning in various fields requires robust methods\nto create synthetic tabular data. Data should preserve key characteristics\nwhile addressing data scarcity challenges. Current approaches based on\nGenerative Adversarial Networks, such as the state-of-the-art CTGAN model,\nstruggle with the complex structures inherent in tabular data. These data often\ncontain both continuous and discrete features with non-Gaussian distributions.\nTherefore, we propose a novel Variational Autoencoder (VAE)-based model that\naddresses these limitations. Inspired by the TVAE model, our approach\nincorporates a Bayesian Gaussian Mixture model (BGM) within the VAE\narchitecture. This avoids the limitations imposed by assuming a strictly\nGaussian latent space, allowing for a more accurate representation of the\nunderlying data distribution during data generation. Furthermore, our model\noffers enhanced flexibility by allowing the use of various differentiable\ndistributions for individual features, making it possible to handle both\ncontinuous and discrete data types. We thoroughly validate our model on three\nreal-world datasets with mixed data types, including two medically relevant\nones, based on their resemblance and utility. This evaluation demonstrates\nsignificant outperformance against CTGAN and TVAE, establishing its potential\nas a valuable tool for generating synthetic tabular data in various domains,\nparticularly in healthcare.\n", "link": "http://arxiv.org/abs/2404.08434v1", "date": "2024-04-12", "relevancy": 2.4603, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5091}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4761}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20improved%20tabular%20data%20generator%20with%20VAE-GMM%20integration&body=Title%3A%20An%20improved%20tabular%20data%20generator%20with%20VAE-GMM%20integration%0AAuthor%3A%20Patricia%20A.%20Apell%C3%A1niz%20and%20Juan%20Parras%20and%20Santiago%20Zazo%0AAbstract%3A%20%20%20The%20rising%20use%20of%20machine%20learning%20in%20various%20fields%20requires%20robust%20methods%0Ato%20create%20synthetic%20tabular%20data.%20Data%20should%20preserve%20key%20characteristics%0Awhile%20addressing%20data%20scarcity%20challenges.%20Current%20approaches%20based%20on%0AGenerative%20Adversarial%20Networks%2C%20such%20as%20the%20state-of-the-art%20CTGAN%20model%2C%0Astruggle%20with%20the%20complex%20structures%20inherent%20in%20tabular%20data.%20These%20data%20often%0Acontain%20both%20continuous%20and%20discrete%20features%20with%20non-Gaussian%20distributions.%0ATherefore%2C%20we%20propose%20a%20novel%20Variational%20Autoencoder%20%28VAE%29-based%20model%20that%0Aaddresses%20these%20limitations.%20Inspired%20by%20the%20TVAE%20model%2C%20our%20approach%0Aincorporates%20a%20Bayesian%20Gaussian%20Mixture%20model%20%28BGM%29%20within%20the%20VAE%0Aarchitecture.%20This%20avoids%20the%20limitations%20imposed%20by%20assuming%20a%20strictly%0AGaussian%20latent%20space%2C%20allowing%20for%20a%20more%20accurate%20representation%20of%20the%0Aunderlying%20data%20distribution%20during%20data%20generation.%20Furthermore%2C%20our%20model%0Aoffers%20enhanced%20flexibility%20by%20allowing%20the%20use%20of%20various%20differentiable%0Adistributions%20for%20individual%20features%2C%20making%20it%20possible%20to%20handle%20both%0Acontinuous%20and%20discrete%20data%20types.%20We%20thoroughly%20validate%20our%20model%20on%20three%0Areal-world%20datasets%20with%20mixed%20data%20types%2C%20including%20two%20medically%20relevant%0Aones%2C%20based%20on%20their%20resemblance%20and%20utility.%20This%20evaluation%20demonstrates%0Asignificant%20outperformance%20against%20CTGAN%20and%20TVAE%2C%20establishing%20its%20potential%0Aas%20a%20valuable%20tool%20for%20generating%20synthetic%20tabular%20data%20in%20various%20domains%2C%0Aparticularly%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08434v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20improved%20tabular%20data%20generator%20with%20VAE-GMM%20integration&entry.906535625=Patricia%20A.%20Apell%C3%A1niz%20and%20Juan%20Parras%20and%20Santiago%20Zazo&entry.1292438233=%20%20The%20rising%20use%20of%20machine%20learning%20in%20various%20fields%20requires%20robust%20methods%0Ato%20create%20synthetic%20tabular%20data.%20Data%20should%20preserve%20key%20characteristics%0Awhile%20addressing%20data%20scarcity%20challenges.%20Current%20approaches%20based%20on%0AGenerative%20Adversarial%20Networks%2C%20such%20as%20the%20state-of-the-art%20CTGAN%20model%2C%0Astruggle%20with%20the%20complex%20structures%20inherent%20in%20tabular%20data.%20These%20data%20often%0Acontain%20both%20continuous%20and%20discrete%20features%20with%20non-Gaussian%20distributions.%0ATherefore%2C%20we%20propose%20a%20novel%20Variational%20Autoencoder%20%28VAE%29-based%20model%20that%0Aaddresses%20these%20limitations.%20Inspired%20by%20the%20TVAE%20model%2C%20our%20approach%0Aincorporates%20a%20Bayesian%20Gaussian%20Mixture%20model%20%28BGM%29%20within%20the%20VAE%0Aarchitecture.%20This%20avoids%20the%20limitations%20imposed%20by%20assuming%20a%20strictly%0AGaussian%20latent%20space%2C%20allowing%20for%20a%20more%20accurate%20representation%20of%20the%0Aunderlying%20data%20distribution%20during%20data%20generation.%20Furthermore%2C%20our%20model%0Aoffers%20enhanced%20flexibility%20by%20allowing%20the%20use%20of%20various%20differentiable%0Adistributions%20for%20individual%20features%2C%20making%20it%20possible%20to%20handle%20both%0Acontinuous%20and%20discrete%20data%20types.%20We%20thoroughly%20validate%20our%20model%20on%20three%0Areal-world%20datasets%20with%20mixed%20data%20types%2C%20including%20two%20medically%20relevant%0Aones%2C%20based%20on%20their%20resemblance%20and%20utility.%20This%20evaluation%20demonstrates%0Asignificant%20outperformance%20against%20CTGAN%20and%20TVAE%2C%20establishing%20its%20potential%0Aas%20a%20valuable%20tool%20for%20generating%20synthetic%20tabular%20data%20in%20various%20domains%2C%0Aparticularly%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08434v1&entry.124074799=Read"},
{"title": "ChatGPT and general-purpose AI count fruits in pictures surprisingly\n  well", "author": "Konlavach Mengsuwan and Juan Camilo Rivera Palacio and Masahiro Ryo", "abstract": "  Object counting is a popular task in deep learning applications in various\ndomains, including agriculture. A conventional deep learning approach requires\na large amount of training data, often a logistic problem in a real-world\napplication. To address this issue, we examined how well ChatGPT (GPT4V) and a\ngeneral-purpose AI (foundation model for object counting, T-Rex) can count the\nnumber of fruit bodies (coffee cherries) in 100 images. The foundation model\nwith few-shot learning outperformed the trained YOLOv8 model (R2 = 0.923 and\n0.900, respectively). ChatGPT also showed some interesting potential,\nespecially when few-shot learning with human feedback was applied (R2 = 0.360\nand 0.460, respectively). Moreover, we examined the time required for\nimplementation as a practical question. Obtaining the results with the\nfoundation model and ChatGPT were much shorter than the YOLOv8 model (0.83 hrs,\n1.75 hrs, and 161 hrs). We interpret these results as two surprises for deep\nlearning users in applied domains: a foundation model with few-shot\ndomain-specific learning can drastically save time and effort compared to the\nconventional approach, and ChatGPT can reveal a relatively good performance.\nBoth approaches do not need coding skills, which can foster AI education and\ndissemination.\n", "link": "http://arxiv.org/abs/2404.08515v1", "date": "2024-04-12", "relevancy": 2.4538, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4964}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4954}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20and%20general-purpose%20AI%20count%20fruits%20in%20pictures%20surprisingly%0A%20%20well&body=Title%3A%20ChatGPT%20and%20general-purpose%20AI%20count%20fruits%20in%20pictures%20surprisingly%0A%20%20well%0AAuthor%3A%20Konlavach%20Mengsuwan%20and%20Juan%20Camilo%20Rivera%20Palacio%20and%20Masahiro%20Ryo%0AAbstract%3A%20%20%20Object%20counting%20is%20a%20popular%20task%20in%20deep%20learning%20applications%20in%20various%0Adomains%2C%20including%20agriculture.%20A%20conventional%20deep%20learning%20approach%20requires%0Aa%20large%20amount%20of%20training%20data%2C%20often%20a%20logistic%20problem%20in%20a%20real-world%0Aapplication.%20To%20address%20this%20issue%2C%20we%20examined%20how%20well%20ChatGPT%20%28GPT4V%29%20and%20a%0Ageneral-purpose%20AI%20%28foundation%20model%20for%20object%20counting%2C%20T-Rex%29%20can%20count%20the%0Anumber%20of%20fruit%20bodies%20%28coffee%20cherries%29%20in%20100%20images.%20The%20foundation%20model%0Awith%20few-shot%20learning%20outperformed%20the%20trained%20YOLOv8%20model%20%28R2%20%3D%200.923%20and%0A0.900%2C%20respectively%29.%20ChatGPT%20also%20showed%20some%20interesting%20potential%2C%0Aespecially%20when%20few-shot%20learning%20with%20human%20feedback%20was%20applied%20%28R2%20%3D%200.360%0Aand%200.460%2C%20respectively%29.%20Moreover%2C%20we%20examined%20the%20time%20required%20for%0Aimplementation%20as%20a%20practical%20question.%20Obtaining%20the%20results%20with%20the%0Afoundation%20model%20and%20ChatGPT%20were%20much%20shorter%20than%20the%20YOLOv8%20model%20%280.83%20hrs%2C%0A1.75%20hrs%2C%20and%20161%20hrs%29.%20We%20interpret%20these%20results%20as%20two%20surprises%20for%20deep%0Alearning%20users%20in%20applied%20domains%3A%20a%20foundation%20model%20with%20few-shot%0Adomain-specific%20learning%20can%20drastically%20save%20time%20and%20effort%20compared%20to%20the%0Aconventional%20approach%2C%20and%20ChatGPT%20can%20reveal%20a%20relatively%20good%20performance.%0ABoth%20approaches%20do%20not%20need%20coding%20skills%2C%20which%20can%20foster%20AI%20education%20and%0Adissemination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08515v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20and%20general-purpose%20AI%20count%20fruits%20in%20pictures%20surprisingly%0A%20%20well&entry.906535625=Konlavach%20Mengsuwan%20and%20Juan%20Camilo%20Rivera%20Palacio%20and%20Masahiro%20Ryo&entry.1292438233=%20%20Object%20counting%20is%20a%20popular%20task%20in%20deep%20learning%20applications%20in%20various%0Adomains%2C%20including%20agriculture.%20A%20conventional%20deep%20learning%20approach%20requires%0Aa%20large%20amount%20of%20training%20data%2C%20often%20a%20logistic%20problem%20in%20a%20real-world%0Aapplication.%20To%20address%20this%20issue%2C%20we%20examined%20how%20well%20ChatGPT%20%28GPT4V%29%20and%20a%0Ageneral-purpose%20AI%20%28foundation%20model%20for%20object%20counting%2C%20T-Rex%29%20can%20count%20the%0Anumber%20of%20fruit%20bodies%20%28coffee%20cherries%29%20in%20100%20images.%20The%20foundation%20model%0Awith%20few-shot%20learning%20outperformed%20the%20trained%20YOLOv8%20model%20%28R2%20%3D%200.923%20and%0A0.900%2C%20respectively%29.%20ChatGPT%20also%20showed%20some%20interesting%20potential%2C%0Aespecially%20when%20few-shot%20learning%20with%20human%20feedback%20was%20applied%20%28R2%20%3D%200.360%0Aand%200.460%2C%20respectively%29.%20Moreover%2C%20we%20examined%20the%20time%20required%20for%0Aimplementation%20as%20a%20practical%20question.%20Obtaining%20the%20results%20with%20the%0Afoundation%20model%20and%20ChatGPT%20were%20much%20shorter%20than%20the%20YOLOv8%20model%20%280.83%20hrs%2C%0A1.75%20hrs%2C%20and%20161%20hrs%29.%20We%20interpret%20these%20results%20as%20two%20surprises%20for%20deep%0Alearning%20users%20in%20applied%20domains%3A%20a%20foundation%20model%20with%20few-shot%0Adomain-specific%20learning%20can%20drastically%20save%20time%20and%20effort%20compared%20to%20the%0Aconventional%20approach%2C%20and%20ChatGPT%20can%20reveal%20a%20relatively%20good%20performance.%0ABoth%20approaches%20do%20not%20need%20coding%20skills%2C%20which%20can%20foster%20AI%20education%20and%0Adissemination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08515v1&entry.124074799=Read"},
{"title": "Learning representations of learning representations", "author": "Rita Gonz\u00e1lez-M\u00e1rquez and Dmitry Kobak", "abstract": "  The ICLR conference is unique among the top machine learning conferences in\nthat all submitted papers are openly available. Here we present the ICLR\ndataset consisting of abstracts of all 24 thousand ICLR submissions from\n2017-2024 with meta-data, decision scores, and custom keyword-based labels. We\nfind that on this dataset, bag-of-words representation outperforms most\ndedicated sentence transformer models in terms of $k$NN classification\naccuracy, and the top performing language models barely outperform TF-IDF. We\nsee this as a challenge for the NLP community. Furthermore, we use the ICLR\ndataset to study how the field of machine learning has changed over the last\nseven years, finding some improvement in gender balance. Using a 2D embedding\nof the abstracts' texts, we describe a shift in research topics from 2017 to\n2024 and identify hedgehogs and foxes among the authors with the highest number\nof ICLR submissions.\n", "link": "http://arxiv.org/abs/2404.08403v1", "date": "2024-04-12", "relevancy": 2.4255, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4826}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20representations%20of%20learning%20representations&body=Title%3A%20Learning%20representations%20of%20learning%20representations%0AAuthor%3A%20Rita%20Gonz%C3%A1lez-M%C3%A1rquez%20and%20Dmitry%20Kobak%0AAbstract%3A%20%20%20The%20ICLR%20conference%20is%20unique%20among%20the%20top%20machine%20learning%20conferences%20in%0Athat%20all%20submitted%20papers%20are%20openly%20available.%20Here%20we%20present%20the%20ICLR%0Adataset%20consisting%20of%20abstracts%20of%20all%2024%20thousand%20ICLR%20submissions%20from%0A2017-2024%20with%20meta-data%2C%20decision%20scores%2C%20and%20custom%20keyword-based%20labels.%20We%0Afind%20that%20on%20this%20dataset%2C%20bag-of-words%20representation%20outperforms%20most%0Adedicated%20sentence%20transformer%20models%20in%20terms%20of%20%24k%24NN%20classification%0Aaccuracy%2C%20and%20the%20top%20performing%20language%20models%20barely%20outperform%20TF-IDF.%20We%0Asee%20this%20as%20a%20challenge%20for%20the%20NLP%20community.%20Furthermore%2C%20we%20use%20the%20ICLR%0Adataset%20to%20study%20how%20the%20field%20of%20machine%20learning%20has%20changed%20over%20the%20last%0Aseven%20years%2C%20finding%20some%20improvement%20in%20gender%20balance.%20Using%20a%202D%20embedding%0Aof%20the%20abstracts%27%20texts%2C%20we%20describe%20a%20shift%20in%20research%20topics%20from%202017%20to%0A2024%20and%20identify%20hedgehogs%20and%20foxes%20among%20the%20authors%20with%20the%20highest%20number%0Aof%20ICLR%20submissions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08403v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20representations%20of%20learning%20representations&entry.906535625=Rita%20Gonz%C3%A1lez-M%C3%A1rquez%20and%20Dmitry%20Kobak&entry.1292438233=%20%20The%20ICLR%20conference%20is%20unique%20among%20the%20top%20machine%20learning%20conferences%20in%0Athat%20all%20submitted%20papers%20are%20openly%20available.%20Here%20we%20present%20the%20ICLR%0Adataset%20consisting%20of%20abstracts%20of%20all%2024%20thousand%20ICLR%20submissions%20from%0A2017-2024%20with%20meta-data%2C%20decision%20scores%2C%20and%20custom%20keyword-based%20labels.%20We%0Afind%20that%20on%20this%20dataset%2C%20bag-of-words%20representation%20outperforms%20most%0Adedicated%20sentence%20transformer%20models%20in%20terms%20of%20%24k%24NN%20classification%0Aaccuracy%2C%20and%20the%20top%20performing%20language%20models%20barely%20outperform%20TF-IDF.%20We%0Asee%20this%20as%20a%20challenge%20for%20the%20NLP%20community.%20Furthermore%2C%20we%20use%20the%20ICLR%0Adataset%20to%20study%20how%20the%20field%20of%20machine%20learning%20has%20changed%20over%20the%20last%0Aseven%20years%2C%20finding%20some%20improvement%20in%20gender%20balance.%20Using%20a%202D%20embedding%0Aof%20the%20abstracts%27%20texts%2C%20we%20describe%20a%20shift%20in%20research%20topics%20from%202017%20to%0A2024%20and%20identify%20hedgehogs%20and%20foxes%20among%20the%20authors%20with%20the%20highest%20number%0Aof%20ICLR%20submissions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08403v1&entry.124074799=Read"},
{"title": "Graph data augmentation with Gromow-Wasserstein Barycenters", "author": "Andrea Ponti", "abstract": "  Graphs are ubiquitous in various fields, and deep learning methods have been\nsuccessful applied in graph classification tasks. However, building large and\ndiverse graph datasets for training can be expensive. While augmentation\ntechniques exist for structured data like images or numerical data, the\naugmentation of graph data remains challenging. This is primarily due to the\ncomplex and non-Euclidean nature of graph data. In this paper, it has been\nproposed a novel augmentation strategy for graphs that operates in a\nnon-Euclidean space. This approach leverages graphon estimation, which models\nthe generative mechanism of networks sequences. Computational results\ndemonstrate the effectiveness of the proposed augmentation framework in\nimproving the performance of graph classification models. Additionally, using a\nnon-Euclidean distance, specifically the Gromow-Wasserstein distance, results\nin better approximations of the graphon. This framework also provides a means\nto validate different graphon estimation approaches, particularly in real-world\nscenarios where the true graphon is unknown.\n", "link": "http://arxiv.org/abs/2404.08376v1", "date": "2024-04-12", "relevancy": 2.4176, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5014}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4797}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4695}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20data%20augmentation%20with%20Gromow-Wasserstein%20Barycenters&body=Title%3A%20Graph%20data%20augmentation%20with%20Gromow-Wasserstein%20Barycenters%0AAuthor%3A%20Andrea%20Ponti%0AAbstract%3A%20%20%20Graphs%20are%20ubiquitous%20in%20various%20fields%2C%20and%20deep%20learning%20methods%20have%20been%0Asuccessful%20applied%20in%20graph%20classification%20tasks.%20However%2C%20building%20large%20and%0Adiverse%20graph%20datasets%20for%20training%20can%20be%20expensive.%20While%20augmentation%0Atechniques%20exist%20for%20structured%20data%20like%20images%20or%20numerical%20data%2C%20the%0Aaugmentation%20of%20graph%20data%20remains%20challenging.%20This%20is%20primarily%20due%20to%20the%0Acomplex%20and%20non-Euclidean%20nature%20of%20graph%20data.%20In%20this%20paper%2C%20it%20has%20been%0Aproposed%20a%20novel%20augmentation%20strategy%20for%20graphs%20that%20operates%20in%20a%0Anon-Euclidean%20space.%20This%20approach%20leverages%20graphon%20estimation%2C%20which%20models%0Athe%20generative%20mechanism%20of%20networks%20sequences.%20Computational%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20augmentation%20framework%20in%0Aimproving%20the%20performance%20of%20graph%20classification%20models.%20Additionally%2C%20using%20a%0Anon-Euclidean%20distance%2C%20specifically%20the%20Gromow-Wasserstein%20distance%2C%20results%0Ain%20better%20approximations%20of%20the%20graphon.%20This%20framework%20also%20provides%20a%20means%0Ato%20validate%20different%20graphon%20estimation%20approaches%2C%20particularly%20in%20real-world%0Ascenarios%20where%20the%20true%20graphon%20is%20unknown.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08376v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20data%20augmentation%20with%20Gromow-Wasserstein%20Barycenters&entry.906535625=Andrea%20Ponti&entry.1292438233=%20%20Graphs%20are%20ubiquitous%20in%20various%20fields%2C%20and%20deep%20learning%20methods%20have%20been%0Asuccessful%20applied%20in%20graph%20classification%20tasks.%20However%2C%20building%20large%20and%0Adiverse%20graph%20datasets%20for%20training%20can%20be%20expensive.%20While%20augmentation%0Atechniques%20exist%20for%20structured%20data%20like%20images%20or%20numerical%20data%2C%20the%0Aaugmentation%20of%20graph%20data%20remains%20challenging.%20This%20is%20primarily%20due%20to%20the%0Acomplex%20and%20non-Euclidean%20nature%20of%20graph%20data.%20In%20this%20paper%2C%20it%20has%20been%0Aproposed%20a%20novel%20augmentation%20strategy%20for%20graphs%20that%20operates%20in%20a%0Anon-Euclidean%20space.%20This%20approach%20leverages%20graphon%20estimation%2C%20which%20models%0Athe%20generative%20mechanism%20of%20networks%20sequences.%20Computational%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20augmentation%20framework%20in%0Aimproving%20the%20performance%20of%20graph%20classification%20models.%20Additionally%2C%20using%20a%0Anon-Euclidean%20distance%2C%20specifically%20the%20Gromow-Wasserstein%20distance%2C%20results%0Ain%20better%20approximations%20of%20the%20graphon.%20This%20framework%20also%20provides%20a%20means%0Ato%20validate%20different%20graphon%20estimation%20approaches%2C%20particularly%20in%20real-world%0Ascenarios%20where%20the%20true%20graphon%20is%20unknown.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08376v1&entry.124074799=Read"},
{"title": "Pre-training Small Base LMs with Fewer Tokens", "author": "Sunny Sanyal and Sujay Sanghavi and Alexandros G. Dimakis", "abstract": "  We study the effectiveness of a simple approach to develop a small base\nlanguage model (LM) starting from an existing large base LM: first inherit a\nfew transformer blocks from the larger LM, and then train this smaller model on\na very small subset (0.1\\%) of the raw pretraining data of the larger model. We\ncall our simple recipe Inheritune and first demonstrate it for building a small\nbase LM with 1.5B parameters using 1B tokens (and a starting few layers of\nlarger LM of 3B parameters); we do this using a single A6000 GPU for less than\nhalf a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark,\nthe resulting model compares favorably to publicly available base models of\n1B-2B size, some of which have been trained using 50-1000 times more tokens.\n  We investigate Inheritune in a slightly different setting where we train\nsmall LMs utilizing larger LMs and their full pre-training dataset. Here we\nshow that smaller LMs trained utilizing some of the layers of GPT2-medium\n(355M) and GPT-2-large (770M) can effectively match the val loss of their\nbigger counterparts when trained from scratch for the same number of training\nsteps on OpenWebText dataset with 9B tokens. We analyze our recipe with\nextensive experiments and demonstrate it efficacy on diverse settings. Our code\nis available at https://github.com/sanyalsunny111/LLM-Inheritune.\n", "link": "http://arxiv.org/abs/2404.08634v1", "date": "2024-04-12", "relevancy": 2.4068, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4996}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4805}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pre-training%20Small%20Base%20LMs%20with%20Fewer%20Tokens&body=Title%3A%20Pre-training%20Small%20Base%20LMs%20with%20Fewer%20Tokens%0AAuthor%3A%20Sunny%20Sanyal%20and%20Sujay%20Sanghavi%20and%20Alexandros%20G.%20Dimakis%0AAbstract%3A%20%20%20We%20study%20the%20effectiveness%20of%20a%20simple%20approach%20to%20develop%20a%20small%20base%0Alanguage%20model%20%28LM%29%20starting%20from%20an%20existing%20large%20base%20LM%3A%20first%20inherit%20a%0Afew%20transformer%20blocks%20from%20the%20larger%20LM%2C%20and%20then%20train%20this%20smaller%20model%20on%0Aa%20very%20small%20subset%20%280.1%5C%25%29%20of%20the%20raw%20pretraining%20data%20of%20the%20larger%20model.%20We%0Acall%20our%20simple%20recipe%20Inheritune%20and%20first%20demonstrate%20it%20for%20building%20a%20small%0Abase%20LM%20with%201.5B%20parameters%20using%201B%20tokens%20%28and%20a%20starting%20few%20layers%20of%0Alarger%20LM%20of%203B%20parameters%29%3B%20we%20do%20this%20using%20a%20single%20A6000%20GPU%20for%20less%20than%0Ahalf%20a%20day.%20Across%209%20diverse%20evaluation%20datasets%20as%20well%20as%20the%20MMLU%20benchmark%2C%0Athe%20resulting%20model%20compares%20favorably%20to%20publicly%20available%20base%20models%20of%0A1B-2B%20size%2C%20some%20of%20which%20have%20been%20trained%20using%2050-1000%20times%20more%20tokens.%0A%20%20We%20investigate%20Inheritune%20in%20a%20slightly%20different%20setting%20where%20we%20train%0Asmall%20LMs%20utilizing%20larger%20LMs%20and%20their%20full%20pre-training%20dataset.%20Here%20we%0Ashow%20that%20smaller%20LMs%20trained%20utilizing%20some%20of%20the%20layers%20of%20GPT2-medium%0A%28355M%29%20and%20GPT-2-large%20%28770M%29%20can%20effectively%20match%20the%20val%20loss%20of%20their%0Abigger%20counterparts%20when%20trained%20from%20scratch%20for%20the%20same%20number%20of%20training%0Asteps%20on%20OpenWebText%20dataset%20with%209B%20tokens.%20We%20analyze%20our%20recipe%20with%0Aextensive%20experiments%20and%20demonstrate%20it%20efficacy%20on%20diverse%20settings.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/sanyalsunny111/LLM-Inheritune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20Small%20Base%20LMs%20with%20Fewer%20Tokens&entry.906535625=Sunny%20Sanyal%20and%20Sujay%20Sanghavi%20and%20Alexandros%20G.%20Dimakis&entry.1292438233=%20%20We%20study%20the%20effectiveness%20of%20a%20simple%20approach%20to%20develop%20a%20small%20base%0Alanguage%20model%20%28LM%29%20starting%20from%20an%20existing%20large%20base%20LM%3A%20first%20inherit%20a%0Afew%20transformer%20blocks%20from%20the%20larger%20LM%2C%20and%20then%20train%20this%20smaller%20model%20on%0Aa%20very%20small%20subset%20%280.1%5C%25%29%20of%20the%20raw%20pretraining%20data%20of%20the%20larger%20model.%20We%0Acall%20our%20simple%20recipe%20Inheritune%20and%20first%20demonstrate%20it%20for%20building%20a%20small%0Abase%20LM%20with%201.5B%20parameters%20using%201B%20tokens%20%28and%20a%20starting%20few%20layers%20of%0Alarger%20LM%20of%203B%20parameters%29%3B%20we%20do%20this%20using%20a%20single%20A6000%20GPU%20for%20less%20than%0Ahalf%20a%20day.%20Across%209%20diverse%20evaluation%20datasets%20as%20well%20as%20the%20MMLU%20benchmark%2C%0Athe%20resulting%20model%20compares%20favorably%20to%20publicly%20available%20base%20models%20of%0A1B-2B%20size%2C%20some%20of%20which%20have%20been%20trained%20using%2050-1000%20times%20more%20tokens.%0A%20%20We%20investigate%20Inheritune%20in%20a%20slightly%20different%20setting%20where%20we%20train%0Asmall%20LMs%20utilizing%20larger%20LMs%20and%20their%20full%20pre-training%20dataset.%20Here%20we%0Ashow%20that%20smaller%20LMs%20trained%20utilizing%20some%20of%20the%20layers%20of%20GPT2-medium%0A%28355M%29%20and%20GPT-2-large%20%28770M%29%20can%20effectively%20match%20the%20val%20loss%20of%20their%0Abigger%20counterparts%20when%20trained%20from%20scratch%20for%20the%20same%20number%20of%20training%0Asteps%20on%20OpenWebText%20dataset%20with%209B%20tokens.%20We%20analyze%20our%20recipe%20with%0Aextensive%20experiments%20and%20demonstrate%20it%20efficacy%20on%20diverse%20settings.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/sanyalsunny111/LLM-Inheritune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08634v1&entry.124074799=Read"},
{"title": "Hyperbolic Delaunay Geometric Alignment", "author": "Aniss Aiman Medbouhi and Giovanni Luca Marchetti and Vladislav Polianskii and Alexander Kravberg and Petra Poklukar and Anastasia Varava and Danica Kragic", "abstract": "  Hyperbolic machine learning is an emerging field aimed at representing data\nwith a hierarchical structure. However, there is a lack of tools for evaluation\nand analysis of the resulting hyperbolic data representations. To this end, we\npropose Hyperbolic Delaunay Geometric Alignment (HyperDGA) -- a similarity\nscore for comparing datasets in a hyperbolic space. The core idea is counting\nthe edges of the hyperbolic Delaunay graph connecting datapoints across the\ngiven sets. We provide an empirical investigation on synthetic and real-life\nbiological data and demonstrate that HyperDGA outperforms the hyperbolic\nversion of classical distances between sets. Furthermore, we showcase the\npotential of HyperDGA for evaluating latent representations inferred by a\nHyperbolic Variational Auto-Encoder.\n", "link": "http://arxiv.org/abs/2404.08608v1", "date": "2024-04-12", "relevancy": 2.3762, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4998}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4491}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Delaunay%20Geometric%20Alignment&body=Title%3A%20Hyperbolic%20Delaunay%20Geometric%20Alignment%0AAuthor%3A%20Aniss%20Aiman%20Medbouhi%20and%20Giovanni%20Luca%20Marchetti%20and%20Vladislav%20Polianskii%20and%20Alexander%20Kravberg%20and%20Petra%20Poklukar%20and%20Anastasia%20Varava%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Hyperbolic%20machine%20learning%20is%20an%20emerging%20field%20aimed%20at%20representing%20data%0Awith%20a%20hierarchical%20structure.%20However%2C%20there%20is%20a%20lack%20of%20tools%20for%20evaluation%0Aand%20analysis%20of%20the%20resulting%20hyperbolic%20data%20representations.%20To%20this%20end%2C%20we%0Apropose%20Hyperbolic%20Delaunay%20Geometric%20Alignment%20%28HyperDGA%29%20--%20a%20similarity%0Ascore%20for%20comparing%20datasets%20in%20a%20hyperbolic%20space.%20The%20core%20idea%20is%20counting%0Athe%20edges%20of%20the%20hyperbolic%20Delaunay%20graph%20connecting%20datapoints%20across%20the%0Agiven%20sets.%20We%20provide%20an%20empirical%20investigation%20on%20synthetic%20and%20real-life%0Abiological%20data%20and%20demonstrate%20that%20HyperDGA%20outperforms%20the%20hyperbolic%0Aversion%20of%20classical%20distances%20between%20sets.%20Furthermore%2C%20we%20showcase%20the%0Apotential%20of%20HyperDGA%20for%20evaluating%20latent%20representations%20inferred%20by%20a%0AHyperbolic%20Variational%20Auto-Encoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08608v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Delaunay%20Geometric%20Alignment&entry.906535625=Aniss%20Aiman%20Medbouhi%20and%20Giovanni%20Luca%20Marchetti%20and%20Vladislav%20Polianskii%20and%20Alexander%20Kravberg%20and%20Petra%20Poklukar%20and%20Anastasia%20Varava%20and%20Danica%20Kragic&entry.1292438233=%20%20Hyperbolic%20machine%20learning%20is%20an%20emerging%20field%20aimed%20at%20representing%20data%0Awith%20a%20hierarchical%20structure.%20However%2C%20there%20is%20a%20lack%20of%20tools%20for%20evaluation%0Aand%20analysis%20of%20the%20resulting%20hyperbolic%20data%20representations.%20To%20this%20end%2C%20we%0Apropose%20Hyperbolic%20Delaunay%20Geometric%20Alignment%20%28HyperDGA%29%20--%20a%20similarity%0Ascore%20for%20comparing%20datasets%20in%20a%20hyperbolic%20space.%20The%20core%20idea%20is%20counting%0Athe%20edges%20of%20the%20hyperbolic%20Delaunay%20graph%20connecting%20datapoints%20across%20the%0Agiven%20sets.%20We%20provide%20an%20empirical%20investigation%20on%20synthetic%20and%20real-life%0Abiological%20data%20and%20demonstrate%20that%20HyperDGA%20outperforms%20the%20hyperbolic%0Aversion%20of%20classical%20distances%20between%20sets.%20Furthermore%2C%20we%20showcase%20the%0Apotential%20of%20HyperDGA%20for%20evaluating%20latent%20representations%20inferred%20by%20a%0AHyperbolic%20Variational%20Auto-Encoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08608v1&entry.124074799=Read"},
{"title": "Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly\n  Detection", "author": "Zhiwei Yang and Jing Liu and Peng Wu", "abstract": "  Weakly supervised video anomaly detection (WSVAD) is a challenging task.\nGenerating fine-grained pseudo-labels based on weak-label and then\nself-training a classifier is currently a promising solution. However, since\nthe existing methods use only RGB visual modality and the utilization of\ncategory text information is neglected, thus limiting the generation of more\naccurate pseudo-labels and affecting the performance of self-training. Inspired\nby the manual labeling process based on the event description, in this paper,\nwe propose a novel pseudo-label generation and self-training framework based on\nText Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer\nthe rich language-visual knowledge of the contrastive language-image\npre-training (CLIP) model for aligning the video event description text and\ncorresponding video frames to generate pseudo-labels. Specifically, We first\nfine-tune the CLIP for domain adaptation by designing two ranking losses and a\ndistributional inconsistency loss. Further, we propose a learnable text prompt\nmechanism with the assist of a normality visual prompt to further improve the\nmatching accuracy of video event description text and video frames. Then, we\ndesign a pseudo-label generation module based on the normality guidance to\ninfer reliable frame-level pseudo-labels. Finally, we introduce a temporal\ncontext self-adaptive learning module to learn the temporal dependencies of\ndifferent video events more flexibly and accurately. Extensive experiments show\nthat our method achieves state-of-the-art performance on two benchmark\ndatasets, UCF-Crime and XD-Viole\n", "link": "http://arxiv.org/abs/2404.08531v1", "date": "2024-04-12", "relevancy": 2.3695, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6038}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5894}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5822}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Text%20Prompt%20with%20Normality%20Guidance%20for%20Weakly%20Supervised%20Video%20Anomaly%0A%20%20Detection&body=Title%3A%20Text%20Prompt%20with%20Normality%20Guidance%20for%20Weakly%20Supervised%20Video%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Zhiwei%20Yang%20and%20Jing%20Liu%20and%20Peng%20Wu%0AAbstract%3A%20%20%20Weakly%20supervised%20video%20anomaly%20detection%20%28WSVAD%29%20is%20a%20challenging%20task.%0AGenerating%20fine-grained%20pseudo-labels%20based%20on%20weak-label%20and%20then%0Aself-training%20a%20classifier%20is%20currently%20a%20promising%20solution.%20However%2C%20since%0Athe%20existing%20methods%20use%20only%20RGB%20visual%20modality%20and%20the%20utilization%20of%0Acategory%20text%20information%20is%20neglected%2C%20thus%20limiting%20the%20generation%20of%20more%0Aaccurate%20pseudo-labels%20and%20affecting%20the%20performance%20of%20self-training.%20Inspired%0Aby%20the%20manual%20labeling%20process%20based%20on%20the%20event%20description%2C%20in%20this%20paper%2C%0Awe%20propose%20a%20novel%20pseudo-label%20generation%20and%20self-training%20framework%20based%20on%0AText%20Prompt%20with%20Normality%20Guidance%20%28TPWNG%29%20for%20WSVAD.%20Our%20idea%20is%20to%20transfer%0Athe%20rich%20language-visual%20knowledge%20of%20the%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20model%20for%20aligning%20the%20video%20event%20description%20text%20and%0Acorresponding%20video%20frames%20to%20generate%20pseudo-labels.%20Specifically%2C%20We%20first%0Afine-tune%20the%20CLIP%20for%20domain%20adaptation%20by%20designing%20two%20ranking%20losses%20and%20a%0Adistributional%20inconsistency%20loss.%20Further%2C%20we%20propose%20a%20learnable%20text%20prompt%0Amechanism%20with%20the%20assist%20of%20a%20normality%20visual%20prompt%20to%20further%20improve%20the%0Amatching%20accuracy%20of%20video%20event%20description%20text%20and%20video%20frames.%20Then%2C%20we%0Adesign%20a%20pseudo-label%20generation%20module%20based%20on%20the%20normality%20guidance%20to%0Ainfer%20reliable%20frame-level%20pseudo-labels.%20Finally%2C%20we%20introduce%20a%20temporal%0Acontext%20self-adaptive%20learning%20module%20to%20learn%20the%20temporal%20dependencies%20of%0Adifferent%20video%20events%20more%20flexibly%20and%20accurately.%20Extensive%20experiments%20show%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20two%20benchmark%0Adatasets%2C%20UCF-Crime%20and%20XD-Viole%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08531v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20Prompt%20with%20Normality%20Guidance%20for%20Weakly%20Supervised%20Video%20Anomaly%0A%20%20Detection&entry.906535625=Zhiwei%20Yang%20and%20Jing%20Liu%20and%20Peng%20Wu&entry.1292438233=%20%20Weakly%20supervised%20video%20anomaly%20detection%20%28WSVAD%29%20is%20a%20challenging%20task.%0AGenerating%20fine-grained%20pseudo-labels%20based%20on%20weak-label%20and%20then%0Aself-training%20a%20classifier%20is%20currently%20a%20promising%20solution.%20However%2C%20since%0Athe%20existing%20methods%20use%20only%20RGB%20visual%20modality%20and%20the%20utilization%20of%0Acategory%20text%20information%20is%20neglected%2C%20thus%20limiting%20the%20generation%20of%20more%0Aaccurate%20pseudo-labels%20and%20affecting%20the%20performance%20of%20self-training.%20Inspired%0Aby%20the%20manual%20labeling%20process%20based%20on%20the%20event%20description%2C%20in%20this%20paper%2C%0Awe%20propose%20a%20novel%20pseudo-label%20generation%20and%20self-training%20framework%20based%20on%0AText%20Prompt%20with%20Normality%20Guidance%20%28TPWNG%29%20for%20WSVAD.%20Our%20idea%20is%20to%20transfer%0Athe%20rich%20language-visual%20knowledge%20of%20the%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20model%20for%20aligning%20the%20video%20event%20description%20text%20and%0Acorresponding%20video%20frames%20to%20generate%20pseudo-labels.%20Specifically%2C%20We%20first%0Afine-tune%20the%20CLIP%20for%20domain%20adaptation%20by%20designing%20two%20ranking%20losses%20and%20a%0Adistributional%20inconsistency%20loss.%20Further%2C%20we%20propose%20a%20learnable%20text%20prompt%0Amechanism%20with%20the%20assist%20of%20a%20normality%20visual%20prompt%20to%20further%20improve%20the%0Amatching%20accuracy%20of%20video%20event%20description%20text%20and%20video%20frames.%20Then%2C%20we%0Adesign%20a%20pseudo-label%20generation%20module%20based%20on%20the%20normality%20guidance%20to%0Ainfer%20reliable%20frame-level%20pseudo-labels.%20Finally%2C%20we%20introduce%20a%20temporal%0Acontext%20self-adaptive%20learning%20module%20to%20learn%20the%20temporal%20dependencies%20of%0Adifferent%20video%20events%20more%20flexibly%20and%20accurately.%20Extensive%20experiments%20show%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20two%20benchmark%0Adatasets%2C%20UCF-Crime%20and%20XD-Viole%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08531v1&entry.124074799=Read"},
{"title": "Direct May Not Be the Best: An Incremental Evolution View of Pose\n  Generation", "author": "Yuelong Li and Tengfei Xiao and Lei Geng and Jianming Wang", "abstract": "  Pose diversity is an inherent representative characteristic of 2D images. Due\nto the 3D to 2D projection mechanism, there is evident content discrepancy\namong distinct pose images. This is the main obstacle bothering pose\ntransformation related researches. To deal with this challenge, we propose a\nfine-grained incremental evolution centered pose generation framework, rather\nthan traditional direct one-to-one in a rush. Since proposed approach actually\nbypasses the theoretical difficulty of directly modeling dramatic non-linear\nvariation, the incurred content distortion and blurring could be effectively\nconstrained, at the same time the various individual pose details, especially\nclothes texture, could be precisely maintained. In order to systematically\nguide the evolution course, both global and incremental evolution constraints\nare elaborately designed and merged into the overall frame?work. And a novel\ntriple-path knowledge fusion structure is worked out to take full advantage of\nall available valuable knowledge to conduct high-quality pose synthesis. In\naddition, our framework could generate a series of valuable byproducts, namely\nthe various intermediate poses. Extensive experiments have been conducted to\nverify the effectiveness of the proposed approach. Code is available at\nhttps://github.com/Xiaofei-CN/Incremental-Evolution-Pose-Generation.\n", "link": "http://arxiv.org/abs/2404.08419v1", "date": "2024-04-12", "relevancy": 2.3559, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.577}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Direct%20May%20Not%20Be%20the%20Best%3A%20An%20Incremental%20Evolution%20View%20of%20Pose%0A%20%20Generation&body=Title%3A%20Direct%20May%20Not%20Be%20the%20Best%3A%20An%20Incremental%20Evolution%20View%20of%20Pose%0A%20%20Generation%0AAuthor%3A%20Yuelong%20Li%20and%20Tengfei%20Xiao%20and%20Lei%20Geng%20and%20Jianming%20Wang%0AAbstract%3A%20%20%20Pose%20diversity%20is%20an%20inherent%20representative%20characteristic%20of%202D%20images.%20Due%0Ato%20the%203D%20to%202D%20projection%20mechanism%2C%20there%20is%20evident%20content%20discrepancy%0Aamong%20distinct%20pose%20images.%20This%20is%20the%20main%20obstacle%20bothering%20pose%0Atransformation%20related%20researches.%20To%20deal%20with%20this%20challenge%2C%20we%20propose%20a%0Afine-grained%20incremental%20evolution%20centered%20pose%20generation%20framework%2C%20rather%0Athan%20traditional%20direct%20one-to-one%20in%20a%20rush.%20Since%20proposed%20approach%20actually%0Abypasses%20the%20theoretical%20difficulty%20of%20directly%20modeling%20dramatic%20non-linear%0Avariation%2C%20the%20incurred%20content%20distortion%20and%20blurring%20could%20be%20effectively%0Aconstrained%2C%20at%20the%20same%20time%20the%20various%20individual%20pose%20details%2C%20especially%0Aclothes%20texture%2C%20could%20be%20precisely%20maintained.%20In%20order%20to%20systematically%0Aguide%20the%20evolution%20course%2C%20both%20global%20and%20incremental%20evolution%20constraints%0Aare%20elaborately%20designed%20and%20merged%20into%20the%20overall%20frame%3Fwork.%20And%20a%20novel%0Atriple-path%20knowledge%20fusion%20structure%20is%20worked%20out%20to%20take%20full%20advantage%20of%0Aall%20available%20valuable%20knowledge%20to%20conduct%20high-quality%20pose%20synthesis.%20In%0Aaddition%2C%20our%20framework%20could%20generate%20a%20series%20of%20valuable%20byproducts%2C%20namely%0Athe%20various%20intermediate%20poses.%20Extensive%20experiments%20have%20been%20conducted%20to%0Averify%20the%20effectiveness%20of%20the%20proposed%20approach.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xiaofei-CN/Incremental-Evolution-Pose-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08419v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20May%20Not%20Be%20the%20Best%3A%20An%20Incremental%20Evolution%20View%20of%20Pose%0A%20%20Generation&entry.906535625=Yuelong%20Li%20and%20Tengfei%20Xiao%20and%20Lei%20Geng%20and%20Jianming%20Wang&entry.1292438233=%20%20Pose%20diversity%20is%20an%20inherent%20representative%20characteristic%20of%202D%20images.%20Due%0Ato%20the%203D%20to%202D%20projection%20mechanism%2C%20there%20is%20evident%20content%20discrepancy%0Aamong%20distinct%20pose%20images.%20This%20is%20the%20main%20obstacle%20bothering%20pose%0Atransformation%20related%20researches.%20To%20deal%20with%20this%20challenge%2C%20we%20propose%20a%0Afine-grained%20incremental%20evolution%20centered%20pose%20generation%20framework%2C%20rather%0Athan%20traditional%20direct%20one-to-one%20in%20a%20rush.%20Since%20proposed%20approach%20actually%0Abypasses%20the%20theoretical%20difficulty%20of%20directly%20modeling%20dramatic%20non-linear%0Avariation%2C%20the%20incurred%20content%20distortion%20and%20blurring%20could%20be%20effectively%0Aconstrained%2C%20at%20the%20same%20time%20the%20various%20individual%20pose%20details%2C%20especially%0Aclothes%20texture%2C%20could%20be%20precisely%20maintained.%20In%20order%20to%20systematically%0Aguide%20the%20evolution%20course%2C%20both%20global%20and%20incremental%20evolution%20constraints%0Aare%20elaborately%20designed%20and%20merged%20into%20the%20overall%20frame%3Fwork.%20And%20a%20novel%0Atriple-path%20knowledge%20fusion%20structure%20is%20worked%20out%20to%20take%20full%20advantage%20of%0Aall%20available%20valuable%20knowledge%20to%20conduct%20high-quality%20pose%20synthesis.%20In%0Aaddition%2C%20our%20framework%20could%20generate%20a%20series%20of%20valuable%20byproducts%2C%20namely%0Athe%20various%20intermediate%20poses.%20Extensive%20experiments%20have%20been%20conducted%20to%0Averify%20the%20effectiveness%20of%20the%20proposed%20approach.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xiaofei-CN/Incremental-Evolution-Pose-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08419v1&entry.124074799=Read"},
{"title": "Masked Image Modeling as a Framework for Self-Supervised Learning across\n  Eye Movements", "author": "Robin Weiler and Matthias Brucklacher and Cyriel M. A. Pennartz and Sander M. Boht\u00e9", "abstract": "  To make sense of their surroundings, intelligent systems must transform\ncomplex sensory inputs to structured codes that are reduced to task-relevant\ninformation such as object category. Biological agents achieve this in a\nlargely autonomous manner, presumably via self-\\allowbreak super-\\allowbreak\nvised learning. Whereas previous attempts to model the underlying mechanisms\nwere largely discriminative in nature, there is ample evidence that the brain\nemploys a generative model of the world. Here, we propose that eye movements,\nin combination with the focused nature of primate vision, constitute a\ngenerative, self-supervised task of predicting and revealing visual\ninformation. We construct a proof-of-principle model starting from the\nframework of masked image modeling (MIM), a common approach in deep\nrepresentation learning. To do so, we analyze how core components of MIM such\nas masking technique and data augmentation influence the formation of\ncategory-specific representations. This allows us not only to better understand\nthe principles behind MIM, but to then reassemble a MIM more in line with the\nfocused nature of biological perception. From a theoretical angle, we find that\nMIM disentangles neurons in latent space, a property that has been suggested to\nstructure visual representations in primates, without explicit regulation.\nTogether with previous findings of invariance learning, this highlights an\ninteresting connection of MIM to latent regularization approaches for\nself-supervised learning. The source code is available under\nhttps://github.com/RobinWeiler/FocusMIM\n", "link": "http://arxiv.org/abs/2404.08526v1", "date": "2024-04-12", "relevancy": 2.2697, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements&body=Title%3A%20Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements%0AAuthor%3A%20Robin%20Weiler%20and%20Matthias%20Brucklacher%20and%20Cyriel%20M.%20A.%20Pennartz%20and%20Sander%20M.%20Boht%C3%A9%0AAbstract%3A%20%20%20To%20make%20sense%20of%20their%20surroundings%2C%20intelligent%20systems%20must%20transform%0Acomplex%20sensory%20inputs%20to%20structured%20codes%20that%20are%20reduced%20to%20task-relevant%0Ainformation%20such%20as%20object%20category.%20Biological%20agents%20achieve%20this%20in%20a%0Alargely%20autonomous%20manner%2C%20presumably%20via%20self-%5Callowbreak%20super-%5Callowbreak%0Avised%20learning.%20Whereas%20previous%20attempts%20to%20model%20the%20underlying%20mechanisms%0Awere%20largely%20discriminative%20in%20nature%2C%20there%20is%20ample%20evidence%20that%20the%20brain%0Aemploys%20a%20generative%20model%20of%20the%20world.%20Here%2C%20we%20propose%20that%20eye%20movements%2C%0Ain%20combination%20with%20the%20focused%20nature%20of%20primate%20vision%2C%20constitute%20a%0Agenerative%2C%20self-supervised%20task%20of%20predicting%20and%20revealing%20visual%0Ainformation.%20We%20construct%20a%20proof-of-principle%20model%20starting%20from%20the%0Aframework%20of%20masked%20image%20modeling%20%28MIM%29%2C%20a%20common%20approach%20in%20deep%0Arepresentation%20learning.%20To%20do%20so%2C%20we%20analyze%20how%20core%20components%20of%20MIM%20such%0Aas%20masking%20technique%20and%20data%20augmentation%20influence%20the%20formation%20of%0Acategory-specific%20representations.%20This%20allows%20us%20not%20only%20to%20better%20understand%0Athe%20principles%20behind%20MIM%2C%20but%20to%20then%20reassemble%20a%20MIM%20more%20in%20line%20with%20the%0Afocused%20nature%20of%20biological%20perception.%20From%20a%20theoretical%20angle%2C%20we%20find%20that%0AMIM%20disentangles%20neurons%20in%20latent%20space%2C%20a%20property%20that%20has%20been%20suggested%20to%0Astructure%20visual%20representations%20in%20primates%2C%20without%20explicit%20regulation.%0ATogether%20with%20previous%20findings%20of%20invariance%20learning%2C%20this%20highlights%20an%0Ainteresting%20connection%20of%20MIM%20to%20latent%20regularization%20approaches%20for%0Aself-supervised%20learning.%20The%20source%20code%20is%20available%20under%0Ahttps%3A//github.com/RobinWeiler/FocusMIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08526v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements&entry.906535625=Robin%20Weiler%20and%20Matthias%20Brucklacher%20and%20Cyriel%20M.%20A.%20Pennartz%20and%20Sander%20M.%20Boht%C3%A9&entry.1292438233=%20%20To%20make%20sense%20of%20their%20surroundings%2C%20intelligent%20systems%20must%20transform%0Acomplex%20sensory%20inputs%20to%20structured%20codes%20that%20are%20reduced%20to%20task-relevant%0Ainformation%20such%20as%20object%20category.%20Biological%20agents%20achieve%20this%20in%20a%0Alargely%20autonomous%20manner%2C%20presumably%20via%20self-%5Callowbreak%20super-%5Callowbreak%0Avised%20learning.%20Whereas%20previous%20attempts%20to%20model%20the%20underlying%20mechanisms%0Awere%20largely%20discriminative%20in%20nature%2C%20there%20is%20ample%20evidence%20that%20the%20brain%0Aemploys%20a%20generative%20model%20of%20the%20world.%20Here%2C%20we%20propose%20that%20eye%20movements%2C%0Ain%20combination%20with%20the%20focused%20nature%20of%20primate%20vision%2C%20constitute%20a%0Agenerative%2C%20self-supervised%20task%20of%20predicting%20and%20revealing%20visual%0Ainformation.%20We%20construct%20a%20proof-of-principle%20model%20starting%20from%20the%0Aframework%20of%20masked%20image%20modeling%20%28MIM%29%2C%20a%20common%20approach%20in%20deep%0Arepresentation%20learning.%20To%20do%20so%2C%20we%20analyze%20how%20core%20components%20of%20MIM%20such%0Aas%20masking%20technique%20and%20data%20augmentation%20influence%20the%20formation%20of%0Acategory-specific%20representations.%20This%20allows%20us%20not%20only%20to%20better%20understand%0Athe%20principles%20behind%20MIM%2C%20but%20to%20then%20reassemble%20a%20MIM%20more%20in%20line%20with%20the%0Afocused%20nature%20of%20biological%20perception.%20From%20a%20theoretical%20angle%2C%20we%20find%20that%0AMIM%20disentangles%20neurons%20in%20latent%20space%2C%20a%20property%20that%20has%20been%20suggested%20to%0Astructure%20visual%20representations%20in%20primates%2C%20without%20explicit%20regulation.%0ATogether%20with%20previous%20findings%20of%20invariance%20learning%2C%20this%20highlights%20an%0Ainteresting%20connection%20of%20MIM%20to%20latent%20regularization%20approaches%20for%0Aself-supervised%20learning.%20The%20source%20code%20is%20available%20under%0Ahttps%3A//github.com/RobinWeiler/FocusMIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08526v1&entry.124074799=Read"},
{"title": "Probing the 3D Awareness of Visual Foundation Models", "author": "Mohamed El Banani and Amit Raj and Kevis-Kokitsi Maninis and Abhishek Kar and Yuanzhen Li and Michael Rubinstein and Deqing Sun and Leonidas Guibas and Justin Johnson and Varun Jampani", "abstract": "  Recent advances in large-scale pretraining have yielded visual foundation\nmodels with strong capabilities. Not only can recent models generalize to\narbitrary images for their training task, their intermediate representations\nare useful for other visual tasks such as detection and segmentation. Given\nthat such models can classify, delineate, and localize objects in 2D, we ask\nwhether they also represent their 3D structure? In this work, we analyze the 3D\nawareness of visual foundation models. We posit that 3D awareness implies that\nrepresentations (1) encode the 3D structure of the scene and (2) consistently\nrepresent the surface across views. We conduct a series of experiments using\ntask-specific probes and zero-shot inference procedures on frozen features. Our\nexperiments reveal several limitations of the current models. Our code and\nanalysis can be found at https://github.com/mbanani/probe3d.\n", "link": "http://arxiv.org/abs/2404.08636v1", "date": "2024-04-12", "relevancy": 2.2638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5658}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probing%20the%203D%20Awareness%20of%20Visual%20Foundation%20Models&body=Title%3A%20Probing%20the%203D%20Awareness%20of%20Visual%20Foundation%20Models%0AAuthor%3A%20Mohamed%20El%20Banani%20and%20Amit%20Raj%20and%20Kevis-Kokitsi%20Maninis%20and%20Abhishek%20Kar%20and%20Yuanzhen%20Li%20and%20Michael%20Rubinstein%20and%20Deqing%20Sun%20and%20Leonidas%20Guibas%20and%20Justin%20Johnson%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20Recent%20advances%20in%20large-scale%20pretraining%20have%20yielded%20visual%20foundation%0Amodels%20with%20strong%20capabilities.%20Not%20only%20can%20recent%20models%20generalize%20to%0Aarbitrary%20images%20for%20their%20training%20task%2C%20their%20intermediate%20representations%0Aare%20useful%20for%20other%20visual%20tasks%20such%20as%20detection%20and%20segmentation.%20Given%0Athat%20such%20models%20can%20classify%2C%20delineate%2C%20and%20localize%20objects%20in%202D%2C%20we%20ask%0Awhether%20they%20also%20represent%20their%203D%20structure%3F%20In%20this%20work%2C%20we%20analyze%20the%203D%0Aawareness%20of%20visual%20foundation%20models.%20We%20posit%20that%203D%20awareness%20implies%20that%0Arepresentations%20%281%29%20encode%20the%203D%20structure%20of%20the%20scene%20and%20%282%29%20consistently%0Arepresent%20the%20surface%20across%20views.%20We%20conduct%20a%20series%20of%20experiments%20using%0Atask-specific%20probes%20and%20zero-shot%20inference%20procedures%20on%20frozen%20features.%20Our%0Aexperiments%20reveal%20several%20limitations%20of%20the%20current%20models.%20Our%20code%20and%0Aanalysis%20can%20be%20found%20at%20https%3A//github.com/mbanani/probe3d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08636v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%203D%20Awareness%20of%20Visual%20Foundation%20Models&entry.906535625=Mohamed%20El%20Banani%20and%20Amit%20Raj%20and%20Kevis-Kokitsi%20Maninis%20and%20Abhishek%20Kar%20and%20Yuanzhen%20Li%20and%20Michael%20Rubinstein%20and%20Deqing%20Sun%20and%20Leonidas%20Guibas%20and%20Justin%20Johnson%20and%20Varun%20Jampani&entry.1292438233=%20%20Recent%20advances%20in%20large-scale%20pretraining%20have%20yielded%20visual%20foundation%0Amodels%20with%20strong%20capabilities.%20Not%20only%20can%20recent%20models%20generalize%20to%0Aarbitrary%20images%20for%20their%20training%20task%2C%20their%20intermediate%20representations%0Aare%20useful%20for%20other%20visual%20tasks%20such%20as%20detection%20and%20segmentation.%20Given%0Athat%20such%20models%20can%20classify%2C%20delineate%2C%20and%20localize%20objects%20in%202D%2C%20we%20ask%0Awhether%20they%20also%20represent%20their%203D%20structure%3F%20In%20this%20work%2C%20we%20analyze%20the%203D%0Aawareness%20of%20visual%20foundation%20models.%20We%20posit%20that%203D%20awareness%20implies%20that%0Arepresentations%20%281%29%20encode%20the%203D%20structure%20of%20the%20scene%20and%20%282%29%20consistently%0Arepresent%20the%20surface%20across%20views.%20We%20conduct%20a%20series%20of%20experiments%20using%0Atask-specific%20probes%20and%20zero-shot%20inference%20procedures%20on%20frozen%20features.%20Our%0Aexperiments%20reveal%20several%20limitations%20of%20the%20current%20models.%20Our%20code%20and%0Aanalysis%20can%20be%20found%20at%20https%3A//github.com/mbanani/probe3d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08636v1&entry.124074799=Read"},
{"title": "IDD-X: A Multi-View Dataset for Ego-relative Important Object\n  Localization and Explanation in Dense and Unstructured Traffic", "author": "Chirag Parikh and Rohit Saluja and C. V. Jawahar and Ravi Kiran Sarvadevabhatla", "abstract": "  Intelligent vehicle systems require a deep understanding of the interplay\nbetween road conditions, surrounding entities, and the ego vehicle's driving\nbehavior for safe and efficient navigation. This is particularly critical in\ndeveloping countries where traffic situations are often dense and unstructured\nwith heterogeneous road occupants. Existing datasets, predominantly geared\ntowards structured and sparse traffic scenarios, fall short of capturing the\ncomplexity of driving in such environments. To fill this gap, we present IDD-X,\na large-scale dual-view driving video dataset. With 697K bounding boxes, 9K\nimportant object tracks, and 1-12 objects per video, IDD-X offers comprehensive\nego-relative annotations for multiple important road objects covering 10\ncategories and 19 explanation label categories. The dataset also incorporates\nrearview information to provide a more complete representation of the driving\nenvironment. We also introduce custom-designed deep networks aimed at multiple\nimportant object localization and per-object explanation prediction. Overall,\nour dataset and introduced prediction models form the foundation for studying\nhow road conditions and surrounding entities affect driving behavior in complex\ntraffic situations.\n", "link": "http://arxiv.org/abs/2404.08561v1", "date": "2024-04-12", "relevancy": 2.2327, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5709}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5623}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.549}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IDD-X%3A%20A%20Multi-View%20Dataset%20for%20Ego-relative%20Important%20Object%0A%20%20Localization%20and%20Explanation%20in%20Dense%20and%20Unstructured%20Traffic&body=Title%3A%20IDD-X%3A%20A%20Multi-View%20Dataset%20for%20Ego-relative%20Important%20Object%0A%20%20Localization%20and%20Explanation%20in%20Dense%20and%20Unstructured%20Traffic%0AAuthor%3A%20Chirag%20Parikh%20and%20Rohit%20Saluja%20and%20C.%20V.%20Jawahar%20and%20Ravi%20Kiran%20Sarvadevabhatla%0AAbstract%3A%20%20%20Intelligent%20vehicle%20systems%20require%20a%20deep%20understanding%20of%20the%20interplay%0Abetween%20road%20conditions%2C%20surrounding%20entities%2C%20and%20the%20ego%20vehicle%27s%20driving%0Abehavior%20for%20safe%20and%20efficient%20navigation.%20This%20is%20particularly%20critical%20in%0Adeveloping%20countries%20where%20traffic%20situations%20are%20often%20dense%20and%20unstructured%0Awith%20heterogeneous%20road%20occupants.%20Existing%20datasets%2C%20predominantly%20geared%0Atowards%20structured%20and%20sparse%20traffic%20scenarios%2C%20fall%20short%20of%20capturing%20the%0Acomplexity%20of%20driving%20in%20such%20environments.%20To%20fill%20this%20gap%2C%20we%20present%20IDD-X%2C%0Aa%20large-scale%20dual-view%20driving%20video%20dataset.%20With%20697K%20bounding%20boxes%2C%209K%0Aimportant%20object%20tracks%2C%20and%201-12%20objects%20per%20video%2C%20IDD-X%20offers%20comprehensive%0Aego-relative%20annotations%20for%20multiple%20important%20road%20objects%20covering%2010%0Acategories%20and%2019%20explanation%20label%20categories.%20The%20dataset%20also%20incorporates%0Arearview%20information%20to%20provide%20a%20more%20complete%20representation%20of%20the%20driving%0Aenvironment.%20We%20also%20introduce%20custom-designed%20deep%20networks%20aimed%20at%20multiple%0Aimportant%20object%20localization%20and%20per-object%20explanation%20prediction.%20Overall%2C%0Aour%20dataset%20and%20introduced%20prediction%20models%20form%20the%20foundation%20for%20studying%0Ahow%20road%20conditions%20and%20surrounding%20entities%20affect%20driving%20behavior%20in%20complex%0Atraffic%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08561v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDD-X%3A%20A%20Multi-View%20Dataset%20for%20Ego-relative%20Important%20Object%0A%20%20Localization%20and%20Explanation%20in%20Dense%20and%20Unstructured%20Traffic&entry.906535625=Chirag%20Parikh%20and%20Rohit%20Saluja%20and%20C.%20V.%20Jawahar%20and%20Ravi%20Kiran%20Sarvadevabhatla&entry.1292438233=%20%20Intelligent%20vehicle%20systems%20require%20a%20deep%20understanding%20of%20the%20interplay%0Abetween%20road%20conditions%2C%20surrounding%20entities%2C%20and%20the%20ego%20vehicle%27s%20driving%0Abehavior%20for%20safe%20and%20efficient%20navigation.%20This%20is%20particularly%20critical%20in%0Adeveloping%20countries%20where%20traffic%20situations%20are%20often%20dense%20and%20unstructured%0Awith%20heterogeneous%20road%20occupants.%20Existing%20datasets%2C%20predominantly%20geared%0Atowards%20structured%20and%20sparse%20traffic%20scenarios%2C%20fall%20short%20of%20capturing%20the%0Acomplexity%20of%20driving%20in%20such%20environments.%20To%20fill%20this%20gap%2C%20we%20present%20IDD-X%2C%0Aa%20large-scale%20dual-view%20driving%20video%20dataset.%20With%20697K%20bounding%20boxes%2C%209K%0Aimportant%20object%20tracks%2C%20and%201-12%20objects%20per%20video%2C%20IDD-X%20offers%20comprehensive%0Aego-relative%20annotations%20for%20multiple%20important%20road%20objects%20covering%2010%0Acategories%20and%2019%20explanation%20label%20categories.%20The%20dataset%20also%20incorporates%0Arearview%20information%20to%20provide%20a%20more%20complete%20representation%20of%20the%20driving%0Aenvironment.%20We%20also%20introduce%20custom-designed%20deep%20networks%20aimed%20at%20multiple%0Aimportant%20object%20localization%20and%20per-object%20explanation%20prediction.%20Overall%2C%0Aour%20dataset%20and%20introduced%20prediction%20models%20form%20the%20foundation%20for%20studying%0Ahow%20road%20conditions%20and%20surrounding%20entities%20affect%20driving%20behavior%20in%20complex%0Atraffic%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08561v1&entry.124074799=Read"},
{"title": "FusionPortableV2: A Unified Multi-Sensor Dataset for Generalized SLAM\n  Across Diverse Platforms and Scalable Environments", "author": "Hexiang Wei and Jianhao Jiao and Xiangcheng Hu and Jingwen Yu and Xupeng Xie and Jin Wu and Yilong Zhu and Yuxuan Liu and Lujia Wang and Ming Liu", "abstract": "  Simultaneous Localization and Mapping (SLAM) technology has been widely\napplied in various robotic scenarios, from rescue operations to autonomous\ndriving. However, the generalization of SLAM algorithms remains a significant\nchallenge, as current datasets often lack scalability in terms of platforms and\nenvironments. To address this limitation, we present FusionPortableV2, a\nmulti-sensor SLAM dataset featuring notable sensor diversity, varied motion\npatterns, and a wide range of environmental scenarios. Our dataset comprises\n$27$ sequences, spanning over $2.5$ hours and collected from four distinct\nplatforms: a handheld suite, wheeled and legged robots, and vehicles. These\nsequences cover diverse settings, including buildings, campuses, and urban\nareas, with a total length of $38.7km$. Additionally, the dataset includes\nground-truth (GT) trajectories and RGB point cloud maps covering approximately\n$0.3km^2$. To validate the utility of our dataset in advancing SLAM research,\nwe assess several state-of-the-art (SOTA) SLAM algorithms. Furthermore, we\ndemonstrate the dataset's broad applicability beyond traditional SLAM tasks by\ninvestigating its potential for monocular depth estimation. The complete\ndataset, including sensor data, GT, and calibration details, is accessible at\nhttps://fusionportable.github.io/dataset/fusionportable_v2.\n", "link": "http://arxiv.org/abs/2404.08563v1", "date": "2024-04-12", "relevancy": 2.227, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FusionPortableV2%3A%20A%20Unified%20Multi-Sensor%20Dataset%20for%20Generalized%20SLAM%0A%20%20Across%20Diverse%20Platforms%20and%20Scalable%20Environments&body=Title%3A%20FusionPortableV2%3A%20A%20Unified%20Multi-Sensor%20Dataset%20for%20Generalized%20SLAM%0A%20%20Across%20Diverse%20Platforms%20and%20Scalable%20Environments%0AAuthor%3A%20Hexiang%20Wei%20and%20Jianhao%20Jiao%20and%20Xiangcheng%20Hu%20and%20Jingwen%20Yu%20and%20Xupeng%20Xie%20and%20Jin%20Wu%20and%20Yilong%20Zhu%20and%20Yuxuan%20Liu%20and%20Lujia%20Wang%20and%20Ming%20Liu%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20technology%20has%20been%20widely%0Aapplied%20in%20various%20robotic%20scenarios%2C%20from%20rescue%20operations%20to%20autonomous%0Adriving.%20However%2C%20the%20generalization%20of%20SLAM%20algorithms%20remains%20a%20significant%0Achallenge%2C%20as%20current%20datasets%20often%20lack%20scalability%20in%20terms%20of%20platforms%20and%0Aenvironments.%20To%20address%20this%20limitation%2C%20we%20present%20FusionPortableV2%2C%20a%0Amulti-sensor%20SLAM%20dataset%20featuring%20notable%20sensor%20diversity%2C%20varied%20motion%0Apatterns%2C%20and%20a%20wide%20range%20of%20environmental%20scenarios.%20Our%20dataset%20comprises%0A%2427%24%20sequences%2C%20spanning%20over%20%242.5%24%20hours%20and%20collected%20from%20four%20distinct%0Aplatforms%3A%20a%20handheld%20suite%2C%20wheeled%20and%20legged%20robots%2C%20and%20vehicles.%20These%0Asequences%20cover%20diverse%20settings%2C%20including%20buildings%2C%20campuses%2C%20and%20urban%0Aareas%2C%20with%20a%20total%20length%20of%20%2438.7km%24.%20Additionally%2C%20the%20dataset%20includes%0Aground-truth%20%28GT%29%20trajectories%20and%20RGB%20point%20cloud%20maps%20covering%20approximately%0A%240.3km%5E2%24.%20To%20validate%20the%20utility%20of%20our%20dataset%20in%20advancing%20SLAM%20research%2C%0Awe%20assess%20several%20state-of-the-art%20%28SOTA%29%20SLAM%20algorithms.%20Furthermore%2C%20we%0Ademonstrate%20the%20dataset%27s%20broad%20applicability%20beyond%20traditional%20SLAM%20tasks%20by%0Ainvestigating%20its%20potential%20for%20monocular%20depth%20estimation.%20The%20complete%0Adataset%2C%20including%20sensor%20data%2C%20GT%2C%20and%20calibration%20details%2C%20is%20accessible%20at%0Ahttps%3A//fusionportable.github.io/dataset/fusionportable_v2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08563v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionPortableV2%3A%20A%20Unified%20Multi-Sensor%20Dataset%20for%20Generalized%20SLAM%0A%20%20Across%20Diverse%20Platforms%20and%20Scalable%20Environments&entry.906535625=Hexiang%20Wei%20and%20Jianhao%20Jiao%20and%20Xiangcheng%20Hu%20and%20Jingwen%20Yu%20and%20Xupeng%20Xie%20and%20Jin%20Wu%20and%20Yilong%20Zhu%20and%20Yuxuan%20Liu%20and%20Lujia%20Wang%20and%20Ming%20Liu&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20technology%20has%20been%20widely%0Aapplied%20in%20various%20robotic%20scenarios%2C%20from%20rescue%20operations%20to%20autonomous%0Adriving.%20However%2C%20the%20generalization%20of%20SLAM%20algorithms%20remains%20a%20significant%0Achallenge%2C%20as%20current%20datasets%20often%20lack%20scalability%20in%20terms%20of%20platforms%20and%0Aenvironments.%20To%20address%20this%20limitation%2C%20we%20present%20FusionPortableV2%2C%20a%0Amulti-sensor%20SLAM%20dataset%20featuring%20notable%20sensor%20diversity%2C%20varied%20motion%0Apatterns%2C%20and%20a%20wide%20range%20of%20environmental%20scenarios.%20Our%20dataset%20comprises%0A%2427%24%20sequences%2C%20spanning%20over%20%242.5%24%20hours%20and%20collected%20from%20four%20distinct%0Aplatforms%3A%20a%20handheld%20suite%2C%20wheeled%20and%20legged%20robots%2C%20and%20vehicles.%20These%0Asequences%20cover%20diverse%20settings%2C%20including%20buildings%2C%20campuses%2C%20and%20urban%0Aareas%2C%20with%20a%20total%20length%20of%20%2438.7km%24.%20Additionally%2C%20the%20dataset%20includes%0Aground-truth%20%28GT%29%20trajectories%20and%20RGB%20point%20cloud%20maps%20covering%20approximately%0A%240.3km%5E2%24.%20To%20validate%20the%20utility%20of%20our%20dataset%20in%20advancing%20SLAM%20research%2C%0Awe%20assess%20several%20state-of-the-art%20%28SOTA%29%20SLAM%20algorithms.%20Furthermore%2C%20we%0Ademonstrate%20the%20dataset%27s%20broad%20applicability%20beyond%20traditional%20SLAM%20tasks%20by%0Ainvestigating%20its%20potential%20for%20monocular%20depth%20estimation.%20The%20complete%0Adataset%2C%20including%20sensor%20data%2C%20GT%2C%20and%20calibration%20details%2C%20is%20accessible%20at%0Ahttps%3A//fusionportable.github.io/dataset/fusionportable_v2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08563v1&entry.124074799=Read"},
{"title": "Improving Referring Image Segmentation using Vision-Aware Text Features", "author": "Hai Nguyen-Truong and E-Ro Nguyen and Tuan-Anh Vu and Minh-Triet Tran and Binh-Son Hua and Sai-Kit Yeung", "abstract": "  Referring image segmentation is a challenging task that involves generating\npixel-wise segmentation masks based on natural language descriptions. Existing\nmethods have relied mostly on visual features to generate the segmentation\nmasks while treating text features as supporting components. This over-reliance\non visual features can lead to suboptimal results, especially in complex\nscenarios where text prompts are ambiguous or context-dependent. To overcome\nthese challenges, we present a novel framework VATEX to improve referring image\nsegmentation by enhancing object and context understanding with Vision-Aware\nText Feature. Our method involves using CLIP to derive a CLIP Prior that\nintegrates an object-centric visual heatmap with text description, which can be\nused as the initial query in DETR-based architecture for the segmentation task.\nFurthermore, by observing that there are multiple ways to describe an instance\nin an image, we enforce feature similarity between text variations referring to\nthe same visual input by two components: a novel Contextual Multimodal Decoder\nthat turns text embeddings into vision-aware text features, and a Meaning\nConsistency Constraint to ensure further the coherent and consistent\ninterpretation of language expressions with the context understanding obtained\nfrom the image. Our method achieves a significant performance improvement on\nthree benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at:\nhttps://nero1342.github.io/VATEX\\_RIS.\n", "link": "http://arxiv.org/abs/2404.08590v1", "date": "2024-04-12", "relevancy": 2.2056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5366}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Referring%20Image%20Segmentation%20using%20Vision-Aware%20Text%20Features&body=Title%3A%20Improving%20Referring%20Image%20Segmentation%20using%20Vision-Aware%20Text%20Features%0AAuthor%3A%20Hai%20Nguyen-Truong%20and%20E-Ro%20Nguyen%20and%20Tuan-Anh%20Vu%20and%20Minh-Triet%20Tran%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20Referring%20image%20segmentation%20is%20a%20challenging%20task%20that%20involves%20generating%0Apixel-wise%20segmentation%20masks%20based%20on%20natural%20language%20descriptions.%20Existing%0Amethods%20have%20relied%20mostly%20on%20visual%20features%20to%20generate%20the%20segmentation%0Amasks%20while%20treating%20text%20features%20as%20supporting%20components.%20This%20over-reliance%0Aon%20visual%20features%20can%20lead%20to%20suboptimal%20results%2C%20especially%20in%20complex%0Ascenarios%20where%20text%20prompts%20are%20ambiguous%20or%20context-dependent.%20To%20overcome%0Athese%20challenges%2C%20we%20present%20a%20novel%20framework%20VATEX%20to%20improve%20referring%20image%0Asegmentation%20by%20enhancing%20object%20and%20context%20understanding%20with%20Vision-Aware%0AText%20Feature.%20Our%20method%20involves%20using%20CLIP%20to%20derive%20a%20CLIP%20Prior%20that%0Aintegrates%20an%20object-centric%20visual%20heatmap%20with%20text%20description%2C%20which%20can%20be%0Aused%20as%20the%20initial%20query%20in%20DETR-based%20architecture%20for%20the%20segmentation%20task.%0AFurthermore%2C%20by%20observing%20that%20there%20are%20multiple%20ways%20to%20describe%20an%20instance%0Ain%20an%20image%2C%20we%20enforce%20feature%20similarity%20between%20text%20variations%20referring%20to%0Athe%20same%20visual%20input%20by%20two%20components%3A%20a%20novel%20Contextual%20Multimodal%20Decoder%0Athat%20turns%20text%20embeddings%20into%20vision-aware%20text%20features%2C%20and%20a%20Meaning%0AConsistency%20Constraint%20to%20ensure%20further%20the%20coherent%20and%20consistent%0Ainterpretation%20of%20language%20expressions%20with%20the%20context%20understanding%20obtained%0Afrom%20the%20image.%20Our%20method%20achieves%20a%20significant%20performance%20improvement%20on%0Athree%20benchmark%20datasets%20RefCOCO%2C%20RefCOCO%2B%20and%20G-Ref.%20Code%20is%20available%20at%3A%0Ahttps%3A//nero1342.github.io/VATEX%5C_RIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08590v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Referring%20Image%20Segmentation%20using%20Vision-Aware%20Text%20Features&entry.906535625=Hai%20Nguyen-Truong%20and%20E-Ro%20Nguyen%20and%20Tuan-Anh%20Vu%20and%20Minh-Triet%20Tran%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20Referring%20image%20segmentation%20is%20a%20challenging%20task%20that%20involves%20generating%0Apixel-wise%20segmentation%20masks%20based%20on%20natural%20language%20descriptions.%20Existing%0Amethods%20have%20relied%20mostly%20on%20visual%20features%20to%20generate%20the%20segmentation%0Amasks%20while%20treating%20text%20features%20as%20supporting%20components.%20This%20over-reliance%0Aon%20visual%20features%20can%20lead%20to%20suboptimal%20results%2C%20especially%20in%20complex%0Ascenarios%20where%20text%20prompts%20are%20ambiguous%20or%20context-dependent.%20To%20overcome%0Athese%20challenges%2C%20we%20present%20a%20novel%20framework%20VATEX%20to%20improve%20referring%20image%0Asegmentation%20by%20enhancing%20object%20and%20context%20understanding%20with%20Vision-Aware%0AText%20Feature.%20Our%20method%20involves%20using%20CLIP%20to%20derive%20a%20CLIP%20Prior%20that%0Aintegrates%20an%20object-centric%20visual%20heatmap%20with%20text%20description%2C%20which%20can%20be%0Aused%20as%20the%20initial%20query%20in%20DETR-based%20architecture%20for%20the%20segmentation%20task.%0AFurthermore%2C%20by%20observing%20that%20there%20are%20multiple%20ways%20to%20describe%20an%20instance%0Ain%20an%20image%2C%20we%20enforce%20feature%20similarity%20between%20text%20variations%20referring%20to%0Athe%20same%20visual%20input%20by%20two%20components%3A%20a%20novel%20Contextual%20Multimodal%20Decoder%0Athat%20turns%20text%20embeddings%20into%20vision-aware%20text%20features%2C%20and%20a%20Meaning%0AConsistency%20Constraint%20to%20ensure%20further%20the%20coherent%20and%20consistent%0Ainterpretation%20of%20language%20expressions%20with%20the%20context%20understanding%20obtained%0Afrom%20the%20image.%20Our%20method%20achieves%20a%20significant%20performance%20improvement%20on%0Athree%20benchmark%20datasets%20RefCOCO%2C%20RefCOCO%2B%20and%20G-Ref.%20Code%20is%20available%20at%3A%0Ahttps%3A//nero1342.github.io/VATEX%5C_RIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08590v1&entry.124074799=Read"},
{"title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models", "author": "Yuzhang Shang and Mu Cai and Bingxin Xu and Yong Jae Lee and Yan Yan", "abstract": "  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 18 times on average, and achieve\ncomparable performance across diverse visual question-answering and reasoning\ntasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n", "link": "http://arxiv.org/abs/2403.15388v4", "date": "2024-04-12", "relevancy": 2.2045, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5488}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4887}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models&body=Title%3A%20LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models%0AAuthor%3A%20Yuzhang%20Shang%20and%20Mu%20Cai%20and%20Bingxin%20Xu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20significant%20reasoning%20capabilities%0Aby%20connecting%20a%20visual%20encoder%20and%20a%20large%20language%20model.%20LMMs%20typically%20use%20a%0Afixed%20amount%20of%20visual%20tokens%2C%20such%20as%20the%20penultimate%20layer%20features%20in%20the%0ACLIP%20visual%20encoder%2C%20as%20the%20prefix%20content.%20Recent%20LMMs%20incorporate%20more%0Acomplex%20visual%20inputs%2C%20such%20as%20high-resolution%20images%20and%20videos%2C%20which%0Aincrease%20the%20number%20of%20visual%20tokens%20significantly.%20However%2C%20due%20to%20the%20design%0Aof%20the%20Transformer%20architecture%2C%20computational%20costs%20associated%20with%20these%0Amodels%20tend%20to%20increase%20quadratically%20with%20the%20number%20of%20input%20tokens.%20To%0Atackle%20this%20problem%2C%20we%20explore%20a%20token%20reduction%20mechanism%20and%20find%2C%20similar%0Ato%20prior%20work%2C%20that%20many%20visual%20tokens%20are%20spatially%20redundant.%20Based%20on%20this%2C%0Awe%20propose%20PruMerge%2C%20a%20novel%20adaptive%20visual%20token%20reduction%20approach%2C%20which%0Alargely%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20comparable%20model%0Aperformance.%20We%20first%20select%20the%20unpruned%20visual%20tokens%20based%20on%20their%0Asimilarity%20to%20class%20tokens%20and%20spatial%20tokens.%20We%20then%20cluster%20the%20pruned%0Atokens%20based%20on%20key%20similarity%20and%20merge%20the%20clustered%20tokens%20with%20the%20unpruned%0Atokens%20to%20supplement%20their%20information.%20Empirically%2C%20when%20applied%20to%20LLaVA-1.5%2C%0Aour%20approach%20can%20compress%20the%20visual%20tokens%20by%2018%20times%20on%20average%2C%20and%20achieve%0Acomparable%20performance%20across%20diverse%20visual%20question-answering%20and%20reasoning%0Atasks.%20Code%20and%20checkpoints%20are%20at%20https%3A//llava-prumerge.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15388v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models&entry.906535625=Yuzhang%20Shang%20and%20Mu%20Cai%20and%20Bingxin%20Xu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20significant%20reasoning%20capabilities%0Aby%20connecting%20a%20visual%20encoder%20and%20a%20large%20language%20model.%20LMMs%20typically%20use%20a%0Afixed%20amount%20of%20visual%20tokens%2C%20such%20as%20the%20penultimate%20layer%20features%20in%20the%0ACLIP%20visual%20encoder%2C%20as%20the%20prefix%20content.%20Recent%20LMMs%20incorporate%20more%0Acomplex%20visual%20inputs%2C%20such%20as%20high-resolution%20images%20and%20videos%2C%20which%0Aincrease%20the%20number%20of%20visual%20tokens%20significantly.%20However%2C%20due%20to%20the%20design%0Aof%20the%20Transformer%20architecture%2C%20computational%20costs%20associated%20with%20these%0Amodels%20tend%20to%20increase%20quadratically%20with%20the%20number%20of%20input%20tokens.%20To%0Atackle%20this%20problem%2C%20we%20explore%20a%20token%20reduction%20mechanism%20and%20find%2C%20similar%0Ato%20prior%20work%2C%20that%20many%20visual%20tokens%20are%20spatially%20redundant.%20Based%20on%20this%2C%0Awe%20propose%20PruMerge%2C%20a%20novel%20adaptive%20visual%20token%20reduction%20approach%2C%20which%0Alargely%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20comparable%20model%0Aperformance.%20We%20first%20select%20the%20unpruned%20visual%20tokens%20based%20on%20their%0Asimilarity%20to%20class%20tokens%20and%20spatial%20tokens.%20We%20then%20cluster%20the%20pruned%0Atokens%20based%20on%20key%20similarity%20and%20merge%20the%20clustered%20tokens%20with%20the%20unpruned%0Atokens%20to%20supplement%20their%20information.%20Empirically%2C%20when%20applied%20to%20LLaVA-1.5%2C%0Aour%20approach%20can%20compress%20the%20visual%20tokens%20by%2018%20times%20on%20average%2C%20and%20achieve%0Acomparable%20performance%20across%20diverse%20visual%20question-answering%20and%20reasoning%0Atasks.%20Code%20and%20checkpoints%20are%20at%20https%3A//llava-prumerge.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15388v4&entry.124074799=Read"},
{"title": "Harnessing the Power of Large Language Model for Uncertainty Aware Graph\n  Processing", "author": "Zhenyu Qian and Yiming Qian and Yuting Song and Fei Gao and Hai Jin and Chen Yu and Xia Xie", "abstract": "  Handling graph data is one of the most difficult tasks. Traditional\ntechniques, such as those based on geometry and matrix factorization, rely on\nassumptions about the data relations that become inadequate when handling large\nand complex graph data. On the other hand, deep learning approaches demonstrate\npromising results in handling large graph data, but they often fall short of\nproviding interpretable explanations. To equip the graph processing with both\nhigh accuracy and explainability, we introduce a novel approach that harnesses\nthe power of a large language model (LLM), enhanced by an uncertainty-aware\nmodule to provide a confidence score on the generated answer. We experiment\nwith our approach on two graph processing tasks: few-shot knowledge graph\ncompletion and graph classification. Our results demonstrate that through\nparameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms\nby a substantial margin across ten diverse benchmark datasets. Moreover, to\naddress the challenge of explainability, we propose an uncertainty estimation\nbased on perturbation, along with a calibration scheme to quantify the\nconfidence scores of the generated answers. Our confidence measure achieves an\nAUC of 0.8 or higher on seven out of the ten datasets in predicting the\ncorrectness of the answer generated by LLM.\n", "link": "http://arxiv.org/abs/2404.00589v2", "date": "2024-04-12", "relevancy": 2.16, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5887}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.511}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Power%20of%20Large%20Language%20Model%20for%20Uncertainty%20Aware%20Graph%0A%20%20Processing&body=Title%3A%20Harnessing%20the%20Power%20of%20Large%20Language%20Model%20for%20Uncertainty%20Aware%20Graph%0A%20%20Processing%0AAuthor%3A%20Zhenyu%20Qian%20and%20Yiming%20Qian%20and%20Yuting%20Song%20and%20Fei%20Gao%20and%20Hai%20Jin%20and%20Chen%20Yu%20and%20Xia%20Xie%0AAbstract%3A%20%20%20Handling%20graph%20data%20is%20one%20of%20the%20most%20difficult%20tasks.%20Traditional%0Atechniques%2C%20such%20as%20those%20based%20on%20geometry%20and%20matrix%20factorization%2C%20rely%20on%0Aassumptions%20about%20the%20data%20relations%20that%20become%20inadequate%20when%20handling%20large%0Aand%20complex%20graph%20data.%20On%20the%20other%20hand%2C%20deep%20learning%20approaches%20demonstrate%0Apromising%20results%20in%20handling%20large%20graph%20data%2C%20but%20they%20often%20fall%20short%20of%0Aproviding%20interpretable%20explanations.%20To%20equip%20the%20graph%20processing%20with%20both%0Ahigh%20accuracy%20and%20explainability%2C%20we%20introduce%20a%20novel%20approach%20that%20harnesses%0Athe%20power%20of%20a%20large%20language%20model%20%28LLM%29%2C%20enhanced%20by%20an%20uncertainty-aware%0Amodule%20to%20provide%20a%20confidence%20score%20on%20the%20generated%20answer.%20We%20experiment%0Awith%20our%20approach%20on%20two%20graph%20processing%20tasks%3A%20few-shot%20knowledge%20graph%0Acompletion%20and%20graph%20classification.%20Our%20results%20demonstrate%20that%20through%0Aparameter%20efficient%20fine-tuning%2C%20the%20LLM%20surpasses%20state-of-the-art%20algorithms%0Aby%20a%20substantial%20margin%20across%20ten%20diverse%20benchmark%20datasets.%20Moreover%2C%20to%0Aaddress%20the%20challenge%20of%20explainability%2C%20we%20propose%20an%20uncertainty%20estimation%0Abased%20on%20perturbation%2C%20along%20with%20a%20calibration%20scheme%20to%20quantify%20the%0Aconfidence%20scores%20of%20the%20generated%20answers.%20Our%20confidence%20measure%20achieves%20an%0AAUC%20of%200.8%20or%20higher%20on%20seven%20out%20of%20the%20ten%20datasets%20in%20predicting%20the%0Acorrectness%20of%20the%20answer%20generated%20by%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00589v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Power%20of%20Large%20Language%20Model%20for%20Uncertainty%20Aware%20Graph%0A%20%20Processing&entry.906535625=Zhenyu%20Qian%20and%20Yiming%20Qian%20and%20Yuting%20Song%20and%20Fei%20Gao%20and%20Hai%20Jin%20and%20Chen%20Yu%20and%20Xia%20Xie&entry.1292438233=%20%20Handling%20graph%20data%20is%20one%20of%20the%20most%20difficult%20tasks.%20Traditional%0Atechniques%2C%20such%20as%20those%20based%20on%20geometry%20and%20matrix%20factorization%2C%20rely%20on%0Aassumptions%20about%20the%20data%20relations%20that%20become%20inadequate%20when%20handling%20large%0Aand%20complex%20graph%20data.%20On%20the%20other%20hand%2C%20deep%20learning%20approaches%20demonstrate%0Apromising%20results%20in%20handling%20large%20graph%20data%2C%20but%20they%20often%20fall%20short%20of%0Aproviding%20interpretable%20explanations.%20To%20equip%20the%20graph%20processing%20with%20both%0Ahigh%20accuracy%20and%20explainability%2C%20we%20introduce%20a%20novel%20approach%20that%20harnesses%0Athe%20power%20of%20a%20large%20language%20model%20%28LLM%29%2C%20enhanced%20by%20an%20uncertainty-aware%0Amodule%20to%20provide%20a%20confidence%20score%20on%20the%20generated%20answer.%20We%20experiment%0Awith%20our%20approach%20on%20two%20graph%20processing%20tasks%3A%20few-shot%20knowledge%20graph%0Acompletion%20and%20graph%20classification.%20Our%20results%20demonstrate%20that%20through%0Aparameter%20efficient%20fine-tuning%2C%20the%20LLM%20surpasses%20state-of-the-art%20algorithms%0Aby%20a%20substantial%20margin%20across%20ten%20diverse%20benchmark%20datasets.%20Moreover%2C%20to%0Aaddress%20the%20challenge%20of%20explainability%2C%20we%20propose%20an%20uncertainty%20estimation%0Abased%20on%20perturbation%2C%20along%20with%20a%20calibration%20scheme%20to%20quantify%20the%0Aconfidence%20scores%20of%20the%20generated%20answers.%20Our%20confidence%20measure%20achieves%20an%0AAUC%20of%200.8%20or%20higher%20on%20seven%20out%20of%20the%20ten%20datasets%20in%20predicting%20the%0Acorrectness%20of%20the%20answer%20generated%20by%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00589v2&entry.124074799=Read"},
{"title": "MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face\n  Forgery Detection", "author": "Chenqi Kong and Anwei Luo and Song Xia and Yi Yu and Haoliang Li and Alex C. Kot", "abstract": "  Deepfakes have recently raised significant trust issues and security concerns\namong the public. Compared to CNN face forgery detectors, ViT-based methods\ntake advantage of the expressivity of transformers, achieving superior\ndetection performance. However, these approaches still exhibit the following\nlimitations: (1). Fully fine-tuning ViT-based models from ImageNet weights\ndemands substantial computational and storage resources; (2). ViT-based methods\nstruggle to capture local forgery clues, leading to model bias and limited\ngeneralizability. To tackle these challenges, this work introduces\nMixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalized\nyet parameter-efficient ViT-based approach. MoE-FFD only updates lightweight\nLow-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbone\nfrozen, thereby achieving parameter-efficient training. Moreover, MoE-FFD\nleverages the expressivity of transformers and local priors of CNNs to\nsimultaneously extract global and local forgery clues. Additionally, novel MoE\nmodules are designed to scale the model's capacity and select optimal forgery\nexperts, further enhancing forgery detection performance. The proposed MoE\nlearning scheme can be seamlessly adapted to various transformer backbones in a\nplug-and-play manner. Extensive experimental results demonstrate that the\nproposed method achieves state-of-the-art face forgery detection performance\nwith reduced parameter overhead. The code will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2404.08452v1", "date": "2024-04-12", "relevancy": 2.147, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.56}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5206}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection&body=Title%3A%20MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection%0AAuthor%3A%20Chenqi%20Kong%20and%20Anwei%20Luo%20and%20Song%20Xia%20and%20Yi%20Yu%20and%20Haoliang%20Li%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20Deepfakes%20have%20recently%20raised%20significant%20trust%20issues%20and%20security%20concerns%0Aamong%20the%20public.%20Compared%20to%20CNN%20face%20forgery%20detectors%2C%20ViT-based%20methods%0Atake%20advantage%20of%20the%20expressivity%20of%20transformers%2C%20achieving%20superior%0Adetection%20performance.%20However%2C%20these%20approaches%20still%20exhibit%20the%20following%0Alimitations%3A%20%281%29.%20Fully%20fine-tuning%20ViT-based%20models%20from%20ImageNet%20weights%0Ademands%20substantial%20computational%20and%20storage%20resources%3B%20%282%29.%20ViT-based%20methods%0Astruggle%20to%20capture%20local%20forgery%20clues%2C%20leading%20to%20model%20bias%20and%20limited%0Ageneralizability.%20To%20tackle%20these%20challenges%2C%20this%20work%20introduces%0AMixture-of-Experts%20modules%20for%20Face%20Forgery%20Detection%20%28MoE-FFD%29%2C%20a%20generalized%0Ayet%20parameter-efficient%20ViT-based%20approach.%20MoE-FFD%20only%20updates%20lightweight%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20Adapter%20layers%20while%20keeping%20the%20ViT%20backbone%0Afrozen%2C%20thereby%20achieving%20parameter-efficient%20training.%20Moreover%2C%20MoE-FFD%0Aleverages%20the%20expressivity%20of%20transformers%20and%20local%20priors%20of%20CNNs%20to%0Asimultaneously%20extract%20global%20and%20local%20forgery%20clues.%20Additionally%2C%20novel%20MoE%0Amodules%20are%20designed%20to%20scale%20the%20model%27s%20capacity%20and%20select%20optimal%20forgery%0Aexperts%2C%20further%20enhancing%20forgery%20detection%20performance.%20The%20proposed%20MoE%0Alearning%20scheme%20can%20be%20seamlessly%20adapted%20to%20various%20transformer%20backbones%20in%20a%0Aplug-and-play%20manner.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20face%20forgery%20detection%20performance%0Awith%20reduced%20parameter%20overhead.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08452v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-FFD%3A%20Mixture%20of%20Experts%20for%20Generalized%20and%20Parameter-Efficient%20Face%0A%20%20Forgery%20Detection&entry.906535625=Chenqi%20Kong%20and%20Anwei%20Luo%20and%20Song%20Xia%20and%20Yi%20Yu%20and%20Haoliang%20Li%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20Deepfakes%20have%20recently%20raised%20significant%20trust%20issues%20and%20security%20concerns%0Aamong%20the%20public.%20Compared%20to%20CNN%20face%20forgery%20detectors%2C%20ViT-based%20methods%0Atake%20advantage%20of%20the%20expressivity%20of%20transformers%2C%20achieving%20superior%0Adetection%20performance.%20However%2C%20these%20approaches%20still%20exhibit%20the%20following%0Alimitations%3A%20%281%29.%20Fully%20fine-tuning%20ViT-based%20models%20from%20ImageNet%20weights%0Ademands%20substantial%20computational%20and%20storage%20resources%3B%20%282%29.%20ViT-based%20methods%0Astruggle%20to%20capture%20local%20forgery%20clues%2C%20leading%20to%20model%20bias%20and%20limited%0Ageneralizability.%20To%20tackle%20these%20challenges%2C%20this%20work%20introduces%0AMixture-of-Experts%20modules%20for%20Face%20Forgery%20Detection%20%28MoE-FFD%29%2C%20a%20generalized%0Ayet%20parameter-efficient%20ViT-based%20approach.%20MoE-FFD%20only%20updates%20lightweight%0ALow-Rank%20Adaptation%20%28LoRA%29%20and%20Adapter%20layers%20while%20keeping%20the%20ViT%20backbone%0Afrozen%2C%20thereby%20achieving%20parameter-efficient%20training.%20Moreover%2C%20MoE-FFD%0Aleverages%20the%20expressivity%20of%20transformers%20and%20local%20priors%20of%20CNNs%20to%0Asimultaneously%20extract%20global%20and%20local%20forgery%20clues.%20Additionally%2C%20novel%20MoE%0Amodules%20are%20designed%20to%20scale%20the%20model%27s%20capacity%20and%20select%20optimal%20forgery%0Aexperts%2C%20further%20enhancing%20forgery%20detection%20performance.%20The%20proposed%20MoE%0Alearning%20scheme%20can%20be%20seamlessly%20adapted%20to%20various%20transformer%20backbones%20in%20a%0Aplug-and-play%20manner.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20face%20forgery%20detection%20performance%0Awith%20reduced%20parameter%20overhead.%20The%20code%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08452v1&entry.124074799=Read"},
{"title": "OTTER: Improving Zero-Shot Classification via Optimal Transport", "author": "Changho Shin and Jitian Zhao and Sonia Cromp and Harit Vishwakarma and Frederic Sala", "abstract": "  Popular zero-shot models suffer due to artifacts inherited from pretraining.\nA particularly detrimental artifact, caused by unbalanced web-scale pretraining\ndata, is mismatched label distribution. Existing approaches that seek to repair\nthe label distribution are not suitable in zero-shot settings, as they have\nincompatible requirements such as access to labeled downstream task data or\nknowledge of the true label balance in the pretraining distribution. We\nsidestep these challenges and introduce a simple and lightweight approach to\nadjust pretrained model predictions via optimal transport. Our technique\nrequires only an estimate of the label distribution of a downstream task.\nTheoretically, we characterize the improvement produced by our procedure under\ncertain mild conditions and provide bounds on the error caused by\nmisspecification. Empirically, we validate our method in a wide array of\nzero-shot image and text classification tasks, improving accuracy by 4.8% and\n15.9% on average, and beating baselines like Prior Matching -- often by\nsignificant margins -- in 17 out of 21 datasets.\n", "link": "http://arxiv.org/abs/2404.08461v1", "date": "2024-04-12", "relevancy": 2.1431, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5391}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5137}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OTTER%3A%20Improving%20Zero-Shot%20Classification%20via%20Optimal%20Transport&body=Title%3A%20OTTER%3A%20Improving%20Zero-Shot%20Classification%20via%20Optimal%20Transport%0AAuthor%3A%20Changho%20Shin%20and%20Jitian%20Zhao%20and%20Sonia%20Cromp%20and%20Harit%20Vishwakarma%20and%20Frederic%20Sala%0AAbstract%3A%20%20%20Popular%20zero-shot%20models%20suffer%20due%20to%20artifacts%20inherited%20from%20pretraining.%0AA%20particularly%20detrimental%20artifact%2C%20caused%20by%20unbalanced%20web-scale%20pretraining%0Adata%2C%20is%20mismatched%20label%20distribution.%20Existing%20approaches%20that%20seek%20to%20repair%0Athe%20label%20distribution%20are%20not%20suitable%20in%20zero-shot%20settings%2C%20as%20they%20have%0Aincompatible%20requirements%20such%20as%20access%20to%20labeled%20downstream%20task%20data%20or%0Aknowledge%20of%20the%20true%20label%20balance%20in%20the%20pretraining%20distribution.%20We%0Asidestep%20these%20challenges%20and%20introduce%20a%20simple%20and%20lightweight%20approach%20to%0Aadjust%20pretrained%20model%20predictions%20via%20optimal%20transport.%20Our%20technique%0Arequires%20only%20an%20estimate%20of%20the%20label%20distribution%20of%20a%20downstream%20task.%0ATheoretically%2C%20we%20characterize%20the%20improvement%20produced%20by%20our%20procedure%20under%0Acertain%20mild%20conditions%20and%20provide%20bounds%20on%20the%20error%20caused%20by%0Amisspecification.%20Empirically%2C%20we%20validate%20our%20method%20in%20a%20wide%20array%20of%0Azero-shot%20image%20and%20text%20classification%20tasks%2C%20improving%20accuracy%20by%204.8%25%20and%0A15.9%25%20on%20average%2C%20and%20beating%20baselines%20like%20Prior%20Matching%20--%20often%20by%0Asignificant%20margins%20--%20in%2017%20out%20of%2021%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08461v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OTTER%3A%20Improving%20Zero-Shot%20Classification%20via%20Optimal%20Transport&entry.906535625=Changho%20Shin%20and%20Jitian%20Zhao%20and%20Sonia%20Cromp%20and%20Harit%20Vishwakarma%20and%20Frederic%20Sala&entry.1292438233=%20%20Popular%20zero-shot%20models%20suffer%20due%20to%20artifacts%20inherited%20from%20pretraining.%0AA%20particularly%20detrimental%20artifact%2C%20caused%20by%20unbalanced%20web-scale%20pretraining%0Adata%2C%20is%20mismatched%20label%20distribution.%20Existing%20approaches%20that%20seek%20to%20repair%0Athe%20label%20distribution%20are%20not%20suitable%20in%20zero-shot%20settings%2C%20as%20they%20have%0Aincompatible%20requirements%20such%20as%20access%20to%20labeled%20downstream%20task%20data%20or%0Aknowledge%20of%20the%20true%20label%20balance%20in%20the%20pretraining%20distribution.%20We%0Asidestep%20these%20challenges%20and%20introduce%20a%20simple%20and%20lightweight%20approach%20to%0Aadjust%20pretrained%20model%20predictions%20via%20optimal%20transport.%20Our%20technique%0Arequires%20only%20an%20estimate%20of%20the%20label%20distribution%20of%20a%20downstream%20task.%0ATheoretically%2C%20we%20characterize%20the%20improvement%20produced%20by%20our%20procedure%20under%0Acertain%20mild%20conditions%20and%20provide%20bounds%20on%20the%20error%20caused%20by%0Amisspecification.%20Empirically%2C%20we%20validate%20our%20method%20in%20a%20wide%20array%20of%0Azero-shot%20image%20and%20text%20classification%20tasks%2C%20improving%20accuracy%20by%204.8%25%20and%0A15.9%25%20on%20average%2C%20and%20beating%20baselines%20like%20Prior%20Matching%20--%20often%20by%0Asignificant%20margins%20--%20in%2017%20out%20of%2021%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08461v1&entry.124074799=Read"},
{"title": "MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for\n  Dynamic Facial Expression Recognition", "author": "Linhuang Wang and Xin Kang and Fei Ding and Satoshi Nakagawa and Fuji Ren", "abstract": "  Unlike typical video action recognition, Dynamic Facial Expression\nRecognition (DFER) does not involve distinct moving targets but relies on\nlocalized changes in facial muscles. Addressing this distinctive attribute, we\npropose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Our\napproach takes spatial features of different scales extracted by CNN and feeds\nthem into a Multi-scale Embedding Layer (MELayer). The MELayer extracts\nmulti-scale spatial information and encodes these features before sending them\ninto a Temporal Transformer (T-Former). The T-Former simultaneously extracts\ntemporal information while continually integrating multi-scale spatial\ninformation. This process culminates in the generation of multi-scale\nspatio-temporal features that are utilized for the final classification. Our\nmethod achieves state-of-the-art results on two in-the-wild datasets.\nFurthermore, a series of ablation experiments and visualizations provide\nfurther validation of our approach's proficiency in leveraging spatio-temporal\ninformation within DFER.\n", "link": "http://arxiv.org/abs/2404.08433v1", "date": "2024-04-12", "relevancy": 2.1293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5319}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5305}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MSSTNet%3A%20A%20Multi-Scale%20Spatio-Temporal%20CNN-Transformer%20Network%20for%0A%20%20Dynamic%20Facial%20Expression%20Recognition&body=Title%3A%20MSSTNet%3A%20A%20Multi-Scale%20Spatio-Temporal%20CNN-Transformer%20Network%20for%0A%20%20Dynamic%20Facial%20Expression%20Recognition%0AAuthor%3A%20Linhuang%20Wang%20and%20Xin%20Kang%20and%20Fei%20Ding%20and%20Satoshi%20Nakagawa%20and%20Fuji%20Ren%0AAbstract%3A%20%20%20Unlike%20typical%20video%20action%20recognition%2C%20Dynamic%20Facial%20Expression%0ARecognition%20%28DFER%29%20does%20not%20involve%20distinct%20moving%20targets%20but%20relies%20on%0Alocalized%20changes%20in%20facial%20muscles.%20Addressing%20this%20distinctive%20attribute%2C%20we%0Apropose%20a%20Multi-Scale%20Spatio-temporal%20CNN-Transformer%20network%20%28MSSTNet%29.%20Our%0Aapproach%20takes%20spatial%20features%20of%20different%20scales%20extracted%20by%20CNN%20and%20feeds%0Athem%20into%20a%20Multi-scale%20Embedding%20Layer%20%28MELayer%29.%20The%20MELayer%20extracts%0Amulti-scale%20spatial%20information%20and%20encodes%20these%20features%20before%20sending%20them%0Ainto%20a%20Temporal%20Transformer%20%28T-Former%29.%20The%20T-Former%20simultaneously%20extracts%0Atemporal%20information%20while%20continually%20integrating%20multi-scale%20spatial%0Ainformation.%20This%20process%20culminates%20in%20the%20generation%20of%20multi-scale%0Aspatio-temporal%20features%20that%20are%20utilized%20for%20the%20final%20classification.%20Our%0Amethod%20achieves%20state-of-the-art%20results%20on%20two%20in-the-wild%20datasets.%0AFurthermore%2C%20a%20series%20of%20ablation%20experiments%20and%20visualizations%20provide%0Afurther%20validation%20of%20our%20approach%27s%20proficiency%20in%20leveraging%20spatio-temporal%0Ainformation%20within%20DFER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08433v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSSTNet%3A%20A%20Multi-Scale%20Spatio-Temporal%20CNN-Transformer%20Network%20for%0A%20%20Dynamic%20Facial%20Expression%20Recognition&entry.906535625=Linhuang%20Wang%20and%20Xin%20Kang%20and%20Fei%20Ding%20and%20Satoshi%20Nakagawa%20and%20Fuji%20Ren&entry.1292438233=%20%20Unlike%20typical%20video%20action%20recognition%2C%20Dynamic%20Facial%20Expression%0ARecognition%20%28DFER%29%20does%20not%20involve%20distinct%20moving%20targets%20but%20relies%20on%0Alocalized%20changes%20in%20facial%20muscles.%20Addressing%20this%20distinctive%20attribute%2C%20we%0Apropose%20a%20Multi-Scale%20Spatio-temporal%20CNN-Transformer%20network%20%28MSSTNet%29.%20Our%0Aapproach%20takes%20spatial%20features%20of%20different%20scales%20extracted%20by%20CNN%20and%20feeds%0Athem%20into%20a%20Multi-scale%20Embedding%20Layer%20%28MELayer%29.%20The%20MELayer%20extracts%0Amulti-scale%20spatial%20information%20and%20encodes%20these%20features%20before%20sending%20them%0Ainto%20a%20Temporal%20Transformer%20%28T-Former%29.%20The%20T-Former%20simultaneously%20extracts%0Atemporal%20information%20while%20continually%20integrating%20multi-scale%20spatial%0Ainformation.%20This%20process%20culminates%20in%20the%20generation%20of%20multi-scale%0Aspatio-temporal%20features%20that%20are%20utilized%20for%20the%20final%20classification.%20Our%0Amethod%20achieves%20state-of-the-art%20results%20on%20two%20in-the-wild%20datasets.%0AFurthermore%2C%20a%20series%20of%20ablation%20experiments%20and%20visualizations%20provide%0Afurther%20validation%20of%20our%20approach%27s%20proficiency%20in%20leveraging%20spatio-temporal%0Ainformation%20within%20DFER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08433v1&entry.124074799=Read"},
{"title": "FoodLMM: A Versatile Food Assistant using Large Multi-modal Model", "author": "Yuehao Yin and Huiyan Qi and Bin Zhu and Jingjing Chen and Yu-Gang Jiang and Chong-Wah Ngo", "abstract": "  Large Multi-modal Models (LMMs) have made impressive progress in many\nvision-language tasks. Nevertheless, the performance of general LMMs in\nspecific domains is still far from satisfactory. This paper proposes FoodLMM, a\nversatile food assistant based on LMMs with various capabilities, including\nfood recognition, ingredient recognition, recipe generation, nutrition\nestimation, food segmentation and multi-round conversation. To facilitate\nFoodLMM to deal with tasks beyond pure text output, we introduce a series of\nnovel task-specific tokens and heads, enabling the model to predict food\nnutritional values and multiple segmentation masks. We adopt a two-stage\ntraining strategy. In the first stage, we utilize multiple public food\nbenchmarks for multi-task learning by leveraging the instruct-following\nparadigm. In the second stage, we construct a multi-round conversation dataset\nand a reasoning segmentation dataset to fine-tune the model, enabling it to\nconduct professional dialogues and generate segmentation masks based on complex\nreasoning in the food domain. Our fine-tuned FoodLMM achieves state-of-the-art\nresults across several food benchmarks. We will make our code, models and\ndatasets publicly available.\n", "link": "http://arxiv.org/abs/2312.14991v2", "date": "2024-04-12", "relevancy": 2.1286, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5289}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FoodLMM%3A%20A%20Versatile%20Food%20Assistant%20using%20Large%20Multi-modal%20Model&body=Title%3A%20FoodLMM%3A%20A%20Versatile%20Food%20Assistant%20using%20Large%20Multi-modal%20Model%0AAuthor%3A%20Yuehao%20Yin%20and%20Huiyan%20Qi%20and%20Bin%20Zhu%20and%20Jingjing%20Chen%20and%20Yu-Gang%20Jiang%20and%20Chong-Wah%20Ngo%0AAbstract%3A%20%20%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20made%20impressive%20progress%20in%20many%0Avision-language%20tasks.%20Nevertheless%2C%20the%20performance%20of%20general%20LMMs%20in%0Aspecific%20domains%20is%20still%20far%20from%20satisfactory.%20This%20paper%20proposes%20FoodLMM%2C%20a%0Aversatile%20food%20assistant%20based%20on%20LMMs%20with%20various%20capabilities%2C%20including%0Afood%20recognition%2C%20ingredient%20recognition%2C%20recipe%20generation%2C%20nutrition%0Aestimation%2C%20food%20segmentation%20and%20multi-round%20conversation.%20To%20facilitate%0AFoodLMM%20to%20deal%20with%20tasks%20beyond%20pure%20text%20output%2C%20we%20introduce%20a%20series%20of%0Anovel%20task-specific%20tokens%20and%20heads%2C%20enabling%20the%20model%20to%20predict%20food%0Anutritional%20values%20and%20multiple%20segmentation%20masks.%20We%20adopt%20a%20two-stage%0Atraining%20strategy.%20In%20the%20first%20stage%2C%20we%20utilize%20multiple%20public%20food%0Abenchmarks%20for%20multi-task%20learning%20by%20leveraging%20the%20instruct-following%0Aparadigm.%20In%20the%20second%20stage%2C%20we%20construct%20a%20multi-round%20conversation%20dataset%0Aand%20a%20reasoning%20segmentation%20dataset%20to%20fine-tune%20the%20model%2C%20enabling%20it%20to%0Aconduct%20professional%20dialogues%20and%20generate%20segmentation%20masks%20based%20on%20complex%0Areasoning%20in%20the%20food%20domain.%20Our%20fine-tuned%20FoodLMM%20achieves%20state-of-the-art%0Aresults%20across%20several%20food%20benchmarks.%20We%20will%20make%20our%20code%2C%20models%20and%0Adatasets%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14991v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoodLMM%3A%20A%20Versatile%20Food%20Assistant%20using%20Large%20Multi-modal%20Model&entry.906535625=Yuehao%20Yin%20and%20Huiyan%20Qi%20and%20Bin%20Zhu%20and%20Jingjing%20Chen%20and%20Yu-Gang%20Jiang%20and%20Chong-Wah%20Ngo&entry.1292438233=%20%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20made%20impressive%20progress%20in%20many%0Avision-language%20tasks.%20Nevertheless%2C%20the%20performance%20of%20general%20LMMs%20in%0Aspecific%20domains%20is%20still%20far%20from%20satisfactory.%20This%20paper%20proposes%20FoodLMM%2C%20a%0Aversatile%20food%20assistant%20based%20on%20LMMs%20with%20various%20capabilities%2C%20including%0Afood%20recognition%2C%20ingredient%20recognition%2C%20recipe%20generation%2C%20nutrition%0Aestimation%2C%20food%20segmentation%20and%20multi-round%20conversation.%20To%20facilitate%0AFoodLMM%20to%20deal%20with%20tasks%20beyond%20pure%20text%20output%2C%20we%20introduce%20a%20series%20of%0Anovel%20task-specific%20tokens%20and%20heads%2C%20enabling%20the%20model%20to%20predict%20food%0Anutritional%20values%20and%20multiple%20segmentation%20masks.%20We%20adopt%20a%20two-stage%0Atraining%20strategy.%20In%20the%20first%20stage%2C%20we%20utilize%20multiple%20public%20food%0Abenchmarks%20for%20multi-task%20learning%20by%20leveraging%20the%20instruct-following%0Aparadigm.%20In%20the%20second%20stage%2C%20we%20construct%20a%20multi-round%20conversation%20dataset%0Aand%20a%20reasoning%20segmentation%20dataset%20to%20fine-tune%20the%20model%2C%20enabling%20it%20to%0Aconduct%20professional%20dialogues%20and%20generate%20segmentation%20masks%20based%20on%20complex%0Areasoning%20in%20the%20food%20domain.%20Our%20fine-tuned%20FoodLMM%20achieves%20state-of-the-art%0Aresults%20across%20several%20food%20benchmarks.%20We%20will%20make%20our%20code%2C%20models%20and%0Adatasets%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14991v2&entry.124074799=Read"},
{"title": "On-line Motion Planning Using Bernstein Polynomials for Enhanced Target\n  Localization in Autonomous Vehicles", "author": "Camilla Tabasso and Venanzio Cichella", "abstract": "  The use of autonomous vehicles for target localization in modern applications\nhas emphasized their superior efficiency, improved safety, and cost advantages\nover human-operated methods. For localization tasks, autonomous vehicles can be\nused to increase efficiency and ensure that the target is localized as quickly\nand precisely as possible. However, devising a motion planning scheme to\nachieve these objectives in a computationally efficient manner suitable for\nreal-time implementation is not straightforward. In this paper, we introduce a\nmotion planning solution for enhanced target localization, leveraging Bernstein\npolynomial basis functions to approximate the probability distribution of the\ntarget's trajectory. This allows us to derive estimation performance criteria\nwhich are used by the motion planner to enhance the estimator efficacy. To\nconclude, we present simulation results that validate the effectiveness of the\nsuggested algorithm.\n", "link": "http://arxiv.org/abs/2210.03187v2", "date": "2024-04-12", "relevancy": 2.1235, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5469}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On-line%20Motion%20Planning%20Using%20Bernstein%20Polynomials%20for%20Enhanced%20Target%0A%20%20Localization%20in%20Autonomous%20Vehicles&body=Title%3A%20On-line%20Motion%20Planning%20Using%20Bernstein%20Polynomials%20for%20Enhanced%20Target%0A%20%20Localization%20in%20Autonomous%20Vehicles%0AAuthor%3A%20Camilla%20Tabasso%20and%20Venanzio%20Cichella%0AAbstract%3A%20%20%20The%20use%20of%20autonomous%20vehicles%20for%20target%20localization%20in%20modern%20applications%0Ahas%20emphasized%20their%20superior%20efficiency%2C%20improved%20safety%2C%20and%20cost%20advantages%0Aover%20human-operated%20methods.%20For%20localization%20tasks%2C%20autonomous%20vehicles%20can%20be%0Aused%20to%20increase%20efficiency%20and%20ensure%20that%20the%20target%20is%20localized%20as%20quickly%0Aand%20precisely%20as%20possible.%20However%2C%20devising%20a%20motion%20planning%20scheme%20to%0Aachieve%20these%20objectives%20in%20a%20computationally%20efficient%20manner%20suitable%20for%0Areal-time%20implementation%20is%20not%20straightforward.%20In%20this%20paper%2C%20we%20introduce%20a%0Amotion%20planning%20solution%20for%20enhanced%20target%20localization%2C%20leveraging%20Bernstein%0Apolynomial%20basis%20functions%20to%20approximate%20the%20probability%20distribution%20of%20the%0Atarget%27s%20trajectory.%20This%20allows%20us%20to%20derive%20estimation%20performance%20criteria%0Awhich%20are%20used%20by%20the%20motion%20planner%20to%20enhance%20the%20estimator%20efficacy.%20To%0Aconclude%2C%20we%20present%20simulation%20results%20that%20validate%20the%20effectiveness%20of%20the%0Asuggested%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.03187v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-line%20Motion%20Planning%20Using%20Bernstein%20Polynomials%20for%20Enhanced%20Target%0A%20%20Localization%20in%20Autonomous%20Vehicles&entry.906535625=Camilla%20Tabasso%20and%20Venanzio%20Cichella&entry.1292438233=%20%20The%20use%20of%20autonomous%20vehicles%20for%20target%20localization%20in%20modern%20applications%0Ahas%20emphasized%20their%20superior%20efficiency%2C%20improved%20safety%2C%20and%20cost%20advantages%0Aover%20human-operated%20methods.%20For%20localization%20tasks%2C%20autonomous%20vehicles%20can%20be%0Aused%20to%20increase%20efficiency%20and%20ensure%20that%20the%20target%20is%20localized%20as%20quickly%0Aand%20precisely%20as%20possible.%20However%2C%20devising%20a%20motion%20planning%20scheme%20to%0Aachieve%20these%20objectives%20in%20a%20computationally%20efficient%20manner%20suitable%20for%0Areal-time%20implementation%20is%20not%20straightforward.%20In%20this%20paper%2C%20we%20introduce%20a%0Amotion%20planning%20solution%20for%20enhanced%20target%20localization%2C%20leveraging%20Bernstein%0Apolynomial%20basis%20functions%20to%20approximate%20the%20probability%20distribution%20of%20the%0Atarget%27s%20trajectory.%20This%20allows%20us%20to%20derive%20estimation%20performance%20criteria%0Awhich%20are%20used%20by%20the%20motion%20planner%20to%20enhance%20the%20estimator%20efficacy.%20To%0Aconclude%2C%20we%20present%20simulation%20results%20that%20validate%20the%20effectiveness%20of%20the%0Asuggested%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.03187v2&entry.124074799=Read"},
{"title": "Training-free Boost for Open-Vocabulary Object Detection with Confidence\n  Aggregation", "author": "Yanhao Zheng and Kai Liu", "abstract": "  Open-vocabulary object detection (OVOD) aims at localizing and recognizing\nvisual objects from novel classes unseen at the training time. Whereas,\nempirical studies reveal that advanced detectors generally assign lower scores\nto those novel instances, which are inadvertently suppressed during inference\nby commonly adopted greedy strategies like Non-Maximum Suppression (NMS),\nleading to sub-optimal detection performance for novel classes. This paper\nsystematically investigates this problem with the commonly-adopted two-stage\nOVOD paradigm. Specifically, in the region-proposal stage, proposals that\ncontain novel instances showcase lower objectness scores, since they are\ntreated as background proposals during the training phase. Meanwhile, in the\nobject-classification stage, novel objects share lower region-text similarities\n(i.e., classification scores) due to the biased visual-language alignment by\nseen training samples. To alleviate this problem, this paper introduces two\nadvanced measures to adjust confidence scores and conserve erroneously\ndismissed objects: (1) a class-agnostic localization quality estimate via\noverlap degree of region/object proposals, and (2) a text-guided visual\nsimilarity estimate with proxy prototypes for novel classes. Integrated with\nadjusting techniques specifically designed for the region-proposal and\nobject-classification stages, this paper derives the aggregated confidence\nestimate for the open-vocabulary object detection paradigm (AggDet). Our AggDet\nis a generic and training-free post-processing scheme, which consistently\nbolsters open-vocabulary detectors across model scales and architecture\ndesigns. For instance, AggDet receives 3.3% and 1.5% gains on OV-COCO and\nOV-LVIS benchmarks respectively, without any training cost.\n", "link": "http://arxiv.org/abs/2404.08603v1", "date": "2024-04-12", "relevancy": 2.1219, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5555}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5261}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Training-free%20Boost%20for%20Open-Vocabulary%20Object%20Detection%20with%20Confidence%0A%20%20Aggregation&body=Title%3A%20Training-free%20Boost%20for%20Open-Vocabulary%20Object%20Detection%20with%20Confidence%0A%20%20Aggregation%0AAuthor%3A%20Yanhao%20Zheng%20and%20Kai%20Liu%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20%28OVOD%29%20aims%20at%20localizing%20and%20recognizing%0Avisual%20objects%20from%20novel%20classes%20unseen%20at%20the%20training%20time.%20Whereas%2C%0Aempirical%20studies%20reveal%20that%20advanced%20detectors%20generally%20assign%20lower%20scores%0Ato%20those%20novel%20instances%2C%20which%20are%20inadvertently%20suppressed%20during%20inference%0Aby%20commonly%20adopted%20greedy%20strategies%20like%20Non-Maximum%20Suppression%20%28NMS%29%2C%0Aleading%20to%20sub-optimal%20detection%20performance%20for%20novel%20classes.%20This%20paper%0Asystematically%20investigates%20this%20problem%20with%20the%20commonly-adopted%20two-stage%0AOVOD%20paradigm.%20Specifically%2C%20in%20the%20region-proposal%20stage%2C%20proposals%20that%0Acontain%20novel%20instances%20showcase%20lower%20objectness%20scores%2C%20since%20they%20are%0Atreated%20as%20background%20proposals%20during%20the%20training%20phase.%20Meanwhile%2C%20in%20the%0Aobject-classification%20stage%2C%20novel%20objects%20share%20lower%20region-text%20similarities%0A%28i.e.%2C%20classification%20scores%29%20due%20to%20the%20biased%20visual-language%20alignment%20by%0Aseen%20training%20samples.%20To%20alleviate%20this%20problem%2C%20this%20paper%20introduces%20two%0Aadvanced%20measures%20to%20adjust%20confidence%20scores%20and%20conserve%20erroneously%0Adismissed%20objects%3A%20%281%29%20a%20class-agnostic%20localization%20quality%20estimate%20via%0Aoverlap%20degree%20of%20region/object%20proposals%2C%20and%20%282%29%20a%20text-guided%20visual%0Asimilarity%20estimate%20with%20proxy%20prototypes%20for%20novel%20classes.%20Integrated%20with%0Aadjusting%20techniques%20specifically%20designed%20for%20the%20region-proposal%20and%0Aobject-classification%20stages%2C%20this%20paper%20derives%20the%20aggregated%20confidence%0Aestimate%20for%20the%20open-vocabulary%20object%20detection%20paradigm%20%28AggDet%29.%20Our%20AggDet%0Ais%20a%20generic%20and%20training-free%20post-processing%20scheme%2C%20which%20consistently%0Abolsters%20open-vocabulary%20detectors%20across%20model%20scales%20and%20architecture%0Adesigns.%20For%20instance%2C%20AggDet%20receives%203.3%25%20and%201.5%25%20gains%20on%20OV-COCO%20and%0AOV-LVIS%20benchmarks%20respectively%2C%20without%20any%20training%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08603v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Boost%20for%20Open-Vocabulary%20Object%20Detection%20with%20Confidence%0A%20%20Aggregation&entry.906535625=Yanhao%20Zheng%20and%20Kai%20Liu&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20%28OVOD%29%20aims%20at%20localizing%20and%20recognizing%0Avisual%20objects%20from%20novel%20classes%20unseen%20at%20the%20training%20time.%20Whereas%2C%0Aempirical%20studies%20reveal%20that%20advanced%20detectors%20generally%20assign%20lower%20scores%0Ato%20those%20novel%20instances%2C%20which%20are%20inadvertently%20suppressed%20during%20inference%0Aby%20commonly%20adopted%20greedy%20strategies%20like%20Non-Maximum%20Suppression%20%28NMS%29%2C%0Aleading%20to%20sub-optimal%20detection%20performance%20for%20novel%20classes.%20This%20paper%0Asystematically%20investigates%20this%20problem%20with%20the%20commonly-adopted%20two-stage%0AOVOD%20paradigm.%20Specifically%2C%20in%20the%20region-proposal%20stage%2C%20proposals%20that%0Acontain%20novel%20instances%20showcase%20lower%20objectness%20scores%2C%20since%20they%20are%0Atreated%20as%20background%20proposals%20during%20the%20training%20phase.%20Meanwhile%2C%20in%20the%0Aobject-classification%20stage%2C%20novel%20objects%20share%20lower%20region-text%20similarities%0A%28i.e.%2C%20classification%20scores%29%20due%20to%20the%20biased%20visual-language%20alignment%20by%0Aseen%20training%20samples.%20To%20alleviate%20this%20problem%2C%20this%20paper%20introduces%20two%0Aadvanced%20measures%20to%20adjust%20confidence%20scores%20and%20conserve%20erroneously%0Adismissed%20objects%3A%20%281%29%20a%20class-agnostic%20localization%20quality%20estimate%20via%0Aoverlap%20degree%20of%20region/object%20proposals%2C%20and%20%282%29%20a%20text-guided%20visual%0Asimilarity%20estimate%20with%20proxy%20prototypes%20for%20novel%20classes.%20Integrated%20with%0Aadjusting%20techniques%20specifically%20designed%20for%20the%20region-proposal%20and%0Aobject-classification%20stages%2C%20this%20paper%20derives%20the%20aggregated%20confidence%0Aestimate%20for%20the%20open-vocabulary%20object%20detection%20paradigm%20%28AggDet%29.%20Our%20AggDet%0Ais%20a%20generic%20and%20training-free%20post-processing%20scheme%2C%20which%20consistently%0Abolsters%20open-vocabulary%20detectors%20across%20model%20scales%20and%20architecture%0Adesigns.%20For%20instance%2C%20AggDet%20receives%203.3%25%20and%201.5%25%20gains%20on%20OV-COCO%20and%0AOV-LVIS%20benchmarks%20respectively%2C%20without%20any%20training%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08603v1&entry.124074799=Read"},
{"title": "Transformer based Pluralistic Image Completion with Reduced Information\n  Loss", "author": "Qiankun Liu and Yuqi Jiang and Zhentao Tan and Dongdong Chen and Ying Fu and Qi Chu and Gang Hua and Nenghai Yu", "abstract": "  Transformer based methods have achieved great success in image inpainting\nrecently. However, we find that these solutions regard each pixel as a token,\nthus suffering from an information loss issue from two aspects: 1) They\ndownsample the input image into much lower resolutions for efficiency\nconsideration. 2) They quantize $256^3$ RGB values to a small number (such as\n512) of quantized color values. The indices of quantized pixels are used as\ntokens for the inputs and prediction targets of the transformer. To mitigate\nthese issues, we propose a new transformer based framework called \"PUT\".\nSpecifically, to avoid input downsampling while maintaining computation\nefficiency, we design a patch-based auto-encoder P-VQVAE. The encoder converts\nthe masked image into non-overlapped patch tokens and the decoder recovers the\nmasked regions from the inpainted tokens while keeping the unmasked regions\nunchanged. To eliminate the information loss caused by input quantization, an\nUn-quantized Transformer is applied. It directly takes features from the\nP-VQVAE encoder as input without any quantization and only regards the\nquantized tokens as prediction targets. Furthermore, to make the inpainting\nprocess more controllable, we introduce semantic and structural conditions as\nextra guidance. Extensive experiments show that our method greatly outperforms\nexisting transformer based methods on image fidelity and achieves much higher\ndiversity and better fidelity than state-of-the-art pluralistic inpainting\nmethods on complex large-scale datasets (e.g., ImageNet). Codes are available\nat https://github.com/liuqk3/PUT.\n", "link": "http://arxiv.org/abs/2404.00513v2", "date": "2024-04-12", "relevancy": 2.1152, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5361}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5338}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5209}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transformer%20based%20Pluralistic%20Image%20Completion%20with%20Reduced%20Information%0A%20%20Loss&body=Title%3A%20Transformer%20based%20Pluralistic%20Image%20Completion%20with%20Reduced%20Information%0A%20%20Loss%0AAuthor%3A%20Qiankun%20Liu%20and%20Yuqi%20Jiang%20and%20Zhentao%20Tan%20and%20Dongdong%20Chen%20and%20Ying%20Fu%20and%20Qi%20Chu%20and%20Gang%20Hua%20and%20Nenghai%20Yu%0AAbstract%3A%20%20%20Transformer%20based%20methods%20have%20achieved%20great%20success%20in%20image%20inpainting%0Arecently.%20However%2C%20we%20find%20that%20these%20solutions%20regard%20each%20pixel%20as%20a%20token%2C%0Athus%20suffering%20from%20an%20information%20loss%20issue%20from%20two%20aspects%3A%201%29%20They%0Adownsample%20the%20input%20image%20into%20much%20lower%20resolutions%20for%20efficiency%0Aconsideration.%202%29%20They%20quantize%20%24256%5E3%24%20RGB%20values%20to%20a%20small%20number%20%28such%20as%0A512%29%20of%20quantized%20color%20values.%20The%20indices%20of%20quantized%20pixels%20are%20used%20as%0Atokens%20for%20the%20inputs%20and%20prediction%20targets%20of%20the%20transformer.%20To%20mitigate%0Athese%20issues%2C%20we%20propose%20a%20new%20transformer%20based%20framework%20called%20%22PUT%22.%0ASpecifically%2C%20to%20avoid%20input%20downsampling%20while%20maintaining%20computation%0Aefficiency%2C%20we%20design%20a%20patch-based%20auto-encoder%20P-VQVAE.%20The%20encoder%20converts%0Athe%20masked%20image%20into%20non-overlapped%20patch%20tokens%20and%20the%20decoder%20recovers%20the%0Amasked%20regions%20from%20the%20inpainted%20tokens%20while%20keeping%20the%20unmasked%20regions%0Aunchanged.%20To%20eliminate%20the%20information%20loss%20caused%20by%20input%20quantization%2C%20an%0AUn-quantized%20Transformer%20is%20applied.%20It%20directly%20takes%20features%20from%20the%0AP-VQVAE%20encoder%20as%20input%20without%20any%20quantization%20and%20only%20regards%20the%0Aquantized%20tokens%20as%20prediction%20targets.%20Furthermore%2C%20to%20make%20the%20inpainting%0Aprocess%20more%20controllable%2C%20we%20introduce%20semantic%20and%20structural%20conditions%20as%0Aextra%20guidance.%20Extensive%20experiments%20show%20that%20our%20method%20greatly%20outperforms%0Aexisting%20transformer%20based%20methods%20on%20image%20fidelity%20and%20achieves%20much%20higher%0Adiversity%20and%20better%20fidelity%20than%20state-of-the-art%20pluralistic%20inpainting%0Amethods%20on%20complex%20large-scale%20datasets%20%28e.g.%2C%20ImageNet%29.%20Codes%20are%20available%0Aat%20https%3A//github.com/liuqk3/PUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00513v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20based%20Pluralistic%20Image%20Completion%20with%20Reduced%20Information%0A%20%20Loss&entry.906535625=Qiankun%20Liu%20and%20Yuqi%20Jiang%20and%20Zhentao%20Tan%20and%20Dongdong%20Chen%20and%20Ying%20Fu%20and%20Qi%20Chu%20and%20Gang%20Hua%20and%20Nenghai%20Yu&entry.1292438233=%20%20Transformer%20based%20methods%20have%20achieved%20great%20success%20in%20image%20inpainting%0Arecently.%20However%2C%20we%20find%20that%20these%20solutions%20regard%20each%20pixel%20as%20a%20token%2C%0Athus%20suffering%20from%20an%20information%20loss%20issue%20from%20two%20aspects%3A%201%29%20They%0Adownsample%20the%20input%20image%20into%20much%20lower%20resolutions%20for%20efficiency%0Aconsideration.%202%29%20They%20quantize%20%24256%5E3%24%20RGB%20values%20to%20a%20small%20number%20%28such%20as%0A512%29%20of%20quantized%20color%20values.%20The%20indices%20of%20quantized%20pixels%20are%20used%20as%0Atokens%20for%20the%20inputs%20and%20prediction%20targets%20of%20the%20transformer.%20To%20mitigate%0Athese%20issues%2C%20we%20propose%20a%20new%20transformer%20based%20framework%20called%20%22PUT%22.%0ASpecifically%2C%20to%20avoid%20input%20downsampling%20while%20maintaining%20computation%0Aefficiency%2C%20we%20design%20a%20patch-based%20auto-encoder%20P-VQVAE.%20The%20encoder%20converts%0Athe%20masked%20image%20into%20non-overlapped%20patch%20tokens%20and%20the%20decoder%20recovers%20the%0Amasked%20regions%20from%20the%20inpainted%20tokens%20while%20keeping%20the%20unmasked%20regions%0Aunchanged.%20To%20eliminate%20the%20information%20loss%20caused%20by%20input%20quantization%2C%20an%0AUn-quantized%20Transformer%20is%20applied.%20It%20directly%20takes%20features%20from%20the%0AP-VQVAE%20encoder%20as%20input%20without%20any%20quantization%20and%20only%20regards%20the%0Aquantized%20tokens%20as%20prediction%20targets.%20Furthermore%2C%20to%20make%20the%20inpainting%0Aprocess%20more%20controllable%2C%20we%20introduce%20semantic%20and%20structural%20conditions%20as%0Aextra%20guidance.%20Extensive%20experiments%20show%20that%20our%20method%20greatly%20outperforms%0Aexisting%20transformer%20based%20methods%20on%20image%20fidelity%20and%20achieves%20much%20higher%0Adiversity%20and%20better%20fidelity%20than%20state-of-the-art%20pluralistic%20inpainting%0Amethods%20on%20complex%20large-scale%20datasets%20%28e.g.%2C%20ImageNet%29.%20Codes%20are%20available%0Aat%20https%3A//github.com/liuqk3/PUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00513v2&entry.124074799=Read"},
{"title": "EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams", "author": "Christen Millerdurai and Hiroyasu Akada and Jian Wang and Diogo Luvizon and Christian Theobalt and Vladislav Golyanik", "abstract": "  Monocular egocentric 3D human motion capture is a challenging and actively\nresearched problem. Existing methods use synchronously operating visual sensors\n(e.g. RGB cameras) and often fail under low lighting and fast motions, which\ncan be restricting in many applications involving head-mounted devices. In\nresponse to the existing limitations, this paper 1) introduces a new problem,\ni.e., 3D human motion capture from an egocentric monocular event camera with a\nfisheye lens, and 2) proposes the first approach to it called EventEgo3D\n(EE3D). Event streams have high temporal resolution and provide reliable cues\nfor 3D human motion capture under high-speed human motions and rapidly changing\nillumination. The proposed EE3D framework is specifically tailored for learning\nwith event streams in the LNES representation, enabling high 3D reconstruction\naccuracy. We also design a prototype of a mobile head-mounted device with an\nevent camera and record a real dataset with event observations and the\nground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D\ndemonstrates robustness and superior 3D accuracy compared to existing solutions\nacross various challenging experiments while supporting real-time 3D pose\nupdate rates of 140Hz.\n", "link": "http://arxiv.org/abs/2404.08640v1", "date": "2024-04-12", "relevancy": 2.1111, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5382}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EventEgo3D%3A%203D%20Human%20Motion%20Capture%20from%20Egocentric%20Event%20Streams&body=Title%3A%20EventEgo3D%3A%203D%20Human%20Motion%20Capture%20from%20Egocentric%20Event%20Streams%0AAuthor%3A%20Christen%20Millerdurai%20and%20Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Diogo%20Luvizon%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Monocular%20egocentric%203D%20human%20motion%20capture%20is%20a%20challenging%20and%20actively%0Aresearched%20problem.%20Existing%20methods%20use%20synchronously%20operating%20visual%20sensors%0A%28e.g.%20RGB%20cameras%29%20and%20often%20fail%20under%20low%20lighting%20and%20fast%20motions%2C%20which%0Acan%20be%20restricting%20in%20many%20applications%20involving%20head-mounted%20devices.%20In%0Aresponse%20to%20the%20existing%20limitations%2C%20this%20paper%201%29%20introduces%20a%20new%20problem%2C%0Ai.e.%2C%203D%20human%20motion%20capture%20from%20an%20egocentric%20monocular%20event%20camera%20with%20a%0Afisheye%20lens%2C%20and%202%29%20proposes%20the%20first%20approach%20to%20it%20called%20EventEgo3D%0A%28EE3D%29.%20Event%20streams%20have%20high%20temporal%20resolution%20and%20provide%20reliable%20cues%0Afor%203D%20human%20motion%20capture%20under%20high-speed%20human%20motions%20and%20rapidly%20changing%0Aillumination.%20The%20proposed%20EE3D%20framework%20is%20specifically%20tailored%20for%20learning%0Awith%20event%20streams%20in%20the%20LNES%20representation%2C%20enabling%20high%203D%20reconstruction%0Aaccuracy.%20We%20also%20design%20a%20prototype%20of%20a%20mobile%20head-mounted%20device%20with%20an%0Aevent%20camera%20and%20record%20a%20real%20dataset%20with%20event%20observations%20and%20the%0Aground-truth%203D%20human%20poses%20%28in%20addition%20to%20the%20synthetic%20dataset%29.%20Our%20EE3D%0Ademonstrates%20robustness%20and%20superior%203D%20accuracy%20compared%20to%20existing%20solutions%0Aacross%20various%20challenging%20experiments%20while%20supporting%20real-time%203D%20pose%0Aupdate%20rates%20of%20140Hz.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventEgo3D%3A%203D%20Human%20Motion%20Capture%20from%20Egocentric%20Event%20Streams&entry.906535625=Christen%20Millerdurai%20and%20Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Diogo%20Luvizon%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Monocular%20egocentric%203D%20human%20motion%20capture%20is%20a%20challenging%20and%20actively%0Aresearched%20problem.%20Existing%20methods%20use%20synchronously%20operating%20visual%20sensors%0A%28e.g.%20RGB%20cameras%29%20and%20often%20fail%20under%20low%20lighting%20and%20fast%20motions%2C%20which%0Acan%20be%20restricting%20in%20many%20applications%20involving%20head-mounted%20devices.%20In%0Aresponse%20to%20the%20existing%20limitations%2C%20this%20paper%201%29%20introduces%20a%20new%20problem%2C%0Ai.e.%2C%203D%20human%20motion%20capture%20from%20an%20egocentric%20monocular%20event%20camera%20with%20a%0Afisheye%20lens%2C%20and%202%29%20proposes%20the%20first%20approach%20to%20it%20called%20EventEgo3D%0A%28EE3D%29.%20Event%20streams%20have%20high%20temporal%20resolution%20and%20provide%20reliable%20cues%0Afor%203D%20human%20motion%20capture%20under%20high-speed%20human%20motions%20and%20rapidly%20changing%0Aillumination.%20The%20proposed%20EE3D%20framework%20is%20specifically%20tailored%20for%20learning%0Awith%20event%20streams%20in%20the%20LNES%20representation%2C%20enabling%20high%203D%20reconstruction%0Aaccuracy.%20We%20also%20design%20a%20prototype%20of%20a%20mobile%20head-mounted%20device%20with%20an%0Aevent%20camera%20and%20record%20a%20real%20dataset%20with%20event%20observations%20and%20the%0Aground-truth%203D%20human%20poses%20%28in%20addition%20to%20the%20synthetic%20dataset%29.%20Our%20EE3D%0Ademonstrates%20robustness%20and%20superior%203D%20accuracy%20compared%20to%20existing%20solutions%0Aacross%20various%20challenging%20experiments%20while%20supporting%20real-time%203D%20pose%0Aupdate%20rates%20of%20140Hz.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08640v1&entry.124074799=Read"},
{"title": "SpectralMamba: Efficient Mamba for Hyperspectral Image Classification", "author": "Jing Yao and Danfeng Hong and Chenyu Li and Jocelyn Chanussot", "abstract": "  Recurrent neural networks and Transformers have recently dominated most\napplications in hyperspectral (HS) imaging, owing to their capability to\ncapture long-range dependencies from spectrum sequences. However, despite the\nsuccess of these sequential architectures, the non-ignorable inefficiency\ncaused by either difficulty in parallelization or computationally prohibitive\nattention still hinders their practicality, especially for large-scale\nobservation in remote sensing scenarios. To address this issue, we herein\npropose SpectralMamba -- a novel state space model incorporated efficient deep\nlearning framework for HS image classification. SpectralMamba features the\nsimplified but adequate modeling of HS data dynamics at two levels. First, in\nspatial-spectral space, a dynamical mask is learned by efficient convolutions\nto simultaneously encode spatial regularity and spectral peculiarity, thus\nattenuating the spectral variability and confusion in discriminative\nrepresentation learning. Second, the merged spectrum can then be efficiently\noperated in the hidden state space with all parameters learned input-dependent,\nyielding selectively focused responses without reliance on redundant attention\nor imparallelizable recurrence. To explore the room for further computational\ndownsizing, a piece-wise scanning mechanism is employed in-between,\ntransferring approximately continuous spectrum into sequences with squeezed\nlength while maintaining short- and long-term contextual profiles among\nhundreds of bands. Through extensive experiments on four benchmark HS datasets\nacquired by satellite-, aircraft-, and UAV-borne imagers, SpectralMamba\nsurprisingly creates promising win-wins from both performance and efficiency\nperspectives.\n", "link": "http://arxiv.org/abs/2404.08489v1", "date": "2024-04-12", "relevancy": 2.0885, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5419}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5326}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5038}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SpectralMamba%3A%20Efficient%20Mamba%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20SpectralMamba%3A%20Efficient%20Mamba%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Jing%20Yao%20and%20Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Jocelyn%20Chanussot%0AAbstract%3A%20%20%20Recurrent%20neural%20networks%20and%20Transformers%20have%20recently%20dominated%20most%0Aapplications%20in%20hyperspectral%20%28HS%29%20imaging%2C%20owing%20to%20their%20capability%20to%0Acapture%20long-range%20dependencies%20from%20spectrum%20sequences.%20However%2C%20despite%20the%0Asuccess%20of%20these%20sequential%20architectures%2C%20the%20non-ignorable%20inefficiency%0Acaused%20by%20either%20difficulty%20in%20parallelization%20or%20computationally%20prohibitive%0Aattention%20still%20hinders%20their%20practicality%2C%20especially%20for%20large-scale%0Aobservation%20in%20remote%20sensing%20scenarios.%20To%20address%20this%20issue%2C%20we%20herein%0Apropose%20SpectralMamba%20--%20a%20novel%20state%20space%20model%20incorporated%20efficient%20deep%0Alearning%20framework%20for%20HS%20image%20classification.%20SpectralMamba%20features%20the%0Asimplified%20but%20adequate%20modeling%20of%20HS%20data%20dynamics%20at%20two%20levels.%20First%2C%20in%0Aspatial-spectral%20space%2C%20a%20dynamical%20mask%20is%20learned%20by%20efficient%20convolutions%0Ato%20simultaneously%20encode%20spatial%20regularity%20and%20spectral%20peculiarity%2C%20thus%0Aattenuating%20the%20spectral%20variability%20and%20confusion%20in%20discriminative%0Arepresentation%20learning.%20Second%2C%20the%20merged%20spectrum%20can%20then%20be%20efficiently%0Aoperated%20in%20the%20hidden%20state%20space%20with%20all%20parameters%20learned%20input-dependent%2C%0Ayielding%20selectively%20focused%20responses%20without%20reliance%20on%20redundant%20attention%0Aor%20imparallelizable%20recurrence.%20To%20explore%20the%20room%20for%20further%20computational%0Adownsizing%2C%20a%20piece-wise%20scanning%20mechanism%20is%20employed%20in-between%2C%0Atransferring%20approximately%20continuous%20spectrum%20into%20sequences%20with%20squeezed%0Alength%20while%20maintaining%20short-%20and%20long-term%20contextual%20profiles%20among%0Ahundreds%20of%20bands.%20Through%20extensive%20experiments%20on%20four%20benchmark%20HS%20datasets%0Aacquired%20by%20satellite-%2C%20aircraft-%2C%20and%20UAV-borne%20imagers%2C%20SpectralMamba%0Asurprisingly%20creates%20promising%20win-wins%20from%20both%20performance%20and%20efficiency%0Aperspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08489v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralMamba%3A%20Efficient%20Mamba%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Jing%20Yao%20and%20Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Jocelyn%20Chanussot&entry.1292438233=%20%20Recurrent%20neural%20networks%20and%20Transformers%20have%20recently%20dominated%20most%0Aapplications%20in%20hyperspectral%20%28HS%29%20imaging%2C%20owing%20to%20their%20capability%20to%0Acapture%20long-range%20dependencies%20from%20spectrum%20sequences.%20However%2C%20despite%20the%0Asuccess%20of%20these%20sequential%20architectures%2C%20the%20non-ignorable%20inefficiency%0Acaused%20by%20either%20difficulty%20in%20parallelization%20or%20computationally%20prohibitive%0Aattention%20still%20hinders%20their%20practicality%2C%20especially%20for%20large-scale%0Aobservation%20in%20remote%20sensing%20scenarios.%20To%20address%20this%20issue%2C%20we%20herein%0Apropose%20SpectralMamba%20--%20a%20novel%20state%20space%20model%20incorporated%20efficient%20deep%0Alearning%20framework%20for%20HS%20image%20classification.%20SpectralMamba%20features%20the%0Asimplified%20but%20adequate%20modeling%20of%20HS%20data%20dynamics%20at%20two%20levels.%20First%2C%20in%0Aspatial-spectral%20space%2C%20a%20dynamical%20mask%20is%20learned%20by%20efficient%20convolutions%0Ato%20simultaneously%20encode%20spatial%20regularity%20and%20spectral%20peculiarity%2C%20thus%0Aattenuating%20the%20spectral%20variability%20and%20confusion%20in%20discriminative%0Arepresentation%20learning.%20Second%2C%20the%20merged%20spectrum%20can%20then%20be%20efficiently%0Aoperated%20in%20the%20hidden%20state%20space%20with%20all%20parameters%20learned%20input-dependent%2C%0Ayielding%20selectively%20focused%20responses%20without%20reliance%20on%20redundant%20attention%0Aor%20imparallelizable%20recurrence.%20To%20explore%20the%20room%20for%20further%20computational%0Adownsizing%2C%20a%20piece-wise%20scanning%20mechanism%20is%20employed%20in-between%2C%0Atransferring%20approximately%20continuous%20spectrum%20into%20sequences%20with%20squeezed%0Alength%20while%20maintaining%20short-%20and%20long-term%20contextual%20profiles%20among%0Ahundreds%20of%20bands.%20Through%20extensive%20experiments%20on%20four%20benchmark%20HS%20datasets%0Aacquired%20by%20satellite-%2C%20aircraft-%2C%20and%20UAV-borne%20imagers%2C%20SpectralMamba%0Asurprisingly%20creates%20promising%20win-wins%20from%20both%20performance%20and%20efficiency%0Aperspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08489v1&entry.124074799=Read"},
{"title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination", "author": "Anant Khandelwal", "abstract": "  The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33% in overall performance, by 1% in base-to-novel generalization, and by\n2.84% in cross-dataset transfer tasks.\n", "link": "http://arxiv.org/abs/2404.07520v2", "date": "2024-04-12", "relevancy": 2.0868, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4997}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PromptSync%3A%20Bridging%20Domain%20Gaps%20in%20Vision-Language%20Models%20through%0A%20%20Class-Aware%20Prototype%20Alignment%20and%20Discrimination&body=Title%3A%20PromptSync%3A%20Bridging%20Domain%20Gaps%20in%20Vision-Language%20Models%20through%0A%20%20Class-Aware%20Prototype%20Alignment%20and%20Discrimination%0AAuthor%3A%20Anant%20Khandelwal%0AAbstract%3A%20%20%20The%20potential%20for%20zero-shot%20generalization%20in%20vision-language%20%28V-L%29%20models%0Asuch%20as%20CLIP%20has%20spurred%20their%20widespread%20adoption%20in%20addressing%20numerous%0Adownstream%20tasks.%20Previous%20methods%20have%20employed%20test-time%20prompt%20tuning%20to%0Aadapt%20the%20model%20to%20unseen%20domains%2C%20but%20they%20overlooked%20the%20issue%20of%20imbalanced%0Aclass%20distributions.%20In%20this%20study%2C%20we%20explicitly%20address%20this%20problem%20by%0Aemploying%20class-aware%20prototype%20alignment%20weighted%20by%20mean%20class%20probabilities%0Aobtained%20for%20the%20test%20sample%20and%20filtered%20augmented%20views.%20Additionally%2C%20we%0Aensure%20that%20the%20class%20probabilities%20are%20as%20accurate%20as%20possible%20by%20performing%0Aprototype%20discrimination%20using%20contrastive%20learning.%20The%20combination%20of%0Aalignment%20and%20discriminative%20loss%20serves%20as%20a%20geometric%20regularizer%2C%20preventing%0Athe%20prompt%20representation%20from%20collapsing%20onto%20a%20single%20class%20and%20effectively%0Abridging%20the%20distribution%20gap%20between%20the%20source%20and%20test%20domains.%20Our%20method%2C%0Anamed%20PromptSync%2C%20synchronizes%20the%20prompts%20for%20each%20test%20sample%20on%20both%20the%0Atext%20and%20vision%20branches%20of%20the%20V-L%20model.%20In%20empirical%20evaluations%20on%20the%0Adomain%20generalization%20benchmark%2C%20our%20method%20outperforms%20previous%20best%20methods%0Aby%202.33%25%20in%20overall%20performance%2C%20by%201%25%20in%20base-to-novel%20generalization%2C%20and%20by%0A2.84%25%20in%20cross-dataset%20transfer%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07520v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptSync%3A%20Bridging%20Domain%20Gaps%20in%20Vision-Language%20Models%20through%0A%20%20Class-Aware%20Prototype%20Alignment%20and%20Discrimination&entry.906535625=Anant%20Khandelwal&entry.1292438233=%20%20The%20potential%20for%20zero-shot%20generalization%20in%20vision-language%20%28V-L%29%20models%0Asuch%20as%20CLIP%20has%20spurred%20their%20widespread%20adoption%20in%20addressing%20numerous%0Adownstream%20tasks.%20Previous%20methods%20have%20employed%20test-time%20prompt%20tuning%20to%0Aadapt%20the%20model%20to%20unseen%20domains%2C%20but%20they%20overlooked%20the%20issue%20of%20imbalanced%0Aclass%20distributions.%20In%20this%20study%2C%20we%20explicitly%20address%20this%20problem%20by%0Aemploying%20class-aware%20prototype%20alignment%20weighted%20by%20mean%20class%20probabilities%0Aobtained%20for%20the%20test%20sample%20and%20filtered%20augmented%20views.%20Additionally%2C%20we%0Aensure%20that%20the%20class%20probabilities%20are%20as%20accurate%20as%20possible%20by%20performing%0Aprototype%20discrimination%20using%20contrastive%20learning.%20The%20combination%20of%0Aalignment%20and%20discriminative%20loss%20serves%20as%20a%20geometric%20regularizer%2C%20preventing%0Athe%20prompt%20representation%20from%20collapsing%20onto%20a%20single%20class%20and%20effectively%0Abridging%20the%20distribution%20gap%20between%20the%20source%20and%20test%20domains.%20Our%20method%2C%0Anamed%20PromptSync%2C%20synchronizes%20the%20prompts%20for%20each%20test%20sample%20on%20both%20the%0Atext%20and%20vision%20branches%20of%20the%20V-L%20model.%20In%20empirical%20evaluations%20on%20the%0Adomain%20generalization%20benchmark%2C%20our%20method%20outperforms%20previous%20best%20methods%0Aby%202.33%25%20in%20overall%20performance%2C%20by%201%25%20in%20base-to-novel%20generalization%2C%20and%20by%0A2.84%25%20in%20cross-dataset%20transfer%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07520v2&entry.124074799=Read"},
{"title": "3D Human Scan With A Moving Event Camera", "author": "Kai Kohyama and Shintaro Shiba and Yoshimitsu Aoki", "abstract": "  Capturing the 3D human body is one of the important tasks in computer vision\nwith a wide range of applications such as virtual reality and sports analysis.\nHowever, conventional frame cameras are limited by their temporal resolution\nand dynamic range, which imposes constraints in real-world application setups.\nEvent cameras have the advantages of high temporal resolution and high dynamic\nrange (HDR), but the development of event-based methods is necessary to handle\ndata with different characteristics. This paper proposes a novel event-based\nmethod for 3D pose estimation and human mesh recovery. Prior work on\nevent-based human mesh recovery require frames (images) as well as event data.\nThe proposed method solely relies on events; it carves 3D voxels by moving the\nevent camera around a stationary body, reconstructs the human pose and mesh by\nattenuated rays, and fit statistical body models, preserving high-frequency\ndetails. The experimental results show that the proposed method outperforms\nconventional frame-based methods in the estimation accuracy of both pose and\nbody mesh. We also demonstrate results in challenging situations where a\nconventional camera has motion blur. This is the first to demonstrate\nevent-only human mesh recovery, and we hope that it is the first step toward\nachieving robust and accurate 3D human body scanning from vision sensors.\n", "link": "http://arxiv.org/abs/2404.08504v1", "date": "2024-04-12", "relevancy": 2.0815, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera&body=Title%3A%203D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera%0AAuthor%3A%20Kai%20Kohyama%20and%20Shintaro%20Shiba%20and%20Yoshimitsu%20Aoki%0AAbstract%3A%20%20%20Capturing%20the%203D%20human%20body%20is%20one%20of%20the%20important%20tasks%20in%20computer%20vision%0Awith%20a%20wide%20range%20of%20applications%20such%20as%20virtual%20reality%20and%20sports%20analysis.%0AHowever%2C%20conventional%20frame%20cameras%20are%20limited%20by%20their%20temporal%20resolution%0Aand%20dynamic%20range%2C%20which%20imposes%20constraints%20in%20real-world%20application%20setups.%0AEvent%20cameras%20have%20the%20advantages%20of%20high%20temporal%20resolution%20and%20high%20dynamic%0Arange%20%28HDR%29%2C%20but%20the%20development%20of%20event-based%20methods%20is%20necessary%20to%20handle%0Adata%20with%20different%20characteristics.%20This%20paper%20proposes%20a%20novel%20event-based%0Amethod%20for%203D%20pose%20estimation%20and%20human%20mesh%20recovery.%20Prior%20work%20on%0Aevent-based%20human%20mesh%20recovery%20require%20frames%20%28images%29%20as%20well%20as%20event%20data.%0AThe%20proposed%20method%20solely%20relies%20on%20events%3B%20it%20carves%203D%20voxels%20by%20moving%20the%0Aevent%20camera%20around%20a%20stationary%20body%2C%20reconstructs%20the%20human%20pose%20and%20mesh%20by%0Aattenuated%20rays%2C%20and%20fit%20statistical%20body%20models%2C%20preserving%20high-frequency%0Adetails.%20The%20experimental%20results%20show%20that%20the%20proposed%20method%20outperforms%0Aconventional%20frame-based%20methods%20in%20the%20estimation%20accuracy%20of%20both%20pose%20and%0Abody%20mesh.%20We%20also%20demonstrate%20results%20in%20challenging%20situations%20where%20a%0Aconventional%20camera%20has%20motion%20blur.%20This%20is%20the%20first%20to%20demonstrate%0Aevent-only%20human%20mesh%20recovery%2C%20and%20we%20hope%20that%20it%20is%20the%20first%20step%20toward%0Aachieving%20robust%20and%20accurate%203D%20human%20body%20scanning%20from%20vision%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08504v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera&entry.906535625=Kai%20Kohyama%20and%20Shintaro%20Shiba%20and%20Yoshimitsu%20Aoki&entry.1292438233=%20%20Capturing%20the%203D%20human%20body%20is%20one%20of%20the%20important%20tasks%20in%20computer%20vision%0Awith%20a%20wide%20range%20of%20applications%20such%20as%20virtual%20reality%20and%20sports%20analysis.%0AHowever%2C%20conventional%20frame%20cameras%20are%20limited%20by%20their%20temporal%20resolution%0Aand%20dynamic%20range%2C%20which%20imposes%20constraints%20in%20real-world%20application%20setups.%0AEvent%20cameras%20have%20the%20advantages%20of%20high%20temporal%20resolution%20and%20high%20dynamic%0Arange%20%28HDR%29%2C%20but%20the%20development%20of%20event-based%20methods%20is%20necessary%20to%20handle%0Adata%20with%20different%20characteristics.%20This%20paper%20proposes%20a%20novel%20event-based%0Amethod%20for%203D%20pose%20estimation%20and%20human%20mesh%20recovery.%20Prior%20work%20on%0Aevent-based%20human%20mesh%20recovery%20require%20frames%20%28images%29%20as%20well%20as%20event%20data.%0AThe%20proposed%20method%20solely%20relies%20on%20events%3B%20it%20carves%203D%20voxels%20by%20moving%20the%0Aevent%20camera%20around%20a%20stationary%20body%2C%20reconstructs%20the%20human%20pose%20and%20mesh%20by%0Aattenuated%20rays%2C%20and%20fit%20statistical%20body%20models%2C%20preserving%20high-frequency%0Adetails.%20The%20experimental%20results%20show%20that%20the%20proposed%20method%20outperforms%0Aconventional%20frame-based%20methods%20in%20the%20estimation%20accuracy%20of%20both%20pose%20and%0Abody%20mesh.%20We%20also%20demonstrate%20results%20in%20challenging%20situations%20where%20a%0Aconventional%20camera%20has%20motion%20blur.%20This%20is%20the%20first%20to%20demonstrate%0Aevent-only%20human%20mesh%20recovery%2C%20and%20we%20hope%20that%20it%20is%20the%20first%20step%20toward%0Aachieving%20robust%20and%20accurate%203D%20human%20body%20scanning%20from%20vision%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08504v1&entry.124074799=Read"},
{"title": "Identifying Important Group of Pixels using Interactions", "author": "Kosuke Sumiyasu and Kazuhiko Kawamoto and Hiroshi Kera", "abstract": "  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI ($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization by Grad-CAM, Attention rollout, and\nShapley value. While prior studies have suffered from the exponential\ncomputational cost in the computation of Shapley value and interactions, we\nshow that this can be reduced to quadratic cost for our task. The code is\navailable at https://github.com/KosukeSumiyasu/MoXI.\n", "link": "http://arxiv.org/abs/2401.03785v2", "date": "2024-04-12", "relevancy": 2.08, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5214}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions&body=Title%3A%20Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions%0AAuthor%3A%20Kosuke%20Sumiyasu%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera%0AAbstract%3A%20%20%20To%20better%20understand%20the%20behavior%20of%20image%20classifiers%2C%20it%20is%20useful%20to%0Avisualize%20the%20contribution%20of%20individual%20pixels%20to%20the%20model%20prediction.%20In%0Athis%20study%2C%20we%20propose%20a%20method%2C%20MoXI%20%28%24%5Ctextbf%7BMo%7D%24del%20e%24%5Ctextbf%7BX%7D%24planation%0Aby%20%24%5Ctextbf%7BI%7D%24nteractions%29%2C%20that%20efficiently%20and%20accurately%20identifies%20a%20group%0Aof%20pixels%20with%20high%20prediction%20confidence.%20The%20proposed%20method%20employs%0Agame-theoretic%20concepts%2C%20Shapley%20values%20and%20interactions%2C%20taking%20into%20account%0Athe%20effects%20of%20individual%20pixels%20and%20the%20cooperative%20influence%20of%20pixels%20on%0Amodel%20confidence.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20our%0Amethod%20better%20identifies%20the%20pixels%20that%20are%20highly%20contributing%20to%20the%20model%0Aoutputs%20than%20widely-used%20visualization%20by%20Grad-CAM%2C%20Attention%20rollout%2C%20and%0AShapley%20value.%20While%20prior%20studies%20have%20suffered%20from%20the%20exponential%0Acomputational%20cost%20in%20the%20computation%20of%20Shapley%20value%20and%20interactions%2C%20we%0Ashow%20that%20this%20can%20be%20reduced%20to%20quadratic%20cost%20for%20our%20task.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KosukeSumiyasu/MoXI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03785v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions&entry.906535625=Kosuke%20Sumiyasu%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera&entry.1292438233=%20%20To%20better%20understand%20the%20behavior%20of%20image%20classifiers%2C%20it%20is%20useful%20to%0Avisualize%20the%20contribution%20of%20individual%20pixels%20to%20the%20model%20prediction.%20In%0Athis%20study%2C%20we%20propose%20a%20method%2C%20MoXI%20%28%24%5Ctextbf%7BMo%7D%24del%20e%24%5Ctextbf%7BX%7D%24planation%0Aby%20%24%5Ctextbf%7BI%7D%24nteractions%29%2C%20that%20efficiently%20and%20accurately%20identifies%20a%20group%0Aof%20pixels%20with%20high%20prediction%20confidence.%20The%20proposed%20method%20employs%0Agame-theoretic%20concepts%2C%20Shapley%20values%20and%20interactions%2C%20taking%20into%20account%0Athe%20effects%20of%20individual%20pixels%20and%20the%20cooperative%20influence%20of%20pixels%20on%0Amodel%20confidence.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20our%0Amethod%20better%20identifies%20the%20pixels%20that%20are%20highly%20contributing%20to%20the%20model%0Aoutputs%20than%20widely-used%20visualization%20by%20Grad-CAM%2C%20Attention%20rollout%2C%20and%0AShapley%20value.%20While%20prior%20studies%20have%20suffered%20from%20the%20exponential%0Acomputational%20cost%20in%20the%20computation%20of%20Shapley%20value%20and%20interactions%2C%20we%0Ashow%20that%20this%20can%20be%20reduced%20to%20quadratic%20cost%20for%20our%20task.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KosukeSumiyasu/MoXI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03785v2&entry.124074799=Read"},
{"title": "Regularized Gradient Clipping Provably Trains Wide and Deep Neural\n  Networks", "author": "Matteo Tucat and Anirbit Mukherjee", "abstract": "  In this work, we instantiate a regularized form of the gradient clipping\nalgorithm and prove that it can converge to the global minima of deep neural\nnetwork loss functions provided that the net is of sufficient width. We present\nempirical evidence that our theoretically founded regularized gradient clipping\nalgorithm is also competitive with the state-of-the-art deep-learning\nheuristics. Hence the algorithm presented here constitutes a new approach to\nrigorous deep learning.\n  The modification we do to standard gradient clipping is designed to leverage\nthe PL* condition, a variant of the Polyak-Lojasiewicz inequality which was\nrecently proven to be true for various neural networks for any depth within a\nneighborhood of the initialisation.\n", "link": "http://arxiv.org/abs/2404.08624v1", "date": "2024-04-12", "relevancy": 2.0785, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5591}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4844}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Regularized%20Gradient%20Clipping%20Provably%20Trains%20Wide%20and%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20Regularized%20Gradient%20Clipping%20Provably%20Trains%20Wide%20and%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Matteo%20Tucat%20and%20Anirbit%20Mukherjee%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20instantiate%20a%20regularized%20form%20of%20the%20gradient%20clipping%0Aalgorithm%20and%20prove%20that%20it%20can%20converge%20to%20the%20global%20minima%20of%20deep%20neural%0Anetwork%20loss%20functions%20provided%20that%20the%20net%20is%20of%20sufficient%20width.%20We%20present%0Aempirical%20evidence%20that%20our%20theoretically%20founded%20regularized%20gradient%20clipping%0Aalgorithm%20is%20also%20competitive%20with%20the%20state-of-the-art%20deep-learning%0Aheuristics.%20Hence%20the%20algorithm%20presented%20here%20constitutes%20a%20new%20approach%20to%0Arigorous%20deep%20learning.%0A%20%20The%20modification%20we%20do%20to%20standard%20gradient%20clipping%20is%20designed%20to%20leverage%0Athe%20PL%2A%20condition%2C%20a%20variant%20of%20the%20Polyak-Lojasiewicz%20inequality%20which%20was%0Arecently%20proven%20to%20be%20true%20for%20various%20neural%20networks%20for%20any%20depth%20within%20a%0Aneighborhood%20of%20the%20initialisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Gradient%20Clipping%20Provably%20Trains%20Wide%20and%20Deep%20Neural%0A%20%20Networks&entry.906535625=Matteo%20Tucat%20and%20Anirbit%20Mukherjee&entry.1292438233=%20%20In%20this%20work%2C%20we%20instantiate%20a%20regularized%20form%20of%20the%20gradient%20clipping%0Aalgorithm%20and%20prove%20that%20it%20can%20converge%20to%20the%20global%20minima%20of%20deep%20neural%0Anetwork%20loss%20functions%20provided%20that%20the%20net%20is%20of%20sufficient%20width.%20We%20present%0Aempirical%20evidence%20that%20our%20theoretically%20founded%20regularized%20gradient%20clipping%0Aalgorithm%20is%20also%20competitive%20with%20the%20state-of-the-art%20deep-learning%0Aheuristics.%20Hence%20the%20algorithm%20presented%20here%20constitutes%20a%20new%20approach%20to%0Arigorous%20deep%20learning.%0A%20%20The%20modification%20we%20do%20to%20standard%20gradient%20clipping%20is%20designed%20to%20leverage%0Athe%20PL%2A%20condition%2C%20a%20variant%20of%20the%20Polyak-Lojasiewicz%20inequality%20which%20was%0Arecently%20proven%20to%20be%20true%20for%20various%20neural%20networks%20for%20any%20depth%20within%20a%0Aneighborhood%20of%20the%20initialisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08624v1&entry.124074799=Read"},
{"title": "Pathological Primitive Segmentation Based on Visual Foundation Model\n  with Zero-Shot Mask Generation", "author": "Abu Bakor Hayat Arnob and Xiangxue Wang and Yiping Jiao and Xiao Gan and Wenlong Ming and Jun Xu", "abstract": "  Medical image processing usually requires a model trained with carefully\ncrafted datasets due to unique image characteristics and domain-specific\nchallenges, especially in pathology. Primitive detection and segmentation in\ndigitized tissue samples are essential for objective and automated diagnosis\nand prognosis of cancer. SAM (Segment Anything Model) has recently been\ndeveloped to segment general objects from natural images with high accuracy,\nbut it requires human prompts to generate masks. In this work, we present a\nnovel approach that adapts pre-trained natural image encoders of SAM for\ndetection-based region proposals. Regions proposed by a pre-trained encoder are\nsent to cascaded feature propagation layers for projection. Then, local\nsemantic and global context is aggregated from multi-scale for bounding box\nlocalization and classification. Finally, the SAM decoder uses the identified\nbounding boxes as essential prompts to generate a comprehensive primitive\nsegmentation map. The entire base framework, SAM, requires no additional\ntraining or fine-tuning but could produce an end-to-end result for two\nfundamental segmentation tasks in pathology. Our method compares with\nstate-of-the-art models in F1 score for nuclei detection and binary/multiclass\npanoptic(bPQ/mPQ) and mask quality(dice) for segmentation quality on the\nPanNuke dataset while offering end-to-end efficiency. Our model also achieves\nremarkable Average Precision (+4.5%) on the secondary dataset (HuBMAP Kidney)\ncompared to Faster RCNN. The code is publicly available at\nhttps://github.com/learner-codec/autoprom_sam.\n", "link": "http://arxiv.org/abs/2404.08584v1", "date": "2024-04-12", "relevancy": 2.0772, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pathological%20Primitive%20Segmentation%20Based%20on%20Visual%20Foundation%20Model%0A%20%20with%20Zero-Shot%20Mask%20Generation&body=Title%3A%20Pathological%20Primitive%20Segmentation%20Based%20on%20Visual%20Foundation%20Model%0A%20%20with%20Zero-Shot%20Mask%20Generation%0AAuthor%3A%20Abu%20Bakor%20Hayat%20Arnob%20and%20Xiangxue%20Wang%20and%20Yiping%20Jiao%20and%20Xiao%20Gan%20and%20Wenlong%20Ming%20and%20Jun%20Xu%0AAbstract%3A%20%20%20Medical%20image%20processing%20usually%20requires%20a%20model%20trained%20with%20carefully%0Acrafted%20datasets%20due%20to%20unique%20image%20characteristics%20and%20domain-specific%0Achallenges%2C%20especially%20in%20pathology.%20Primitive%20detection%20and%20segmentation%20in%0Adigitized%20tissue%20samples%20are%20essential%20for%20objective%20and%20automated%20diagnosis%0Aand%20prognosis%20of%20cancer.%20SAM%20%28Segment%20Anything%20Model%29%20has%20recently%20been%0Adeveloped%20to%20segment%20general%20objects%20from%20natural%20images%20with%20high%20accuracy%2C%0Abut%20it%20requires%20human%20prompts%20to%20generate%20masks.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20approach%20that%20adapts%20pre-trained%20natural%20image%20encoders%20of%20SAM%20for%0Adetection-based%20region%20proposals.%20Regions%20proposed%20by%20a%20pre-trained%20encoder%20are%0Asent%20to%20cascaded%20feature%20propagation%20layers%20for%20projection.%20Then%2C%20local%0Asemantic%20and%20global%20context%20is%20aggregated%20from%20multi-scale%20for%20bounding%20box%0Alocalization%20and%20classification.%20Finally%2C%20the%20SAM%20decoder%20uses%20the%20identified%0Abounding%20boxes%20as%20essential%20prompts%20to%20generate%20a%20comprehensive%20primitive%0Asegmentation%20map.%20The%20entire%20base%20framework%2C%20SAM%2C%20requires%20no%20additional%0Atraining%20or%20fine-tuning%20but%20could%20produce%20an%20end-to-end%20result%20for%20two%0Afundamental%20segmentation%20tasks%20in%20pathology.%20Our%20method%20compares%20with%0Astate-of-the-art%20models%20in%20F1%20score%20for%20nuclei%20detection%20and%20binary/multiclass%0Apanoptic%28bPQ/mPQ%29%20and%20mask%20quality%28dice%29%20for%20segmentation%20quality%20on%20the%0APanNuke%20dataset%20while%20offering%20end-to-end%20efficiency.%20Our%20model%20also%20achieves%0Aremarkable%20Average%20Precision%20%28%2B4.5%25%29%20on%20the%20secondary%20dataset%20%28HuBMAP%20Kidney%29%0Acompared%20to%20Faster%20RCNN.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/learner-codec/autoprom_sam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pathological%20Primitive%20Segmentation%20Based%20on%20Visual%20Foundation%20Model%0A%20%20with%20Zero-Shot%20Mask%20Generation&entry.906535625=Abu%20Bakor%20Hayat%20Arnob%20and%20Xiangxue%20Wang%20and%20Yiping%20Jiao%20and%20Xiao%20Gan%20and%20Wenlong%20Ming%20and%20Jun%20Xu&entry.1292438233=%20%20Medical%20image%20processing%20usually%20requires%20a%20model%20trained%20with%20carefully%0Acrafted%20datasets%20due%20to%20unique%20image%20characteristics%20and%20domain-specific%0Achallenges%2C%20especially%20in%20pathology.%20Primitive%20detection%20and%20segmentation%20in%0Adigitized%20tissue%20samples%20are%20essential%20for%20objective%20and%20automated%20diagnosis%0Aand%20prognosis%20of%20cancer.%20SAM%20%28Segment%20Anything%20Model%29%20has%20recently%20been%0Adeveloped%20to%20segment%20general%20objects%20from%20natural%20images%20with%20high%20accuracy%2C%0Abut%20it%20requires%20human%20prompts%20to%20generate%20masks.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20approach%20that%20adapts%20pre-trained%20natural%20image%20encoders%20of%20SAM%20for%0Adetection-based%20region%20proposals.%20Regions%20proposed%20by%20a%20pre-trained%20encoder%20are%0Asent%20to%20cascaded%20feature%20propagation%20layers%20for%20projection.%20Then%2C%20local%0Asemantic%20and%20global%20context%20is%20aggregated%20from%20multi-scale%20for%20bounding%20box%0Alocalization%20and%20classification.%20Finally%2C%20the%20SAM%20decoder%20uses%20the%20identified%0Abounding%20boxes%20as%20essential%20prompts%20to%20generate%20a%20comprehensive%20primitive%0Asegmentation%20map.%20The%20entire%20base%20framework%2C%20SAM%2C%20requires%20no%20additional%0Atraining%20or%20fine-tuning%20but%20could%20produce%20an%20end-to-end%20result%20for%20two%0Afundamental%20segmentation%20tasks%20in%20pathology.%20Our%20method%20compares%20with%0Astate-of-the-art%20models%20in%20F1%20score%20for%20nuclei%20detection%20and%20binary/multiclass%0Apanoptic%28bPQ/mPQ%29%20and%20mask%20quality%28dice%29%20for%20segmentation%20quality%20on%20the%0APanNuke%20dataset%20while%20offering%20end-to-end%20efficiency.%20Our%20model%20also%20achieves%0Aremarkable%20Average%20Precision%20%28%2B4.5%25%29%20on%20the%20secondary%20dataset%20%28HuBMAP%20Kidney%29%0Acompared%20to%20Faster%20RCNN.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/learner-codec/autoprom_sam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08584v1&entry.124074799=Read"},
{"title": "MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image\n  Fusion", "author": "Zhe Li and Haiwei Pan and Kejia Zhang and Yuhua Wang and Fengming Yu", "abstract": "  Multi-modality image fusion (MMIF) aims to integrate complementary\ninformation from different modalities into a single fused image to represent\nthe imaging scene and facilitate downstream visual tasks comprehensively. In\nrecent years, significant progress has been made in MMIF tasks due to advances\nin deep neural networks. However, existing methods cannot effectively and\nefficiently extract modality-specific and modality-fused features constrained\nby the inherent local reductive bias (CNN) or quadratic computational\ncomplexity (Transformers). To overcome this issue, we propose a Mamba-based\nDual-phase Fusion (MambaDFuse) model. Firstly, a dual-level feature extractor\nis designed to capture long-range features from single-modality images by\nextracting low and high-level features from CNN and Mamba blocks. Then, a\ndual-phase feature fusion module is proposed to obtain fusion features that\ncombine complementary information from different modalities. It uses the\nchannel exchange method for shallow fusion and the enhanced Multi-modal Mamba\n(M3) blocks for deep fusion. Finally, the fused image reconstruction module\nutilizes the inverse transformation of the feature extraction to generate the\nfused result. Through extensive experiments, our approach achieves promising\nfusion results in infrared-visible image fusion and medical image fusion.\nAdditionally, in a unified benchmark, MambaDFuse has also demonstrated improved\nperformance in downstream tasks such as object detection. Code with checkpoints\nwill be available after the peer-review process.\n", "link": "http://arxiv.org/abs/2404.08406v1", "date": "2024-04-12", "relevancy": 2.067, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5114}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaDFuse%3A%20A%20Mamba-based%20Dual-phase%20Model%20for%20Multi-modality%20Image%0A%20%20Fusion&body=Title%3A%20MambaDFuse%3A%20A%20Mamba-based%20Dual-phase%20Model%20for%20Multi-modality%20Image%0A%20%20Fusion%0AAuthor%3A%20Zhe%20Li%20and%20Haiwei%20Pan%20and%20Kejia%20Zhang%20and%20Yuhua%20Wang%20and%20Fengming%20Yu%0AAbstract%3A%20%20%20Multi-modality%20image%20fusion%20%28MMIF%29%20aims%20to%20integrate%20complementary%0Ainformation%20from%20different%20modalities%20into%20a%20single%20fused%20image%20to%20represent%0Athe%20imaging%20scene%20and%20facilitate%20downstream%20visual%20tasks%20comprehensively.%20In%0Arecent%20years%2C%20significant%20progress%20has%20been%20made%20in%20MMIF%20tasks%20due%20to%20advances%0Ain%20deep%20neural%20networks.%20However%2C%20existing%20methods%20cannot%20effectively%20and%0Aefficiently%20extract%20modality-specific%20and%20modality-fused%20features%20constrained%0Aby%20the%20inherent%20local%20reductive%20bias%20%28CNN%29%20or%20quadratic%20computational%0Acomplexity%20%28Transformers%29.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20Mamba-based%0ADual-phase%20Fusion%20%28MambaDFuse%29%20model.%20Firstly%2C%20a%20dual-level%20feature%20extractor%0Ais%20designed%20to%20capture%20long-range%20features%20from%20single-modality%20images%20by%0Aextracting%20low%20and%20high-level%20features%20from%20CNN%20and%20Mamba%20blocks.%20Then%2C%20a%0Adual-phase%20feature%20fusion%20module%20is%20proposed%20to%20obtain%20fusion%20features%20that%0Acombine%20complementary%20information%20from%20different%20modalities.%20It%20uses%20the%0Achannel%20exchange%20method%20for%20shallow%20fusion%20and%20the%20enhanced%20Multi-modal%20Mamba%0A%28M3%29%20blocks%20for%20deep%20fusion.%20Finally%2C%20the%20fused%20image%20reconstruction%20module%0Autilizes%20the%20inverse%20transformation%20of%20the%20feature%20extraction%20to%20generate%20the%0Afused%20result.%20Through%20extensive%20experiments%2C%20our%20approach%20achieves%20promising%0Afusion%20results%20in%20infrared-visible%20image%20fusion%20and%20medical%20image%20fusion.%0AAdditionally%2C%20in%20a%20unified%20benchmark%2C%20MambaDFuse%20has%20also%20demonstrated%20improved%0Aperformance%20in%20downstream%20tasks%20such%20as%20object%20detection.%20Code%20with%20checkpoints%0Awill%20be%20available%20after%20the%20peer-review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08406v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaDFuse%3A%20A%20Mamba-based%20Dual-phase%20Model%20for%20Multi-modality%20Image%0A%20%20Fusion&entry.906535625=Zhe%20Li%20and%20Haiwei%20Pan%20and%20Kejia%20Zhang%20and%20Yuhua%20Wang%20and%20Fengming%20Yu&entry.1292438233=%20%20Multi-modality%20image%20fusion%20%28MMIF%29%20aims%20to%20integrate%20complementary%0Ainformation%20from%20different%20modalities%20into%20a%20single%20fused%20image%20to%20represent%0Athe%20imaging%20scene%20and%20facilitate%20downstream%20visual%20tasks%20comprehensively.%20In%0Arecent%20years%2C%20significant%20progress%20has%20been%20made%20in%20MMIF%20tasks%20due%20to%20advances%0Ain%20deep%20neural%20networks.%20However%2C%20existing%20methods%20cannot%20effectively%20and%0Aefficiently%20extract%20modality-specific%20and%20modality-fused%20features%20constrained%0Aby%20the%20inherent%20local%20reductive%20bias%20%28CNN%29%20or%20quadratic%20computational%0Acomplexity%20%28Transformers%29.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20Mamba-based%0ADual-phase%20Fusion%20%28MambaDFuse%29%20model.%20Firstly%2C%20a%20dual-level%20feature%20extractor%0Ais%20designed%20to%20capture%20long-range%20features%20from%20single-modality%20images%20by%0Aextracting%20low%20and%20high-level%20features%20from%20CNN%20and%20Mamba%20blocks.%20Then%2C%20a%0Adual-phase%20feature%20fusion%20module%20is%20proposed%20to%20obtain%20fusion%20features%20that%0Acombine%20complementary%20information%20from%20different%20modalities.%20It%20uses%20the%0Achannel%20exchange%20method%20for%20shallow%20fusion%20and%20the%20enhanced%20Multi-modal%20Mamba%0A%28M3%29%20blocks%20for%20deep%20fusion.%20Finally%2C%20the%20fused%20image%20reconstruction%20module%0Autilizes%20the%20inverse%20transformation%20of%20the%20feature%20extraction%20to%20generate%20the%0Afused%20result.%20Through%20extensive%20experiments%2C%20our%20approach%20achieves%20promising%0Afusion%20results%20in%20infrared-visible%20image%20fusion%20and%20medical%20image%20fusion.%0AAdditionally%2C%20in%20a%20unified%20benchmark%2C%20MambaDFuse%20has%20also%20demonstrated%20improved%0Aperformance%20in%20downstream%20tasks%20such%20as%20object%20detection.%20Code%20with%20checkpoints%0Awill%20be%20available%20after%20the%20peer-review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08406v1&entry.124074799=Read"},
{"title": "WonderJourney: Going from Anywhere to Everywhere", "author": "Hong-Xing Yu and Haoyi Duan and Junhwa Hur and Kyle Sargent and Michael Rubinstein and William T. Freeman and Forrester Cole and Deqing Sun and Noah Snavely and Jiajun Wu and Charles Herrmann", "abstract": "  We introduce WonderJourney, a modularized framework for perpetual 3D scene\ngeneration. Unlike prior work on view generation that focuses on a single type\nof scenes, we start at any user-provided location (by a text description or an\nimage) and generate a journey through a long sequence of diverse yet coherently\nconnected 3D scenes. We leverage an LLM to generate textual descriptions of the\nscenes in this journey, a text-driven point cloud generation pipeline to make a\ncompelling and coherent sequence of 3D scenes, and a large VLM to verify the\ngenerated scenes. We show compelling, diverse visual results across various\nscene types and styles, forming imaginary \"wonderjourneys\". Project website:\nhttps://kovenyu.com/WonderJourney/\n", "link": "http://arxiv.org/abs/2312.03884v2", "date": "2024-04-12", "relevancy": 2.0392, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.526}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5198}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4896}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WonderJourney%3A%20Going%20from%20Anywhere%20to%20Everywhere&body=Title%3A%20WonderJourney%3A%20Going%20from%20Anywhere%20to%20Everywhere%0AAuthor%3A%20Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Junhwa%20Hur%20and%20Kyle%20Sargent%20and%20Michael%20Rubinstein%20and%20William%20T.%20Freeman%20and%20Forrester%20Cole%20and%20Deqing%20Sun%20and%20Noah%20Snavely%20and%20Jiajun%20Wu%20and%20Charles%20Herrmann%0AAbstract%3A%20%20%20We%20introduce%20WonderJourney%2C%20a%20modularized%20framework%20for%20perpetual%203D%20scene%0Ageneration.%20Unlike%20prior%20work%20on%20view%20generation%20that%20focuses%20on%20a%20single%20type%0Aof%20scenes%2C%20we%20start%20at%20any%20user-provided%20location%20%28by%20a%20text%20description%20or%20an%0Aimage%29%20and%20generate%20a%20journey%20through%20a%20long%20sequence%20of%20diverse%20yet%20coherently%0Aconnected%203D%20scenes.%20We%20leverage%20an%20LLM%20to%20generate%20textual%20descriptions%20of%20the%0Ascenes%20in%20this%20journey%2C%20a%20text-driven%20point%20cloud%20generation%20pipeline%20to%20make%20a%0Acompelling%20and%20coherent%20sequence%20of%203D%20scenes%2C%20and%20a%20large%20VLM%20to%20verify%20the%0Agenerated%20scenes.%20We%20show%20compelling%2C%20diverse%20visual%20results%20across%20various%0Ascene%20types%20and%20styles%2C%20forming%20imaginary%20%22wonderjourneys%22.%20Project%20website%3A%0Ahttps%3A//kovenyu.com/WonderJourney/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03884v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WonderJourney%3A%20Going%20from%20Anywhere%20to%20Everywhere&entry.906535625=Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Junhwa%20Hur%20and%20Kyle%20Sargent%20and%20Michael%20Rubinstein%20and%20William%20T.%20Freeman%20and%20Forrester%20Cole%20and%20Deqing%20Sun%20and%20Noah%20Snavely%20and%20Jiajun%20Wu%20and%20Charles%20Herrmann&entry.1292438233=%20%20We%20introduce%20WonderJourney%2C%20a%20modularized%20framework%20for%20perpetual%203D%20scene%0Ageneration.%20Unlike%20prior%20work%20on%20view%20generation%20that%20focuses%20on%20a%20single%20type%0Aof%20scenes%2C%20we%20start%20at%20any%20user-provided%20location%20%28by%20a%20text%20description%20or%20an%0Aimage%29%20and%20generate%20a%20journey%20through%20a%20long%20sequence%20of%20diverse%20yet%20coherently%0Aconnected%203D%20scenes.%20We%20leverage%20an%20LLM%20to%20generate%20textual%20descriptions%20of%20the%0Ascenes%20in%20this%20journey%2C%20a%20text-driven%20point%20cloud%20generation%20pipeline%20to%20make%20a%0Acompelling%20and%20coherent%20sequence%20of%203D%20scenes%2C%20and%20a%20large%20VLM%20to%20verify%20the%0Agenerated%20scenes.%20We%20show%20compelling%2C%20diverse%20visual%20results%20across%20various%0Ascene%20types%20and%20styles%2C%20forming%20imaginary%20%22wonderjourneys%22.%20Project%20website%3A%0Ahttps%3A//kovenyu.com/WonderJourney/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03884v2&entry.124074799=Read"},
{"title": "OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering", "author": "Jingrui Ye and Zongkai Zhang and Yujiao Jiang and Qingmin Liao and Wenming Yang and Zongqing Lu", "abstract": "  Rendering dynamic 3D human from monocular videos is crucial for various\napplications such as virtual reality and digital entertainment. Most methods\nassume the people is in an unobstructed scene, while various objects may cause\nthe occlusion of body parts in real-life scenarios. Previous method utilizing\nNeRF for surface rendering to recover the occluded areas, but it requiring more\nthan one day to train and several seconds to render, failing to meet the\nrequirements of real-time interactive applications. To address these issues, we\npropose OccGaussian based on 3D Gaussian Splatting, which can be trained within\n6 minutes and produces high-quality human renderings up to 160 FPS with\noccluded input. OccGaussian initializes 3D Gaussian distributions in the\ncanonical space, and we perform occlusion feature query at occluded regions,\nthe aggregated pixel-align feature is extracted to compensate for the missing\ninformation. Then we use Gaussian Feature MLP to further process the feature\nalong with the occlusion-aware loss functions to better perceive the occluded\narea. Extensive experiments both in simulated and real-world occlusions,\ndemonstrate that our method achieves comparable or even superior performance\ncompared to the state-of-the-art method. And we improving training and\ninference speeds by 250x and 800x, respectively. Our code will be available for\nresearch purposes.\n", "link": "http://arxiv.org/abs/2404.08449v1", "date": "2024-04-12", "relevancy": 2.0306, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5099}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5076}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5054}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OccGaussian%3A%203D%20Gaussian%20Splatting%20for%20Occluded%20Human%20Rendering&body=Title%3A%20OccGaussian%3A%203D%20Gaussian%20Splatting%20for%20Occluded%20Human%20Rendering%0AAuthor%3A%20Jingrui%20Ye%20and%20Zongkai%20Zhang%20and%20Yujiao%20Jiang%20and%20Qingmin%20Liao%20and%20Wenming%20Yang%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Rendering%20dynamic%203D%20human%20from%20monocular%20videos%20is%20crucial%20for%20various%0Aapplications%20such%20as%20virtual%20reality%20and%20digital%20entertainment.%20Most%20methods%0Aassume%20the%20people%20is%20in%20an%20unobstructed%20scene%2C%20while%20various%20objects%20may%20cause%0Athe%20occlusion%20of%20body%20parts%20in%20real-life%20scenarios.%20Previous%20method%20utilizing%0ANeRF%20for%20surface%20rendering%20to%20recover%20the%20occluded%20areas%2C%20but%20it%20requiring%20more%0Athan%20one%20day%20to%20train%20and%20several%20seconds%20to%20render%2C%20failing%20to%20meet%20the%0Arequirements%20of%20real-time%20interactive%20applications.%20To%20address%20these%20issues%2C%20we%0Apropose%20OccGaussian%20based%20on%203D%20Gaussian%20Splatting%2C%20which%20can%20be%20trained%20within%0A6%20minutes%20and%20produces%20high-quality%20human%20renderings%20up%20to%20160%20FPS%20with%0Aoccluded%20input.%20OccGaussian%20initializes%203D%20Gaussian%20distributions%20in%20the%0Acanonical%20space%2C%20and%20we%20perform%20occlusion%20feature%20query%20at%20occluded%20regions%2C%0Athe%20aggregated%20pixel-align%20feature%20is%20extracted%20to%20compensate%20for%20the%20missing%0Ainformation.%20Then%20we%20use%20Gaussian%20Feature%20MLP%20to%20further%20process%20the%20feature%0Aalong%20with%20the%20occlusion-aware%20loss%20functions%20to%20better%20perceive%20the%20occluded%0Aarea.%20Extensive%20experiments%20both%20in%20simulated%20and%20real-world%20occlusions%2C%0Ademonstrate%20that%20our%20method%20achieves%20comparable%20or%20even%20superior%20performance%0Acompared%20to%20the%20state-of-the-art%20method.%20And%20we%20improving%20training%20and%0Ainference%20speeds%20by%20250x%20and%20800x%2C%20respectively.%20Our%20code%20will%20be%20available%20for%0Aresearch%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08449v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccGaussian%3A%203D%20Gaussian%20Splatting%20for%20Occluded%20Human%20Rendering&entry.906535625=Jingrui%20Ye%20and%20Zongkai%20Zhang%20and%20Yujiao%20Jiang%20and%20Qingmin%20Liao%20and%20Wenming%20Yang%20and%20Zongqing%20Lu&entry.1292438233=%20%20Rendering%20dynamic%203D%20human%20from%20monocular%20videos%20is%20crucial%20for%20various%0Aapplications%20such%20as%20virtual%20reality%20and%20digital%20entertainment.%20Most%20methods%0Aassume%20the%20people%20is%20in%20an%20unobstructed%20scene%2C%20while%20various%20objects%20may%20cause%0Athe%20occlusion%20of%20body%20parts%20in%20real-life%20scenarios.%20Previous%20method%20utilizing%0ANeRF%20for%20surface%20rendering%20to%20recover%20the%20occluded%20areas%2C%20but%20it%20requiring%20more%0Athan%20one%20day%20to%20train%20and%20several%20seconds%20to%20render%2C%20failing%20to%20meet%20the%0Arequirements%20of%20real-time%20interactive%20applications.%20To%20address%20these%20issues%2C%20we%0Apropose%20OccGaussian%20based%20on%203D%20Gaussian%20Splatting%2C%20which%20can%20be%20trained%20within%0A6%20minutes%20and%20produces%20high-quality%20human%20renderings%20up%20to%20160%20FPS%20with%0Aoccluded%20input.%20OccGaussian%20initializes%203D%20Gaussian%20distributions%20in%20the%0Acanonical%20space%2C%20and%20we%20perform%20occlusion%20feature%20query%20at%20occluded%20regions%2C%0Athe%20aggregated%20pixel-align%20feature%20is%20extracted%20to%20compensate%20for%20the%20missing%0Ainformation.%20Then%20we%20use%20Gaussian%20Feature%20MLP%20to%20further%20process%20the%20feature%0Aalong%20with%20the%20occlusion-aware%20loss%20functions%20to%20better%20perceive%20the%20occluded%0Aarea.%20Extensive%20experiments%20both%20in%20simulated%20and%20real-world%20occlusions%2C%0Ademonstrate%20that%20our%20method%20achieves%20comparable%20or%20even%20superior%20performance%0Acompared%20to%20the%20state-of-the-art%20method.%20And%20we%20improving%20training%20and%0Ainference%20speeds%20by%20250x%20and%20800x%2C%20respectively.%20Our%20code%20will%20be%20available%20for%0Aresearch%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08449v1&entry.124074799=Read"},
{"title": "Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking", "author": "Tianyu Zhu and Myong Chol Jung and Jesse Clark", "abstract": "  Contrastive learning has gained widespread adoption for retrieval tasks due\nto its minimal requirement for manual annotations. However, popular contrastive\nframeworks typically learn from binary relevance, making them ineffective at\nincorporating direct fine-grained rankings. In this paper, we curate a\nlarge-scale dataset featuring detailed relevance scores for each query-document\npair to facilitate future research and evaluation. Subsequently, we propose\nGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),\nwhich is designed to learn from fine-grained rankings beyond binary relevance\nscores. Our results show that GCL achieves a 94.5% increase in NDCG@10 for\nin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative\nto the CLIP baseline and involving ground truth rankings.\n", "link": "http://arxiv.org/abs/2404.08535v1", "date": "2024-04-12", "relevancy": 2.0305, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalized%20Contrastive%20Learning%20for%20Multi-Modal%20Retrieval%20and%20Ranking&body=Title%3A%20Generalized%20Contrastive%20Learning%20for%20Multi-Modal%20Retrieval%20and%20Ranking%0AAuthor%3A%20Tianyu%20Zhu%20and%20Myong%20Chol%20Jung%20and%20Jesse%20Clark%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20gained%20widespread%20adoption%20for%20retrieval%20tasks%20due%0Ato%20its%20minimal%20requirement%20for%20manual%20annotations.%20However%2C%20popular%20contrastive%0Aframeworks%20typically%20learn%20from%20binary%20relevance%2C%20making%20them%20ineffective%20at%0Aincorporating%20direct%20fine-grained%20rankings.%20In%20this%20paper%2C%20we%20curate%20a%0Alarge-scale%20dataset%20featuring%20detailed%20relevance%20scores%20for%20each%20query-document%0Apair%20to%20facilitate%20future%20research%20and%20evaluation.%20Subsequently%2C%20we%20propose%0AGeneralized%20Contrastive%20Learning%20for%20Multi-Modal%20Retrieval%20and%20Ranking%20%28GCL%29%2C%0Awhich%20is%20designed%20to%20learn%20from%20fine-grained%20rankings%20beyond%20binary%20relevance%0Ascores.%20Our%20results%20show%20that%20GCL%20achieves%20a%2094.5%25%20increase%20in%20NDCG%4010%20for%0Ain-domain%20and%2026.3%20to%2048.8%25%20increases%20for%20cold-start%20evaluations%2C%20all%20relative%0Ato%20the%20CLIP%20baseline%20and%20involving%20ground%20truth%20rankings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08535v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Contrastive%20Learning%20for%20Multi-Modal%20Retrieval%20and%20Ranking&entry.906535625=Tianyu%20Zhu%20and%20Myong%20Chol%20Jung%20and%20Jesse%20Clark&entry.1292438233=%20%20Contrastive%20learning%20has%20gained%20widespread%20adoption%20for%20retrieval%20tasks%20due%0Ato%20its%20minimal%20requirement%20for%20manual%20annotations.%20However%2C%20popular%20contrastive%0Aframeworks%20typically%20learn%20from%20binary%20relevance%2C%20making%20them%20ineffective%20at%0Aincorporating%20direct%20fine-grained%20rankings.%20In%20this%20paper%2C%20we%20curate%20a%0Alarge-scale%20dataset%20featuring%20detailed%20relevance%20scores%20for%20each%20query-document%0Apair%20to%20facilitate%20future%20research%20and%20evaluation.%20Subsequently%2C%20we%20propose%0AGeneralized%20Contrastive%20Learning%20for%20Multi-Modal%20Retrieval%20and%20Ranking%20%28GCL%29%2C%0Awhich%20is%20designed%20to%20learn%20from%20fine-grained%20rankings%20beyond%20binary%20relevance%0Ascores.%20Our%20results%20show%20that%20GCL%20achieves%20a%2094.5%25%20increase%20in%20NDCG%4010%20for%0Ain-domain%20and%2026.3%20to%2048.8%25%20increases%20for%20cold-start%20evaluations%2C%20all%20relative%0Ato%20the%20CLIP%20baseline%20and%20involving%20ground%20truth%20rankings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08535v1&entry.124074799=Read"},
{"title": "Federated Optimization with Doubly Regularized Drift Correction", "author": "Xiaowen Jiang and Anton Rodomanov and Sebastian U. Stich", "abstract": "  Federated learning is a distributed optimization paradigm that allows\ntraining machine learning models across decentralized devices while keeping the\ndata localized. The standard method, FedAvg, suffers from client drift which\ncan hamper performance and increase communication costs over centralized\nmethods. Previous works proposed various strategies to mitigate drift, yet none\nhave shown uniformly improved communication-computation trade-offs over vanilla\ngradient descent.\n  In this work, we revisit DANE, an established method in distributed\noptimization. We show that (i) DANE can achieve the desired communication\nreduction under Hessian similarity constraints. Furthermore, (ii) we present an\nextension, DANE+, which supports arbitrary inexact local solvers and has more\nfreedom to choose how to aggregate the local updates. We propose (iii) a novel\nmethod, FedRed, which has improved local computational complexity and retains\nthe same communication complexity compared to DANE/DANE+. This is achieved by\nusing doubly regularized drift correction.\n", "link": "http://arxiv.org/abs/2404.08447v1", "date": "2024-04-12", "relevancy": 2.0284, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5349}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4837}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Optimization%20with%20Doubly%20Regularized%20Drift%20Correction&body=Title%3A%20Federated%20Optimization%20with%20Doubly%20Regularized%20Drift%20Correction%0AAuthor%3A%20Xiaowen%20Jiang%20and%20Anton%20Rodomanov%20and%20Sebastian%20U.%20Stich%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20distributed%20optimization%20paradigm%20that%20allows%0Atraining%20machine%20learning%20models%20across%20decentralized%20devices%20while%20keeping%20the%0Adata%20localized.%20The%20standard%20method%2C%20FedAvg%2C%20suffers%20from%20client%20drift%20which%0Acan%20hamper%20performance%20and%20increase%20communication%20costs%20over%20centralized%0Amethods.%20Previous%20works%20proposed%20various%20strategies%20to%20mitigate%20drift%2C%20yet%20none%0Ahave%20shown%20uniformly%20improved%20communication-computation%20trade-offs%20over%20vanilla%0Agradient%20descent.%0A%20%20In%20this%20work%2C%20we%20revisit%20DANE%2C%20an%20established%20method%20in%20distributed%0Aoptimization.%20We%20show%20that%20%28i%29%20DANE%20can%20achieve%20the%20desired%20communication%0Areduction%20under%20Hessian%20similarity%20constraints.%20Furthermore%2C%20%28ii%29%20we%20present%20an%0Aextension%2C%20DANE%2B%2C%20which%20supports%20arbitrary%20inexact%20local%20solvers%20and%20has%20more%0Afreedom%20to%20choose%20how%20to%20aggregate%20the%20local%20updates.%20We%20propose%20%28iii%29%20a%20novel%0Amethod%2C%20FedRed%2C%20which%20has%20improved%20local%20computational%20complexity%20and%20retains%0Athe%20same%20communication%20complexity%20compared%20to%20DANE/DANE%2B.%20This%20is%20achieved%20by%0Ausing%20doubly%20regularized%20drift%20correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08447v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Optimization%20with%20Doubly%20Regularized%20Drift%20Correction&entry.906535625=Xiaowen%20Jiang%20and%20Anton%20Rodomanov%20and%20Sebastian%20U.%20Stich&entry.1292438233=%20%20Federated%20learning%20is%20a%20distributed%20optimization%20paradigm%20that%20allows%0Atraining%20machine%20learning%20models%20across%20decentralized%20devices%20while%20keeping%20the%0Adata%20localized.%20The%20standard%20method%2C%20FedAvg%2C%20suffers%20from%20client%20drift%20which%0Acan%20hamper%20performance%20and%20increase%20communication%20costs%20over%20centralized%0Amethods.%20Previous%20works%20proposed%20various%20strategies%20to%20mitigate%20drift%2C%20yet%20none%0Ahave%20shown%20uniformly%20improved%20communication-computation%20trade-offs%20over%20vanilla%0Agradient%20descent.%0A%20%20In%20this%20work%2C%20we%20revisit%20DANE%2C%20an%20established%20method%20in%20distributed%0Aoptimization.%20We%20show%20that%20%28i%29%20DANE%20can%20achieve%20the%20desired%20communication%0Areduction%20under%20Hessian%20similarity%20constraints.%20Furthermore%2C%20%28ii%29%20we%20present%20an%0Aextension%2C%20DANE%2B%2C%20which%20supports%20arbitrary%20inexact%20local%20solvers%20and%20has%20more%0Afreedom%20to%20choose%20how%20to%20aggregate%20the%20local%20updates.%20We%20propose%20%28iii%29%20a%20novel%0Amethod%2C%20FedRed%2C%20which%20has%20improved%20local%20computational%20complexity%20and%20retains%0Athe%20same%20communication%20complexity%20compared%20to%20DANE/DANE%2B.%20This%20is%20achieved%20by%0Ausing%20doubly%20regularized%20drift%20correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08447v1&entry.124074799=Read"},
{"title": "Adapting the Segment Anything Model During Usage in Novel Situations", "author": "Robin Sch\u00f6n and Julian Lorenz and Katja Ludwig and Rainer Lienhart", "abstract": "  The interactive segmentation task consists in the creation of object\nsegmentation masks based on user interactions. The most common way to guide a\nmodel towards producing a correct segmentation consists in clicks on the object\nand background. The recently published Segment Anything Model (SAM) supports a\ngeneralized version of the interactive segmentation problem and has been\ntrained on an object segmentation dataset which contains 1.1B masks. Though\nbeing trained extensively and with the explicit purpose of serving as a\nfoundation model, we show significant limitations of SAM when being applied for\ninteractive segmentation on novel domains or object types. On the used\ndatasets, SAM displays a failure rate $\\text{FR}_{30}@90$ of up to $72.6 \\%$.\nSince we still want such foundation models to be immediately applicable, we\npresent a framework that can adapt SAM during immediate usage. For this we will\nleverage the user interactions and masks, which are constructed during the\ninteractive segmentation process. We use this information to generate\npseudo-labels, which we use to compute a loss function and optimize a part of\nthe SAM model. The presented method causes a relative reduction of up to $48.1\n\\%$ in the $\\text{FR}_{20}@85$ and $46.6 \\%$ in the $\\text{FR}_{30}@90$\nmetrics.\n", "link": "http://arxiv.org/abs/2404.08421v1", "date": "2024-04-12", "relevancy": 2.0226, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4834}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adapting%20the%20Segment%20Anything%20Model%20During%20Usage%20in%20Novel%20Situations&body=Title%3A%20Adapting%20the%20Segment%20Anything%20Model%20During%20Usage%20in%20Novel%20Situations%0AAuthor%3A%20Robin%20Sch%C3%B6n%20and%20Julian%20Lorenz%20and%20Katja%20Ludwig%20and%20Rainer%20Lienhart%0AAbstract%3A%20%20%20The%20interactive%20segmentation%20task%20consists%20in%20the%20creation%20of%20object%0Asegmentation%20masks%20based%20on%20user%20interactions.%20The%20most%20common%20way%20to%20guide%20a%0Amodel%20towards%20producing%20a%20correct%20segmentation%20consists%20in%20clicks%20on%20the%20object%0Aand%20background.%20The%20recently%20published%20Segment%20Anything%20Model%20%28SAM%29%20supports%20a%0Ageneralized%20version%20of%20the%20interactive%20segmentation%20problem%20and%20has%20been%0Atrained%20on%20an%20object%20segmentation%20dataset%20which%20contains%201.1B%20masks.%20Though%0Abeing%20trained%20extensively%20and%20with%20the%20explicit%20purpose%20of%20serving%20as%20a%0Afoundation%20model%2C%20we%20show%20significant%20limitations%20of%20SAM%20when%20being%20applied%20for%0Ainteractive%20segmentation%20on%20novel%20domains%20or%20object%20types.%20On%20the%20used%0Adatasets%2C%20SAM%20displays%20a%20failure%20rate%20%24%5Ctext%7BFR%7D_%7B30%7D%4090%24%20of%20up%20to%20%2472.6%20%5C%25%24.%0ASince%20we%20still%20want%20such%20foundation%20models%20to%20be%20immediately%20applicable%2C%20we%0Apresent%20a%20framework%20that%20can%20adapt%20SAM%20during%20immediate%20usage.%20For%20this%20we%20will%0Aleverage%20the%20user%20interactions%20and%20masks%2C%20which%20are%20constructed%20during%20the%0Ainteractive%20segmentation%20process.%20We%20use%20this%20information%20to%20generate%0Apseudo-labels%2C%20which%20we%20use%20to%20compute%20a%20loss%20function%20and%20optimize%20a%20part%20of%0Athe%20SAM%20model.%20The%20presented%20method%20causes%20a%20relative%20reduction%20of%20up%20to%20%2448.1%0A%5C%25%24%20in%20the%20%24%5Ctext%7BFR%7D_%7B20%7D%4085%24%20and%20%2446.6%20%5C%25%24%20in%20the%20%24%5Ctext%7BFR%7D_%7B30%7D%4090%24%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08421v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20the%20Segment%20Anything%20Model%20During%20Usage%20in%20Novel%20Situations&entry.906535625=Robin%20Sch%C3%B6n%20and%20Julian%20Lorenz%20and%20Katja%20Ludwig%20and%20Rainer%20Lienhart&entry.1292438233=%20%20The%20interactive%20segmentation%20task%20consists%20in%20the%20creation%20of%20object%0Asegmentation%20masks%20based%20on%20user%20interactions.%20The%20most%20common%20way%20to%20guide%20a%0Amodel%20towards%20producing%20a%20correct%20segmentation%20consists%20in%20clicks%20on%20the%20object%0Aand%20background.%20The%20recently%20published%20Segment%20Anything%20Model%20%28SAM%29%20supports%20a%0Ageneralized%20version%20of%20the%20interactive%20segmentation%20problem%20and%20has%20been%0Atrained%20on%20an%20object%20segmentation%20dataset%20which%20contains%201.1B%20masks.%20Though%0Abeing%20trained%20extensively%20and%20with%20the%20explicit%20purpose%20of%20serving%20as%20a%0Afoundation%20model%2C%20we%20show%20significant%20limitations%20of%20SAM%20when%20being%20applied%20for%0Ainteractive%20segmentation%20on%20novel%20domains%20or%20object%20types.%20On%20the%20used%0Adatasets%2C%20SAM%20displays%20a%20failure%20rate%20%24%5Ctext%7BFR%7D_%7B30%7D%4090%24%20of%20up%20to%20%2472.6%20%5C%25%24.%0ASince%20we%20still%20want%20such%20foundation%20models%20to%20be%20immediately%20applicable%2C%20we%0Apresent%20a%20framework%20that%20can%20adapt%20SAM%20during%20immediate%20usage.%20For%20this%20we%20will%0Aleverage%20the%20user%20interactions%20and%20masks%2C%20which%20are%20constructed%20during%20the%0Ainteractive%20segmentation%20process.%20We%20use%20this%20information%20to%20generate%0Apseudo-labels%2C%20which%20we%20use%20to%20compute%20a%20loss%20function%20and%20optimize%20a%20part%20of%0Athe%20SAM%20model.%20The%20presented%20method%20causes%20a%20relative%20reduction%20of%20up%20to%20%2448.1%0A%5C%25%24%20in%20the%20%24%5Ctext%7BFR%7D_%7B20%7D%4085%24%20and%20%2446.6%20%5C%25%24%20in%20the%20%24%5Ctext%7BFR%7D_%7B30%7D%4090%24%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08421v1&entry.124074799=Read"},
{"title": "FashionFail: Addressing Failure Cases in Fashion Object Detection and\n  Segmentation", "author": "Riza Velioglu and Robin Chan and Barbara Hammer", "abstract": "  In the realm of fashion object detection and segmentation for online shopping\nimages, existing state-of-the-art fashion parsing models encounter limitations,\nparticularly when exposed to non-model-worn apparel and close-up shots. To\naddress these failures, we introduce FashionFail; a new fashion dataset with\ne-commerce images for object detection and segmentation. The dataset is\nefficiently curated using our novel annotation tool that leverages recent\nfoundation models. The primary objective of FashionFail is to serve as a test\nbed for evaluating the robustness of models. Our analysis reveals the\nshortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer.\nAdditionally, we propose a baseline approach using naive data augmentation to\nmitigate common failure cases and improve model robustness. Through this work,\nwe aim to inspire and support further research in fashion item detection and\nsegmentation for industrial applications. The dataset, annotation tool, code,\nand models are available at \\url{https://rizavelioglu.github.io/fashionfail/}.\n", "link": "http://arxiv.org/abs/2404.08582v1", "date": "2024-04-12", "relevancy": 2.0225, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4776}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4725}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FashionFail%3A%20Addressing%20Failure%20Cases%20in%20Fashion%20Object%20Detection%20and%0A%20%20Segmentation&body=Title%3A%20FashionFail%3A%20Addressing%20Failure%20Cases%20in%20Fashion%20Object%20Detection%20and%0A%20%20Segmentation%0AAuthor%3A%20Riza%20Velioglu%20and%20Robin%20Chan%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20In%20the%20realm%20of%20fashion%20object%20detection%20and%20segmentation%20for%20online%20shopping%0Aimages%2C%20existing%20state-of-the-art%20fashion%20parsing%20models%20encounter%20limitations%2C%0Aparticularly%20when%20exposed%20to%20non-model-worn%20apparel%20and%20close-up%20shots.%20To%0Aaddress%20these%20failures%2C%20we%20introduce%20FashionFail%3B%20a%20new%20fashion%20dataset%20with%0Ae-commerce%20images%20for%20object%20detection%20and%20segmentation.%20The%20dataset%20is%0Aefficiently%20curated%20using%20our%20novel%20annotation%20tool%20that%20leverages%20recent%0Afoundation%20models.%20The%20primary%20objective%20of%20FashionFail%20is%20to%20serve%20as%20a%20test%0Abed%20for%20evaluating%20the%20robustness%20of%20models.%20Our%20analysis%20reveals%20the%0Ashortcomings%20of%20leading%20models%2C%20such%20as%20Attribute-Mask%20R-CNN%20and%20Fashionformer.%0AAdditionally%2C%20we%20propose%20a%20baseline%20approach%20using%20naive%20data%20augmentation%20to%0Amitigate%20common%20failure%20cases%20and%20improve%20model%20robustness.%20Through%20this%20work%2C%0Awe%20aim%20to%20inspire%20and%20support%20further%20research%20in%20fashion%20item%20detection%20and%0Asegmentation%20for%20industrial%20applications.%20The%20dataset%2C%20annotation%20tool%2C%20code%2C%0Aand%20models%20are%20available%20at%20%5Curl%7Bhttps%3A//rizavelioglu.github.io/fashionfail/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08582v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionFail%3A%20Addressing%20Failure%20Cases%20in%20Fashion%20Object%20Detection%20and%0A%20%20Segmentation&entry.906535625=Riza%20Velioglu%20and%20Robin%20Chan%20and%20Barbara%20Hammer&entry.1292438233=%20%20In%20the%20realm%20of%20fashion%20object%20detection%20and%20segmentation%20for%20online%20shopping%0Aimages%2C%20existing%20state-of-the-art%20fashion%20parsing%20models%20encounter%20limitations%2C%0Aparticularly%20when%20exposed%20to%20non-model-worn%20apparel%20and%20close-up%20shots.%20To%0Aaddress%20these%20failures%2C%20we%20introduce%20FashionFail%3B%20a%20new%20fashion%20dataset%20with%0Ae-commerce%20images%20for%20object%20detection%20and%20segmentation.%20The%20dataset%20is%0Aefficiently%20curated%20using%20our%20novel%20annotation%20tool%20that%20leverages%20recent%0Afoundation%20models.%20The%20primary%20objective%20of%20FashionFail%20is%20to%20serve%20as%20a%20test%0Abed%20for%20evaluating%20the%20robustness%20of%20models.%20Our%20analysis%20reveals%20the%0Ashortcomings%20of%20leading%20models%2C%20such%20as%20Attribute-Mask%20R-CNN%20and%20Fashionformer.%0AAdditionally%2C%20we%20propose%20a%20baseline%20approach%20using%20naive%20data%20augmentation%20to%0Amitigate%20common%20failure%20cases%20and%20improve%20model%20robustness.%20Through%20this%20work%2C%0Awe%20aim%20to%20inspire%20and%20support%20further%20research%20in%20fashion%20item%20detection%20and%0Asegmentation%20for%20industrial%20applications.%20The%20dataset%2C%20annotation%20tool%2C%20code%2C%0Aand%20models%20are%20available%20at%20%5Curl%7Bhttps%3A//rizavelioglu.github.io/fashionfail/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08582v1&entry.124074799=Read"},
{"title": "Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature\n  Extraction and Personalized Integration Framework", "author": "Dongbo Xi and Zhen Chen and Yuexian Wang and He Cui and Chong Peng and Fuzhen Zhuang and Peng Yan", "abstract": "  Feed recommendation is currently the mainstream mode for many real-world\napplications (e.g., TikTok, Dianping), it is usually necessary to model and\npredict user interests in multiple scenarios (domains) within and even outside\nthe application. Multi-domain learning is a typical solution in this regard.\nWhile considerable efforts have been made in this regard, there are still two\nlong-standing challenges: (1) Accurately depicting the differences among\ndomains using domain features is crucial for enhancing the performance of each\ndomain. However, manually designing domain features and models for numerous\ndomains can be a laborious task. (2) Users typically have limited impressions\nin only a few domains. Extracting features automatically from other domains and\nleveraging them to improve the predictive capabilities of each domain has\nconsistently posed a challenging problem. In this paper, we propose an\nAutomatic Domain Feature Extraction and Personalized Integration (DFEI)\nframework for the large-scale multi-domain recommendation. The framework\nautomatically transforms the behavior of each individual user into an\naggregation of all user behaviors within the domain, which serves as the domain\nfeatures. Unlike offline feature engineering methods, the extracted domain\nfeatures are higher-order representations and directly related to the target\nlabel. Besides, by personalized integration of domain features from other\ndomains for each user and the innovation in the training mode, the DFEI\nframework can yield more accurate conversion identification. Experimental\nresults on both public and industrial datasets, consisting of over 20 domains,\nclearly demonstrate that the proposed framework achieves significantly better\nperformance compared with SOTA baselines. Furthermore, we have released the\nsource code of the proposed framework at https://github.com/xidongbo/DFEI.\n", "link": "http://arxiv.org/abs/2404.08361v1", "date": "2024-04-12", "relevancy": 2.0163, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5163}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4837}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Multi-Domain%20Recommendation%3A%20an%20Automatic%20Domain%20Feature%0A%20%20Extraction%20and%20Personalized%20Integration%20Framework&body=Title%3A%20Large-Scale%20Multi-Domain%20Recommendation%3A%20an%20Automatic%20Domain%20Feature%0A%20%20Extraction%20and%20Personalized%20Integration%20Framework%0AAuthor%3A%20Dongbo%20Xi%20and%20Zhen%20Chen%20and%20Yuexian%20Wang%20and%20He%20Cui%20and%20Chong%20Peng%20and%20Fuzhen%20Zhuang%20and%20Peng%20Yan%0AAbstract%3A%20%20%20Feed%20recommendation%20is%20currently%20the%20mainstream%20mode%20for%20many%20real-world%0Aapplications%20%28e.g.%2C%20TikTok%2C%20Dianping%29%2C%20it%20is%20usually%20necessary%20to%20model%20and%0Apredict%20user%20interests%20in%20multiple%20scenarios%20%28domains%29%20within%20and%20even%20outside%0Athe%20application.%20Multi-domain%20learning%20is%20a%20typical%20solution%20in%20this%20regard.%0AWhile%20considerable%20efforts%20have%20been%20made%20in%20this%20regard%2C%20there%20are%20still%20two%0Along-standing%20challenges%3A%20%281%29%20Accurately%20depicting%20the%20differences%20among%0Adomains%20using%20domain%20features%20is%20crucial%20for%20enhancing%20the%20performance%20of%20each%0Adomain.%20However%2C%20manually%20designing%20domain%20features%20and%20models%20for%20numerous%0Adomains%20can%20be%20a%20laborious%20task.%20%282%29%20Users%20typically%20have%20limited%20impressions%0Ain%20only%20a%20few%20domains.%20Extracting%20features%20automatically%20from%20other%20domains%20and%0Aleveraging%20them%20to%20improve%20the%20predictive%20capabilities%20of%20each%20domain%20has%0Aconsistently%20posed%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20an%0AAutomatic%20Domain%20Feature%20Extraction%20and%20Personalized%20Integration%20%28DFEI%29%0Aframework%20for%20the%20large-scale%20multi-domain%20recommendation.%20The%20framework%0Aautomatically%20transforms%20the%20behavior%20of%20each%20individual%20user%20into%20an%0Aaggregation%20of%20all%20user%20behaviors%20within%20the%20domain%2C%20which%20serves%20as%20the%20domain%0Afeatures.%20Unlike%20offline%20feature%20engineering%20methods%2C%20the%20extracted%20domain%0Afeatures%20are%20higher-order%20representations%20and%20directly%20related%20to%20the%20target%0Alabel.%20Besides%2C%20by%20personalized%20integration%20of%20domain%20features%20from%20other%0Adomains%20for%20each%20user%20and%20the%20innovation%20in%20the%20training%20mode%2C%20the%20DFEI%0Aframework%20can%20yield%20more%20accurate%20conversion%20identification.%20Experimental%0Aresults%20on%20both%20public%20and%20industrial%20datasets%2C%20consisting%20of%20over%2020%20domains%2C%0Aclearly%20demonstrate%20that%20the%20proposed%20framework%20achieves%20significantly%20better%0Aperformance%20compared%20with%20SOTA%20baselines.%20Furthermore%2C%20we%20have%20released%20the%0Asource%20code%20of%20the%20proposed%20framework%20at%20https%3A//github.com/xidongbo/DFEI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08361v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Multi-Domain%20Recommendation%3A%20an%20Automatic%20Domain%20Feature%0A%20%20Extraction%20and%20Personalized%20Integration%20Framework&entry.906535625=Dongbo%20Xi%20and%20Zhen%20Chen%20and%20Yuexian%20Wang%20and%20He%20Cui%20and%20Chong%20Peng%20and%20Fuzhen%20Zhuang%20and%20Peng%20Yan&entry.1292438233=%20%20Feed%20recommendation%20is%20currently%20the%20mainstream%20mode%20for%20many%20real-world%0Aapplications%20%28e.g.%2C%20TikTok%2C%20Dianping%29%2C%20it%20is%20usually%20necessary%20to%20model%20and%0Apredict%20user%20interests%20in%20multiple%20scenarios%20%28domains%29%20within%20and%20even%20outside%0Athe%20application.%20Multi-domain%20learning%20is%20a%20typical%20solution%20in%20this%20regard.%0AWhile%20considerable%20efforts%20have%20been%20made%20in%20this%20regard%2C%20there%20are%20still%20two%0Along-standing%20challenges%3A%20%281%29%20Accurately%20depicting%20the%20differences%20among%0Adomains%20using%20domain%20features%20is%20crucial%20for%20enhancing%20the%20performance%20of%20each%0Adomain.%20However%2C%20manually%20designing%20domain%20features%20and%20models%20for%20numerous%0Adomains%20can%20be%20a%20laborious%20task.%20%282%29%20Users%20typically%20have%20limited%20impressions%0Ain%20only%20a%20few%20domains.%20Extracting%20features%20automatically%20from%20other%20domains%20and%0Aleveraging%20them%20to%20improve%20the%20predictive%20capabilities%20of%20each%20domain%20has%0Aconsistently%20posed%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%20propose%20an%0AAutomatic%20Domain%20Feature%20Extraction%20and%20Personalized%20Integration%20%28DFEI%29%0Aframework%20for%20the%20large-scale%20multi-domain%20recommendation.%20The%20framework%0Aautomatically%20transforms%20the%20behavior%20of%20each%20individual%20user%20into%20an%0Aaggregation%20of%20all%20user%20behaviors%20within%20the%20domain%2C%20which%20serves%20as%20the%20domain%0Afeatures.%20Unlike%20offline%20feature%20engineering%20methods%2C%20the%20extracted%20domain%0Afeatures%20are%20higher-order%20representations%20and%20directly%20related%20to%20the%20target%0Alabel.%20Besides%2C%20by%20personalized%20integration%20of%20domain%20features%20from%20other%0Adomains%20for%20each%20user%20and%20the%20innovation%20in%20the%20training%20mode%2C%20the%20DFEI%0Aframework%20can%20yield%20more%20accurate%20conversion%20identification.%20Experimental%0Aresults%20on%20both%20public%20and%20industrial%20datasets%2C%20consisting%20of%20over%2020%20domains%2C%0Aclearly%20demonstrate%20that%20the%20proposed%20framework%20achieves%20significantly%20better%0Aperformance%20compared%20with%20SOTA%20baselines.%20Furthermore%2C%20we%20have%20released%20the%0Asource%20code%20of%20the%20proposed%20framework%20at%20https%3A//github.com/xidongbo/DFEI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08361v1&entry.124074799=Read"},
{"title": "Benchmarking the Cell Image Segmentation Models Robustness under the\n  Microscope Optical Aberrations", "author": "Boyuan Peng and Jiaju Chen and Qihui Ye and Minjiang Chen and Peiwu Qin and Chenggang Yan and Dongmei Yu and Zhenglin Chen", "abstract": "  Cell segmentation is essential in biomedical research for analyzing cellular\nmorphology and behavior. Deep learning methods, particularly convolutional\nneural networks (CNNs), have revolutionized cell segmentation by extracting\nintricate features from images. However, the robustness of these methods under\nmicroscope optical aberrations remains a critical challenge. This study\ncomprehensively evaluates the performance of cell instance segmentation models\nunder simulated aberration conditions using the DynamicNuclearNet (DNN) and\nLIVECell datasets. Aberrations, including Astigmatism, Coma, Spherical, and\nTrefoil, were simulated using Zernike polynomial equations. Various\nsegmentation models, such as Mask R-CNN with different network heads (FPN, C3)\nand backbones (ResNet, VGG19, SwinS), were trained and tested under aberrated\nconditions. Results indicate that FPN combined with SwinS demonstrates superior\nrobustness in handling simple cell images affected by minor aberrations.\nConversely, Cellpose2.0 proves effective for complex cell images under similar\nconditions. Our findings provide insights into selecting appropriate\nsegmentation models based on cell morphology and aberration severity, enhancing\nthe reliability of cell segmentation in biomedical applications. Further\nresearch is warranted to validate these methods with diverse aberration types\nand emerging segmentation models. Overall, this research aims to guide\nresearchers in effectively utilizing cell segmentation models in the presence\nof minor optical aberrations.\n", "link": "http://arxiv.org/abs/2404.08549v1", "date": "2024-04-12", "relevancy": 2.0153, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Cell%20Image%20Segmentation%20Models%20Robustness%20under%20the%0A%20%20Microscope%20Optical%20Aberrations&body=Title%3A%20Benchmarking%20the%20Cell%20Image%20Segmentation%20Models%20Robustness%20under%20the%0A%20%20Microscope%20Optical%20Aberrations%0AAuthor%3A%20Boyuan%20Peng%20and%20Jiaju%20Chen%20and%20Qihui%20Ye%20and%20Minjiang%20Chen%20and%20Peiwu%20Qin%20and%20Chenggang%20Yan%20and%20Dongmei%20Yu%20and%20Zhenglin%20Chen%0AAbstract%3A%20%20%20Cell%20segmentation%20is%20essential%20in%20biomedical%20research%20for%20analyzing%20cellular%0Amorphology%20and%20behavior.%20Deep%20learning%20methods%2C%20particularly%20convolutional%0Aneural%20networks%20%28CNNs%29%2C%20have%20revolutionized%20cell%20segmentation%20by%20extracting%0Aintricate%20features%20from%20images.%20However%2C%20the%20robustness%20of%20these%20methods%20under%0Amicroscope%20optical%20aberrations%20remains%20a%20critical%20challenge.%20This%20study%0Acomprehensively%20evaluates%20the%20performance%20of%20cell%20instance%20segmentation%20models%0Aunder%20simulated%20aberration%20conditions%20using%20the%20DynamicNuclearNet%20%28DNN%29%20and%0ALIVECell%20datasets.%20Aberrations%2C%20including%20Astigmatism%2C%20Coma%2C%20Spherical%2C%20and%0ATrefoil%2C%20were%20simulated%20using%20Zernike%20polynomial%20equations.%20Various%0Asegmentation%20models%2C%20such%20as%20Mask%20R-CNN%20with%20different%20network%20heads%20%28FPN%2C%20C3%29%0Aand%20backbones%20%28ResNet%2C%20VGG19%2C%20SwinS%29%2C%20were%20trained%20and%20tested%20under%20aberrated%0Aconditions.%20Results%20indicate%20that%20FPN%20combined%20with%20SwinS%20demonstrates%20superior%0Arobustness%20in%20handling%20simple%20cell%20images%20affected%20by%20minor%20aberrations.%0AConversely%2C%20Cellpose2.0%20proves%20effective%20for%20complex%20cell%20images%20under%20similar%0Aconditions.%20Our%20findings%20provide%20insights%20into%20selecting%20appropriate%0Asegmentation%20models%20based%20on%20cell%20morphology%20and%20aberration%20severity%2C%20enhancing%0Athe%20reliability%20of%20cell%20segmentation%20in%20biomedical%20applications.%20Further%0Aresearch%20is%20warranted%20to%20validate%20these%20methods%20with%20diverse%20aberration%20types%0Aand%20emerging%20segmentation%20models.%20Overall%2C%20this%20research%20aims%20to%20guide%0Aresearchers%20in%20effectively%20utilizing%20cell%20segmentation%20models%20in%20the%20presence%0Aof%20minor%20optical%20aberrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08549v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Cell%20Image%20Segmentation%20Models%20Robustness%20under%20the%0A%20%20Microscope%20Optical%20Aberrations&entry.906535625=Boyuan%20Peng%20and%20Jiaju%20Chen%20and%20Qihui%20Ye%20and%20Minjiang%20Chen%20and%20Peiwu%20Qin%20and%20Chenggang%20Yan%20and%20Dongmei%20Yu%20and%20Zhenglin%20Chen&entry.1292438233=%20%20Cell%20segmentation%20is%20essential%20in%20biomedical%20research%20for%20analyzing%20cellular%0Amorphology%20and%20behavior.%20Deep%20learning%20methods%2C%20particularly%20convolutional%0Aneural%20networks%20%28CNNs%29%2C%20have%20revolutionized%20cell%20segmentation%20by%20extracting%0Aintricate%20features%20from%20images.%20However%2C%20the%20robustness%20of%20these%20methods%20under%0Amicroscope%20optical%20aberrations%20remains%20a%20critical%20challenge.%20This%20study%0Acomprehensively%20evaluates%20the%20performance%20of%20cell%20instance%20segmentation%20models%0Aunder%20simulated%20aberration%20conditions%20using%20the%20DynamicNuclearNet%20%28DNN%29%20and%0ALIVECell%20datasets.%20Aberrations%2C%20including%20Astigmatism%2C%20Coma%2C%20Spherical%2C%20and%0ATrefoil%2C%20were%20simulated%20using%20Zernike%20polynomial%20equations.%20Various%0Asegmentation%20models%2C%20such%20as%20Mask%20R-CNN%20with%20different%20network%20heads%20%28FPN%2C%20C3%29%0Aand%20backbones%20%28ResNet%2C%20VGG19%2C%20SwinS%29%2C%20were%20trained%20and%20tested%20under%20aberrated%0Aconditions.%20Results%20indicate%20that%20FPN%20combined%20with%20SwinS%20demonstrates%20superior%0Arobustness%20in%20handling%20simple%20cell%20images%20affected%20by%20minor%20aberrations.%0AConversely%2C%20Cellpose2.0%20proves%20effective%20for%20complex%20cell%20images%20under%20similar%0Aconditions.%20Our%20findings%20provide%20insights%20into%20selecting%20appropriate%0Asegmentation%20models%20based%20on%20cell%20morphology%20and%20aberration%20severity%2C%20enhancing%0Athe%20reliability%20of%20cell%20segmentation%20in%20biomedical%20applications.%20Further%0Aresearch%20is%20warranted%20to%20validate%20these%20methods%20with%20diverse%20aberration%20types%0Aand%20emerging%20segmentation%20models.%20Overall%2C%20this%20research%20aims%20to%20guide%0Aresearchers%20in%20effectively%20utilizing%20cell%20segmentation%20models%20in%20the%20presence%0Aof%20minor%20optical%20aberrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08549v1&entry.124074799=Read"},
{"title": "Generating Synthetic Time Series Data for Cyber-Physical Systems", "author": "Alexander Sommers and Somayeh Bakhtiari Ramezani and Logan Cummins and Sudip Mittal and Shahram Rahimi and Maria Seale and Joseph Jaboure", "abstract": "  Data augmentation is an important facilitator of deep learning applications\nin the time series domain. A gap is identified in the literature, demonstrating\nsparse exploration of the transformer, the dominant sequence model, for data\naugmentation in time series. A architecture hybridizing several successful\npriors is put forth and tested using a powerful time domain similarity metric.\nResults suggest the challenge of this domain, and several valuable directions\nfor future work.\n", "link": "http://arxiv.org/abs/2404.08601v1", "date": "2024-04-12", "relevancy": 2.0073, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5378}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4973}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generating%20Synthetic%20Time%20Series%20Data%20for%20Cyber-Physical%20Systems&body=Title%3A%20Generating%20Synthetic%20Time%20Series%20Data%20for%20Cyber-Physical%20Systems%0AAuthor%3A%20Alexander%20Sommers%20and%20Somayeh%20Bakhtiari%20Ramezani%20and%20Logan%20Cummins%20and%20Sudip%20Mittal%20and%20Shahram%20Rahimi%20and%20Maria%20Seale%20and%20Joseph%20Jaboure%0AAbstract%3A%20%20%20Data%20augmentation%20is%20an%20important%20facilitator%20of%20deep%20learning%20applications%0Ain%20the%20time%20series%20domain.%20A%20gap%20is%20identified%20in%20the%20literature%2C%20demonstrating%0Asparse%20exploration%20of%20the%20transformer%2C%20the%20dominant%20sequence%20model%2C%20for%20data%0Aaugmentation%20in%20time%20series.%20A%20architecture%20hybridizing%20several%20successful%0Apriors%20is%20put%20forth%20and%20tested%20using%20a%20powerful%20time%20domain%20similarity%20metric.%0AResults%20suggest%20the%20challenge%20of%20this%20domain%2C%20and%20several%20valuable%20directions%0Afor%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08601v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Synthetic%20Time%20Series%20Data%20for%20Cyber-Physical%20Systems&entry.906535625=Alexander%20Sommers%20and%20Somayeh%20Bakhtiari%20Ramezani%20and%20Logan%20Cummins%20and%20Sudip%20Mittal%20and%20Shahram%20Rahimi%20and%20Maria%20Seale%20and%20Joseph%20Jaboure&entry.1292438233=%20%20Data%20augmentation%20is%20an%20important%20facilitator%20of%20deep%20learning%20applications%0Ain%20the%20time%20series%20domain.%20A%20gap%20is%20identified%20in%20the%20literature%2C%20demonstrating%0Asparse%20exploration%20of%20the%20transformer%2C%20the%20dominant%20sequence%20model%2C%20for%20data%0Aaugmentation%20in%20time%20series.%20A%20architecture%20hybridizing%20several%20successful%0Apriors%20is%20put%20forth%20and%20tested%20using%20a%20powerful%20time%20domain%20similarity%20metric.%0AResults%20suggest%20the%20challenge%20of%20this%20domain%2C%20and%20several%20valuable%20directions%0Afor%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08601v1&entry.124074799=Read"},
{"title": "LaSagnA: Language-based Segmentation Assistant for Complex Queries", "author": "Cong Wei and Haoxian Tan and Yujie Zhong and Yujiu Yang and Lin Ma", "abstract": "  Recent advancements have empowered Large Language Models for Vision (vLLMs)\nto generate detailed perceptual outcomes, including bounding boxes and masks.\nNonetheless, there are two constraints that restrict the further application of\nthese vLLMs: the incapability of handling multiple targets per query and the\nfailure to identify the absence of query objects in the image. In this study,\nwe acknowledge that the main cause of these problems is the insufficient\ncomplexity of training queries. Consequently, we define the general sequence\nformat for complex queries. Then we incorporate a semantic segmentation task in\nthe current pipeline to fulfill the requirements of training data. Furthermore,\nwe present three novel strategies to effectively handle the challenges arising\nfrom the direct integration of the proposed format. The effectiveness of our\nmodel in processing complex queries is validated by the comparable results with\nconventional methods on both close-set and open-set semantic segmentation\ndatasets. Additionally, we outperform a series of vLLMs in reasoning and\nreferring segmentation, showcasing our model's remarkable capabilities. We\nrelease the code at https://github.com/congvvc/LaSagnA.\n", "link": "http://arxiv.org/abs/2404.08506v1", "date": "2024-04-12", "relevancy": 2.0061, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaSagnA%3A%20Language-based%20Segmentation%20Assistant%20for%20Complex%20Queries&body=Title%3A%20LaSagnA%3A%20Language-based%20Segmentation%20Assistant%20for%20Complex%20Queries%0AAuthor%3A%20Cong%20Wei%20and%20Haoxian%20Tan%20and%20Yujie%20Zhong%20and%20Yujiu%20Yang%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Recent%20advancements%20have%20empowered%20Large%20Language%20Models%20for%20Vision%20%28vLLMs%29%0Ato%20generate%20detailed%20perceptual%20outcomes%2C%20including%20bounding%20boxes%20and%20masks.%0ANonetheless%2C%20there%20are%20two%20constraints%20that%20restrict%20the%20further%20application%20of%0Athese%20vLLMs%3A%20the%20incapability%20of%20handling%20multiple%20targets%20per%20query%20and%20the%0Afailure%20to%20identify%20the%20absence%20of%20query%20objects%20in%20the%20image.%20In%20this%20study%2C%0Awe%20acknowledge%20that%20the%20main%20cause%20of%20these%20problems%20is%20the%20insufficient%0Acomplexity%20of%20training%20queries.%20Consequently%2C%20we%20define%20the%20general%20sequence%0Aformat%20for%20complex%20queries.%20Then%20we%20incorporate%20a%20semantic%20segmentation%20task%20in%0Athe%20current%20pipeline%20to%20fulfill%20the%20requirements%20of%20training%20data.%20Furthermore%2C%0Awe%20present%20three%20novel%20strategies%20to%20effectively%20handle%20the%20challenges%20arising%0Afrom%20the%20direct%20integration%20of%20the%20proposed%20format.%20The%20effectiveness%20of%20our%0Amodel%20in%20processing%20complex%20queries%20is%20validated%20by%20the%20comparable%20results%20with%0Aconventional%20methods%20on%20both%20close-set%20and%20open-set%20semantic%20segmentation%0Adatasets.%20Additionally%2C%20we%20outperform%20a%20series%20of%20vLLMs%20in%20reasoning%20and%0Areferring%20segmentation%2C%20showcasing%20our%20model%27s%20remarkable%20capabilities.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/congvvc/LaSagnA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08506v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaSagnA%3A%20Language-based%20Segmentation%20Assistant%20for%20Complex%20Queries&entry.906535625=Cong%20Wei%20and%20Haoxian%20Tan%20and%20Yujie%20Zhong%20and%20Yujiu%20Yang%20and%20Lin%20Ma&entry.1292438233=%20%20Recent%20advancements%20have%20empowered%20Large%20Language%20Models%20for%20Vision%20%28vLLMs%29%0Ato%20generate%20detailed%20perceptual%20outcomes%2C%20including%20bounding%20boxes%20and%20masks.%0ANonetheless%2C%20there%20are%20two%20constraints%20that%20restrict%20the%20further%20application%20of%0Athese%20vLLMs%3A%20the%20incapability%20of%20handling%20multiple%20targets%20per%20query%20and%20the%0Afailure%20to%20identify%20the%20absence%20of%20query%20objects%20in%20the%20image.%20In%20this%20study%2C%0Awe%20acknowledge%20that%20the%20main%20cause%20of%20these%20problems%20is%20the%20insufficient%0Acomplexity%20of%20training%20queries.%20Consequently%2C%20we%20define%20the%20general%20sequence%0Aformat%20for%20complex%20queries.%20Then%20we%20incorporate%20a%20semantic%20segmentation%20task%20in%0Athe%20current%20pipeline%20to%20fulfill%20the%20requirements%20of%20training%20data.%20Furthermore%2C%0Awe%20present%20three%20novel%20strategies%20to%20effectively%20handle%20the%20challenges%20arising%0Afrom%20the%20direct%20integration%20of%20the%20proposed%20format.%20The%20effectiveness%20of%20our%0Amodel%20in%20processing%20complex%20queries%20is%20validated%20by%20the%20comparable%20results%20with%0Aconventional%20methods%20on%20both%20close-set%20and%20open-set%20semantic%20segmentation%0Adatasets.%20Additionally%2C%20we%20outperform%20a%20series%20of%20vLLMs%20in%20reasoning%20and%0Areferring%20segmentation%2C%20showcasing%20our%20model%27s%20remarkable%20capabilities.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/congvvc/LaSagnA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08506v1&entry.124074799=Read"},
{"title": "On the Independence Assumption in Neurosymbolic Learning", "author": "Emile van Krieken and Pasquale Minervini and Edoardo M. Ponti and Antonio Vergari", "abstract": "  State-of-the-art neurosymbolic learning systems use probabilistic reasoning\nto guide neural networks towards predictions that conform to logical\nconstraints over symbols. Many such systems assume that the probabilities of\nthe considered symbols are conditionally independent given the input to\nsimplify learning and reasoning. We study and criticise this assumption,\nhighlighting how it can hinder optimisation and prevent uncertainty\nquantification. We prove that loss functions bias conditionally independent\nneural networks to become overconfident in their predictions. As a result, they\nare unable to represent uncertainty over multiple valid options. Furthermore,\nwe prove that these loss functions are difficult to optimise: they are\nnon-convex, and their minima are usually highly disconnected. Our theoretical\nanalysis gives the foundation for replacing the conditional independence\nassumption and designing more expressive neurosymbolic probabilistic models.\n", "link": "http://arxiv.org/abs/2404.08458v1", "date": "2024-04-12", "relevancy": 1.9906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5243}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4566}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning&body=Title%3A%20On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning%0AAuthor%3A%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini%20and%20Edoardo%20M.%20Ponti%20and%20Antonio%20Vergari%0AAbstract%3A%20%20%20State-of-the-art%20neurosymbolic%20learning%20systems%20use%20probabilistic%20reasoning%0Ato%20guide%20neural%20networks%20towards%20predictions%20that%20conform%20to%20logical%0Aconstraints%20over%20symbols.%20Many%20such%20systems%20assume%20that%20the%20probabilities%20of%0Athe%20considered%20symbols%20are%20conditionally%20independent%20given%20the%20input%20to%0Asimplify%20learning%20and%20reasoning.%20We%20study%20and%20criticise%20this%20assumption%2C%0Ahighlighting%20how%20it%20can%20hinder%20optimisation%20and%20prevent%20uncertainty%0Aquantification.%20We%20prove%20that%20loss%20functions%20bias%20conditionally%20independent%0Aneural%20networks%20to%20become%20overconfident%20in%20their%20predictions.%20As%20a%20result%2C%20they%0Aare%20unable%20to%20represent%20uncertainty%20over%20multiple%20valid%20options.%20Furthermore%2C%0Awe%20prove%20that%20these%20loss%20functions%20are%20difficult%20to%20optimise%3A%20they%20are%0Anon-convex%2C%20and%20their%20minima%20are%20usually%20highly%20disconnected.%20Our%20theoretical%0Aanalysis%20gives%20the%20foundation%20for%20replacing%20the%20conditional%20independence%0Aassumption%20and%20designing%20more%20expressive%20neurosymbolic%20probabilistic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08458v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning&entry.906535625=Emile%20van%20Krieken%20and%20Pasquale%20Minervini%20and%20Edoardo%20M.%20Ponti%20and%20Antonio%20Vergari&entry.1292438233=%20%20State-of-the-art%20neurosymbolic%20learning%20systems%20use%20probabilistic%20reasoning%0Ato%20guide%20neural%20networks%20towards%20predictions%20that%20conform%20to%20logical%0Aconstraints%20over%20symbols.%20Many%20such%20systems%20assume%20that%20the%20probabilities%20of%0Athe%20considered%20symbols%20are%20conditionally%20independent%20given%20the%20input%20to%0Asimplify%20learning%20and%20reasoning.%20We%20study%20and%20criticise%20this%20assumption%2C%0Ahighlighting%20how%20it%20can%20hinder%20optimisation%20and%20prevent%20uncertainty%0Aquantification.%20We%20prove%20that%20loss%20functions%20bias%20conditionally%20independent%0Aneural%20networks%20to%20become%20overconfident%20in%20their%20predictions.%20As%20a%20result%2C%20they%0Aare%20unable%20to%20represent%20uncertainty%20over%20multiple%20valid%20options.%20Furthermore%2C%0Awe%20prove%20that%20these%20loss%20functions%20are%20difficult%20to%20optimise%3A%20they%20are%0Anon-convex%2C%20and%20their%20minima%20are%20usually%20highly%20disconnected.%20Our%20theoretical%0Aanalysis%20gives%20the%20foundation%20for%20replacing%20the%20conditional%20independence%0Aassumption%20and%20designing%20more%20expressive%20neurosymbolic%20probabilistic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08458v1&entry.124074799=Read"},
{"title": "A novel Fourier neural operator framework for classification of\n  multi-sized images: Application to three dimensional digital porous media", "author": "Ali Kashefi and Tapan Mukerji", "abstract": "  Fourier neural operators (FNOs) are invariant with respect to the size of\ninput images, and thus images with any size can be fed into FNO-based\nframeworks without any modification of network architectures, in contrast to\ntraditional convolutional neural networks (CNNs). Leveraging the advantage of\nFNOs, we propose a novel deep-learning framework for classifying images with\nvarying sizes. Particularly, we simultaneously train the proposed network on\nmulti-sized images. As a practical application, we consider the problem of\npredicting the label (e.g., permeability) of three-dimensional digital porous\nmedia. To construct the framework, an intuitive approach is to connect FNO\nlayers to a classifier using adaptive max pooling. First, we show that this\napproach is only effective for porous media with fixed sizes, whereas it fails\nfor porous media of varying sizes. To overcome this limitation, we introduce\nour approach: instead of using adaptive max pooling, we use static max pooling\nwith the size of channel width of FNO layers. Since the channel width of the\nFNO layers is independent of input image size, the introduced framework can\nhandle multi-sized images during training. We show the effectiveness of the\nintroduced framework and compare its performance with the intuitive approach\nthrough the example of the classification of three-dimensional digital porous\nmedia of varying sizes.\n", "link": "http://arxiv.org/abs/2402.11568v2", "date": "2024-04-12", "relevancy": 1.9843, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5091}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4924}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20novel%20Fourier%20neural%20operator%20framework%20for%20classification%20of%0A%20%20multi-sized%20images%3A%20Application%20to%20three%20dimensional%20digital%20porous%20media&body=Title%3A%20A%20novel%20Fourier%20neural%20operator%20framework%20for%20classification%20of%0A%20%20multi-sized%20images%3A%20Application%20to%20three%20dimensional%20digital%20porous%20media%0AAuthor%3A%20Ali%20Kashefi%20and%20Tapan%20Mukerji%0AAbstract%3A%20%20%20Fourier%20neural%20operators%20%28FNOs%29%20are%20invariant%20with%20respect%20to%20the%20size%20of%0Ainput%20images%2C%20and%20thus%20images%20with%20any%20size%20can%20be%20fed%20into%20FNO-based%0Aframeworks%20without%20any%20modification%20of%20network%20architectures%2C%20in%20contrast%20to%0Atraditional%20convolutional%20neural%20networks%20%28CNNs%29.%20Leveraging%20the%20advantage%20of%0AFNOs%2C%20we%20propose%20a%20novel%20deep-learning%20framework%20for%20classifying%20images%20with%0Avarying%20sizes.%20Particularly%2C%20we%20simultaneously%20train%20the%20proposed%20network%20on%0Amulti-sized%20images.%20As%20a%20practical%20application%2C%20we%20consider%20the%20problem%20of%0Apredicting%20the%20label%20%28e.g.%2C%20permeability%29%20of%20three-dimensional%20digital%20porous%0Amedia.%20To%20construct%20the%20framework%2C%20an%20intuitive%20approach%20is%20to%20connect%20FNO%0Alayers%20to%20a%20classifier%20using%20adaptive%20max%20pooling.%20First%2C%20we%20show%20that%20this%0Aapproach%20is%20only%20effective%20for%20porous%20media%20with%20fixed%20sizes%2C%20whereas%20it%20fails%0Afor%20porous%20media%20of%20varying%20sizes.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0Aour%20approach%3A%20instead%20of%20using%20adaptive%20max%20pooling%2C%20we%20use%20static%20max%20pooling%0Awith%20the%20size%20of%20channel%20width%20of%20FNO%20layers.%20Since%20the%20channel%20width%20of%20the%0AFNO%20layers%20is%20independent%20of%20input%20image%20size%2C%20the%20introduced%20framework%20can%0Ahandle%20multi-sized%20images%20during%20training.%20We%20show%20the%20effectiveness%20of%20the%0Aintroduced%20framework%20and%20compare%20its%20performance%20with%20the%20intuitive%20approach%0Athrough%20the%20example%20of%20the%20classification%20of%20three-dimensional%20digital%20porous%0Amedia%20of%20varying%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11568v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20novel%20Fourier%20neural%20operator%20framework%20for%20classification%20of%0A%20%20multi-sized%20images%3A%20Application%20to%20three%20dimensional%20digital%20porous%20media&entry.906535625=Ali%20Kashefi%20and%20Tapan%20Mukerji&entry.1292438233=%20%20Fourier%20neural%20operators%20%28FNOs%29%20are%20invariant%20with%20respect%20to%20the%20size%20of%0Ainput%20images%2C%20and%20thus%20images%20with%20any%20size%20can%20be%20fed%20into%20FNO-based%0Aframeworks%20without%20any%20modification%20of%20network%20architectures%2C%20in%20contrast%20to%0Atraditional%20convolutional%20neural%20networks%20%28CNNs%29.%20Leveraging%20the%20advantage%20of%0AFNOs%2C%20we%20propose%20a%20novel%20deep-learning%20framework%20for%20classifying%20images%20with%0Avarying%20sizes.%20Particularly%2C%20we%20simultaneously%20train%20the%20proposed%20network%20on%0Amulti-sized%20images.%20As%20a%20practical%20application%2C%20we%20consider%20the%20problem%20of%0Apredicting%20the%20label%20%28e.g.%2C%20permeability%29%20of%20three-dimensional%20digital%20porous%0Amedia.%20To%20construct%20the%20framework%2C%20an%20intuitive%20approach%20is%20to%20connect%20FNO%0Alayers%20to%20a%20classifier%20using%20adaptive%20max%20pooling.%20First%2C%20we%20show%20that%20this%0Aapproach%20is%20only%20effective%20for%20porous%20media%20with%20fixed%20sizes%2C%20whereas%20it%20fails%0Afor%20porous%20media%20of%20varying%20sizes.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0Aour%20approach%3A%20instead%20of%20using%20adaptive%20max%20pooling%2C%20we%20use%20static%20max%20pooling%0Awith%20the%20size%20of%20channel%20width%20of%20FNO%20layers.%20Since%20the%20channel%20width%20of%20the%0AFNO%20layers%20is%20independent%20of%20input%20image%20size%2C%20the%20introduced%20framework%20can%0Ahandle%20multi-sized%20images%20during%20training.%20We%20show%20the%20effectiveness%20of%20the%0Aintroduced%20framework%20and%20compare%20its%20performance%20with%20the%20intuitive%20approach%0Athrough%20the%20example%20of%20the%20classification%20of%20three-dimensional%20digital%20porous%0Amedia%20of%20varying%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11568v2&entry.124074799=Read"},
{"title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher\n  Language Selection and Cross-lingual Self-Distillation", "author": "Haozhe Zhao and Zefan Cai and Shuzheng Si and Liang Chen and Yufeng He and Kaikai An and Baobao Chang", "abstract": "  Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive\nperformance on cross-language tasks, yet significant performance disparities\nexist across different languages within the same mPLM. Previous studies\nendeavored to narrow these disparities by supervise fine-tuning the mPLMs with\nmultilingual data. However, obtaining labeled multilingual data is\ntime-consuming, and fine-tuning mPLM with limited labeled multilingual data\nmerely encapsulates the knowledge specific to the labeled data. Therefore, we\nintroduce ALSACE to leverage the learned knowledge from the well-performing\nlanguages to guide under-performing ones within the same mPLM, eliminating the\nneed for additional labeled multilingual data. Experiments show that ALSACE\neffectively mitigates language-level performance disparity across various mPLMs\nwhile showing the competitive performance on different multilingual NLU tasks,\nranging from full resource to limited resource settings. The code for our\napproach is available at https://github.com/pkunlp-icler/ALSACE.\n", "link": "http://arxiv.org/abs/2404.08491v1", "date": "2024-04-12", "relevancy": 1.9633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4954}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4702}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Language-Level%20Performance%20Disparity%20in%20mPLMs%20via%20Teacher%0A%20%20Language%20Selection%20and%20Cross-lingual%20Self-Distillation&body=Title%3A%20Mitigating%20Language-Level%20Performance%20Disparity%20in%20mPLMs%20via%20Teacher%0A%20%20Language%20Selection%20and%20Cross-lingual%20Self-Distillation%0AAuthor%3A%20Haozhe%20Zhao%20and%20Zefan%20Cai%20and%20Shuzheng%20Si%20and%20Liang%20Chen%20and%20Yufeng%20He%20and%20Kaikai%20An%20and%20Baobao%20Chang%0AAbstract%3A%20%20%20Large-scale%20multilingual%20Pretrained%20Language%20Models%20%28mPLMs%29%20yield%20impressive%0Aperformance%20on%20cross-language%20tasks%2C%20yet%20significant%20performance%20disparities%0Aexist%20across%20different%20languages%20within%20the%20same%20mPLM.%20Previous%20studies%0Aendeavored%20to%20narrow%20these%20disparities%20by%20supervise%20fine-tuning%20the%20mPLMs%20with%0Amultilingual%20data.%20However%2C%20obtaining%20labeled%20multilingual%20data%20is%0Atime-consuming%2C%20and%20fine-tuning%20mPLM%20with%20limited%20labeled%20multilingual%20data%0Amerely%20encapsulates%20the%20knowledge%20specific%20to%20the%20labeled%20data.%20Therefore%2C%20we%0Aintroduce%20ALSACE%20to%20leverage%20the%20learned%20knowledge%20from%20the%20well-performing%0Alanguages%20to%20guide%20under-performing%20ones%20within%20the%20same%20mPLM%2C%20eliminating%20the%0Aneed%20for%20additional%20labeled%20multilingual%20data.%20Experiments%20show%20that%20ALSACE%0Aeffectively%20mitigates%20language-level%20performance%20disparity%20across%20various%20mPLMs%0Awhile%20showing%20the%20competitive%20performance%20on%20different%20multilingual%20NLU%20tasks%2C%0Aranging%20from%20full%20resource%20to%20limited%20resource%20settings.%20The%20code%20for%20our%0Aapproach%20is%20available%20at%20https%3A//github.com/pkunlp-icler/ALSACE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08491v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Language-Level%20Performance%20Disparity%20in%20mPLMs%20via%20Teacher%0A%20%20Language%20Selection%20and%20Cross-lingual%20Self-Distillation&entry.906535625=Haozhe%20Zhao%20and%20Zefan%20Cai%20and%20Shuzheng%20Si%20and%20Liang%20Chen%20and%20Yufeng%20He%20and%20Kaikai%20An%20and%20Baobao%20Chang&entry.1292438233=%20%20Large-scale%20multilingual%20Pretrained%20Language%20Models%20%28mPLMs%29%20yield%20impressive%0Aperformance%20on%20cross-language%20tasks%2C%20yet%20significant%20performance%20disparities%0Aexist%20across%20different%20languages%20within%20the%20same%20mPLM.%20Previous%20studies%0Aendeavored%20to%20narrow%20these%20disparities%20by%20supervise%20fine-tuning%20the%20mPLMs%20with%0Amultilingual%20data.%20However%2C%20obtaining%20labeled%20multilingual%20data%20is%0Atime-consuming%2C%20and%20fine-tuning%20mPLM%20with%20limited%20labeled%20multilingual%20data%0Amerely%20encapsulates%20the%20knowledge%20specific%20to%20the%20labeled%20data.%20Therefore%2C%20we%0Aintroduce%20ALSACE%20to%20leverage%20the%20learned%20knowledge%20from%20the%20well-performing%0Alanguages%20to%20guide%20under-performing%20ones%20within%20the%20same%20mPLM%2C%20eliminating%20the%0Aneed%20for%20additional%20labeled%20multilingual%20data.%20Experiments%20show%20that%20ALSACE%0Aeffectively%20mitigates%20language-level%20performance%20disparity%20across%20various%20mPLMs%0Awhile%20showing%20the%20competitive%20performance%20on%20different%20multilingual%20NLU%20tasks%2C%0Aranging%20from%20full%20resource%20to%20limited%20resource%20settings.%20The%20code%20for%20our%0Aapproach%20is%20available%20at%20https%3A//github.com/pkunlp-icler/ALSACE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08491v1&entry.124074799=Read"},
{"title": "Sliding down the stairs: how correlated latent variables accelerate\n  learning with neural networks", "author": "Lorenzo Bardone and Sebastian Goldt", "abstract": "  Neural networks extract features from data using stochastic gradient descent\n(SGD). In particular, higher-order input cumulants (HOCs) are crucial for their\nperformance. However, extracting information from the $p$th cumulant of\n$d$-dimensional inputs is computationally hard: the number of samples required\nto recover a single direction from an order-$p$ tensor (tensor PCA) using\nonline SGD grows as $d^{p-1}$, which is prohibitive for high-dimensional\ninputs. This result raises the question of how neural networks extract relevant\ndirections from the HOCs of their inputs efficiently. Here, we show that\ncorrelations between latent variables along the directions encoded in different\ninput cumulants speed up learning from higher-order correlations. We show this\neffect analytically by deriving nearly sharp thresholds for the number of\nsamples required by a single neuron to weakly-recover these directions using\nonline SGD from a random start in high dimensions. Our analytical results are\nconfirmed in simulations of two-layer neural networks and unveil a new\nmechanism for hierarchical learning in neural networks.\n", "link": "http://arxiv.org/abs/2404.08602v1", "date": "2024-04-12", "relevancy": 1.9591, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5095}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4823}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sliding%20down%20the%20stairs%3A%20how%20correlated%20latent%20variables%20accelerate%0A%20%20learning%20with%20neural%20networks&body=Title%3A%20Sliding%20down%20the%20stairs%3A%20how%20correlated%20latent%20variables%20accelerate%0A%20%20learning%20with%20neural%20networks%0AAuthor%3A%20Lorenzo%20Bardone%20and%20Sebastian%20Goldt%0AAbstract%3A%20%20%20Neural%20networks%20extract%20features%20from%20data%20using%20stochastic%20gradient%20descent%0A%28SGD%29.%20In%20particular%2C%20higher-order%20input%20cumulants%20%28HOCs%29%20are%20crucial%20for%20their%0Aperformance.%20However%2C%20extracting%20information%20from%20the%20%24p%24th%20cumulant%20of%0A%24d%24-dimensional%20inputs%20is%20computationally%20hard%3A%20the%20number%20of%20samples%20required%0Ato%20recover%20a%20single%20direction%20from%20an%20order-%24p%24%20tensor%20%28tensor%20PCA%29%20using%0Aonline%20SGD%20grows%20as%20%24d%5E%7Bp-1%7D%24%2C%20which%20is%20prohibitive%20for%20high-dimensional%0Ainputs.%20This%20result%20raises%20the%20question%20of%20how%20neural%20networks%20extract%20relevant%0Adirections%20from%20the%20HOCs%20of%20their%20inputs%20efficiently.%20Here%2C%20we%20show%20that%0Acorrelations%20between%20latent%20variables%20along%20the%20directions%20encoded%20in%20different%0Ainput%20cumulants%20speed%20up%20learning%20from%20higher-order%20correlations.%20We%20show%20this%0Aeffect%20analytically%20by%20deriving%20nearly%20sharp%20thresholds%20for%20the%20number%20of%0Asamples%20required%20by%20a%20single%20neuron%20to%20weakly-recover%20these%20directions%20using%0Aonline%20SGD%20from%20a%20random%20start%20in%20high%20dimensions.%20Our%20analytical%20results%20are%0Aconfirmed%20in%20simulations%20of%20two-layer%20neural%20networks%20and%20unveil%20a%20new%0Amechanism%20for%20hierarchical%20learning%20in%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08602v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sliding%20down%20the%20stairs%3A%20how%20correlated%20latent%20variables%20accelerate%0A%20%20learning%20with%20neural%20networks&entry.906535625=Lorenzo%20Bardone%20and%20Sebastian%20Goldt&entry.1292438233=%20%20Neural%20networks%20extract%20features%20from%20data%20using%20stochastic%20gradient%20descent%0A%28SGD%29.%20In%20particular%2C%20higher-order%20input%20cumulants%20%28HOCs%29%20are%20crucial%20for%20their%0Aperformance.%20However%2C%20extracting%20information%20from%20the%20%24p%24th%20cumulant%20of%0A%24d%24-dimensional%20inputs%20is%20computationally%20hard%3A%20the%20number%20of%20samples%20required%0Ato%20recover%20a%20single%20direction%20from%20an%20order-%24p%24%20tensor%20%28tensor%20PCA%29%20using%0Aonline%20SGD%20grows%20as%20%24d%5E%7Bp-1%7D%24%2C%20which%20is%20prohibitive%20for%20high-dimensional%0Ainputs.%20This%20result%20raises%20the%20question%20of%20how%20neural%20networks%20extract%20relevant%0Adirections%20from%20the%20HOCs%20of%20their%20inputs%20efficiently.%20Here%2C%20we%20show%20that%0Acorrelations%20between%20latent%20variables%20along%20the%20directions%20encoded%20in%20different%0Ainput%20cumulants%20speed%20up%20learning%20from%20higher-order%20correlations.%20We%20show%20this%0Aeffect%20analytically%20by%20deriving%20nearly%20sharp%20thresholds%20for%20the%20number%20of%0Asamples%20required%20by%20a%20single%20neuron%20to%20weakly-recover%20these%20directions%20using%0Aonline%20SGD%20from%20a%20random%20start%20in%20high%20dimensions.%20Our%20analytical%20results%20are%0Aconfirmed%20in%20simulations%20of%20two-layer%20neural%20networks%20and%20unveil%20a%20new%0Amechanism%20for%20hierarchical%20learning%20in%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08602v1&entry.124074799=Read"},
{"title": "Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids\n  Representations", "author": "Francesco Saverio Pezzicoli and Guillaume Charpiat and Fran\u00e7ois P. Landes", "abstract": "  The difficult problem of relating the static structure of glassy liquids and\ntheir dynamics is a good target for Machine Learning, an approach which excels\nat finding complex patterns hidden in data. Indeed, this approach is currently\na hot topic in the glassy liquids community, where the state of the art\nconsists in Graph Neural Networks (GNNs), which have great expressive power but\nare heavy models and lack interpretability. Inspired by recent advances in the\nfield of Machine Learning group-equivariant representations, we build a GNN\nthat learns a robust representation of the glass' static structure by\nconstraining it to preserve the roto-translation (SE(3)) equivariance. We show\nthat this constraint significantly improves the predictive power at comparable\nor reduced number of parameters but most importantly, improves the ability to\ngeneralize to unseen temperatures. While remaining a Deep network, our model\nhas improved interpretability compared to other GNNs, as the action of our\nbasic convolution layer relates directly to well-known rotation-invariant\nexpert features. Through transfer-learning experiments displaying unprecedented\nperformance, we demonstrate that our network learns a robust representation,\nwhich allows us to push forward the idea of a learned structural order\nparameter for glasses.\n", "link": "http://arxiv.org/abs/2211.03226v3", "date": "2024-04-12", "relevancy": 1.954, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4983}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4887}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4786}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rotation-equivariant%20Graph%20Neural%20Networks%20for%20Learning%20Glassy%20Liquids%0A%20%20Representations&body=Title%3A%20Rotation-equivariant%20Graph%20Neural%20Networks%20for%20Learning%20Glassy%20Liquids%0A%20%20Representations%0AAuthor%3A%20Francesco%20Saverio%20Pezzicoli%20and%20Guillaume%20Charpiat%20and%20Fran%C3%A7ois%20P.%20Landes%0AAbstract%3A%20%20%20The%20difficult%20problem%20of%20relating%20the%20static%20structure%20of%20glassy%20liquids%20and%0Atheir%20dynamics%20is%20a%20good%20target%20for%20Machine%20Learning%2C%20an%20approach%20which%20excels%0Aat%20finding%20complex%20patterns%20hidden%20in%20data.%20Indeed%2C%20this%20approach%20is%20currently%0Aa%20hot%20topic%20in%20the%20glassy%20liquids%20community%2C%20where%20the%20state%20of%20the%20art%0Aconsists%20in%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20have%20great%20expressive%20power%20but%0Aare%20heavy%20models%20and%20lack%20interpretability.%20Inspired%20by%20recent%20advances%20in%20the%0Afield%20of%20Machine%20Learning%20group-equivariant%20representations%2C%20we%20build%20a%20GNN%0Athat%20learns%20a%20robust%20representation%20of%20the%20glass%27%20static%20structure%20by%0Aconstraining%20it%20to%20preserve%20the%20roto-translation%20%28SE%283%29%29%20equivariance.%20We%20show%0Athat%20this%20constraint%20significantly%20improves%20the%20predictive%20power%20at%20comparable%0Aor%20reduced%20number%20of%20parameters%20but%20most%20importantly%2C%20improves%20the%20ability%20to%0Ageneralize%20to%20unseen%20temperatures.%20While%20remaining%20a%20Deep%20network%2C%20our%20model%0Ahas%20improved%20interpretability%20compared%20to%20other%20GNNs%2C%20as%20the%20action%20of%20our%0Abasic%20convolution%20layer%20relates%20directly%20to%20well-known%20rotation-invariant%0Aexpert%20features.%20Through%20transfer-learning%20experiments%20displaying%20unprecedented%0Aperformance%2C%20we%20demonstrate%20that%20our%20network%20learns%20a%20robust%20representation%2C%0Awhich%20allows%20us%20to%20push%20forward%20the%20idea%20of%20a%20learned%20structural%20order%0Aparameter%20for%20glasses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.03226v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotation-equivariant%20Graph%20Neural%20Networks%20for%20Learning%20Glassy%20Liquids%0A%20%20Representations&entry.906535625=Francesco%20Saverio%20Pezzicoli%20and%20Guillaume%20Charpiat%20and%20Fran%C3%A7ois%20P.%20Landes&entry.1292438233=%20%20The%20difficult%20problem%20of%20relating%20the%20static%20structure%20of%20glassy%20liquids%20and%0Atheir%20dynamics%20is%20a%20good%20target%20for%20Machine%20Learning%2C%20an%20approach%20which%20excels%0Aat%20finding%20complex%20patterns%20hidden%20in%20data.%20Indeed%2C%20this%20approach%20is%20currently%0Aa%20hot%20topic%20in%20the%20glassy%20liquids%20community%2C%20where%20the%20state%20of%20the%20art%0Aconsists%20in%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20have%20great%20expressive%20power%20but%0Aare%20heavy%20models%20and%20lack%20interpretability.%20Inspired%20by%20recent%20advances%20in%20the%0Afield%20of%20Machine%20Learning%20group-equivariant%20representations%2C%20we%20build%20a%20GNN%0Athat%20learns%20a%20robust%20representation%20of%20the%20glass%27%20static%20structure%20by%0Aconstraining%20it%20to%20preserve%20the%20roto-translation%20%28SE%283%29%29%20equivariance.%20We%20show%0Athat%20this%20constraint%20significantly%20improves%20the%20predictive%20power%20at%20comparable%0Aor%20reduced%20number%20of%20parameters%20but%20most%20importantly%2C%20improves%20the%20ability%20to%0Ageneralize%20to%20unseen%20temperatures.%20While%20remaining%20a%20Deep%20network%2C%20our%20model%0Ahas%20improved%20interpretability%20compared%20to%20other%20GNNs%2C%20as%20the%20action%20of%20our%0Abasic%20convolution%20layer%20relates%20directly%20to%20well-known%20rotation-invariant%0Aexpert%20features.%20Through%20transfer-learning%20experiments%20displaying%20unprecedented%0Aperformance%2C%20we%20demonstrate%20that%20our%20network%20learns%20a%20robust%20representation%2C%0Awhich%20allows%20us%20to%20push%20forward%20the%20idea%20of%20a%20learned%20structural%20order%0Aparameter%20for%20glasses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.03226v3&entry.124074799=Read"},
{"title": "COCONut: Modernizing COCO Segmentation", "author": "Xueqing Deng and Qihang Yu and Peng Wang and Xiaohui Shen and Liang-Chieh Chen", "abstract": "  In recent decades, the vision community has witnessed remarkable progress in\nvisual recognition, partially owing to advancements in dataset benchmarks.\nNotably, the established COCO benchmark has propelled the development of modern\ndetection and segmentation systems. However, the COCO segmentation benchmark\nhas seen comparatively slow improvement over the last decade. Originally\nequipped with coarse polygon annotations for thing instances, it gradually\nincorporated coarse superpixel annotations for stuff regions, which were\nsubsequently heuristically amalgamated to yield panoptic segmentation\nannotations. These annotations, executed by different groups of raters, have\nresulted not only in coarse segmentation masks but also in inconsistencies\nbetween segmentation types. In this study, we undertake a comprehensive\nreevaluation of the COCO segmentation annotations. By enhancing the annotation\nquality and expanding the dataset to encompass 383K images with more than 5.18M\npanoptic masks, we introduce COCONut, the COCO Next Universal segmenTation\ndataset. COCONut harmonizes segmentation annotations across semantic, instance,\nand panoptic segmentation with meticulously crafted high-quality masks, and\nestablishes a robust benchmark for all segmentation tasks. To our knowledge,\nCOCONut stands as the inaugural large-scale universal segmentation dataset,\nverified by human raters. We anticipate that the release of COCONut will\nsignificantly contribute to the community's ability to assess the progress of\nnovel neural networks.\n", "link": "http://arxiv.org/abs/2404.08639v1", "date": "2024-04-12", "relevancy": 1.945, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5109}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4577}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COCONut%3A%20Modernizing%20COCO%20Segmentation&body=Title%3A%20COCONut%3A%20Modernizing%20COCO%20Segmentation%0AAuthor%3A%20Xueqing%20Deng%20and%20Qihang%20Yu%20and%20Peng%20Wang%20and%20Xiaohui%20Shen%20and%20Liang-Chieh%20Chen%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20the%20vision%20community%20has%20witnessed%20remarkable%20progress%20in%0Avisual%20recognition%2C%20partially%20owing%20to%20advancements%20in%20dataset%20benchmarks.%0ANotably%2C%20the%20established%20COCO%20benchmark%20has%20propelled%20the%20development%20of%20modern%0Adetection%20and%20segmentation%20systems.%20However%2C%20the%20COCO%20segmentation%20benchmark%0Ahas%20seen%20comparatively%20slow%20improvement%20over%20the%20last%20decade.%20Originally%0Aequipped%20with%20coarse%20polygon%20annotations%20for%20thing%20instances%2C%20it%20gradually%0Aincorporated%20coarse%20superpixel%20annotations%20for%20stuff%20regions%2C%20which%20were%0Asubsequently%20heuristically%20amalgamated%20to%20yield%20panoptic%20segmentation%0Aannotations.%20These%20annotations%2C%20executed%20by%20different%20groups%20of%20raters%2C%20have%0Aresulted%20not%20only%20in%20coarse%20segmentation%20masks%20but%20also%20in%20inconsistencies%0Abetween%20segmentation%20types.%20In%20this%20study%2C%20we%20undertake%20a%20comprehensive%0Areevaluation%20of%20the%20COCO%20segmentation%20annotations.%20By%20enhancing%20the%20annotation%0Aquality%20and%20expanding%20the%20dataset%20to%20encompass%20383K%20images%20with%20more%20than%205.18M%0Apanoptic%20masks%2C%20we%20introduce%20COCONut%2C%20the%20COCO%20Next%20Universal%20segmenTation%0Adataset.%20COCONut%20harmonizes%20segmentation%20annotations%20across%20semantic%2C%20instance%2C%0Aand%20panoptic%20segmentation%20with%20meticulously%20crafted%20high-quality%20masks%2C%20and%0Aestablishes%20a%20robust%20benchmark%20for%20all%20segmentation%20tasks.%20To%20our%20knowledge%2C%0ACOCONut%20stands%20as%20the%20inaugural%20large-scale%20universal%20segmentation%20dataset%2C%0Averified%20by%20human%20raters.%20We%20anticipate%20that%20the%20release%20of%20COCONut%20will%0Asignificantly%20contribute%20to%20the%20community%27s%20ability%20to%20assess%20the%20progress%20of%0Anovel%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COCONut%3A%20Modernizing%20COCO%20Segmentation&entry.906535625=Xueqing%20Deng%20and%20Qihang%20Yu%20and%20Peng%20Wang%20and%20Xiaohui%20Shen%20and%20Liang-Chieh%20Chen&entry.1292438233=%20%20In%20recent%20decades%2C%20the%20vision%20community%20has%20witnessed%20remarkable%20progress%20in%0Avisual%20recognition%2C%20partially%20owing%20to%20advancements%20in%20dataset%20benchmarks.%0ANotably%2C%20the%20established%20COCO%20benchmark%20has%20propelled%20the%20development%20of%20modern%0Adetection%20and%20segmentation%20systems.%20However%2C%20the%20COCO%20segmentation%20benchmark%0Ahas%20seen%20comparatively%20slow%20improvement%20over%20the%20last%20decade.%20Originally%0Aequipped%20with%20coarse%20polygon%20annotations%20for%20thing%20instances%2C%20it%20gradually%0Aincorporated%20coarse%20superpixel%20annotations%20for%20stuff%20regions%2C%20which%20were%0Asubsequently%20heuristically%20amalgamated%20to%20yield%20panoptic%20segmentation%0Aannotations.%20These%20annotations%2C%20executed%20by%20different%20groups%20of%20raters%2C%20have%0Aresulted%20not%20only%20in%20coarse%20segmentation%20masks%20but%20also%20in%20inconsistencies%0Abetween%20segmentation%20types.%20In%20this%20study%2C%20we%20undertake%20a%20comprehensive%0Areevaluation%20of%20the%20COCO%20segmentation%20annotations.%20By%20enhancing%20the%20annotation%0Aquality%20and%20expanding%20the%20dataset%20to%20encompass%20383K%20images%20with%20more%20than%205.18M%0Apanoptic%20masks%2C%20we%20introduce%20COCONut%2C%20the%20COCO%20Next%20Universal%20segmenTation%0Adataset.%20COCONut%20harmonizes%20segmentation%20annotations%20across%20semantic%2C%20instance%2C%0Aand%20panoptic%20segmentation%20with%20meticulously%20crafted%20high-quality%20masks%2C%20and%0Aestablishes%20a%20robust%20benchmark%20for%20all%20segmentation%20tasks.%20To%20our%20knowledge%2C%0ACOCONut%20stands%20as%20the%20inaugural%20large-scale%20universal%20segmentation%20dataset%2C%0Averified%20by%20human%20raters.%20We%20anticipate%20that%20the%20release%20of%20COCONut%20will%0Asignificantly%20contribute%20to%20the%20community%27s%20ability%20to%20assess%20the%20progress%20of%0Anovel%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08639v1&entry.124074799=Read"},
{"title": "Advanced wood species identification based on multiple anatomical\n  sections and using deep feature transfer and fusion", "author": "Kallil M. Zielinski and Leonardo Scabini and Lucas C. Ribas and N\u00fabia R. da Silva and Hans Beeckman and Jan Verwaeren and Odemir M. Bruno and Bernard De Baets", "abstract": "  In recent years, we have seen many advancements in wood species\nidentification. Methods like DNA analysis, Near Infrared (NIR) spectroscopy,\nand Direct Analysis in Real Time (DART) mass spectrometry complement the\nlong-established wood anatomical assessment of cell and tissue morphology.\nHowever, most of these methods have some limitations such as high costs, the\nneed for skilled experts for data interpretation, and the lack of good datasets\nfor professional reference. Therefore, most of these methods, and certainly the\nwood anatomical assessment, may benefit from tools based on Artificial\nIntelligence. In this paper, we apply two transfer learning techniques with\nConvolutional Neural Networks (CNNs) to a multi-view Congolese wood species\ndataset including sections from different orientations and viewed at different\nmicroscopic magnifications. We explore two feature extraction methods in\ndetail, namely Global Average Pooling (GAP) and Random Encoding of Aggregated\nDeep Activation Maps (RADAM), for efficient and accurate wood species\nidentification. Our results indicate superior accuracy on diverse datasets and\nanatomical sections, surpassing the results of other methods. Our proposal\nrepresents a significant advancement in wood species identification, offering a\nrobust tool to support the conservation of forest ecosystems and promote\nsustainable forestry practices.\n", "link": "http://arxiv.org/abs/2404.08585v1", "date": "2024-04-12", "relevancy": 1.9365, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4659}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advanced%20wood%20species%20identification%20based%20on%20multiple%20anatomical%0A%20%20sections%20and%20using%20deep%20feature%20transfer%20and%20fusion&body=Title%3A%20Advanced%20wood%20species%20identification%20based%20on%20multiple%20anatomical%0A%20%20sections%20and%20using%20deep%20feature%20transfer%20and%20fusion%0AAuthor%3A%20Kallil%20M.%20Zielinski%20and%20Leonardo%20Scabini%20and%20Lucas%20C.%20Ribas%20and%20N%C3%BAbia%20R.%20da%20Silva%20and%20Hans%20Beeckman%20and%20Jan%20Verwaeren%20and%20Odemir%20M.%20Bruno%20and%20Bernard%20De%20Baets%0AAbstract%3A%20%20%20In%20recent%20years%2C%20we%20have%20seen%20many%20advancements%20in%20wood%20species%0Aidentification.%20Methods%20like%20DNA%20analysis%2C%20Near%20Infrared%20%28NIR%29%20spectroscopy%2C%0Aand%20Direct%20Analysis%20in%20Real%20Time%20%28DART%29%20mass%20spectrometry%20complement%20the%0Along-established%20wood%20anatomical%20assessment%20of%20cell%20and%20tissue%20morphology.%0AHowever%2C%20most%20of%20these%20methods%20have%20some%20limitations%20such%20as%20high%20costs%2C%20the%0Aneed%20for%20skilled%20experts%20for%20data%20interpretation%2C%20and%20the%20lack%20of%20good%20datasets%0Afor%20professional%20reference.%20Therefore%2C%20most%20of%20these%20methods%2C%20and%20certainly%20the%0Awood%20anatomical%20assessment%2C%20may%20benefit%20from%20tools%20based%20on%20Artificial%0AIntelligence.%20In%20this%20paper%2C%20we%20apply%20two%20transfer%20learning%20techniques%20with%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20to%20a%20multi-view%20Congolese%20wood%20species%0Adataset%20including%20sections%20from%20different%20orientations%20and%20viewed%20at%20different%0Amicroscopic%20magnifications.%20We%20explore%20two%20feature%20extraction%20methods%20in%0Adetail%2C%20namely%20Global%20Average%20Pooling%20%28GAP%29%20and%20Random%20Encoding%20of%20Aggregated%0ADeep%20Activation%20Maps%20%28RADAM%29%2C%20for%20efficient%20and%20accurate%20wood%20species%0Aidentification.%20Our%20results%20indicate%20superior%20accuracy%20on%20diverse%20datasets%20and%0Aanatomical%20sections%2C%20surpassing%20the%20results%20of%20other%20methods.%20Our%20proposal%0Arepresents%20a%20significant%20advancement%20in%20wood%20species%20identification%2C%20offering%20a%0Arobust%20tool%20to%20support%20the%20conservation%20of%20forest%20ecosystems%20and%20promote%0Asustainable%20forestry%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08585v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20wood%20species%20identification%20based%20on%20multiple%20anatomical%0A%20%20sections%20and%20using%20deep%20feature%20transfer%20and%20fusion&entry.906535625=Kallil%20M.%20Zielinski%20and%20Leonardo%20Scabini%20and%20Lucas%20C.%20Ribas%20and%20N%C3%BAbia%20R.%20da%20Silva%20and%20Hans%20Beeckman%20and%20Jan%20Verwaeren%20and%20Odemir%20M.%20Bruno%20and%20Bernard%20De%20Baets&entry.1292438233=%20%20In%20recent%20years%2C%20we%20have%20seen%20many%20advancements%20in%20wood%20species%0Aidentification.%20Methods%20like%20DNA%20analysis%2C%20Near%20Infrared%20%28NIR%29%20spectroscopy%2C%0Aand%20Direct%20Analysis%20in%20Real%20Time%20%28DART%29%20mass%20spectrometry%20complement%20the%0Along-established%20wood%20anatomical%20assessment%20of%20cell%20and%20tissue%20morphology.%0AHowever%2C%20most%20of%20these%20methods%20have%20some%20limitations%20such%20as%20high%20costs%2C%20the%0Aneed%20for%20skilled%20experts%20for%20data%20interpretation%2C%20and%20the%20lack%20of%20good%20datasets%0Afor%20professional%20reference.%20Therefore%2C%20most%20of%20these%20methods%2C%20and%20certainly%20the%0Awood%20anatomical%20assessment%2C%20may%20benefit%20from%20tools%20based%20on%20Artificial%0AIntelligence.%20In%20this%20paper%2C%20we%20apply%20two%20transfer%20learning%20techniques%20with%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20to%20a%20multi-view%20Congolese%20wood%20species%0Adataset%20including%20sections%20from%20different%20orientations%20and%20viewed%20at%20different%0Amicroscopic%20magnifications.%20We%20explore%20two%20feature%20extraction%20methods%20in%0Adetail%2C%20namely%20Global%20Average%20Pooling%20%28GAP%29%20and%20Random%20Encoding%20of%20Aggregated%0ADeep%20Activation%20Maps%20%28RADAM%29%2C%20for%20efficient%20and%20accurate%20wood%20species%0Aidentification.%20Our%20results%20indicate%20superior%20accuracy%20on%20diverse%20datasets%20and%0Aanatomical%20sections%2C%20surpassing%20the%20results%20of%20other%20methods.%20Our%20proposal%0Arepresents%20a%20significant%20advancement%20in%20wood%20species%20identification%2C%20offering%20a%0Arobust%20tool%20to%20support%20the%20conservation%20of%20forest%20ecosystems%20and%20promote%0Asustainable%20forestry%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08585v1&entry.124074799=Read"},
{"title": "Advancing Forest Fire Prevention: Deep Reinforcement Learning for\n  Effective Firebreak Placement", "author": "Lucas Murray and Tatiana Castillo and Jaime Carrasco and Andr\u00e9s Weintraub and Richard Weber and Isaac Mart\u00edn de Diego and Jos\u00e9 Ram\u00f3n Gonz\u00e1lez and Jordi Garc\u00eda-Gonzalo", "abstract": "  Over the past decades, the increase in both frequency and intensity of\nlarge-scale wildfires due to climate change has emerged as a significant\nnatural threat. The pressing need to design resilient landscapes capable of\nwithstanding such disasters has become paramount, requiring the development of\nadvanced decision-support tools. Existing methodologies, including Mixed\nInteger Programming, Stochastic Optimization, and Network Theory, have proven\neffective but are hindered by computational demands, limiting their\napplicability.\n  In response to this challenge, we propose using artificial intelligence\ntechniques, specifically Deep Reinforcement Learning, to address the complex\nproblem of firebreak placement in the landscape. We employ value-function based\napproaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double\nDeep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with\nConvolutional Neural Networks, we have successfully implemented a computational\nagent capable of learning firebreak locations within a forest environment,\nachieving good results.\n  Furthermore, we incorporate a pre-training loop, initially teaching our agent\nto mimic a heuristic-based algorithm and observe that it consistently exceeds\nthe performance of these solutions. Our findings underscore the immense\npotential of Deep Reinforcement Learning for operational research challenges,\nespecially in fire prevention. Our approach demonstrates convergence with\nhighly favorable results in problem instances as large as 40 x 40 cells,\nmarking a significant milestone in applying Reinforcement Learning to this\ncritical issue.\n  To the best of our knowledge, this study represents a pioneering effort in\nusing Reinforcement Learning to address the aforementioned problem, offering\npromising perspectives in fire prevention and landscape management\n", "link": "http://arxiv.org/abs/2404.08523v1", "date": "2024-04-12", "relevancy": 1.9266, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4806}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4723}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advancing%20Forest%20Fire%20Prevention%3A%20Deep%20Reinforcement%20Learning%20for%0A%20%20Effective%20Firebreak%20Placement&body=Title%3A%20Advancing%20Forest%20Fire%20Prevention%3A%20Deep%20Reinforcement%20Learning%20for%0A%20%20Effective%20Firebreak%20Placement%0AAuthor%3A%20Lucas%20Murray%20and%20Tatiana%20Castillo%20and%20Jaime%20Carrasco%20and%20Andr%C3%A9s%20Weintraub%20and%20Richard%20Weber%20and%20Isaac%20Mart%C3%ADn%20de%20Diego%20and%20Jos%C3%A9%20Ram%C3%B3n%20Gonz%C3%A1lez%20and%20Jordi%20Garc%C3%ADa-Gonzalo%0AAbstract%3A%20%20%20Over%20the%20past%20decades%2C%20the%20increase%20in%20both%20frequency%20and%20intensity%20of%0Alarge-scale%20wildfires%20due%20to%20climate%20change%20has%20emerged%20as%20a%20significant%0Anatural%20threat.%20The%20pressing%20need%20to%20design%20resilient%20landscapes%20capable%20of%0Awithstanding%20such%20disasters%20has%20become%20paramount%2C%20requiring%20the%20development%20of%0Aadvanced%20decision-support%20tools.%20Existing%20methodologies%2C%20including%20Mixed%0AInteger%20Programming%2C%20Stochastic%20Optimization%2C%20and%20Network%20Theory%2C%20have%20proven%0Aeffective%20but%20are%20hindered%20by%20computational%20demands%2C%20limiting%20their%0Aapplicability.%0A%20%20In%20response%20to%20this%20challenge%2C%20we%20propose%20using%20artificial%20intelligence%0Atechniques%2C%20specifically%20Deep%20Reinforcement%20Learning%2C%20to%20address%20the%20complex%0Aproblem%20of%20firebreak%20placement%20in%20the%20landscape.%20We%20employ%20value-function%20based%0Aapproaches%20like%20Deep%20Q-Learning%2C%20Double%20Deep%20Q-Learning%2C%20and%20Dueling%20Double%0ADeep%20Q-Learning.%20Utilizing%20the%20Cell2Fire%20fire%20spread%20simulator%20combined%20with%0AConvolutional%20Neural%20Networks%2C%20we%20have%20successfully%20implemented%20a%20computational%0Aagent%20capable%20of%20learning%20firebreak%20locations%20within%20a%20forest%20environment%2C%0Aachieving%20good%20results.%0A%20%20Furthermore%2C%20we%20incorporate%20a%20pre-training%20loop%2C%20initially%20teaching%20our%20agent%0Ato%20mimic%20a%20heuristic-based%20algorithm%20and%20observe%20that%20it%20consistently%20exceeds%0Athe%20performance%20of%20these%20solutions.%20Our%20findings%20underscore%20the%20immense%0Apotential%20of%20Deep%20Reinforcement%20Learning%20for%20operational%20research%20challenges%2C%0Aespecially%20in%20fire%20prevention.%20Our%20approach%20demonstrates%20convergence%20with%0Ahighly%20favorable%20results%20in%20problem%20instances%20as%20large%20as%2040%20x%2040%20cells%2C%0Amarking%20a%20significant%20milestone%20in%20applying%20Reinforcement%20Learning%20to%20this%0Acritical%20issue.%0A%20%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20represents%20a%20pioneering%20effort%20in%0Ausing%20Reinforcement%20Learning%20to%20address%20the%20aforementioned%20problem%2C%20offering%0Apromising%20perspectives%20in%20fire%20prevention%20and%20landscape%20management%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08523v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Forest%20Fire%20Prevention%3A%20Deep%20Reinforcement%20Learning%20for%0A%20%20Effective%20Firebreak%20Placement&entry.906535625=Lucas%20Murray%20and%20Tatiana%20Castillo%20and%20Jaime%20Carrasco%20and%20Andr%C3%A9s%20Weintraub%20and%20Richard%20Weber%20and%20Isaac%20Mart%C3%ADn%20de%20Diego%20and%20Jos%C3%A9%20Ram%C3%B3n%20Gonz%C3%A1lez%20and%20Jordi%20Garc%C3%ADa-Gonzalo&entry.1292438233=%20%20Over%20the%20past%20decades%2C%20the%20increase%20in%20both%20frequency%20and%20intensity%20of%0Alarge-scale%20wildfires%20due%20to%20climate%20change%20has%20emerged%20as%20a%20significant%0Anatural%20threat.%20The%20pressing%20need%20to%20design%20resilient%20landscapes%20capable%20of%0Awithstanding%20such%20disasters%20has%20become%20paramount%2C%20requiring%20the%20development%20of%0Aadvanced%20decision-support%20tools.%20Existing%20methodologies%2C%20including%20Mixed%0AInteger%20Programming%2C%20Stochastic%20Optimization%2C%20and%20Network%20Theory%2C%20have%20proven%0Aeffective%20but%20are%20hindered%20by%20computational%20demands%2C%20limiting%20their%0Aapplicability.%0A%20%20In%20response%20to%20this%20challenge%2C%20we%20propose%20using%20artificial%20intelligence%0Atechniques%2C%20specifically%20Deep%20Reinforcement%20Learning%2C%20to%20address%20the%20complex%0Aproblem%20of%20firebreak%20placement%20in%20the%20landscape.%20We%20employ%20value-function%20based%0Aapproaches%20like%20Deep%20Q-Learning%2C%20Double%20Deep%20Q-Learning%2C%20and%20Dueling%20Double%0ADeep%20Q-Learning.%20Utilizing%20the%20Cell2Fire%20fire%20spread%20simulator%20combined%20with%0AConvolutional%20Neural%20Networks%2C%20we%20have%20successfully%20implemented%20a%20computational%0Aagent%20capable%20of%20learning%20firebreak%20locations%20within%20a%20forest%20environment%2C%0Aachieving%20good%20results.%0A%20%20Furthermore%2C%20we%20incorporate%20a%20pre-training%20loop%2C%20initially%20teaching%20our%20agent%0Ato%20mimic%20a%20heuristic-based%20algorithm%20and%20observe%20that%20it%20consistently%20exceeds%0Athe%20performance%20of%20these%20solutions.%20Our%20findings%20underscore%20the%20immense%0Apotential%20of%20Deep%20Reinforcement%20Learning%20for%20operational%20research%20challenges%2C%0Aespecially%20in%20fire%20prevention.%20Our%20approach%20demonstrates%20convergence%20with%0Ahighly%20favorable%20results%20in%20problem%20instances%20as%20large%20as%2040%20x%2040%20cells%2C%0Amarking%20a%20significant%20milestone%20in%20applying%20Reinforcement%20Learning%20to%20this%0Acritical%20issue.%0A%20%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20represents%20a%20pioneering%20effort%20in%0Ausing%20Reinforcement%20Learning%20to%20address%20the%20aforementioned%20problem%2C%20offering%0Apromising%20perspectives%20in%20fire%20prevention%20and%20landscape%20management%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08523v1&entry.124074799=Read"},
{"title": "Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot\n  Generalization", "author": "Fei Liu and Xi Lin and Zhenkun Wang and Qingfu Zhang and Xialiang Tong and Mingxuan Yuan", "abstract": "  Vehicle routing problems (VRPs), which can be found in numerous real-world\napplications, have been an important research topic for several decades.\nRecently, the neural combinatorial optimization (NCO) approach that leverages a\nlearning-based model to solve VRPs without manual algorithm design has gained\nsubstantial attention. However, current NCO methods typically require building\none model for each routing problem, which significantly hinders their practical\napplication for real-world industry problems with diverse attributes. In this\nwork, we make the first attempt to tackle the crucial challenge of\ncross-problem generalization. In particular, we formulate VRPs as different\ncombinations of a set of shared underlying attributes and solve them\nsimultaneously via a single model through attribute composition. In this way,\nour proposed model can successfully solve VRPs with unseen attribute\ncombinations in a zero-shot generalization manner. Extensive experiments are\nconducted on eleven VRP variants, benchmark datasets, and industry logistic\nscenarios. The results show that the unified model demonstrates superior\nperformance in the eleven VRPs, reducing the average gap to around 5% from over\n20% in the existing approach and achieving a significant performance boost on\nbenchmark datasets as well as a real-world logistics application. The source\ncode is included in https://github.com/FeiLiu36/MTNCO.\n", "link": "http://arxiv.org/abs/2402.16891v2", "date": "2024-04-12", "relevancy": 1.9131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4729}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Task%20Learning%20for%20Routing%20Problem%20with%20Cross-Problem%20Zero-Shot%0A%20%20Generalization&body=Title%3A%20Multi-Task%20Learning%20for%20Routing%20Problem%20with%20Cross-Problem%20Zero-Shot%0A%20%20Generalization%0AAuthor%3A%20Fei%20Liu%20and%20Xi%20Lin%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan%0AAbstract%3A%20%20%20Vehicle%20routing%20problems%20%28VRPs%29%2C%20which%20can%20be%20found%20in%20numerous%20real-world%0Aapplications%2C%20have%20been%20an%20important%20research%20topic%20for%20several%20decades.%0ARecently%2C%20the%20neural%20combinatorial%20optimization%20%28NCO%29%20approach%20that%20leverages%20a%0Alearning-based%20model%20to%20solve%20VRPs%20without%20manual%20algorithm%20design%20has%20gained%0Asubstantial%20attention.%20However%2C%20current%20NCO%20methods%20typically%20require%20building%0Aone%20model%20for%20each%20routing%20problem%2C%20which%20significantly%20hinders%20their%20practical%0Aapplication%20for%20real-world%20industry%20problems%20with%20diverse%20attributes.%20In%20this%0Awork%2C%20we%20make%20the%20first%20attempt%20to%20tackle%20the%20crucial%20challenge%20of%0Across-problem%20generalization.%20In%20particular%2C%20we%20formulate%20VRPs%20as%20different%0Acombinations%20of%20a%20set%20of%20shared%20underlying%20attributes%20and%20solve%20them%0Asimultaneously%20via%20a%20single%20model%20through%20attribute%20composition.%20In%20this%20way%2C%0Aour%20proposed%20model%20can%20successfully%20solve%20VRPs%20with%20unseen%20attribute%0Acombinations%20in%20a%20zero-shot%20generalization%20manner.%20Extensive%20experiments%20are%0Aconducted%20on%20eleven%20VRP%20variants%2C%20benchmark%20datasets%2C%20and%20industry%20logistic%0Ascenarios.%20The%20results%20show%20that%20the%20unified%20model%20demonstrates%20superior%0Aperformance%20in%20the%20eleven%20VRPs%2C%20reducing%20the%20average%20gap%20to%20around%205%25%20from%20over%0A20%25%20in%20the%20existing%20approach%20and%20achieving%20a%20significant%20performance%20boost%20on%0Abenchmark%20datasets%20as%20well%20as%20a%20real-world%20logistics%20application.%20The%20source%0Acode%20is%20included%20in%20https%3A//github.com/FeiLiu36/MTNCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16891v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Task%20Learning%20for%20Routing%20Problem%20with%20Cross-Problem%20Zero-Shot%0A%20%20Generalization&entry.906535625=Fei%20Liu%20and%20Xi%20Lin%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang%20and%20Xialiang%20Tong%20and%20Mingxuan%20Yuan&entry.1292438233=%20%20Vehicle%20routing%20problems%20%28VRPs%29%2C%20which%20can%20be%20found%20in%20numerous%20real-world%0Aapplications%2C%20have%20been%20an%20important%20research%20topic%20for%20several%20decades.%0ARecently%2C%20the%20neural%20combinatorial%20optimization%20%28NCO%29%20approach%20that%20leverages%20a%0Alearning-based%20model%20to%20solve%20VRPs%20without%20manual%20algorithm%20design%20has%20gained%0Asubstantial%20attention.%20However%2C%20current%20NCO%20methods%20typically%20require%20building%0Aone%20model%20for%20each%20routing%20problem%2C%20which%20significantly%20hinders%20their%20practical%0Aapplication%20for%20real-world%20industry%20problems%20with%20diverse%20attributes.%20In%20this%0Awork%2C%20we%20make%20the%20first%20attempt%20to%20tackle%20the%20crucial%20challenge%20of%0Across-problem%20generalization.%20In%20particular%2C%20we%20formulate%20VRPs%20as%20different%0Acombinations%20of%20a%20set%20of%20shared%20underlying%20attributes%20and%20solve%20them%0Asimultaneously%20via%20a%20single%20model%20through%20attribute%20composition.%20In%20this%20way%2C%0Aour%20proposed%20model%20can%20successfully%20solve%20VRPs%20with%20unseen%20attribute%0Acombinations%20in%20a%20zero-shot%20generalization%20manner.%20Extensive%20experiments%20are%0Aconducted%20on%20eleven%20VRP%20variants%2C%20benchmark%20datasets%2C%20and%20industry%20logistic%0Ascenarios.%20The%20results%20show%20that%20the%20unified%20model%20demonstrates%20superior%0Aperformance%20in%20the%20eleven%20VRPs%2C%20reducing%20the%20average%20gap%20to%20around%205%25%20from%20over%0A20%25%20in%20the%20existing%20approach%20and%20achieving%20a%20significant%20performance%20boost%20on%0Abenchmark%20datasets%20as%20well%20as%20a%20real-world%20logistics%20application.%20The%20source%0Acode%20is%20included%20in%20https%3A//github.com/FeiLiu36/MTNCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16891v2&entry.124074799=Read"},
{"title": "Mitigating Receiver Impact on Radio Frequency Fingerprint Identification\n  via Domain Adaptation", "author": "Liu Yang and Qiang Li and Xiaoyang Ren and Yi Fang and Shafei Wang", "abstract": "  Radio Frequency Fingerprint Identification (RFFI), which exploits non-ideal\nhardware-induced unique distortion resident in the transmit signals to identify\nan emitter, is emerging as a means to enhance the security of communication\nsystems. Recently, machine learning has achieved great success in developing\nstate-of-the-art RFFI models. However, few works consider cross-receiver RFFI\nproblems, where the RFFI model is trained and deployed on different receivers.\nDue to altered receiver characteristics, direct deployment of RFFI model on a\nnew receiver leads to significant performance degradation. To address this\nissue, we formulate the cross-receiver RFFI as a model adaptation problem,\nwhich adapts the trained model to unlabeled signals from a new receiver. We\nfirst develop a theoretical generalization error bound for the adaptation\nmodel. Motivated by the bound, we propose a novel method to solve the\ncross-receiver RFFI problem, which includes domain alignment and adaptive\npseudo-labeling. The former aims at finding a feature space where both domains\nexhibit similar distributions, effectively reducing the domain discrepancy.\nMeanwhile, the latter employs a dynamic pseudo-labeling scheme to implicitly\ntransfer the label information from the labeled receiver to the new receiver.\nExperimental results indicate that the proposed method can effectively mitigate\nthe receiver impact and improve the cross-receiver RFFI performance.\n", "link": "http://arxiv.org/abs/2404.08566v1", "date": "2024-04-12", "relevancy": 1.9103, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4802}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4748}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Receiver%20Impact%20on%20Radio%20Frequency%20Fingerprint%20Identification%0A%20%20via%20Domain%20Adaptation&body=Title%3A%20Mitigating%20Receiver%20Impact%20on%20Radio%20Frequency%20Fingerprint%20Identification%0A%20%20via%20Domain%20Adaptation%0AAuthor%3A%20Liu%20Yang%20and%20Qiang%20Li%20and%20Xiaoyang%20Ren%20and%20Yi%20Fang%20and%20Shafei%20Wang%0AAbstract%3A%20%20%20Radio%20Frequency%20Fingerprint%20Identification%20%28RFFI%29%2C%20which%20exploits%20non-ideal%0Ahardware-induced%20unique%20distortion%20resident%20in%20the%20transmit%20signals%20to%20identify%0Aan%20emitter%2C%20is%20emerging%20as%20a%20means%20to%20enhance%20the%20security%20of%20communication%0Asystems.%20Recently%2C%20machine%20learning%20has%20achieved%20great%20success%20in%20developing%0Astate-of-the-art%20RFFI%20models.%20However%2C%20few%20works%20consider%20cross-receiver%20RFFI%0Aproblems%2C%20where%20the%20RFFI%20model%20is%20trained%20and%20deployed%20on%20different%20receivers.%0ADue%20to%20altered%20receiver%20characteristics%2C%20direct%20deployment%20of%20RFFI%20model%20on%20a%0Anew%20receiver%20leads%20to%20significant%20performance%20degradation.%20To%20address%20this%0Aissue%2C%20we%20formulate%20the%20cross-receiver%20RFFI%20as%20a%20model%20adaptation%20problem%2C%0Awhich%20adapts%20the%20trained%20model%20to%20unlabeled%20signals%20from%20a%20new%20receiver.%20We%0Afirst%20develop%20a%20theoretical%20generalization%20error%20bound%20for%20the%20adaptation%0Amodel.%20Motivated%20by%20the%20bound%2C%20we%20propose%20a%20novel%20method%20to%20solve%20the%0Across-receiver%20RFFI%20problem%2C%20which%20includes%20domain%20alignment%20and%20adaptive%0Apseudo-labeling.%20The%20former%20aims%20at%20finding%20a%20feature%20space%20where%20both%20domains%0Aexhibit%20similar%20distributions%2C%20effectively%20reducing%20the%20domain%20discrepancy.%0AMeanwhile%2C%20the%20latter%20employs%20a%20dynamic%20pseudo-labeling%20scheme%20to%20implicitly%0Atransfer%20the%20label%20information%20from%20the%20labeled%20receiver%20to%20the%20new%20receiver.%0AExperimental%20results%20indicate%20that%20the%20proposed%20method%20can%20effectively%20mitigate%0Athe%20receiver%20impact%20and%20improve%20the%20cross-receiver%20RFFI%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08566v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Receiver%20Impact%20on%20Radio%20Frequency%20Fingerprint%20Identification%0A%20%20via%20Domain%20Adaptation&entry.906535625=Liu%20Yang%20and%20Qiang%20Li%20and%20Xiaoyang%20Ren%20and%20Yi%20Fang%20and%20Shafei%20Wang&entry.1292438233=%20%20Radio%20Frequency%20Fingerprint%20Identification%20%28RFFI%29%2C%20which%20exploits%20non-ideal%0Ahardware-induced%20unique%20distortion%20resident%20in%20the%20transmit%20signals%20to%20identify%0Aan%20emitter%2C%20is%20emerging%20as%20a%20means%20to%20enhance%20the%20security%20of%20communication%0Asystems.%20Recently%2C%20machine%20learning%20has%20achieved%20great%20success%20in%20developing%0Astate-of-the-art%20RFFI%20models.%20However%2C%20few%20works%20consider%20cross-receiver%20RFFI%0Aproblems%2C%20where%20the%20RFFI%20model%20is%20trained%20and%20deployed%20on%20different%20receivers.%0ADue%20to%20altered%20receiver%20characteristics%2C%20direct%20deployment%20of%20RFFI%20model%20on%20a%0Anew%20receiver%20leads%20to%20significant%20performance%20degradation.%20To%20address%20this%0Aissue%2C%20we%20formulate%20the%20cross-receiver%20RFFI%20as%20a%20model%20adaptation%20problem%2C%0Awhich%20adapts%20the%20trained%20model%20to%20unlabeled%20signals%20from%20a%20new%20receiver.%20We%0Afirst%20develop%20a%20theoretical%20generalization%20error%20bound%20for%20the%20adaptation%0Amodel.%20Motivated%20by%20the%20bound%2C%20we%20propose%20a%20novel%20method%20to%20solve%20the%0Across-receiver%20RFFI%20problem%2C%20which%20includes%20domain%20alignment%20and%20adaptive%0Apseudo-labeling.%20The%20former%20aims%20at%20finding%20a%20feature%20space%20where%20both%20domains%0Aexhibit%20similar%20distributions%2C%20effectively%20reducing%20the%20domain%20discrepancy.%0AMeanwhile%2C%20the%20latter%20employs%20a%20dynamic%20pseudo-labeling%20scheme%20to%20implicitly%0Atransfer%20the%20label%20information%20from%20the%20labeled%20receiver%20to%20the%20new%20receiver.%0AExperimental%20results%20indicate%20that%20the%20proposed%20method%20can%20effectively%20mitigate%0Athe%20receiver%20impact%20and%20improve%20the%20cross-receiver%20RFFI%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08566v1&entry.124074799=Read"},
{"title": "Calibration of Continual Learning Models", "author": "Lanpei Li and Elia Piccoli and Andrea Cossu and Davide Bacciu and Vincenzo Lomonaco", "abstract": "  Continual Learning (CL) focuses on maximizing the predictive performance of a\nmodel across a non-stationary stream of data. Unfortunately, CL models tend to\nforget previous knowledge, thus often underperforming when compared with an\noffline model trained jointly on the entire data stream. Given that any CL\nmodel will eventually make mistakes, it is of crucial importance to build\ncalibrated CL models: models that can reliably tell their confidence when\nmaking a prediction. Model calibration is an active research topic in machine\nlearning, yet to be properly investigated in CL. We provide the first empirical\nstudy of the behavior of calibration approaches in CL, showing that CL\nstrategies do not inherently learn calibrated models. To mitigate this issue,\nwe design a continual calibration approach that improves the performance of\npost-processing calibration methods over a wide range of different benchmarks\nand CL strategies. CL does not necessarily need perfect predictive models, but\nrather it can benefit from reliable predictive models. We believe our study on\ncontinual calibration represents a first step towards this direction.\n", "link": "http://arxiv.org/abs/2404.07817v2", "date": "2024-04-12", "relevancy": 1.9101, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Calibration%20of%20Continual%20Learning%20Models&body=Title%3A%20Calibration%20of%20Continual%20Learning%20Models%0AAuthor%3A%20Lanpei%20Li%20and%20Elia%20Piccoli%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20focuses%20on%20maximizing%20the%20predictive%20performance%20of%20a%0Amodel%20across%20a%20non-stationary%20stream%20of%20data.%20Unfortunately%2C%20CL%20models%20tend%20to%0Aforget%20previous%20knowledge%2C%20thus%20often%20underperforming%20when%20compared%20with%20an%0Aoffline%20model%20trained%20jointly%20on%20the%20entire%20data%20stream.%20Given%20that%20any%20CL%0Amodel%20will%20eventually%20make%20mistakes%2C%20it%20is%20of%20crucial%20importance%20to%20build%0Acalibrated%20CL%20models%3A%20models%20that%20can%20reliably%20tell%20their%20confidence%20when%0Amaking%20a%20prediction.%20Model%20calibration%20is%20an%20active%20research%20topic%20in%20machine%0Alearning%2C%20yet%20to%20be%20properly%20investigated%20in%20CL.%20We%20provide%20the%20first%20empirical%0Astudy%20of%20the%20behavior%20of%20calibration%20approaches%20in%20CL%2C%20showing%20that%20CL%0Astrategies%20do%20not%20inherently%20learn%20calibrated%20models.%20To%20mitigate%20this%20issue%2C%0Awe%20design%20a%20continual%20calibration%20approach%20that%20improves%20the%20performance%20of%0Apost-processing%20calibration%20methods%20over%20a%20wide%20range%20of%20different%20benchmarks%0Aand%20CL%20strategies.%20CL%20does%20not%20necessarily%20need%20perfect%20predictive%20models%2C%20but%0Arather%20it%20can%20benefit%20from%20reliable%20predictive%20models.%20We%20believe%20our%20study%20on%0Acontinual%20calibration%20represents%20a%20first%20step%20towards%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07817v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20of%20Continual%20Learning%20Models&entry.906535625=Lanpei%20Li%20and%20Elia%20Piccoli%20and%20Andrea%20Cossu%20and%20Davide%20Bacciu%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20focuses%20on%20maximizing%20the%20predictive%20performance%20of%20a%0Amodel%20across%20a%20non-stationary%20stream%20of%20data.%20Unfortunately%2C%20CL%20models%20tend%20to%0Aforget%20previous%20knowledge%2C%20thus%20often%20underperforming%20when%20compared%20with%20an%0Aoffline%20model%20trained%20jointly%20on%20the%20entire%20data%20stream.%20Given%20that%20any%20CL%0Amodel%20will%20eventually%20make%20mistakes%2C%20it%20is%20of%20crucial%20importance%20to%20build%0Acalibrated%20CL%20models%3A%20models%20that%20can%20reliably%20tell%20their%20confidence%20when%0Amaking%20a%20prediction.%20Model%20calibration%20is%20an%20active%20research%20topic%20in%20machine%0Alearning%2C%20yet%20to%20be%20properly%20investigated%20in%20CL.%20We%20provide%20the%20first%20empirical%0Astudy%20of%20the%20behavior%20of%20calibration%20approaches%20in%20CL%2C%20showing%20that%20CL%0Astrategies%20do%20not%20inherently%20learn%20calibrated%20models.%20To%20mitigate%20this%20issue%2C%0Awe%20design%20a%20continual%20calibration%20approach%20that%20improves%20the%20performance%20of%0Apost-processing%20calibration%20methods%20over%20a%20wide%20range%20of%20different%20benchmarks%0Aand%20CL%20strategies.%20CL%20does%20not%20necessarily%20need%20perfect%20predictive%20models%2C%20but%0Arather%20it%20can%20benefit%20from%20reliable%20predictive%20models.%20We%20believe%20our%20study%20on%0Acontinual%20calibration%20represents%20a%20first%20step%20towards%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07817v2&entry.124074799=Read"},
{"title": "A backward differential deep learning-based algorithm for solving\n  high-dimensional nonlinear backward stochastic differential equations", "author": "Lorenc Kapllani and Long Teng", "abstract": "  In this work, we propose a novel backward differential deep learning-based\nalgorithm for solving high-dimensional nonlinear backward stochastic\ndifferential equations (BSDEs), where the deep neural network (DNN) models are\ntrained not only on the inputs and labels but also the differentials of the\ncorresponding labels. This is motivated by the fact that differential deep\nlearning can provide an efficient approximation of the labels and their\nderivatives with respect to inputs. The BSDEs are reformulated as differential\ndeep learning problems by using Malliavin calculus. The Malliavin derivatives\nof solution to a BSDE satisfy themselves another BSDE, resulting thus in a\nsystem of BSDEs. Such formulation requires the estimation of the solution, its\ngradient, and the Hessian matrix, represented by the triple of processes\n$\\left(Y, Z, \\Gamma\\right).$ All the integrals within this system are\ndiscretized by using the Euler-Maruyama method. Subsequently, DNNs are employed\nto approximate the triple of these unknown processes. The DNN parameters are\nbackwardly optimized at each time step by minimizing a differential learning\ntype loss function, which is defined as a weighted sum of the dynamics of the\ndiscretized BSDE system, with the first term providing the dynamics of the\nprocess $Y$ and the other the process $Z$. An error analysis is carried out to\nshow the convergence of the proposed algorithm. Various numerical experiments\nup to $50$ dimensions are provided to demonstrate the high efficiency. Both\ntheoretically and numerically, it is demonstrated that our proposed scheme is\nmore efficient compared to other contemporary deep learning-based\nmethodologies, especially in the computation of the process $\\Gamma$.\n", "link": "http://arxiv.org/abs/2404.08456v1", "date": "2024-04-12", "relevancy": 1.9002, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5065}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4929}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4365}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20backward%20differential%20deep%20learning-based%20algorithm%20for%20solving%0A%20%20high-dimensional%20nonlinear%20backward%20stochastic%20differential%20equations&body=Title%3A%20A%20backward%20differential%20deep%20learning-based%20algorithm%20for%20solving%0A%20%20high-dimensional%20nonlinear%20backward%20stochastic%20differential%20equations%0AAuthor%3A%20Lorenc%20Kapllani%20and%20Long%20Teng%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20backward%20differential%20deep%20learning-based%0Aalgorithm%20for%20solving%20high-dimensional%20nonlinear%20backward%20stochastic%0Adifferential%20equations%20%28BSDEs%29%2C%20where%20the%20deep%20neural%20network%20%28DNN%29%20models%20are%0Atrained%20not%20only%20on%20the%20inputs%20and%20labels%20but%20also%20the%20differentials%20of%20the%0Acorresponding%20labels.%20This%20is%20motivated%20by%20the%20fact%20that%20differential%20deep%0Alearning%20can%20provide%20an%20efficient%20approximation%20of%20the%20labels%20and%20their%0Aderivatives%20with%20respect%20to%20inputs.%20The%20BSDEs%20are%20reformulated%20as%20differential%0Adeep%20learning%20problems%20by%20using%20Malliavin%20calculus.%20The%20Malliavin%20derivatives%0Aof%20solution%20to%20a%20BSDE%20satisfy%20themselves%20another%20BSDE%2C%20resulting%20thus%20in%20a%0Asystem%20of%20BSDEs.%20Such%20formulation%20requires%20the%20estimation%20of%20the%20solution%2C%20its%0Agradient%2C%20and%20the%20Hessian%20matrix%2C%20represented%20by%20the%20triple%20of%20processes%0A%24%5Cleft%28Y%2C%20Z%2C%20%5CGamma%5Cright%29.%24%20All%20the%20integrals%20within%20this%20system%20are%0Adiscretized%20by%20using%20the%20Euler-Maruyama%20method.%20Subsequently%2C%20DNNs%20are%20employed%0Ato%20approximate%20the%20triple%20of%20these%20unknown%20processes.%20The%20DNN%20parameters%20are%0Abackwardly%20optimized%20at%20each%20time%20step%20by%20minimizing%20a%20differential%20learning%0Atype%20loss%20function%2C%20which%20is%20defined%20as%20a%20weighted%20sum%20of%20the%20dynamics%20of%20the%0Adiscretized%20BSDE%20system%2C%20with%20the%20first%20term%20providing%20the%20dynamics%20of%20the%0Aprocess%20%24Y%24%20and%20the%20other%20the%20process%20%24Z%24.%20An%20error%20analysis%20is%20carried%20out%20to%0Ashow%20the%20convergence%20of%20the%20proposed%20algorithm.%20Various%20numerical%20experiments%0Aup%20to%20%2450%24%20dimensions%20are%20provided%20to%20demonstrate%20the%20high%20efficiency.%20Both%0Atheoretically%20and%20numerically%2C%20it%20is%20demonstrated%20that%20our%20proposed%20scheme%20is%0Amore%20efficient%20compared%20to%20other%20contemporary%20deep%20learning-based%0Amethodologies%2C%20especially%20in%20the%20computation%20of%20the%20process%20%24%5CGamma%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08456v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20backward%20differential%20deep%20learning-based%20algorithm%20for%20solving%0A%20%20high-dimensional%20nonlinear%20backward%20stochastic%20differential%20equations&entry.906535625=Lorenc%20Kapllani%20and%20Long%20Teng&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20backward%20differential%20deep%20learning-based%0Aalgorithm%20for%20solving%20high-dimensional%20nonlinear%20backward%20stochastic%0Adifferential%20equations%20%28BSDEs%29%2C%20where%20the%20deep%20neural%20network%20%28DNN%29%20models%20are%0Atrained%20not%20only%20on%20the%20inputs%20and%20labels%20but%20also%20the%20differentials%20of%20the%0Acorresponding%20labels.%20This%20is%20motivated%20by%20the%20fact%20that%20differential%20deep%0Alearning%20can%20provide%20an%20efficient%20approximation%20of%20the%20labels%20and%20their%0Aderivatives%20with%20respect%20to%20inputs.%20The%20BSDEs%20are%20reformulated%20as%20differential%0Adeep%20learning%20problems%20by%20using%20Malliavin%20calculus.%20The%20Malliavin%20derivatives%0Aof%20solution%20to%20a%20BSDE%20satisfy%20themselves%20another%20BSDE%2C%20resulting%20thus%20in%20a%0Asystem%20of%20BSDEs.%20Such%20formulation%20requires%20the%20estimation%20of%20the%20solution%2C%20its%0Agradient%2C%20and%20the%20Hessian%20matrix%2C%20represented%20by%20the%20triple%20of%20processes%0A%24%5Cleft%28Y%2C%20Z%2C%20%5CGamma%5Cright%29.%24%20All%20the%20integrals%20within%20this%20system%20are%0Adiscretized%20by%20using%20the%20Euler-Maruyama%20method.%20Subsequently%2C%20DNNs%20are%20employed%0Ato%20approximate%20the%20triple%20of%20these%20unknown%20processes.%20The%20DNN%20parameters%20are%0Abackwardly%20optimized%20at%20each%20time%20step%20by%20minimizing%20a%20differential%20learning%0Atype%20loss%20function%2C%20which%20is%20defined%20as%20a%20weighted%20sum%20of%20the%20dynamics%20of%20the%0Adiscretized%20BSDE%20system%2C%20with%20the%20first%20term%20providing%20the%20dynamics%20of%20the%0Aprocess%20%24Y%24%20and%20the%20other%20the%20process%20%24Z%24.%20An%20error%20analysis%20is%20carried%20out%20to%0Ashow%20the%20convergence%20of%20the%20proposed%20algorithm.%20Various%20numerical%20experiments%0Aup%20to%20%2450%24%20dimensions%20are%20provided%20to%20demonstrate%20the%20high%20efficiency.%20Both%0Atheoretically%20and%20numerically%2C%20it%20is%20demonstrated%20that%20our%20proposed%20scheme%20is%0Amore%20efficient%20compared%20to%20other%20contemporary%20deep%20learning-based%0Amethodologies%2C%20especially%20in%20the%20computation%20of%20the%20process%20%24%5CGamma%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08456v1&entry.124074799=Read"},
{"title": "Solving Parametric PDEs with Radial Basis Functions and Deep Neural\n  Networks", "author": "Guanhang Lei and Zhen Lei and Lei Shi and Chenyu Zeng", "abstract": "  We propose the POD-DNN, a novel algorithm leveraging deep neural networks\n(DNNs) along with radial basis functions (RBFs) in the context of the proper\northogonal decomposition (POD) reduced basis method (RBM), aimed at\napproximating the parametric mapping of parametric partial differential\nequations on irregular domains. The POD-DNN algorithm capitalizes on the\nlow-dimensional characteristics of the solution manifold for parametric\nequations, alongside the inherent offline-online computational strategy of RBM\nand DNNs. In numerical experiments, POD-DNN demonstrates significantly\naccelerated computation speeds during the online phase. Compared to other\nalgorithms that utilize RBF without integrating DNNs, POD-DNN substantially\nimproves the computational speed in the online inference process. Furthermore,\nunder reasonable assumptions, we have rigorously derived upper bounds on the\ncomplexity of approximating parametric mappings with POD-DNN, thereby providing\na theoretical analysis of the algorithm's empirical performance.\n", "link": "http://arxiv.org/abs/2404.06834v2", "date": "2024-04-12", "relevancy": 1.8954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4874}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Solving%20Parametric%20PDEs%20with%20Radial%20Basis%20Functions%20and%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20Solving%20Parametric%20PDEs%20with%20Radial%20Basis%20Functions%20and%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Guanhang%20Lei%20and%20Zhen%20Lei%20and%20Lei%20Shi%20and%20Chenyu%20Zeng%0AAbstract%3A%20%20%20We%20propose%20the%20POD-DNN%2C%20a%20novel%20algorithm%20leveraging%20deep%20neural%20networks%0A%28DNNs%29%20along%20with%20radial%20basis%20functions%20%28RBFs%29%20in%20the%20context%20of%20the%20proper%0Aorthogonal%20decomposition%20%28POD%29%20reduced%20basis%20method%20%28RBM%29%2C%20aimed%20at%0Aapproximating%20the%20parametric%20mapping%20of%20parametric%20partial%20differential%0Aequations%20on%20irregular%20domains.%20The%20POD-DNN%20algorithm%20capitalizes%20on%20the%0Alow-dimensional%20characteristics%20of%20the%20solution%20manifold%20for%20parametric%0Aequations%2C%20alongside%20the%20inherent%20offline-online%20computational%20strategy%20of%20RBM%0Aand%20DNNs.%20In%20numerical%20experiments%2C%20POD-DNN%20demonstrates%20significantly%0Aaccelerated%20computation%20speeds%20during%20the%20online%20phase.%20Compared%20to%20other%0Aalgorithms%20that%20utilize%20RBF%20without%20integrating%20DNNs%2C%20POD-DNN%20substantially%0Aimproves%20the%20computational%20speed%20in%20the%20online%20inference%20process.%20Furthermore%2C%0Aunder%20reasonable%20assumptions%2C%20we%20have%20rigorously%20derived%20upper%20bounds%20on%20the%0Acomplexity%20of%20approximating%20parametric%20mappings%20with%20POD-DNN%2C%20thereby%20providing%0Aa%20theoretical%20analysis%20of%20the%20algorithm%27s%20empirical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06834v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Parametric%20PDEs%20with%20Radial%20Basis%20Functions%20and%20Deep%20Neural%0A%20%20Networks&entry.906535625=Guanhang%20Lei%20and%20Zhen%20Lei%20and%20Lei%20Shi%20and%20Chenyu%20Zeng&entry.1292438233=%20%20We%20propose%20the%20POD-DNN%2C%20a%20novel%20algorithm%20leveraging%20deep%20neural%20networks%0A%28DNNs%29%20along%20with%20radial%20basis%20functions%20%28RBFs%29%20in%20the%20context%20of%20the%20proper%0Aorthogonal%20decomposition%20%28POD%29%20reduced%20basis%20method%20%28RBM%29%2C%20aimed%20at%0Aapproximating%20the%20parametric%20mapping%20of%20parametric%20partial%20differential%0Aequations%20on%20irregular%20domains.%20The%20POD-DNN%20algorithm%20capitalizes%20on%20the%0Alow-dimensional%20characteristics%20of%20the%20solution%20manifold%20for%20parametric%0Aequations%2C%20alongside%20the%20inherent%20offline-online%20computational%20strategy%20of%20RBM%0Aand%20DNNs.%20In%20numerical%20experiments%2C%20POD-DNN%20demonstrates%20significantly%0Aaccelerated%20computation%20speeds%20during%20the%20online%20phase.%20Compared%20to%20other%0Aalgorithms%20that%20utilize%20RBF%20without%20integrating%20DNNs%2C%20POD-DNN%20substantially%0Aimproves%20the%20computational%20speed%20in%20the%20online%20inference%20process.%20Furthermore%2C%0Aunder%20reasonable%20assumptions%2C%20we%20have%20rigorously%20derived%20upper%20bounds%20on%20the%0Acomplexity%20of%20approximating%20parametric%20mappings%20with%20POD-DNN%2C%20thereby%20providing%0Aa%20theoretical%20analysis%20of%20the%20algorithm%27s%20empirical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06834v2&entry.124074799=Read"},
{"title": "Safe Start Regions for Medical Steerable Needle Automation", "author": "Janine Hoelscher and Inbar Fried and Spiros Tsalikis and Jason Akulian and Robert J. Webster III and Ron Alterovitz", "abstract": "  Steerable needles are minimally invasive devices that enable novel medical\nprocedures by following curved paths to avoid critical anatomical obstacles.\nPlanning algorithms can be used to find a steerable needle motion plan to a\ntarget. Deployment typically consists of a physician manually inserting the\nsteerable needle into tissue at the motion plan's start pose and handing off\ncontrol to a robot, which then autonomously steers it to the target along the\nplan. The handoff between human and robot is critical for procedure success, as\neven small deviations from the start pose change the steerable needle's\nworkspace and there is no guarantee that the target will still be reachable. We\nintroduce a metric that evaluates the robustness to such start pose deviations.\nWhen measuring this robustness to deviations, we consider the tradeoff between\nbeing robust to changes in position versus changes in orientation. We evaluate\nour metric through simulation in an abstract, a liver, and a lung planning\nscenario. Our evaluation shows that our metric can be combined with different\nmotion planners and that it efficiently determines large, safe start regions.\n", "link": "http://arxiv.org/abs/2404.08558v1", "date": "2024-04-12", "relevancy": 1.8906, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.493}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4562}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20Start%20Regions%20for%20Medical%20Steerable%20Needle%20Automation&body=Title%3A%20Safe%20Start%20Regions%20for%20Medical%20Steerable%20Needle%20Automation%0AAuthor%3A%20Janine%20Hoelscher%20and%20Inbar%20Fried%20and%20Spiros%20Tsalikis%20and%20Jason%20Akulian%20and%20Robert%20J.%20Webster%20III%20and%20Ron%20Alterovitz%0AAbstract%3A%20%20%20Steerable%20needles%20are%20minimally%20invasive%20devices%20that%20enable%20novel%20medical%0Aprocedures%20by%20following%20curved%20paths%20to%20avoid%20critical%20anatomical%20obstacles.%0APlanning%20algorithms%20can%20be%20used%20to%20find%20a%20steerable%20needle%20motion%20plan%20to%20a%0Atarget.%20Deployment%20typically%20consists%20of%20a%20physician%20manually%20inserting%20the%0Asteerable%20needle%20into%20tissue%20at%20the%20motion%20plan%27s%20start%20pose%20and%20handing%20off%0Acontrol%20to%20a%20robot%2C%20which%20then%20autonomously%20steers%20it%20to%20the%20target%20along%20the%0Aplan.%20The%20handoff%20between%20human%20and%20robot%20is%20critical%20for%20procedure%20success%2C%20as%0Aeven%20small%20deviations%20from%20the%20start%20pose%20change%20the%20steerable%20needle%27s%0Aworkspace%20and%20there%20is%20no%20guarantee%20that%20the%20target%20will%20still%20be%20reachable.%20We%0Aintroduce%20a%20metric%20that%20evaluates%20the%20robustness%20to%20such%20start%20pose%20deviations.%0AWhen%20measuring%20this%20robustness%20to%20deviations%2C%20we%20consider%20the%20tradeoff%20between%0Abeing%20robust%20to%20changes%20in%20position%20versus%20changes%20in%20orientation.%20We%20evaluate%0Aour%20metric%20through%20simulation%20in%20an%20abstract%2C%20a%20liver%2C%20and%20a%20lung%20planning%0Ascenario.%20Our%20evaluation%20shows%20that%20our%20metric%20can%20be%20combined%20with%20different%0Amotion%20planners%20and%20that%20it%20efficiently%20determines%20large%2C%20safe%20start%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08558v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Start%20Regions%20for%20Medical%20Steerable%20Needle%20Automation&entry.906535625=Janine%20Hoelscher%20and%20Inbar%20Fried%20and%20Spiros%20Tsalikis%20and%20Jason%20Akulian%20and%20Robert%20J.%20Webster%20III%20and%20Ron%20Alterovitz&entry.1292438233=%20%20Steerable%20needles%20are%20minimally%20invasive%20devices%20that%20enable%20novel%20medical%0Aprocedures%20by%20following%20curved%20paths%20to%20avoid%20critical%20anatomical%20obstacles.%0APlanning%20algorithms%20can%20be%20used%20to%20find%20a%20steerable%20needle%20motion%20plan%20to%20a%0Atarget.%20Deployment%20typically%20consists%20of%20a%20physician%20manually%20inserting%20the%0Asteerable%20needle%20into%20tissue%20at%20the%20motion%20plan%27s%20start%20pose%20and%20handing%20off%0Acontrol%20to%20a%20robot%2C%20which%20then%20autonomously%20steers%20it%20to%20the%20target%20along%20the%0Aplan.%20The%20handoff%20between%20human%20and%20robot%20is%20critical%20for%20procedure%20success%2C%20as%0Aeven%20small%20deviations%20from%20the%20start%20pose%20change%20the%20steerable%20needle%27s%0Aworkspace%20and%20there%20is%20no%20guarantee%20that%20the%20target%20will%20still%20be%20reachable.%20We%0Aintroduce%20a%20metric%20that%20evaluates%20the%20robustness%20to%20such%20start%20pose%20deviations.%0AWhen%20measuring%20this%20robustness%20to%20deviations%2C%20we%20consider%20the%20tradeoff%20between%0Abeing%20robust%20to%20changes%20in%20position%20versus%20changes%20in%20orientation.%20We%20evaluate%0Aour%20metric%20through%20simulation%20in%20an%20abstract%2C%20a%20liver%2C%20and%20a%20lung%20planning%0Ascenario.%20Our%20evaluation%20shows%20that%20our%20metric%20can%20be%20combined%20with%20different%0Amotion%20planners%20and%20that%20it%20efficiently%20determines%20large%2C%20safe%20start%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08558v1&entry.124074799=Read"},
{"title": "Lossy Image Compression with Foundation Diffusion Models", "author": "Lucas Relic and Roberto Azevedo and Markus Gross and Christopher Schroers", "abstract": "  Incorporating diffusion models in the image compression domain has the\npotential to produce realistic and detailed reconstructions, especially at\nextremely low bitrates. Previous methods focus on using diffusion models as\nexpressive decoders robust to quantization errors in the conditioning signals,\nyet achieving competitive results in this manner requires costly training of\nthe diffusion model and long inference times due to the iterative generative\nprocess. In this work we formulate the removal of quantization error as a\ndenoising task, using diffusion to recover lost information in the transmitted\nimage latent. Our approach allows us to perform less than 10\\% of the full\ndiffusion generative process and requires no architectural changes to the\ndiffusion model, enabling the use of foundation models as a strong prior\nwithout additional fine tuning of the backbone. Our proposed codec outperforms\nprevious methods in quantitative realism metrics, and we verify that our\nreconstructions are qualitatively preferred by end users, even when other\nmethods use twice the bitrate.\n", "link": "http://arxiv.org/abs/2404.08580v1", "date": "2024-04-12", "relevancy": 1.8837, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6918}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6154}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5954}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lossy%20Image%20Compression%20with%20Foundation%20Diffusion%20Models&body=Title%3A%20Lossy%20Image%20Compression%20with%20Foundation%20Diffusion%20Models%0AAuthor%3A%20Lucas%20Relic%20and%20Roberto%20Azevedo%20and%20Markus%20Gross%20and%20Christopher%20Schroers%0AAbstract%3A%20%20%20Incorporating%20diffusion%20models%20in%20the%20image%20compression%20domain%20has%20the%0Apotential%20to%20produce%20realistic%20and%20detailed%20reconstructions%2C%20especially%20at%0Aextremely%20low%20bitrates.%20Previous%20methods%20focus%20on%20using%20diffusion%20models%20as%0Aexpressive%20decoders%20robust%20to%20quantization%20errors%20in%20the%20conditioning%20signals%2C%0Ayet%20achieving%20competitive%20results%20in%20this%20manner%20requires%20costly%20training%20of%0Athe%20diffusion%20model%20and%20long%20inference%20times%20due%20to%20the%20iterative%20generative%0Aprocess.%20In%20this%20work%20we%20formulate%20the%20removal%20of%20quantization%20error%20as%20a%0Adenoising%20task%2C%20using%20diffusion%20to%20recover%20lost%20information%20in%20the%20transmitted%0Aimage%20latent.%20Our%20approach%20allows%20us%20to%20perform%20less%20than%2010%5C%25%20of%20the%20full%0Adiffusion%20generative%20process%20and%20requires%20no%20architectural%20changes%20to%20the%0Adiffusion%20model%2C%20enabling%20the%20use%20of%20foundation%20models%20as%20a%20strong%20prior%0Awithout%20additional%20fine%20tuning%20of%20the%20backbone.%20Our%20proposed%20codec%20outperforms%0Aprevious%20methods%20in%20quantitative%20realism%20metrics%2C%20and%20we%20verify%20that%20our%0Areconstructions%20are%20qualitatively%20preferred%20by%20end%20users%2C%20even%20when%20other%0Amethods%20use%20twice%20the%20bitrate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08580v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossy%20Image%20Compression%20with%20Foundation%20Diffusion%20Models&entry.906535625=Lucas%20Relic%20and%20Roberto%20Azevedo%20and%20Markus%20Gross%20and%20Christopher%20Schroers&entry.1292438233=%20%20Incorporating%20diffusion%20models%20in%20the%20image%20compression%20domain%20has%20the%0Apotential%20to%20produce%20realistic%20and%20detailed%20reconstructions%2C%20especially%20at%0Aextremely%20low%20bitrates.%20Previous%20methods%20focus%20on%20using%20diffusion%20models%20as%0Aexpressive%20decoders%20robust%20to%20quantization%20errors%20in%20the%20conditioning%20signals%2C%0Ayet%20achieving%20competitive%20results%20in%20this%20manner%20requires%20costly%20training%20of%0Athe%20diffusion%20model%20and%20long%20inference%20times%20due%20to%20the%20iterative%20generative%0Aprocess.%20In%20this%20work%20we%20formulate%20the%20removal%20of%20quantization%20error%20as%20a%0Adenoising%20task%2C%20using%20diffusion%20to%20recover%20lost%20information%20in%20the%20transmitted%0Aimage%20latent.%20Our%20approach%20allows%20us%20to%20perform%20less%20than%2010%5C%25%20of%20the%20full%0Adiffusion%20generative%20process%20and%20requires%20no%20architectural%20changes%20to%20the%0Adiffusion%20model%2C%20enabling%20the%20use%20of%20foundation%20models%20as%20a%20strong%20prior%0Awithout%20additional%20fine%20tuning%20of%20the%20backbone.%20Our%20proposed%20codec%20outperforms%0Aprevious%20methods%20in%20quantitative%20realism%20metrics%2C%20and%20we%20verify%20that%20our%0Areconstructions%20are%20qualitatively%20preferred%20by%20end%20users%2C%20even%20when%20other%0Amethods%20use%20twice%20the%20bitrate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08580v1&entry.124074799=Read"},
{"title": "Efficient Masked Face Recognition Method during the COVID-19 Pandemic", "author": "Walid Hariri", "abstract": "  The coronavirus disease (COVID-19) is an unparalleled crisis leading to a\nhuge number of casualties and security problems. In order to reduce the spread\nof coronavirus, people often wear masks to protect themselves. This makes face\nrecognition a very difficult task since certain parts of the face are hidden. A\nprimary focus of researchers during the ongoing coronavirus pandemic is to come\nup with suggestions to handle this problem through rapid and efficient\nsolutions. In this paper, we propose a reliable method based on occlusion\nremoval and deep learning-based features in order to address the problem of the\nmasked face recognition process. The first step is to remove the masked face\nregion. Next, we apply three pre-trained deep Convolutional Neural Networks\n(CNN) namely, VGG-16, AlexNet, and ResNet-50, and use them to extract deep\nfeatures from the obtained regions (mostly eyes and forehead regions). The\nBag-of-features paradigm is then applied to the feature maps of the last\nconvolutional layer in order to quantize them and to get a slight\nrepresentation comparing to the fully connected layer of classical CNN.\nFinally, Multilayer Perceptron (MLP) is applied for the classification process.\nExperimental results on Real-World-Masked-Face-Dataset show high recognition\nperformance compared to other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2105.03026v2", "date": "2024-04-12", "relevancy": 1.8834, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4862}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4602}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4599}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Masked%20Face%20Recognition%20Method%20during%20the%20COVID-19%20Pandemic&body=Title%3A%20Efficient%20Masked%20Face%20Recognition%20Method%20during%20the%20COVID-19%20Pandemic%0AAuthor%3A%20Walid%20Hariri%0AAbstract%3A%20%20%20The%20coronavirus%20disease%20%28COVID-19%29%20is%20an%20unparalleled%20crisis%20leading%20to%20a%0Ahuge%20number%20of%20casualties%20and%20security%20problems.%20In%20order%20to%20reduce%20the%20spread%0Aof%20coronavirus%2C%20people%20often%20wear%20masks%20to%20protect%20themselves.%20This%20makes%20face%0Arecognition%20a%20very%20difficult%20task%20since%20certain%20parts%20of%20the%20face%20are%20hidden.%20A%0Aprimary%20focus%20of%20researchers%20during%20the%20ongoing%20coronavirus%20pandemic%20is%20to%20come%0Aup%20with%20suggestions%20to%20handle%20this%20problem%20through%20rapid%20and%20efficient%0Asolutions.%20In%20this%20paper%2C%20we%20propose%20a%20reliable%20method%20based%20on%20occlusion%0Aremoval%20and%20deep%20learning-based%20features%20in%20order%20to%20address%20the%20problem%20of%20the%0Amasked%20face%20recognition%20process.%20The%20first%20step%20is%20to%20remove%20the%20masked%20face%0Aregion.%20Next%2C%20we%20apply%20three%20pre-trained%20deep%20Convolutional%20Neural%20Networks%0A%28CNN%29%20namely%2C%20VGG-16%2C%20AlexNet%2C%20and%20ResNet-50%2C%20and%20use%20them%20to%20extract%20deep%0Afeatures%20from%20the%20obtained%20regions%20%28mostly%20eyes%20and%20forehead%20regions%29.%20The%0ABag-of-features%20paradigm%20is%20then%20applied%20to%20the%20feature%20maps%20of%20the%20last%0Aconvolutional%20layer%20in%20order%20to%20quantize%20them%20and%20to%20get%20a%20slight%0Arepresentation%20comparing%20to%20the%20fully%20connected%20layer%20of%20classical%20CNN.%0AFinally%2C%20Multilayer%20Perceptron%20%28MLP%29%20is%20applied%20for%20the%20classification%20process.%0AExperimental%20results%20on%20Real-World-Masked-Face-Dataset%20show%20high%20recognition%0Aperformance%20compared%20to%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2105.03026v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Masked%20Face%20Recognition%20Method%20during%20the%20COVID-19%20Pandemic&entry.906535625=Walid%20Hariri&entry.1292438233=%20%20The%20coronavirus%20disease%20%28COVID-19%29%20is%20an%20unparalleled%20crisis%20leading%20to%20a%0Ahuge%20number%20of%20casualties%20and%20security%20problems.%20In%20order%20to%20reduce%20the%20spread%0Aof%20coronavirus%2C%20people%20often%20wear%20masks%20to%20protect%20themselves.%20This%20makes%20face%0Arecognition%20a%20very%20difficult%20task%20since%20certain%20parts%20of%20the%20face%20are%20hidden.%20A%0Aprimary%20focus%20of%20researchers%20during%20the%20ongoing%20coronavirus%20pandemic%20is%20to%20come%0Aup%20with%20suggestions%20to%20handle%20this%20problem%20through%20rapid%20and%20efficient%0Asolutions.%20In%20this%20paper%2C%20we%20propose%20a%20reliable%20method%20based%20on%20occlusion%0Aremoval%20and%20deep%20learning-based%20features%20in%20order%20to%20address%20the%20problem%20of%20the%0Amasked%20face%20recognition%20process.%20The%20first%20step%20is%20to%20remove%20the%20masked%20face%0Aregion.%20Next%2C%20we%20apply%20three%20pre-trained%20deep%20Convolutional%20Neural%20Networks%0A%28CNN%29%20namely%2C%20VGG-16%2C%20AlexNet%2C%20and%20ResNet-50%2C%20and%20use%20them%20to%20extract%20deep%0Afeatures%20from%20the%20obtained%20regions%20%28mostly%20eyes%20and%20forehead%20regions%29.%20The%0ABag-of-features%20paradigm%20is%20then%20applied%20to%20the%20feature%20maps%20of%20the%20last%0Aconvolutional%20layer%20in%20order%20to%20quantize%20them%20and%20to%20get%20a%20slight%0Arepresentation%20comparing%20to%20the%20fully%20connected%20layer%20of%20classical%20CNN.%0AFinally%2C%20Multilayer%20Perceptron%20%28MLP%29%20is%20applied%20for%20the%20classification%20process.%0AExperimental%20results%20on%20Real-World-Masked-Face-Dataset%20show%20high%20recognition%0Aperformance%20compared%20to%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2105.03026v2&entry.124074799=Read"},
{"title": "Adversarially Robust Spiking Neural Networks Through Conversion", "author": "Ozan \u00d6zdenizci and Robert Legenstein", "abstract": "  Spiking neural networks (SNNs) provide an energy-efficient alternative to a\nvariety of artificial neural network (ANN) based AI applications. As the\nprogress in neuromorphic computing with SNNs expands their use in applications,\nthe problem of adversarial robustness of SNNs becomes more pronounced. To the\ncontrary of the widely explored end-to-end adversarial training based\nsolutions, we address the limited progress in scalable robust SNN training\nmethods by proposing an adversarially robust ANN-to-SNN conversion algorithm.\nOur method provides an efficient approach to embrace various computationally\ndemanding robust learning objectives that have been proposed for ANNs. During a\npost-conversion robust finetuning phase, our method adversarially optimizes\nboth layer-wise firing thresholds and synaptic connectivity weights of the SNN\nto maintain transferred robustness gains from the pre-trained ANN. We perform\nexperimental evaluations in a novel setting proposed to rigorously assess the\nrobustness of SNNs, where numerous adaptive adversarial attacks that account\nfor the spike-based operation dynamics are considered. Results show that our\napproach yields a scalable state-of-the-art solution for adversarially robust\ndeep SNNs with low-latency.\n", "link": "http://arxiv.org/abs/2311.09266v2", "date": "2024-04-12", "relevancy": 1.8823, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5232}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4324}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarially%20Robust%20Spiking%20Neural%20Networks%20Through%20Conversion&body=Title%3A%20Adversarially%20Robust%20Spiking%20Neural%20Networks%20Through%20Conversion%0AAuthor%3A%20Ozan%20%C3%96zdenizci%20and%20Robert%20Legenstein%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20provide%20an%20energy-efficient%20alternative%20to%20a%0Avariety%20of%20artificial%20neural%20network%20%28ANN%29%20based%20AI%20applications.%20As%20the%0Aprogress%20in%20neuromorphic%20computing%20with%20SNNs%20expands%20their%20use%20in%20applications%2C%0Athe%20problem%20of%20adversarial%20robustness%20of%20SNNs%20becomes%20more%20pronounced.%20To%20the%0Acontrary%20of%20the%20widely%20explored%20end-to-end%20adversarial%20training%20based%0Asolutions%2C%20we%20address%20the%20limited%20progress%20in%20scalable%20robust%20SNN%20training%0Amethods%20by%20proposing%20an%20adversarially%20robust%20ANN-to-SNN%20conversion%20algorithm.%0AOur%20method%20provides%20an%20efficient%20approach%20to%20embrace%20various%20computationally%0Ademanding%20robust%20learning%20objectives%20that%20have%20been%20proposed%20for%20ANNs.%20During%20a%0Apost-conversion%20robust%20finetuning%20phase%2C%20our%20method%20adversarially%20optimizes%0Aboth%20layer-wise%20firing%20thresholds%20and%20synaptic%20connectivity%20weights%20of%20the%20SNN%0Ato%20maintain%20transferred%20robustness%20gains%20from%20the%20pre-trained%20ANN.%20We%20perform%0Aexperimental%20evaluations%20in%20a%20novel%20setting%20proposed%20to%20rigorously%20assess%20the%0Arobustness%20of%20SNNs%2C%20where%20numerous%20adaptive%20adversarial%20attacks%20that%20account%0Afor%20the%20spike-based%20operation%20dynamics%20are%20considered.%20Results%20show%20that%20our%0Aapproach%20yields%20a%20scalable%20state-of-the-art%20solution%20for%20adversarially%20robust%0Adeep%20SNNs%20with%20low-latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09266v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarially%20Robust%20Spiking%20Neural%20Networks%20Through%20Conversion&entry.906535625=Ozan%20%C3%96zdenizci%20and%20Robert%20Legenstein&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20provide%20an%20energy-efficient%20alternative%20to%20a%0Avariety%20of%20artificial%20neural%20network%20%28ANN%29%20based%20AI%20applications.%20As%20the%0Aprogress%20in%20neuromorphic%20computing%20with%20SNNs%20expands%20their%20use%20in%20applications%2C%0Athe%20problem%20of%20adversarial%20robustness%20of%20SNNs%20becomes%20more%20pronounced.%20To%20the%0Acontrary%20of%20the%20widely%20explored%20end-to-end%20adversarial%20training%20based%0Asolutions%2C%20we%20address%20the%20limited%20progress%20in%20scalable%20robust%20SNN%20training%0Amethods%20by%20proposing%20an%20adversarially%20robust%20ANN-to-SNN%20conversion%20algorithm.%0AOur%20method%20provides%20an%20efficient%20approach%20to%20embrace%20various%20computationally%0Ademanding%20robust%20learning%20objectives%20that%20have%20been%20proposed%20for%20ANNs.%20During%20a%0Apost-conversion%20robust%20finetuning%20phase%2C%20our%20method%20adversarially%20optimizes%0Aboth%20layer-wise%20firing%20thresholds%20and%20synaptic%20connectivity%20weights%20of%20the%20SNN%0Ato%20maintain%20transferred%20robustness%20gains%20from%20the%20pre-trained%20ANN.%20We%20perform%0Aexperimental%20evaluations%20in%20a%20novel%20setting%20proposed%20to%20rigorously%20assess%20the%0Arobustness%20of%20SNNs%2C%20where%20numerous%20adaptive%20adversarial%20attacks%20that%20account%0Afor%20the%20spike-based%20operation%20dynamics%20are%20considered.%20Results%20show%20that%20our%0Aapproach%20yields%20a%20scalable%20state-of-the-art%20solution%20for%20adversarially%20robust%0Adeep%20SNNs%20with%20low-latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09266v2&entry.124074799=Read"},
{"title": "Data-Driven Preference Sampling for Pareto Front Learning", "author": "Rongguang Ye and Lei Chen and Weiduo Liao and Jinyuan Zhang and Hisao Ishibuchi", "abstract": "  Pareto front learning is a technique that introduces preference vectors in a\nneural network to approximate the Pareto front. Previous Pareto front learning\nmethods have demonstrated high performance in approximating simple Pareto\nfronts. These methods often sample preference vectors from a fixed Dirichlet\ndistribution. However, no fixed sampling distribution can be adapted to diverse\nPareto fronts. Efficiently sampling preference vectors and accurately\nestimating the Pareto front is a challenge. To address this challenge, we\npropose a data-driven preference vector sampling framework for Pareto front\nlearning. We utilize the posterior information of the objective functions to\nadjust the parameters of the sampling distribution flexibly. In this manner,\nthe proposed method can sample preference vectors from the location of the\nPareto front with a high probability. Moreover, we design the distribution of\nthe preference vector as a mixture of Dirichlet distributions to improve the\nperformance of the model in disconnected Pareto fronts. Extensive experiments\nvalidate the superiority of the proposed method compared with state-of-the-art\nalgorithms.\n", "link": "http://arxiv.org/abs/2404.08397v1", "date": "2024-04-12", "relevancy": 1.8781, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4708}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4569}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Preference%20Sampling%20for%20Pareto%20Front%20Learning&body=Title%3A%20Data-Driven%20Preference%20Sampling%20for%20Pareto%20Front%20Learning%0AAuthor%3A%20Rongguang%20Ye%20and%20Lei%20Chen%20and%20Weiduo%20Liao%20and%20Jinyuan%20Zhang%20and%20Hisao%20Ishibuchi%0AAbstract%3A%20%20%20Pareto%20front%20learning%20is%20a%20technique%20that%20introduces%20preference%20vectors%20in%20a%0Aneural%20network%20to%20approximate%20the%20Pareto%20front.%20Previous%20Pareto%20front%20learning%0Amethods%20have%20demonstrated%20high%20performance%20in%20approximating%20simple%20Pareto%0Afronts.%20These%20methods%20often%20sample%20preference%20vectors%20from%20a%20fixed%20Dirichlet%0Adistribution.%20However%2C%20no%20fixed%20sampling%20distribution%20can%20be%20adapted%20to%20diverse%0APareto%20fronts.%20Efficiently%20sampling%20preference%20vectors%20and%20accurately%0Aestimating%20the%20Pareto%20front%20is%20a%20challenge.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20data-driven%20preference%20vector%20sampling%20framework%20for%20Pareto%20front%0Alearning.%20We%20utilize%20the%20posterior%20information%20of%20the%20objective%20functions%20to%0Aadjust%20the%20parameters%20of%20the%20sampling%20distribution%20flexibly.%20In%20this%20manner%2C%0Athe%20proposed%20method%20can%20sample%20preference%20vectors%20from%20the%20location%20of%20the%0APareto%20front%20with%20a%20high%20probability.%20Moreover%2C%20we%20design%20the%20distribution%20of%0Athe%20preference%20vector%20as%20a%20mixture%20of%20Dirichlet%20distributions%20to%20improve%20the%0Aperformance%20of%20the%20model%20in%20disconnected%20Pareto%20fronts.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20the%20proposed%20method%20compared%20with%20state-of-the-art%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08397v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Preference%20Sampling%20for%20Pareto%20Front%20Learning&entry.906535625=Rongguang%20Ye%20and%20Lei%20Chen%20and%20Weiduo%20Liao%20and%20Jinyuan%20Zhang%20and%20Hisao%20Ishibuchi&entry.1292438233=%20%20Pareto%20front%20learning%20is%20a%20technique%20that%20introduces%20preference%20vectors%20in%20a%0Aneural%20network%20to%20approximate%20the%20Pareto%20front.%20Previous%20Pareto%20front%20learning%0Amethods%20have%20demonstrated%20high%20performance%20in%20approximating%20simple%20Pareto%0Afronts.%20These%20methods%20often%20sample%20preference%20vectors%20from%20a%20fixed%20Dirichlet%0Adistribution.%20However%2C%20no%20fixed%20sampling%20distribution%20can%20be%20adapted%20to%20diverse%0APareto%20fronts.%20Efficiently%20sampling%20preference%20vectors%20and%20accurately%0Aestimating%20the%20Pareto%20front%20is%20a%20challenge.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20data-driven%20preference%20vector%20sampling%20framework%20for%20Pareto%20front%0Alearning.%20We%20utilize%20the%20posterior%20information%20of%20the%20objective%20functions%20to%0Aadjust%20the%20parameters%20of%20the%20sampling%20distribution%20flexibly.%20In%20this%20manner%2C%0Athe%20proposed%20method%20can%20sample%20preference%20vectors%20from%20the%20location%20of%20the%0APareto%20front%20with%20a%20high%20probability.%20Moreover%2C%20we%20design%20the%20distribution%20of%0Athe%20preference%20vector%20as%20a%20mixture%20of%20Dirichlet%20distributions%20to%20improve%20the%0Aperformance%20of%20the%20model%20in%20disconnected%20Pareto%20fronts.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20the%20proposed%20method%20compared%20with%20state-of-the-art%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08397v1&entry.124074799=Read"},
{"title": "Incremental Learning with Concept Drift Detection and Prototype-based\n  Embeddings for Graph Stream Classification", "author": "Kleanthis Malialis and Jin Li and Christos G. Panayiotou and Marios M. Polycarpou", "abstract": "  Data stream mining aims at extracting meaningful knowledge from continually\nevolving data streams, addressing the challenges posed by nonstationary\nenvironments, particularly, concept drift which refers to a change in the\nunderlying data distribution over time. Graph structures offer a powerful\nmodelling tool to represent complex systems, such as, critical infrastructure\nsystems and social networks. Learning from graph streams becomes a necessity to\nunderstand the dynamics of graph structures and to facilitate informed\ndecision-making. This work introduces a novel method for graph stream\nclassification which operates under the general setting where a data generating\nprocess produces graphs with varying nodes and edges over time. The method uses\nincremental learning for continual model adaptation, selecting representative\ngraphs (prototypes) for each class, and creating graph embeddings.\nAdditionally, it incorporates a loss-based concept drift detection mechanism to\nrecalculate graph prototypes when drift is detected.\n", "link": "http://arxiv.org/abs/2404.02572v2", "date": "2024-04-12", "relevancy": 1.8718, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4631}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Incremental%20Learning%20with%20Concept%20Drift%20Detection%20and%20Prototype-based%0A%20%20Embeddings%20for%20Graph%20Stream%20Classification&body=Title%3A%20Incremental%20Learning%20with%20Concept%20Drift%20Detection%20and%20Prototype-based%0A%20%20Embeddings%20for%20Graph%20Stream%20Classification%0AAuthor%3A%20Kleanthis%20Malialis%20and%20Jin%20Li%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou%0AAbstract%3A%20%20%20Data%20stream%20mining%20aims%20at%20extracting%20meaningful%20knowledge%20from%20continually%0Aevolving%20data%20streams%2C%20addressing%20the%20challenges%20posed%20by%20nonstationary%0Aenvironments%2C%20particularly%2C%20concept%20drift%20which%20refers%20to%20a%20change%20in%20the%0Aunderlying%20data%20distribution%20over%20time.%20Graph%20structures%20offer%20a%20powerful%0Amodelling%20tool%20to%20represent%20complex%20systems%2C%20such%20as%2C%20critical%20infrastructure%0Asystems%20and%20social%20networks.%20Learning%20from%20graph%20streams%20becomes%20a%20necessity%20to%0Aunderstand%20the%20dynamics%20of%20graph%20structures%20and%20to%20facilitate%20informed%0Adecision-making.%20This%20work%20introduces%20a%20novel%20method%20for%20graph%20stream%0Aclassification%20which%20operates%20under%20the%20general%20setting%20where%20a%20data%20generating%0Aprocess%20produces%20graphs%20with%20varying%20nodes%20and%20edges%20over%20time.%20The%20method%20uses%0Aincremental%20learning%20for%20continual%20model%20adaptation%2C%20selecting%20representative%0Agraphs%20%28prototypes%29%20for%20each%20class%2C%20and%20creating%20graph%20embeddings.%0AAdditionally%2C%20it%20incorporates%20a%20loss-based%20concept%20drift%20detection%20mechanism%20to%0Arecalculate%20graph%20prototypes%20when%20drift%20is%20detected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02572v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Learning%20with%20Concept%20Drift%20Detection%20and%20Prototype-based%0A%20%20Embeddings%20for%20Graph%20Stream%20Classification&entry.906535625=Kleanthis%20Malialis%20and%20Jin%20Li%20and%20Christos%20G.%20Panayiotou%20and%20Marios%20M.%20Polycarpou&entry.1292438233=%20%20Data%20stream%20mining%20aims%20at%20extracting%20meaningful%20knowledge%20from%20continually%0Aevolving%20data%20streams%2C%20addressing%20the%20challenges%20posed%20by%20nonstationary%0Aenvironments%2C%20particularly%2C%20concept%20drift%20which%20refers%20to%20a%20change%20in%20the%0Aunderlying%20data%20distribution%20over%20time.%20Graph%20structures%20offer%20a%20powerful%0Amodelling%20tool%20to%20represent%20complex%20systems%2C%20such%20as%2C%20critical%20infrastructure%0Asystems%20and%20social%20networks.%20Learning%20from%20graph%20streams%20becomes%20a%20necessity%20to%0Aunderstand%20the%20dynamics%20of%20graph%20structures%20and%20to%20facilitate%20informed%0Adecision-making.%20This%20work%20introduces%20a%20novel%20method%20for%20graph%20stream%0Aclassification%20which%20operates%20under%20the%20general%20setting%20where%20a%20data%20generating%0Aprocess%20produces%20graphs%20with%20varying%20nodes%20and%20edges%20over%20time.%20The%20method%20uses%0Aincremental%20learning%20for%20continual%20model%20adaptation%2C%20selecting%20representative%0Agraphs%20%28prototypes%29%20for%20each%20class%2C%20and%20creating%20graph%20embeddings.%0AAdditionally%2C%20it%20incorporates%20a%20loss-based%20concept%20drift%20detection%20mechanism%20to%0Arecalculate%20graph%20prototypes%20when%20drift%20is%20detected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02572v2&entry.124074799=Read"},
{"title": "Complexity of Probabilistic Reasoning for Neurosymbolic Classification\n  Techniques", "author": "Arthur Ledaguenel and C\u00e9line Hudelot and Mostepha Khouadjia", "abstract": "  Neurosymbolic artificial intelligence is a growing field of research aiming\nto combine neural network learning capabilities with the reasoning abilities of\nsymbolic systems. Informed multi-label classification is a sub-field of\nneurosymbolic AI which studies how to leverage prior knowledge to improve\nneural classification systems. A well known family of neurosymbolic techniques\nfor informed classification use probabilistic reasoning to integrate this\nknowledge during learning, inference or both. Therefore, the asymptotic\ncomplexity of probabilistic reasoning is of cardinal importance to assess the\nscalability of such techniques. However, this topic is rarely tackled in the\nneurosymbolic literature, which can lead to a poor understanding of the limits\nof probabilistic neurosymbolic techniques. In this paper, we introduce a\nformalism for informed supervised classification tasks and techniques. We then\nbuild upon this formalism to define three abstract neurosymbolic techniques\nbased on probabilistic reasoning. Finally, we show computational complexity\nresults on several representation languages for prior knowledge commonly found\nin the neurosymbolic literature.\n", "link": "http://arxiv.org/abs/2404.08404v1", "date": "2024-04-12", "relevancy": 1.8697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4205}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Complexity%20of%20Probabilistic%20Reasoning%20for%20Neurosymbolic%20Classification%0A%20%20Techniques&body=Title%3A%20Complexity%20of%20Probabilistic%20Reasoning%20for%20Neurosymbolic%20Classification%0A%20%20Techniques%0AAuthor%3A%20Arthur%20Ledaguenel%20and%20C%C3%A9line%20Hudelot%20and%20Mostepha%20Khouadjia%0AAbstract%3A%20%20%20Neurosymbolic%20artificial%20intelligence%20is%20a%20growing%20field%20of%20research%20aiming%0Ato%20combine%20neural%20network%20learning%20capabilities%20with%20the%20reasoning%20abilities%20of%0Asymbolic%20systems.%20Informed%20multi-label%20classification%20is%20a%20sub-field%20of%0Aneurosymbolic%20AI%20which%20studies%20how%20to%20leverage%20prior%20knowledge%20to%20improve%0Aneural%20classification%20systems.%20A%20well%20known%20family%20of%20neurosymbolic%20techniques%0Afor%20informed%20classification%20use%20probabilistic%20reasoning%20to%20integrate%20this%0Aknowledge%20during%20learning%2C%20inference%20or%20both.%20Therefore%2C%20the%20asymptotic%0Acomplexity%20of%20probabilistic%20reasoning%20is%20of%20cardinal%20importance%20to%20assess%20the%0Ascalability%20of%20such%20techniques.%20However%2C%20this%20topic%20is%20rarely%20tackled%20in%20the%0Aneurosymbolic%20literature%2C%20which%20can%20lead%20to%20a%20poor%20understanding%20of%20the%20limits%0Aof%20probabilistic%20neurosymbolic%20techniques.%20In%20this%20paper%2C%20we%20introduce%20a%0Aformalism%20for%20informed%20supervised%20classification%20tasks%20and%20techniques.%20We%20then%0Abuild%20upon%20this%20formalism%20to%20define%20three%20abstract%20neurosymbolic%20techniques%0Abased%20on%20probabilistic%20reasoning.%20Finally%2C%20we%20show%20computational%20complexity%0Aresults%20on%20several%20representation%20languages%20for%20prior%20knowledge%20commonly%20found%0Ain%20the%20neurosymbolic%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08404v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complexity%20of%20Probabilistic%20Reasoning%20for%20Neurosymbolic%20Classification%0A%20%20Techniques&entry.906535625=Arthur%20Ledaguenel%20and%20C%C3%A9line%20Hudelot%20and%20Mostepha%20Khouadjia&entry.1292438233=%20%20Neurosymbolic%20artificial%20intelligence%20is%20a%20growing%20field%20of%20research%20aiming%0Ato%20combine%20neural%20network%20learning%20capabilities%20with%20the%20reasoning%20abilities%20of%0Asymbolic%20systems.%20Informed%20multi-label%20classification%20is%20a%20sub-field%20of%0Aneurosymbolic%20AI%20which%20studies%20how%20to%20leverage%20prior%20knowledge%20to%20improve%0Aneural%20classification%20systems.%20A%20well%20known%20family%20of%20neurosymbolic%20techniques%0Afor%20informed%20classification%20use%20probabilistic%20reasoning%20to%20integrate%20this%0Aknowledge%20during%20learning%2C%20inference%20or%20both.%20Therefore%2C%20the%20asymptotic%0Acomplexity%20of%20probabilistic%20reasoning%20is%20of%20cardinal%20importance%20to%20assess%20the%0Ascalability%20of%20such%20techniques.%20However%2C%20this%20topic%20is%20rarely%20tackled%20in%20the%0Aneurosymbolic%20literature%2C%20which%20can%20lead%20to%20a%20poor%20understanding%20of%20the%20limits%0Aof%20probabilistic%20neurosymbolic%20techniques.%20In%20this%20paper%2C%20we%20introduce%20a%0Aformalism%20for%20informed%20supervised%20classification%20tasks%20and%20techniques.%20We%20then%0Abuild%20upon%20this%20formalism%20to%20define%20three%20abstract%20neurosymbolic%20techniques%0Abased%20on%20probabilistic%20reasoning.%20Finally%2C%20we%20show%20computational%20complexity%0Aresults%20on%20several%20representation%20languages%20for%20prior%20knowledge%20commonly%20found%0Ain%20the%20neurosymbolic%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08404v1&entry.124074799=Read"},
{"title": "Dataset Reset Policy Optimization for RLHF", "author": "Jonathan D. Chang and Wenhao Shan and Owen Oertell and Kiant\u00e9 Brantley and Dipendra Misra and Jason D. Lee and Wen Sun", "abstract": "  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n", "link": "http://arxiv.org/abs/2404.08495v1", "date": "2024-04-12", "relevancy": 1.8644, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dataset%20Reset%20Policy%20Optimization%20for%20RLHF&body=Title%3A%20Dataset%20Reset%20Policy%20Optimization%20for%20RLHF%0AAuthor%3A%20Jonathan%20D.%20Chang%20and%20Wenhao%20Shan%20and%20Owen%20Oertell%20and%20Kiant%C3%A9%20Brantley%20and%20Dipendra%20Misra%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20from%20Human%20Preference-based%20feedback%20is%20a%20popular%0Aparadigm%20for%20fine-tuning%20generative%20models%2C%20which%20has%20produced%20impressive%0Amodels%20such%20as%20GPT-4%20and%20Claude3%20Opus.%20This%20framework%20often%20consists%20of%20two%0Asteps%3A%20learning%20a%20reward%20model%20from%20an%20offline%20preference%20dataset%20followed%20by%0Arunning%20online%20RL%20to%20optimize%20the%20learned%20reward%20model.%20In%20this%20work%2C%0Aleveraging%20the%20idea%20of%20reset%2C%20we%20propose%20a%20new%20RLHF%20algorithm%20with%20provable%0Aguarantees.%20Motivated%20by%20the%20fact%20that%20offline%20preference%20dataset%20provides%0Ainformative%20states%20%28i.e.%2C%20data%20that%20is%20preferred%20by%20the%20labelers%29%2C%20our%20new%0Aalgorithm%2C%20Dataset%20Reset%20Policy%20Optimization%20%28DR-PO%29%2C%20integrates%20the%20existing%0Aoffline%20preference%20dataset%20into%20the%20online%20policy%20training%20procedure%20via%0Adataset%20reset%3A%20it%20directly%20resets%20the%20policy%20optimizer%20to%20the%20states%20in%20the%0Aoffline%20dataset%2C%20instead%20of%20always%20starting%20from%20the%20initial%20state%0Adistribution.%20In%20theory%2C%20we%20show%20that%20DR-PO%20learns%20to%20perform%20at%20least%20as%20good%0Aas%20any%20policy%20that%20is%20covered%20by%20the%20offline%20dataset%20under%20general%20function%0Aapproximation%20with%20finite%20sample%20complexity.%20In%20experiments%2C%20we%20demonstrate%0Athat%20on%20both%20the%20TL%3BDR%20summarization%20and%20the%20Anthropic%20Helpful%20Harmful%20%28HH%29%0Adataset%2C%20the%20generation%20from%20DR-PO%20is%20better%20than%20that%20from%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Direction%20Preference%20Optimization%20%28DPO%29%2C%20under%20the%0Ametric%20of%20GPT4%20win-rate.%20Code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/Cornell-RL/drpo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Reset%20Policy%20Optimization%20for%20RLHF&entry.906535625=Jonathan%20D.%20Chang%20and%20Wenhao%20Shan%20and%20Owen%20Oertell%20and%20Kiant%C3%A9%20Brantley%20and%20Dipendra%20Misra%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20from%20Human%20Preference-based%20feedback%20is%20a%20popular%0Aparadigm%20for%20fine-tuning%20generative%20models%2C%20which%20has%20produced%20impressive%0Amodels%20such%20as%20GPT-4%20and%20Claude3%20Opus.%20This%20framework%20often%20consists%20of%20two%0Asteps%3A%20learning%20a%20reward%20model%20from%20an%20offline%20preference%20dataset%20followed%20by%0Arunning%20online%20RL%20to%20optimize%20the%20learned%20reward%20model.%20In%20this%20work%2C%0Aleveraging%20the%20idea%20of%20reset%2C%20we%20propose%20a%20new%20RLHF%20algorithm%20with%20provable%0Aguarantees.%20Motivated%20by%20the%20fact%20that%20offline%20preference%20dataset%20provides%0Ainformative%20states%20%28i.e.%2C%20data%20that%20is%20preferred%20by%20the%20labelers%29%2C%20our%20new%0Aalgorithm%2C%20Dataset%20Reset%20Policy%20Optimization%20%28DR-PO%29%2C%20integrates%20the%20existing%0Aoffline%20preference%20dataset%20into%20the%20online%20policy%20training%20procedure%20via%0Adataset%20reset%3A%20it%20directly%20resets%20the%20policy%20optimizer%20to%20the%20states%20in%20the%0Aoffline%20dataset%2C%20instead%20of%20always%20starting%20from%20the%20initial%20state%0Adistribution.%20In%20theory%2C%20we%20show%20that%20DR-PO%20learns%20to%20perform%20at%20least%20as%20good%0Aas%20any%20policy%20that%20is%20covered%20by%20the%20offline%20dataset%20under%20general%20function%0Aapproximation%20with%20finite%20sample%20complexity.%20In%20experiments%2C%20we%20demonstrate%0Athat%20on%20both%20the%20TL%3BDR%20summarization%20and%20the%20Anthropic%20Helpful%20Harmful%20%28HH%29%0Adataset%2C%20the%20generation%20from%20DR-PO%20is%20better%20than%20that%20from%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Direction%20Preference%20Optimization%20%28DPO%29%2C%20under%20the%0Ametric%20of%20GPT4%20win-rate.%20Code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/Cornell-RL/drpo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08495v1&entry.124074799=Read"},
{"title": "Contrastive Graph Pooling for Explainable Classification of Brain\n  Networks", "author": "Jiaxing Xu and Qingtian Bian and Xinhang Li and Aihu Zhang and Yiping Ke and Miao Qiao and Wei Zhang and Wei Khang Jeremy Sim and Bal\u00e1zs Guly\u00e1s", "abstract": "  Functional magnetic resonance imaging (fMRI) is a commonly used technique to\nmeasure neural activation. Its application has been particularly important in\nidentifying underlying neurodegenerative conditions such as Parkinson's,\nAlzheimer's, and Autism. Recent analysis of fMRI data models the brain as a\ngraph and extracts features by graph neural networks (GNNs). However, the\nunique characteristics of fMRI data require a special design of GNN. Tailoring\nGNN to generate effective and domain-explainable features remains challenging.\nIn this paper, we propose a contrastive dual-attention block and a\ndifferentiable graph pooling method called ContrastPool to better utilize GNN\nfor brain networks, meeting fMRI-specific requirements. We apply our method to\n5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its\nsuperiority over state-of-the-art baselines. Our case study confirms that the\npatterns extracted by our method match the domain knowledge in neuroscience\nliterature, and disclose direct and interesting insights. Our contributions\nunderscore the potential of ContrastPool for advancing the understanding of\nbrain networks and neurodegenerative conditions. The source code is available\nat https://github.com/AngusMonroe/ContrastPool.\n", "link": "http://arxiv.org/abs/2307.11133v2", "date": "2024-04-12", "relevancy": 1.8591, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4517}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4485}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Graph%20Pooling%20for%20Explainable%20Classification%20of%20Brain%0A%20%20Networks&body=Title%3A%20Contrastive%20Graph%20Pooling%20for%20Explainable%20Classification%20of%20Brain%0A%20%20Networks%0AAuthor%3A%20Jiaxing%20Xu%20and%20Qingtian%20Bian%20and%20Xinhang%20Li%20and%20Aihu%20Zhang%20and%20Yiping%20Ke%20and%20Miao%20Qiao%20and%20Wei%20Zhang%20and%20Wei%20Khang%20Jeremy%20Sim%20and%20Bal%C3%A1zs%20Guly%C3%A1s%0AAbstract%3A%20%20%20Functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20is%20a%20commonly%20used%20technique%20to%0Ameasure%20neural%20activation.%20Its%20application%20has%20been%20particularly%20important%20in%0Aidentifying%20underlying%20neurodegenerative%20conditions%20such%20as%20Parkinson%27s%2C%0AAlzheimer%27s%2C%20and%20Autism.%20Recent%20analysis%20of%20fMRI%20data%20models%20the%20brain%20as%20a%0Agraph%20and%20extracts%20features%20by%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%20the%0Aunique%20characteristics%20of%20fMRI%20data%20require%20a%20special%20design%20of%20GNN.%20Tailoring%0AGNN%20to%20generate%20effective%20and%20domain-explainable%20features%20remains%20challenging.%0AIn%20this%20paper%2C%20we%20propose%20a%20contrastive%20dual-attention%20block%20and%20a%0Adifferentiable%20graph%20pooling%20method%20called%20ContrastPool%20to%20better%20utilize%20GNN%0Afor%20brain%20networks%2C%20meeting%20fMRI-specific%20requirements.%20We%20apply%20our%20method%20to%0A5%20resting-state%20fMRI%20brain%20network%20datasets%20of%203%20diseases%20and%20demonstrate%20its%0Asuperiority%20over%20state-of-the-art%20baselines.%20Our%20case%20study%20confirms%20that%20the%0Apatterns%20extracted%20by%20our%20method%20match%20the%20domain%20knowledge%20in%20neuroscience%0Aliterature%2C%20and%20disclose%20direct%20and%20interesting%20insights.%20Our%20contributions%0Aunderscore%20the%20potential%20of%20ContrastPool%20for%20advancing%20the%20understanding%20of%0Abrain%20networks%20and%20neurodegenerative%20conditions.%20The%20source%20code%20is%20available%0Aat%20https%3A//github.com/AngusMonroe/ContrastPool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.11133v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Graph%20Pooling%20for%20Explainable%20Classification%20of%20Brain%0A%20%20Networks&entry.906535625=Jiaxing%20Xu%20and%20Qingtian%20Bian%20and%20Xinhang%20Li%20and%20Aihu%20Zhang%20and%20Yiping%20Ke%20and%20Miao%20Qiao%20and%20Wei%20Zhang%20and%20Wei%20Khang%20Jeremy%20Sim%20and%20Bal%C3%A1zs%20Guly%C3%A1s&entry.1292438233=%20%20Functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20is%20a%20commonly%20used%20technique%20to%0Ameasure%20neural%20activation.%20Its%20application%20has%20been%20particularly%20important%20in%0Aidentifying%20underlying%20neurodegenerative%20conditions%20such%20as%20Parkinson%27s%2C%0AAlzheimer%27s%2C%20and%20Autism.%20Recent%20analysis%20of%20fMRI%20data%20models%20the%20brain%20as%20a%0Agraph%20and%20extracts%20features%20by%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%20the%0Aunique%20characteristics%20of%20fMRI%20data%20require%20a%20special%20design%20of%20GNN.%20Tailoring%0AGNN%20to%20generate%20effective%20and%20domain-explainable%20features%20remains%20challenging.%0AIn%20this%20paper%2C%20we%20propose%20a%20contrastive%20dual-attention%20block%20and%20a%0Adifferentiable%20graph%20pooling%20method%20called%20ContrastPool%20to%20better%20utilize%20GNN%0Afor%20brain%20networks%2C%20meeting%20fMRI-specific%20requirements.%20We%20apply%20our%20method%20to%0A5%20resting-state%20fMRI%20brain%20network%20datasets%20of%203%20diseases%20and%20demonstrate%20its%0Asuperiority%20over%20state-of-the-art%20baselines.%20Our%20case%20study%20confirms%20that%20the%0Apatterns%20extracted%20by%20our%20method%20match%20the%20domain%20knowledge%20in%20neuroscience%0Aliterature%2C%20and%20disclose%20direct%20and%20interesting%20insights.%20Our%20contributions%0Aunderscore%20the%20potential%20of%20ContrastPool%20for%20advancing%20the%20understanding%20of%0Abrain%20networks%20and%20neurodegenerative%20conditions.%20The%20source%20code%20is%20available%0Aat%20https%3A//github.com/AngusMonroe/ContrastPool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11133v2&entry.124074799=Read"},
{"title": "Small Models Are (Still) Effective Cross-Domain Argument Extractors", "author": "William Gantt and Aaron Steven White", "abstract": "  Effective ontology transfer has been a major goal of recent work on event\nargument extraction (EAE). Two methods in particular -- question answering (QA)\nand template infilling (TI) -- have emerged as promising approaches to this\nproblem. However, detailed explorations of these techniques' ability to\nactually enable this transfer are lacking. In this work, we provide such a\nstudy, exploring zero-shot transfer using both techniques on six major EAE\ndatasets at both the sentence and document levels. Further, we challenge the\ngrowing reliance on LLMs for zero-shot extraction, showing that vastly smaller\nmodels trained on an appropriate source ontology can yield zero-shot\nperformance superior to that of GPT-3.5 or GPT-4.\n", "link": "http://arxiv.org/abs/2404.08579v1", "date": "2024-04-12", "relevancy": 1.8558, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4633}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.462}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Small%20Models%20Are%20%28Still%29%20Effective%20Cross-Domain%20Argument%20Extractors&body=Title%3A%20Small%20Models%20Are%20%28Still%29%20Effective%20Cross-Domain%20Argument%20Extractors%0AAuthor%3A%20William%20Gantt%20and%20Aaron%20Steven%20White%0AAbstract%3A%20%20%20Effective%20ontology%20transfer%20has%20been%20a%20major%20goal%20of%20recent%20work%20on%20event%0Aargument%20extraction%20%28EAE%29.%20Two%20methods%20in%20particular%20--%20question%20answering%20%28QA%29%0Aand%20template%20infilling%20%28TI%29%20--%20have%20emerged%20as%20promising%20approaches%20to%20this%0Aproblem.%20However%2C%20detailed%20explorations%20of%20these%20techniques%27%20ability%20to%0Aactually%20enable%20this%20transfer%20are%20lacking.%20In%20this%20work%2C%20we%20provide%20such%20a%0Astudy%2C%20exploring%20zero-shot%20transfer%20using%20both%20techniques%20on%20six%20major%20EAE%0Adatasets%20at%20both%20the%20sentence%20and%20document%20levels.%20Further%2C%20we%20challenge%20the%0Agrowing%20reliance%20on%20LLMs%20for%20zero-shot%20extraction%2C%20showing%20that%20vastly%20smaller%0Amodels%20trained%20on%20an%20appropriate%20source%20ontology%20can%20yield%20zero-shot%0Aperformance%20superior%20to%20that%20of%20GPT-3.5%20or%20GPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Models%20Are%20%28Still%29%20Effective%20Cross-Domain%20Argument%20Extractors&entry.906535625=William%20Gantt%20and%20Aaron%20Steven%20White&entry.1292438233=%20%20Effective%20ontology%20transfer%20has%20been%20a%20major%20goal%20of%20recent%20work%20on%20event%0Aargument%20extraction%20%28EAE%29.%20Two%20methods%20in%20particular%20--%20question%20answering%20%28QA%29%0Aand%20template%20infilling%20%28TI%29%20--%20have%20emerged%20as%20promising%20approaches%20to%20this%0Aproblem.%20However%2C%20detailed%20explorations%20of%20these%20techniques%27%20ability%20to%0Aactually%20enable%20this%20transfer%20are%20lacking.%20In%20this%20work%2C%20we%20provide%20such%20a%0Astudy%2C%20exploring%20zero-shot%20transfer%20using%20both%20techniques%20on%20six%20major%20EAE%0Adatasets%20at%20both%20the%20sentence%20and%20document%20levels.%20Further%2C%20we%20challenge%20the%0Agrowing%20reliance%20on%20LLMs%20for%20zero-shot%20extraction%2C%20showing%20that%20vastly%20smaller%0Amodels%20trained%20on%20an%20appropriate%20source%20ontology%20can%20yield%20zero-shot%0Aperformance%20superior%20to%20that%20of%20GPT-3.5%20or%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08579v1&entry.124074799=Read"},
{"title": "Few-Shot Cross-System Anomaly Trace Classification for\n  Microservice-based systems", "author": "Yuqing Wang and Mika V. M\u00e4ntyl\u00e4 and Serge Demeyer and Mutlu Beyazit and Joanna Kisaakye and Jesse Nyyss\u00f6l\u00e4", "abstract": "  Microservice-based systems (MSS) may experience failures in various fault\ncategories due to their complex and dynamic nature. To effectively handle\nfailures, AIOps tools utilize trace-based anomaly detection and root cause\nanalysis. In this paper, we propose a novel framework for few-shot abnormal\ntrace classification for MSS. Our framework comprises two main components: (1)\nMulti-Head Attention Autoencoder for constructing system-specific trace\nrepresentations, which enables (2) Transformer Encoder-based Model-Agnostic\nMeta-Learning to perform effective and efficient few-shot learning for abnormal\ntrace classification. The proposed framework is evaluated on two representative\nMSS, Trainticket and OnlineBoutique, with open datasets. The results show that\nour framework can adapt the learned knowledge to classify new, unseen abnormal\ntraces of novel fault categories both within the same system it was initially\ntrained on and even in the different MSS. Within the same MSS, our framework\nachieves an average accuracy of 93.26\\% and 85.2\\% across 50 meta-testing tasks\nfor Trainticket and OnlineBoutique, respectively, when provided with 10\ninstances for each task. In a cross-system context, our framework gets an\naverage accuracy of 92.19\\% and 84.77\\% for the same meta-testing tasks of the\nrespective system, also with 10 instances provided for each task. Our work\ndemonstrates the applicability of achieving few-shot abnormal trace\nclassification for MSS and shows how it can enable cross-system adaptability.\nThis opens an avenue for building more generalized AIOps tools that require\nless system-specific data labeling for anomaly detection and root cause\nanalysis.\n", "link": "http://arxiv.org/abs/2403.18998v3", "date": "2024-04-12", "relevancy": 1.8467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4599}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4409}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Cross-System%20Anomaly%20Trace%20Classification%20for%0A%20%20Microservice-based%20systems&body=Title%3A%20Few-Shot%20Cross-System%20Anomaly%20Trace%20Classification%20for%0A%20%20Microservice-based%20systems%0AAuthor%3A%20Yuqing%20Wang%20and%20Mika%20V.%20M%C3%A4ntyl%C3%A4%20and%20Serge%20Demeyer%20and%20Mutlu%20Beyazit%20and%20Joanna%20Kisaakye%20and%20Jesse%20Nyyss%C3%B6l%C3%A4%0AAbstract%3A%20%20%20Microservice-based%20systems%20%28MSS%29%20may%20experience%20failures%20in%20various%20fault%0Acategories%20due%20to%20their%20complex%20and%20dynamic%20nature.%20To%20effectively%20handle%0Afailures%2C%20AIOps%20tools%20utilize%20trace-based%20anomaly%20detection%20and%20root%20cause%0Aanalysis.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20few-shot%20abnormal%0Atrace%20classification%20for%20MSS.%20Our%20framework%20comprises%20two%20main%20components%3A%20%281%29%0AMulti-Head%20Attention%20Autoencoder%20for%20constructing%20system-specific%20trace%0Arepresentations%2C%20which%20enables%20%282%29%20Transformer%20Encoder-based%20Model-Agnostic%0AMeta-Learning%20to%20perform%20effective%20and%20efficient%20few-shot%20learning%20for%20abnormal%0Atrace%20classification.%20The%20proposed%20framework%20is%20evaluated%20on%20two%20representative%0AMSS%2C%20Trainticket%20and%20OnlineBoutique%2C%20with%20open%20datasets.%20The%20results%20show%20that%0Aour%20framework%20can%20adapt%20the%20learned%20knowledge%20to%20classify%20new%2C%20unseen%20abnormal%0Atraces%20of%20novel%20fault%20categories%20both%20within%20the%20same%20system%20it%20was%20initially%0Atrained%20on%20and%20even%20in%20the%20different%20MSS.%20Within%20the%20same%20MSS%2C%20our%20framework%0Aachieves%20an%20average%20accuracy%20of%2093.26%5C%25%20and%2085.2%5C%25%20across%2050%20meta-testing%20tasks%0Afor%20Trainticket%20and%20OnlineBoutique%2C%20respectively%2C%20when%20provided%20with%2010%0Ainstances%20for%20each%20task.%20In%20a%20cross-system%20context%2C%20our%20framework%20gets%20an%0Aaverage%20accuracy%20of%2092.19%5C%25%20and%2084.77%5C%25%20for%20the%20same%20meta-testing%20tasks%20of%20the%0Arespective%20system%2C%20also%20with%2010%20instances%20provided%20for%20each%20task.%20Our%20work%0Ademonstrates%20the%20applicability%20of%20achieving%20few-shot%20abnormal%20trace%0Aclassification%20for%20MSS%20and%20shows%20how%20it%20can%20enable%20cross-system%20adaptability.%0AThis%20opens%20an%20avenue%20for%20building%20more%20generalized%20AIOps%20tools%20that%20require%0Aless%20system-specific%20data%20labeling%20for%20anomaly%20detection%20and%20root%20cause%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18998v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Cross-System%20Anomaly%20Trace%20Classification%20for%0A%20%20Microservice-based%20systems&entry.906535625=Yuqing%20Wang%20and%20Mika%20V.%20M%C3%A4ntyl%C3%A4%20and%20Serge%20Demeyer%20and%20Mutlu%20Beyazit%20and%20Joanna%20Kisaakye%20and%20Jesse%20Nyyss%C3%B6l%C3%A4&entry.1292438233=%20%20Microservice-based%20systems%20%28MSS%29%20may%20experience%20failures%20in%20various%20fault%0Acategories%20due%20to%20their%20complex%20and%20dynamic%20nature.%20To%20effectively%20handle%0Afailures%2C%20AIOps%20tools%20utilize%20trace-based%20anomaly%20detection%20and%20root%20cause%0Aanalysis.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20few-shot%20abnormal%0Atrace%20classification%20for%20MSS.%20Our%20framework%20comprises%20two%20main%20components%3A%20%281%29%0AMulti-Head%20Attention%20Autoencoder%20for%20constructing%20system-specific%20trace%0Arepresentations%2C%20which%20enables%20%282%29%20Transformer%20Encoder-based%20Model-Agnostic%0AMeta-Learning%20to%20perform%20effective%20and%20efficient%20few-shot%20learning%20for%20abnormal%0Atrace%20classification.%20The%20proposed%20framework%20is%20evaluated%20on%20two%20representative%0AMSS%2C%20Trainticket%20and%20OnlineBoutique%2C%20with%20open%20datasets.%20The%20results%20show%20that%0Aour%20framework%20can%20adapt%20the%20learned%20knowledge%20to%20classify%20new%2C%20unseen%20abnormal%0Atraces%20of%20novel%20fault%20categories%20both%20within%20the%20same%20system%20it%20was%20initially%0Atrained%20on%20and%20even%20in%20the%20different%20MSS.%20Within%20the%20same%20MSS%2C%20our%20framework%0Aachieves%20an%20average%20accuracy%20of%2093.26%5C%25%20and%2085.2%5C%25%20across%2050%20meta-testing%20tasks%0Afor%20Trainticket%20and%20OnlineBoutique%2C%20respectively%2C%20when%20provided%20with%2010%0Ainstances%20for%20each%20task.%20In%20a%20cross-system%20context%2C%20our%20framework%20gets%20an%0Aaverage%20accuracy%20of%2092.19%5C%25%20and%2084.77%5C%25%20for%20the%20same%20meta-testing%20tasks%20of%20the%0Arespective%20system%2C%20also%20with%2010%20instances%20provided%20for%20each%20task.%20Our%20work%0Ademonstrates%20the%20applicability%20of%20achieving%20few-shot%20abnormal%20trace%0Aclassification%20for%20MSS%20and%20shows%20how%20it%20can%20enable%20cross-system%20adaptability.%0AThis%20opens%20an%20avenue%20for%20building%20more%20generalized%20AIOps%20tools%20that%20require%0Aless%20system-specific%20data%20labeling%20for%20anomaly%20detection%20and%20root%20cause%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18998v3&entry.124074799=Read"},
{"title": "Multi-Agent eXperimenter (MAX)", "author": "\u00d6nder G\u00fcrcan", "abstract": "  We present a novel multi-agent simulator named Multi-Agent eXperimenter (MAX)\nthat is designed to simulate blockchain experiments involving large numbers of\nagents of different types acting in one or several environments. The\narchitecture of MAX is highly modular, enabling easy addition of new models.\n", "link": "http://arxiv.org/abs/2404.08398v1", "date": "2024-04-12", "relevancy": 1.8444, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4906}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4726}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4378}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20eXperimenter%20%28MAX%29&body=Title%3A%20Multi-Agent%20eXperimenter%20%28MAX%29%0AAuthor%3A%20%C3%96nder%20G%C3%BCrcan%0AAbstract%3A%20%20%20We%20present%20a%20novel%20multi-agent%20simulator%20named%20Multi-Agent%20eXperimenter%20%28MAX%29%0Athat%20is%20designed%20to%20simulate%20blockchain%20experiments%20involving%20large%20numbers%20of%0Aagents%20of%20different%20types%20acting%20in%20one%20or%20several%20environments.%20The%0Aarchitecture%20of%20MAX%20is%20highly%20modular%2C%20enabling%20easy%20addition%20of%20new%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08398v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20eXperimenter%20%28MAX%29&entry.906535625=%C3%96nder%20G%C3%BCrcan&entry.1292438233=%20%20We%20present%20a%20novel%20multi-agent%20simulator%20named%20Multi-Agent%20eXperimenter%20%28MAX%29%0Athat%20is%20designed%20to%20simulate%20blockchain%20experiments%20involving%20large%20numbers%20of%0Aagents%20of%20different%20types%20acting%20in%20one%20or%20several%20environments.%20The%0Aarchitecture%20of%20MAX%20is%20highly%20modular%2C%20enabling%20easy%20addition%20of%20new%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08398v1&entry.124074799=Read"},
{"title": "New Efficient Visual OILU Markers", "author": "Youssef Chahir and Messaoud Mostefai and Hamza Saida", "abstract": "  Basic patterns are the source of a wide range of more or less complex\ngeometric structures. We will exploit such patterns to develop new efficient\nvisual markers. Besides being projective invariants, the proposed markers allow\nproducing rich panel of unique identifiers, highly required for\nresource-intensive navigation and augmented reality applications. The spiral\ntopology of our markers permits the validation of an accurate identification\nscheme, which is based on level set methods. The robustness of the markers\nagainst acquisition and geometric distortions is validated by extensive\nexperimental tests.\n", "link": "http://arxiv.org/abs/2404.08477v1", "date": "2024-04-12", "relevancy": 1.8405, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4697}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4587}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4511}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20New%20Efficient%20Visual%20OILU%20Markers&body=Title%3A%20New%20Efficient%20Visual%20OILU%20Markers%0AAuthor%3A%20Youssef%20Chahir%20and%20Messaoud%20Mostefai%20and%20Hamza%20Saida%0AAbstract%3A%20%20%20Basic%20patterns%20are%20the%20source%20of%20a%20wide%20range%20of%20more%20or%20less%20complex%0Ageometric%20structures.%20We%20will%20exploit%20such%20patterns%20to%20develop%20new%20efficient%0Avisual%20markers.%20Besides%20being%20projective%20invariants%2C%20the%20proposed%20markers%20allow%0Aproducing%20rich%20panel%20of%20unique%20identifiers%2C%20highly%20required%20for%0Aresource-intensive%20navigation%20and%20augmented%20reality%20applications.%20The%20spiral%0Atopology%20of%20our%20markers%20permits%20the%20validation%20of%20an%20accurate%20identification%0Ascheme%2C%20which%20is%20based%20on%20level%20set%20methods.%20The%20robustness%20of%20the%20markers%0Aagainst%20acquisition%20and%20geometric%20distortions%20is%20validated%20by%20extensive%0Aexperimental%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08477v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Efficient%20Visual%20OILU%20Markers&entry.906535625=Youssef%20Chahir%20and%20Messaoud%20Mostefai%20and%20Hamza%20Saida&entry.1292438233=%20%20Basic%20patterns%20are%20the%20source%20of%20a%20wide%20range%20of%20more%20or%20less%20complex%0Ageometric%20structures.%20We%20will%20exploit%20such%20patterns%20to%20develop%20new%20efficient%0Avisual%20markers.%20Besides%20being%20projective%20invariants%2C%20the%20proposed%20markers%20allow%0Aproducing%20rich%20panel%20of%20unique%20identifiers%2C%20highly%20required%20for%0Aresource-intensive%20navigation%20and%20augmented%20reality%20applications.%20The%20spiral%0Atopology%20of%20our%20markers%20permits%20the%20validation%20of%20an%20accurate%20identification%0Ascheme%2C%20which%20is%20based%20on%20level%20set%20methods.%20The%20robustness%20of%20the%20markers%0Aagainst%20acquisition%20and%20geometric%20distortions%20is%20validated%20by%20extensive%0Aexperimental%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08477v1&entry.124074799=Read"},
{"title": "Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin\n  Lymphoma Patients Using a Longitudinally-Aware Segmentation Network", "author": "Xin Tie and Muheon Shin and Changhee Lee and Scott B. Perlman and Zachary Huemann and Amy J. Weisman and Sharon M. Castellino and Kara M. Kelly and Kathleen M. McCarten and Adina L. Alazraki and Junjie Hu and Steve Y. Cho and Tyler J. Bradshaw", "abstract": "  $\\textbf{Purpose}$: Automatic quantification of longitudinal changes in PET\nscans for lymphoma patients has proven challenging, as residual disease in\ninterim-therapy scans is often subtle and difficult to detect. Our goal was to\ndevelop a longitudinally-aware segmentation network (LAS-Net) that can quantify\nserial PET/CT images for pediatric Hodgkin lymphoma patients.\n$\\textbf{Materials and Methods}$: This retrospective study included baseline\n(PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two\nChildren's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net\nincorporates longitudinal cross-attention, allowing relevant features from PET1\nto inform the analysis of PET2. Model performance was evaluated using Dice\ncoefficients for PET1 and detection F1 scores for PET2. Additionally, we\nextracted and compared quantitative PET metrics, including metabolic tumor\nvolume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and\n$\\Delta$SUVmax in PET2, against physician measurements. We quantified their\nagreement using Spearman's $\\rho$ correlations and employed bootstrap\nresampling for statistical analysis. $\\textbf{Results}$: LAS-Net detected\nresidual lymphoma in PET2 with an F1 score of 0.606 (precision/recall:\n0.615/0.600), outperforming all comparator methods (P<0.01). For baseline\nsegmentation, LAS-Net achieved a mean Dice score of 0.772. In PET\nquantification, LAS-Net's measurements of qPET, $\\Delta$SUVmax, MTV and TLG\nwere strongly correlated with physician measurements, with Spearman's $\\rho$ of\n0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a\nslight decrease, in an external testing cohort. $\\textbf{Conclusion}$: LAS-Net\nachieved high performance in quantifying PET metrics across serial scans,\nhighlighting the value of longitudinal awareness in evaluating multi-time-point\nimaging datasets.\n", "link": "http://arxiv.org/abs/2404.08611v1", "date": "2024-04-12", "relevancy": 1.8403, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4693}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.45}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Quantification%20of%20Serial%20PET/CT%20Images%20for%20Pediatric%20Hodgkin%0A%20%20Lymphoma%20Patients%20Using%20a%20Longitudinally-Aware%20Segmentation%20Network&body=Title%3A%20Automatic%20Quantification%20of%20Serial%20PET/CT%20Images%20for%20Pediatric%20Hodgkin%0A%20%20Lymphoma%20Patients%20Using%20a%20Longitudinally-Aware%20Segmentation%20Network%0AAuthor%3A%20Xin%20Tie%20and%20Muheon%20Shin%20and%20Changhee%20Lee%20and%20Scott%20B.%20Perlman%20and%20Zachary%20Huemann%20and%20Amy%20J.%20Weisman%20and%20Sharon%20M.%20Castellino%20and%20Kara%20M.%20Kelly%20and%20Kathleen%20M.%20McCarten%20and%20Adina%20L.%20Alazraki%20and%20Junjie%20Hu%20and%20Steve%20Y.%20Cho%20and%20Tyler%20J.%20Bradshaw%0AAbstract%3A%20%20%20%24%5Ctextbf%7BPurpose%7D%24%3A%20Automatic%20quantification%20of%20longitudinal%20changes%20in%20PET%0Ascans%20for%20lymphoma%20patients%20has%20proven%20challenging%2C%20as%20residual%20disease%20in%0Ainterim-therapy%20scans%20is%20often%20subtle%20and%20difficult%20to%20detect.%20Our%20goal%20was%20to%0Adevelop%20a%20longitudinally-aware%20segmentation%20network%20%28LAS-Net%29%20that%20can%20quantify%0Aserial%20PET/CT%20images%20for%20pediatric%20Hodgkin%20lymphoma%20patients.%0A%24%5Ctextbf%7BMaterials%20and%20Methods%7D%24%3A%20This%20retrospective%20study%20included%20baseline%0A%28PET1%29%20and%20interim%20%28PET2%29%20PET/CT%20images%20from%20297%20patients%20enrolled%20in%20two%0AChildren%27s%20Oncology%20Group%20clinical%20trials%20%28AHOD1331%20and%20AHOD0831%29.%20LAS-Net%0Aincorporates%20longitudinal%20cross-attention%2C%20allowing%20relevant%20features%20from%20PET1%0Ato%20inform%20the%20analysis%20of%20PET2.%20Model%20performance%20was%20evaluated%20using%20Dice%0Acoefficients%20for%20PET1%20and%20detection%20F1%20scores%20for%20PET2.%20Additionally%2C%20we%0Aextracted%20and%20compared%20quantitative%20PET%20metrics%2C%20including%20metabolic%20tumor%0Avolume%20%28MTV%29%20and%20total%20lesion%20glycolysis%20%28TLG%29%20in%20PET1%2C%20as%20well%20as%20qPET%20and%0A%24%5CDelta%24SUVmax%20in%20PET2%2C%20against%20physician%20measurements.%20We%20quantified%20their%0Aagreement%20using%20Spearman%27s%20%24%5Crho%24%20correlations%20and%20employed%20bootstrap%0Aresampling%20for%20statistical%20analysis.%20%24%5Ctextbf%7BResults%7D%24%3A%20LAS-Net%20detected%0Aresidual%20lymphoma%20in%20PET2%20with%20an%20F1%20score%20of%200.606%20%28precision/recall%3A%0A0.615/0.600%29%2C%20outperforming%20all%20comparator%20methods%20%28P%3C0.01%29.%20For%20baseline%0Asegmentation%2C%20LAS-Net%20achieved%20a%20mean%20Dice%20score%20of%200.772.%20In%20PET%0Aquantification%2C%20LAS-Net%27s%20measurements%20of%20qPET%2C%20%24%5CDelta%24SUVmax%2C%20MTV%20and%20TLG%0Awere%20strongly%20correlated%20with%20physician%20measurements%2C%20with%20Spearman%27s%20%24%5Crho%24%20of%0A0.78%2C%200.80%2C%200.93%20and%200.96%2C%20respectively.%20The%20performance%20remained%20high%2C%20with%20a%0Aslight%20decrease%2C%20in%20an%20external%20testing%20cohort.%20%24%5Ctextbf%7BConclusion%7D%24%3A%20LAS-Net%0Aachieved%20high%20performance%20in%20quantifying%20PET%20metrics%20across%20serial%20scans%2C%0Ahighlighting%20the%20value%20of%20longitudinal%20awareness%20in%20evaluating%20multi-time-point%0Aimaging%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Quantification%20of%20Serial%20PET/CT%20Images%20for%20Pediatric%20Hodgkin%0A%20%20Lymphoma%20Patients%20Using%20a%20Longitudinally-Aware%20Segmentation%20Network&entry.906535625=Xin%20Tie%20and%20Muheon%20Shin%20and%20Changhee%20Lee%20and%20Scott%20B.%20Perlman%20and%20Zachary%20Huemann%20and%20Amy%20J.%20Weisman%20and%20Sharon%20M.%20Castellino%20and%20Kara%20M.%20Kelly%20and%20Kathleen%20M.%20McCarten%20and%20Adina%20L.%20Alazraki%20and%20Junjie%20Hu%20and%20Steve%20Y.%20Cho%20and%20Tyler%20J.%20Bradshaw&entry.1292438233=%20%20%24%5Ctextbf%7BPurpose%7D%24%3A%20Automatic%20quantification%20of%20longitudinal%20changes%20in%20PET%0Ascans%20for%20lymphoma%20patients%20has%20proven%20challenging%2C%20as%20residual%20disease%20in%0Ainterim-therapy%20scans%20is%20often%20subtle%20and%20difficult%20to%20detect.%20Our%20goal%20was%20to%0Adevelop%20a%20longitudinally-aware%20segmentation%20network%20%28LAS-Net%29%20that%20can%20quantify%0Aserial%20PET/CT%20images%20for%20pediatric%20Hodgkin%20lymphoma%20patients.%0A%24%5Ctextbf%7BMaterials%20and%20Methods%7D%24%3A%20This%20retrospective%20study%20included%20baseline%0A%28PET1%29%20and%20interim%20%28PET2%29%20PET/CT%20images%20from%20297%20patients%20enrolled%20in%20two%0AChildren%27s%20Oncology%20Group%20clinical%20trials%20%28AHOD1331%20and%20AHOD0831%29.%20LAS-Net%0Aincorporates%20longitudinal%20cross-attention%2C%20allowing%20relevant%20features%20from%20PET1%0Ato%20inform%20the%20analysis%20of%20PET2.%20Model%20performance%20was%20evaluated%20using%20Dice%0Acoefficients%20for%20PET1%20and%20detection%20F1%20scores%20for%20PET2.%20Additionally%2C%20we%0Aextracted%20and%20compared%20quantitative%20PET%20metrics%2C%20including%20metabolic%20tumor%0Avolume%20%28MTV%29%20and%20total%20lesion%20glycolysis%20%28TLG%29%20in%20PET1%2C%20as%20well%20as%20qPET%20and%0A%24%5CDelta%24SUVmax%20in%20PET2%2C%20against%20physician%20measurements.%20We%20quantified%20their%0Aagreement%20using%20Spearman%27s%20%24%5Crho%24%20correlations%20and%20employed%20bootstrap%0Aresampling%20for%20statistical%20analysis.%20%24%5Ctextbf%7BResults%7D%24%3A%20LAS-Net%20detected%0Aresidual%20lymphoma%20in%20PET2%20with%20an%20F1%20score%20of%200.606%20%28precision/recall%3A%0A0.615/0.600%29%2C%20outperforming%20all%20comparator%20methods%20%28P%3C0.01%29.%20For%20baseline%0Asegmentation%2C%20LAS-Net%20achieved%20a%20mean%20Dice%20score%20of%200.772.%20In%20PET%0Aquantification%2C%20LAS-Net%27s%20measurements%20of%20qPET%2C%20%24%5CDelta%24SUVmax%2C%20MTV%20and%20TLG%0Awere%20strongly%20correlated%20with%20physician%20measurements%2C%20with%20Spearman%27s%20%24%5Crho%24%20of%0A0.78%2C%200.80%2C%200.93%20and%200.96%2C%20respectively.%20The%20performance%20remained%20high%2C%20with%20a%0Aslight%20decrease%2C%20in%20an%20external%20testing%20cohort.%20%24%5Ctextbf%7BConclusion%7D%24%3A%20LAS-Net%0Aachieved%20high%20performance%20in%20quantifying%20PET%20metrics%20across%20serial%20scans%2C%0Ahighlighting%20the%20value%20of%20longitudinal%20awareness%20in%20evaluating%20multi-time-point%0Aimaging%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08611v1&entry.124074799=Read"},
{"title": "RFFNet: Large-Scale Interpretable Kernel Methods via Random Fourier\n  Features", "author": "Mateus P. Otto and Rafael Izbicki", "abstract": "  Kernel methods provide a flexible and theoretically grounded approach to\nnonlinear and nonparametric learning. While memory and run-time requirements\nhinder their applicability to large datasets, many low-rank kernel\napproximations, such as random Fourier features, were recently developed to\nscale up such kernel methods. However, these scalable approaches are based on\napproximations of isotropic kernels, which cannot remove the influence of\nirrelevant features. In this work, we design random Fourier features for a\nfamily of automatic relevance determination (ARD) kernels, and introduce\nRFFNet, a new large-scale kernel method that learns the kernel relevances' on\nthe fly via first-order stochastic optimization. We present an effective\ninitialization scheme for the method's non-convex objective function, evaluate\nif hard-thresholding RFFNet's learned relevances yield a sensible rule for\nvariable selection, and perform an extensive ablation study of RFFNet's\ncomponents. Numerical validation on simulated and real-world data shows that\nour approach has a small memory footprint and run-time, achieves low prediction\nerror, and effectively identifies relevant features, thus leading to more\ninterpretable solutions. We supply users with an efficient, PyTorch-based\nlibrary, that adheres to the scikit-learn standard API and code for fully\nreproducing our results.\n", "link": "http://arxiv.org/abs/2211.06410v2", "date": "2024-04-12", "relevancy": 1.8343, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4686}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4606}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RFFNet%3A%20Large-Scale%20Interpretable%20Kernel%20Methods%20via%20Random%20Fourier%0A%20%20Features&body=Title%3A%20RFFNet%3A%20Large-Scale%20Interpretable%20Kernel%20Methods%20via%20Random%20Fourier%0A%20%20Features%0AAuthor%3A%20Mateus%20P.%20Otto%20and%20Rafael%20Izbicki%0AAbstract%3A%20%20%20Kernel%20methods%20provide%20a%20flexible%20and%20theoretically%20grounded%20approach%20to%0Anonlinear%20and%20nonparametric%20learning.%20While%20memory%20and%20run-time%20requirements%0Ahinder%20their%20applicability%20to%20large%20datasets%2C%20many%20low-rank%20kernel%0Aapproximations%2C%20such%20as%20random%20Fourier%20features%2C%20were%20recently%20developed%20to%0Ascale%20up%20such%20kernel%20methods.%20However%2C%20these%20scalable%20approaches%20are%20based%20on%0Aapproximations%20of%20isotropic%20kernels%2C%20which%20cannot%20remove%20the%20influence%20of%0Airrelevant%20features.%20In%20this%20work%2C%20we%20design%20random%20Fourier%20features%20for%20a%0Afamily%20of%20automatic%20relevance%20determination%20%28ARD%29%20kernels%2C%20and%20introduce%0ARFFNet%2C%20a%20new%20large-scale%20kernel%20method%20that%20learns%20the%20kernel%20relevances%27%20on%0Athe%20fly%20via%20first-order%20stochastic%20optimization.%20We%20present%20an%20effective%0Ainitialization%20scheme%20for%20the%20method%27s%20non-convex%20objective%20function%2C%20evaluate%0Aif%20hard-thresholding%20RFFNet%27s%20learned%20relevances%20yield%20a%20sensible%20rule%20for%0Avariable%20selection%2C%20and%20perform%20an%20extensive%20ablation%20study%20of%20RFFNet%27s%0Acomponents.%20Numerical%20validation%20on%20simulated%20and%20real-world%20data%20shows%20that%0Aour%20approach%20has%20a%20small%20memory%20footprint%20and%20run-time%2C%20achieves%20low%20prediction%0Aerror%2C%20and%20effectively%20identifies%20relevant%20features%2C%20thus%20leading%20to%20more%0Ainterpretable%20solutions.%20We%20supply%20users%20with%20an%20efficient%2C%20PyTorch-based%0Alibrary%2C%20that%20adheres%20to%20the%20scikit-learn%20standard%20API%20and%20code%20for%20fully%0Areproducing%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.06410v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RFFNet%3A%20Large-Scale%20Interpretable%20Kernel%20Methods%20via%20Random%20Fourier%0A%20%20Features&entry.906535625=Mateus%20P.%20Otto%20and%20Rafael%20Izbicki&entry.1292438233=%20%20Kernel%20methods%20provide%20a%20flexible%20and%20theoretically%20grounded%20approach%20to%0Anonlinear%20and%20nonparametric%20learning.%20While%20memory%20and%20run-time%20requirements%0Ahinder%20their%20applicability%20to%20large%20datasets%2C%20many%20low-rank%20kernel%0Aapproximations%2C%20such%20as%20random%20Fourier%20features%2C%20were%20recently%20developed%20to%0Ascale%20up%20such%20kernel%20methods.%20However%2C%20these%20scalable%20approaches%20are%20based%20on%0Aapproximations%20of%20isotropic%20kernels%2C%20which%20cannot%20remove%20the%20influence%20of%0Airrelevant%20features.%20In%20this%20work%2C%20we%20design%20random%20Fourier%20features%20for%20a%0Afamily%20of%20automatic%20relevance%20determination%20%28ARD%29%20kernels%2C%20and%20introduce%0ARFFNet%2C%20a%20new%20large-scale%20kernel%20method%20that%20learns%20the%20kernel%20relevances%27%20on%0Athe%20fly%20via%20first-order%20stochastic%20optimization.%20We%20present%20an%20effective%0Ainitialization%20scheme%20for%20the%20method%27s%20non-convex%20objective%20function%2C%20evaluate%0Aif%20hard-thresholding%20RFFNet%27s%20learned%20relevances%20yield%20a%20sensible%20rule%20for%0Avariable%20selection%2C%20and%20perform%20an%20extensive%20ablation%20study%20of%20RFFNet%27s%0Acomponents.%20Numerical%20validation%20on%20simulated%20and%20real-world%20data%20shows%20that%0Aour%20approach%20has%20a%20small%20memory%20footprint%20and%20run-time%2C%20achieves%20low%20prediction%0Aerror%2C%20and%20effectively%20identifies%20relevant%20features%2C%20thus%20leading%20to%20more%0Ainterpretable%20solutions.%20We%20supply%20users%20with%20an%20efficient%2C%20PyTorch-based%0Alibrary%2C%20that%20adheres%20to%20the%20scikit-learn%20standard%20API%20and%20code%20for%20fully%0Areproducing%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.06410v2&entry.124074799=Read"},
{"title": "Beyond Bayesian Model Averaging over Paths in Probabilistic Programs\n  with Stochastic Support", "author": "Tim Reichelt and Luke Ong and Tom Rainforth", "abstract": "  The posterior in probabilistic programs with stochastic support decomposes as\na weighted sum of the local posterior distributions associated with each\npossible program path. We show that making predictions with this full posterior\nimplicitly performs a Bayesian model averaging (BMA) over paths. This is\npotentially problematic, as BMA weights can be unstable due to model\nmisspecification or inference approximations, leading to sub-optimal\npredictions in turn. To remedy this issue, we propose alternative mechanisms\nfor path weighting: one based on stacking and one based on ideas from\nPAC-Bayes. We show how both can be implemented as a cheap post-processing step\non top of existing inference engines. In our experiments, we find them to be\nmore robust and lead to better predictions compared to the default BMA weights.\n", "link": "http://arxiv.org/abs/2310.14888v2", "date": "2024-04-12", "relevancy": 1.8237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.438}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Beyond%20Bayesian%20Model%20Averaging%20over%20Paths%20in%20Probabilistic%20Programs%0A%20%20with%20Stochastic%20Support&body=Title%3A%20Beyond%20Bayesian%20Model%20Averaging%20over%20Paths%20in%20Probabilistic%20Programs%0A%20%20with%20Stochastic%20Support%0AAuthor%3A%20Tim%20Reichelt%20and%20Luke%20Ong%20and%20Tom%20Rainforth%0AAbstract%3A%20%20%20The%20posterior%20in%20probabilistic%20programs%20with%20stochastic%20support%20decomposes%20as%0Aa%20weighted%20sum%20of%20the%20local%20posterior%20distributions%20associated%20with%20each%0Apossible%20program%20path.%20We%20show%20that%20making%20predictions%20with%20this%20full%20posterior%0Aimplicitly%20performs%20a%20Bayesian%20model%20averaging%20%28BMA%29%20over%20paths.%20This%20is%0Apotentially%20problematic%2C%20as%20BMA%20weights%20can%20be%20unstable%20due%20to%20model%0Amisspecification%20or%20inference%20approximations%2C%20leading%20to%20sub-optimal%0Apredictions%20in%20turn.%20To%20remedy%20this%20issue%2C%20we%20propose%20alternative%20mechanisms%0Afor%20path%20weighting%3A%20one%20based%20on%20stacking%20and%20one%20based%20on%20ideas%20from%0APAC-Bayes.%20We%20show%20how%20both%20can%20be%20implemented%20as%20a%20cheap%20post-processing%20step%0Aon%20top%20of%20existing%20inference%20engines.%20In%20our%20experiments%2C%20we%20find%20them%20to%20be%0Amore%20robust%20and%20lead%20to%20better%20predictions%20compared%20to%20the%20default%20BMA%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14888v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Bayesian%20Model%20Averaging%20over%20Paths%20in%20Probabilistic%20Programs%0A%20%20with%20Stochastic%20Support&entry.906535625=Tim%20Reichelt%20and%20Luke%20Ong%20and%20Tom%20Rainforth&entry.1292438233=%20%20The%20posterior%20in%20probabilistic%20programs%20with%20stochastic%20support%20decomposes%20as%0Aa%20weighted%20sum%20of%20the%20local%20posterior%20distributions%20associated%20with%20each%0Apossible%20program%20path.%20We%20show%20that%20making%20predictions%20with%20this%20full%20posterior%0Aimplicitly%20performs%20a%20Bayesian%20model%20averaging%20%28BMA%29%20over%20paths.%20This%20is%0Apotentially%20problematic%2C%20as%20BMA%20weights%20can%20be%20unstable%20due%20to%20model%0Amisspecification%20or%20inference%20approximations%2C%20leading%20to%20sub-optimal%0Apredictions%20in%20turn.%20To%20remedy%20this%20issue%2C%20we%20propose%20alternative%20mechanisms%0Afor%20path%20weighting%3A%20one%20based%20on%20stacking%20and%20one%20based%20on%20ideas%20from%0APAC-Bayes.%20We%20show%20how%20both%20can%20be%20implemented%20as%20a%20cheap%20post-processing%20step%0Aon%20top%20of%20existing%20inference%20engines.%20In%20our%20experiments%2C%20we%20find%20them%20to%20be%0Amore%20robust%20and%20lead%20to%20better%20predictions%20compared%20to%20the%20default%20BMA%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14888v2&entry.124074799=Read"},
{"title": "Analyzing Decades-Long Environmental Changes in Namibia Using Archival\n  Aerial Photography and Deep Learning", "author": "Girmaw Abebe Tadesse and Caleb Robinson and Gilles Quentin Hacheme and Akram Zaytar and Rahul Dodhia and Tsering Wangyal Shawa and Juan M. Lavista Ferres and Emmanuel H. Kreike", "abstract": "  This study explores object detection in historical aerial photographs of\nNamibia to identify long-term environmental changes. Specifically, we aim to\nidentify key objects -- \\textit{Waterholes}, \\textit{Omuti homesteads}, and\n\\textit{Big trees} -- around Oshikango in Namibia using sub-meter gray-scale\naerial imagery from 1943 and 1972. In this work, we propose a workflow for\nanalyzing historical aerial imagery using a deep semantic segmentation model on\nsparse hand-labels. To this end, we employ a number of strategies including\nclass-weighting, pseudo-labeling and empirical p-value-based filtering to\nbalance skewed and sparse representations of objects in the ground truth data.\nResults demonstrate the benefits of these different training strategies\nresulting in an average $F_1=0.661$ and $F_1=0.755$ over the three objects of\ninterest for the 1943 and 1972 imagery, respectively. We also identified that\nthe average size of Waterhole and Big trees increased while the average size of\nOmutis decreased between 1943 and 1972 reflecting some of the local effects of\nthe massive post-Second World War economic, agricultural, demographic, and\nenvironmental changes. This work also highlights the untapped potential of\nhistorical aerial photographs in understanding long-term environmental changes\nbeyond Namibia (and Africa). With the lack of adequate satellite technology in\nthe past, archival aerial photography offers a great alternative to uncover\ndecades-long environmental changes.\n", "link": "http://arxiv.org/abs/2404.08544v1", "date": "2024-04-12", "relevancy": 1.8035, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4544}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4527}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4374}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Decades-Long%20Environmental%20Changes%20in%20Namibia%20Using%20Archival%0A%20%20Aerial%20Photography%20and%20Deep%20Learning&body=Title%3A%20Analyzing%20Decades-Long%20Environmental%20Changes%20in%20Namibia%20Using%20Archival%0A%20%20Aerial%20Photography%20and%20Deep%20Learning%0AAuthor%3A%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Gilles%20Quentin%20Hacheme%20and%20Akram%20Zaytar%20and%20Rahul%20Dodhia%20and%20Tsering%20Wangyal%20Shawa%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Emmanuel%20H.%20Kreike%0AAbstract%3A%20%20%20This%20study%20explores%20object%20detection%20in%20historical%20aerial%20photographs%20of%0ANamibia%20to%20identify%20long-term%20environmental%20changes.%20Specifically%2C%20we%20aim%20to%0Aidentify%20key%20objects%20--%20%5Ctextit%7BWaterholes%7D%2C%20%5Ctextit%7BOmuti%20homesteads%7D%2C%20and%0A%5Ctextit%7BBig%20trees%7D%20--%20around%20Oshikango%20in%20Namibia%20using%20sub-meter%20gray-scale%0Aaerial%20imagery%20from%201943%20and%201972.%20In%20this%20work%2C%20we%20propose%20a%20workflow%20for%0Aanalyzing%20historical%20aerial%20imagery%20using%20a%20deep%20semantic%20segmentation%20model%20on%0Asparse%20hand-labels.%20To%20this%20end%2C%20we%20employ%20a%20number%20of%20strategies%20including%0Aclass-weighting%2C%20pseudo-labeling%20and%20empirical%20p-value-based%20filtering%20to%0Abalance%20skewed%20and%20sparse%20representations%20of%20objects%20in%20the%20ground%20truth%20data.%0AResults%20demonstrate%20the%20benefits%20of%20these%20different%20training%20strategies%0Aresulting%20in%20an%20average%20%24F_1%3D0.661%24%20and%20%24F_1%3D0.755%24%20over%20the%20three%20objects%20of%0Ainterest%20for%20the%201943%20and%201972%20imagery%2C%20respectively.%20We%20also%20identified%20that%0Athe%20average%20size%20of%20Waterhole%20and%20Big%20trees%20increased%20while%20the%20average%20size%20of%0AOmutis%20decreased%20between%201943%20and%201972%20reflecting%20some%20of%20the%20local%20effects%20of%0Athe%20massive%20post-Second%20World%20War%20economic%2C%20agricultural%2C%20demographic%2C%20and%0Aenvironmental%20changes.%20This%20work%20also%20highlights%20the%20untapped%20potential%20of%0Ahistorical%20aerial%20photographs%20in%20understanding%20long-term%20environmental%20changes%0Abeyond%20Namibia%20%28and%20Africa%29.%20With%20the%20lack%20of%20adequate%20satellite%20technology%20in%0Athe%20past%2C%20archival%20aerial%20photography%20offers%20a%20great%20alternative%20to%20uncover%0Adecades-long%20environmental%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08544v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Decades-Long%20Environmental%20Changes%20in%20Namibia%20Using%20Archival%0A%20%20Aerial%20Photography%20and%20Deep%20Learning&entry.906535625=Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Gilles%20Quentin%20Hacheme%20and%20Akram%20Zaytar%20and%20Rahul%20Dodhia%20and%20Tsering%20Wangyal%20Shawa%20and%20Juan%20M.%20Lavista%20Ferres%20and%20Emmanuel%20H.%20Kreike&entry.1292438233=%20%20This%20study%20explores%20object%20detection%20in%20historical%20aerial%20photographs%20of%0ANamibia%20to%20identify%20long-term%20environmental%20changes.%20Specifically%2C%20we%20aim%20to%0Aidentify%20key%20objects%20--%20%5Ctextit%7BWaterholes%7D%2C%20%5Ctextit%7BOmuti%20homesteads%7D%2C%20and%0A%5Ctextit%7BBig%20trees%7D%20--%20around%20Oshikango%20in%20Namibia%20using%20sub-meter%20gray-scale%0Aaerial%20imagery%20from%201943%20and%201972.%20In%20this%20work%2C%20we%20propose%20a%20workflow%20for%0Aanalyzing%20historical%20aerial%20imagery%20using%20a%20deep%20semantic%20segmentation%20model%20on%0Asparse%20hand-labels.%20To%20this%20end%2C%20we%20employ%20a%20number%20of%20strategies%20including%0Aclass-weighting%2C%20pseudo-labeling%20and%20empirical%20p-value-based%20filtering%20to%0Abalance%20skewed%20and%20sparse%20representations%20of%20objects%20in%20the%20ground%20truth%20data.%0AResults%20demonstrate%20the%20benefits%20of%20these%20different%20training%20strategies%0Aresulting%20in%20an%20average%20%24F_1%3D0.661%24%20and%20%24F_1%3D0.755%24%20over%20the%20three%20objects%20of%0Ainterest%20for%20the%201943%20and%201972%20imagery%2C%20respectively.%20We%20also%20identified%20that%0Athe%20average%20size%20of%20Waterhole%20and%20Big%20trees%20increased%20while%20the%20average%20size%20of%0AOmutis%20decreased%20between%201943%20and%201972%20reflecting%20some%20of%20the%20local%20effects%20of%0Athe%20massive%20post-Second%20World%20War%20economic%2C%20agricultural%2C%20demographic%2C%20and%0Aenvironmental%20changes.%20This%20work%20also%20highlights%20the%20untapped%20potential%20of%0Ahistorical%20aerial%20photographs%20in%20understanding%20long-term%20environmental%20changes%0Abeyond%20Namibia%20%28and%20Africa%29.%20With%20the%20lack%20of%20adequate%20satellite%20technology%20in%0Athe%20past%2C%20archival%20aerial%20photography%20offers%20a%20great%20alternative%20to%20uncover%0Adecades-long%20environmental%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08544v1&entry.124074799=Read"},
{"title": "Multimodal Learning for Materials", "author": "Viggo Moro and Charlotte Loh and Rumen Dangovski and Ali Ghorashi and Andrew Ma and Zhuo Chen and Samuel Kim and Peter Y. Lu and Thomas Christensen and Marin Solja\u010di\u0107", "abstract": "  Artificial intelligence is transforming computational materials science,\nimproving the prediction of material properties, and accelerating the discovery\nof novel materials. Recently, publicly available material data repositories\nhave grown rapidly. This growth encompasses not only more materials, but also a\ngreater variety and quantity of their associated properties. Existing machine\nlearning efforts in materials science focus primarily on single-modality tasks,\ni.e., relationships between materials and a single physical property, thus not\ntaking advantage of the rich and multimodal set of material properties. Here,\nwe introduce Multimodal Learning for Materials (MultiMat), which enables\nself-supervised multi-modality training of foundation models for materials. We\ndemonstrate our framework's potential using data from the Materials Project\ndatabase on multiple axes: (i) MultiMat achieves state-of-the-art performance\nfor challenging material property prediction tasks; (ii) MultiMat enables novel\nand accurate material discovery via latent space similarity, enabling screening\nfor stable materials with desired properties; and (iii) MultiMat encodes\ninterpretable emergent features that may provide novel scientific insights.\n", "link": "http://arxiv.org/abs/2312.00111v3", "date": "2024-04-12", "relevancy": 1.7921, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5923}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5848}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Learning%20for%20Materials&body=Title%3A%20Multimodal%20Learning%20for%20Materials%0AAuthor%3A%20Viggo%20Moro%20and%20Charlotte%20Loh%20and%20Rumen%20Dangovski%20and%20Ali%20Ghorashi%20and%20Andrew%20Ma%20and%20Zhuo%20Chen%20and%20Samuel%20Kim%20and%20Peter%20Y.%20Lu%20and%20Thomas%20Christensen%20and%20Marin%20Solja%C4%8Di%C4%87%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20transforming%20computational%20materials%20science%2C%0Aimproving%20the%20prediction%20of%20material%20properties%2C%20and%20accelerating%20the%20discovery%0Aof%20novel%20materials.%20Recently%2C%20publicly%20available%20material%20data%20repositories%0Ahave%20grown%20rapidly.%20This%20growth%20encompasses%20not%20only%20more%20materials%2C%20but%20also%20a%0Agreater%20variety%20and%20quantity%20of%20their%20associated%20properties.%20Existing%20machine%0Alearning%20efforts%20in%20materials%20science%20focus%20primarily%20on%20single-modality%20tasks%2C%0Ai.e.%2C%20relationships%20between%20materials%20and%20a%20single%20physical%20property%2C%20thus%20not%0Ataking%20advantage%20of%20the%20rich%20and%20multimodal%20set%20of%20material%20properties.%20Here%2C%0Awe%20introduce%20Multimodal%20Learning%20for%20Materials%20%28MultiMat%29%2C%20which%20enables%0Aself-supervised%20multi-modality%20training%20of%20foundation%20models%20for%20materials.%20We%0Ademonstrate%20our%20framework%27s%20potential%20using%20data%20from%20the%20Materials%20Project%0Adatabase%20on%20multiple%20axes%3A%20%28i%29%20MultiMat%20achieves%20state-of-the-art%20performance%0Afor%20challenging%20material%20property%20prediction%20tasks%3B%20%28ii%29%20MultiMat%20enables%20novel%0Aand%20accurate%20material%20discovery%20via%20latent%20space%20similarity%2C%20enabling%20screening%0Afor%20stable%20materials%20with%20desired%20properties%3B%20and%20%28iii%29%20MultiMat%20encodes%0Ainterpretable%20emergent%20features%20that%20may%20provide%20novel%20scientific%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00111v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learning%20for%20Materials&entry.906535625=Viggo%20Moro%20and%20Charlotte%20Loh%20and%20Rumen%20Dangovski%20and%20Ali%20Ghorashi%20and%20Andrew%20Ma%20and%20Zhuo%20Chen%20and%20Samuel%20Kim%20and%20Peter%20Y.%20Lu%20and%20Thomas%20Christensen%20and%20Marin%20Solja%C4%8Di%C4%87&entry.1292438233=%20%20Artificial%20intelligence%20is%20transforming%20computational%20materials%20science%2C%0Aimproving%20the%20prediction%20of%20material%20properties%2C%20and%20accelerating%20the%20discovery%0Aof%20novel%20materials.%20Recently%2C%20publicly%20available%20material%20data%20repositories%0Ahave%20grown%20rapidly.%20This%20growth%20encompasses%20not%20only%20more%20materials%2C%20but%20also%20a%0Agreater%20variety%20and%20quantity%20of%20their%20associated%20properties.%20Existing%20machine%0Alearning%20efforts%20in%20materials%20science%20focus%20primarily%20on%20single-modality%20tasks%2C%0Ai.e.%2C%20relationships%20between%20materials%20and%20a%20single%20physical%20property%2C%20thus%20not%0Ataking%20advantage%20of%20the%20rich%20and%20multimodal%20set%20of%20material%20properties.%20Here%2C%0Awe%20introduce%20Multimodal%20Learning%20for%20Materials%20%28MultiMat%29%2C%20which%20enables%0Aself-supervised%20multi-modality%20training%20of%20foundation%20models%20for%20materials.%20We%0Ademonstrate%20our%20framework%27s%20potential%20using%20data%20from%20the%20Materials%20Project%0Adatabase%20on%20multiple%20axes%3A%20%28i%29%20MultiMat%20achieves%20state-of-the-art%20performance%0Afor%20challenging%20material%20property%20prediction%20tasks%3B%20%28ii%29%20MultiMat%20enables%20novel%0Aand%20accurate%20material%20discovery%20via%20latent%20space%20similarity%2C%20enabling%20screening%0Afor%20stable%20materials%20with%20desired%20properties%3B%20and%20%28iii%29%20MultiMat%20encodes%0Ainterpretable%20emergent%20features%20that%20may%20provide%20novel%20scientific%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00111v3&entry.124074799=Read"},
{"title": "Evolutionary Preference Sampling for Pareto Set Learning", "author": "Rongguang Ye and Longcan Chen and Jinyuan Zhang and Hisao Ishibuchi", "abstract": "  Recently, Pareto Set Learning (PSL) has been proposed for learning the entire\nPareto set using a neural network. PSL employs preference vectors to scalarize\nmultiple objectives, facilitating the learning of mappings from preference\nvectors to specific Pareto optimal solutions. Previous PSL methods have shown\ntheir effectiveness in solving artificial multi-objective optimization problems\n(MOPs) with uniform preference vector sampling. The quality of the learned\nPareto set is influenced by the sampling strategy of the preference vector, and\nthe sampling of the preference vector needs to be decided based on the Pareto\nfront shape. However, a fixed preference sampling strategy cannot\nsimultaneously adapt the Pareto front of multiple MOPs. To address this\nlimitation, this paper proposes an Evolutionary Preference Sampling (EPS)\nstrategy to efficiently sample preference vectors. Inspired by evolutionary\nalgorithms, we consider preference sampling as an evolutionary process to\ngenerate preference vectors for neural network training. We integrate the EPS\nstrategy into five advanced PSL methods. Extensive experiments demonstrate that\nour proposed method has a faster convergence speed than baseline algorithms on\n7 testing problems. Our implementation is available at\nhttps://github.com/rG223/EPS.\n", "link": "http://arxiv.org/abs/2404.08414v1", "date": "2024-04-12", "relevancy": 1.791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4674}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evolutionary%20Preference%20Sampling%20for%20Pareto%20Set%20Learning&body=Title%3A%20Evolutionary%20Preference%20Sampling%20for%20Pareto%20Set%20Learning%0AAuthor%3A%20Rongguang%20Ye%20and%20Longcan%20Chen%20and%20Jinyuan%20Zhang%20and%20Hisao%20Ishibuchi%0AAbstract%3A%20%20%20Recently%2C%20Pareto%20Set%20Learning%20%28PSL%29%20has%20been%20proposed%20for%20learning%20the%20entire%0APareto%20set%20using%20a%20neural%20network.%20PSL%20employs%20preference%20vectors%20to%20scalarize%0Amultiple%20objectives%2C%20facilitating%20the%20learning%20of%20mappings%20from%20preference%0Avectors%20to%20specific%20Pareto%20optimal%20solutions.%20Previous%20PSL%20methods%20have%20shown%0Atheir%20effectiveness%20in%20solving%20artificial%20multi-objective%20optimization%20problems%0A%28MOPs%29%20with%20uniform%20preference%20vector%20sampling.%20The%20quality%20of%20the%20learned%0APareto%20set%20is%20influenced%20by%20the%20sampling%20strategy%20of%20the%20preference%20vector%2C%20and%0Athe%20sampling%20of%20the%20preference%20vector%20needs%20to%20be%20decided%20based%20on%20the%20Pareto%0Afront%20shape.%20However%2C%20a%20fixed%20preference%20sampling%20strategy%20cannot%0Asimultaneously%20adapt%20the%20Pareto%20front%20of%20multiple%20MOPs.%20To%20address%20this%0Alimitation%2C%20this%20paper%20proposes%20an%20Evolutionary%20Preference%20Sampling%20%28EPS%29%0Astrategy%20to%20efficiently%20sample%20preference%20vectors.%20Inspired%20by%20evolutionary%0Aalgorithms%2C%20we%20consider%20preference%20sampling%20as%20an%20evolutionary%20process%20to%0Agenerate%20preference%20vectors%20for%20neural%20network%20training.%20We%20integrate%20the%20EPS%0Astrategy%20into%20five%20advanced%20PSL%20methods.%20Extensive%20experiments%20demonstrate%20that%0Aour%20proposed%20method%20has%20a%20faster%20convergence%20speed%20than%20baseline%20algorithms%20on%0A7%20testing%20problems.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/rG223/EPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08414v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolutionary%20Preference%20Sampling%20for%20Pareto%20Set%20Learning&entry.906535625=Rongguang%20Ye%20and%20Longcan%20Chen%20and%20Jinyuan%20Zhang%20and%20Hisao%20Ishibuchi&entry.1292438233=%20%20Recently%2C%20Pareto%20Set%20Learning%20%28PSL%29%20has%20been%20proposed%20for%20learning%20the%20entire%0APareto%20set%20using%20a%20neural%20network.%20PSL%20employs%20preference%20vectors%20to%20scalarize%0Amultiple%20objectives%2C%20facilitating%20the%20learning%20of%20mappings%20from%20preference%0Avectors%20to%20specific%20Pareto%20optimal%20solutions.%20Previous%20PSL%20methods%20have%20shown%0Atheir%20effectiveness%20in%20solving%20artificial%20multi-objective%20optimization%20problems%0A%28MOPs%29%20with%20uniform%20preference%20vector%20sampling.%20The%20quality%20of%20the%20learned%0APareto%20set%20is%20influenced%20by%20the%20sampling%20strategy%20of%20the%20preference%20vector%2C%20and%0Athe%20sampling%20of%20the%20preference%20vector%20needs%20to%20be%20decided%20based%20on%20the%20Pareto%0Afront%20shape.%20However%2C%20a%20fixed%20preference%20sampling%20strategy%20cannot%0Asimultaneously%20adapt%20the%20Pareto%20front%20of%20multiple%20MOPs.%20To%20address%20this%0Alimitation%2C%20this%20paper%20proposes%20an%20Evolutionary%20Preference%20Sampling%20%28EPS%29%0Astrategy%20to%20efficiently%20sample%20preference%20vectors.%20Inspired%20by%20evolutionary%0Aalgorithms%2C%20we%20consider%20preference%20sampling%20as%20an%20evolutionary%20process%20to%0Agenerate%20preference%20vectors%20for%20neural%20network%20training.%20We%20integrate%20the%20EPS%0Astrategy%20into%20five%20advanced%20PSL%20methods.%20Extensive%20experiments%20demonstrate%20that%0Aour%20proposed%20method%20has%20a%20faster%20convergence%20speed%20than%20baseline%20algorithms%20on%0A7%20testing%20problems.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/rG223/EPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08414v1&entry.124074799=Read"},
{"title": "PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction", "author": "Siming Shan and Pengkai Wang and Song Chen and Jiaxu Liu and Chao Xu and Shengze Cai", "abstract": "  The use of machine learning in fluid dynamics is becoming more common to\nexpedite the computation when solving forward and inverse problems of partial\ndifferential equations. Yet, a notable challenge with existing convolutional\nneural network (CNN)-based methods for data fidelity enhancement is their\nreliance on specific low-fidelity data patterns and distributions during the\ntraining phase. In addition, the CNN-based method essentially treats the flow\nreconstruction task as a computer vision task that prioritizes the element-wise\nprecision which lacks a physical and mathematical explanation. This dependence\ncan dramatically affect the models' effectiveness in real-world scenarios,\nespecially when the low-fidelity input deviates from the training data or\ncontains noise not accounted for during training. The introduction of diffusion\nmodels in this context shows promise for improving performance and\ngeneralizability. Unlike direct mapping from a specific low-fidelity to a\nhigh-fidelity distribution, diffusion models learn to transition from any\nlow-fidelity distribution towards a high-fidelity one. Our proposed model -\nPhysics-informed Residual Diffusion, demonstrates the capability to elevate the\nquality of data from both standard low-fidelity inputs, to low-fidelity inputs\nwith injected Gaussian noise, and randomly collected samples. By integrating\nphysics-based insights into the objective function, it further refines the\naccuracy and the fidelity of the inferred high-quality data. Experimental\nresults have shown that our approach can effectively reconstruct high-quality\noutcomes for two-dimensional turbulent flows from a range of low-fidelity input\nconditions without requiring retraining.\n", "link": "http://arxiv.org/abs/2404.08412v1", "date": "2024-04-12", "relevancy": 1.774, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6648}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5799}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5665}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction&body=Title%3A%20PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction%0AAuthor%3A%20Siming%20Shan%20and%20Pengkai%20Wang%20and%20Song%20Chen%20and%20Jiaxu%20Liu%20and%20Chao%20Xu%20and%20Shengze%20Cai%0AAbstract%3A%20%20%20The%20use%20of%20machine%20learning%20in%20fluid%20dynamics%20is%20becoming%20more%20common%20to%0Aexpedite%20the%20computation%20when%20solving%20forward%20and%20inverse%20problems%20of%20partial%0Adifferential%20equations.%20Yet%2C%20a%20notable%20challenge%20with%20existing%20convolutional%0Aneural%20network%20%28CNN%29-based%20methods%20for%20data%20fidelity%20enhancement%20is%20their%0Areliance%20on%20specific%20low-fidelity%20data%20patterns%20and%20distributions%20during%20the%0Atraining%20phase.%20In%20addition%2C%20the%20CNN-based%20method%20essentially%20treats%20the%20flow%0Areconstruction%20task%20as%20a%20computer%20vision%20task%20that%20prioritizes%20the%20element-wise%0Aprecision%20which%20lacks%20a%20physical%20and%20mathematical%20explanation.%20This%20dependence%0Acan%20dramatically%20affect%20the%20models%27%20effectiveness%20in%20real-world%20scenarios%2C%0Aespecially%20when%20the%20low-fidelity%20input%20deviates%20from%20the%20training%20data%20or%0Acontains%20noise%20not%20accounted%20for%20during%20training.%20The%20introduction%20of%20diffusion%0Amodels%20in%20this%20context%20shows%20promise%20for%20improving%20performance%20and%0Ageneralizability.%20Unlike%20direct%20mapping%20from%20a%20specific%20low-fidelity%20to%20a%0Ahigh-fidelity%20distribution%2C%20diffusion%20models%20learn%20to%20transition%20from%20any%0Alow-fidelity%20distribution%20towards%20a%20high-fidelity%20one.%20Our%20proposed%20model%20-%0APhysics-informed%20Residual%20Diffusion%2C%20demonstrates%20the%20capability%20to%20elevate%20the%0Aquality%20of%20data%20from%20both%20standard%20low-fidelity%20inputs%2C%20to%20low-fidelity%20inputs%0Awith%20injected%20Gaussian%20noise%2C%20and%20randomly%20collected%20samples.%20By%20integrating%0Aphysics-based%20insights%20into%20the%20objective%20function%2C%20it%20further%20refines%20the%0Aaccuracy%20and%20the%20fidelity%20of%20the%20inferred%20high-quality%20data.%20Experimental%0Aresults%20have%20shown%20that%20our%20approach%20can%20effectively%20reconstruct%20high-quality%0Aoutcomes%20for%20two-dimensional%20turbulent%20flows%20from%20a%20range%20of%20low-fidelity%20input%0Aconditions%20without%20requiring%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction&entry.906535625=Siming%20Shan%20and%20Pengkai%20Wang%20and%20Song%20Chen%20and%20Jiaxu%20Liu%20and%20Chao%20Xu%20and%20Shengze%20Cai&entry.1292438233=%20%20The%20use%20of%20machine%20learning%20in%20fluid%20dynamics%20is%20becoming%20more%20common%20to%0Aexpedite%20the%20computation%20when%20solving%20forward%20and%20inverse%20problems%20of%20partial%0Adifferential%20equations.%20Yet%2C%20a%20notable%20challenge%20with%20existing%20convolutional%0Aneural%20network%20%28CNN%29-based%20methods%20for%20data%20fidelity%20enhancement%20is%20their%0Areliance%20on%20specific%20low-fidelity%20data%20patterns%20and%20distributions%20during%20the%0Atraining%20phase.%20In%20addition%2C%20the%20CNN-based%20method%20essentially%20treats%20the%20flow%0Areconstruction%20task%20as%20a%20computer%20vision%20task%20that%20prioritizes%20the%20element-wise%0Aprecision%20which%20lacks%20a%20physical%20and%20mathematical%20explanation.%20This%20dependence%0Acan%20dramatically%20affect%20the%20models%27%20effectiveness%20in%20real-world%20scenarios%2C%0Aespecially%20when%20the%20low-fidelity%20input%20deviates%20from%20the%20training%20data%20or%0Acontains%20noise%20not%20accounted%20for%20during%20training.%20The%20introduction%20of%20diffusion%0Amodels%20in%20this%20context%20shows%20promise%20for%20improving%20performance%20and%0Ageneralizability.%20Unlike%20direct%20mapping%20from%20a%20specific%20low-fidelity%20to%20a%0Ahigh-fidelity%20distribution%2C%20diffusion%20models%20learn%20to%20transition%20from%20any%0Alow-fidelity%20distribution%20towards%20a%20high-fidelity%20one.%20Our%20proposed%20model%20-%0APhysics-informed%20Residual%20Diffusion%2C%20demonstrates%20the%20capability%20to%20elevate%20the%0Aquality%20of%20data%20from%20both%20standard%20low-fidelity%20inputs%2C%20to%20low-fidelity%20inputs%0Awith%20injected%20Gaussian%20noise%2C%20and%20randomly%20collected%20samples.%20By%20integrating%0Aphysics-based%20insights%20into%20the%20objective%20function%2C%20it%20further%20refines%20the%0Aaccuracy%20and%20the%20fidelity%20of%20the%20inferred%20high-quality%20data.%20Experimental%0Aresults%20have%20shown%20that%20our%20approach%20can%20effectively%20reconstruct%20high-quality%0Aoutcomes%20for%20two-dimensional%20turbulent%20flows%20from%20a%20range%20of%20low-fidelity%20input%0Aconditions%20without%20requiring%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08412v1&entry.124074799=Read"},
{"title": "Learning Adaptive Multi-Objective Robot Navigation with Demonstrations", "author": "Jorge de Heuvel and Tharun Sethuraman and Maren Bennewitz", "abstract": "  Preference-aligned robot navigation in human environments is typically\nachieved through learning-based approaches, utilizing demonstrations and user\nfeedback for personalization. However, personal preferences are subject to\nchange and might even be context-dependent. Yet traditional reinforcement\nlearning (RL) approaches with a static reward function often fall short in\nadapting to these varying user preferences. This paper introduces a framework\nthat combines multi-objective reinforcement learning (MORL) with\ndemonstration-based learning. Our approach allows for dynamic adaptation to\nchanging user preferences without retraining. Through rigorous evaluations,\nincluding sim-to-real and robot-to-robot transfers, we demonstrate our\nframework's capability to reflect user preferences accurately while achieving\nhigh navigational performance in terms of collision avoidance and goal\npursuance.\n", "link": "http://arxiv.org/abs/2404.04857v2", "date": "2024-04-12", "relevancy": 1.7738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6145}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5869}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.579}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Adaptive%20Multi-Objective%20Robot%20Navigation%20with%20Demonstrations&body=Title%3A%20Learning%20Adaptive%20Multi-Objective%20Robot%20Navigation%20with%20Demonstrations%0AAuthor%3A%20Jorge%20de%20Heuvel%20and%20Tharun%20Sethuraman%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20Preference-aligned%20robot%20navigation%20in%20human%20environments%20is%20typically%0Aachieved%20through%20learning-based%20approaches%2C%20utilizing%20demonstrations%20and%20user%0Afeedback%20for%20personalization.%20However%2C%20personal%20preferences%20are%20subject%20to%0Achange%20and%20might%20even%20be%20context-dependent.%20Yet%20traditional%20reinforcement%0Alearning%20%28RL%29%20approaches%20with%20a%20static%20reward%20function%20often%20fall%20short%20in%0Aadapting%20to%20these%20varying%20user%20preferences.%20This%20paper%20introduces%20a%20framework%0Athat%20combines%20multi-objective%20reinforcement%20learning%20%28MORL%29%20with%0Ademonstration-based%20learning.%20Our%20approach%20allows%20for%20dynamic%20adaptation%20to%0Achanging%20user%20preferences%20without%20retraining.%20Through%20rigorous%20evaluations%2C%0Aincluding%20sim-to-real%20and%20robot-to-robot%20transfers%2C%20we%20demonstrate%20our%0Aframework%27s%20capability%20to%20reflect%20user%20preferences%20accurately%20while%20achieving%0Ahigh%20navigational%20performance%20in%20terms%20of%20collision%20avoidance%20and%20goal%0Apursuance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04857v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Adaptive%20Multi-Objective%20Robot%20Navigation%20with%20Demonstrations&entry.906535625=Jorge%20de%20Heuvel%20and%20Tharun%20Sethuraman%20and%20Maren%20Bennewitz&entry.1292438233=%20%20Preference-aligned%20robot%20navigation%20in%20human%20environments%20is%20typically%0Aachieved%20through%20learning-based%20approaches%2C%20utilizing%20demonstrations%20and%20user%0Afeedback%20for%20personalization.%20However%2C%20personal%20preferences%20are%20subject%20to%0Achange%20and%20might%20even%20be%20context-dependent.%20Yet%20traditional%20reinforcement%0Alearning%20%28RL%29%20approaches%20with%20a%20static%20reward%20function%20often%20fall%20short%20in%0Aadapting%20to%20these%20varying%20user%20preferences.%20This%20paper%20introduces%20a%20framework%0Athat%20combines%20multi-objective%20reinforcement%20learning%20%28MORL%29%20with%0Ademonstration-based%20learning.%20Our%20approach%20allows%20for%20dynamic%20adaptation%20to%0Achanging%20user%20preferences%20without%20retraining.%20Through%20rigorous%20evaluations%2C%0Aincluding%20sim-to-real%20and%20robot-to-robot%20transfers%2C%20we%20demonstrate%20our%0Aframework%27s%20capability%20to%20reflect%20user%20preferences%20accurately%20while%20achieving%0Ahigh%20navigational%20performance%20in%20terms%20of%20collision%20avoidance%20and%20goal%0Apursuance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04857v2&entry.124074799=Read"},
{"title": "Decoding AI: The inside story of data analysis in ChatGPT", "author": "Ozan Evkaya and Miguel de Carvalho", "abstract": "  As a result of recent advancements in generative AI, the field of Data\nScience is prone to various changes. This review critically examines the Data\nAnalysis (DA) capabilities of ChatGPT assessing its performance across a wide\nrange of tasks. While DA provides researchers and practitioners with\nunprecedented analytical capabilities, it is far from being perfect, and it is\nimportant to recognize and address its limitations.\n", "link": "http://arxiv.org/abs/2404.08480v1", "date": "2024-04-12", "relevancy": 1.769, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4663}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4347}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4212}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decoding%20AI%3A%20The%20inside%20story%20of%20data%20analysis%20in%20ChatGPT&body=Title%3A%20Decoding%20AI%3A%20The%20inside%20story%20of%20data%20analysis%20in%20ChatGPT%0AAuthor%3A%20Ozan%20Evkaya%20and%20Miguel%20de%20Carvalho%0AAbstract%3A%20%20%20As%20a%20result%20of%20recent%20advancements%20in%20generative%20AI%2C%20the%20field%20of%20Data%0AScience%20is%20prone%20to%20various%20changes.%20This%20review%20critically%20examines%20the%20Data%0AAnalysis%20%28DA%29%20capabilities%20of%20ChatGPT%20assessing%20its%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20While%20DA%20provides%20researchers%20and%20practitioners%20with%0Aunprecedented%20analytical%20capabilities%2C%20it%20is%20far%20from%20being%20perfect%2C%20and%20it%20is%0Aimportant%20to%20recognize%20and%20address%20its%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08480v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20AI%3A%20The%20inside%20story%20of%20data%20analysis%20in%20ChatGPT&entry.906535625=Ozan%20Evkaya%20and%20Miguel%20de%20Carvalho&entry.1292438233=%20%20As%20a%20result%20of%20recent%20advancements%20in%20generative%20AI%2C%20the%20field%20of%20Data%0AScience%20is%20prone%20to%20various%20changes.%20This%20review%20critically%20examines%20the%20Data%0AAnalysis%20%28DA%29%20capabilities%20of%20ChatGPT%20assessing%20its%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20While%20DA%20provides%20researchers%20and%20practitioners%20with%0Aunprecedented%20analytical%20capabilities%2C%20it%20is%20far%20from%20being%20perfect%2C%20and%20it%20is%0Aimportant%20to%20recognize%20and%20address%20its%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08480v1&entry.124074799=Read"},
{"title": "Toward Reliable Human Pose Forecasting with Uncertainty", "author": "Saeed Saadatnejad and Mehrshad Mirmohammadi and Matin Daghyani and Parham Saremi and Yashar Zoroofchi Benisi and Amirhossein Alimohammadi and Zahra Tehraninasab and Taylor Mordan and Alexandre Alahi", "abstract": "  Recently, there has been an arms race of pose forecasting methods aimed at\nsolving the spatio-temporal task of predicting a sequence of future 3D poses of\na person given a sequence of past observed ones. However, the lack of unified\nbenchmarks and limited uncertainty analysis have hindered progress in the\nfield. To address this, we first develop an open-source library for human pose\nforecasting, including multiple models, supporting several datasets, and\nemploying standardized evaluation metrics, with the aim of promoting research\nand moving toward a unified and consistent evaluation. Second, we devise two\ntypes of uncertainty in the problem to increase performance and convey better\ntrust: 1) we propose a method for modeling aleatoric uncertainty by using\nuncertainty priors to inject knowledge about the pattern of uncertainty. This\nfocuses the capacity of the model in the direction of more meaningful\nsupervision while reducing the number of learned parameters and improving\nstability; 2) we introduce a novel approach for quantifying the epistemic\nuncertainty of any model through clustering and measuring the entropy of its\nassignments. Our experiments demonstrate up to $25\\%$ improvements in\nforecasting at short horizons, with no loss on longer horizons on Human3.6M,\nAMSS, and 3DPW datasets, and better performance in uncertainty estimation. The\ncode is available online at https://github.com/vita-epfl/UnPOSed.\n", "link": "http://arxiv.org/abs/2304.06707v2", "date": "2024-04-12", "relevancy": 1.7432, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6106}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5767}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20Reliable%20Human%20Pose%20Forecasting%20with%20Uncertainty&body=Title%3A%20Toward%20Reliable%20Human%20Pose%20Forecasting%20with%20Uncertainty%0AAuthor%3A%20Saeed%20Saadatnejad%20and%20Mehrshad%20Mirmohammadi%20and%20Matin%20Daghyani%20and%20Parham%20Saremi%20and%20Yashar%20Zoroofchi%20Benisi%20and%20Amirhossein%20Alimohammadi%20and%20Zahra%20Tehraninasab%20and%20Taylor%20Mordan%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20an%20arms%20race%20of%20pose%20forecasting%20methods%20aimed%20at%0Asolving%20the%20spatio-temporal%20task%20of%20predicting%20a%20sequence%20of%20future%203D%20poses%20of%0Aa%20person%20given%20a%20sequence%20of%20past%20observed%20ones.%20However%2C%20the%20lack%20of%20unified%0Abenchmarks%20and%20limited%20uncertainty%20analysis%20have%20hindered%20progress%20in%20the%0Afield.%20To%20address%20this%2C%20we%20first%20develop%20an%20open-source%20library%20for%20human%20pose%0Aforecasting%2C%20including%20multiple%20models%2C%20supporting%20several%20datasets%2C%20and%0Aemploying%20standardized%20evaluation%20metrics%2C%20with%20the%20aim%20of%20promoting%20research%0Aand%20moving%20toward%20a%20unified%20and%20consistent%20evaluation.%20Second%2C%20we%20devise%20two%0Atypes%20of%20uncertainty%20in%20the%20problem%20to%20increase%20performance%20and%20convey%20better%0Atrust%3A%201%29%20we%20propose%20a%20method%20for%20modeling%20aleatoric%20uncertainty%20by%20using%0Auncertainty%20priors%20to%20inject%20knowledge%20about%20the%20pattern%20of%20uncertainty.%20This%0Afocuses%20the%20capacity%20of%20the%20model%20in%20the%20direction%20of%20more%20meaningful%0Asupervision%20while%20reducing%20the%20number%20of%20learned%20parameters%20and%20improving%0Astability%3B%202%29%20we%20introduce%20a%20novel%20approach%20for%20quantifying%20the%20epistemic%0Auncertainty%20of%20any%20model%20through%20clustering%20and%20measuring%20the%20entropy%20of%20its%0Aassignments.%20Our%20experiments%20demonstrate%20up%20to%20%2425%5C%25%24%20improvements%20in%0Aforecasting%20at%20short%20horizons%2C%20with%20no%20loss%20on%20longer%20horizons%20on%20Human3.6M%2C%0AAMSS%2C%20and%203DPW%20datasets%2C%20and%20better%20performance%20in%20uncertainty%20estimation.%20The%0Acode%20is%20available%20online%20at%20https%3A//github.com/vita-epfl/UnPOSed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06707v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Reliable%20Human%20Pose%20Forecasting%20with%20Uncertainty&entry.906535625=Saeed%20Saadatnejad%20and%20Mehrshad%20Mirmohammadi%20and%20Matin%20Daghyani%20and%20Parham%20Saremi%20and%20Yashar%20Zoroofchi%20Benisi%20and%20Amirhossein%20Alimohammadi%20and%20Zahra%20Tehraninasab%20and%20Taylor%20Mordan%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20an%20arms%20race%20of%20pose%20forecasting%20methods%20aimed%20at%0Asolving%20the%20spatio-temporal%20task%20of%20predicting%20a%20sequence%20of%20future%203D%20poses%20of%0Aa%20person%20given%20a%20sequence%20of%20past%20observed%20ones.%20However%2C%20the%20lack%20of%20unified%0Abenchmarks%20and%20limited%20uncertainty%20analysis%20have%20hindered%20progress%20in%20the%0Afield.%20To%20address%20this%2C%20we%20first%20develop%20an%20open-source%20library%20for%20human%20pose%0Aforecasting%2C%20including%20multiple%20models%2C%20supporting%20several%20datasets%2C%20and%0Aemploying%20standardized%20evaluation%20metrics%2C%20with%20the%20aim%20of%20promoting%20research%0Aand%20moving%20toward%20a%20unified%20and%20consistent%20evaluation.%20Second%2C%20we%20devise%20two%0Atypes%20of%20uncertainty%20in%20the%20problem%20to%20increase%20performance%20and%20convey%20better%0Atrust%3A%201%29%20we%20propose%20a%20method%20for%20modeling%20aleatoric%20uncertainty%20by%20using%0Auncertainty%20priors%20to%20inject%20knowledge%20about%20the%20pattern%20of%20uncertainty.%20This%0Afocuses%20the%20capacity%20of%20the%20model%20in%20the%20direction%20of%20more%20meaningful%0Asupervision%20while%20reducing%20the%20number%20of%20learned%20parameters%20and%20improving%0Astability%3B%202%29%20we%20introduce%20a%20novel%20approach%20for%20quantifying%20the%20epistemic%0Auncertainty%20of%20any%20model%20through%20clustering%20and%20measuring%20the%20entropy%20of%20its%0Aassignments.%20Our%20experiments%20demonstrate%20up%20to%20%2425%5C%25%24%20improvements%20in%0Aforecasting%20at%20short%20horizons%2C%20with%20no%20loss%20on%20longer%20horizons%20on%20Human3.6M%2C%0AAMSS%2C%20and%203DPW%20datasets%2C%20and%20better%20performance%20in%20uncertainty%20estimation.%20The%0Acode%20is%20available%20online%20at%20https%3A//github.com/vita-epfl/UnPOSed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06707v2&entry.124074799=Read"},
{"title": "On the Robustness of Language Guidance for Low-Level Vision Tasks:\n  Findings from Depth Estimation", "author": "Agneet Chatterjee and Tejas Gokhale and Chitta Baral and Yezhou Yang", "abstract": "  Recent advances in monocular depth estimation have been made by incorporating\nnatural language as additional guidance. Although yielding impressive results,\nthe impact of the language prior, particularly in terms of generalization and\nrobustness, remains unexplored. In this paper, we address this gap by\nquantifying the impact of this prior and introduce methods to benchmark its\neffectiveness across various settings. We generate \"low-level\" sentences that\nconvey object-centric, three-dimensional spatial relationships, incorporate\nthem as additional language priors and evaluate their downstream impact on\ndepth estimation. Our key finding is that current language-guided depth\nestimators perform optimally only with scene-level descriptions and\ncounter-intuitively fare worse with low level descriptions. Despite leveraging\nadditional data, these methods are not robust to directed adversarial attacks\nand decline in performance with an increase in distribution shift. Finally, to\nprovide a foundation for future research, we identify points of failures and\noffer insights to better understand these shortcomings. With an increasing\nnumber of methods using language for depth estimation, our findings highlight\nthe opportunities and pitfalls that require careful consideration for effective\ndeployment in real-world settings\n", "link": "http://arxiv.org/abs/2404.08540v1", "date": "2024-04-12", "relevancy": 1.7426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5962}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5651}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Language%20Guidance%20for%20Low-Level%20Vision%20Tasks%3A%0A%20%20Findings%20from%20Depth%20Estimation&body=Title%3A%20On%20the%20Robustness%20of%20Language%20Guidance%20for%20Low-Level%20Vision%20Tasks%3A%0A%20%20Findings%20from%20Depth%20Estimation%0AAuthor%3A%20Agneet%20Chatterjee%20and%20Tejas%20Gokhale%20and%20Chitta%20Baral%20and%20Yezhou%20Yang%0AAbstract%3A%20%20%20Recent%20advances%20in%20monocular%20depth%20estimation%20have%20been%20made%20by%20incorporating%0Anatural%20language%20as%20additional%20guidance.%20Although%20yielding%20impressive%20results%2C%0Athe%20impact%20of%20the%20language%20prior%2C%20particularly%20in%20terms%20of%20generalization%20and%0Arobustness%2C%20remains%20unexplored.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%0Aquantifying%20the%20impact%20of%20this%20prior%20and%20introduce%20methods%20to%20benchmark%20its%0Aeffectiveness%20across%20various%20settings.%20We%20generate%20%22low-level%22%20sentences%20that%0Aconvey%20object-centric%2C%20three-dimensional%20spatial%20relationships%2C%20incorporate%0Athem%20as%20additional%20language%20priors%20and%20evaluate%20their%20downstream%20impact%20on%0Adepth%20estimation.%20Our%20key%20finding%20is%20that%20current%20language-guided%20depth%0Aestimators%20perform%20optimally%20only%20with%20scene-level%20descriptions%20and%0Acounter-intuitively%20fare%20worse%20with%20low%20level%20descriptions.%20Despite%20leveraging%0Aadditional%20data%2C%20these%20methods%20are%20not%20robust%20to%20directed%20adversarial%20attacks%0Aand%20decline%20in%20performance%20with%20an%20increase%20in%20distribution%20shift.%20Finally%2C%20to%0Aprovide%20a%20foundation%20for%20future%20research%2C%20we%20identify%20points%20of%20failures%20and%0Aoffer%20insights%20to%20better%20understand%20these%20shortcomings.%20With%20an%20increasing%0Anumber%20of%20methods%20using%20language%20for%20depth%20estimation%2C%20our%20findings%20highlight%0Athe%20opportunities%20and%20pitfalls%20that%20require%20careful%20consideration%20for%20effective%0Adeployment%20in%20real-world%20settings%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08540v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Language%20Guidance%20for%20Low-Level%20Vision%20Tasks%3A%0A%20%20Findings%20from%20Depth%20Estimation&entry.906535625=Agneet%20Chatterjee%20and%20Tejas%20Gokhale%20and%20Chitta%20Baral%20and%20Yezhou%20Yang&entry.1292438233=%20%20Recent%20advances%20in%20monocular%20depth%20estimation%20have%20been%20made%20by%20incorporating%0Anatural%20language%20as%20additional%20guidance.%20Although%20yielding%20impressive%20results%2C%0Athe%20impact%20of%20the%20language%20prior%2C%20particularly%20in%20terms%20of%20generalization%20and%0Arobustness%2C%20remains%20unexplored.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%0Aquantifying%20the%20impact%20of%20this%20prior%20and%20introduce%20methods%20to%20benchmark%20its%0Aeffectiveness%20across%20various%20settings.%20We%20generate%20%22low-level%22%20sentences%20that%0Aconvey%20object-centric%2C%20three-dimensional%20spatial%20relationships%2C%20incorporate%0Athem%20as%20additional%20language%20priors%20and%20evaluate%20their%20downstream%20impact%20on%0Adepth%20estimation.%20Our%20key%20finding%20is%20that%20current%20language-guided%20depth%0Aestimators%20perform%20optimally%20only%20with%20scene-level%20descriptions%20and%0Acounter-intuitively%20fare%20worse%20with%20low%20level%20descriptions.%20Despite%20leveraging%0Aadditional%20data%2C%20these%20methods%20are%20not%20robust%20to%20directed%20adversarial%20attacks%0Aand%20decline%20in%20performance%20with%20an%20increase%20in%20distribution%20shift.%20Finally%2C%20to%0Aprovide%20a%20foundation%20for%20future%20research%2C%20we%20identify%20points%20of%20failures%20and%0Aoffer%20insights%20to%20better%20understand%20these%20shortcomings.%20With%20an%20increasing%0Anumber%20of%20methods%20using%20language%20for%20depth%20estimation%2C%20our%20findings%20highlight%0Athe%20opportunities%20and%20pitfalls%20that%20require%20careful%20consideration%20for%20effective%0Adeployment%20in%20real-world%20settings%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08540v1&entry.124074799=Read"},
{"title": "ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification", "author": "Ahmad Sajedi and Samir Khaki and Yuri A. Lawryshyn and Konstantinos N. Plataniotis", "abstract": "  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n", "link": "http://arxiv.org/abs/2401.01448v2", "date": "2024-04-12", "relevancy": 1.7406, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5783}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ProbMCL%3A%20Simple%20Probabilistic%20Contrastive%20Learning%20for%20Multi-label%0A%20%20Visual%20Classification&body=Title%3A%20ProbMCL%3A%20Simple%20Probabilistic%20Contrastive%20Learning%20for%20Multi-label%0A%20%20Visual%20Classification%0AAuthor%3A%20Ahmad%20Sajedi%20and%20Samir%20Khaki%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis%0AAbstract%3A%20%20%20Multi-label%20image%20classification%20presents%20a%20challenging%20task%20in%20many%20domains%2C%0Aincluding%20computer%20vision%20and%20medical%20imaging.%20Recent%20advancements%20have%0Aintroduced%20graph-based%20and%20transformer-based%20methods%20to%20improve%20performance%20and%0Acapture%20label%20dependencies.%20However%2C%20these%20methods%20often%20include%20complex%0Amodules%20that%20entail%20heavy%20computation%20and%20lack%20interpretability.%20In%20this%20paper%2C%0Awe%20propose%20Probabilistic%20Multi-label%20Contrastive%20Learning%20%28ProbMCL%29%2C%20a%20novel%0Aframework%20to%20address%20these%20challenges%20in%20multi-label%20image%20classification%0Atasks.%20Our%20simple%20yet%20effective%20approach%20employs%20supervised%20contrastive%0Alearning%2C%20in%20which%20samples%20that%20share%20enough%20labels%20with%20an%20anchor%20image%20based%0Aon%20a%20decision%20threshold%20are%20introduced%20as%20a%20positive%20set.%20This%20structure%0Acaptures%20label%20dependencies%20by%20pulling%20positive%20pair%20embeddings%20together%20and%0Apushing%20away%20negative%20samples%20that%20fall%20below%20the%20threshold.%20We%20enhance%0Arepresentation%20learning%20by%20incorporating%20a%20mixture%20density%20network%20into%0Acontrastive%20learning%20and%20generating%20Gaussian%20mixture%20distributions%20to%20explore%0Athe%20epistemic%20uncertainty%20of%20the%20feature%20encoder.%20We%20validate%20the%20effectiveness%0Aof%20our%20framework%20through%20experimentation%20with%20datasets%20from%20the%20computer%20vision%0Aand%20medical%20imaging%20domains.%20Our%20method%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20while%20achieving%20a%20low%20computational%20footprint%20on%20both%0Adatasets.%20Visualization%20analyses%20also%20demonstrate%20that%20ProbMCL-learned%0Aclassifiers%20maintain%20a%20meaningful%20semantic%20topology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01448v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbMCL%3A%20Simple%20Probabilistic%20Contrastive%20Learning%20for%20Multi-label%0A%20%20Visual%20Classification&entry.906535625=Ahmad%20Sajedi%20and%20Samir%20Khaki%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis&entry.1292438233=%20%20Multi-label%20image%20classification%20presents%20a%20challenging%20task%20in%20many%20domains%2C%0Aincluding%20computer%20vision%20and%20medical%20imaging.%20Recent%20advancements%20have%0Aintroduced%20graph-based%20and%20transformer-based%20methods%20to%20improve%20performance%20and%0Acapture%20label%20dependencies.%20However%2C%20these%20methods%20often%20include%20complex%0Amodules%20that%20entail%20heavy%20computation%20and%20lack%20interpretability.%20In%20this%20paper%2C%0Awe%20propose%20Probabilistic%20Multi-label%20Contrastive%20Learning%20%28ProbMCL%29%2C%20a%20novel%0Aframework%20to%20address%20these%20challenges%20in%20multi-label%20image%20classification%0Atasks.%20Our%20simple%20yet%20effective%20approach%20employs%20supervised%20contrastive%0Alearning%2C%20in%20which%20samples%20that%20share%20enough%20labels%20with%20an%20anchor%20image%20based%0Aon%20a%20decision%20threshold%20are%20introduced%20as%20a%20positive%20set.%20This%20structure%0Acaptures%20label%20dependencies%20by%20pulling%20positive%20pair%20embeddings%20together%20and%0Apushing%20away%20negative%20samples%20that%20fall%20below%20the%20threshold.%20We%20enhance%0Arepresentation%20learning%20by%20incorporating%20a%20mixture%20density%20network%20into%0Acontrastive%20learning%20and%20generating%20Gaussian%20mixture%20distributions%20to%20explore%0Athe%20epistemic%20uncertainty%20of%20the%20feature%20encoder.%20We%20validate%20the%20effectiveness%0Aof%20our%20framework%20through%20experimentation%20with%20datasets%20from%20the%20computer%20vision%0Aand%20medical%20imaging%20domains.%20Our%20method%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20while%20achieving%20a%20low%20computational%20footprint%20on%20both%0Adatasets.%20Visualization%20analyses%20also%20demonstrate%20that%20ProbMCL-learned%0Aclassifiers%20maintain%20a%20meaningful%20semantic%20topology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01448v2&entry.124074799=Read"},
{"title": "Generalization in diffusion models arises from geometry-adaptive\n  harmonic representations", "author": "Zahra Kadkhodaie and Florentin Guth and Eero P. Simoncelli and St\u00e9phane Mallat", "abstract": "  Deep neural networks (DNNs) trained for image denoising are able to generate\nhigh-quality samples with score-based reverse diffusion algorithms. These\nimpressive capabilities seem to imply an escape from the curse of\ndimensionality, but recent reports of memorization of the training set raise\nthe question of whether these networks are learning the \"true\" continuous\ndensity of the data. Here, we show that two DNNs trained on non-overlapping\nsubsets of a dataset learn nearly the same score function, and thus the same\ndensity, when the number of training images is large enough. In this regime of\nstrong generalization, diffusion-generated images are distinct from the\ntraining set, and are of high visual quality, suggesting that the inductive\nbiases of the DNNs are well-aligned with the data density. We analyze the\nlearned denoising functions and show that the inductive biases give rise to a\nshrinkage operation in a basis adapted to the underlying image. Examination of\nthese bases reveals oscillating harmonic structures along contours and in\nhomogeneous regions. We demonstrate that trained denoisers are inductively\nbiased towards these geometry-adaptive harmonic bases since they arise not only\nwhen the network is trained on photographic images, but also when it is trained\non image classes supported on low-dimensional manifolds for which the harmonic\nbasis is suboptimal. Finally, we show that when trained on regular image\nclasses for which the optimal basis is known to be geometry-adaptive and\nharmonic, the denoising performance of the networks is near-optimal.\n", "link": "http://arxiv.org/abs/2310.02557v3", "date": "2024-04-12", "relevancy": 1.7105, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5963}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5535}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalization%20in%20diffusion%20models%20arises%20from%20geometry-adaptive%0A%20%20harmonic%20representations&body=Title%3A%20Generalization%20in%20diffusion%20models%20arises%20from%20geometry-adaptive%0A%20%20harmonic%20representations%0AAuthor%3A%20Zahra%20Kadkhodaie%20and%20Florentin%20Guth%20and%20Eero%20P.%20Simoncelli%20and%20St%C3%A9phane%20Mallat%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20trained%20for%20image%20denoising%20are%20able%20to%20generate%0Ahigh-quality%20samples%20with%20score-based%20reverse%20diffusion%20algorithms.%20These%0Aimpressive%20capabilities%20seem%20to%20imply%20an%20escape%20from%20the%20curse%20of%0Adimensionality%2C%20but%20recent%20reports%20of%20memorization%20of%20the%20training%20set%20raise%0Athe%20question%20of%20whether%20these%20networks%20are%20learning%20the%20%22true%22%20continuous%0Adensity%20of%20the%20data.%20Here%2C%20we%20show%20that%20two%20DNNs%20trained%20on%20non-overlapping%0Asubsets%20of%20a%20dataset%20learn%20nearly%20the%20same%20score%20function%2C%20and%20thus%20the%20same%0Adensity%2C%20when%20the%20number%20of%20training%20images%20is%20large%20enough.%20In%20this%20regime%20of%0Astrong%20generalization%2C%20diffusion-generated%20images%20are%20distinct%20from%20the%0Atraining%20set%2C%20and%20are%20of%20high%20visual%20quality%2C%20suggesting%20that%20the%20inductive%0Abiases%20of%20the%20DNNs%20are%20well-aligned%20with%20the%20data%20density.%20We%20analyze%20the%0Alearned%20denoising%20functions%20and%20show%20that%20the%20inductive%20biases%20give%20rise%20to%20a%0Ashrinkage%20operation%20in%20a%20basis%20adapted%20to%20the%20underlying%20image.%20Examination%20of%0Athese%20bases%20reveals%20oscillating%20harmonic%20structures%20along%20contours%20and%20in%0Ahomogeneous%20regions.%20We%20demonstrate%20that%20trained%20denoisers%20are%20inductively%0Abiased%20towards%20these%20geometry-adaptive%20harmonic%20bases%20since%20they%20arise%20not%20only%0Awhen%20the%20network%20is%20trained%20on%20photographic%20images%2C%20but%20also%20when%20it%20is%20trained%0Aon%20image%20classes%20supported%20on%20low-dimensional%20manifolds%20for%20which%20the%20harmonic%0Abasis%20is%20suboptimal.%20Finally%2C%20we%20show%20that%20when%20trained%20on%20regular%20image%0Aclasses%20for%20which%20the%20optimal%20basis%20is%20known%20to%20be%20geometry-adaptive%20and%0Aharmonic%2C%20the%20denoising%20performance%20of%20the%20networks%20is%20near-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02557v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20in%20diffusion%20models%20arises%20from%20geometry-adaptive%0A%20%20harmonic%20representations&entry.906535625=Zahra%20Kadkhodaie%20and%20Florentin%20Guth%20and%20Eero%20P.%20Simoncelli%20and%20St%C3%A9phane%20Mallat&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20trained%20for%20image%20denoising%20are%20able%20to%20generate%0Ahigh-quality%20samples%20with%20score-based%20reverse%20diffusion%20algorithms.%20These%0Aimpressive%20capabilities%20seem%20to%20imply%20an%20escape%20from%20the%20curse%20of%0Adimensionality%2C%20but%20recent%20reports%20of%20memorization%20of%20the%20training%20set%20raise%0Athe%20question%20of%20whether%20these%20networks%20are%20learning%20the%20%22true%22%20continuous%0Adensity%20of%20the%20data.%20Here%2C%20we%20show%20that%20two%20DNNs%20trained%20on%20non-overlapping%0Asubsets%20of%20a%20dataset%20learn%20nearly%20the%20same%20score%20function%2C%20and%20thus%20the%20same%0Adensity%2C%20when%20the%20number%20of%20training%20images%20is%20large%20enough.%20In%20this%20regime%20of%0Astrong%20generalization%2C%20diffusion-generated%20images%20are%20distinct%20from%20the%0Atraining%20set%2C%20and%20are%20of%20high%20visual%20quality%2C%20suggesting%20that%20the%20inductive%0Abiases%20of%20the%20DNNs%20are%20well-aligned%20with%20the%20data%20density.%20We%20analyze%20the%0Alearned%20denoising%20functions%20and%20show%20that%20the%20inductive%20biases%20give%20rise%20to%20a%0Ashrinkage%20operation%20in%20a%20basis%20adapted%20to%20the%20underlying%20image.%20Examination%20of%0Athese%20bases%20reveals%20oscillating%20harmonic%20structures%20along%20contours%20and%20in%0Ahomogeneous%20regions.%20We%20demonstrate%20that%20trained%20denoisers%20are%20inductively%0Abiased%20towards%20these%20geometry-adaptive%20harmonic%20bases%20since%20they%20arise%20not%20only%0Awhen%20the%20network%20is%20trained%20on%20photographic%20images%2C%20but%20also%20when%20it%20is%20trained%0Aon%20image%20classes%20supported%20on%20low-dimensional%20manifolds%20for%20which%20the%20harmonic%0Abasis%20is%20suboptimal.%20Finally%2C%20we%20show%20that%20when%20trained%20on%20regular%20image%0Aclasses%20for%20which%20the%20optimal%20basis%20is%20known%20to%20be%20geometry-adaptive%20and%0Aharmonic%2C%20the%20denoising%20performance%20of%20the%20networks%20is%20near-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02557v3&entry.124074799=Read"},
{"title": "NIR-Assisted Image Denoising: A Selective Fusion Approach and A\n  Real-World Benchmark Datase", "author": "Rongjian Xu and Zhilu Zhang and Renlong Wu and Wangmeng Zuo", "abstract": "  Despite the significant progress in image denoising, it is still challenging\nto restore fine-scale details while removing noise, especially in extremely\nlow-light environments. Leveraging near-infrared (NIR) images to assist visible\nRGB image denoising shows the potential to address this issue, becoming a\npromising technology. Nonetheless, existing works still struggle with taking\nadvantage of NIR information effectively for real-world image denoising, due to\nthe content inconsistency between NIR-RGB images and the scarcity of real-world\npaired datasets. To alleviate the problem, we propose an efficient Selective\nFusion Module (SFM), which can be plug-and-played into the advanced denoising\nnetworks to merge the deep NIR-RGB features. Specifically, we sequentially\nperform the global and local modulation for NIR and RGB features, and then\nintegrate the two modulated features. Furthermore, we present a Real-world\nNIR-Assisted Image Denoising (Real-NAID) dataset, which covers diverse\nscenarios as well as various noise levels. Extensive experiments on both\nsynthetic and our real-world datasets demonstrate that the proposed method\nachieves better results than state-of-the-art ones. The dataset, codes, and\npre-trained models will be publicly available at\nhttps://github.com/ronjonxu/NAID.\n", "link": "http://arxiv.org/abs/2404.08514v1", "date": "2024-04-12", "relevancy": 1.7062, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5886}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5492}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5387}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NIR-Assisted%20Image%20Denoising%3A%20A%20Selective%20Fusion%20Approach%20and%20A%0A%20%20Real-World%20Benchmark%20Datase&body=Title%3A%20NIR-Assisted%20Image%20Denoising%3A%20A%20Selective%20Fusion%20Approach%20and%20A%0A%20%20Real-World%20Benchmark%20Datase%0AAuthor%3A%20Rongjian%20Xu%20and%20Zhilu%20Zhang%20and%20Renlong%20Wu%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Despite%20the%20significant%20progress%20in%20image%20denoising%2C%20it%20is%20still%20challenging%0Ato%20restore%20fine-scale%20details%20while%20removing%20noise%2C%20especially%20in%20extremely%0Alow-light%20environments.%20Leveraging%20near-infrared%20%28NIR%29%20images%20to%20assist%20visible%0ARGB%20image%20denoising%20shows%20the%20potential%20to%20address%20this%20issue%2C%20becoming%20a%0Apromising%20technology.%20Nonetheless%2C%20existing%20works%20still%20struggle%20with%20taking%0Aadvantage%20of%20NIR%20information%20effectively%20for%20real-world%20image%20denoising%2C%20due%20to%0Athe%20content%20inconsistency%20between%20NIR-RGB%20images%20and%20the%20scarcity%20of%20real-world%0Apaired%20datasets.%20To%20alleviate%20the%20problem%2C%20we%20propose%20an%20efficient%20Selective%0AFusion%20Module%20%28SFM%29%2C%20which%20can%20be%20plug-and-played%20into%20the%20advanced%20denoising%0Anetworks%20to%20merge%20the%20deep%20NIR-RGB%20features.%20Specifically%2C%20we%20sequentially%0Aperform%20the%20global%20and%20local%20modulation%20for%20NIR%20and%20RGB%20features%2C%20and%20then%0Aintegrate%20the%20two%20modulated%20features.%20Furthermore%2C%20we%20present%20a%20Real-world%0ANIR-Assisted%20Image%20Denoising%20%28Real-NAID%29%20dataset%2C%20which%20covers%20diverse%0Ascenarios%20as%20well%20as%20various%20noise%20levels.%20Extensive%20experiments%20on%20both%0Asynthetic%20and%20our%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20better%20results%20than%20state-of-the-art%20ones.%20The%20dataset%2C%20codes%2C%20and%0Apre-trained%20models%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/ronjonxu/NAID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08514v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIR-Assisted%20Image%20Denoising%3A%20A%20Selective%20Fusion%20Approach%20and%20A%0A%20%20Real-World%20Benchmark%20Datase&entry.906535625=Rongjian%20Xu%20and%20Zhilu%20Zhang%20and%20Renlong%20Wu%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Despite%20the%20significant%20progress%20in%20image%20denoising%2C%20it%20is%20still%20challenging%0Ato%20restore%20fine-scale%20details%20while%20removing%20noise%2C%20especially%20in%20extremely%0Alow-light%20environments.%20Leveraging%20near-infrared%20%28NIR%29%20images%20to%20assist%20visible%0ARGB%20image%20denoising%20shows%20the%20potential%20to%20address%20this%20issue%2C%20becoming%20a%0Apromising%20technology.%20Nonetheless%2C%20existing%20works%20still%20struggle%20with%20taking%0Aadvantage%20of%20NIR%20information%20effectively%20for%20real-world%20image%20denoising%2C%20due%20to%0Athe%20content%20inconsistency%20between%20NIR-RGB%20images%20and%20the%20scarcity%20of%20real-world%0Apaired%20datasets.%20To%20alleviate%20the%20problem%2C%20we%20propose%20an%20efficient%20Selective%0AFusion%20Module%20%28SFM%29%2C%20which%20can%20be%20plug-and-played%20into%20the%20advanced%20denoising%0Anetworks%20to%20merge%20the%20deep%20NIR-RGB%20features.%20Specifically%2C%20we%20sequentially%0Aperform%20the%20global%20and%20local%20modulation%20for%20NIR%20and%20RGB%20features%2C%20and%20then%0Aintegrate%20the%20two%20modulated%20features.%20Furthermore%2C%20we%20present%20a%20Real-world%0ANIR-Assisted%20Image%20Denoising%20%28Real-NAID%29%20dataset%2C%20which%20covers%20diverse%0Ascenarios%20as%20well%20as%20various%20noise%20levels.%20Extensive%20experiments%20on%20both%0Asynthetic%20and%20our%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20better%20results%20than%20state-of-the-art%20ones.%20The%20dataset%2C%20codes%2C%20and%0Apre-trained%20models%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/ronjonxu/NAID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08514v1&entry.124074799=Read"},
{"title": "Approximate Stein Classes for Truncated Density Estimation", "author": "Daniel J. Williams and Song Liu", "abstract": "  Estimating truncated density models is difficult, as these models have\nintractable normalising constants and hard to satisfy boundary conditions.\nScore matching can be adapted to solve the truncated density estimation\nproblem, but requires a continuous weighting function which takes zero at the\nboundary and is positive elsewhere. Evaluation of such a weighting function\n(and its gradient) often requires a closed-form expression of the truncation\nboundary and finding a solution to a complicated optimisation problem. In this\npaper, we propose approximate Stein classes, which in turn leads to a relaxed\nStein identity for truncated density estimation. We develop a novel discrepancy\nmeasure, truncated kernelised Stein discrepancy (TKSD), which does not require\nfixing a weighting function in advance, and can be evaluated using only samples\non the boundary. We estimate a truncated density model by minimising the\nLagrangian dual of TKSD. Finally, experiments show the accuracy of our method\nto be an improvement over previous works even without the explicit functional\nform of the boundary.\n", "link": "http://arxiv.org/abs/2306.00602v2", "date": "2024-04-12", "relevancy": 1.7062, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4362}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4289}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3966}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Approximate%20Stein%20Classes%20for%20Truncated%20Density%20Estimation&body=Title%3A%20Approximate%20Stein%20Classes%20for%20Truncated%20Density%20Estimation%0AAuthor%3A%20Daniel%20J.%20Williams%20and%20Song%20Liu%0AAbstract%3A%20%20%20Estimating%20truncated%20density%20models%20is%20difficult%2C%20as%20these%20models%20have%0Aintractable%20normalising%20constants%20and%20hard%20to%20satisfy%20boundary%20conditions.%0AScore%20matching%20can%20be%20adapted%20to%20solve%20the%20truncated%20density%20estimation%0Aproblem%2C%20but%20requires%20a%20continuous%20weighting%20function%20which%20takes%20zero%20at%20the%0Aboundary%20and%20is%20positive%20elsewhere.%20Evaluation%20of%20such%20a%20weighting%20function%0A%28and%20its%20gradient%29%20often%20requires%20a%20closed-form%20expression%20of%20the%20truncation%0Aboundary%20and%20finding%20a%20solution%20to%20a%20complicated%20optimisation%20problem.%20In%20this%0Apaper%2C%20we%20propose%20approximate%20Stein%20classes%2C%20which%20in%20turn%20leads%20to%20a%20relaxed%0AStein%20identity%20for%20truncated%20density%20estimation.%20We%20develop%20a%20novel%20discrepancy%0Ameasure%2C%20truncated%20kernelised%20Stein%20discrepancy%20%28TKSD%29%2C%20which%20does%20not%20require%0Afixing%20a%20weighting%20function%20in%20advance%2C%20and%20can%20be%20evaluated%20using%20only%20samples%0Aon%20the%20boundary.%20We%20estimate%20a%20truncated%20density%20model%20by%20minimising%20the%0ALagrangian%20dual%20of%20TKSD.%20Finally%2C%20experiments%20show%20the%20accuracy%20of%20our%20method%0Ato%20be%20an%20improvement%20over%20previous%20works%20even%20without%20the%20explicit%20functional%0Aform%20of%20the%20boundary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00602v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20Stein%20Classes%20for%20Truncated%20Density%20Estimation&entry.906535625=Daniel%20J.%20Williams%20and%20Song%20Liu&entry.1292438233=%20%20Estimating%20truncated%20density%20models%20is%20difficult%2C%20as%20these%20models%20have%0Aintractable%20normalising%20constants%20and%20hard%20to%20satisfy%20boundary%20conditions.%0AScore%20matching%20can%20be%20adapted%20to%20solve%20the%20truncated%20density%20estimation%0Aproblem%2C%20but%20requires%20a%20continuous%20weighting%20function%20which%20takes%20zero%20at%20the%0Aboundary%20and%20is%20positive%20elsewhere.%20Evaluation%20of%20such%20a%20weighting%20function%0A%28and%20its%20gradient%29%20often%20requires%20a%20closed-form%20expression%20of%20the%20truncation%0Aboundary%20and%20finding%20a%20solution%20to%20a%20complicated%20optimisation%20problem.%20In%20this%0Apaper%2C%20we%20propose%20approximate%20Stein%20classes%2C%20which%20in%20turn%20leads%20to%20a%20relaxed%0AStein%20identity%20for%20truncated%20density%20estimation.%20We%20develop%20a%20novel%20discrepancy%0Ameasure%2C%20truncated%20kernelised%20Stein%20discrepancy%20%28TKSD%29%2C%20which%20does%20not%20require%0Afixing%20a%20weighting%20function%20in%20advance%2C%20and%20can%20be%20evaluated%20using%20only%20samples%0Aon%20the%20boundary.%20We%20estimate%20a%20truncated%20density%20model%20by%20minimising%20the%0ALagrangian%20dual%20of%20TKSD.%20Finally%2C%20experiments%20show%20the%20accuracy%20of%20our%20method%0Ato%20be%20an%20improvement%20over%20previous%20works%20even%20without%20the%20explicit%20functional%0Aform%20of%20the%20boundary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00602v2&entry.124074799=Read"},
{"title": "Incremental Extractive Opinion Summarization Using Cover Trees", "author": "Somnath Basu Roy Chowdhury and Nicholas Monath and Avinava Dubey and Manzil Zaheer and Andrew McCallum and Amr Ahmed and Snigdha Chaturvedi", "abstract": "  Extractive opinion summarization involves automatically producing a summary\nof text about an entity (e.g., a product's reviews) by extracting\nrepresentative sentences that capture prevalent opinions in the review set.\nTypically, in online marketplaces user reviews accumulate over time, and\nopinion summaries need to be updated periodically to provide customers with\nup-to-date information. In this work, we study the task of extractive opinion\nsummarization in an incremental setting, where the underlying review set\nevolves over time. Many of the state-of-the-art extractive opinion\nsummarization approaches are centrality-based, such as CentroidRank (Radev et\nal., 2004; Chowdhury et al., 2022). CentroidRank performs extractive\nsummarization by selecting a subset of review sentences closest to the centroid\nin the representation space as the summary. However, these methods are not\ncapable of operating efficiently in an incremental setting, where reviews\narrive one at a time. In this paper, we present an efficient algorithm for\naccurately computing the CentroidRank summaries in an incremental setting. Our\napproach, CoverSumm, relies on indexing review representations in a cover tree\nand maintaining a reservoir of candidate summary review sentences. CoverSumm's\nefficacy is supported by a theoretical and empirical analysis of running time.\nEmpirically, on a diverse collection of data (both real and synthetically\ncreated to illustrate scaling considerations), we demonstrate that CoverSumm is\nup to 36x faster than baseline methods, and capable of adapting to nuanced\nchanges in data distribution. We also conduct human evaluations of the\ngenerated summaries and find that CoverSumm is capable of producing informative\nsummaries consistent with the underlying review set.\n", "link": "http://arxiv.org/abs/2401.08047v2", "date": "2024-04-12", "relevancy": 1.7041, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4314}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4265}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Incremental%20Extractive%20Opinion%20Summarization%20Using%20Cover%20Trees&body=Title%3A%20Incremental%20Extractive%20Opinion%20Summarization%20Using%20Cover%20Trees%0AAuthor%3A%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Nicholas%20Monath%20and%20Avinava%20Dubey%20and%20Manzil%20Zaheer%20and%20Andrew%20McCallum%20and%20Amr%20Ahmed%20and%20Snigdha%20Chaturvedi%0AAbstract%3A%20%20%20Extractive%20opinion%20summarization%20involves%20automatically%20producing%20a%20summary%0Aof%20text%20about%20an%20entity%20%28e.g.%2C%20a%20product%27s%20reviews%29%20by%20extracting%0Arepresentative%20sentences%20that%20capture%20prevalent%20opinions%20in%20the%20review%20set.%0ATypically%2C%20in%20online%20marketplaces%20user%20reviews%20accumulate%20over%20time%2C%20and%0Aopinion%20summaries%20need%20to%20be%20updated%20periodically%20to%20provide%20customers%20with%0Aup-to-date%20information.%20In%20this%20work%2C%20we%20study%20the%20task%20of%20extractive%20opinion%0Asummarization%20in%20an%20incremental%20setting%2C%20where%20the%20underlying%20review%20set%0Aevolves%20over%20time.%20Many%20of%20the%20state-of-the-art%20extractive%20opinion%0Asummarization%20approaches%20are%20centrality-based%2C%20such%20as%20CentroidRank%20%28Radev%20et%0Aal.%2C%202004%3B%20Chowdhury%20et%20al.%2C%202022%29.%20CentroidRank%20performs%20extractive%0Asummarization%20by%20selecting%20a%20subset%20of%20review%20sentences%20closest%20to%20the%20centroid%0Ain%20the%20representation%20space%20as%20the%20summary.%20However%2C%20these%20methods%20are%20not%0Acapable%20of%20operating%20efficiently%20in%20an%20incremental%20setting%2C%20where%20reviews%0Aarrive%20one%20at%20a%20time.%20In%20this%20paper%2C%20we%20present%20an%20efficient%20algorithm%20for%0Aaccurately%20computing%20the%20CentroidRank%20summaries%20in%20an%20incremental%20setting.%20Our%0Aapproach%2C%20CoverSumm%2C%20relies%20on%20indexing%20review%20representations%20in%20a%20cover%20tree%0Aand%20maintaining%20a%20reservoir%20of%20candidate%20summary%20review%20sentences.%20CoverSumm%27s%0Aefficacy%20is%20supported%20by%20a%20theoretical%20and%20empirical%20analysis%20of%20running%20time.%0AEmpirically%2C%20on%20a%20diverse%20collection%20of%20data%20%28both%20real%20and%20synthetically%0Acreated%20to%20illustrate%20scaling%20considerations%29%2C%20we%20demonstrate%20that%20CoverSumm%20is%0Aup%20to%2036x%20faster%20than%20baseline%20methods%2C%20and%20capable%20of%20adapting%20to%20nuanced%0Achanges%20in%20data%20distribution.%20We%20also%20conduct%20human%20evaluations%20of%20the%0Agenerated%20summaries%20and%20find%20that%20CoverSumm%20is%20capable%20of%20producing%20informative%0Asummaries%20consistent%20with%20the%20underlying%20review%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08047v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Extractive%20Opinion%20Summarization%20Using%20Cover%20Trees&entry.906535625=Somnath%20Basu%20Roy%20Chowdhury%20and%20Nicholas%20Monath%20and%20Avinava%20Dubey%20and%20Manzil%20Zaheer%20and%20Andrew%20McCallum%20and%20Amr%20Ahmed%20and%20Snigdha%20Chaturvedi&entry.1292438233=%20%20Extractive%20opinion%20summarization%20involves%20automatically%20producing%20a%20summary%0Aof%20text%20about%20an%20entity%20%28e.g.%2C%20a%20product%27s%20reviews%29%20by%20extracting%0Arepresentative%20sentences%20that%20capture%20prevalent%20opinions%20in%20the%20review%20set.%0ATypically%2C%20in%20online%20marketplaces%20user%20reviews%20accumulate%20over%20time%2C%20and%0Aopinion%20summaries%20need%20to%20be%20updated%20periodically%20to%20provide%20customers%20with%0Aup-to-date%20information.%20In%20this%20work%2C%20we%20study%20the%20task%20of%20extractive%20opinion%0Asummarization%20in%20an%20incremental%20setting%2C%20where%20the%20underlying%20review%20set%0Aevolves%20over%20time.%20Many%20of%20the%20state-of-the-art%20extractive%20opinion%0Asummarization%20approaches%20are%20centrality-based%2C%20such%20as%20CentroidRank%20%28Radev%20et%0Aal.%2C%202004%3B%20Chowdhury%20et%20al.%2C%202022%29.%20CentroidRank%20performs%20extractive%0Asummarization%20by%20selecting%20a%20subset%20of%20review%20sentences%20closest%20to%20the%20centroid%0Ain%20the%20representation%20space%20as%20the%20summary.%20However%2C%20these%20methods%20are%20not%0Acapable%20of%20operating%20efficiently%20in%20an%20incremental%20setting%2C%20where%20reviews%0Aarrive%20one%20at%20a%20time.%20In%20this%20paper%2C%20we%20present%20an%20efficient%20algorithm%20for%0Aaccurately%20computing%20the%20CentroidRank%20summaries%20in%20an%20incremental%20setting.%20Our%0Aapproach%2C%20CoverSumm%2C%20relies%20on%20indexing%20review%20representations%20in%20a%20cover%20tree%0Aand%20maintaining%20a%20reservoir%20of%20candidate%20summary%20review%20sentences.%20CoverSumm%27s%0Aefficacy%20is%20supported%20by%20a%20theoretical%20and%20empirical%20analysis%20of%20running%20time.%0AEmpirically%2C%20on%20a%20diverse%20collection%20of%20data%20%28both%20real%20and%20synthetically%0Acreated%20to%20illustrate%20scaling%20considerations%29%2C%20we%20demonstrate%20that%20CoverSumm%20is%0Aup%20to%2036x%20faster%20than%20baseline%20methods%2C%20and%20capable%20of%20adapting%20to%20nuanced%0Achanges%20in%20data%20distribution.%20We%20also%20conduct%20human%20evaluations%20of%20the%0Agenerated%20summaries%20and%20find%20that%20CoverSumm%20is%20capable%20of%20producing%20informative%0Asummaries%20consistent%20with%20the%20underlying%20review%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08047v2&entry.124074799=Read"},
{"title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task", "author": "Hassan Ali and Philipp Allgeuer and Stefan Wermter", "abstract": "  Intention-based Human-Robot Interaction (HRI) systems allow robots to\nperceive and interpret user actions to proactively interact with humans and\nadapt to their behavior. Therefore, intention prediction is pivotal in creating\na natural interactive collaboration between humans and robots. In this paper,\nwe examine the use of Large Language Models (LLMs) for inferring human\nintention during a collaborative object categorization task with a physical\nrobot. We introduce a hierarchical approach for interpreting user non-verbal\ncues, like hand gestures, body poses, and facial expressions and combining them\nwith environment states and user verbal cues captured using an existing\nAutomatic Speech Recognition (ASR) system. Our evaluation demonstrates the\npotential of LLMs to interpret non-verbal cues and to combine them with their\ncontext-understanding capabilities and real-world knowledge to support\nintention prediction during human-robot interaction.\n", "link": "http://arxiv.org/abs/2404.08424v1", "date": "2024-04-12", "relevancy": 1.5907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6037}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Comparing%20Apples%20to%20Oranges%3A%20LLM-powered%20Multimodal%20Intention%20Prediction%0A%20%20in%20an%20Object%20Categorization%20Task&body=Title%3A%20Comparing%20Apples%20to%20Oranges%3A%20LLM-powered%20Multimodal%20Intention%20Prediction%0A%20%20in%20an%20Object%20Categorization%20Task%0AAuthor%3A%20Hassan%20Ali%20and%20Philipp%20Allgeuer%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20Intention-based%20Human-Robot%20Interaction%20%28HRI%29%20systems%20allow%20robots%20to%0Aperceive%20and%20interpret%20user%20actions%20to%20proactively%20interact%20with%20humans%20and%0Aadapt%20to%20their%20behavior.%20Therefore%2C%20intention%20prediction%20is%20pivotal%20in%20creating%0Aa%20natural%20interactive%20collaboration%20between%20humans%20and%20robots.%20In%20this%20paper%2C%0Awe%20examine%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20inferring%20human%0Aintention%20during%20a%20collaborative%20object%20categorization%20task%20with%20a%20physical%0Arobot.%20We%20introduce%20a%20hierarchical%20approach%20for%20interpreting%20user%20non-verbal%0Acues%2C%20like%20hand%20gestures%2C%20body%20poses%2C%20and%20facial%20expressions%20and%20combining%20them%0Awith%20environment%20states%20and%20user%20verbal%20cues%20captured%20using%20an%20existing%0AAutomatic%20Speech%20Recognition%20%28ASR%29%20system.%20Our%20evaluation%20demonstrates%20the%0Apotential%20of%20LLMs%20to%20interpret%20non-verbal%20cues%20and%20to%20combine%20them%20with%20their%0Acontext-understanding%20capabilities%20and%20real-world%20knowledge%20to%20support%0Aintention%20prediction%20during%20human-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08424v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Apples%20to%20Oranges%3A%20LLM-powered%20Multimodal%20Intention%20Prediction%0A%20%20in%20an%20Object%20Categorization%20Task&entry.906535625=Hassan%20Ali%20and%20Philipp%20Allgeuer%20and%20Stefan%20Wermter&entry.1292438233=%20%20Intention-based%20Human-Robot%20Interaction%20%28HRI%29%20systems%20allow%20robots%20to%0Aperceive%20and%20interpret%20user%20actions%20to%20proactively%20interact%20with%20humans%20and%0Aadapt%20to%20their%20behavior.%20Therefore%2C%20intention%20prediction%20is%20pivotal%20in%20creating%0Aa%20natural%20interactive%20collaboration%20between%20humans%20and%20robots.%20In%20this%20paper%2C%0Awe%20examine%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%20inferring%20human%0Aintention%20during%20a%20collaborative%20object%20categorization%20task%20with%20a%20physical%0Arobot.%20We%20introduce%20a%20hierarchical%20approach%20for%20interpreting%20user%20non-verbal%0Acues%2C%20like%20hand%20gestures%2C%20body%20poses%2C%20and%20facial%20expressions%20and%20combining%20them%0Awith%20environment%20states%20and%20user%20verbal%20cues%20captured%20using%20an%20existing%0AAutomatic%20Speech%20Recognition%20%28ASR%29%20system.%20Our%20evaluation%20demonstrates%20the%0Apotential%20of%20LLMs%20to%20interpret%20non-verbal%20cues%20and%20to%20combine%20them%20with%20their%0Acontext-understanding%20capabilities%20and%20real-world%20knowledge%20to%20support%0Aintention%20prediction%20during%20human-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08424v1&entry.124074799=Read"},
{"title": "Enhancing MAP-Elites with Multiple Parallel Evolution Strategies", "author": "Manon Flageat and Bryan Lim and Antoine Cully", "abstract": "  With the development of fast and massively parallel evaluations in many\ndomains, Quality-Diversity (QD) algorithms, that already proved promising in a\nlarge range of applications, have seen their potential multiplied. However, we\nhave yet to understand how to best use a large number of evaluations as using\nthem for random variations alone is not always effective. High-dimensional\nsearch spaces are a typical situation where random variations struggle to\neffectively search. Another situation is uncertain settings where solutions can\nappear better than they truly are and naively evaluating more solutions might\nmislead QD algorithms. In this work, we propose MAP-Elites-Multi-ES (MEMES), a\nnovel QD algorithm based on Evolution Strategies (ES) designed to exploit fast\nparallel evaluations more effectively. MEMES maintains multiple (up to 100)\nsimultaneous ES processes, each with its own independent objective and reset\nmechanism designed for QD optimisation, all on just a single GPU. We show that\nMEMES outperforms both gradient-based and mutation-based QD algorithms on\nblack-box optimisation and QD-Reinforcement-Learning tasks, demonstrating its\nbenefit across domains. Additionally, our approach outperforms sampling-based\nQD methods in uncertain domains when given the same evaluation budget. Overall,\nMEMES generates reproducible solutions that are high-performing and diverse\nthrough large-scale ES optimisation on easily accessible hardware.\n", "link": "http://arxiv.org/abs/2303.06137v2", "date": "2024-04-12", "relevancy": 1.3655, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4569}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20MAP-Elites%20with%20Multiple%20Parallel%20Evolution%20Strategies&body=Title%3A%20Enhancing%20MAP-Elites%20with%20Multiple%20Parallel%20Evolution%20Strategies%0AAuthor%3A%20Manon%20Flageat%20and%20Bryan%20Lim%20and%20Antoine%20Cully%0AAbstract%3A%20%20%20With%20the%20development%20of%20fast%20and%20massively%20parallel%20evaluations%20in%20many%0Adomains%2C%20Quality-Diversity%20%28QD%29%20algorithms%2C%20that%20already%20proved%20promising%20in%20a%0Alarge%20range%20of%20applications%2C%20have%20seen%20their%20potential%20multiplied.%20However%2C%20we%0Ahave%20yet%20to%20understand%20how%20to%20best%20use%20a%20large%20number%20of%20evaluations%20as%20using%0Athem%20for%20random%20variations%20alone%20is%20not%20always%20effective.%20High-dimensional%0Asearch%20spaces%20are%20a%20typical%20situation%20where%20random%20variations%20struggle%20to%0Aeffectively%20search.%20Another%20situation%20is%20uncertain%20settings%20where%20solutions%20can%0Aappear%20better%20than%20they%20truly%20are%20and%20naively%20evaluating%20more%20solutions%20might%0Amislead%20QD%20algorithms.%20In%20this%20work%2C%20we%20propose%20MAP-Elites-Multi-ES%20%28MEMES%29%2C%20a%0Anovel%20QD%20algorithm%20based%20on%20Evolution%20Strategies%20%28ES%29%20designed%20to%20exploit%20fast%0Aparallel%20evaluations%20more%20effectively.%20MEMES%20maintains%20multiple%20%28up%20to%20100%29%0Asimultaneous%20ES%20processes%2C%20each%20with%20its%20own%20independent%20objective%20and%20reset%0Amechanism%20designed%20for%20QD%20optimisation%2C%20all%20on%20just%20a%20single%20GPU.%20We%20show%20that%0AMEMES%20outperforms%20both%20gradient-based%20and%20mutation-based%20QD%20algorithms%20on%0Ablack-box%20optimisation%20and%20QD-Reinforcement-Learning%20tasks%2C%20demonstrating%20its%0Abenefit%20across%20domains.%20Additionally%2C%20our%20approach%20outperforms%20sampling-based%0AQD%20methods%20in%20uncertain%20domains%20when%20given%20the%20same%20evaluation%20budget.%20Overall%2C%0AMEMES%20generates%20reproducible%20solutions%20that%20are%20high-performing%20and%20diverse%0Athrough%20large-scale%20ES%20optimisation%20on%20easily%20accessible%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06137v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20MAP-Elites%20with%20Multiple%20Parallel%20Evolution%20Strategies&entry.906535625=Manon%20Flageat%20and%20Bryan%20Lim%20and%20Antoine%20Cully&entry.1292438233=%20%20With%20the%20development%20of%20fast%20and%20massively%20parallel%20evaluations%20in%20many%0Adomains%2C%20Quality-Diversity%20%28QD%29%20algorithms%2C%20that%20already%20proved%20promising%20in%20a%0Alarge%20range%20of%20applications%2C%20have%20seen%20their%20potential%20multiplied.%20However%2C%20we%0Ahave%20yet%20to%20understand%20how%20to%20best%20use%20a%20large%20number%20of%20evaluations%20as%20using%0Athem%20for%20random%20variations%20alone%20is%20not%20always%20effective.%20High-dimensional%0Asearch%20spaces%20are%20a%20typical%20situation%20where%20random%20variations%20struggle%20to%0Aeffectively%20search.%20Another%20situation%20is%20uncertain%20settings%20where%20solutions%20can%0Aappear%20better%20than%20they%20truly%20are%20and%20naively%20evaluating%20more%20solutions%20might%0Amislead%20QD%20algorithms.%20In%20this%20work%2C%20we%20propose%20MAP-Elites-Multi-ES%20%28MEMES%29%2C%20a%0Anovel%20QD%20algorithm%20based%20on%20Evolution%20Strategies%20%28ES%29%20designed%20to%20exploit%20fast%0Aparallel%20evaluations%20more%20effectively.%20MEMES%20maintains%20multiple%20%28up%20to%20100%29%0Asimultaneous%20ES%20processes%2C%20each%20with%20its%20own%20independent%20objective%20and%20reset%0Amechanism%20designed%20for%20QD%20optimisation%2C%20all%20on%20just%20a%20single%20GPU.%20We%20show%20that%0AMEMES%20outperforms%20both%20gradient-based%20and%20mutation-based%20QD%20algorithms%20on%0Ablack-box%20optimisation%20and%20QD-Reinforcement-Learning%20tasks%2C%20demonstrating%20its%0Abenefit%20across%20domains.%20Additionally%2C%20our%20approach%20outperforms%20sampling-based%0AQD%20methods%20in%20uncertain%20domains%20when%20given%20the%20same%20evaluation%20budget.%20Overall%2C%0AMEMES%20generates%20reproducible%20solutions%20that%20are%20high-performing%20and%20diverse%0Athrough%20large-scale%20ES%20optimisation%20on%20easily%20accessible%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06137v2&entry.124074799=Read"},
{"title": "Automated Verification of Equivalence Properties in Advanced Logic\n  Programs -- Bachelor Thesis", "author": "Jan Heuer", "abstract": "  With the increase in industrial applications using Answer Set Programming,\nthe need for formal verification tools, particularly for critical applications,\nhas also increased. During the program optimisation process, it would be\ndesirable to have a tool which can automatically verify whether an optimised\nsubprogram can replace the original subprogram. Formally this corresponds to\nthe problem of verifying the strong equivalence of two programs. In order to do\nso, the translation tool anthem was developed. It can be used in conjunction\nwith an automated theorem prover for classical logic to verify that two\nprograms are strongly equivalent. With the current version of anthem, only the\nstrong equivalence of positive programs with a restricted input language can be\nverified. This is a result of the translation $\\tau^*$ implemented in anthem\nthat produces formulas in the logic of here-and-there, which coincides with\nclassical logic only for positive programs. This thesis extends anthem in order\nto overcome these limitations. First, the transformation $\\sigma^*$ is\npresented, which transforms formulas from the logic of here-and-there to\nclassical logic. A theorem formalises how $\\sigma^*$ can be used to express\nequivalence in the logic of here-and-there in classical logic. Second, the\ntranslation $\\tau^*$ is extended to programs containing pools. Another theorem\nshows how $\\sigma^*$ can be combined with $\\tau^*$ to express the strong\nequivalence of two programs in classical logic. With $\\sigma^*$ and the\nextended $\\tau^*$, it is possible to express the strong equivalence of logic\nprograms containing negation, simple choices, and pools. Both the extended\n$\\tau^*$ and $\\sigma^*$ are implemented in a new version of anthem. Several\nexamples of logic programs containing pools, negation, and simple choice rules,\nwhich the new version of anthem can translate to classical logic, are\npresented. Some a...\n", "link": "http://arxiv.org/abs/2310.19806v3", "date": "2024-04-12", "relevancy": 1.119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4041}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3658}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3635}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automated%20Verification%20of%20Equivalence%20Properties%20in%20Advanced%20Logic%0A%20%20Programs%20--%20Bachelor%20Thesis&body=Title%3A%20Automated%20Verification%20of%20Equivalence%20Properties%20in%20Advanced%20Logic%0A%20%20Programs%20--%20Bachelor%20Thesis%0AAuthor%3A%20Jan%20Heuer%0AAbstract%3A%20%20%20With%20the%20increase%20in%20industrial%20applications%20using%20Answer%20Set%20Programming%2C%0Athe%20need%20for%20formal%20verification%20tools%2C%20particularly%20for%20critical%20applications%2C%0Ahas%20also%20increased.%20During%20the%20program%20optimisation%20process%2C%20it%20would%20be%0Adesirable%20to%20have%20a%20tool%20which%20can%20automatically%20verify%20whether%20an%20optimised%0Asubprogram%20can%20replace%20the%20original%20subprogram.%20Formally%20this%20corresponds%20to%0Athe%20problem%20of%20verifying%20the%20strong%20equivalence%20of%20two%20programs.%20In%20order%20to%20do%0Aso%2C%20the%20translation%20tool%20anthem%20was%20developed.%20It%20can%20be%20used%20in%20conjunction%0Awith%20an%20automated%20theorem%20prover%20for%20classical%20logic%20to%20verify%20that%20two%0Aprograms%20are%20strongly%20equivalent.%20With%20the%20current%20version%20of%20anthem%2C%20only%20the%0Astrong%20equivalence%20of%20positive%20programs%20with%20a%20restricted%20input%20language%20can%20be%0Averified.%20This%20is%20a%20result%20of%20the%20translation%20%24%5Ctau%5E%2A%24%20implemented%20in%20anthem%0Athat%20produces%20formulas%20in%20the%20logic%20of%20here-and-there%2C%20which%20coincides%20with%0Aclassical%20logic%20only%20for%20positive%20programs.%20This%20thesis%20extends%20anthem%20in%20order%0Ato%20overcome%20these%20limitations.%20First%2C%20the%20transformation%20%24%5Csigma%5E%2A%24%20is%0Apresented%2C%20which%20transforms%20formulas%20from%20the%20logic%20of%20here-and-there%20to%0Aclassical%20logic.%20A%20theorem%20formalises%20how%20%24%5Csigma%5E%2A%24%20can%20be%20used%20to%20express%0Aequivalence%20in%20the%20logic%20of%20here-and-there%20in%20classical%20logic.%20Second%2C%20the%0Atranslation%20%24%5Ctau%5E%2A%24%20is%20extended%20to%20programs%20containing%20pools.%20Another%20theorem%0Ashows%20how%20%24%5Csigma%5E%2A%24%20can%20be%20combined%20with%20%24%5Ctau%5E%2A%24%20to%20express%20the%20strong%0Aequivalence%20of%20two%20programs%20in%20classical%20logic.%20With%20%24%5Csigma%5E%2A%24%20and%20the%0Aextended%20%24%5Ctau%5E%2A%24%2C%20it%20is%20possible%20to%20express%20the%20strong%20equivalence%20of%20logic%0Aprograms%20containing%20negation%2C%20simple%20choices%2C%20and%20pools.%20Both%20the%20extended%0A%24%5Ctau%5E%2A%24%20and%20%24%5Csigma%5E%2A%24%20are%20implemented%20in%20a%20new%20version%20of%20anthem.%20Several%0Aexamples%20of%20logic%20programs%20containing%20pools%2C%20negation%2C%20and%20simple%20choice%20rules%2C%0Awhich%20the%20new%20version%20of%20anthem%20can%20translate%20to%20classical%20logic%2C%20are%0Apresented.%20Some%20a...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19806v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Verification%20of%20Equivalence%20Properties%20in%20Advanced%20Logic%0A%20%20Programs%20--%20Bachelor%20Thesis&entry.906535625=Jan%20Heuer&entry.1292438233=%20%20With%20the%20increase%20in%20industrial%20applications%20using%20Answer%20Set%20Programming%2C%0Athe%20need%20for%20formal%20verification%20tools%2C%20particularly%20for%20critical%20applications%2C%0Ahas%20also%20increased.%20During%20the%20program%20optimisation%20process%2C%20it%20would%20be%0Adesirable%20to%20have%20a%20tool%20which%20can%20automatically%20verify%20whether%20an%20optimised%0Asubprogram%20can%20replace%20the%20original%20subprogram.%20Formally%20this%20corresponds%20to%0Athe%20problem%20of%20verifying%20the%20strong%20equivalence%20of%20two%20programs.%20In%20order%20to%20do%0Aso%2C%20the%20translation%20tool%20anthem%20was%20developed.%20It%20can%20be%20used%20in%20conjunction%0Awith%20an%20automated%20theorem%20prover%20for%20classical%20logic%20to%20verify%20that%20two%0Aprograms%20are%20strongly%20equivalent.%20With%20the%20current%20version%20of%20anthem%2C%20only%20the%0Astrong%20equivalence%20of%20positive%20programs%20with%20a%20restricted%20input%20language%20can%20be%0Averified.%20This%20is%20a%20result%20of%20the%20translation%20%24%5Ctau%5E%2A%24%20implemented%20in%20anthem%0Athat%20produces%20formulas%20in%20the%20logic%20of%20here-and-there%2C%20which%20coincides%20with%0Aclassical%20logic%20only%20for%20positive%20programs.%20This%20thesis%20extends%20anthem%20in%20order%0Ato%20overcome%20these%20limitations.%20First%2C%20the%20transformation%20%24%5Csigma%5E%2A%24%20is%0Apresented%2C%20which%20transforms%20formulas%20from%20the%20logic%20of%20here-and-there%20to%0Aclassical%20logic.%20A%20theorem%20formalises%20how%20%24%5Csigma%5E%2A%24%20can%20be%20used%20to%20express%0Aequivalence%20in%20the%20logic%20of%20here-and-there%20in%20classical%20logic.%20Second%2C%20the%0Atranslation%20%24%5Ctau%5E%2A%24%20is%20extended%20to%20programs%20containing%20pools.%20Another%20theorem%0Ashows%20how%20%24%5Csigma%5E%2A%24%20can%20be%20combined%20with%20%24%5Ctau%5E%2A%24%20to%20express%20the%20strong%0Aequivalence%20of%20two%20programs%20in%20classical%20logic.%20With%20%24%5Csigma%5E%2A%24%20and%20the%0Aextended%20%24%5Ctau%5E%2A%24%2C%20it%20is%20possible%20to%20express%20the%20strong%20equivalence%20of%20logic%0Aprograms%20containing%20negation%2C%20simple%20choices%2C%20and%20pools.%20Both%20the%20extended%0A%24%5Ctau%5E%2A%24%20and%20%24%5Csigma%5E%2A%24%20are%20implemented%20in%20a%20new%20version%20of%20anthem.%20Several%0Aexamples%20of%20logic%20programs%20containing%20pools%2C%20negation%2C%20and%20simple%20choice%20rules%2C%0Awhich%20the%20new%20version%20of%20anthem%20can%20translate%20to%20classical%20logic%2C%20are%0Apresented.%20Some%20a...%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19806v3&entry.124074799=Read"},
{"title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\n  Human Feedback for LLMs", "author": "Shreyas Chaudhari and Pranjal Aggarwal and Vishvak Murahari and Tanmay Rajpurohit and Ashwin Kalyan and Karthik Narasimhan and Ameet Deshpande and Bruno Castro da Silva", "abstract": "  State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.\n", "link": "http://arxiv.org/abs/2404.08555v1", "date": "2024-04-12", "relevancy": 1.3847, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4865}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4588}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RLHF%20Deciphered%3A%20A%20Critical%20Analysis%20of%20Reinforcement%20Learning%20from%0A%20%20Human%20Feedback%20for%20LLMs&body=Title%3A%20RLHF%20Deciphered%3A%20A%20Critical%20Analysis%20of%20Reinforcement%20Learning%20from%0A%20%20Human%20Feedback%20for%20LLMs%0AAuthor%3A%20Shreyas%20Chaudhari%20and%20Pranjal%20Aggarwal%20and%20Vishvak%20Murahari%20and%20Tanmay%20Rajpurohit%20and%20Ashwin%20Kalyan%20and%20Karthik%20Narasimhan%20and%20Ameet%20Deshpande%20and%20Bruno%20Castro%20da%20Silva%0AAbstract%3A%20%20%20State-of-the-art%20large%20language%20models%20%28LLMs%29%20have%20become%20indispensable%20tools%0Afor%20various%20tasks.%20However%2C%20training%20LLMs%20to%20serve%20as%20effective%20assistants%20for%0Ahumans%20requires%20careful%20consideration.%20A%20promising%20approach%20is%20reinforcement%0Alearning%20from%20human%20feedback%20%28RLHF%29%2C%20which%20leverages%20human%20feedback%20to%20update%0Athe%20model%20in%20accordance%20with%20human%20preferences%20and%20mitigate%20issues%20like%0Atoxicity%20and%20hallucinations.%20Yet%2C%20an%20understanding%20of%20RLHF%20for%20LLMs%20is%20largely%0Aentangled%20with%20initial%20design%20choices%20that%20popularized%20the%20method%20and%20current%0Aresearch%20focuses%20on%20augmenting%20those%20choices%20rather%20than%20fundamentally%0Aimproving%20the%20framework.%20In%20this%20paper%2C%20we%20analyze%20RLHF%20through%20the%20lens%20of%0Areinforcement%20learning%20principles%20to%20develop%20an%20understanding%20of%20its%0Afundamentals%2C%20dedicating%20substantial%20focus%20to%20the%20core%20component%20of%20RLHF%20--%20the%0Areward%20model.%20Our%20study%20investigates%20modeling%20choices%2C%20caveats%20of%20function%0Aapproximation%2C%20and%20their%20implications%20on%20RLHF%20training%20algorithms%2C%20highlighting%0Athe%20underlying%20assumptions%20made%20about%20the%20expressivity%20of%20reward.%20Our%20analysis%0Aimproves%20the%20understanding%20of%20the%20role%20of%20reward%20models%20and%20methods%20for%20their%0Atraining%2C%20concurrently%20revealing%20limitations%20of%20the%20current%20methodology.%20We%0Acharacterize%20these%20limitations%2C%20including%20incorrect%20generalization%2C%20model%0Amisspecification%2C%20and%20the%20sparsity%20of%20feedback%2C%20along%20with%20their%20impact%20on%20the%0Aperformance%20of%20a%20language%20model.%20The%20discussion%20and%20analysis%20are%20substantiated%0Aby%20a%20categorical%20review%20of%20current%20literature%2C%20serving%20as%20a%20reference%20for%0Aresearchers%20and%20practitioners%20to%20understand%20the%20challenges%20of%20RLHF%20and%20build%0Aupon%20existing%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08555v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLHF%20Deciphered%3A%20A%20Critical%20Analysis%20of%20Reinforcement%20Learning%20from%0A%20%20Human%20Feedback%20for%20LLMs&entry.906535625=Shreyas%20Chaudhari%20and%20Pranjal%20Aggarwal%20and%20Vishvak%20Murahari%20and%20Tanmay%20Rajpurohit%20and%20Ashwin%20Kalyan%20and%20Karthik%20Narasimhan%20and%20Ameet%20Deshpande%20and%20Bruno%20Castro%20da%20Silva&entry.1292438233=%20%20State-of-the-art%20large%20language%20models%20%28LLMs%29%20have%20become%20indispensable%20tools%0Afor%20various%20tasks.%20However%2C%20training%20LLMs%20to%20serve%20as%20effective%20assistants%20for%0Ahumans%20requires%20careful%20consideration.%20A%20promising%20approach%20is%20reinforcement%0Alearning%20from%20human%20feedback%20%28RLHF%29%2C%20which%20leverages%20human%20feedback%20to%20update%0Athe%20model%20in%20accordance%20with%20human%20preferences%20and%20mitigate%20issues%20like%0Atoxicity%20and%20hallucinations.%20Yet%2C%20an%20understanding%20of%20RLHF%20for%20LLMs%20is%20largely%0Aentangled%20with%20initial%20design%20choices%20that%20popularized%20the%20method%20and%20current%0Aresearch%20focuses%20on%20augmenting%20those%20choices%20rather%20than%20fundamentally%0Aimproving%20the%20framework.%20In%20this%20paper%2C%20we%20analyze%20RLHF%20through%20the%20lens%20of%0Areinforcement%20learning%20principles%20to%20develop%20an%20understanding%20of%20its%0Afundamentals%2C%20dedicating%20substantial%20focus%20to%20the%20core%20component%20of%20RLHF%20--%20the%0Areward%20model.%20Our%20study%20investigates%20modeling%20choices%2C%20caveats%20of%20function%0Aapproximation%2C%20and%20their%20implications%20on%20RLHF%20training%20algorithms%2C%20highlighting%0Athe%20underlying%20assumptions%20made%20about%20the%20expressivity%20of%20reward.%20Our%20analysis%0Aimproves%20the%20understanding%20of%20the%20role%20of%20reward%20models%20and%20methods%20for%20their%0Atraining%2C%20concurrently%20revealing%20limitations%20of%20the%20current%20methodology.%20We%0Acharacterize%20these%20limitations%2C%20including%20incorrect%20generalization%2C%20model%0Amisspecification%2C%20and%20the%20sparsity%20of%20feedback%2C%20along%20with%20their%20impact%20on%20the%0Aperformance%20of%20a%20language%20model.%20The%20discussion%20and%20analysis%20are%20substantiated%0Aby%20a%20categorical%20review%20of%20current%20literature%2C%20serving%20as%20a%20reference%20for%0Aresearchers%20and%20practitioners%20to%20understand%20the%20challenges%20of%20RLHF%20and%20build%0Aupon%20existing%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08555v1&entry.124074799=Read"},
{"title": "Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing\n  Clues", "author": "Xianhua He and Dashuang Liang and Song Yang and Zhanlong Hao and Hui Ma and Binjie Mao and Xi Li and Yao Wang and Pengfei Yan and Ajian Liu", "abstract": "  Face recognition systems are frequently subjected to a variety of physical\nand digital attacks of different types. Previous methods have achieved\nsatisfactory performance in scenarios that address physical attacks and digital\nattacks, respectively. However, few methods are considered to integrate a model\nthat simultaneously addresses both physical and digital attacks, implying the\nnecessity to develop and maintain multiple models. To jointly detect physical\nand digital attacks within a single model, we propose an innovative approach\nthat can adapt to any network architecture. Our approach mainly contains two\ntypes of data augmentation, which we call Simulated Physical Spoofing Clues\naugmentation (SPSC) and Simulated Digital Spoofing Clues augmentation (SDSC).\nSPSC and SDSC augment live samples into simulated attack samples by simulating\nspoofing clues of physical and digital attacks, respectively, which\nsignificantly improve the capability of the model to detect \"unseen\" attack\ntypes. Extensive experiments show that SPSC and SDSC can achieve\nstate-of-the-art generalization in Protocols 2.1 and 2.2 of the UniAttackData\ndataset, respectively. Our method won first place in \"Unified Physical-Digital\nFace Attack Detection\" of the 5th Face Anti-spoofing Challenge@CVPR2024. Our\nfinal submission obtains 3.75% APCER, 0.93% BPCER, and 2.34% ACER,\nrespectively. Our code is available at\nhttps://github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.\n", "link": "http://arxiv.org/abs/2404.08450v1", "date": "2024-04-12", "relevancy": 1.481, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4964}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4888}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Joint%20Physical-Digital%20Facial%20Attack%20Detection%20Via%20Simulating%20Spoofing%0A%20%20Clues&body=Title%3A%20Joint%20Physical-Digital%20Facial%20Attack%20Detection%20Via%20Simulating%20Spoofing%0A%20%20Clues%0AAuthor%3A%20Xianhua%20He%20and%20Dashuang%20Liang%20and%20Song%20Yang%20and%20Zhanlong%20Hao%20and%20Hui%20Ma%20and%20Binjie%20Mao%20and%20Xi%20Li%20and%20Yao%20Wang%20and%20Pengfei%20Yan%20and%20Ajian%20Liu%0AAbstract%3A%20%20%20Face%20recognition%20systems%20are%20frequently%20subjected%20to%20a%20variety%20of%20physical%0Aand%20digital%20attacks%20of%20different%20types.%20Previous%20methods%20have%20achieved%0Asatisfactory%20performance%20in%20scenarios%20that%20address%20physical%20attacks%20and%20digital%0Aattacks%2C%20respectively.%20However%2C%20few%20methods%20are%20considered%20to%20integrate%20a%20model%0Athat%20simultaneously%20addresses%20both%20physical%20and%20digital%20attacks%2C%20implying%20the%0Anecessity%20to%20develop%20and%20maintain%20multiple%20models.%20To%20jointly%20detect%20physical%0Aand%20digital%20attacks%20within%20a%20single%20model%2C%20we%20propose%20an%20innovative%20approach%0Athat%20can%20adapt%20to%20any%20network%20architecture.%20Our%20approach%20mainly%20contains%20two%0Atypes%20of%20data%20augmentation%2C%20which%20we%20call%20Simulated%20Physical%20Spoofing%20Clues%0Aaugmentation%20%28SPSC%29%20and%20Simulated%20Digital%20Spoofing%20Clues%20augmentation%20%28SDSC%29.%0ASPSC%20and%20SDSC%20augment%20live%20samples%20into%20simulated%20attack%20samples%20by%20simulating%0Aspoofing%20clues%20of%20physical%20and%20digital%20attacks%2C%20respectively%2C%20which%0Asignificantly%20improve%20the%20capability%20of%20the%20model%20to%20detect%20%22unseen%22%20attack%0Atypes.%20Extensive%20experiments%20show%20that%20SPSC%20and%20SDSC%20can%20achieve%0Astate-of-the-art%20generalization%20in%20Protocols%202.1%20and%202.2%20of%20the%20UniAttackData%0Adataset%2C%20respectively.%20Our%20method%20won%20first%20place%20in%20%22Unified%20Physical-Digital%0AFace%20Attack%20Detection%22%20of%20the%205th%20Face%20Anti-spoofing%20Challenge%40CVPR2024.%20Our%0Afinal%20submission%20obtains%203.75%25%20APCER%2C%200.93%25%20BPCER%2C%20and%202.34%25%20ACER%2C%0Arespectively.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08450v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Physical-Digital%20Facial%20Attack%20Detection%20Via%20Simulating%20Spoofing%0A%20%20Clues&entry.906535625=Xianhua%20He%20and%20Dashuang%20Liang%20and%20Song%20Yang%20and%20Zhanlong%20Hao%20and%20Hui%20Ma%20and%20Binjie%20Mao%20and%20Xi%20Li%20and%20Yao%20Wang%20and%20Pengfei%20Yan%20and%20Ajian%20Liu&entry.1292438233=%20%20Face%20recognition%20systems%20are%20frequently%20subjected%20to%20a%20variety%20of%20physical%0Aand%20digital%20attacks%20of%20different%20types.%20Previous%20methods%20have%20achieved%0Asatisfactory%20performance%20in%20scenarios%20that%20address%20physical%20attacks%20and%20digital%0Aattacks%2C%20respectively.%20However%2C%20few%20methods%20are%20considered%20to%20integrate%20a%20model%0Athat%20simultaneously%20addresses%20both%20physical%20and%20digital%20attacks%2C%20implying%20the%0Anecessity%20to%20develop%20and%20maintain%20multiple%20models.%20To%20jointly%20detect%20physical%0Aand%20digital%20attacks%20within%20a%20single%20model%2C%20we%20propose%20an%20innovative%20approach%0Athat%20can%20adapt%20to%20any%20network%20architecture.%20Our%20approach%20mainly%20contains%20two%0Atypes%20of%20data%20augmentation%2C%20which%20we%20call%20Simulated%20Physical%20Spoofing%20Clues%0Aaugmentation%20%28SPSC%29%20and%20Simulated%20Digital%20Spoofing%20Clues%20augmentation%20%28SDSC%29.%0ASPSC%20and%20SDSC%20augment%20live%20samples%20into%20simulated%20attack%20samples%20by%20simulating%0Aspoofing%20clues%20of%20physical%20and%20digital%20attacks%2C%20respectively%2C%20which%0Asignificantly%20improve%20the%20capability%20of%20the%20model%20to%20detect%20%22unseen%22%20attack%0Atypes.%20Extensive%20experiments%20show%20that%20SPSC%20and%20SDSC%20can%20achieve%0Astate-of-the-art%20generalization%20in%20Protocols%202.1%20and%202.2%20of%20the%20UniAttackData%0Adataset%2C%20respectively.%20Our%20method%20won%20first%20place%20in%20%22Unified%20Physical-Digital%0AFace%20Attack%20Detection%22%20of%20the%205th%20Face%20Anti-spoofing%20Challenge%40CVPR2024.%20Our%0Afinal%20submission%20obtains%203.75%25%20APCER%2C%200.93%25%20BPCER%2C%20and%202.34%25%20ACER%2C%0Arespectively.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08450v1&entry.124074799=Read"},
{"title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction", "author": "Haoran Qiu and Weichao Mao and Archit Patke and Shengkun Cui and Saurabh Jha and Chen Wang and Hubertus Franke and Zbigniew T. Kalbarczyk and Tamer Ba\u015far and Ravishankar K. Iyer", "abstract": "  Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.\n", "link": "http://arxiv.org/abs/2404.08509v1", "date": "2024-04-12", "relevancy": 0.9579, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4564}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Interactive%20LLM%20Serving%20with%20Proxy%20Model-based%20Sequence%20Length%0A%20%20Prediction&body=Title%3A%20Efficient%20Interactive%20LLM%20Serving%20with%20Proxy%20Model-based%20Sequence%20Length%0A%20%20Prediction%0AAuthor%3A%20Haoran%20Qiu%20and%20Weichao%20Mao%20and%20Archit%20Patke%20and%20Shengkun%20Cui%20and%20Saurabh%20Jha%20and%20Chen%20Wang%20and%20Hubertus%20Franke%20and%20Zbigniew%20T.%20Kalbarczyk%20and%20Tamer%20Ba%C5%9Far%20and%20Ravishankar%20K.%20Iyer%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20driving%20a%20new%20wave%20of%20interactive%20AI%0Aapplications%20across%20numerous%20domains.%20However%2C%20efficiently%20serving%20LLM%0Ainference%20requests%20is%20challenging%20due%20to%20their%20unpredictable%20execution%20times%0Aoriginating%20from%20the%20autoregressive%20nature%20of%20generative%20models.%20Existing%20LLM%0Aserving%20systems%20exploit%20first-come-first-serve%20%28FCFS%29%20scheduling%2C%20suffering%0Afrom%20head-of-line%20blocking%20issues.%20To%20address%20the%20non-deterministic%20nature%20of%0ALLMs%20and%20enable%20efficient%20interactive%20LLM%20serving%2C%20we%20present%20a%20speculative%0Ashortest-job-first%20%28SSJF%29%20scheduler%20that%20uses%20a%20light%20proxy%20model%20to%20predict%0ALLM%20output%20sequence%20lengths.%20Our%20open-source%20SSJF%20implementation%20does%20not%0Arequire%20changes%20to%20memory%20management%20or%20batching%20strategies.%20Evaluations%20on%0Areal-world%20datasets%20and%20production%20workload%20traces%20show%20that%20SSJF%20reduces%0Aaverage%20job%20completion%20times%20by%2030.5-39.6%25%20and%20increases%20throughput%20by%202.2-3.6x%0Acompared%20to%20FCFS%20schedulers%2C%20across%20no%20batching%2C%20dynamic%20batching%2C%20and%0Acontinuous%20batching%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08509v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Interactive%20LLM%20Serving%20with%20Proxy%20Model-based%20Sequence%20Length%0A%20%20Prediction&entry.906535625=Haoran%20Qiu%20and%20Weichao%20Mao%20and%20Archit%20Patke%20and%20Shengkun%20Cui%20and%20Saurabh%20Jha%20and%20Chen%20Wang%20and%20Hubertus%20Franke%20and%20Zbigniew%20T.%20Kalbarczyk%20and%20Tamer%20Ba%C5%9Far%20and%20Ravishankar%20K.%20Iyer&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20driving%20a%20new%20wave%20of%20interactive%20AI%0Aapplications%20across%20numerous%20domains.%20However%2C%20efficiently%20serving%20LLM%0Ainference%20requests%20is%20challenging%20due%20to%20their%20unpredictable%20execution%20times%0Aoriginating%20from%20the%20autoregressive%20nature%20of%20generative%20models.%20Existing%20LLM%0Aserving%20systems%20exploit%20first-come-first-serve%20%28FCFS%29%20scheduling%2C%20suffering%0Afrom%20head-of-line%20blocking%20issues.%20To%20address%20the%20non-deterministic%20nature%20of%0ALLMs%20and%20enable%20efficient%20interactive%20LLM%20serving%2C%20we%20present%20a%20speculative%0Ashortest-job-first%20%28SSJF%29%20scheduler%20that%20uses%20a%20light%20proxy%20model%20to%20predict%0ALLM%20output%20sequence%20lengths.%20Our%20open-source%20SSJF%20implementation%20does%20not%0Arequire%20changes%20to%20memory%20management%20or%20batching%20strategies.%20Evaluations%20on%0Areal-world%20datasets%20and%20production%20workload%20traces%20show%20that%20SSJF%20reduces%0Aaverage%20job%20completion%20times%20by%2030.5-39.6%25%20and%20increases%20throughput%20by%202.2-3.6x%0Acompared%20to%20FCFS%20schedulers%2C%20across%20no%20batching%2C%20dynamic%20batching%2C%20and%0Acontinuous%20batching%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08509v1&entry.124074799=Read"},
{"title": "SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike\n  Camera", "author": "Gaole Dai and Zhenyu Wang and Qinwen Xu and Ming Lu and Wen Chen and Boxin Shi and Shanghang Zhang and Tiejun Huang", "abstract": "  One of the most critical factors in achieving sharp Novel View Synthesis\n(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) is the quality of the training images. However,\nConventional RGB cameras are susceptible to motion blur. In contrast,\nneuromorphic cameras like event and spike cameras inherently capture more\ncomprehensive temporal information, which can provide a sharp representation of\nthe scene as additional training data. Recent methods have explored the\nintegration of event cameras to improve the quality of NVS. The event-RGB\napproaches have some limitations, such as high training costs and the inability\nto work effectively in the background. Instead, our study introduces a new\nmethod that uses the spike camera to overcome these limitations. By considering\ntexture reconstruction from spike streams as ground truth, we design the\nTexture from Spike (TfS) loss. Since the spike camera relies on temporal\nintegration instead of temporal differentiation used by event cameras, our\nproposed TfS loss maintains manageable training costs. It handles foreground\nobjects with backgrounds simultaneously. We also provide a real-world dataset\ncaptured with our spike-RGB camera system to facilitate future research\nendeavors. We conduct extensive experiments using synthetic and real-world\ndatasets to demonstrate that our design can enhance novel view synthesis across\nNeRF and 3DGS. The code and dataset will be made available for public access.\n", "link": "http://arxiv.org/abs/2404.06710v3", "date": "2024-04-12", "relevancy": 1.5802, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5474}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4922}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SpikeNVS%3A%20Enhancing%20Novel%20View%20Synthesis%20from%20Blurry%20Images%20via%20Spike%0A%20%20Camera&body=Title%3A%20SpikeNVS%3A%20Enhancing%20Novel%20View%20Synthesis%20from%20Blurry%20Images%20via%20Spike%0A%20%20Camera%0AAuthor%3A%20Gaole%20Dai%20and%20Zhenyu%20Wang%20and%20Qinwen%20Xu%20and%20Ming%20Lu%20and%20Wen%20Chen%20and%20Boxin%20Shi%20and%20Shanghang%20Zhang%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%20One%20of%20the%20most%20critical%20factors%20in%20achieving%20sharp%20Novel%20View%20Synthesis%0A%28NVS%29%20using%20neural%20field%20methods%20like%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20is%20the%20quality%20of%20the%20training%20images.%20However%2C%0AConventional%20RGB%20cameras%20are%20susceptible%20to%20motion%20blur.%20In%20contrast%2C%0Aneuromorphic%20cameras%20like%20event%20and%20spike%20cameras%20inherently%20capture%20more%0Acomprehensive%20temporal%20information%2C%20which%20can%20provide%20a%20sharp%20representation%20of%0Athe%20scene%20as%20additional%20training%20data.%20Recent%20methods%20have%20explored%20the%0Aintegration%20of%20event%20cameras%20to%20improve%20the%20quality%20of%20NVS.%20The%20event-RGB%0Aapproaches%20have%20some%20limitations%2C%20such%20as%20high%20training%20costs%20and%20the%20inability%0Ato%20work%20effectively%20in%20the%20background.%20Instead%2C%20our%20study%20introduces%20a%20new%0Amethod%20that%20uses%20the%20spike%20camera%20to%20overcome%20these%20limitations.%20By%20considering%0Atexture%20reconstruction%20from%20spike%20streams%20as%20ground%20truth%2C%20we%20design%20the%0ATexture%20from%20Spike%20%28TfS%29%20loss.%20Since%20the%20spike%20camera%20relies%20on%20temporal%0Aintegration%20instead%20of%20temporal%20differentiation%20used%20by%20event%20cameras%2C%20our%0Aproposed%20TfS%20loss%20maintains%20manageable%20training%20costs.%20It%20handles%20foreground%0Aobjects%20with%20backgrounds%20simultaneously.%20We%20also%20provide%20a%20real-world%20dataset%0Acaptured%20with%20our%20spike-RGB%20camera%20system%20to%20facilitate%20future%20research%0Aendeavors.%20We%20conduct%20extensive%20experiments%20using%20synthetic%20and%20real-world%0Adatasets%20to%20demonstrate%20that%20our%20design%20can%20enhance%20novel%20view%20synthesis%20across%0ANeRF%20and%203DGS.%20The%20code%20and%20dataset%20will%20be%20made%20available%20for%20public%20access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06710v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeNVS%3A%20Enhancing%20Novel%20View%20Synthesis%20from%20Blurry%20Images%20via%20Spike%0A%20%20Camera&entry.906535625=Gaole%20Dai%20and%20Zhenyu%20Wang%20and%20Qinwen%20Xu%20and%20Ming%20Lu%20and%20Wen%20Chen%20and%20Boxin%20Shi%20and%20Shanghang%20Zhang%20and%20Tiejun%20Huang&entry.1292438233=%20%20One%20of%20the%20most%20critical%20factors%20in%20achieving%20sharp%20Novel%20View%20Synthesis%0A%28NVS%29%20using%20neural%20field%20methods%20like%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29%20is%20the%20quality%20of%20the%20training%20images.%20However%2C%0AConventional%20RGB%20cameras%20are%20susceptible%20to%20motion%20blur.%20In%20contrast%2C%0Aneuromorphic%20cameras%20like%20event%20and%20spike%20cameras%20inherently%20capture%20more%0Acomprehensive%20temporal%20information%2C%20which%20can%20provide%20a%20sharp%20representation%20of%0Athe%20scene%20as%20additional%20training%20data.%20Recent%20methods%20have%20explored%20the%0Aintegration%20of%20event%20cameras%20to%20improve%20the%20quality%20of%20NVS.%20The%20event-RGB%0Aapproaches%20have%20some%20limitations%2C%20such%20as%20high%20training%20costs%20and%20the%20inability%0Ato%20work%20effectively%20in%20the%20background.%20Instead%2C%20our%20study%20introduces%20a%20new%0Amethod%20that%20uses%20the%20spike%20camera%20to%20overcome%20these%20limitations.%20By%20considering%0Atexture%20reconstruction%20from%20spike%20streams%20as%20ground%20truth%2C%20we%20design%20the%0ATexture%20from%20Spike%20%28TfS%29%20loss.%20Since%20the%20spike%20camera%20relies%20on%20temporal%0Aintegration%20instead%20of%20temporal%20differentiation%20used%20by%20event%20cameras%2C%20our%0Aproposed%20TfS%20loss%20maintains%20manageable%20training%20costs.%20It%20handles%20foreground%0Aobjects%20with%20backgrounds%20simultaneously.%20We%20also%20provide%20a%20real-world%20dataset%0Acaptured%20with%20our%20spike-RGB%20camera%20system%20to%20facilitate%20future%20research%0Aendeavors.%20We%20conduct%20extensive%20experiments%20using%20synthetic%20and%20real-world%0Adatasets%20to%20demonstrate%20that%20our%20design%20can%20enhance%20novel%20view%20synthesis%20across%0ANeRF%20and%203DGS.%20The%20code%20and%20dataset%20will%20be%20made%20available%20for%20public%20access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06710v3&entry.124074799=Read"},
{"title": "Look at the Text: Instruction-Tuned Language Models are More Robust\n  Multiple Choice Selectors than You Think", "author": "Xinpeng Wang and Chengzhi Hu and Bolei Ma and Paul R\u00f6ttger and Barbara Plank", "abstract": "  Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation.\n", "link": "http://arxiv.org/abs/2404.08382v1", "date": "2024-04-12", "relevancy": 0.8955, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.451}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4485}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Look%20at%20the%20Text%3A%20Instruction-Tuned%20Language%20Models%20are%20More%20Robust%0A%20%20Multiple%20Choice%20Selectors%20than%20You%20Think&body=Title%3A%20Look%20at%20the%20Text%3A%20Instruction-Tuned%20Language%20Models%20are%20More%20Robust%0A%20%20Multiple%20Choice%20Selectors%20than%20You%20Think%0AAuthor%3A%20Xinpeng%20Wang%20and%20Chengzhi%20Hu%20and%20Bolei%20Ma%20and%20Paul%20R%C3%B6ttger%20and%20Barbara%20Plank%0AAbstract%3A%20%20%20Multiple%20choice%20questions%20%28MCQs%29%20are%20commonly%20used%20to%20evaluate%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20One%20common%20way%20to%20evaluate%20the%0Amodel%20response%20is%20to%20rank%20the%20candidate%20answers%20based%20on%20the%20log%20probability%20of%0Athe%20first%20token%20prediction.%20An%20alternative%20way%20is%20to%20examine%20the%20text%20output.%0APrior%20work%20has%20shown%20that%20first%20token%20probabilities%20lack%20robustness%20to%20changes%0Ain%20MCQ%20phrasing%2C%20and%20that%20first%20token%20probabilities%20do%20not%20match%20text%20answers%0Afor%20instruction-tuned%20models.%20Therefore%2C%20in%20this%20paper%2C%20we%20investigate%20the%0Arobustness%20of%20text%20answers.%20We%20show%20that%20the%20text%20answers%20are%20more%20robust%20to%0Aquestion%20perturbations%20than%20the%20first%20token%20probabilities%2C%20when%20the%20first%20token%0Aanswers%20mismatch%20the%20text%20answers.%20The%20difference%20in%20robustness%20increases%20as%0Athe%20mismatch%20rate%20becomes%20greater.%20As%20the%20mismatch%20reaches%20over%2050%5C%25%2C%20the%20text%0Aanswer%20is%20more%20robust%20to%20option%20order%20changes%20than%20the%20debiased%20first%20token%0Aprobabilities%20using%20state-of-the-art%20debiasing%20methods%20such%20as%20PriDe.%20Our%0Afindings%20provide%20further%20evidence%20for%20the%20benefits%20of%20text%20answer%20evaluation%0Aover%20first%20token%20probability%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08382v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20at%20the%20Text%3A%20Instruction-Tuned%20Language%20Models%20are%20More%20Robust%0A%20%20Multiple%20Choice%20Selectors%20than%20You%20Think&entry.906535625=Xinpeng%20Wang%20and%20Chengzhi%20Hu%20and%20Bolei%20Ma%20and%20Paul%20R%C3%B6ttger%20and%20Barbara%20Plank&entry.1292438233=%20%20Multiple%20choice%20questions%20%28MCQs%29%20are%20commonly%20used%20to%20evaluate%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20One%20common%20way%20to%20evaluate%20the%0Amodel%20response%20is%20to%20rank%20the%20candidate%20answers%20based%20on%20the%20log%20probability%20of%0Athe%20first%20token%20prediction.%20An%20alternative%20way%20is%20to%20examine%20the%20text%20output.%0APrior%20work%20has%20shown%20that%20first%20token%20probabilities%20lack%20robustness%20to%20changes%0Ain%20MCQ%20phrasing%2C%20and%20that%20first%20token%20probabilities%20do%20not%20match%20text%20answers%0Afor%20instruction-tuned%20models.%20Therefore%2C%20in%20this%20paper%2C%20we%20investigate%20the%0Arobustness%20of%20text%20answers.%20We%20show%20that%20the%20text%20answers%20are%20more%20robust%20to%0Aquestion%20perturbations%20than%20the%20first%20token%20probabilities%2C%20when%20the%20first%20token%0Aanswers%20mismatch%20the%20text%20answers.%20The%20difference%20in%20robustness%20increases%20as%0Athe%20mismatch%20rate%20becomes%20greater.%20As%20the%20mismatch%20reaches%20over%2050%5C%25%2C%20the%20text%0Aanswer%20is%20more%20robust%20to%20option%20order%20changes%20than%20the%20debiased%20first%20token%0Aprobabilities%20using%20state-of-the-art%20debiasing%20methods%20such%20as%20PriDe.%20Our%0Afindings%20provide%20further%20evidence%20for%20the%20benefits%20of%20text%20answer%20evaluation%0Aover%20first%20token%20probability%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08382v1&entry.124074799=Read"},
{"title": "Scalability in Building Component Data Annotation: Enhancing Facade\n  Material Classification with Synthetic Data", "author": "Josie Harrison and Alexander Hollberg and Yinan Yu", "abstract": "  Computer vision models trained on Google Street View images can create\nmaterial cadastres. However, current approaches need manually annotated\ndatasets that are difficult to obtain and often have class imbalance. To\naddress these challenges, this paper fine-tuned a Swin Transformer model on a\nsynthetic dataset generated with DALL-E and compared the performance to a\nsimilar manually annotated dataset. Although manual annotation remains the gold\nstandard, the synthetic dataset performance demonstrates a reasonable\nalternative. The findings will ease annotation needed to develop material\ncadastres, offering architects insights into opportunities for material reuse,\nthus contributing to the reduction of demolition waste.\n", "link": "http://arxiv.org/abs/2404.08557v1", "date": "2024-04-12", "relevancy": 1.5333, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5298}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scalability%20in%20Building%20Component%20Data%20Annotation%3A%20Enhancing%20Facade%0A%20%20Material%20Classification%20with%20Synthetic%20Data&body=Title%3A%20Scalability%20in%20Building%20Component%20Data%20Annotation%3A%20Enhancing%20Facade%0A%20%20Material%20Classification%20with%20Synthetic%20Data%0AAuthor%3A%20Josie%20Harrison%20and%20Alexander%20Hollberg%20and%20Yinan%20Yu%0AAbstract%3A%20%20%20Computer%20vision%20models%20trained%20on%20Google%20Street%20View%20images%20can%20create%0Amaterial%20cadastres.%20However%2C%20current%20approaches%20need%20manually%20annotated%0Adatasets%20that%20are%20difficult%20to%20obtain%20and%20often%20have%20class%20imbalance.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20fine-tuned%20a%20Swin%20Transformer%20model%20on%20a%0Asynthetic%20dataset%20generated%20with%20DALL-E%20and%20compared%20the%20performance%20to%20a%0Asimilar%20manually%20annotated%20dataset.%20Although%20manual%20annotation%20remains%20the%20gold%0Astandard%2C%20the%20synthetic%20dataset%20performance%20demonstrates%20a%20reasonable%0Aalternative.%20The%20findings%20will%20ease%20annotation%20needed%20to%20develop%20material%0Acadastres%2C%20offering%20architects%20insights%20into%20opportunities%20for%20material%20reuse%2C%0Athus%20contributing%20to%20the%20reduction%20of%20demolition%20waste.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08557v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalability%20in%20Building%20Component%20Data%20Annotation%3A%20Enhancing%20Facade%0A%20%20Material%20Classification%20with%20Synthetic%20Data&entry.906535625=Josie%20Harrison%20and%20Alexander%20Hollberg%20and%20Yinan%20Yu&entry.1292438233=%20%20Computer%20vision%20models%20trained%20on%20Google%20Street%20View%20images%20can%20create%0Amaterial%20cadastres.%20However%2C%20current%20approaches%20need%20manually%20annotated%0Adatasets%20that%20are%20difficult%20to%20obtain%20and%20often%20have%20class%20imbalance.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20fine-tuned%20a%20Swin%20Transformer%20model%20on%20a%0Asynthetic%20dataset%20generated%20with%20DALL-E%20and%20compared%20the%20performance%20to%20a%0Asimilar%20manually%20annotated%20dataset.%20Although%20manual%20annotation%20remains%20the%20gold%0Astandard%2C%20the%20synthetic%20dataset%20performance%20demonstrates%20a%20reasonable%0Aalternative.%20The%20findings%20will%20ease%20annotation%20needed%20to%20develop%20material%0Acadastres%2C%20offering%20architects%20insights%20into%20opportunities%20for%20material%20reuse%2C%0Athus%20contributing%20to%20the%20reduction%20of%20demolition%20waste.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08557v1&entry.124074799=Read"},
{"title": "Enhancing Autonomous Vehicle Training with Language Model Integration\n  and Critical Scenario Generation", "author": "Hanlin Tian and Kethan Reddy and Yuxiang Feng and Mohammed Quddus and Yiannis Demiris and Panagiotis Angeloudis", "abstract": "  This paper introduces CRITICAL, a novel closed-loop framework for autonomous\nvehicle (AV) training and testing. CRITICAL stands out for its ability to\ngenerate diverse scenarios, focusing on critical driving situations that target\nspecific learning and performance gaps identified in the Reinforcement Learning\n(RL) agent. The framework achieves this by integrating real-world traffic\ndynamics, driving behavior analysis, surrogate safety measures, and an optional\nLarge Language Model (LLM) component. It is proven that the establishment of a\nclosed feedback loop between the data generation pipeline and the training\nprocess can enhance the learning rate during training, elevate overall system\nperformance, and augment safety resilience. Our evaluations, conducted using\nthe Proximal Policy Optimization (PPO) and the HighwayEnv simulation\nenvironment, demonstrate noticeable performance improvements with the\nintegration of critical case generation and LLM analysis, indicating CRITICAL's\npotential to improve the robustness of AV systems and streamline the generation\nof critical scenarios. This ultimately serves to hasten the development of AV\nagents, expand the general scope of RL training, and ameliorate validation\nefforts for AV safety.\n", "link": "http://arxiv.org/abs/2404.08570v1", "date": "2024-04-12", "relevancy": 1.6243, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5558}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5271}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Autonomous%20Vehicle%20Training%20with%20Language%20Model%20Integration%0A%20%20and%20Critical%20Scenario%20Generation&body=Title%3A%20Enhancing%20Autonomous%20Vehicle%20Training%20with%20Language%20Model%20Integration%0A%20%20and%20Critical%20Scenario%20Generation%0AAuthor%3A%20Hanlin%20Tian%20and%20Kethan%20Reddy%20and%20Yuxiang%20Feng%20and%20Mohammed%20Quddus%20and%20Yiannis%20Demiris%20and%20Panagiotis%20Angeloudis%0AAbstract%3A%20%20%20This%20paper%20introduces%20CRITICAL%2C%20a%20novel%20closed-loop%20framework%20for%20autonomous%0Avehicle%20%28AV%29%20training%20and%20testing.%20CRITICAL%20stands%20out%20for%20its%20ability%20to%0Agenerate%20diverse%20scenarios%2C%20focusing%20on%20critical%20driving%20situations%20that%20target%0Aspecific%20learning%20and%20performance%20gaps%20identified%20in%20the%20Reinforcement%20Learning%0A%28RL%29%20agent.%20The%20framework%20achieves%20this%20by%20integrating%20real-world%20traffic%0Adynamics%2C%20driving%20behavior%20analysis%2C%20surrogate%20safety%20measures%2C%20and%20an%20optional%0ALarge%20Language%20Model%20%28LLM%29%20component.%20It%20is%20proven%20that%20the%20establishment%20of%20a%0Aclosed%20feedback%20loop%20between%20the%20data%20generation%20pipeline%20and%20the%20training%0Aprocess%20can%20enhance%20the%20learning%20rate%20during%20training%2C%20elevate%20overall%20system%0Aperformance%2C%20and%20augment%20safety%20resilience.%20Our%20evaluations%2C%20conducted%20using%0Athe%20Proximal%20Policy%20Optimization%20%28PPO%29%20and%20the%20HighwayEnv%20simulation%0Aenvironment%2C%20demonstrate%20noticeable%20performance%20improvements%20with%20the%0Aintegration%20of%20critical%20case%20generation%20and%20LLM%20analysis%2C%20indicating%20CRITICAL%27s%0Apotential%20to%20improve%20the%20robustness%20of%20AV%20systems%20and%20streamline%20the%20generation%0Aof%20critical%20scenarios.%20This%20ultimately%20serves%20to%20hasten%20the%20development%20of%20AV%0Aagents%2C%20expand%20the%20general%20scope%20of%20RL%20training%2C%20and%20ameliorate%20validation%0Aefforts%20for%20AV%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08570v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Autonomous%20Vehicle%20Training%20with%20Language%20Model%20Integration%0A%20%20and%20Critical%20Scenario%20Generation&entry.906535625=Hanlin%20Tian%20and%20Kethan%20Reddy%20and%20Yuxiang%20Feng%20and%20Mohammed%20Quddus%20and%20Yiannis%20Demiris%20and%20Panagiotis%20Angeloudis&entry.1292438233=%20%20This%20paper%20introduces%20CRITICAL%2C%20a%20novel%20closed-loop%20framework%20for%20autonomous%0Avehicle%20%28AV%29%20training%20and%20testing.%20CRITICAL%20stands%20out%20for%20its%20ability%20to%0Agenerate%20diverse%20scenarios%2C%20focusing%20on%20critical%20driving%20situations%20that%20target%0Aspecific%20learning%20and%20performance%20gaps%20identified%20in%20the%20Reinforcement%20Learning%0A%28RL%29%20agent.%20The%20framework%20achieves%20this%20by%20integrating%20real-world%20traffic%0Adynamics%2C%20driving%20behavior%20analysis%2C%20surrogate%20safety%20measures%2C%20and%20an%20optional%0ALarge%20Language%20Model%20%28LLM%29%20component.%20It%20is%20proven%20that%20the%20establishment%20of%20a%0Aclosed%20feedback%20loop%20between%20the%20data%20generation%20pipeline%20and%20the%20training%0Aprocess%20can%20enhance%20the%20learning%20rate%20during%20training%2C%20elevate%20overall%20system%0Aperformance%2C%20and%20augment%20safety%20resilience.%20Our%20evaluations%2C%20conducted%20using%0Athe%20Proximal%20Policy%20Optimization%20%28PPO%29%20and%20the%20HighwayEnv%20simulation%0Aenvironment%2C%20demonstrate%20noticeable%20performance%20improvements%20with%20the%0Aintegration%20of%20critical%20case%20generation%20and%20LLM%20analysis%2C%20indicating%20CRITICAL%27s%0Apotential%20to%20improve%20the%20robustness%20of%20AV%20systems%20and%20streamline%20the%20generation%0Aof%20critical%20scenarios.%20This%20ultimately%20serves%20to%20hasten%20the%20development%20of%20AV%0Aagents%2C%20expand%20the%20general%20scope%20of%20RL%20training%2C%20and%20ameliorate%20validation%0Aefforts%20for%20AV%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08570v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


