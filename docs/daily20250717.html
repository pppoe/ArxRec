<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250716.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion\n  Priors", "author": "Subin Jeon and In Cho and Junyoung Hong and Seon Joo Kim", "abstract": "  This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.\n", "link": "http://arxiv.org/abs/2507.12336v1", "date": "2025-07-16", "relevancy": 3.3375, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6594}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Monocular%203D%20Keypoint%20Discovery%20from%20Multi-View%20Diffusion%0A%20%20Priors&body=Title%3A%20Unsupervised%20Monocular%203D%20Keypoint%20Discovery%20from%20Multi-View%20Diffusion%0A%20%20Priors%0AAuthor%3A%20Subin%20Jeon%20and%20In%20Cho%20and%20Junyoung%20Hong%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20This%20paper%20introduces%20KeyDiff3D%2C%20a%20framework%20for%20unsupervised%20monocular%203D%0Akeypoints%20estimation%20that%20accurately%20predicts%203D%20keypoints%20from%20a%20single%20image.%0AWhile%20previous%20methods%20rely%20on%20manual%20annotations%20or%20calibrated%20multi-view%0Aimages%2C%20both%20of%20which%20are%20expensive%20to%20collect%2C%20our%20method%20enables%20monocular%203D%0Akeypoints%20estimation%20using%20only%20a%20collection%20of%20single-view%20images.%20To%20achieve%0Athis%2C%20we%20leverage%20powerful%20geometric%20priors%20embedded%20in%20a%20pretrained%20multi-view%0Adiffusion%20model.%20In%20our%20framework%2C%20this%20model%20generates%20multi-view%20images%20from%0Aa%20single%20image%2C%20serving%20as%20a%20supervision%20signal%20to%20provide%203D%20geometric%20cues%20to%0Aour%20model.%20We%20also%20use%20the%20diffusion%20model%20as%20a%20powerful%202D%20multi-view%20feature%0Aextractor%20and%20construct%203D%20feature%20volumes%20from%20its%20intermediate%0Arepresentations.%20This%20transforms%20implicit%203D%20priors%20learned%20by%20the%20diffusion%0Amodel%20into%20explicit%203D%20features.%20Beyond%20accurate%20keypoints%20estimation%2C%20we%0Afurther%20introduce%20a%20pipeline%20that%20enables%20manipulation%20of%203D%20objects%20generated%0Aby%20the%20diffusion%20model.%20Experimental%20results%20on%20diverse%20aspects%20and%20datasets%2C%0Aincluding%20Human3.6M%2C%20Stanford%20Dogs%2C%20and%20several%20in-the-wild%20and%20out-of-domain%0Adatasets%2C%20highlight%20the%20effectiveness%20of%20our%20method%20in%20terms%20of%20accuracy%2C%0Ageneralization%2C%20and%20its%20ability%20to%20enable%20manipulation%20of%203D%20objects%20generated%0Aby%20the%20diffusion%20model%20from%20a%20single%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Monocular%25203D%2520Keypoint%2520Discovery%2520from%2520Multi-View%2520Diffusion%250A%2520%2520Priors%26entry.906535625%3DSubin%2520Jeon%2520and%2520In%2520Cho%2520and%2520Junyoung%2520Hong%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520KeyDiff3D%252C%2520a%2520framework%2520for%2520unsupervised%2520monocular%25203D%250Akeypoints%2520estimation%2520that%2520accurately%2520predicts%25203D%2520keypoints%2520from%2520a%2520single%2520image.%250AWhile%2520previous%2520methods%2520rely%2520on%2520manual%2520annotations%2520or%2520calibrated%2520multi-view%250Aimages%252C%2520both%2520of%2520which%2520are%2520expensive%2520to%2520collect%252C%2520our%2520method%2520enables%2520monocular%25203D%250Akeypoints%2520estimation%2520using%2520only%2520a%2520collection%2520of%2520single-view%2520images.%2520To%2520achieve%250Athis%252C%2520we%2520leverage%2520powerful%2520geometric%2520priors%2520embedded%2520in%2520a%2520pretrained%2520multi-view%250Adiffusion%2520model.%2520In%2520our%2520framework%252C%2520this%2520model%2520generates%2520multi-view%2520images%2520from%250Aa%2520single%2520image%252C%2520serving%2520as%2520a%2520supervision%2520signal%2520to%2520provide%25203D%2520geometric%2520cues%2520to%250Aour%2520model.%2520We%2520also%2520use%2520the%2520diffusion%2520model%2520as%2520a%2520powerful%25202D%2520multi-view%2520feature%250Aextractor%2520and%2520construct%25203D%2520feature%2520volumes%2520from%2520its%2520intermediate%250Arepresentations.%2520This%2520transforms%2520implicit%25203D%2520priors%2520learned%2520by%2520the%2520diffusion%250Amodel%2520into%2520explicit%25203D%2520features.%2520Beyond%2520accurate%2520keypoints%2520estimation%252C%2520we%250Afurther%2520introduce%2520a%2520pipeline%2520that%2520enables%2520manipulation%2520of%25203D%2520objects%2520generated%250Aby%2520the%2520diffusion%2520model.%2520Experimental%2520results%2520on%2520diverse%2520aspects%2520and%2520datasets%252C%250Aincluding%2520Human3.6M%252C%2520Stanford%2520Dogs%252C%2520and%2520several%2520in-the-wild%2520and%2520out-of-domain%250Adatasets%252C%2520highlight%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520terms%2520of%2520accuracy%252C%250Ageneralization%252C%2520and%2520its%2520ability%2520to%2520enable%2520manipulation%2520of%25203D%2520objects%2520generated%250Aby%2520the%2520diffusion%2520model%2520from%2520a%2520single%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Monocular%203D%20Keypoint%20Discovery%20from%20Multi-View%20Diffusion%0A%20%20Priors&entry.906535625=Subin%20Jeon%20and%20In%20Cho%20and%20Junyoung%20Hong%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20This%20paper%20introduces%20KeyDiff3D%2C%20a%20framework%20for%20unsupervised%20monocular%203D%0Akeypoints%20estimation%20that%20accurately%20predicts%203D%20keypoints%20from%20a%20single%20image.%0AWhile%20previous%20methods%20rely%20on%20manual%20annotations%20or%20calibrated%20multi-view%0Aimages%2C%20both%20of%20which%20are%20expensive%20to%20collect%2C%20our%20method%20enables%20monocular%203D%0Akeypoints%20estimation%20using%20only%20a%20collection%20of%20single-view%20images.%20To%20achieve%0Athis%2C%20we%20leverage%20powerful%20geometric%20priors%20embedded%20in%20a%20pretrained%20multi-view%0Adiffusion%20model.%20In%20our%20framework%2C%20this%20model%20generates%20multi-view%20images%20from%0Aa%20single%20image%2C%20serving%20as%20a%20supervision%20signal%20to%20provide%203D%20geometric%20cues%20to%0Aour%20model.%20We%20also%20use%20the%20diffusion%20model%20as%20a%20powerful%202D%20multi-view%20feature%0Aextractor%20and%20construct%203D%20feature%20volumes%20from%20its%20intermediate%0Arepresentations.%20This%20transforms%20implicit%203D%20priors%20learned%20by%20the%20diffusion%0Amodel%20into%20explicit%203D%20features.%20Beyond%20accurate%20keypoints%20estimation%2C%20we%0Afurther%20introduce%20a%20pipeline%20that%20enables%20manipulation%20of%203D%20objects%20generated%0Aby%20the%20diffusion%20model.%20Experimental%20results%20on%20diverse%20aspects%20and%20datasets%2C%0Aincluding%20Human3.6M%2C%20Stanford%20Dogs%2C%20and%20several%20in-the-wild%20and%20out-of-domain%0Adatasets%2C%20highlight%20the%20effectiveness%20of%20our%20method%20in%20terms%20of%20accuracy%2C%0Ageneralization%2C%20and%20its%20ability%20to%20enable%20manipulation%20of%203D%20objects%20generated%0Aby%20the%20diffusion%20model%20from%20a%20single%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12336v1&entry.124074799=Read"},
{"title": "PhysX: Physical-Grounded 3D Asset Generation", "author": "Ziang Cao and Zhaoxi Chen and Linag Pan and Ziwei Liu", "abstract": "  3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.\n", "link": "http://arxiv.org/abs/2507.12465v1", "date": "2025-07-16", "relevancy": 3.0611, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7064}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5674}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysX%3A%20Physical-Grounded%203D%20Asset%20Generation&body=Title%3A%20PhysX%3A%20Physical-Grounded%203D%20Asset%20Generation%0AAuthor%3A%20Ziang%20Cao%20and%20Zhaoxi%20Chen%20and%20Linag%20Pan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%203D%20modeling%20is%20moving%20from%20virtual%20to%20physical.%20Existing%203D%20generation%0Aprimarily%20emphasizes%20geometries%20and%20textures%20while%20neglecting%20physical-grounded%0Amodeling.%20Consequently%2C%20despite%20the%20rapid%20development%20of%203D%20generative%20models%2C%0Athe%20synthesized%203D%20assets%20often%20overlook%20rich%20and%20important%20physical%0Aproperties%2C%20hampering%20their%20real-world%20application%20in%20physical%20domains%20like%0Asimulation%20and%20embodied%20AI.%20As%20an%20initial%20attempt%20to%20address%20this%20challenge%2C%20we%0Apropose%20%5Ctextbf%7BPhysX%7D%2C%20an%20end-to-end%20paradigm%20for%20physical-grounded%203D%20asset%0Ageneration.%201%29%20To%20bridge%20the%20critical%20gap%20in%20physics-annotated%203D%20datasets%2C%20we%0Apresent%20PhysXNet%20-%20the%20first%20physics-grounded%203D%20dataset%20systematically%0Aannotated%20across%20five%20foundational%20dimensions%3A%20absolute%20scale%2C%20material%2C%0Aaffordance%2C%20kinematics%2C%20and%20function%20description.%20In%20particular%2C%20we%20devise%20a%0Ascalable%20human-in-the-loop%20annotation%20pipeline%20based%20on%20vision-language%20models%2C%0Awhich%20enables%20efficient%20creation%20of%20physics-first%20assets%20from%20raw%203D%20assets.2%29%0AFurthermore%2C%20we%20propose%20%5Ctextbf%7BPhysXGen%7D%2C%20a%20feed-forward%20framework%20for%0Aphysics-grounded%20image-to-3D%20asset%20generation%2C%20injecting%20physical%20knowledge%0Ainto%20the%20pre-trained%203D%20structural%20space.%20Specifically%2C%20PhysXGen%20employs%20a%0Adual-branch%20architecture%20to%20explicitly%20model%20the%20latent%20correlations%20between%203D%0Astructures%20and%20physical%20properties%2C%20thereby%20producing%203D%20assets%20with%20plausible%0Aphysical%20predictions%20while%20preserving%20the%20native%20geometry%20quality.%20Extensive%0Aexperiments%20validate%20the%20superior%20performance%20and%20promising%20generalization%0Acapability%20of%20our%20framework.%20All%20the%20code%2C%20data%2C%20and%20models%20will%20be%20released%20to%0Afacilitate%20future%20research%20in%20generative%20physical%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysX%253A%2520Physical-Grounded%25203D%2520Asset%2520Generation%26entry.906535625%3DZiang%2520Cao%2520and%2520Zhaoxi%2520Chen%2520and%2520Linag%2520Pan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%25203D%2520modeling%2520is%2520moving%2520from%2520virtual%2520to%2520physical.%2520Existing%25203D%2520generation%250Aprimarily%2520emphasizes%2520geometries%2520and%2520textures%2520while%2520neglecting%2520physical-grounded%250Amodeling.%2520Consequently%252C%2520despite%2520the%2520rapid%2520development%2520of%25203D%2520generative%2520models%252C%250Athe%2520synthesized%25203D%2520assets%2520often%2520overlook%2520rich%2520and%2520important%2520physical%250Aproperties%252C%2520hampering%2520their%2520real-world%2520application%2520in%2520physical%2520domains%2520like%250Asimulation%2520and%2520embodied%2520AI.%2520As%2520an%2520initial%2520attempt%2520to%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520%255Ctextbf%257BPhysX%257D%252C%2520an%2520end-to-end%2520paradigm%2520for%2520physical-grounded%25203D%2520asset%250Ageneration.%25201%2529%2520To%2520bridge%2520the%2520critical%2520gap%2520in%2520physics-annotated%25203D%2520datasets%252C%2520we%250Apresent%2520PhysXNet%2520-%2520the%2520first%2520physics-grounded%25203D%2520dataset%2520systematically%250Aannotated%2520across%2520five%2520foundational%2520dimensions%253A%2520absolute%2520scale%252C%2520material%252C%250Aaffordance%252C%2520kinematics%252C%2520and%2520function%2520description.%2520In%2520particular%252C%2520we%2520devise%2520a%250Ascalable%2520human-in-the-loop%2520annotation%2520pipeline%2520based%2520on%2520vision-language%2520models%252C%250Awhich%2520enables%2520efficient%2520creation%2520of%2520physics-first%2520assets%2520from%2520raw%25203D%2520assets.2%2529%250AFurthermore%252C%2520we%2520propose%2520%255Ctextbf%257BPhysXGen%257D%252C%2520a%2520feed-forward%2520framework%2520for%250Aphysics-grounded%2520image-to-3D%2520asset%2520generation%252C%2520injecting%2520physical%2520knowledge%250Ainto%2520the%2520pre-trained%25203D%2520structural%2520space.%2520Specifically%252C%2520PhysXGen%2520employs%2520a%250Adual-branch%2520architecture%2520to%2520explicitly%2520model%2520the%2520latent%2520correlations%2520between%25203D%250Astructures%2520and%2520physical%2520properties%252C%2520thereby%2520producing%25203D%2520assets%2520with%2520plausible%250Aphysical%2520predictions%2520while%2520preserving%2520the%2520native%2520geometry%2520quality.%2520Extensive%250Aexperiments%2520validate%2520the%2520superior%2520performance%2520and%2520promising%2520generalization%250Acapability%2520of%2520our%2520framework.%2520All%2520the%2520code%252C%2520data%252C%2520and%2520models%2520will%2520be%2520released%2520to%250Afacilitate%2520future%2520research%2520in%2520generative%2520physical%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysX%3A%20Physical-Grounded%203D%20Asset%20Generation&entry.906535625=Ziang%20Cao%20and%20Zhaoxi%20Chen%20and%20Linag%20Pan%20and%20Ziwei%20Liu&entry.1292438233=%20%203D%20modeling%20is%20moving%20from%20virtual%20to%20physical.%20Existing%203D%20generation%0Aprimarily%20emphasizes%20geometries%20and%20textures%20while%20neglecting%20physical-grounded%0Amodeling.%20Consequently%2C%20despite%20the%20rapid%20development%20of%203D%20generative%20models%2C%0Athe%20synthesized%203D%20assets%20often%20overlook%20rich%20and%20important%20physical%0Aproperties%2C%20hampering%20their%20real-world%20application%20in%20physical%20domains%20like%0Asimulation%20and%20embodied%20AI.%20As%20an%20initial%20attempt%20to%20address%20this%20challenge%2C%20we%0Apropose%20%5Ctextbf%7BPhysX%7D%2C%20an%20end-to-end%20paradigm%20for%20physical-grounded%203D%20asset%0Ageneration.%201%29%20To%20bridge%20the%20critical%20gap%20in%20physics-annotated%203D%20datasets%2C%20we%0Apresent%20PhysXNet%20-%20the%20first%20physics-grounded%203D%20dataset%20systematically%0Aannotated%20across%20five%20foundational%20dimensions%3A%20absolute%20scale%2C%20material%2C%0Aaffordance%2C%20kinematics%2C%20and%20function%20description.%20In%20particular%2C%20we%20devise%20a%0Ascalable%20human-in-the-loop%20annotation%20pipeline%20based%20on%20vision-language%20models%2C%0Awhich%20enables%20efficient%20creation%20of%20physics-first%20assets%20from%20raw%203D%20assets.2%29%0AFurthermore%2C%20we%20propose%20%5Ctextbf%7BPhysXGen%7D%2C%20a%20feed-forward%20framework%20for%0Aphysics-grounded%20image-to-3D%20asset%20generation%2C%20injecting%20physical%20knowledge%0Ainto%20the%20pre-trained%203D%20structural%20space.%20Specifically%2C%20PhysXGen%20employs%20a%0Adual-branch%20architecture%20to%20explicitly%20model%20the%20latent%20correlations%20between%203D%0Astructures%20and%20physical%20properties%2C%20thereby%20producing%203D%20assets%20with%20plausible%0Aphysical%20predictions%20while%20preserving%20the%20native%20geometry%20quality.%20Extensive%0Aexperiments%20validate%20the%20superior%20performance%20and%20promising%20generalization%0Acapability%20of%20our%20framework.%20All%20the%20code%2C%20data%2C%20and%20models%20will%20be%20released%20to%0Afacilitate%20future%20research%20in%20generative%20physical%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12465v1&entry.124074799=Read"},
{"title": "Describe Anything Model for Visual Question Answering on Text-rich\n  Images", "author": "Yen-Linh Vu and Dinh-Thang Duong and Truong-Binh Duong and Anh-Khoi Nguyen and Thanh-Huy Nguyen and Le Thien Phuc Nguyen and Jianhua Xing and Xingjian Li and Tianyang Wang and Ulas Bagci and Min Xu", "abstract": "  Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.\n", "link": "http://arxiv.org/abs/2507.12441v1", "date": "2025-07-16", "relevancy": 2.9242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Describe%20Anything%20Model%20for%20Visual%20Question%20Answering%20on%20Text-rich%0A%20%20Images&body=Title%3A%20Describe%20Anything%20Model%20for%20Visual%20Question%20Answering%20on%20Text-rich%0A%20%20Images%0AAuthor%3A%20Yen-Linh%20Vu%20and%20Dinh-Thang%20Duong%20and%20Truong-Binh%20Duong%20and%20Anh-Khoi%20Nguyen%20and%20Thanh-Huy%20Nguyen%20and%20Le%20Thien%20Phuc%20Nguyen%20and%20Jianhua%20Xing%20and%20Xingjian%20Li%20and%20Tianyang%20Wang%20and%20Ulas%20Bagci%20and%20Min%20Xu%0AAbstract%3A%20%20%20Recent%20progress%20has%20been%20made%20in%20region-aware%20vision-language%20modeling%2C%0Aparticularly%20with%20the%20emergence%20of%20the%20Describe%20Anything%20Model%20%28DAM%29.%20DAM%20is%0Acapable%20of%20generating%20detailed%20descriptions%20of%20any%20specific%20image%20areas%20or%0Aobjects%20without%20the%20need%20for%20additional%20localized%20image-text%20alignment%0Asupervision.%20We%20hypothesize%20that%20such%20region-level%20descriptive%20capability%20is%0Abeneficial%20for%20the%20task%20of%20Visual%20Question%20Answering%20%28VQA%29%2C%20especially%20in%0Achallenging%20scenarios%20involving%20images%20with%20dense%20text.%20In%20such%20settings%2C%20the%0Afine-grained%20extraction%20of%20textual%20information%20is%20crucial%20to%20producing%20correct%0Aanswers.%20Motivated%20by%20this%2C%20we%20introduce%20DAM-QA%2C%20a%20framework%20with%20a%20tailored%0Aevaluation%20protocol%2C%20developed%20to%20investigate%20and%20harness%20the%20region-aware%0Acapabilities%20from%20DAM%20for%20the%20text-rich%20VQA%20problem%20that%20requires%20reasoning%0Aover%20text-based%20information%20within%20images.%20DAM-QA%20incorporates%20a%20mechanism%20that%0Aaggregates%20answers%20from%20multiple%20regional%20views%20of%20image%20content%2C%20enabling%20more%0Aeffective%20identification%20of%20evidence%20that%20may%20be%20tied%20to%20text-related%20elements.%0AExperiments%20on%20six%20VQA%20benchmarks%20show%20that%20our%20approach%20consistently%0Aoutperforms%20the%20baseline%20DAM%2C%20with%20a%20notable%207%2B%20point%20gain%20on%20DocVQA.%20DAM-QA%0Aalso%20achieves%20the%20best%20overall%20performance%20among%20region-aware%20models%20with%20fewer%0Aparameters%2C%20significantly%20narrowing%20the%20gap%20with%20strong%20generalist%20VLMs.%20These%0Aresults%20highlight%20the%20potential%20of%20DAM-like%20models%20for%20text-rich%20and%20broader%0AVQA%20tasks%20when%20paired%20with%20efficient%20usage%20and%20integration%20strategies.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/Linvyl/DAM-QA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescribe%2520Anything%2520Model%2520for%2520Visual%2520Question%2520Answering%2520on%2520Text-rich%250A%2520%2520Images%26entry.906535625%3DYen-Linh%2520Vu%2520and%2520Dinh-Thang%2520Duong%2520and%2520Truong-Binh%2520Duong%2520and%2520Anh-Khoi%2520Nguyen%2520and%2520Thanh-Huy%2520Nguyen%2520and%2520Le%2520Thien%2520Phuc%2520Nguyen%2520and%2520Jianhua%2520Xing%2520and%2520Xingjian%2520Li%2520and%2520Tianyang%2520Wang%2520and%2520Ulas%2520Bagci%2520and%2520Min%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520progress%2520has%2520been%2520made%2520in%2520region-aware%2520vision-language%2520modeling%252C%250Aparticularly%2520with%2520the%2520emergence%2520of%2520the%2520Describe%2520Anything%2520Model%2520%2528DAM%2529.%2520DAM%2520is%250Acapable%2520of%2520generating%2520detailed%2520descriptions%2520of%2520any%2520specific%2520image%2520areas%2520or%250Aobjects%2520without%2520the%2520need%2520for%2520additional%2520localized%2520image-text%2520alignment%250Asupervision.%2520We%2520hypothesize%2520that%2520such%2520region-level%2520descriptive%2520capability%2520is%250Abeneficial%2520for%2520the%2520task%2520of%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520especially%2520in%250Achallenging%2520scenarios%2520involving%2520images%2520with%2520dense%2520text.%2520In%2520such%2520settings%252C%2520the%250Afine-grained%2520extraction%2520of%2520textual%2520information%2520is%2520crucial%2520to%2520producing%2520correct%250Aanswers.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520DAM-QA%252C%2520a%2520framework%2520with%2520a%2520tailored%250Aevaluation%2520protocol%252C%2520developed%2520to%2520investigate%2520and%2520harness%2520the%2520region-aware%250Acapabilities%2520from%2520DAM%2520for%2520the%2520text-rich%2520VQA%2520problem%2520that%2520requires%2520reasoning%250Aover%2520text-based%2520information%2520within%2520images.%2520DAM-QA%2520incorporates%2520a%2520mechanism%2520that%250Aaggregates%2520answers%2520from%2520multiple%2520regional%2520views%2520of%2520image%2520content%252C%2520enabling%2520more%250Aeffective%2520identification%2520of%2520evidence%2520that%2520may%2520be%2520tied%2520to%2520text-related%2520elements.%250AExperiments%2520on%2520six%2520VQA%2520benchmarks%2520show%2520that%2520our%2520approach%2520consistently%250Aoutperforms%2520the%2520baseline%2520DAM%252C%2520with%2520a%2520notable%25207%252B%2520point%2520gain%2520on%2520DocVQA.%2520DAM-QA%250Aalso%2520achieves%2520the%2520best%2520overall%2520performance%2520among%2520region-aware%2520models%2520with%2520fewer%250Aparameters%252C%2520significantly%2520narrowing%2520the%2520gap%2520with%2520strong%2520generalist%2520VLMs.%2520These%250Aresults%2520highlight%2520the%2520potential%2520of%2520DAM-like%2520models%2520for%2520text-rich%2520and%2520broader%250AVQA%2520tasks%2520when%2520paired%2520with%2520efficient%2520usage%2520and%2520integration%2520strategies.%2520Our%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/Linvyl/DAM-QA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Describe%20Anything%20Model%20for%20Visual%20Question%20Answering%20on%20Text-rich%0A%20%20Images&entry.906535625=Yen-Linh%20Vu%20and%20Dinh-Thang%20Duong%20and%20Truong-Binh%20Duong%20and%20Anh-Khoi%20Nguyen%20and%20Thanh-Huy%20Nguyen%20and%20Le%20Thien%20Phuc%20Nguyen%20and%20Jianhua%20Xing%20and%20Xingjian%20Li%20and%20Tianyang%20Wang%20and%20Ulas%20Bagci%20and%20Min%20Xu&entry.1292438233=%20%20Recent%20progress%20has%20been%20made%20in%20region-aware%20vision-language%20modeling%2C%0Aparticularly%20with%20the%20emergence%20of%20the%20Describe%20Anything%20Model%20%28DAM%29.%20DAM%20is%0Acapable%20of%20generating%20detailed%20descriptions%20of%20any%20specific%20image%20areas%20or%0Aobjects%20without%20the%20need%20for%20additional%20localized%20image-text%20alignment%0Asupervision.%20We%20hypothesize%20that%20such%20region-level%20descriptive%20capability%20is%0Abeneficial%20for%20the%20task%20of%20Visual%20Question%20Answering%20%28VQA%29%2C%20especially%20in%0Achallenging%20scenarios%20involving%20images%20with%20dense%20text.%20In%20such%20settings%2C%20the%0Afine-grained%20extraction%20of%20textual%20information%20is%20crucial%20to%20producing%20correct%0Aanswers.%20Motivated%20by%20this%2C%20we%20introduce%20DAM-QA%2C%20a%20framework%20with%20a%20tailored%0Aevaluation%20protocol%2C%20developed%20to%20investigate%20and%20harness%20the%20region-aware%0Acapabilities%20from%20DAM%20for%20the%20text-rich%20VQA%20problem%20that%20requires%20reasoning%0Aover%20text-based%20information%20within%20images.%20DAM-QA%20incorporates%20a%20mechanism%20that%0Aaggregates%20answers%20from%20multiple%20regional%20views%20of%20image%20content%2C%20enabling%20more%0Aeffective%20identification%20of%20evidence%20that%20may%20be%20tied%20to%20text-related%20elements.%0AExperiments%20on%20six%20VQA%20benchmarks%20show%20that%20our%20approach%20consistently%0Aoutperforms%20the%20baseline%20DAM%2C%20with%20a%20notable%207%2B%20point%20gain%20on%20DocVQA.%20DAM-QA%0Aalso%20achieves%20the%20best%20overall%20performance%20among%20region-aware%20models%20with%20fewer%0Aparameters%2C%20significantly%20narrowing%20the%20gap%20with%20strong%20generalist%20VLMs.%20These%0Aresults%20highlight%20the%20potential%20of%20DAM-like%20models%20for%20text-rich%20and%20broader%0AVQA%20tasks%20when%20paired%20with%20efficient%20usage%20and%20integration%20strategies.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/Linvyl/DAM-QA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12441v1&entry.124074799=Read"},
{"title": "DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth\n  Observation Applications", "author": "Ibrahim Fayad and Max Zimmer and Martin Schwartz and Fabian Gieseke and Philippe Ciais and Gabriel Belouze and Sarah Brood and Aurelien De Truchis and Alexandre d'Aspremont", "abstract": "  Significant efforts have been directed towards adapting self-supervised\nmultimodal learning for Earth observation applications. However, most current\nmethods produce coarse patch-sized embeddings, limiting their effectiveness and\nintegration with other modalities like LiDAR. To close this gap, we present\nDUNIA, an approach to learn pixel-sized embeddings through cross-modal\nalignment between images and full-waveform LiDAR data. As the model is trained\nin a contrastive manner, the embeddings can be directly leveraged in the\ncontext of a variety of environmental monitoring tasks in a zero-shot setting.\nIn our experiments, we demonstrate the effectiveness of the embeddings for\nseven such tasks: canopy height mapping, fractional canopy cover, land cover\nmapping, tree species identification, plant area index, crop type\nclassification, and per-pixel waveform-based vertical structure mapping. The\nresults show that the embeddings, along with zero-shot classifiers, often\noutperform specialized supervised models, even in low-data regimes. In the\nfine-tuning setting, we show strong performances near or better than the\nstate-of-the-art on five out of six tasks.\n", "link": "http://arxiv.org/abs/2502.17066v2", "date": "2025-07-16", "relevancy": 2.8119, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications&body=Title%3A%20DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications%0AAuthor%3A%20Ibrahim%20Fayad%20and%20Max%20Zimmer%20and%20Martin%20Schwartz%20and%20Fabian%20Gieseke%20and%20Philippe%20Ciais%20and%20Gabriel%20Belouze%20and%20Sarah%20Brood%20and%20Aurelien%20De%20Truchis%20and%20Alexandre%20d%27Aspremont%0AAbstract%3A%20%20%20Significant%20efforts%20have%20been%20directed%20towards%20adapting%20self-supervised%0Amultimodal%20learning%20for%20Earth%20observation%20applications.%20However%2C%20most%20current%0Amethods%20produce%20coarse%20patch-sized%20embeddings%2C%20limiting%20their%20effectiveness%20and%0Aintegration%20with%20other%20modalities%20like%20LiDAR.%20To%20close%20this%20gap%2C%20we%20present%0ADUNIA%2C%20an%20approach%20to%20learn%20pixel-sized%20embeddings%20through%20cross-modal%0Aalignment%20between%20images%20and%20full-waveform%20LiDAR%20data.%20As%20the%20model%20is%20trained%0Ain%20a%20contrastive%20manner%2C%20the%20embeddings%20can%20be%20directly%20leveraged%20in%20the%0Acontext%20of%20a%20variety%20of%20environmental%20monitoring%20tasks%20in%20a%20zero-shot%20setting.%0AIn%20our%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20embeddings%20for%0Aseven%20such%20tasks%3A%20canopy%20height%20mapping%2C%20fractional%20canopy%20cover%2C%20land%20cover%0Amapping%2C%20tree%20species%20identification%2C%20plant%20area%20index%2C%20crop%20type%0Aclassification%2C%20and%20per-pixel%20waveform-based%20vertical%20structure%20mapping.%20The%0Aresults%20show%20that%20the%20embeddings%2C%20along%20with%20zero-shot%20classifiers%2C%20often%0Aoutperform%20specialized%20supervised%20models%2C%20even%20in%20low-data%20regimes.%20In%20the%0Afine-tuning%20setting%2C%20we%20show%20strong%20performances%20near%20or%20better%20than%20the%0Astate-of-the-art%20on%20five%20out%20of%20six%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUNIA%253A%2520Pixel-Sized%2520Embeddings%2520via%2520Cross-Modal%2520Alignment%2520for%2520Earth%250A%2520%2520Observation%2520Applications%26entry.906535625%3DIbrahim%2520Fayad%2520and%2520Max%2520Zimmer%2520and%2520Martin%2520Schwartz%2520and%2520Fabian%2520Gieseke%2520and%2520Philippe%2520Ciais%2520and%2520Gabriel%2520Belouze%2520and%2520Sarah%2520Brood%2520and%2520Aurelien%2520De%2520Truchis%2520and%2520Alexandre%2520d%2527Aspremont%26entry.1292438233%3D%2520%2520Significant%2520efforts%2520have%2520been%2520directed%2520towards%2520adapting%2520self-supervised%250Amultimodal%2520learning%2520for%2520Earth%2520observation%2520applications.%2520However%252C%2520most%2520current%250Amethods%2520produce%2520coarse%2520patch-sized%2520embeddings%252C%2520limiting%2520their%2520effectiveness%2520and%250Aintegration%2520with%2520other%2520modalities%2520like%2520LiDAR.%2520To%2520close%2520this%2520gap%252C%2520we%2520present%250ADUNIA%252C%2520an%2520approach%2520to%2520learn%2520pixel-sized%2520embeddings%2520through%2520cross-modal%250Aalignment%2520between%2520images%2520and%2520full-waveform%2520LiDAR%2520data.%2520As%2520the%2520model%2520is%2520trained%250Ain%2520a%2520contrastive%2520manner%252C%2520the%2520embeddings%2520can%2520be%2520directly%2520leveraged%2520in%2520the%250Acontext%2520of%2520a%2520variety%2520of%2520environmental%2520monitoring%2520tasks%2520in%2520a%2520zero-shot%2520setting.%250AIn%2520our%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520embeddings%2520for%250Aseven%2520such%2520tasks%253A%2520canopy%2520height%2520mapping%252C%2520fractional%2520canopy%2520cover%252C%2520land%2520cover%250Amapping%252C%2520tree%2520species%2520identification%252C%2520plant%2520area%2520index%252C%2520crop%2520type%250Aclassification%252C%2520and%2520per-pixel%2520waveform-based%2520vertical%2520structure%2520mapping.%2520The%250Aresults%2520show%2520that%2520the%2520embeddings%252C%2520along%2520with%2520zero-shot%2520classifiers%252C%2520often%250Aoutperform%2520specialized%2520supervised%2520models%252C%2520even%2520in%2520low-data%2520regimes.%2520In%2520the%250Afine-tuning%2520setting%252C%2520we%2520show%2520strong%2520performances%2520near%2520or%2520better%2520than%2520the%250Astate-of-the-art%2520on%2520five%2520out%2520of%2520six%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications&entry.906535625=Ibrahim%20Fayad%20and%20Max%20Zimmer%20and%20Martin%20Schwartz%20and%20Fabian%20Gieseke%20and%20Philippe%20Ciais%20and%20Gabriel%20Belouze%20and%20Sarah%20Brood%20and%20Aurelien%20De%20Truchis%20and%20Alexandre%20d%27Aspremont&entry.1292438233=%20%20Significant%20efforts%20have%20been%20directed%20towards%20adapting%20self-supervised%0Amultimodal%20learning%20for%20Earth%20observation%20applications.%20However%2C%20most%20current%0Amethods%20produce%20coarse%20patch-sized%20embeddings%2C%20limiting%20their%20effectiveness%20and%0Aintegration%20with%20other%20modalities%20like%20LiDAR.%20To%20close%20this%20gap%2C%20we%20present%0ADUNIA%2C%20an%20approach%20to%20learn%20pixel-sized%20embeddings%20through%20cross-modal%0Aalignment%20between%20images%20and%20full-waveform%20LiDAR%20data.%20As%20the%20model%20is%20trained%0Ain%20a%20contrastive%20manner%2C%20the%20embeddings%20can%20be%20directly%20leveraged%20in%20the%0Acontext%20of%20a%20variety%20of%20environmental%20monitoring%20tasks%20in%20a%20zero-shot%20setting.%0AIn%20our%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20embeddings%20for%0Aseven%20such%20tasks%3A%20canopy%20height%20mapping%2C%20fractional%20canopy%20cover%2C%20land%20cover%0Amapping%2C%20tree%20species%20identification%2C%20plant%20area%20index%2C%20crop%20type%0Aclassification%2C%20and%20per-pixel%20waveform-based%20vertical%20structure%20mapping.%20The%0Aresults%20show%20that%20the%20embeddings%2C%20along%20with%20zero-shot%20classifiers%2C%20often%0Aoutperform%20specialized%20supervised%20models%2C%20even%20in%20low-data%20regimes.%20In%20the%0Afine-tuning%20setting%2C%20we%20show%20strong%20performances%20near%20or%20better%20than%20the%0Astate-of-the-art%20on%20five%20out%20of%20six%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17066v2&entry.124074799=Read"},
{"title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic\n  Surveillance Environments", "author": "Hayat Ullah and Abbas Khan and Arslan Munir and Hari Kalva", "abstract": "  Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.\n", "link": "http://arxiv.org/abs/2507.12396v1", "date": "2025-07-16", "relevancy": 2.8048, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments&body=Title%3A%20OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments%0AAuthor%3A%20Hayat%20Ullah%20and%20Abbas%20Khan%20and%20Arslan%20Munir%20and%20Hari%20Kalva%0AAbstract%3A%20%20%20Realistic%20human%20surveillance%20datasets%20are%20crucial%20for%20training%20and%20evaluating%0Acomputer%20vision%20models%20under%20real-world%20conditions%2C%20facilitating%20the%0Adevelopment%20of%20robust%20algorithms%20for%20human%20and%20human-interacting%20object%0Adetection%20in%20complex%20environments.%20These%20datasets%20need%20to%20offer%20diverse%20and%0Achallenging%20data%20to%20enable%20a%20comprehensive%20assessment%20of%20model%20performance%20and%0Athe%20creation%20of%20more%20reliable%20surveillance%20systems%20for%20public%20safety.%20To%20this%0Aend%2C%20we%20present%20two%20visual%20object%20detection%20benchmarks%20named%20OD-VIRAT%20Large%20and%0AOD-VIRAT%20Tiny%2C%20aiming%20at%20advancing%20visual%20understanding%20tasks%20in%20surveillance%0Aimagery.%20The%20video%20sequences%20in%20both%20benchmarks%20cover%2010%20different%20scenes%20of%0Ahuman%20surveillance%20recorded%20from%20significant%20height%20and%20distance.%20The%20proposed%0Abenchmarks%20offer%20rich%20annotations%20of%20bounding%20boxes%20and%20categories%2C%20where%0AOD-VIRAT%20Large%20has%208.7%20million%20annotated%20instances%20in%20599%2C996%20images%20and%0AOD-VIRAT%20Tiny%20has%20288%2C901%20annotated%20instances%20in%2019%2C860%20images.%20This%20work%20also%0Afocuses%20on%20benchmarking%20state-of-the-art%20object%20detection%20architectures%2C%0Aincluding%20RETMDET%2C%20YOLOX%2C%20RetinaNet%2C%20DETR%2C%20and%20Deformable-DETR%20on%20this%20object%0Adetection-specific%20variant%20of%20VIRAT%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Ais%20the%20first%20work%20to%20examine%20the%20performance%20of%20these%20recently%20published%0Astate-of-the-art%20object%20detection%20architectures%20on%20realistic%20surveillance%0Aimagery%20under%20challenging%20conditions%20such%20as%20complex%20backgrounds%2C%20occluded%0Aobjects%2C%20and%20small-scale%20objects.%20The%20proposed%20benchmarking%20and%20experimental%0Asettings%20will%20help%20in%20providing%20insights%20concerning%20the%20performance%20of%20selected%0Aobject%20detection%20models%20and%20set%20the%20base%20for%20developing%20more%20efficient%20and%0Arobust%20object%20detection%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOD-VIRAT%253A%2520A%2520Large-Scale%2520Benchmark%2520for%2520Object%2520Detection%2520in%2520Realistic%250A%2520%2520Surveillance%2520Environments%26entry.906535625%3DHayat%2520Ullah%2520and%2520Abbas%2520Khan%2520and%2520Arslan%2520Munir%2520and%2520Hari%2520Kalva%26entry.1292438233%3D%2520%2520Realistic%2520human%2520surveillance%2520datasets%2520are%2520crucial%2520for%2520training%2520and%2520evaluating%250Acomputer%2520vision%2520models%2520under%2520real-world%2520conditions%252C%2520facilitating%2520the%250Adevelopment%2520of%2520robust%2520algorithms%2520for%2520human%2520and%2520human-interacting%2520object%250Adetection%2520in%2520complex%2520environments.%2520These%2520datasets%2520need%2520to%2520offer%2520diverse%2520and%250Achallenging%2520data%2520to%2520enable%2520a%2520comprehensive%2520assessment%2520of%2520model%2520performance%2520and%250Athe%2520creation%2520of%2520more%2520reliable%2520surveillance%2520systems%2520for%2520public%2520safety.%2520To%2520this%250Aend%252C%2520we%2520present%2520two%2520visual%2520object%2520detection%2520benchmarks%2520named%2520OD-VIRAT%2520Large%2520and%250AOD-VIRAT%2520Tiny%252C%2520aiming%2520at%2520advancing%2520visual%2520understanding%2520tasks%2520in%2520surveillance%250Aimagery.%2520The%2520video%2520sequences%2520in%2520both%2520benchmarks%2520cover%252010%2520different%2520scenes%2520of%250Ahuman%2520surveillance%2520recorded%2520from%2520significant%2520height%2520and%2520distance.%2520The%2520proposed%250Abenchmarks%2520offer%2520rich%2520annotations%2520of%2520bounding%2520boxes%2520and%2520categories%252C%2520where%250AOD-VIRAT%2520Large%2520has%25208.7%2520million%2520annotated%2520instances%2520in%2520599%252C996%2520images%2520and%250AOD-VIRAT%2520Tiny%2520has%2520288%252C901%2520annotated%2520instances%2520in%252019%252C860%2520images.%2520This%2520work%2520also%250Afocuses%2520on%2520benchmarking%2520state-of-the-art%2520object%2520detection%2520architectures%252C%250Aincluding%2520RETMDET%252C%2520YOLOX%252C%2520RetinaNet%252C%2520DETR%252C%2520and%2520Deformable-DETR%2520on%2520this%2520object%250Adetection-specific%2520variant%2520of%2520VIRAT%2520dataset.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520it%250Ais%2520the%2520first%2520work%2520to%2520examine%2520the%2520performance%2520of%2520these%2520recently%2520published%250Astate-of-the-art%2520object%2520detection%2520architectures%2520on%2520realistic%2520surveillance%250Aimagery%2520under%2520challenging%2520conditions%2520such%2520as%2520complex%2520backgrounds%252C%2520occluded%250Aobjects%252C%2520and%2520small-scale%2520objects.%2520The%2520proposed%2520benchmarking%2520and%2520experimental%250Asettings%2520will%2520help%2520in%2520providing%2520insights%2520concerning%2520the%2520performance%2520of%2520selected%250Aobject%2520detection%2520models%2520and%2520set%2520the%2520base%2520for%2520developing%2520more%2520efficient%2520and%250Arobust%2520object%2520detection%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments&entry.906535625=Hayat%20Ullah%20and%20Abbas%20Khan%20and%20Arslan%20Munir%20and%20Hari%20Kalva&entry.1292438233=%20%20Realistic%20human%20surveillance%20datasets%20are%20crucial%20for%20training%20and%20evaluating%0Acomputer%20vision%20models%20under%20real-world%20conditions%2C%20facilitating%20the%0Adevelopment%20of%20robust%20algorithms%20for%20human%20and%20human-interacting%20object%0Adetection%20in%20complex%20environments.%20These%20datasets%20need%20to%20offer%20diverse%20and%0Achallenging%20data%20to%20enable%20a%20comprehensive%20assessment%20of%20model%20performance%20and%0Athe%20creation%20of%20more%20reliable%20surveillance%20systems%20for%20public%20safety.%20To%20this%0Aend%2C%20we%20present%20two%20visual%20object%20detection%20benchmarks%20named%20OD-VIRAT%20Large%20and%0AOD-VIRAT%20Tiny%2C%20aiming%20at%20advancing%20visual%20understanding%20tasks%20in%20surveillance%0Aimagery.%20The%20video%20sequences%20in%20both%20benchmarks%20cover%2010%20different%20scenes%20of%0Ahuman%20surveillance%20recorded%20from%20significant%20height%20and%20distance.%20The%20proposed%0Abenchmarks%20offer%20rich%20annotations%20of%20bounding%20boxes%20and%20categories%2C%20where%0AOD-VIRAT%20Large%20has%208.7%20million%20annotated%20instances%20in%20599%2C996%20images%20and%0AOD-VIRAT%20Tiny%20has%20288%2C901%20annotated%20instances%20in%2019%2C860%20images.%20This%20work%20also%0Afocuses%20on%20benchmarking%20state-of-the-art%20object%20detection%20architectures%2C%0Aincluding%20RETMDET%2C%20YOLOX%2C%20RetinaNet%2C%20DETR%2C%20and%20Deformable-DETR%20on%20this%20object%0Adetection-specific%20variant%20of%20VIRAT%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Ais%20the%20first%20work%20to%20examine%20the%20performance%20of%20these%20recently%20published%0Astate-of-the-art%20object%20detection%20architectures%20on%20realistic%20surveillance%0Aimagery%20under%20challenging%20conditions%20such%20as%20complex%20backgrounds%2C%20occluded%0Aobjects%2C%20and%20small-scale%20objects.%20The%20proposed%20benchmarking%20and%20experimental%0Asettings%20will%20help%20in%20providing%20insights%20concerning%20the%20performance%20of%20selected%0Aobject%20detection%20models%20and%20set%20the%20base%20for%20developing%20more%20efficient%20and%0Arobust%20object%20detection%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12396v1&entry.124074799=Read"},
{"title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "author": "Milica \u0160kipina and Nikola Jovi\u0161i\u0107 and Nicola Dall'Asen and Vanja \u0160venda and Anil Osman Tur and Slobodan Ili\u0107 and Elisa Ricci and Dubravko \u0106ulibrk", "abstract": "  Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final model,\nsignificantly aiding the noise removal process. This design enables MAMBO to\ngenerate highly realistic mammograms of up to 3840x3840 pixels. Importantly,\nthis approach can be used to enhance the training of classification models and\nextended to anomaly segmentation. Experiments, both numerical and radiologist\nvalidation, assess MAMBO's capabilities in image generation, super-resolution,\nand anomaly segmentation, highlighting its potential to enhance mammography\nanalysis for more accurate diagnoses and earlier lesion detection. The source\ncode used in this study is publicly available at:\nhttps://github.com/iai-rs/mambo.\n", "link": "http://arxiv.org/abs/2506.08677v2", "date": "2025-07-16", "relevancy": 2.7644, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5557}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAMBO%3A%20High-Resolution%20Generative%20Approach%20for%20Mammography%20Images&body=Title%3A%20MAMBO%3A%20High-Resolution%20Generative%20Approach%20for%20Mammography%20Images%0AAuthor%3A%20Milica%20%C5%A0kipina%20and%20Nikola%20Jovi%C5%A1i%C4%87%20and%20Nicola%20Dall%27Asen%20and%20Vanja%20%C5%A0venda%20and%20Anil%20Osman%20Tur%20and%20Slobodan%20Ili%C4%87%20and%20Elisa%20Ricci%20and%20Dubravko%20%C4%86ulibrk%0AAbstract%3A%20%20%20Mammography%20is%20the%20gold%20standard%20for%20the%20detection%20and%20diagnosis%20of%20breast%0Acancer.%20This%20procedure%20can%20be%20significantly%20enhanced%20with%20Artificial%0AIntelligence%20%28AI%29-based%20software%2C%20which%20assists%20radiologists%20in%20identifying%0Aabnormalities.%20However%2C%20training%20AI%20systems%20requires%20large%20and%20diverse%0Adatasets%2C%20which%20are%20often%20difficult%20to%20obtain%20due%20to%20privacy%20and%20ethical%0Aconstraints.%20To%20address%20this%20issue%2C%20the%20paper%20introduces%20MAMmography%20ensemBle%0AmOdel%20%28MAMBO%29%2C%20a%20novel%20patch-based%20diffusion%20approach%20designed%20to%20generate%0Afull-resolution%20mammograms.%20Diffusion%20models%20have%20shown%20breakthrough%20results%20in%0Arealistic%20image%20generation%2C%20yet%20few%20studies%20have%20focused%20on%20mammograms%2C%20and%0Anone%20have%20successfully%20generated%20high-resolution%20outputs%20required%20to%20capture%0Afine-grained%20features%20of%20small%20lesions.%20To%20achieve%20this%2C%20MAMBO%20integrates%0Aseparate%20diffusion%20models%20to%20capture%20both%20local%20and%20global%20%28image-level%29%0Acontexts.%20The%20contextual%20information%20is%20then%20fed%20into%20the%20final%20model%2C%0Asignificantly%20aiding%20the%20noise%20removal%20process.%20This%20design%20enables%20MAMBO%20to%0Agenerate%20highly%20realistic%20mammograms%20of%20up%20to%203840x3840%20pixels.%20Importantly%2C%0Athis%20approach%20can%20be%20used%20to%20enhance%20the%20training%20of%20classification%20models%20and%0Aextended%20to%20anomaly%20segmentation.%20Experiments%2C%20both%20numerical%20and%20radiologist%0Avalidation%2C%20assess%20MAMBO%27s%20capabilities%20in%20image%20generation%2C%20super-resolution%2C%0Aand%20anomaly%20segmentation%2C%20highlighting%20its%20potential%20to%20enhance%20mammography%0Aanalysis%20for%20more%20accurate%20diagnoses%20and%20earlier%20lesion%20detection.%20The%20source%0Acode%20used%20in%20this%20study%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/iai-rs/mambo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAMBO%253A%2520High-Resolution%2520Generative%2520Approach%2520for%2520Mammography%2520Images%26entry.906535625%3DMilica%2520%25C5%25A0kipina%2520and%2520Nikola%2520Jovi%25C5%25A1i%25C4%2587%2520and%2520Nicola%2520Dall%2527Asen%2520and%2520Vanja%2520%25C5%25A0venda%2520and%2520Anil%2520Osman%2520Tur%2520and%2520Slobodan%2520Ili%25C4%2587%2520and%2520Elisa%2520Ricci%2520and%2520Dubravko%2520%25C4%2586ulibrk%26entry.1292438233%3D%2520%2520Mammography%2520is%2520the%2520gold%2520standard%2520for%2520the%2520detection%2520and%2520diagnosis%2520of%2520breast%250Acancer.%2520This%2520procedure%2520can%2520be%2520significantly%2520enhanced%2520with%2520Artificial%250AIntelligence%2520%2528AI%2529-based%2520software%252C%2520which%2520assists%2520radiologists%2520in%2520identifying%250Aabnormalities.%2520However%252C%2520training%2520AI%2520systems%2520requires%2520large%2520and%2520diverse%250Adatasets%252C%2520which%2520are%2520often%2520difficult%2520to%2520obtain%2520due%2520to%2520privacy%2520and%2520ethical%250Aconstraints.%2520To%2520address%2520this%2520issue%252C%2520the%2520paper%2520introduces%2520MAMmography%2520ensemBle%250AmOdel%2520%2528MAMBO%2529%252C%2520a%2520novel%2520patch-based%2520diffusion%2520approach%2520designed%2520to%2520generate%250Afull-resolution%2520mammograms.%2520Diffusion%2520models%2520have%2520shown%2520breakthrough%2520results%2520in%250Arealistic%2520image%2520generation%252C%2520yet%2520few%2520studies%2520have%2520focused%2520on%2520mammograms%252C%2520and%250Anone%2520have%2520successfully%2520generated%2520high-resolution%2520outputs%2520required%2520to%2520capture%250Afine-grained%2520features%2520of%2520small%2520lesions.%2520To%2520achieve%2520this%252C%2520MAMBO%2520integrates%250Aseparate%2520diffusion%2520models%2520to%2520capture%2520both%2520local%2520and%2520global%2520%2528image-level%2529%250Acontexts.%2520The%2520contextual%2520information%2520is%2520then%2520fed%2520into%2520the%2520final%2520model%252C%250Asignificantly%2520aiding%2520the%2520noise%2520removal%2520process.%2520This%2520design%2520enables%2520MAMBO%2520to%250Agenerate%2520highly%2520realistic%2520mammograms%2520of%2520up%2520to%25203840x3840%2520pixels.%2520Importantly%252C%250Athis%2520approach%2520can%2520be%2520used%2520to%2520enhance%2520the%2520training%2520of%2520classification%2520models%2520and%250Aextended%2520to%2520anomaly%2520segmentation.%2520Experiments%252C%2520both%2520numerical%2520and%2520radiologist%250Avalidation%252C%2520assess%2520MAMBO%2527s%2520capabilities%2520in%2520image%2520generation%252C%2520super-resolution%252C%250Aand%2520anomaly%2520segmentation%252C%2520highlighting%2520its%2520potential%2520to%2520enhance%2520mammography%250Aanalysis%2520for%2520more%2520accurate%2520diagnoses%2520and%2520earlier%2520lesion%2520detection.%2520The%2520source%250Acode%2520used%2520in%2520this%2520study%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/iai-rs/mambo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAMBO%3A%20High-Resolution%20Generative%20Approach%20for%20Mammography%20Images&entry.906535625=Milica%20%C5%A0kipina%20and%20Nikola%20Jovi%C5%A1i%C4%87%20and%20Nicola%20Dall%27Asen%20and%20Vanja%20%C5%A0venda%20and%20Anil%20Osman%20Tur%20and%20Slobodan%20Ili%C4%87%20and%20Elisa%20Ricci%20and%20Dubravko%20%C4%86ulibrk&entry.1292438233=%20%20Mammography%20is%20the%20gold%20standard%20for%20the%20detection%20and%20diagnosis%20of%20breast%0Acancer.%20This%20procedure%20can%20be%20significantly%20enhanced%20with%20Artificial%0AIntelligence%20%28AI%29-based%20software%2C%20which%20assists%20radiologists%20in%20identifying%0Aabnormalities.%20However%2C%20training%20AI%20systems%20requires%20large%20and%20diverse%0Adatasets%2C%20which%20are%20often%20difficult%20to%20obtain%20due%20to%20privacy%20and%20ethical%0Aconstraints.%20To%20address%20this%20issue%2C%20the%20paper%20introduces%20MAMmography%20ensemBle%0AmOdel%20%28MAMBO%29%2C%20a%20novel%20patch-based%20diffusion%20approach%20designed%20to%20generate%0Afull-resolution%20mammograms.%20Diffusion%20models%20have%20shown%20breakthrough%20results%20in%0Arealistic%20image%20generation%2C%20yet%20few%20studies%20have%20focused%20on%20mammograms%2C%20and%0Anone%20have%20successfully%20generated%20high-resolution%20outputs%20required%20to%20capture%0Afine-grained%20features%20of%20small%20lesions.%20To%20achieve%20this%2C%20MAMBO%20integrates%0Aseparate%20diffusion%20models%20to%20capture%20both%20local%20and%20global%20%28image-level%29%0Acontexts.%20The%20contextual%20information%20is%20then%20fed%20into%20the%20final%20model%2C%0Asignificantly%20aiding%20the%20noise%20removal%20process.%20This%20design%20enables%20MAMBO%20to%0Agenerate%20highly%20realistic%20mammograms%20of%20up%20to%203840x3840%20pixels.%20Importantly%2C%0Athis%20approach%20can%20be%20used%20to%20enhance%20the%20training%20of%20classification%20models%20and%0Aextended%20to%20anomaly%20segmentation.%20Experiments%2C%20both%20numerical%20and%20radiologist%0Avalidation%2C%20assess%20MAMBO%27s%20capabilities%20in%20image%20generation%2C%20super-resolution%2C%0Aand%20anomaly%20segmentation%2C%20highlighting%20its%20potential%20to%20enhance%20mammography%0Aanalysis%20for%20more%20accurate%20diagnoses%20and%20earlier%20lesion%20detection.%20The%20source%0Acode%20used%20in%20this%20study%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/iai-rs/mambo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08677v2&entry.124074799=Read"},
{"title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "author": "Qiang Li and Qingsen Yan and Haojian Huang and Peng Wu and Haokui Zhang and Yanning Zhang", "abstract": "  With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.\n", "link": "http://arxiv.org/abs/2507.10432v3", "date": "2025-07-16", "relevancy": 2.7174, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5562}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5425}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment&body=Title%3A%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0AAuthor%3A%20Qiang%20Li%20and%20Qingsen%20Yan%20and%20Haojian%20Huang%20and%20Peng%20Wu%20and%20Haokui%20Zhang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20Artificial%20Intelligence%20Generated%20Image%20%28AGI%29%0Atechnology%2C%20the%20accurate%20assessment%20of%20their%20quality%20has%20become%20an%20increasingly%0Avital%20requirement.%20Prevailing%20methods%20typically%20rely%20on%20cross-modal%20models%20like%0ACLIP%20or%20BLIP%20to%20evaluate%20text-image%20alignment%20and%20visual%20quality.%20However%2C%20when%0Aapplied%20to%20AGIs%2C%20these%20methods%20encounter%20two%20primary%20challenges%3A%20semantic%0Amisalignment%20and%20details%20perception%20missing.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0A%28SC-AGIQA%29%2C%20a%20unified%20framework%20that%20leverages%20text-visual%20semantic%20constraints%0Ato%20significantly%20enhance%20the%20comprehensive%20evaluation%20of%20both%20text-image%0Aconsistency%20and%20perceptual%20distortion%20in%20AI-generated%20images.%20Our%20approach%0Aintegrates%20key%20capabilities%20from%20multiple%20models%20and%20tackles%20the%20aforementioned%0Achallenges%20by%20introducing%20two%20core%20modules%3A%20the%20Text-assisted%20Semantic%0AAlignment%20Module%20%28TSAM%29%2C%20which%20leverages%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20bridge%20the%20semantic%20gap%20by%20generating%20an%20image%20description%20and%0Acomparing%20it%20against%20the%20original%20prompt%20for%20a%20refined%20consistency%20check%2C%20and%0Athe%20Frequency-domain%20Fine-Grained%20Degradation%20Perception%20Module%20%28FFDPM%29%2C%20which%0Adraws%20inspiration%20from%20Human%20Visual%20System%20%28HVS%29%20properties%20by%20employing%0Afrequency%20domain%20analysis%20combined%20with%20perceptual%20sensitivity%20weighting%20to%0Abetter%20quantify%20subtle%20visual%20distortions%20and%20enhance%20the%20capture%20of%0Afine-grained%20visual%20quality%20details%20in%20images.%20Extensive%20experiments%20conducted%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20SC-AGIQA%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mozhu1/SC-AGIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Visual%2520Semantic%2520Constrained%2520AI-Generated%2520Image%2520Quality%2520Assessment%26entry.906535625%3DQiang%2520Li%2520and%2520Qingsen%2520Yan%2520and%2520Haojian%2520Huang%2520and%2520Peng%2520Wu%2520and%2520Haokui%2520Zhang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520in%2520Artificial%2520Intelligence%2520Generated%2520Image%2520%2528AGI%2529%250Atechnology%252C%2520the%2520accurate%2520assessment%2520of%2520their%2520quality%2520has%2520become%2520an%2520increasingly%250Avital%2520requirement.%2520Prevailing%2520methods%2520typically%2520rely%2520on%2520cross-modal%2520models%2520like%250ACLIP%2520or%2520BLIP%2520to%2520evaluate%2520text-image%2520alignment%2520and%2520visual%2520quality.%2520However%252C%2520when%250Aapplied%2520to%2520AGIs%252C%2520these%2520methods%2520encounter%2520two%2520primary%2520challenges%253A%2520semantic%250Amisalignment%2520and%2520details%2520perception%2520missing.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Text-Visual%2520Semantic%2520Constrained%2520AI-Generated%2520Image%2520Quality%2520Assessment%250A%2528SC-AGIQA%2529%252C%2520a%2520unified%2520framework%2520that%2520leverages%2520text-visual%2520semantic%2520constraints%250Ato%2520significantly%2520enhance%2520the%2520comprehensive%2520evaluation%2520of%2520both%2520text-image%250Aconsistency%2520and%2520perceptual%2520distortion%2520in%2520AI-generated%2520images.%2520Our%2520approach%250Aintegrates%2520key%2520capabilities%2520from%2520multiple%2520models%2520and%2520tackles%2520the%2520aforementioned%250Achallenges%2520by%2520introducing%2520two%2520core%2520modules%253A%2520the%2520Text-assisted%2520Semantic%250AAlignment%2520Module%2520%2528TSAM%2529%252C%2520which%2520leverages%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520to%2520bridge%2520the%2520semantic%2520gap%2520by%2520generating%2520an%2520image%2520description%2520and%250Acomparing%2520it%2520against%2520the%2520original%2520prompt%2520for%2520a%2520refined%2520consistency%2520check%252C%2520and%250Athe%2520Frequency-domain%2520Fine-Grained%2520Degradation%2520Perception%2520Module%2520%2528FFDPM%2529%252C%2520which%250Adraws%2520inspiration%2520from%2520Human%2520Visual%2520System%2520%2528HVS%2529%2520properties%2520by%2520employing%250Afrequency%2520domain%2520analysis%2520combined%2520with%2520perceptual%2520sensitivity%2520weighting%2520to%250Abetter%2520quantify%2520subtle%2520visual%2520distortions%2520and%2520enhance%2520the%2520capture%2520of%250Afine-grained%2520visual%2520quality%2520details%2520in%2520images.%2520Extensive%2520experiments%2520conducted%250Aon%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%2520SC-AGIQA%2520outperforms%2520existing%250Astate-of-the-art%2520methods.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mozhu1/SC-AGIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment&entry.906535625=Qiang%20Li%20and%20Qingsen%20Yan%20and%20Haojian%20Huang%20and%20Peng%20Wu%20and%20Haokui%20Zhang%20and%20Yanning%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20Artificial%20Intelligence%20Generated%20Image%20%28AGI%29%0Atechnology%2C%20the%20accurate%20assessment%20of%20their%20quality%20has%20become%20an%20increasingly%0Avital%20requirement.%20Prevailing%20methods%20typically%20rely%20on%20cross-modal%20models%20like%0ACLIP%20or%20BLIP%20to%20evaluate%20text-image%20alignment%20and%20visual%20quality.%20However%2C%20when%0Aapplied%20to%20AGIs%2C%20these%20methods%20encounter%20two%20primary%20challenges%3A%20semantic%0Amisalignment%20and%20details%20perception%20missing.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Text-Visual%20Semantic%20Constrained%20AI-Generated%20Image%20Quality%20Assessment%0A%28SC-AGIQA%29%2C%20a%20unified%20framework%20that%20leverages%20text-visual%20semantic%20constraints%0Ato%20significantly%20enhance%20the%20comprehensive%20evaluation%20of%20both%20text-image%0Aconsistency%20and%20perceptual%20distortion%20in%20AI-generated%20images.%20Our%20approach%0Aintegrates%20key%20capabilities%20from%20multiple%20models%20and%20tackles%20the%20aforementioned%0Achallenges%20by%20introducing%20two%20core%20modules%3A%20the%20Text-assisted%20Semantic%0AAlignment%20Module%20%28TSAM%29%2C%20which%20leverages%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20bridge%20the%20semantic%20gap%20by%20generating%20an%20image%20description%20and%0Acomparing%20it%20against%20the%20original%20prompt%20for%20a%20refined%20consistency%20check%2C%20and%0Athe%20Frequency-domain%20Fine-Grained%20Degradation%20Perception%20Module%20%28FFDPM%29%2C%20which%0Adraws%20inspiration%20from%20Human%20Visual%20System%20%28HVS%29%20properties%20by%20employing%0Afrequency%20domain%20analysis%20combined%20with%20perceptual%20sensitivity%20weighting%20to%0Abetter%20quantify%20subtle%20visual%20distortions%20and%20enhance%20the%20capture%20of%0Afine-grained%20visual%20quality%20details%20in%20images.%20Extensive%20experiments%20conducted%0Aon%20multiple%20benchmark%20datasets%20demonstrate%20that%20SC-AGIQA%20outperforms%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/mozhu1/SC-AGIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10432v3&entry.124074799=Read"},
{"title": "Dual Dimensions Geometric Representation Learning Based Document\n  Dewarping", "author": "Heng Li and Qingcai Chen and Xiangping Wu", "abstract": "  Document image dewarping remains a challenging task in the deep learning era.\nWhile existing methods have improved by leveraging text line awareness, they\ntypically focus only on a single horizontal dimension. In this paper, we\npropose a fine-grained deformation perception model that focuses on Dual\nDimensions of document horizontal-vertical-lines to improve document Dewarping\ncalled D2Dewarp. It can perceive distortion trends in different directions\nacross document details. To combine the horizontal and vertical granularity\nfeatures, an effective fusion module based on X and Y coordinate is designed to\nfacilitate interaction and constraint between the two dimensions for feature\ncomplementarity. Due to the lack of annotated line features in current public\ndewarping datasets, we also propose an automatic fine-grained annotation method\nusing public document texture images and an automatic rendering engine to build\na new large-scale distortion training dataset. The code and dataset will be\npublicly released. On public Chinese and English benchmarks, both quantitative\nand qualitative results show that our method achieves better rectification\nresults compared with the state-of-the-art methods. The dataset will be\npublicly available at https://github.com/xiaomore/DocDewarpHV\n", "link": "http://arxiv.org/abs/2507.08492v2", "date": "2025-07-16", "relevancy": 2.7136, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Dimensions%20Geometric%20Representation%20Learning%20Based%20Document%0A%20%20Dewarping&body=Title%3A%20Dual%20Dimensions%20Geometric%20Representation%20Learning%20Based%20Document%0A%20%20Dewarping%0AAuthor%3A%20Heng%20Li%20and%20Qingcai%20Chen%20and%20Xiangping%20Wu%0AAbstract%3A%20%20%20Document%20image%20dewarping%20remains%20a%20challenging%20task%20in%20the%20deep%20learning%20era.%0AWhile%20existing%20methods%20have%20improved%20by%20leveraging%20text%20line%20awareness%2C%20they%0Atypically%20focus%20only%20on%20a%20single%20horizontal%20dimension.%20In%20this%20paper%2C%20we%0Apropose%20a%20fine-grained%20deformation%20perception%20model%20that%20focuses%20on%20Dual%0ADimensions%20of%20document%20horizontal-vertical-lines%20to%20improve%20document%20Dewarping%0Acalled%20D2Dewarp.%20It%20can%20perceive%20distortion%20trends%20in%20different%20directions%0Aacross%20document%20details.%20To%20combine%20the%20horizontal%20and%20vertical%20granularity%0Afeatures%2C%20an%20effective%20fusion%20module%20based%20on%20X%20and%20Y%20coordinate%20is%20designed%20to%0Afacilitate%20interaction%20and%20constraint%20between%20the%20two%20dimensions%20for%20feature%0Acomplementarity.%20Due%20to%20the%20lack%20of%20annotated%20line%20features%20in%20current%20public%0Adewarping%20datasets%2C%20we%20also%20propose%20an%20automatic%20fine-grained%20annotation%20method%0Ausing%20public%20document%20texture%20images%20and%20an%20automatic%20rendering%20engine%20to%20build%0Aa%20new%20large-scale%20distortion%20training%20dataset.%20The%20code%20and%20dataset%20will%20be%0Apublicly%20released.%20On%20public%20Chinese%20and%20English%20benchmarks%2C%20both%20quantitative%0Aand%20qualitative%20results%20show%20that%20our%20method%20achieves%20better%20rectification%0Aresults%20compared%20with%20the%20state-of-the-art%20methods.%20The%20dataset%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/xiaomore/DocDewarpHV%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Dimensions%2520Geometric%2520Representation%2520Learning%2520Based%2520Document%250A%2520%2520Dewarping%26entry.906535625%3DHeng%2520Li%2520and%2520Qingcai%2520Chen%2520and%2520Xiangping%2520Wu%26entry.1292438233%3D%2520%2520Document%2520image%2520dewarping%2520remains%2520a%2520challenging%2520task%2520in%2520the%2520deep%2520learning%2520era.%250AWhile%2520existing%2520methods%2520have%2520improved%2520by%2520leveraging%2520text%2520line%2520awareness%252C%2520they%250Atypically%2520focus%2520only%2520on%2520a%2520single%2520horizontal%2520dimension.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520fine-grained%2520deformation%2520perception%2520model%2520that%2520focuses%2520on%2520Dual%250ADimensions%2520of%2520document%2520horizontal-vertical-lines%2520to%2520improve%2520document%2520Dewarping%250Acalled%2520D2Dewarp.%2520It%2520can%2520perceive%2520distortion%2520trends%2520in%2520different%2520directions%250Aacross%2520document%2520details.%2520To%2520combine%2520the%2520horizontal%2520and%2520vertical%2520granularity%250Afeatures%252C%2520an%2520effective%2520fusion%2520module%2520based%2520on%2520X%2520and%2520Y%2520coordinate%2520is%2520designed%2520to%250Afacilitate%2520interaction%2520and%2520constraint%2520between%2520the%2520two%2520dimensions%2520for%2520feature%250Acomplementarity.%2520Due%2520to%2520the%2520lack%2520of%2520annotated%2520line%2520features%2520in%2520current%2520public%250Adewarping%2520datasets%252C%2520we%2520also%2520propose%2520an%2520automatic%2520fine-grained%2520annotation%2520method%250Ausing%2520public%2520document%2520texture%2520images%2520and%2520an%2520automatic%2520rendering%2520engine%2520to%2520build%250Aa%2520new%2520large-scale%2520distortion%2520training%2520dataset.%2520The%2520code%2520and%2520dataset%2520will%2520be%250Apublicly%2520released.%2520On%2520public%2520Chinese%2520and%2520English%2520benchmarks%252C%2520both%2520quantitative%250Aand%2520qualitative%2520results%2520show%2520that%2520our%2520method%2520achieves%2520better%2520rectification%250Aresults%2520compared%2520with%2520the%2520state-of-the-art%2520methods.%2520The%2520dataset%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/xiaomore/DocDewarpHV%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Dimensions%20Geometric%20Representation%20Learning%20Based%20Document%0A%20%20Dewarping&entry.906535625=Heng%20Li%20and%20Qingcai%20Chen%20and%20Xiangping%20Wu&entry.1292438233=%20%20Document%20image%20dewarping%20remains%20a%20challenging%20task%20in%20the%20deep%20learning%20era.%0AWhile%20existing%20methods%20have%20improved%20by%20leveraging%20text%20line%20awareness%2C%20they%0Atypically%20focus%20only%20on%20a%20single%20horizontal%20dimension.%20In%20this%20paper%2C%20we%0Apropose%20a%20fine-grained%20deformation%20perception%20model%20that%20focuses%20on%20Dual%0ADimensions%20of%20document%20horizontal-vertical-lines%20to%20improve%20document%20Dewarping%0Acalled%20D2Dewarp.%20It%20can%20perceive%20distortion%20trends%20in%20different%20directions%0Aacross%20document%20details.%20To%20combine%20the%20horizontal%20and%20vertical%20granularity%0Afeatures%2C%20an%20effective%20fusion%20module%20based%20on%20X%20and%20Y%20coordinate%20is%20designed%20to%0Afacilitate%20interaction%20and%20constraint%20between%20the%20two%20dimensions%20for%20feature%0Acomplementarity.%20Due%20to%20the%20lack%20of%20annotated%20line%20features%20in%20current%20public%0Adewarping%20datasets%2C%20we%20also%20propose%20an%20automatic%20fine-grained%20annotation%20method%0Ausing%20public%20document%20texture%20images%20and%20an%20automatic%20rendering%20engine%20to%20build%0Aa%20new%20large-scale%20distortion%20training%20dataset.%20The%20code%20and%20dataset%20will%20be%0Apublicly%20released.%20On%20public%20Chinese%20and%20English%20benchmarks%2C%20both%20quantitative%0Aand%20qualitative%20results%20show%20that%20our%20method%20achieves%20better%20rectification%0Aresults%20compared%20with%20the%20state-of-the-art%20methods.%20The%20dataset%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/xiaomore/DocDewarpHV%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08492v2&entry.124074799=Read"},
{"title": "Linearly-Interpretable Concept Embedding Models for Text Analysis", "author": "Francesco De Santis and Philippe Bich and Gabriele Ciravegna and Pietro Barbiero and Danilo Giordano and Tania Cerquitelli", "abstract": "  Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.\n", "link": "http://arxiv.org/abs/2406.14335v2", "date": "2025-07-16", "relevancy": 2.7046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linearly-Interpretable%20Concept%20Embedding%20Models%20for%20Text%20Analysis&body=Title%3A%20Linearly-Interpretable%20Concept%20Embedding%20Models%20for%20Text%20Analysis%0AAuthor%3A%20Francesco%20De%20Santis%20and%20Philippe%20Bich%20and%20Gabriele%20Ciravegna%20and%20Pietro%20Barbiero%20and%20Danilo%20Giordano%20and%20Tania%20Cerquitelli%0AAbstract%3A%20%20%20Despite%20their%20success%2C%20Large-Language%20Models%20%28LLMs%29%20still%20face%20criticism%20due%0Ato%20their%20lack%20of%20interpretability.%20Traditional%20post-hoc%20interpretation%20methods%2C%0Abased%20on%20attention%20and%20gradient-based%20analysis%2C%20offer%20limited%20insights%20as%20they%0Aonly%20approximate%20the%20model%27s%20decision-making%20processes%20and%20have%20been%20proved%20to%0Abe%20unreliable.%20For%20this%20reason%2C%20Concept-Bottleneck%20Models%20%28CBMs%29%20have%20been%0Alately%20proposed%20in%20the%20textual%20field%20to%20provide%20interpretable%20predictions%20based%0Aon%20human-understandable%20concepts.%20However%2C%20CBMs%20still%20exhibit%20several%0Alimitations%20due%20to%20their%20architectural%20constraints%20limiting%20their%20expressivity%2C%0Ato%20the%20absence%20of%20task-interpretability%20when%20employing%20non-linear%20task%0Apredictors%20and%20for%20requiring%20extensive%20annotations%20that%20are%20impractical%20for%0Areal-world%20text%20data.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20proposing%20a%0Anovel%20Linearly%20Interpretable%20Concept%20Embedding%20Model%20%28LICEM%29%20going%20beyond%20the%0Acurrent%20accuracy-interpretability%20trade-off.%20LICEMs%20classification%20accuracy%20is%0Abetter%20than%20existing%20interpretable%20models%20and%20matches%20black-box%20ones.%20We%20show%0Athat%20the%20explanations%20provided%20by%20our%20models%20are%20more%20interveneable%20and%0Acausally%20consistent%20with%20respect%20to%20existing%20solutions.%20Finally%2C%20we%20show%20that%0ALICEMs%20can%20be%20trained%20without%20requiring%20any%20concept%20supervision%2C%20as%20concepts%0Acan%20be%20automatically%20predicted%20when%20using%20an%20LLM%20backbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinearly-Interpretable%2520Concept%2520Embedding%2520Models%2520for%2520Text%2520Analysis%26entry.906535625%3DFrancesco%2520De%2520Santis%2520and%2520Philippe%2520Bich%2520and%2520Gabriele%2520Ciravegna%2520and%2520Pietro%2520Barbiero%2520and%2520Danilo%2520Giordano%2520and%2520Tania%2520Cerquitelli%26entry.1292438233%3D%2520%2520Despite%2520their%2520success%252C%2520Large-Language%2520Models%2520%2528LLMs%2529%2520still%2520face%2520criticism%2520due%250Ato%2520their%2520lack%2520of%2520interpretability.%2520Traditional%2520post-hoc%2520interpretation%2520methods%252C%250Abased%2520on%2520attention%2520and%2520gradient-based%2520analysis%252C%2520offer%2520limited%2520insights%2520as%2520they%250Aonly%2520approximate%2520the%2520model%2527s%2520decision-making%2520processes%2520and%2520have%2520been%2520proved%2520to%250Abe%2520unreliable.%2520For%2520this%2520reason%252C%2520Concept-Bottleneck%2520Models%2520%2528CBMs%2529%2520have%2520been%250Alately%2520proposed%2520in%2520the%2520textual%2520field%2520to%2520provide%2520interpretable%2520predictions%2520based%250Aon%2520human-understandable%2520concepts.%2520However%252C%2520CBMs%2520still%2520exhibit%2520several%250Alimitations%2520due%2520to%2520their%2520architectural%2520constraints%2520limiting%2520their%2520expressivity%252C%250Ato%2520the%2520absence%2520of%2520task-interpretability%2520when%2520employing%2520non-linear%2520task%250Apredictors%2520and%2520for%2520requiring%2520extensive%2520annotations%2520that%2520are%2520impractical%2520for%250Areal-world%2520text%2520data.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%2520proposing%2520a%250Anovel%2520Linearly%2520Interpretable%2520Concept%2520Embedding%2520Model%2520%2528LICEM%2529%2520going%2520beyond%2520the%250Acurrent%2520accuracy-interpretability%2520trade-off.%2520LICEMs%2520classification%2520accuracy%2520is%250Abetter%2520than%2520existing%2520interpretable%2520models%2520and%2520matches%2520black-box%2520ones.%2520We%2520show%250Athat%2520the%2520explanations%2520provided%2520by%2520our%2520models%2520are%2520more%2520interveneable%2520and%250Acausally%2520consistent%2520with%2520respect%2520to%2520existing%2520solutions.%2520Finally%252C%2520we%2520show%2520that%250ALICEMs%2520can%2520be%2520trained%2520without%2520requiring%2520any%2520concept%2520supervision%252C%2520as%2520concepts%250Acan%2520be%2520automatically%2520predicted%2520when%2520using%2520an%2520LLM%2520backbone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linearly-Interpretable%20Concept%20Embedding%20Models%20for%20Text%20Analysis&entry.906535625=Francesco%20De%20Santis%20and%20Philippe%20Bich%20and%20Gabriele%20Ciravegna%20and%20Pietro%20Barbiero%20and%20Danilo%20Giordano%20and%20Tania%20Cerquitelli&entry.1292438233=%20%20Despite%20their%20success%2C%20Large-Language%20Models%20%28LLMs%29%20still%20face%20criticism%20due%0Ato%20their%20lack%20of%20interpretability.%20Traditional%20post-hoc%20interpretation%20methods%2C%0Abased%20on%20attention%20and%20gradient-based%20analysis%2C%20offer%20limited%20insights%20as%20they%0Aonly%20approximate%20the%20model%27s%20decision-making%20processes%20and%20have%20been%20proved%20to%0Abe%20unreliable.%20For%20this%20reason%2C%20Concept-Bottleneck%20Models%20%28CBMs%29%20have%20been%0Alately%20proposed%20in%20the%20textual%20field%20to%20provide%20interpretable%20predictions%20based%0Aon%20human-understandable%20concepts.%20However%2C%20CBMs%20still%20exhibit%20several%0Alimitations%20due%20to%20their%20architectural%20constraints%20limiting%20their%20expressivity%2C%0Ato%20the%20absence%20of%20task-interpretability%20when%20employing%20non-linear%20task%0Apredictors%20and%20for%20requiring%20extensive%20annotations%20that%20are%20impractical%20for%0Areal-world%20text%20data.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20proposing%20a%0Anovel%20Linearly%20Interpretable%20Concept%20Embedding%20Model%20%28LICEM%29%20going%20beyond%20the%0Acurrent%20accuracy-interpretability%20trade-off.%20LICEMs%20classification%20accuracy%20is%0Abetter%20than%20existing%20interpretable%20models%20and%20matches%20black-box%20ones.%20We%20show%0Athat%20the%20explanations%20provided%20by%20our%20models%20are%20more%20interveneable%20and%0Acausally%20consistent%20with%20respect%20to%20existing%20solutions.%20Finally%2C%20we%20show%20that%0ALICEMs%20can%20be%20trained%20without%20requiring%20any%20concept%20supervision%2C%20as%20concepts%0Acan%20be%20automatically%20predicted%20when%20using%20an%20LLM%20backbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14335v2&entry.124074799=Read"},
{"title": "The Utility of the Virtual Imaging Trials Methodology for Objective\n  Characterization of AI Systems and Training Data", "author": "Fakrul Islam Tushar and Lavsen Dahal and Saman Sotoudeh-Paima and Ehsan Abadi and W. Paul Segars and Ehsan Samei and Joseph Y. Lo", "abstract": "  Purpose: The credibility of Artificial Intelligence (AI) models for medical\nimaging continues to be a challenge, affected by the diversity of models, the\ndata used to train the models, and applicability of their combination to\nproduce reproducible results for new data. Approach: In this work we aimed to\nexplore if the emerging Virtual Imaging Trials (VIT) methodologies can provide\nan objective resource to approach this challenge. The study was conducted for\nthe case example of COVID-19 diagnosis using clinical and virtual computed\ntomography (CT) and chest radiography (CXR) processed with convolutional neural\nnetworks. Multiple AI models were developed and tested using 3D ResNet-like and\n2D EfficientNetv2 architectures across diverse datasets. Results: The\nperformance differences were evaluated in terms of the area under the curve\n(AUC) and the DeLong method for AUC confidence intervals. The models trained on\nthe most diverse datasets showed the highest external testing performance, with\nAUC values ranging from 0.73-0.76 for CT and 0.70-0.73 for CXR. Internal\ntesting yielded higher AUC values (0.77 -0.85 for CT and 0.77-1.0 for CXR),\nhighlighting a substantial drop in performance during external validation,\nwhich underscores the importance of diverse and comprehensive training and\ntesting data. Most notably, VIT approach provided objective assessment of the\nutility of diverse models and datasets while further providing insight into the\ninfluence of dataset characteristics, patient factors, and imaging physics on\nAI efficacy. Conclusions: The VIT approach can be used to enhance model\ntransparency and reliability, offering nuanced insights into the factors\ndriving AI performance and bridging the gap between experimental and clinical\nsettings.\n", "link": "http://arxiv.org/abs/2308.09730v5", "date": "2025-07-16", "relevancy": 2.6836, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Utility%20of%20the%20Virtual%20Imaging%20Trials%20Methodology%20for%20Objective%0A%20%20Characterization%20of%20AI%20Systems%20and%20Training%20Data&body=Title%3A%20The%20Utility%20of%20the%20Virtual%20Imaging%20Trials%20Methodology%20for%20Objective%0A%20%20Characterization%20of%20AI%20Systems%20and%20Training%20Data%0AAuthor%3A%20Fakrul%20Islam%20Tushar%20and%20Lavsen%20Dahal%20and%20Saman%20Sotoudeh-Paima%20and%20Ehsan%20Abadi%20and%20W.%20Paul%20Segars%20and%20Ehsan%20Samei%20and%20Joseph%20Y.%20Lo%0AAbstract%3A%20%20%20Purpose%3A%20The%20credibility%20of%20Artificial%20Intelligence%20%28AI%29%20models%20for%20medical%0Aimaging%20continues%20to%20be%20a%20challenge%2C%20affected%20by%20the%20diversity%20of%20models%2C%20the%0Adata%20used%20to%20train%20the%20models%2C%20and%20applicability%20of%20their%20combination%20to%0Aproduce%20reproducible%20results%20for%20new%20data.%20Approach%3A%20In%20this%20work%20we%20aimed%20to%0Aexplore%20if%20the%20emerging%20Virtual%20Imaging%20Trials%20%28VIT%29%20methodologies%20can%20provide%0Aan%20objective%20resource%20to%20approach%20this%20challenge.%20The%20study%20was%20conducted%20for%0Athe%20case%20example%20of%20COVID-19%20diagnosis%20using%20clinical%20and%20virtual%20computed%0Atomography%20%28CT%29%20and%20chest%20radiography%20%28CXR%29%20processed%20with%20convolutional%20neural%0Anetworks.%20Multiple%20AI%20models%20were%20developed%20and%20tested%20using%203D%20ResNet-like%20and%0A2D%20EfficientNetv2%20architectures%20across%20diverse%20datasets.%20Results%3A%20The%0Aperformance%20differences%20were%20evaluated%20in%20terms%20of%20the%20area%20under%20the%20curve%0A%28AUC%29%20and%20the%20DeLong%20method%20for%20AUC%20confidence%20intervals.%20The%20models%20trained%20on%0Athe%20most%20diverse%20datasets%20showed%20the%20highest%20external%20testing%20performance%2C%20with%0AAUC%20values%20ranging%20from%200.73-0.76%20for%20CT%20and%200.70-0.73%20for%20CXR.%20Internal%0Atesting%20yielded%20higher%20AUC%20values%20%280.77%20-0.85%20for%20CT%20and%200.77-1.0%20for%20CXR%29%2C%0Ahighlighting%20a%20substantial%20drop%20in%20performance%20during%20external%20validation%2C%0Awhich%20underscores%20the%20importance%20of%20diverse%20and%20comprehensive%20training%20and%0Atesting%20data.%20Most%20notably%2C%20VIT%20approach%20provided%20objective%20assessment%20of%20the%0Autility%20of%20diverse%20models%20and%20datasets%20while%20further%20providing%20insight%20into%20the%0Ainfluence%20of%20dataset%20characteristics%2C%20patient%20factors%2C%20and%20imaging%20physics%20on%0AAI%20efficacy.%20Conclusions%3A%20The%20VIT%20approach%20can%20be%20used%20to%20enhance%20model%0Atransparency%20and%20reliability%2C%20offering%20nuanced%20insights%20into%20the%20factors%0Adriving%20AI%20performance%20and%20bridging%20the%20gap%20between%20experimental%20and%20clinical%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09730v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Utility%2520of%2520the%2520Virtual%2520Imaging%2520Trials%2520Methodology%2520for%2520Objective%250A%2520%2520Characterization%2520of%2520AI%2520Systems%2520and%2520Training%2520Data%26entry.906535625%3DFakrul%2520Islam%2520Tushar%2520and%2520Lavsen%2520Dahal%2520and%2520Saman%2520Sotoudeh-Paima%2520and%2520Ehsan%2520Abadi%2520and%2520W.%2520Paul%2520Segars%2520and%2520Ehsan%2520Samei%2520and%2520Joseph%2520Y.%2520Lo%26entry.1292438233%3D%2520%2520Purpose%253A%2520The%2520credibility%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520models%2520for%2520medical%250Aimaging%2520continues%2520to%2520be%2520a%2520challenge%252C%2520affected%2520by%2520the%2520diversity%2520of%2520models%252C%2520the%250Adata%2520used%2520to%2520train%2520the%2520models%252C%2520and%2520applicability%2520of%2520their%2520combination%2520to%250Aproduce%2520reproducible%2520results%2520for%2520new%2520data.%2520Approach%253A%2520In%2520this%2520work%2520we%2520aimed%2520to%250Aexplore%2520if%2520the%2520emerging%2520Virtual%2520Imaging%2520Trials%2520%2528VIT%2529%2520methodologies%2520can%2520provide%250Aan%2520objective%2520resource%2520to%2520approach%2520this%2520challenge.%2520The%2520study%2520was%2520conducted%2520for%250Athe%2520case%2520example%2520of%2520COVID-19%2520diagnosis%2520using%2520clinical%2520and%2520virtual%2520computed%250Atomography%2520%2528CT%2529%2520and%2520chest%2520radiography%2520%2528CXR%2529%2520processed%2520with%2520convolutional%2520neural%250Anetworks.%2520Multiple%2520AI%2520models%2520were%2520developed%2520and%2520tested%2520using%25203D%2520ResNet-like%2520and%250A2D%2520EfficientNetv2%2520architectures%2520across%2520diverse%2520datasets.%2520Results%253A%2520The%250Aperformance%2520differences%2520were%2520evaluated%2520in%2520terms%2520of%2520the%2520area%2520under%2520the%2520curve%250A%2528AUC%2529%2520and%2520the%2520DeLong%2520method%2520for%2520AUC%2520confidence%2520intervals.%2520The%2520models%2520trained%2520on%250Athe%2520most%2520diverse%2520datasets%2520showed%2520the%2520highest%2520external%2520testing%2520performance%252C%2520with%250AAUC%2520values%2520ranging%2520from%25200.73-0.76%2520for%2520CT%2520and%25200.70-0.73%2520for%2520CXR.%2520Internal%250Atesting%2520yielded%2520higher%2520AUC%2520values%2520%25280.77%2520-0.85%2520for%2520CT%2520and%25200.77-1.0%2520for%2520CXR%2529%252C%250Ahighlighting%2520a%2520substantial%2520drop%2520in%2520performance%2520during%2520external%2520validation%252C%250Awhich%2520underscores%2520the%2520importance%2520of%2520diverse%2520and%2520comprehensive%2520training%2520and%250Atesting%2520data.%2520Most%2520notably%252C%2520VIT%2520approach%2520provided%2520objective%2520assessment%2520of%2520the%250Autility%2520of%2520diverse%2520models%2520and%2520datasets%2520while%2520further%2520providing%2520insight%2520into%2520the%250Ainfluence%2520of%2520dataset%2520characteristics%252C%2520patient%2520factors%252C%2520and%2520imaging%2520physics%2520on%250AAI%2520efficacy.%2520Conclusions%253A%2520The%2520VIT%2520approach%2520can%2520be%2520used%2520to%2520enhance%2520model%250Atransparency%2520and%2520reliability%252C%2520offering%2520nuanced%2520insights%2520into%2520the%2520factors%250Adriving%2520AI%2520performance%2520and%2520bridging%2520the%2520gap%2520between%2520experimental%2520and%2520clinical%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09730v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Utility%20of%20the%20Virtual%20Imaging%20Trials%20Methodology%20for%20Objective%0A%20%20Characterization%20of%20AI%20Systems%20and%20Training%20Data&entry.906535625=Fakrul%20Islam%20Tushar%20and%20Lavsen%20Dahal%20and%20Saman%20Sotoudeh-Paima%20and%20Ehsan%20Abadi%20and%20W.%20Paul%20Segars%20and%20Ehsan%20Samei%20and%20Joseph%20Y.%20Lo&entry.1292438233=%20%20Purpose%3A%20The%20credibility%20of%20Artificial%20Intelligence%20%28AI%29%20models%20for%20medical%0Aimaging%20continues%20to%20be%20a%20challenge%2C%20affected%20by%20the%20diversity%20of%20models%2C%20the%0Adata%20used%20to%20train%20the%20models%2C%20and%20applicability%20of%20their%20combination%20to%0Aproduce%20reproducible%20results%20for%20new%20data.%20Approach%3A%20In%20this%20work%20we%20aimed%20to%0Aexplore%20if%20the%20emerging%20Virtual%20Imaging%20Trials%20%28VIT%29%20methodologies%20can%20provide%0Aan%20objective%20resource%20to%20approach%20this%20challenge.%20The%20study%20was%20conducted%20for%0Athe%20case%20example%20of%20COVID-19%20diagnosis%20using%20clinical%20and%20virtual%20computed%0Atomography%20%28CT%29%20and%20chest%20radiography%20%28CXR%29%20processed%20with%20convolutional%20neural%0Anetworks.%20Multiple%20AI%20models%20were%20developed%20and%20tested%20using%203D%20ResNet-like%20and%0A2D%20EfficientNetv2%20architectures%20across%20diverse%20datasets.%20Results%3A%20The%0Aperformance%20differences%20were%20evaluated%20in%20terms%20of%20the%20area%20under%20the%20curve%0A%28AUC%29%20and%20the%20DeLong%20method%20for%20AUC%20confidence%20intervals.%20The%20models%20trained%20on%0Athe%20most%20diverse%20datasets%20showed%20the%20highest%20external%20testing%20performance%2C%20with%0AAUC%20values%20ranging%20from%200.73-0.76%20for%20CT%20and%200.70-0.73%20for%20CXR.%20Internal%0Atesting%20yielded%20higher%20AUC%20values%20%280.77%20-0.85%20for%20CT%20and%200.77-1.0%20for%20CXR%29%2C%0Ahighlighting%20a%20substantial%20drop%20in%20performance%20during%20external%20validation%2C%0Awhich%20underscores%20the%20importance%20of%20diverse%20and%20comprehensive%20training%20and%0Atesting%20data.%20Most%20notably%2C%20VIT%20approach%20provided%20objective%20assessment%20of%20the%0Autility%20of%20diverse%20models%20and%20datasets%20while%20further%20providing%20insight%20into%20the%0Ainfluence%20of%20dataset%20characteristics%2C%20patient%20factors%2C%20and%20imaging%20physics%20on%0AAI%20efficacy.%20Conclusions%3A%20The%20VIT%20approach%20can%20be%20used%20to%20enhance%20model%0Atransparency%20and%20reliability%2C%20offering%20nuanced%20insights%20into%20the%20factors%0Adriving%20AI%20performance%20and%20bridging%20the%20gap%20between%20experimental%20and%20clinical%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09730v5&entry.124074799=Read"},
{"title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing\n  through Non-invasive BCI", "author": "Weichen Dai and Yuxuan Huang and Li Zhu and Dongjun Liu and Yu Zhang and Qibin Zhao and Andrzej Cichocki and Fabio Babiloni and Ke Li and Jianyu Qiu and Gangyong Jia and Wanzeng Kong and Qing Wu", "abstract": "  Humans possess a remarkable capacity for spatial cognition, allowing for\nself-localization even in novel or unfamiliar environments. While hippocampal\nneurons encoding position and orientation are well documented, the large-scale\nneural dynamics supporting spatial representation, particularly during\nnaturalistic, passive experience, remain poorly understood. Here, we\ndemonstrate for the first time that non-invasive brain-computer interfaces\n(BCIs) based on electroencephalography (EEG) can decode spontaneous,\nfine-grained egocentric 6D pose, comprising three-dimensional position and\norientation, during passive viewing of egocentric video. Despite EEG's limited\nspatial resolution and high signal noise, we find that spatially coherent\nvisual input (i.e., continuous and structured motion) reliably evokes decodable\nspatial representations, aligning with participants' subjective sense of\nspatial engagement. Decoding performance further improves when visual input is\npresented at a frame rate of 100 ms per image, suggesting alignment with\nintrinsic neural temporal dynamics. Using gradient-based backpropagation\nthrough a neural decoding model, we identify distinct EEG channels contributing\nto position -- and orientation specific -- components, revealing a distributed\nyet complementary neural encoding scheme. These findings indicate that the\nbrain's spatial systems operate spontaneously and continuously, even under\npassive conditions, challenging traditional distinctions between active and\npassive spatial cognition. Our results offer a non-invasive window into the\nautomatic construction of egocentric spatial maps and advance our understanding\nof how the human mind transforms everyday sensory experience into structured\ninternal representations.\n", "link": "http://arxiv.org/abs/2507.12417v1", "date": "2025-07-16", "relevancy": 2.6395, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spontaneous%20Spatial%20Cognition%20Emerges%20during%20Egocentric%20Video%20Viewing%0A%20%20through%20Non-invasive%20BCI&body=Title%3A%20Spontaneous%20Spatial%20Cognition%20Emerges%20during%20Egocentric%20Video%20Viewing%0A%20%20through%20Non-invasive%20BCI%0AAuthor%3A%20Weichen%20Dai%20and%20Yuxuan%20Huang%20and%20Li%20Zhu%20and%20Dongjun%20Liu%20and%20Yu%20Zhang%20and%20Qibin%20Zhao%20and%20Andrzej%20Cichocki%20and%20Fabio%20Babiloni%20and%20Ke%20Li%20and%20Jianyu%20Qiu%20and%20Gangyong%20Jia%20and%20Wanzeng%20Kong%20and%20Qing%20Wu%0AAbstract%3A%20%20%20Humans%20possess%20a%20remarkable%20capacity%20for%20spatial%20cognition%2C%20allowing%20for%0Aself-localization%20even%20in%20novel%20or%20unfamiliar%20environments.%20While%20hippocampal%0Aneurons%20encoding%20position%20and%20orientation%20are%20well%20documented%2C%20the%20large-scale%0Aneural%20dynamics%20supporting%20spatial%20representation%2C%20particularly%20during%0Anaturalistic%2C%20passive%20experience%2C%20remain%20poorly%20understood.%20Here%2C%20we%0Ademonstrate%20for%20the%20first%20time%20that%20non-invasive%20brain-computer%20interfaces%0A%28BCIs%29%20based%20on%20electroencephalography%20%28EEG%29%20can%20decode%20spontaneous%2C%0Afine-grained%20egocentric%206D%20pose%2C%20comprising%20three-dimensional%20position%20and%0Aorientation%2C%20during%20passive%20viewing%20of%20egocentric%20video.%20Despite%20EEG%27s%20limited%0Aspatial%20resolution%20and%20high%20signal%20noise%2C%20we%20find%20that%20spatially%20coherent%0Avisual%20input%20%28i.e.%2C%20continuous%20and%20structured%20motion%29%20reliably%20evokes%20decodable%0Aspatial%20representations%2C%20aligning%20with%20participants%27%20subjective%20sense%20of%0Aspatial%20engagement.%20Decoding%20performance%20further%20improves%20when%20visual%20input%20is%0Apresented%20at%20a%20frame%20rate%20of%20100%20ms%20per%20image%2C%20suggesting%20alignment%20with%0Aintrinsic%20neural%20temporal%20dynamics.%20Using%20gradient-based%20backpropagation%0Athrough%20a%20neural%20decoding%20model%2C%20we%20identify%20distinct%20EEG%20channels%20contributing%0Ato%20position%20--%20and%20orientation%20specific%20--%20components%2C%20revealing%20a%20distributed%0Ayet%20complementary%20neural%20encoding%20scheme.%20These%20findings%20indicate%20that%20the%0Abrain%27s%20spatial%20systems%20operate%20spontaneously%20and%20continuously%2C%20even%20under%0Apassive%20conditions%2C%20challenging%20traditional%20distinctions%20between%20active%20and%0Apassive%20spatial%20cognition.%20Our%20results%20offer%20a%20non-invasive%20window%20into%20the%0Aautomatic%20construction%20of%20egocentric%20spatial%20maps%20and%20advance%20our%20understanding%0Aof%20how%20the%20human%20mind%20transforms%20everyday%20sensory%20experience%20into%20structured%0Ainternal%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpontaneous%2520Spatial%2520Cognition%2520Emerges%2520during%2520Egocentric%2520Video%2520Viewing%250A%2520%2520through%2520Non-invasive%2520BCI%26entry.906535625%3DWeichen%2520Dai%2520and%2520Yuxuan%2520Huang%2520and%2520Li%2520Zhu%2520and%2520Dongjun%2520Liu%2520and%2520Yu%2520Zhang%2520and%2520Qibin%2520Zhao%2520and%2520Andrzej%2520Cichocki%2520and%2520Fabio%2520Babiloni%2520and%2520Ke%2520Li%2520and%2520Jianyu%2520Qiu%2520and%2520Gangyong%2520Jia%2520and%2520Wanzeng%2520Kong%2520and%2520Qing%2520Wu%26entry.1292438233%3D%2520%2520Humans%2520possess%2520a%2520remarkable%2520capacity%2520for%2520spatial%2520cognition%252C%2520allowing%2520for%250Aself-localization%2520even%2520in%2520novel%2520or%2520unfamiliar%2520environments.%2520While%2520hippocampal%250Aneurons%2520encoding%2520position%2520and%2520orientation%2520are%2520well%2520documented%252C%2520the%2520large-scale%250Aneural%2520dynamics%2520supporting%2520spatial%2520representation%252C%2520particularly%2520during%250Anaturalistic%252C%2520passive%2520experience%252C%2520remain%2520poorly%2520understood.%2520Here%252C%2520we%250Ademonstrate%2520for%2520the%2520first%2520time%2520that%2520non-invasive%2520brain-computer%2520interfaces%250A%2528BCIs%2529%2520based%2520on%2520electroencephalography%2520%2528EEG%2529%2520can%2520decode%2520spontaneous%252C%250Afine-grained%2520egocentric%25206D%2520pose%252C%2520comprising%2520three-dimensional%2520position%2520and%250Aorientation%252C%2520during%2520passive%2520viewing%2520of%2520egocentric%2520video.%2520Despite%2520EEG%2527s%2520limited%250Aspatial%2520resolution%2520and%2520high%2520signal%2520noise%252C%2520we%2520find%2520that%2520spatially%2520coherent%250Avisual%2520input%2520%2528i.e.%252C%2520continuous%2520and%2520structured%2520motion%2529%2520reliably%2520evokes%2520decodable%250Aspatial%2520representations%252C%2520aligning%2520with%2520participants%2527%2520subjective%2520sense%2520of%250Aspatial%2520engagement.%2520Decoding%2520performance%2520further%2520improves%2520when%2520visual%2520input%2520is%250Apresented%2520at%2520a%2520frame%2520rate%2520of%2520100%2520ms%2520per%2520image%252C%2520suggesting%2520alignment%2520with%250Aintrinsic%2520neural%2520temporal%2520dynamics.%2520Using%2520gradient-based%2520backpropagation%250Athrough%2520a%2520neural%2520decoding%2520model%252C%2520we%2520identify%2520distinct%2520EEG%2520channels%2520contributing%250Ato%2520position%2520--%2520and%2520orientation%2520specific%2520--%2520components%252C%2520revealing%2520a%2520distributed%250Ayet%2520complementary%2520neural%2520encoding%2520scheme.%2520These%2520findings%2520indicate%2520that%2520the%250Abrain%2527s%2520spatial%2520systems%2520operate%2520spontaneously%2520and%2520continuously%252C%2520even%2520under%250Apassive%2520conditions%252C%2520challenging%2520traditional%2520distinctions%2520between%2520active%2520and%250Apassive%2520spatial%2520cognition.%2520Our%2520results%2520offer%2520a%2520non-invasive%2520window%2520into%2520the%250Aautomatic%2520construction%2520of%2520egocentric%2520spatial%2520maps%2520and%2520advance%2520our%2520understanding%250Aof%2520how%2520the%2520human%2520mind%2520transforms%2520everyday%2520sensory%2520experience%2520into%2520structured%250Ainternal%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spontaneous%20Spatial%20Cognition%20Emerges%20during%20Egocentric%20Video%20Viewing%0A%20%20through%20Non-invasive%20BCI&entry.906535625=Weichen%20Dai%20and%20Yuxuan%20Huang%20and%20Li%20Zhu%20and%20Dongjun%20Liu%20and%20Yu%20Zhang%20and%20Qibin%20Zhao%20and%20Andrzej%20Cichocki%20and%20Fabio%20Babiloni%20and%20Ke%20Li%20and%20Jianyu%20Qiu%20and%20Gangyong%20Jia%20and%20Wanzeng%20Kong%20and%20Qing%20Wu&entry.1292438233=%20%20Humans%20possess%20a%20remarkable%20capacity%20for%20spatial%20cognition%2C%20allowing%20for%0Aself-localization%20even%20in%20novel%20or%20unfamiliar%20environments.%20While%20hippocampal%0Aneurons%20encoding%20position%20and%20orientation%20are%20well%20documented%2C%20the%20large-scale%0Aneural%20dynamics%20supporting%20spatial%20representation%2C%20particularly%20during%0Anaturalistic%2C%20passive%20experience%2C%20remain%20poorly%20understood.%20Here%2C%20we%0Ademonstrate%20for%20the%20first%20time%20that%20non-invasive%20brain-computer%20interfaces%0A%28BCIs%29%20based%20on%20electroencephalography%20%28EEG%29%20can%20decode%20spontaneous%2C%0Afine-grained%20egocentric%206D%20pose%2C%20comprising%20three-dimensional%20position%20and%0Aorientation%2C%20during%20passive%20viewing%20of%20egocentric%20video.%20Despite%20EEG%27s%20limited%0Aspatial%20resolution%20and%20high%20signal%20noise%2C%20we%20find%20that%20spatially%20coherent%0Avisual%20input%20%28i.e.%2C%20continuous%20and%20structured%20motion%29%20reliably%20evokes%20decodable%0Aspatial%20representations%2C%20aligning%20with%20participants%27%20subjective%20sense%20of%0Aspatial%20engagement.%20Decoding%20performance%20further%20improves%20when%20visual%20input%20is%0Apresented%20at%20a%20frame%20rate%20of%20100%20ms%20per%20image%2C%20suggesting%20alignment%20with%0Aintrinsic%20neural%20temporal%20dynamics.%20Using%20gradient-based%20backpropagation%0Athrough%20a%20neural%20decoding%20model%2C%20we%20identify%20distinct%20EEG%20channels%20contributing%0Ato%20position%20--%20and%20orientation%20specific%20--%20components%2C%20revealing%20a%20distributed%0Ayet%20complementary%20neural%20encoding%20scheme.%20These%20findings%20indicate%20that%20the%0Abrain%27s%20spatial%20systems%20operate%20spontaneously%20and%20continuously%2C%20even%20under%0Apassive%20conditions%2C%20challenging%20traditional%20distinctions%20between%20active%20and%0Apassive%20spatial%20cognition.%20Our%20results%20offer%20a%20non-invasive%20window%20into%20the%0Aautomatic%20construction%20of%20egocentric%20spatial%20maps%20and%20advance%20our%20understanding%0Aof%20how%20the%20human%20mind%20transforms%20everyday%20sensory%20experience%20into%20structured%0Ainternal%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12417v1&entry.124074799=Read"},
{"title": "Sparse Orthogonal Parameters Tuning for Continual Learning", "author": "Hai-Jian Ke and Kun-Peng Ning and Yu-Yang Liu and Jia-Yu Yao and Yong-Hong Tian and Li Yuan", "abstract": "  Continual learning methods based on pre-trained models (PTM) have recently\ngained attention which adapt to successive downstream tasks without\ncatastrophic forgetting. These methods typically refrain from updating the\npre-trained parameters and instead employ additional adapters, prompts, and\nclassifiers. In this paper, we from a novel perspective investigate the benefit\nof sparse orthogonal parameters for continual learning. We found that merging\nsparse orthogonality of models learned from multiple streaming tasks has great\npotential in addressing catastrophic forgetting. Leveraging this insight, we\npropose a novel yet effective method called SoTU (Sparse Orthogonal Parameters\nTUning). We hypothesize that the effectiveness of SoTU lies in the\ntransformation of knowledge learned from multiple domains into the fusion of\northogonal delta parameters. Experimental evaluations on diverse CL benchmarks\ndemonstrate the effectiveness of the proposed approach. Notably, SoTU achieves\noptimal feature representation for streaming data without necessitating complex\nclassifier designs, making it a Plug-and-Play solution.\n", "link": "http://arxiv.org/abs/2411.02813v2", "date": "2025-07-16", "relevancy": 2.5518, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5097}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Orthogonal%20Parameters%20Tuning%20for%20Continual%20Learning&body=Title%3A%20Sparse%20Orthogonal%20Parameters%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Hai-Jian%20Ke%20and%20Kun-Peng%20Ning%20and%20Yu-Yang%20Liu%20and%20Jia-Yu%20Yao%20and%20Yong-Hong%20Tian%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Continual%20learning%20methods%20based%20on%20pre-trained%20models%20%28PTM%29%20have%20recently%0Agained%20attention%20which%20adapt%20to%20successive%20downstream%20tasks%20without%0Acatastrophic%20forgetting.%20These%20methods%20typically%20refrain%20from%20updating%20the%0Apre-trained%20parameters%20and%20instead%20employ%20additional%20adapters%2C%20prompts%2C%20and%0Aclassifiers.%20In%20this%20paper%2C%20we%20from%20a%20novel%20perspective%20investigate%20the%20benefit%0Aof%20sparse%20orthogonal%20parameters%20for%20continual%20learning.%20We%20found%20that%20merging%0Asparse%20orthogonality%20of%20models%20learned%20from%20multiple%20streaming%20tasks%20has%20great%0Apotential%20in%20addressing%20catastrophic%20forgetting.%20Leveraging%20this%20insight%2C%20we%0Apropose%20a%20novel%20yet%20effective%20method%20called%20SoTU%20%28Sparse%20Orthogonal%20Parameters%0ATUning%29.%20We%20hypothesize%20that%20the%20effectiveness%20of%20SoTU%20lies%20in%20the%0Atransformation%20of%20knowledge%20learned%20from%20multiple%20domains%20into%20the%20fusion%20of%0Aorthogonal%20delta%20parameters.%20Experimental%20evaluations%20on%20diverse%20CL%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach.%20Notably%2C%20SoTU%20achieves%0Aoptimal%20feature%20representation%20for%20streaming%20data%20without%20necessitating%20complex%0Aclassifier%20designs%2C%20making%20it%20a%20Plug-and-Play%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Orthogonal%2520Parameters%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DHai-Jian%2520Ke%2520and%2520Kun-Peng%2520Ning%2520and%2520Yu-Yang%2520Liu%2520and%2520Jia-Yu%2520Yao%2520and%2520Yong-Hong%2520Tian%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Continual%2520learning%2520methods%2520based%2520on%2520pre-trained%2520models%2520%2528PTM%2529%2520have%2520recently%250Agained%2520attention%2520which%2520adapt%2520to%2520successive%2520downstream%2520tasks%2520without%250Acatastrophic%2520forgetting.%2520These%2520methods%2520typically%2520refrain%2520from%2520updating%2520the%250Apre-trained%2520parameters%2520and%2520instead%2520employ%2520additional%2520adapters%252C%2520prompts%252C%2520and%250Aclassifiers.%2520In%2520this%2520paper%252C%2520we%2520from%2520a%2520novel%2520perspective%2520investigate%2520the%2520benefit%250Aof%2520sparse%2520orthogonal%2520parameters%2520for%2520continual%2520learning.%2520We%2520found%2520that%2520merging%250Asparse%2520orthogonality%2520of%2520models%2520learned%2520from%2520multiple%2520streaming%2520tasks%2520has%2520great%250Apotential%2520in%2520addressing%2520catastrophic%2520forgetting.%2520Leveraging%2520this%2520insight%252C%2520we%250Apropose%2520a%2520novel%2520yet%2520effective%2520method%2520called%2520SoTU%2520%2528Sparse%2520Orthogonal%2520Parameters%250ATUning%2529.%2520We%2520hypothesize%2520that%2520the%2520effectiveness%2520of%2520SoTU%2520lies%2520in%2520the%250Atransformation%2520of%2520knowledge%2520learned%2520from%2520multiple%2520domains%2520into%2520the%2520fusion%2520of%250Aorthogonal%2520delta%2520parameters.%2520Experimental%2520evaluations%2520on%2520diverse%2520CL%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%2520Notably%252C%2520SoTU%2520achieves%250Aoptimal%2520feature%2520representation%2520for%2520streaming%2520data%2520without%2520necessitating%2520complex%250Aclassifier%2520designs%252C%2520making%2520it%2520a%2520Plug-and-Play%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Orthogonal%20Parameters%20Tuning%20for%20Continual%20Learning&entry.906535625=Hai-Jian%20Ke%20and%20Kun-Peng%20Ning%20and%20Yu-Yang%20Liu%20and%20Jia-Yu%20Yao%20and%20Yong-Hong%20Tian%20and%20Li%20Yuan&entry.1292438233=%20%20Continual%20learning%20methods%20based%20on%20pre-trained%20models%20%28PTM%29%20have%20recently%0Agained%20attention%20which%20adapt%20to%20successive%20downstream%20tasks%20without%0Acatastrophic%20forgetting.%20These%20methods%20typically%20refrain%20from%20updating%20the%0Apre-trained%20parameters%20and%20instead%20employ%20additional%20adapters%2C%20prompts%2C%20and%0Aclassifiers.%20In%20this%20paper%2C%20we%20from%20a%20novel%20perspective%20investigate%20the%20benefit%0Aof%20sparse%20orthogonal%20parameters%20for%20continual%20learning.%20We%20found%20that%20merging%0Asparse%20orthogonality%20of%20models%20learned%20from%20multiple%20streaming%20tasks%20has%20great%0Apotential%20in%20addressing%20catastrophic%20forgetting.%20Leveraging%20this%20insight%2C%20we%0Apropose%20a%20novel%20yet%20effective%20method%20called%20SoTU%20%28Sparse%20Orthogonal%20Parameters%0ATUning%29.%20We%20hypothesize%20that%20the%20effectiveness%20of%20SoTU%20lies%20in%20the%0Atransformation%20of%20knowledge%20learned%20from%20multiple%20domains%20into%20the%20fusion%20of%0Aorthogonal%20delta%20parameters.%20Experimental%20evaluations%20on%20diverse%20CL%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach.%20Notably%2C%20SoTU%20achieves%0Aoptimal%20feature%20representation%20for%20streaming%20data%20without%20necessitating%20complex%0Aclassifier%20designs%2C%20making%20it%20a%20Plug-and-Play%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02813v2&entry.124074799=Read"},
{"title": "A Framework for Nonstationary Gaussian Processes with Neural Network\n  Parameters", "author": "Zachary James and Joseph Guinness", "abstract": "  Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.\n", "link": "http://arxiv.org/abs/2507.12262v1", "date": "2025-07-16", "relevancy": 2.5512, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5268}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5199}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Nonstationary%20Gaussian%20Processes%20with%20Neural%20Network%0A%20%20Parameters&body=Title%3A%20A%20Framework%20for%20Nonstationary%20Gaussian%20Processes%20with%20Neural%20Network%0A%20%20Parameters%0AAuthor%3A%20Zachary%20James%20and%20Joseph%20Guinness%0AAbstract%3A%20%20%20Gaussian%20processes%20have%20become%20a%20popular%20tool%20for%20nonparametric%20regression%0Abecause%20of%20their%20flexibility%20and%20uncertainty%20quantification.%20However%2C%20they%0Aoften%20use%20stationary%20kernels%2C%20which%20limit%20the%20expressiveness%20of%20the%20model%20and%0Amay%20be%20unsuitable%20for%20many%20datasets.%20We%20propose%20a%20framework%20that%20uses%0Anonstationary%20kernels%20whose%20parameters%20vary%20across%20the%20feature%20space%2C%20modeling%0Athese%20parameters%20as%20the%20output%20of%20a%20neural%20network%20that%20takes%20the%20features%20as%0Ainput.%20The%20neural%20network%20and%20Gaussian%20process%20are%20trained%20jointly%20using%20the%0Achain%20rule%20to%20calculate%20derivatives.%20Our%20method%20clearly%20describes%20the%20behavior%0Aof%20the%20nonstationary%20parameters%20and%20is%20compatible%20with%20approximation%20methods%0Afor%20scaling%20to%20large%20datasets.%20It%20is%20flexible%20and%20easily%20adapts%20to%20different%0Anonstationary%20kernels%20without%20needing%20to%20redesign%20the%20optimization%20procedure.%0AOur%20methods%20are%20implemented%20with%20the%20GPyTorch%20library%20and%20can%20be%20readily%0Amodified.%20We%20test%20a%20nonstationary%20variance%20and%20noise%20variant%20of%20our%20method%20on%0Aseveral%20machine%20learning%20datasets%20and%20find%20that%20it%20achieves%20better%20accuracy%20and%0Alog-score%20than%20both%20a%20stationary%20model%20and%20a%20hierarchical%20model%20approximated%0Awith%20variational%20inference.%20Similar%20results%20are%20observed%20for%20a%20model%20with%20only%0Anonstationary%20variance.%20We%20also%20demonstrate%20our%20approach%27s%20ability%20to%20recover%0Athe%20nonstationary%20parameters%20of%20a%20spatial%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Nonstationary%2520Gaussian%2520Processes%2520with%2520Neural%2520Network%250A%2520%2520Parameters%26entry.906535625%3DZachary%2520James%2520and%2520Joseph%2520Guinness%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520have%2520become%2520a%2520popular%2520tool%2520for%2520nonparametric%2520regression%250Abecause%2520of%2520their%2520flexibility%2520and%2520uncertainty%2520quantification.%2520However%252C%2520they%250Aoften%2520use%2520stationary%2520kernels%252C%2520which%2520limit%2520the%2520expressiveness%2520of%2520the%2520model%2520and%250Amay%2520be%2520unsuitable%2520for%2520many%2520datasets.%2520We%2520propose%2520a%2520framework%2520that%2520uses%250Anonstationary%2520kernels%2520whose%2520parameters%2520vary%2520across%2520the%2520feature%2520space%252C%2520modeling%250Athese%2520parameters%2520as%2520the%2520output%2520of%2520a%2520neural%2520network%2520that%2520takes%2520the%2520features%2520as%250Ainput.%2520The%2520neural%2520network%2520and%2520Gaussian%2520process%2520are%2520trained%2520jointly%2520using%2520the%250Achain%2520rule%2520to%2520calculate%2520derivatives.%2520Our%2520method%2520clearly%2520describes%2520the%2520behavior%250Aof%2520the%2520nonstationary%2520parameters%2520and%2520is%2520compatible%2520with%2520approximation%2520methods%250Afor%2520scaling%2520to%2520large%2520datasets.%2520It%2520is%2520flexible%2520and%2520easily%2520adapts%2520to%2520different%250Anonstationary%2520kernels%2520without%2520needing%2520to%2520redesign%2520the%2520optimization%2520procedure.%250AOur%2520methods%2520are%2520implemented%2520with%2520the%2520GPyTorch%2520library%2520and%2520can%2520be%2520readily%250Amodified.%2520We%2520test%2520a%2520nonstationary%2520variance%2520and%2520noise%2520variant%2520of%2520our%2520method%2520on%250Aseveral%2520machine%2520learning%2520datasets%2520and%2520find%2520that%2520it%2520achieves%2520better%2520accuracy%2520and%250Alog-score%2520than%2520both%2520a%2520stationary%2520model%2520and%2520a%2520hierarchical%2520model%2520approximated%250Awith%2520variational%2520inference.%2520Similar%2520results%2520are%2520observed%2520for%2520a%2520model%2520with%2520only%250Anonstationary%2520variance.%2520We%2520also%2520demonstrate%2520our%2520approach%2527s%2520ability%2520to%2520recover%250Athe%2520nonstationary%2520parameters%2520of%2520a%2520spatial%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Nonstationary%20Gaussian%20Processes%20with%20Neural%20Network%0A%20%20Parameters&entry.906535625=Zachary%20James%20and%20Joseph%20Guinness&entry.1292438233=%20%20Gaussian%20processes%20have%20become%20a%20popular%20tool%20for%20nonparametric%20regression%0Abecause%20of%20their%20flexibility%20and%20uncertainty%20quantification.%20However%2C%20they%0Aoften%20use%20stationary%20kernels%2C%20which%20limit%20the%20expressiveness%20of%20the%20model%20and%0Amay%20be%20unsuitable%20for%20many%20datasets.%20We%20propose%20a%20framework%20that%20uses%0Anonstationary%20kernels%20whose%20parameters%20vary%20across%20the%20feature%20space%2C%20modeling%0Athese%20parameters%20as%20the%20output%20of%20a%20neural%20network%20that%20takes%20the%20features%20as%0Ainput.%20The%20neural%20network%20and%20Gaussian%20process%20are%20trained%20jointly%20using%20the%0Achain%20rule%20to%20calculate%20derivatives.%20Our%20method%20clearly%20describes%20the%20behavior%0Aof%20the%20nonstationary%20parameters%20and%20is%20compatible%20with%20approximation%20methods%0Afor%20scaling%20to%20large%20datasets.%20It%20is%20flexible%20and%20easily%20adapts%20to%20different%0Anonstationary%20kernels%20without%20needing%20to%20redesign%20the%20optimization%20procedure.%0AOur%20methods%20are%20implemented%20with%20the%20GPyTorch%20library%20and%20can%20be%20readily%0Amodified.%20We%20test%20a%20nonstationary%20variance%20and%20noise%20variant%20of%20our%20method%20on%0Aseveral%20machine%20learning%20datasets%20and%20find%20that%20it%20achieves%20better%20accuracy%20and%0Alog-score%20than%20both%20a%20stationary%20model%20and%20a%20hierarchical%20model%20approximated%0Awith%20variational%20inference.%20Similar%20results%20are%20observed%20for%20a%20model%20with%20only%0Anonstationary%20variance.%20We%20also%20demonstrate%20our%20approach%27s%20ability%20to%20recover%0Athe%20nonstationary%20parameters%20of%20a%20spatial%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12262v1&entry.124074799=Read"},
{"title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization", "author": "Prashanth Vijayaraghavan and Apoorva Nitsure and Charles Mackin and Luyao Shi and Stefano Ambrogio and Arvind Haran and Viresh Paruthi and Ali Elzein and Dan Coops and David Beymer and Tyler Baldwin and Ehsan Degan", "abstract": "  Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.\n", "link": "http://arxiv.org/abs/2507.12308v1", "date": "2025-07-16", "relevancy": 2.5506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Descriptions%3A%20Improving%20Code%20LLMs%20for%20VHDL%20Code%20Generation%20and%0A%20%20Summarization&body=Title%3A%20Chain-of-Descriptions%3A%20Improving%20Code%20LLMs%20for%20VHDL%20Code%20Generation%20and%0A%20%20Summarization%0AAuthor%3A%20Prashanth%20Vijayaraghavan%20and%20Apoorva%20Nitsure%20and%20Charles%20Mackin%20and%20Luyao%20Shi%20and%20Stefano%20Ambrogio%20and%20Arvind%20Haran%20and%20Viresh%20Paruthi%20and%20Ali%20Elzein%20and%20Dan%20Coops%20and%20David%20Beymer%20and%20Tyler%20Baldwin%20and%20Ehsan%20Degan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20widely%20used%20across%20diverse%20NLP%20tasks%0Aand%20domains%2C%20demonstrating%20their%20adaptability%20and%20effectiveness.%20In%20the%20realm%0Aof%20Electronic%20Design%20Automation%20%28EDA%29%2C%20LLMs%20show%20promise%20for%20tasks%20like%0ARegister-Transfer%20Level%20%28RTL%29%20code%20generation%20and%20summarization.%20However%2C%0Adespite%20the%20proliferation%20of%20LLMs%20for%20general%20code-related%20tasks%2C%20there%27s%20a%0Adearth%20of%20research%20focused%20on%20evaluating%20and%20refining%20these%20models%20for%20hardware%0Adescription%20languages%20%28HDLs%29%2C%20notably%20VHDL.%20In%20this%20study%2C%20we%20evaluate%20the%0Aperformance%20of%20existing%20code%20LLMs%20for%20VHDL%20code%20generation%20and%20summarization%0Ausing%20various%20metrics%20and%20two%20datasets%20--%20VHDL-Eval%20and%20VHDL-Xform.%20The%20latter%2C%0Aan%20in-house%20dataset%2C%20aims%20to%20gauge%20LLMs%27%20understanding%20of%20functionally%0Aequivalent%20code.%20Our%20findings%20reveal%20consistent%20underperformance%20of%20these%0Amodels%20across%20different%20metrics%2C%20underscoring%20a%20significant%20gap%20in%20their%0Asuitability%20for%20this%20domain.%20To%20address%20this%20challenge%2C%20we%20propose%0AChain-of-Descriptions%20%28CoDes%29%2C%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%0ALLMs%20for%20VHDL%20code%20generation%20and%20summarization%20tasks.%20CoDes%20involves%0Agenerating%20a%20series%20of%20intermediate%20descriptive%20steps%20based%20on%3A%20%28i%29%20the%20problem%0Astatement%20for%20code%20generation%2C%20and%20%28ii%29%20the%20VHDL%20code%20for%20summarization.%20These%0Asteps%20are%20then%20integrated%20with%20the%20original%20input%20prompt%20%28problem%20statement%20or%0Acode%29%20and%20provided%20as%20input%20to%20the%20LLMs%20to%20generate%20the%20final%20output.%20Our%0Aexperiments%20demonstrate%20that%20the%20CoDes%20approach%20significantly%20surpasses%20the%0Astandard%20prompting%20strategy%20across%20various%20metrics%20on%20both%20datasets.%20This%0Amethod%20not%20only%20improves%20the%20quality%20of%20VHDL%20code%20generation%20and%20summarization%0Abut%20also%20serves%20as%20a%20framework%20for%20future%20research%20aimed%20at%20enhancing%20code%20LLMs%0Afor%20VHDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Descriptions%253A%2520Improving%2520Code%2520LLMs%2520for%2520VHDL%2520Code%2520Generation%2520and%250A%2520%2520Summarization%26entry.906535625%3DPrashanth%2520Vijayaraghavan%2520and%2520Apoorva%2520Nitsure%2520and%2520Charles%2520Mackin%2520and%2520Luyao%2520Shi%2520and%2520Stefano%2520Ambrogio%2520and%2520Arvind%2520Haran%2520and%2520Viresh%2520Paruthi%2520and%2520Ali%2520Elzein%2520and%2520Dan%2520Coops%2520and%2520David%2520Beymer%2520and%2520Tyler%2520Baldwin%2520and%2520Ehsan%2520Degan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520widely%2520used%2520across%2520diverse%2520NLP%2520tasks%250Aand%2520domains%252C%2520demonstrating%2520their%2520adaptability%2520and%2520effectiveness.%2520In%2520the%2520realm%250Aof%2520Electronic%2520Design%2520Automation%2520%2528EDA%2529%252C%2520LLMs%2520show%2520promise%2520for%2520tasks%2520like%250ARegister-Transfer%2520Level%2520%2528RTL%2529%2520code%2520generation%2520and%2520summarization.%2520However%252C%250Adespite%2520the%2520proliferation%2520of%2520LLMs%2520for%2520general%2520code-related%2520tasks%252C%2520there%2527s%2520a%250Adearth%2520of%2520research%2520focused%2520on%2520evaluating%2520and%2520refining%2520these%2520models%2520for%2520hardware%250Adescription%2520languages%2520%2528HDLs%2529%252C%2520notably%2520VHDL.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520the%250Aperformance%2520of%2520existing%2520code%2520LLMs%2520for%2520VHDL%2520code%2520generation%2520and%2520summarization%250Ausing%2520various%2520metrics%2520and%2520two%2520datasets%2520--%2520VHDL-Eval%2520and%2520VHDL-Xform.%2520The%2520latter%252C%250Aan%2520in-house%2520dataset%252C%2520aims%2520to%2520gauge%2520LLMs%2527%2520understanding%2520of%2520functionally%250Aequivalent%2520code.%2520Our%2520findings%2520reveal%2520consistent%2520underperformance%2520of%2520these%250Amodels%2520across%2520different%2520metrics%252C%2520underscoring%2520a%2520significant%2520gap%2520in%2520their%250Asuitability%2520for%2520this%2520domain.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AChain-of-Descriptions%2520%2528CoDes%2529%252C%2520a%2520novel%2520approach%2520to%2520enhance%2520the%2520performance%2520of%250ALLMs%2520for%2520VHDL%2520code%2520generation%2520and%2520summarization%2520tasks.%2520CoDes%2520involves%250Agenerating%2520a%2520series%2520of%2520intermediate%2520descriptive%2520steps%2520based%2520on%253A%2520%2528i%2529%2520the%2520problem%250Astatement%2520for%2520code%2520generation%252C%2520and%2520%2528ii%2529%2520the%2520VHDL%2520code%2520for%2520summarization.%2520These%250Asteps%2520are%2520then%2520integrated%2520with%2520the%2520original%2520input%2520prompt%2520%2528problem%2520statement%2520or%250Acode%2529%2520and%2520provided%2520as%2520input%2520to%2520the%2520LLMs%2520to%2520generate%2520the%2520final%2520output.%2520Our%250Aexperiments%2520demonstrate%2520that%2520the%2520CoDes%2520approach%2520significantly%2520surpasses%2520the%250Astandard%2520prompting%2520strategy%2520across%2520various%2520metrics%2520on%2520both%2520datasets.%2520This%250Amethod%2520not%2520only%2520improves%2520the%2520quality%2520of%2520VHDL%2520code%2520generation%2520and%2520summarization%250Abut%2520also%2520serves%2520as%2520a%2520framework%2520for%2520future%2520research%2520aimed%2520at%2520enhancing%2520code%2520LLMs%250Afor%2520VHDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Descriptions%3A%20Improving%20Code%20LLMs%20for%20VHDL%20Code%20Generation%20and%0A%20%20Summarization&entry.906535625=Prashanth%20Vijayaraghavan%20and%20Apoorva%20Nitsure%20and%20Charles%20Mackin%20and%20Luyao%20Shi%20and%20Stefano%20Ambrogio%20and%20Arvind%20Haran%20and%20Viresh%20Paruthi%20and%20Ali%20Elzein%20and%20Dan%20Coops%20and%20David%20Beymer%20and%20Tyler%20Baldwin%20and%20Ehsan%20Degan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20widely%20used%20across%20diverse%20NLP%20tasks%0Aand%20domains%2C%20demonstrating%20their%20adaptability%20and%20effectiveness.%20In%20the%20realm%0Aof%20Electronic%20Design%20Automation%20%28EDA%29%2C%20LLMs%20show%20promise%20for%20tasks%20like%0ARegister-Transfer%20Level%20%28RTL%29%20code%20generation%20and%20summarization.%20However%2C%0Adespite%20the%20proliferation%20of%20LLMs%20for%20general%20code-related%20tasks%2C%20there%27s%20a%0Adearth%20of%20research%20focused%20on%20evaluating%20and%20refining%20these%20models%20for%20hardware%0Adescription%20languages%20%28HDLs%29%2C%20notably%20VHDL.%20In%20this%20study%2C%20we%20evaluate%20the%0Aperformance%20of%20existing%20code%20LLMs%20for%20VHDL%20code%20generation%20and%20summarization%0Ausing%20various%20metrics%20and%20two%20datasets%20--%20VHDL-Eval%20and%20VHDL-Xform.%20The%20latter%2C%0Aan%20in-house%20dataset%2C%20aims%20to%20gauge%20LLMs%27%20understanding%20of%20functionally%0Aequivalent%20code.%20Our%20findings%20reveal%20consistent%20underperformance%20of%20these%0Amodels%20across%20different%20metrics%2C%20underscoring%20a%20significant%20gap%20in%20their%0Asuitability%20for%20this%20domain.%20To%20address%20this%20challenge%2C%20we%20propose%0AChain-of-Descriptions%20%28CoDes%29%2C%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%0ALLMs%20for%20VHDL%20code%20generation%20and%20summarization%20tasks.%20CoDes%20involves%0Agenerating%20a%20series%20of%20intermediate%20descriptive%20steps%20based%20on%3A%20%28i%29%20the%20problem%0Astatement%20for%20code%20generation%2C%20and%20%28ii%29%20the%20VHDL%20code%20for%20summarization.%20These%0Asteps%20are%20then%20integrated%20with%20the%20original%20input%20prompt%20%28problem%20statement%20or%0Acode%29%20and%20provided%20as%20input%20to%20the%20LLMs%20to%20generate%20the%20final%20output.%20Our%0Aexperiments%20demonstrate%20that%20the%20CoDes%20approach%20significantly%20surpasses%20the%0Astandard%20prompting%20strategy%20across%20various%20metrics%20on%20both%20datasets.%20This%0Amethod%20not%20only%20improves%20the%20quality%20of%20VHDL%20code%20generation%20and%20summarization%0Abut%20also%20serves%20as%20a%20framework%20for%20future%20research%20aimed%20at%20enhancing%20code%20LLMs%0Afor%20VHDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12308v1&entry.124074799=Read"},
{"title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "author": "Yuan-Chen Shu and Zhiwei Lin and Yongtao Wang", "abstract": "  To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.\n", "link": "http://arxiv.org/abs/2507.12297v1", "date": "2025-07-16", "relevancy": 2.5239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RegCL%3A%20Continual%20Adaptation%20of%20Segment%20Anything%20Model%20via%20Model%20Merging&body=Title%3A%20RegCL%3A%20Continual%20Adaptation%20of%20Segment%20Anything%20Model%20via%20Model%20Merging%0AAuthor%3A%20Yuan-Chen%20Shu%20and%20Zhiwei%20Lin%20and%20Yongtao%20Wang%0AAbstract%3A%20%20%20To%20address%20the%20performance%20limitations%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20in%0Aspecific%20domains%2C%20existing%20works%20primarily%20adopt%20adapter-based%20one-step%0Aadaptation%20paradigms.%20However%2C%20some%20of%20these%20methods%20are%20specific%20developed%20for%0Aspecific%20domains.%20If%20used%20on%20other%20domains%20may%20lead%20to%20performance%20degradation.%0AThis%20issue%20of%20catastrophic%20forgetting%20severely%20limits%20the%20model%27s%20scalability.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20RegCL%2C%20a%20novel%20non-replay%20continual%0Alearning%20%28CL%29%20framework%20designed%20for%20efficient%20multi-domain%20knowledge%0Aintegration%20through%20model%20merging.%20Specifically%2C%20RegCL%20incorporates%20the%20model%0Amerging%20algorithm%20into%20the%20continual%20learning%20paradigm%20by%20merging%20the%0Aparameters%20of%20SAM%27s%20adaptation%20modules%20%28e.g.%2C%20LoRA%20modules%29%20trained%20on%0Adifferent%20domains.%20The%20merging%20process%20is%20guided%20by%20weight%20optimization%2C%20which%0Aminimizes%20prediction%20discrepancies%20between%20the%20merged%20model%20and%20each%20of%20the%0Adomain-specific%20models.%20RegCL%20effectively%20consolidates%20multi-domain%20knowledge%0Awhile%20maintaining%20parameter%20efficiency%2C%20i.e.%2C%20the%20model%20size%20remains%20constant%0Aregardless%20of%20the%20number%20of%20tasks%2C%20and%20no%20historical%20data%20storage%20is%20required.%0AExperimental%20results%20demonstrate%20that%20RegCL%20achieves%20favorable%20continual%0Alearning%20performance%20across%20multiple%20downstream%20datasets%2C%20validating%20its%0Aeffectiveness%20in%20dynamic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegCL%253A%2520Continual%2520Adaptation%2520of%2520Segment%2520Anything%2520Model%2520via%2520Model%2520Merging%26entry.906535625%3DYuan-Chen%2520Shu%2520and%2520Zhiwei%2520Lin%2520and%2520Yongtao%2520Wang%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520performance%2520limitations%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520in%250Aspecific%2520domains%252C%2520existing%2520works%2520primarily%2520adopt%2520adapter-based%2520one-step%250Aadaptation%2520paradigms.%2520However%252C%2520some%2520of%2520these%2520methods%2520are%2520specific%2520developed%2520for%250Aspecific%2520domains.%2520If%2520used%2520on%2520other%2520domains%2520may%2520lead%2520to%2520performance%2520degradation.%250AThis%2520issue%2520of%2520catastrophic%2520forgetting%2520severely%2520limits%2520the%2520model%2527s%2520scalability.%250ATo%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520RegCL%252C%2520a%2520novel%2520non-replay%2520continual%250Alearning%2520%2528CL%2529%2520framework%2520designed%2520for%2520efficient%2520multi-domain%2520knowledge%250Aintegration%2520through%2520model%2520merging.%2520Specifically%252C%2520RegCL%2520incorporates%2520the%2520model%250Amerging%2520algorithm%2520into%2520the%2520continual%2520learning%2520paradigm%2520by%2520merging%2520the%250Aparameters%2520of%2520SAM%2527s%2520adaptation%2520modules%2520%2528e.g.%252C%2520LoRA%2520modules%2529%2520trained%2520on%250Adifferent%2520domains.%2520The%2520merging%2520process%2520is%2520guided%2520by%2520weight%2520optimization%252C%2520which%250Aminimizes%2520prediction%2520discrepancies%2520between%2520the%2520merged%2520model%2520and%2520each%2520of%2520the%250Adomain-specific%2520models.%2520RegCL%2520effectively%2520consolidates%2520multi-domain%2520knowledge%250Awhile%2520maintaining%2520parameter%2520efficiency%252C%2520i.e.%252C%2520the%2520model%2520size%2520remains%2520constant%250Aregardless%2520of%2520the%2520number%2520of%2520tasks%252C%2520and%2520no%2520historical%2520data%2520storage%2520is%2520required.%250AExperimental%2520results%2520demonstrate%2520that%2520RegCL%2520achieves%2520favorable%2520continual%250Alearning%2520performance%2520across%2520multiple%2520downstream%2520datasets%252C%2520validating%2520its%250Aeffectiveness%2520in%2520dynamic%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RegCL%3A%20Continual%20Adaptation%20of%20Segment%20Anything%20Model%20via%20Model%20Merging&entry.906535625=Yuan-Chen%20Shu%20and%20Zhiwei%20Lin%20and%20Yongtao%20Wang&entry.1292438233=%20%20To%20address%20the%20performance%20limitations%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20in%0Aspecific%20domains%2C%20existing%20works%20primarily%20adopt%20adapter-based%20one-step%0Aadaptation%20paradigms.%20However%2C%20some%20of%20these%20methods%20are%20specific%20developed%20for%0Aspecific%20domains.%20If%20used%20on%20other%20domains%20may%20lead%20to%20performance%20degradation.%0AThis%20issue%20of%20catastrophic%20forgetting%20severely%20limits%20the%20model%27s%20scalability.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20RegCL%2C%20a%20novel%20non-replay%20continual%0Alearning%20%28CL%29%20framework%20designed%20for%20efficient%20multi-domain%20knowledge%0Aintegration%20through%20model%20merging.%20Specifically%2C%20RegCL%20incorporates%20the%20model%0Amerging%20algorithm%20into%20the%20continual%20learning%20paradigm%20by%20merging%20the%0Aparameters%20of%20SAM%27s%20adaptation%20modules%20%28e.g.%2C%20LoRA%20modules%29%20trained%20on%0Adifferent%20domains.%20The%20merging%20process%20is%20guided%20by%20weight%20optimization%2C%20which%0Aminimizes%20prediction%20discrepancies%20between%20the%20merged%20model%20and%20each%20of%20the%0Adomain-specific%20models.%20RegCL%20effectively%20consolidates%20multi-domain%20knowledge%0Awhile%20maintaining%20parameter%20efficiency%2C%20i.e.%2C%20the%20model%20size%20remains%20constant%0Aregardless%20of%20the%20number%20of%20tasks%2C%20and%20no%20historical%20data%20storage%20is%20required.%0AExperimental%20results%20demonstrate%20that%20RegCL%20achieves%20favorable%20continual%0Alearning%20performance%20across%20multiple%20downstream%20datasets%2C%20validating%20its%0Aeffectiveness%20in%20dynamic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12297v1&entry.124074799=Read"},
{"title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "author": "Muhammed Furkan Dasdelen and Hyesu Lim and Michele Buck and Katharina S. G\u00f6tze and Carsten Marr and Steffen Schneider", "abstract": "  Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.\n", "link": "http://arxiv.org/abs/2507.12464v1", "date": "2025-07-16", "relevancy": 2.5213, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CytoSAE%3A%20Interpretable%20Cell%20Embeddings%20for%20Hematology&body=Title%3A%20CytoSAE%3A%20Interpretable%20Cell%20Embeddings%20for%20Hematology%0AAuthor%3A%20Muhammed%20Furkan%20Dasdelen%20and%20Hyesu%20Lim%20and%20Michele%20Buck%20and%20Katharina%20S.%20G%C3%B6tze%20and%20Carsten%20Marr%20and%20Steffen%20Schneider%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20emerged%20as%20a%20promising%20tool%20for%20mechanistic%0Ainterpretability%20of%20transformer-based%20foundation%20models.%20Very%20recently%2C%20SAEs%0Awere%20also%20adopted%20for%20the%20visual%20domain%2C%20enabling%20the%20discovery%20of%20visual%0Aconcepts%20and%20their%20patch-wise%20attribution%20to%20tokens%20in%20the%20transformer%20model.%0AWhile%20a%20growing%20number%20of%20foundation%20models%20emerged%20for%20medical%20imaging%2C%20tools%0Afor%20explaining%20their%20inferences%20are%20still%20lacking.%20In%20this%20work%2C%20we%20show%20the%0Aapplicability%20of%20SAEs%20for%20hematology.%20We%20propose%20CytoSAE%2C%20a%20sparse%20autoencoder%0Awhich%20is%20trained%20on%20over%2040%2C000%20peripheral%20blood%20single-cell%20images.%20CytoSAE%0Ageneralizes%20to%20diverse%20and%20out-of-domain%20datasets%2C%20including%20bone%20marrow%0Acytology%2C%20where%20it%20identifies%20morphologically%20relevant%20concepts%20which%20we%0Avalidated%20with%20medical%20experts.%20Furthermore%2C%20we%20demonstrate%20scenarios%20in%20which%0ACytoSAE%20can%20generate%20patient-specific%20and%20disease-specific%20concepts%2C%20enabling%0Athe%20detection%20of%20pathognomonic%20cells%20and%20localized%20cellular%20abnormalities%20at%0Athe%20patch%20level.%20We%20quantified%20the%20effect%20of%20concepts%20on%20a%20patient-level%20AML%0Asubtype%20classification%20task%20and%20show%20that%20CytoSAE%20concepts%20reach%20performance%0Acomparable%20to%20the%20state-of-the-art%2C%20while%20offering%20explainability%20on%20the%0Asub-cellular%20level.%20Source%20code%20and%20model%20weights%20are%20available%20at%0Ahttps%3A//github.com/dynamical-inference/cytosae.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCytoSAE%253A%2520Interpretable%2520Cell%2520Embeddings%2520for%2520Hematology%26entry.906535625%3DMuhammed%2520Furkan%2520Dasdelen%2520and%2520Hyesu%2520Lim%2520and%2520Michele%2520Buck%2520and%2520Katharina%2520S.%2520G%25C3%25B6tze%2520and%2520Carsten%2520Marr%2520and%2520Steffen%2520Schneider%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520emerged%2520as%2520a%2520promising%2520tool%2520for%2520mechanistic%250Ainterpretability%2520of%2520transformer-based%2520foundation%2520models.%2520Very%2520recently%252C%2520SAEs%250Awere%2520also%2520adopted%2520for%2520the%2520visual%2520domain%252C%2520enabling%2520the%2520discovery%2520of%2520visual%250Aconcepts%2520and%2520their%2520patch-wise%2520attribution%2520to%2520tokens%2520in%2520the%2520transformer%2520model.%250AWhile%2520a%2520growing%2520number%2520of%2520foundation%2520models%2520emerged%2520for%2520medical%2520imaging%252C%2520tools%250Afor%2520explaining%2520their%2520inferences%2520are%2520still%2520lacking.%2520In%2520this%2520work%252C%2520we%2520show%2520the%250Aapplicability%2520of%2520SAEs%2520for%2520hematology.%2520We%2520propose%2520CytoSAE%252C%2520a%2520sparse%2520autoencoder%250Awhich%2520is%2520trained%2520on%2520over%252040%252C000%2520peripheral%2520blood%2520single-cell%2520images.%2520CytoSAE%250Ageneralizes%2520to%2520diverse%2520and%2520out-of-domain%2520datasets%252C%2520including%2520bone%2520marrow%250Acytology%252C%2520where%2520it%2520identifies%2520morphologically%2520relevant%2520concepts%2520which%2520we%250Avalidated%2520with%2520medical%2520experts.%2520Furthermore%252C%2520we%2520demonstrate%2520scenarios%2520in%2520which%250ACytoSAE%2520can%2520generate%2520patient-specific%2520and%2520disease-specific%2520concepts%252C%2520enabling%250Athe%2520detection%2520of%2520pathognomonic%2520cells%2520and%2520localized%2520cellular%2520abnormalities%2520at%250Athe%2520patch%2520level.%2520We%2520quantified%2520the%2520effect%2520of%2520concepts%2520on%2520a%2520patient-level%2520AML%250Asubtype%2520classification%2520task%2520and%2520show%2520that%2520CytoSAE%2520concepts%2520reach%2520performance%250Acomparable%2520to%2520the%2520state-of-the-art%252C%2520while%2520offering%2520explainability%2520on%2520the%250Asub-cellular%2520level.%2520Source%2520code%2520and%2520model%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/dynamical-inference/cytosae.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CytoSAE%3A%20Interpretable%20Cell%20Embeddings%20for%20Hematology&entry.906535625=Muhammed%20Furkan%20Dasdelen%20and%20Hyesu%20Lim%20and%20Michele%20Buck%20and%20Katharina%20S.%20G%C3%B6tze%20and%20Carsten%20Marr%20and%20Steffen%20Schneider&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20emerged%20as%20a%20promising%20tool%20for%20mechanistic%0Ainterpretability%20of%20transformer-based%20foundation%20models.%20Very%20recently%2C%20SAEs%0Awere%20also%20adopted%20for%20the%20visual%20domain%2C%20enabling%20the%20discovery%20of%20visual%0Aconcepts%20and%20their%20patch-wise%20attribution%20to%20tokens%20in%20the%20transformer%20model.%0AWhile%20a%20growing%20number%20of%20foundation%20models%20emerged%20for%20medical%20imaging%2C%20tools%0Afor%20explaining%20their%20inferences%20are%20still%20lacking.%20In%20this%20work%2C%20we%20show%20the%0Aapplicability%20of%20SAEs%20for%20hematology.%20We%20propose%20CytoSAE%2C%20a%20sparse%20autoencoder%0Awhich%20is%20trained%20on%20over%2040%2C000%20peripheral%20blood%20single-cell%20images.%20CytoSAE%0Ageneralizes%20to%20diverse%20and%20out-of-domain%20datasets%2C%20including%20bone%20marrow%0Acytology%2C%20where%20it%20identifies%20morphologically%20relevant%20concepts%20which%20we%0Avalidated%20with%20medical%20experts.%20Furthermore%2C%20we%20demonstrate%20scenarios%20in%20which%0ACytoSAE%20can%20generate%20patient-specific%20and%20disease-specific%20concepts%2C%20enabling%0Athe%20detection%20of%20pathognomonic%20cells%20and%20localized%20cellular%20abnormalities%20at%0Athe%20patch%20level.%20We%20quantified%20the%20effect%20of%20concepts%20on%20a%20patient-level%20AML%0Asubtype%20classification%20task%20and%20show%20that%20CytoSAE%20concepts%20reach%20performance%0Acomparable%20to%20the%20state-of-the-art%2C%20while%20offering%20explainability%20on%20the%0Asub-cellular%20level.%20Source%20code%20and%20model%20weights%20are%20available%20at%0Ahttps%3A//github.com/dynamical-inference/cytosae.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12464v1&entry.124074799=Read"},
{"title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray\n  Diagnosis", "author": "Trong-Thang Pham and Anh Nguyen and Zhigang Deng and Carol C. Wu and Hien Van Nguyen and Ngan Le", "abstract": "  Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.\n", "link": "http://arxiv.org/abs/2507.12461v1", "date": "2025-07-16", "relevancy": 2.4962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Radiologist%27s%20Intention%20from%20Eye%20Movements%20in%20Chest%20X-ray%0A%20%20Diagnosis&body=Title%3A%20Interpreting%20Radiologist%27s%20Intention%20from%20Eye%20Movements%20in%20Chest%20X-ray%0A%20%20Diagnosis%0AAuthor%3A%20Trong-Thang%20Pham%20and%20Anh%20Nguyen%20and%20Zhigang%20Deng%20and%20Carol%20C.%20Wu%20and%20Hien%20Van%20Nguyen%20and%20Ngan%20Le%0AAbstract%3A%20%20%20Radiologists%20rely%20on%20eye%20movements%20to%20navigate%20and%20interpret%20medical%20images.%0AA%20trained%20radiologist%20possesses%20knowledge%20about%20the%20potential%20diseases%20that%20may%0Abe%20present%20in%20the%20images%20and%2C%20when%20searching%2C%20follows%20a%20mental%20checklist%20to%0Alocate%20them%20using%20their%20gaze.%20This%20is%20a%20key%20observation%2C%20yet%20existing%20models%0Afail%20to%20capture%20the%20underlying%20intent%20behind%20each%20fixation.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20deep%20learning-based%20approach%2C%20RadGazeIntent%2C%20designed%20to%20model%20this%0Abehavior%3A%20having%20an%20intention%20to%20find%20something%20and%20actively%20searching%20for%20it.%0AOur%20transformer-based%20architecture%20processes%20both%20the%20temporal%20and%20spatial%0Adimensions%20of%20gaze%20data%2C%20transforming%20fine-grained%20fixation%20features%20into%0Acoarse%2C%20meaningful%20representations%20of%20diagnostic%20intent%20to%20interpret%0Aradiologists%27%20goals.%20To%20capture%20the%20nuances%20of%20radiologists%27%20varied%0Aintention-driven%20behaviors%2C%20we%20process%20existing%20medical%20eye-tracking%20datasets%0Ato%20create%20three%20intention-labeled%20subsets%3A%20RadSeq%20%28Systematic%20Sequential%0ASearch%29%2C%20RadExplore%20%28Uncertainty-driven%20Exploration%29%2C%20and%20RadHybrid%20%28Hybrid%0APattern%29.%20Experimental%20results%20demonstrate%20RadGazeIntent%27s%20ability%20to%20predict%0Awhich%20findings%20radiologists%20are%20examining%20at%20specific%20moments%2C%20outperforming%0Abaseline%20methods%20across%20all%20intention-labeled%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Radiologist%2527s%2520Intention%2520from%2520Eye%2520Movements%2520in%2520Chest%2520X-ray%250A%2520%2520Diagnosis%26entry.906535625%3DTrong-Thang%2520Pham%2520and%2520Anh%2520Nguyen%2520and%2520Zhigang%2520Deng%2520and%2520Carol%2520C.%2520Wu%2520and%2520Hien%2520Van%2520Nguyen%2520and%2520Ngan%2520Le%26entry.1292438233%3D%2520%2520Radiologists%2520rely%2520on%2520eye%2520movements%2520to%2520navigate%2520and%2520interpret%2520medical%2520images.%250AA%2520trained%2520radiologist%2520possesses%2520knowledge%2520about%2520the%2520potential%2520diseases%2520that%2520may%250Abe%2520present%2520in%2520the%2520images%2520and%252C%2520when%2520searching%252C%2520follows%2520a%2520mental%2520checklist%2520to%250Alocate%2520them%2520using%2520their%2520gaze.%2520This%2520is%2520a%2520key%2520observation%252C%2520yet%2520existing%2520models%250Afail%2520to%2520capture%2520the%2520underlying%2520intent%2520behind%2520each%2520fixation.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520deep%2520learning-based%2520approach%252C%2520RadGazeIntent%252C%2520designed%2520to%2520model%2520this%250Abehavior%253A%2520having%2520an%2520intention%2520to%2520find%2520something%2520and%2520actively%2520searching%2520for%2520it.%250AOur%2520transformer-based%2520architecture%2520processes%2520both%2520the%2520temporal%2520and%2520spatial%250Adimensions%2520of%2520gaze%2520data%252C%2520transforming%2520fine-grained%2520fixation%2520features%2520into%250Acoarse%252C%2520meaningful%2520representations%2520of%2520diagnostic%2520intent%2520to%2520interpret%250Aradiologists%2527%2520goals.%2520To%2520capture%2520the%2520nuances%2520of%2520radiologists%2527%2520varied%250Aintention-driven%2520behaviors%252C%2520we%2520process%2520existing%2520medical%2520eye-tracking%2520datasets%250Ato%2520create%2520three%2520intention-labeled%2520subsets%253A%2520RadSeq%2520%2528Systematic%2520Sequential%250ASearch%2529%252C%2520RadExplore%2520%2528Uncertainty-driven%2520Exploration%2529%252C%2520and%2520RadHybrid%2520%2528Hybrid%250APattern%2529.%2520Experimental%2520results%2520demonstrate%2520RadGazeIntent%2527s%2520ability%2520to%2520predict%250Awhich%2520findings%2520radiologists%2520are%2520examining%2520at%2520specific%2520moments%252C%2520outperforming%250Abaseline%2520methods%2520across%2520all%2520intention-labeled%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Radiologist%27s%20Intention%20from%20Eye%20Movements%20in%20Chest%20X-ray%0A%20%20Diagnosis&entry.906535625=Trong-Thang%20Pham%20and%20Anh%20Nguyen%20and%20Zhigang%20Deng%20and%20Carol%20C.%20Wu%20and%20Hien%20Van%20Nguyen%20and%20Ngan%20Le&entry.1292438233=%20%20Radiologists%20rely%20on%20eye%20movements%20to%20navigate%20and%20interpret%20medical%20images.%0AA%20trained%20radiologist%20possesses%20knowledge%20about%20the%20potential%20diseases%20that%20may%0Abe%20present%20in%20the%20images%20and%2C%20when%20searching%2C%20follows%20a%20mental%20checklist%20to%0Alocate%20them%20using%20their%20gaze.%20This%20is%20a%20key%20observation%2C%20yet%20existing%20models%0Afail%20to%20capture%20the%20underlying%20intent%20behind%20each%20fixation.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20deep%20learning-based%20approach%2C%20RadGazeIntent%2C%20designed%20to%20model%20this%0Abehavior%3A%20having%20an%20intention%20to%20find%20something%20and%20actively%20searching%20for%20it.%0AOur%20transformer-based%20architecture%20processes%20both%20the%20temporal%20and%20spatial%0Adimensions%20of%20gaze%20data%2C%20transforming%20fine-grained%20fixation%20features%20into%0Acoarse%2C%20meaningful%20representations%20of%20diagnostic%20intent%20to%20interpret%0Aradiologists%27%20goals.%20To%20capture%20the%20nuances%20of%20radiologists%27%20varied%0Aintention-driven%20behaviors%2C%20we%20process%20existing%20medical%20eye-tracking%20datasets%0Ato%20create%20three%20intention-labeled%20subsets%3A%20RadSeq%20%28Systematic%20Sequential%0ASearch%29%2C%20RadExplore%20%28Uncertainty-driven%20Exploration%29%2C%20and%20RadHybrid%20%28Hybrid%0APattern%29.%20Experimental%20results%20demonstrate%20RadGazeIntent%27s%20ability%20to%20predict%0Awhich%20findings%20radiologists%20are%20examining%20at%20specific%20moments%2C%20outperforming%0Abaseline%20methods%20across%20all%20intention-labeled%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12461v1&entry.124074799=Read"},
{"title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt\n  Online Learning", "author": "M. Anwar Ma'sum and Mahardhika Pratama and Savitha Ramasamy and Lin Liu and Habibullah Habibullah and Ryszard Kowalczyk", "abstract": "  The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.\n", "link": "http://arxiv.org/abs/2507.12305v1", "date": "2025-07-16", "relevancy": 2.4895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROL%20%3A%20Rehearsal%20Free%20Continual%20Learning%20in%20Streaming%20Data%20via%20Prompt%0A%20%20Online%20Learning&body=Title%3A%20PROL%20%3A%20Rehearsal%20Free%20Continual%20Learning%20in%20Streaming%20Data%20via%20Prompt%0A%20%20Online%20Learning%0AAuthor%3A%20M.%20Anwar%20Ma%27sum%20and%20Mahardhika%20Pratama%20and%20Savitha%20Ramasamy%20and%20Lin%20Liu%20and%20Habibullah%20Habibullah%20and%20Ryszard%20Kowalczyk%0AAbstract%3A%20%20%20The%20data%20privacy%20constraint%20in%20online%20continual%20learning%20%28OCL%29%2C%20where%20the%0Adata%20can%20be%20seen%20only%20once%2C%20complicates%20the%20catastrophic%20forgetting%20problem%20in%0Astreaming%20data.%20A%20common%20approach%20applied%20by%20the%20current%20SOTAs%20in%20OCL%20is%20with%0Athe%20use%20of%20memory%20saving%20exemplars%20or%20features%20from%20previous%20classes%20to%20be%0Areplayed%20in%20the%20current%20task.%20On%20the%20other%20hand%2C%20the%20prompt-based%20approach%0Aperforms%20excellently%20in%20continual%20learning%20but%20with%20the%20cost%20of%20a%20growing%0Anumber%20of%20trainable%20parameters.%20The%20first%20approach%20may%20not%20be%20applicable%20in%0Apractice%20due%20to%20data%20openness%20policy%2C%20while%20the%20second%20approach%20has%20the%20issue%0Aof%20throughput%20associated%20with%20the%20streaming%20data.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20prompt-based%20method%20for%20online%20continual%20learning%20that%20includes%204%20main%0Acomponents%3A%20%281%29%20single%20light-weight%20prompt%20generator%20as%20a%20general%20knowledge%2C%0A%282%29%20trainable%20scaler-and-shifter%20as%20specific%20knowledge%2C%20%283%29%20pre-trained%20model%0A%28PTM%29%20generalization%20preserving%2C%20and%20%284%29%20hard-soft%20updates%20mechanism.%20Our%0Aproposed%20method%20achieves%20significantly%20higher%20performance%20than%20the%20current%0ASOTAs%20in%20CIFAR100%2C%20ImageNet-R%2C%20ImageNet-A%2C%20and%20CUB%20dataset.%20Our%20complexity%0Aanalysis%20shows%20that%20our%20method%20requires%20a%20relatively%20smaller%20number%20of%0Aparameters%20and%20achieves%20moderate%20training%20time%2C%20inference%20time%2C%20and%20throughput.%0AFor%20further%20study%2C%20the%20source%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/anwarmaxsum/PROL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROL%2520%253A%2520Rehearsal%2520Free%2520Continual%2520Learning%2520in%2520Streaming%2520Data%2520via%2520Prompt%250A%2520%2520Online%2520Learning%26entry.906535625%3DM.%2520Anwar%2520Ma%2527sum%2520and%2520Mahardhika%2520Pratama%2520and%2520Savitha%2520Ramasamy%2520and%2520Lin%2520Liu%2520and%2520Habibullah%2520Habibullah%2520and%2520Ryszard%2520Kowalczyk%26entry.1292438233%3D%2520%2520The%2520data%2520privacy%2520constraint%2520in%2520online%2520continual%2520learning%2520%2528OCL%2529%252C%2520where%2520the%250Adata%2520can%2520be%2520seen%2520only%2520once%252C%2520complicates%2520the%2520catastrophic%2520forgetting%2520problem%2520in%250Astreaming%2520data.%2520A%2520common%2520approach%2520applied%2520by%2520the%2520current%2520SOTAs%2520in%2520OCL%2520is%2520with%250Athe%2520use%2520of%2520memory%2520saving%2520exemplars%2520or%2520features%2520from%2520previous%2520classes%2520to%2520be%250Areplayed%2520in%2520the%2520current%2520task.%2520On%2520the%2520other%2520hand%252C%2520the%2520prompt-based%2520approach%250Aperforms%2520excellently%2520in%2520continual%2520learning%2520but%2520with%2520the%2520cost%2520of%2520a%2520growing%250Anumber%2520of%2520trainable%2520parameters.%2520The%2520first%2520approach%2520may%2520not%2520be%2520applicable%2520in%250Apractice%2520due%2520to%2520data%2520openness%2520policy%252C%2520while%2520the%2520second%2520approach%2520has%2520the%2520issue%250Aof%2520throughput%2520associated%2520with%2520the%2520streaming%2520data.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Anovel%2520prompt-based%2520method%2520for%2520online%2520continual%2520learning%2520that%2520includes%25204%2520main%250Acomponents%253A%2520%25281%2529%2520single%2520light-weight%2520prompt%2520generator%2520as%2520a%2520general%2520knowledge%252C%250A%25282%2529%2520trainable%2520scaler-and-shifter%2520as%2520specific%2520knowledge%252C%2520%25283%2529%2520pre-trained%2520model%250A%2528PTM%2529%2520generalization%2520preserving%252C%2520and%2520%25284%2529%2520hard-soft%2520updates%2520mechanism.%2520Our%250Aproposed%2520method%2520achieves%2520significantly%2520higher%2520performance%2520than%2520the%2520current%250ASOTAs%2520in%2520CIFAR100%252C%2520ImageNet-R%252C%2520ImageNet-A%252C%2520and%2520CUB%2520dataset.%2520Our%2520complexity%250Aanalysis%2520shows%2520that%2520our%2520method%2520requires%2520a%2520relatively%2520smaller%2520number%2520of%250Aparameters%2520and%2520achieves%2520moderate%2520training%2520time%252C%2520inference%2520time%252C%2520and%2520throughput.%250AFor%2520further%2520study%252C%2520the%2520source%2520code%2520of%2520our%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/anwarmaxsum/PROL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROL%20%3A%20Rehearsal%20Free%20Continual%20Learning%20in%20Streaming%20Data%20via%20Prompt%0A%20%20Online%20Learning&entry.906535625=M.%20Anwar%20Ma%27sum%20and%20Mahardhika%20Pratama%20and%20Savitha%20Ramasamy%20and%20Lin%20Liu%20and%20Habibullah%20Habibullah%20and%20Ryszard%20Kowalczyk&entry.1292438233=%20%20The%20data%20privacy%20constraint%20in%20online%20continual%20learning%20%28OCL%29%2C%20where%20the%0Adata%20can%20be%20seen%20only%20once%2C%20complicates%20the%20catastrophic%20forgetting%20problem%20in%0Astreaming%20data.%20A%20common%20approach%20applied%20by%20the%20current%20SOTAs%20in%20OCL%20is%20with%0Athe%20use%20of%20memory%20saving%20exemplars%20or%20features%20from%20previous%20classes%20to%20be%0Areplayed%20in%20the%20current%20task.%20On%20the%20other%20hand%2C%20the%20prompt-based%20approach%0Aperforms%20excellently%20in%20continual%20learning%20but%20with%20the%20cost%20of%20a%20growing%0Anumber%20of%20trainable%20parameters.%20The%20first%20approach%20may%20not%20be%20applicable%20in%0Apractice%20due%20to%20data%20openness%20policy%2C%20while%20the%20second%20approach%20has%20the%20issue%0Aof%20throughput%20associated%20with%20the%20streaming%20data.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20prompt-based%20method%20for%20online%20continual%20learning%20that%20includes%204%20main%0Acomponents%3A%20%281%29%20single%20light-weight%20prompt%20generator%20as%20a%20general%20knowledge%2C%0A%282%29%20trainable%20scaler-and-shifter%20as%20specific%20knowledge%2C%20%283%29%20pre-trained%20model%0A%28PTM%29%20generalization%20preserving%2C%20and%20%284%29%20hard-soft%20updates%20mechanism.%20Our%0Aproposed%20method%20achieves%20significantly%20higher%20performance%20than%20the%20current%0ASOTAs%20in%20CIFAR100%2C%20ImageNet-R%2C%20ImageNet-A%2C%20and%20CUB%20dataset.%20Our%20complexity%0Aanalysis%20shows%20that%20our%20method%20requires%20a%20relatively%20smaller%20number%20of%0Aparameters%20and%20achieves%20moderate%20training%20time%2C%20inference%20time%2C%20and%20throughput.%0AFor%20further%20study%2C%20the%20source%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/anwarmaxsum/PROL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12305v1&entry.124074799=Read"},
{"title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal\n  Data", "author": "Dzung Dinh and Boqi Chen and Marc Niethammer and Junier Oliva", "abstract": "  In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.\n", "link": "http://arxiv.org/abs/2507.12412v1", "date": "2025-07-16", "relevancy": 2.4703, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4933}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOCTA%3A%20Non-Greedy%20Objective%20Cost-Tradeoff%20Acquisition%20for%20Longitudinal%0A%20%20Data&body=Title%3A%20NOCTA%3A%20Non-Greedy%20Objective%20Cost-Tradeoff%20Acquisition%20for%20Longitudinal%0A%20%20Data%0AAuthor%3A%20Dzung%20Dinh%20and%20Boqi%20Chen%20and%20Marc%20Niethammer%20and%20Junier%20Oliva%0AAbstract%3A%20%20%20In%20many%20critical%20applications%2C%20resource%20constraints%20limit%20the%20amount%20of%0Ainformation%20that%20can%20be%20gathered%20to%20make%20predictions.%20For%20example%2C%20in%0Ahealthcare%2C%20patient%20data%20often%20spans%20diverse%20features%20ranging%20from%20lab%20tests%20to%0Aimaging%20studies.%20Each%20feature%20may%20carry%20different%20information%20and%20must%20be%0Aacquired%20at%20a%20respective%20cost%20of%20time%2C%20money%2C%20or%20risk%20to%20the%20patient.%20Moreover%2C%0Atemporal%20prediction%20tasks%2C%20where%20both%20instance%20features%20and%20labels%20evolve%20over%0Atime%2C%20introduce%20additional%20complexity%20in%20deciding%20when%20or%20what%20information%20is%0Aimportant.%20In%20this%20work%2C%20we%20propose%20NOCTA%2C%20a%20Non-Greedy%20Objective%20Cost-Tradeoff%0AAcquisition%20method%20that%20sequentially%20acquires%20the%20most%20informative%20features%20at%0Ainference%20time%20while%20accounting%20for%20both%20temporal%20dynamics%20and%20acquisition%0Acost.%20We%20first%20introduce%20a%20cohesive%20estimation%20target%20for%20our%20NOCTA%20setting%2C%0Aand%20then%20develop%20two%20complementary%20estimators%3A%201%29%20a%20non-parametric%20method%20based%0Aon%20nearest%20neighbors%20to%20guide%20the%20acquisition%20%28NOCTA-NP%29%2C%20and%202%29%20a%20parametric%0Amethod%20that%20directly%20predicts%20the%20utility%20of%20potential%20acquisitions%20%28NOCTA-P%29.%0AExperiments%20on%20synthetic%20and%20real-world%20medical%20datasets%20demonstrate%20that%20both%0ANOCTA%20variants%20outperform%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOCTA%253A%2520Non-Greedy%2520Objective%2520Cost-Tradeoff%2520Acquisition%2520for%2520Longitudinal%250A%2520%2520Data%26entry.906535625%3DDzung%2520Dinh%2520and%2520Boqi%2520Chen%2520and%2520Marc%2520Niethammer%2520and%2520Junier%2520Oliva%26entry.1292438233%3D%2520%2520In%2520many%2520critical%2520applications%252C%2520resource%2520constraints%2520limit%2520the%2520amount%2520of%250Ainformation%2520that%2520can%2520be%2520gathered%2520to%2520make%2520predictions.%2520For%2520example%252C%2520in%250Ahealthcare%252C%2520patient%2520data%2520often%2520spans%2520diverse%2520features%2520ranging%2520from%2520lab%2520tests%2520to%250Aimaging%2520studies.%2520Each%2520feature%2520may%2520carry%2520different%2520information%2520and%2520must%2520be%250Aacquired%2520at%2520a%2520respective%2520cost%2520of%2520time%252C%2520money%252C%2520or%2520risk%2520to%2520the%2520patient.%2520Moreover%252C%250Atemporal%2520prediction%2520tasks%252C%2520where%2520both%2520instance%2520features%2520and%2520labels%2520evolve%2520over%250Atime%252C%2520introduce%2520additional%2520complexity%2520in%2520deciding%2520when%2520or%2520what%2520information%2520is%250Aimportant.%2520In%2520this%2520work%252C%2520we%2520propose%2520NOCTA%252C%2520a%2520Non-Greedy%2520Objective%2520Cost-Tradeoff%250AAcquisition%2520method%2520that%2520sequentially%2520acquires%2520the%2520most%2520informative%2520features%2520at%250Ainference%2520time%2520while%2520accounting%2520for%2520both%2520temporal%2520dynamics%2520and%2520acquisition%250Acost.%2520We%2520first%2520introduce%2520a%2520cohesive%2520estimation%2520target%2520for%2520our%2520NOCTA%2520setting%252C%250Aand%2520then%2520develop%2520two%2520complementary%2520estimators%253A%25201%2529%2520a%2520non-parametric%2520method%2520based%250Aon%2520nearest%2520neighbors%2520to%2520guide%2520the%2520acquisition%2520%2528NOCTA-NP%2529%252C%2520and%25202%2529%2520a%2520parametric%250Amethod%2520that%2520directly%2520predicts%2520the%2520utility%2520of%2520potential%2520acquisitions%2520%2528NOCTA-P%2529.%250AExperiments%2520on%2520synthetic%2520and%2520real-world%2520medical%2520datasets%2520demonstrate%2520that%2520both%250ANOCTA%2520variants%2520outperform%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOCTA%3A%20Non-Greedy%20Objective%20Cost-Tradeoff%20Acquisition%20for%20Longitudinal%0A%20%20Data&entry.906535625=Dzung%20Dinh%20and%20Boqi%20Chen%20and%20Marc%20Niethammer%20and%20Junier%20Oliva&entry.1292438233=%20%20In%20many%20critical%20applications%2C%20resource%20constraints%20limit%20the%20amount%20of%0Ainformation%20that%20can%20be%20gathered%20to%20make%20predictions.%20For%20example%2C%20in%0Ahealthcare%2C%20patient%20data%20often%20spans%20diverse%20features%20ranging%20from%20lab%20tests%20to%0Aimaging%20studies.%20Each%20feature%20may%20carry%20different%20information%20and%20must%20be%0Aacquired%20at%20a%20respective%20cost%20of%20time%2C%20money%2C%20or%20risk%20to%20the%20patient.%20Moreover%2C%0Atemporal%20prediction%20tasks%2C%20where%20both%20instance%20features%20and%20labels%20evolve%20over%0Atime%2C%20introduce%20additional%20complexity%20in%20deciding%20when%20or%20what%20information%20is%0Aimportant.%20In%20this%20work%2C%20we%20propose%20NOCTA%2C%20a%20Non-Greedy%20Objective%20Cost-Tradeoff%0AAcquisition%20method%20that%20sequentially%20acquires%20the%20most%20informative%20features%20at%0Ainference%20time%20while%20accounting%20for%20both%20temporal%20dynamics%20and%20acquisition%0Acost.%20We%20first%20introduce%20a%20cohesive%20estimation%20target%20for%20our%20NOCTA%20setting%2C%0Aand%20then%20develop%20two%20complementary%20estimators%3A%201%29%20a%20non-parametric%20method%20based%0Aon%20nearest%20neighbors%20to%20guide%20the%20acquisition%20%28NOCTA-NP%29%2C%20and%202%29%20a%20parametric%0Amethod%20that%20directly%20predicts%20the%20utility%20of%20potential%20acquisitions%20%28NOCTA-P%29.%0AExperiments%20on%20synthetic%20and%20real-world%20medical%20datasets%20demonstrate%20that%20both%0ANOCTA%20variants%20outperform%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12412v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs.\n  Normal Mitosis Classification with Cross-Dataset Evaluation", "author": "Sweta Banerjee and Viktoria Weiss and Taryn A. Donovan and Rutger H. J. Fick and Thomas Conrad and Jonas Ammeling and Nils Porsche and Robert Klopfleisch and Christopher Kaltenecker and Katharina Breininger and Marc Aubreville and Christof A. Bertram", "abstract": "  Atypical mitosis marks a deviation in the cell division process that has been\nshown be an independent prognostic marker for tumor malignancy. However,\natypical mitosis classification remains challenging due to low prevalence, at\ntimes subtle morphological differences from normal mitotic figures, low\ninter-rater agreement among pathologists, and class imbalance in datasets.\nBuilding on the Atypical Mitosis dataset for Breast Cancer (AMi-Br), this study\npresents a comprehensive benchmark comparing deep learning approaches for\nautomated atypical mitotic figure (AMF) classification, including end-to-end\ntrained deep learning models, foundation models with linear probing, and\nfoundation models fine-tuned with low-rank adaptation (LoRA). For rigorous\nevaluation, we further introduce two new held-out AMF datasets - AtNorM-Br, a\ndataset of mitotic figures from the TCGA breast cancer cohort, and AtNorM-MD, a\nmulti-domain dataset of mitotic figures from a subset of the MIDOG++ training\nset. We found average balanced accuracy values of up to 0.8135, 0.7788, and\n0.7723 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and AtNorM-MD\ndatasets, respectively. Our work shows that atypical mitotic figure\nclassification, while being a challenging problem, can be effectively addressed\nthrough the use of recent advances in transfer learning and model fine-tuning\ntechniques. We make all code and data used in this paper available in this\ngithub repository: https://github.com/DeepMicroscopy/AMi-Br_Benchmark.\n", "link": "http://arxiv.org/abs/2506.21444v2", "date": "2025-07-16", "relevancy": 2.4531, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Learning%20and%20Vision%20Foundation%20Models%20for%20Atypical%20vs.%0A%20%20Normal%20Mitosis%20Classification%20with%20Cross-Dataset%20Evaluation&body=Title%3A%20Benchmarking%20Deep%20Learning%20and%20Vision%20Foundation%20Models%20for%20Atypical%20vs.%0A%20%20Normal%20Mitosis%20Classification%20with%20Cross-Dataset%20Evaluation%0AAuthor%3A%20Sweta%20Banerjee%20and%20Viktoria%20Weiss%20and%20Taryn%20A.%20Donovan%20and%20Rutger%20H.%20J.%20Fick%20and%20Thomas%20Conrad%20and%20Jonas%20Ammeling%20and%20Nils%20Porsche%20and%20Robert%20Klopfleisch%20and%20Christopher%20Kaltenecker%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville%20and%20Christof%20A.%20Bertram%0AAbstract%3A%20%20%20Atypical%20mitosis%20marks%20a%20deviation%20in%20the%20cell%20division%20process%20that%20has%20been%0Ashown%20be%20an%20independent%20prognostic%20marker%20for%20tumor%20malignancy.%20However%2C%0Aatypical%20mitosis%20classification%20remains%20challenging%20due%20to%20low%20prevalence%2C%20at%0Atimes%20subtle%20morphological%20differences%20from%20normal%20mitotic%20figures%2C%20low%0Ainter-rater%20agreement%20among%20pathologists%2C%20and%20class%20imbalance%20in%20datasets.%0ABuilding%20on%20the%20Atypical%20Mitosis%20dataset%20for%20Breast%20Cancer%20%28AMi-Br%29%2C%20this%20study%0Apresents%20a%20comprehensive%20benchmark%20comparing%20deep%20learning%20approaches%20for%0Aautomated%20atypical%20mitotic%20figure%20%28AMF%29%20classification%2C%20including%20end-to-end%0Atrained%20deep%20learning%20models%2C%20foundation%20models%20with%20linear%20probing%2C%20and%0Afoundation%20models%20fine-tuned%20with%20low-rank%20adaptation%20%28LoRA%29.%20For%20rigorous%0Aevaluation%2C%20we%20further%20introduce%20two%20new%20held-out%20AMF%20datasets%20-%20AtNorM-Br%2C%20a%0Adataset%20of%20mitotic%20figures%20from%20the%20TCGA%20breast%20cancer%20cohort%2C%20and%20AtNorM-MD%2C%20a%0Amulti-domain%20dataset%20of%20mitotic%20figures%20from%20a%20subset%20of%20the%20MIDOG%2B%2B%20training%0Aset.%20We%20found%20average%20balanced%20accuracy%20values%20of%20up%20to%200.8135%2C%200.7788%2C%20and%0A0.7723%20on%20the%20in-domain%20AMi-Br%20and%20the%20out-of-domain%20AtNorm-Br%20and%20AtNorM-MD%0Adatasets%2C%20respectively.%20Our%20work%20shows%20that%20atypical%20mitotic%20figure%0Aclassification%2C%20while%20being%20a%20challenging%20problem%2C%20can%20be%20effectively%20addressed%0Athrough%20the%20use%20of%20recent%20advances%20in%20transfer%20learning%20and%20model%20fine-tuning%0Atechniques.%20We%20make%20all%20code%20and%20data%20used%20in%20this%20paper%20available%20in%20this%0Agithub%20repository%3A%20https%3A//github.com/DeepMicroscopy/AMi-Br_Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Deep%2520Learning%2520and%2520Vision%2520Foundation%2520Models%2520for%2520Atypical%2520vs.%250A%2520%2520Normal%2520Mitosis%2520Classification%2520with%2520Cross-Dataset%2520Evaluation%26entry.906535625%3DSweta%2520Banerjee%2520and%2520Viktoria%2520Weiss%2520and%2520Taryn%2520A.%2520Donovan%2520and%2520Rutger%2520H.%2520J.%2520Fick%2520and%2520Thomas%2520Conrad%2520and%2520Jonas%2520Ammeling%2520and%2520Nils%2520Porsche%2520and%2520Robert%2520Klopfleisch%2520and%2520Christopher%2520Kaltenecker%2520and%2520Katharina%2520Breininger%2520and%2520Marc%2520Aubreville%2520and%2520Christof%2520A.%2520Bertram%26entry.1292438233%3D%2520%2520Atypical%2520mitosis%2520marks%2520a%2520deviation%2520in%2520the%2520cell%2520division%2520process%2520that%2520has%2520been%250Ashown%2520be%2520an%2520independent%2520prognostic%2520marker%2520for%2520tumor%2520malignancy.%2520However%252C%250Aatypical%2520mitosis%2520classification%2520remains%2520challenging%2520due%2520to%2520low%2520prevalence%252C%2520at%250Atimes%2520subtle%2520morphological%2520differences%2520from%2520normal%2520mitotic%2520figures%252C%2520low%250Ainter-rater%2520agreement%2520among%2520pathologists%252C%2520and%2520class%2520imbalance%2520in%2520datasets.%250ABuilding%2520on%2520the%2520Atypical%2520Mitosis%2520dataset%2520for%2520Breast%2520Cancer%2520%2528AMi-Br%2529%252C%2520this%2520study%250Apresents%2520a%2520comprehensive%2520benchmark%2520comparing%2520deep%2520learning%2520approaches%2520for%250Aautomated%2520atypical%2520mitotic%2520figure%2520%2528AMF%2529%2520classification%252C%2520including%2520end-to-end%250Atrained%2520deep%2520learning%2520models%252C%2520foundation%2520models%2520with%2520linear%2520probing%252C%2520and%250Afoundation%2520models%2520fine-tuned%2520with%2520low-rank%2520adaptation%2520%2528LoRA%2529.%2520For%2520rigorous%250Aevaluation%252C%2520we%2520further%2520introduce%2520two%2520new%2520held-out%2520AMF%2520datasets%2520-%2520AtNorM-Br%252C%2520a%250Adataset%2520of%2520mitotic%2520figures%2520from%2520the%2520TCGA%2520breast%2520cancer%2520cohort%252C%2520and%2520AtNorM-MD%252C%2520a%250Amulti-domain%2520dataset%2520of%2520mitotic%2520figures%2520from%2520a%2520subset%2520of%2520the%2520MIDOG%252B%252B%2520training%250Aset.%2520We%2520found%2520average%2520balanced%2520accuracy%2520values%2520of%2520up%2520to%25200.8135%252C%25200.7788%252C%2520and%250A0.7723%2520on%2520the%2520in-domain%2520AMi-Br%2520and%2520the%2520out-of-domain%2520AtNorm-Br%2520and%2520AtNorM-MD%250Adatasets%252C%2520respectively.%2520Our%2520work%2520shows%2520that%2520atypical%2520mitotic%2520figure%250Aclassification%252C%2520while%2520being%2520a%2520challenging%2520problem%252C%2520can%2520be%2520effectively%2520addressed%250Athrough%2520the%2520use%2520of%2520recent%2520advances%2520in%2520transfer%2520learning%2520and%2520model%2520fine-tuning%250Atechniques.%2520We%2520make%2520all%2520code%2520and%2520data%2520used%2520in%2520this%2520paper%2520available%2520in%2520this%250Agithub%2520repository%253A%2520https%253A//github.com/DeepMicroscopy/AMi-Br_Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Learning%20and%20Vision%20Foundation%20Models%20for%20Atypical%20vs.%0A%20%20Normal%20Mitosis%20Classification%20with%20Cross-Dataset%20Evaluation&entry.906535625=Sweta%20Banerjee%20and%20Viktoria%20Weiss%20and%20Taryn%20A.%20Donovan%20and%20Rutger%20H.%20J.%20Fick%20and%20Thomas%20Conrad%20and%20Jonas%20Ammeling%20and%20Nils%20Porsche%20and%20Robert%20Klopfleisch%20and%20Christopher%20Kaltenecker%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville%20and%20Christof%20A.%20Bertram&entry.1292438233=%20%20Atypical%20mitosis%20marks%20a%20deviation%20in%20the%20cell%20division%20process%20that%20has%20been%0Ashown%20be%20an%20independent%20prognostic%20marker%20for%20tumor%20malignancy.%20However%2C%0Aatypical%20mitosis%20classification%20remains%20challenging%20due%20to%20low%20prevalence%2C%20at%0Atimes%20subtle%20morphological%20differences%20from%20normal%20mitotic%20figures%2C%20low%0Ainter-rater%20agreement%20among%20pathologists%2C%20and%20class%20imbalance%20in%20datasets.%0ABuilding%20on%20the%20Atypical%20Mitosis%20dataset%20for%20Breast%20Cancer%20%28AMi-Br%29%2C%20this%20study%0Apresents%20a%20comprehensive%20benchmark%20comparing%20deep%20learning%20approaches%20for%0Aautomated%20atypical%20mitotic%20figure%20%28AMF%29%20classification%2C%20including%20end-to-end%0Atrained%20deep%20learning%20models%2C%20foundation%20models%20with%20linear%20probing%2C%20and%0Afoundation%20models%20fine-tuned%20with%20low-rank%20adaptation%20%28LoRA%29.%20For%20rigorous%0Aevaluation%2C%20we%20further%20introduce%20two%20new%20held-out%20AMF%20datasets%20-%20AtNorM-Br%2C%20a%0Adataset%20of%20mitotic%20figures%20from%20the%20TCGA%20breast%20cancer%20cohort%2C%20and%20AtNorM-MD%2C%20a%0Amulti-domain%20dataset%20of%20mitotic%20figures%20from%20a%20subset%20of%20the%20MIDOG%2B%2B%20training%0Aset.%20We%20found%20average%20balanced%20accuracy%20values%20of%20up%20to%200.8135%2C%200.7788%2C%20and%0A0.7723%20on%20the%20in-domain%20AMi-Br%20and%20the%20out-of-domain%20AtNorm-Br%20and%20AtNorM-MD%0Adatasets%2C%20respectively.%20Our%20work%20shows%20that%20atypical%20mitotic%20figure%0Aclassification%2C%20while%20being%20a%20challenging%20problem%2C%20can%20be%20effectively%20addressed%0Athrough%20the%20use%20of%20recent%20advances%20in%20transfer%20learning%20and%20model%20fine-tuning%0Atechniques.%20We%20make%20all%20code%20and%20data%20used%20in%20this%20paper%20available%20in%20this%0Agithub%20repository%3A%20https%3A//github.com/DeepMicroscopy/AMi-Br_Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21444v2&entry.124074799=Read"},
{"title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "author": "Yuxi Xiao and Jianyuan Wang and Nan Xue and Nikita Karaev and Yuri Makarov and Bingyi Kang and Xing Zhu and Hujun Bao and Yujun Shen and Xiaowei Zhou", "abstract": "  We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.\n", "link": "http://arxiv.org/abs/2507.12462v1", "date": "2025-07-16", "relevancy": 2.4517, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6193}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialTrackerV2%3A%203D%20Point%20Tracking%20Made%20Easy&body=Title%3A%20SpatialTrackerV2%3A%203D%20Point%20Tracking%20Made%20Easy%0AAuthor%3A%20Yuxi%20Xiao%20and%20Jianyuan%20Wang%20and%20Nan%20Xue%20and%20Nikita%20Karaev%20and%20Yuri%20Makarov%20and%20Bingyi%20Kang%20and%20Xing%20Zhu%20and%20Hujun%20Bao%20and%20Yujun%20Shen%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20We%20present%20SpatialTrackerV2%2C%20a%20feed-forward%203D%20point%20tracking%20method%20for%0Amonocular%20videos.%20Going%20beyond%20modular%20pipelines%20built%20on%20off-the-shelf%0Acomponents%20for%203D%20tracking%2C%20our%20approach%20unifies%20the%20intrinsic%20connections%0Abetween%20point%20tracking%2C%20monocular%20depth%2C%20and%20camera%20pose%20estimation%20into%20a%0Ahigh-performing%20and%20feedforward%203D%20point%20tracker.%20It%20decomposes%20world-space%203D%0Amotion%20into%20scene%20geometry%2C%20camera%20ego-motion%2C%20and%20pixel-wise%20object%20motion%2C%0Awith%20a%20fully%20differentiable%20and%20end-to-end%20architecture%2C%20allowing%20scalable%0Atraining%20across%20a%20wide%20range%20of%20datasets%2C%20including%20synthetic%20sequences%2C%20posed%0ARGB-D%20videos%2C%20and%20unlabeled%20in-the-wild%20footage.%20By%20learning%20geometry%20and%0Amotion%20jointly%20from%20such%20heterogeneous%20data%2C%20SpatialTrackerV2%20outperforms%0Aexisting%203D%20tracking%20methods%20by%2030%25%2C%20and%20matches%20the%20accuracy%20of%20leading%0Adynamic%203D%20reconstruction%20approaches%20while%20running%2050%24%5Ctimes%24%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialTrackerV2%253A%25203D%2520Point%2520Tracking%2520Made%2520Easy%26entry.906535625%3DYuxi%2520Xiao%2520and%2520Jianyuan%2520Wang%2520and%2520Nan%2520Xue%2520and%2520Nikita%2520Karaev%2520and%2520Yuri%2520Makarov%2520and%2520Bingyi%2520Kang%2520and%2520Xing%2520Zhu%2520and%2520Hujun%2520Bao%2520and%2520Yujun%2520Shen%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520SpatialTrackerV2%252C%2520a%2520feed-forward%25203D%2520point%2520tracking%2520method%2520for%250Amonocular%2520videos.%2520Going%2520beyond%2520modular%2520pipelines%2520built%2520on%2520off-the-shelf%250Acomponents%2520for%25203D%2520tracking%252C%2520our%2520approach%2520unifies%2520the%2520intrinsic%2520connections%250Abetween%2520point%2520tracking%252C%2520monocular%2520depth%252C%2520and%2520camera%2520pose%2520estimation%2520into%2520a%250Ahigh-performing%2520and%2520feedforward%25203D%2520point%2520tracker.%2520It%2520decomposes%2520world-space%25203D%250Amotion%2520into%2520scene%2520geometry%252C%2520camera%2520ego-motion%252C%2520and%2520pixel-wise%2520object%2520motion%252C%250Awith%2520a%2520fully%2520differentiable%2520and%2520end-to-end%2520architecture%252C%2520allowing%2520scalable%250Atraining%2520across%2520a%2520wide%2520range%2520of%2520datasets%252C%2520including%2520synthetic%2520sequences%252C%2520posed%250ARGB-D%2520videos%252C%2520and%2520unlabeled%2520in-the-wild%2520footage.%2520By%2520learning%2520geometry%2520and%250Amotion%2520jointly%2520from%2520such%2520heterogeneous%2520data%252C%2520SpatialTrackerV2%2520outperforms%250Aexisting%25203D%2520tracking%2520methods%2520by%252030%2525%252C%2520and%2520matches%2520the%2520accuracy%2520of%2520leading%250Adynamic%25203D%2520reconstruction%2520approaches%2520while%2520running%252050%2524%255Ctimes%2524%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialTrackerV2%3A%203D%20Point%20Tracking%20Made%20Easy&entry.906535625=Yuxi%20Xiao%20and%20Jianyuan%20Wang%20and%20Nan%20Xue%20and%20Nikita%20Karaev%20and%20Yuri%20Makarov%20and%20Bingyi%20Kang%20and%20Xing%20Zhu%20and%20Hujun%20Bao%20and%20Yujun%20Shen%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20We%20present%20SpatialTrackerV2%2C%20a%20feed-forward%203D%20point%20tracking%20method%20for%0Amonocular%20videos.%20Going%20beyond%20modular%20pipelines%20built%20on%20off-the-shelf%0Acomponents%20for%203D%20tracking%2C%20our%20approach%20unifies%20the%20intrinsic%20connections%0Abetween%20point%20tracking%2C%20monocular%20depth%2C%20and%20camera%20pose%20estimation%20into%20a%0Ahigh-performing%20and%20feedforward%203D%20point%20tracker.%20It%20decomposes%20world-space%203D%0Amotion%20into%20scene%20geometry%2C%20camera%20ego-motion%2C%20and%20pixel-wise%20object%20motion%2C%0Awith%20a%20fully%20differentiable%20and%20end-to-end%20architecture%2C%20allowing%20scalable%0Atraining%20across%20a%20wide%20range%20of%20datasets%2C%20including%20synthetic%20sequences%2C%20posed%0ARGB-D%20videos%2C%20and%20unlabeled%20in-the-wild%20footage.%20By%20learning%20geometry%20and%0Amotion%20jointly%20from%20such%20heterogeneous%20data%2C%20SpatialTrackerV2%20outperforms%0Aexisting%203D%20tracking%20methods%20by%2030%25%2C%20and%20matches%20the%20accuracy%20of%20leading%0Adynamic%203D%20reconstruction%20approaches%20while%20running%2050%24%5Ctimes%24%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12462v1&entry.124074799=Read"},
{"title": "Cluster Contrast for Unsupervised Visual Representation Learning", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.\n", "link": "http://arxiv.org/abs/2507.12359v1", "date": "2025-07-16", "relevancy": 2.4347, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5332}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning&body=Title%3A%20Cluster%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20We%20introduce%20Cluster%20Contrast%20%28CueCo%29%2C%20a%20novel%20approach%20to%20unsupervised%0Avisual%20representation%20learning%20that%20effectively%20combines%20the%20strengths%20of%0Acontrastive%20learning%20and%20clustering%20methods.%20Inspired%20by%20recent%20advancements%2C%0ACueCo%20is%20designed%20to%20simultaneously%20scatter%20and%20align%20feature%20representations%0Awithin%20the%20feature%20space.%20This%20method%20utilizes%20two%20neural%20networks%2C%20a%20query%20and%0Aa%20key%2C%20where%20the%20key%20network%20is%20updated%20through%20a%20slow-moving%20average%20of%20the%0Aquery%20outputs.%20CueCo%20employs%20a%20contrastive%20loss%20to%20push%20dissimilar%20features%0Aapart%2C%20enhancing%20inter-class%20separation%2C%20and%20a%20clustering%20objective%20to%20pull%0Atogether%20features%20of%20the%20same%20cluster%2C%20promoting%20intra-class%20compactness.%20Our%0Amethod%20achieves%2091.40%25%20top-1%20classification%20accuracy%20on%20CIFAR-10%2C%2068.56%25%20on%0ACIFAR-100%2C%20and%2078.65%25%20on%20ImageNet-100%20using%20linear%20evaluation%20with%20a%20ResNet-18%0Abackbone.%20By%20integrating%20contrastive%20learning%20with%20clustering%2C%20CueCo%20sets%20a%20new%0Adirection%20for%20advancing%20unsupervised%20visual%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster%2520Contrast%2520for%2520Unsupervised%2520Visual%2520Representation%2520Learning%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cluster%2520Contrast%2520%2528CueCo%2529%252C%2520a%2520novel%2520approach%2520to%2520unsupervised%250Avisual%2520representation%2520learning%2520that%2520effectively%2520combines%2520the%2520strengths%2520of%250Acontrastive%2520learning%2520and%2520clustering%2520methods.%2520Inspired%2520by%2520recent%2520advancements%252C%250ACueCo%2520is%2520designed%2520to%2520simultaneously%2520scatter%2520and%2520align%2520feature%2520representations%250Awithin%2520the%2520feature%2520space.%2520This%2520method%2520utilizes%2520two%2520neural%2520networks%252C%2520a%2520query%2520and%250Aa%2520key%252C%2520where%2520the%2520key%2520network%2520is%2520updated%2520through%2520a%2520slow-moving%2520average%2520of%2520the%250Aquery%2520outputs.%2520CueCo%2520employs%2520a%2520contrastive%2520loss%2520to%2520push%2520dissimilar%2520features%250Aapart%252C%2520enhancing%2520inter-class%2520separation%252C%2520and%2520a%2520clustering%2520objective%2520to%2520pull%250Atogether%2520features%2520of%2520the%2520same%2520cluster%252C%2520promoting%2520intra-class%2520compactness.%2520Our%250Amethod%2520achieves%252091.40%2525%2520top-1%2520classification%2520accuracy%2520on%2520CIFAR-10%252C%252068.56%2525%2520on%250ACIFAR-100%252C%2520and%252078.65%2525%2520on%2520ImageNet-100%2520using%2520linear%2520evaluation%2520with%2520a%2520ResNet-18%250Abackbone.%2520By%2520integrating%2520contrastive%2520learning%2520with%2520clustering%252C%2520CueCo%2520sets%2520a%2520new%250Adirection%2520for%2520advancing%2520unsupervised%2520visual%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20We%20introduce%20Cluster%20Contrast%20%28CueCo%29%2C%20a%20novel%20approach%20to%20unsupervised%0Avisual%20representation%20learning%20that%20effectively%20combines%20the%20strengths%20of%0Acontrastive%20learning%20and%20clustering%20methods.%20Inspired%20by%20recent%20advancements%2C%0ACueCo%20is%20designed%20to%20simultaneously%20scatter%20and%20align%20feature%20representations%0Awithin%20the%20feature%20space.%20This%20method%20utilizes%20two%20neural%20networks%2C%20a%20query%20and%0Aa%20key%2C%20where%20the%20key%20network%20is%20updated%20through%20a%20slow-moving%20average%20of%20the%0Aquery%20outputs.%20CueCo%20employs%20a%20contrastive%20loss%20to%20push%20dissimilar%20features%0Aapart%2C%20enhancing%20inter-class%20separation%2C%20and%20a%20clustering%20objective%20to%20pull%0Atogether%20features%20of%20the%20same%20cluster%2C%20promoting%20intra-class%20compactness.%20Our%0Amethod%20achieves%2091.40%25%20top-1%20classification%20accuracy%20on%20CIFAR-10%2C%2068.56%25%20on%0ACIFAR-100%2C%20and%2078.65%25%20on%20ImageNet-100%20using%20linear%20evaluation%20with%20a%20ResNet-18%0Abackbone.%20By%20integrating%20contrastive%20learning%20with%20clustering%2C%20CueCo%20sets%20a%20new%0Adirection%20for%20advancing%20unsupervised%20visual%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12359v1&entry.124074799=Read"},
{"title": "Mixture of Raytraced Experts", "author": "Andrea Perin and Giacomo Lagomarsini and Claudio Gallicchio and Giuseppe Nuti", "abstract": "  We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing\n", "link": "http://arxiv.org/abs/2507.12419v1", "date": "2025-07-16", "relevancy": 2.4137, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4814}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Raytraced%20Experts&body=Title%3A%20Mixture%20of%20Raytraced%20Experts%0AAuthor%3A%20Andrea%20Perin%20and%20Giacomo%20Lagomarsini%20and%20Claudio%20Gallicchio%20and%20Giuseppe%20Nuti%0AAbstract%3A%20%20%20We%20introduce%20a%20Mixture%20of%20Raytraced%20Experts%2C%20a%20stacked%20Mixture%20of%20Experts%0A%28MoE%29%20architecture%20which%20can%20dynamically%20select%20sequences%20of%20experts%2C%20producing%0Acomputational%20graphs%20of%20variable%20width%20and%20depth.%20Existing%20MoE%20architectures%0Agenerally%20require%20a%20fixed%20amount%20of%20computation%20for%20a%20given%20sample.%20Our%0Aapproach%2C%20in%20contrast%2C%20yields%20predictions%20with%20increasing%20accuracy%20as%20the%0Acomputation%20cycles%20through%20the%20experts%27%20sequence.%20We%20train%20our%20model%20by%0Aiteratively%20sampling%20from%20a%20set%20of%20candidate%20experts%2C%20unfolding%20the%20sequence%0Aakin%20to%20how%20Recurrent%20Neural%20Networks%20are%20trained.%20Our%20method%20does%20not%20require%0Aload-balancing%20mechanisms%2C%20and%20preliminary%20experiments%20show%20a%20reduction%20in%0Atraining%20epochs%20of%2010%5C%25%20to%2040%5C%25%20with%20a%20comparable/higher%20accuracy.%20These%0Aresults%20point%20to%20new%20research%20directions%20in%20the%20field%20of%20MoEs%2C%20allowing%20the%0Adesign%20of%20potentially%20faster%20and%20more%20expressive%20models.%20The%20code%20is%20available%0Aat%20https%3A//github.com/nutig/RayTracing%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Raytraced%2520Experts%26entry.906535625%3DAndrea%2520Perin%2520and%2520Giacomo%2520Lagomarsini%2520and%2520Claudio%2520Gallicchio%2520and%2520Giuseppe%2520Nuti%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520Mixture%2520of%2520Raytraced%2520Experts%252C%2520a%2520stacked%2520Mixture%2520of%2520Experts%250A%2528MoE%2529%2520architecture%2520which%2520can%2520dynamically%2520select%2520sequences%2520of%2520experts%252C%2520producing%250Acomputational%2520graphs%2520of%2520variable%2520width%2520and%2520depth.%2520Existing%2520MoE%2520architectures%250Agenerally%2520require%2520a%2520fixed%2520amount%2520of%2520computation%2520for%2520a%2520given%2520sample.%2520Our%250Aapproach%252C%2520in%2520contrast%252C%2520yields%2520predictions%2520with%2520increasing%2520accuracy%2520as%2520the%250Acomputation%2520cycles%2520through%2520the%2520experts%2527%2520sequence.%2520We%2520train%2520our%2520model%2520by%250Aiteratively%2520sampling%2520from%2520a%2520set%2520of%2520candidate%2520experts%252C%2520unfolding%2520the%2520sequence%250Aakin%2520to%2520how%2520Recurrent%2520Neural%2520Networks%2520are%2520trained.%2520Our%2520method%2520does%2520not%2520require%250Aload-balancing%2520mechanisms%252C%2520and%2520preliminary%2520experiments%2520show%2520a%2520reduction%2520in%250Atraining%2520epochs%2520of%252010%255C%2525%2520to%252040%255C%2525%2520with%2520a%2520comparable/higher%2520accuracy.%2520These%250Aresults%2520point%2520to%2520new%2520research%2520directions%2520in%2520the%2520field%2520of%2520MoEs%252C%2520allowing%2520the%250Adesign%2520of%2520potentially%2520faster%2520and%2520more%2520expressive%2520models.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/nutig/RayTracing%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Raytraced%20Experts&entry.906535625=Andrea%20Perin%20and%20Giacomo%20Lagomarsini%20and%20Claudio%20Gallicchio%20and%20Giuseppe%20Nuti&entry.1292438233=%20%20We%20introduce%20a%20Mixture%20of%20Raytraced%20Experts%2C%20a%20stacked%20Mixture%20of%20Experts%0A%28MoE%29%20architecture%20which%20can%20dynamically%20select%20sequences%20of%20experts%2C%20producing%0Acomputational%20graphs%20of%20variable%20width%20and%20depth.%20Existing%20MoE%20architectures%0Agenerally%20require%20a%20fixed%20amount%20of%20computation%20for%20a%20given%20sample.%20Our%0Aapproach%2C%20in%20contrast%2C%20yields%20predictions%20with%20increasing%20accuracy%20as%20the%0Acomputation%20cycles%20through%20the%20experts%27%20sequence.%20We%20train%20our%20model%20by%0Aiteratively%20sampling%20from%20a%20set%20of%20candidate%20experts%2C%20unfolding%20the%20sequence%0Aakin%20to%20how%20Recurrent%20Neural%20Networks%20are%20trained.%20Our%20method%20does%20not%20require%0Aload-balancing%20mechanisms%2C%20and%20preliminary%20experiments%20show%20a%20reduction%20in%0Atraining%20epochs%20of%2010%5C%25%20to%2040%5C%25%20with%20a%20comparable/higher%20accuracy.%20These%0Aresults%20point%20to%20new%20research%20directions%20in%20the%20field%20of%20MoEs%2C%20allowing%20the%0Adesign%20of%20potentially%20faster%20and%20more%20expressive%20models.%20The%20code%20is%20available%0Aat%20https%3A//github.com/nutig/RayTracing%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12419v1&entry.124074799=Read"},
{"title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance\n  Scenarios", "author": "Van-Hoang-Anh Phan and Chi-Tam Nguyen and Doan-Trung Au and Thanh-Danh Phan and Minh-Thien Duong and My-Ha Le", "abstract": "  Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8\n", "link": "http://arxiv.org/abs/2507.12449v1", "date": "2025-07-16", "relevancy": 2.4093, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6064}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-based%20Perception%20for%20Autonomous%20Vehicles%20in%20Obstacle%20Avoidance%0A%20%20Scenarios&body=Title%3A%20Vision-based%20Perception%20for%20Autonomous%20Vehicles%20in%20Obstacle%20Avoidance%0A%20%20Scenarios%0AAuthor%3A%20Van-Hoang-Anh%20Phan%20and%20Chi-Tam%20Nguyen%20and%20Doan-Trung%20Au%20and%20Thanh-Danh%20Phan%20and%20Minh-Thien%20Duong%20and%20My-Ha%20Le%0AAbstract%3A%20%20%20Obstacle%20avoidance%20is%20essential%20for%20ensuring%20the%20safety%20of%20autonomous%0Avehicles.%20Accurate%20perception%20and%20motion%20planning%20are%20crucial%20to%20enabling%0Avehicles%20to%20navigate%20complex%20environments%20while%20avoiding%20collisions.%20In%20this%0Apaper%2C%20we%20propose%20an%20efficient%20obstacle%20avoidance%20pipeline%20that%20leverages%20a%0Acamera-only%20perception%20module%20and%20a%20Frenet-Pure%20Pursuit-based%20planning%0Astrategy.%20By%20integrating%20advancements%20in%20computer%20vision%2C%20the%20system%20utilizes%0AYOLOv11%20for%20object%20detection%20and%20state-of-the-art%20monocular%20depth%20estimation%0Amodels%2C%20such%20as%20Depth%20Anything%20V2%2C%20to%20estimate%20object%20distances.%20A%20comparative%0Aanalysis%20of%20these%20models%20provides%20valuable%20insights%20into%20their%20accuracy%2C%0Aefficiency%2C%20and%20robustness%20in%20real-world%20conditions.%20The%20system%20is%20evaluated%20in%0Adiverse%20scenarios%20on%20a%20university%20campus%2C%20demonstrating%20its%20effectiveness%20in%0Ahandling%20various%20obstacles%20and%20enhancing%20autonomous%20navigation.%20The%20video%0Apresenting%20the%20results%20of%20the%20obstacle%20avoidance%20experiments%20is%20available%20at%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DFoXiO5S_tA8%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-based%2520Perception%2520for%2520Autonomous%2520Vehicles%2520in%2520Obstacle%2520Avoidance%250A%2520%2520Scenarios%26entry.906535625%3DVan-Hoang-Anh%2520Phan%2520and%2520Chi-Tam%2520Nguyen%2520and%2520Doan-Trung%2520Au%2520and%2520Thanh-Danh%2520Phan%2520and%2520Minh-Thien%2520Duong%2520and%2520My-Ha%2520Le%26entry.1292438233%3D%2520%2520Obstacle%2520avoidance%2520is%2520essential%2520for%2520ensuring%2520the%2520safety%2520of%2520autonomous%250Avehicles.%2520Accurate%2520perception%2520and%2520motion%2520planning%2520are%2520crucial%2520to%2520enabling%250Avehicles%2520to%2520navigate%2520complex%2520environments%2520while%2520avoiding%2520collisions.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520efficient%2520obstacle%2520avoidance%2520pipeline%2520that%2520leverages%2520a%250Acamera-only%2520perception%2520module%2520and%2520a%2520Frenet-Pure%2520Pursuit-based%2520planning%250Astrategy.%2520By%2520integrating%2520advancements%2520in%2520computer%2520vision%252C%2520the%2520system%2520utilizes%250AYOLOv11%2520for%2520object%2520detection%2520and%2520state-of-the-art%2520monocular%2520depth%2520estimation%250Amodels%252C%2520such%2520as%2520Depth%2520Anything%2520V2%252C%2520to%2520estimate%2520object%2520distances.%2520A%2520comparative%250Aanalysis%2520of%2520these%2520models%2520provides%2520valuable%2520insights%2520into%2520their%2520accuracy%252C%250Aefficiency%252C%2520and%2520robustness%2520in%2520real-world%2520conditions.%2520The%2520system%2520is%2520evaluated%2520in%250Adiverse%2520scenarios%2520on%2520a%2520university%2520campus%252C%2520demonstrating%2520its%2520effectiveness%2520in%250Ahandling%2520various%2520obstacles%2520and%2520enhancing%2520autonomous%2520navigation.%2520The%2520video%250Apresenting%2520the%2520results%2520of%2520the%2520obstacle%2520avoidance%2520experiments%2520is%2520available%2520at%253A%250Ahttps%253A//www.youtube.com/watch%253Fv%253DFoXiO5S_tA8%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20Perception%20for%20Autonomous%20Vehicles%20in%20Obstacle%20Avoidance%0A%20%20Scenarios&entry.906535625=Van-Hoang-Anh%20Phan%20and%20Chi-Tam%20Nguyen%20and%20Doan-Trung%20Au%20and%20Thanh-Danh%20Phan%20and%20Minh-Thien%20Duong%20and%20My-Ha%20Le&entry.1292438233=%20%20Obstacle%20avoidance%20is%20essential%20for%20ensuring%20the%20safety%20of%20autonomous%0Avehicles.%20Accurate%20perception%20and%20motion%20planning%20are%20crucial%20to%20enabling%0Avehicles%20to%20navigate%20complex%20environments%20while%20avoiding%20collisions.%20In%20this%0Apaper%2C%20we%20propose%20an%20efficient%20obstacle%20avoidance%20pipeline%20that%20leverages%20a%0Acamera-only%20perception%20module%20and%20a%20Frenet-Pure%20Pursuit-based%20planning%0Astrategy.%20By%20integrating%20advancements%20in%20computer%20vision%2C%20the%20system%20utilizes%0AYOLOv11%20for%20object%20detection%20and%20state-of-the-art%20monocular%20depth%20estimation%0Amodels%2C%20such%20as%20Depth%20Anything%20V2%2C%20to%20estimate%20object%20distances.%20A%20comparative%0Aanalysis%20of%20these%20models%20provides%20valuable%20insights%20into%20their%20accuracy%2C%0Aefficiency%2C%20and%20robustness%20in%20real-world%20conditions.%20The%20system%20is%20evaluated%20in%0Adiverse%20scenarios%20on%20a%20university%20campus%2C%20demonstrating%20its%20effectiveness%20in%0Ahandling%20various%20obstacles%20and%20enhancing%20autonomous%20navigation.%20The%20video%0Apresenting%20the%20results%20of%20the%20obstacle%20avoidance%20experiments%20is%20available%20at%3A%0Ahttps%3A//www.youtube.com/watch%3Fv%3DFoXiO5S_tA8%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12449v1&entry.124074799=Read"},
{"title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large\n  Language Models for Robotic Path Planning", "author": "Jacinto Colan and Ana Davila and Yasuhisa Hasegawa", "abstract": "  Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.\n", "link": "http://arxiv.org/abs/2507.12391v1", "date": "2025-07-16", "relevancy": 2.3599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Value%20of%20Visual%20Input%3A%20A%20Benchmark%20of%20Multimodal%20Large%0A%20%20Language%20Models%20for%20Robotic%20Path%20Planning&body=Title%3A%20Assessing%20the%20Value%20of%20Visual%20Input%3A%20A%20Benchmark%20of%20Multimodal%20Large%0A%20%20Language%20Models%20for%20Robotic%20Path%20Planning%0AAuthor%3A%20Jacinto%20Colan%20and%20Ana%20Davila%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20show%20potential%20for%20enhancing%20robotic%20path%0Aplanning.%20This%20paper%20assesses%20visual%20input%27s%20utility%20for%20multimodal%20LLMs%20in%0Asuch%20tasks%20via%20a%20comprehensive%20benchmark.%20We%20evaluated%2015%20multimodal%20LLMs%20on%0Agenerating%20valid%20and%20optimal%20paths%20in%202D%20grid%20environments%2C%20simulating%0Asimplified%20robotic%20planning%2C%20comparing%20text-only%20versus%20text-plus-visual%20inputs%0Aacross%20varying%20model%20sizes%20and%20grid%20complexities.%20Our%20results%20indicate%20moderate%0Asuccess%20rates%20on%20simpler%20small%20grids%2C%20where%20visual%20input%20or%20few-shot%20text%0Aprompting%20offered%20some%20benefits.%20However%2C%20performance%20significantly%20degraded%20on%0Alarger%20grids%2C%20highlighting%20a%20scalability%20challenge.%20While%20larger%20models%0Agenerally%20achieved%20higher%20average%20success%2C%20the%20visual%20modality%20was%20not%0Auniversally%20dominant%20over%20well-structured%20text%20for%20these%20multimodal%20systems%2C%0Aand%20successful%20paths%20on%20simpler%20grids%20were%20generally%20of%20high%20quality.%20These%0Aresults%20indicate%20current%20limitations%20in%20robust%20spatial%20reasoning%2C%20constraint%0Aadherence%2C%20and%20scalable%20multimodal%20integration%2C%20identifying%20areas%20for%20future%0ALLM%20development%20in%20robotic%20path%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Value%2520of%2520Visual%2520Input%253A%2520A%2520Benchmark%2520of%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%2520for%2520Robotic%2520Path%2520Planning%26entry.906535625%3DJacinto%2520Colan%2520and%2520Ana%2520Davila%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520potential%2520for%2520enhancing%2520robotic%2520path%250Aplanning.%2520This%2520paper%2520assesses%2520visual%2520input%2527s%2520utility%2520for%2520multimodal%2520LLMs%2520in%250Asuch%2520tasks%2520via%2520a%2520comprehensive%2520benchmark.%2520We%2520evaluated%252015%2520multimodal%2520LLMs%2520on%250Agenerating%2520valid%2520and%2520optimal%2520paths%2520in%25202D%2520grid%2520environments%252C%2520simulating%250Asimplified%2520robotic%2520planning%252C%2520comparing%2520text-only%2520versus%2520text-plus-visual%2520inputs%250Aacross%2520varying%2520model%2520sizes%2520and%2520grid%2520complexities.%2520Our%2520results%2520indicate%2520moderate%250Asuccess%2520rates%2520on%2520simpler%2520small%2520grids%252C%2520where%2520visual%2520input%2520or%2520few-shot%2520text%250Aprompting%2520offered%2520some%2520benefits.%2520However%252C%2520performance%2520significantly%2520degraded%2520on%250Alarger%2520grids%252C%2520highlighting%2520a%2520scalability%2520challenge.%2520While%2520larger%2520models%250Agenerally%2520achieved%2520higher%2520average%2520success%252C%2520the%2520visual%2520modality%2520was%2520not%250Auniversally%2520dominant%2520over%2520well-structured%2520text%2520for%2520these%2520multimodal%2520systems%252C%250Aand%2520successful%2520paths%2520on%2520simpler%2520grids%2520were%2520generally%2520of%2520high%2520quality.%2520These%250Aresults%2520indicate%2520current%2520limitations%2520in%2520robust%2520spatial%2520reasoning%252C%2520constraint%250Aadherence%252C%2520and%2520scalable%2520multimodal%2520integration%252C%2520identifying%2520areas%2520for%2520future%250ALLM%2520development%2520in%2520robotic%2520path%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Value%20of%20Visual%20Input%3A%20A%20Benchmark%20of%20Multimodal%20Large%0A%20%20Language%20Models%20for%20Robotic%20Path%20Planning&entry.906535625=Jacinto%20Colan%20and%20Ana%20Davila%20and%20Yasuhisa%20Hasegawa&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20show%20potential%20for%20enhancing%20robotic%20path%0Aplanning.%20This%20paper%20assesses%20visual%20input%27s%20utility%20for%20multimodal%20LLMs%20in%0Asuch%20tasks%20via%20a%20comprehensive%20benchmark.%20We%20evaluated%2015%20multimodal%20LLMs%20on%0Agenerating%20valid%20and%20optimal%20paths%20in%202D%20grid%20environments%2C%20simulating%0Asimplified%20robotic%20planning%2C%20comparing%20text-only%20versus%20text-plus-visual%20inputs%0Aacross%20varying%20model%20sizes%20and%20grid%20complexities.%20Our%20results%20indicate%20moderate%0Asuccess%20rates%20on%20simpler%20small%20grids%2C%20where%20visual%20input%20or%20few-shot%20text%0Aprompting%20offered%20some%20benefits.%20However%2C%20performance%20significantly%20degraded%20on%0Alarger%20grids%2C%20highlighting%20a%20scalability%20challenge.%20While%20larger%20models%0Agenerally%20achieved%20higher%20average%20success%2C%20the%20visual%20modality%20was%20not%0Auniversally%20dominant%20over%20well-structured%20text%20for%20these%20multimodal%20systems%2C%0Aand%20successful%20paths%20on%20simpler%20grids%20were%20generally%20of%20high%20quality.%20These%0Aresults%20indicate%20current%20limitations%20in%20robust%20spatial%20reasoning%2C%20constraint%0Aadherence%2C%20and%20scalable%20multimodal%20integration%2C%20identifying%20areas%20for%20future%0ALLM%20development%20in%20robotic%20path%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12391v1&entry.124074799=Read"},
{"title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation\n  with Document and Live Images", "author": "Raul Ismayilov and Dzemila Sero and Luuk Spreeuwers", "abstract": "  Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nalong with a dataset of 14,889 synthetic identities. We generate synthetic\nfaces with user-defined identity attribute distributions, offering both\ndocument-style and trusted live capture images. The dataset generated using the\nFLUXSynID framework shows improved alignment with real-world identity\ndistributions and greater inter-class diversity compared to prior work. Our\nwork is publicly released to support biometric research, including face\nrecognition and morphing attack detection.\n", "link": "http://arxiv.org/abs/2505.07530v3", "date": "2025-07-16", "relevancy": 2.3143, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.565}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images&body=Title%3A%20FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images%0AAuthor%3A%20Raul%20Ismayilov%20and%20Dzemila%20Sero%20and%20Luuk%20Spreeuwers%0AAbstract%3A%20%20%20Synthetic%20face%20datasets%20are%20increasingly%20used%20to%20overcome%20the%20limitations%20of%0Areal-world%20biometric%20data%2C%20including%20privacy%20concerns%2C%20demographic%20imbalance%2C%0Aand%20high%20collection%20costs.%20However%2C%20many%20existing%20methods%20lack%20fine-grained%0Acontrol%20over%20identity%20attributes%20and%20fail%20to%20produce%20paired%2C%0Aidentity-consistent%20images%20under%20structured%20capture%20conditions.%20We%20introduce%0AFLUXSynID%2C%20a%20framework%20for%20generating%20high-resolution%20synthetic%20face%20datasets%0Aalong%20with%20a%20dataset%20of%2014%2C889%20synthetic%20identities.%20We%20generate%20synthetic%0Afaces%20with%20user-defined%20identity%20attribute%20distributions%2C%20offering%20both%0Adocument-style%20and%20trusted%20live%20capture%20images.%20The%20dataset%20generated%20using%20the%0AFLUXSynID%20framework%20shows%20improved%20alignment%20with%20real-world%20identity%0Adistributions%20and%20greater%20inter-class%20diversity%20compared%20to%20prior%20work.%20Our%0Awork%20is%20publicly%20released%20to%20support%20biometric%20research%2C%20including%20face%0Arecognition%20and%20morphing%20attack%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLUXSynID%253A%2520A%2520Framework%2520for%2520Identity-Controlled%2520Synthetic%2520Face%2520Generation%250A%2520%2520with%2520Document%2520and%2520Live%2520Images%26entry.906535625%3DRaul%2520Ismayilov%2520and%2520Dzemila%2520Sero%2520and%2520Luuk%2520Spreeuwers%26entry.1292438233%3D%2520%2520Synthetic%2520face%2520datasets%2520are%2520increasingly%2520used%2520to%2520overcome%2520the%2520limitations%2520of%250Areal-world%2520biometric%2520data%252C%2520including%2520privacy%2520concerns%252C%2520demographic%2520imbalance%252C%250Aand%2520high%2520collection%2520costs.%2520However%252C%2520many%2520existing%2520methods%2520lack%2520fine-grained%250Acontrol%2520over%2520identity%2520attributes%2520and%2520fail%2520to%2520produce%2520paired%252C%250Aidentity-consistent%2520images%2520under%2520structured%2520capture%2520conditions.%2520We%2520introduce%250AFLUXSynID%252C%2520a%2520framework%2520for%2520generating%2520high-resolution%2520synthetic%2520face%2520datasets%250Aalong%2520with%2520a%2520dataset%2520of%252014%252C889%2520synthetic%2520identities.%2520We%2520generate%2520synthetic%250Afaces%2520with%2520user-defined%2520identity%2520attribute%2520distributions%252C%2520offering%2520both%250Adocument-style%2520and%2520trusted%2520live%2520capture%2520images.%2520The%2520dataset%2520generated%2520using%2520the%250AFLUXSynID%2520framework%2520shows%2520improved%2520alignment%2520with%2520real-world%2520identity%250Adistributions%2520and%2520greater%2520inter-class%2520diversity%2520compared%2520to%2520prior%2520work.%2520Our%250Awork%2520is%2520publicly%2520released%2520to%2520support%2520biometric%2520research%252C%2520including%2520face%250Arecognition%2520and%2520morphing%2520attack%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLUXSynID%3A%20A%20Framework%20for%20Identity-Controlled%20Synthetic%20Face%20Generation%0A%20%20with%20Document%20and%20Live%20Images&entry.906535625=Raul%20Ismayilov%20and%20Dzemila%20Sero%20and%20Luuk%20Spreeuwers&entry.1292438233=%20%20Synthetic%20face%20datasets%20are%20increasingly%20used%20to%20overcome%20the%20limitations%20of%0Areal-world%20biometric%20data%2C%20including%20privacy%20concerns%2C%20demographic%20imbalance%2C%0Aand%20high%20collection%20costs.%20However%2C%20many%20existing%20methods%20lack%20fine-grained%0Acontrol%20over%20identity%20attributes%20and%20fail%20to%20produce%20paired%2C%0Aidentity-consistent%20images%20under%20structured%20capture%20conditions.%20We%20introduce%0AFLUXSynID%2C%20a%20framework%20for%20generating%20high-resolution%20synthetic%20face%20datasets%0Aalong%20with%20a%20dataset%20of%2014%2C889%20synthetic%20identities.%20We%20generate%20synthetic%0Afaces%20with%20user-defined%20identity%20attribute%20distributions%2C%20offering%20both%0Adocument-style%20and%20trusted%20live%20capture%20images.%20The%20dataset%20generated%20using%20the%0AFLUXSynID%20framework%20shows%20improved%20alignment%20with%20real-world%20identity%0Adistributions%20and%20greater%20inter-class%20diversity%20compared%20to%20prior%20work.%20Our%0Awork%20is%20publicly%20released%20to%20support%20biometric%20research%2C%20including%20face%0Arecognition%20and%20morphing%20attack%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07530v3&entry.124074799=Read"},
{"title": "4D-MISR: A unified model for low-dose super-resolution imaging via\n  feature fusion", "author": "Zifei Wang and Zian Mao and Xiaoya He and Xi Huang and Haoran Zhang and Chun Cheng and Shufen Chu and Tingzheng Hou and Xiaoqin Zeng and Yujun Xie", "abstract": "  While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.\n", "link": "http://arxiv.org/abs/2507.09953v2", "date": "2025-07-16", "relevancy": 2.2875, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion&body=Title%3A%204D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion%0AAuthor%3A%20Zifei%20Wang%20and%20Zian%20Mao%20and%20Xiaoya%20He%20and%20Xi%20Huang%20and%20Haoran%20Zhang%20and%20Chun%20Cheng%20and%20Shufen%20Chu%20and%20Tingzheng%20Hou%20and%20Xiaoqin%20Zeng%20and%20Yujun%20Xie%0AAbstract%3A%20%20%20While%20electron%20microscopy%20offers%20crucial%20atomic-resolution%20insights%20into%0Astructure-property%20relationships%2C%20radiation%20damage%20severely%20limits%20its%20use%20on%0Abeam-sensitive%20materials%20like%20proteins%20and%202D%20materials.%20To%20overcome%20this%0Achallenge%2C%20we%20push%20beyond%20the%20electron%20dose%20limits%20of%20conventional%20electron%0Amicroscopy%20by%20adapting%20principles%20from%20multi-image%20super-resolution%20%28MISR%29%20that%0Ahave%20been%20widely%20used%20in%20remote%20sensing.%20Our%20method%20fuses%20multiple%0Alow-resolution%2C%20sub-pixel-shifted%20views%20and%20enhances%20the%20reconstruction%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20that%20integrates%20features%20from%20synthetic%2C%0Amulti-angle%20observations.%20We%20developed%20a%20dual-path%2C%20attention-guided%20network%0Afor%204D-STEM%20that%20achieves%20atomic-scale%20super-resolution%20from%20ultra-low-dose%0Adata.%20This%20provides%20robust%20atomic-scale%20visualization%20across%20amorphous%2C%0Asemi-crystalline%2C%20and%20crystalline%20beam-sensitive%20specimens.%20Systematic%0Aevaluations%20on%20representative%20materials%20demonstrate%20comparable%20spatial%0Aresolution%20to%20conventional%20ptychography%20under%20ultra-low-dose%20conditions.%20Our%0Awork%20expands%20the%20capabilities%20of%204D-STEM%2C%20offering%20a%20new%20and%20generalizable%0Amethod%20for%20the%20structural%20analysis%20of%20radiation-vulnerable%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-MISR%253A%2520A%2520unified%2520model%2520for%2520low-dose%2520super-resolution%2520imaging%2520via%250A%2520%2520feature%2520fusion%26entry.906535625%3DZifei%2520Wang%2520and%2520Zian%2520Mao%2520and%2520Xiaoya%2520He%2520and%2520Xi%2520Huang%2520and%2520Haoran%2520Zhang%2520and%2520Chun%2520Cheng%2520and%2520Shufen%2520Chu%2520and%2520Tingzheng%2520Hou%2520and%2520Xiaoqin%2520Zeng%2520and%2520Yujun%2520Xie%26entry.1292438233%3D%2520%2520While%2520electron%2520microscopy%2520offers%2520crucial%2520atomic-resolution%2520insights%2520into%250Astructure-property%2520relationships%252C%2520radiation%2520damage%2520severely%2520limits%2520its%2520use%2520on%250Abeam-sensitive%2520materials%2520like%2520proteins%2520and%25202D%2520materials.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520push%2520beyond%2520the%2520electron%2520dose%2520limits%2520of%2520conventional%2520electron%250Amicroscopy%2520by%2520adapting%2520principles%2520from%2520multi-image%2520super-resolution%2520%2528MISR%2529%2520that%250Ahave%2520been%2520widely%2520used%2520in%2520remote%2520sensing.%2520Our%2520method%2520fuses%2520multiple%250Alow-resolution%252C%2520sub-pixel-shifted%2520views%2520and%2520enhances%2520the%2520reconstruction%2520with%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520that%2520integrates%2520features%2520from%2520synthetic%252C%250Amulti-angle%2520observations.%2520We%2520developed%2520a%2520dual-path%252C%2520attention-guided%2520network%250Afor%25204D-STEM%2520that%2520achieves%2520atomic-scale%2520super-resolution%2520from%2520ultra-low-dose%250Adata.%2520This%2520provides%2520robust%2520atomic-scale%2520visualization%2520across%2520amorphous%252C%250Asemi-crystalline%252C%2520and%2520crystalline%2520beam-sensitive%2520specimens.%2520Systematic%250Aevaluations%2520on%2520representative%2520materials%2520demonstrate%2520comparable%2520spatial%250Aresolution%2520to%2520conventional%2520ptychography%2520under%2520ultra-low-dose%2520conditions.%2520Our%250Awork%2520expands%2520the%2520capabilities%2520of%25204D-STEM%252C%2520offering%2520a%2520new%2520and%2520generalizable%250Amethod%2520for%2520the%2520structural%2520analysis%2520of%2520radiation-vulnerable%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-MISR%3A%20A%20unified%20model%20for%20low-dose%20super-resolution%20imaging%20via%0A%20%20feature%20fusion&entry.906535625=Zifei%20Wang%20and%20Zian%20Mao%20and%20Xiaoya%20He%20and%20Xi%20Huang%20and%20Haoran%20Zhang%20and%20Chun%20Cheng%20and%20Shufen%20Chu%20and%20Tingzheng%20Hou%20and%20Xiaoqin%20Zeng%20and%20Yujun%20Xie&entry.1292438233=%20%20While%20electron%20microscopy%20offers%20crucial%20atomic-resolution%20insights%20into%0Astructure-property%20relationships%2C%20radiation%20damage%20severely%20limits%20its%20use%20on%0Abeam-sensitive%20materials%20like%20proteins%20and%202D%20materials.%20To%20overcome%20this%0Achallenge%2C%20we%20push%20beyond%20the%20electron%20dose%20limits%20of%20conventional%20electron%0Amicroscopy%20by%20adapting%20principles%20from%20multi-image%20super-resolution%20%28MISR%29%20that%0Ahave%20been%20widely%20used%20in%20remote%20sensing.%20Our%20method%20fuses%20multiple%0Alow-resolution%2C%20sub-pixel-shifted%20views%20and%20enhances%20the%20reconstruction%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20that%20integrates%20features%20from%20synthetic%2C%0Amulti-angle%20observations.%20We%20developed%20a%20dual-path%2C%20attention-guided%20network%0Afor%204D-STEM%20that%20achieves%20atomic-scale%20super-resolution%20from%20ultra-low-dose%0Adata.%20This%20provides%20robust%20atomic-scale%20visualization%20across%20amorphous%2C%0Asemi-crystalline%2C%20and%20crystalline%20beam-sensitive%20specimens.%20Systematic%0Aevaluations%20on%20representative%20materials%20demonstrate%20comparable%20spatial%0Aresolution%20to%20conventional%20ptychography%20under%20ultra-low-dose%20conditions.%20Our%0Awork%20expands%20the%20capabilities%20of%204D-STEM%2C%20offering%20a%20new%20and%20generalizable%0Amethod%20for%20the%20structural%20analysis%20of%20radiation-vulnerable%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09953v2&entry.124074799=Read"},
{"title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under\n  Uncertainty", "author": "Meera Hahn and Wenjun Zeng and Nithish Kannen and Rich Galt and Kartikeya Badola and Been Kim and Zi Wang", "abstract": "  User prompts for generative AI models are often underspecified, leading to a\nmisalignment between the user intent and models' understanding. As a result,\nusers commonly have to painstakingly refine their prompts. We study this\nalignment problem in text-to-image (T2I) generation and propose a prototype for\nproactive T2I agents equipped with an interface to (1) actively ask\nclarification questions when uncertain, and (2) present their uncertainty about\nuser intent as an understandable and editable belief graph. We build simple\nprototypes for such agents and propose a new scalable and automated evaluation\napproach using two agents, one with a ground truth intent (an image) while the\nother tries to ask as few questions as possible to align with the ground truth.\nWe experiment over three image-text datasets: ImageInWords (Garg et al., 2024),\nCOCO (Lin et al., 2014) and DesignBench, a benchmark we curated with strong\nartistic and design elements. Experiments over the three datasets demonstrate\nthe proposed T2I agents' ability to ask informative questions and elicit\ncrucial information to achieve successful alignment with at least 2 times\nhigher VQAScore (Lin et al., 2024) than the standard T2I generation. Moreover,\nwe conducted human studies and observed that at least 90% of human subjects\nfound these agents and their belief graphs helpful for their T2I workflow,\nhighlighting the effectiveness of our approach. Code and DesignBench can be\nfound at https://github.com/google-deepmind/proactive_t2i_agents.\n", "link": "http://arxiv.org/abs/2412.06771v2", "date": "2025-07-16", "relevancy": 2.268, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5741}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5625}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proactive%20Agents%20for%20Multi-Turn%20Text-to-Image%20Generation%20Under%0A%20%20Uncertainty&body=Title%3A%20Proactive%20Agents%20for%20Multi-Turn%20Text-to-Image%20Generation%20Under%0A%20%20Uncertainty%0AAuthor%3A%20Meera%20Hahn%20and%20Wenjun%20Zeng%20and%20Nithish%20Kannen%20and%20Rich%20Galt%20and%20Kartikeya%20Badola%20and%20Been%20Kim%20and%20Zi%20Wang%0AAbstract%3A%20%20%20User%20prompts%20for%20generative%20AI%20models%20are%20often%20underspecified%2C%20leading%20to%20a%0Amisalignment%20between%20the%20user%20intent%20and%20models%27%20understanding.%20As%20a%20result%2C%0Ausers%20commonly%20have%20to%20painstakingly%20refine%20their%20prompts.%20We%20study%20this%0Aalignment%20problem%20in%20text-to-image%20%28T2I%29%20generation%20and%20propose%20a%20prototype%20for%0Aproactive%20T2I%20agents%20equipped%20with%20an%20interface%20to%20%281%29%20actively%20ask%0Aclarification%20questions%20when%20uncertain%2C%20and%20%282%29%20present%20their%20uncertainty%20about%0Auser%20intent%20as%20an%20understandable%20and%20editable%20belief%20graph.%20We%20build%20simple%0Aprototypes%20for%20such%20agents%20and%20propose%20a%20new%20scalable%20and%20automated%20evaluation%0Aapproach%20using%20two%20agents%2C%20one%20with%20a%20ground%20truth%20intent%20%28an%20image%29%20while%20the%0Aother%20tries%20to%20ask%20as%20few%20questions%20as%20possible%20to%20align%20with%20the%20ground%20truth.%0AWe%20experiment%20over%20three%20image-text%20datasets%3A%20ImageInWords%20%28Garg%20et%20al.%2C%202024%29%2C%0ACOCO%20%28Lin%20et%20al.%2C%202014%29%20and%20DesignBench%2C%20a%20benchmark%20we%20curated%20with%20strong%0Aartistic%20and%20design%20elements.%20Experiments%20over%20the%20three%20datasets%20demonstrate%0Athe%20proposed%20T2I%20agents%27%20ability%20to%20ask%20informative%20questions%20and%20elicit%0Acrucial%20information%20to%20achieve%20successful%20alignment%20with%20at%20least%202%20times%0Ahigher%20VQAScore%20%28Lin%20et%20al.%2C%202024%29%20than%20the%20standard%20T2I%20generation.%20Moreover%2C%0Awe%20conducted%20human%20studies%20and%20observed%20that%20at%20least%2090%25%20of%20human%20subjects%0Afound%20these%20agents%20and%20their%20belief%20graphs%20helpful%20for%20their%20T2I%20workflow%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20approach.%20Code%20and%20DesignBench%20can%20be%0Afound%20at%20https%3A//github.com/google-deepmind/proactive_t2i_agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactive%2520Agents%2520for%2520Multi-Turn%2520Text-to-Image%2520Generation%2520Under%250A%2520%2520Uncertainty%26entry.906535625%3DMeera%2520Hahn%2520and%2520Wenjun%2520Zeng%2520and%2520Nithish%2520Kannen%2520and%2520Rich%2520Galt%2520and%2520Kartikeya%2520Badola%2520and%2520Been%2520Kim%2520and%2520Zi%2520Wang%26entry.1292438233%3D%2520%2520User%2520prompts%2520for%2520generative%2520AI%2520models%2520are%2520often%2520underspecified%252C%2520leading%2520to%2520a%250Amisalignment%2520between%2520the%2520user%2520intent%2520and%2520models%2527%2520understanding.%2520As%2520a%2520result%252C%250Ausers%2520commonly%2520have%2520to%2520painstakingly%2520refine%2520their%2520prompts.%2520We%2520study%2520this%250Aalignment%2520problem%2520in%2520text-to-image%2520%2528T2I%2529%2520generation%2520and%2520propose%2520a%2520prototype%2520for%250Aproactive%2520T2I%2520agents%2520equipped%2520with%2520an%2520interface%2520to%2520%25281%2529%2520actively%2520ask%250Aclarification%2520questions%2520when%2520uncertain%252C%2520and%2520%25282%2529%2520present%2520their%2520uncertainty%2520about%250Auser%2520intent%2520as%2520an%2520understandable%2520and%2520editable%2520belief%2520graph.%2520We%2520build%2520simple%250Aprototypes%2520for%2520such%2520agents%2520and%2520propose%2520a%2520new%2520scalable%2520and%2520automated%2520evaluation%250Aapproach%2520using%2520two%2520agents%252C%2520one%2520with%2520a%2520ground%2520truth%2520intent%2520%2528an%2520image%2529%2520while%2520the%250Aother%2520tries%2520to%2520ask%2520as%2520few%2520questions%2520as%2520possible%2520to%2520align%2520with%2520the%2520ground%2520truth.%250AWe%2520experiment%2520over%2520three%2520image-text%2520datasets%253A%2520ImageInWords%2520%2528Garg%2520et%2520al.%252C%25202024%2529%252C%250ACOCO%2520%2528Lin%2520et%2520al.%252C%25202014%2529%2520and%2520DesignBench%252C%2520a%2520benchmark%2520we%2520curated%2520with%2520strong%250Aartistic%2520and%2520design%2520elements.%2520Experiments%2520over%2520the%2520three%2520datasets%2520demonstrate%250Athe%2520proposed%2520T2I%2520agents%2527%2520ability%2520to%2520ask%2520informative%2520questions%2520and%2520elicit%250Acrucial%2520information%2520to%2520achieve%2520successful%2520alignment%2520with%2520at%2520least%25202%2520times%250Ahigher%2520VQAScore%2520%2528Lin%2520et%2520al.%252C%25202024%2529%2520than%2520the%2520standard%2520T2I%2520generation.%2520Moreover%252C%250Awe%2520conducted%2520human%2520studies%2520and%2520observed%2520that%2520at%2520least%252090%2525%2520of%2520human%2520subjects%250Afound%2520these%2520agents%2520and%2520their%2520belief%2520graphs%2520helpful%2520for%2520their%2520T2I%2520workflow%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Code%2520and%2520DesignBench%2520can%2520be%250Afound%2520at%2520https%253A//github.com/google-deepmind/proactive_t2i_agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proactive%20Agents%20for%20Multi-Turn%20Text-to-Image%20Generation%20Under%0A%20%20Uncertainty&entry.906535625=Meera%20Hahn%20and%20Wenjun%20Zeng%20and%20Nithish%20Kannen%20and%20Rich%20Galt%20and%20Kartikeya%20Badola%20and%20Been%20Kim%20and%20Zi%20Wang&entry.1292438233=%20%20User%20prompts%20for%20generative%20AI%20models%20are%20often%20underspecified%2C%20leading%20to%20a%0Amisalignment%20between%20the%20user%20intent%20and%20models%27%20understanding.%20As%20a%20result%2C%0Ausers%20commonly%20have%20to%20painstakingly%20refine%20their%20prompts.%20We%20study%20this%0Aalignment%20problem%20in%20text-to-image%20%28T2I%29%20generation%20and%20propose%20a%20prototype%20for%0Aproactive%20T2I%20agents%20equipped%20with%20an%20interface%20to%20%281%29%20actively%20ask%0Aclarification%20questions%20when%20uncertain%2C%20and%20%282%29%20present%20their%20uncertainty%20about%0Auser%20intent%20as%20an%20understandable%20and%20editable%20belief%20graph.%20We%20build%20simple%0Aprototypes%20for%20such%20agents%20and%20propose%20a%20new%20scalable%20and%20automated%20evaluation%0Aapproach%20using%20two%20agents%2C%20one%20with%20a%20ground%20truth%20intent%20%28an%20image%29%20while%20the%0Aother%20tries%20to%20ask%20as%20few%20questions%20as%20possible%20to%20align%20with%20the%20ground%20truth.%0AWe%20experiment%20over%20three%20image-text%20datasets%3A%20ImageInWords%20%28Garg%20et%20al.%2C%202024%29%2C%0ACOCO%20%28Lin%20et%20al.%2C%202014%29%20and%20DesignBench%2C%20a%20benchmark%20we%20curated%20with%20strong%0Aartistic%20and%20design%20elements.%20Experiments%20over%20the%20three%20datasets%20demonstrate%0Athe%20proposed%20T2I%20agents%27%20ability%20to%20ask%20informative%20questions%20and%20elicit%0Acrucial%20information%20to%20achieve%20successful%20alignment%20with%20at%20least%202%20times%0Ahigher%20VQAScore%20%28Lin%20et%20al.%2C%202024%29%20than%20the%20standard%20T2I%20generation.%20Moreover%2C%0Awe%20conducted%20human%20studies%20and%20observed%20that%20at%20least%2090%25%20of%20human%20subjects%0Afound%20these%20agents%20and%20their%20belief%20graphs%20helpful%20for%20their%20T2I%20workflow%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20approach.%20Code%20and%20DesignBench%20can%20be%0Afound%20at%20https%3A//github.com/google-deepmind/proactive_t2i_agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06771v2&entry.124074799=Read"},
{"title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos", "author": "Ruihan Yang and Qinxi Yu and Yecheng Wu and Rui Yan and Borui Li and An-Chieh Cheng and Xueyan Zou and Yunhao Fang and Hongxu Yin and Sifei Liu and Song Han and Yao Lu and Xiaolong Wang", "abstract": "  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n", "link": "http://arxiv.org/abs/2507.12440v1", "date": "2025-07-16", "relevancy": 2.2588, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5839}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5676}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos&body=Title%3A%20EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos%0AAuthor%3A%20Ruihan%20Yang%20and%20Qinxi%20Yu%20and%20Yecheng%20Wu%20and%20Rui%20Yan%20and%20Borui%20Li%20and%20An-Chieh%20Cheng%20and%20Xueyan%20Zou%20and%20Yunhao%20Fang%20and%20Hongxu%20Yin%20and%20Sifei%20Liu%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Real%20robot%20data%20collection%20for%20imitation%20learning%20has%20led%20to%20significant%0Aadvancements%20in%20robotic%20manipulation.%20However%2C%20the%20requirement%20for%20robot%0Ahardware%20in%20the%20process%20fundamentally%20constrains%20the%20scale%20of%20the%20data.%20In%20this%0Apaper%2C%20we%20explore%20training%20Vision-Language-Action%20%28VLA%29%20models%20using%20egocentric%0Ahuman%20videos.%20The%20benefit%20of%20using%20human%20videos%20is%20not%20only%20for%20their%20scale%20but%0Amore%20importantly%20for%20the%20richness%20of%20scenes%20and%20tasks.%20With%20a%20VLA%20trained%20on%0Ahuman%20video%20that%20predicts%20human%20wrist%20and%20hand%20actions%2C%20we%20can%20perform%20Inverse%0AKinematics%20and%20retargeting%20to%20convert%20the%20human%20actions%20to%20robot%20actions.%20We%0Afine-tune%20the%20model%20using%20a%20few%20robot%20manipulation%20demonstrations%20to%20obtain%20the%0Arobot%20policy%2C%20namely%20EgoVLA.%20We%20propose%20a%20simulation%20benchmark%20called%20Isaac%0AHumanoid%20Manipulation%20Benchmark%2C%20where%20we%20design%20diverse%20bimanual%20manipulation%0Atasks%20with%20demonstrations.%20We%20fine-tune%20and%20evaluate%20EgoVLA%20with%20Isaac%20Humanoid%0AManipulation%20Benchmark%20and%20show%20significant%20improvements%20over%20baselines%20and%0Aablate%20the%20importance%20of%20human%20data.%20Videos%20can%20be%20found%20on%20our%20website%3A%0Ahttps%3A//rchalyang.github.io/EgoVLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoVLA%253A%2520Learning%2520Vision-Language-Action%2520Models%2520from%2520Egocentric%2520Human%250A%2520%2520Videos%26entry.906535625%3DRuihan%2520Yang%2520and%2520Qinxi%2520Yu%2520and%2520Yecheng%2520Wu%2520and%2520Rui%2520Yan%2520and%2520Borui%2520Li%2520and%2520An-Chieh%2520Cheng%2520and%2520Xueyan%2520Zou%2520and%2520Yunhao%2520Fang%2520and%2520Hongxu%2520Yin%2520and%2520Sifei%2520Liu%2520and%2520Song%2520Han%2520and%2520Yao%2520Lu%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Real%2520robot%2520data%2520collection%2520for%2520imitation%2520learning%2520has%2520led%2520to%2520significant%250Aadvancements%2520in%2520robotic%2520manipulation.%2520However%252C%2520the%2520requirement%2520for%2520robot%250Ahardware%2520in%2520the%2520process%2520fundamentally%2520constrains%2520the%2520scale%2520of%2520the%2520data.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520training%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520using%2520egocentric%250Ahuman%2520videos.%2520The%2520benefit%2520of%2520using%2520human%2520videos%2520is%2520not%2520only%2520for%2520their%2520scale%2520but%250Amore%2520importantly%2520for%2520the%2520richness%2520of%2520scenes%2520and%2520tasks.%2520With%2520a%2520VLA%2520trained%2520on%250Ahuman%2520video%2520that%2520predicts%2520human%2520wrist%2520and%2520hand%2520actions%252C%2520we%2520can%2520perform%2520Inverse%250AKinematics%2520and%2520retargeting%2520to%2520convert%2520the%2520human%2520actions%2520to%2520robot%2520actions.%2520We%250Afine-tune%2520the%2520model%2520using%2520a%2520few%2520robot%2520manipulation%2520demonstrations%2520to%2520obtain%2520the%250Arobot%2520policy%252C%2520namely%2520EgoVLA.%2520We%2520propose%2520a%2520simulation%2520benchmark%2520called%2520Isaac%250AHumanoid%2520Manipulation%2520Benchmark%252C%2520where%2520we%2520design%2520diverse%2520bimanual%2520manipulation%250Atasks%2520with%2520demonstrations.%2520We%2520fine-tune%2520and%2520evaluate%2520EgoVLA%2520with%2520Isaac%2520Humanoid%250AManipulation%2520Benchmark%2520and%2520show%2520significant%2520improvements%2520over%2520baselines%2520and%250Aablate%2520the%2520importance%2520of%2520human%2520data.%2520Videos%2520can%2520be%2520found%2520on%2520our%2520website%253A%250Ahttps%253A//rchalyang.github.io/EgoVLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoVLA%3A%20Learning%20Vision-Language-Action%20Models%20from%20Egocentric%20Human%0A%20%20Videos&entry.906535625=Ruihan%20Yang%20and%20Qinxi%20Yu%20and%20Yecheng%20Wu%20and%20Rui%20Yan%20and%20Borui%20Li%20and%20An-Chieh%20Cheng%20and%20Xueyan%20Zou%20and%20Yunhao%20Fang%20and%20Hongxu%20Yin%20and%20Sifei%20Liu%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Real%20robot%20data%20collection%20for%20imitation%20learning%20has%20led%20to%20significant%0Aadvancements%20in%20robotic%20manipulation.%20However%2C%20the%20requirement%20for%20robot%0Ahardware%20in%20the%20process%20fundamentally%20constrains%20the%20scale%20of%20the%20data.%20In%20this%0Apaper%2C%20we%20explore%20training%20Vision-Language-Action%20%28VLA%29%20models%20using%20egocentric%0Ahuman%20videos.%20The%20benefit%20of%20using%20human%20videos%20is%20not%20only%20for%20their%20scale%20but%0Amore%20importantly%20for%20the%20richness%20of%20scenes%20and%20tasks.%20With%20a%20VLA%20trained%20on%0Ahuman%20video%20that%20predicts%20human%20wrist%20and%20hand%20actions%2C%20we%20can%20perform%20Inverse%0AKinematics%20and%20retargeting%20to%20convert%20the%20human%20actions%20to%20robot%20actions.%20We%0Afine-tune%20the%20model%20using%20a%20few%20robot%20manipulation%20demonstrations%20to%20obtain%20the%0Arobot%20policy%2C%20namely%20EgoVLA.%20We%20propose%20a%20simulation%20benchmark%20called%20Isaac%0AHumanoid%20Manipulation%20Benchmark%2C%20where%20we%20design%20diverse%20bimanual%20manipulation%0Atasks%20with%20demonstrations.%20We%20fine-tune%20and%20evaluate%20EgoVLA%20with%20Isaac%20Humanoid%0AManipulation%20Benchmark%20and%20show%20significant%20improvements%20over%20baselines%20and%0Aablate%20the%20importance%20of%20human%20data.%20Videos%20can%20be%20found%20on%20our%20website%3A%0Ahttps%3A//rchalyang.github.io/EgoVLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12440v1&entry.124074799=Read"},
{"title": "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy", "author": "Linh Tran and Timothy Castiglia and Stacy Patterson and Ana Milanova", "abstract": "  We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.\n", "link": "http://arxiv.org/abs/2501.13916v3", "date": "2025-07-16", "relevancy": 2.2412, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4741}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4409}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PBM-VFL%3A%20Vertical%20Federated%20Learning%20with%20Feature%20and%20Sample%20Privacy&body=Title%3A%20PBM-VFL%3A%20Vertical%20Federated%20Learning%20with%20Feature%20and%20Sample%20Privacy%0AAuthor%3A%20Linh%20Tran%20and%20Timothy%20Castiglia%20and%20Stacy%20Patterson%20and%20Ana%20Milanova%0AAbstract%3A%20%20%20We%20present%20Poisson%20Binomial%20Mechanism%20Vertical%20Federated%20Learning%20%28PBM-VFL%29%2C%0Aa%20communication-efficient%20Vertical%20Federated%20Learning%20algorithm%20with%0ADifferential%20Privacy%20guarantees.%20PBM-VFL%20combines%20Secure%20Multi-Party%0AComputation%20with%20the%20recently%20introduced%20Poisson%20Binomial%20Mechanism%20to%20protect%0Aparties%27%20private%20datasets%20during%20model%20training.%20We%20define%20the%20novel%20concept%20of%0Afeature%20privacy%20and%20analyze%20end-to-end%20feature%20and%20sample%20privacy%20of%20our%0Aalgorithm.%20We%20compare%20sample%20privacy%20loss%20in%20VFL%20with%20privacy%20loss%20in%20HFL.%20We%0Aalso%20provide%20the%20first%20theoretical%20characterization%20of%20the%20relationship%20between%0Aprivacy%20budget%2C%20convergence%20error%2C%20and%20communication%20cost%20in%0Adifferentially-private%20VFL.%20Finally%2C%20we%20empirically%20show%20that%20our%20model%0Aperforms%20well%20with%20high%20levels%20of%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13916v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPBM-VFL%253A%2520Vertical%2520Federated%2520Learning%2520with%2520Feature%2520and%2520Sample%2520Privacy%26entry.906535625%3DLinh%2520Tran%2520and%2520Timothy%2520Castiglia%2520and%2520Stacy%2520Patterson%2520and%2520Ana%2520Milanova%26entry.1292438233%3D%2520%2520We%2520present%2520Poisson%2520Binomial%2520Mechanism%2520Vertical%2520Federated%2520Learning%2520%2528PBM-VFL%2529%252C%250Aa%2520communication-efficient%2520Vertical%2520Federated%2520Learning%2520algorithm%2520with%250ADifferential%2520Privacy%2520guarantees.%2520PBM-VFL%2520combines%2520Secure%2520Multi-Party%250AComputation%2520with%2520the%2520recently%2520introduced%2520Poisson%2520Binomial%2520Mechanism%2520to%2520protect%250Aparties%2527%2520private%2520datasets%2520during%2520model%2520training.%2520We%2520define%2520the%2520novel%2520concept%2520of%250Afeature%2520privacy%2520and%2520analyze%2520end-to-end%2520feature%2520and%2520sample%2520privacy%2520of%2520our%250Aalgorithm.%2520We%2520compare%2520sample%2520privacy%2520loss%2520in%2520VFL%2520with%2520privacy%2520loss%2520in%2520HFL.%2520We%250Aalso%2520provide%2520the%2520first%2520theoretical%2520characterization%2520of%2520the%2520relationship%2520between%250Aprivacy%2520budget%252C%2520convergence%2520error%252C%2520and%2520communication%2520cost%2520in%250Adifferentially-private%2520VFL.%2520Finally%252C%2520we%2520empirically%2520show%2520that%2520our%2520model%250Aperforms%2520well%2520with%2520high%2520levels%2520of%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13916v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PBM-VFL%3A%20Vertical%20Federated%20Learning%20with%20Feature%20and%20Sample%20Privacy&entry.906535625=Linh%20Tran%20and%20Timothy%20Castiglia%20and%20Stacy%20Patterson%20and%20Ana%20Milanova&entry.1292438233=%20%20We%20present%20Poisson%20Binomial%20Mechanism%20Vertical%20Federated%20Learning%20%28PBM-VFL%29%2C%0Aa%20communication-efficient%20Vertical%20Federated%20Learning%20algorithm%20with%0ADifferential%20Privacy%20guarantees.%20PBM-VFL%20combines%20Secure%20Multi-Party%0AComputation%20with%20the%20recently%20introduced%20Poisson%20Binomial%20Mechanism%20to%20protect%0Aparties%27%20private%20datasets%20during%20model%20training.%20We%20define%20the%20novel%20concept%20of%0Afeature%20privacy%20and%20analyze%20end-to-end%20feature%20and%20sample%20privacy%20of%20our%0Aalgorithm.%20We%20compare%20sample%20privacy%20loss%20in%20VFL%20with%20privacy%20loss%20in%20HFL.%20We%0Aalso%20provide%20the%20first%20theoretical%20characterization%20of%20the%20relationship%20between%0Aprivacy%20budget%2C%20convergence%20error%2C%20and%20communication%20cost%20in%0Adifferentially-private%20VFL.%20Finally%2C%20we%20empirically%20show%20that%20our%20model%0Aperforms%20well%20with%20high%20levels%20of%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13916v3&entry.124074799=Read"},
{"title": "Heat Kernel Goes Topological", "author": "Maximilian Krahn and Vikas Garg", "abstract": "  Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.\n", "link": "http://arxiv.org/abs/2507.12380v1", "date": "2025-07-16", "relevancy": 2.2335, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.449}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heat%20Kernel%20Goes%20Topological&body=Title%3A%20Heat%20Kernel%20Goes%20Topological%0AAuthor%3A%20Maximilian%20Krahn%20and%20Vikas%20Garg%0AAbstract%3A%20%20%20Topological%20neural%20networks%20have%20emerged%20as%20powerful%20successors%20of%20graph%0Aneural%20networks.%20However%2C%20they%20typically%20involve%20higher-order%20message%20passing%2C%0Awhich%20incurs%20significant%20computational%20expense.%20We%20circumvent%20this%20issue%20with%20a%0Anovel%20topological%20framework%20that%20introduces%20a%20Laplacian%20operator%20on%0Acombinatorial%20complexes%20%28CCs%29%2C%20enabling%20efficient%20computation%20of%20heat%20kernels%0Athat%20serve%20as%20node%20descriptors.%20Our%20approach%20captures%20multiscale%20information%0Aand%20enables%20permutation-equivariant%20representations%2C%20allowing%20easy%20integration%0Ainto%20modern%20transformer-based%20architectures.%0A%20%20Theoretically%2C%20the%20proposed%20method%20is%20maximally%20expressive%20because%20it%20can%0Adistinguish%20arbitrary%20non-isomorphic%20CCs.%20Empirically%2C%20it%20significantly%0Aoutperforms%20existing%20topological%20methods%20in%20terms%20of%20computational%20efficiency.%0ABesides%20demonstrating%20competitive%20performance%20with%20the%20state-of-the-art%0Adescriptors%20on%20standard%20molecular%20datasets%2C%20it%20exhibits%20superior%20capability%20in%0Adistinguishing%20complex%20topological%20structures%20and%20avoiding%20blind%20spots%20on%0Atopological%20benchmarks.%20Overall%2C%20this%20work%20advances%20topological%20deep%20learning%0Aby%20providing%20expressive%20yet%20scalable%20representations%2C%20thereby%20opening%20up%0Aexciting%20avenues%20for%20molecular%20classification%20and%20property%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeat%2520Kernel%2520Goes%2520Topological%26entry.906535625%3DMaximilian%2520Krahn%2520and%2520Vikas%2520Garg%26entry.1292438233%3D%2520%2520Topological%2520neural%2520networks%2520have%2520emerged%2520as%2520powerful%2520successors%2520of%2520graph%250Aneural%2520networks.%2520However%252C%2520they%2520typically%2520involve%2520higher-order%2520message%2520passing%252C%250Awhich%2520incurs%2520significant%2520computational%2520expense.%2520We%2520circumvent%2520this%2520issue%2520with%2520a%250Anovel%2520topological%2520framework%2520that%2520introduces%2520a%2520Laplacian%2520operator%2520on%250Acombinatorial%2520complexes%2520%2528CCs%2529%252C%2520enabling%2520efficient%2520computation%2520of%2520heat%2520kernels%250Athat%2520serve%2520as%2520node%2520descriptors.%2520Our%2520approach%2520captures%2520multiscale%2520information%250Aand%2520enables%2520permutation-equivariant%2520representations%252C%2520allowing%2520easy%2520integration%250Ainto%2520modern%2520transformer-based%2520architectures.%250A%2520%2520Theoretically%252C%2520the%2520proposed%2520method%2520is%2520maximally%2520expressive%2520because%2520it%2520can%250Adistinguish%2520arbitrary%2520non-isomorphic%2520CCs.%2520Empirically%252C%2520it%2520significantly%250Aoutperforms%2520existing%2520topological%2520methods%2520in%2520terms%2520of%2520computational%2520efficiency.%250ABesides%2520demonstrating%2520competitive%2520performance%2520with%2520the%2520state-of-the-art%250Adescriptors%2520on%2520standard%2520molecular%2520datasets%252C%2520it%2520exhibits%2520superior%2520capability%2520in%250Adistinguishing%2520complex%2520topological%2520structures%2520and%2520avoiding%2520blind%2520spots%2520on%250Atopological%2520benchmarks.%2520Overall%252C%2520this%2520work%2520advances%2520topological%2520deep%2520learning%250Aby%2520providing%2520expressive%2520yet%2520scalable%2520representations%252C%2520thereby%2520opening%2520up%250Aexciting%2520avenues%2520for%2520molecular%2520classification%2520and%2520property%2520prediction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heat%20Kernel%20Goes%20Topological&entry.906535625=Maximilian%20Krahn%20and%20Vikas%20Garg&entry.1292438233=%20%20Topological%20neural%20networks%20have%20emerged%20as%20powerful%20successors%20of%20graph%0Aneural%20networks.%20However%2C%20they%20typically%20involve%20higher-order%20message%20passing%2C%0Awhich%20incurs%20significant%20computational%20expense.%20We%20circumvent%20this%20issue%20with%20a%0Anovel%20topological%20framework%20that%20introduces%20a%20Laplacian%20operator%20on%0Acombinatorial%20complexes%20%28CCs%29%2C%20enabling%20efficient%20computation%20of%20heat%20kernels%0Athat%20serve%20as%20node%20descriptors.%20Our%20approach%20captures%20multiscale%20information%0Aand%20enables%20permutation-equivariant%20representations%2C%20allowing%20easy%20integration%0Ainto%20modern%20transformer-based%20architectures.%0A%20%20Theoretically%2C%20the%20proposed%20method%20is%20maximally%20expressive%20because%20it%20can%0Adistinguish%20arbitrary%20non-isomorphic%20CCs.%20Empirically%2C%20it%20significantly%0Aoutperforms%20existing%20topological%20methods%20in%20terms%20of%20computational%20efficiency.%0ABesides%20demonstrating%20competitive%20performance%20with%20the%20state-of-the-art%0Adescriptors%20on%20standard%20molecular%20datasets%2C%20it%20exhibits%20superior%20capability%20in%0Adistinguishing%20complex%20topological%20structures%20and%20avoiding%20blind%20spots%20on%0Atopological%20benchmarks.%20Overall%2C%20this%20work%20advances%20topological%20deep%20learning%0Aby%20providing%20expressive%20yet%20scalable%20representations%2C%20thereby%20opening%20up%0Aexciting%20avenues%20for%20molecular%20classification%20and%20property%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12380v1&entry.124074799=Read"},
{"title": "ViTally Consistent: Scaling Biological Representation Learning for Cell\n  Microscopy", "author": "Kian Kenyon-Dean and Zitong Jerry Wang and John Urbanik and Konstantin Donhauser and Jason Hartford and Saber Saberian and Nil Sahin and Ihab Bendidi and Safiye Celik and Marta Fay and Juan Sebastian Rodriguez Vera and Imran S Haque and Oren Kraus", "abstract": "  Large-scale cell microscopy screens are used in drug discovery and molecular\nbiology research to study the effects of millions of chemical and genetic\nperturbations on cells. To use these images in downstream analysis, we need\nmodels that can map each image into a feature space that represents diverse\nbiological phenotypes consistently, in the sense that perturbations with\nsimilar biological effects have similar representations. In this work, we\npresent the largest foundation model for cell microscopy data to date, a new\n1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image\ncrops. Compared to a previous published ViT-L/8 MAE, our new model achieves a\n60% improvement in linear separability of genetic perturbations and obtains the\nbest overall performance on whole-genome biological relationship recall and\nreplicate consistency benchmarks. Beyond scaling, we developed two key methods\nthat improve performance: (1) training on a curated and diverse dataset; and,\n(2) using biologically motivated linear probing tasks to search across each\ntransformer block for the best candidate representation of whole-genome\nscreens. We find that many self-supervised vision transformers, pretrained on\neither natural or microscopy images, yield significantly more biologically\nmeaningful representations of microscopy images in their intermediate blocks\nthan in their typically used final blocks. More broadly, our approach and\nresults provide insights toward a general strategy for successfully building\nfoundation models for large-scale biological data.\n", "link": "http://arxiv.org/abs/2411.02572v2", "date": "2025-07-16", "relevancy": 2.2175, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTally%20Consistent%3A%20Scaling%20Biological%20Representation%20Learning%20for%20Cell%0A%20%20Microscopy&body=Title%3A%20ViTally%20Consistent%3A%20Scaling%20Biological%20Representation%20Learning%20for%20Cell%0A%20%20Microscopy%0AAuthor%3A%20Kian%20Kenyon-Dean%20and%20Zitong%20Jerry%20Wang%20and%20John%20Urbanik%20and%20Konstantin%20Donhauser%20and%20Jason%20Hartford%20and%20Saber%20Saberian%20and%20Nil%20Sahin%20and%20Ihab%20Bendidi%20and%20Safiye%20Celik%20and%20Marta%20Fay%20and%20Juan%20Sebastian%20Rodriguez%20Vera%20and%20Imran%20S%20Haque%20and%20Oren%20Kraus%0AAbstract%3A%20%20%20Large-scale%20cell%20microscopy%20screens%20are%20used%20in%20drug%20discovery%20and%20molecular%0Abiology%20research%20to%20study%20the%20effects%20of%20millions%20of%20chemical%20and%20genetic%0Aperturbations%20on%20cells.%20To%20use%20these%20images%20in%20downstream%20analysis%2C%20we%20need%0Amodels%20that%20can%20map%20each%20image%20into%20a%20feature%20space%20that%20represents%20diverse%0Abiological%20phenotypes%20consistently%2C%20in%20the%20sense%20that%20perturbations%20with%0Asimilar%20biological%20effects%20have%20similar%20representations.%20In%20this%20work%2C%20we%0Apresent%20the%20largest%20foundation%20model%20for%20cell%20microscopy%20data%20to%20date%2C%20a%20new%0A1.9%20billion-parameter%20ViT-G/8%20MAE%20trained%20on%20over%208%20billion%20microscopy%20image%0Acrops.%20Compared%20to%20a%20previous%20published%20ViT-L/8%20MAE%2C%20our%20new%20model%20achieves%20a%0A60%25%20improvement%20in%20linear%20separability%20of%20genetic%20perturbations%20and%20obtains%20the%0Abest%20overall%20performance%20on%20whole-genome%20biological%20relationship%20recall%20and%0Areplicate%20consistency%20benchmarks.%20Beyond%20scaling%2C%20we%20developed%20two%20key%20methods%0Athat%20improve%20performance%3A%20%281%29%20training%20on%20a%20curated%20and%20diverse%20dataset%3B%20and%2C%0A%282%29%20using%20biologically%20motivated%20linear%20probing%20tasks%20to%20search%20across%20each%0Atransformer%20block%20for%20the%20best%20candidate%20representation%20of%20whole-genome%0Ascreens.%20We%20find%20that%20many%20self-supervised%20vision%20transformers%2C%20pretrained%20on%0Aeither%20natural%20or%20microscopy%20images%2C%20yield%20significantly%20more%20biologically%0Ameaningful%20representations%20of%20microscopy%20images%20in%20their%20intermediate%20blocks%0Athan%20in%20their%20typically%20used%20final%20blocks.%20More%20broadly%2C%20our%20approach%20and%0Aresults%20provide%20insights%20toward%20a%20general%20strategy%20for%20successfully%20building%0Afoundation%20models%20for%20large-scale%20biological%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTally%2520Consistent%253A%2520Scaling%2520Biological%2520Representation%2520Learning%2520for%2520Cell%250A%2520%2520Microscopy%26entry.906535625%3DKian%2520Kenyon-Dean%2520and%2520Zitong%2520Jerry%2520Wang%2520and%2520John%2520Urbanik%2520and%2520Konstantin%2520Donhauser%2520and%2520Jason%2520Hartford%2520and%2520Saber%2520Saberian%2520and%2520Nil%2520Sahin%2520and%2520Ihab%2520Bendidi%2520and%2520Safiye%2520Celik%2520and%2520Marta%2520Fay%2520and%2520Juan%2520Sebastian%2520Rodriguez%2520Vera%2520and%2520Imran%2520S%2520Haque%2520and%2520Oren%2520Kraus%26entry.1292438233%3D%2520%2520Large-scale%2520cell%2520microscopy%2520screens%2520are%2520used%2520in%2520drug%2520discovery%2520and%2520molecular%250Abiology%2520research%2520to%2520study%2520the%2520effects%2520of%2520millions%2520of%2520chemical%2520and%2520genetic%250Aperturbations%2520on%2520cells.%2520To%2520use%2520these%2520images%2520in%2520downstream%2520analysis%252C%2520we%2520need%250Amodels%2520that%2520can%2520map%2520each%2520image%2520into%2520a%2520feature%2520space%2520that%2520represents%2520diverse%250Abiological%2520phenotypes%2520consistently%252C%2520in%2520the%2520sense%2520that%2520perturbations%2520with%250Asimilar%2520biological%2520effects%2520have%2520similar%2520representations.%2520In%2520this%2520work%252C%2520we%250Apresent%2520the%2520largest%2520foundation%2520model%2520for%2520cell%2520microscopy%2520data%2520to%2520date%252C%2520a%2520new%250A1.9%2520billion-parameter%2520ViT-G/8%2520MAE%2520trained%2520on%2520over%25208%2520billion%2520microscopy%2520image%250Acrops.%2520Compared%2520to%2520a%2520previous%2520published%2520ViT-L/8%2520MAE%252C%2520our%2520new%2520model%2520achieves%2520a%250A60%2525%2520improvement%2520in%2520linear%2520separability%2520of%2520genetic%2520perturbations%2520and%2520obtains%2520the%250Abest%2520overall%2520performance%2520on%2520whole-genome%2520biological%2520relationship%2520recall%2520and%250Areplicate%2520consistency%2520benchmarks.%2520Beyond%2520scaling%252C%2520we%2520developed%2520two%2520key%2520methods%250Athat%2520improve%2520performance%253A%2520%25281%2529%2520training%2520on%2520a%2520curated%2520and%2520diverse%2520dataset%253B%2520and%252C%250A%25282%2529%2520using%2520biologically%2520motivated%2520linear%2520probing%2520tasks%2520to%2520search%2520across%2520each%250Atransformer%2520block%2520for%2520the%2520best%2520candidate%2520representation%2520of%2520whole-genome%250Ascreens.%2520We%2520find%2520that%2520many%2520self-supervised%2520vision%2520transformers%252C%2520pretrained%2520on%250Aeither%2520natural%2520or%2520microscopy%2520images%252C%2520yield%2520significantly%2520more%2520biologically%250Ameaningful%2520representations%2520of%2520microscopy%2520images%2520in%2520their%2520intermediate%2520blocks%250Athan%2520in%2520their%2520typically%2520used%2520final%2520blocks.%2520More%2520broadly%252C%2520our%2520approach%2520and%250Aresults%2520provide%2520insights%2520toward%2520a%2520general%2520strategy%2520for%2520successfully%2520building%250Afoundation%2520models%2520for%2520large-scale%2520biological%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTally%20Consistent%3A%20Scaling%20Biological%20Representation%20Learning%20for%20Cell%0A%20%20Microscopy&entry.906535625=Kian%20Kenyon-Dean%20and%20Zitong%20Jerry%20Wang%20and%20John%20Urbanik%20and%20Konstantin%20Donhauser%20and%20Jason%20Hartford%20and%20Saber%20Saberian%20and%20Nil%20Sahin%20and%20Ihab%20Bendidi%20and%20Safiye%20Celik%20and%20Marta%20Fay%20and%20Juan%20Sebastian%20Rodriguez%20Vera%20and%20Imran%20S%20Haque%20and%20Oren%20Kraus&entry.1292438233=%20%20Large-scale%20cell%20microscopy%20screens%20are%20used%20in%20drug%20discovery%20and%20molecular%0Abiology%20research%20to%20study%20the%20effects%20of%20millions%20of%20chemical%20and%20genetic%0Aperturbations%20on%20cells.%20To%20use%20these%20images%20in%20downstream%20analysis%2C%20we%20need%0Amodels%20that%20can%20map%20each%20image%20into%20a%20feature%20space%20that%20represents%20diverse%0Abiological%20phenotypes%20consistently%2C%20in%20the%20sense%20that%20perturbations%20with%0Asimilar%20biological%20effects%20have%20similar%20representations.%20In%20this%20work%2C%20we%0Apresent%20the%20largest%20foundation%20model%20for%20cell%20microscopy%20data%20to%20date%2C%20a%20new%0A1.9%20billion-parameter%20ViT-G/8%20MAE%20trained%20on%20over%208%20billion%20microscopy%20image%0Acrops.%20Compared%20to%20a%20previous%20published%20ViT-L/8%20MAE%2C%20our%20new%20model%20achieves%20a%0A60%25%20improvement%20in%20linear%20separability%20of%20genetic%20perturbations%20and%20obtains%20the%0Abest%20overall%20performance%20on%20whole-genome%20biological%20relationship%20recall%20and%0Areplicate%20consistency%20benchmarks.%20Beyond%20scaling%2C%20we%20developed%20two%20key%20methods%0Athat%20improve%20performance%3A%20%281%29%20training%20on%20a%20curated%20and%20diverse%20dataset%3B%20and%2C%0A%282%29%20using%20biologically%20motivated%20linear%20probing%20tasks%20to%20search%20across%20each%0Atransformer%20block%20for%20the%20best%20candidate%20representation%20of%20whole-genome%0Ascreens.%20We%20find%20that%20many%20self-supervised%20vision%20transformers%2C%20pretrained%20on%0Aeither%20natural%20or%20microscopy%20images%2C%20yield%20significantly%20more%20biologically%0Ameaningful%20representations%20of%20microscopy%20images%20in%20their%20intermediate%20blocks%0Athan%20in%20their%20typically%20used%20final%20blocks.%20More%20broadly%2C%20our%20approach%20and%0Aresults%20provide%20insights%20toward%20a%20general%20strategy%20for%20successfully%20building%0Afoundation%20models%20for%20large-scale%20biological%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02572v2&entry.124074799=Read"},
{"title": "Quantifying calibration error in modern neural networks through evidence\n  based theory", "author": "Koffi Ismael Ouattara", "abstract": "  Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.\n", "link": "http://arxiv.org/abs/2411.00265v2", "date": "2025-07-16", "relevancy": 2.2139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5847}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20calibration%20error%20in%20modern%20neural%20networks%20through%20evidence%0A%20%20based%20theory&body=Title%3A%20Quantifying%20calibration%20error%20in%20modern%20neural%20networks%20through%20evidence%0A%20%20based%20theory%0AAuthor%3A%20Koffi%20Ismael%20Ouattara%0AAbstract%3A%20%20%20Trustworthiness%20in%20neural%20networks%20is%20crucial%20for%20their%20deployment%20in%0Acritical%20applications%2C%20where%20reliability%2C%20confidence%2C%20and%20uncertainty%20play%0Apivotal%20roles%20in%20decision-making.%20Traditional%20performance%20metrics%20such%20as%0Aaccuracy%20and%20precision%20fail%20to%20capture%20these%20aspects%2C%20particularly%20in%20cases%0Awhere%20models%20exhibit%20overconfidence.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20a%20novel%20framework%20for%20quantifying%20the%20trustworthiness%20of%20neural%0Anetworks%20by%20incorporating%20subjective%20logic%20into%20the%20evaluation%20of%20Expected%0ACalibration%20Error%20%28ECE%29.%20This%20method%20provides%20a%20comprehensive%20measure%20of%20trust%2C%0Adisbelief%2C%20and%20uncertainty%20by%20clustering%20predicted%20probabilities%20and%20fusing%0Aopinions%20using%20appropriate%20fusion%20operators.%20We%20demonstrate%20the%20effectiveness%0Aof%20this%20approach%20through%20experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%2C%20where%0Apost-calibration%20results%20indicate%20improved%20trustworthiness.%20The%20proposed%0Aframework%20offers%20a%20more%20interpretable%20and%20nuanced%20assessment%20of%20AI%20models%2C%20with%0Apotential%20applications%20in%20sensitive%20domains%20such%20as%20healthcare%20and%20autonomous%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520calibration%2520error%2520in%2520modern%2520neural%2520networks%2520through%2520evidence%250A%2520%2520based%2520theory%26entry.906535625%3DKoffi%2520Ismael%2520Ouattara%26entry.1292438233%3D%2520%2520Trustworthiness%2520in%2520neural%2520networks%2520is%2520crucial%2520for%2520their%2520deployment%2520in%250Acritical%2520applications%252C%2520where%2520reliability%252C%2520confidence%252C%2520and%2520uncertainty%2520play%250Apivotal%2520roles%2520in%2520decision-making.%2520Traditional%2520performance%2520metrics%2520such%2520as%250Aaccuracy%2520and%2520precision%2520fail%2520to%2520capture%2520these%2520aspects%252C%2520particularly%2520in%2520cases%250Awhere%2520models%2520exhibit%2520overconfidence.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520framework%2520for%2520quantifying%2520the%2520trustworthiness%2520of%2520neural%250Anetworks%2520by%2520incorporating%2520subjective%2520logic%2520into%2520the%2520evaluation%2520of%2520Expected%250ACalibration%2520Error%2520%2528ECE%2529.%2520This%2520method%2520provides%2520a%2520comprehensive%2520measure%2520of%2520trust%252C%250Adisbelief%252C%2520and%2520uncertainty%2520by%2520clustering%2520predicted%2520probabilities%2520and%2520fusing%250Aopinions%2520using%2520appropriate%2520fusion%2520operators.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520this%2520approach%2520through%2520experiments%2520on%2520MNIST%2520and%2520CIFAR-10%2520datasets%252C%2520where%250Apost-calibration%2520results%2520indicate%2520improved%2520trustworthiness.%2520The%2520proposed%250Aframework%2520offers%2520a%2520more%2520interpretable%2520and%2520nuanced%2520assessment%2520of%2520AI%2520models%252C%2520with%250Apotential%2520applications%2520in%2520sensitive%2520domains%2520such%2520as%2520healthcare%2520and%2520autonomous%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20calibration%20error%20in%20modern%20neural%20networks%20through%20evidence%0A%20%20based%20theory&entry.906535625=Koffi%20Ismael%20Ouattara&entry.1292438233=%20%20Trustworthiness%20in%20neural%20networks%20is%20crucial%20for%20their%20deployment%20in%0Acritical%20applications%2C%20where%20reliability%2C%20confidence%2C%20and%20uncertainty%20play%0Apivotal%20roles%20in%20decision-making.%20Traditional%20performance%20metrics%20such%20as%0Aaccuracy%20and%20precision%20fail%20to%20capture%20these%20aspects%2C%20particularly%20in%20cases%0Awhere%20models%20exhibit%20overconfidence.%20To%20address%20these%20limitations%2C%20this%20paper%0Aintroduces%20a%20novel%20framework%20for%20quantifying%20the%20trustworthiness%20of%20neural%0Anetworks%20by%20incorporating%20subjective%20logic%20into%20the%20evaluation%20of%20Expected%0ACalibration%20Error%20%28ECE%29.%20This%20method%20provides%20a%20comprehensive%20measure%20of%20trust%2C%0Adisbelief%2C%20and%20uncertainty%20by%20clustering%20predicted%20probabilities%20and%20fusing%0Aopinions%20using%20appropriate%20fusion%20operators.%20We%20demonstrate%20the%20effectiveness%0Aof%20this%20approach%20through%20experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%2C%20where%0Apost-calibration%20results%20indicate%20improved%20trustworthiness.%20The%20proposed%0Aframework%20offers%20a%20more%20interpretable%20and%20nuanced%20assessment%20of%20AI%20models%2C%20with%0Apotential%20applications%20in%20sensitive%20domains%20such%20as%20healthcare%20and%20autonomous%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00265v2&entry.124074799=Read"},
{"title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "author": "Santosh Vasa and Aditi Ramadwar and Jnana Rama Krishna Darabattula and Md Zafar Anwar and Stanislaw Antol and Andrei Vatavu and Thomas Monninger and Sihao Ding", "abstract": "  Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.\n", "link": "http://arxiv.org/abs/2507.12414v1", "date": "2025-07-16", "relevancy": 2.1946, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoVDC%3A%20Automated%20Vision%20Data%20Cleaning%20Using%20Vision-Language%20Models&body=Title%3A%20AutoVDC%3A%20Automated%20Vision%20Data%20Cleaning%20Using%20Vision-Language%20Models%0AAuthor%3A%20Santosh%20Vasa%20and%20Aditi%20Ramadwar%20and%20Jnana%20Rama%20Krishna%20Darabattula%20and%20Md%20Zafar%20Anwar%20and%20Stanislaw%20Antol%20and%20Andrei%20Vatavu%20and%20Thomas%20Monninger%20and%20Sihao%20Ding%0AAbstract%3A%20%20%20Training%20of%20autonomous%20driving%20systems%20requires%20extensive%20datasets%20with%0Aprecise%20annotations%20to%20attain%20robust%20performance.%20Human%20annotations%20suffer%20from%0Aimperfections%2C%20and%20multiple%20iterations%20are%20often%20needed%20to%20produce%20high-quality%0Adatasets.%20However%2C%20manually%20reviewing%20large%20datasets%20is%20laborious%20and%0Aexpensive.%20In%20this%20paper%2C%20we%20introduce%20AutoVDC%20%28Automated%20Vision%20Data%20Cleaning%29%0Aframework%20and%20investigate%20the%20utilization%20of%20Vision-Language%20Models%20%28VLMs%29%20to%0Aautomatically%20identify%20erroneous%20annotations%20in%20vision%20datasets%2C%20thereby%0Aenabling%20users%20to%20eliminate%20these%20errors%20and%20enhance%20data%20quality.%20We%20validate%0Aour%20approach%20using%20the%20KITTI%20and%20nuImages%20datasets%2C%20which%20contain%20object%0Adetection%20benchmarks%20for%20autonomous%20driving.%20To%20test%20the%20effectiveness%20of%0AAutoVDC%2C%20we%20create%20dataset%20variants%20with%20intentionally%20injected%20erroneous%0Aannotations%20and%20observe%20the%20error%20detection%20rate%20of%20our%20approach.%20Additionally%2C%0Awe%20compare%20the%20detection%20rates%20using%20different%20VLMs%20and%20explore%20the%20impact%20of%0AVLM%20fine-tuning%20on%20our%20pipeline.%20The%20results%20demonstrate%20our%20method%27s%20high%0Aperformance%20in%20error%20detection%20and%20data%20cleaning%20experiments%2C%20indicating%20its%0Apotential%20to%20significantly%20improve%20the%20reliability%20and%20accuracy%20of%20large-scale%0Aproduction%20datasets%20in%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoVDC%253A%2520Automated%2520Vision%2520Data%2520Cleaning%2520Using%2520Vision-Language%2520Models%26entry.906535625%3DSantosh%2520Vasa%2520and%2520Aditi%2520Ramadwar%2520and%2520Jnana%2520Rama%2520Krishna%2520Darabattula%2520and%2520Md%2520Zafar%2520Anwar%2520and%2520Stanislaw%2520Antol%2520and%2520Andrei%2520Vatavu%2520and%2520Thomas%2520Monninger%2520and%2520Sihao%2520Ding%26entry.1292438233%3D%2520%2520Training%2520of%2520autonomous%2520driving%2520systems%2520requires%2520extensive%2520datasets%2520with%250Aprecise%2520annotations%2520to%2520attain%2520robust%2520performance.%2520Human%2520annotations%2520suffer%2520from%250Aimperfections%252C%2520and%2520multiple%2520iterations%2520are%2520often%2520needed%2520to%2520produce%2520high-quality%250Adatasets.%2520However%252C%2520manually%2520reviewing%2520large%2520datasets%2520is%2520laborious%2520and%250Aexpensive.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AutoVDC%2520%2528Automated%2520Vision%2520Data%2520Cleaning%2529%250Aframework%2520and%2520investigate%2520the%2520utilization%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%250Aautomatically%2520identify%2520erroneous%2520annotations%2520in%2520vision%2520datasets%252C%2520thereby%250Aenabling%2520users%2520to%2520eliminate%2520these%2520errors%2520and%2520enhance%2520data%2520quality.%2520We%2520validate%250Aour%2520approach%2520using%2520the%2520KITTI%2520and%2520nuImages%2520datasets%252C%2520which%2520contain%2520object%250Adetection%2520benchmarks%2520for%2520autonomous%2520driving.%2520To%2520test%2520the%2520effectiveness%2520of%250AAutoVDC%252C%2520we%2520create%2520dataset%2520variants%2520with%2520intentionally%2520injected%2520erroneous%250Aannotations%2520and%2520observe%2520the%2520error%2520detection%2520rate%2520of%2520our%2520approach.%2520Additionally%252C%250Awe%2520compare%2520the%2520detection%2520rates%2520using%2520different%2520VLMs%2520and%2520explore%2520the%2520impact%2520of%250AVLM%2520fine-tuning%2520on%2520our%2520pipeline.%2520The%2520results%2520demonstrate%2520our%2520method%2527s%2520high%250Aperformance%2520in%2520error%2520detection%2520and%2520data%2520cleaning%2520experiments%252C%2520indicating%2520its%250Apotential%2520to%2520significantly%2520improve%2520the%2520reliability%2520and%2520accuracy%2520of%2520large-scale%250Aproduction%2520datasets%2520in%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoVDC%3A%20Automated%20Vision%20Data%20Cleaning%20Using%20Vision-Language%20Models&entry.906535625=Santosh%20Vasa%20and%20Aditi%20Ramadwar%20and%20Jnana%20Rama%20Krishna%20Darabattula%20and%20Md%20Zafar%20Anwar%20and%20Stanislaw%20Antol%20and%20Andrei%20Vatavu%20and%20Thomas%20Monninger%20and%20Sihao%20Ding&entry.1292438233=%20%20Training%20of%20autonomous%20driving%20systems%20requires%20extensive%20datasets%20with%0Aprecise%20annotations%20to%20attain%20robust%20performance.%20Human%20annotations%20suffer%20from%0Aimperfections%2C%20and%20multiple%20iterations%20are%20often%20needed%20to%20produce%20high-quality%0Adatasets.%20However%2C%20manually%20reviewing%20large%20datasets%20is%20laborious%20and%0Aexpensive.%20In%20this%20paper%2C%20we%20introduce%20AutoVDC%20%28Automated%20Vision%20Data%20Cleaning%29%0Aframework%20and%20investigate%20the%20utilization%20of%20Vision-Language%20Models%20%28VLMs%29%20to%0Aautomatically%20identify%20erroneous%20annotations%20in%20vision%20datasets%2C%20thereby%0Aenabling%20users%20to%20eliminate%20these%20errors%20and%20enhance%20data%20quality.%20We%20validate%0Aour%20approach%20using%20the%20KITTI%20and%20nuImages%20datasets%2C%20which%20contain%20object%0Adetection%20benchmarks%20for%20autonomous%20driving.%20To%20test%20the%20effectiveness%20of%0AAutoVDC%2C%20we%20create%20dataset%20variants%20with%20intentionally%20injected%20erroneous%0Aannotations%20and%20observe%20the%20error%20detection%20rate%20of%20our%20approach.%20Additionally%2C%0Awe%20compare%20the%20detection%20rates%20using%20different%20VLMs%20and%20explore%20the%20impact%20of%0AVLM%20fine-tuning%20on%20our%20pipeline.%20The%20results%20demonstrate%20our%20method%27s%20high%0Aperformance%20in%20error%20detection%20and%20data%20cleaning%20experiments%2C%20indicating%20its%0Apotential%20to%20significantly%20improve%20the%20reliability%20and%20accuracy%20of%20large-scale%0Aproduction%20datasets%20in%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12414v1&entry.124074799=Read"},
{"title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality", "author": "Yiqin Zhao and Mallesham Dasari and Tian Guo", "abstract": "  High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.02179v3", "date": "2025-07-16", "relevancy": 2.172, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5508}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5433}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality&body=Title%3A%20CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality%0AAuthor%3A%20Yiqin%20Zhao%20and%20Mallesham%20Dasari%20and%20Tian%20Guo%0AAbstract%3A%20%20%20High-quality%20environment%20lighting%20is%20essential%20for%20creating%20immersive%20mobile%0Aaugmented%20reality%20%28AR%29%20experiences.%20However%2C%20achieving%20visually%20coherent%0Aestimation%20for%20mobile%20AR%20is%20challenging%20due%20to%20several%20key%20limitations%20in%20AR%0Adevice%20sensing%20capabilities%2C%20including%20low%20camera%20FoV%20and%20limited%20pixel%20dynamic%0Aranges.%20Recent%20advancements%20in%20generative%20AI%2C%20which%20can%20generate%20high-quality%0Aimages%20from%20different%20types%20of%20prompts%2C%20including%20texts%20and%20images%2C%20present%20a%0Apotential%20solution%20for%20high-quality%20lighting%20estimation.%20Still%2C%20to%20effectively%0Ause%20generative%20image%20diffusion%20models%2C%20we%20must%20address%20two%20key%20limitations%20of%0Acontent%20quality%20and%20slow%20inference.%20In%20this%20work%2C%20we%20design%20and%20implement%20a%0Agenerative%20lighting%20estimation%20system%20called%20CleAR%20that%20can%20produce%0Ahigh-quality%2C%20diverse%20environment%20maps%20in%20the%20format%20of%20360%7B%5Cdeg%7D%20HDR%20images.%0ASpecifically%2C%20we%20design%20a%20two-step%20generation%20pipeline%20guided%20by%20AR%20environment%0Acontext%20data%20to%20ensure%20the%20output%20aligns%20with%20the%20physical%20environment%27s%20visual%0Acontext%20and%20color%20appearance.%20To%20improve%20the%20estimation%20robustness%20under%0Adifferent%20lighting%20conditions%2C%20we%20design%20a%20real-time%20refinement%20component%20to%0Aadjust%20lighting%20estimation%20results%20on%20AR%20devices.%20Through%20a%20combination%20of%0Aquantitative%20and%20qualitative%20evaluations%2C%20we%20show%20that%20CleAR%20outperforms%0Astate-of-the-art%20lighting%20estimation%20methods%20on%20both%20estimation%20accuracy%2C%0Alatency%2C%20and%20robustness%2C%20and%20is%20rated%20by%2031%20participants%20as%20producing%20better%0Arenderings%20for%20most%20virtual%20objects.%20For%20example%2C%20CleAR%20achieves%2051%25%20to%2056%25%0Aaccuracy%20improvement%20on%20virtual%20object%20renderings%20across%20objects%20of%20three%0Adistinctive%20types%20of%20materials%20and%20reflective%20properties.%20CleAR%20produces%0Alighting%20estimates%20of%20comparable%20or%20better%20quality%20in%20just%203.2%20seconds%20--%20over%0A110X%20faster%20than%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02179v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCleAR%253A%2520Robust%2520Context-Guided%2520Generative%2520Lighting%2520Estimation%2520for%2520Mobile%250A%2520%2520Augmented%2520Reality%26entry.906535625%3DYiqin%2520Zhao%2520and%2520Mallesham%2520Dasari%2520and%2520Tian%2520Guo%26entry.1292438233%3D%2520%2520High-quality%2520environment%2520lighting%2520is%2520essential%2520for%2520creating%2520immersive%2520mobile%250Aaugmented%2520reality%2520%2528AR%2529%2520experiences.%2520However%252C%2520achieving%2520visually%2520coherent%250Aestimation%2520for%2520mobile%2520AR%2520is%2520challenging%2520due%2520to%2520several%2520key%2520limitations%2520in%2520AR%250Adevice%2520sensing%2520capabilities%252C%2520including%2520low%2520camera%2520FoV%2520and%2520limited%2520pixel%2520dynamic%250Aranges.%2520Recent%2520advancements%2520in%2520generative%2520AI%252C%2520which%2520can%2520generate%2520high-quality%250Aimages%2520from%2520different%2520types%2520of%2520prompts%252C%2520including%2520texts%2520and%2520images%252C%2520present%2520a%250Apotential%2520solution%2520for%2520high-quality%2520lighting%2520estimation.%2520Still%252C%2520to%2520effectively%250Ause%2520generative%2520image%2520diffusion%2520models%252C%2520we%2520must%2520address%2520two%2520key%2520limitations%2520of%250Acontent%2520quality%2520and%2520slow%2520inference.%2520In%2520this%2520work%252C%2520we%2520design%2520and%2520implement%2520a%250Agenerative%2520lighting%2520estimation%2520system%2520called%2520CleAR%2520that%2520can%2520produce%250Ahigh-quality%252C%2520diverse%2520environment%2520maps%2520in%2520the%2520format%2520of%2520360%257B%255Cdeg%257D%2520HDR%2520images.%250ASpecifically%252C%2520we%2520design%2520a%2520two-step%2520generation%2520pipeline%2520guided%2520by%2520AR%2520environment%250Acontext%2520data%2520to%2520ensure%2520the%2520output%2520aligns%2520with%2520the%2520physical%2520environment%2527s%2520visual%250Acontext%2520and%2520color%2520appearance.%2520To%2520improve%2520the%2520estimation%2520robustness%2520under%250Adifferent%2520lighting%2520conditions%252C%2520we%2520design%2520a%2520real-time%2520refinement%2520component%2520to%250Aadjust%2520lighting%2520estimation%2520results%2520on%2520AR%2520devices.%2520Through%2520a%2520combination%2520of%250Aquantitative%2520and%2520qualitative%2520evaluations%252C%2520we%2520show%2520that%2520CleAR%2520outperforms%250Astate-of-the-art%2520lighting%2520estimation%2520methods%2520on%2520both%2520estimation%2520accuracy%252C%250Alatency%252C%2520and%2520robustness%252C%2520and%2520is%2520rated%2520by%252031%2520participants%2520as%2520producing%2520better%250Arenderings%2520for%2520most%2520virtual%2520objects.%2520For%2520example%252C%2520CleAR%2520achieves%252051%2525%2520to%252056%2525%250Aaccuracy%2520improvement%2520on%2520virtual%2520object%2520renderings%2520across%2520objects%2520of%2520three%250Adistinctive%2520types%2520of%2520materials%2520and%2520reflective%2520properties.%2520CleAR%2520produces%250Alighting%2520estimates%2520of%2520comparable%2520or%2520better%2520quality%2520in%2520just%25203.2%2520seconds%2520--%2520over%250A110X%2520faster%2520than%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02179v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality&entry.906535625=Yiqin%20Zhao%20and%20Mallesham%20Dasari%20and%20Tian%20Guo&entry.1292438233=%20%20High-quality%20environment%20lighting%20is%20essential%20for%20creating%20immersive%20mobile%0Aaugmented%20reality%20%28AR%29%20experiences.%20However%2C%20achieving%20visually%20coherent%0Aestimation%20for%20mobile%20AR%20is%20challenging%20due%20to%20several%20key%20limitations%20in%20AR%0Adevice%20sensing%20capabilities%2C%20including%20low%20camera%20FoV%20and%20limited%20pixel%20dynamic%0Aranges.%20Recent%20advancements%20in%20generative%20AI%2C%20which%20can%20generate%20high-quality%0Aimages%20from%20different%20types%20of%20prompts%2C%20including%20texts%20and%20images%2C%20present%20a%0Apotential%20solution%20for%20high-quality%20lighting%20estimation.%20Still%2C%20to%20effectively%0Ause%20generative%20image%20diffusion%20models%2C%20we%20must%20address%20two%20key%20limitations%20of%0Acontent%20quality%20and%20slow%20inference.%20In%20this%20work%2C%20we%20design%20and%20implement%20a%0Agenerative%20lighting%20estimation%20system%20called%20CleAR%20that%20can%20produce%0Ahigh-quality%2C%20diverse%20environment%20maps%20in%20the%20format%20of%20360%7B%5Cdeg%7D%20HDR%20images.%0ASpecifically%2C%20we%20design%20a%20two-step%20generation%20pipeline%20guided%20by%20AR%20environment%0Acontext%20data%20to%20ensure%20the%20output%20aligns%20with%20the%20physical%20environment%27s%20visual%0Acontext%20and%20color%20appearance.%20To%20improve%20the%20estimation%20robustness%20under%0Adifferent%20lighting%20conditions%2C%20we%20design%20a%20real-time%20refinement%20component%20to%0Aadjust%20lighting%20estimation%20results%20on%20AR%20devices.%20Through%20a%20combination%20of%0Aquantitative%20and%20qualitative%20evaluations%2C%20we%20show%20that%20CleAR%20outperforms%0Astate-of-the-art%20lighting%20estimation%20methods%20on%20both%20estimation%20accuracy%2C%0Alatency%2C%20and%20robustness%2C%20and%20is%20rated%20by%2031%20participants%20as%20producing%20better%0Arenderings%20for%20most%20virtual%20objects.%20For%20example%2C%20CleAR%20achieves%2051%25%20to%2056%25%0Aaccuracy%20improvement%20on%20virtual%20object%20renderings%20across%20objects%20of%20three%0Adistinctive%20types%20of%20materials%20and%20reflective%20properties.%20CleAR%20produces%0Alighting%20estimates%20of%20comparable%20or%20better%20quality%20in%20just%203.2%20seconds%20--%20over%0A110X%20faster%20than%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02179v3&entry.124074799=Read"},
{"title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning", "author": "Atticus Wang and Joshua Engels and Oliver Clive-Griffin and Senthooran Rajamanoharan and Neel Nanda", "abstract": "  Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.\n", "link": "http://arxiv.org/abs/2507.08218v2", "date": "2025-07-16", "relevancy": 2.0792, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Mechanistic%20Explanations%20for%20Out-Of-Context%20Reasoning&body=Title%3A%20Simple%20Mechanistic%20Explanations%20for%20Out-Of-Context%20Reasoning%0AAuthor%3A%20Atticus%20Wang%20and%20Joshua%20Engels%20and%20Oliver%20Clive-Griffin%20and%20Senthooran%20Rajamanoharan%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Out-of-context%20reasoning%20%28OOCR%29%20is%20a%20phenomenon%20in%20which%20fine-tuned%20LLMs%0Aexhibit%20surprisingly%20deep%20out-of-distribution%20generalization.%20Rather%20than%0Alearning%20shallow%20heuristics%2C%20they%20implicitly%20internalize%20and%20act%20on%20the%0Aconsequences%20of%20observations%20scattered%20throughout%20the%20fine-tuning%20data.%20In%20this%0Awork%2C%20we%20investigate%20this%20phenomenon%20mechanistically%20and%20find%20that%20many%0Ainstances%20of%20OOCR%20in%20the%20literature%20have%20a%20simple%20explanation%3A%20the%20LoRA%0Afine-tuning%20essentially%20adds%20a%20constant%20steering%20vector%2C%20steering%20the%20model%0Atowards%20a%20general%20concept.%20This%20improves%20performance%20on%20the%20fine-tuning%20task%0Aand%20in%20many%20other%20concept-related%20domains%2C%20causing%20the%20surprising%0Ageneralization.%20Moreover%2C%20we%20can%20directly%20train%20steering%20vectors%20for%20these%0Atasks%20from%20scratch%2C%20which%20also%20induces%20OOCR.%20We%20find%20that%20our%20results%20hold%20even%0Afor%20a%20task%20that%20seems%20like%20it%20must%20involve%20conditional%20behavior%20%28model%0Abackdoors%29%3B%20it%20turns%20out%20that%20unconditionally%20adding%20a%20steering%20vector%20is%0Asufficient.%20Overall%2C%20our%20work%20presents%20one%20explanation%20of%20what%20gets%20learned%0Aduring%20fine-tuning%20for%20OOCR%20tasks%2C%20contributing%20to%20the%20key%20question%20of%20why%20LLMs%0Acan%20reason%20out%20of%20context%2C%20an%20advanced%20capability%20that%20is%20highly%20relevant%20to%0Atheir%20safe%20and%20reliable%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Mechanistic%2520Explanations%2520for%2520Out-Of-Context%2520Reasoning%26entry.906535625%3DAtticus%2520Wang%2520and%2520Joshua%2520Engels%2520and%2520Oliver%2520Clive-Griffin%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Out-of-context%2520reasoning%2520%2528OOCR%2529%2520is%2520a%2520phenomenon%2520in%2520which%2520fine-tuned%2520LLMs%250Aexhibit%2520surprisingly%2520deep%2520out-of-distribution%2520generalization.%2520Rather%2520than%250Alearning%2520shallow%2520heuristics%252C%2520they%2520implicitly%2520internalize%2520and%2520act%2520on%2520the%250Aconsequences%2520of%2520observations%2520scattered%2520throughout%2520the%2520fine-tuning%2520data.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520this%2520phenomenon%2520mechanistically%2520and%2520find%2520that%2520many%250Ainstances%2520of%2520OOCR%2520in%2520the%2520literature%2520have%2520a%2520simple%2520explanation%253A%2520the%2520LoRA%250Afine-tuning%2520essentially%2520adds%2520a%2520constant%2520steering%2520vector%252C%2520steering%2520the%2520model%250Atowards%2520a%2520general%2520concept.%2520This%2520improves%2520performance%2520on%2520the%2520fine-tuning%2520task%250Aand%2520in%2520many%2520other%2520concept-related%2520domains%252C%2520causing%2520the%2520surprising%250Ageneralization.%2520Moreover%252C%2520we%2520can%2520directly%2520train%2520steering%2520vectors%2520for%2520these%250Atasks%2520from%2520scratch%252C%2520which%2520also%2520induces%2520OOCR.%2520We%2520find%2520that%2520our%2520results%2520hold%2520even%250Afor%2520a%2520task%2520that%2520seems%2520like%2520it%2520must%2520involve%2520conditional%2520behavior%2520%2528model%250Abackdoors%2529%253B%2520it%2520turns%2520out%2520that%2520unconditionally%2520adding%2520a%2520steering%2520vector%2520is%250Asufficient.%2520Overall%252C%2520our%2520work%2520presents%2520one%2520explanation%2520of%2520what%2520gets%2520learned%250Aduring%2520fine-tuning%2520for%2520OOCR%2520tasks%252C%2520contributing%2520to%2520the%2520key%2520question%2520of%2520why%2520LLMs%250Acan%2520reason%2520out%2520of%2520context%252C%2520an%2520advanced%2520capability%2520that%2520is%2520highly%2520relevant%2520to%250Atheir%2520safe%2520and%2520reliable%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Mechanistic%20Explanations%20for%20Out-Of-Context%20Reasoning&entry.906535625=Atticus%20Wang%20and%20Joshua%20Engels%20and%20Oliver%20Clive-Griffin%20and%20Senthooran%20Rajamanoharan%20and%20Neel%20Nanda&entry.1292438233=%20%20Out-of-context%20reasoning%20%28OOCR%29%20is%20a%20phenomenon%20in%20which%20fine-tuned%20LLMs%0Aexhibit%20surprisingly%20deep%20out-of-distribution%20generalization.%20Rather%20than%0Alearning%20shallow%20heuristics%2C%20they%20implicitly%20internalize%20and%20act%20on%20the%0Aconsequences%20of%20observations%20scattered%20throughout%20the%20fine-tuning%20data.%20In%20this%0Awork%2C%20we%20investigate%20this%20phenomenon%20mechanistically%20and%20find%20that%20many%0Ainstances%20of%20OOCR%20in%20the%20literature%20have%20a%20simple%20explanation%3A%20the%20LoRA%0Afine-tuning%20essentially%20adds%20a%20constant%20steering%20vector%2C%20steering%20the%20model%0Atowards%20a%20general%20concept.%20This%20improves%20performance%20on%20the%20fine-tuning%20task%0Aand%20in%20many%20other%20concept-related%20domains%2C%20causing%20the%20surprising%0Ageneralization.%20Moreover%2C%20we%20can%20directly%20train%20steering%20vectors%20for%20these%0Atasks%20from%20scratch%2C%20which%20also%20induces%20OOCR.%20We%20find%20that%20our%20results%20hold%20even%0Afor%20a%20task%20that%20seems%20like%20it%20must%20involve%20conditional%20behavior%20%28model%0Abackdoors%29%3B%20it%20turns%20out%20that%20unconditionally%20adding%20a%20steering%20vector%20is%0Asufficient.%20Overall%2C%20our%20work%20presents%20one%20explanation%20of%20what%20gets%20learned%0Aduring%20fine-tuning%20for%20OOCR%20tasks%2C%20contributing%20to%20the%20key%20question%20of%20why%20LLMs%0Acan%20reason%20out%20of%20context%2C%20an%20advanced%20capability%20that%20is%20highly%20relevant%20to%0Atheir%20safe%20and%20reliable%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08218v2&entry.124074799=Read"},
{"title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data", "author": "Chandana Cheerla", "abstract": "  Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot\n", "link": "http://arxiv.org/abs/2507.12425v1", "date": "2025-07-16", "relevancy": 2.0769, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5311}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5243}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Retrieval-Augmented%20Generation%20for%20Structured%20Enterprise%20and%0A%20%20Internal%20Data&body=Title%3A%20Advancing%20Retrieval-Augmented%20Generation%20for%20Structured%20Enterprise%20and%0A%20%20Internal%20Data%0AAuthor%3A%20Chandana%20Cheerla%0AAbstract%3A%20%20%20Organizations%20increasingly%20rely%20on%20proprietary%20enterprise%20data%2C%20including%20HR%0Arecords%2C%20structured%20reports%2C%20and%20tabular%20documents%2C%20for%20critical%0Adecision-making.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20strong%20generative%0Acapabilities%2C%20they%20are%20limited%20by%20static%20pretraining%2C%20short%20context%20windows%2C%0Aand%20challenges%20in%20processing%20heterogeneous%20data%20formats.%20Conventional%0ARetrieval-Augmented%20Generation%20%28RAG%29%20frameworks%20address%20some%20of%20these%20gaps%20but%0Aoften%20struggle%20with%20structured%20and%20semi-structured%20data.%0A%20%20This%20work%20proposes%20an%20advanced%20RAG%20framework%20that%20combines%20hybrid%20retrieval%0Astrategies%20using%20dense%20embeddings%20%28all-mpnet-base-v2%29%20and%20BM25%2C%20enhanced%20by%0Ametadata-aware%20filtering%20with%20SpaCy%20NER%20and%20cross-encoder%20reranking.%20The%0Aframework%20applies%20semantic%20chunking%20to%20maintain%20textual%20coherence%20and%20retains%0Atabular%20data%20structures%20to%20preserve%20row-column%20integrity.%20Quantized%20indexing%0Aoptimizes%20retrieval%20efficiency%2C%20while%20human-in-the-loop%20feedback%20and%0Aconversation%20memory%20improve%20adaptability.%0A%20%20Experiments%20on%20enterprise%20datasets%20show%20notable%20improvements%3A%20Precision%405%0Aincreased%20by%2015%20percent%20%2890%20versus%2075%29%2C%20Recall%405%20by%2013%20percent%20%2887%20versus%2074%29%2C%0Aand%20Mean%20Reciprocal%20Rank%20by%2016%20percent%20%280.85%20versus%200.69%29.%20Qualitative%0Aevaluations%20show%20higher%20scores%20in%20Faithfulness%20%284.6%20versus%203.0%29%2C%20Completeness%0A%284.2%20versus%202.5%29%2C%20and%20Relevance%20%284.5%20versus%203.2%29%20on%20a%205-point%20Likert%20scale.%0AThese%20results%20demonstrate%20the%20framework%27s%20effectiveness%20in%20delivering%20accurate%2C%0Acomprehensive%2C%20and%20contextually%20relevant%20responses%20for%20enterprise%20tasks.%20Future%0Awork%20includes%20extending%20to%20multimodal%20data%20and%20integrating%20agent-based%0Aretrieval.%20The%20source%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/CheerlaChandana/Enterprise-Chatbot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Retrieval-Augmented%2520Generation%2520for%2520Structured%2520Enterprise%2520and%250A%2520%2520Internal%2520Data%26entry.906535625%3DChandana%2520Cheerla%26entry.1292438233%3D%2520%2520Organizations%2520increasingly%2520rely%2520on%2520proprietary%2520enterprise%2520data%252C%2520including%2520HR%250Arecords%252C%2520structured%2520reports%252C%2520and%2520tabular%2520documents%252C%2520for%2520critical%250Adecision-making.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520strong%2520generative%250Acapabilities%252C%2520they%2520are%2520limited%2520by%2520static%2520pretraining%252C%2520short%2520context%2520windows%252C%250Aand%2520challenges%2520in%2520processing%2520heterogeneous%2520data%2520formats.%2520Conventional%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520frameworks%2520address%2520some%2520of%2520these%2520gaps%2520but%250Aoften%2520struggle%2520with%2520structured%2520and%2520semi-structured%2520data.%250A%2520%2520This%2520work%2520proposes%2520an%2520advanced%2520RAG%2520framework%2520that%2520combines%2520hybrid%2520retrieval%250Astrategies%2520using%2520dense%2520embeddings%2520%2528all-mpnet-base-v2%2529%2520and%2520BM25%252C%2520enhanced%2520by%250Ametadata-aware%2520filtering%2520with%2520SpaCy%2520NER%2520and%2520cross-encoder%2520reranking.%2520The%250Aframework%2520applies%2520semantic%2520chunking%2520to%2520maintain%2520textual%2520coherence%2520and%2520retains%250Atabular%2520data%2520structures%2520to%2520preserve%2520row-column%2520integrity.%2520Quantized%2520indexing%250Aoptimizes%2520retrieval%2520efficiency%252C%2520while%2520human-in-the-loop%2520feedback%2520and%250Aconversation%2520memory%2520improve%2520adaptability.%250A%2520%2520Experiments%2520on%2520enterprise%2520datasets%2520show%2520notable%2520improvements%253A%2520Precision%25405%250Aincreased%2520by%252015%2520percent%2520%252890%2520versus%252075%2529%252C%2520Recall%25405%2520by%252013%2520percent%2520%252887%2520versus%252074%2529%252C%250Aand%2520Mean%2520Reciprocal%2520Rank%2520by%252016%2520percent%2520%25280.85%2520versus%25200.69%2529.%2520Qualitative%250Aevaluations%2520show%2520higher%2520scores%2520in%2520Faithfulness%2520%25284.6%2520versus%25203.0%2529%252C%2520Completeness%250A%25284.2%2520versus%25202.5%2529%252C%2520and%2520Relevance%2520%25284.5%2520versus%25203.2%2529%2520on%2520a%25205-point%2520Likert%2520scale.%250AThese%2520results%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%2520in%2520delivering%2520accurate%252C%250Acomprehensive%252C%2520and%2520contextually%2520relevant%2520responses%2520for%2520enterprise%2520tasks.%2520Future%250Awork%2520includes%2520extending%2520to%2520multimodal%2520data%2520and%2520integrating%2520agent-based%250Aretrieval.%2520The%2520source%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/CheerlaChandana/Enterprise-Chatbot%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Retrieval-Augmented%20Generation%20for%20Structured%20Enterprise%20and%0A%20%20Internal%20Data&entry.906535625=Chandana%20Cheerla&entry.1292438233=%20%20Organizations%20increasingly%20rely%20on%20proprietary%20enterprise%20data%2C%20including%20HR%0Arecords%2C%20structured%20reports%2C%20and%20tabular%20documents%2C%20for%20critical%0Adecision-making.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20strong%20generative%0Acapabilities%2C%20they%20are%20limited%20by%20static%20pretraining%2C%20short%20context%20windows%2C%0Aand%20challenges%20in%20processing%20heterogeneous%20data%20formats.%20Conventional%0ARetrieval-Augmented%20Generation%20%28RAG%29%20frameworks%20address%20some%20of%20these%20gaps%20but%0Aoften%20struggle%20with%20structured%20and%20semi-structured%20data.%0A%20%20This%20work%20proposes%20an%20advanced%20RAG%20framework%20that%20combines%20hybrid%20retrieval%0Astrategies%20using%20dense%20embeddings%20%28all-mpnet-base-v2%29%20and%20BM25%2C%20enhanced%20by%0Ametadata-aware%20filtering%20with%20SpaCy%20NER%20and%20cross-encoder%20reranking.%20The%0Aframework%20applies%20semantic%20chunking%20to%20maintain%20textual%20coherence%20and%20retains%0Atabular%20data%20structures%20to%20preserve%20row-column%20integrity.%20Quantized%20indexing%0Aoptimizes%20retrieval%20efficiency%2C%20while%20human-in-the-loop%20feedback%20and%0Aconversation%20memory%20improve%20adaptability.%0A%20%20Experiments%20on%20enterprise%20datasets%20show%20notable%20improvements%3A%20Precision%405%0Aincreased%20by%2015%20percent%20%2890%20versus%2075%29%2C%20Recall%405%20by%2013%20percent%20%2887%20versus%2074%29%2C%0Aand%20Mean%20Reciprocal%20Rank%20by%2016%20percent%20%280.85%20versus%200.69%29.%20Qualitative%0Aevaluations%20show%20higher%20scores%20in%20Faithfulness%20%284.6%20versus%203.0%29%2C%20Completeness%0A%284.2%20versus%202.5%29%2C%20and%20Relevance%20%284.5%20versus%203.2%29%20on%20a%205-point%20Likert%20scale.%0AThese%20results%20demonstrate%20the%20framework%27s%20effectiveness%20in%20delivering%20accurate%2C%0Acomprehensive%2C%20and%20contextually%20relevant%20responses%20for%20enterprise%20tasks.%20Future%0Awork%20includes%20extending%20to%20multimodal%20data%20and%20integrating%20agent-based%0Aretrieval.%20The%20source%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/CheerlaChandana/Enterprise-Chatbot%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12425v1&entry.124074799=Read"},
{"title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "author": "Feng Xiao and Jicong Fan", "abstract": "  Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.\n", "link": "http://arxiv.org/abs/2507.12295v1", "date": "2025-07-16", "relevancy": 2.0701, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5364}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-ADBench%3A%20Text%20Anomaly%20Detection%20Benchmark%20based%20on%20LLMs%20Embedding&body=Title%3A%20Text-ADBench%3A%20Text%20Anomaly%20Detection%20Benchmark%20based%20on%20LLMs%20Embedding%0AAuthor%3A%20Feng%20Xiao%20and%20Jicong%20Fan%0AAbstract%3A%20%20%20Text%20anomaly%20detection%20is%20a%20critical%20task%20in%20natural%20language%20processing%0A%28NLP%29%2C%20with%20applications%20spanning%20fraud%20detection%2C%20misinformation%0Aidentification%2C%20spam%20detection%20and%20content%20moderation%2C%20etc.%20Despite%20significant%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20and%20anomaly%20detection%20algorithms%2C%20the%0Aabsence%20of%20standardized%20and%20comprehensive%20benchmarks%20for%20evaluating%20the%0Aexisting%20anomaly%20detection%20methods%20on%20text%20data%20limits%20rigorous%20comparison%20and%0Adevelopment%20of%20innovative%20approaches.%20This%20work%20performs%20a%20comprehensive%0Aempirical%20study%20and%20introduces%20a%20benchmark%20for%20text%20anomaly%20detection%2C%0Aleveraging%20embeddings%20from%20diverse%20pre-trained%20language%20models%20across%20a%20wide%0Aarray%20of%20text%20datasets.%20Our%20work%20systematically%20evaluates%20the%20effectiveness%20of%0Aembedding-based%20text%20anomaly%20detection%20by%20incorporating%20%281%29%20early%20language%0Amodels%20%28GloVe%2C%20BERT%29%3B%20%282%29%20multiple%20LLMs%20%28LLaMa-2%2C%20LLama-3%2C%20Mistral%2C%20OpenAI%0A%28small%2C%20ada%2C%20large%29%29%3B%20%283%29%20multi-domain%20text%20datasets%20%28news%2C%20social%20media%2C%0Ascientific%20publications%29%3B%20%284%29%20comprehensive%20evaluation%20metrics%20%28AUROC%2C%20AUPRC%29.%0AOur%20experiments%20reveal%20a%20critical%20empirical%20insight%3A%20embedding%20quality%0Asignificantly%20governs%20anomaly%20detection%20efficacy%2C%20and%20deep%20learning-based%0Aapproaches%20demonstrate%20no%20performance%20advantage%20over%20conventional%20shallow%0Aalgorithms%20%28e.g.%2C%20KNN%2C%20Isolation%20Forest%29%20when%20leveraging%20LLM-derived%0Aembeddings.In%20addition%2C%20we%20observe%20strongly%20low-rank%20characteristics%20in%0Across-model%20performance%20matrices%2C%20which%20enables%20an%20efficient%20strategy%20for%20rapid%0Amodel%20evaluation%20%28or%20embedding%20evaluation%29%20and%20selection%20in%20practical%0Aapplications.%20Furthermore%2C%20by%20open-sourcing%20our%20benchmark%20toolkit%20that%20includes%0Aall%20embeddings%20from%20different%20models%20and%20code%20at%0Ahttps%3A//github.com/jicongfan/Text-Anomaly-Detection-Benchmark%2C%20this%20work%0Aprovides%20a%20foundation%20for%20future%20research%20in%20robust%20and%20scalable%20text%20anomaly%0Adetection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-ADBench%253A%2520Text%2520Anomaly%2520Detection%2520Benchmark%2520based%2520on%2520LLMs%2520Embedding%26entry.906535625%3DFeng%2520Xiao%2520and%2520Jicong%2520Fan%26entry.1292438233%3D%2520%2520Text%2520anomaly%2520detection%2520is%2520a%2520critical%2520task%2520in%2520natural%2520language%2520processing%250A%2528NLP%2529%252C%2520with%2520applications%2520spanning%2520fraud%2520detection%252C%2520misinformation%250Aidentification%252C%2520spam%2520detection%2520and%2520content%2520moderation%252C%2520etc.%2520Despite%2520significant%250Aadvances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520anomaly%2520detection%2520algorithms%252C%2520the%250Aabsence%2520of%2520standardized%2520and%2520comprehensive%2520benchmarks%2520for%2520evaluating%2520the%250Aexisting%2520anomaly%2520detection%2520methods%2520on%2520text%2520data%2520limits%2520rigorous%2520comparison%2520and%250Adevelopment%2520of%2520innovative%2520approaches.%2520This%2520work%2520performs%2520a%2520comprehensive%250Aempirical%2520study%2520and%2520introduces%2520a%2520benchmark%2520for%2520text%2520anomaly%2520detection%252C%250Aleveraging%2520embeddings%2520from%2520diverse%2520pre-trained%2520language%2520models%2520across%2520a%2520wide%250Aarray%2520of%2520text%2520datasets.%2520Our%2520work%2520systematically%2520evaluates%2520the%2520effectiveness%2520of%250Aembedding-based%2520text%2520anomaly%2520detection%2520by%2520incorporating%2520%25281%2529%2520early%2520language%250Amodels%2520%2528GloVe%252C%2520BERT%2529%253B%2520%25282%2529%2520multiple%2520LLMs%2520%2528LLaMa-2%252C%2520LLama-3%252C%2520Mistral%252C%2520OpenAI%250A%2528small%252C%2520ada%252C%2520large%2529%2529%253B%2520%25283%2529%2520multi-domain%2520text%2520datasets%2520%2528news%252C%2520social%2520media%252C%250Ascientific%2520publications%2529%253B%2520%25284%2529%2520comprehensive%2520evaluation%2520metrics%2520%2528AUROC%252C%2520AUPRC%2529.%250AOur%2520experiments%2520reveal%2520a%2520critical%2520empirical%2520insight%253A%2520embedding%2520quality%250Asignificantly%2520governs%2520anomaly%2520detection%2520efficacy%252C%2520and%2520deep%2520learning-based%250Aapproaches%2520demonstrate%2520no%2520performance%2520advantage%2520over%2520conventional%2520shallow%250Aalgorithms%2520%2528e.g.%252C%2520KNN%252C%2520Isolation%2520Forest%2529%2520when%2520leveraging%2520LLM-derived%250Aembeddings.In%2520addition%252C%2520we%2520observe%2520strongly%2520low-rank%2520characteristics%2520in%250Across-model%2520performance%2520matrices%252C%2520which%2520enables%2520an%2520efficient%2520strategy%2520for%2520rapid%250Amodel%2520evaluation%2520%2528or%2520embedding%2520evaluation%2529%2520and%2520selection%2520in%2520practical%250Aapplications.%2520Furthermore%252C%2520by%2520open-sourcing%2520our%2520benchmark%2520toolkit%2520that%2520includes%250Aall%2520embeddings%2520from%2520different%2520models%2520and%2520code%2520at%250Ahttps%253A//github.com/jicongfan/Text-Anomaly-Detection-Benchmark%252C%2520this%2520work%250Aprovides%2520a%2520foundation%2520for%2520future%2520research%2520in%2520robust%2520and%2520scalable%2520text%2520anomaly%250Adetection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-ADBench%3A%20Text%20Anomaly%20Detection%20Benchmark%20based%20on%20LLMs%20Embedding&entry.906535625=Feng%20Xiao%20and%20Jicong%20Fan&entry.1292438233=%20%20Text%20anomaly%20detection%20is%20a%20critical%20task%20in%20natural%20language%20processing%0A%28NLP%29%2C%20with%20applications%20spanning%20fraud%20detection%2C%20misinformation%0Aidentification%2C%20spam%20detection%20and%20content%20moderation%2C%20etc.%20Despite%20significant%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20and%20anomaly%20detection%20algorithms%2C%20the%0Aabsence%20of%20standardized%20and%20comprehensive%20benchmarks%20for%20evaluating%20the%0Aexisting%20anomaly%20detection%20methods%20on%20text%20data%20limits%20rigorous%20comparison%20and%0Adevelopment%20of%20innovative%20approaches.%20This%20work%20performs%20a%20comprehensive%0Aempirical%20study%20and%20introduces%20a%20benchmark%20for%20text%20anomaly%20detection%2C%0Aleveraging%20embeddings%20from%20diverse%20pre-trained%20language%20models%20across%20a%20wide%0Aarray%20of%20text%20datasets.%20Our%20work%20systematically%20evaluates%20the%20effectiveness%20of%0Aembedding-based%20text%20anomaly%20detection%20by%20incorporating%20%281%29%20early%20language%0Amodels%20%28GloVe%2C%20BERT%29%3B%20%282%29%20multiple%20LLMs%20%28LLaMa-2%2C%20LLama-3%2C%20Mistral%2C%20OpenAI%0A%28small%2C%20ada%2C%20large%29%29%3B%20%283%29%20multi-domain%20text%20datasets%20%28news%2C%20social%20media%2C%0Ascientific%20publications%29%3B%20%284%29%20comprehensive%20evaluation%20metrics%20%28AUROC%2C%20AUPRC%29.%0AOur%20experiments%20reveal%20a%20critical%20empirical%20insight%3A%20embedding%20quality%0Asignificantly%20governs%20anomaly%20detection%20efficacy%2C%20and%20deep%20learning-based%0Aapproaches%20demonstrate%20no%20performance%20advantage%20over%20conventional%20shallow%0Aalgorithms%20%28e.g.%2C%20KNN%2C%20Isolation%20Forest%29%20when%20leveraging%20LLM-derived%0Aembeddings.In%20addition%2C%20we%20observe%20strongly%20low-rank%20characteristics%20in%0Across-model%20performance%20matrices%2C%20which%20enables%20an%20efficient%20strategy%20for%20rapid%0Amodel%20evaluation%20%28or%20embedding%20evaluation%29%20and%20selection%20in%20practical%0Aapplications.%20Furthermore%2C%20by%20open-sourcing%20our%20benchmark%20toolkit%20that%20includes%0Aall%20embeddings%20from%20different%20models%20and%20code%20at%0Ahttps%3A//github.com/jicongfan/Text-Anomaly-Detection-Benchmark%2C%20this%20work%0Aprovides%20a%20foundation%20for%20future%20research%20in%20robust%20and%20scalable%20text%20anomaly%0Adetection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12295v1&entry.124074799=Read"},
{"title": "Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep\n  Learning", "author": "Benjamin Prada and Shion Matsumoto and Abdul Malik Zekri and Ankur Mali", "abstract": "  We present the first theoretical framework that connects predictive coding\n(PC), a biologically inspired local learning rule, with the minimum description\nlength (MDL) principle in deep networks. We prove that layerwise PC performs\nblock-coordinate descent on the MDL two-part code objective, thereby jointly\nminimizing empirical risk and model complexity. Using Hoeffding's inequality\nand a prefix-code prior, we derive a novel generalization bound of the form\n$R(\\theta) \\le \\hat{R}(\\theta) + \\frac{L(\\theta)}{N}$, capturing the tradeoff\nbetween fit and compression. We further prove that each PC sweep monotonically\ndecreases the empirical two-part codelength, yielding tighter high-probability\nrisk bounds than unconstrained gradient descent. Finally, we show that repeated\nPC updates converge to a block-coordinate stationary point, providing an\napproximate MDL-optimal solution. To our knowledge, this is the first result\noffering formal generalization and convergence guarantees for PC-trained deep\nmodels, positioning PC as a theoretically grounded and biologically plausible\nalternative to backpropagation.\n", "link": "http://arxiv.org/abs/2505.14635v2", "date": "2025-07-16", "relevancy": 2.0564, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5136}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Predictive%20Coding%20and%20MDL%3A%20A%20Two-Part%20Code%20Framework%20for%20Deep%0A%20%20Learning&body=Title%3A%20Bridging%20Predictive%20Coding%20and%20MDL%3A%20A%20Two-Part%20Code%20Framework%20for%20Deep%0A%20%20Learning%0AAuthor%3A%20Benjamin%20Prada%20and%20Shion%20Matsumoto%20and%20Abdul%20Malik%20Zekri%20and%20Ankur%20Mali%0AAbstract%3A%20%20%20We%20present%20the%20first%20theoretical%20framework%20that%20connects%20predictive%20coding%0A%28PC%29%2C%20a%20biologically%20inspired%20local%20learning%20rule%2C%20with%20the%20minimum%20description%0Alength%20%28MDL%29%20principle%20in%20deep%20networks.%20We%20prove%20that%20layerwise%20PC%20performs%0Ablock-coordinate%20descent%20on%20the%20MDL%20two-part%20code%20objective%2C%20thereby%20jointly%0Aminimizing%20empirical%20risk%20and%20model%20complexity.%20Using%20Hoeffding%27s%20inequality%0Aand%20a%20prefix-code%20prior%2C%20we%20derive%20a%20novel%20generalization%20bound%20of%20the%20form%0A%24R%28%5Ctheta%29%20%5Cle%20%5Chat%7BR%7D%28%5Ctheta%29%20%2B%20%5Cfrac%7BL%28%5Ctheta%29%7D%7BN%7D%24%2C%20capturing%20the%20tradeoff%0Abetween%20fit%20and%20compression.%20We%20further%20prove%20that%20each%20PC%20sweep%20monotonically%0Adecreases%20the%20empirical%20two-part%20codelength%2C%20yielding%20tighter%20high-probability%0Arisk%20bounds%20than%20unconstrained%20gradient%20descent.%20Finally%2C%20we%20show%20that%20repeated%0APC%20updates%20converge%20to%20a%20block-coordinate%20stationary%20point%2C%20providing%20an%0Aapproximate%20MDL-optimal%20solution.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20result%0Aoffering%20formal%20generalization%20and%20convergence%20guarantees%20for%20PC-trained%20deep%0Amodels%2C%20positioning%20PC%20as%20a%20theoretically%20grounded%20and%20biologically%20plausible%0Aalternative%20to%20backpropagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Predictive%2520Coding%2520and%2520MDL%253A%2520A%2520Two-Part%2520Code%2520Framework%2520for%2520Deep%250A%2520%2520Learning%26entry.906535625%3DBenjamin%2520Prada%2520and%2520Shion%2520Matsumoto%2520and%2520Abdul%2520Malik%2520Zekri%2520and%2520Ankur%2520Mali%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520theoretical%2520framework%2520that%2520connects%2520predictive%2520coding%250A%2528PC%2529%252C%2520a%2520biologically%2520inspired%2520local%2520learning%2520rule%252C%2520with%2520the%2520minimum%2520description%250Alength%2520%2528MDL%2529%2520principle%2520in%2520deep%2520networks.%2520We%2520prove%2520that%2520layerwise%2520PC%2520performs%250Ablock-coordinate%2520descent%2520on%2520the%2520MDL%2520two-part%2520code%2520objective%252C%2520thereby%2520jointly%250Aminimizing%2520empirical%2520risk%2520and%2520model%2520complexity.%2520Using%2520Hoeffding%2527s%2520inequality%250Aand%2520a%2520prefix-code%2520prior%252C%2520we%2520derive%2520a%2520novel%2520generalization%2520bound%2520of%2520the%2520form%250A%2524R%2528%255Ctheta%2529%2520%255Cle%2520%255Chat%257BR%257D%2528%255Ctheta%2529%2520%252B%2520%255Cfrac%257BL%2528%255Ctheta%2529%257D%257BN%257D%2524%252C%2520capturing%2520the%2520tradeoff%250Abetween%2520fit%2520and%2520compression.%2520We%2520further%2520prove%2520that%2520each%2520PC%2520sweep%2520monotonically%250Adecreases%2520the%2520empirical%2520two-part%2520codelength%252C%2520yielding%2520tighter%2520high-probability%250Arisk%2520bounds%2520than%2520unconstrained%2520gradient%2520descent.%2520Finally%252C%2520we%2520show%2520that%2520repeated%250APC%2520updates%2520converge%2520to%2520a%2520block-coordinate%2520stationary%2520point%252C%2520providing%2520an%250Aapproximate%2520MDL-optimal%2520solution.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520result%250Aoffering%2520formal%2520generalization%2520and%2520convergence%2520guarantees%2520for%2520PC-trained%2520deep%250Amodels%252C%2520positioning%2520PC%2520as%2520a%2520theoretically%2520grounded%2520and%2520biologically%2520plausible%250Aalternative%2520to%2520backpropagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Predictive%20Coding%20and%20MDL%3A%20A%20Two-Part%20Code%20Framework%20for%20Deep%0A%20%20Learning&entry.906535625=Benjamin%20Prada%20and%20Shion%20Matsumoto%20and%20Abdul%20Malik%20Zekri%20and%20Ankur%20Mali&entry.1292438233=%20%20We%20present%20the%20first%20theoretical%20framework%20that%20connects%20predictive%20coding%0A%28PC%29%2C%20a%20biologically%20inspired%20local%20learning%20rule%2C%20with%20the%20minimum%20description%0Alength%20%28MDL%29%20principle%20in%20deep%20networks.%20We%20prove%20that%20layerwise%20PC%20performs%0Ablock-coordinate%20descent%20on%20the%20MDL%20two-part%20code%20objective%2C%20thereby%20jointly%0Aminimizing%20empirical%20risk%20and%20model%20complexity.%20Using%20Hoeffding%27s%20inequality%0Aand%20a%20prefix-code%20prior%2C%20we%20derive%20a%20novel%20generalization%20bound%20of%20the%20form%0A%24R%28%5Ctheta%29%20%5Cle%20%5Chat%7BR%7D%28%5Ctheta%29%20%2B%20%5Cfrac%7BL%28%5Ctheta%29%7D%7BN%7D%24%2C%20capturing%20the%20tradeoff%0Abetween%20fit%20and%20compression.%20We%20further%20prove%20that%20each%20PC%20sweep%20monotonically%0Adecreases%20the%20empirical%20two-part%20codelength%2C%20yielding%20tighter%20high-probability%0Arisk%20bounds%20than%20unconstrained%20gradient%20descent.%20Finally%2C%20we%20show%20that%20repeated%0APC%20updates%20converge%20to%20a%20block-coordinate%20stationary%20point%2C%20providing%20an%0Aapproximate%20MDL-optimal%20solution.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20result%0Aoffering%20formal%20generalization%20and%20convergence%20guarantees%20for%20PC-trained%20deep%0Amodels%2C%20positioning%20PC%20as%20a%20theoretically%20grounded%20and%20biologically%20plausible%0Aalternative%20to%20backpropagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14635v2&entry.124074799=Read"},
{"title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal\n  Inference in Neural Networks", "author": "Yi Li and David Mccoy and Nolan Gunter and Kaitlyn Lee and Alejandro Schuler and Mark van der Laan", "abstract": "  Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.\n", "link": "http://arxiv.org/abs/2507.12435v1", "date": "2025-07-16", "relevancy": 2.0543, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5272}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targeted%20Deep%20Architectures%3A%20A%20TMLE-Based%20Framework%20for%20Robust%20Causal%0A%20%20Inference%20in%20Neural%20Networks&body=Title%3A%20Targeted%20Deep%20Architectures%3A%20A%20TMLE-Based%20Framework%20for%20Robust%20Causal%0A%20%20Inference%20in%20Neural%20Networks%0AAuthor%3A%20Yi%20Li%20and%20David%20Mccoy%20and%20Nolan%20Gunter%20and%20Kaitlyn%20Lee%20and%20Alejandro%20Schuler%20and%20Mark%20van%20der%20Laan%0AAbstract%3A%20%20%20Modern%20deep%20neural%20networks%20are%20powerful%20predictive%20tools%20yet%20often%20lack%0Avalid%20inference%20for%20causal%20parameters%2C%20such%20as%20treatment%20effects%20or%20entire%0Asurvival%20curves.%20While%20frameworks%20like%20Double%20Machine%20Learning%20%28DML%29%20and%0ATargeted%20Maximum%20Likelihood%20Estimation%20%28TMLE%29%20can%20debias%20machine-learning%20fits%2C%0Aexisting%20neural%20implementations%20either%20rely%20on%20%22targeted%20losses%22%20that%20do%20not%0Aguarantee%20solving%20the%20efficient%20influence%20function%20equation%20or%20computationally%0Aexpensive%20post-hoc%20%22fluctuations%22%20for%20multi-parameter%20settings.%20We%20propose%0ATargeted%20Deep%20Architectures%20%28TDA%29%2C%20a%20new%20framework%20that%20embeds%20TMLE%20directly%0Ainto%20the%20network%27s%20parameter%20space%20with%20no%20restrictions%20on%20the%20backbone%0Aarchitecture.%20Specifically%2C%20TDA%20partitions%20model%20parameters%20-%20freezing%20all%20but%0Aa%20small%20%22targeting%22%20subset%20-%20and%20iteratively%20updates%20them%20along%20a%20targeting%0Agradient%2C%20derived%20from%20projecting%20the%20influence%20functions%20onto%20the%20span%20of%20the%0Agradients%20of%20the%20loss%20with%20respect%20to%20weights.%20This%20procedure%20yields%20plug-in%0Aestimates%20that%20remove%20first-order%20bias%20and%20produce%20asymptotically%20valid%0Aconfidence%20intervals.%20Crucially%2C%20TDA%20easily%20extends%20to%20multi-dimensional%20causal%0Aestimands%20%28e.g.%2C%20entire%20survival%20curves%29%20by%20merging%20separate%20targeting%0Agradients%20into%20a%20single%20universal%20targeting%20update.%20Theoretically%2C%20TDA%20inherits%0Aclassical%20TMLE%20properties%2C%20including%20double%20robustness%20and%20semiparametric%0Aefficiency.%20Empirically%2C%20on%20the%20benchmark%20IHDP%20dataset%20%28average%20treatment%0Aeffects%29%20and%20simulated%20survival%20data%20with%20informative%20censoring%2C%20TDA%20reduces%0Abias%20and%20improves%20coverage%20relative%20to%20both%20standard%20neural-network%20estimators%0Aand%20prior%20post-hoc%20approaches.%20In%20doing%20so%2C%20TDA%20establishes%20a%20direct%2C%20scalable%0Apathway%20toward%20rigorous%20causal%20inference%20within%20modern%20deep%20architectures%20for%0Acomplex%20multi-parameter%20targets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargeted%2520Deep%2520Architectures%253A%2520A%2520TMLE-Based%2520Framework%2520for%2520Robust%2520Causal%250A%2520%2520Inference%2520in%2520Neural%2520Networks%26entry.906535625%3DYi%2520Li%2520and%2520David%2520Mccoy%2520and%2520Nolan%2520Gunter%2520and%2520Kaitlyn%2520Lee%2520and%2520Alejandro%2520Schuler%2520and%2520Mark%2520van%2520der%2520Laan%26entry.1292438233%3D%2520%2520Modern%2520deep%2520neural%2520networks%2520are%2520powerful%2520predictive%2520tools%2520yet%2520often%2520lack%250Avalid%2520inference%2520for%2520causal%2520parameters%252C%2520such%2520as%2520treatment%2520effects%2520or%2520entire%250Asurvival%2520curves.%2520While%2520frameworks%2520like%2520Double%2520Machine%2520Learning%2520%2528DML%2529%2520and%250ATargeted%2520Maximum%2520Likelihood%2520Estimation%2520%2528TMLE%2529%2520can%2520debias%2520machine-learning%2520fits%252C%250Aexisting%2520neural%2520implementations%2520either%2520rely%2520on%2520%2522targeted%2520losses%2522%2520that%2520do%2520not%250Aguarantee%2520solving%2520the%2520efficient%2520influence%2520function%2520equation%2520or%2520computationally%250Aexpensive%2520post-hoc%2520%2522fluctuations%2522%2520for%2520multi-parameter%2520settings.%2520We%2520propose%250ATargeted%2520Deep%2520Architectures%2520%2528TDA%2529%252C%2520a%2520new%2520framework%2520that%2520embeds%2520TMLE%2520directly%250Ainto%2520the%2520network%2527s%2520parameter%2520space%2520with%2520no%2520restrictions%2520on%2520the%2520backbone%250Aarchitecture.%2520Specifically%252C%2520TDA%2520partitions%2520model%2520parameters%2520-%2520freezing%2520all%2520but%250Aa%2520small%2520%2522targeting%2522%2520subset%2520-%2520and%2520iteratively%2520updates%2520them%2520along%2520a%2520targeting%250Agradient%252C%2520derived%2520from%2520projecting%2520the%2520influence%2520functions%2520onto%2520the%2520span%2520of%2520the%250Agradients%2520of%2520the%2520loss%2520with%2520respect%2520to%2520weights.%2520This%2520procedure%2520yields%2520plug-in%250Aestimates%2520that%2520remove%2520first-order%2520bias%2520and%2520produce%2520asymptotically%2520valid%250Aconfidence%2520intervals.%2520Crucially%252C%2520TDA%2520easily%2520extends%2520to%2520multi-dimensional%2520causal%250Aestimands%2520%2528e.g.%252C%2520entire%2520survival%2520curves%2529%2520by%2520merging%2520separate%2520targeting%250Agradients%2520into%2520a%2520single%2520universal%2520targeting%2520update.%2520Theoretically%252C%2520TDA%2520inherits%250Aclassical%2520TMLE%2520properties%252C%2520including%2520double%2520robustness%2520and%2520semiparametric%250Aefficiency.%2520Empirically%252C%2520on%2520the%2520benchmark%2520IHDP%2520dataset%2520%2528average%2520treatment%250Aeffects%2529%2520and%2520simulated%2520survival%2520data%2520with%2520informative%2520censoring%252C%2520TDA%2520reduces%250Abias%2520and%2520improves%2520coverage%2520relative%2520to%2520both%2520standard%2520neural-network%2520estimators%250Aand%2520prior%2520post-hoc%2520approaches.%2520In%2520doing%2520so%252C%2520TDA%2520establishes%2520a%2520direct%252C%2520scalable%250Apathway%2520toward%2520rigorous%2520causal%2520inference%2520within%2520modern%2520deep%2520architectures%2520for%250Acomplex%2520multi-parameter%2520targets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20Deep%20Architectures%3A%20A%20TMLE-Based%20Framework%20for%20Robust%20Causal%0A%20%20Inference%20in%20Neural%20Networks&entry.906535625=Yi%20Li%20and%20David%20Mccoy%20and%20Nolan%20Gunter%20and%20Kaitlyn%20Lee%20and%20Alejandro%20Schuler%20and%20Mark%20van%20der%20Laan&entry.1292438233=%20%20Modern%20deep%20neural%20networks%20are%20powerful%20predictive%20tools%20yet%20often%20lack%0Avalid%20inference%20for%20causal%20parameters%2C%20such%20as%20treatment%20effects%20or%20entire%0Asurvival%20curves.%20While%20frameworks%20like%20Double%20Machine%20Learning%20%28DML%29%20and%0ATargeted%20Maximum%20Likelihood%20Estimation%20%28TMLE%29%20can%20debias%20machine-learning%20fits%2C%0Aexisting%20neural%20implementations%20either%20rely%20on%20%22targeted%20losses%22%20that%20do%20not%0Aguarantee%20solving%20the%20efficient%20influence%20function%20equation%20or%20computationally%0Aexpensive%20post-hoc%20%22fluctuations%22%20for%20multi-parameter%20settings.%20We%20propose%0ATargeted%20Deep%20Architectures%20%28TDA%29%2C%20a%20new%20framework%20that%20embeds%20TMLE%20directly%0Ainto%20the%20network%27s%20parameter%20space%20with%20no%20restrictions%20on%20the%20backbone%0Aarchitecture.%20Specifically%2C%20TDA%20partitions%20model%20parameters%20-%20freezing%20all%20but%0Aa%20small%20%22targeting%22%20subset%20-%20and%20iteratively%20updates%20them%20along%20a%20targeting%0Agradient%2C%20derived%20from%20projecting%20the%20influence%20functions%20onto%20the%20span%20of%20the%0Agradients%20of%20the%20loss%20with%20respect%20to%20weights.%20This%20procedure%20yields%20plug-in%0Aestimates%20that%20remove%20first-order%20bias%20and%20produce%20asymptotically%20valid%0Aconfidence%20intervals.%20Crucially%2C%20TDA%20easily%20extends%20to%20multi-dimensional%20causal%0Aestimands%20%28e.g.%2C%20entire%20survival%20curves%29%20by%20merging%20separate%20targeting%0Agradients%20into%20a%20single%20universal%20targeting%20update.%20Theoretically%2C%20TDA%20inherits%0Aclassical%20TMLE%20properties%2C%20including%20double%20robustness%20and%20semiparametric%0Aefficiency.%20Empirically%2C%20on%20the%20benchmark%20IHDP%20dataset%20%28average%20treatment%0Aeffects%29%20and%20simulated%20survival%20data%20with%20informative%20censoring%2C%20TDA%20reduces%0Abias%20and%20improves%20coverage%20relative%20to%20both%20standard%20neural-network%20estimators%0Aand%20prior%20post-hoc%20approaches.%20In%20doing%20so%2C%20TDA%20establishes%20a%20direct%2C%20scalable%0Apathway%20toward%20rigorous%20causal%20inference%20within%20modern%20deep%20architectures%20for%0Acomplex%20multi-parameter%20targets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12435v1&entry.124074799=Read"},
{"title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object\n  Multi-Class Representation and Factorization", "author": "Yifei Zhou and Xuchu Huang and Chenyu Ni and Min Zhou and Zheyu Yan and Xunzhao Yin and Cheng Zhuo", "abstract": "  Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.\n", "link": "http://arxiv.org/abs/2507.12366v1", "date": "2025-07-16", "relevancy": 2.0517, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FactorHD%3A%20A%20Hyperdimensional%20Computing%20Model%20for%20Multi-Object%0A%20%20Multi-Class%20Representation%20and%20Factorization&body=Title%3A%20FactorHD%3A%20A%20Hyperdimensional%20Computing%20Model%20for%20Multi-Object%0A%20%20Multi-Class%20Representation%20and%20Factorization%0AAuthor%3A%20Yifei%20Zhou%20and%20Xuchu%20Huang%20and%20Chenyu%20Ni%20and%20Min%20Zhou%20and%20Zheyu%20Yan%20and%20Xunzhao%20Yin%20and%20Cheng%20Zhuo%0AAbstract%3A%20%20%20Neuro-symbolic%20artificial%20intelligence%20%28neuro-symbolic%20AI%29%20excels%20in%20logical%0Aanalysis%20and%20reasoning.%20Hyperdimensional%20Computing%20%28HDC%29%2C%20a%20promising%0Abrain-inspired%20computational%20model%2C%20is%20integral%20to%20neuro-symbolic%20AI.%20Various%0AHDC%20models%20have%20been%20proposed%20to%20represent%20class-instance%20and%20class-class%0Arelations%2C%20but%20when%20representing%20the%20more%20complex%20class-subclass%20relation%2C%0Awhere%20multiple%20objects%20associate%20different%20levels%20of%20classes%20and%20subclasses%2C%0Athey%20face%20challenges%20for%20factorization%2C%20a%20crucial%20task%20for%20neuro-symbolic%20AI%0Asystems.%20In%20this%20article%2C%20we%20propose%20FactorHD%2C%20a%20novel%20HDC%20model%20capable%20of%0Arepresenting%20and%20factorizing%20the%20complex%20class-subclass%20relation%20efficiently.%0AFactorHD%20features%20a%20symbolic%20encoding%20method%20that%20embeds%20an%20extra%20memorization%0Aclause%2C%20preserving%20more%20information%20for%20multiple%20objects.%20In%20addition%2C%20it%0Aemploys%20an%20efficient%20factorization%20algorithm%20that%20selectively%20eliminates%0Aredundant%20classes%20by%20identifying%20the%20memorization%20clause%20of%20the%20target%20class.%0ASuch%20model%20significantly%20enhances%20computing%20efficiency%20and%20accuracy%20in%0Arepresenting%20and%20factorizing%20multiple%20objects%20with%20class-subclass%20relation%2C%0Aovercoming%20limitations%20of%20existing%20HDC%20models%20such%20as%20%22superposition%0Acatastrophe%22%20and%20%22the%20problem%20of%202%22.%20Evaluations%20show%20that%20FactorHD%20achieves%0Aapproximately%205667x%20speedup%20at%20a%20representation%20size%20of%2010%5E9%20compared%20to%0Aexisting%20HDC%20models.%20When%20integrated%20with%20the%20ResNet-18%20neural%20network%2C%0AFactorHD%20achieves%2092.48%25%20factorization%20accuracy%20on%20the%20Cifar-10%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorHD%253A%2520A%2520Hyperdimensional%2520Computing%2520Model%2520for%2520Multi-Object%250A%2520%2520Multi-Class%2520Representation%2520and%2520Factorization%26entry.906535625%3DYifei%2520Zhou%2520and%2520Xuchu%2520Huang%2520and%2520Chenyu%2520Ni%2520and%2520Min%2520Zhou%2520and%2520Zheyu%2520Yan%2520and%2520Xunzhao%2520Yin%2520and%2520Cheng%2520Zhuo%26entry.1292438233%3D%2520%2520Neuro-symbolic%2520artificial%2520intelligence%2520%2528neuro-symbolic%2520AI%2529%2520excels%2520in%2520logical%250Aanalysis%2520and%2520reasoning.%2520Hyperdimensional%2520Computing%2520%2528HDC%2529%252C%2520a%2520promising%250Abrain-inspired%2520computational%2520model%252C%2520is%2520integral%2520to%2520neuro-symbolic%2520AI.%2520Various%250AHDC%2520models%2520have%2520been%2520proposed%2520to%2520represent%2520class-instance%2520and%2520class-class%250Arelations%252C%2520but%2520when%2520representing%2520the%2520more%2520complex%2520class-subclass%2520relation%252C%250Awhere%2520multiple%2520objects%2520associate%2520different%2520levels%2520of%2520classes%2520and%2520subclasses%252C%250Athey%2520face%2520challenges%2520for%2520factorization%252C%2520a%2520crucial%2520task%2520for%2520neuro-symbolic%2520AI%250Asystems.%2520In%2520this%2520article%252C%2520we%2520propose%2520FactorHD%252C%2520a%2520novel%2520HDC%2520model%2520capable%2520of%250Arepresenting%2520and%2520factorizing%2520the%2520complex%2520class-subclass%2520relation%2520efficiently.%250AFactorHD%2520features%2520a%2520symbolic%2520encoding%2520method%2520that%2520embeds%2520an%2520extra%2520memorization%250Aclause%252C%2520preserving%2520more%2520information%2520for%2520multiple%2520objects.%2520In%2520addition%252C%2520it%250Aemploys%2520an%2520efficient%2520factorization%2520algorithm%2520that%2520selectively%2520eliminates%250Aredundant%2520classes%2520by%2520identifying%2520the%2520memorization%2520clause%2520of%2520the%2520target%2520class.%250ASuch%2520model%2520significantly%2520enhances%2520computing%2520efficiency%2520and%2520accuracy%2520in%250Arepresenting%2520and%2520factorizing%2520multiple%2520objects%2520with%2520class-subclass%2520relation%252C%250Aovercoming%2520limitations%2520of%2520existing%2520HDC%2520models%2520such%2520as%2520%2522superposition%250Acatastrophe%2522%2520and%2520%2522the%2520problem%2520of%25202%2522.%2520Evaluations%2520show%2520that%2520FactorHD%2520achieves%250Aapproximately%25205667x%2520speedup%2520at%2520a%2520representation%2520size%2520of%252010%255E9%2520compared%2520to%250Aexisting%2520HDC%2520models.%2520When%2520integrated%2520with%2520the%2520ResNet-18%2520neural%2520network%252C%250AFactorHD%2520achieves%252092.48%2525%2520factorization%2520accuracy%2520on%2520the%2520Cifar-10%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FactorHD%3A%20A%20Hyperdimensional%20Computing%20Model%20for%20Multi-Object%0A%20%20Multi-Class%20Representation%20and%20Factorization&entry.906535625=Yifei%20Zhou%20and%20Xuchu%20Huang%20and%20Chenyu%20Ni%20and%20Min%20Zhou%20and%20Zheyu%20Yan%20and%20Xunzhao%20Yin%20and%20Cheng%20Zhuo&entry.1292438233=%20%20Neuro-symbolic%20artificial%20intelligence%20%28neuro-symbolic%20AI%29%20excels%20in%20logical%0Aanalysis%20and%20reasoning.%20Hyperdimensional%20Computing%20%28HDC%29%2C%20a%20promising%0Abrain-inspired%20computational%20model%2C%20is%20integral%20to%20neuro-symbolic%20AI.%20Various%0AHDC%20models%20have%20been%20proposed%20to%20represent%20class-instance%20and%20class-class%0Arelations%2C%20but%20when%20representing%20the%20more%20complex%20class-subclass%20relation%2C%0Awhere%20multiple%20objects%20associate%20different%20levels%20of%20classes%20and%20subclasses%2C%0Athey%20face%20challenges%20for%20factorization%2C%20a%20crucial%20task%20for%20neuro-symbolic%20AI%0Asystems.%20In%20this%20article%2C%20we%20propose%20FactorHD%2C%20a%20novel%20HDC%20model%20capable%20of%0Arepresenting%20and%20factorizing%20the%20complex%20class-subclass%20relation%20efficiently.%0AFactorHD%20features%20a%20symbolic%20encoding%20method%20that%20embeds%20an%20extra%20memorization%0Aclause%2C%20preserving%20more%20information%20for%20multiple%20objects.%20In%20addition%2C%20it%0Aemploys%20an%20efficient%20factorization%20algorithm%20that%20selectively%20eliminates%0Aredundant%20classes%20by%20identifying%20the%20memorization%20clause%20of%20the%20target%20class.%0ASuch%20model%20significantly%20enhances%20computing%20efficiency%20and%20accuracy%20in%0Arepresenting%20and%20factorizing%20multiple%20objects%20with%20class-subclass%20relation%2C%0Aovercoming%20limitations%20of%20existing%20HDC%20models%20such%20as%20%22superposition%0Acatastrophe%22%20and%20%22the%20problem%20of%202%22.%20Evaluations%20show%20that%20FactorHD%20achieves%0Aapproximately%205667x%20speedup%20at%20a%20representation%20size%20of%2010%5E9%20compared%20to%0Aexisting%20HDC%20models.%20When%20integrated%20with%20the%20ResNet-18%20neural%20network%2C%0AFactorHD%20achieves%2092.48%25%20factorization%20accuracy%20on%20the%20Cifar-10%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12366v1&entry.124074799=Read"},
{"title": "Language Models Improve When Pretraining Data Matches Target Tasks", "author": "David Mizrahi and Anders Boesen Lindbo Larsen and Jesse Allardice and Suzie Petryk and Yuri Gorokhov and Jeffrey Li and Alex Fang and Josh Gardner and Tom Gunter and Afshin Dehghan", "abstract": "  Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.\n", "link": "http://arxiv.org/abs/2507.12466v1", "date": "2025-07-16", "relevancy": 2.0438, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Improve%20When%20Pretraining%20Data%20Matches%20Target%20Tasks&body=Title%3A%20Language%20Models%20Improve%20When%20Pretraining%20Data%20Matches%20Target%20Tasks%0AAuthor%3A%20David%20Mizrahi%20and%20Anders%20Boesen%20Lindbo%20Larsen%20and%20Jesse%20Allardice%20and%20Suzie%20Petryk%20and%20Yuri%20Gorokhov%20and%20Jeffrey%20Li%20and%20Alex%20Fang%20and%20Josh%20Gardner%20and%20Tom%20Gunter%20and%20Afshin%20Dehghan%0AAbstract%3A%20%20%20Every%20data%20selection%20method%20inherently%20has%20a%20target.%20In%20practice%2C%20these%0Atargets%20often%20emerge%20implicitly%20through%20benchmark-driven%20iteration%3A%20researchers%0Adevelop%20selection%20strategies%2C%20train%20models%2C%20measure%20benchmark%20performance%2C%20then%0Arefine%20accordingly.%20This%20raises%20a%20natural%20question%3A%20what%20happens%20when%20we%20make%0Athis%20optimization%20explicit%3F%20To%20explore%20this%2C%20we%20propose%20benchmark-targeted%0Aranking%20%28BETR%29%2C%20a%20simple%20method%20that%20selects%20pretraining%20documents%20based%20on%0Asimilarity%20to%20benchmark%20training%20examples.%20BETR%20embeds%20benchmark%20examples%20and%20a%0Asample%20of%20pretraining%20documents%20in%20a%20shared%20space%2C%20scores%20this%20sample%20by%0Asimilarity%20to%20benchmarks%2C%20then%20trains%20a%20lightweight%20classifier%20to%20predict%20these%0Ascores%20for%20the%20full%20corpus.%20We%20compare%20data%20selection%20methods%20by%20training%20over%0A500%20models%20spanning%20%2410%5E%7B19%7D%24%20to%20%2410%5E%7B22%7D%24%20FLOPs%20and%20fitting%20scaling%20laws%20to%0Athem.%20From%20this%2C%20we%20find%20that%20simply%20aligning%20pretraining%20data%20to%20evaluation%0Abenchmarks%20using%20BETR%20achieves%20a%202.1x%20compute%20multiplier%20over%20DCLM-Baseline%0A%284.7x%20over%20unfiltered%20data%29%20and%20improves%20performance%20on%209%20out%20of%2010%20tasks%0Aacross%20all%20scales.%20BETR%20also%20generalizes%20well%3A%20when%20targeting%20a%20diverse%20set%20of%0Abenchmarks%20disjoint%20from%20our%20evaluation%20suite%2C%20it%20still%20matches%20or%20outperforms%0Abaselines.%20Our%20scaling%20analysis%20further%20reveals%20a%20clear%20trend%3A%20larger%20models%0Arequire%20less%20aggressive%20filtering.%20Overall%2C%20our%20findings%20show%20that%20directly%0Amatching%20pretraining%20data%20to%20target%20tasks%20precisely%20shapes%20model%20capabilities%0Aand%20highlight%20that%20optimal%20selection%20strategies%20must%20adapt%20to%20model%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Improve%2520When%2520Pretraining%2520Data%2520Matches%2520Target%2520Tasks%26entry.906535625%3DDavid%2520Mizrahi%2520and%2520Anders%2520Boesen%2520Lindbo%2520Larsen%2520and%2520Jesse%2520Allardice%2520and%2520Suzie%2520Petryk%2520and%2520Yuri%2520Gorokhov%2520and%2520Jeffrey%2520Li%2520and%2520Alex%2520Fang%2520and%2520Josh%2520Gardner%2520and%2520Tom%2520Gunter%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3D%2520%2520Every%2520data%2520selection%2520method%2520inherently%2520has%2520a%2520target.%2520In%2520practice%252C%2520these%250Atargets%2520often%2520emerge%2520implicitly%2520through%2520benchmark-driven%2520iteration%253A%2520researchers%250Adevelop%2520selection%2520strategies%252C%2520train%2520models%252C%2520measure%2520benchmark%2520performance%252C%2520then%250Arefine%2520accordingly.%2520This%2520raises%2520a%2520natural%2520question%253A%2520what%2520happens%2520when%2520we%2520make%250Athis%2520optimization%2520explicit%253F%2520To%2520explore%2520this%252C%2520we%2520propose%2520benchmark-targeted%250Aranking%2520%2528BETR%2529%252C%2520a%2520simple%2520method%2520that%2520selects%2520pretraining%2520documents%2520based%2520on%250Asimilarity%2520to%2520benchmark%2520training%2520examples.%2520BETR%2520embeds%2520benchmark%2520examples%2520and%2520a%250Asample%2520of%2520pretraining%2520documents%2520in%2520a%2520shared%2520space%252C%2520scores%2520this%2520sample%2520by%250Asimilarity%2520to%2520benchmarks%252C%2520then%2520trains%2520a%2520lightweight%2520classifier%2520to%2520predict%2520these%250Ascores%2520for%2520the%2520full%2520corpus.%2520We%2520compare%2520data%2520selection%2520methods%2520by%2520training%2520over%250A500%2520models%2520spanning%2520%252410%255E%257B19%257D%2524%2520to%2520%252410%255E%257B22%257D%2524%2520FLOPs%2520and%2520fitting%2520scaling%2520laws%2520to%250Athem.%2520From%2520this%252C%2520we%2520find%2520that%2520simply%2520aligning%2520pretraining%2520data%2520to%2520evaluation%250Abenchmarks%2520using%2520BETR%2520achieves%2520a%25202.1x%2520compute%2520multiplier%2520over%2520DCLM-Baseline%250A%25284.7x%2520over%2520unfiltered%2520data%2529%2520and%2520improves%2520performance%2520on%25209%2520out%2520of%252010%2520tasks%250Aacross%2520all%2520scales.%2520BETR%2520also%2520generalizes%2520well%253A%2520when%2520targeting%2520a%2520diverse%2520set%2520of%250Abenchmarks%2520disjoint%2520from%2520our%2520evaluation%2520suite%252C%2520it%2520still%2520matches%2520or%2520outperforms%250Abaselines.%2520Our%2520scaling%2520analysis%2520further%2520reveals%2520a%2520clear%2520trend%253A%2520larger%2520models%250Arequire%2520less%2520aggressive%2520filtering.%2520Overall%252C%2520our%2520findings%2520show%2520that%2520directly%250Amatching%2520pretraining%2520data%2520to%2520target%2520tasks%2520precisely%2520shapes%2520model%2520capabilities%250Aand%2520highlight%2520that%2520optimal%2520selection%2520strategies%2520must%2520adapt%2520to%2520model%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Improve%20When%20Pretraining%20Data%20Matches%20Target%20Tasks&entry.906535625=David%20Mizrahi%20and%20Anders%20Boesen%20Lindbo%20Larsen%20and%20Jesse%20Allardice%20and%20Suzie%20Petryk%20and%20Yuri%20Gorokhov%20and%20Jeffrey%20Li%20and%20Alex%20Fang%20and%20Josh%20Gardner%20and%20Tom%20Gunter%20and%20Afshin%20Dehghan&entry.1292438233=%20%20Every%20data%20selection%20method%20inherently%20has%20a%20target.%20In%20practice%2C%20these%0Atargets%20often%20emerge%20implicitly%20through%20benchmark-driven%20iteration%3A%20researchers%0Adevelop%20selection%20strategies%2C%20train%20models%2C%20measure%20benchmark%20performance%2C%20then%0Arefine%20accordingly.%20This%20raises%20a%20natural%20question%3A%20what%20happens%20when%20we%20make%0Athis%20optimization%20explicit%3F%20To%20explore%20this%2C%20we%20propose%20benchmark-targeted%0Aranking%20%28BETR%29%2C%20a%20simple%20method%20that%20selects%20pretraining%20documents%20based%20on%0Asimilarity%20to%20benchmark%20training%20examples.%20BETR%20embeds%20benchmark%20examples%20and%20a%0Asample%20of%20pretraining%20documents%20in%20a%20shared%20space%2C%20scores%20this%20sample%20by%0Asimilarity%20to%20benchmarks%2C%20then%20trains%20a%20lightweight%20classifier%20to%20predict%20these%0Ascores%20for%20the%20full%20corpus.%20We%20compare%20data%20selection%20methods%20by%20training%20over%0A500%20models%20spanning%20%2410%5E%7B19%7D%24%20to%20%2410%5E%7B22%7D%24%20FLOPs%20and%20fitting%20scaling%20laws%20to%0Athem.%20From%20this%2C%20we%20find%20that%20simply%20aligning%20pretraining%20data%20to%20evaluation%0Abenchmarks%20using%20BETR%20achieves%20a%202.1x%20compute%20multiplier%20over%20DCLM-Baseline%0A%284.7x%20over%20unfiltered%20data%29%20and%20improves%20performance%20on%209%20out%20of%2010%20tasks%0Aacross%20all%20scales.%20BETR%20also%20generalizes%20well%3A%20when%20targeting%20a%20diverse%20set%20of%0Abenchmarks%20disjoint%20from%20our%20evaluation%20suite%2C%20it%20still%20matches%20or%20outperforms%0Abaselines.%20Our%20scaling%20analysis%20further%20reveals%20a%20clear%20trend%3A%20larger%20models%0Arequire%20less%20aggressive%20filtering.%20Overall%2C%20our%20findings%20show%20that%20directly%0Amatching%20pretraining%20data%20to%20target%20tasks%20precisely%20shapes%20model%20capabilities%0Aand%20highlight%20that%20optimal%20selection%20strategies%20must%20adapt%20to%20model%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12466v1&entry.124074799=Read"},
{"title": "On the Statistical Properties of Generative Adversarial Models for Low\n  Intrinsic Data Dimension", "author": "Saptarshi Chakraborty and Peter L. Bartlett", "abstract": "  Despite the remarkable empirical successes of Generative Adversarial Networks\n(GANs), the theoretical guarantees for their statistical accuracy remain rather\npessimistic. In particular, the data distributions on which GANs are applied,\nsuch as natural images, are often hypothesized to have an intrinsic\nlow-dimensional structure in a typically high-dimensional feature space, but\nthis is often not reflected in the derived rates in the state-of-the-art\nanalyses. In this paper, we attempt to bridge the gap between the theory and\npractice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),\nby deriving statistical guarantees on the estimated densities in terms of the\nintrinsic dimension of the data and the latent space. We analytically show that\nif one has access to $n$ samples from the unknown target distribution and the\nnetwork architectures are properly chosen, the expected Wasserstein-1 distance\nof the estimates from the target scales as $O\\left( n^{-1/d_\\mu } \\right)$ for\nGANs and $\\tilde{O}\\left( n^{-1/(d_\\mu+\\ell)} \\right)$ for BiGANs, where\n$d_\\mu$ and $\\ell$ are the upper Wasserstein-1 dimension of the\ndata-distribution and latent-space dimension, respectively. The theoretical\nanalyses not only suggest that these methods successfully avoid the curse of\ndimensionality, in the sense that the exponent of $n$ in the error rates does\nnot depend on the data dimension but also serve to bridge the gap between the\ntheoretical analyses of GANs and the known sharp rates from optimal transport\nliterature. Additionally, we demonstrate that GANs can effectively achieve the\nminimax optimal rate even for non-smooth underlying distributions, with the use\nof interpolating generator networks.\n", "link": "http://arxiv.org/abs/2401.15801v2", "date": "2025-07-16", "relevancy": 2.0436, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.51}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Statistical%20Properties%20of%20Generative%20Adversarial%20Models%20for%20Low%0A%20%20Intrinsic%20Data%20Dimension&body=Title%3A%20On%20the%20Statistical%20Properties%20of%20Generative%20Adversarial%20Models%20for%20Low%0A%20%20Intrinsic%20Data%20Dimension%0AAuthor%3A%20Saptarshi%20Chakraborty%20and%20Peter%20L.%20Bartlett%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20empirical%20successes%20of%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20the%20theoretical%20guarantees%20for%20their%20statistical%20accuracy%20remain%20rather%0Apessimistic.%20In%20particular%2C%20the%20data%20distributions%20on%20which%20GANs%20are%20applied%2C%0Asuch%20as%20natural%20images%2C%20are%20often%20hypothesized%20to%20have%20an%20intrinsic%0Alow-dimensional%20structure%20in%20a%20typically%20high-dimensional%20feature%20space%2C%20but%0Athis%20is%20often%20not%20reflected%20in%20the%20derived%20rates%20in%20the%20state-of-the-art%0Aanalyses.%20In%20this%20paper%2C%20we%20attempt%20to%20bridge%20the%20gap%20between%20the%20theory%20and%0Apractice%20of%20GANs%20and%20their%20bidirectional%20variant%2C%20Bi-directional%20GANs%20%28BiGANs%29%2C%0Aby%20deriving%20statistical%20guarantees%20on%20the%20estimated%20densities%20in%20terms%20of%20the%0Aintrinsic%20dimension%20of%20the%20data%20and%20the%20latent%20space.%20We%20analytically%20show%20that%0Aif%20one%20has%20access%20to%20%24n%24%20samples%20from%20the%20unknown%20target%20distribution%20and%20the%0Anetwork%20architectures%20are%20properly%20chosen%2C%20the%20expected%20Wasserstein-1%20distance%0Aof%20the%20estimates%20from%20the%20target%20scales%20as%20%24O%5Cleft%28%20n%5E%7B-1/d_%5Cmu%20%7D%20%5Cright%29%24%20for%0AGANs%20and%20%24%5Ctilde%7BO%7D%5Cleft%28%20n%5E%7B-1/%28d_%5Cmu%2B%5Cell%29%7D%20%5Cright%29%24%20for%20BiGANs%2C%20where%0A%24d_%5Cmu%24%20and%20%24%5Cell%24%20are%20the%20upper%20Wasserstein-1%20dimension%20of%20the%0Adata-distribution%20and%20latent-space%20dimension%2C%20respectively.%20The%20theoretical%0Aanalyses%20not%20only%20suggest%20that%20these%20methods%20successfully%20avoid%20the%20curse%20of%0Adimensionality%2C%20in%20the%20sense%20that%20the%20exponent%20of%20%24n%24%20in%20the%20error%20rates%20does%0Anot%20depend%20on%20the%20data%20dimension%20but%20also%20serve%20to%20bridge%20the%20gap%20between%20the%0Atheoretical%20analyses%20of%20GANs%20and%20the%20known%20sharp%20rates%20from%20optimal%20transport%0Aliterature.%20Additionally%2C%20we%20demonstrate%20that%20GANs%20can%20effectively%20achieve%20the%0Aminimax%20optimal%20rate%20even%20for%20non-smooth%20underlying%20distributions%2C%20with%20the%20use%0Aof%20interpolating%20generator%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Statistical%2520Properties%2520of%2520Generative%2520Adversarial%2520Models%2520for%2520Low%250A%2520%2520Intrinsic%2520Data%2520Dimension%26entry.906535625%3DSaptarshi%2520Chakraborty%2520and%2520Peter%2520L.%2520Bartlett%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520empirical%2520successes%2520of%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%252C%2520the%2520theoretical%2520guarantees%2520for%2520their%2520statistical%2520accuracy%2520remain%2520rather%250Apessimistic.%2520In%2520particular%252C%2520the%2520data%2520distributions%2520on%2520which%2520GANs%2520are%2520applied%252C%250Asuch%2520as%2520natural%2520images%252C%2520are%2520often%2520hypothesized%2520to%2520have%2520an%2520intrinsic%250Alow-dimensional%2520structure%2520in%2520a%2520typically%2520high-dimensional%2520feature%2520space%252C%2520but%250Athis%2520is%2520often%2520not%2520reflected%2520in%2520the%2520derived%2520rates%2520in%2520the%2520state-of-the-art%250Aanalyses.%2520In%2520this%2520paper%252C%2520we%2520attempt%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520theory%2520and%250Apractice%2520of%2520GANs%2520and%2520their%2520bidirectional%2520variant%252C%2520Bi-directional%2520GANs%2520%2528BiGANs%2529%252C%250Aby%2520deriving%2520statistical%2520guarantees%2520on%2520the%2520estimated%2520densities%2520in%2520terms%2520of%2520the%250Aintrinsic%2520dimension%2520of%2520the%2520data%2520and%2520the%2520latent%2520space.%2520We%2520analytically%2520show%2520that%250Aif%2520one%2520has%2520access%2520to%2520%2524n%2524%2520samples%2520from%2520the%2520unknown%2520target%2520distribution%2520and%2520the%250Anetwork%2520architectures%2520are%2520properly%2520chosen%252C%2520the%2520expected%2520Wasserstein-1%2520distance%250Aof%2520the%2520estimates%2520from%2520the%2520target%2520scales%2520as%2520%2524O%255Cleft%2528%2520n%255E%257B-1/d_%255Cmu%2520%257D%2520%255Cright%2529%2524%2520for%250AGANs%2520and%2520%2524%255Ctilde%257BO%257D%255Cleft%2528%2520n%255E%257B-1/%2528d_%255Cmu%252B%255Cell%2529%257D%2520%255Cright%2529%2524%2520for%2520BiGANs%252C%2520where%250A%2524d_%255Cmu%2524%2520and%2520%2524%255Cell%2524%2520are%2520the%2520upper%2520Wasserstein-1%2520dimension%2520of%2520the%250Adata-distribution%2520and%2520latent-space%2520dimension%252C%2520respectively.%2520The%2520theoretical%250Aanalyses%2520not%2520only%2520suggest%2520that%2520these%2520methods%2520successfully%2520avoid%2520the%2520curse%2520of%250Adimensionality%252C%2520in%2520the%2520sense%2520that%2520the%2520exponent%2520of%2520%2524n%2524%2520in%2520the%2520error%2520rates%2520does%250Anot%2520depend%2520on%2520the%2520data%2520dimension%2520but%2520also%2520serve%2520to%2520bridge%2520the%2520gap%2520between%2520the%250Atheoretical%2520analyses%2520of%2520GANs%2520and%2520the%2520known%2520sharp%2520rates%2520from%2520optimal%2520transport%250Aliterature.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520GANs%2520can%2520effectively%2520achieve%2520the%250Aminimax%2520optimal%2520rate%2520even%2520for%2520non-smooth%2520underlying%2520distributions%252C%2520with%2520the%2520use%250Aof%2520interpolating%2520generator%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Statistical%20Properties%20of%20Generative%20Adversarial%20Models%20for%20Low%0A%20%20Intrinsic%20Data%20Dimension&entry.906535625=Saptarshi%20Chakraborty%20and%20Peter%20L.%20Bartlett&entry.1292438233=%20%20Despite%20the%20remarkable%20empirical%20successes%20of%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20the%20theoretical%20guarantees%20for%20their%20statistical%20accuracy%20remain%20rather%0Apessimistic.%20In%20particular%2C%20the%20data%20distributions%20on%20which%20GANs%20are%20applied%2C%0Asuch%20as%20natural%20images%2C%20are%20often%20hypothesized%20to%20have%20an%20intrinsic%0Alow-dimensional%20structure%20in%20a%20typically%20high-dimensional%20feature%20space%2C%20but%0Athis%20is%20often%20not%20reflected%20in%20the%20derived%20rates%20in%20the%20state-of-the-art%0Aanalyses.%20In%20this%20paper%2C%20we%20attempt%20to%20bridge%20the%20gap%20between%20the%20theory%20and%0Apractice%20of%20GANs%20and%20their%20bidirectional%20variant%2C%20Bi-directional%20GANs%20%28BiGANs%29%2C%0Aby%20deriving%20statistical%20guarantees%20on%20the%20estimated%20densities%20in%20terms%20of%20the%0Aintrinsic%20dimension%20of%20the%20data%20and%20the%20latent%20space.%20We%20analytically%20show%20that%0Aif%20one%20has%20access%20to%20%24n%24%20samples%20from%20the%20unknown%20target%20distribution%20and%20the%0Anetwork%20architectures%20are%20properly%20chosen%2C%20the%20expected%20Wasserstein-1%20distance%0Aof%20the%20estimates%20from%20the%20target%20scales%20as%20%24O%5Cleft%28%20n%5E%7B-1/d_%5Cmu%20%7D%20%5Cright%29%24%20for%0AGANs%20and%20%24%5Ctilde%7BO%7D%5Cleft%28%20n%5E%7B-1/%28d_%5Cmu%2B%5Cell%29%7D%20%5Cright%29%24%20for%20BiGANs%2C%20where%0A%24d_%5Cmu%24%20and%20%24%5Cell%24%20are%20the%20upper%20Wasserstein-1%20dimension%20of%20the%0Adata-distribution%20and%20latent-space%20dimension%2C%20respectively.%20The%20theoretical%0Aanalyses%20not%20only%20suggest%20that%20these%20methods%20successfully%20avoid%20the%20curse%20of%0Adimensionality%2C%20in%20the%20sense%20that%20the%20exponent%20of%20%24n%24%20in%20the%20error%20rates%20does%0Anot%20depend%20on%20the%20data%20dimension%20but%20also%20serve%20to%20bridge%20the%20gap%20between%20the%0Atheoretical%20analyses%20of%20GANs%20and%20the%20known%20sharp%20rates%20from%20optimal%20transport%0Aliterature.%20Additionally%2C%20we%20demonstrate%20that%20GANs%20can%20effectively%20achieve%20the%0Aminimax%20optimal%20rate%20even%20for%20non-smooth%20underlying%20distributions%2C%20with%20the%20use%0Aof%20interpolating%20generator%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15801v2&entry.124074799=Read"},
{"title": "Efficient Calisthenics Skills Classification through Foreground Instance\n  Selection and Depth Estimation", "author": "Antonio Finocchiaro and Giovanni Maria Farinella and Antonino Furnari", "abstract": "  Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.\n", "link": "http://arxiv.org/abs/2507.12292v1", "date": "2025-07-16", "relevancy": 2.042, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5104}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Calisthenics%20Skills%20Classification%20through%20Foreground%20Instance%0A%20%20Selection%20and%20Depth%20Estimation&body=Title%3A%20Efficient%20Calisthenics%20Skills%20Classification%20through%20Foreground%20Instance%0A%20%20Selection%20and%20Depth%20Estimation%0AAuthor%3A%20Antonio%20Finocchiaro%20and%20Giovanni%20Maria%20Farinella%20and%20Antonino%20Furnari%0AAbstract%3A%20%20%20Calisthenics%20skill%20classification%20is%20the%20computer%20vision%20task%20of%20inferring%0Athe%20skill%20performed%20by%20an%20athlete%20from%20images%2C%20enabling%20automatic%20performance%0Aassessment%20and%20personalized%20analytics.%20Traditional%20methods%20for%20calisthenics%0Askill%20recognition%20are%20based%20on%20pose%20estimation%20methods%20to%20determine%20the%0Aposition%20of%20skeletal%20data%20from%20images%2C%20which%20is%20later%20fed%20to%20a%20classification%0Aalgorithm%20to%20infer%20the%20performed%20skill.%20Despite%20the%20progress%20in%20human%20pose%0Aestimation%20algorithms%2C%20they%20still%20involve%20high%20computational%20costs%2C%20long%0Ainference%20times%2C%20and%20complex%20setups%2C%20which%20limit%20the%20applicability%20of%20such%0Aapproaches%20in%20real-time%20applications%20or%20mobile%20devices.%20This%20work%20proposes%20a%0Adirect%20approach%20to%20calisthenics%20skill%20recognition%2C%20which%20leverages%20depth%0Aestimation%20and%20athlete%20patch%20retrieval%20to%20avoid%20the%20computationally%20expensive%0Ahuman%20pose%20estimation%20module.%20Using%20Depth%20Anything%20V2%20for%20depth%20estimation%20and%0AYOLOv10%20for%20athlete%20localization%2C%20we%20segment%20the%20subject%20from%20the%20background%0Arather%20than%20relying%20on%20traditional%20pose%20estimation%20techniques.%20This%20strategy%0Aincreases%20efficiency%2C%20reduces%20inference%20time%2C%20and%20improves%20classification%0Aaccuracy.%20Our%20approach%20significantly%20outperforms%20skeleton-based%20methods%2C%0Aachieving%2038.3x%20faster%20inference%20with%20RGB%20image%20patches%20and%20improved%0Aclassification%20accuracy%20with%20depth%20patches%20%280.837%20vs.%200.815%29.%20Beyond%20these%0Aperformance%20gains%2C%20the%20modular%20design%20of%20our%20pipeline%20allows%20for%20flexible%0Areplacement%20of%20components%2C%20enabling%20future%20enhancements%20and%20adaptation%20to%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Calisthenics%2520Skills%2520Classification%2520through%2520Foreground%2520Instance%250A%2520%2520Selection%2520and%2520Depth%2520Estimation%26entry.906535625%3DAntonio%2520Finocchiaro%2520and%2520Giovanni%2520Maria%2520Farinella%2520and%2520Antonino%2520Furnari%26entry.1292438233%3D%2520%2520Calisthenics%2520skill%2520classification%2520is%2520the%2520computer%2520vision%2520task%2520of%2520inferring%250Athe%2520skill%2520performed%2520by%2520an%2520athlete%2520from%2520images%252C%2520enabling%2520automatic%2520performance%250Aassessment%2520and%2520personalized%2520analytics.%2520Traditional%2520methods%2520for%2520calisthenics%250Askill%2520recognition%2520are%2520based%2520on%2520pose%2520estimation%2520methods%2520to%2520determine%2520the%250Aposition%2520of%2520skeletal%2520data%2520from%2520images%252C%2520which%2520is%2520later%2520fed%2520to%2520a%2520classification%250Aalgorithm%2520to%2520infer%2520the%2520performed%2520skill.%2520Despite%2520the%2520progress%2520in%2520human%2520pose%250Aestimation%2520algorithms%252C%2520they%2520still%2520involve%2520high%2520computational%2520costs%252C%2520long%250Ainference%2520times%252C%2520and%2520complex%2520setups%252C%2520which%2520limit%2520the%2520applicability%2520of%2520such%250Aapproaches%2520in%2520real-time%2520applications%2520or%2520mobile%2520devices.%2520This%2520work%2520proposes%2520a%250Adirect%2520approach%2520to%2520calisthenics%2520skill%2520recognition%252C%2520which%2520leverages%2520depth%250Aestimation%2520and%2520athlete%2520patch%2520retrieval%2520to%2520avoid%2520the%2520computationally%2520expensive%250Ahuman%2520pose%2520estimation%2520module.%2520Using%2520Depth%2520Anything%2520V2%2520for%2520depth%2520estimation%2520and%250AYOLOv10%2520for%2520athlete%2520localization%252C%2520we%2520segment%2520the%2520subject%2520from%2520the%2520background%250Arather%2520than%2520relying%2520on%2520traditional%2520pose%2520estimation%2520techniques.%2520This%2520strategy%250Aincreases%2520efficiency%252C%2520reduces%2520inference%2520time%252C%2520and%2520improves%2520classification%250Aaccuracy.%2520Our%2520approach%2520significantly%2520outperforms%2520skeleton-based%2520methods%252C%250Aachieving%252038.3x%2520faster%2520inference%2520with%2520RGB%2520image%2520patches%2520and%2520improved%250Aclassification%2520accuracy%2520with%2520depth%2520patches%2520%25280.837%2520vs.%25200.815%2529.%2520Beyond%2520these%250Aperformance%2520gains%252C%2520the%2520modular%2520design%2520of%2520our%2520pipeline%2520allows%2520for%2520flexible%250Areplacement%2520of%2520components%252C%2520enabling%2520future%2520enhancements%2520and%2520adaptation%2520to%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Calisthenics%20Skills%20Classification%20through%20Foreground%20Instance%0A%20%20Selection%20and%20Depth%20Estimation&entry.906535625=Antonio%20Finocchiaro%20and%20Giovanni%20Maria%20Farinella%20and%20Antonino%20Furnari&entry.1292438233=%20%20Calisthenics%20skill%20classification%20is%20the%20computer%20vision%20task%20of%20inferring%0Athe%20skill%20performed%20by%20an%20athlete%20from%20images%2C%20enabling%20automatic%20performance%0Aassessment%20and%20personalized%20analytics.%20Traditional%20methods%20for%20calisthenics%0Askill%20recognition%20are%20based%20on%20pose%20estimation%20methods%20to%20determine%20the%0Aposition%20of%20skeletal%20data%20from%20images%2C%20which%20is%20later%20fed%20to%20a%20classification%0Aalgorithm%20to%20infer%20the%20performed%20skill.%20Despite%20the%20progress%20in%20human%20pose%0Aestimation%20algorithms%2C%20they%20still%20involve%20high%20computational%20costs%2C%20long%0Ainference%20times%2C%20and%20complex%20setups%2C%20which%20limit%20the%20applicability%20of%20such%0Aapproaches%20in%20real-time%20applications%20or%20mobile%20devices.%20This%20work%20proposes%20a%0Adirect%20approach%20to%20calisthenics%20skill%20recognition%2C%20which%20leverages%20depth%0Aestimation%20and%20athlete%20patch%20retrieval%20to%20avoid%20the%20computationally%20expensive%0Ahuman%20pose%20estimation%20module.%20Using%20Depth%20Anything%20V2%20for%20depth%20estimation%20and%0AYOLOv10%20for%20athlete%20localization%2C%20we%20segment%20the%20subject%20from%20the%20background%0Arather%20than%20relying%20on%20traditional%20pose%20estimation%20techniques.%20This%20strategy%0Aincreases%20efficiency%2C%20reduces%20inference%20time%2C%20and%20improves%20classification%0Aaccuracy.%20Our%20approach%20significantly%20outperforms%20skeleton-based%20methods%2C%0Aachieving%2038.3x%20faster%20inference%20with%20RGB%20image%20patches%20and%20improved%0Aclassification%20accuracy%20with%20depth%20patches%20%280.837%20vs.%200.815%29.%20Beyond%20these%0Aperformance%20gains%2C%20the%20modular%20design%20of%20our%20pipeline%20allows%20for%20flexible%0Areplacement%20of%20components%2C%20enabling%20future%20enhancements%20and%20adaptation%20to%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12292v1&entry.124074799=Read"},
{"title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge", "author": "Wenqing Wu and Chengzhi Zhang and Yi Zhao", "abstract": "  Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.\n", "link": "http://arxiv.org/abs/2507.11330v2", "date": "2025-07-16", "relevancy": 2.026, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Novelty%20Evaluation%20of%20Academic%20Paper%3A%20A%20Collaborative%20Approach%0A%20%20Integrating%20Human%20and%20Large%20Language%20Model%20Knowledge&body=Title%3A%20Automated%20Novelty%20Evaluation%20of%20Academic%20Paper%3A%20A%20Collaborative%20Approach%0A%20%20Integrating%20Human%20and%20Large%20Language%20Model%20Knowledge%0AAuthor%3A%20Wenqing%20Wu%20and%20Chengzhi%20Zhang%20and%20Yi%20Zhao%0AAbstract%3A%20%20%20Novelty%20is%20a%20crucial%20criterion%20in%20the%20peer%20review%20process%20for%20evaluating%0Aacademic%20papers.%20Traditionally%2C%20it%27s%20judged%20by%20experts%20or%20measure%20by%20unique%0Areference%20combinations.%20Both%20methods%20have%20limitations%3A%20experts%20have%20limited%0Aknowledge%2C%20and%20the%20effectiveness%20of%20the%20combination%20method%20is%20uncertain.%0AMoreover%2C%20it%27s%20unclear%20if%20unique%20citations%20truly%20measure%20novelty.%20The%20large%0Alanguage%20model%20%28LLM%29%20possesses%20a%20wealth%20of%20knowledge%2C%20while%20human%20experts%0Apossess%20judgment%20abilities%20that%20the%20LLM%20does%20not%20possess.%20Therefore%2C%20our%0Aresearch%20integrates%20the%20knowledge%20and%20abilities%20of%20LLM%20and%20human%20experts%20to%0Aaddress%20the%20limitations%20of%20novelty%20assessment.%20One%20of%20the%20most%20common%20types%20of%0Anovelty%20in%20academic%20papers%20is%20the%20introduction%20of%20new%20methods.%20In%20this%20paper%2C%0Awe%20propose%20leveraging%20human%20knowledge%20and%20LLM%20to%20assist%20pretrained%20language%0Amodels%20%28PLMs%2C%20e.g.%20BERT%20etc.%29%20in%20predicting%20the%20method%20novelty%20of%20papers.%0ASpecifically%2C%20we%20extract%20sentences%20related%20to%20the%20novelty%20of%20the%20academic%20paper%0Afrom%20peer%20review%20reports%20and%20use%20LLM%20to%20summarize%20the%20methodology%20section%20of%0Athe%20academic%20paper%2C%20which%20are%20then%20used%20to%20fine-tune%20PLMs.%20In%20addition%2C%20we%20have%0Adesigned%20a%20text-guided%20fusion%20module%20with%20novel%20Sparse-Attention%20to%20better%0Aintegrate%20human%20and%20LLM%20knowledge.%20We%20compared%20the%20method%20we%20proposed%20with%20a%0Alarge%20number%20of%20baselines.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Novelty%2520Evaluation%2520of%2520Academic%2520Paper%253A%2520A%2520Collaborative%2520Approach%250A%2520%2520Integrating%2520Human%2520and%2520Large%2520Language%2520Model%2520Knowledge%26entry.906535625%3DWenqing%2520Wu%2520and%2520Chengzhi%2520Zhang%2520and%2520Yi%2520Zhao%26entry.1292438233%3D%2520%2520Novelty%2520is%2520a%2520crucial%2520criterion%2520in%2520the%2520peer%2520review%2520process%2520for%2520evaluating%250Aacademic%2520papers.%2520Traditionally%252C%2520it%2527s%2520judged%2520by%2520experts%2520or%2520measure%2520by%2520unique%250Areference%2520combinations.%2520Both%2520methods%2520have%2520limitations%253A%2520experts%2520have%2520limited%250Aknowledge%252C%2520and%2520the%2520effectiveness%2520of%2520the%2520combination%2520method%2520is%2520uncertain.%250AMoreover%252C%2520it%2527s%2520unclear%2520if%2520unique%2520citations%2520truly%2520measure%2520novelty.%2520The%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520possesses%2520a%2520wealth%2520of%2520knowledge%252C%2520while%2520human%2520experts%250Apossess%2520judgment%2520abilities%2520that%2520the%2520LLM%2520does%2520not%2520possess.%2520Therefore%252C%2520our%250Aresearch%2520integrates%2520the%2520knowledge%2520and%2520abilities%2520of%2520LLM%2520and%2520human%2520experts%2520to%250Aaddress%2520the%2520limitations%2520of%2520novelty%2520assessment.%2520One%2520of%2520the%2520most%2520common%2520types%2520of%250Anovelty%2520in%2520academic%2520papers%2520is%2520the%2520introduction%2520of%2520new%2520methods.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520leveraging%2520human%2520knowledge%2520and%2520LLM%2520to%2520assist%2520pretrained%2520language%250Amodels%2520%2528PLMs%252C%2520e.g.%2520BERT%2520etc.%2529%2520in%2520predicting%2520the%2520method%2520novelty%2520of%2520papers.%250ASpecifically%252C%2520we%2520extract%2520sentences%2520related%2520to%2520the%2520novelty%2520of%2520the%2520academic%2520paper%250Afrom%2520peer%2520review%2520reports%2520and%2520use%2520LLM%2520to%2520summarize%2520the%2520methodology%2520section%2520of%250Athe%2520academic%2520paper%252C%2520which%2520are%2520then%2520used%2520to%2520fine-tune%2520PLMs.%2520In%2520addition%252C%2520we%2520have%250Adesigned%2520a%2520text-guided%2520fusion%2520module%2520with%2520novel%2520Sparse-Attention%2520to%2520better%250Aintegrate%2520human%2520and%2520LLM%2520knowledge.%2520We%2520compared%2520the%2520method%2520we%2520proposed%2520with%2520a%250Alarge%2520number%2520of%2520baselines.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Novelty%20Evaluation%20of%20Academic%20Paper%3A%20A%20Collaborative%20Approach%0A%20%20Integrating%20Human%20and%20Large%20Language%20Model%20Knowledge&entry.906535625=Wenqing%20Wu%20and%20Chengzhi%20Zhang%20and%20Yi%20Zhao&entry.1292438233=%20%20Novelty%20is%20a%20crucial%20criterion%20in%20the%20peer%20review%20process%20for%20evaluating%0Aacademic%20papers.%20Traditionally%2C%20it%27s%20judged%20by%20experts%20or%20measure%20by%20unique%0Areference%20combinations.%20Both%20methods%20have%20limitations%3A%20experts%20have%20limited%0Aknowledge%2C%20and%20the%20effectiveness%20of%20the%20combination%20method%20is%20uncertain.%0AMoreover%2C%20it%27s%20unclear%20if%20unique%20citations%20truly%20measure%20novelty.%20The%20large%0Alanguage%20model%20%28LLM%29%20possesses%20a%20wealth%20of%20knowledge%2C%20while%20human%20experts%0Apossess%20judgment%20abilities%20that%20the%20LLM%20does%20not%20possess.%20Therefore%2C%20our%0Aresearch%20integrates%20the%20knowledge%20and%20abilities%20of%20LLM%20and%20human%20experts%20to%0Aaddress%20the%20limitations%20of%20novelty%20assessment.%20One%20of%20the%20most%20common%20types%20of%0Anovelty%20in%20academic%20papers%20is%20the%20introduction%20of%20new%20methods.%20In%20this%20paper%2C%0Awe%20propose%20leveraging%20human%20knowledge%20and%20LLM%20to%20assist%20pretrained%20language%0Amodels%20%28PLMs%2C%20e.g.%20BERT%20etc.%29%20in%20predicting%20the%20method%20novelty%20of%20papers.%0ASpecifically%2C%20we%20extract%20sentences%20related%20to%20the%20novelty%20of%20the%20academic%20paper%0Afrom%20peer%20review%20reports%20and%20use%20LLM%20to%20summarize%20the%20methodology%20section%20of%0Athe%20academic%20paper%2C%20which%20are%20then%20used%20to%20fine-tune%20PLMs.%20In%20addition%2C%20we%20have%0Adesigned%20a%20text-guided%20fusion%20module%20with%20novel%20Sparse-Attention%20to%20better%0Aintegrate%20human%20and%20LLM%20knowledge.%20We%20compared%20the%20method%20we%20proposed%20with%20a%0Alarge%20number%20of%20baselines.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11330v2&entry.124074799=Read"},
{"title": "SoK: Semantic Privacy in Large Language Models", "author": "Baihe Ma and Yanna Jiang and Xu Wang and Guangsheng Yu and Qin Wang and Caijun Sun and Chen Li and Xuelei Qi and Ying He and Wei Ni and Ren Ping Liu", "abstract": "  As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.\n", "link": "http://arxiv.org/abs/2506.23603v2", "date": "2025-07-16", "relevancy": 2.0216, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Semantic%20Privacy%20in%20Large%20Language%20Models&body=Title%3A%20SoK%3A%20Semantic%20Privacy%20in%20Large%20Language%20Models%0AAuthor%3A%20Baihe%20Ma%20and%20Yanna%20Jiang%20and%20Xu%20Wang%20and%20Guangsheng%20Yu%20and%20Qin%20Wang%20and%20Caijun%20Sun%20and%20Chen%20Li%20and%20Xuelei%20Qi%20and%20Ying%20He%20and%20Wei%20Ni%20and%20Ren%20Ping%20Liu%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%0Adomains%2C%20traditional%20data%20privacy%20measures%20prove%20inadequate%20for%20protecting%0Ainformation%20that%20is%20implicit%2C%20contextual%2C%20or%20inferable%20-%20what%20we%20define%20as%0Asemantic%20privacy.%20This%20Systematization%20of%20Knowledge%20%28SoK%29%20introduces%20a%0Alifecycle-centric%20framework%20to%20analyze%20how%20semantic%20privacy%20risks%20emerge%20across%0Ainput%20processing%2C%20pretraining%2C%20fine-tuning%2C%20and%20alignment%20stages%20of%20LLMs.%20We%0Acategorize%20key%20attack%20vectors%20and%20assess%20how%20current%20defenses%2C%20such%20as%0Adifferential%20privacy%2C%20embedding%20encryption%2C%20edge%20computing%2C%20and%20unlearning%2C%0Aaddress%20these%20threats.%20Our%20analysis%20reveals%20critical%20gaps%20in%20semantic-level%0Aprotection%2C%20especially%20against%20contextual%20inference%20and%20latent%20representation%0Aleakage.%20We%20conclude%20by%20outlining%20open%20challenges%2C%20including%20quantifying%0Asemantic%20leakage%2C%20protecting%20multimodal%20inputs%2C%20balancing%20de-identification%0Awith%20generation%20quality%2C%20and%20ensuring%20transparency%20in%20privacy%20enforcement.%20This%0Awork%20aims%20to%20inform%20future%20research%20on%20designing%20robust%2C%20semantically%20aware%0Aprivacy-preserving%20techniques%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520Semantic%2520Privacy%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DBaihe%2520Ma%2520and%2520Yanna%2520Jiang%2520and%2520Xu%2520Wang%2520and%2520Guangsheng%2520Yu%2520and%2520Qin%2520Wang%2520and%2520Caijun%2520Sun%2520and%2520Chen%2520Li%2520and%2520Xuelei%2520Qi%2520and%2520Ying%2520He%2520and%2520Wei%2520Ni%2520and%2520Ren%2520Ping%2520Liu%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520sensitive%250Adomains%252C%2520traditional%2520data%2520privacy%2520measures%2520prove%2520inadequate%2520for%2520protecting%250Ainformation%2520that%2520is%2520implicit%252C%2520contextual%252C%2520or%2520inferable%2520-%2520what%2520we%2520define%2520as%250Asemantic%2520privacy.%2520This%2520Systematization%2520of%2520Knowledge%2520%2528SoK%2529%2520introduces%2520a%250Alifecycle-centric%2520framework%2520to%2520analyze%2520how%2520semantic%2520privacy%2520risks%2520emerge%2520across%250Ainput%2520processing%252C%2520pretraining%252C%2520fine-tuning%252C%2520and%2520alignment%2520stages%2520of%2520LLMs.%2520We%250Acategorize%2520key%2520attack%2520vectors%2520and%2520assess%2520how%2520current%2520defenses%252C%2520such%2520as%250Adifferential%2520privacy%252C%2520embedding%2520encryption%252C%2520edge%2520computing%252C%2520and%2520unlearning%252C%250Aaddress%2520these%2520threats.%2520Our%2520analysis%2520reveals%2520critical%2520gaps%2520in%2520semantic-level%250Aprotection%252C%2520especially%2520against%2520contextual%2520inference%2520and%2520latent%2520representation%250Aleakage.%2520We%2520conclude%2520by%2520outlining%2520open%2520challenges%252C%2520including%2520quantifying%250Asemantic%2520leakage%252C%2520protecting%2520multimodal%2520inputs%252C%2520balancing%2520de-identification%250Awith%2520generation%2520quality%252C%2520and%2520ensuring%2520transparency%2520in%2520privacy%2520enforcement.%2520This%250Awork%2520aims%2520to%2520inform%2520future%2520research%2520on%2520designing%2520robust%252C%2520semantically%2520aware%250Aprivacy-preserving%2520techniques%2520for%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Semantic%20Privacy%20in%20Large%20Language%20Models&entry.906535625=Baihe%20Ma%20and%20Yanna%20Jiang%20and%20Xu%20Wang%20and%20Guangsheng%20Yu%20and%20Qin%20Wang%20and%20Caijun%20Sun%20and%20Chen%20Li%20and%20Xuelei%20Qi%20and%20Ying%20He%20and%20Wei%20Ni%20and%20Ren%20Ping%20Liu&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%0Adomains%2C%20traditional%20data%20privacy%20measures%20prove%20inadequate%20for%20protecting%0Ainformation%20that%20is%20implicit%2C%20contextual%2C%20or%20inferable%20-%20what%20we%20define%20as%0Asemantic%20privacy.%20This%20Systematization%20of%20Knowledge%20%28SoK%29%20introduces%20a%0Alifecycle-centric%20framework%20to%20analyze%20how%20semantic%20privacy%20risks%20emerge%20across%0Ainput%20processing%2C%20pretraining%2C%20fine-tuning%2C%20and%20alignment%20stages%20of%20LLMs.%20We%0Acategorize%20key%20attack%20vectors%20and%20assess%20how%20current%20defenses%2C%20such%20as%0Adifferential%20privacy%2C%20embedding%20encryption%2C%20edge%20computing%2C%20and%20unlearning%2C%0Aaddress%20these%20threats.%20Our%20analysis%20reveals%20critical%20gaps%20in%20semantic-level%0Aprotection%2C%20especially%20against%20contextual%20inference%20and%20latent%20representation%0Aleakage.%20We%20conclude%20by%20outlining%20open%20challenges%2C%20including%20quantifying%0Asemantic%20leakage%2C%20protecting%20multimodal%20inputs%2C%20balancing%20de-identification%0Awith%20generation%20quality%2C%20and%20ensuring%20transparency%20in%20privacy%20enforcement.%20This%0Awork%20aims%20to%20inform%20future%20research%20on%20designing%20robust%2C%20semantically%20aware%0Aprivacy-preserving%20techniques%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23603v2&entry.124074799=Read"},
{"title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "author": "Suman Adhya and Debarshi Kumar Sanyal", "abstract": "  Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.\n", "link": "http://arxiv.org/abs/2507.12451v1", "date": "2025-07-16", "relevancy": 2.021, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2WTM%3A%20Spherical%20Sliced-Wasserstein%20Autoencoder%20for%20Topic%20Modeling&body=Title%3A%20S2WTM%3A%20Spherical%20Sliced-Wasserstein%20Autoencoder%20for%20Topic%20Modeling%0AAuthor%3A%20Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal%0AAbstract%3A%20%20%20Modeling%20latent%20representations%20in%20a%20hyperspherical%20space%20has%20proven%0Aeffective%20for%20capturing%20directional%20similarities%20in%20high-dimensional%20text%20data%2C%0Abenefiting%20topic%20modeling.%20Variational%20autoencoder-based%20neural%20topic%20models%0A%28VAE-NTMs%29%20commonly%20adopt%20the%20von%20Mises-Fisher%20prior%20to%20encode%20hyperspherical%0Astructure.%20However%2C%20VAE-NTMs%20often%20suffer%20from%20posterior%20collapse%2C%20where%20the%20KL%0Adivergence%20term%20in%20the%20objective%20function%20highly%20diminishes%2C%20leading%20to%0Aineffective%20latent%20representations.%20To%20mitigate%20this%20issue%20while%20modeling%0Ahyperspherical%20structure%20in%20the%20latent%20space%2C%20we%20propose%20the%20Spherical%20Sliced%0AWasserstein%20Autoencoder%20for%20Topic%20Modeling%20%28S2WTM%29.%20S2WTM%20employs%20a%20prior%0Adistribution%20supported%20on%20the%20unit%20hypersphere%20and%20leverages%20the%20Spherical%0ASliced-Wasserstein%20distance%20to%20align%20the%20aggregated%20posterior%20distribution%20with%0Athe%20prior.%20Experimental%20results%20demonstrate%20that%20S2WTM%20outperforms%0Astate-of-the-art%20topic%20models%2C%20generating%20more%20coherent%20and%20diverse%20topics%0Awhile%20improving%20performance%20on%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2WTM%253A%2520Spherical%2520Sliced-Wasserstein%2520Autoencoder%2520for%2520Topic%2520Modeling%26entry.906535625%3DSuman%2520Adhya%2520and%2520Debarshi%2520Kumar%2520Sanyal%26entry.1292438233%3D%2520%2520Modeling%2520latent%2520representations%2520in%2520a%2520hyperspherical%2520space%2520has%2520proven%250Aeffective%2520for%2520capturing%2520directional%2520similarities%2520in%2520high-dimensional%2520text%2520data%252C%250Abenefiting%2520topic%2520modeling.%2520Variational%2520autoencoder-based%2520neural%2520topic%2520models%250A%2528VAE-NTMs%2529%2520commonly%2520adopt%2520the%2520von%2520Mises-Fisher%2520prior%2520to%2520encode%2520hyperspherical%250Astructure.%2520However%252C%2520VAE-NTMs%2520often%2520suffer%2520from%2520posterior%2520collapse%252C%2520where%2520the%2520KL%250Adivergence%2520term%2520in%2520the%2520objective%2520function%2520highly%2520diminishes%252C%2520leading%2520to%250Aineffective%2520latent%2520representations.%2520To%2520mitigate%2520this%2520issue%2520while%2520modeling%250Ahyperspherical%2520structure%2520in%2520the%2520latent%2520space%252C%2520we%2520propose%2520the%2520Spherical%2520Sliced%250AWasserstein%2520Autoencoder%2520for%2520Topic%2520Modeling%2520%2528S2WTM%2529.%2520S2WTM%2520employs%2520a%2520prior%250Adistribution%2520supported%2520on%2520the%2520unit%2520hypersphere%2520and%2520leverages%2520the%2520Spherical%250ASliced-Wasserstein%2520distance%2520to%2520align%2520the%2520aggregated%2520posterior%2520distribution%2520with%250Athe%2520prior.%2520Experimental%2520results%2520demonstrate%2520that%2520S2WTM%2520outperforms%250Astate-of-the-art%2520topic%2520models%252C%2520generating%2520more%2520coherent%2520and%2520diverse%2520topics%250Awhile%2520improving%2520performance%2520on%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2WTM%3A%20Spherical%20Sliced-Wasserstein%20Autoencoder%20for%20Topic%20Modeling&entry.906535625=Suman%20Adhya%20and%20Debarshi%20Kumar%20Sanyal&entry.1292438233=%20%20Modeling%20latent%20representations%20in%20a%20hyperspherical%20space%20has%20proven%0Aeffective%20for%20capturing%20directional%20similarities%20in%20high-dimensional%20text%20data%2C%0Abenefiting%20topic%20modeling.%20Variational%20autoencoder-based%20neural%20topic%20models%0A%28VAE-NTMs%29%20commonly%20adopt%20the%20von%20Mises-Fisher%20prior%20to%20encode%20hyperspherical%0Astructure.%20However%2C%20VAE-NTMs%20often%20suffer%20from%20posterior%20collapse%2C%20where%20the%20KL%0Adivergence%20term%20in%20the%20objective%20function%20highly%20diminishes%2C%20leading%20to%0Aineffective%20latent%20representations.%20To%20mitigate%20this%20issue%20while%20modeling%0Ahyperspherical%20structure%20in%20the%20latent%20space%2C%20we%20propose%20the%20Spherical%20Sliced%0AWasserstein%20Autoencoder%20for%20Topic%20Modeling%20%28S2WTM%29.%20S2WTM%20employs%20a%20prior%0Adistribution%20supported%20on%20the%20unit%20hypersphere%20and%20leverages%20the%20Spherical%0ASliced-Wasserstein%20distance%20to%20align%20the%20aggregated%20posterior%20distribution%20with%0Athe%20prior.%20Experimental%20results%20demonstrate%20that%20S2WTM%20outperforms%0Astate-of-the-art%20topic%20models%2C%20generating%20more%20coherent%20and%20diverse%20topics%0Awhile%20improving%20performance%20on%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12451v1&entry.124074799=Read"},
{"title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature\n  Representation", "author": "Ashkan Shakarami and Azade Farshad and Yousef Yeganeh and Lorenzo Nicole and Peter Schuffler and Stefano Ghidoni and Nassir Navab", "abstract": "  We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.\n", "link": "http://arxiv.org/abs/2507.12427v1", "date": "2025-07-16", "relevancy": 2.0092, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unit-Based%20Histopathology%20Tissue%20Segmentation%20via%20Multi-Level%20Feature%0A%20%20Representation&body=Title%3A%20Unit-Based%20Histopathology%20Tissue%20Segmentation%20via%20Multi-Level%20Feature%0A%20%20Representation%0AAuthor%3A%20Ashkan%20Shakarami%20and%20Azade%20Farshad%20and%20Yousef%20Yeganeh%20and%20Lorenzo%20Nicole%20and%20Peter%20Schuffler%20and%20Stefano%20Ghidoni%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20We%20propose%20UTS%2C%20a%20unit-based%20tissue%20segmentation%20framework%20for%20histopathology%0Athat%20classifies%20each%20fixed-size%2032%20%2A%2032%20tile%2C%20rather%20than%20each%20pixel%2C%20as%20the%0Asegmentation%20unit.%20This%20approach%20reduces%20annotation%20effort%20and%20improves%0Acomputational%20efficiency%20without%20compromising%20accuracy.%20To%20implement%20this%0Aapproach%2C%20we%20introduce%20a%20Multi-Level%20Vision%20Transformer%20%28L-ViT%29%2C%20which%20benefits%0Athe%20multi-level%20feature%20representation%20to%20capture%20both%20fine-grained%20morphology%0Aand%20global%20tissue%20context.%20Trained%20to%20segment%20breast%20tissue%20into%20three%0Acategories%20%28infiltrating%20tumor%2C%20non-neoplastic%20stroma%2C%20and%20fat%29%2C%20UTS%20supports%0Aclinically%20relevant%20tasks%20such%20as%20tumor-stroma%20quantification%20and%20surgical%0Amargin%20assessment.%20Evaluated%20on%20386%2C371%20tiles%20from%20459%20H%26E-stained%20regions%2C%20it%0Aoutperforms%20U-Net%20variants%20and%20transformer-based%20baselines.%20Code%20and%20Dataset%0Awill%20be%20available%20at%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnit-Based%2520Histopathology%2520Tissue%2520Segmentation%2520via%2520Multi-Level%2520Feature%250A%2520%2520Representation%26entry.906535625%3DAshkan%2520Shakarami%2520and%2520Azade%2520Farshad%2520and%2520Yousef%2520Yeganeh%2520and%2520Lorenzo%2520Nicole%2520and%2520Peter%2520Schuffler%2520and%2520Stefano%2520Ghidoni%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520We%2520propose%2520UTS%252C%2520a%2520unit-based%2520tissue%2520segmentation%2520framework%2520for%2520histopathology%250Athat%2520classifies%2520each%2520fixed-size%252032%2520%252A%252032%2520tile%252C%2520rather%2520than%2520each%2520pixel%252C%2520as%2520the%250Asegmentation%2520unit.%2520This%2520approach%2520reduces%2520annotation%2520effort%2520and%2520improves%250Acomputational%2520efficiency%2520without%2520compromising%2520accuracy.%2520To%2520implement%2520this%250Aapproach%252C%2520we%2520introduce%2520a%2520Multi-Level%2520Vision%2520Transformer%2520%2528L-ViT%2529%252C%2520which%2520benefits%250Athe%2520multi-level%2520feature%2520representation%2520to%2520capture%2520both%2520fine-grained%2520morphology%250Aand%2520global%2520tissue%2520context.%2520Trained%2520to%2520segment%2520breast%2520tissue%2520into%2520three%250Acategories%2520%2528infiltrating%2520tumor%252C%2520non-neoplastic%2520stroma%252C%2520and%2520fat%2529%252C%2520UTS%2520supports%250Aclinically%2520relevant%2520tasks%2520such%2520as%2520tumor-stroma%2520quantification%2520and%2520surgical%250Amargin%2520assessment.%2520Evaluated%2520on%2520386%252C371%2520tiles%2520from%2520459%2520H%2526E-stained%2520regions%252C%2520it%250Aoutperforms%2520U-Net%2520variants%2520and%2520transformer-based%2520baselines.%2520Code%2520and%2520Dataset%250Awill%2520be%2520available%2520at%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unit-Based%20Histopathology%20Tissue%20Segmentation%20via%20Multi-Level%20Feature%0A%20%20Representation&entry.906535625=Ashkan%20Shakarami%20and%20Azade%20Farshad%20and%20Yousef%20Yeganeh%20and%20Lorenzo%20Nicole%20and%20Peter%20Schuffler%20and%20Stefano%20Ghidoni%20and%20Nassir%20Navab&entry.1292438233=%20%20We%20propose%20UTS%2C%20a%20unit-based%20tissue%20segmentation%20framework%20for%20histopathology%0Athat%20classifies%20each%20fixed-size%2032%20%2A%2032%20tile%2C%20rather%20than%20each%20pixel%2C%20as%20the%0Asegmentation%20unit.%20This%20approach%20reduces%20annotation%20effort%20and%20improves%0Acomputational%20efficiency%20without%20compromising%20accuracy.%20To%20implement%20this%0Aapproach%2C%20we%20introduce%20a%20Multi-Level%20Vision%20Transformer%20%28L-ViT%29%2C%20which%20benefits%0Athe%20multi-level%20feature%20representation%20to%20capture%20both%20fine-grained%20morphology%0Aand%20global%20tissue%20context.%20Trained%20to%20segment%20breast%20tissue%20into%20three%0Acategories%20%28infiltrating%20tumor%2C%20non-neoplastic%20stroma%2C%20and%20fat%29%2C%20UTS%20supports%0Aclinically%20relevant%20tasks%20such%20as%20tumor-stroma%20quantification%20and%20surgical%0Amargin%20assessment.%20Evaluated%20on%20386%2C371%20tiles%20from%20459%20H%26E-stained%20regions%2C%20it%0Aoutperforms%20U-Net%20variants%20and%20transformer-based%20baselines.%20Code%20and%20Dataset%0Awill%20be%20available%20at%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12427v1&entry.124074799=Read"},
{"title": "Traffic-Aware Pedestrian Intention Prediction", "author": "Fahimeh Orvati Nia and Hai Lin", "abstract": "  Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.\n", "link": "http://arxiv.org/abs/2507.12433v1", "date": "2025-07-16", "relevancy": 2.0081, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traffic-Aware%20Pedestrian%20Intention%20Prediction&body=Title%3A%20Traffic-Aware%20Pedestrian%20Intention%20Prediction%0AAuthor%3A%20Fahimeh%20Orvati%20Nia%20and%20Hai%20Lin%0AAbstract%3A%20%20%20Accurate%20pedestrian%20intention%20estimation%20is%20crucial%20for%20the%20safe%20navigation%0Aof%20autonomous%20vehicles%20%28AVs%29%20and%20hence%20attracts%20a%20lot%20of%20research%20attention.%0AHowever%2C%20current%20models%20often%20fail%20to%20adequately%20consider%20dynamic%20traffic%0Asignals%20and%20contextual%20scene%20information%2C%20which%20are%20critical%20for%20real-world%0Aapplications.%20This%20paper%20presents%20a%20Traffic-Aware%20Spatio-Temporal%20Graph%0AConvolutional%20Network%20%28TA-STGCN%29%20that%20integrates%20traffic%20signs%20and%20their%20states%0A%28Red%2C%20Yellow%2C%20Green%29%20into%20pedestrian%20intention%20prediction.%20Our%20approach%0Aintroduces%20the%20integration%20of%20dynamic%20traffic%20signal%20states%20and%20bounding%20box%0Asize%20as%20key%20features%2C%20allowing%20the%20model%20to%20capture%20both%20spatial%20and%20temporal%0Adependencies%20in%20complex%20urban%20environments.%20The%20model%20surpasses%20existing%0Amethods%20in%20accuracy.%20Specifically%2C%20TA-STGCN%20achieves%20a%204.75%25%20higher%20accuracy%0Acompared%20to%20the%20baseline%20model%20on%20the%20PIE%20dataset%2C%20demonstrating%20its%0Aeffectiveness%20in%20improving%20pedestrian%20intention%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraffic-Aware%2520Pedestrian%2520Intention%2520Prediction%26entry.906535625%3DFahimeh%2520Orvati%2520Nia%2520and%2520Hai%2520Lin%26entry.1292438233%3D%2520%2520Accurate%2520pedestrian%2520intention%2520estimation%2520is%2520crucial%2520for%2520the%2520safe%2520navigation%250Aof%2520autonomous%2520vehicles%2520%2528AVs%2529%2520and%2520hence%2520attracts%2520a%2520lot%2520of%2520research%2520attention.%250AHowever%252C%2520current%2520models%2520often%2520fail%2520to%2520adequately%2520consider%2520dynamic%2520traffic%250Asignals%2520and%2520contextual%2520scene%2520information%252C%2520which%2520are%2520critical%2520for%2520real-world%250Aapplications.%2520This%2520paper%2520presents%2520a%2520Traffic-Aware%2520Spatio-Temporal%2520Graph%250AConvolutional%2520Network%2520%2528TA-STGCN%2529%2520that%2520integrates%2520traffic%2520signs%2520and%2520their%2520states%250A%2528Red%252C%2520Yellow%252C%2520Green%2529%2520into%2520pedestrian%2520intention%2520prediction.%2520Our%2520approach%250Aintroduces%2520the%2520integration%2520of%2520dynamic%2520traffic%2520signal%2520states%2520and%2520bounding%2520box%250Asize%2520as%2520key%2520features%252C%2520allowing%2520the%2520model%2520to%2520capture%2520both%2520spatial%2520and%2520temporal%250Adependencies%2520in%2520complex%2520urban%2520environments.%2520The%2520model%2520surpasses%2520existing%250Amethods%2520in%2520accuracy.%2520Specifically%252C%2520TA-STGCN%2520achieves%2520a%25204.75%2525%2520higher%2520accuracy%250Acompared%2520to%2520the%2520baseline%2520model%2520on%2520the%2520PIE%2520dataset%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520improving%2520pedestrian%2520intention%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traffic-Aware%20Pedestrian%20Intention%20Prediction&entry.906535625=Fahimeh%20Orvati%20Nia%20and%20Hai%20Lin&entry.1292438233=%20%20Accurate%20pedestrian%20intention%20estimation%20is%20crucial%20for%20the%20safe%20navigation%0Aof%20autonomous%20vehicles%20%28AVs%29%20and%20hence%20attracts%20a%20lot%20of%20research%20attention.%0AHowever%2C%20current%20models%20often%20fail%20to%20adequately%20consider%20dynamic%20traffic%0Asignals%20and%20contextual%20scene%20information%2C%20which%20are%20critical%20for%20real-world%0Aapplications.%20This%20paper%20presents%20a%20Traffic-Aware%20Spatio-Temporal%20Graph%0AConvolutional%20Network%20%28TA-STGCN%29%20that%20integrates%20traffic%20signs%20and%20their%20states%0A%28Red%2C%20Yellow%2C%20Green%29%20into%20pedestrian%20intention%20prediction.%20Our%20approach%0Aintroduces%20the%20integration%20of%20dynamic%20traffic%20signal%20states%20and%20bounding%20box%0Asize%20as%20key%20features%2C%20allowing%20the%20model%20to%20capture%20both%20spatial%20and%20temporal%0Adependencies%20in%20complex%20urban%20environments.%20The%20model%20surpasses%20existing%0Amethods%20in%20accuracy.%20Specifically%2C%20TA-STGCN%20achieves%20a%204.75%25%20higher%20accuracy%0Acompared%20to%20the%20baseline%20model%20on%20the%20PIE%20dataset%2C%20demonstrating%20its%0Aeffectiveness%20in%20improving%20pedestrian%20intention%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12433v1&entry.124074799=Read"},
{"title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning", "author": "Ziru Liu and Cheng Gong and Xinyu Fu and Yaofang Liu and Ran Chen and Shoubo Hu and Suiyun Zhang and Rui Liu and Qingfu Zhang and Dandan Tu", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.\n", "link": "http://arxiv.org/abs/2507.10628v2", "date": "2025-07-16", "relevancy": 1.9887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.496}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GHPO%3A%20Adaptive%20Guidance%20for%20Stable%20and%20Efficient%20LLM%20Reinforcement%0A%20%20Learning&body=Title%3A%20GHPO%3A%20Adaptive%20Guidance%20for%20Stable%20and%20Efficient%20LLM%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ziru%20Liu%20and%20Cheng%20Gong%20and%20Xinyu%20Fu%20and%20Yaofang%20Liu%20and%20Ran%20Chen%20and%20Shoubo%20Hu%20and%20Suiyun%20Zhang%20and%20Rui%20Liu%20and%20Qingfu%20Zhang%20and%20Dandan%20Tu%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%20emerged%20as%0Aa%20powerful%20paradigm%20for%20facilitating%20the%20self-improvement%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20particularly%20in%20the%20domain%20of%20complex%20reasoning%20tasks.%20However%2C%0Aprevailing%20on-policy%20RL%20methods%20often%20contend%20with%20significant%20training%0Ainstability%20and%20inefficiency.%20This%20is%20primarily%20due%20to%20a%20capacity-difficulty%0Amismatch%2C%20where%20the%20complexity%20of%20training%20data%20frequently%20outpaces%20the%20model%27s%0Acurrent%20capabilities%2C%20leading%20to%20critically%20sparse%20reward%20signals%20and%20stalled%0Alearning%20progress.%20This%20challenge%20is%20particularly%20acute%20for%20smaller%2C%20more%0Aresource-efficient%20LLMs.%20To%20overcome%20this%2C%20we%20introduce%20the%20Guided%20Hybrid%0APolicy%20Optimization%20%28GHPO%29%2C%20a%20novel%20difficulty-aware%20reinforcement%20learning%0Aframework.%20GHPO%20dynamically%20calibrates%20task%20difficulty%20by%20employing%20adaptive%0Aprompt%20refinement%20to%20provide%20targeted%20guidance.%20This%20unique%20approach%20adaptively%0Abalances%20direct%20imitation%20learning%20for%20problems%20currently%20beyond%20the%20model%27s%0Areach%20with%20exploration-based%20reinforcement%20learning%20for%20more%20manageable%20tasks%2C%0Aeffectively%20creating%20a%20smooth%20and%20optimized%20learning%20curriculum.%20Extensive%0Aexperiments%20demonstrate%20that%20GHPO%20achieves%20an%20average%20performance%20gain%20of%0Aapproximately%205%25%20across%20six%20challenging%20mathematics%20benchmarks%2C%20consistently%0Aoutperforming%20strong%20on-policy%20reinforcement%20learning%20and%20curriculum%20learning%0Abaselines.%20Further%20analysis%20confirms%20that%20our%20framework%20significantly%20enhances%0Aboth%20training%20stability%20and%20final%20reasoning%20performance%2C%20thus%20offering%20a%0Ascalable%20and%20efficient%20solution%20for%20developing%20powerful%20and%20robust%20reasoning%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGHPO%253A%2520Adaptive%2520Guidance%2520for%2520Stable%2520and%2520Efficient%2520LLM%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DZiru%2520Liu%2520and%2520Cheng%2520Gong%2520and%2520Xinyu%2520Fu%2520and%2520Yaofang%2520Liu%2520and%2520Ran%2520Chen%2520and%2520Shoubo%2520Hu%2520and%2520Suiyun%2520Zhang%2520and%2520Rui%2520Liu%2520and%2520Qingfu%2520Zhang%2520and%2520Dandan%2520Tu%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520recently%2520emerged%2520as%250Aa%2520powerful%2520paradigm%2520for%2520facilitating%2520the%2520self-improvement%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520particularly%2520in%2520the%2520domain%2520of%2520complex%2520reasoning%2520tasks.%2520However%252C%250Aprevailing%2520on-policy%2520RL%2520methods%2520often%2520contend%2520with%2520significant%2520training%250Ainstability%2520and%2520inefficiency.%2520This%2520is%2520primarily%2520due%2520to%2520a%2520capacity-difficulty%250Amismatch%252C%2520where%2520the%2520complexity%2520of%2520training%2520data%2520frequently%2520outpaces%2520the%2520model%2527s%250Acurrent%2520capabilities%252C%2520leading%2520to%2520critically%2520sparse%2520reward%2520signals%2520and%2520stalled%250Alearning%2520progress.%2520This%2520challenge%2520is%2520particularly%2520acute%2520for%2520smaller%252C%2520more%250Aresource-efficient%2520LLMs.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520the%2520Guided%2520Hybrid%250APolicy%2520Optimization%2520%2528GHPO%2529%252C%2520a%2520novel%2520difficulty-aware%2520reinforcement%2520learning%250Aframework.%2520GHPO%2520dynamically%2520calibrates%2520task%2520difficulty%2520by%2520employing%2520adaptive%250Aprompt%2520refinement%2520to%2520provide%2520targeted%2520guidance.%2520This%2520unique%2520approach%2520adaptively%250Abalances%2520direct%2520imitation%2520learning%2520for%2520problems%2520currently%2520beyond%2520the%2520model%2527s%250Areach%2520with%2520exploration-based%2520reinforcement%2520learning%2520for%2520more%2520manageable%2520tasks%252C%250Aeffectively%2520creating%2520a%2520smooth%2520and%2520optimized%2520learning%2520curriculum.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520GHPO%2520achieves%2520an%2520average%2520performance%2520gain%2520of%250Aapproximately%25205%2525%2520across%2520six%2520challenging%2520mathematics%2520benchmarks%252C%2520consistently%250Aoutperforming%2520strong%2520on-policy%2520reinforcement%2520learning%2520and%2520curriculum%2520learning%250Abaselines.%2520Further%2520analysis%2520confirms%2520that%2520our%2520framework%2520significantly%2520enhances%250Aboth%2520training%2520stability%2520and%2520final%2520reasoning%2520performance%252C%2520thus%2520offering%2520a%250Ascalable%2520and%2520efficient%2520solution%2520for%2520developing%2520powerful%2520and%2520robust%2520reasoning%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GHPO%3A%20Adaptive%20Guidance%20for%20Stable%20and%20Efficient%20LLM%20Reinforcement%0A%20%20Learning&entry.906535625=Ziru%20Liu%20and%20Cheng%20Gong%20and%20Xinyu%20Fu%20and%20Yaofang%20Liu%20and%20Ran%20Chen%20and%20Shoubo%20Hu%20and%20Suiyun%20Zhang%20and%20Rui%20Liu%20and%20Qingfu%20Zhang%20and%20Dandan%20Tu&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%20emerged%20as%0Aa%20powerful%20paradigm%20for%20facilitating%20the%20self-improvement%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20particularly%20in%20the%20domain%20of%20complex%20reasoning%20tasks.%20However%2C%0Aprevailing%20on-policy%20RL%20methods%20often%20contend%20with%20significant%20training%0Ainstability%20and%20inefficiency.%20This%20is%20primarily%20due%20to%20a%20capacity-difficulty%0Amismatch%2C%20where%20the%20complexity%20of%20training%20data%20frequently%20outpaces%20the%20model%27s%0Acurrent%20capabilities%2C%20leading%20to%20critically%20sparse%20reward%20signals%20and%20stalled%0Alearning%20progress.%20This%20challenge%20is%20particularly%20acute%20for%20smaller%2C%20more%0Aresource-efficient%20LLMs.%20To%20overcome%20this%2C%20we%20introduce%20the%20Guided%20Hybrid%0APolicy%20Optimization%20%28GHPO%29%2C%20a%20novel%20difficulty-aware%20reinforcement%20learning%0Aframework.%20GHPO%20dynamically%20calibrates%20task%20difficulty%20by%20employing%20adaptive%0Aprompt%20refinement%20to%20provide%20targeted%20guidance.%20This%20unique%20approach%20adaptively%0Abalances%20direct%20imitation%20learning%20for%20problems%20currently%20beyond%20the%20model%27s%0Areach%20with%20exploration-based%20reinforcement%20learning%20for%20more%20manageable%20tasks%2C%0Aeffectively%20creating%20a%20smooth%20and%20optimized%20learning%20curriculum.%20Extensive%0Aexperiments%20demonstrate%20that%20GHPO%20achieves%20an%20average%20performance%20gain%20of%0Aapproximately%205%25%20across%20six%20challenging%20mathematics%20benchmarks%2C%20consistently%0Aoutperforming%20strong%20on-policy%20reinforcement%20learning%20and%20curriculum%20learning%0Abaselines.%20Further%20analysis%20confirms%20that%20our%20framework%20significantly%20enhances%0Aboth%20training%20stability%20and%20final%20reasoning%20performance%2C%20thus%20offering%20a%0Ascalable%20and%20efficient%20solution%20for%20developing%20powerful%20and%20robust%20reasoning%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10628v2&entry.124074799=Read"},
{"title": "Active Deep Kernel Learning of Molecular Properties: Realizing Dynamic\n  Structural Embeddings", "author": "Ayana Ghosh and Maxim Ziatdinov and Sergei V. Kalinin", "abstract": "  As vast databases of chemical identities become increasingly available, the\nchallenge shifts to how we effectively explore and leverage these resources to\nstudy molecular properties. This paper presents an active learning approach for\nmolecular discovery using Deep Kernel Learning (DKL), demonstrated on the QM9\ndataset. DKL links structural embeddings directly to properties, creating\norganized latent spaces that prioritize relevant property information. By\niteratively recalculating embedding vectors in alignment with target\nproperties, DKL uncovers concentrated maxima representing key molecular\nproperties and reveals unexplored regions with potential for innovation. This\napproach underscores DKL's potential in advancing molecular research and\ndiscovery.\n", "link": "http://arxiv.org/abs/2403.01234v2", "date": "2025-07-16", "relevancy": 1.9761, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Deep%20Kernel%20Learning%20of%20Molecular%20Properties%3A%20Realizing%20Dynamic%0A%20%20Structural%20Embeddings&body=Title%3A%20Active%20Deep%20Kernel%20Learning%20of%20Molecular%20Properties%3A%20Realizing%20Dynamic%0A%20%20Structural%20Embeddings%0AAuthor%3A%20Ayana%20Ghosh%20and%20Maxim%20Ziatdinov%20and%20Sergei%20V.%20Kalinin%0AAbstract%3A%20%20%20As%20vast%20databases%20of%20chemical%20identities%20become%20increasingly%20available%2C%20the%0Achallenge%20shifts%20to%20how%20we%20effectively%20explore%20and%20leverage%20these%20resources%20to%0Astudy%20molecular%20properties.%20This%20paper%20presents%20an%20active%20learning%20approach%20for%0Amolecular%20discovery%20using%20Deep%20Kernel%20Learning%20%28DKL%29%2C%20demonstrated%20on%20the%20QM9%0Adataset.%20DKL%20links%20structural%20embeddings%20directly%20to%20properties%2C%20creating%0Aorganized%20latent%20spaces%20that%20prioritize%20relevant%20property%20information.%20By%0Aiteratively%20recalculating%20embedding%20vectors%20in%20alignment%20with%20target%0Aproperties%2C%20DKL%20uncovers%20concentrated%20maxima%20representing%20key%20molecular%0Aproperties%20and%20reveals%20unexplored%20regions%20with%20potential%20for%20innovation.%20This%0Aapproach%20underscores%20DKL%27s%20potential%20in%20advancing%20molecular%20research%20and%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Deep%2520Kernel%2520Learning%2520of%2520Molecular%2520Properties%253A%2520Realizing%2520Dynamic%250A%2520%2520Structural%2520Embeddings%26entry.906535625%3DAyana%2520Ghosh%2520and%2520Maxim%2520Ziatdinov%2520and%2520Sergei%2520V.%2520Kalinin%26entry.1292438233%3D%2520%2520As%2520vast%2520databases%2520of%2520chemical%2520identities%2520become%2520increasingly%2520available%252C%2520the%250Achallenge%2520shifts%2520to%2520how%2520we%2520effectively%2520explore%2520and%2520leverage%2520these%2520resources%2520to%250Astudy%2520molecular%2520properties.%2520This%2520paper%2520presents%2520an%2520active%2520learning%2520approach%2520for%250Amolecular%2520discovery%2520using%2520Deep%2520Kernel%2520Learning%2520%2528DKL%2529%252C%2520demonstrated%2520on%2520the%2520QM9%250Adataset.%2520DKL%2520links%2520structural%2520embeddings%2520directly%2520to%2520properties%252C%2520creating%250Aorganized%2520latent%2520spaces%2520that%2520prioritize%2520relevant%2520property%2520information.%2520By%250Aiteratively%2520recalculating%2520embedding%2520vectors%2520in%2520alignment%2520with%2520target%250Aproperties%252C%2520DKL%2520uncovers%2520concentrated%2520maxima%2520representing%2520key%2520molecular%250Aproperties%2520and%2520reveals%2520unexplored%2520regions%2520with%2520potential%2520for%2520innovation.%2520This%250Aapproach%2520underscores%2520DKL%2527s%2520potential%2520in%2520advancing%2520molecular%2520research%2520and%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Deep%20Kernel%20Learning%20of%20Molecular%20Properties%3A%20Realizing%20Dynamic%0A%20%20Structural%20Embeddings&entry.906535625=Ayana%20Ghosh%20and%20Maxim%20Ziatdinov%20and%20Sergei%20V.%20Kalinin&entry.1292438233=%20%20As%20vast%20databases%20of%20chemical%20identities%20become%20increasingly%20available%2C%20the%0Achallenge%20shifts%20to%20how%20we%20effectively%20explore%20and%20leverage%20these%20resources%20to%0Astudy%20molecular%20properties.%20This%20paper%20presents%20an%20active%20learning%20approach%20for%0Amolecular%20discovery%20using%20Deep%20Kernel%20Learning%20%28DKL%29%2C%20demonstrated%20on%20the%20QM9%0Adataset.%20DKL%20links%20structural%20embeddings%20directly%20to%20properties%2C%20creating%0Aorganized%20latent%20spaces%20that%20prioritize%20relevant%20property%20information.%20By%0Aiteratively%20recalculating%20embedding%20vectors%20in%20alignment%20with%20target%0Aproperties%2C%20DKL%20uncovers%20concentrated%20maxima%20representing%20key%20molecular%0Aproperties%20and%20reveals%20unexplored%20regions%20with%20potential%20for%20innovation.%20This%0Aapproach%20underscores%20DKL%27s%20potential%20in%20advancing%20molecular%20research%20and%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01234v2&entry.124074799=Read"},
{"title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons", "author": "Emre Can Acikgoz and Carl Guo and Suvodip Dey and Akul Datta and Takyoung Kim and Gokhan Tur and Dilek Hakkani-T\u00fcr", "abstract": "  Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.\n", "link": "http://arxiv.org/abs/2504.19982v2", "date": "2025-07-16", "relevancy": 1.9628, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons&body=Title%3A%20TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons%0AAuthor%3A%20Emre%20Can%20Acikgoz%20and%20Carl%20Guo%20and%20Suvodip%20Dey%20and%20Akul%20Datta%20and%20Takyoung%20Kim%20and%20Gokhan%20Tur%20and%20Dilek%20Hakkani-T%C3%BCr%0AAbstract%3A%20%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20are%20experiencing%20a%20revolution%20driven%20by%0ALarge%20Language%20Models%20%28LLMs%29%2C%20yet%20the%20evaluation%20methodologies%20for%20these%0Asystems%20remain%20insufficient%20for%20their%20growing%20sophistication.%20While%20traditional%0Aautomatic%20metrics%20effectively%20assessed%20earlier%20modular%20systems%2C%20they%20focus%0Asolely%20on%20the%20dialogue%20level%20and%20cannot%20detect%20critical%20intermediate%20errors%0Athat%20can%20arise%20during%20user-agent%20interactions.%20In%20this%20paper%2C%20we%20introduce%0ATD-EVAL%20%28Turn%20and%20Dialogue-level%20Evaluation%29%2C%20a%20two-step%20evaluation%20framework%0Athat%20unifies%20fine-grained%20turn-level%20analysis%20with%20holistic%20dialogue-level%0Acomparisons.%20At%20turn%20level%2C%20we%20evaluate%20each%20response%20along%20three%20TOD-specific%0Adimensions%3A%20conversation%20cohesion%2C%20backend%20knowledge%20consistency%2C%20and%20policy%0Acompliance.%20Meanwhile%2C%20we%20design%20TOD%20Agent%20Arena%20that%20uses%20pairwise%20comparisons%0Ato%20provide%20a%20measure%20of%20dialogue-level%20quality.%20Through%20experiments%20on%20MultiWOZ%0A2.4%20and%20%7B%5Ctau%7D-Bench%2C%20we%20demonstrate%20that%20TD-EVAL%20effectively%20identifies%20the%0Aconversational%20errors%20that%20conventional%20metrics%20miss.%20Furthermore%2C%20TD-EVAL%0Aexhibits%20better%20alignment%20with%20human%20judgments%20than%20traditional%20and%20LLM-based%0Ametrics.%20These%20findings%20demonstrate%20that%20TD-EVAL%20introduces%20a%20new%20paradigm%20for%0ATOD%20system%20evaluation%2C%20efficiently%20assessing%20both%20turn%20and%20system%20levels%20with%20a%0Aplug-and-play%20framework%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD-EVAL%253A%2520Revisiting%2520Task-Oriented%2520Dialogue%2520Evaluation%2520by%2520Combining%250A%2520%2520Turn-Level%2520Precision%2520with%2520Dialogue-Level%2520Comparisons%26entry.906535625%3DEmre%2520Can%2520Acikgoz%2520and%2520Carl%2520Guo%2520and%2520Suvodip%2520Dey%2520and%2520Akul%2520Datta%2520and%2520Takyoung%2520Kim%2520and%2520Gokhan%2520Tur%2520and%2520Dilek%2520Hakkani-T%25C3%25BCr%26entry.1292438233%3D%2520%2520Task-oriented%2520dialogue%2520%2528TOD%2529%2520systems%2520are%2520experiencing%2520a%2520revolution%2520driven%2520by%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yet%2520the%2520evaluation%2520methodologies%2520for%2520these%250Asystems%2520remain%2520insufficient%2520for%2520their%2520growing%2520sophistication.%2520While%2520traditional%250Aautomatic%2520metrics%2520effectively%2520assessed%2520earlier%2520modular%2520systems%252C%2520they%2520focus%250Asolely%2520on%2520the%2520dialogue%2520level%2520and%2520cannot%2520detect%2520critical%2520intermediate%2520errors%250Athat%2520can%2520arise%2520during%2520user-agent%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ATD-EVAL%2520%2528Turn%2520and%2520Dialogue-level%2520Evaluation%2529%252C%2520a%2520two-step%2520evaluation%2520framework%250Athat%2520unifies%2520fine-grained%2520turn-level%2520analysis%2520with%2520holistic%2520dialogue-level%250Acomparisons.%2520At%2520turn%2520level%252C%2520we%2520evaluate%2520each%2520response%2520along%2520three%2520TOD-specific%250Adimensions%253A%2520conversation%2520cohesion%252C%2520backend%2520knowledge%2520consistency%252C%2520and%2520policy%250Acompliance.%2520Meanwhile%252C%2520we%2520design%2520TOD%2520Agent%2520Arena%2520that%2520uses%2520pairwise%2520comparisons%250Ato%2520provide%2520a%2520measure%2520of%2520dialogue-level%2520quality.%2520Through%2520experiments%2520on%2520MultiWOZ%250A2.4%2520and%2520%257B%255Ctau%257D-Bench%252C%2520we%2520demonstrate%2520that%2520TD-EVAL%2520effectively%2520identifies%2520the%250Aconversational%2520errors%2520that%2520conventional%2520metrics%2520miss.%2520Furthermore%252C%2520TD-EVAL%250Aexhibits%2520better%2520alignment%2520with%2520human%2520judgments%2520than%2520traditional%2520and%2520LLM-based%250Ametrics.%2520These%2520findings%2520demonstrate%2520that%2520TD-EVAL%2520introduces%2520a%2520new%2520paradigm%2520for%250ATOD%2520system%2520evaluation%252C%2520efficiently%2520assessing%2520both%2520turn%2520and%2520system%2520levels%2520with%2520a%250Aplug-and-play%2520framework%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD-EVAL%3A%20Revisiting%20Task-Oriented%20Dialogue%20Evaluation%20by%20Combining%0A%20%20Turn-Level%20Precision%20with%20Dialogue-Level%20Comparisons&entry.906535625=Emre%20Can%20Acikgoz%20and%20Carl%20Guo%20and%20Suvodip%20Dey%20and%20Akul%20Datta%20and%20Takyoung%20Kim%20and%20Gokhan%20Tur%20and%20Dilek%20Hakkani-T%C3%BCr&entry.1292438233=%20%20Task-oriented%20dialogue%20%28TOD%29%20systems%20are%20experiencing%20a%20revolution%20driven%20by%0ALarge%20Language%20Models%20%28LLMs%29%2C%20yet%20the%20evaluation%20methodologies%20for%20these%0Asystems%20remain%20insufficient%20for%20their%20growing%20sophistication.%20While%20traditional%0Aautomatic%20metrics%20effectively%20assessed%20earlier%20modular%20systems%2C%20they%20focus%0Asolely%20on%20the%20dialogue%20level%20and%20cannot%20detect%20critical%20intermediate%20errors%0Athat%20can%20arise%20during%20user-agent%20interactions.%20In%20this%20paper%2C%20we%20introduce%0ATD-EVAL%20%28Turn%20and%20Dialogue-level%20Evaluation%29%2C%20a%20two-step%20evaluation%20framework%0Athat%20unifies%20fine-grained%20turn-level%20analysis%20with%20holistic%20dialogue-level%0Acomparisons.%20At%20turn%20level%2C%20we%20evaluate%20each%20response%20along%20three%20TOD-specific%0Adimensions%3A%20conversation%20cohesion%2C%20backend%20knowledge%20consistency%2C%20and%20policy%0Acompliance.%20Meanwhile%2C%20we%20design%20TOD%20Agent%20Arena%20that%20uses%20pairwise%20comparisons%0Ato%20provide%20a%20measure%20of%20dialogue-level%20quality.%20Through%20experiments%20on%20MultiWOZ%0A2.4%20and%20%7B%5Ctau%7D-Bench%2C%20we%20demonstrate%20that%20TD-EVAL%20effectively%20identifies%20the%0Aconversational%20errors%20that%20conventional%20metrics%20miss.%20Furthermore%2C%20TD-EVAL%0Aexhibits%20better%20alignment%20with%20human%20judgments%20than%20traditional%20and%20LLM-based%0Ametrics.%20These%20findings%20demonstrate%20that%20TD-EVAL%20introduces%20a%20new%20paradigm%20for%0ATOD%20system%20evaluation%2C%20efficiently%20assessing%20both%20turn%20and%20system%20levels%20with%20a%0Aplug-and-play%20framework%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19982v2&entry.124074799=Read"},
{"title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in\n  Composed Image Retrieval", "author": "Jaehyun Kwak and Ramahdani Muhammad Izaaz Inhar and Se-Young Yun and Sung-Ju Lee", "abstract": "  Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.\n", "link": "http://arxiv.org/abs/2507.12416v1", "date": "2025-07-16", "relevancy": 1.9569, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5019}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4895}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuRe%3A%20Query-Relevant%20Retrieval%20through%20Hard%20Negative%20Sampling%20in%0A%20%20Composed%20Image%20Retrieval&body=Title%3A%20QuRe%3A%20Query-Relevant%20Retrieval%20through%20Hard%20Negative%20Sampling%20in%0A%20%20Composed%20Image%20Retrieval%0AAuthor%3A%20Jaehyun%20Kwak%20and%20Ramahdani%20Muhammad%20Izaaz%20Inhar%20and%20Se-Young%20Yun%20and%20Sung-Ju%20Lee%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20retrieves%20relevant%20images%20based%20on%20a%20reference%0Aimage%20and%20accompanying%20text%20describing%20desired%20modifications.%20However%2C%20existing%0ACIR%20methods%20only%20focus%20on%20retrieving%20the%20target%20image%20and%20disregard%20the%0Arelevance%20of%20other%20images.%20This%20limitation%20arises%20because%20most%20methods%0Aemploying%20contrastive%20learning-which%20treats%20the%20target%20image%20as%20positive%20and%0Aall%20other%20images%20in%20the%20batch%20as%20negatives-can%20inadvertently%20include%20false%0Anegatives.%20This%20may%20result%20in%20retrieving%20irrelevant%20images%2C%20reducing%20user%0Asatisfaction%20even%20when%20the%20target%20image%20is%20retrieved.%20To%20address%20this%20issue%2C%20we%0Apropose%20Query-Relevant%20Retrieval%20through%20Hard%20Negative%20Sampling%20%28QuRe%29%2C%20which%0Aoptimizes%20a%20reward%20model%20objective%20to%20reduce%20false%20negatives.%20Additionally%2C%20we%0Aintroduce%20a%20hard%20negative%20sampling%20strategy%20that%20selects%20images%20positioned%0Abetween%20two%20steep%20drops%20in%20relevance%20scores%20following%20the%20target%20image%2C%20to%0Aeffectively%20filter%20false%20negatives.%20In%20order%20to%20evaluate%20CIR%20models%20on%20their%0Aalignment%20with%20human%20satisfaction%2C%20we%20create%20Human-Preference%20FashionIQ%0A%28HP-FashionIQ%29%2C%20a%20new%20dataset%20that%20explicitly%20captures%20user%20preferences%20beyond%0Atarget%20retrieval.%20Extensive%20experiments%20demonstrate%20that%20QuRe%20achieves%0Astate-of-the-art%20performance%20on%20FashionIQ%20and%20CIRR%20datasets%20while%20exhibiting%0Athe%20strongest%20alignment%20with%20human%20preferences%20on%20the%20HP-FashionIQ%20dataset.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/jackwaky/QuRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuRe%253A%2520Query-Relevant%2520Retrieval%2520through%2520Hard%2520Negative%2520Sampling%2520in%250A%2520%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DJaehyun%2520Kwak%2520and%2520Ramahdani%2520Muhammad%2520Izaaz%2520Inhar%2520and%2520Se-Young%2520Yun%2520and%2520Sung-Ju%2520Lee%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520retrieves%2520relevant%2520images%2520based%2520on%2520a%2520reference%250Aimage%2520and%2520accompanying%2520text%2520describing%2520desired%2520modifications.%2520However%252C%2520existing%250ACIR%2520methods%2520only%2520focus%2520on%2520retrieving%2520the%2520target%2520image%2520and%2520disregard%2520the%250Arelevance%2520of%2520other%2520images.%2520This%2520limitation%2520arises%2520because%2520most%2520methods%250Aemploying%2520contrastive%2520learning-which%2520treats%2520the%2520target%2520image%2520as%2520positive%2520and%250Aall%2520other%2520images%2520in%2520the%2520batch%2520as%2520negatives-can%2520inadvertently%2520include%2520false%250Anegatives.%2520This%2520may%2520result%2520in%2520retrieving%2520irrelevant%2520images%252C%2520reducing%2520user%250Asatisfaction%2520even%2520when%2520the%2520target%2520image%2520is%2520retrieved.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520Query-Relevant%2520Retrieval%2520through%2520Hard%2520Negative%2520Sampling%2520%2528QuRe%2529%252C%2520which%250Aoptimizes%2520a%2520reward%2520model%2520objective%2520to%2520reduce%2520false%2520negatives.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520hard%2520negative%2520sampling%2520strategy%2520that%2520selects%2520images%2520positioned%250Abetween%2520two%2520steep%2520drops%2520in%2520relevance%2520scores%2520following%2520the%2520target%2520image%252C%2520to%250Aeffectively%2520filter%2520false%2520negatives.%2520In%2520order%2520to%2520evaluate%2520CIR%2520models%2520on%2520their%250Aalignment%2520with%2520human%2520satisfaction%252C%2520we%2520create%2520Human-Preference%2520FashionIQ%250A%2528HP-FashionIQ%2529%252C%2520a%2520new%2520dataset%2520that%2520explicitly%2520captures%2520user%2520preferences%2520beyond%250Atarget%2520retrieval.%2520Extensive%2520experiments%2520demonstrate%2520that%2520QuRe%2520achieves%250Astate-of-the-art%2520performance%2520on%2520FashionIQ%2520and%2520CIRR%2520datasets%2520while%2520exhibiting%250Athe%2520strongest%2520alignment%2520with%2520human%2520preferences%2520on%2520the%2520HP-FashionIQ%2520dataset.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/jackwaky/QuRe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuRe%3A%20Query-Relevant%20Retrieval%20through%20Hard%20Negative%20Sampling%20in%0A%20%20Composed%20Image%20Retrieval&entry.906535625=Jaehyun%20Kwak%20and%20Ramahdani%20Muhammad%20Izaaz%20Inhar%20and%20Se-Young%20Yun%20and%20Sung-Ju%20Lee&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20retrieves%20relevant%20images%20based%20on%20a%20reference%0Aimage%20and%20accompanying%20text%20describing%20desired%20modifications.%20However%2C%20existing%0ACIR%20methods%20only%20focus%20on%20retrieving%20the%20target%20image%20and%20disregard%20the%0Arelevance%20of%20other%20images.%20This%20limitation%20arises%20because%20most%20methods%0Aemploying%20contrastive%20learning-which%20treats%20the%20target%20image%20as%20positive%20and%0Aall%20other%20images%20in%20the%20batch%20as%20negatives-can%20inadvertently%20include%20false%0Anegatives.%20This%20may%20result%20in%20retrieving%20irrelevant%20images%2C%20reducing%20user%0Asatisfaction%20even%20when%20the%20target%20image%20is%20retrieved.%20To%20address%20this%20issue%2C%20we%0Apropose%20Query-Relevant%20Retrieval%20through%20Hard%20Negative%20Sampling%20%28QuRe%29%2C%20which%0Aoptimizes%20a%20reward%20model%20objective%20to%20reduce%20false%20negatives.%20Additionally%2C%20we%0Aintroduce%20a%20hard%20negative%20sampling%20strategy%20that%20selects%20images%20positioned%0Abetween%20two%20steep%20drops%20in%20relevance%20scores%20following%20the%20target%20image%2C%20to%0Aeffectively%20filter%20false%20negatives.%20In%20order%20to%20evaluate%20CIR%20models%20on%20their%0Aalignment%20with%20human%20satisfaction%2C%20we%20create%20Human-Preference%20FashionIQ%0A%28HP-FashionIQ%29%2C%20a%20new%20dataset%20that%20explicitly%20captures%20user%20preferences%20beyond%0Atarget%20retrieval.%20Extensive%20experiments%20demonstrate%20that%20QuRe%20achieves%0Astate-of-the-art%20performance%20on%20FashionIQ%20and%20CIRR%20datasets%20while%20exhibiting%0Athe%20strongest%20alignment%20with%20human%20preferences%20on%20the%20HP-FashionIQ%20dataset.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/jackwaky/QuRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12416v1&entry.124074799=Read"},
{"title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI\n  Training and Inference through Concept Shift", "author": "Jiamin Chang and Haoyang Li and Hammond Pearce and Ruoxi Sun and Bo Li and Minhui Xue", "abstract": "  The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.\n", "link": "http://arxiv.org/abs/2504.21042v3", "date": "2025-07-16", "relevancy": 1.9562, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%27s%20Pulling%20the%20Strings%3F%20Evaluating%20Integrity%20and%20Attribution%20in%20AI%0A%20%20Training%20and%20Inference%20through%20Concept%20Shift&body=Title%3A%20What%27s%20Pulling%20the%20Strings%3F%20Evaluating%20Integrity%20and%20Attribution%20in%20AI%0A%20%20Training%20and%20Inference%20through%20Concept%20Shift%0AAuthor%3A%20Jiamin%20Chang%20and%20Haoyang%20Li%20and%20Hammond%20Pearce%20and%20Ruoxi%20Sun%20and%20Bo%20Li%20and%20Minhui%20Xue%0AAbstract%3A%20%20%20The%20growing%20adoption%20of%20artificial%20intelligence%20%28AI%29%20has%20amplified%20concerns%0Aabout%20trustworthiness%2C%20including%20integrity%2C%20privacy%2C%20robustness%2C%20and%20bias.%20To%0Aassess%20and%20attribute%20these%20threats%2C%20we%20propose%20ConceptLens%2C%20a%20generic%20framework%0Athat%20leverages%20pre-trained%20multimodal%20models%20to%20identify%20the%20root%20causes%20of%0Aintegrity%20threats%20by%20analyzing%20Concept%20Shift%20in%20probing%20samples.%20ConceptLens%0Ademonstrates%20strong%20detection%20performance%20for%20vanilla%20data%20poisoning%20attacks%0Aand%20uncovers%20vulnerabilities%20to%20bias%20injection%2C%20such%20as%20the%20generation%20of%0Acovert%20advertisements%20through%20malicious%20concept%20shifts.%20It%20identifies%20privacy%0Arisks%20in%20unaltered%20but%20high-risk%20samples%2C%20filters%20them%20before%20training%2C%20and%0Aprovides%20insights%20into%20model%20weaknesses%20arising%20from%20incomplete%20or%20imbalanced%0Atraining%20data.%20Additionally%2C%20at%20the%20model%20level%2C%20it%20attributes%20concepts%20that%0Athe%20target%20model%20is%20overly%20dependent%20on%2C%20identifies%20misleading%20concepts%2C%20and%0Aexplains%20how%20disrupting%20key%20concepts%20negatively%20impacts%20the%20model.%20Furthermore%2C%0Ait%20uncovers%20sociological%20biases%20in%20generative%20content%2C%20revealing%20disparities%0Aacross%20sociological%20contexts.%20Strikingly%2C%20ConceptLens%20reveals%20how%20safe%20training%0Aand%20inference%20data%20can%20be%20unintentionally%20and%20easily%20exploited%2C%20potentially%0Aundermining%20safety%20alignment.%20Our%20study%20informs%20actionable%20insights%20to%20breed%0Atrust%20in%20AI%20systems%2C%20thereby%20speeding%20adoption%20and%20driving%20greater%20innovation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21042v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2527s%2520Pulling%2520the%2520Strings%253F%2520Evaluating%2520Integrity%2520and%2520Attribution%2520in%2520AI%250A%2520%2520Training%2520and%2520Inference%2520through%2520Concept%2520Shift%26entry.906535625%3DJiamin%2520Chang%2520and%2520Haoyang%2520Li%2520and%2520Hammond%2520Pearce%2520and%2520Ruoxi%2520Sun%2520and%2520Bo%2520Li%2520and%2520Minhui%2520Xue%26entry.1292438233%3D%2520%2520The%2520growing%2520adoption%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520amplified%2520concerns%250Aabout%2520trustworthiness%252C%2520including%2520integrity%252C%2520privacy%252C%2520robustness%252C%2520and%2520bias.%2520To%250Aassess%2520and%2520attribute%2520these%2520threats%252C%2520we%2520propose%2520ConceptLens%252C%2520a%2520generic%2520framework%250Athat%2520leverages%2520pre-trained%2520multimodal%2520models%2520to%2520identify%2520the%2520root%2520causes%2520of%250Aintegrity%2520threats%2520by%2520analyzing%2520Concept%2520Shift%2520in%2520probing%2520samples.%2520ConceptLens%250Ademonstrates%2520strong%2520detection%2520performance%2520for%2520vanilla%2520data%2520poisoning%2520attacks%250Aand%2520uncovers%2520vulnerabilities%2520to%2520bias%2520injection%252C%2520such%2520as%2520the%2520generation%2520of%250Acovert%2520advertisements%2520through%2520malicious%2520concept%2520shifts.%2520It%2520identifies%2520privacy%250Arisks%2520in%2520unaltered%2520but%2520high-risk%2520samples%252C%2520filters%2520them%2520before%2520training%252C%2520and%250Aprovides%2520insights%2520into%2520model%2520weaknesses%2520arising%2520from%2520incomplete%2520or%2520imbalanced%250Atraining%2520data.%2520Additionally%252C%2520at%2520the%2520model%2520level%252C%2520it%2520attributes%2520concepts%2520that%250Athe%2520target%2520model%2520is%2520overly%2520dependent%2520on%252C%2520identifies%2520misleading%2520concepts%252C%2520and%250Aexplains%2520how%2520disrupting%2520key%2520concepts%2520negatively%2520impacts%2520the%2520model.%2520Furthermore%252C%250Ait%2520uncovers%2520sociological%2520biases%2520in%2520generative%2520content%252C%2520revealing%2520disparities%250Aacross%2520sociological%2520contexts.%2520Strikingly%252C%2520ConceptLens%2520reveals%2520how%2520safe%2520training%250Aand%2520inference%2520data%2520can%2520be%2520unintentionally%2520and%2520easily%2520exploited%252C%2520potentially%250Aundermining%2520safety%2520alignment.%2520Our%2520study%2520informs%2520actionable%2520insights%2520to%2520breed%250Atrust%2520in%2520AI%2520systems%252C%2520thereby%2520speeding%2520adoption%2520and%2520driving%2520greater%2520innovation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21042v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%27s%20Pulling%20the%20Strings%3F%20Evaluating%20Integrity%20and%20Attribution%20in%20AI%0A%20%20Training%20and%20Inference%20through%20Concept%20Shift&entry.906535625=Jiamin%20Chang%20and%20Haoyang%20Li%20and%20Hammond%20Pearce%20and%20Ruoxi%20Sun%20and%20Bo%20Li%20and%20Minhui%20Xue&entry.1292438233=%20%20The%20growing%20adoption%20of%20artificial%20intelligence%20%28AI%29%20has%20amplified%20concerns%0Aabout%20trustworthiness%2C%20including%20integrity%2C%20privacy%2C%20robustness%2C%20and%20bias.%20To%0Aassess%20and%20attribute%20these%20threats%2C%20we%20propose%20ConceptLens%2C%20a%20generic%20framework%0Athat%20leverages%20pre-trained%20multimodal%20models%20to%20identify%20the%20root%20causes%20of%0Aintegrity%20threats%20by%20analyzing%20Concept%20Shift%20in%20probing%20samples.%20ConceptLens%0Ademonstrates%20strong%20detection%20performance%20for%20vanilla%20data%20poisoning%20attacks%0Aand%20uncovers%20vulnerabilities%20to%20bias%20injection%2C%20such%20as%20the%20generation%20of%0Acovert%20advertisements%20through%20malicious%20concept%20shifts.%20It%20identifies%20privacy%0Arisks%20in%20unaltered%20but%20high-risk%20samples%2C%20filters%20them%20before%20training%2C%20and%0Aprovides%20insights%20into%20model%20weaknesses%20arising%20from%20incomplete%20or%20imbalanced%0Atraining%20data.%20Additionally%2C%20at%20the%20model%20level%2C%20it%20attributes%20concepts%20that%0Athe%20target%20model%20is%20overly%20dependent%20on%2C%20identifies%20misleading%20concepts%2C%20and%0Aexplains%20how%20disrupting%20key%20concepts%20negatively%20impacts%20the%20model.%20Furthermore%2C%0Ait%20uncovers%20sociological%20biases%20in%20generative%20content%2C%20revealing%20disparities%0Aacross%20sociological%20contexts.%20Strikingly%2C%20ConceptLens%20reveals%20how%20safe%20training%0Aand%20inference%20data%20can%20be%20unintentionally%20and%20easily%20exploited%2C%20potentially%0Aundermining%20safety%20alignment.%20Our%20study%20informs%20actionable%20insights%20to%20breed%0Atrust%20in%20AI%20systems%2C%20thereby%20speeding%20adoption%20and%20driving%20greater%20innovation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21042v3&entry.124074799=Read"},
{"title": "Can We Predict Alignment Before Models Finish Thinking? Towards\n  Monitoring Misaligned Reasoning Models", "author": "Yik Siu Chan and Zheng-Xin Yong and Stephen H. Bach", "abstract": "  Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.\n", "link": "http://arxiv.org/abs/2507.12428v1", "date": "2025-07-16", "relevancy": 1.9485, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Predict%20Alignment%20Before%20Models%20Finish%20Thinking%3F%20Towards%0A%20%20Monitoring%20Misaligned%20Reasoning%20Models&body=Title%3A%20Can%20We%20Predict%20Alignment%20Before%20Models%20Finish%20Thinking%3F%20Towards%0A%20%20Monitoring%20Misaligned%20Reasoning%20Models%0AAuthor%3A%20Yik%20Siu%20Chan%20and%20Zheng-Xin%20Yong%20and%20Stephen%20H.%20Bach%0AAbstract%3A%20%20%20Open-weights%20reasoning%20language%20models%20generate%20long%20chains-of-thought%20%28CoTs%29%0Abefore%20producing%20a%20final%20response%2C%20which%20improves%20performance%20but%20introduces%0Aadditional%20alignment%20risks%2C%20with%20harmful%20content%20often%20appearing%20in%20both%20the%0ACoTs%20and%20the%20final%20outputs.%20In%20this%20work%2C%20we%20investigate%20if%20we%20can%20use%20CoTs%20to%0Apredict%20final%20response%20misalignment.%20We%20evaluate%20a%20range%20of%20monitoring%0Aapproaches%2C%20including%20humans%2C%20highly-capable%20large%20language%20models%2C%20and%20text%0Aclassifiers%2C%20using%20either%20CoT%20text%20or%20activations.%20First%2C%20we%20find%20that%20a%20simple%0Alinear%20probe%20trained%20on%20CoT%20activations%20can%20significantly%20outperform%20all%0Atext-based%20methods%20in%20predicting%20whether%20a%20final%20response%20will%20be%20safe%20or%0Aunsafe.%20CoT%20texts%20are%20often%20unfaithful%20and%20can%20mislead%20humans%20and%20classifiers%2C%0Awhile%20model%20latents%20%28i.e.%2C%20CoT%20activations%29%20offer%20a%20more%20reliable%20predictive%0Asignal.%20Second%2C%20the%20probe%20makes%20accurate%20predictions%20before%20reasoning%0Acompletes%2C%20achieving%20strong%20performance%20even%20when%20applied%20to%20early%20CoT%0Asegments.%20These%20findings%20generalize%20across%20model%20sizes%2C%20families%2C%20and%20safety%0Abenchmarks%2C%20suggesting%20that%20lightweight%20probes%20could%20enable%20real-time%20safety%0Amonitoring%20and%20early%20intervention%20during%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Predict%2520Alignment%2520Before%2520Models%2520Finish%2520Thinking%253F%2520Towards%250A%2520%2520Monitoring%2520Misaligned%2520Reasoning%2520Models%26entry.906535625%3DYik%2520Siu%2520Chan%2520and%2520Zheng-Xin%2520Yong%2520and%2520Stephen%2520H.%2520Bach%26entry.1292438233%3D%2520%2520Open-weights%2520reasoning%2520language%2520models%2520generate%2520long%2520chains-of-thought%2520%2528CoTs%2529%250Abefore%2520producing%2520a%2520final%2520response%252C%2520which%2520improves%2520performance%2520but%2520introduces%250Aadditional%2520alignment%2520risks%252C%2520with%2520harmful%2520content%2520often%2520appearing%2520in%2520both%2520the%250ACoTs%2520and%2520the%2520final%2520outputs.%2520In%2520this%2520work%252C%2520we%2520investigate%2520if%2520we%2520can%2520use%2520CoTs%2520to%250Apredict%2520final%2520response%2520misalignment.%2520We%2520evaluate%2520a%2520range%2520of%2520monitoring%250Aapproaches%252C%2520including%2520humans%252C%2520highly-capable%2520large%2520language%2520models%252C%2520and%2520text%250Aclassifiers%252C%2520using%2520either%2520CoT%2520text%2520or%2520activations.%2520First%252C%2520we%2520find%2520that%2520a%2520simple%250Alinear%2520probe%2520trained%2520on%2520CoT%2520activations%2520can%2520significantly%2520outperform%2520all%250Atext-based%2520methods%2520in%2520predicting%2520whether%2520a%2520final%2520response%2520will%2520be%2520safe%2520or%250Aunsafe.%2520CoT%2520texts%2520are%2520often%2520unfaithful%2520and%2520can%2520mislead%2520humans%2520and%2520classifiers%252C%250Awhile%2520model%2520latents%2520%2528i.e.%252C%2520CoT%2520activations%2529%2520offer%2520a%2520more%2520reliable%2520predictive%250Asignal.%2520Second%252C%2520the%2520probe%2520makes%2520accurate%2520predictions%2520before%2520reasoning%250Acompletes%252C%2520achieving%2520strong%2520performance%2520even%2520when%2520applied%2520to%2520early%2520CoT%250Asegments.%2520These%2520findings%2520generalize%2520across%2520model%2520sizes%252C%2520families%252C%2520and%2520safety%250Abenchmarks%252C%2520suggesting%2520that%2520lightweight%2520probes%2520could%2520enable%2520real-time%2520safety%250Amonitoring%2520and%2520early%2520intervention%2520during%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Predict%20Alignment%20Before%20Models%20Finish%20Thinking%3F%20Towards%0A%20%20Monitoring%20Misaligned%20Reasoning%20Models&entry.906535625=Yik%20Siu%20Chan%20and%20Zheng-Xin%20Yong%20and%20Stephen%20H.%20Bach&entry.1292438233=%20%20Open-weights%20reasoning%20language%20models%20generate%20long%20chains-of-thought%20%28CoTs%29%0Abefore%20producing%20a%20final%20response%2C%20which%20improves%20performance%20but%20introduces%0Aadditional%20alignment%20risks%2C%20with%20harmful%20content%20often%20appearing%20in%20both%20the%0ACoTs%20and%20the%20final%20outputs.%20In%20this%20work%2C%20we%20investigate%20if%20we%20can%20use%20CoTs%20to%0Apredict%20final%20response%20misalignment.%20We%20evaluate%20a%20range%20of%20monitoring%0Aapproaches%2C%20including%20humans%2C%20highly-capable%20large%20language%20models%2C%20and%20text%0Aclassifiers%2C%20using%20either%20CoT%20text%20or%20activations.%20First%2C%20we%20find%20that%20a%20simple%0Alinear%20probe%20trained%20on%20CoT%20activations%20can%20significantly%20outperform%20all%0Atext-based%20methods%20in%20predicting%20whether%20a%20final%20response%20will%20be%20safe%20or%0Aunsafe.%20CoT%20texts%20are%20often%20unfaithful%20and%20can%20mislead%20humans%20and%20classifiers%2C%0Awhile%20model%20latents%20%28i.e.%2C%20CoT%20activations%29%20offer%20a%20more%20reliable%20predictive%0Asignal.%20Second%2C%20the%20probe%20makes%20accurate%20predictions%20before%20reasoning%0Acompletes%2C%20achieving%20strong%20performance%20even%20when%20applied%20to%20early%20CoT%0Asegments.%20These%20findings%20generalize%20across%20model%20sizes%2C%20families%2C%20and%20safety%0Abenchmarks%2C%20suggesting%20that%20lightweight%20probes%20could%20enable%20real-time%20safety%0Amonitoring%20and%20early%20intervention%20during%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12428v1&entry.124074799=Read"},
{"title": "Probing for Arithmetic Errors in Language Models", "author": "Yucheng Sun and Alessandro Stolfo and Mrinmaya Sachan", "abstract": "  We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.\n", "link": "http://arxiv.org/abs/2507.12379v1", "date": "2025-07-16", "relevancy": 1.9376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20for%20Arithmetic%20Errors%20in%20Language%20Models&body=Title%3A%20Probing%20for%20Arithmetic%20Errors%20in%20Language%20Models%0AAuthor%3A%20Yucheng%20Sun%20and%20Alessandro%20Stolfo%20and%20Mrinmaya%20Sachan%0AAbstract%3A%20%20%20We%20investigate%20whether%20internal%20activations%20in%20language%20models%20can%20be%20used%20to%0Adetect%20arithmetic%20errors.%20Starting%20with%20a%20controlled%20setting%20of%203-digit%0Aaddition%2C%20we%20show%20that%20simple%20probes%20can%20accurately%20decode%20both%20the%20model%27s%0Apredicted%20output%20and%20the%20correct%20answer%20from%20hidden%20states%2C%20regardless%20of%0Awhether%20the%20model%27s%20output%20is%20correct.%20Building%20on%20this%2C%20we%20train%20lightweight%0Aerror%20detectors%20that%20predict%20model%20correctness%20with%20over%2090%25%20accuracy.%20We%20then%0Aextend%20our%20analysis%20to%20structured%20chain-of-thought%20traces%20on%20addition-only%0AGSM8K%20problems%20and%20find%20that%20probes%20trained%20on%20simple%20arithmetic%20generalize%0Awell%20to%20this%20more%20complex%20setting%2C%20revealing%20consistent%20internal%0Arepresentations.%20Finally%2C%20we%20demonstrate%20that%20these%20probes%20can%20guide%20selective%0Are-prompting%20of%20erroneous%20reasoning%20steps%2C%20improving%20task%20accuracy%20with%20minimal%0Adisruption%20to%20correct%20outputs.%20Our%20findings%20suggest%20that%20arithmetic%20errors%20can%0Abe%20anticipated%20from%20internal%20activations%20alone%2C%20and%20that%20simple%20probes%20offer%20a%0Aviable%20path%20toward%20lightweight%20model%20self-correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520for%2520Arithmetic%2520Errors%2520in%2520Language%2520Models%26entry.906535625%3DYucheng%2520Sun%2520and%2520Alessandro%2520Stolfo%2520and%2520Mrinmaya%2520Sachan%26entry.1292438233%3D%2520%2520We%2520investigate%2520whether%2520internal%2520activations%2520in%2520language%2520models%2520can%2520be%2520used%2520to%250Adetect%2520arithmetic%2520errors.%2520Starting%2520with%2520a%2520controlled%2520setting%2520of%25203-digit%250Aaddition%252C%2520we%2520show%2520that%2520simple%2520probes%2520can%2520accurately%2520decode%2520both%2520the%2520model%2527s%250Apredicted%2520output%2520and%2520the%2520correct%2520answer%2520from%2520hidden%2520states%252C%2520regardless%2520of%250Awhether%2520the%2520model%2527s%2520output%2520is%2520correct.%2520Building%2520on%2520this%252C%2520we%2520train%2520lightweight%250Aerror%2520detectors%2520that%2520predict%2520model%2520correctness%2520with%2520over%252090%2525%2520accuracy.%2520We%2520then%250Aextend%2520our%2520analysis%2520to%2520structured%2520chain-of-thought%2520traces%2520on%2520addition-only%250AGSM8K%2520problems%2520and%2520find%2520that%2520probes%2520trained%2520on%2520simple%2520arithmetic%2520generalize%250Awell%2520to%2520this%2520more%2520complex%2520setting%252C%2520revealing%2520consistent%2520internal%250Arepresentations.%2520Finally%252C%2520we%2520demonstrate%2520that%2520these%2520probes%2520can%2520guide%2520selective%250Are-prompting%2520of%2520erroneous%2520reasoning%2520steps%252C%2520improving%2520task%2520accuracy%2520with%2520minimal%250Adisruption%2520to%2520correct%2520outputs.%2520Our%2520findings%2520suggest%2520that%2520arithmetic%2520errors%2520can%250Abe%2520anticipated%2520from%2520internal%2520activations%2520alone%252C%2520and%2520that%2520simple%2520probes%2520offer%2520a%250Aviable%2520path%2520toward%2520lightweight%2520model%2520self-correction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20for%20Arithmetic%20Errors%20in%20Language%20Models&entry.906535625=Yucheng%20Sun%20and%20Alessandro%20Stolfo%20and%20Mrinmaya%20Sachan&entry.1292438233=%20%20We%20investigate%20whether%20internal%20activations%20in%20language%20models%20can%20be%20used%20to%0Adetect%20arithmetic%20errors.%20Starting%20with%20a%20controlled%20setting%20of%203-digit%0Aaddition%2C%20we%20show%20that%20simple%20probes%20can%20accurately%20decode%20both%20the%20model%27s%0Apredicted%20output%20and%20the%20correct%20answer%20from%20hidden%20states%2C%20regardless%20of%0Awhether%20the%20model%27s%20output%20is%20correct.%20Building%20on%20this%2C%20we%20train%20lightweight%0Aerror%20detectors%20that%20predict%20model%20correctness%20with%20over%2090%25%20accuracy.%20We%20then%0Aextend%20our%20analysis%20to%20structured%20chain-of-thought%20traces%20on%20addition-only%0AGSM8K%20problems%20and%20find%20that%20probes%20trained%20on%20simple%20arithmetic%20generalize%0Awell%20to%20this%20more%20complex%20setting%2C%20revealing%20consistent%20internal%0Arepresentations.%20Finally%2C%20we%20demonstrate%20that%20these%20probes%20can%20guide%20selective%0Are-prompting%20of%20erroneous%20reasoning%20steps%2C%20improving%20task%20accuracy%20with%20minimal%0Adisruption%20to%20correct%20outputs.%20Our%20findings%20suggest%20that%20arithmetic%20errors%20can%0Abe%20anticipated%20from%20internal%20activations%20alone%2C%20and%20that%20simple%20probes%20offer%20a%0Aviable%20path%20toward%20lightweight%20model%20self-correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12379v1&entry.124074799=Read"},
{"title": "Large Language Models are Unreliable for Cyber Threat Intelligence", "author": "Emanuele Mezzi and Fabio Massacci and Katja Tuma", "abstract": "  Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.\n", "link": "http://arxiv.org/abs/2503.23175v2", "date": "2025-07-16", "relevancy": 1.9311, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20are%20Unreliable%20for%20Cyber%20Threat%20Intelligence&body=Title%3A%20Large%20Language%20Models%20are%20Unreliable%20for%20Cyber%20Threat%20Intelligence%0AAuthor%3A%20Emanuele%20Mezzi%20and%20Fabio%20Massacci%20and%20Katja%20Tuma%0AAbstract%3A%20%20%20Several%20recent%20works%20have%20argued%20that%20Large%20Language%20Models%20%28LLMs%29%20can%20be%0Aused%20to%20tame%20the%20data%20deluge%20in%20the%20cybersecurity%20field%2C%20by%20improving%20the%0Aautomation%20of%20Cyber%20Threat%20Intelligence%20%28CTI%29%20tasks.%20This%20work%20presents%20an%0Aevaluation%20methodology%20that%20other%20than%20allowing%20to%20test%20LLMs%20on%20CTI%20tasks%20when%0Ausing%20zero-shot%20learning%2C%20few-shot%20learning%20and%20fine-tuning%2C%20also%20allows%20to%0Aquantify%20their%20consistency%20and%20their%20confidence%20level.%20We%20run%20experiments%20with%0Athree%20state-of-the-art%20LLMs%20and%20a%20dataset%20of%20350%20threat%20intelligence%20reports%0Aand%20present%20new%20evidence%20of%20potential%20security%20risks%20in%20relying%20on%20LLMs%20for%0ACTI.%20We%20show%20how%20LLMs%20cannot%20guarantee%20sufficient%20performance%20on%20real-size%0Areports%20while%20also%20being%20inconsistent%20and%20overconfident.%20Few-shot%20learning%20and%0Afine-tuning%20only%20partially%20improve%20the%20results%2C%20thus%20posing%20doubts%20about%20the%0Apossibility%20of%20using%20LLMs%20for%20CTI%20scenarios%2C%20where%20labelled%20datasets%20are%0Alacking%20and%20where%20confidence%20is%20a%20fundamental%20factor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520are%2520Unreliable%2520for%2520Cyber%2520Threat%2520Intelligence%26entry.906535625%3DEmanuele%2520Mezzi%2520and%2520Fabio%2520Massacci%2520and%2520Katja%2520Tuma%26entry.1292438233%3D%2520%2520Several%2520recent%2520works%2520have%2520argued%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520be%250Aused%2520to%2520tame%2520the%2520data%2520deluge%2520in%2520the%2520cybersecurity%2520field%252C%2520by%2520improving%2520the%250Aautomation%2520of%2520Cyber%2520Threat%2520Intelligence%2520%2528CTI%2529%2520tasks.%2520This%2520work%2520presents%2520an%250Aevaluation%2520methodology%2520that%2520other%2520than%2520allowing%2520to%2520test%2520LLMs%2520on%2520CTI%2520tasks%2520when%250Ausing%2520zero-shot%2520learning%252C%2520few-shot%2520learning%2520and%2520fine-tuning%252C%2520also%2520allows%2520to%250Aquantify%2520their%2520consistency%2520and%2520their%2520confidence%2520level.%2520We%2520run%2520experiments%2520with%250Athree%2520state-of-the-art%2520LLMs%2520and%2520a%2520dataset%2520of%2520350%2520threat%2520intelligence%2520reports%250Aand%2520present%2520new%2520evidence%2520of%2520potential%2520security%2520risks%2520in%2520relying%2520on%2520LLMs%2520for%250ACTI.%2520We%2520show%2520how%2520LLMs%2520cannot%2520guarantee%2520sufficient%2520performance%2520on%2520real-size%250Areports%2520while%2520also%2520being%2520inconsistent%2520and%2520overconfident.%2520Few-shot%2520learning%2520and%250Afine-tuning%2520only%2520partially%2520improve%2520the%2520results%252C%2520thus%2520posing%2520doubts%2520about%2520the%250Apossibility%2520of%2520using%2520LLMs%2520for%2520CTI%2520scenarios%252C%2520where%2520labelled%2520datasets%2520are%250Alacking%2520and%2520where%2520confidence%2520is%2520a%2520fundamental%2520factor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20are%20Unreliable%20for%20Cyber%20Threat%20Intelligence&entry.906535625=Emanuele%20Mezzi%20and%20Fabio%20Massacci%20and%20Katja%20Tuma&entry.1292438233=%20%20Several%20recent%20works%20have%20argued%20that%20Large%20Language%20Models%20%28LLMs%29%20can%20be%0Aused%20to%20tame%20the%20data%20deluge%20in%20the%20cybersecurity%20field%2C%20by%20improving%20the%0Aautomation%20of%20Cyber%20Threat%20Intelligence%20%28CTI%29%20tasks.%20This%20work%20presents%20an%0Aevaluation%20methodology%20that%20other%20than%20allowing%20to%20test%20LLMs%20on%20CTI%20tasks%20when%0Ausing%20zero-shot%20learning%2C%20few-shot%20learning%20and%20fine-tuning%2C%20also%20allows%20to%0Aquantify%20their%20consistency%20and%20their%20confidence%20level.%20We%20run%20experiments%20with%0Athree%20state-of-the-art%20LLMs%20and%20a%20dataset%20of%20350%20threat%20intelligence%20reports%0Aand%20present%20new%20evidence%20of%20potential%20security%20risks%20in%20relying%20on%20LLMs%20for%0ACTI.%20We%20show%20how%20LLMs%20cannot%20guarantee%20sufficient%20performance%20on%20real-size%0Areports%20while%20also%20being%20inconsistent%20and%20overconfident.%20Few-shot%20learning%20and%0Afine-tuning%20only%20partially%20improve%20the%20results%2C%20thus%20posing%20doubts%20about%20the%0Apossibility%20of%20using%20LLMs%20for%20CTI%20scenarios%2C%20where%20labelled%20datasets%20are%0Alacking%20and%20where%20confidence%20is%20a%20fundamental%20factor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23175v2&entry.124074799=Read"},
{"title": "Towards Understanding Link Predictor Generalizability Under Distribution\n  Shifts", "author": "Jay Revolinsky and Harry Shomer and Jiliang Tang", "abstract": "  State-of-the-art link prediction (LP) models demonstrate impressive benchmark\nresults. However, popular benchmark datasets often assume that training,\nvalidation, and testing samples are representative of the overall dataset\ndistribution. In real-world situations, this assumption is often incorrect;\nuncontrolled factors lead new dataset samples to come from a different\ndistribution than training samples. Additionally, the majority of recent work\nwith graph dataset shift focuses on node- and graph-level tasks, largely\nignoring link-level tasks. To bridge this gap, we introduce a novel splitting\nstrategy, known as LPShift, which utilizes structural properties to induce a\ncontrolled distribution shift. We verify LPShift's effect through empirical\nevaluation of SOTA LP models on 16 LPShift variants of original dataset splits,\nwith results indicating drastic changes to model performance. Additional\nexperiments demonstrate graph structure has a strong influence on the success\nof current generalization methods. Source Code Available Here:\nhttps://github.com/revolins/LPShift\n", "link": "http://arxiv.org/abs/2406.08788v3", "date": "2025-07-16", "relevancy": 1.919, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4807}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Link%20Predictor%20Generalizability%20Under%20Distribution%0A%20%20Shifts&body=Title%3A%20Towards%20Understanding%20Link%20Predictor%20Generalizability%20Under%20Distribution%0A%20%20Shifts%0AAuthor%3A%20Jay%20Revolinsky%20and%20Harry%20Shomer%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20State-of-the-art%20link%20prediction%20%28LP%29%20models%20demonstrate%20impressive%20benchmark%0Aresults.%20However%2C%20popular%20benchmark%20datasets%20often%20assume%20that%20training%2C%0Avalidation%2C%20and%20testing%20samples%20are%20representative%20of%20the%20overall%20dataset%0Adistribution.%20In%20real-world%20situations%2C%20this%20assumption%20is%20often%20incorrect%3B%0Auncontrolled%20factors%20lead%20new%20dataset%20samples%20to%20come%20from%20a%20different%0Adistribution%20than%20training%20samples.%20Additionally%2C%20the%20majority%20of%20recent%20work%0Awith%20graph%20dataset%20shift%20focuses%20on%20node-%20and%20graph-level%20tasks%2C%20largely%0Aignoring%20link-level%20tasks.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20splitting%0Astrategy%2C%20known%20as%20LPShift%2C%20which%20utilizes%20structural%20properties%20to%20induce%20a%0Acontrolled%20distribution%20shift.%20We%20verify%20LPShift%27s%20effect%20through%20empirical%0Aevaluation%20of%20SOTA%20LP%20models%20on%2016%20LPShift%20variants%20of%20original%20dataset%20splits%2C%0Awith%20results%20indicating%20drastic%20changes%20to%20model%20performance.%20Additional%0Aexperiments%20demonstrate%20graph%20structure%20has%20a%20strong%20influence%20on%20the%20success%0Aof%20current%20generalization%20methods.%20Source%20Code%20Available%20Here%3A%0Ahttps%3A//github.com/revolins/LPShift%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08788v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Link%2520Predictor%2520Generalizability%2520Under%2520Distribution%250A%2520%2520Shifts%26entry.906535625%3DJay%2520Revolinsky%2520and%2520Harry%2520Shomer%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520State-of-the-art%2520link%2520prediction%2520%2528LP%2529%2520models%2520demonstrate%2520impressive%2520benchmark%250Aresults.%2520However%252C%2520popular%2520benchmark%2520datasets%2520often%2520assume%2520that%2520training%252C%250Avalidation%252C%2520and%2520testing%2520samples%2520are%2520representative%2520of%2520the%2520overall%2520dataset%250Adistribution.%2520In%2520real-world%2520situations%252C%2520this%2520assumption%2520is%2520often%2520incorrect%253B%250Auncontrolled%2520factors%2520lead%2520new%2520dataset%2520samples%2520to%2520come%2520from%2520a%2520different%250Adistribution%2520than%2520training%2520samples.%2520Additionally%252C%2520the%2520majority%2520of%2520recent%2520work%250Awith%2520graph%2520dataset%2520shift%2520focuses%2520on%2520node-%2520and%2520graph-level%2520tasks%252C%2520largely%250Aignoring%2520link-level%2520tasks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520splitting%250Astrategy%252C%2520known%2520as%2520LPShift%252C%2520which%2520utilizes%2520structural%2520properties%2520to%2520induce%2520a%250Acontrolled%2520distribution%2520shift.%2520We%2520verify%2520LPShift%2527s%2520effect%2520through%2520empirical%250Aevaluation%2520of%2520SOTA%2520LP%2520models%2520on%252016%2520LPShift%2520variants%2520of%2520original%2520dataset%2520splits%252C%250Awith%2520results%2520indicating%2520drastic%2520changes%2520to%2520model%2520performance.%2520Additional%250Aexperiments%2520demonstrate%2520graph%2520structure%2520has%2520a%2520strong%2520influence%2520on%2520the%2520success%250Aof%2520current%2520generalization%2520methods.%2520Source%2520Code%2520Available%2520Here%253A%250Ahttps%253A//github.com/revolins/LPShift%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08788v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Link%20Predictor%20Generalizability%20Under%20Distribution%0A%20%20Shifts&entry.906535625=Jay%20Revolinsky%20and%20Harry%20Shomer%20and%20Jiliang%20Tang&entry.1292438233=%20%20State-of-the-art%20link%20prediction%20%28LP%29%20models%20demonstrate%20impressive%20benchmark%0Aresults.%20However%2C%20popular%20benchmark%20datasets%20often%20assume%20that%20training%2C%0Avalidation%2C%20and%20testing%20samples%20are%20representative%20of%20the%20overall%20dataset%0Adistribution.%20In%20real-world%20situations%2C%20this%20assumption%20is%20often%20incorrect%3B%0Auncontrolled%20factors%20lead%20new%20dataset%20samples%20to%20come%20from%20a%20different%0Adistribution%20than%20training%20samples.%20Additionally%2C%20the%20majority%20of%20recent%20work%0Awith%20graph%20dataset%20shift%20focuses%20on%20node-%20and%20graph-level%20tasks%2C%20largely%0Aignoring%20link-level%20tasks.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20splitting%0Astrategy%2C%20known%20as%20LPShift%2C%20which%20utilizes%20structural%20properties%20to%20induce%20a%0Acontrolled%20distribution%20shift.%20We%20verify%20LPShift%27s%20effect%20through%20empirical%0Aevaluation%20of%20SOTA%20LP%20models%20on%2016%20LPShift%20variants%20of%20original%20dataset%20splits%2C%0Awith%20results%20indicating%20drastic%20changes%20to%20model%20performance.%20Additional%0Aexperiments%20demonstrate%20graph%20structure%20has%20a%20strong%20influence%20on%20the%20success%0Aof%20current%20generalization%20methods.%20Source%20Code%20Available%20Here%3A%0Ahttps%3A//github.com/revolins/LPShift%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08788v3&entry.124074799=Read"},
{"title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple,\n  General, and Scalable Solution", "author": "Jonas G\u00f6sgens and Niklas Jansen and Hector Geffner", "abstract": "  Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.\n", "link": "http://arxiv.org/abs/2411.14995v3", "date": "2025-07-16", "relevancy": 1.9175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&body=Title%3A%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution%0AAuthor%3A%20Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner%0AAbstract%3A%20%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14995v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Lifted%2520STRIPS%2520Models%2520from%2520Action%2520Traces%2520Alone%253A%2520A%2520Simple%252C%250A%2520%2520General%252C%2520and%2520Scalable%2520Solution%26entry.906535625%3DJonas%2520G%25C3%25B6sgens%2520and%2520Niklas%2520Jansen%2520and%2520Hector%2520Geffner%26entry.1292438233%3D%2520%2520Learning%2520STRIPS%2520action%2520models%2520from%2520action%2520traces%2520alone%2520is%2520a%2520challenging%250Aproblem%2520as%2520it%2520involves%2520learning%2520the%2520domain%2520predicates%2520as%2520well.%2520In%2520this%2520work%252C%2520a%250Anovel%2520approach%2520is%2520introduced%2520which%252C%2520like%2520the%2520well-known%2520LOCM%2520systems%252C%2520is%250Ascalable%252C%2520but%2520like%2520SAT%2520approaches%252C%2520is%2520sound%2520and%2520complete.%2520Furthermore%252C%2520the%250Aapproach%2520is%2520general%2520and%2520imposes%2520no%2520restrictions%2520on%2520the%2520hidden%2520domain%2520or%2520the%250Anumber%2520or%2520arity%2520of%2520the%2520predicates.%2520The%2520new%2520learning%2520method%2520is%2520based%2520on%2520an%250A%255Cemph%257Befficient%252C%2520novel%2520test%257D%2520that%2520checks%2520whether%2520the%2520assumption%2520that%2520a%250Apredicate%2520is%2520affected%2520by%2520a%2520set%2520of%2520action%2520patterns%252C%2520namely%252C%2520actions%2520with%250Aspecific%2520argument%2520positions%252C%2520is%2520consistent%2520with%2520the%2520traces.%2520The%2520predicates%2520and%250Aaction%2520patterns%2520that%2520pass%2520the%2520test%2520provide%2520the%2520basis%2520for%2520the%2520learned%2520domain%250Athat%2520is%2520then%2520easily%2520completed%2520with%2520preconditions%2520and%2520static%2520predicates.%2520The%2520new%250Amethod%2520is%2520studied%2520theoretically%2520and%2520experimentally.%2520For%2520the%2520latter%252C%2520the%2520method%250Ais%2520evaluated%2520on%2520traces%2520and%2520graphs%2520obtained%2520from%2520standard%2520classical%2520domains%2520like%250Athe%25208-puzzle%252C%2520which%2520involve%2520hundreds%2520of%2520thousands%2520of%2520states%2520and%2520transitions.%250AThe%2520learned%2520representations%2520are%2520then%2520verified%2520on%2520larger%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14995v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&entry.906535625=Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner&entry.1292438233=%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14995v3&entry.124074799=Read"},
{"title": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "author": "Wei Du and Branislav Kisacanin and George Armstrong and Shubham Toshniwal and Ivan Moshkov and Alexan Ayrapetyan and Sadegh Mahdavi and Dan Zhao and Shizhe Diao and Dragan Masulovic and Marius Stanean and Advaith Avadhanam and Max Wang and Ashmit Dutta and Shitij Govil and Sri Yanamandara and Mihir Tandon and Sriram Ananthakrishnan and Vedant Rathi and David Zhang and Joonseok Kang and Leon Luo and Titu Andreescu and Boris Ginsburg and Igor Gitman", "abstract": "  Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.\n", "link": "http://arxiv.org/abs/2507.09850v3", "date": "2025-07-16", "relevancy": 1.9159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Challenge%20of%20Teaching%20Reasoning%20to%20LLMs%20Without%20RL%20or%20Distillation&body=Title%3A%20The%20Challenge%20of%20Teaching%20Reasoning%20to%20LLMs%20Without%20RL%20or%20Distillation%0AAuthor%3A%20Wei%20Du%20and%20Branislav%20Kisacanin%20and%20George%20Armstrong%20and%20Shubham%20Toshniwal%20and%20Ivan%20Moshkov%20and%20Alexan%20Ayrapetyan%20and%20Sadegh%20Mahdavi%20and%20Dan%20Zhao%20and%20Shizhe%20Diao%20and%20Dragan%20Masulovic%20and%20Marius%20Stanean%20and%20Advaith%20Avadhanam%20and%20Max%20Wang%20and%20Ashmit%20Dutta%20and%20Shitij%20Govil%20and%20Sri%20Yanamandara%20and%20Mihir%20Tandon%20and%20Sriram%20Ananthakrishnan%20and%20Vedant%20Rathi%20and%20David%20Zhang%20and%20Joonseok%20Kang%20and%20Leon%20Luo%20and%20Titu%20Andreescu%20and%20Boris%20Ginsburg%20and%20Igor%20Gitman%0AAbstract%3A%20%20%20Reasoning-capable%20language%20models%20achieve%20state-of-the-art%20performance%20in%0Adiverse%20complex%20tasks%20by%20generating%20long%2C%20explicit%20Chain-of-Thought%20%28CoT%29%0Atraces.%20While%20recent%20works%20show%20that%20base%20models%20can%20acquire%20such%20reasoning%0Atraces%20via%20reinforcement%20learning%20or%20distillation%20from%20stronger%20models%20like%0ADeepSeek-R1%2C%20previous%20works%20demonstrate%20that%20even%20short%20CoT%20prompting%20without%0Afine-tuning%20is%20able%20to%20improve%20reasoning.%20We%20ask%20whether%20long%20CoT%20can%20be%0Ainduced%20in%20a%20base%20model%20using%20only%20prompting%20or%20minimal%20tuning.%20Using%20just%2020%0Along%20CoT%20examples%20from%20the%20reasoning%20model%20%5Ctexttt%7BQwQ-32B-Preview%7D%2C%20we%20lightly%0Afine-tune%20the%20base%20model%20%5Ctexttt%7BQwen2.5-32B%7D.%20The%20resulting%20model%20outperforms%0Athe%20much%20larger%20%5Ctexttt%7BQwen2.5-Math-72B-Instruct%7D%2C%20showing%20that%20a%20handful%20of%0Ahigh-quality%20examples%20can%20unlock%20strong%20reasoning%20capabilities.%20We%20further%0Aexplore%20using%20CoT%20data%20from%20non-reasoning%20models%20and%20human%20annotators%2C%20enhanced%0Awith%20prompt%20engineering%2C%20multi-pass%20editing%2C%20and%20structural%20guidance.%20However%2C%0Aneither%20matches%20the%20performance%20of%20reasoning%20model%20traces%2C%20suggesting%20that%0Acertain%20latent%20qualities%20of%20expert%20CoT%20are%20difficult%20to%20replicate.%20We%20analyze%0Akey%20properties%20of%20reasoning%20data%2C%20such%20as%20problem%20difficulty%2C%20diversity%2C%20and%0Aanswer%20length%2C%20that%20influence%20reasoning%20distillation.%20While%20challenges%20remain%2C%0Awe%20are%20optimistic%20that%20carefully%20curated%20human-written%20CoT%2C%20even%20in%20small%0Aquantities%2C%20can%20activate%20reasoning%20behaviors%20in%20base%20models.%20We%20release%20our%0Ahuman-authored%20dataset%20across%20refinement%20stages%20and%20invite%20further%0Ainvestigation%20into%20what%20makes%20small-scale%20reasoning%20supervision%20so%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09850v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Challenge%2520of%2520Teaching%2520Reasoning%2520to%2520LLMs%2520Without%2520RL%2520or%2520Distillation%26entry.906535625%3DWei%2520Du%2520and%2520Branislav%2520Kisacanin%2520and%2520George%2520Armstrong%2520and%2520Shubham%2520Toshniwal%2520and%2520Ivan%2520Moshkov%2520and%2520Alexan%2520Ayrapetyan%2520and%2520Sadegh%2520Mahdavi%2520and%2520Dan%2520Zhao%2520and%2520Shizhe%2520Diao%2520and%2520Dragan%2520Masulovic%2520and%2520Marius%2520Stanean%2520and%2520Advaith%2520Avadhanam%2520and%2520Max%2520Wang%2520and%2520Ashmit%2520Dutta%2520and%2520Shitij%2520Govil%2520and%2520Sri%2520Yanamandara%2520and%2520Mihir%2520Tandon%2520and%2520Sriram%2520Ananthakrishnan%2520and%2520Vedant%2520Rathi%2520and%2520David%2520Zhang%2520and%2520Joonseok%2520Kang%2520and%2520Leon%2520Luo%2520and%2520Titu%2520Andreescu%2520and%2520Boris%2520Ginsburg%2520and%2520Igor%2520Gitman%26entry.1292438233%3D%2520%2520Reasoning-capable%2520language%2520models%2520achieve%2520state-of-the-art%2520performance%2520in%250Adiverse%2520complex%2520tasks%2520by%2520generating%2520long%252C%2520explicit%2520Chain-of-Thought%2520%2528CoT%2529%250Atraces.%2520While%2520recent%2520works%2520show%2520that%2520base%2520models%2520can%2520acquire%2520such%2520reasoning%250Atraces%2520via%2520reinforcement%2520learning%2520or%2520distillation%2520from%2520stronger%2520models%2520like%250ADeepSeek-R1%252C%2520previous%2520works%2520demonstrate%2520that%2520even%2520short%2520CoT%2520prompting%2520without%250Afine-tuning%2520is%2520able%2520to%2520improve%2520reasoning.%2520We%2520ask%2520whether%2520long%2520CoT%2520can%2520be%250Ainduced%2520in%2520a%2520base%2520model%2520using%2520only%2520prompting%2520or%2520minimal%2520tuning.%2520Using%2520just%252020%250Along%2520CoT%2520examples%2520from%2520the%2520reasoning%2520model%2520%255Ctexttt%257BQwQ-32B-Preview%257D%252C%2520we%2520lightly%250Afine-tune%2520the%2520base%2520model%2520%255Ctexttt%257BQwen2.5-32B%257D.%2520The%2520resulting%2520model%2520outperforms%250Athe%2520much%2520larger%2520%255Ctexttt%257BQwen2.5-Math-72B-Instruct%257D%252C%2520showing%2520that%2520a%2520handful%2520of%250Ahigh-quality%2520examples%2520can%2520unlock%2520strong%2520reasoning%2520capabilities.%2520We%2520further%250Aexplore%2520using%2520CoT%2520data%2520from%2520non-reasoning%2520models%2520and%2520human%2520annotators%252C%2520enhanced%250Awith%2520prompt%2520engineering%252C%2520multi-pass%2520editing%252C%2520and%2520structural%2520guidance.%2520However%252C%250Aneither%2520matches%2520the%2520performance%2520of%2520reasoning%2520model%2520traces%252C%2520suggesting%2520that%250Acertain%2520latent%2520qualities%2520of%2520expert%2520CoT%2520are%2520difficult%2520to%2520replicate.%2520We%2520analyze%250Akey%2520properties%2520of%2520reasoning%2520data%252C%2520such%2520as%2520problem%2520difficulty%252C%2520diversity%252C%2520and%250Aanswer%2520length%252C%2520that%2520influence%2520reasoning%2520distillation.%2520While%2520challenges%2520remain%252C%250Awe%2520are%2520optimistic%2520that%2520carefully%2520curated%2520human-written%2520CoT%252C%2520even%2520in%2520small%250Aquantities%252C%2520can%2520activate%2520reasoning%2520behaviors%2520in%2520base%2520models.%2520We%2520release%2520our%250Ahuman-authored%2520dataset%2520across%2520refinement%2520stages%2520and%2520invite%2520further%250Ainvestigation%2520into%2520what%2520makes%2520small-scale%2520reasoning%2520supervision%2520so%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09850v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Challenge%20of%20Teaching%20Reasoning%20to%20LLMs%20Without%20RL%20or%20Distillation&entry.906535625=Wei%20Du%20and%20Branislav%20Kisacanin%20and%20George%20Armstrong%20and%20Shubham%20Toshniwal%20and%20Ivan%20Moshkov%20and%20Alexan%20Ayrapetyan%20and%20Sadegh%20Mahdavi%20and%20Dan%20Zhao%20and%20Shizhe%20Diao%20and%20Dragan%20Masulovic%20and%20Marius%20Stanean%20and%20Advaith%20Avadhanam%20and%20Max%20Wang%20and%20Ashmit%20Dutta%20and%20Shitij%20Govil%20and%20Sri%20Yanamandara%20and%20Mihir%20Tandon%20and%20Sriram%20Ananthakrishnan%20and%20Vedant%20Rathi%20and%20David%20Zhang%20and%20Joonseok%20Kang%20and%20Leon%20Luo%20and%20Titu%20Andreescu%20and%20Boris%20Ginsburg%20and%20Igor%20Gitman&entry.1292438233=%20%20Reasoning-capable%20language%20models%20achieve%20state-of-the-art%20performance%20in%0Adiverse%20complex%20tasks%20by%20generating%20long%2C%20explicit%20Chain-of-Thought%20%28CoT%29%0Atraces.%20While%20recent%20works%20show%20that%20base%20models%20can%20acquire%20such%20reasoning%0Atraces%20via%20reinforcement%20learning%20or%20distillation%20from%20stronger%20models%20like%0ADeepSeek-R1%2C%20previous%20works%20demonstrate%20that%20even%20short%20CoT%20prompting%20without%0Afine-tuning%20is%20able%20to%20improve%20reasoning.%20We%20ask%20whether%20long%20CoT%20can%20be%0Ainduced%20in%20a%20base%20model%20using%20only%20prompting%20or%20minimal%20tuning.%20Using%20just%2020%0Along%20CoT%20examples%20from%20the%20reasoning%20model%20%5Ctexttt%7BQwQ-32B-Preview%7D%2C%20we%20lightly%0Afine-tune%20the%20base%20model%20%5Ctexttt%7BQwen2.5-32B%7D.%20The%20resulting%20model%20outperforms%0Athe%20much%20larger%20%5Ctexttt%7BQwen2.5-Math-72B-Instruct%7D%2C%20showing%20that%20a%20handful%20of%0Ahigh-quality%20examples%20can%20unlock%20strong%20reasoning%20capabilities.%20We%20further%0Aexplore%20using%20CoT%20data%20from%20non-reasoning%20models%20and%20human%20annotators%2C%20enhanced%0Awith%20prompt%20engineering%2C%20multi-pass%20editing%2C%20and%20structural%20guidance.%20However%2C%0Aneither%20matches%20the%20performance%20of%20reasoning%20model%20traces%2C%20suggesting%20that%0Acertain%20latent%20qualities%20of%20expert%20CoT%20are%20difficult%20to%20replicate.%20We%20analyze%0Akey%20properties%20of%20reasoning%20data%2C%20such%20as%20problem%20difficulty%2C%20diversity%2C%20and%0Aanswer%20length%2C%20that%20influence%20reasoning%20distillation.%20While%20challenges%20remain%2C%0Awe%20are%20optimistic%20that%20carefully%20curated%20human-written%20CoT%2C%20even%20in%20small%0Aquantities%2C%20can%20activate%20reasoning%20behaviors%20in%20base%20models.%20We%20release%20our%0Ahuman-authored%20dataset%20across%20refinement%20stages%20and%20invite%20further%0Ainvestigation%20into%20what%20makes%20small-scale%20reasoning%20supervision%20so%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09850v3&entry.124074799=Read"},
{"title": "Neural Network-Guided Symbolic Regression for Interpretable Descriptor\n  Discovery in Perovskite Catalysts", "author": "Yeming Xian and Xiaoming Wang and Yanfa Yan", "abstract": "  Understanding and predicting the activity of oxide perovskite catalysts for\nthe oxygen evolution reaction (OER) requires descriptors that are both accurate\nand physically interpretable. While symbolic regression (SR) offers a path to\ndiscover such formulas, its performance degrades with high-dimensional inputs\nand small datasets. We present a two-phase framework that combines neural\nnetworks (NN), feature importance analysis, and symbolic regression (SR) to\ndiscover interpretable descriptors for OER activity in oxide perovskites. In\nPhase I, using a small dataset and seven structural features, we reproduce and\nimprove the known {\\mu}/t descriptor by engineering composite features and\napplying symbolic regression, achieving training and validation MAEs of 22.8\nand 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce\ndimensionality, and identify LUMO energy as a key electronic descriptor. A\nfinal formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved\naccuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong\nphysical interpretability. Our results demonstrate that NN-guided symbolic\nregression enables accurate, interpretable, and physically meaningful\ndescriptor discovery in data-scarce regimes, indicating interpretability need\nnot sacrifice accuracy for materials informatics.\n", "link": "http://arxiv.org/abs/2507.12404v1", "date": "2025-07-16", "relevancy": 1.9101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network-Guided%20Symbolic%20Regression%20for%20Interpretable%20Descriptor%0A%20%20Discovery%20in%20Perovskite%20Catalysts&body=Title%3A%20Neural%20Network-Guided%20Symbolic%20Regression%20for%20Interpretable%20Descriptor%0A%20%20Discovery%20in%20Perovskite%20Catalysts%0AAuthor%3A%20Yeming%20Xian%20and%20Xiaoming%20Wang%20and%20Yanfa%20Yan%0AAbstract%3A%20%20%20Understanding%20and%20predicting%20the%20activity%20of%20oxide%20perovskite%20catalysts%20for%0Athe%20oxygen%20evolution%20reaction%20%28OER%29%20requires%20descriptors%20that%20are%20both%20accurate%0Aand%20physically%20interpretable.%20While%20symbolic%20regression%20%28SR%29%20offers%20a%20path%20to%0Adiscover%20such%20formulas%2C%20its%20performance%20degrades%20with%20high-dimensional%20inputs%0Aand%20small%20datasets.%20We%20present%20a%20two-phase%20framework%20that%20combines%20neural%0Anetworks%20%28NN%29%2C%20feature%20importance%20analysis%2C%20and%20symbolic%20regression%20%28SR%29%20to%0Adiscover%20interpretable%20descriptors%20for%20OER%20activity%20in%20oxide%20perovskites.%20In%0APhase%20I%2C%20using%20a%20small%20dataset%20and%20seven%20structural%20features%2C%20we%20reproduce%20and%0Aimprove%20the%20known%20%7B%5Cmu%7D/t%20descriptor%20by%20engineering%20composite%20features%20and%0Aapplying%20symbolic%20regression%2C%20achieving%20training%20and%20validation%20MAEs%20of%2022.8%0Aand%2020.8%20meV%2C%20respectively.%20In%20Phase%20II%2C%20we%20expand%20to%20164%20features%2C%20reduce%0Adimensionality%2C%20and%20identify%20LUMO%20energy%20as%20a%20key%20electronic%20descriptor.%20A%0Afinal%20formula%20using%20%7B%5Cmu%7D/t%2C%20%7B%5Cmu%7D/RA%2C%20and%20LUMO%20energy%20achieves%20improved%0Aaccuracy%20%28training%20and%20validation%20MAEs%20of%2022.1%20and%2020.6%20meV%29%20with%20strong%0Aphysical%20interpretability.%20Our%20results%20demonstrate%20that%20NN-guided%20symbolic%0Aregression%20enables%20accurate%2C%20interpretable%2C%20and%20physically%20meaningful%0Adescriptor%20discovery%20in%20data-scarce%20regimes%2C%20indicating%20interpretability%20need%0Anot%20sacrifice%20accuracy%20for%20materials%20informatics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network-Guided%2520Symbolic%2520Regression%2520for%2520Interpretable%2520Descriptor%250A%2520%2520Discovery%2520in%2520Perovskite%2520Catalysts%26entry.906535625%3DYeming%2520Xian%2520and%2520Xiaoming%2520Wang%2520and%2520Yanfa%2520Yan%26entry.1292438233%3D%2520%2520Understanding%2520and%2520predicting%2520the%2520activity%2520of%2520oxide%2520perovskite%2520catalysts%2520for%250Athe%2520oxygen%2520evolution%2520reaction%2520%2528OER%2529%2520requires%2520descriptors%2520that%2520are%2520both%2520accurate%250Aand%2520physically%2520interpretable.%2520While%2520symbolic%2520regression%2520%2528SR%2529%2520offers%2520a%2520path%2520to%250Adiscover%2520such%2520formulas%252C%2520its%2520performance%2520degrades%2520with%2520high-dimensional%2520inputs%250Aand%2520small%2520datasets.%2520We%2520present%2520a%2520two-phase%2520framework%2520that%2520combines%2520neural%250Anetworks%2520%2528NN%2529%252C%2520feature%2520importance%2520analysis%252C%2520and%2520symbolic%2520regression%2520%2528SR%2529%2520to%250Adiscover%2520interpretable%2520descriptors%2520for%2520OER%2520activity%2520in%2520oxide%2520perovskites.%2520In%250APhase%2520I%252C%2520using%2520a%2520small%2520dataset%2520and%2520seven%2520structural%2520features%252C%2520we%2520reproduce%2520and%250Aimprove%2520the%2520known%2520%257B%255Cmu%257D/t%2520descriptor%2520by%2520engineering%2520composite%2520features%2520and%250Aapplying%2520symbolic%2520regression%252C%2520achieving%2520training%2520and%2520validation%2520MAEs%2520of%252022.8%250Aand%252020.8%2520meV%252C%2520respectively.%2520In%2520Phase%2520II%252C%2520we%2520expand%2520to%2520164%2520features%252C%2520reduce%250Adimensionality%252C%2520and%2520identify%2520LUMO%2520energy%2520as%2520a%2520key%2520electronic%2520descriptor.%2520A%250Afinal%2520formula%2520using%2520%257B%255Cmu%257D/t%252C%2520%257B%255Cmu%257D/RA%252C%2520and%2520LUMO%2520energy%2520achieves%2520improved%250Aaccuracy%2520%2528training%2520and%2520validation%2520MAEs%2520of%252022.1%2520and%252020.6%2520meV%2529%2520with%2520strong%250Aphysical%2520interpretability.%2520Our%2520results%2520demonstrate%2520that%2520NN-guided%2520symbolic%250Aregression%2520enables%2520accurate%252C%2520interpretable%252C%2520and%2520physically%2520meaningful%250Adescriptor%2520discovery%2520in%2520data-scarce%2520regimes%252C%2520indicating%2520interpretability%2520need%250Anot%2520sacrifice%2520accuracy%2520for%2520materials%2520informatics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network-Guided%20Symbolic%20Regression%20for%20Interpretable%20Descriptor%0A%20%20Discovery%20in%20Perovskite%20Catalysts&entry.906535625=Yeming%20Xian%20and%20Xiaoming%20Wang%20and%20Yanfa%20Yan&entry.1292438233=%20%20Understanding%20and%20predicting%20the%20activity%20of%20oxide%20perovskite%20catalysts%20for%0Athe%20oxygen%20evolution%20reaction%20%28OER%29%20requires%20descriptors%20that%20are%20both%20accurate%0Aand%20physically%20interpretable.%20While%20symbolic%20regression%20%28SR%29%20offers%20a%20path%20to%0Adiscover%20such%20formulas%2C%20its%20performance%20degrades%20with%20high-dimensional%20inputs%0Aand%20small%20datasets.%20We%20present%20a%20two-phase%20framework%20that%20combines%20neural%0Anetworks%20%28NN%29%2C%20feature%20importance%20analysis%2C%20and%20symbolic%20regression%20%28SR%29%20to%0Adiscover%20interpretable%20descriptors%20for%20OER%20activity%20in%20oxide%20perovskites.%20In%0APhase%20I%2C%20using%20a%20small%20dataset%20and%20seven%20structural%20features%2C%20we%20reproduce%20and%0Aimprove%20the%20known%20%7B%5Cmu%7D/t%20descriptor%20by%20engineering%20composite%20features%20and%0Aapplying%20symbolic%20regression%2C%20achieving%20training%20and%20validation%20MAEs%20of%2022.8%0Aand%2020.8%20meV%2C%20respectively.%20In%20Phase%20II%2C%20we%20expand%20to%20164%20features%2C%20reduce%0Adimensionality%2C%20and%20identify%20LUMO%20energy%20as%20a%20key%20electronic%20descriptor.%20A%0Afinal%20formula%20using%20%7B%5Cmu%7D/t%2C%20%7B%5Cmu%7D/RA%2C%20and%20LUMO%20energy%20achieves%20improved%0Aaccuracy%20%28training%20and%20validation%20MAEs%20of%2022.1%20and%2020.6%20meV%29%20with%20strong%0Aphysical%20interpretability.%20Our%20results%20demonstrate%20that%20NN-guided%20symbolic%0Aregression%20enables%20accurate%2C%20interpretable%2C%20and%20physically%20meaningful%0Adescriptor%20discovery%20in%20data-scarce%20regimes%2C%20indicating%20interpretability%20need%0Anot%20sacrifice%20accuracy%20for%20materials%20informatics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12404v1&entry.124074799=Read"},
{"title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities", "author": "Diganta Misra and Nizar Islah and Victor May and Brice Rauby and Zihan Wang and Justine Gehring and Antonio Orvieto and Muawiz Chaudhary and Eilif B. Muller and Irina Rish and Samira Ebrahimi Kahou and Massimo Caccia", "abstract": "  The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.\n", "link": "http://arxiv.org/abs/2507.12367v1", "date": "2025-07-16", "relevancy": 1.8682, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4753}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4612}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GitChameleon%3A%20Evaluating%20AI%20Code%20Generation%20Against%20Python%20Library%0A%20%20Version%20Incompatibilities&body=Title%3A%20GitChameleon%3A%20Evaluating%20AI%20Code%20Generation%20Against%20Python%20Library%0A%20%20Version%20Incompatibilities%0AAuthor%3A%20Diganta%20Misra%20and%20Nizar%20Islah%20and%20Victor%20May%20and%20Brice%20Rauby%20and%20Zihan%20Wang%20and%20Justine%20Gehring%20and%20Antonio%20Orvieto%20and%20Muawiz%20Chaudhary%20and%20Eilif%20B.%20Muller%20and%20Irina%20Rish%20and%20Samira%20Ebrahimi%20Kahou%20and%20Massimo%20Caccia%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20software%20libraries%20poses%20a%20considerable%20hurdle%20for%0Acode%20generation%2C%20necessitating%20continuous%20adaptation%20to%20frequent%20version%0Aupdates%20while%20preserving%20backward%20compatibility.%20While%20existing%20code%20evolution%0Abenchmarks%20provide%20valuable%20insights%2C%20they%20typically%20lack%20execution-based%0Aevaluation%20for%20generating%20code%20compliant%20with%20specific%20library%20versions.%20To%0Aaddress%20this%2C%20we%20introduce%20GitChameleon%2C%20a%20novel%2C%20meticulously%20curated%20dataset%0Acomprising%20328%20Python%20code%20completion%20problems%2C%20each%20conditioned%20on%20specific%0Alibrary%20versions%20and%20accompanied%20by%20executable%20unit%20tests.%20GitChameleon%0Arigorously%20evaluates%20the%20capacity%20of%20contemporary%20large%20language%20models%20%28LLMs%29%2C%0ALLM-powered%20agents%2C%20code%20assistants%2C%20and%20RAG%20systems%20to%20perform%0Aversion-conditioned%20code%20generation%20that%20demonstrates%20functional%20accuracy%0Athrough%20execution.%20Our%20extensive%20evaluations%20indicate%20that%20state-of-the-art%0Asystems%20encounter%20significant%20challenges%20with%20this%20task%3B%20enterprise%20models%0Aachieving%20baseline%20success%20rates%20in%20the%2048-51%5C%25%20range%2C%20underscoring%20the%0Aintricacy%20of%20the%20problem.%20By%20offering%20an%20execution-based%20benchmark%20emphasizing%0Athe%20dynamic%20nature%20of%20code%20libraries%2C%20GitChameleon%20enables%20a%20clearer%0Aunderstanding%20of%20this%20challenge%20and%20helps%20guide%20the%20development%20of%20more%0Aadaptable%20and%20dependable%20AI%20code%20generation%20methods.%20We%20make%20the%20dataset%20and%0Aevaluation%20code%20publicly%20available%20at%0Ahttps%3A//github.com/mrcabbage972/GitChameleonBenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGitChameleon%253A%2520Evaluating%2520AI%2520Code%2520Generation%2520Against%2520Python%2520Library%250A%2520%2520Version%2520Incompatibilities%26entry.906535625%3DDiganta%2520Misra%2520and%2520Nizar%2520Islah%2520and%2520Victor%2520May%2520and%2520Brice%2520Rauby%2520and%2520Zihan%2520Wang%2520and%2520Justine%2520Gehring%2520and%2520Antonio%2520Orvieto%2520and%2520Muawiz%2520Chaudhary%2520and%2520Eilif%2520B.%2520Muller%2520and%2520Irina%2520Rish%2520and%2520Samira%2520Ebrahimi%2520Kahou%2520and%2520Massimo%2520Caccia%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520software%2520libraries%2520poses%2520a%2520considerable%2520hurdle%2520for%250Acode%2520generation%252C%2520necessitating%2520continuous%2520adaptation%2520to%2520frequent%2520version%250Aupdates%2520while%2520preserving%2520backward%2520compatibility.%2520While%2520existing%2520code%2520evolution%250Abenchmarks%2520provide%2520valuable%2520insights%252C%2520they%2520typically%2520lack%2520execution-based%250Aevaluation%2520for%2520generating%2520code%2520compliant%2520with%2520specific%2520library%2520versions.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520GitChameleon%252C%2520a%2520novel%252C%2520meticulously%2520curated%2520dataset%250Acomprising%2520328%2520Python%2520code%2520completion%2520problems%252C%2520each%2520conditioned%2520on%2520specific%250Alibrary%2520versions%2520and%2520accompanied%2520by%2520executable%2520unit%2520tests.%2520GitChameleon%250Arigorously%2520evaluates%2520the%2520capacity%2520of%2520contemporary%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250ALLM-powered%2520agents%252C%2520code%2520assistants%252C%2520and%2520RAG%2520systems%2520to%2520perform%250Aversion-conditioned%2520code%2520generation%2520that%2520demonstrates%2520functional%2520accuracy%250Athrough%2520execution.%2520Our%2520extensive%2520evaluations%2520indicate%2520that%2520state-of-the-art%250Asystems%2520encounter%2520significant%2520challenges%2520with%2520this%2520task%253B%2520enterprise%2520models%250Aachieving%2520baseline%2520success%2520rates%2520in%2520the%252048-51%255C%2525%2520range%252C%2520underscoring%2520the%250Aintricacy%2520of%2520the%2520problem.%2520By%2520offering%2520an%2520execution-based%2520benchmark%2520emphasizing%250Athe%2520dynamic%2520nature%2520of%2520code%2520libraries%252C%2520GitChameleon%2520enables%2520a%2520clearer%250Aunderstanding%2520of%2520this%2520challenge%2520and%2520helps%2520guide%2520the%2520development%2520of%2520more%250Aadaptable%2520and%2520dependable%2520AI%2520code%2520generation%2520methods.%2520We%2520make%2520the%2520dataset%2520and%250Aevaluation%2520code%2520publicly%2520available%2520at%250Ahttps%253A//github.com/mrcabbage972/GitChameleonBenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GitChameleon%3A%20Evaluating%20AI%20Code%20Generation%20Against%20Python%20Library%0A%20%20Version%20Incompatibilities&entry.906535625=Diganta%20Misra%20and%20Nizar%20Islah%20and%20Victor%20May%20and%20Brice%20Rauby%20and%20Zihan%20Wang%20and%20Justine%20Gehring%20and%20Antonio%20Orvieto%20and%20Muawiz%20Chaudhary%20and%20Eilif%20B.%20Muller%20and%20Irina%20Rish%20and%20Samira%20Ebrahimi%20Kahou%20and%20Massimo%20Caccia&entry.1292438233=%20%20The%20rapid%20evolution%20of%20software%20libraries%20poses%20a%20considerable%20hurdle%20for%0Acode%20generation%2C%20necessitating%20continuous%20adaptation%20to%20frequent%20version%0Aupdates%20while%20preserving%20backward%20compatibility.%20While%20existing%20code%20evolution%0Abenchmarks%20provide%20valuable%20insights%2C%20they%20typically%20lack%20execution-based%0Aevaluation%20for%20generating%20code%20compliant%20with%20specific%20library%20versions.%20To%0Aaddress%20this%2C%20we%20introduce%20GitChameleon%2C%20a%20novel%2C%20meticulously%20curated%20dataset%0Acomprising%20328%20Python%20code%20completion%20problems%2C%20each%20conditioned%20on%20specific%0Alibrary%20versions%20and%20accompanied%20by%20executable%20unit%20tests.%20GitChameleon%0Arigorously%20evaluates%20the%20capacity%20of%20contemporary%20large%20language%20models%20%28LLMs%29%2C%0ALLM-powered%20agents%2C%20code%20assistants%2C%20and%20RAG%20systems%20to%20perform%0Aversion-conditioned%20code%20generation%20that%20demonstrates%20functional%20accuracy%0Athrough%20execution.%20Our%20extensive%20evaluations%20indicate%20that%20state-of-the-art%0Asystems%20encounter%20significant%20challenges%20with%20this%20task%3B%20enterprise%20models%0Aachieving%20baseline%20success%20rates%20in%20the%2048-51%5C%25%20range%2C%20underscoring%20the%0Aintricacy%20of%20the%20problem.%20By%20offering%20an%20execution-based%20benchmark%20emphasizing%0Athe%20dynamic%20nature%20of%20code%20libraries%2C%20GitChameleon%20enables%20a%20clearer%0Aunderstanding%20of%20this%20challenge%20and%20helps%20guide%20the%20development%20of%20more%0Aadaptable%20and%20dependable%20AI%20code%20generation%20methods.%20We%20make%20the%20dataset%20and%0Aevaluation%20code%20publicly%20available%20at%0Ahttps%3A//github.com/mrcabbage972/GitChameleonBenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12367v1&entry.124074799=Read"},
{"title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks", "author": "Artem Chervyakov and Alexander Kharitonov and Pavel Zadorozhny and Adamenko Pavel and Rodion Levichev and Dmitrii Vorobev and Dmitrii Salikhov and Aidar Valeev and Alena Pestova and Maria Dziuba and Ilseyar Alimova and Artem Zavgorodnev and Aleksandr Medvedev and Stanislav Moiseev and Elena Bruches and Daniil Grebenkin and Roman Derunets and Vikulov Vladimir and Anton Emelyanov and Dmitrii Babaev and Vladimir V. Ivanov and Valentin Malykh and Alena Fenogenova", "abstract": "  Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.\n", "link": "http://arxiv.org/abs/2507.12284v1", "date": "2025-07-16", "relevancy": 1.8524, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERA%20Code%3A%20A%20Unified%20Framework%20for%20Evaluating%20Code%20Generation%20Across%0A%20%20Tasks&body=Title%3A%20MERA%20Code%3A%20A%20Unified%20Framework%20for%20Evaluating%20Code%20Generation%20Across%0A%20%20Tasks%0AAuthor%3A%20Artem%20Chervyakov%20and%20Alexander%20Kharitonov%20and%20Pavel%20Zadorozhny%20and%20Adamenko%20Pavel%20and%20Rodion%20Levichev%20and%20Dmitrii%20Vorobev%20and%20Dmitrii%20Salikhov%20and%20Aidar%20Valeev%20and%20Alena%20Pestova%20and%20Maria%20Dziuba%20and%20Ilseyar%20Alimova%20and%20Artem%20Zavgorodnev%20and%20Aleksandr%20Medvedev%20and%20Stanislav%20Moiseev%20and%20Elena%20Bruches%20and%20Daniil%20Grebenkin%20and%20Roman%20Derunets%20and%20Vikulov%20Vladimir%20and%20Anton%20Emelyanov%20and%20Dmitrii%20Babaev%20and%20Vladimir%20V.%20Ivanov%20and%20Valentin%20Malykh%20and%20Alena%20Fenogenova%0AAbstract%3A%20%20%20Advancements%20in%20LLMs%20have%20enhanced%20task%20automation%20in%20software%20engineering%3B%0Ahowever%2C%20current%20evaluations%20primarily%20focus%20on%20natural%20language%20tasks%2C%0Aoverlooking%20code%20quality.%20Most%20benchmarks%20prioritize%20high-level%20reasoning%20over%0Aexecutable%20code%20and%20real-world%20performance%2C%20leaving%20gaps%20in%20understanding%20true%0Acapabilities%20and%20risks%20associated%20with%20these%20models%20in%20production.%20To%20address%0Athis%20issue%2C%20we%20propose%20MERA%20Code%2C%20a%20new%20addition%20to%20the%20MERA%20benchmark%20family%2C%0Aspecifically%20focused%20on%20evaluating%20code%20for%20the%20latest%20code%20generation%20LLMs%20in%0ARussian.%20This%20benchmark%20includes%2011%20evaluation%20tasks%20that%20span%208%20programming%0Alanguages.%20Our%20proposed%20evaluation%20methodology%20features%20a%20taxonomy%20that%0Aoutlines%20the%20practical%20coding%20skills%20necessary%20for%20models%20to%20complete%20these%0Atasks.%20The%20benchmark%20comprises%20an%20open-source%20codebase%20for%20users%20to%20conduct%0AMERA%20assessments%2C%20a%20scoring%20system%20compatible%20with%20various%20programming%0Aenvironments%2C%20and%20a%20platform%20featuring%20a%20leaderboard%20and%20submission%20system.%20We%0Aevaluate%20open%20LLMs%20and%20frontier%20API%20models%2C%20analyzing%20their%20limitations%20in%0Aterms%20of%20practical%20coding%20tasks%20in%20non-English%20languages.%20We%20are%20publicly%0Areleasing%20MERA%20to%20guide%20future%20research%2C%20anticipate%20groundbreaking%20features%20in%0Amodel%20development%2C%20and%20standardize%20evaluation%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERA%2520Code%253A%2520A%2520Unified%2520Framework%2520for%2520Evaluating%2520Code%2520Generation%2520Across%250A%2520%2520Tasks%26entry.906535625%3DArtem%2520Chervyakov%2520and%2520Alexander%2520Kharitonov%2520and%2520Pavel%2520Zadorozhny%2520and%2520Adamenko%2520Pavel%2520and%2520Rodion%2520Levichev%2520and%2520Dmitrii%2520Vorobev%2520and%2520Dmitrii%2520Salikhov%2520and%2520Aidar%2520Valeev%2520and%2520Alena%2520Pestova%2520and%2520Maria%2520Dziuba%2520and%2520Ilseyar%2520Alimova%2520and%2520Artem%2520Zavgorodnev%2520and%2520Aleksandr%2520Medvedev%2520and%2520Stanislav%2520Moiseev%2520and%2520Elena%2520Bruches%2520and%2520Daniil%2520Grebenkin%2520and%2520Roman%2520Derunets%2520and%2520Vikulov%2520Vladimir%2520and%2520Anton%2520Emelyanov%2520and%2520Dmitrii%2520Babaev%2520and%2520Vladimir%2520V.%2520Ivanov%2520and%2520Valentin%2520Malykh%2520and%2520Alena%2520Fenogenova%26entry.1292438233%3D%2520%2520Advancements%2520in%2520LLMs%2520have%2520enhanced%2520task%2520automation%2520in%2520software%2520engineering%253B%250Ahowever%252C%2520current%2520evaluations%2520primarily%2520focus%2520on%2520natural%2520language%2520tasks%252C%250Aoverlooking%2520code%2520quality.%2520Most%2520benchmarks%2520prioritize%2520high-level%2520reasoning%2520over%250Aexecutable%2520code%2520and%2520real-world%2520performance%252C%2520leaving%2520gaps%2520in%2520understanding%2520true%250Acapabilities%2520and%2520risks%2520associated%2520with%2520these%2520models%2520in%2520production.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520MERA%2520Code%252C%2520a%2520new%2520addition%2520to%2520the%2520MERA%2520benchmark%2520family%252C%250Aspecifically%2520focused%2520on%2520evaluating%2520code%2520for%2520the%2520latest%2520code%2520generation%2520LLMs%2520in%250ARussian.%2520This%2520benchmark%2520includes%252011%2520evaluation%2520tasks%2520that%2520span%25208%2520programming%250Alanguages.%2520Our%2520proposed%2520evaluation%2520methodology%2520features%2520a%2520taxonomy%2520that%250Aoutlines%2520the%2520practical%2520coding%2520skills%2520necessary%2520for%2520models%2520to%2520complete%2520these%250Atasks.%2520The%2520benchmark%2520comprises%2520an%2520open-source%2520codebase%2520for%2520users%2520to%2520conduct%250AMERA%2520assessments%252C%2520a%2520scoring%2520system%2520compatible%2520with%2520various%2520programming%250Aenvironments%252C%2520and%2520a%2520platform%2520featuring%2520a%2520leaderboard%2520and%2520submission%2520system.%2520We%250Aevaluate%2520open%2520LLMs%2520and%2520frontier%2520API%2520models%252C%2520analyzing%2520their%2520limitations%2520in%250Aterms%2520of%2520practical%2520coding%2520tasks%2520in%2520non-English%2520languages.%2520We%2520are%2520publicly%250Areleasing%2520MERA%2520to%2520guide%2520future%2520research%252C%2520anticipate%2520groundbreaking%2520features%2520in%250Amodel%2520development%252C%2520and%2520standardize%2520evaluation%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERA%20Code%3A%20A%20Unified%20Framework%20for%20Evaluating%20Code%20Generation%20Across%0A%20%20Tasks&entry.906535625=Artem%20Chervyakov%20and%20Alexander%20Kharitonov%20and%20Pavel%20Zadorozhny%20and%20Adamenko%20Pavel%20and%20Rodion%20Levichev%20and%20Dmitrii%20Vorobev%20and%20Dmitrii%20Salikhov%20and%20Aidar%20Valeev%20and%20Alena%20Pestova%20and%20Maria%20Dziuba%20and%20Ilseyar%20Alimova%20and%20Artem%20Zavgorodnev%20and%20Aleksandr%20Medvedev%20and%20Stanislav%20Moiseev%20and%20Elena%20Bruches%20and%20Daniil%20Grebenkin%20and%20Roman%20Derunets%20and%20Vikulov%20Vladimir%20and%20Anton%20Emelyanov%20and%20Dmitrii%20Babaev%20and%20Vladimir%20V.%20Ivanov%20and%20Valentin%20Malykh%20and%20Alena%20Fenogenova&entry.1292438233=%20%20Advancements%20in%20LLMs%20have%20enhanced%20task%20automation%20in%20software%20engineering%3B%0Ahowever%2C%20current%20evaluations%20primarily%20focus%20on%20natural%20language%20tasks%2C%0Aoverlooking%20code%20quality.%20Most%20benchmarks%20prioritize%20high-level%20reasoning%20over%0Aexecutable%20code%20and%20real-world%20performance%2C%20leaving%20gaps%20in%20understanding%20true%0Acapabilities%20and%20risks%20associated%20with%20these%20models%20in%20production.%20To%20address%0Athis%20issue%2C%20we%20propose%20MERA%20Code%2C%20a%20new%20addition%20to%20the%20MERA%20benchmark%20family%2C%0Aspecifically%20focused%20on%20evaluating%20code%20for%20the%20latest%20code%20generation%20LLMs%20in%0ARussian.%20This%20benchmark%20includes%2011%20evaluation%20tasks%20that%20span%208%20programming%0Alanguages.%20Our%20proposed%20evaluation%20methodology%20features%20a%20taxonomy%20that%0Aoutlines%20the%20practical%20coding%20skills%20necessary%20for%20models%20to%20complete%20these%0Atasks.%20The%20benchmark%20comprises%20an%20open-source%20codebase%20for%20users%20to%20conduct%0AMERA%20assessments%2C%20a%20scoring%20system%20compatible%20with%20various%20programming%0Aenvironments%2C%20and%20a%20platform%20featuring%20a%20leaderboard%20and%20submission%20system.%20We%0Aevaluate%20open%20LLMs%20and%20frontier%20API%20models%2C%20analyzing%20their%20limitations%20in%0Aterms%20of%20practical%20coding%20tasks%20in%20non-English%20languages.%20We%20are%20publicly%0Areleasing%20MERA%20to%20guide%20future%20research%2C%20anticipate%20groundbreaking%20features%20in%0Amodel%20development%2C%20and%20standardize%20evaluation%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12284v1&entry.124074799=Read"},
{"title": "Structured and Balanced Multi-Component and Multi-Layer Neural Networks", "author": "Shijun Zhang and Hongkai Zhao and Yimin Zhong and Haomin Zhou", "abstract": "  In this work, we propose a balanced multi-component and multi-layer neural\nnetwork (MMNN) structure to accurately and efficiently approximate functions\nwith complex features, in terms of both degrees of freedom and computational\ncost. The main idea is inspired by a multi-component approach, in which each\ncomponent can be effectively approximated by a single-layer network, combined\nwith a multi-layer decomposition strategy to capture the complexity of the\ntarget function. Although MMNNs can be viewed as a simple modification of fully\nconnected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by\nintroducing balanced multi-component structures, they achieve a significant\nreduction in training parameters, a much more efficient training process, and\nimproved accuracy compared to FCNNs or MLPs. Extensive numerical experiments\ndemonstrate the effectiveness of MMNNs in approximating highly oscillatory\nfunctions and their ability to automatically adapt to localized features.\n", "link": "http://arxiv.org/abs/2407.00765v3", "date": "2025-07-16", "relevancy": 1.847, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20and%20Balanced%20Multi-Component%20and%20Multi-Layer%20Neural%20Networks&body=Title%3A%20Structured%20and%20Balanced%20Multi-Component%20and%20Multi-Layer%20Neural%20Networks%0AAuthor%3A%20Shijun%20Zhang%20and%20Hongkai%20Zhao%20and%20Yimin%20Zhong%20and%20Haomin%20Zhou%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20balanced%20multi-component%20and%20multi-layer%20neural%0Anetwork%20%28MMNN%29%20structure%20to%20accurately%20and%20efficiently%20approximate%20functions%0Awith%20complex%20features%2C%20in%20terms%20of%20both%20degrees%20of%20freedom%20and%20computational%0Acost.%20The%20main%20idea%20is%20inspired%20by%20a%20multi-component%20approach%2C%20in%20which%20each%0Acomponent%20can%20be%20effectively%20approximated%20by%20a%20single-layer%20network%2C%20combined%0Awith%20a%20multi-layer%20decomposition%20strategy%20to%20capture%20the%20complexity%20of%20the%0Atarget%20function.%20Although%20MMNNs%20can%20be%20viewed%20as%20a%20simple%20modification%20of%20fully%0Aconnected%20neural%20networks%20%28FCNNs%29%20or%20multi-layer%20perceptrons%20%28MLPs%29%20by%0Aintroducing%20balanced%20multi-component%20structures%2C%20they%20achieve%20a%20significant%0Areduction%20in%20training%20parameters%2C%20a%20much%20more%20efficient%20training%20process%2C%20and%0Aimproved%20accuracy%20compared%20to%20FCNNs%20or%20MLPs.%20Extensive%20numerical%20experiments%0Ademonstrate%20the%20effectiveness%20of%20MMNNs%20in%20approximating%20highly%20oscillatory%0Afunctions%20and%20their%20ability%20to%20automatically%20adapt%20to%20localized%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520and%2520Balanced%2520Multi-Component%2520and%2520Multi-Layer%2520Neural%2520Networks%26entry.906535625%3DShijun%2520Zhang%2520and%2520Hongkai%2520Zhao%2520and%2520Yimin%2520Zhong%2520and%2520Haomin%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520balanced%2520multi-component%2520and%2520multi-layer%2520neural%250Anetwork%2520%2528MMNN%2529%2520structure%2520to%2520accurately%2520and%2520efficiently%2520approximate%2520functions%250Awith%2520complex%2520features%252C%2520in%2520terms%2520of%2520both%2520degrees%2520of%2520freedom%2520and%2520computational%250Acost.%2520The%2520main%2520idea%2520is%2520inspired%2520by%2520a%2520multi-component%2520approach%252C%2520in%2520which%2520each%250Acomponent%2520can%2520be%2520effectively%2520approximated%2520by%2520a%2520single-layer%2520network%252C%2520combined%250Awith%2520a%2520multi-layer%2520decomposition%2520strategy%2520to%2520capture%2520the%2520complexity%2520of%2520the%250Atarget%2520function.%2520Although%2520MMNNs%2520can%2520be%2520viewed%2520as%2520a%2520simple%2520modification%2520of%2520fully%250Aconnected%2520neural%2520networks%2520%2528FCNNs%2529%2520or%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520by%250Aintroducing%2520balanced%2520multi-component%2520structures%252C%2520they%2520achieve%2520a%2520significant%250Areduction%2520in%2520training%2520parameters%252C%2520a%2520much%2520more%2520efficient%2520training%2520process%252C%2520and%250Aimproved%2520accuracy%2520compared%2520to%2520FCNNs%2520or%2520MLPs.%2520Extensive%2520numerical%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520MMNNs%2520in%2520approximating%2520highly%2520oscillatory%250Afunctions%2520and%2520their%2520ability%2520to%2520automatically%2520adapt%2520to%2520localized%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20and%20Balanced%20Multi-Component%20and%20Multi-Layer%20Neural%20Networks&entry.906535625=Shijun%20Zhang%20and%20Hongkai%20Zhao%20and%20Yimin%20Zhong%20and%20Haomin%20Zhou&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20balanced%20multi-component%20and%20multi-layer%20neural%0Anetwork%20%28MMNN%29%20structure%20to%20accurately%20and%20efficiently%20approximate%20functions%0Awith%20complex%20features%2C%20in%20terms%20of%20both%20degrees%20of%20freedom%20and%20computational%0Acost.%20The%20main%20idea%20is%20inspired%20by%20a%20multi-component%20approach%2C%20in%20which%20each%0Acomponent%20can%20be%20effectively%20approximated%20by%20a%20single-layer%20network%2C%20combined%0Awith%20a%20multi-layer%20decomposition%20strategy%20to%20capture%20the%20complexity%20of%20the%0Atarget%20function.%20Although%20MMNNs%20can%20be%20viewed%20as%20a%20simple%20modification%20of%20fully%0Aconnected%20neural%20networks%20%28FCNNs%29%20or%20multi-layer%20perceptrons%20%28MLPs%29%20by%0Aintroducing%20balanced%20multi-component%20structures%2C%20they%20achieve%20a%20significant%0Areduction%20in%20training%20parameters%2C%20a%20much%20more%20efficient%20training%20process%2C%20and%0Aimproved%20accuracy%20compared%20to%20FCNNs%20or%20MLPs.%20Extensive%20numerical%20experiments%0Ademonstrate%20the%20effectiveness%20of%20MMNNs%20in%20approximating%20highly%20oscillatory%0Afunctions%20and%20their%20ability%20to%20automatically%20adapt%20to%20localized%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00765v3&entry.124074799=Read"},
{"title": "Planning-Aware Code Infilling via Horizon-Length Prediction", "author": "Yifeng Ding and Hantian Ding and Shiqi Wang and Qing Sun and Varun Kumar and Zijian Wang", "abstract": "  Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios.\n", "link": "http://arxiv.org/abs/2410.03103v3", "date": "2025-07-16", "relevancy": 1.8396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning-Aware%20Code%20Infilling%20via%20Horizon-Length%20Prediction&body=Title%3A%20Planning-Aware%20Code%20Infilling%20via%20Horizon-Length%20Prediction%0AAuthor%3A%20Yifeng%20Ding%20and%20Hantian%20Ding%20and%20Shiqi%20Wang%20and%20Qing%20Sun%20and%20Varun%20Kumar%20and%20Zijian%20Wang%0AAbstract%3A%20%20%20Fill-in-the-Middle%20%28FIM%29%2C%20or%20infilling%2C%20has%20become%20integral%20to%20code%20language%0Amodels%2C%20enabling%20generation%20of%20missing%20code%20given%20both%20left%20and%20right%20contexts.%0AHowever%2C%20the%20current%20FIM%20training%20paradigm%20which%20performs%20next-token%20prediction%0A%28NTP%29%20over%20reordered%20sequence%20often%20leads%20to%20models%20struggling%20to%20generate%0Acontent%20that%20aligns%20well%20with%20the%20surrounding%20context.%20We%20hypothesize%20that%20NTP%0Aalone%20is%20insufficient%20for%20models%20to%20learn%20effective%20planning%20conditioned%20on%20the%0Adistant%20right%20context%2C%20a%20critical%20factor%20for%20successful%20code%20infilling.%20To%0Aovercome%20this%2C%20we%20propose%20Horizon-Length%20Prediction%20%28HLP%29%2C%20a%20novel%20training%0Aobjective%20that%20teaches%20models%20to%20predict%20the%20number%20of%20remaining%20middle%20tokens%0Aat%20each%20step.%20HLP%20advances%20FIM%20with%20lookahead%20planning%2C%20enabling%20models%20to%0Ainherently%20learn%20infilling%20boundaries%20for%20arbitrary%20left%20and%20right%20contexts%0Awithout%20relying%20on%20dataset-specific%20post-processing.%20Our%20evaluation%20across%0Adifferent%20model%20families%20and%20sizes%20shows%20that%20HLP%20significantly%20improves%20FIM%0Aperformance%20by%20up%20to%2024%25%20relatively%20on%20diverse%20benchmarks%2C%20across%20file-level%0Aand%20repository-level.%20Furthermore%2C%20the%20enhanced%20planning%20capability%20gained%0Athrough%20HLP%20boosts%20model%20performance%20on%20code%20reasoning.%20Importantly%2C%20HLP%20incurs%0Anegligible%20training%20overhead%20and%20no%20additional%20inference%20cost%2C%20ensuring%20its%0Apracticality%20for%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03103v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning-Aware%2520Code%2520Infilling%2520via%2520Horizon-Length%2520Prediction%26entry.906535625%3DYifeng%2520Ding%2520and%2520Hantian%2520Ding%2520and%2520Shiqi%2520Wang%2520and%2520Qing%2520Sun%2520and%2520Varun%2520Kumar%2520and%2520Zijian%2520Wang%26entry.1292438233%3D%2520%2520Fill-in-the-Middle%2520%2528FIM%2529%252C%2520or%2520infilling%252C%2520has%2520become%2520integral%2520to%2520code%2520language%250Amodels%252C%2520enabling%2520generation%2520of%2520missing%2520code%2520given%2520both%2520left%2520and%2520right%2520contexts.%250AHowever%252C%2520the%2520current%2520FIM%2520training%2520paradigm%2520which%2520performs%2520next-token%2520prediction%250A%2528NTP%2529%2520over%2520reordered%2520sequence%2520often%2520leads%2520to%2520models%2520struggling%2520to%2520generate%250Acontent%2520that%2520aligns%2520well%2520with%2520the%2520surrounding%2520context.%2520We%2520hypothesize%2520that%2520NTP%250Aalone%2520is%2520insufficient%2520for%2520models%2520to%2520learn%2520effective%2520planning%2520conditioned%2520on%2520the%250Adistant%2520right%2520context%252C%2520a%2520critical%2520factor%2520for%2520successful%2520code%2520infilling.%2520To%250Aovercome%2520this%252C%2520we%2520propose%2520Horizon-Length%2520Prediction%2520%2528HLP%2529%252C%2520a%2520novel%2520training%250Aobjective%2520that%2520teaches%2520models%2520to%2520predict%2520the%2520number%2520of%2520remaining%2520middle%2520tokens%250Aat%2520each%2520step.%2520HLP%2520advances%2520FIM%2520with%2520lookahead%2520planning%252C%2520enabling%2520models%2520to%250Ainherently%2520learn%2520infilling%2520boundaries%2520for%2520arbitrary%2520left%2520and%2520right%2520contexts%250Awithout%2520relying%2520on%2520dataset-specific%2520post-processing.%2520Our%2520evaluation%2520across%250Adifferent%2520model%2520families%2520and%2520sizes%2520shows%2520that%2520HLP%2520significantly%2520improves%2520FIM%250Aperformance%2520by%2520up%2520to%252024%2525%2520relatively%2520on%2520diverse%2520benchmarks%252C%2520across%2520file-level%250Aand%2520repository-level.%2520Furthermore%252C%2520the%2520enhanced%2520planning%2520capability%2520gained%250Athrough%2520HLP%2520boosts%2520model%2520performance%2520on%2520code%2520reasoning.%2520Importantly%252C%2520HLP%2520incurs%250Anegligible%2520training%2520overhead%2520and%2520no%2520additional%2520inference%2520cost%252C%2520ensuring%2520its%250Apracticality%2520for%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03103v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning-Aware%20Code%20Infilling%20via%20Horizon-Length%20Prediction&entry.906535625=Yifeng%20Ding%20and%20Hantian%20Ding%20and%20Shiqi%20Wang%20and%20Qing%20Sun%20and%20Varun%20Kumar%20and%20Zijian%20Wang&entry.1292438233=%20%20Fill-in-the-Middle%20%28FIM%29%2C%20or%20infilling%2C%20has%20become%20integral%20to%20code%20language%0Amodels%2C%20enabling%20generation%20of%20missing%20code%20given%20both%20left%20and%20right%20contexts.%0AHowever%2C%20the%20current%20FIM%20training%20paradigm%20which%20performs%20next-token%20prediction%0A%28NTP%29%20over%20reordered%20sequence%20often%20leads%20to%20models%20struggling%20to%20generate%0Acontent%20that%20aligns%20well%20with%20the%20surrounding%20context.%20We%20hypothesize%20that%20NTP%0Aalone%20is%20insufficient%20for%20models%20to%20learn%20effective%20planning%20conditioned%20on%20the%0Adistant%20right%20context%2C%20a%20critical%20factor%20for%20successful%20code%20infilling.%20To%0Aovercome%20this%2C%20we%20propose%20Horizon-Length%20Prediction%20%28HLP%29%2C%20a%20novel%20training%0Aobjective%20that%20teaches%20models%20to%20predict%20the%20number%20of%20remaining%20middle%20tokens%0Aat%20each%20step.%20HLP%20advances%20FIM%20with%20lookahead%20planning%2C%20enabling%20models%20to%0Ainherently%20learn%20infilling%20boundaries%20for%20arbitrary%20left%20and%20right%20contexts%0Awithout%20relying%20on%20dataset-specific%20post-processing.%20Our%20evaluation%20across%0Adifferent%20model%20families%20and%20sizes%20shows%20that%20HLP%20significantly%20improves%20FIM%0Aperformance%20by%20up%20to%2024%25%20relatively%20on%20diverse%20benchmarks%2C%20across%20file-level%0Aand%20repository-level.%20Furthermore%2C%20the%20enhanced%20planning%20capability%20gained%0Athrough%20HLP%20boosts%20model%20performance%20on%20code%20reasoning.%20Importantly%2C%20HLP%20incurs%0Anegligible%20training%20overhead%20and%20no%20additional%20inference%20cost%2C%20ensuring%20its%0Apracticality%20for%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03103v3&entry.124074799=Read"},
{"title": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research", "author": "Shomir Wilson", "abstract": "  Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.\n", "link": "http://arxiv.org/abs/2507.10559v2", "date": "2025-07-16", "relevancy": 1.8341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NLP%20Meets%20the%20World%3A%20Toward%20Improving%20Conversations%20With%20the%20Public%0A%20%20About%20Natural%20Language%20Processing%20Research&body=Title%3A%20NLP%20Meets%20the%20World%3A%20Toward%20Improving%20Conversations%20With%20the%20Public%0A%20%20About%20Natural%20Language%20Processing%20Research%0AAuthor%3A%20Shomir%20Wilson%0AAbstract%3A%20%20%20Recent%20developments%20in%20large%20language%20models%20%28LLMs%29%20have%20been%20accompanied%20by%0Arapidly%20growing%20public%20interest%20in%20natural%20language%20processing%20%28NLP%29.%20This%0Aattention%20is%20reflected%20by%20major%20news%20venues%2C%20which%20sometimes%20invite%20NLP%0Aresearchers%20to%20share%20their%20knowledge%20and%20views%20with%20a%20wide%20audience.%0ARecognizing%20the%20opportunities%20of%20the%20present%2C%20for%20both%20the%20research%20field%20and%0Afor%20individual%20researchers%2C%20this%20paper%20shares%20recommendations%20for%20communicating%0Awith%20a%20general%20audience%20about%20the%20capabilities%20and%20limitations%20of%20NLP.%20These%0Arecommendations%20cover%20three%20themes%3A%20vague%20terminology%20as%20an%20obstacle%20to%20public%0Aunderstanding%2C%20unreasonable%20expectations%20as%20obstacles%20to%20sustainable%20growth%2C%0Aand%20ethical%20failures%20as%20obstacles%20to%20continued%20support.%20Published%20NLP%20research%0Aand%20popular%20news%20coverage%20are%20cited%20to%20illustrate%20these%20themes%20with%20examples.%0AThe%20recommendations%20promote%20effective%2C%20transparent%20communication%20with%20the%0Ageneral%20public%20about%20NLP%2C%20in%20order%20to%20strengthen%20public%20understanding%20and%0Aencourage%20support%20for%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNLP%2520Meets%2520the%2520World%253A%2520Toward%2520Improving%2520Conversations%2520With%2520the%2520Public%250A%2520%2520About%2520Natural%2520Language%2520Processing%2520Research%26entry.906535625%3DShomir%2520Wilson%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520accompanied%2520by%250Arapidly%2520growing%2520public%2520interest%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529.%2520This%250Aattention%2520is%2520reflected%2520by%2520major%2520news%2520venues%252C%2520which%2520sometimes%2520invite%2520NLP%250Aresearchers%2520to%2520share%2520their%2520knowledge%2520and%2520views%2520with%2520a%2520wide%2520audience.%250ARecognizing%2520the%2520opportunities%2520of%2520the%2520present%252C%2520for%2520both%2520the%2520research%2520field%2520and%250Afor%2520individual%2520researchers%252C%2520this%2520paper%2520shares%2520recommendations%2520for%2520communicating%250Awith%2520a%2520general%2520audience%2520about%2520the%2520capabilities%2520and%2520limitations%2520of%2520NLP.%2520These%250Arecommendations%2520cover%2520three%2520themes%253A%2520vague%2520terminology%2520as%2520an%2520obstacle%2520to%2520public%250Aunderstanding%252C%2520unreasonable%2520expectations%2520as%2520obstacles%2520to%2520sustainable%2520growth%252C%250Aand%2520ethical%2520failures%2520as%2520obstacles%2520to%2520continued%2520support.%2520Published%2520NLP%2520research%250Aand%2520popular%2520news%2520coverage%2520are%2520cited%2520to%2520illustrate%2520these%2520themes%2520with%2520examples.%250AThe%2520recommendations%2520promote%2520effective%252C%2520transparent%2520communication%2520with%2520the%250Ageneral%2520public%2520about%2520NLP%252C%2520in%2520order%2520to%2520strengthen%2520public%2520understanding%2520and%250Aencourage%2520support%2520for%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NLP%20Meets%20the%20World%3A%20Toward%20Improving%20Conversations%20With%20the%20Public%0A%20%20About%20Natural%20Language%20Processing%20Research&entry.906535625=Shomir%20Wilson&entry.1292438233=%20%20Recent%20developments%20in%20large%20language%20models%20%28LLMs%29%20have%20been%20accompanied%20by%0Arapidly%20growing%20public%20interest%20in%20natural%20language%20processing%20%28NLP%29.%20This%0Aattention%20is%20reflected%20by%20major%20news%20venues%2C%20which%20sometimes%20invite%20NLP%0Aresearchers%20to%20share%20their%20knowledge%20and%20views%20with%20a%20wide%20audience.%0ARecognizing%20the%20opportunities%20of%20the%20present%2C%20for%20both%20the%20research%20field%20and%0Afor%20individual%20researchers%2C%20this%20paper%20shares%20recommendations%20for%20communicating%0Awith%20a%20general%20audience%20about%20the%20capabilities%20and%20limitations%20of%20NLP.%20These%0Arecommendations%20cover%20three%20themes%3A%20vague%20terminology%20as%20an%20obstacle%20to%20public%0Aunderstanding%2C%20unreasonable%20expectations%20as%20obstacles%20to%20sustainable%20growth%2C%0Aand%20ethical%20failures%20as%20obstacles%20to%20continued%20support.%20Published%20NLP%20research%0Aand%20popular%20news%20coverage%20are%20cited%20to%20illustrate%20these%20themes%20with%20examples.%0AThe%20recommendations%20promote%20effective%2C%20transparent%20communication%20with%20the%0Ageneral%20public%20about%20NLP%2C%20in%20order%20to%20strengthen%20public%20understanding%20and%0Aencourage%20support%20for%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10559v2&entry.124074799=Read"},
{"title": "Improving Reinforcement Learning Sample-Efficiency using Local\n  Approximation", "author": "Mohit Prashant and Arvind Easwaran", "abstract": "  In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.\n", "link": "http://arxiv.org/abs/2507.12383v1", "date": "2025-07-16", "relevancy": 1.8274, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4506}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Reinforcement%20Learning%20Sample-Efficiency%20using%20Local%0A%20%20Approximation&body=Title%3A%20Improving%20Reinforcement%20Learning%20Sample-Efficiency%20using%20Local%0A%20%20Approximation%0AAuthor%3A%20Mohit%20Prashant%20and%20Arvind%20Easwaran%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20derive%20Probably%20Approximately%20Correct%20%28PAC%29%20bounds%20on%20the%0Aasymptotic%20sample-complexity%20for%20RL%20within%20the%20infinite-horizon%20Markov%20Decision%0AProcess%20%28MDP%29%20setting%20that%20are%20sharper%20than%20those%20in%20existing%20literature.%20The%0Apremise%20of%20our%20study%20is%20twofold%3A%20firstly%2C%20the%20further%20two%20states%20are%20from%20each%0Aother%2C%20transition-wise%2C%20the%20less%20relevant%20the%20value%20of%20the%20first%20state%20is%20when%0Alearning%20the%20%24%5Cepsilon%24-optimal%20value%20of%20the%20second%3B%20secondly%2C%20the%20amount%20of%0A%27effort%27%2C%20sample-complexity-wise%2C%20expended%20in%20learning%20the%20%24%5Cepsilon%24-optimal%0Avalue%20of%20a%20state%20is%20independent%20of%20the%20number%20of%20samples%20required%20to%20learn%20the%0A%24%5Cepsilon%24-optimal%20value%20of%20a%20second%20state%20that%20is%20a%20sufficient%20number%20of%0Atransitions%20away%20from%20the%20first.%20Inversely%2C%20states%20within%20each%20other%27s%20vicinity%0Ahave%20values%20that%20are%20dependent%20on%20each%20other%20and%20will%20require%20a%20similar%20number%0Aof%20samples%20to%20learn.%20By%20approximating%20the%20original%20MDP%20using%20smaller%20MDPs%0Aconstructed%20using%20subsets%20of%20the%20original%27s%20state-space%2C%20we%20are%20able%20to%20reduce%0Athe%20sample-complexity%20by%20a%20logarithmic%20factor%20to%20%24O%28SA%20%5Clog%20A%29%24%20timesteps%2C%0Awhere%20%24S%24%20and%20%24A%24%20are%20the%20state%20and%20action%20space%20sizes.%20We%20are%20able%20to%20extend%0Athese%20results%20to%20an%20infinite-horizon%2C%20model-free%20setting%20by%20constructing%20a%0APAC-MDP%20algorithm%20with%20the%20aforementioned%20sample-complexity.%20We%20conclude%20with%0Ashowing%20how%20significant%20the%20improvement%20is%20by%20comparing%20our%20algorithm%20against%0Aprior%20work%20in%20an%20experimental%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Reinforcement%2520Learning%2520Sample-Efficiency%2520using%2520Local%250A%2520%2520Approximation%26entry.906535625%3DMohit%2520Prashant%2520and%2520Arvind%2520Easwaran%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520derive%2520Probably%2520Approximately%2520Correct%2520%2528PAC%2529%2520bounds%2520on%2520the%250Aasymptotic%2520sample-complexity%2520for%2520RL%2520within%2520the%2520infinite-horizon%2520Markov%2520Decision%250AProcess%2520%2528MDP%2529%2520setting%2520that%2520are%2520sharper%2520than%2520those%2520in%2520existing%2520literature.%2520The%250Apremise%2520of%2520our%2520study%2520is%2520twofold%253A%2520firstly%252C%2520the%2520further%2520two%2520states%2520are%2520from%2520each%250Aother%252C%2520transition-wise%252C%2520the%2520less%2520relevant%2520the%2520value%2520of%2520the%2520first%2520state%2520is%2520when%250Alearning%2520the%2520%2524%255Cepsilon%2524-optimal%2520value%2520of%2520the%2520second%253B%2520secondly%252C%2520the%2520amount%2520of%250A%2527effort%2527%252C%2520sample-complexity-wise%252C%2520expended%2520in%2520learning%2520the%2520%2524%255Cepsilon%2524-optimal%250Avalue%2520of%2520a%2520state%2520is%2520independent%2520of%2520the%2520number%2520of%2520samples%2520required%2520to%2520learn%2520the%250A%2524%255Cepsilon%2524-optimal%2520value%2520of%2520a%2520second%2520state%2520that%2520is%2520a%2520sufficient%2520number%2520of%250Atransitions%2520away%2520from%2520the%2520first.%2520Inversely%252C%2520states%2520within%2520each%2520other%2527s%2520vicinity%250Ahave%2520values%2520that%2520are%2520dependent%2520on%2520each%2520other%2520and%2520will%2520require%2520a%2520similar%2520number%250Aof%2520samples%2520to%2520learn.%2520By%2520approximating%2520the%2520original%2520MDP%2520using%2520smaller%2520MDPs%250Aconstructed%2520using%2520subsets%2520of%2520the%2520original%2527s%2520state-space%252C%2520we%2520are%2520able%2520to%2520reduce%250Athe%2520sample-complexity%2520by%2520a%2520logarithmic%2520factor%2520to%2520%2524O%2528SA%2520%255Clog%2520A%2529%2524%2520timesteps%252C%250Awhere%2520%2524S%2524%2520and%2520%2524A%2524%2520are%2520the%2520state%2520and%2520action%2520space%2520sizes.%2520We%2520are%2520able%2520to%2520extend%250Athese%2520results%2520to%2520an%2520infinite-horizon%252C%2520model-free%2520setting%2520by%2520constructing%2520a%250APAC-MDP%2520algorithm%2520with%2520the%2520aforementioned%2520sample-complexity.%2520We%2520conclude%2520with%250Ashowing%2520how%2520significant%2520the%2520improvement%2520is%2520by%2520comparing%2520our%2520algorithm%2520against%250Aprior%2520work%2520in%2520an%2520experimental%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Reinforcement%20Learning%20Sample-Efficiency%20using%20Local%0A%20%20Approximation&entry.906535625=Mohit%20Prashant%20and%20Arvind%20Easwaran&entry.1292438233=%20%20In%20this%20study%2C%20we%20derive%20Probably%20Approximately%20Correct%20%28PAC%29%20bounds%20on%20the%0Aasymptotic%20sample-complexity%20for%20RL%20within%20the%20infinite-horizon%20Markov%20Decision%0AProcess%20%28MDP%29%20setting%20that%20are%20sharper%20than%20those%20in%20existing%20literature.%20The%0Apremise%20of%20our%20study%20is%20twofold%3A%20firstly%2C%20the%20further%20two%20states%20are%20from%20each%0Aother%2C%20transition-wise%2C%20the%20less%20relevant%20the%20value%20of%20the%20first%20state%20is%20when%0Alearning%20the%20%24%5Cepsilon%24-optimal%20value%20of%20the%20second%3B%20secondly%2C%20the%20amount%20of%0A%27effort%27%2C%20sample-complexity-wise%2C%20expended%20in%20learning%20the%20%24%5Cepsilon%24-optimal%0Avalue%20of%20a%20state%20is%20independent%20of%20the%20number%20of%20samples%20required%20to%20learn%20the%0A%24%5Cepsilon%24-optimal%20value%20of%20a%20second%20state%20that%20is%20a%20sufficient%20number%20of%0Atransitions%20away%20from%20the%20first.%20Inversely%2C%20states%20within%20each%20other%27s%20vicinity%0Ahave%20values%20that%20are%20dependent%20on%20each%20other%20and%20will%20require%20a%20similar%20number%0Aof%20samples%20to%20learn.%20By%20approximating%20the%20original%20MDP%20using%20smaller%20MDPs%0Aconstructed%20using%20subsets%20of%20the%20original%27s%20state-space%2C%20we%20are%20able%20to%20reduce%0Athe%20sample-complexity%20by%20a%20logarithmic%20factor%20to%20%24O%28SA%20%5Clog%20A%29%24%20timesteps%2C%0Awhere%20%24S%24%20and%20%24A%24%20are%20the%20state%20and%20action%20space%20sizes.%20We%20are%20able%20to%20extend%0Athese%20results%20to%20an%20infinite-horizon%2C%20model-free%20setting%20by%20constructing%20a%0APAC-MDP%20algorithm%20with%20the%20aforementioned%20sample-complexity.%20We%20conclude%20with%0Ashowing%20how%20significant%20the%20improvement%20is%20by%20comparing%20our%20algorithm%20against%0Aprior%20work%20in%20an%20experimental%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12383v1&entry.124074799=Read"},
{"title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot", "author": "Luca Garello and Francesca Cocchella and Alessandra Sciutti and Manuel Catalano and Francesco Rea", "abstract": "  Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.\n", "link": "http://arxiv.org/abs/2507.12273v1", "date": "2025-07-16", "relevancy": 1.8241, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6382}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6267}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Gen%20Museum%20Guides%3A%20Autonomous%20Navigation%20and%20Visitor%20Interaction%0A%20%20with%20an%20Agentic%20Robot&body=Title%3A%20Next-Gen%20Museum%20Guides%3A%20Autonomous%20Navigation%20and%20Visitor%20Interaction%0A%20%20with%20an%20Agentic%20Robot%0AAuthor%3A%20Luca%20Garello%20and%20Francesca%20Cocchella%20and%20Alessandra%20Sciutti%20and%20Manuel%20Catalano%20and%20Francesco%20Rea%0AAbstract%3A%20%20%20Autonomous%20robots%20are%20increasingly%20being%20tested%20into%20public%20spaces%20to%20enhance%0Auser%20experiences%2C%20particularly%20in%20cultural%20and%20educational%20settings.%20This%20paper%0Apresents%20the%20design%2C%20implementation%2C%20and%20evaluation%20of%20the%20autonomous%20museum%0Aguide%20robot%20Alter-Ego%20equipped%20with%20advanced%20navigation%20and%20interactive%0Acapabilities.%20The%20robot%20leverages%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%0Ato%20provide%20real-time%2C%20context%20aware%20question-and-answer%20%28Q%26A%29%20interactions%2C%0Aallowing%20visitors%20to%20engage%20in%20conversations%20about%20exhibits.%20It%20also%20employs%0Arobust%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20techniques%2C%20enabling%0Aseamless%20navigation%20through%20museum%20spaces%20and%20route%20adaptation%20based%20on%20user%0Arequests.%20The%20system%20was%20tested%20in%20a%20real%20museum%20environment%20with%2034%0Aparticipants%2C%20combining%20qualitative%20analysis%20of%20visitor-robot%20conversations%20and%0Aquantitative%20analysis%20of%20pre%20and%20post%20interaction%20surveys.%20Results%20showed%20that%0Athe%20robot%20was%20generally%20well-received%20and%20contributed%20to%20an%20engaging%20museum%0Aexperience%2C%20despite%20some%20limitations%20in%20comprehension%20and%20responsiveness.%20This%0Astudy%20sheds%20light%20on%20HRI%20in%20cultural%20spaces%2C%20highlighting%20not%20only%20the%0Apotential%20of%20AI-driven%20robotics%20to%20support%20accessibility%20and%20knowledge%0Aacquisition%2C%20but%20also%20the%20current%20limitations%20and%20challenges%20of%20deploying%20such%0Atechnologies%20in%20complex%2C%20real-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Gen%2520Museum%2520Guides%253A%2520Autonomous%2520Navigation%2520and%2520Visitor%2520Interaction%250A%2520%2520with%2520an%2520Agentic%2520Robot%26entry.906535625%3DLuca%2520Garello%2520and%2520Francesca%2520Cocchella%2520and%2520Alessandra%2520Sciutti%2520and%2520Manuel%2520Catalano%2520and%2520Francesco%2520Rea%26entry.1292438233%3D%2520%2520Autonomous%2520robots%2520are%2520increasingly%2520being%2520tested%2520into%2520public%2520spaces%2520to%2520enhance%250Auser%2520experiences%252C%2520particularly%2520in%2520cultural%2520and%2520educational%2520settings.%2520This%2520paper%250Apresents%2520the%2520design%252C%2520implementation%252C%2520and%2520evaluation%2520of%2520the%2520autonomous%2520museum%250Aguide%2520robot%2520Alter-Ego%2520equipped%2520with%2520advanced%2520navigation%2520and%2520interactive%250Acapabilities.%2520The%2520robot%2520leverages%2520state-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ato%2520provide%2520real-time%252C%2520context%2520aware%2520question-and-answer%2520%2528Q%2526A%2529%2520interactions%252C%250Aallowing%2520visitors%2520to%2520engage%2520in%2520conversations%2520about%2520exhibits.%2520It%2520also%2520employs%250Arobust%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520techniques%252C%2520enabling%250Aseamless%2520navigation%2520through%2520museum%2520spaces%2520and%2520route%2520adaptation%2520based%2520on%2520user%250Arequests.%2520The%2520system%2520was%2520tested%2520in%2520a%2520real%2520museum%2520environment%2520with%252034%250Aparticipants%252C%2520combining%2520qualitative%2520analysis%2520of%2520visitor-robot%2520conversations%2520and%250Aquantitative%2520analysis%2520of%2520pre%2520and%2520post%2520interaction%2520surveys.%2520Results%2520showed%2520that%250Athe%2520robot%2520was%2520generally%2520well-received%2520and%2520contributed%2520to%2520an%2520engaging%2520museum%250Aexperience%252C%2520despite%2520some%2520limitations%2520in%2520comprehension%2520and%2520responsiveness.%2520This%250Astudy%2520sheds%2520light%2520on%2520HRI%2520in%2520cultural%2520spaces%252C%2520highlighting%2520not%2520only%2520the%250Apotential%2520of%2520AI-driven%2520robotics%2520to%2520support%2520accessibility%2520and%2520knowledge%250Aacquisition%252C%2520but%2520also%2520the%2520current%2520limitations%2520and%2520challenges%2520of%2520deploying%2520such%250Atechnologies%2520in%2520complex%252C%2520real-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Gen%20Museum%20Guides%3A%20Autonomous%20Navigation%20and%20Visitor%20Interaction%0A%20%20with%20an%20Agentic%20Robot&entry.906535625=Luca%20Garello%20and%20Francesca%20Cocchella%20and%20Alessandra%20Sciutti%20and%20Manuel%20Catalano%20and%20Francesco%20Rea&entry.1292438233=%20%20Autonomous%20robots%20are%20increasingly%20being%20tested%20into%20public%20spaces%20to%20enhance%0Auser%20experiences%2C%20particularly%20in%20cultural%20and%20educational%20settings.%20This%20paper%0Apresents%20the%20design%2C%20implementation%2C%20and%20evaluation%20of%20the%20autonomous%20museum%0Aguide%20robot%20Alter-Ego%20equipped%20with%20advanced%20navigation%20and%20interactive%0Acapabilities.%20The%20robot%20leverages%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%0Ato%20provide%20real-time%2C%20context%20aware%20question-and-answer%20%28Q%26A%29%20interactions%2C%0Aallowing%20visitors%20to%20engage%20in%20conversations%20about%20exhibits.%20It%20also%20employs%0Arobust%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20techniques%2C%20enabling%0Aseamless%20navigation%20through%20museum%20spaces%20and%20route%20adaptation%20based%20on%20user%0Arequests.%20The%20system%20was%20tested%20in%20a%20real%20museum%20environment%20with%2034%0Aparticipants%2C%20combining%20qualitative%20analysis%20of%20visitor-robot%20conversations%20and%0Aquantitative%20analysis%20of%20pre%20and%20post%20interaction%20surveys.%20Results%20showed%20that%0Athe%20robot%20was%20generally%20well-received%20and%20contributed%20to%20an%20engaging%20museum%0Aexperience%2C%20despite%20some%20limitations%20in%20comprehension%20and%20responsiveness.%20This%0Astudy%20sheds%20light%20on%20HRI%20in%20cultural%20spaces%2C%20highlighting%20not%20only%20the%0Apotential%20of%20AI-driven%20robotics%20to%20support%20accessibility%20and%20knowledge%0Aacquisition%2C%20but%20also%20the%20current%20limitations%20and%20challenges%20of%20deploying%20such%0Atechnologies%20in%20complex%2C%20real-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12273v1&entry.124074799=Read"},
{"title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "author": "Daniel Commey and Rebecca A. Sarpong and Griffith S. Klogo and Winful Bagyl-Bac and Garth V. Crosby", "abstract": "  Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.\n", "link": "http://arxiv.org/abs/2507.12439v1", "date": "2025-07-16", "relevancy": 1.8179, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20Incentive%20Mechanism%20for%20Poison-Resilient%20Federated%20Learning&body=Title%3A%20A%20Bayesian%20Incentive%20Mechanism%20for%20Poison-Resilient%20Federated%20Learning%0AAuthor%3A%20Daniel%20Commey%20and%20Rebecca%20A.%20Sarpong%20and%20Griffith%20S.%20Klogo%20and%20Winful%20Bagyl-Bac%20and%20Garth%20V.%20Crosby%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adecentralized%20clients%20while%20preserving%20data%20privacy.%20However%2C%20its%0Aopen-participation%20nature%20exposes%20it%20to%20data-poisoning%20attacks%2C%20in%20which%0Amalicious%20actors%20submit%20corrupted%20model%20updates%20to%20degrade%20the%20global%20model.%0AExisting%20defenses%20are%20often%20reactive%2C%20relying%20on%20statistical%20aggregation%20rules%0Athat%20can%20be%20computationally%20expensive%20and%20that%20typically%20assume%20an%20honest%0Amajority.%20This%20paper%20introduces%20a%20proactive%2C%20economic%20defense%3A%20a%20lightweight%0ABayesian%20incentive%20mechanism%20that%20makes%20malicious%20behavior%20economically%0Airrational.%20Each%20training%20round%20is%20modeled%20as%20a%20Bayesian%20game%20of%20incomplete%0Ainformation%20in%20which%20the%20server%2C%20acting%20as%20the%20principal%2C%20uses%20a%20small%2C%20private%0Avalidation%20dataset%20to%20verify%20update%20quality%20before%20issuing%20payments.%20The%20design%0Asatisfies%20Individual%20Rationality%20%28IR%29%20for%20benevolent%20clients%2C%20ensuring%20their%0Aparticipation%20is%20profitable%2C%20and%20Incentive%20Compatibility%20%28IC%29%2C%20making%20poisoning%0Aan%20economically%20dominated%20strategy.%20Extensive%20experiments%20on%20non-IID%20partitions%0Aof%20MNIST%20and%20FashionMNIST%20demonstrate%20robustness%3A%20with%2050%25%20label-flipping%0Aadversaries%20on%20MNIST%2C%20the%20mechanism%20maintains%2096.7%25%20accuracy%2C%20only%200.3%0Apercentage%20points%20lower%20than%20in%20a%20scenario%20with%2030%25%20label-flipping%20adversaries.%0AThis%20outcome%20is%2051.7%20percentage%20points%20better%20than%20standard%20FedAvg%2C%20which%0Acollapses%20under%20the%20same%2050%25%20attack.%20The%20mechanism%20is%20computationally%20light%2C%0Abudget-bounded%2C%20and%20readily%20integrates%20into%20existing%20FL%20frameworks%2C%20offering%20a%0Apractical%20route%20to%20economically%20robust%20and%20sustainable%20FL%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bayesian%2520Incentive%2520Mechanism%2520for%2520Poison-Resilient%2520Federated%2520Learning%26entry.906535625%3DDaniel%2520Commey%2520and%2520Rebecca%2520A.%2520Sarpong%2520and%2520Griffith%2520S.%2520Klogo%2520and%2520Winful%2520Bagyl-Bac%2520and%2520Garth%2520V.%2520Crosby%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adecentralized%2520clients%2520while%2520preserving%2520data%2520privacy.%2520However%252C%2520its%250Aopen-participation%2520nature%2520exposes%2520it%2520to%2520data-poisoning%2520attacks%252C%2520in%2520which%250Amalicious%2520actors%2520submit%2520corrupted%2520model%2520updates%2520to%2520degrade%2520the%2520global%2520model.%250AExisting%2520defenses%2520are%2520often%2520reactive%252C%2520relying%2520on%2520statistical%2520aggregation%2520rules%250Athat%2520can%2520be%2520computationally%2520expensive%2520and%2520that%2520typically%2520assume%2520an%2520honest%250Amajority.%2520This%2520paper%2520introduces%2520a%2520proactive%252C%2520economic%2520defense%253A%2520a%2520lightweight%250ABayesian%2520incentive%2520mechanism%2520that%2520makes%2520malicious%2520behavior%2520economically%250Airrational.%2520Each%2520training%2520round%2520is%2520modeled%2520as%2520a%2520Bayesian%2520game%2520of%2520incomplete%250Ainformation%2520in%2520which%2520the%2520server%252C%2520acting%2520as%2520the%2520principal%252C%2520uses%2520a%2520small%252C%2520private%250Avalidation%2520dataset%2520to%2520verify%2520update%2520quality%2520before%2520issuing%2520payments.%2520The%2520design%250Asatisfies%2520Individual%2520Rationality%2520%2528IR%2529%2520for%2520benevolent%2520clients%252C%2520ensuring%2520their%250Aparticipation%2520is%2520profitable%252C%2520and%2520Incentive%2520Compatibility%2520%2528IC%2529%252C%2520making%2520poisoning%250Aan%2520economically%2520dominated%2520strategy.%2520Extensive%2520experiments%2520on%2520non-IID%2520partitions%250Aof%2520MNIST%2520and%2520FashionMNIST%2520demonstrate%2520robustness%253A%2520with%252050%2525%2520label-flipping%250Aadversaries%2520on%2520MNIST%252C%2520the%2520mechanism%2520maintains%252096.7%2525%2520accuracy%252C%2520only%25200.3%250Apercentage%2520points%2520lower%2520than%2520in%2520a%2520scenario%2520with%252030%2525%2520label-flipping%2520adversaries.%250AThis%2520outcome%2520is%252051.7%2520percentage%2520points%2520better%2520than%2520standard%2520FedAvg%252C%2520which%250Acollapses%2520under%2520the%2520same%252050%2525%2520attack.%2520The%2520mechanism%2520is%2520computationally%2520light%252C%250Abudget-bounded%252C%2520and%2520readily%2520integrates%2520into%2520existing%2520FL%2520frameworks%252C%2520offering%2520a%250Apractical%2520route%2520to%2520economically%2520robust%2520and%2520sustainable%2520FL%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20Incentive%20Mechanism%20for%20Poison-Resilient%20Federated%20Learning&entry.906535625=Daniel%20Commey%20and%20Rebecca%20A.%20Sarpong%20and%20Griffith%20S.%20Klogo%20and%20Winful%20Bagyl-Bac%20and%20Garth%20V.%20Crosby&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adecentralized%20clients%20while%20preserving%20data%20privacy.%20However%2C%20its%0Aopen-participation%20nature%20exposes%20it%20to%20data-poisoning%20attacks%2C%20in%20which%0Amalicious%20actors%20submit%20corrupted%20model%20updates%20to%20degrade%20the%20global%20model.%0AExisting%20defenses%20are%20often%20reactive%2C%20relying%20on%20statistical%20aggregation%20rules%0Athat%20can%20be%20computationally%20expensive%20and%20that%20typically%20assume%20an%20honest%0Amajority.%20This%20paper%20introduces%20a%20proactive%2C%20economic%20defense%3A%20a%20lightweight%0ABayesian%20incentive%20mechanism%20that%20makes%20malicious%20behavior%20economically%0Airrational.%20Each%20training%20round%20is%20modeled%20as%20a%20Bayesian%20game%20of%20incomplete%0Ainformation%20in%20which%20the%20server%2C%20acting%20as%20the%20principal%2C%20uses%20a%20small%2C%20private%0Avalidation%20dataset%20to%20verify%20update%20quality%20before%20issuing%20payments.%20The%20design%0Asatisfies%20Individual%20Rationality%20%28IR%29%20for%20benevolent%20clients%2C%20ensuring%20their%0Aparticipation%20is%20profitable%2C%20and%20Incentive%20Compatibility%20%28IC%29%2C%20making%20poisoning%0Aan%20economically%20dominated%20strategy.%20Extensive%20experiments%20on%20non-IID%20partitions%0Aof%20MNIST%20and%20FashionMNIST%20demonstrate%20robustness%3A%20with%2050%25%20label-flipping%0Aadversaries%20on%20MNIST%2C%20the%20mechanism%20maintains%2096.7%25%20accuracy%2C%20only%200.3%0Apercentage%20points%20lower%20than%20in%20a%20scenario%20with%2030%25%20label-flipping%20adversaries.%0AThis%20outcome%20is%2051.7%20percentage%20points%20better%20than%20standard%20FedAvg%2C%20which%0Acollapses%20under%20the%20same%2050%25%20attack.%20The%20mechanism%20is%20computationally%20light%2C%0Abudget-bounded%2C%20and%20readily%20integrates%20into%20existing%20FL%20frameworks%2C%20offering%20a%0Apractical%20route%20to%20economically%20robust%20and%20sustainable%20FL%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12439v1&entry.124074799=Read"},
{"title": "MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable\n  Player in Basketball", "author": "Haifeng Sun and Yu Xiong and Runze Wu and Kai Wang and Lan Zhang and Changjie Fan and Shaojie Tang and Xiang-Yang Li", "abstract": "  The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry.\n", "link": "http://arxiv.org/abs/2506.04602v2", "date": "2025-07-16", "relevancy": 1.7712, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVP-Shapley%3A%20Feature-based%20Modeling%20for%20Evaluating%20the%20Most%20Valuable%0A%20%20Player%20in%20Basketball&body=Title%3A%20MVP-Shapley%3A%20Feature-based%20Modeling%20for%20Evaluating%20the%20Most%20Valuable%0A%20%20Player%20in%20Basketball%0AAuthor%3A%20Haifeng%20Sun%20and%20Yu%20Xiong%20and%20Runze%20Wu%20and%20Kai%20Wang%20and%20Lan%20Zhang%20and%20Changjie%20Fan%20and%20Shaojie%20Tang%20and%20Xiang-Yang%20Li%0AAbstract%3A%20%20%20The%20burgeoning%20growth%20of%20the%20esports%20and%20multiplayer%20online%20gaming%20community%0Ahas%20highlighted%20the%20critical%20importance%20of%20evaluating%20the%20Most%20Valuable%20Player%0A%28MVP%29.%20The%20establishment%20of%20an%20explainable%20and%20practical%20MVP%20evaluation%20method%0Ais%20very%20challenging.%20In%20our%20study%2C%20we%20specifically%20focus%20on%20play-by-play%20data%2C%0Awhich%20records%20related%20events%20during%20the%20game%2C%20such%20as%20assists%20and%20points.%20We%0Aaim%20to%20address%20the%20challenges%20by%20introducing%20a%20new%20MVP%20evaluation%20framework%2C%0Adenoted%20as%20%5Coursys%2C%20which%20leverages%20Shapley%20values.%20This%20approach%20encompasses%0Afeature%20processing%2C%20win-loss%20model%20training%2C%20Shapley%20value%20allocation%2C%20and%20MVP%0Aranking%20determination%20based%20on%20players%27%20contributions.%20Additionally%2C%20we%0Aoptimize%20our%20algorithm%20to%20align%20with%20expert%20voting%20results%20from%20the%20perspective%0Aof%20causality.%20Finally%2C%20we%20substantiated%20the%20efficacy%20of%20our%20method%20through%0Avalidation%20using%20the%20NBA%20dataset%20and%20the%20Dunk%20City%20Dynasty%20dataset%20and%0Aimplemented%20online%20deployment%20in%20the%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVP-Shapley%253A%2520Feature-based%2520Modeling%2520for%2520Evaluating%2520the%2520Most%2520Valuable%250A%2520%2520Player%2520in%2520Basketball%26entry.906535625%3DHaifeng%2520Sun%2520and%2520Yu%2520Xiong%2520and%2520Runze%2520Wu%2520and%2520Kai%2520Wang%2520and%2520Lan%2520Zhang%2520and%2520Changjie%2520Fan%2520and%2520Shaojie%2520Tang%2520and%2520Xiang-Yang%2520Li%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520growth%2520of%2520the%2520esports%2520and%2520multiplayer%2520online%2520gaming%2520community%250Ahas%2520highlighted%2520the%2520critical%2520importance%2520of%2520evaluating%2520the%2520Most%2520Valuable%2520Player%250A%2528MVP%2529.%2520The%2520establishment%2520of%2520an%2520explainable%2520and%2520practical%2520MVP%2520evaluation%2520method%250Ais%2520very%2520challenging.%2520In%2520our%2520study%252C%2520we%2520specifically%2520focus%2520on%2520play-by-play%2520data%252C%250Awhich%2520records%2520related%2520events%2520during%2520the%2520game%252C%2520such%2520as%2520assists%2520and%2520points.%2520We%250Aaim%2520to%2520address%2520the%2520challenges%2520by%2520introducing%2520a%2520new%2520MVP%2520evaluation%2520framework%252C%250Adenoted%2520as%2520%255Coursys%252C%2520which%2520leverages%2520Shapley%2520values.%2520This%2520approach%2520encompasses%250Afeature%2520processing%252C%2520win-loss%2520model%2520training%252C%2520Shapley%2520value%2520allocation%252C%2520and%2520MVP%250Aranking%2520determination%2520based%2520on%2520players%2527%2520contributions.%2520Additionally%252C%2520we%250Aoptimize%2520our%2520algorithm%2520to%2520align%2520with%2520expert%2520voting%2520results%2520from%2520the%2520perspective%250Aof%2520causality.%2520Finally%252C%2520we%2520substantiated%2520the%2520efficacy%2520of%2520our%2520method%2520through%250Avalidation%2520using%2520the%2520NBA%2520dataset%2520and%2520the%2520Dunk%2520City%2520Dynasty%2520dataset%2520and%250Aimplemented%2520online%2520deployment%2520in%2520the%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVP-Shapley%3A%20Feature-based%20Modeling%20for%20Evaluating%20the%20Most%20Valuable%0A%20%20Player%20in%20Basketball&entry.906535625=Haifeng%20Sun%20and%20Yu%20Xiong%20and%20Runze%20Wu%20and%20Kai%20Wang%20and%20Lan%20Zhang%20and%20Changjie%20Fan%20and%20Shaojie%20Tang%20and%20Xiang-Yang%20Li&entry.1292438233=%20%20The%20burgeoning%20growth%20of%20the%20esports%20and%20multiplayer%20online%20gaming%20community%0Ahas%20highlighted%20the%20critical%20importance%20of%20evaluating%20the%20Most%20Valuable%20Player%0A%28MVP%29.%20The%20establishment%20of%20an%20explainable%20and%20practical%20MVP%20evaluation%20method%0Ais%20very%20challenging.%20In%20our%20study%2C%20we%20specifically%20focus%20on%20play-by-play%20data%2C%0Awhich%20records%20related%20events%20during%20the%20game%2C%20such%20as%20assists%20and%20points.%20We%0Aaim%20to%20address%20the%20challenges%20by%20introducing%20a%20new%20MVP%20evaluation%20framework%2C%0Adenoted%20as%20%5Coursys%2C%20which%20leverages%20Shapley%20values.%20This%20approach%20encompasses%0Afeature%20processing%2C%20win-loss%20model%20training%2C%20Shapley%20value%20allocation%2C%20and%20MVP%0Aranking%20determination%20based%20on%20players%27%20contributions.%20Additionally%2C%20we%0Aoptimize%20our%20algorithm%20to%20align%20with%20expert%20voting%20results%20from%20the%20perspective%0Aof%20causality.%20Finally%2C%20we%20substantiated%20the%20efficacy%20of%20our%20method%20through%0Avalidation%20using%20the%20NBA%20dataset%20and%20the%20Dunk%20City%20Dynasty%20dataset%20and%0Aimplemented%20online%20deployment%20in%20the%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04602v2&entry.124074799=Read"},
{"title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with\n  Fine-Grained Categorization", "author": "Martin Ki\u0161\u0161 and Michal Hradi\u0161 and Martina Dvo\u0159\u00e1kov\u00e1 and V\u00e1clav Jirou\u0161ek and Filip Kersch", "abstract": "  We introduce the AnnoPage Dataset, a novel collection of 7,550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.\n", "link": "http://arxiv.org/abs/2503.22526v3", "date": "2025-07-16", "relevancy": 1.7701, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.45}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4429}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnnoPage%20Dataset%3A%20Dataset%20of%20Non-Textual%20Elements%20in%20Documents%20with%0A%20%20Fine-Grained%20Categorization&body=Title%3A%20AnnoPage%20Dataset%3A%20Dataset%20of%20Non-Textual%20Elements%20in%20Documents%20with%0A%20%20Fine-Grained%20Categorization%0AAuthor%3A%20Martin%20Ki%C5%A1%C5%A1%20and%20Michal%20Hradi%C5%A1%20and%20Martina%20Dvo%C5%99%C3%A1kov%C3%A1%20and%20V%C3%A1clav%20Jirou%C5%A1ek%20and%20Filip%20Kersch%0AAbstract%3A%20%20%20We%20introduce%20the%20AnnoPage%20Dataset%2C%20a%20novel%20collection%20of%207%2C550%20pages%20from%0Ahistorical%20documents%2C%20primarily%20in%20Czech%20and%20German%2C%20spanning%20from%201485%20to%20the%0Apresent%2C%20focusing%20on%20the%20late%2019th%20and%20early%2020th%20centuries.%20The%20dataset%20is%0Adesigned%20to%20support%20research%20in%20document%20layout%20analysis%20and%20object%20detection.%0AEach%20page%20is%20annotated%20with%20axis-aligned%20bounding%20boxes%20%28AABB%29%20representing%0Aelements%20of%2025%20categories%20of%20non-textual%20elements%2C%20such%20as%20images%2C%20maps%2C%0Adecorative%20elements%2C%20or%20charts%2C%20following%20the%20Czech%20Methodology%20of%20image%0Adocument%20processing.%20The%20annotations%20were%20created%20by%20expert%20librarians%20to%0Aensure%20accuracy%20and%20consistency.%20The%20dataset%20also%20incorporates%20pages%20from%0Amultiple%2C%20mainly%20historical%2C%20document%20datasets%20to%20enhance%20variability%20and%0Amaintain%20continuity.%20The%20dataset%20is%20divided%20into%20development%20and%20test%20subsets%2C%0Awith%20the%20test%20set%20carefully%20selected%20to%20maintain%20the%20category%20distribution.%20We%0Aprovide%20baseline%20results%20using%20YOLO%20and%20DETR%20object%20detectors%2C%20offering%20a%0Areference%20point%20for%20future%20research.%20The%20AnnoPage%20Dataset%20is%20publicly%20available%0Aon%20Zenodo%20%28https%3A//doi.org/10.5281/zenodo.12788419%29%2C%20along%20with%20ground-truth%0Aannotations%20in%20YOLO%20format.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22526v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnoPage%2520Dataset%253A%2520Dataset%2520of%2520Non-Textual%2520Elements%2520in%2520Documents%2520with%250A%2520%2520Fine-Grained%2520Categorization%26entry.906535625%3DMartin%2520Ki%25C5%25A1%25C5%25A1%2520and%2520Michal%2520Hradi%25C5%25A1%2520and%2520Martina%2520Dvo%25C5%2599%25C3%25A1kov%25C3%25A1%2520and%2520V%25C3%25A1clav%2520Jirou%25C5%25A1ek%2520and%2520Filip%2520Kersch%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520AnnoPage%2520Dataset%252C%2520a%2520novel%2520collection%2520of%25207%252C550%2520pages%2520from%250Ahistorical%2520documents%252C%2520primarily%2520in%2520Czech%2520and%2520German%252C%2520spanning%2520from%25201485%2520to%2520the%250Apresent%252C%2520focusing%2520on%2520the%2520late%252019th%2520and%2520early%252020th%2520centuries.%2520The%2520dataset%2520is%250Adesigned%2520to%2520support%2520research%2520in%2520document%2520layout%2520analysis%2520and%2520object%2520detection.%250AEach%2520page%2520is%2520annotated%2520with%2520axis-aligned%2520bounding%2520boxes%2520%2528AABB%2529%2520representing%250Aelements%2520of%252025%2520categories%2520of%2520non-textual%2520elements%252C%2520such%2520as%2520images%252C%2520maps%252C%250Adecorative%2520elements%252C%2520or%2520charts%252C%2520following%2520the%2520Czech%2520Methodology%2520of%2520image%250Adocument%2520processing.%2520The%2520annotations%2520were%2520created%2520by%2520expert%2520librarians%2520to%250Aensure%2520accuracy%2520and%2520consistency.%2520The%2520dataset%2520also%2520incorporates%2520pages%2520from%250Amultiple%252C%2520mainly%2520historical%252C%2520document%2520datasets%2520to%2520enhance%2520variability%2520and%250Amaintain%2520continuity.%2520The%2520dataset%2520is%2520divided%2520into%2520development%2520and%2520test%2520subsets%252C%250Awith%2520the%2520test%2520set%2520carefully%2520selected%2520to%2520maintain%2520the%2520category%2520distribution.%2520We%250Aprovide%2520baseline%2520results%2520using%2520YOLO%2520and%2520DETR%2520object%2520detectors%252C%2520offering%2520a%250Areference%2520point%2520for%2520future%2520research.%2520The%2520AnnoPage%2520Dataset%2520is%2520publicly%2520available%250Aon%2520Zenodo%2520%2528https%253A//doi.org/10.5281/zenodo.12788419%2529%252C%2520along%2520with%2520ground-truth%250Aannotations%2520in%2520YOLO%2520format.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22526v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnnoPage%20Dataset%3A%20Dataset%20of%20Non-Textual%20Elements%20in%20Documents%20with%0A%20%20Fine-Grained%20Categorization&entry.906535625=Martin%20Ki%C5%A1%C5%A1%20and%20Michal%20Hradi%C5%A1%20and%20Martina%20Dvo%C5%99%C3%A1kov%C3%A1%20and%20V%C3%A1clav%20Jirou%C5%A1ek%20and%20Filip%20Kersch&entry.1292438233=%20%20We%20introduce%20the%20AnnoPage%20Dataset%2C%20a%20novel%20collection%20of%207%2C550%20pages%20from%0Ahistorical%20documents%2C%20primarily%20in%20Czech%20and%20German%2C%20spanning%20from%201485%20to%20the%0Apresent%2C%20focusing%20on%20the%20late%2019th%20and%20early%2020th%20centuries.%20The%20dataset%20is%0Adesigned%20to%20support%20research%20in%20document%20layout%20analysis%20and%20object%20detection.%0AEach%20page%20is%20annotated%20with%20axis-aligned%20bounding%20boxes%20%28AABB%29%20representing%0Aelements%20of%2025%20categories%20of%20non-textual%20elements%2C%20such%20as%20images%2C%20maps%2C%0Adecorative%20elements%2C%20or%20charts%2C%20following%20the%20Czech%20Methodology%20of%20image%0Adocument%20processing.%20The%20annotations%20were%20created%20by%20expert%20librarians%20to%0Aensure%20accuracy%20and%20consistency.%20The%20dataset%20also%20incorporates%20pages%20from%0Amultiple%2C%20mainly%20historical%2C%20document%20datasets%20to%20enhance%20variability%20and%0Amaintain%20continuity.%20The%20dataset%20is%20divided%20into%20development%20and%20test%20subsets%2C%0Awith%20the%20test%20set%20carefully%20selected%20to%20maintain%20the%20category%20distribution.%20We%0Aprovide%20baseline%20results%20using%20YOLO%20and%20DETR%20object%20detectors%2C%20offering%20a%0Areference%20point%20for%20future%20research.%20The%20AnnoPage%20Dataset%20is%20publicly%20available%0Aon%20Zenodo%20%28https%3A//doi.org/10.5281/zenodo.12788419%29%2C%20along%20with%20ground-truth%0Aannotations%20in%20YOLO%20format.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22526v3&entry.124074799=Read"},
{"title": "Design and Development of an Automated Contact Angle Tester (ACAT) for\n  Surface Wettability Measurement", "author": "Connor Burgess and Kyle Douin and Amir Kordijazi", "abstract": "  The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform.\n", "link": "http://arxiv.org/abs/2507.12431v1", "date": "2025-07-16", "relevancy": 1.7667, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4394}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Development%20of%20an%20Automated%20Contact%20Angle%20Tester%20%28ACAT%29%20for%0A%20%20Surface%20Wettability%20Measurement&body=Title%3A%20Design%20and%20Development%20of%20an%20Automated%20Contact%20Angle%20Tester%20%28ACAT%29%20for%0A%20%20Surface%20Wettability%20Measurement%0AAuthor%3A%20Connor%20Burgess%20and%20Kyle%20Douin%20and%20Amir%20Kordijazi%0AAbstract%3A%20%20%20The%20Automated%20Contact%20Angle%20Tester%20%28ACAT%29%20is%20a%20fully%20integrated%20robotic%20work%0Acell%20developed%20to%20automate%20the%20measurement%20of%20surface%20wettability%20on%203D-printed%0Amaterials.%20Designed%20for%20precision%2C%20repeatability%2C%20and%20safety%2C%20ACAT%20addresses%0Athe%20limitations%20of%20manual%20contact%20angle%20testing%20by%20combining%20programmable%0Arobotics%2C%20precise%20liquid%20dispensing%2C%20and%20a%20modular%20software-hardware%0Aarchitecture.%20The%20system%20is%20composed%20of%20three%20core%20subsystems%3A%20%281%29%20an%0Aelectrical%20system%20including%20power%2C%20control%2C%20and%20safety%20circuits%20compliant%20with%0Aindustrial%20standards%20such%20as%20NEC%2070%2C%20NFPA%2079%2C%20and%20UL%20508A%3B%20%282%29%20a%20software%0Acontrol%20system%20based%20on%20a%20Raspberry%20Pi%20and%20Python%2C%20featuring%20fault%20detection%2C%0AGPIO%20logic%2C%20and%20operator%20interfaces%3B%20and%20%283%29%20a%20mechanical%20system%20that%20includes%0Aa%203-axis%20Cartesian%20robot%2C%20pneumatic%20actuation%2C%20and%20a%20precision%20liquid%20dispenser%0Aenclosed%20within%20a%20safety-certified%20frame.%20The%20ACAT%20enables%20high-throughput%2C%0Aautomated%20surface%20characterization%20and%20provides%20a%20robust%20platform%20for%20future%0Aintegration%20into%20smart%20manufacturing%20and%20materials%20discovery%20workflows.%20This%0Apaper%20details%20the%20design%20methodology%2C%20implementation%20strategies%2C%20and%20system%0Aintegration%20required%20to%20develop%20the%20ACAT%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Development%2520of%2520an%2520Automated%2520Contact%2520Angle%2520Tester%2520%2528ACAT%2529%2520for%250A%2520%2520Surface%2520Wettability%2520Measurement%26entry.906535625%3DConnor%2520Burgess%2520and%2520Kyle%2520Douin%2520and%2520Amir%2520Kordijazi%26entry.1292438233%3D%2520%2520The%2520Automated%2520Contact%2520Angle%2520Tester%2520%2528ACAT%2529%2520is%2520a%2520fully%2520integrated%2520robotic%2520work%250Acell%2520developed%2520to%2520automate%2520the%2520measurement%2520of%2520surface%2520wettability%2520on%25203D-printed%250Amaterials.%2520Designed%2520for%2520precision%252C%2520repeatability%252C%2520and%2520safety%252C%2520ACAT%2520addresses%250Athe%2520limitations%2520of%2520manual%2520contact%2520angle%2520testing%2520by%2520combining%2520programmable%250Arobotics%252C%2520precise%2520liquid%2520dispensing%252C%2520and%2520a%2520modular%2520software-hardware%250Aarchitecture.%2520The%2520system%2520is%2520composed%2520of%2520three%2520core%2520subsystems%253A%2520%25281%2529%2520an%250Aelectrical%2520system%2520including%2520power%252C%2520control%252C%2520and%2520safety%2520circuits%2520compliant%2520with%250Aindustrial%2520standards%2520such%2520as%2520NEC%252070%252C%2520NFPA%252079%252C%2520and%2520UL%2520508A%253B%2520%25282%2529%2520a%2520software%250Acontrol%2520system%2520based%2520on%2520a%2520Raspberry%2520Pi%2520and%2520Python%252C%2520featuring%2520fault%2520detection%252C%250AGPIO%2520logic%252C%2520and%2520operator%2520interfaces%253B%2520and%2520%25283%2529%2520a%2520mechanical%2520system%2520that%2520includes%250Aa%25203-axis%2520Cartesian%2520robot%252C%2520pneumatic%2520actuation%252C%2520and%2520a%2520precision%2520liquid%2520dispenser%250Aenclosed%2520within%2520a%2520safety-certified%2520frame.%2520The%2520ACAT%2520enables%2520high-throughput%252C%250Aautomated%2520surface%2520characterization%2520and%2520provides%2520a%2520robust%2520platform%2520for%2520future%250Aintegration%2520into%2520smart%2520manufacturing%2520and%2520materials%2520discovery%2520workflows.%2520This%250Apaper%2520details%2520the%2520design%2520methodology%252C%2520implementation%2520strategies%252C%2520and%2520system%250Aintegration%2520required%2520to%2520develop%2520the%2520ACAT%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Development%20of%20an%20Automated%20Contact%20Angle%20Tester%20%28ACAT%29%20for%0A%20%20Surface%20Wettability%20Measurement&entry.906535625=Connor%20Burgess%20and%20Kyle%20Douin%20and%20Amir%20Kordijazi&entry.1292438233=%20%20The%20Automated%20Contact%20Angle%20Tester%20%28ACAT%29%20is%20a%20fully%20integrated%20robotic%20work%0Acell%20developed%20to%20automate%20the%20measurement%20of%20surface%20wettability%20on%203D-printed%0Amaterials.%20Designed%20for%20precision%2C%20repeatability%2C%20and%20safety%2C%20ACAT%20addresses%0Athe%20limitations%20of%20manual%20contact%20angle%20testing%20by%20combining%20programmable%0Arobotics%2C%20precise%20liquid%20dispensing%2C%20and%20a%20modular%20software-hardware%0Aarchitecture.%20The%20system%20is%20composed%20of%20three%20core%20subsystems%3A%20%281%29%20an%0Aelectrical%20system%20including%20power%2C%20control%2C%20and%20safety%20circuits%20compliant%20with%0Aindustrial%20standards%20such%20as%20NEC%2070%2C%20NFPA%2079%2C%20and%20UL%20508A%3B%20%282%29%20a%20software%0Acontrol%20system%20based%20on%20a%20Raspberry%20Pi%20and%20Python%2C%20featuring%20fault%20detection%2C%0AGPIO%20logic%2C%20and%20operator%20interfaces%3B%20and%20%283%29%20a%20mechanical%20system%20that%20includes%0Aa%203-axis%20Cartesian%20robot%2C%20pneumatic%20actuation%2C%20and%20a%20precision%20liquid%20dispenser%0Aenclosed%20within%20a%20safety-certified%20frame.%20The%20ACAT%20enables%20high-throughput%2C%0Aautomated%20surface%20characterization%20and%20provides%20a%20robust%20platform%20for%20future%0Aintegration%20into%20smart%20manufacturing%20and%20materials%20discovery%20workflows.%20This%0Apaper%20details%20the%20design%20methodology%2C%20implementation%20strategies%2C%20and%20system%0Aintegration%20required%20to%20develop%20the%20ACAT%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12431v1&entry.124074799=Read"},
{"title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical\n  Image Segmentation", "author": "Kaiwen Huang and Yi Zhou and Huazhu Fu and Yizhe Zhang and Chen Gong and Tao Zhou", "abstract": "  Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.\n", "link": "http://arxiv.org/abs/2507.12382v1", "date": "2025-07-16", "relevancy": 1.7626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6155}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.56}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-driven%20Multiplanar%20Visual%20Interaction%20for%20Semi-supervised%20Medical%0A%20%20Image%20Segmentation&body=Title%3A%20Text-driven%20Multiplanar%20Visual%20Interaction%20for%20Semi-supervised%20Medical%0A%20%20Image%20Segmentation%0AAuthor%3A%20Kaiwen%20Huang%20and%20Yi%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Chen%20Gong%20and%20Tao%20Zhou%0AAbstract%3A%20%20%20Semi-supervised%20medical%20image%20segmentation%20is%20a%20crucial%20technique%20for%0Aalleviating%20the%20high%20cost%20of%20data%20annotation.%20When%20labeled%20data%20is%20limited%2C%0Atextual%20information%20can%20provide%20additional%20context%20to%20enhance%20visual%20semantic%0Aunderstanding.%20However%2C%20research%20exploring%20the%20use%20of%20textual%20data%20to%20enhance%0Avisual%20semantic%20embeddings%20in%203D%20medical%20imaging%20tasks%20remains%20scarce.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20text-driven%20multiplanar%20visual%20interaction%20framework%0Afor%20semi-supervised%20medical%20image%20segmentation%20%28termed%20Text-SemiSeg%29%2C%20which%0Aconsists%20of%20three%20main%20modules%3A%20Text-enhanced%20Multiplanar%20Representation%20%28TMR%29%2C%0ACategory-aware%20Semantic%20Alignment%20%28CSA%29%2C%20and%20Dynamic%20Cognitive%20Augmentation%0A%28DCA%29.%20Specifically%2C%20TMR%20facilitates%20text-visual%20interaction%20through%20planar%0Amapping%2C%20thereby%20enhancing%20the%20category%20awareness%20of%20visual%20features.%20CSA%0Aperforms%20cross-modal%20semantic%20alignment%20between%20the%20text%20features%20with%0Aintroduced%20learnable%20variables%20and%20the%20intermediate%20layer%20of%20visual%20features.%0ADCA%20reduces%20the%20distribution%20discrepancy%20between%20labeled%20and%20unlabeled%20data%0Athrough%20their%20interaction%2C%20thus%20improving%20the%20model%27s%20robustness.%20Finally%2C%0Aexperiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20model%20effectively%0Aenhances%20visual%20features%20with%20textual%20information%20and%20outperforms%20other%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/taozh2017/Text-SemiSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-driven%2520Multiplanar%2520Visual%2520Interaction%2520for%2520Semi-supervised%2520Medical%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DKaiwen%2520Huang%2520and%2520Yi%2520Zhou%2520and%2520Huazhu%2520Fu%2520and%2520Yizhe%2520Zhang%2520and%2520Chen%2520Gong%2520and%2520Tao%2520Zhou%26entry.1292438233%3D%2520%2520Semi-supervised%2520medical%2520image%2520segmentation%2520is%2520a%2520crucial%2520technique%2520for%250Aalleviating%2520the%2520high%2520cost%2520of%2520data%2520annotation.%2520When%2520labeled%2520data%2520is%2520limited%252C%250Atextual%2520information%2520can%2520provide%2520additional%2520context%2520to%2520enhance%2520visual%2520semantic%250Aunderstanding.%2520However%252C%2520research%2520exploring%2520the%2520use%2520of%2520textual%2520data%2520to%2520enhance%250Avisual%2520semantic%2520embeddings%2520in%25203D%2520medical%2520imaging%2520tasks%2520remains%2520scarce.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520text-driven%2520multiplanar%2520visual%2520interaction%2520framework%250Afor%2520semi-supervised%2520medical%2520image%2520segmentation%2520%2528termed%2520Text-SemiSeg%2529%252C%2520which%250Aconsists%2520of%2520three%2520main%2520modules%253A%2520Text-enhanced%2520Multiplanar%2520Representation%2520%2528TMR%2529%252C%250ACategory-aware%2520Semantic%2520Alignment%2520%2528CSA%2529%252C%2520and%2520Dynamic%2520Cognitive%2520Augmentation%250A%2528DCA%2529.%2520Specifically%252C%2520TMR%2520facilitates%2520text-visual%2520interaction%2520through%2520planar%250Amapping%252C%2520thereby%2520enhancing%2520the%2520category%2520awareness%2520of%2520visual%2520features.%2520CSA%250Aperforms%2520cross-modal%2520semantic%2520alignment%2520between%2520the%2520text%2520features%2520with%250Aintroduced%2520learnable%2520variables%2520and%2520the%2520intermediate%2520layer%2520of%2520visual%2520features.%250ADCA%2520reduces%2520the%2520distribution%2520discrepancy%2520between%2520labeled%2520and%2520unlabeled%2520data%250Athrough%2520their%2520interaction%252C%2520thus%2520improving%2520the%2520model%2527s%2520robustness.%2520Finally%252C%250Aexperiments%2520on%2520three%2520public%2520datasets%2520demonstrate%2520that%2520our%2520model%2520effectively%250Aenhances%2520visual%2520features%2520with%2520textual%2520information%2520and%2520outperforms%2520other%250Amethods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/taozh2017/Text-SemiSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-driven%20Multiplanar%20Visual%20Interaction%20for%20Semi-supervised%20Medical%0A%20%20Image%20Segmentation&entry.906535625=Kaiwen%20Huang%20and%20Yi%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Chen%20Gong%20and%20Tao%20Zhou&entry.1292438233=%20%20Semi-supervised%20medical%20image%20segmentation%20is%20a%20crucial%20technique%20for%0Aalleviating%20the%20high%20cost%20of%20data%20annotation.%20When%20labeled%20data%20is%20limited%2C%0Atextual%20information%20can%20provide%20additional%20context%20to%20enhance%20visual%20semantic%0Aunderstanding.%20However%2C%20research%20exploring%20the%20use%20of%20textual%20data%20to%20enhance%0Avisual%20semantic%20embeddings%20in%203D%20medical%20imaging%20tasks%20remains%20scarce.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20text-driven%20multiplanar%20visual%20interaction%20framework%0Afor%20semi-supervised%20medical%20image%20segmentation%20%28termed%20Text-SemiSeg%29%2C%20which%0Aconsists%20of%20three%20main%20modules%3A%20Text-enhanced%20Multiplanar%20Representation%20%28TMR%29%2C%0ACategory-aware%20Semantic%20Alignment%20%28CSA%29%2C%20and%20Dynamic%20Cognitive%20Augmentation%0A%28DCA%29.%20Specifically%2C%20TMR%20facilitates%20text-visual%20interaction%20through%20planar%0Amapping%2C%20thereby%20enhancing%20the%20category%20awareness%20of%20visual%20features.%20CSA%0Aperforms%20cross-modal%20semantic%20alignment%20between%20the%20text%20features%20with%0Aintroduced%20learnable%20variables%20and%20the%20intermediate%20layer%20of%20visual%20features.%0ADCA%20reduces%20the%20distribution%20discrepancy%20between%20labeled%20and%20unlabeled%20data%0Athrough%20their%20interaction%2C%20thus%20improving%20the%20model%27s%20robustness.%20Finally%2C%0Aexperiments%20on%20three%20public%20datasets%20demonstrate%20that%20our%20model%20effectively%0Aenhances%20visual%20features%20with%20textual%20information%20and%20outperforms%20other%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/taozh2017/Text-SemiSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12382v1&entry.124074799=Read"},
{"title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "author": "Florian E. Dorner and Yatong Chen and Andr\u00e9 F. Cruz and Fanny Yang", "abstract": "  Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.\n", "link": "http://arxiv.org/abs/2507.12399v1", "date": "2025-07-16", "relevancy": 1.7578, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4713}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4363}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROC-n-reroll%3A%20How%20verifier%20imperfection%20affects%20test-time%20scaling&body=Title%3A%20ROC-n-reroll%3A%20How%20verifier%20imperfection%20affects%20test-time%20scaling%0AAuthor%3A%20Florian%20E.%20Dorner%20and%20Yatong%20Chen%20and%20Andr%C3%A9%20F.%20Cruz%20and%20Fanny%20Yang%0AAbstract%3A%20%20%20Test-time%20scaling%20aims%20to%20improve%20language%20model%20performance%20by%20leveraging%0Aadditional%20compute%20during%20inference.%20While%20many%20works%20have%20empirically%20studied%0Atechniques%20like%20Best-of-N%20%28BoN%29%20and%20rejection%20sampling%20that%20make%20use%20of%20a%0Averifier%20to%20enable%20test-time%20scaling%2C%20there%20is%20little%20theoretical%20understanding%0Aof%20how%20verifier%20imperfection%20affects%20performance.%20In%20this%20work%2C%20we%20address%20this%0Agap.%20Specifically%2C%20we%20prove%20how%20instance-level%20accuracy%20of%20these%20methods%20is%0Aprecisely%20characterized%20by%20the%20geometry%20of%20the%20verifier%27s%20ROC%20curve.%0AInterestingly%2C%20while%20scaling%20is%20determined%20by%20the%20local%20geometry%20of%20the%20ROC%0Acurve%20for%20rejection%20sampling%2C%20it%20depends%20on%20global%20properties%20of%20the%20ROC%20curve%0Afor%20BoN.%20As%20a%20consequence%20when%20the%20ROC%20curve%20is%20unknown%2C%20it%20is%20impossible%20to%0Aextrapolate%20the%20performance%20of%20rejection%20sampling%20based%20on%20the%20low-compute%0Aregime.%20Furthermore%2C%20while%20rejection%20sampling%20outperforms%20BoN%20for%20fixed%0Acompute%2C%20in%20the%20infinite-compute%20limit%20both%20methods%20converge%20to%20the%20same%20level%0Aof%20accuracy%2C%20determined%20by%20the%20slope%20of%20the%20ROC%20curve%20near%20the%20origin.%20Our%0Atheoretical%20results%20are%20confirmed%20by%20experiments%20on%20GSM8K%20using%20different%0Aversions%20of%20Llama%20and%20Qwen%20to%20generate%20and%20verify%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROC-n-reroll%253A%2520How%2520verifier%2520imperfection%2520affects%2520test-time%2520scaling%26entry.906535625%3DFlorian%2520E.%2520Dorner%2520and%2520Yatong%2520Chen%2520and%2520Andr%25C3%25A9%2520F.%2520Cruz%2520and%2520Fanny%2520Yang%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520aims%2520to%2520improve%2520language%2520model%2520performance%2520by%2520leveraging%250Aadditional%2520compute%2520during%2520inference.%2520While%2520many%2520works%2520have%2520empirically%2520studied%250Atechniques%2520like%2520Best-of-N%2520%2528BoN%2529%2520and%2520rejection%2520sampling%2520that%2520make%2520use%2520of%2520a%250Averifier%2520to%2520enable%2520test-time%2520scaling%252C%2520there%2520is%2520little%2520theoretical%2520understanding%250Aof%2520how%2520verifier%2520imperfection%2520affects%2520performance.%2520In%2520this%2520work%252C%2520we%2520address%2520this%250Agap.%2520Specifically%252C%2520we%2520prove%2520how%2520instance-level%2520accuracy%2520of%2520these%2520methods%2520is%250Aprecisely%2520characterized%2520by%2520the%2520geometry%2520of%2520the%2520verifier%2527s%2520ROC%2520curve.%250AInterestingly%252C%2520while%2520scaling%2520is%2520determined%2520by%2520the%2520local%2520geometry%2520of%2520the%2520ROC%250Acurve%2520for%2520rejection%2520sampling%252C%2520it%2520depends%2520on%2520global%2520properties%2520of%2520the%2520ROC%2520curve%250Afor%2520BoN.%2520As%2520a%2520consequence%2520when%2520the%2520ROC%2520curve%2520is%2520unknown%252C%2520it%2520is%2520impossible%2520to%250Aextrapolate%2520the%2520performance%2520of%2520rejection%2520sampling%2520based%2520on%2520the%2520low-compute%250Aregime.%2520Furthermore%252C%2520while%2520rejection%2520sampling%2520outperforms%2520BoN%2520for%2520fixed%250Acompute%252C%2520in%2520the%2520infinite-compute%2520limit%2520both%2520methods%2520converge%2520to%2520the%2520same%2520level%250Aof%2520accuracy%252C%2520determined%2520by%2520the%2520slope%2520of%2520the%2520ROC%2520curve%2520near%2520the%2520origin.%2520Our%250Atheoretical%2520results%2520are%2520confirmed%2520by%2520experiments%2520on%2520GSM8K%2520using%2520different%250Aversions%2520of%2520Llama%2520and%2520Qwen%2520to%2520generate%2520and%2520verify%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROC-n-reroll%3A%20How%20verifier%20imperfection%20affects%20test-time%20scaling&entry.906535625=Florian%20E.%20Dorner%20and%20Yatong%20Chen%20and%20Andr%C3%A9%20F.%20Cruz%20and%20Fanny%20Yang&entry.1292438233=%20%20Test-time%20scaling%20aims%20to%20improve%20language%20model%20performance%20by%20leveraging%0Aadditional%20compute%20during%20inference.%20While%20many%20works%20have%20empirically%20studied%0Atechniques%20like%20Best-of-N%20%28BoN%29%20and%20rejection%20sampling%20that%20make%20use%20of%20a%0Averifier%20to%20enable%20test-time%20scaling%2C%20there%20is%20little%20theoretical%20understanding%0Aof%20how%20verifier%20imperfection%20affects%20performance.%20In%20this%20work%2C%20we%20address%20this%0Agap.%20Specifically%2C%20we%20prove%20how%20instance-level%20accuracy%20of%20these%20methods%20is%0Aprecisely%20characterized%20by%20the%20geometry%20of%20the%20verifier%27s%20ROC%20curve.%0AInterestingly%2C%20while%20scaling%20is%20determined%20by%20the%20local%20geometry%20of%20the%20ROC%0Acurve%20for%20rejection%20sampling%2C%20it%20depends%20on%20global%20properties%20of%20the%20ROC%20curve%0Afor%20BoN.%20As%20a%20consequence%20when%20the%20ROC%20curve%20is%20unknown%2C%20it%20is%20impossible%20to%0Aextrapolate%20the%20performance%20of%20rejection%20sampling%20based%20on%20the%20low-compute%0Aregime.%20Furthermore%2C%20while%20rejection%20sampling%20outperforms%20BoN%20for%20fixed%0Acompute%2C%20in%20the%20infinite-compute%20limit%20both%20methods%20converge%20to%20the%20same%20level%0Aof%20accuracy%2C%20determined%20by%20the%20slope%20of%20the%20ROC%20curve%20near%20the%20origin.%20Our%0Atheoretical%20results%20are%20confirmed%20by%20experiments%20on%20GSM8K%20using%20different%0Aversions%20of%20Llama%20and%20Qwen%20to%20generate%20and%20verify%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12399v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs.\n  Deep Learning", "author": "Joris Suurmeijer and Ivo Pascal de Jong and Matias Valdenegro-Toro and Andreea Ioana Sburlea", "abstract": "  Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.\n", "link": "http://arxiv.org/abs/2507.07511v2", "date": "2025-07-16", "relevancy": 1.755, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5867}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Motor%20Imagery%20BCI%20--%20Machine%20Learning%20vs.%0A%20%20Deep%20Learning&body=Title%3A%20Uncertainty%20Quantification%20for%20Motor%20Imagery%20BCI%20--%20Machine%20Learning%20vs.%0A%20%20Deep%20Learning%0AAuthor%3A%20Joris%20Suurmeijer%20and%20Ivo%20Pascal%20de%20Jong%20and%20Matias%20Valdenegro-Toro%20and%20Andreea%20Ioana%20Sburlea%0AAbstract%3A%20%20%20Brain-computer%20interfaces%20%28BCIs%29%20turn%20brain%20signals%20into%20functionally%20useful%0Aoutput%2C%20but%20they%20are%20not%20always%20accurate.%20A%20good%20Machine%20Learning%20classifier%0Ashould%20be%20able%20to%20indicate%20how%20confident%20it%20is%20about%20a%20given%20classification%2C%20by%0Agiving%20a%20probability%20for%20its%20classification.%20Standard%20classifiers%20for%20Motor%0AImagery%20BCIs%20do%20give%20such%20probabilities%2C%20but%20research%20on%20uncertainty%0Aquantification%20has%20been%20limited%20to%20Deep%20Learning.%20We%20compare%20the%20uncertainty%0Aquantification%20ability%20of%20established%20BCI%20classifiers%20using%20Common%20Spatial%0APatterns%20%28CSP-LDA%29%20and%20Riemannian%20Geometry%20%28MDRM%29%20to%20specialized%20methods%20in%0ADeep%20Learning%20%28Deep%20Ensembles%20and%20Direct%20Uncertainty%20Quantification%29%20as%20well%20as%0Astandard%20Convolutional%20Neural%20Networks%20%28CNNs%29.%0A%20%20We%20found%20that%20the%20overconfidence%20typically%20seen%20in%20Deep%20Learning%20is%20not%20a%0Aproblem%20in%20CSP-LDA%20and%20MDRM.%20We%20found%20that%20MDRM%20is%20underconfident%2C%20which%20we%0Asolved%20by%20adding%20Temperature%20Scaling%20%28MDRM-T%29.%20CSP-LDA%20and%20MDRM-T%20give%20the%20best%0Auncertainty%20estimates%2C%20but%20Deep%20Ensembles%20and%20standard%20CNNs%20give%20the%20best%0Aclassifications.%20We%20show%20that%20all%20models%20are%20able%20to%20separate%20between%20easy%20and%0Adifficult%20estimates%2C%20so%20that%20we%20can%20increase%20the%20accuracy%20of%20a%20Motor%20Imagery%0ABCI%20by%20rejecting%20samples%20that%20are%20ambiguous.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Motor%2520Imagery%2520BCI%2520--%2520Machine%2520Learning%2520vs.%250A%2520%2520Deep%2520Learning%26entry.906535625%3DJoris%2520Suurmeijer%2520and%2520Ivo%2520Pascal%2520de%2520Jong%2520and%2520Matias%2520Valdenegro-Toro%2520and%2520Andreea%2520Ioana%2520Sburlea%26entry.1292438233%3D%2520%2520Brain-computer%2520interfaces%2520%2528BCIs%2529%2520turn%2520brain%2520signals%2520into%2520functionally%2520useful%250Aoutput%252C%2520but%2520they%2520are%2520not%2520always%2520accurate.%2520A%2520good%2520Machine%2520Learning%2520classifier%250Ashould%2520be%2520able%2520to%2520indicate%2520how%2520confident%2520it%2520is%2520about%2520a%2520given%2520classification%252C%2520by%250Agiving%2520a%2520probability%2520for%2520its%2520classification.%2520Standard%2520classifiers%2520for%2520Motor%250AImagery%2520BCIs%2520do%2520give%2520such%2520probabilities%252C%2520but%2520research%2520on%2520uncertainty%250Aquantification%2520has%2520been%2520limited%2520to%2520Deep%2520Learning.%2520We%2520compare%2520the%2520uncertainty%250Aquantification%2520ability%2520of%2520established%2520BCI%2520classifiers%2520using%2520Common%2520Spatial%250APatterns%2520%2528CSP-LDA%2529%2520and%2520Riemannian%2520Geometry%2520%2528MDRM%2529%2520to%2520specialized%2520methods%2520in%250ADeep%2520Learning%2520%2528Deep%2520Ensembles%2520and%2520Direct%2520Uncertainty%2520Quantification%2529%2520as%2520well%2520as%250Astandard%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529.%250A%2520%2520We%2520found%2520that%2520the%2520overconfidence%2520typically%2520seen%2520in%2520Deep%2520Learning%2520is%2520not%2520a%250Aproblem%2520in%2520CSP-LDA%2520and%2520MDRM.%2520We%2520found%2520that%2520MDRM%2520is%2520underconfident%252C%2520which%2520we%250Asolved%2520by%2520adding%2520Temperature%2520Scaling%2520%2528MDRM-T%2529.%2520CSP-LDA%2520and%2520MDRM-T%2520give%2520the%2520best%250Auncertainty%2520estimates%252C%2520but%2520Deep%2520Ensembles%2520and%2520standard%2520CNNs%2520give%2520the%2520best%250Aclassifications.%2520We%2520show%2520that%2520all%2520models%2520are%2520able%2520to%2520separate%2520between%2520easy%2520and%250Adifficult%2520estimates%252C%2520so%2520that%2520we%2520can%2520increase%2520the%2520accuracy%2520of%2520a%2520Motor%2520Imagery%250ABCI%2520by%2520rejecting%2520samples%2520that%2520are%2520ambiguous.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Motor%20Imagery%20BCI%20--%20Machine%20Learning%20vs.%0A%20%20Deep%20Learning&entry.906535625=Joris%20Suurmeijer%20and%20Ivo%20Pascal%20de%20Jong%20and%20Matias%20Valdenegro-Toro%20and%20Andreea%20Ioana%20Sburlea&entry.1292438233=%20%20Brain-computer%20interfaces%20%28BCIs%29%20turn%20brain%20signals%20into%20functionally%20useful%0Aoutput%2C%20but%20they%20are%20not%20always%20accurate.%20A%20good%20Machine%20Learning%20classifier%0Ashould%20be%20able%20to%20indicate%20how%20confident%20it%20is%20about%20a%20given%20classification%2C%20by%0Agiving%20a%20probability%20for%20its%20classification.%20Standard%20classifiers%20for%20Motor%0AImagery%20BCIs%20do%20give%20such%20probabilities%2C%20but%20research%20on%20uncertainty%0Aquantification%20has%20been%20limited%20to%20Deep%20Learning.%20We%20compare%20the%20uncertainty%0Aquantification%20ability%20of%20established%20BCI%20classifiers%20using%20Common%20Spatial%0APatterns%20%28CSP-LDA%29%20and%20Riemannian%20Geometry%20%28MDRM%29%20to%20specialized%20methods%20in%0ADeep%20Learning%20%28Deep%20Ensembles%20and%20Direct%20Uncertainty%20Quantification%29%20as%20well%20as%0Astandard%20Convolutional%20Neural%20Networks%20%28CNNs%29.%0A%20%20We%20found%20that%20the%20overconfidence%20typically%20seen%20in%20Deep%20Learning%20is%20not%20a%0Aproblem%20in%20CSP-LDA%20and%20MDRM.%20We%20found%20that%20MDRM%20is%20underconfident%2C%20which%20we%0Asolved%20by%20adding%20Temperature%20Scaling%20%28MDRM-T%29.%20CSP-LDA%20and%20MDRM-T%20give%20the%20best%0Auncertainty%20estimates%2C%20but%20Deep%20Ensembles%20and%20standard%20CNNs%20give%20the%20best%0Aclassifications.%20We%20show%20that%20all%20models%20are%20able%20to%20separate%20between%20easy%20and%0Adifficult%20estimates%2C%20so%20that%20we%20can%20increase%20the%20accuracy%20of%20a%20Motor%20Imagery%0ABCI%20by%20rejecting%20samples%20that%20are%20ambiguous.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07511v2&entry.124074799=Read"},
{"title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "author": "Zihao Xue and Zhen Bi and Long Ma and Zhenlin Hu and Yan Wang and Zhenfang Liu and Qing Sheng and Jie Xiao and Jungang Lou", "abstract": "  While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.\n", "link": "http://arxiv.org/abs/2507.12314v1", "date": "2025-07-16", "relevancy": 1.7334, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4367}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thought%20Purity%3A%20Defense%20Paradigm%20For%20Chain-of-Thought%20Attack&body=Title%3A%20Thought%20Purity%3A%20Defense%20Paradigm%20For%20Chain-of-Thought%20Attack%0AAuthor%3A%20Zihao%20Xue%20and%20Zhen%20Bi%20and%20Long%20Ma%20and%20Zhenlin%20Hu%20and%20Yan%20Wang%20and%20Zhenfang%20Liu%20and%20Qing%20Sheng%20and%20Jie%20Xiao%20and%20Jungang%20Lou%0AAbstract%3A%20%20%20While%20reinforcement%20learning-trained%20Large%20Reasoning%20Models%20%28LRMs%2C%20e.g.%2C%0ADeepseek-R1%29%20demonstrate%20advanced%20reasoning%20capabilities%20in%20the%20evolving%20Large%0ALanguage%20Models%20%28LLMs%29%20domain%2C%20their%20susceptibility%20to%20security%20threats%20remains%0Aa%20critical%20vulnerability.%20This%20weakness%20is%20particularly%20evident%20in%0AChain-of-Thought%20%28CoT%29%20generation%20processes%2C%20where%20adversarial%20methods%20like%0Abackdoor%20prompt%20attacks%20can%20systematically%20subvert%20the%20model%27s%20core%20reasoning%0Amechanisms.%20The%20emerging%20Chain-of-Thought%20Attack%20%28CoTA%29%20reveals%20this%0Avulnerability%20through%20exploiting%20prompt%20controllability%2C%20simultaneously%0Adegrading%20both%20CoT%20safety%20and%20task%20performance%20with%20low-cost%20interventions.%20To%0Aaddress%20this%20compounded%20security-performance%20vulnerability%2C%20we%20propose%20Thought%0APurity%20%28TP%29%3A%20a%20defense%20paradigm%20that%20systematically%20strengthens%20resistance%20to%0Amalicious%20content%20while%20preserving%20operational%20efficacy.%20Our%20solution%20achieves%0Athis%20through%20three%20synergistic%20components%3A%20%281%29%20a%20safety-optimized%20data%0Aprocessing%20pipeline%20%282%29%20reinforcement%20learning-enhanced%20rule%20constraints%20%283%29%0Aadaptive%20monitoring%20metrics.%20Our%20approach%20establishes%20the%20first%20comprehensive%0Adefense%20mechanism%20against%20CoTA%20vulnerabilities%20in%20reinforcement%0Alearning-aligned%20reasoning%20systems%2C%20significantly%20advancing%20the%0Asecurity-functionality%20equilibrium%20for%20next-generation%20AI%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThought%2520Purity%253A%2520Defense%2520Paradigm%2520For%2520Chain-of-Thought%2520Attack%26entry.906535625%3DZihao%2520Xue%2520and%2520Zhen%2520Bi%2520and%2520Long%2520Ma%2520and%2520Zhenlin%2520Hu%2520and%2520Yan%2520Wang%2520and%2520Zhenfang%2520Liu%2520and%2520Qing%2520Sheng%2520and%2520Jie%2520Xiao%2520and%2520Jungang%2520Lou%26entry.1292438233%3D%2520%2520While%2520reinforcement%2520learning-trained%2520Large%2520Reasoning%2520Models%2520%2528LRMs%252C%2520e.g.%252C%250ADeepseek-R1%2529%2520demonstrate%2520advanced%2520reasoning%2520capabilities%2520in%2520the%2520evolving%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520domain%252C%2520their%2520susceptibility%2520to%2520security%2520threats%2520remains%250Aa%2520critical%2520vulnerability.%2520This%2520weakness%2520is%2520particularly%2520evident%2520in%250AChain-of-Thought%2520%2528CoT%2529%2520generation%2520processes%252C%2520where%2520adversarial%2520methods%2520like%250Abackdoor%2520prompt%2520attacks%2520can%2520systematically%2520subvert%2520the%2520model%2527s%2520core%2520reasoning%250Amechanisms.%2520The%2520emerging%2520Chain-of-Thought%2520Attack%2520%2528CoTA%2529%2520reveals%2520this%250Avulnerability%2520through%2520exploiting%2520prompt%2520controllability%252C%2520simultaneously%250Adegrading%2520both%2520CoT%2520safety%2520and%2520task%2520performance%2520with%2520low-cost%2520interventions.%2520To%250Aaddress%2520this%2520compounded%2520security-performance%2520vulnerability%252C%2520we%2520propose%2520Thought%250APurity%2520%2528TP%2529%253A%2520a%2520defense%2520paradigm%2520that%2520systematically%2520strengthens%2520resistance%2520to%250Amalicious%2520content%2520while%2520preserving%2520operational%2520efficacy.%2520Our%2520solution%2520achieves%250Athis%2520through%2520three%2520synergistic%2520components%253A%2520%25281%2529%2520a%2520safety-optimized%2520data%250Aprocessing%2520pipeline%2520%25282%2529%2520reinforcement%2520learning-enhanced%2520rule%2520constraints%2520%25283%2529%250Aadaptive%2520monitoring%2520metrics.%2520Our%2520approach%2520establishes%2520the%2520first%2520comprehensive%250Adefense%2520mechanism%2520against%2520CoTA%2520vulnerabilities%2520in%2520reinforcement%250Alearning-aligned%2520reasoning%2520systems%252C%2520significantly%2520advancing%2520the%250Asecurity-functionality%2520equilibrium%2520for%2520next-generation%2520AI%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thought%20Purity%3A%20Defense%20Paradigm%20For%20Chain-of-Thought%20Attack&entry.906535625=Zihao%20Xue%20and%20Zhen%20Bi%20and%20Long%20Ma%20and%20Zhenlin%20Hu%20and%20Yan%20Wang%20and%20Zhenfang%20Liu%20and%20Qing%20Sheng%20and%20Jie%20Xiao%20and%20Jungang%20Lou&entry.1292438233=%20%20While%20reinforcement%20learning-trained%20Large%20Reasoning%20Models%20%28LRMs%2C%20e.g.%2C%0ADeepseek-R1%29%20demonstrate%20advanced%20reasoning%20capabilities%20in%20the%20evolving%20Large%0ALanguage%20Models%20%28LLMs%29%20domain%2C%20their%20susceptibility%20to%20security%20threats%20remains%0Aa%20critical%20vulnerability.%20This%20weakness%20is%20particularly%20evident%20in%0AChain-of-Thought%20%28CoT%29%20generation%20processes%2C%20where%20adversarial%20methods%20like%0Abackdoor%20prompt%20attacks%20can%20systematically%20subvert%20the%20model%27s%20core%20reasoning%0Amechanisms.%20The%20emerging%20Chain-of-Thought%20Attack%20%28CoTA%29%20reveals%20this%0Avulnerability%20through%20exploiting%20prompt%20controllability%2C%20simultaneously%0Adegrading%20both%20CoT%20safety%20and%20task%20performance%20with%20low-cost%20interventions.%20To%0Aaddress%20this%20compounded%20security-performance%20vulnerability%2C%20we%20propose%20Thought%0APurity%20%28TP%29%3A%20a%20defense%20paradigm%20that%20systematically%20strengthens%20resistance%20to%0Amalicious%20content%20while%20preserving%20operational%20efficacy.%20Our%20solution%20achieves%0Athis%20through%20three%20synergistic%20components%3A%20%281%29%20a%20safety-optimized%20data%0Aprocessing%20pipeline%20%282%29%20reinforcement%20learning-enhanced%20rule%20constraints%20%283%29%0Aadaptive%20monitoring%20metrics.%20Our%20approach%20establishes%20the%20first%20comprehensive%0Adefense%20mechanism%20against%20CoTA%20vulnerabilities%20in%20reinforcement%0Alearning-aligned%20reasoning%20systems%2C%20significantly%20advancing%20the%0Asecurity-functionality%20equilibrium%20for%20next-generation%20AI%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12314v1&entry.124074799=Read"},
{"title": "Cost-aware Stopping for Bayesian Optimization", "author": "Qian Xie and Linda Cai and Alexander Terenin and Peter I. Frazier and Ziv Scully", "abstract": "  In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.\n", "link": "http://arxiv.org/abs/2507.12453v1", "date": "2025-07-16", "relevancy": 1.7144, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4757}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4312}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization&body=Title%3A%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization%0AAuthor%3A%20Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully%0AAbstract%3A%20%20%20In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%0Aof%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%0Afunctions%20is%20an%20important%20practical%20consideration.%20While%20several%20adaptive%0Astopping%20rules%20have%20been%20proposed%2C%20in%20the%20cost-aware%20setting%20they%20lack%0Aguarantees%20ensuring%20they%20stop%20before%20incurring%20excessive%20function%20evaluation%0Acosts.%20We%20propose%20a%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%0Aadapts%20to%20varying%20evaluation%20costs%20and%20is%20free%20of%20heuristic%20tuning.%20Our%20rule%20is%0Agrounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%0Afunctions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%0Aimprovement%20per%20cost.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%0Acumulative%20evaluation%20cost%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20these%0Atwo%20acquisition%20functions.%20In%20experiments%20on%20synthetic%20and%20empirical%20tasks%2C%0Aincluding%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20we%0Ashow%20that%20combining%20our%20stopping%20rule%20with%20the%20PBGI%20acquisition%20function%0Aconsistently%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%0Apairs%20in%20terms%20of%20cost-adjusted%20simple%20regret%2C%20a%20metric%20capturing%20trade-offs%0Abetween%20solution%20quality%20and%20cumulative%20evaluation%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-aware%2520Stopping%2520for%2520Bayesian%2520Optimization%26entry.906535625%3DQian%2520Xie%2520and%2520Linda%2520Cai%2520and%2520Alexander%2520Terenin%2520and%2520Peter%2520I.%2520Frazier%2520and%2520Ziv%2520Scully%26entry.1292438233%3D%2520%2520In%2520automated%2520machine%2520learning%252C%2520scientific%2520discovery%252C%2520and%2520other%2520applications%250Aof%2520Bayesian%2520optimization%252C%2520deciding%2520when%2520to%2520stop%2520evaluating%2520expensive%2520black-box%250Afunctions%2520is%2520an%2520important%2520practical%2520consideration.%2520While%2520several%2520adaptive%250Astopping%2520rules%2520have%2520been%2520proposed%252C%2520in%2520the%2520cost-aware%2520setting%2520they%2520lack%250Aguarantees%2520ensuring%2520they%2520stop%2520before%2520incurring%2520excessive%2520function%2520evaluation%250Acosts.%2520We%2520propose%2520a%2520cost-aware%2520stopping%2520rule%2520for%2520Bayesian%2520optimization%2520that%250Aadapts%2520to%2520varying%2520evaluation%2520costs%2520and%2520is%2520free%2520of%2520heuristic%2520tuning.%2520Our%2520rule%2520is%250Agrounded%2520in%2520a%2520theoretical%2520connection%2520to%2520state-of-the-art%2520cost-aware%2520acquisition%250Afunctions%252C%2520namely%2520the%2520Pandora%2527s%2520Box%2520Gittins%2520Index%2520%2528PBGI%2529%2520and%2520log%2520expected%250Aimprovement%2520per%2520cost.%2520We%2520prove%2520a%2520theoretical%2520guarantee%2520bounding%2520the%2520expected%250Acumulative%2520evaluation%2520cost%2520incurred%2520by%2520our%2520stopping%2520rule%2520when%2520paired%2520with%2520these%250Atwo%2520acquisition%2520functions.%2520In%2520experiments%2520on%2520synthetic%2520and%2520empirical%2520tasks%252C%250Aincluding%2520hyperparameter%2520optimization%2520and%2520neural%2520architecture%2520size%2520search%252C%2520we%250Ashow%2520that%2520combining%2520our%2520stopping%2520rule%2520with%2520the%2520PBGI%2520acquisition%2520function%250Aconsistently%2520matches%2520or%2520outperforms%2520other%2520acquisition-function--stopping-rule%250Apairs%2520in%2520terms%2520of%2520cost-adjusted%2520simple%2520regret%252C%2520a%2520metric%2520capturing%2520trade-offs%250Abetween%2520solution%2520quality%2520and%2520cumulative%2520evaluation%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-aware%20Stopping%20for%20Bayesian%20Optimization&entry.906535625=Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully&entry.1292438233=%20%20In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%0Aof%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%0Afunctions%20is%20an%20important%20practical%20consideration.%20While%20several%20adaptive%0Astopping%20rules%20have%20been%20proposed%2C%20in%20the%20cost-aware%20setting%20they%20lack%0Aguarantees%20ensuring%20they%20stop%20before%20incurring%20excessive%20function%20evaluation%0Acosts.%20We%20propose%20a%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%0Aadapts%20to%20varying%20evaluation%20costs%20and%20is%20free%20of%20heuristic%20tuning.%20Our%20rule%20is%0Agrounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%0Afunctions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%0Aimprovement%20per%20cost.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%0Acumulative%20evaluation%20cost%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20these%0Atwo%20acquisition%20functions.%20In%20experiments%20on%20synthetic%20and%20empirical%20tasks%2C%0Aincluding%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20we%0Ashow%20that%20combining%20our%20stopping%20rule%20with%20the%20PBGI%20acquisition%20function%0Aconsistently%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%0Apairs%20in%20terms%20of%20cost-adjusted%20simple%20regret%2C%20a%20metric%20capturing%20trade-offs%0Abetween%20solution%20quality%20and%20cumulative%20evaluation%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12453v1&entry.124074799=Read"},
{"title": "SPA: Efficient User-Preference Alignment against Uncertainty in Medical\n  Image Segmentation", "author": "Jiayuan Zhu and Junde Wu and Cheng Ouyang and Konstantinos Kamnitsas and J. Alison Noble", "abstract": "  Medical image segmentation data inherently contain uncertainty. This can stem\nfrom both imperfect image quality and variability in labeling preferences on\nambiguous pixels, which depend on annotator expertise and the clinical context\nof the annotations. For instance, a boundary pixel might be labeled as tumor in\ndiagnosis to avoid under-estimation of severity, but as normal tissue in\nradiotherapy to prevent damage to sensitive structures. As segmentation\npreferences vary across downstream applications, it is often desirable for an\nimage segmentation model to offer user-adaptable predictions rather than a\nfixed output. While prior uncertainty-aware and interactive methods offer\nadaptability, they are inefficient at test time: uncertainty-aware models\nrequire users to choose from numerous similar outputs, while interactive models\ndemand significant user input through click or box prompts to refine\nsegmentation. To address these challenges, we propose \\textbf{SPA}, a new\n\\textbf{S}egmentation \\textbf{P}reference \\textbf{A}lignment framework that\nefficiently adapts to diverse test-time preferences with minimal human\ninteraction. By presenting users with a select few, distinct segmentation\ncandidates that best capture uncertainties, it reduces the user workload to\nreach the preferred segmentation. To accommodate user preference, we introduce\na probabilistic mechanism that leverages user feedback to adapt a model's\nsegmentation preference. The proposed framework is evaluated on several medical\nimage segmentation tasks: color fundus images, lung lesion and kidney CT scans,\nMRI scans of brain and prostate. SPA shows 1) a significant reduction in user\ntime and effort compared to existing interactive segmentation approaches, 2)\nstrong adaptability based on human feedback, and 3) state-of-the-art image\nsegmentation performance across different imaging modalities and semantic\nlabels.\n", "link": "http://arxiv.org/abs/2411.15513v2", "date": "2025-07-16", "relevancy": 1.6739, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5586}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPA%3A%20Efficient%20User-Preference%20Alignment%20against%20Uncertainty%20in%20Medical%0A%20%20Image%20Segmentation&body=Title%3A%20SPA%3A%20Efficient%20User-Preference%20Alignment%20against%20Uncertainty%20in%20Medical%0A%20%20Image%20Segmentation%0AAuthor%3A%20Jiayuan%20Zhu%20and%20Junde%20Wu%20and%20Cheng%20Ouyang%20and%20Konstantinos%20Kamnitsas%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20data%20inherently%20contain%20uncertainty.%20This%20can%20stem%0Afrom%20both%20imperfect%20image%20quality%20and%20variability%20in%20labeling%20preferences%20on%0Aambiguous%20pixels%2C%20which%20depend%20on%20annotator%20expertise%20and%20the%20clinical%20context%0Aof%20the%20annotations.%20For%20instance%2C%20a%20boundary%20pixel%20might%20be%20labeled%20as%20tumor%20in%0Adiagnosis%20to%20avoid%20under-estimation%20of%20severity%2C%20but%20as%20normal%20tissue%20in%0Aradiotherapy%20to%20prevent%20damage%20to%20sensitive%20structures.%20As%20segmentation%0Apreferences%20vary%20across%20downstream%20applications%2C%20it%20is%20often%20desirable%20for%20an%0Aimage%20segmentation%20model%20to%20offer%20user-adaptable%20predictions%20rather%20than%20a%0Afixed%20output.%20While%20prior%20uncertainty-aware%20and%20interactive%20methods%20offer%0Aadaptability%2C%20they%20are%20inefficient%20at%20test%20time%3A%20uncertainty-aware%20models%0Arequire%20users%20to%20choose%20from%20numerous%20similar%20outputs%2C%20while%20interactive%20models%0Ademand%20significant%20user%20input%20through%20click%20or%20box%20prompts%20to%20refine%0Asegmentation.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BSPA%7D%2C%20a%20new%0A%5Ctextbf%7BS%7Degmentation%20%5Ctextbf%7BP%7Dreference%20%5Ctextbf%7BA%7Dlignment%20framework%20that%0Aefficiently%20adapts%20to%20diverse%20test-time%20preferences%20with%20minimal%20human%0Ainteraction.%20By%20presenting%20users%20with%20a%20select%20few%2C%20distinct%20segmentation%0Acandidates%20that%20best%20capture%20uncertainties%2C%20it%20reduces%20the%20user%20workload%20to%0Areach%20the%20preferred%20segmentation.%20To%20accommodate%20user%20preference%2C%20we%20introduce%0Aa%20probabilistic%20mechanism%20that%20leverages%20user%20feedback%20to%20adapt%20a%20model%27s%0Asegmentation%20preference.%20The%20proposed%20framework%20is%20evaluated%20on%20several%20medical%0Aimage%20segmentation%20tasks%3A%20color%20fundus%20images%2C%20lung%20lesion%20and%20kidney%20CT%20scans%2C%0AMRI%20scans%20of%20brain%20and%20prostate.%20SPA%20shows%201%29%20a%20significant%20reduction%20in%20user%0Atime%20and%20effort%20compared%20to%20existing%20interactive%20segmentation%20approaches%2C%202%29%0Astrong%20adaptability%20based%20on%20human%20feedback%2C%20and%203%29%20state-of-the-art%20image%0Asegmentation%20performance%20across%20different%20imaging%20modalities%20and%20semantic%0Alabels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPA%253A%2520Efficient%2520User-Preference%2520Alignment%2520against%2520Uncertainty%2520in%2520Medical%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DJiayuan%2520Zhu%2520and%2520Junde%2520Wu%2520and%2520Cheng%2520Ouyang%2520and%2520Konstantinos%2520Kamnitsas%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520data%2520inherently%2520contain%2520uncertainty.%2520This%2520can%2520stem%250Afrom%2520both%2520imperfect%2520image%2520quality%2520and%2520variability%2520in%2520labeling%2520preferences%2520on%250Aambiguous%2520pixels%252C%2520which%2520depend%2520on%2520annotator%2520expertise%2520and%2520the%2520clinical%2520context%250Aof%2520the%2520annotations.%2520For%2520instance%252C%2520a%2520boundary%2520pixel%2520might%2520be%2520labeled%2520as%2520tumor%2520in%250Adiagnosis%2520to%2520avoid%2520under-estimation%2520of%2520severity%252C%2520but%2520as%2520normal%2520tissue%2520in%250Aradiotherapy%2520to%2520prevent%2520damage%2520to%2520sensitive%2520structures.%2520As%2520segmentation%250Apreferences%2520vary%2520across%2520downstream%2520applications%252C%2520it%2520is%2520often%2520desirable%2520for%2520an%250Aimage%2520segmentation%2520model%2520to%2520offer%2520user-adaptable%2520predictions%2520rather%2520than%2520a%250Afixed%2520output.%2520While%2520prior%2520uncertainty-aware%2520and%2520interactive%2520methods%2520offer%250Aadaptability%252C%2520they%2520are%2520inefficient%2520at%2520test%2520time%253A%2520uncertainty-aware%2520models%250Arequire%2520users%2520to%2520choose%2520from%2520numerous%2520similar%2520outputs%252C%2520while%2520interactive%2520models%250Ademand%2520significant%2520user%2520input%2520through%2520click%2520or%2520box%2520prompts%2520to%2520refine%250Asegmentation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BSPA%257D%252C%2520a%2520new%250A%255Ctextbf%257BS%257Degmentation%2520%255Ctextbf%257BP%257Dreference%2520%255Ctextbf%257BA%257Dlignment%2520framework%2520that%250Aefficiently%2520adapts%2520to%2520diverse%2520test-time%2520preferences%2520with%2520minimal%2520human%250Ainteraction.%2520By%2520presenting%2520users%2520with%2520a%2520select%2520few%252C%2520distinct%2520segmentation%250Acandidates%2520that%2520best%2520capture%2520uncertainties%252C%2520it%2520reduces%2520the%2520user%2520workload%2520to%250Areach%2520the%2520preferred%2520segmentation.%2520To%2520accommodate%2520user%2520preference%252C%2520we%2520introduce%250Aa%2520probabilistic%2520mechanism%2520that%2520leverages%2520user%2520feedback%2520to%2520adapt%2520a%2520model%2527s%250Asegmentation%2520preference.%2520The%2520proposed%2520framework%2520is%2520evaluated%2520on%2520several%2520medical%250Aimage%2520segmentation%2520tasks%253A%2520color%2520fundus%2520images%252C%2520lung%2520lesion%2520and%2520kidney%2520CT%2520scans%252C%250AMRI%2520scans%2520of%2520brain%2520and%2520prostate.%2520SPA%2520shows%25201%2529%2520a%2520significant%2520reduction%2520in%2520user%250Atime%2520and%2520effort%2520compared%2520to%2520existing%2520interactive%2520segmentation%2520approaches%252C%25202%2529%250Astrong%2520adaptability%2520based%2520on%2520human%2520feedback%252C%2520and%25203%2529%2520state-of-the-art%2520image%250Asegmentation%2520performance%2520across%2520different%2520imaging%2520modalities%2520and%2520semantic%250Alabels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPA%3A%20Efficient%20User-Preference%20Alignment%20against%20Uncertainty%20in%20Medical%0A%20%20Image%20Segmentation&entry.906535625=Jiayuan%20Zhu%20and%20Junde%20Wu%20and%20Cheng%20Ouyang%20and%20Konstantinos%20Kamnitsas%20and%20J.%20Alison%20Noble&entry.1292438233=%20%20Medical%20image%20segmentation%20data%20inherently%20contain%20uncertainty.%20This%20can%20stem%0Afrom%20both%20imperfect%20image%20quality%20and%20variability%20in%20labeling%20preferences%20on%0Aambiguous%20pixels%2C%20which%20depend%20on%20annotator%20expertise%20and%20the%20clinical%20context%0Aof%20the%20annotations.%20For%20instance%2C%20a%20boundary%20pixel%20might%20be%20labeled%20as%20tumor%20in%0Adiagnosis%20to%20avoid%20under-estimation%20of%20severity%2C%20but%20as%20normal%20tissue%20in%0Aradiotherapy%20to%20prevent%20damage%20to%20sensitive%20structures.%20As%20segmentation%0Apreferences%20vary%20across%20downstream%20applications%2C%20it%20is%20often%20desirable%20for%20an%0Aimage%20segmentation%20model%20to%20offer%20user-adaptable%20predictions%20rather%20than%20a%0Afixed%20output.%20While%20prior%20uncertainty-aware%20and%20interactive%20methods%20offer%0Aadaptability%2C%20they%20are%20inefficient%20at%20test%20time%3A%20uncertainty-aware%20models%0Arequire%20users%20to%20choose%20from%20numerous%20similar%20outputs%2C%20while%20interactive%20models%0Ademand%20significant%20user%20input%20through%20click%20or%20box%20prompts%20to%20refine%0Asegmentation.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BSPA%7D%2C%20a%20new%0A%5Ctextbf%7BS%7Degmentation%20%5Ctextbf%7BP%7Dreference%20%5Ctextbf%7BA%7Dlignment%20framework%20that%0Aefficiently%20adapts%20to%20diverse%20test-time%20preferences%20with%20minimal%20human%0Ainteraction.%20By%20presenting%20users%20with%20a%20select%20few%2C%20distinct%20segmentation%0Acandidates%20that%20best%20capture%20uncertainties%2C%20it%20reduces%20the%20user%20workload%20to%0Areach%20the%20preferred%20segmentation.%20To%20accommodate%20user%20preference%2C%20we%20introduce%0Aa%20probabilistic%20mechanism%20that%20leverages%20user%20feedback%20to%20adapt%20a%20model%27s%0Asegmentation%20preference.%20The%20proposed%20framework%20is%20evaluated%20on%20several%20medical%0Aimage%20segmentation%20tasks%3A%20color%20fundus%20images%2C%20lung%20lesion%20and%20kidney%20CT%20scans%2C%0AMRI%20scans%20of%20brain%20and%20prostate.%20SPA%20shows%201%29%20a%20significant%20reduction%20in%20user%0Atime%20and%20effort%20compared%20to%20existing%20interactive%20segmentation%20approaches%2C%202%29%0Astrong%20adaptability%20based%20on%20human%20feedback%2C%20and%203%29%20state-of-the-art%20image%0Asegmentation%20performance%20across%20different%20imaging%20modalities%20and%20semantic%0Alabels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15513v2&entry.124074799=Read"},
{"title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative\n  Text-to-Audio Diffusion Models", "author": "Riccardo Passoni and Francesca Ronchini and Luca Comanducci and Romain Serizel and Fabio Antonacci", "abstract": "  Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models.\n", "link": "http://arxiv.org/abs/2505.07615v2", "date": "2025-07-16", "relevancy": 1.6333, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5988}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffused%20Responsibility%3A%20Analyzing%20the%20Energy%20Consumption%20of%20Generative%0A%20%20Text-to-Audio%20Diffusion%20Models&body=Title%3A%20Diffused%20Responsibility%3A%20Analyzing%20the%20Energy%20Consumption%20of%20Generative%0A%20%20Text-to-Audio%20Diffusion%20Models%0AAuthor%3A%20Riccardo%20Passoni%20and%20Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Romain%20Serizel%20and%20Fabio%20Antonacci%0AAbstract%3A%20%20%20Text-to-audio%20models%20have%20recently%20emerged%20as%20a%20powerful%20technology%20for%0Agenerating%20sound%20from%20textual%20descriptions.%20However%2C%20their%20high%20computational%0Ademands%20raise%20concerns%20about%20energy%20consumption%20and%20environmental%20impact.%20In%0Athis%20paper%2C%20we%20conduct%20an%20analysis%20of%20the%20energy%20usage%20of%207%20state-of-the-art%0Atext-to-audio%20diffusion-based%20generative%20models%2C%20evaluating%20to%20what%20extent%0Avariations%20in%20generation%20parameters%20affect%20energy%20consumption%20at%20inference%0Atime.%20We%20also%20aim%20to%20identify%20an%20optimal%20balance%20between%20audio%20quality%20and%0Aenergy%20consumption%20by%20considering%20Pareto-optimal%20solutions%20across%20all%20selected%0Amodels.%20Our%20findings%20provide%20insights%20into%20the%20trade-offs%20between%20performance%0Aand%20environmental%20impact%2C%20contributing%20to%20the%20development%20of%20more%20efficient%0Agenerative%20audio%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffused%2520Responsibility%253A%2520Analyzing%2520the%2520Energy%2520Consumption%2520of%2520Generative%250A%2520%2520Text-to-Audio%2520Diffusion%2520Models%26entry.906535625%3DRiccardo%2520Passoni%2520and%2520Francesca%2520Ronchini%2520and%2520Luca%2520Comanducci%2520and%2520Romain%2520Serizel%2520and%2520Fabio%2520Antonacci%26entry.1292438233%3D%2520%2520Text-to-audio%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520technology%2520for%250Agenerating%2520sound%2520from%2520textual%2520descriptions.%2520However%252C%2520their%2520high%2520computational%250Ademands%2520raise%2520concerns%2520about%2520energy%2520consumption%2520and%2520environmental%2520impact.%2520In%250Athis%2520paper%252C%2520we%2520conduct%2520an%2520analysis%2520of%2520the%2520energy%2520usage%2520of%25207%2520state-of-the-art%250Atext-to-audio%2520diffusion-based%2520generative%2520models%252C%2520evaluating%2520to%2520what%2520extent%250Avariations%2520in%2520generation%2520parameters%2520affect%2520energy%2520consumption%2520at%2520inference%250Atime.%2520We%2520also%2520aim%2520to%2520identify%2520an%2520optimal%2520balance%2520between%2520audio%2520quality%2520and%250Aenergy%2520consumption%2520by%2520considering%2520Pareto-optimal%2520solutions%2520across%2520all%2520selected%250Amodels.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%2520trade-offs%2520between%2520performance%250Aand%2520environmental%2520impact%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%2520efficient%250Agenerative%2520audio%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffused%20Responsibility%3A%20Analyzing%20the%20Energy%20Consumption%20of%20Generative%0A%20%20Text-to-Audio%20Diffusion%20Models&entry.906535625=Riccardo%20Passoni%20and%20Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Romain%20Serizel%20and%20Fabio%20Antonacci&entry.1292438233=%20%20Text-to-audio%20models%20have%20recently%20emerged%20as%20a%20powerful%20technology%20for%0Agenerating%20sound%20from%20textual%20descriptions.%20However%2C%20their%20high%20computational%0Ademands%20raise%20concerns%20about%20energy%20consumption%20and%20environmental%20impact.%20In%0Athis%20paper%2C%20we%20conduct%20an%20analysis%20of%20the%20energy%20usage%20of%207%20state-of-the-art%0Atext-to-audio%20diffusion-based%20generative%20models%2C%20evaluating%20to%20what%20extent%0Avariations%20in%20generation%20parameters%20affect%20energy%20consumption%20at%20inference%0Atime.%20We%20also%20aim%20to%20identify%20an%20optimal%20balance%20between%20audio%20quality%20and%0Aenergy%20consumption%20by%20considering%20Pareto-optimal%20solutions%20across%20all%20selected%0Amodels.%20Our%20findings%20provide%20insights%20into%20the%20trade-offs%20between%20performance%0Aand%20environmental%20impact%2C%20contributing%20to%20the%20development%20of%20more%20efficient%0Agenerative%20audio%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07615v2&entry.124074799=Read"},
{"title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding", "author": "Renjie Li and Ruijie Ye and Mingyang Wu and Hao Frank Yang and Zhiwen Fan and Hezhen Hu and Zhengzhong Tu", "abstract": "  Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.\n", "link": "http://arxiv.org/abs/2507.12463v1", "date": "2025-07-16", "relevancy": 1.6066, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5364}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMHU%3A%20A%20Massive-Scale%20Multimodal%20Benchmark%20for%20Human%20Behavior%0A%20%20Understanding&body=Title%3A%20MMHU%3A%20A%20Massive-Scale%20Multimodal%20Benchmark%20for%20Human%20Behavior%0A%20%20Understanding%0AAuthor%3A%20Renjie%20Li%20and%20Ruijie%20Ye%20and%20Mingyang%20Wu%20and%20Hao%20Frank%20Yang%20and%20Zhiwen%20Fan%20and%20Hezhen%20Hu%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Humans%20are%20integral%20components%20of%20the%20transportation%20ecosystem%2C%20and%0Aunderstanding%20their%20behaviors%20is%20crucial%20to%20facilitating%20the%20development%20of%0Asafe%20driving%20systems.%20Although%20recent%20progress%20has%20explored%20various%20aspects%20of%0Ahuman%20behavior%24%5Cunicode%7Bx2014%7D%24such%20as%20motion%2C%20trajectories%2C%20and%0Aintention%24%5Cunicode%7Bx2014%7D%24a%20comprehensive%20benchmark%20for%20evaluating%20human%0Abehavior%20understanding%20in%20autonomous%20driving%20remains%20unavailable.%20In%20this%20work%2C%0Awe%20propose%20%24%5Ctextbf%7BMMHU%7D%24%2C%20a%20large-scale%20benchmark%20for%20human%20behavior%20analysis%0Afeaturing%20rich%20annotations%2C%20such%20as%20human%20motion%20and%20trajectories%2C%20text%0Adescription%20for%20human%20motions%2C%20human%20intention%2C%20and%20critical%20behavior%20labels%0Arelevant%20to%20driving%20safety.%20Our%20dataset%20encompasses%2057k%20human%20motion%20clips%20and%0A1.73M%20frames%20gathered%20from%20diverse%20sources%2C%20including%20established%20driving%0Adatasets%20such%20as%20Waymo%2C%20in-the-wild%20videos%20from%20YouTube%2C%20and%20self-collected%0Adata.%20A%20human-in-the-loop%20annotation%20pipeline%20is%20developed%20to%20generate%20rich%0Abehavior%20captions.%20We%20provide%20a%20thorough%20dataset%20analysis%20and%20benchmark%0Amultiple%20tasks%24%5Cunicode%7Bx2014%7D%24ranging%20from%20motion%20prediction%20to%20motion%0Ageneration%20and%20human%20behavior%20question%20answering%24%5Cunicode%7Bx2014%7D%24thereby%0Aoffering%20a%20broad%20evaluation%20suite.%20Project%20page%20%3A%0Ahttps%3A//MMHU-Benchmark.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMHU%253A%2520A%2520Massive-Scale%2520Multimodal%2520Benchmark%2520for%2520Human%2520Behavior%250A%2520%2520Understanding%26entry.906535625%3DRenjie%2520Li%2520and%2520Ruijie%2520Ye%2520and%2520Mingyang%2520Wu%2520and%2520Hao%2520Frank%2520Yang%2520and%2520Zhiwen%2520Fan%2520and%2520Hezhen%2520Hu%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Humans%2520are%2520integral%2520components%2520of%2520the%2520transportation%2520ecosystem%252C%2520and%250Aunderstanding%2520their%2520behaviors%2520is%2520crucial%2520to%2520facilitating%2520the%2520development%2520of%250Asafe%2520driving%2520systems.%2520Although%2520recent%2520progress%2520has%2520explored%2520various%2520aspects%2520of%250Ahuman%2520behavior%2524%255Cunicode%257Bx2014%257D%2524such%2520as%2520motion%252C%2520trajectories%252C%2520and%250Aintention%2524%255Cunicode%257Bx2014%257D%2524a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520human%250Abehavior%2520understanding%2520in%2520autonomous%2520driving%2520remains%2520unavailable.%2520In%2520this%2520work%252C%250Awe%2520propose%2520%2524%255Ctextbf%257BMMHU%257D%2524%252C%2520a%2520large-scale%2520benchmark%2520for%2520human%2520behavior%2520analysis%250Afeaturing%2520rich%2520annotations%252C%2520such%2520as%2520human%2520motion%2520and%2520trajectories%252C%2520text%250Adescription%2520for%2520human%2520motions%252C%2520human%2520intention%252C%2520and%2520critical%2520behavior%2520labels%250Arelevant%2520to%2520driving%2520safety.%2520Our%2520dataset%2520encompasses%252057k%2520human%2520motion%2520clips%2520and%250A1.73M%2520frames%2520gathered%2520from%2520diverse%2520sources%252C%2520including%2520established%2520driving%250Adatasets%2520such%2520as%2520Waymo%252C%2520in-the-wild%2520videos%2520from%2520YouTube%252C%2520and%2520self-collected%250Adata.%2520A%2520human-in-the-loop%2520annotation%2520pipeline%2520is%2520developed%2520to%2520generate%2520rich%250Abehavior%2520captions.%2520We%2520provide%2520a%2520thorough%2520dataset%2520analysis%2520and%2520benchmark%250Amultiple%2520tasks%2524%255Cunicode%257Bx2014%257D%2524ranging%2520from%2520motion%2520prediction%2520to%2520motion%250Ageneration%2520and%2520human%2520behavior%2520question%2520answering%2524%255Cunicode%257Bx2014%257D%2524thereby%250Aoffering%2520a%2520broad%2520evaluation%2520suite.%2520Project%2520page%2520%253A%250Ahttps%253A//MMHU-Benchmark.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMHU%3A%20A%20Massive-Scale%20Multimodal%20Benchmark%20for%20Human%20Behavior%0A%20%20Understanding&entry.906535625=Renjie%20Li%20and%20Ruijie%20Ye%20and%20Mingyang%20Wu%20and%20Hao%20Frank%20Yang%20and%20Zhiwen%20Fan%20and%20Hezhen%20Hu%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Humans%20are%20integral%20components%20of%20the%20transportation%20ecosystem%2C%20and%0Aunderstanding%20their%20behaviors%20is%20crucial%20to%20facilitating%20the%20development%20of%0Asafe%20driving%20systems.%20Although%20recent%20progress%20has%20explored%20various%20aspects%20of%0Ahuman%20behavior%24%5Cunicode%7Bx2014%7D%24such%20as%20motion%2C%20trajectories%2C%20and%0Aintention%24%5Cunicode%7Bx2014%7D%24a%20comprehensive%20benchmark%20for%20evaluating%20human%0Abehavior%20understanding%20in%20autonomous%20driving%20remains%20unavailable.%20In%20this%20work%2C%0Awe%20propose%20%24%5Ctextbf%7BMMHU%7D%24%2C%20a%20large-scale%20benchmark%20for%20human%20behavior%20analysis%0Afeaturing%20rich%20annotations%2C%20such%20as%20human%20motion%20and%20trajectories%2C%20text%0Adescription%20for%20human%20motions%2C%20human%20intention%2C%20and%20critical%20behavior%20labels%0Arelevant%20to%20driving%20safety.%20Our%20dataset%20encompasses%2057k%20human%20motion%20clips%20and%0A1.73M%20frames%20gathered%20from%20diverse%20sources%2C%20including%20established%20driving%0Adatasets%20such%20as%20Waymo%2C%20in-the-wild%20videos%20from%20YouTube%2C%20and%20self-collected%0Adata.%20A%20human-in-the-loop%20annotation%20pipeline%20is%20developed%20to%20generate%20rich%0Abehavior%20captions.%20We%20provide%20a%20thorough%20dataset%20analysis%20and%20benchmark%0Amultiple%20tasks%24%5Cunicode%7Bx2014%7D%24ranging%20from%20motion%20prediction%20to%20motion%0Ageneration%20and%20human%20behavior%20question%20answering%24%5Cunicode%7Bx2014%7D%24thereby%0Aoffering%20a%20broad%20evaluation%20suite.%20Project%20page%20%3A%0Ahttps%3A//MMHU-Benchmark.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12463v1&entry.124074799=Read"},
{"title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid\n  Language Model Performance with Long Context Length", "author": "Saptarshi Mitra and Rachid Karami and Haocheng Xu and Sitao Huang and Hyoukjun Kwon", "abstract": "  The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.\n", "link": "http://arxiv.org/abs/2507.12442v1", "date": "2025-07-16", "relevancy": 1.5865, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5523}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20State%20Space%20Model%20%28SSM%29%20and%20SSM-Transformer%20Hybrid%0A%20%20Language%20Model%20Performance%20with%20Long%20Context%20Length&body=Title%3A%20Characterizing%20State%20Space%20Model%20%28SSM%29%20and%20SSM-Transformer%20Hybrid%0A%20%20Language%20Model%20Performance%20with%20Long%20Context%20Length%0AAuthor%3A%20Saptarshi%20Mitra%20and%20Rachid%20Karami%20and%20Haocheng%20Xu%20and%20Sitao%20Huang%20and%20Hyoukjun%20Kwon%0AAbstract%3A%20%20%20The%20demand%20for%20machine%20intelligence%20capable%20of%20processing%20continuous%2C%0Along-context%20inputs%20on%20local%20devices%20is%20growing%20rapidly.%20However%2C%20the%20quadratic%0Acomplexity%20and%20memory%20requirements%20of%20traditional%20Transformer%20architectures%0Amake%20them%20inefficient%20and%20often%20unusable%20for%20these%20tasks.%20This%20has%20spurred%20a%0Aparadigm%20shift%20towards%20new%20architectures%20like%20State%20Space%20Models%20%28SSMs%29%20and%0Ahybrids%2C%20which%20promise%20near-linear%20scaling.%20While%20most%20current%20research%20focuses%0Aon%20the%20accuracy%20and%20theoretical%20throughput%20of%20these%20models%2C%20a%20systematic%0Aperformance%20characterization%20on%20practical%20consumer%20hardware%20is%20critically%0Aneeded%20to%20guide%20system-level%20optimization%20and%20unlock%20new%20applications.%0A%20%20To%20address%20this%20gap%2C%20we%20present%20a%20comprehensive%2C%20comparative%20benchmarking%20of%0Acarefully%20selected%20Transformer%2C%20SSM%2C%20and%20hybrid%20models%20specifically%20for%0Along-context%20inference%20on%20consumer%20and%20embedded%20GPUs.%20Our%20analysis%20reveals%20that%0ASSMs%20are%20not%20only%20viable%20but%20superior%20for%20this%20domain%2C%20capable%20of%20processing%0Asequences%20up%20to%20220K%20tokens%20on%20a%2024GB%20consumer%20GPU-approximately%204x%20longer%20than%0Acomparable%20Transformers.%20While%20Transformers%20may%20be%20up%20to%201.8x%20faster%20at%20short%0Asequences%2C%20SSMs%20demonstrate%20a%20dramatic%20performance%20inversion%2C%20becoming%20up%20to%204x%0Afaster%20at%20very%20long%20contexts%20%28~57K%20tokens%29.%20Our%20operator-level%20analysis%20reveals%0Athat%20custom%2C%20hardware-aware%20SSM%20kernels%20dominate%20the%20inference%20runtime%2C%0Aaccounting%20for%20over%2055%25%20of%20latency%20on%20edge%20platforms%2C%20identifying%20them%20as%20a%0Aprimary%20target%20for%20future%20hardware%20acceleration.%20We%20also%20provide%20detailed%2C%0Adevice-specific%20characterization%20results%20to%20guide%20system%20co-design%20for%20the%0Aedge.%20To%20foster%20further%20research%2C%20we%20will%20open-source%20our%20characterization%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520State%2520Space%2520Model%2520%2528SSM%2529%2520and%2520SSM-Transformer%2520Hybrid%250A%2520%2520Language%2520Model%2520Performance%2520with%2520Long%2520Context%2520Length%26entry.906535625%3DSaptarshi%2520Mitra%2520and%2520Rachid%2520Karami%2520and%2520Haocheng%2520Xu%2520and%2520Sitao%2520Huang%2520and%2520Hyoukjun%2520Kwon%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520machine%2520intelligence%2520capable%2520of%2520processing%2520continuous%252C%250Along-context%2520inputs%2520on%2520local%2520devices%2520is%2520growing%2520rapidly.%2520However%252C%2520the%2520quadratic%250Acomplexity%2520and%2520memory%2520requirements%2520of%2520traditional%2520Transformer%2520architectures%250Amake%2520them%2520inefficient%2520and%2520often%2520unusable%2520for%2520these%2520tasks.%2520This%2520has%2520spurred%2520a%250Aparadigm%2520shift%2520towards%2520new%2520architectures%2520like%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520and%250Ahybrids%252C%2520which%2520promise%2520near-linear%2520scaling.%2520While%2520most%2520current%2520research%2520focuses%250Aon%2520the%2520accuracy%2520and%2520theoretical%2520throughput%2520of%2520these%2520models%252C%2520a%2520systematic%250Aperformance%2520characterization%2520on%2520practical%2520consumer%2520hardware%2520is%2520critically%250Aneeded%2520to%2520guide%2520system-level%2520optimization%2520and%2520unlock%2520new%2520applications.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520comprehensive%252C%2520comparative%2520benchmarking%2520of%250Acarefully%2520selected%2520Transformer%252C%2520SSM%252C%2520and%2520hybrid%2520models%2520specifically%2520for%250Along-context%2520inference%2520on%2520consumer%2520and%2520embedded%2520GPUs.%2520Our%2520analysis%2520reveals%2520that%250ASSMs%2520are%2520not%2520only%2520viable%2520but%2520superior%2520for%2520this%2520domain%252C%2520capable%2520of%2520processing%250Asequences%2520up%2520to%2520220K%2520tokens%2520on%2520a%252024GB%2520consumer%2520GPU-approximately%25204x%2520longer%2520than%250Acomparable%2520Transformers.%2520While%2520Transformers%2520may%2520be%2520up%2520to%25201.8x%2520faster%2520at%2520short%250Asequences%252C%2520SSMs%2520demonstrate%2520a%2520dramatic%2520performance%2520inversion%252C%2520becoming%2520up%2520to%25204x%250Afaster%2520at%2520very%2520long%2520contexts%2520%2528~57K%2520tokens%2529.%2520Our%2520operator-level%2520analysis%2520reveals%250Athat%2520custom%252C%2520hardware-aware%2520SSM%2520kernels%2520dominate%2520the%2520inference%2520runtime%252C%250Aaccounting%2520for%2520over%252055%2525%2520of%2520latency%2520on%2520edge%2520platforms%252C%2520identifying%2520them%2520as%2520a%250Aprimary%2520target%2520for%2520future%2520hardware%2520acceleration.%2520We%2520also%2520provide%2520detailed%252C%250Adevice-specific%2520characterization%2520results%2520to%2520guide%2520system%2520co-design%2520for%2520the%250Aedge.%2520To%2520foster%2520further%2520research%252C%2520we%2520will%2520open-source%2520our%2520characterization%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20State%20Space%20Model%20%28SSM%29%20and%20SSM-Transformer%20Hybrid%0A%20%20Language%20Model%20Performance%20with%20Long%20Context%20Length&entry.906535625=Saptarshi%20Mitra%20and%20Rachid%20Karami%20and%20Haocheng%20Xu%20and%20Sitao%20Huang%20and%20Hyoukjun%20Kwon&entry.1292438233=%20%20The%20demand%20for%20machine%20intelligence%20capable%20of%20processing%20continuous%2C%0Along-context%20inputs%20on%20local%20devices%20is%20growing%20rapidly.%20However%2C%20the%20quadratic%0Acomplexity%20and%20memory%20requirements%20of%20traditional%20Transformer%20architectures%0Amake%20them%20inefficient%20and%20often%20unusable%20for%20these%20tasks.%20This%20has%20spurred%20a%0Aparadigm%20shift%20towards%20new%20architectures%20like%20State%20Space%20Models%20%28SSMs%29%20and%0Ahybrids%2C%20which%20promise%20near-linear%20scaling.%20While%20most%20current%20research%20focuses%0Aon%20the%20accuracy%20and%20theoretical%20throughput%20of%20these%20models%2C%20a%20systematic%0Aperformance%20characterization%20on%20practical%20consumer%20hardware%20is%20critically%0Aneeded%20to%20guide%20system-level%20optimization%20and%20unlock%20new%20applications.%0A%20%20To%20address%20this%20gap%2C%20we%20present%20a%20comprehensive%2C%20comparative%20benchmarking%20of%0Acarefully%20selected%20Transformer%2C%20SSM%2C%20and%20hybrid%20models%20specifically%20for%0Along-context%20inference%20on%20consumer%20and%20embedded%20GPUs.%20Our%20analysis%20reveals%20that%0ASSMs%20are%20not%20only%20viable%20but%20superior%20for%20this%20domain%2C%20capable%20of%20processing%0Asequences%20up%20to%20220K%20tokens%20on%20a%2024GB%20consumer%20GPU-approximately%204x%20longer%20than%0Acomparable%20Transformers.%20While%20Transformers%20may%20be%20up%20to%201.8x%20faster%20at%20short%0Asequences%2C%20SSMs%20demonstrate%20a%20dramatic%20performance%20inversion%2C%20becoming%20up%20to%204x%0Afaster%20at%20very%20long%20contexts%20%28~57K%20tokens%29.%20Our%20operator-level%20analysis%20reveals%0Athat%20custom%2C%20hardware-aware%20SSM%20kernels%20dominate%20the%20inference%20runtime%2C%0Aaccounting%20for%20over%2055%25%20of%20latency%20on%20edge%20platforms%2C%20identifying%20them%20as%20a%0Aprimary%20target%20for%20future%20hardware%20acceleration.%20We%20also%20provide%20detailed%2C%0Adevice-specific%20characterization%20results%20to%20guide%20system%20co-design%20for%20the%0Aedge.%20To%20foster%20further%20research%2C%20we%20will%20open-source%20our%20characterization%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12442v1&entry.124074799=Read"},
{"title": "Neural Polar Decoders for Deletion Channels", "author": "Ziv Aharoni and Henry D. Pfister", "abstract": "  This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.\n", "link": "http://arxiv.org/abs/2507.12329v1", "date": "2025-07-16", "relevancy": 1.5827, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.412}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4051}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Polar%20Decoders%20for%20Deletion%20Channels&body=Title%3A%20Neural%20Polar%20Decoders%20for%20Deletion%20Channels%0AAuthor%3A%20Ziv%20Aharoni%20and%20Henry%20D.%20Pfister%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20neural%20polar%20decoder%20%28NPD%29%20for%20deletion%20channels%20with%0Aa%20constant%20deletion%20rate.%20Existing%20polar%20decoders%20for%20deletion%20channels%20exhibit%0Ahigh%20computational%20complexity%20of%20%24O%28N%5E4%29%24%2C%20where%20%24N%24%20is%20the%20block%20length.%20This%0Alimits%20the%20application%20of%20polar%20codes%20for%20deletion%20channels%20to%0Ashort-to-moderate%20block%20lengths.%20In%20this%20work%2C%20we%20demonstrate%20that%20employing%0ANPDs%20for%20deletion%20channels%20can%20reduce%20the%20computational%20complexity.%20First%2C%20we%0Aextend%20the%20architecture%20of%20the%20NPD%20to%20support%20deletion%20channels.%20Specifically%2C%0Athe%20NPD%20architecture%20consists%20of%20four%20neural%20networks%20%28NNs%29%2C%20each%20replicating%0Afundamental%20successive%20cancellation%20%28SC%29%20decoder%20operations.%20To%20support%0Adeletion%20channels%2C%20we%20change%20the%20architecture%20of%20only%20one.%20The%20computational%0Acomplexity%20of%20the%20NPD%20is%20%24O%28AN%5Clog%20N%29%24%2C%20where%20the%20parameter%20%24A%24%20represents%20a%0Acomputational%20budget%20determined%20by%20the%20user%20and%20is%20independent%20of%20the%20channel.%0AWe%20evaluate%20the%20new%20extended%20NPD%20for%20deletion%20channels%20with%20deletion%20rates%0A%24%5Cdelta%5Cin%5C%7B0.01%2C%200.1%5C%7D%24%20and%20we%20verify%20the%20NPD%20with%20the%20ground%20truth%20given%20by%0Athe%20trellis%20decoder%20by%20Tal%20et%20al.%20We%20further%20show%20that%20due%20to%20the%20reduced%0Acomplexity%20of%20the%20NPD%2C%20we%20are%20able%20to%20incorporate%20list%20decoding%20and%20further%0Aimprove%20performance.%20We%20believe%20that%20the%20extended%20NPD%20presented%20here%20could%20have%0Aapplications%20in%20future%20technologies%20like%20DNA%20storage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Polar%2520Decoders%2520for%2520Deletion%2520Channels%26entry.906535625%3DZiv%2520Aharoni%2520and%2520Henry%2520D.%2520Pfister%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520neural%2520polar%2520decoder%2520%2528NPD%2529%2520for%2520deletion%2520channels%2520with%250Aa%2520constant%2520deletion%2520rate.%2520Existing%2520polar%2520decoders%2520for%2520deletion%2520channels%2520exhibit%250Ahigh%2520computational%2520complexity%2520of%2520%2524O%2528N%255E4%2529%2524%252C%2520where%2520%2524N%2524%2520is%2520the%2520block%2520length.%2520This%250Alimits%2520the%2520application%2520of%2520polar%2520codes%2520for%2520deletion%2520channels%2520to%250Ashort-to-moderate%2520block%2520lengths.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520employing%250ANPDs%2520for%2520deletion%2520channels%2520can%2520reduce%2520the%2520computational%2520complexity.%2520First%252C%2520we%250Aextend%2520the%2520architecture%2520of%2520the%2520NPD%2520to%2520support%2520deletion%2520channels.%2520Specifically%252C%250Athe%2520NPD%2520architecture%2520consists%2520of%2520four%2520neural%2520networks%2520%2528NNs%2529%252C%2520each%2520replicating%250Afundamental%2520successive%2520cancellation%2520%2528SC%2529%2520decoder%2520operations.%2520To%2520support%250Adeletion%2520channels%252C%2520we%2520change%2520the%2520architecture%2520of%2520only%2520one.%2520The%2520computational%250Acomplexity%2520of%2520the%2520NPD%2520is%2520%2524O%2528AN%255Clog%2520N%2529%2524%252C%2520where%2520the%2520parameter%2520%2524A%2524%2520represents%2520a%250Acomputational%2520budget%2520determined%2520by%2520the%2520user%2520and%2520is%2520independent%2520of%2520the%2520channel.%250AWe%2520evaluate%2520the%2520new%2520extended%2520NPD%2520for%2520deletion%2520channels%2520with%2520deletion%2520rates%250A%2524%255Cdelta%255Cin%255C%257B0.01%252C%25200.1%255C%257D%2524%2520and%2520we%2520verify%2520the%2520NPD%2520with%2520the%2520ground%2520truth%2520given%2520by%250Athe%2520trellis%2520decoder%2520by%2520Tal%2520et%2520al.%2520We%2520further%2520show%2520that%2520due%2520to%2520the%2520reduced%250Acomplexity%2520of%2520the%2520NPD%252C%2520we%2520are%2520able%2520to%2520incorporate%2520list%2520decoding%2520and%2520further%250Aimprove%2520performance.%2520We%2520believe%2520that%2520the%2520extended%2520NPD%2520presented%2520here%2520could%2520have%250Aapplications%2520in%2520future%2520technologies%2520like%2520DNA%2520storage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Polar%20Decoders%20for%20Deletion%20Channels&entry.906535625=Ziv%20Aharoni%20and%20Henry%20D.%20Pfister&entry.1292438233=%20%20This%20paper%20introduces%20a%20neural%20polar%20decoder%20%28NPD%29%20for%20deletion%20channels%20with%0Aa%20constant%20deletion%20rate.%20Existing%20polar%20decoders%20for%20deletion%20channels%20exhibit%0Ahigh%20computational%20complexity%20of%20%24O%28N%5E4%29%24%2C%20where%20%24N%24%20is%20the%20block%20length.%20This%0Alimits%20the%20application%20of%20polar%20codes%20for%20deletion%20channels%20to%0Ashort-to-moderate%20block%20lengths.%20In%20this%20work%2C%20we%20demonstrate%20that%20employing%0ANPDs%20for%20deletion%20channels%20can%20reduce%20the%20computational%20complexity.%20First%2C%20we%0Aextend%20the%20architecture%20of%20the%20NPD%20to%20support%20deletion%20channels.%20Specifically%2C%0Athe%20NPD%20architecture%20consists%20of%20four%20neural%20networks%20%28NNs%29%2C%20each%20replicating%0Afundamental%20successive%20cancellation%20%28SC%29%20decoder%20operations.%20To%20support%0Adeletion%20channels%2C%20we%20change%20the%20architecture%20of%20only%20one.%20The%20computational%0Acomplexity%20of%20the%20NPD%20is%20%24O%28AN%5Clog%20N%29%24%2C%20where%20the%20parameter%20%24A%24%20represents%20a%0Acomputational%20budget%20determined%20by%20the%20user%20and%20is%20independent%20of%20the%20channel.%0AWe%20evaluate%20the%20new%20extended%20NPD%20for%20deletion%20channels%20with%20deletion%20rates%0A%24%5Cdelta%5Cin%5C%7B0.01%2C%200.1%5C%7D%24%20and%20we%20verify%20the%20NPD%20with%20the%20ground%20truth%20given%20by%0Athe%20trellis%20decoder%20by%20Tal%20et%20al.%20We%20further%20show%20that%20due%20to%20the%20reduced%0Acomplexity%20of%20the%20NPD%2C%20we%20are%20able%20to%20incorporate%20list%20decoding%20and%20further%0Aimprove%20performance.%20We%20believe%20that%20the%20extended%20NPD%20presented%20here%20could%20have%0Aapplications%20in%20future%20technologies%20like%20DNA%20storage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12329v1&entry.124074799=Read"},
{"title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog\n  CAM with Inherent Soft Boundaries", "author": "Bo Wen and Guoyun Gao and Zhicheng Xu and Ruibin Mao and Xiaojuan Qi and X. Sharon Hu and Xunzhao Yin and Can Li", "abstract": "  The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.\n", "link": "http://arxiv.org/abs/2507.12384v1", "date": "2025-07-16", "relevancy": 1.5774, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.556}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Tree-based%20Machine%20Learning%20by%20%24MoS_2%24%20Flash-based%20Analog%0A%20%20CAM%20with%20Inherent%20Soft%20Boundaries&body=Title%3A%20Trustworthy%20Tree-based%20Machine%20Learning%20by%20%24MoS_2%24%20Flash-based%20Analog%0A%20%20CAM%20with%20Inherent%20Soft%20Boundaries%0AAuthor%3A%20Bo%20Wen%20and%20Guoyun%20Gao%20and%20Zhicheng%20Xu%20and%20Ruibin%20Mao%20and%20Xiaojuan%20Qi%20and%20X.%20Sharon%20Hu%20and%20Xunzhao%20Yin%20and%20Can%20Li%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%20has%20raised%20concerns%0Aregarding%20its%20trustworthiness%2C%20especially%20in%20terms%20of%20interpretability%20and%0Arobustness.%20Tree-based%20models%20like%20Random%20Forest%20and%20XGBoost%20excel%20in%0Ainterpretability%20and%20accuracy%20for%20tabular%20data%2C%20but%20scaling%20them%20remains%0Acomputationally%20expensive%20due%20to%20poor%20data%20locality%20and%20high%20data%20dependence.%0APrevious%20efforts%20to%20accelerate%20these%20models%20with%20analog%20content%20addressable%0Amemory%20%28CAM%29%20have%20struggled%2C%20due%20to%20the%20fact%20that%20the%20difficult-to-implement%0Asharp%20decision%20boundaries%20are%20highly%20susceptible%20to%20device%20variations%2C%20which%0Aleads%20to%20poor%20hardware%20performance%20and%20vulnerability%20to%20adversarial%20attacks.%0AThis%20work%20presents%20a%20novel%20hardware-software%20co-design%20approach%20using%20%24MoS_2%24%0AFlash-based%20analog%20CAM%20with%20inherent%20soft%20boundaries%2C%20enabling%20efficient%0Ainference%20with%20soft%20tree-based%20models.%20Our%20soft%20tree%20model%20inference%0Aexperiments%20on%20%24MoS_2%24%20analog%20CAM%20arrays%20show%20this%20method%20achieves%20exceptional%0Arobustness%20against%20device%20variation%20and%20adversarial%20attacks%20while%20achieving%0Astate-of-the-art%20accuracy.%20Specifically%2C%20our%20fabricated%20analog%20CAM%20arrays%0Aachieve%20%2496%5C%25%24%20accuracy%20on%20Wisconsin%20Diagnostic%20Breast%20Cancer%20%28WDBC%29%20database%2C%0Awhile%20maintaining%20decision%20explainability.%20Our%20experimentally%20calibrated%20model%0Avalidated%20only%20a%20%240.6%5C%25%24%20accuracy%20drop%20on%20the%20MNIST%20dataset%20under%20%2410%5C%25%24%20device%0Athreshold%20variation%2C%20compared%20to%20a%20%2445.3%5C%25%24%20drop%20for%20traditional%20decision%0Atrees.%20This%20work%20paves%20the%20way%20for%20specialized%20hardware%20that%20enhances%20AI%27s%0Atrustworthiness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Tree-based%2520Machine%2520Learning%2520by%2520%2524MoS_2%2524%2520Flash-based%2520Analog%250A%2520%2520CAM%2520with%2520Inherent%2520Soft%2520Boundaries%26entry.906535625%3DBo%2520Wen%2520and%2520Guoyun%2520Gao%2520and%2520Zhicheng%2520Xu%2520and%2520Ruibin%2520Mao%2520and%2520Xiaojuan%2520Qi%2520and%2520X.%2520Sharon%2520Hu%2520and%2520Xunzhao%2520Yin%2520and%2520Can%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520artificial%2520intelligence%2520has%2520raised%2520concerns%250Aregarding%2520its%2520trustworthiness%252C%2520especially%2520in%2520terms%2520of%2520interpretability%2520and%250Arobustness.%2520Tree-based%2520models%2520like%2520Random%2520Forest%2520and%2520XGBoost%2520excel%2520in%250Ainterpretability%2520and%2520accuracy%2520for%2520tabular%2520data%252C%2520but%2520scaling%2520them%2520remains%250Acomputationally%2520expensive%2520due%2520to%2520poor%2520data%2520locality%2520and%2520high%2520data%2520dependence.%250APrevious%2520efforts%2520to%2520accelerate%2520these%2520models%2520with%2520analog%2520content%2520addressable%250Amemory%2520%2528CAM%2529%2520have%2520struggled%252C%2520due%2520to%2520the%2520fact%2520that%2520the%2520difficult-to-implement%250Asharp%2520decision%2520boundaries%2520are%2520highly%2520susceptible%2520to%2520device%2520variations%252C%2520which%250Aleads%2520to%2520poor%2520hardware%2520performance%2520and%2520vulnerability%2520to%2520adversarial%2520attacks.%250AThis%2520work%2520presents%2520a%2520novel%2520hardware-software%2520co-design%2520approach%2520using%2520%2524MoS_2%2524%250AFlash-based%2520analog%2520CAM%2520with%2520inherent%2520soft%2520boundaries%252C%2520enabling%2520efficient%250Ainference%2520with%2520soft%2520tree-based%2520models.%2520Our%2520soft%2520tree%2520model%2520inference%250Aexperiments%2520on%2520%2524MoS_2%2524%2520analog%2520CAM%2520arrays%2520show%2520this%2520method%2520achieves%2520exceptional%250Arobustness%2520against%2520device%2520variation%2520and%2520adversarial%2520attacks%2520while%2520achieving%250Astate-of-the-art%2520accuracy.%2520Specifically%252C%2520our%2520fabricated%2520analog%2520CAM%2520arrays%250Aachieve%2520%252496%255C%2525%2524%2520accuracy%2520on%2520Wisconsin%2520Diagnostic%2520Breast%2520Cancer%2520%2528WDBC%2529%2520database%252C%250Awhile%2520maintaining%2520decision%2520explainability.%2520Our%2520experimentally%2520calibrated%2520model%250Avalidated%2520only%2520a%2520%25240.6%255C%2525%2524%2520accuracy%2520drop%2520on%2520the%2520MNIST%2520dataset%2520under%2520%252410%255C%2525%2524%2520device%250Athreshold%2520variation%252C%2520compared%2520to%2520a%2520%252445.3%255C%2525%2524%2520drop%2520for%2520traditional%2520decision%250Atrees.%2520This%2520work%2520paves%2520the%2520way%2520for%2520specialized%2520hardware%2520that%2520enhances%2520AI%2527s%250Atrustworthiness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Tree-based%20Machine%20Learning%20by%20%24MoS_2%24%20Flash-based%20Analog%0A%20%20CAM%20with%20Inherent%20Soft%20Boundaries&entry.906535625=Bo%20Wen%20and%20Guoyun%20Gao%20and%20Zhicheng%20Xu%20and%20Ruibin%20Mao%20and%20Xiaojuan%20Qi%20and%20X.%20Sharon%20Hu%20and%20Xunzhao%20Yin%20and%20Can%20Li&entry.1292438233=%20%20The%20rapid%20advancement%20of%20artificial%20intelligence%20has%20raised%20concerns%0Aregarding%20its%20trustworthiness%2C%20especially%20in%20terms%20of%20interpretability%20and%0Arobustness.%20Tree-based%20models%20like%20Random%20Forest%20and%20XGBoost%20excel%20in%0Ainterpretability%20and%20accuracy%20for%20tabular%20data%2C%20but%20scaling%20them%20remains%0Acomputationally%20expensive%20due%20to%20poor%20data%20locality%20and%20high%20data%20dependence.%0APrevious%20efforts%20to%20accelerate%20these%20models%20with%20analog%20content%20addressable%0Amemory%20%28CAM%29%20have%20struggled%2C%20due%20to%20the%20fact%20that%20the%20difficult-to-implement%0Asharp%20decision%20boundaries%20are%20highly%20susceptible%20to%20device%20variations%2C%20which%0Aleads%20to%20poor%20hardware%20performance%20and%20vulnerability%20to%20adversarial%20attacks.%0AThis%20work%20presents%20a%20novel%20hardware-software%20co-design%20approach%20using%20%24MoS_2%24%0AFlash-based%20analog%20CAM%20with%20inherent%20soft%20boundaries%2C%20enabling%20efficient%0Ainference%20with%20soft%20tree-based%20models.%20Our%20soft%20tree%20model%20inference%0Aexperiments%20on%20%24MoS_2%24%20analog%20CAM%20arrays%20show%20this%20method%20achieves%20exceptional%0Arobustness%20against%20device%20variation%20and%20adversarial%20attacks%20while%20achieving%0Astate-of-the-art%20accuracy.%20Specifically%2C%20our%20fabricated%20analog%20CAM%20arrays%0Aachieve%20%2496%5C%25%24%20accuracy%20on%20Wisconsin%20Diagnostic%20Breast%20Cancer%20%28WDBC%29%20database%2C%0Awhile%20maintaining%20decision%20explainability.%20Our%20experimentally%20calibrated%20model%0Avalidated%20only%20a%20%240.6%5C%25%24%20accuracy%20drop%20on%20the%20MNIST%20dataset%20under%20%2410%5C%25%24%20device%0Athreshold%20variation%2C%20compared%20to%20a%20%2445.3%5C%25%24%20drop%20for%20traditional%20decision%0Atrees.%20This%20work%20paves%20the%20way%20for%20specialized%20hardware%20that%20enhances%20AI%27s%0Atrustworthiness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12384v1&entry.124074799=Read"},
{"title": "Flow-GRPO: Training Flow Matching Models via Online RL", "author": "Jie Liu and Gongye Liu and Jiajun Liang and Yangguang Li and Jiaheng Liu and Xintao Wang and Pengfei Wan and Di Zhang and Wanli Ouyang", "abstract": "  We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation.\n", "link": "http://arxiv.org/abs/2505.05470v4", "date": "2025-07-16", "relevancy": 1.5514, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6209}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4893}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow-GRPO%3A%20Training%20Flow%20Matching%20Models%20via%20Online%20RL&body=Title%3A%20Flow-GRPO%3A%20Training%20Flow%20Matching%20Models%20via%20Online%20RL%0AAuthor%3A%20Jie%20Liu%20and%20Gongye%20Liu%20and%20Jiajun%20Liang%20and%20Yangguang%20Li%20and%20Jiaheng%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20We%20propose%20Flow-GRPO%2C%20the%20first%20method%20integrating%20online%20reinforcement%0Alearning%20%28RL%29%20into%20flow%20matching%20models.%20Our%20approach%20uses%20two%20key%20strategies%3A%0A%281%29%20an%20ODE-to-SDE%20conversion%20that%20transforms%20a%20deterministic%20Ordinary%0ADifferential%20Equation%20%28ODE%29%20into%20an%20equivalent%20Stochastic%20Differential%20Equation%0A%28SDE%29%20that%20matches%20the%20original%20model%27s%20marginal%20distribution%20at%20all%20timesteps%2C%0Aenabling%20statistical%20sampling%20for%20RL%20exploration%3B%20and%20%282%29%20a%20Denoising%20Reduction%0Astrategy%20that%20reduces%20training%20denoising%20steps%20while%20retaining%20the%20original%0Ainference%20timestep%20number%2C%20significantly%20improving%20sampling%20efficiency%20without%0Aperformance%20degradation.%20Empirically%2C%20Flow-GRPO%20is%20effective%20across%20multiple%0Atext-to-image%20tasks.%20For%20complex%20compositions%2C%20RL-tuned%20SD3.5%20generates%20nearly%0Aperfect%20object%20counts%2C%20spatial%20relations%2C%20and%20fine-grained%20attributes%2C%20boosting%0AGenEval%20accuracy%20from%2063%25%20to%2095%25.%20In%20visual%20text%20rendering%2C%20its%20accuracy%0Aimproves%20from%2059%25%20to%2092%25%2C%20significantly%20enhancing%20text%20generation.%20Flow-GRPO%0Aalso%20achieves%20substantial%20gains%20in%20human%20preference%20alignment.%20Notably%2C%20very%0Alittle%20reward%20hacking%20occurred%2C%20meaning%20rewards%20did%20not%20increase%20at%20the%20cost%20of%0Aappreciable%20image%20quality%20or%20diversity%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05470v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow-GRPO%253A%2520Training%2520Flow%2520Matching%2520Models%2520via%2520Online%2520RL%26entry.906535625%3DJie%2520Liu%2520and%2520Gongye%2520Liu%2520and%2520Jiajun%2520Liang%2520and%2520Yangguang%2520Li%2520and%2520Jiaheng%2520Liu%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520We%2520propose%2520Flow-GRPO%252C%2520the%2520first%2520method%2520integrating%2520online%2520reinforcement%250Alearning%2520%2528RL%2529%2520into%2520flow%2520matching%2520models.%2520Our%2520approach%2520uses%2520two%2520key%2520strategies%253A%250A%25281%2529%2520an%2520ODE-to-SDE%2520conversion%2520that%2520transforms%2520a%2520deterministic%2520Ordinary%250ADifferential%2520Equation%2520%2528ODE%2529%2520into%2520an%2520equivalent%2520Stochastic%2520Differential%2520Equation%250A%2528SDE%2529%2520that%2520matches%2520the%2520original%2520model%2527s%2520marginal%2520distribution%2520at%2520all%2520timesteps%252C%250Aenabling%2520statistical%2520sampling%2520for%2520RL%2520exploration%253B%2520and%2520%25282%2529%2520a%2520Denoising%2520Reduction%250Astrategy%2520that%2520reduces%2520training%2520denoising%2520steps%2520while%2520retaining%2520the%2520original%250Ainference%2520timestep%2520number%252C%2520significantly%2520improving%2520sampling%2520efficiency%2520without%250Aperformance%2520degradation.%2520Empirically%252C%2520Flow-GRPO%2520is%2520effective%2520across%2520multiple%250Atext-to-image%2520tasks.%2520For%2520complex%2520compositions%252C%2520RL-tuned%2520SD3.5%2520generates%2520nearly%250Aperfect%2520object%2520counts%252C%2520spatial%2520relations%252C%2520and%2520fine-grained%2520attributes%252C%2520boosting%250AGenEval%2520accuracy%2520from%252063%2525%2520to%252095%2525.%2520In%2520visual%2520text%2520rendering%252C%2520its%2520accuracy%250Aimproves%2520from%252059%2525%2520to%252092%2525%252C%2520significantly%2520enhancing%2520text%2520generation.%2520Flow-GRPO%250Aalso%2520achieves%2520substantial%2520gains%2520in%2520human%2520preference%2520alignment.%2520Notably%252C%2520very%250Alittle%2520reward%2520hacking%2520occurred%252C%2520meaning%2520rewards%2520did%2520not%2520increase%2520at%2520the%2520cost%2520of%250Aappreciable%2520image%2520quality%2520or%2520diversity%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05470v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow-GRPO%3A%20Training%20Flow%20Matching%20Models%20via%20Online%20RL&entry.906535625=Jie%20Liu%20and%20Gongye%20Liu%20and%20Jiajun%20Liang%20and%20Yangguang%20Li%20and%20Jiaheng%20Liu%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Wanli%20Ouyang&entry.1292438233=%20%20We%20propose%20Flow-GRPO%2C%20the%20first%20method%20integrating%20online%20reinforcement%0Alearning%20%28RL%29%20into%20flow%20matching%20models.%20Our%20approach%20uses%20two%20key%20strategies%3A%0A%281%29%20an%20ODE-to-SDE%20conversion%20that%20transforms%20a%20deterministic%20Ordinary%0ADifferential%20Equation%20%28ODE%29%20into%20an%20equivalent%20Stochastic%20Differential%20Equation%0A%28SDE%29%20that%20matches%20the%20original%20model%27s%20marginal%20distribution%20at%20all%20timesteps%2C%0Aenabling%20statistical%20sampling%20for%20RL%20exploration%3B%20and%20%282%29%20a%20Denoising%20Reduction%0Astrategy%20that%20reduces%20training%20denoising%20steps%20while%20retaining%20the%20original%0Ainference%20timestep%20number%2C%20significantly%20improving%20sampling%20efficiency%20without%0Aperformance%20degradation.%20Empirically%2C%20Flow-GRPO%20is%20effective%20across%20multiple%0Atext-to-image%20tasks.%20For%20complex%20compositions%2C%20RL-tuned%20SD3.5%20generates%20nearly%0Aperfect%20object%20counts%2C%20spatial%20relations%2C%20and%20fine-grained%20attributes%2C%20boosting%0AGenEval%20accuracy%20from%2063%25%20to%2095%25.%20In%20visual%20text%20rendering%2C%20its%20accuracy%0Aimproves%20from%2059%25%20to%2092%25%2C%20significantly%20enhancing%20text%20generation.%20Flow-GRPO%0Aalso%20achieves%20substantial%20gains%20in%20human%20preference%20alignment.%20Notably%2C%20very%0Alittle%20reward%20hacking%20occurred%2C%20meaning%20rewards%20did%20not%20increase%20at%20the%20cost%20of%0Aappreciable%20image%20quality%20or%20diversity%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05470v4&entry.124074799=Read"},
{"title": "Regrasp Maps for Sequential Manipulation Planning", "author": "Svetlana Levit and Marc Toussaint", "abstract": "  We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.\n", "link": "http://arxiv.org/abs/2507.12407v1", "date": "2025-07-16", "relevancy": 1.5343, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regrasp%20Maps%20for%20Sequential%20Manipulation%20Planning&body=Title%3A%20Regrasp%20Maps%20for%20Sequential%20Manipulation%20Planning%0AAuthor%3A%20Svetlana%20Levit%20and%20Marc%20Toussaint%0AAbstract%3A%20%20%20We%20consider%20manipulation%20problems%20in%20constrained%20and%20cluttered%20settings%2C%0Awhich%20require%20several%20regrasps%20at%20unknown%20locations.%20We%20propose%20to%20inform%20an%0Aoptimization-based%20task%20and%20motion%20planning%20%28TAMP%29%20solver%20with%20possible%20regrasp%0Aareas%20and%20grasp%20sequences%20to%20speed%20up%20the%20search.%20Our%20main%20idea%20is%20to%20use%20a%0Astate%20space%20abstraction%2C%20a%20regrasp%20map%2C%20capturing%20the%20combinations%20of%20available%0Agrasps%20in%20different%20parts%20of%20the%20configuration%20space%2C%20and%20allowing%20us%20to%0Aprovide%20the%20solver%20with%20guesses%20for%20the%20mode%20switches%20and%20additional%0Aconstraints%20for%20the%20object%20placements.%20By%20interleaving%20the%20creation%20of%20regrasp%0Amaps%2C%20their%20adaptation%20based%20on%20failed%20refinements%2C%20and%20solving%20TAMP%0A%28sub%29problems%2C%20we%20are%20able%20to%20provide%20a%20robust%20search%20method%20for%20challenging%0Aregrasp%20manipulation%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegrasp%2520Maps%2520for%2520Sequential%2520Manipulation%2520Planning%26entry.906535625%3DSvetlana%2520Levit%2520and%2520Marc%2520Toussaint%26entry.1292438233%3D%2520%2520We%2520consider%2520manipulation%2520problems%2520in%2520constrained%2520and%2520cluttered%2520settings%252C%250Awhich%2520require%2520several%2520regrasps%2520at%2520unknown%2520locations.%2520We%2520propose%2520to%2520inform%2520an%250Aoptimization-based%2520task%2520and%2520motion%2520planning%2520%2528TAMP%2529%2520solver%2520with%2520possible%2520regrasp%250Aareas%2520and%2520grasp%2520sequences%2520to%2520speed%2520up%2520the%2520search.%2520Our%2520main%2520idea%2520is%2520to%2520use%2520a%250Astate%2520space%2520abstraction%252C%2520a%2520regrasp%2520map%252C%2520capturing%2520the%2520combinations%2520of%2520available%250Agrasps%2520in%2520different%2520parts%2520of%2520the%2520configuration%2520space%252C%2520and%2520allowing%2520us%2520to%250Aprovide%2520the%2520solver%2520with%2520guesses%2520for%2520the%2520mode%2520switches%2520and%2520additional%250Aconstraints%2520for%2520the%2520object%2520placements.%2520By%2520interleaving%2520the%2520creation%2520of%2520regrasp%250Amaps%252C%2520their%2520adaptation%2520based%2520on%2520failed%2520refinements%252C%2520and%2520solving%2520TAMP%250A%2528sub%2529problems%252C%2520we%2520are%2520able%2520to%2520provide%2520a%2520robust%2520search%2520method%2520for%2520challenging%250Aregrasp%2520manipulation%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regrasp%20Maps%20for%20Sequential%20Manipulation%20Planning&entry.906535625=Svetlana%20Levit%20and%20Marc%20Toussaint&entry.1292438233=%20%20We%20consider%20manipulation%20problems%20in%20constrained%20and%20cluttered%20settings%2C%0Awhich%20require%20several%20regrasps%20at%20unknown%20locations.%20We%20propose%20to%20inform%20an%0Aoptimization-based%20task%20and%20motion%20planning%20%28TAMP%29%20solver%20with%20possible%20regrasp%0Aareas%20and%20grasp%20sequences%20to%20speed%20up%20the%20search.%20Our%20main%20idea%20is%20to%20use%20a%0Astate%20space%20abstraction%2C%20a%20regrasp%20map%2C%20capturing%20the%20combinations%20of%20available%0Agrasps%20in%20different%20parts%20of%20the%20configuration%20space%2C%20and%20allowing%20us%20to%0Aprovide%20the%20solver%20with%20guesses%20for%20the%20mode%20switches%20and%20additional%0Aconstraints%20for%20the%20object%20placements.%20By%20interleaving%20the%20creation%20of%20regrasp%0Amaps%2C%20their%20adaptation%20based%20on%20failed%20refinements%2C%20and%20solving%20TAMP%0A%28sub%29problems%2C%20we%20are%20able%20to%20provide%20a%20robust%20search%20method%20for%20challenging%0Aregrasp%20manipulation%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12407v1&entry.124074799=Read"},
{"title": "Reconfigurable legged metamachines that run on autonomous modular legs", "author": "Chen Yu and David Matthews and Jingxian Wang and Jing Gu and Douglas Blackiston and Michael Rubenstein and Sam Kriegman", "abstract": "  Legged machines are becoming increasingly agile and adaptive but they have so\nfar lacked the morphological diversity of legged animals, which have been\nrearranged and reshaped to fill millions of niches. Unlike their biological\ncounterparts, legged machines have largely converged over the past decade to\ncanonical quadrupedal and bipedal architectures that cannot be easily\nreconfigured to meet new tasks or recover from injury. Here we introduce\nautonomous modular legs: agile yet minimal, single-degree-of-freedom jointed\nlinks that can learn complex dynamic behaviors and may be freely attached to\nform legged metamachines at the meter scale. This enables rapid repair,\nredesign, and recombination of highly-dynamic modular agents that move quickly\nand acrobatically (non-quasistatically) through unstructured environments.\nBecause each module is itself a complete agent, legged metamachines are able to\nsustain deep structural damage that would completely disable other legged\nrobots. We also show how to encode the vast space of possible body\nconfigurations into a compact latent design genome that can be efficiently\nexplored, revealing a wide diversity of novel legged forms.\n", "link": "http://arxiv.org/abs/2505.00784v2", "date": "2025-07-16", "relevancy": 1.5031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5507}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5207}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconfigurable%20legged%20metamachines%20that%20run%20on%20autonomous%20modular%20legs&body=Title%3A%20Reconfigurable%20legged%20metamachines%20that%20run%20on%20autonomous%20modular%20legs%0AAuthor%3A%20Chen%20Yu%20and%20David%20Matthews%20and%20Jingxian%20Wang%20and%20Jing%20Gu%20and%20Douglas%20Blackiston%20and%20Michael%20Rubenstein%20and%20Sam%20Kriegman%0AAbstract%3A%20%20%20Legged%20machines%20are%20becoming%20increasingly%20agile%20and%20adaptive%20but%20they%20have%20so%0Afar%20lacked%20the%20morphological%20diversity%20of%20legged%20animals%2C%20which%20have%20been%0Arearranged%20and%20reshaped%20to%20fill%20millions%20of%20niches.%20Unlike%20their%20biological%0Acounterparts%2C%20legged%20machines%20have%20largely%20converged%20over%20the%20past%20decade%20to%0Acanonical%20quadrupedal%20and%20bipedal%20architectures%20that%20cannot%20be%20easily%0Areconfigured%20to%20meet%20new%20tasks%20or%20recover%20from%20injury.%20Here%20we%20introduce%0Aautonomous%20modular%20legs%3A%20agile%20yet%20minimal%2C%20single-degree-of-freedom%20jointed%0Alinks%20that%20can%20learn%20complex%20dynamic%20behaviors%20and%20may%20be%20freely%20attached%20to%0Aform%20legged%20metamachines%20at%20the%20meter%20scale.%20This%20enables%20rapid%20repair%2C%0Aredesign%2C%20and%20recombination%20of%20highly-dynamic%20modular%20agents%20that%20move%20quickly%0Aand%20acrobatically%20%28non-quasistatically%29%20through%20unstructured%20environments.%0ABecause%20each%20module%20is%20itself%20a%20complete%20agent%2C%20legged%20metamachines%20are%20able%20to%0Asustain%20deep%20structural%20damage%20that%20would%20completely%20disable%20other%20legged%0Arobots.%20We%20also%20show%20how%20to%20encode%20the%20vast%20space%20of%20possible%20body%0Aconfigurations%20into%20a%20compact%20latent%20design%20genome%20that%20can%20be%20efficiently%0Aexplored%2C%20revealing%20a%20wide%20diversity%20of%20novel%20legged%20forms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconfigurable%2520legged%2520metamachines%2520that%2520run%2520on%2520autonomous%2520modular%2520legs%26entry.906535625%3DChen%2520Yu%2520and%2520David%2520Matthews%2520and%2520Jingxian%2520Wang%2520and%2520Jing%2520Gu%2520and%2520Douglas%2520Blackiston%2520and%2520Michael%2520Rubenstein%2520and%2520Sam%2520Kriegman%26entry.1292438233%3D%2520%2520Legged%2520machines%2520are%2520becoming%2520increasingly%2520agile%2520and%2520adaptive%2520but%2520they%2520have%2520so%250Afar%2520lacked%2520the%2520morphological%2520diversity%2520of%2520legged%2520animals%252C%2520which%2520have%2520been%250Arearranged%2520and%2520reshaped%2520to%2520fill%2520millions%2520of%2520niches.%2520Unlike%2520their%2520biological%250Acounterparts%252C%2520legged%2520machines%2520have%2520largely%2520converged%2520over%2520the%2520past%2520decade%2520to%250Acanonical%2520quadrupedal%2520and%2520bipedal%2520architectures%2520that%2520cannot%2520be%2520easily%250Areconfigured%2520to%2520meet%2520new%2520tasks%2520or%2520recover%2520from%2520injury.%2520Here%2520we%2520introduce%250Aautonomous%2520modular%2520legs%253A%2520agile%2520yet%2520minimal%252C%2520single-degree-of-freedom%2520jointed%250Alinks%2520that%2520can%2520learn%2520complex%2520dynamic%2520behaviors%2520and%2520may%2520be%2520freely%2520attached%2520to%250Aform%2520legged%2520metamachines%2520at%2520the%2520meter%2520scale.%2520This%2520enables%2520rapid%2520repair%252C%250Aredesign%252C%2520and%2520recombination%2520of%2520highly-dynamic%2520modular%2520agents%2520that%2520move%2520quickly%250Aand%2520acrobatically%2520%2528non-quasistatically%2529%2520through%2520unstructured%2520environments.%250ABecause%2520each%2520module%2520is%2520itself%2520a%2520complete%2520agent%252C%2520legged%2520metamachines%2520are%2520able%2520to%250Asustain%2520deep%2520structural%2520damage%2520that%2520would%2520completely%2520disable%2520other%2520legged%250Arobots.%2520We%2520also%2520show%2520how%2520to%2520encode%2520the%2520vast%2520space%2520of%2520possible%2520body%250Aconfigurations%2520into%2520a%2520compact%2520latent%2520design%2520genome%2520that%2520can%2520be%2520efficiently%250Aexplored%252C%2520revealing%2520a%2520wide%2520diversity%2520of%2520novel%2520legged%2520forms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconfigurable%20legged%20metamachines%20that%20run%20on%20autonomous%20modular%20legs&entry.906535625=Chen%20Yu%20and%20David%20Matthews%20and%20Jingxian%20Wang%20and%20Jing%20Gu%20and%20Douglas%20Blackiston%20and%20Michael%20Rubenstein%20and%20Sam%20Kriegman&entry.1292438233=%20%20Legged%20machines%20are%20becoming%20increasingly%20agile%20and%20adaptive%20but%20they%20have%20so%0Afar%20lacked%20the%20morphological%20diversity%20of%20legged%20animals%2C%20which%20have%20been%0Arearranged%20and%20reshaped%20to%20fill%20millions%20of%20niches.%20Unlike%20their%20biological%0Acounterparts%2C%20legged%20machines%20have%20largely%20converged%20over%20the%20past%20decade%20to%0Acanonical%20quadrupedal%20and%20bipedal%20architectures%20that%20cannot%20be%20easily%0Areconfigured%20to%20meet%20new%20tasks%20or%20recover%20from%20injury.%20Here%20we%20introduce%0Aautonomous%20modular%20legs%3A%20agile%20yet%20minimal%2C%20single-degree-of-freedom%20jointed%0Alinks%20that%20can%20learn%20complex%20dynamic%20behaviors%20and%20may%20be%20freely%20attached%20to%0Aform%20legged%20metamachines%20at%20the%20meter%20scale.%20This%20enables%20rapid%20repair%2C%0Aredesign%2C%20and%20recombination%20of%20highly-dynamic%20modular%20agents%20that%20move%20quickly%0Aand%20acrobatically%20%28non-quasistatically%29%20through%20unstructured%20environments.%0ABecause%20each%20module%20is%20itself%20a%20complete%20agent%2C%20legged%20metamachines%20are%20able%20to%0Asustain%20deep%20structural%20damage%20that%20would%20completely%20disable%20other%20legged%0Arobots.%20We%20also%20show%20how%20to%20encode%20the%20vast%20space%20of%20possible%20body%0Aconfigurations%20into%20a%20compact%20latent%20design%20genome%20that%20can%20be%20efficiently%0Aexplored%2C%20revealing%20a%20wide%20diversity%20of%20novel%20legged%20forms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00784v2&entry.124074799=Read"},
{"title": "MARS: Unleashing the Power of Variance Reduction for Training Large\n  Models", "author": "Huizhuo Yuan and Yifeng Liu and Shuang Wu and Xun Zhou and Quanquan Gu", "abstract": "  Training deep neural networks--and more recently, large models demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin. The\nimplementation of MARS is available at https://github.com/AGI-Arena/MARS.\n", "link": "http://arxiv.org/abs/2411.10438v3", "date": "2025-07-16", "relevancy": 1.5001, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5026}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models&body=Title%3A%20MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models%0AAuthor%3A%20Huizhuo%20Yuan%20and%20Yifeng%20Liu%20and%20Shuang%20Wu%20and%20Xun%20Zhou%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Training%20deep%20neural%20networks--and%20more%20recently%2C%20large%20models%20demands%0Aefficient%20and%20scalable%20optimizers.%20Adaptive%20gradient%20algorithms%20like%20Adam%2C%0AAdamW%2C%20and%20their%20variants%20have%20been%20central%20to%20this%20task.%20Despite%20the%0Adevelopment%20of%20numerous%20variance%20reduction%20algorithms%20in%20the%20past%20decade%20aimed%0Aat%20accelerating%20stochastic%20optimization%20in%20both%20convex%20and%20nonconvex%20settings%2C%0Avariance%20reduction%20has%20not%20found%20widespread%20success%20in%20training%20deep%20neural%0Anetworks%20or%20large%20language%20models.%20Consequently%2C%20it%20has%20remained%20a%20less%20favored%0Aapproach%20in%20modern%20AI.%20In%20this%20paper%2C%20to%20unleash%20the%20power%20of%20variance%0Areduction%20for%20efficient%20training%20of%20large%20models%2C%20we%20propose%20a%20unified%0Aoptimization%20framework%2C%20MARS%20%28Make%20vAriance%20Reduction%20Shine%29%2C%20which%20reconciles%0Apreconditioned%20gradient%20methods%20with%20variance%20reduction%20via%20a%20scaled%20stochastic%0Arecursive%20momentum%20technique.%20Within%20our%20framework%2C%20we%20introduce%20three%0Ainstances%20of%20MARS%20that%20leverage%20preconditioned%20gradient%20updates%20based%20on%20AdamW%2C%0ALion%2C%20and%20Shampoo%2C%20respectively.%20We%20also%20draw%20a%20connection%20between%20our%0Aalgorithms%20and%20existing%20optimizers.%20Experimental%20results%20on%20training%20GPT-2%0Amodels%20indicate%20that%20MARS%20consistently%20outperforms%20AdamW%20by%20a%20large%20margin.%20The%0Aimplementation%20of%20MARS%20is%20available%20at%20https%3A//github.com/AGI-Arena/MARS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10438v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARS%253A%2520Unleashing%2520the%2520Power%2520of%2520Variance%2520Reduction%2520for%2520Training%2520Large%250A%2520%2520Models%26entry.906535625%3DHuizhuo%2520Yuan%2520and%2520Yifeng%2520Liu%2520and%2520Shuang%2520Wu%2520and%2520Xun%2520Zhou%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Training%2520deep%2520neural%2520networks--and%2520more%2520recently%252C%2520large%2520models%2520demands%250Aefficient%2520and%2520scalable%2520optimizers.%2520Adaptive%2520gradient%2520algorithms%2520like%2520Adam%252C%250AAdamW%252C%2520and%2520their%2520variants%2520have%2520been%2520central%2520to%2520this%2520task.%2520Despite%2520the%250Adevelopment%2520of%2520numerous%2520variance%2520reduction%2520algorithms%2520in%2520the%2520past%2520decade%2520aimed%250Aat%2520accelerating%2520stochastic%2520optimization%2520in%2520both%2520convex%2520and%2520nonconvex%2520settings%252C%250Avariance%2520reduction%2520has%2520not%2520found%2520widespread%2520success%2520in%2520training%2520deep%2520neural%250Anetworks%2520or%2520large%2520language%2520models.%2520Consequently%252C%2520it%2520has%2520remained%2520a%2520less%2520favored%250Aapproach%2520in%2520modern%2520AI.%2520In%2520this%2520paper%252C%2520to%2520unleash%2520the%2520power%2520of%2520variance%250Areduction%2520for%2520efficient%2520training%2520of%2520large%2520models%252C%2520we%2520propose%2520a%2520unified%250Aoptimization%2520framework%252C%2520MARS%2520%2528Make%2520vAriance%2520Reduction%2520Shine%2529%252C%2520which%2520reconciles%250Apreconditioned%2520gradient%2520methods%2520with%2520variance%2520reduction%2520via%2520a%2520scaled%2520stochastic%250Arecursive%2520momentum%2520technique.%2520Within%2520our%2520framework%252C%2520we%2520introduce%2520three%250Ainstances%2520of%2520MARS%2520that%2520leverage%2520preconditioned%2520gradient%2520updates%2520based%2520on%2520AdamW%252C%250ALion%252C%2520and%2520Shampoo%252C%2520respectively.%2520We%2520also%2520draw%2520a%2520connection%2520between%2520our%250Aalgorithms%2520and%2520existing%2520optimizers.%2520Experimental%2520results%2520on%2520training%2520GPT-2%250Amodels%2520indicate%2520that%2520MARS%2520consistently%2520outperforms%2520AdamW%2520by%2520a%2520large%2520margin.%2520The%250Aimplementation%2520of%2520MARS%2520is%2520available%2520at%2520https%253A//github.com/AGI-Arena/MARS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10438v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARS%3A%20Unleashing%20the%20Power%20of%20Variance%20Reduction%20for%20Training%20Large%0A%20%20Models&entry.906535625=Huizhuo%20Yuan%20and%20Yifeng%20Liu%20and%20Shuang%20Wu%20and%20Xun%20Zhou%20and%20Quanquan%20Gu&entry.1292438233=%20%20Training%20deep%20neural%20networks--and%20more%20recently%2C%20large%20models%20demands%0Aefficient%20and%20scalable%20optimizers.%20Adaptive%20gradient%20algorithms%20like%20Adam%2C%0AAdamW%2C%20and%20their%20variants%20have%20been%20central%20to%20this%20task.%20Despite%20the%0Adevelopment%20of%20numerous%20variance%20reduction%20algorithms%20in%20the%20past%20decade%20aimed%0Aat%20accelerating%20stochastic%20optimization%20in%20both%20convex%20and%20nonconvex%20settings%2C%0Avariance%20reduction%20has%20not%20found%20widespread%20success%20in%20training%20deep%20neural%0Anetworks%20or%20large%20language%20models.%20Consequently%2C%20it%20has%20remained%20a%20less%20favored%0Aapproach%20in%20modern%20AI.%20In%20this%20paper%2C%20to%20unleash%20the%20power%20of%20variance%0Areduction%20for%20efficient%20training%20of%20large%20models%2C%20we%20propose%20a%20unified%0Aoptimization%20framework%2C%20MARS%20%28Make%20vAriance%20Reduction%20Shine%29%2C%20which%20reconciles%0Apreconditioned%20gradient%20methods%20with%20variance%20reduction%20via%20a%20scaled%20stochastic%0Arecursive%20momentum%20technique.%20Within%20our%20framework%2C%20we%20introduce%20three%0Ainstances%20of%20MARS%20that%20leverage%20preconditioned%20gradient%20updates%20based%20on%20AdamW%2C%0ALion%2C%20and%20Shampoo%2C%20respectively.%20We%20also%20draw%20a%20connection%20between%20our%0Aalgorithms%20and%20existing%20optimizers.%20Experimental%20results%20on%20training%20GPT-2%0Amodels%20indicate%20that%20MARS%20consistently%20outperforms%20AdamW%20by%20a%20large%20margin.%20The%0Aimplementation%20of%20MARS%20is%20available%20at%20https%3A//github.com/AGI-Arena/MARS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10438v3&entry.124074799=Read"},
{"title": "RACER: Rational Artificial Intelligence Car-following-model Enhanced by\n  Reality", "author": "Tianyi Li and Alexander Halatsis and Raphael Stern", "abstract": "  This paper introduces RACER, the Rational Artificial Intelligence\nCar-following model Enhanced by Reality, a cutting-edge deep learning\ncar-following model, that satisfies partial derivative constraints, designed to\npredict Adaptive Cruise Control (ACC) driving behavior while staying\ntheoretically feasible. Unlike conventional models, RACER effectively\nintegrates Rational Driving Constraints (RDCs), crucial tenets of actual\ndriving, resulting in strikingly accurate and realistic predictions. Against\nestablished models like the Optimal Velocity Relative Velocity (OVRV), a\ncar-following Neural Network (NN), and a car-following Physics-Informed Neural\nNetwork (PINN), RACER excels across key metrics, such as acceleration,\nvelocity, and spacing. Notably, it displays a perfect adherence to the RDCs,\nregistering zero violations, in stark contrast to other models. This study\nhighlights the immense value of incorporating physical constraints within AI\nmodels, especially for augmenting safety measures in transportation. It also\npaves the way for future research to test these models against human driving\ndata, with the potential to guide safer and more rational driving behavior. The\nversatility of the proposed model, including its potential to incorporate\nadditional derivative constraints and broader architectural applications,\nenhances its appeal and broadens its impact within the scientific community.\n", "link": "http://arxiv.org/abs/2312.07003v2", "date": "2025-07-16", "relevancy": 1.4944, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4988}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RACER%3A%20Rational%20Artificial%20Intelligence%20Car-following-model%20Enhanced%20by%0A%20%20Reality&body=Title%3A%20RACER%3A%20Rational%20Artificial%20Intelligence%20Car-following-model%20Enhanced%20by%0A%20%20Reality%0AAuthor%3A%20Tianyi%20Li%20and%20Alexander%20Halatsis%20and%20Raphael%20Stern%0AAbstract%3A%20%20%20This%20paper%20introduces%20RACER%2C%20the%20Rational%20Artificial%20Intelligence%0ACar-following%20model%20Enhanced%20by%20Reality%2C%20a%20cutting-edge%20deep%20learning%0Acar-following%20model%2C%20that%20satisfies%20partial%20derivative%20constraints%2C%20designed%20to%0Apredict%20Adaptive%20Cruise%20Control%20%28ACC%29%20driving%20behavior%20while%20staying%0Atheoretically%20feasible.%20Unlike%20conventional%20models%2C%20RACER%20effectively%0Aintegrates%20Rational%20Driving%20Constraints%20%28RDCs%29%2C%20crucial%20tenets%20of%20actual%0Adriving%2C%20resulting%20in%20strikingly%20accurate%20and%20realistic%20predictions.%20Against%0Aestablished%20models%20like%20the%20Optimal%20Velocity%20Relative%20Velocity%20%28OVRV%29%2C%20a%0Acar-following%20Neural%20Network%20%28NN%29%2C%20and%20a%20car-following%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%2C%20RACER%20excels%20across%20key%20metrics%2C%20such%20as%20acceleration%2C%0Avelocity%2C%20and%20spacing.%20Notably%2C%20it%20displays%20a%20perfect%20adherence%20to%20the%20RDCs%2C%0Aregistering%20zero%20violations%2C%20in%20stark%20contrast%20to%20other%20models.%20This%20study%0Ahighlights%20the%20immense%20value%20of%20incorporating%20physical%20constraints%20within%20AI%0Amodels%2C%20especially%20for%20augmenting%20safety%20measures%20in%20transportation.%20It%20also%0Apaves%20the%20way%20for%20future%20research%20to%20test%20these%20models%20against%20human%20driving%0Adata%2C%20with%20the%20potential%20to%20guide%20safer%20and%20more%20rational%20driving%20behavior.%20The%0Aversatility%20of%20the%20proposed%20model%2C%20including%20its%20potential%20to%20incorporate%0Aadditional%20derivative%20constraints%20and%20broader%20architectural%20applications%2C%0Aenhances%20its%20appeal%20and%20broadens%20its%20impact%20within%20the%20scientific%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRACER%253A%2520Rational%2520Artificial%2520Intelligence%2520Car-following-model%2520Enhanced%2520by%250A%2520%2520Reality%26entry.906535625%3DTianyi%2520Li%2520and%2520Alexander%2520Halatsis%2520and%2520Raphael%2520Stern%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520RACER%252C%2520the%2520Rational%2520Artificial%2520Intelligence%250ACar-following%2520model%2520Enhanced%2520by%2520Reality%252C%2520a%2520cutting-edge%2520deep%2520learning%250Acar-following%2520model%252C%2520that%2520satisfies%2520partial%2520derivative%2520constraints%252C%2520designed%2520to%250Apredict%2520Adaptive%2520Cruise%2520Control%2520%2528ACC%2529%2520driving%2520behavior%2520while%2520staying%250Atheoretically%2520feasible.%2520Unlike%2520conventional%2520models%252C%2520RACER%2520effectively%250Aintegrates%2520Rational%2520Driving%2520Constraints%2520%2528RDCs%2529%252C%2520crucial%2520tenets%2520of%2520actual%250Adriving%252C%2520resulting%2520in%2520strikingly%2520accurate%2520and%2520realistic%2520predictions.%2520Against%250Aestablished%2520models%2520like%2520the%2520Optimal%2520Velocity%2520Relative%2520Velocity%2520%2528OVRV%2529%252C%2520a%250Acar-following%2520Neural%2520Network%2520%2528NN%2529%252C%2520and%2520a%2520car-following%2520Physics-Informed%2520Neural%250ANetwork%2520%2528PINN%2529%252C%2520RACER%2520excels%2520across%2520key%2520metrics%252C%2520such%2520as%2520acceleration%252C%250Avelocity%252C%2520and%2520spacing.%2520Notably%252C%2520it%2520displays%2520a%2520perfect%2520adherence%2520to%2520the%2520RDCs%252C%250Aregistering%2520zero%2520violations%252C%2520in%2520stark%2520contrast%2520to%2520other%2520models.%2520This%2520study%250Ahighlights%2520the%2520immense%2520value%2520of%2520incorporating%2520physical%2520constraints%2520within%2520AI%250Amodels%252C%2520especially%2520for%2520augmenting%2520safety%2520measures%2520in%2520transportation.%2520It%2520also%250Apaves%2520the%2520way%2520for%2520future%2520research%2520to%2520test%2520these%2520models%2520against%2520human%2520driving%250Adata%252C%2520with%2520the%2520potential%2520to%2520guide%2520safer%2520and%2520more%2520rational%2520driving%2520behavior.%2520The%250Aversatility%2520of%2520the%2520proposed%2520model%252C%2520including%2520its%2520potential%2520to%2520incorporate%250Aadditional%2520derivative%2520constraints%2520and%2520broader%2520architectural%2520applications%252C%250Aenhances%2520its%2520appeal%2520and%2520broadens%2520its%2520impact%2520within%2520the%2520scientific%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACER%3A%20Rational%20Artificial%20Intelligence%20Car-following-model%20Enhanced%20by%0A%20%20Reality&entry.906535625=Tianyi%20Li%20and%20Alexander%20Halatsis%20and%20Raphael%20Stern&entry.1292438233=%20%20This%20paper%20introduces%20RACER%2C%20the%20Rational%20Artificial%20Intelligence%0ACar-following%20model%20Enhanced%20by%20Reality%2C%20a%20cutting-edge%20deep%20learning%0Acar-following%20model%2C%20that%20satisfies%20partial%20derivative%20constraints%2C%20designed%20to%0Apredict%20Adaptive%20Cruise%20Control%20%28ACC%29%20driving%20behavior%20while%20staying%0Atheoretically%20feasible.%20Unlike%20conventional%20models%2C%20RACER%20effectively%0Aintegrates%20Rational%20Driving%20Constraints%20%28RDCs%29%2C%20crucial%20tenets%20of%20actual%0Adriving%2C%20resulting%20in%20strikingly%20accurate%20and%20realistic%20predictions.%20Against%0Aestablished%20models%20like%20the%20Optimal%20Velocity%20Relative%20Velocity%20%28OVRV%29%2C%20a%0Acar-following%20Neural%20Network%20%28NN%29%2C%20and%20a%20car-following%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%2C%20RACER%20excels%20across%20key%20metrics%2C%20such%20as%20acceleration%2C%0Avelocity%2C%20and%20spacing.%20Notably%2C%20it%20displays%20a%20perfect%20adherence%20to%20the%20RDCs%2C%0Aregistering%20zero%20violations%2C%20in%20stark%20contrast%20to%20other%20models.%20This%20study%0Ahighlights%20the%20immense%20value%20of%20incorporating%20physical%20constraints%20within%20AI%0Amodels%2C%20especially%20for%20augmenting%20safety%20measures%20in%20transportation.%20It%20also%0Apaves%20the%20way%20for%20future%20research%20to%20test%20these%20models%20against%20human%20driving%0Adata%2C%20with%20the%20potential%20to%20guide%20safer%20and%20more%20rational%20driving%20behavior.%20The%0Aversatility%20of%20the%20proposed%20model%2C%20including%20its%20potential%20to%20incorporate%0Aadditional%20derivative%20constraints%20and%20broader%20architectural%20applications%2C%0Aenhances%20its%20appeal%20and%20broadens%20its%20impact%20within%20the%20scientific%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07003v2&entry.124074799=Read"},
{"title": "Improving Lightweight Weed Detection via Knowledge Distillation", "author": "Ahmet O\u011fuz Salt\u0131k and Max Voigt and Sourav Modak and Mike Beckworth and Anthony Stein", "abstract": "  Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.\n", "link": "http://arxiv.org/abs/2507.12344v1", "date": "2025-07-16", "relevancy": 1.486, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5109}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5009}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Lightweight%20Weed%20Detection%20via%20Knowledge%20Distillation&body=Title%3A%20Improving%20Lightweight%20Weed%20Detection%20via%20Knowledge%20Distillation%0AAuthor%3A%20Ahmet%20O%C4%9Fuz%20Salt%C4%B1k%20and%20Max%20Voigt%20and%20Sourav%20Modak%20and%20Mike%20Beckworth%20and%20Anthony%20Stein%0AAbstract%3A%20%20%20Weed%20detection%20is%20a%20critical%20component%20of%20precision%20agriculture%2C%20facilitating%0Atargeted%20herbicide%20application%20and%20reducing%20environmental%20impact.%20However%2C%0Adeploying%20accurate%20object%20detection%20models%20on%20resource-limited%20platforms%0Aremains%20challenging%2C%20particularly%20when%20differentiating%20visually%20similar%20weed%0Aspecies%20commonly%20encountered%20in%20plant%20phenotyping%20applications.%20In%20this%20work%2C%0Awe%20investigate%20Channel-wise%20Knowledge%20Distillation%20%28CWD%29%20and%20Masked%20Generative%0ADistillation%20%28MGD%29%20to%20enhance%20the%20performance%20of%20lightweight%20models%20for%0Areal-time%20smart%20spraying%20systems.%20Utilizing%20YOLO11x%20as%20the%20teacher%20model%20and%0AYOLO11n%20as%20both%20reference%20and%20student%2C%20both%20CWD%20and%20MGD%20effectively%20transfer%0Aknowledge%20from%20the%20teacher%20to%20the%20student%20model.%20Our%20experiments%2C%20conducted%20on%0Aa%20real-world%20dataset%20comprising%20sugar%20beet%20crops%20and%20four%20weed%20types%20%28Cirsium%2C%0AConvolvulus%2C%20Fallopia%2C%20and%20Echinochloa%29%2C%20consistently%20show%20increased%20AP50%0Aacross%20all%20classes.%20The%20distilled%20CWD%20student%20model%20achieves%20a%20notable%0Aimprovement%20of%202.5%25%20and%20MGD%20achieves%201.9%25%20in%20mAP50%20over%20the%20baseline%20without%0Aincreasing%20model%20complexity.%20Additionally%2C%20we%20validate%20real-time%20deployment%0Afeasibility%20by%20evaluating%20the%20student%20YOLO11n%20model%20on%20Jetson%20Orin%20Nano%20and%0ARaspberry%20Pi%205%20embedded%20devices%2C%20performing%20five%20independent%20runs%20to%20evaluate%0Aperformance%20stability%20across%20random%20seeds.%20These%20findings%20confirm%20CWD%20and%20MGD%0Aas%20an%20effective%2C%20efficient%2C%20and%20practical%20approach%20for%20improving%20deep%0Alearning-based%20weed%20detection%20accuracy%20in%20precision%20agriculture%20and%20plant%0Aphenotyping%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Lightweight%2520Weed%2520Detection%2520via%2520Knowledge%2520Distillation%26entry.906535625%3DAhmet%2520O%25C4%259Fuz%2520Salt%25C4%25B1k%2520and%2520Max%2520Voigt%2520and%2520Sourav%2520Modak%2520and%2520Mike%2520Beckworth%2520and%2520Anthony%2520Stein%26entry.1292438233%3D%2520%2520Weed%2520detection%2520is%2520a%2520critical%2520component%2520of%2520precision%2520agriculture%252C%2520facilitating%250Atargeted%2520herbicide%2520application%2520and%2520reducing%2520environmental%2520impact.%2520However%252C%250Adeploying%2520accurate%2520object%2520detection%2520models%2520on%2520resource-limited%2520platforms%250Aremains%2520challenging%252C%2520particularly%2520when%2520differentiating%2520visually%2520similar%2520weed%250Aspecies%2520commonly%2520encountered%2520in%2520plant%2520phenotyping%2520applications.%2520In%2520this%2520work%252C%250Awe%2520investigate%2520Channel-wise%2520Knowledge%2520Distillation%2520%2528CWD%2529%2520and%2520Masked%2520Generative%250ADistillation%2520%2528MGD%2529%2520to%2520enhance%2520the%2520performance%2520of%2520lightweight%2520models%2520for%250Areal-time%2520smart%2520spraying%2520systems.%2520Utilizing%2520YOLO11x%2520as%2520the%2520teacher%2520model%2520and%250AYOLO11n%2520as%2520both%2520reference%2520and%2520student%252C%2520both%2520CWD%2520and%2520MGD%2520effectively%2520transfer%250Aknowledge%2520from%2520the%2520teacher%2520to%2520the%2520student%2520model.%2520Our%2520experiments%252C%2520conducted%2520on%250Aa%2520real-world%2520dataset%2520comprising%2520sugar%2520beet%2520crops%2520and%2520four%2520weed%2520types%2520%2528Cirsium%252C%250AConvolvulus%252C%2520Fallopia%252C%2520and%2520Echinochloa%2529%252C%2520consistently%2520show%2520increased%2520AP50%250Aacross%2520all%2520classes.%2520The%2520distilled%2520CWD%2520student%2520model%2520achieves%2520a%2520notable%250Aimprovement%2520of%25202.5%2525%2520and%2520MGD%2520achieves%25201.9%2525%2520in%2520mAP50%2520over%2520the%2520baseline%2520without%250Aincreasing%2520model%2520complexity.%2520Additionally%252C%2520we%2520validate%2520real-time%2520deployment%250Afeasibility%2520by%2520evaluating%2520the%2520student%2520YOLO11n%2520model%2520on%2520Jetson%2520Orin%2520Nano%2520and%250ARaspberry%2520Pi%25205%2520embedded%2520devices%252C%2520performing%2520five%2520independent%2520runs%2520to%2520evaluate%250Aperformance%2520stability%2520across%2520random%2520seeds.%2520These%2520findings%2520confirm%2520CWD%2520and%2520MGD%250Aas%2520an%2520effective%252C%2520efficient%252C%2520and%2520practical%2520approach%2520for%2520improving%2520deep%250Alearning-based%2520weed%2520detection%2520accuracy%2520in%2520precision%2520agriculture%2520and%2520plant%250Aphenotyping%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Lightweight%20Weed%20Detection%20via%20Knowledge%20Distillation&entry.906535625=Ahmet%20O%C4%9Fuz%20Salt%C4%B1k%20and%20Max%20Voigt%20and%20Sourav%20Modak%20and%20Mike%20Beckworth%20and%20Anthony%20Stein&entry.1292438233=%20%20Weed%20detection%20is%20a%20critical%20component%20of%20precision%20agriculture%2C%20facilitating%0Atargeted%20herbicide%20application%20and%20reducing%20environmental%20impact.%20However%2C%0Adeploying%20accurate%20object%20detection%20models%20on%20resource-limited%20platforms%0Aremains%20challenging%2C%20particularly%20when%20differentiating%20visually%20similar%20weed%0Aspecies%20commonly%20encountered%20in%20plant%20phenotyping%20applications.%20In%20this%20work%2C%0Awe%20investigate%20Channel-wise%20Knowledge%20Distillation%20%28CWD%29%20and%20Masked%20Generative%0ADistillation%20%28MGD%29%20to%20enhance%20the%20performance%20of%20lightweight%20models%20for%0Areal-time%20smart%20spraying%20systems.%20Utilizing%20YOLO11x%20as%20the%20teacher%20model%20and%0AYOLO11n%20as%20both%20reference%20and%20student%2C%20both%20CWD%20and%20MGD%20effectively%20transfer%0Aknowledge%20from%20the%20teacher%20to%20the%20student%20model.%20Our%20experiments%2C%20conducted%20on%0Aa%20real-world%20dataset%20comprising%20sugar%20beet%20crops%20and%20four%20weed%20types%20%28Cirsium%2C%0AConvolvulus%2C%20Fallopia%2C%20and%20Echinochloa%29%2C%20consistently%20show%20increased%20AP50%0Aacross%20all%20classes.%20The%20distilled%20CWD%20student%20model%20achieves%20a%20notable%0Aimprovement%20of%202.5%25%20and%20MGD%20achieves%201.9%25%20in%20mAP50%20over%20the%20baseline%20without%0Aincreasing%20model%20complexity.%20Additionally%2C%20we%20validate%20real-time%20deployment%0Afeasibility%20by%20evaluating%20the%20student%20YOLO11n%20model%20on%20Jetson%20Orin%20Nano%20and%0ARaspberry%20Pi%205%20embedded%20devices%2C%20performing%20five%20independent%20runs%20to%20evaluate%0Aperformance%20stability%20across%20random%20seeds.%20These%20findings%20confirm%20CWD%20and%20MGD%0Aas%20an%20effective%2C%20efficient%2C%20and%20practical%20approach%20for%20improving%20deep%0Alearning-based%20weed%20detection%20accuracy%20in%20precision%20agriculture%20and%20plant%0Aphenotyping%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12344v1&entry.124074799=Read"},
{"title": "\"Is it always watching? Is it always listening?\" Exploring Contextual\n  Privacy and Security Concerns Toward Domestic Social Robots", "author": "Henry Bell and Jabari Kwesi and Hiba Laabadli and Pardis Emami-Naeini", "abstract": "  Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality.\n", "link": "http://arxiv.org/abs/2507.10786v2", "date": "2025-07-16", "relevancy": 1.4713, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5373}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Is%20it%20always%20watching%3F%20Is%20it%20always%20listening%3F%22%20Exploring%20Contextual%0A%20%20Privacy%20and%20Security%20Concerns%20Toward%20Domestic%20Social%20Robots&body=Title%3A%20%22Is%20it%20always%20watching%3F%20Is%20it%20always%20listening%3F%22%20Exploring%20Contextual%0A%20%20Privacy%20and%20Security%20Concerns%20Toward%20Domestic%20Social%20Robots%0AAuthor%3A%20Henry%20Bell%20and%20Jabari%20Kwesi%20and%20Hiba%20Laabadli%20and%20Pardis%20Emami-Naeini%0AAbstract%3A%20%20%20Equipped%20with%20artificial%20intelligence%20%28AI%29%20and%20advanced%20sensing%20capabilities%2C%0Asocial%20robots%20are%20gaining%20interest%20among%20consumers%20in%20the%20United%20States.%20These%0Arobots%20seem%20like%20a%20natural%20evolution%20of%20traditional%20smart%20home%20devices.%0AHowever%2C%20their%20extensive%20data%20collection%20capabilities%2C%20anthropomorphic%0Afeatures%2C%20and%20capacity%20to%20interact%20with%20their%20environment%20make%20social%20robots%20a%0Amore%20significant%20security%20and%20privacy%20threat.%20Increased%20risks%20include%20data%0Alinkage%2C%20unauthorized%20data%20sharing%2C%20and%20the%20physical%20safety%20of%20users%20and%20their%0Ahomes.%20It%20is%20critical%20to%20investigate%20U.S.%20users%27%20security%20and%20privacy%20needs%20and%0Aconcerns%20to%20guide%20the%20design%20of%20social%20robots%20while%20these%20devices%20are%20still%20in%0Athe%20early%20stages%20of%20commercialization%20in%20the%20U.S.%20market.%20Through%2019%0Asemi-structured%20interviews%2C%20we%20identified%20significant%20security%20and%20privacy%0Aconcerns%2C%20highlighting%20the%20need%20for%20transparency%2C%20usability%2C%20and%20robust%20privacy%0Acontrols%20to%20support%20adoption.%20For%20educational%20applications%2C%20participants%0Aworried%20most%20about%20misinformation%2C%20and%20in%20medical%20use%20cases%2C%20they%20worried%20about%0Athe%20reliability%20of%20these%20devices.%20Participants%20were%20also%20concerned%20with%20the%0Adata%20inference%20that%20social%20robots%20could%20enable.%20We%20found%20that%20participants%0Aexpect%20tangible%20privacy%20controls%2C%20indicators%20of%20data%20collection%2C%20and%0Acontext-appropriate%20functionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Is%2520it%2520always%2520watching%253F%2520Is%2520it%2520always%2520listening%253F%2522%2520Exploring%2520Contextual%250A%2520%2520Privacy%2520and%2520Security%2520Concerns%2520Toward%2520Domestic%2520Social%2520Robots%26entry.906535625%3DHenry%2520Bell%2520and%2520Jabari%2520Kwesi%2520and%2520Hiba%2520Laabadli%2520and%2520Pardis%2520Emami-Naeini%26entry.1292438233%3D%2520%2520Equipped%2520with%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520advanced%2520sensing%2520capabilities%252C%250Asocial%2520robots%2520are%2520gaining%2520interest%2520among%2520consumers%2520in%2520the%2520United%2520States.%2520These%250Arobots%2520seem%2520like%2520a%2520natural%2520evolution%2520of%2520traditional%2520smart%2520home%2520devices.%250AHowever%252C%2520their%2520extensive%2520data%2520collection%2520capabilities%252C%2520anthropomorphic%250Afeatures%252C%2520and%2520capacity%2520to%2520interact%2520with%2520their%2520environment%2520make%2520social%2520robots%2520a%250Amore%2520significant%2520security%2520and%2520privacy%2520threat.%2520Increased%2520risks%2520include%2520data%250Alinkage%252C%2520unauthorized%2520data%2520sharing%252C%2520and%2520the%2520physical%2520safety%2520of%2520users%2520and%2520their%250Ahomes.%2520It%2520is%2520critical%2520to%2520investigate%2520U.S.%2520users%2527%2520security%2520and%2520privacy%2520needs%2520and%250Aconcerns%2520to%2520guide%2520the%2520design%2520of%2520social%2520robots%2520while%2520these%2520devices%2520are%2520still%2520in%250Athe%2520early%2520stages%2520of%2520commercialization%2520in%2520the%2520U.S.%2520market.%2520Through%252019%250Asemi-structured%2520interviews%252C%2520we%2520identified%2520significant%2520security%2520and%2520privacy%250Aconcerns%252C%2520highlighting%2520the%2520need%2520for%2520transparency%252C%2520usability%252C%2520and%2520robust%2520privacy%250Acontrols%2520to%2520support%2520adoption.%2520For%2520educational%2520applications%252C%2520participants%250Aworried%2520most%2520about%2520misinformation%252C%2520and%2520in%2520medical%2520use%2520cases%252C%2520they%2520worried%2520about%250Athe%2520reliability%2520of%2520these%2520devices.%2520Participants%2520were%2520also%2520concerned%2520with%2520the%250Adata%2520inference%2520that%2520social%2520robots%2520could%2520enable.%2520We%2520found%2520that%2520participants%250Aexpect%2520tangible%2520privacy%2520controls%252C%2520indicators%2520of%2520data%2520collection%252C%2520and%250Acontext-appropriate%2520functionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Is%20it%20always%20watching%3F%20Is%20it%20always%20listening%3F%22%20Exploring%20Contextual%0A%20%20Privacy%20and%20Security%20Concerns%20Toward%20Domestic%20Social%20Robots&entry.906535625=Henry%20Bell%20and%20Jabari%20Kwesi%20and%20Hiba%20Laabadli%20and%20Pardis%20Emami-Naeini&entry.1292438233=%20%20Equipped%20with%20artificial%20intelligence%20%28AI%29%20and%20advanced%20sensing%20capabilities%2C%0Asocial%20robots%20are%20gaining%20interest%20among%20consumers%20in%20the%20United%20States.%20These%0Arobots%20seem%20like%20a%20natural%20evolution%20of%20traditional%20smart%20home%20devices.%0AHowever%2C%20their%20extensive%20data%20collection%20capabilities%2C%20anthropomorphic%0Afeatures%2C%20and%20capacity%20to%20interact%20with%20their%20environment%20make%20social%20robots%20a%0Amore%20significant%20security%20and%20privacy%20threat.%20Increased%20risks%20include%20data%0Alinkage%2C%20unauthorized%20data%20sharing%2C%20and%20the%20physical%20safety%20of%20users%20and%20their%0Ahomes.%20It%20is%20critical%20to%20investigate%20U.S.%20users%27%20security%20and%20privacy%20needs%20and%0Aconcerns%20to%20guide%20the%20design%20of%20social%20robots%20while%20these%20devices%20are%20still%20in%0Athe%20early%20stages%20of%20commercialization%20in%20the%20U.S.%20market.%20Through%2019%0Asemi-structured%20interviews%2C%20we%20identified%20significant%20security%20and%20privacy%0Aconcerns%2C%20highlighting%20the%20need%20for%20transparency%2C%20usability%2C%20and%20robust%20privacy%0Acontrols%20to%20support%20adoption.%20For%20educational%20applications%2C%20participants%0Aworried%20most%20about%20misinformation%2C%20and%20in%20medical%20use%20cases%2C%20they%20worried%20about%0Athe%20reliability%20of%20these%20devices.%20Participants%20were%20also%20concerned%20with%20the%0Adata%20inference%20that%20social%20robots%20could%20enable.%20We%20found%20that%20participants%0Aexpect%20tangible%20privacy%20controls%2C%20indicators%20of%20data%20collection%2C%20and%0Acontext-appropriate%20functionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10786v2&entry.124074799=Read"},
{"title": "Compositional Discrete Latent Code for High Fidelity, Productive\n  Diffusion Models", "author": "Samuel Lavoie and Michael Noukhovitch and Aaron Courville", "abstract": "  We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.\n", "link": "http://arxiv.org/abs/2507.12318v1", "date": "2025-07-16", "relevancy": 1.2974, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6949}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6256}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Discrete%20Latent%20Code%20for%20High%20Fidelity%2C%20Productive%0A%20%20Diffusion%20Models&body=Title%3A%20Compositional%20Discrete%20Latent%20Code%20for%20High%20Fidelity%2C%20Productive%0A%20%20Diffusion%20Models%0AAuthor%3A%20Samuel%20Lavoie%20and%20Michael%20Noukhovitch%20and%20Aaron%20Courville%0AAbstract%3A%20%20%20We%20argue%20that%20diffusion%20models%27%20success%20in%20modeling%20complex%20distributions%20is%2C%0Afor%20the%20most%20part%2C%20coming%20from%20their%20input%20conditioning.%20This%20paper%0Ainvestigates%20the%20representation%20used%20to%20condition%20diffusion%20models%20from%20the%0Aperspective%20that%20ideal%20representations%20should%20improve%20sample%20fidelity%2C%20be%20easy%0Ato%20generate%2C%20and%20be%20compositional%20to%20allow%20out-of-training%20samples%20generation.%0AWe%20introduce%20Discrete%20Latent%20Code%20%28DLC%29%2C%20an%20image%20representation%20derived%20from%0ASimplicial%20Embeddings%20trained%20with%20a%20self-supervised%20learning%20objective.%20DLCs%0Aare%20sequences%20of%20discrete%20tokens%2C%20as%20opposed%20to%20the%20standard%20continuous%20image%0Aembeddings.%20They%20are%20easy%20to%20generate%20and%20their%20compositionality%20enables%0Asampling%20of%20novel%20images%20beyond%20the%20training%20distribution.%20Diffusion%20models%0Atrained%20with%20DLCs%20have%20improved%20generation%20fidelity%2C%20establishing%20a%20new%0Astate-of-the-art%20for%20unconditional%20image%20generation%20on%20ImageNet.%20Additionally%2C%0Awe%20show%20that%20composing%20DLCs%20allows%20the%20image%20generator%20to%20produce%0Aout-of-distribution%20samples%20that%20coherently%20combine%20the%20semantics%20of%20images%20in%0Adiverse%20ways.%20Finally%2C%20we%20showcase%20how%20DLCs%20can%20enable%20text-to-image%20generation%0Aby%20leveraging%20large-scale%20pretrained%20language%20models.%20We%20efficiently%20finetune%20a%0Atext%20diffusion%20language%20model%20to%20generate%20DLCs%20that%20produce%20novel%20samples%0Aoutside%20of%20the%20image%20generator%20training%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Discrete%2520Latent%2520Code%2520for%2520High%2520Fidelity%252C%2520Productive%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DSamuel%2520Lavoie%2520and%2520Michael%2520Noukhovitch%2520and%2520Aaron%2520Courville%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520diffusion%2520models%2527%2520success%2520in%2520modeling%2520complex%2520distributions%2520is%252C%250Afor%2520the%2520most%2520part%252C%2520coming%2520from%2520their%2520input%2520conditioning.%2520This%2520paper%250Ainvestigates%2520the%2520representation%2520used%2520to%2520condition%2520diffusion%2520models%2520from%2520the%250Aperspective%2520that%2520ideal%2520representations%2520should%2520improve%2520sample%2520fidelity%252C%2520be%2520easy%250Ato%2520generate%252C%2520and%2520be%2520compositional%2520to%2520allow%2520out-of-training%2520samples%2520generation.%250AWe%2520introduce%2520Discrete%2520Latent%2520Code%2520%2528DLC%2529%252C%2520an%2520image%2520representation%2520derived%2520from%250ASimplicial%2520Embeddings%2520trained%2520with%2520a%2520self-supervised%2520learning%2520objective.%2520DLCs%250Aare%2520sequences%2520of%2520discrete%2520tokens%252C%2520as%2520opposed%2520to%2520the%2520standard%2520continuous%2520image%250Aembeddings.%2520They%2520are%2520easy%2520to%2520generate%2520and%2520their%2520compositionality%2520enables%250Asampling%2520of%2520novel%2520images%2520beyond%2520the%2520training%2520distribution.%2520Diffusion%2520models%250Atrained%2520with%2520DLCs%2520have%2520improved%2520generation%2520fidelity%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520for%2520unconditional%2520image%2520generation%2520on%2520ImageNet.%2520Additionally%252C%250Awe%2520show%2520that%2520composing%2520DLCs%2520allows%2520the%2520image%2520generator%2520to%2520produce%250Aout-of-distribution%2520samples%2520that%2520coherently%2520combine%2520the%2520semantics%2520of%2520images%2520in%250Adiverse%2520ways.%2520Finally%252C%2520we%2520showcase%2520how%2520DLCs%2520can%2520enable%2520text-to-image%2520generation%250Aby%2520leveraging%2520large-scale%2520pretrained%2520language%2520models.%2520We%2520efficiently%2520finetune%2520a%250Atext%2520diffusion%2520language%2520model%2520to%2520generate%2520DLCs%2520that%2520produce%2520novel%2520samples%250Aoutside%2520of%2520the%2520image%2520generator%2520training%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Discrete%20Latent%20Code%20for%20High%20Fidelity%2C%20Productive%0A%20%20Diffusion%20Models&entry.906535625=Samuel%20Lavoie%20and%20Michael%20Noukhovitch%20and%20Aaron%20Courville&entry.1292438233=%20%20We%20argue%20that%20diffusion%20models%27%20success%20in%20modeling%20complex%20distributions%20is%2C%0Afor%20the%20most%20part%2C%20coming%20from%20their%20input%20conditioning.%20This%20paper%0Ainvestigates%20the%20representation%20used%20to%20condition%20diffusion%20models%20from%20the%0Aperspective%20that%20ideal%20representations%20should%20improve%20sample%20fidelity%2C%20be%20easy%0Ato%20generate%2C%20and%20be%20compositional%20to%20allow%20out-of-training%20samples%20generation.%0AWe%20introduce%20Discrete%20Latent%20Code%20%28DLC%29%2C%20an%20image%20representation%20derived%20from%0ASimplicial%20Embeddings%20trained%20with%20a%20self-supervised%20learning%20objective.%20DLCs%0Aare%20sequences%20of%20discrete%20tokens%2C%20as%20opposed%20to%20the%20standard%20continuous%20image%0Aembeddings.%20They%20are%20easy%20to%20generate%20and%20their%20compositionality%20enables%0Asampling%20of%20novel%20images%20beyond%20the%20training%20distribution.%20Diffusion%20models%0Atrained%20with%20DLCs%20have%20improved%20generation%20fidelity%2C%20establishing%20a%20new%0Astate-of-the-art%20for%20unconditional%20image%20generation%20on%20ImageNet.%20Additionally%2C%0Awe%20show%20that%20composing%20DLCs%20allows%20the%20image%20generator%20to%20produce%0Aout-of-distribution%20samples%20that%20coherently%20combine%20the%20semantics%20of%20images%20in%0Adiverse%20ways.%20Finally%2C%20we%20showcase%20how%20DLCs%20can%20enable%20text-to-image%20generation%0Aby%20leveraging%20large-scale%20pretrained%20language%20models.%20We%20efficiently%20finetune%20a%0Atext%20diffusion%20language%20model%20to%20generate%20DLCs%20that%20produce%20novel%20samples%0Aoutside%20of%20the%20image%20generator%20training%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12318v1&entry.124074799=Read"},
{"title": "Robot Metabolism: Towards machines that can grow by consuming other\n  machines", "author": "Philippe Martin Wyder and Riyaan Bakhda and Meiqi Zhao and Quinn A. Booth and Matthew E. Modi and Andrew Song and Simon Kang and Jiahao Wu and Priya Patel and Robert T. Kasumi and David Yi and Nihar Niraj Garg and Pranav Jhunjhunwala and Siddharth Bhutoria and Evan H. Tong and Yuhang Hu and Judah Goldfeder and Omer Mustel and Donghan Kim and Hod Lipson", "abstract": "  Biological lifeforms can heal, grow, adapt, and reproduce -- abilities\nessential for sustained survival and development. In contrast, robots today are\nprimarily monolithic machines with limited ability to self-repair, physically\ndevelop, or incorporate material from their environments. While robot minds\nrapidly evolve new behaviors through AI, their bodies remain closed systems,\nunable to systematically integrate material to grow or heal. We argue that\nopen-ended physical adaptation is only possible when robots are designed using\na small repertoire of simple modules. This allows machines to mechanically\nadapt by consuming parts from other machines or their surroundings and shed\nbroken components. We demonstrate this principle on a truss modular robot\nplatform. We show how robots can grow bigger, faster, and more capable by\nconsuming materials from their environment and other robots. We suggest that\nmachine metabolic processes like those demonstrated here will be an essential\npart of any sustained future robot ecology.\n", "link": "http://arxiv.org/abs/2411.11192v2", "date": "2025-07-16", "relevancy": 1.4186, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4621}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20Metabolism%3A%20Towards%20machines%20that%20can%20grow%20by%20consuming%20other%0A%20%20machines&body=Title%3A%20Robot%20Metabolism%3A%20Towards%20machines%20that%20can%20grow%20by%20consuming%20other%0A%20%20machines%0AAuthor%3A%20Philippe%20Martin%20Wyder%20and%20Riyaan%20Bakhda%20and%20Meiqi%20Zhao%20and%20Quinn%20A.%20Booth%20and%20Matthew%20E.%20Modi%20and%20Andrew%20Song%20and%20Simon%20Kang%20and%20Jiahao%20Wu%20and%20Priya%20Patel%20and%20Robert%20T.%20Kasumi%20and%20David%20Yi%20and%20Nihar%20Niraj%20Garg%20and%20Pranav%20Jhunjhunwala%20and%20Siddharth%20Bhutoria%20and%20Evan%20H.%20Tong%20and%20Yuhang%20Hu%20and%20Judah%20Goldfeder%20and%20Omer%20Mustel%20and%20Donghan%20Kim%20and%20Hod%20Lipson%0AAbstract%3A%20%20%20Biological%20lifeforms%20can%20heal%2C%20grow%2C%20adapt%2C%20and%20reproduce%20--%20abilities%0Aessential%20for%20sustained%20survival%20and%20development.%20In%20contrast%2C%20robots%20today%20are%0Aprimarily%20monolithic%20machines%20with%20limited%20ability%20to%20self-repair%2C%20physically%0Adevelop%2C%20or%20incorporate%20material%20from%20their%20environments.%20While%20robot%20minds%0Arapidly%20evolve%20new%20behaviors%20through%20AI%2C%20their%20bodies%20remain%20closed%20systems%2C%0Aunable%20to%20systematically%20integrate%20material%20to%20grow%20or%20heal.%20We%20argue%20that%0Aopen-ended%20physical%20adaptation%20is%20only%20possible%20when%20robots%20are%20designed%20using%0Aa%20small%20repertoire%20of%20simple%20modules.%20This%20allows%20machines%20to%20mechanically%0Aadapt%20by%20consuming%20parts%20from%20other%20machines%20or%20their%20surroundings%20and%20shed%0Abroken%20components.%20We%20demonstrate%20this%20principle%20on%20a%20truss%20modular%20robot%0Aplatform.%20We%20show%20how%20robots%20can%20grow%20bigger%2C%20faster%2C%20and%20more%20capable%20by%0Aconsuming%20materials%20from%20their%20environment%20and%20other%20robots.%20We%20suggest%20that%0Amachine%20metabolic%20processes%20like%20those%20demonstrated%20here%20will%20be%20an%20essential%0Apart%20of%20any%20sustained%20future%20robot%20ecology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520Metabolism%253A%2520Towards%2520machines%2520that%2520can%2520grow%2520by%2520consuming%2520other%250A%2520%2520machines%26entry.906535625%3DPhilippe%2520Martin%2520Wyder%2520and%2520Riyaan%2520Bakhda%2520and%2520Meiqi%2520Zhao%2520and%2520Quinn%2520A.%2520Booth%2520and%2520Matthew%2520E.%2520Modi%2520and%2520Andrew%2520Song%2520and%2520Simon%2520Kang%2520and%2520Jiahao%2520Wu%2520and%2520Priya%2520Patel%2520and%2520Robert%2520T.%2520Kasumi%2520and%2520David%2520Yi%2520and%2520Nihar%2520Niraj%2520Garg%2520and%2520Pranav%2520Jhunjhunwala%2520and%2520Siddharth%2520Bhutoria%2520and%2520Evan%2520H.%2520Tong%2520and%2520Yuhang%2520Hu%2520and%2520Judah%2520Goldfeder%2520and%2520Omer%2520Mustel%2520and%2520Donghan%2520Kim%2520and%2520Hod%2520Lipson%26entry.1292438233%3D%2520%2520Biological%2520lifeforms%2520can%2520heal%252C%2520grow%252C%2520adapt%252C%2520and%2520reproduce%2520--%2520abilities%250Aessential%2520for%2520sustained%2520survival%2520and%2520development.%2520In%2520contrast%252C%2520robots%2520today%2520are%250Aprimarily%2520monolithic%2520machines%2520with%2520limited%2520ability%2520to%2520self-repair%252C%2520physically%250Adevelop%252C%2520or%2520incorporate%2520material%2520from%2520their%2520environments.%2520While%2520robot%2520minds%250Arapidly%2520evolve%2520new%2520behaviors%2520through%2520AI%252C%2520their%2520bodies%2520remain%2520closed%2520systems%252C%250Aunable%2520to%2520systematically%2520integrate%2520material%2520to%2520grow%2520or%2520heal.%2520We%2520argue%2520that%250Aopen-ended%2520physical%2520adaptation%2520is%2520only%2520possible%2520when%2520robots%2520are%2520designed%2520using%250Aa%2520small%2520repertoire%2520of%2520simple%2520modules.%2520This%2520allows%2520machines%2520to%2520mechanically%250Aadapt%2520by%2520consuming%2520parts%2520from%2520other%2520machines%2520or%2520their%2520surroundings%2520and%2520shed%250Abroken%2520components.%2520We%2520demonstrate%2520this%2520principle%2520on%2520a%2520truss%2520modular%2520robot%250Aplatform.%2520We%2520show%2520how%2520robots%2520can%2520grow%2520bigger%252C%2520faster%252C%2520and%2520more%2520capable%2520by%250Aconsuming%2520materials%2520from%2520their%2520environment%2520and%2520other%2520robots.%2520We%2520suggest%2520that%250Amachine%2520metabolic%2520processes%2520like%2520those%2520demonstrated%2520here%2520will%2520be%2520an%2520essential%250Apart%2520of%2520any%2520sustained%2520future%2520robot%2520ecology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Metabolism%3A%20Towards%20machines%20that%20can%20grow%20by%20consuming%20other%0A%20%20machines&entry.906535625=Philippe%20Martin%20Wyder%20and%20Riyaan%20Bakhda%20and%20Meiqi%20Zhao%20and%20Quinn%20A.%20Booth%20and%20Matthew%20E.%20Modi%20and%20Andrew%20Song%20and%20Simon%20Kang%20and%20Jiahao%20Wu%20and%20Priya%20Patel%20and%20Robert%20T.%20Kasumi%20and%20David%20Yi%20and%20Nihar%20Niraj%20Garg%20and%20Pranav%20Jhunjhunwala%20and%20Siddharth%20Bhutoria%20and%20Evan%20H.%20Tong%20and%20Yuhang%20Hu%20and%20Judah%20Goldfeder%20and%20Omer%20Mustel%20and%20Donghan%20Kim%20and%20Hod%20Lipson&entry.1292438233=%20%20Biological%20lifeforms%20can%20heal%2C%20grow%2C%20adapt%2C%20and%20reproduce%20--%20abilities%0Aessential%20for%20sustained%20survival%20and%20development.%20In%20contrast%2C%20robots%20today%20are%0Aprimarily%20monolithic%20machines%20with%20limited%20ability%20to%20self-repair%2C%20physically%0Adevelop%2C%20or%20incorporate%20material%20from%20their%20environments.%20While%20robot%20minds%0Arapidly%20evolve%20new%20behaviors%20through%20AI%2C%20their%20bodies%20remain%20closed%20systems%2C%0Aunable%20to%20systematically%20integrate%20material%20to%20grow%20or%20heal.%20We%20argue%20that%0Aopen-ended%20physical%20adaptation%20is%20only%20possible%20when%20robots%20are%20designed%20using%0Aa%20small%20repertoire%20of%20simple%20modules.%20This%20allows%20machines%20to%20mechanically%0Aadapt%20by%20consuming%20parts%20from%20other%20machines%20or%20their%20surroundings%20and%20shed%0Abroken%20components.%20We%20demonstrate%20this%20principle%20on%20a%20truss%20modular%20robot%0Aplatform.%20We%20show%20how%20robots%20can%20grow%20bigger%2C%20faster%2C%20and%20more%20capable%20by%0Aconsuming%20materials%20from%20their%20environment%20and%20other%20robots.%20We%20suggest%20that%0Amachine%20metabolic%20processes%20like%20those%20demonstrated%20here%20will%20be%20an%20essential%0Apart%20of%20any%20sustained%20future%20robot%20ecology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11192v2&entry.124074799=Read"},
{"title": "LLM-Based Config Synthesis requires Disambiguation", "author": "Rajdeep Mondal and Nikolaj Bjorner and Todd Millstein and Alan Tang and George Varghese", "abstract": "  Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.\n", "link": "http://arxiv.org/abs/2507.12443v1", "date": "2025-07-16", "relevancy": 1.3294, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4254}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Based%20Config%20Synthesis%20requires%20Disambiguation&body=Title%3A%20LLM-Based%20Config%20Synthesis%20requires%20Disambiguation%0AAuthor%3A%20Rajdeep%20Mondal%20and%20Nikolaj%20Bjorner%20and%20Todd%20Millstein%20and%20Alan%20Tang%20and%20George%20Varghese%0AAbstract%3A%20%20%20Beyond%20hallucinations%2C%20another%20problem%20in%20program%20synthesis%20using%20LLMs%20is%0Aambiguity%20in%20user%20intent.%20We%20illustrate%20the%20ambiguity%20problem%20in%20a%20networking%0Acontext%20for%20LLM-based%20incremental%20configuration%20synthesis%20of%20route-maps%20and%0AACLs.%20These%20structures%20frequently%20overlap%20in%20header%20space%2C%20making%20the%20relative%0Apriority%20of%20actions%20impossible%20for%20the%20LLM%20to%20infer%20without%20user%20interaction.%0AMeasurements%20in%20a%20large%20cloud%20identify%20complex%20ACLs%20with%20100%27s%20of%20overlaps%2C%0Ashowing%20ambiguity%20is%20a%20real%20problem.%20We%20propose%20a%20prototype%20system%2C%20Clarify%2C%0Awhich%20uses%20an%20LLM%20augmented%20with%20a%20new%20module%20called%20a%20Disambiguator%20that%20helps%0Aelicit%20user%20intent.%20On%20a%20small%20synthetic%20workload%2C%20Clarify%20incrementally%0Asynthesizes%20routing%20policies%20after%20disambiguation%20and%20then%20verifies%20them.%20Our%0Atreatment%20of%20ambiguities%20is%20useful%20more%20generally%20when%20the%20intent%20of%20updates%0Acan%20be%20correctly%20synthesized%20by%20LLMs%2C%20but%20their%20integration%20is%20ambiguous%20and%0Acan%20lead%20to%20different%20global%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Based%2520Config%2520Synthesis%2520requires%2520Disambiguation%26entry.906535625%3DRajdeep%2520Mondal%2520and%2520Nikolaj%2520Bjorner%2520and%2520Todd%2520Millstein%2520and%2520Alan%2520Tang%2520and%2520George%2520Varghese%26entry.1292438233%3D%2520%2520Beyond%2520hallucinations%252C%2520another%2520problem%2520in%2520program%2520synthesis%2520using%2520LLMs%2520is%250Aambiguity%2520in%2520user%2520intent.%2520We%2520illustrate%2520the%2520ambiguity%2520problem%2520in%2520a%2520networking%250Acontext%2520for%2520LLM-based%2520incremental%2520configuration%2520synthesis%2520of%2520route-maps%2520and%250AACLs.%2520These%2520structures%2520frequently%2520overlap%2520in%2520header%2520space%252C%2520making%2520the%2520relative%250Apriority%2520of%2520actions%2520impossible%2520for%2520the%2520LLM%2520to%2520infer%2520without%2520user%2520interaction.%250AMeasurements%2520in%2520a%2520large%2520cloud%2520identify%2520complex%2520ACLs%2520with%2520100%2527s%2520of%2520overlaps%252C%250Ashowing%2520ambiguity%2520is%2520a%2520real%2520problem.%2520We%2520propose%2520a%2520prototype%2520system%252C%2520Clarify%252C%250Awhich%2520uses%2520an%2520LLM%2520augmented%2520with%2520a%2520new%2520module%2520called%2520a%2520Disambiguator%2520that%2520helps%250Aelicit%2520user%2520intent.%2520On%2520a%2520small%2520synthetic%2520workload%252C%2520Clarify%2520incrementally%250Asynthesizes%2520routing%2520policies%2520after%2520disambiguation%2520and%2520then%2520verifies%2520them.%2520Our%250Atreatment%2520of%2520ambiguities%2520is%2520useful%2520more%2520generally%2520when%2520the%2520intent%2520of%2520updates%250Acan%2520be%2520correctly%2520synthesized%2520by%2520LLMs%252C%2520but%2520their%2520integration%2520is%2520ambiguous%2520and%250Acan%2520lead%2520to%2520different%2520global%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Based%20Config%20Synthesis%20requires%20Disambiguation&entry.906535625=Rajdeep%20Mondal%20and%20Nikolaj%20Bjorner%20and%20Todd%20Millstein%20and%20Alan%20Tang%20and%20George%20Varghese&entry.1292438233=%20%20Beyond%20hallucinations%2C%20another%20problem%20in%20program%20synthesis%20using%20LLMs%20is%0Aambiguity%20in%20user%20intent.%20We%20illustrate%20the%20ambiguity%20problem%20in%20a%20networking%0Acontext%20for%20LLM-based%20incremental%20configuration%20synthesis%20of%20route-maps%20and%0AACLs.%20These%20structures%20frequently%20overlap%20in%20header%20space%2C%20making%20the%20relative%0Apriority%20of%20actions%20impossible%20for%20the%20LLM%20to%20infer%20without%20user%20interaction.%0AMeasurements%20in%20a%20large%20cloud%20identify%20complex%20ACLs%20with%20100%27s%20of%20overlaps%2C%0Ashowing%20ambiguity%20is%20a%20real%20problem.%20We%20propose%20a%20prototype%20system%2C%20Clarify%2C%0Awhich%20uses%20an%20LLM%20augmented%20with%20a%20new%20module%20called%20a%20Disambiguator%20that%20helps%0Aelicit%20user%20intent.%20On%20a%20small%20synthetic%20workload%2C%20Clarify%20incrementally%0Asynthesizes%20routing%20policies%20after%20disambiguation%20and%20then%20verifies%20them.%20Our%0Atreatment%20of%20ambiguities%20is%20useful%20more%20generally%20when%20the%20intent%20of%20updates%0Acan%20be%20correctly%20synthesized%20by%20LLMs%2C%20but%20their%20integration%20is%20ambiguous%20and%0Acan%20lead%20to%20different%20global%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12443v1&entry.124074799=Read"},
{"title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs", "author": "Yangning Li and Weizhi Zhang and Yuyao Yang and Wei-Chieh Huang and Yaozu Wu and Junyu Luo and Yuanchen Bei and Henry Peng Zou and Xiao Luo and Yusheng Zhao and Chunkit Chan and Yankai Chen and Zhongfen Deng and Yinghui Li and Hai-Tao Zheng and Dongyuan Li and Renhe Jiang and Ming Zhang and Yangqiu Song and Philip S. Yu", "abstract": "  Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.\n", "link": "http://arxiv.org/abs/2507.09477v2", "date": "2025-07-16", "relevancy": 1.4703, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5253}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Agentic%20RAG%20with%20Deep%20Reasoning%3A%20A%20Survey%20of%20RAG-Reasoning%0A%20%20Systems%20in%20LLMs&body=Title%3A%20Towards%20Agentic%20RAG%20with%20Deep%20Reasoning%3A%20A%20Survey%20of%20RAG-Reasoning%0A%20%20Systems%20in%20LLMs%0AAuthor%3A%20Yangning%20Li%20and%20Weizhi%20Zhang%20and%20Yuyao%20Yang%20and%20Wei-Chieh%20Huang%20and%20Yaozu%20Wu%20and%20Junyu%20Luo%20and%20Yuanchen%20Bei%20and%20Henry%20Peng%20Zou%20and%20Xiao%20Luo%20and%20Yusheng%20Zhao%20and%20Chunkit%20Chan%20and%20Yankai%20Chen%20and%20Zhongfen%20Deng%20and%20Yinghui%20Li%20and%20Hai-Tao%20Zheng%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ming%20Zhang%20and%20Yangqiu%20Song%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20lifts%20the%20factuality%20of%20Large%20Language%0AModels%20%28LLMs%29%20by%20injecting%20external%20knowledge%2C%20yet%20it%20falls%20short%20on%20problems%0Athat%20demand%20multi-step%20inference%3B%20conversely%2C%20purely%20reasoning-oriented%0Aapproaches%20often%20hallucinate%20or%20mis-ground%20facts.%20This%20survey%20synthesizes%20both%0Astrands%20under%20a%20unified%20reasoning-retrieval%20perspective.%20We%20first%20map%20how%0Aadvanced%20reasoning%20optimizes%20each%20stage%20of%20RAG%20%28Reasoning-Enhanced%20RAG%29.%20Then%2C%0Awe%20show%20how%20retrieved%20knowledge%20of%20different%20type%20supply%20missing%20premises%20and%0Aexpand%20context%20for%20complex%20inference%20%28RAG-Enhanced%20Reasoning%29.%20Finally%2C%20we%0Aspotlight%20emerging%20Synergized%20RAG-Reasoning%20frameworks%2C%20where%20%28agentic%29%20LLMs%0Aiteratively%20interleave%20search%20and%20reasoning%20to%20achieve%20state-of-the-art%0Aperformance%20across%20knowledge-intensive%20benchmarks.%20We%20categorize%20methods%2C%0Adatasets%2C%20and%20open%20challenges%2C%20and%20outline%20research%20avenues%20toward%20deeper%0ARAG-Reasoning%20systems%20that%20are%20more%20effective%2C%20multimodally-adaptive%2C%0Atrustworthy%2C%20and%20human-centric.%20The%20collection%20is%20available%20at%0Ahttps%3A//github.com/DavidZWZ/Awesome-RAG-Reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Agentic%2520RAG%2520with%2520Deep%2520Reasoning%253A%2520A%2520Survey%2520of%2520RAG-Reasoning%250A%2520%2520Systems%2520in%2520LLMs%26entry.906535625%3DYangning%2520Li%2520and%2520Weizhi%2520Zhang%2520and%2520Yuyao%2520Yang%2520and%2520Wei-Chieh%2520Huang%2520and%2520Yaozu%2520Wu%2520and%2520Junyu%2520Luo%2520and%2520Yuanchen%2520Bei%2520and%2520Henry%2520Peng%2520Zou%2520and%2520Xiao%2520Luo%2520and%2520Yusheng%2520Zhao%2520and%2520Chunkit%2520Chan%2520and%2520Yankai%2520Chen%2520and%2520Zhongfen%2520Deng%2520and%2520Yinghui%2520Li%2520and%2520Hai-Tao%2520Zheng%2520and%2520Dongyuan%2520Li%2520and%2520Renhe%2520Jiang%2520and%2520Ming%2520Zhang%2520and%2520Yangqiu%2520Song%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520lifts%2520the%2520factuality%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520by%2520injecting%2520external%2520knowledge%252C%2520yet%2520it%2520falls%2520short%2520on%2520problems%250Athat%2520demand%2520multi-step%2520inference%253B%2520conversely%252C%2520purely%2520reasoning-oriented%250Aapproaches%2520often%2520hallucinate%2520or%2520mis-ground%2520facts.%2520This%2520survey%2520synthesizes%2520both%250Astrands%2520under%2520a%2520unified%2520reasoning-retrieval%2520perspective.%2520We%2520first%2520map%2520how%250Aadvanced%2520reasoning%2520optimizes%2520each%2520stage%2520of%2520RAG%2520%2528Reasoning-Enhanced%2520RAG%2529.%2520Then%252C%250Awe%2520show%2520how%2520retrieved%2520knowledge%2520of%2520different%2520type%2520supply%2520missing%2520premises%2520and%250Aexpand%2520context%2520for%2520complex%2520inference%2520%2528RAG-Enhanced%2520Reasoning%2529.%2520Finally%252C%2520we%250Aspotlight%2520emerging%2520Synergized%2520RAG-Reasoning%2520frameworks%252C%2520where%2520%2528agentic%2529%2520LLMs%250Aiteratively%2520interleave%2520search%2520and%2520reasoning%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520across%2520knowledge-intensive%2520benchmarks.%2520We%2520categorize%2520methods%252C%250Adatasets%252C%2520and%2520open%2520challenges%252C%2520and%2520outline%2520research%2520avenues%2520toward%2520deeper%250ARAG-Reasoning%2520systems%2520that%2520are%2520more%2520effective%252C%2520multimodally-adaptive%252C%250Atrustworthy%252C%2520and%2520human-centric.%2520The%2520collection%2520is%2520available%2520at%250Ahttps%253A//github.com/DavidZWZ/Awesome-RAG-Reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Agentic%20RAG%20with%20Deep%20Reasoning%3A%20A%20Survey%20of%20RAG-Reasoning%0A%20%20Systems%20in%20LLMs&entry.906535625=Yangning%20Li%20and%20Weizhi%20Zhang%20and%20Yuyao%20Yang%20and%20Wei-Chieh%20Huang%20and%20Yaozu%20Wu%20and%20Junyu%20Luo%20and%20Yuanchen%20Bei%20and%20Henry%20Peng%20Zou%20and%20Xiao%20Luo%20and%20Yusheng%20Zhao%20and%20Chunkit%20Chan%20and%20Yankai%20Chen%20and%20Zhongfen%20Deng%20and%20Yinghui%20Li%20and%20Hai-Tao%20Zheng%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ming%20Zhang%20and%20Yangqiu%20Song%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20lifts%20the%20factuality%20of%20Large%20Language%0AModels%20%28LLMs%29%20by%20injecting%20external%20knowledge%2C%20yet%20it%20falls%20short%20on%20problems%0Athat%20demand%20multi-step%20inference%3B%20conversely%2C%20purely%20reasoning-oriented%0Aapproaches%20often%20hallucinate%20or%20mis-ground%20facts.%20This%20survey%20synthesizes%20both%0Astrands%20under%20a%20unified%20reasoning-retrieval%20perspective.%20We%20first%20map%20how%0Aadvanced%20reasoning%20optimizes%20each%20stage%20of%20RAG%20%28Reasoning-Enhanced%20RAG%29.%20Then%2C%0Awe%20show%20how%20retrieved%20knowledge%20of%20different%20type%20supply%20missing%20premises%20and%0Aexpand%20context%20for%20complex%20inference%20%28RAG-Enhanced%20Reasoning%29.%20Finally%2C%20we%0Aspotlight%20emerging%20Synergized%20RAG-Reasoning%20frameworks%2C%20where%20%28agentic%29%20LLMs%0Aiteratively%20interleave%20search%20and%20reasoning%20to%20achieve%20state-of-the-art%0Aperformance%20across%20knowledge-intensive%20benchmarks.%20We%20categorize%20methods%2C%0Adatasets%2C%20and%20open%20challenges%2C%20and%20outline%20research%20avenues%20toward%20deeper%0ARAG-Reasoning%20systems%20that%20are%20more%20effective%2C%20multimodally-adaptive%2C%0Atrustworthy%2C%20and%20human-centric.%20The%20collection%20is%20available%20at%0Ahttps%3A//github.com/DavidZWZ/Awesome-RAG-Reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09477v2&entry.124074799=Read"},
{"title": "Nonlinear Concept Erasure: a Density Matching Approach", "author": "Antoine Saillenfest and Pirmin Lemberger", "abstract": "  Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.\n", "link": "http://arxiv.org/abs/2507.12341v1", "date": "2025-07-16", "relevancy": 1.3969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4699}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4666}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20Concept%20Erasure%3A%20a%20Density%20Matching%20Approach&body=Title%3A%20Nonlinear%20Concept%20Erasure%3A%20a%20Density%20Matching%20Approach%0AAuthor%3A%20Antoine%20Saillenfest%20and%20Pirmin%20Lemberger%0AAbstract%3A%20%20%20Ensuring%20that%20neural%20models%20used%20in%20real-world%20applications%20cannot%20infer%0Asensitive%20information%2C%20such%20as%20demographic%20attributes%20like%20gender%20or%20race%2C%20from%0Atext%20representations%20is%20a%20critical%20challenge%20when%20fairness%20is%20a%20concern.%20We%0Aaddress%20this%20issue%20through%20concept%20erasure%2C%20a%20process%20that%20removes%20information%0Arelated%20to%20a%20specific%20concept%20from%20distributed%20representations%20while%20preserving%0Aas%20much%20of%20the%20remaining%20semantic%20information%20as%20possible.%20Our%20approach%0Ainvolves%20learning%20an%20orthogonal%20projection%20in%20the%20embedding%20space%2C%20designed%20to%0Amake%20the%20class-conditional%20feature%20distributions%20of%20the%20discrete%20concept%20to%0Aerase%20indistinguishable%20after%20projection.%20By%20adjusting%20the%20rank%20of%20the%0Aprojector%2C%20we%20control%20the%20extent%20of%20information%20removal%2C%20while%20its%0Aorthogonality%20ensures%20strict%20preservation%20of%20the%20local%20structure%20of%20the%0Aembeddings.%20Our%20method%2C%20termed%20%24%5Coverline%7B%5Cmathrm%7BL%7D%7D%24EOPARD%2C%20achieves%0Astate-of-the-art%20performance%20in%20nonlinear%20erasure%20of%20a%20discrete%20attribute%20on%0Aclassic%20natural%20language%20processing%20benchmarks.%20Furthermore%2C%20we%20demonstrate%0Athat%20%24%5Coverline%7B%5Cmathrm%7BL%7D%7D%24EOPARD%20effectively%20mitigates%20bias%20in%20deep%20nonlinear%0Aclassifiers%2C%20thereby%20promoting%20fairness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520Concept%2520Erasure%253A%2520a%2520Density%2520Matching%2520Approach%26entry.906535625%3DAntoine%2520Saillenfest%2520and%2520Pirmin%2520Lemberger%26entry.1292438233%3D%2520%2520Ensuring%2520that%2520neural%2520models%2520used%2520in%2520real-world%2520applications%2520cannot%2520infer%250Asensitive%2520information%252C%2520such%2520as%2520demographic%2520attributes%2520like%2520gender%2520or%2520race%252C%2520from%250Atext%2520representations%2520is%2520a%2520critical%2520challenge%2520when%2520fairness%2520is%2520a%2520concern.%2520We%250Aaddress%2520this%2520issue%2520through%2520concept%2520erasure%252C%2520a%2520process%2520that%2520removes%2520information%250Arelated%2520to%2520a%2520specific%2520concept%2520from%2520distributed%2520representations%2520while%2520preserving%250Aas%2520much%2520of%2520the%2520remaining%2520semantic%2520information%2520as%2520possible.%2520Our%2520approach%250Ainvolves%2520learning%2520an%2520orthogonal%2520projection%2520in%2520the%2520embedding%2520space%252C%2520designed%2520to%250Amake%2520the%2520class-conditional%2520feature%2520distributions%2520of%2520the%2520discrete%2520concept%2520to%250Aerase%2520indistinguishable%2520after%2520projection.%2520By%2520adjusting%2520the%2520rank%2520of%2520the%250Aprojector%252C%2520we%2520control%2520the%2520extent%2520of%2520information%2520removal%252C%2520while%2520its%250Aorthogonality%2520ensures%2520strict%2520preservation%2520of%2520the%2520local%2520structure%2520of%2520the%250Aembeddings.%2520Our%2520method%252C%2520termed%2520%2524%255Coverline%257B%255Cmathrm%257BL%257D%257D%2524EOPARD%252C%2520achieves%250Astate-of-the-art%2520performance%2520in%2520nonlinear%2520erasure%2520of%2520a%2520discrete%2520attribute%2520on%250Aclassic%2520natural%2520language%2520processing%2520benchmarks.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520%2524%255Coverline%257B%255Cmathrm%257BL%257D%257D%2524EOPARD%2520effectively%2520mitigates%2520bias%2520in%2520deep%2520nonlinear%250Aclassifiers%252C%2520thereby%2520promoting%2520fairness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20Concept%20Erasure%3A%20a%20Density%20Matching%20Approach&entry.906535625=Antoine%20Saillenfest%20and%20Pirmin%20Lemberger&entry.1292438233=%20%20Ensuring%20that%20neural%20models%20used%20in%20real-world%20applications%20cannot%20infer%0Asensitive%20information%2C%20such%20as%20demographic%20attributes%20like%20gender%20or%20race%2C%20from%0Atext%20representations%20is%20a%20critical%20challenge%20when%20fairness%20is%20a%20concern.%20We%0Aaddress%20this%20issue%20through%20concept%20erasure%2C%20a%20process%20that%20removes%20information%0Arelated%20to%20a%20specific%20concept%20from%20distributed%20representations%20while%20preserving%0Aas%20much%20of%20the%20remaining%20semantic%20information%20as%20possible.%20Our%20approach%0Ainvolves%20learning%20an%20orthogonal%20projection%20in%20the%20embedding%20space%2C%20designed%20to%0Amake%20the%20class-conditional%20feature%20distributions%20of%20the%20discrete%20concept%20to%0Aerase%20indistinguishable%20after%20projection.%20By%20adjusting%20the%20rank%20of%20the%0Aprojector%2C%20we%20control%20the%20extent%20of%20information%20removal%2C%20while%20its%0Aorthogonality%20ensures%20strict%20preservation%20of%20the%20local%20structure%20of%20the%0Aembeddings.%20Our%20method%2C%20termed%20%24%5Coverline%7B%5Cmathrm%7BL%7D%7D%24EOPARD%2C%20achieves%0Astate-of-the-art%20performance%20in%20nonlinear%20erasure%20of%20a%20discrete%20attribute%20on%0Aclassic%20natural%20language%20processing%20benchmarks.%20Furthermore%2C%20we%20demonstrate%0Athat%20%24%5Coverline%7B%5Cmathrm%7BL%7D%7D%24EOPARD%20effectively%20mitigates%20bias%20in%20deep%20nonlinear%0Aclassifiers%2C%20thereby%20promoting%20fairness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12341v1&entry.124074799=Read"},
{"title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents", "author": "Tatiana Petrova and Boris Bliznioukov and Aleksandr Puzikov and Radu State", "abstract": "  The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.\n", "link": "http://arxiv.org/abs/2507.10644v2", "date": "2025-07-16", "relevancy": 1.4664, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4922}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Semantic%20Web%20and%20MAS%20to%20Agentic%20AI%3A%20A%20Unified%20Narrative%20of%20the%20Web%0A%20%20of%20Agents&body=Title%3A%20From%20Semantic%20Web%20and%20MAS%20to%20Agentic%20AI%3A%20A%20Unified%20Narrative%20of%20the%20Web%0A%20%20of%20Agents%0AAuthor%3A%20Tatiana%20Petrova%20and%20Boris%20Bliznioukov%20and%20Aleksandr%20Puzikov%20and%20Radu%20State%0AAbstract%3A%20%20%20The%20concept%20of%20the%20Web%20of%20Agents%20%28WoA%29%2C%20which%20transforms%20the%20static%2C%0Adocument-centric%20Web%20into%20an%20environment%20of%20autonomous%20agents%20acting%20on%20users%27%0Abehalf%2C%20has%20attracted%20growing%20interest%20as%20large%20language%20models%20%28LLMs%29%20become%0Amore%20capable.%20However%2C%20research%20in%20this%20area%20is%20still%20fragmented%20across%0Adifferent%20communities.%20Contemporary%20surveys%20catalog%20the%20latest%20LLM-powered%0Aframeworks%2C%20while%20the%20rich%20histories%20of%20Multi-Agent%20Systems%20%28MAS%29%20and%20the%0ASemantic%20Web%20are%20often%20treated%20as%20separate%2C%20legacy%20domains.%20This%20fragmentation%0Aobscures%20the%20intellectual%20lineage%20of%20modern%20systems%20and%20hinders%20a%20holistic%0Aunderstanding%20of%20the%20field%27s%20trajectory.%20We%20present%20the%20first%20comprehensive%0Aevolutionary%20overview%20of%20the%20WoA.%20We%20show%20that%20modern%20protocols%20like%20A2A%20and%0Athe%20MCP%2C%20are%20direct%20evolutionary%20responses%20to%20the%20well-documented%20limitations%0Aof%20earlier%20standards%20like%20FIPA%20standards%20and%20OWL-based%20semantic%20agents.%20To%0Asystematize%20this%20analysis%2C%20we%20introduce%20a%20four-axis%20taxonomy%20%28semantic%0Afoundation%2C%20communication%20paradigm%2C%20locus%20of%20intelligence%2C%20discovery%0Amechanism%29.%20This%20framework%20provides%20a%20unified%20analytical%20lens%20for%20comparing%0Aagent%20architectures%20across%20all%20generations%2C%20revealing%20a%20clear%20line%20of%20descent%0Awhere%20others%20have%20seen%20a%20disconnect.%20Our%20analysis%20identifies%20a%20paradigm%20shift%0Ain%20the%20%27locus%20of%20intelligence%27%3A%20from%20being%20encoded%20in%20external%20data%20%28Semantic%0AWeb%29%20or%20the%20platform%20%28MAS%29%20to%20being%20embedded%20within%20the%20agent%27s%20core%20model%0A%28LLM%29.%20This%20shift%20is%20foundational%20to%20modern%20Agentic%20AI%2C%20enabling%20the%20scalable%0Aand%20adaptive%20systems%20the%20WoA%20has%20long%20envisioned.%20We%20conclude%20that%20while%20new%0Aprotocols%20are%20essential%2C%20they%20are%20insufficient%20for%20building%20a%20robust%2C%20open%2C%0Atrustworthy%20ecosystem.%20Finally%2C%20we%20argue%20that%20the%20next%20research%20frontier%20lies%0Ain%20solving%20persistent%20socio-technical%20challenges%2C%20and%20we%20map%20out%20a%20new%20agenda%0Afocused%20on%20decentralized%20identity%2C%20economic%20models%2C%20security%2C%20and%20governance%0Afor%20the%20emerging%20WoA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Semantic%2520Web%2520and%2520MAS%2520to%2520Agentic%2520AI%253A%2520A%2520Unified%2520Narrative%2520of%2520the%2520Web%250A%2520%2520of%2520Agents%26entry.906535625%3DTatiana%2520Petrova%2520and%2520Boris%2520Bliznioukov%2520and%2520Aleksandr%2520Puzikov%2520and%2520Radu%2520State%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520the%2520Web%2520of%2520Agents%2520%2528WoA%2529%252C%2520which%2520transforms%2520the%2520static%252C%250Adocument-centric%2520Web%2520into%2520an%2520environment%2520of%2520autonomous%2520agents%2520acting%2520on%2520users%2527%250Abehalf%252C%2520has%2520attracted%2520growing%2520interest%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%250Amore%2520capable.%2520However%252C%2520research%2520in%2520this%2520area%2520is%2520still%2520fragmented%2520across%250Adifferent%2520communities.%2520Contemporary%2520surveys%2520catalog%2520the%2520latest%2520LLM-powered%250Aframeworks%252C%2520while%2520the%2520rich%2520histories%2520of%2520Multi-Agent%2520Systems%2520%2528MAS%2529%2520and%2520the%250ASemantic%2520Web%2520are%2520often%2520treated%2520as%2520separate%252C%2520legacy%2520domains.%2520This%2520fragmentation%250Aobscures%2520the%2520intellectual%2520lineage%2520of%2520modern%2520systems%2520and%2520hinders%2520a%2520holistic%250Aunderstanding%2520of%2520the%2520field%2527s%2520trajectory.%2520We%2520present%2520the%2520first%2520comprehensive%250Aevolutionary%2520overview%2520of%2520the%2520WoA.%2520We%2520show%2520that%2520modern%2520protocols%2520like%2520A2A%2520and%250Athe%2520MCP%252C%2520are%2520direct%2520evolutionary%2520responses%2520to%2520the%2520well-documented%2520limitations%250Aof%2520earlier%2520standards%2520like%2520FIPA%2520standards%2520and%2520OWL-based%2520semantic%2520agents.%2520To%250Asystematize%2520this%2520analysis%252C%2520we%2520introduce%2520a%2520four-axis%2520taxonomy%2520%2528semantic%250Afoundation%252C%2520communication%2520paradigm%252C%2520locus%2520of%2520intelligence%252C%2520discovery%250Amechanism%2529.%2520This%2520framework%2520provides%2520a%2520unified%2520analytical%2520lens%2520for%2520comparing%250Aagent%2520architectures%2520across%2520all%2520generations%252C%2520revealing%2520a%2520clear%2520line%2520of%2520descent%250Awhere%2520others%2520have%2520seen%2520a%2520disconnect.%2520Our%2520analysis%2520identifies%2520a%2520paradigm%2520shift%250Ain%2520the%2520%2527locus%2520of%2520intelligence%2527%253A%2520from%2520being%2520encoded%2520in%2520external%2520data%2520%2528Semantic%250AWeb%2529%2520or%2520the%2520platform%2520%2528MAS%2529%2520to%2520being%2520embedded%2520within%2520the%2520agent%2527s%2520core%2520model%250A%2528LLM%2529.%2520This%2520shift%2520is%2520foundational%2520to%2520modern%2520Agentic%2520AI%252C%2520enabling%2520the%2520scalable%250Aand%2520adaptive%2520systems%2520the%2520WoA%2520has%2520long%2520envisioned.%2520We%2520conclude%2520that%2520while%2520new%250Aprotocols%2520are%2520essential%252C%2520they%2520are%2520insufficient%2520for%2520building%2520a%2520robust%252C%2520open%252C%250Atrustworthy%2520ecosystem.%2520Finally%252C%2520we%2520argue%2520that%2520the%2520next%2520research%2520frontier%2520lies%250Ain%2520solving%2520persistent%2520socio-technical%2520challenges%252C%2520and%2520we%2520map%2520out%2520a%2520new%2520agenda%250Afocused%2520on%2520decentralized%2520identity%252C%2520economic%2520models%252C%2520security%252C%2520and%2520governance%250Afor%2520the%2520emerging%2520WoA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Semantic%20Web%20and%20MAS%20to%20Agentic%20AI%3A%20A%20Unified%20Narrative%20of%20the%20Web%0A%20%20of%20Agents&entry.906535625=Tatiana%20Petrova%20and%20Boris%20Bliznioukov%20and%20Aleksandr%20Puzikov%20and%20Radu%20State&entry.1292438233=%20%20The%20concept%20of%20the%20Web%20of%20Agents%20%28WoA%29%2C%20which%20transforms%20the%20static%2C%0Adocument-centric%20Web%20into%20an%20environment%20of%20autonomous%20agents%20acting%20on%20users%27%0Abehalf%2C%20has%20attracted%20growing%20interest%20as%20large%20language%20models%20%28LLMs%29%20become%0Amore%20capable.%20However%2C%20research%20in%20this%20area%20is%20still%20fragmented%20across%0Adifferent%20communities.%20Contemporary%20surveys%20catalog%20the%20latest%20LLM-powered%0Aframeworks%2C%20while%20the%20rich%20histories%20of%20Multi-Agent%20Systems%20%28MAS%29%20and%20the%0ASemantic%20Web%20are%20often%20treated%20as%20separate%2C%20legacy%20domains.%20This%20fragmentation%0Aobscures%20the%20intellectual%20lineage%20of%20modern%20systems%20and%20hinders%20a%20holistic%0Aunderstanding%20of%20the%20field%27s%20trajectory.%20We%20present%20the%20first%20comprehensive%0Aevolutionary%20overview%20of%20the%20WoA.%20We%20show%20that%20modern%20protocols%20like%20A2A%20and%0Athe%20MCP%2C%20are%20direct%20evolutionary%20responses%20to%20the%20well-documented%20limitations%0Aof%20earlier%20standards%20like%20FIPA%20standards%20and%20OWL-based%20semantic%20agents.%20To%0Asystematize%20this%20analysis%2C%20we%20introduce%20a%20four-axis%20taxonomy%20%28semantic%0Afoundation%2C%20communication%20paradigm%2C%20locus%20of%20intelligence%2C%20discovery%0Amechanism%29.%20This%20framework%20provides%20a%20unified%20analytical%20lens%20for%20comparing%0Aagent%20architectures%20across%20all%20generations%2C%20revealing%20a%20clear%20line%20of%20descent%0Awhere%20others%20have%20seen%20a%20disconnect.%20Our%20analysis%20identifies%20a%20paradigm%20shift%0Ain%20the%20%27locus%20of%20intelligence%27%3A%20from%20being%20encoded%20in%20external%20data%20%28Semantic%0AWeb%29%20or%20the%20platform%20%28MAS%29%20to%20being%20embedded%20within%20the%20agent%27s%20core%20model%0A%28LLM%29.%20This%20shift%20is%20foundational%20to%20modern%20Agentic%20AI%2C%20enabling%20the%20scalable%0Aand%20adaptive%20systems%20the%20WoA%20has%20long%20envisioned.%20We%20conclude%20that%20while%20new%0Aprotocols%20are%20essential%2C%20they%20are%20insufficient%20for%20building%20a%20robust%2C%20open%2C%0Atrustworthy%20ecosystem.%20Finally%2C%20we%20argue%20that%20the%20next%20research%20frontier%20lies%0Ain%20solving%20persistent%20socio-technical%20challenges%2C%20and%20we%20map%20out%20a%20new%20agenda%0Afocused%20on%20decentralized%20identity%2C%20economic%20models%2C%20security%2C%20and%20governance%0Afor%20the%20emerging%20WoA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10644v2&entry.124074799=Read"},
{"title": "BondMatcher: H-Bond Stability Analysis in Molecular Systems", "author": "Thomas Daniel and Malgorzata Olejniczak and Julien Tierny", "abstract": "  This application paper investigates the stability of hydrogen bonds\n(H-bonds), as characterized by the Quantum Theory of Atoms in Molecules\n(QTAIM). First, we contribute a database of 4544 electron densities associated\nto four isomers of water hexamers (the so-called Ring, Book, Cage and Prism),\ngenerated by distorting their equilibrium geometry under various structural\nperturbations, modeling the natural dynamic behavior of molecular systems.\nSecond, we present a new stability measure, called bond occurrence rate,\nassociating each bond path present at equilibrium with its rate of occurrence\nwithin the input ensemble. We also provide an algorithm, called BondMatcher,\nfor its automatic computation, based on a tailored, geometry-aware partial\nisomorphism estimation between the extremum graphs of the considered electron\ndensities. Our new stability measure allows for the automatic identification of\ndensities lacking H-bond paths, enabling further visual inspections.\nSpecifically, the topological analysis enabled by our framework corroborates\nexperimental observations and provides refined geometrical criteria for\ncharacterizing the disappearance of H-bond paths. Our electron density database\nand our C++ implementation are available at this address:\nhttps://github.com/thom-dani/BondMatcher.\n", "link": "http://arxiv.org/abs/2504.03205v2", "date": "2025-07-16", "relevancy": 1.4369, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3782}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3571}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BondMatcher%3A%20H-Bond%20Stability%20Analysis%20in%20Molecular%20Systems&body=Title%3A%20BondMatcher%3A%20H-Bond%20Stability%20Analysis%20in%20Molecular%20Systems%0AAuthor%3A%20Thomas%20Daniel%20and%20Malgorzata%20Olejniczak%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20application%20paper%20investigates%20the%20stability%20of%20hydrogen%20bonds%0A%28H-bonds%29%2C%20as%20characterized%20by%20the%20Quantum%20Theory%20of%20Atoms%20in%20Molecules%0A%28QTAIM%29.%20First%2C%20we%20contribute%20a%20database%20of%204544%20electron%20densities%20associated%0Ato%20four%20isomers%20of%20water%20hexamers%20%28the%20so-called%20Ring%2C%20Book%2C%20Cage%20and%20Prism%29%2C%0Agenerated%20by%20distorting%20their%20equilibrium%20geometry%20under%20various%20structural%0Aperturbations%2C%20modeling%20the%20natural%20dynamic%20behavior%20of%20molecular%20systems.%0ASecond%2C%20we%20present%20a%20new%20stability%20measure%2C%20called%20bond%20occurrence%20rate%2C%0Aassociating%20each%20bond%20path%20present%20at%20equilibrium%20with%20its%20rate%20of%20occurrence%0Awithin%20the%20input%20ensemble.%20We%20also%20provide%20an%20algorithm%2C%20called%20BondMatcher%2C%0Afor%20its%20automatic%20computation%2C%20based%20on%20a%20tailored%2C%20geometry-aware%20partial%0Aisomorphism%20estimation%20between%20the%20extremum%20graphs%20of%20the%20considered%20electron%0Adensities.%20Our%20new%20stability%20measure%20allows%20for%20the%20automatic%20identification%20of%0Adensities%20lacking%20H-bond%20paths%2C%20enabling%20further%20visual%20inspections.%0ASpecifically%2C%20the%20topological%20analysis%20enabled%20by%20our%20framework%20corroborates%0Aexperimental%20observations%20and%20provides%20refined%20geometrical%20criteria%20for%0Acharacterizing%20the%20disappearance%20of%20H-bond%20paths.%20Our%20electron%20density%20database%0Aand%20our%20C%2B%2B%20implementation%20are%20available%20at%20this%20address%3A%0Ahttps%3A//github.com/thom-dani/BondMatcher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBondMatcher%253A%2520H-Bond%2520Stability%2520Analysis%2520in%2520Molecular%2520Systems%26entry.906535625%3DThomas%2520Daniel%2520and%2520Malgorzata%2520Olejniczak%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520application%2520paper%2520investigates%2520the%2520stability%2520of%2520hydrogen%2520bonds%250A%2528H-bonds%2529%252C%2520as%2520characterized%2520by%2520the%2520Quantum%2520Theory%2520of%2520Atoms%2520in%2520Molecules%250A%2528QTAIM%2529.%2520First%252C%2520we%2520contribute%2520a%2520database%2520of%25204544%2520electron%2520densities%2520associated%250Ato%2520four%2520isomers%2520of%2520water%2520hexamers%2520%2528the%2520so-called%2520Ring%252C%2520Book%252C%2520Cage%2520and%2520Prism%2529%252C%250Agenerated%2520by%2520distorting%2520their%2520equilibrium%2520geometry%2520under%2520various%2520structural%250Aperturbations%252C%2520modeling%2520the%2520natural%2520dynamic%2520behavior%2520of%2520molecular%2520systems.%250ASecond%252C%2520we%2520present%2520a%2520new%2520stability%2520measure%252C%2520called%2520bond%2520occurrence%2520rate%252C%250Aassociating%2520each%2520bond%2520path%2520present%2520at%2520equilibrium%2520with%2520its%2520rate%2520of%2520occurrence%250Awithin%2520the%2520input%2520ensemble.%2520We%2520also%2520provide%2520an%2520algorithm%252C%2520called%2520BondMatcher%252C%250Afor%2520its%2520automatic%2520computation%252C%2520based%2520on%2520a%2520tailored%252C%2520geometry-aware%2520partial%250Aisomorphism%2520estimation%2520between%2520the%2520extremum%2520graphs%2520of%2520the%2520considered%2520electron%250Adensities.%2520Our%2520new%2520stability%2520measure%2520allows%2520for%2520the%2520automatic%2520identification%2520of%250Adensities%2520lacking%2520H-bond%2520paths%252C%2520enabling%2520further%2520visual%2520inspections.%250ASpecifically%252C%2520the%2520topological%2520analysis%2520enabled%2520by%2520our%2520framework%2520corroborates%250Aexperimental%2520observations%2520and%2520provides%2520refined%2520geometrical%2520criteria%2520for%250Acharacterizing%2520the%2520disappearance%2520of%2520H-bond%2520paths.%2520Our%2520electron%2520density%2520database%250Aand%2520our%2520C%252B%252B%2520implementation%2520are%2520available%2520at%2520this%2520address%253A%250Ahttps%253A//github.com/thom-dani/BondMatcher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BondMatcher%3A%20H-Bond%20Stability%20Analysis%20in%20Molecular%20Systems&entry.906535625=Thomas%20Daniel%20and%20Malgorzata%20Olejniczak%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20application%20paper%20investigates%20the%20stability%20of%20hydrogen%20bonds%0A%28H-bonds%29%2C%20as%20characterized%20by%20the%20Quantum%20Theory%20of%20Atoms%20in%20Molecules%0A%28QTAIM%29.%20First%2C%20we%20contribute%20a%20database%20of%204544%20electron%20densities%20associated%0Ato%20four%20isomers%20of%20water%20hexamers%20%28the%20so-called%20Ring%2C%20Book%2C%20Cage%20and%20Prism%29%2C%0Agenerated%20by%20distorting%20their%20equilibrium%20geometry%20under%20various%20structural%0Aperturbations%2C%20modeling%20the%20natural%20dynamic%20behavior%20of%20molecular%20systems.%0ASecond%2C%20we%20present%20a%20new%20stability%20measure%2C%20called%20bond%20occurrence%20rate%2C%0Aassociating%20each%20bond%20path%20present%20at%20equilibrium%20with%20its%20rate%20of%20occurrence%0Awithin%20the%20input%20ensemble.%20We%20also%20provide%20an%20algorithm%2C%20called%20BondMatcher%2C%0Afor%20its%20automatic%20computation%2C%20based%20on%20a%20tailored%2C%20geometry-aware%20partial%0Aisomorphism%20estimation%20between%20the%20extremum%20graphs%20of%20the%20considered%20electron%0Adensities.%20Our%20new%20stability%20measure%20allows%20for%20the%20automatic%20identification%20of%0Adensities%20lacking%20H-bond%20paths%2C%20enabling%20further%20visual%20inspections.%0ASpecifically%2C%20the%20topological%20analysis%20enabled%20by%20our%20framework%20corroborates%0Aexperimental%20observations%20and%20provides%20refined%20geometrical%20criteria%20for%0Acharacterizing%20the%20disappearance%20of%20H-bond%20paths.%20Our%20electron%20density%20database%0Aand%20our%20C%2B%2B%20implementation%20are%20available%20at%20this%20address%3A%0Ahttps%3A//github.com/thom-dani/BondMatcher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03205v2&entry.124074799=Read"},
{"title": "Programming Distributed Collective Processes in the eXchange Calculus", "author": "Giorgio Audrito and Roberto Casadei and Ferruccio Damiani and Gianluca Torta and Mirko Viroli", "abstract": "  Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.\n", "link": "http://arxiv.org/abs/2401.11212v5", "date": "2025-07-16", "relevancy": 0.8458, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4432}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programming%20Distributed%20Collective%20Processes%20in%20the%20eXchange%20Calculus&body=Title%3A%20Programming%20Distributed%20Collective%20Processes%20in%20the%20eXchange%20Calculus%0AAuthor%3A%20Giorgio%20Audrito%20and%20Roberto%20Casadei%20and%20Ferruccio%20Damiani%20and%20Gianluca%20Torta%20and%20Mirko%20Viroli%0AAbstract%3A%20%20%20Recent%20trends%20like%20the%20Internet%20of%20Things%20%28IoT%29%20suggest%20a%20vision%20of%20dense%20and%0Amulti-scale%20deployments%20of%20computing%20devices%20in%20nearly%20all%20kinds%20of%0Aenvironments.%20A%20prominent%20engineering%20challenge%20revolves%20around%20programming%20the%0Acollective%20adaptive%20behaviour%20of%20such%20computational%20ecosystems.%20This%20requires%0Aabstractions%20able%20to%20capture%20concepts%20like%20ensembles%20%28dynamic%20groups%20of%0Acooperating%20devices%29%20and%20collective%20tasks%20%28joint%20activities%20carried%20out%20by%0Aensembles%29.%20In%20this%20work%2C%20we%20consider%20collections%20of%20devices%20interacting%20with%0Aneighbours%20and%20that%20execute%20in%20nearly-synchronised%20sense-compute-interact%0Arounds%2C%20where%20the%20computation%20is%20given%20by%20a%20single%20program%20mapping%20sensing%0Avalues%20and%20incoming%20messages%20to%20output%20and%20outcoming%20messages.%20To%20support%0Aprogramming%20whole%20computational%20collectives%2C%20we%20propose%20the%20abstraction%20of%20a%0Adistributed%20collective%20process%2C%20which%20can%20be%20used%20to%20define%20at%20once%20the%0Aensemble%20formation%20logic%20and%20its%20collective%20task.%20We%20formalise%20the%20abstraction%0Ain%20the%20eXchange%20Calculus%20%28XC%29%2C%20a%20core%20functional%20language%20based%20on%20neighbouring%0Avalues%20%28maps%20from%20neighbours%20to%20values%29%20where%20state%20and%20interaction%20is%20handled%0Athrough%20a%20single%20primitive%2C%20exchange%2C%20and%20provide%20a%20corresponding%0Aimplementation%20in%20the%20FCPP%20language.%20Then%2C%20we%20exercise%20distributed%20collective%0Aprocesses%20using%20two%20case%20studies%3A%20multi-hop%20message%20propagation%20and%20distributed%0Amonitoring%20of%20spatial%20properties.%20Finally%2C%20we%20discuss%20the%20features%20of%20the%0Aabstraction%20and%20its%20suitability%20for%20different%20kinds%20of%20distributed%20computing%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11212v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgramming%2520Distributed%2520Collective%2520Processes%2520in%2520the%2520eXchange%2520Calculus%26entry.906535625%3DGiorgio%2520Audrito%2520and%2520Roberto%2520Casadei%2520and%2520Ferruccio%2520Damiani%2520and%2520Gianluca%2520Torta%2520and%2520Mirko%2520Viroli%26entry.1292438233%3D%2520%2520Recent%2520trends%2520like%2520the%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520suggest%2520a%2520vision%2520of%2520dense%2520and%250Amulti-scale%2520deployments%2520of%2520computing%2520devices%2520in%2520nearly%2520all%2520kinds%2520of%250Aenvironments.%2520A%2520prominent%2520engineering%2520challenge%2520revolves%2520around%2520programming%2520the%250Acollective%2520adaptive%2520behaviour%2520of%2520such%2520computational%2520ecosystems.%2520This%2520requires%250Aabstractions%2520able%2520to%2520capture%2520concepts%2520like%2520ensembles%2520%2528dynamic%2520groups%2520of%250Acooperating%2520devices%2529%2520and%2520collective%2520tasks%2520%2528joint%2520activities%2520carried%2520out%2520by%250Aensembles%2529.%2520In%2520this%2520work%252C%2520we%2520consider%2520collections%2520of%2520devices%2520interacting%2520with%250Aneighbours%2520and%2520that%2520execute%2520in%2520nearly-synchronised%2520sense-compute-interact%250Arounds%252C%2520where%2520the%2520computation%2520is%2520given%2520by%2520a%2520single%2520program%2520mapping%2520sensing%250Avalues%2520and%2520incoming%2520messages%2520to%2520output%2520and%2520outcoming%2520messages.%2520To%2520support%250Aprogramming%2520whole%2520computational%2520collectives%252C%2520we%2520propose%2520the%2520abstraction%2520of%2520a%250Adistributed%2520collective%2520process%252C%2520which%2520can%2520be%2520used%2520to%2520define%2520at%2520once%2520the%250Aensemble%2520formation%2520logic%2520and%2520its%2520collective%2520task.%2520We%2520formalise%2520the%2520abstraction%250Ain%2520the%2520eXchange%2520Calculus%2520%2528XC%2529%252C%2520a%2520core%2520functional%2520language%2520based%2520on%2520neighbouring%250Avalues%2520%2528maps%2520from%2520neighbours%2520to%2520values%2529%2520where%2520state%2520and%2520interaction%2520is%2520handled%250Athrough%2520a%2520single%2520primitive%252C%2520exchange%252C%2520and%2520provide%2520a%2520corresponding%250Aimplementation%2520in%2520the%2520FCPP%2520language.%2520Then%252C%2520we%2520exercise%2520distributed%2520collective%250Aprocesses%2520using%2520two%2520case%2520studies%253A%2520multi-hop%2520message%2520propagation%2520and%2520distributed%250Amonitoring%2520of%2520spatial%2520properties.%2520Finally%252C%2520we%2520discuss%2520the%2520features%2520of%2520the%250Aabstraction%2520and%2520its%2520suitability%2520for%2520different%2520kinds%2520of%2520distributed%2520computing%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11212v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programming%20Distributed%20Collective%20Processes%20in%20the%20eXchange%20Calculus&entry.906535625=Giorgio%20Audrito%20and%20Roberto%20Casadei%20and%20Ferruccio%20Damiani%20and%20Gianluca%20Torta%20and%20Mirko%20Viroli&entry.1292438233=%20%20Recent%20trends%20like%20the%20Internet%20of%20Things%20%28IoT%29%20suggest%20a%20vision%20of%20dense%20and%0Amulti-scale%20deployments%20of%20computing%20devices%20in%20nearly%20all%20kinds%20of%0Aenvironments.%20A%20prominent%20engineering%20challenge%20revolves%20around%20programming%20the%0Acollective%20adaptive%20behaviour%20of%20such%20computational%20ecosystems.%20This%20requires%0Aabstractions%20able%20to%20capture%20concepts%20like%20ensembles%20%28dynamic%20groups%20of%0Acooperating%20devices%29%20and%20collective%20tasks%20%28joint%20activities%20carried%20out%20by%0Aensembles%29.%20In%20this%20work%2C%20we%20consider%20collections%20of%20devices%20interacting%20with%0Aneighbours%20and%20that%20execute%20in%20nearly-synchronised%20sense-compute-interact%0Arounds%2C%20where%20the%20computation%20is%20given%20by%20a%20single%20program%20mapping%20sensing%0Avalues%20and%20incoming%20messages%20to%20output%20and%20outcoming%20messages.%20To%20support%0Aprogramming%20whole%20computational%20collectives%2C%20we%20propose%20the%20abstraction%20of%20a%0Adistributed%20collective%20process%2C%20which%20can%20be%20used%20to%20define%20at%20once%20the%0Aensemble%20formation%20logic%20and%20its%20collective%20task.%20We%20formalise%20the%20abstraction%0Ain%20the%20eXchange%20Calculus%20%28XC%29%2C%20a%20core%20functional%20language%20based%20on%20neighbouring%0Avalues%20%28maps%20from%20neighbours%20to%20values%29%20where%20state%20and%20interaction%20is%20handled%0Athrough%20a%20single%20primitive%2C%20exchange%2C%20and%20provide%20a%20corresponding%0Aimplementation%20in%20the%20FCPP%20language.%20Then%2C%20we%20exercise%20distributed%20collective%0Aprocesses%20using%20two%20case%20studies%3A%20multi-hop%20message%20propagation%20and%20distributed%0Amonitoring%20of%20spatial%20properties.%20Finally%2C%20we%20discuss%20the%20features%20of%20the%0Aabstraction%20and%20its%20suitability%20for%20different%20kinds%20of%20distributed%20computing%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11212v5&entry.124074799=Read"},
{"title": "Distilling Invariant Representations with Dual Augmentation", "author": "Nikolaos Giakoumoglou and Tania Stathaki", "abstract": "  Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.\n", "link": "http://arxiv.org/abs/2410.09474v4", "date": "2025-07-16", "relevancy": 1.445, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4872}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Invariant%20Representations%20with%20Dual%20Augmentation&body=Title%3A%20Distilling%20Invariant%20Representations%20with%20Dual%20Augmentation%0AAuthor%3A%20Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20has%20been%20widely%20used%20to%20transfer%20knowledge%20from%0Alarge%2C%20accurate%20models%20%28teachers%29%20to%20smaller%2C%20efficient%20ones%20%28students%29.%20Recent%0Amethods%20have%20explored%20enforcing%20consistency%20by%20incorporating%20causal%0Ainterpretations%20to%20distill%20invariant%20representations.%20In%20this%20work%2C%20we%20extend%0Athis%20line%20of%20research%20by%20introducing%20a%20dual%20augmentation%20strategy%20to%20promote%0Ainvariant%20feature%20learning%20in%20both%20teacher%20and%20student%20models.%20Our%20approach%0Aleverages%20different%20augmentations%20applied%20to%20both%20models%20during%20distillation%2C%0Apushing%20the%20student%20to%20capture%20robust%2C%20transferable%20features.%20This%20dual%0Aaugmentation%20strategy%20complements%20invariant%20causal%20distillation%20by%20ensuring%0Athat%20the%20learned%20representations%20remain%20stable%20across%20a%20wider%20range%20of%20data%0Avariations%20and%20transformations.%20Extensive%20experiments%20on%20CIFAR-100%20demonstrate%0Athe%20effectiveness%20of%20this%20approach%2C%20achieving%20competitive%20results%20in%0Asame-architecture%20KD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09474v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Invariant%2520Representations%2520with%2520Dual%2520Augmentation%26entry.906535625%3DNikolaos%2520Giakoumoglou%2520and%2520Tania%2520Stathaki%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520has%2520been%2520widely%2520used%2520to%2520transfer%2520knowledge%2520from%250Alarge%252C%2520accurate%2520models%2520%2528teachers%2529%2520to%2520smaller%252C%2520efficient%2520ones%2520%2528students%2529.%2520Recent%250Amethods%2520have%2520explored%2520enforcing%2520consistency%2520by%2520incorporating%2520causal%250Ainterpretations%2520to%2520distill%2520invariant%2520representations.%2520In%2520this%2520work%252C%2520we%2520extend%250Athis%2520line%2520of%2520research%2520by%2520introducing%2520a%2520dual%2520augmentation%2520strategy%2520to%2520promote%250Ainvariant%2520feature%2520learning%2520in%2520both%2520teacher%2520and%2520student%2520models.%2520Our%2520approach%250Aleverages%2520different%2520augmentations%2520applied%2520to%2520both%2520models%2520during%2520distillation%252C%250Apushing%2520the%2520student%2520to%2520capture%2520robust%252C%2520transferable%2520features.%2520This%2520dual%250Aaugmentation%2520strategy%2520complements%2520invariant%2520causal%2520distillation%2520by%2520ensuring%250Athat%2520the%2520learned%2520representations%2520remain%2520stable%2520across%2520a%2520wider%2520range%2520of%2520data%250Avariations%2520and%2520transformations.%2520Extensive%2520experiments%2520on%2520CIFAR-100%2520demonstrate%250Athe%2520effectiveness%2520of%2520this%2520approach%252C%2520achieving%2520competitive%2520results%2520in%250Asame-architecture%2520KD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09474v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Invariant%20Representations%20with%20Dual%20Augmentation&entry.906535625=Nikolaos%20Giakoumoglou%20and%20Tania%20Stathaki&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20has%20been%20widely%20used%20to%20transfer%20knowledge%20from%0Alarge%2C%20accurate%20models%20%28teachers%29%20to%20smaller%2C%20efficient%20ones%20%28students%29.%20Recent%0Amethods%20have%20explored%20enforcing%20consistency%20by%20incorporating%20causal%0Ainterpretations%20to%20distill%20invariant%20representations.%20In%20this%20work%2C%20we%20extend%0Athis%20line%20of%20research%20by%20introducing%20a%20dual%20augmentation%20strategy%20to%20promote%0Ainvariant%20feature%20learning%20in%20both%20teacher%20and%20student%20models.%20Our%20approach%0Aleverages%20different%20augmentations%20applied%20to%20both%20models%20during%20distillation%2C%0Apushing%20the%20student%20to%20capture%20robust%2C%20transferable%20features.%20This%20dual%0Aaugmentation%20strategy%20complements%20invariant%20causal%20distillation%20by%20ensuring%0Athat%20the%20learned%20representations%20remain%20stable%20across%20a%20wider%20range%20of%20data%0Avariations%20and%20transformations.%20Extensive%20experiments%20on%20CIFAR-100%20demonstrate%0Athe%20effectiveness%20of%20this%20approach%2C%20achieving%20competitive%20results%20in%0Asame-architecture%20KD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09474v4&entry.124074799=Read"},
{"title": "MirrorCBO: A consensus-based optimization method in the spirit of mirror\n  descent", "author": "Leon Bungert and Franca Hoffmann and Dohyeon Kim and Tim Roith", "abstract": "  In this work we propose MirrorCBO, a consensus-based optimization (CBO)\nmethod which generalizes standard CBO in the same way that mirror descent\ngeneralizes gradient descent. For this we apply the CBO methodology to a swarm\nof dual particles and retain the primal particle positions by applying the\ninverse of the mirror map, which we parametrize as the subdifferential of a\nstrongly convex function $\\phi$. In this way, we combine the advantages of a\nderivative-free non-convex optimization algorithm with those of mirror descent.\nAs a special case, the method extends CBO to optimization problems with convex\nconstraints. Assuming bounds on the Bregman distance associated to $\\phi$, we\nprovide asymptotic convergence results for MirrorCBO with explicit exponential\nrate. Another key contribution is an exploratory numerical study of this new\nalgorithm across different application settings, focusing on (i)\nsparsity-inducing optimization, and (ii) constrained optimization,\ndemonstrating the competitive performance of MirrorCBO. We observe empirically\nthat the method can also be used for optimization on (non-convex) submanifolds\nof Euclidean space, can be adapted to mirrored versions of other recent CBO\nvariants, and that it inherits from mirror descent the capability to select\ndesirable minimizers, like sparse ones. We also include an overview of recent\nCBO approaches for constrained optimization and compare their performance to\nMirrorCBO.\n", "link": "http://arxiv.org/abs/2501.12189v2", "date": "2025-07-16", "relevancy": 1.4044, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4853}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MirrorCBO%3A%20A%20consensus-based%20optimization%20method%20in%20the%20spirit%20of%20mirror%0A%20%20descent&body=Title%3A%20MirrorCBO%3A%20A%20consensus-based%20optimization%20method%20in%20the%20spirit%20of%20mirror%0A%20%20descent%0AAuthor%3A%20Leon%20Bungert%20and%20Franca%20Hoffmann%20and%20Dohyeon%20Kim%20and%20Tim%20Roith%0AAbstract%3A%20%20%20In%20this%20work%20we%20propose%20MirrorCBO%2C%20a%20consensus-based%20optimization%20%28CBO%29%0Amethod%20which%20generalizes%20standard%20CBO%20in%20the%20same%20way%20that%20mirror%20descent%0Ageneralizes%20gradient%20descent.%20For%20this%20we%20apply%20the%20CBO%20methodology%20to%20a%20swarm%0Aof%20dual%20particles%20and%20retain%20the%20primal%20particle%20positions%20by%20applying%20the%0Ainverse%20of%20the%20mirror%20map%2C%20which%20we%20parametrize%20as%20the%20subdifferential%20of%20a%0Astrongly%20convex%20function%20%24%5Cphi%24.%20In%20this%20way%2C%20we%20combine%20the%20advantages%20of%20a%0Aderivative-free%20non-convex%20optimization%20algorithm%20with%20those%20of%20mirror%20descent.%0AAs%20a%20special%20case%2C%20the%20method%20extends%20CBO%20to%20optimization%20problems%20with%20convex%0Aconstraints.%20Assuming%20bounds%20on%20the%20Bregman%20distance%20associated%20to%20%24%5Cphi%24%2C%20we%0Aprovide%20asymptotic%20convergence%20results%20for%20MirrorCBO%20with%20explicit%20exponential%0Arate.%20Another%20key%20contribution%20is%20an%20exploratory%20numerical%20study%20of%20this%20new%0Aalgorithm%20across%20different%20application%20settings%2C%20focusing%20on%20%28i%29%0Asparsity-inducing%20optimization%2C%20and%20%28ii%29%20constrained%20optimization%2C%0Ademonstrating%20the%20competitive%20performance%20of%20MirrorCBO.%20We%20observe%20empirically%0Athat%20the%20method%20can%20also%20be%20used%20for%20optimization%20on%20%28non-convex%29%20submanifolds%0Aof%20Euclidean%20space%2C%20can%20be%20adapted%20to%20mirrored%20versions%20of%20other%20recent%20CBO%0Avariants%2C%20and%20that%20it%20inherits%20from%20mirror%20descent%20the%20capability%20to%20select%0Adesirable%20minimizers%2C%20like%20sparse%20ones.%20We%20also%20include%20an%20overview%20of%20recent%0ACBO%20approaches%20for%20constrained%20optimization%20and%20compare%20their%20performance%20to%0AMirrorCBO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirrorCBO%253A%2520A%2520consensus-based%2520optimization%2520method%2520in%2520the%2520spirit%2520of%2520mirror%250A%2520%2520descent%26entry.906535625%3DLeon%2520Bungert%2520and%2520Franca%2520Hoffmann%2520and%2520Dohyeon%2520Kim%2520and%2520Tim%2520Roith%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520propose%2520MirrorCBO%252C%2520a%2520consensus-based%2520optimization%2520%2528CBO%2529%250Amethod%2520which%2520generalizes%2520standard%2520CBO%2520in%2520the%2520same%2520way%2520that%2520mirror%2520descent%250Ageneralizes%2520gradient%2520descent.%2520For%2520this%2520we%2520apply%2520the%2520CBO%2520methodology%2520to%2520a%2520swarm%250Aof%2520dual%2520particles%2520and%2520retain%2520the%2520primal%2520particle%2520positions%2520by%2520applying%2520the%250Ainverse%2520of%2520the%2520mirror%2520map%252C%2520which%2520we%2520parametrize%2520as%2520the%2520subdifferential%2520of%2520a%250Astrongly%2520convex%2520function%2520%2524%255Cphi%2524.%2520In%2520this%2520way%252C%2520we%2520combine%2520the%2520advantages%2520of%2520a%250Aderivative-free%2520non-convex%2520optimization%2520algorithm%2520with%2520those%2520of%2520mirror%2520descent.%250AAs%2520a%2520special%2520case%252C%2520the%2520method%2520extends%2520CBO%2520to%2520optimization%2520problems%2520with%2520convex%250Aconstraints.%2520Assuming%2520bounds%2520on%2520the%2520Bregman%2520distance%2520associated%2520to%2520%2524%255Cphi%2524%252C%2520we%250Aprovide%2520asymptotic%2520convergence%2520results%2520for%2520MirrorCBO%2520with%2520explicit%2520exponential%250Arate.%2520Another%2520key%2520contribution%2520is%2520an%2520exploratory%2520numerical%2520study%2520of%2520this%2520new%250Aalgorithm%2520across%2520different%2520application%2520settings%252C%2520focusing%2520on%2520%2528i%2529%250Asparsity-inducing%2520optimization%252C%2520and%2520%2528ii%2529%2520constrained%2520optimization%252C%250Ademonstrating%2520the%2520competitive%2520performance%2520of%2520MirrorCBO.%2520We%2520observe%2520empirically%250Athat%2520the%2520method%2520can%2520also%2520be%2520used%2520for%2520optimization%2520on%2520%2528non-convex%2529%2520submanifolds%250Aof%2520Euclidean%2520space%252C%2520can%2520be%2520adapted%2520to%2520mirrored%2520versions%2520of%2520other%2520recent%2520CBO%250Avariants%252C%2520and%2520that%2520it%2520inherits%2520from%2520mirror%2520descent%2520the%2520capability%2520to%2520select%250Adesirable%2520minimizers%252C%2520like%2520sparse%2520ones.%2520We%2520also%2520include%2520an%2520overview%2520of%2520recent%250ACBO%2520approaches%2520for%2520constrained%2520optimization%2520and%2520compare%2520their%2520performance%2520to%250AMirrorCBO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MirrorCBO%3A%20A%20consensus-based%20optimization%20method%20in%20the%20spirit%20of%20mirror%0A%20%20descent&entry.906535625=Leon%20Bungert%20and%20Franca%20Hoffmann%20and%20Dohyeon%20Kim%20and%20Tim%20Roith&entry.1292438233=%20%20In%20this%20work%20we%20propose%20MirrorCBO%2C%20a%20consensus-based%20optimization%20%28CBO%29%0Amethod%20which%20generalizes%20standard%20CBO%20in%20the%20same%20way%20that%20mirror%20descent%0Ageneralizes%20gradient%20descent.%20For%20this%20we%20apply%20the%20CBO%20methodology%20to%20a%20swarm%0Aof%20dual%20particles%20and%20retain%20the%20primal%20particle%20positions%20by%20applying%20the%0Ainverse%20of%20the%20mirror%20map%2C%20which%20we%20parametrize%20as%20the%20subdifferential%20of%20a%0Astrongly%20convex%20function%20%24%5Cphi%24.%20In%20this%20way%2C%20we%20combine%20the%20advantages%20of%20a%0Aderivative-free%20non-convex%20optimization%20algorithm%20with%20those%20of%20mirror%20descent.%0AAs%20a%20special%20case%2C%20the%20method%20extends%20CBO%20to%20optimization%20problems%20with%20convex%0Aconstraints.%20Assuming%20bounds%20on%20the%20Bregman%20distance%20associated%20to%20%24%5Cphi%24%2C%20we%0Aprovide%20asymptotic%20convergence%20results%20for%20MirrorCBO%20with%20explicit%20exponential%0Arate.%20Another%20key%20contribution%20is%20an%20exploratory%20numerical%20study%20of%20this%20new%0Aalgorithm%20across%20different%20application%20settings%2C%20focusing%20on%20%28i%29%0Asparsity-inducing%20optimization%2C%20and%20%28ii%29%20constrained%20optimization%2C%0Ademonstrating%20the%20competitive%20performance%20of%20MirrorCBO.%20We%20observe%20empirically%0Athat%20the%20method%20can%20also%20be%20used%20for%20optimization%20on%20%28non-convex%29%20submanifolds%0Aof%20Euclidean%20space%2C%20can%20be%20adapted%20to%20mirrored%20versions%20of%20other%20recent%20CBO%0Avariants%2C%20and%20that%20it%20inherits%20from%20mirror%20descent%20the%20capability%20to%20select%0Adesirable%20minimizers%2C%20like%20sparse%20ones.%20We%20also%20include%20an%20overview%20of%20recent%0ACBO%20approaches%20for%20constrained%20optimization%20and%20compare%20their%20performance%20to%0AMirrorCBO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12189v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


