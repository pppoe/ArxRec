<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241013.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Cavia: Camera-controllable Multi-view Video Diffusion with\n  View-Integrated Attention", "author": "Dejia Xu and Yifan Jiang and Chen Huang and Liangchen Song and Thorsten Gernoth and Liangliang Cao and Zhangyang Wang and Hao Tang", "abstract": "  In recent years there have been remarkable breakthroughs in image-to-video\ngeneration. However, the 3D consistency and camera controllability of generated\nframes have remained unsolved. Recent studies have attempted to incorporate\ncamera control into the generation process, but their results are often limited\nto simple trajectories or lack the ability to generate consistent videos from\nmultiple distinct camera paths for the same scene. To address these\nlimitations, we introduce Cavia, a novel framework for camera-controllable,\nmulti-view video generation, capable of converting an input image into multiple\nspatiotemporally consistent videos. Our framework extends the spatial and\ntemporal attention modules into view-integrated attention modules, improving\nboth viewpoint and temporal consistency. This flexible design allows for joint\ntraining with diverse curated data sources, including scene-level static\nvideos, object-level synthetic multi-view dynamic videos, and real-world\nmonocular dynamic videos. To our best knowledge, Cavia is the first of its kind\nthat allows the user to precisely specify camera motion while obtaining object\nmotion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art\nmethods in terms of geometric consistency and perceptual quality. Project Page:\nhttps://ir1d.github.io/Cavia/\n", "link": "http://arxiv.org/abs/2410.10774v1", "date": "2024-10-14", "relevancy": 3.4193, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7026}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7026}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cavia%3A%20Camera-controllable%20Multi-view%20Video%20Diffusion%20with%0A%20%20View-Integrated%20Attention&body=Title%3A%20Cavia%3A%20Camera-controllable%20Multi-view%20Video%20Diffusion%20with%0A%20%20View-Integrated%20Attention%0AAuthor%3A%20Dejia%20Xu%20and%20Yifan%20Jiang%20and%20Chen%20Huang%20and%20Liangchen%20Song%20and%20Thorsten%20Gernoth%20and%20Liangliang%20Cao%20and%20Zhangyang%20Wang%20and%20Hao%20Tang%0AAbstract%3A%20%20%20In%20recent%20years%20there%20have%20been%20remarkable%20breakthroughs%20in%20image-to-video%0Ageneration.%20However%2C%20the%203D%20consistency%20and%20camera%20controllability%20of%20generated%0Aframes%20have%20remained%20unsolved.%20Recent%20studies%20have%20attempted%20to%20incorporate%0Acamera%20control%20into%20the%20generation%20process%2C%20but%20their%20results%20are%20often%20limited%0Ato%20simple%20trajectories%20or%20lack%20the%20ability%20to%20generate%20consistent%20videos%20from%0Amultiple%20distinct%20camera%20paths%20for%20the%20same%20scene.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Cavia%2C%20a%20novel%20framework%20for%20camera-controllable%2C%0Amulti-view%20video%20generation%2C%20capable%20of%20converting%20an%20input%20image%20into%20multiple%0Aspatiotemporally%20consistent%20videos.%20Our%20framework%20extends%20the%20spatial%20and%0Atemporal%20attention%20modules%20into%20view-integrated%20attention%20modules%2C%20improving%0Aboth%20viewpoint%20and%20temporal%20consistency.%20This%20flexible%20design%20allows%20for%20joint%0Atraining%20with%20diverse%20curated%20data%20sources%2C%20including%20scene-level%20static%0Avideos%2C%20object-level%20synthetic%20multi-view%20dynamic%20videos%2C%20and%20real-world%0Amonocular%20dynamic%20videos.%20To%20our%20best%20knowledge%2C%20Cavia%20is%20the%20first%20of%20its%20kind%0Athat%20allows%20the%20user%20to%20precisely%20specify%20camera%20motion%20while%20obtaining%20object%0Amotion.%20Extensive%20experiments%20demonstrate%20that%20Cavia%20surpasses%20state-of-the-art%0Amethods%20in%20terms%20of%20geometric%20consistency%20and%20perceptual%20quality.%20Project%20Page%3A%0Ahttps%3A//ir1d.github.io/Cavia/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCavia%253A%2520Camera-controllable%2520Multi-view%2520Video%2520Diffusion%2520with%250A%2520%2520View-Integrated%2520Attention%26entry.906535625%3DDejia%2520Xu%2520and%2520Yifan%2520Jiang%2520and%2520Chen%2520Huang%2520and%2520Liangchen%2520Song%2520and%2520Thorsten%2520Gernoth%2520and%2520Liangliang%2520Cao%2520and%2520Zhangyang%2520Wang%2520and%2520Hao%2520Tang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%2520there%2520have%2520been%2520remarkable%2520breakthroughs%2520in%2520image-to-video%250Ageneration.%2520However%252C%2520the%25203D%2520consistency%2520and%2520camera%2520controllability%2520of%2520generated%250Aframes%2520have%2520remained%2520unsolved.%2520Recent%2520studies%2520have%2520attempted%2520to%2520incorporate%250Acamera%2520control%2520into%2520the%2520generation%2520process%252C%2520but%2520their%2520results%2520are%2520often%2520limited%250Ato%2520simple%2520trajectories%2520or%2520lack%2520the%2520ability%2520to%2520generate%2520consistent%2520videos%2520from%250Amultiple%2520distinct%2520camera%2520paths%2520for%2520the%2520same%2520scene.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520Cavia%252C%2520a%2520novel%2520framework%2520for%2520camera-controllable%252C%250Amulti-view%2520video%2520generation%252C%2520capable%2520of%2520converting%2520an%2520input%2520image%2520into%2520multiple%250Aspatiotemporally%2520consistent%2520videos.%2520Our%2520framework%2520extends%2520the%2520spatial%2520and%250Atemporal%2520attention%2520modules%2520into%2520view-integrated%2520attention%2520modules%252C%2520improving%250Aboth%2520viewpoint%2520and%2520temporal%2520consistency.%2520This%2520flexible%2520design%2520allows%2520for%2520joint%250Atraining%2520with%2520diverse%2520curated%2520data%2520sources%252C%2520including%2520scene-level%2520static%250Avideos%252C%2520object-level%2520synthetic%2520multi-view%2520dynamic%2520videos%252C%2520and%2520real-world%250Amonocular%2520dynamic%2520videos.%2520To%2520our%2520best%2520knowledge%252C%2520Cavia%2520is%2520the%2520first%2520of%2520its%2520kind%250Athat%2520allows%2520the%2520user%2520to%2520precisely%2520specify%2520camera%2520motion%2520while%2520obtaining%2520object%250Amotion.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Cavia%2520surpasses%2520state-of-the-art%250Amethods%2520in%2520terms%2520of%2520geometric%2520consistency%2520and%2520perceptual%2520quality.%2520Project%2520Page%253A%250Ahttps%253A//ir1d.github.io/Cavia/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cavia%3A%20Camera-controllable%20Multi-view%20Video%20Diffusion%20with%0A%20%20View-Integrated%20Attention&entry.906535625=Dejia%20Xu%20and%20Yifan%20Jiang%20and%20Chen%20Huang%20and%20Liangchen%20Song%20and%20Thorsten%20Gernoth%20and%20Liangliang%20Cao%20and%20Zhangyang%20Wang%20and%20Hao%20Tang&entry.1292438233=%20%20In%20recent%20years%20there%20have%20been%20remarkable%20breakthroughs%20in%20image-to-video%0Ageneration.%20However%2C%20the%203D%20consistency%20and%20camera%20controllability%20of%20generated%0Aframes%20have%20remained%20unsolved.%20Recent%20studies%20have%20attempted%20to%20incorporate%0Acamera%20control%20into%20the%20generation%20process%2C%20but%20their%20results%20are%20often%20limited%0Ato%20simple%20trajectories%20or%20lack%20the%20ability%20to%20generate%20consistent%20videos%20from%0Amultiple%20distinct%20camera%20paths%20for%20the%20same%20scene.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Cavia%2C%20a%20novel%20framework%20for%20camera-controllable%2C%0Amulti-view%20video%20generation%2C%20capable%20of%20converting%20an%20input%20image%20into%20multiple%0Aspatiotemporally%20consistent%20videos.%20Our%20framework%20extends%20the%20spatial%20and%0Atemporal%20attention%20modules%20into%20view-integrated%20attention%20modules%2C%20improving%0Aboth%20viewpoint%20and%20temporal%20consistency.%20This%20flexible%20design%20allows%20for%20joint%0Atraining%20with%20diverse%20curated%20data%20sources%2C%20including%20scene-level%20static%0Avideos%2C%20object-level%20synthetic%20multi-view%20dynamic%20videos%2C%20and%20real-world%0Amonocular%20dynamic%20videos.%20To%20our%20best%20knowledge%2C%20Cavia%20is%20the%20first%20of%20its%20kind%0Athat%20allows%20the%20user%20to%20precisely%20specify%20camera%20motion%20while%20obtaining%20object%0Amotion.%20Extensive%20experiments%20demonstrate%20that%20Cavia%20surpasses%20state-of-the-art%0Amethods%20in%20terms%20of%20geometric%20consistency%20and%20perceptual%20quality.%20Project%20Page%3A%0Ahttps%3A//ir1d.github.io/Cavia/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10774v1&entry.124074799=Read"},
{"title": "Towards Foundation Models for 3D Vision: How Close Are We?", "author": "Yiming Zuo and Karhan Kayan and Maggie Wang and Kevin Jeon and Jia Deng and Thomas L. Griffiths", "abstract": "  Building a foundation model for 3D vision is a complex challenge that remains\nunsolved. Towards that goal, it is important to understand the 3D reasoning\ncapabilities of current models as well as identify the gaps between these\nmodels and humans. Therefore, we construct a new 3D visual understanding\nbenchmark that covers fundamental 3D vision tasks in the Visual Question\nAnswering (VQA) format. We evaluate state-of-the-art Vision-Language Models\n(VLMs), specialized models, and human subjects on it. Our results show that\nVLMs generally perform poorly, while the specialized models are accurate but\nnot robust, failing under geometric perturbations. In contrast, human vision\ncontinues to be the most reliable 3D visual system. We further demonstrate that\nneural networks align more closely with human 3D vision mechanisms compared to\nclassical computer vision methods, and Transformer-based networks such as ViT\nalign more closely with human 3D vision mechanisms than CNNs. We hope our study\nwill benefit the future development of foundation models for 3D vision.\n", "link": "http://arxiv.org/abs/2410.10799v1", "date": "2024-10-14", "relevancy": 3.346, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20for%203D%20Vision%3A%20How%20Close%20Are%20We%3F&body=Title%3A%20Towards%20Foundation%20Models%20for%203D%20Vision%3A%20How%20Close%20Are%20We%3F%0AAuthor%3A%20Yiming%20Zuo%20and%20Karhan%20Kayan%20and%20Maggie%20Wang%20and%20Kevin%20Jeon%20and%20Jia%20Deng%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Building%20a%20foundation%20model%20for%203D%20vision%20is%20a%20complex%20challenge%20that%20remains%0Aunsolved.%20Towards%20that%20goal%2C%20it%20is%20important%20to%20understand%20the%203D%20reasoning%0Acapabilities%20of%20current%20models%20as%20well%20as%20identify%20the%20gaps%20between%20these%0Amodels%20and%20humans.%20Therefore%2C%20we%20construct%20a%20new%203D%20visual%20understanding%0Abenchmark%20that%20covers%20fundamental%203D%20vision%20tasks%20in%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20format.%20We%20evaluate%20state-of-the-art%20Vision-Language%20Models%0A%28VLMs%29%2C%20specialized%20models%2C%20and%20human%20subjects%20on%20it.%20Our%20results%20show%20that%0AVLMs%20generally%20perform%20poorly%2C%20while%20the%20specialized%20models%20are%20accurate%20but%0Anot%20robust%2C%20failing%20under%20geometric%20perturbations.%20In%20contrast%2C%20human%20vision%0Acontinues%20to%20be%20the%20most%20reliable%203D%20visual%20system.%20We%20further%20demonstrate%20that%0Aneural%20networks%20align%20more%20closely%20with%20human%203D%20vision%20mechanisms%20compared%20to%0Aclassical%20computer%20vision%20methods%2C%20and%20Transformer-based%20networks%20such%20as%20ViT%0Aalign%20more%20closely%20with%20human%203D%20vision%20mechanisms%20than%20CNNs.%20We%20hope%20our%20study%0Awill%20benefit%20the%20future%20development%20of%20foundation%20models%20for%203D%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520for%25203D%2520Vision%253A%2520How%2520Close%2520Are%2520We%253F%26entry.906535625%3DYiming%2520Zuo%2520and%2520Karhan%2520Kayan%2520and%2520Maggie%2520Wang%2520and%2520Kevin%2520Jeon%2520and%2520Jia%2520Deng%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Building%2520a%2520foundation%2520model%2520for%25203D%2520vision%2520is%2520a%2520complex%2520challenge%2520that%2520remains%250Aunsolved.%2520Towards%2520that%2520goal%252C%2520it%2520is%2520important%2520to%2520understand%2520the%25203D%2520reasoning%250Acapabilities%2520of%2520current%2520models%2520as%2520well%2520as%2520identify%2520the%2520gaps%2520between%2520these%250Amodels%2520and%2520humans.%2520Therefore%252C%2520we%2520construct%2520a%2520new%25203D%2520visual%2520understanding%250Abenchmark%2520that%2520covers%2520fundamental%25203D%2520vision%2520tasks%2520in%2520the%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520format.%2520We%2520evaluate%2520state-of-the-art%2520Vision-Language%2520Models%250A%2528VLMs%2529%252C%2520specialized%2520models%252C%2520and%2520human%2520subjects%2520on%2520it.%2520Our%2520results%2520show%2520that%250AVLMs%2520generally%2520perform%2520poorly%252C%2520while%2520the%2520specialized%2520models%2520are%2520accurate%2520but%250Anot%2520robust%252C%2520failing%2520under%2520geometric%2520perturbations.%2520In%2520contrast%252C%2520human%2520vision%250Acontinues%2520to%2520be%2520the%2520most%2520reliable%25203D%2520visual%2520system.%2520We%2520further%2520demonstrate%2520that%250Aneural%2520networks%2520align%2520more%2520closely%2520with%2520human%25203D%2520vision%2520mechanisms%2520compared%2520to%250Aclassical%2520computer%2520vision%2520methods%252C%2520and%2520Transformer-based%2520networks%2520such%2520as%2520ViT%250Aalign%2520more%2520closely%2520with%2520human%25203D%2520vision%2520mechanisms%2520than%2520CNNs.%2520We%2520hope%2520our%2520study%250Awill%2520benefit%2520the%2520future%2520development%2520of%2520foundation%2520models%2520for%25203D%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20for%203D%20Vision%3A%20How%20Close%20Are%20We%3F&entry.906535625=Yiming%20Zuo%20and%20Karhan%20Kayan%20and%20Maggie%20Wang%20and%20Kevin%20Jeon%20and%20Jia%20Deng%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Building%20a%20foundation%20model%20for%203D%20vision%20is%20a%20complex%20challenge%20that%20remains%0Aunsolved.%20Towards%20that%20goal%2C%20it%20is%20important%20to%20understand%20the%203D%20reasoning%0Acapabilities%20of%20current%20models%20as%20well%20as%20identify%20the%20gaps%20between%20these%0Amodels%20and%20humans.%20Therefore%2C%20we%20construct%20a%20new%203D%20visual%20understanding%0Abenchmark%20that%20covers%20fundamental%203D%20vision%20tasks%20in%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20format.%20We%20evaluate%20state-of-the-art%20Vision-Language%20Models%0A%28VLMs%29%2C%20specialized%20models%2C%20and%20human%20subjects%20on%20it.%20Our%20results%20show%20that%0AVLMs%20generally%20perform%20poorly%2C%20while%20the%20specialized%20models%20are%20accurate%20but%0Anot%20robust%2C%20failing%20under%20geometric%20perturbations.%20In%20contrast%2C%20human%20vision%0Acontinues%20to%20be%20the%20most%20reliable%203D%20visual%20system.%20We%20further%20demonstrate%20that%0Aneural%20networks%20align%20more%20closely%20with%20human%203D%20vision%20mechanisms%20compared%20to%0Aclassical%20computer%20vision%20methods%2C%20and%20Transformer-based%20networks%20such%20as%20ViT%0Aalign%20more%20closely%20with%20human%203D%20vision%20mechanisms%20than%20CNNs.%20We%20hope%20our%20study%0Awill%20benefit%20the%20future%20development%20of%20foundation%20models%20for%203D%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10799v1&entry.124074799=Read"},
{"title": "4-LEGS: 4D Language Embedded Gaussian Splatting", "author": "Gal Fiebelman and Tamir Cohen and Ayellet Morgenstern and Peter Hedman and Hadar Averbuch-Elor", "abstract": "  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. %, \\gal{while introducing a feature-proximity attention\nmechanism that allows for neighboring features in 3D space to interact}. This\nenables an interactive interface where the user can spatiotemporally localize\nevents in the video from text prompts. We demonstrate our system on public 3D\nvideo datasets of people and animals performing various actions.\n", "link": "http://arxiv.org/abs/2410.10719v1", "date": "2024-10-14", "relevancy": 3.3204, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6809}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6683}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting&body=Title%3A%204-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting%0AAuthor%3A%20Gal%20Fiebelman%20and%20Tamir%20Cohen%20and%20Ayellet%20Morgenstern%20and%20Peter%20Hedman%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20The%20emergence%20of%20neural%20representations%20has%20revolutionized%20our%20means%20for%0Adigitally%20viewing%20a%20wide%20range%20of%203D%20scenes%2C%20enabling%20the%20synthesis%20of%0Aphotorealistic%20images%20rendered%20from%20novel%20views.%20Recently%2C%20several%20techniques%0Ahave%20been%20proposed%20for%20connecting%20these%20low-level%20representations%20with%20the%0Ahigh-level%20semantics%20understanding%20embodied%20within%20the%20scene.%20These%20methods%0Aelevate%20the%20rich%20semantic%20understanding%20from%202D%20imagery%20to%203D%20representations%2C%0Adistilling%20high-dimensional%20spatial%20features%20onto%203D%20space.%20In%20our%20work%2C%20we%20are%0Ainterested%20in%20connecting%20language%20with%20a%20dynamic%20modeling%20of%20the%20world.%20We%20show%0Ahow%20to%20lift%20spatio-temporal%20features%20to%20a%204D%20representation%20based%20on%203D%0AGaussian%20Splatting.%20%25%2C%20%5Cgal%7Bwhile%20introducing%20a%20feature-proximity%20attention%0Amechanism%20that%20allows%20for%20neighboring%20features%20in%203D%20space%20to%20interact%7D.%20This%0Aenables%20an%20interactive%20interface%20where%20the%20user%20can%20spatiotemporally%20localize%0Aevents%20in%20the%20video%20from%20text%20prompts.%20We%20demonstrate%20our%20system%20on%20public%203D%0Avideo%20datasets%20of%20people%20and%20animals%20performing%20various%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4-LEGS%253A%25204D%2520Language%2520Embedded%2520Gaussian%2520Splatting%26entry.906535625%3DGal%2520Fiebelman%2520and%2520Tamir%2520Cohen%2520and%2520Ayellet%2520Morgenstern%2520and%2520Peter%2520Hedman%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520neural%2520representations%2520has%2520revolutionized%2520our%2520means%2520for%250Adigitally%2520viewing%2520a%2520wide%2520range%2520of%25203D%2520scenes%252C%2520enabling%2520the%2520synthesis%2520of%250Aphotorealistic%2520images%2520rendered%2520from%2520novel%2520views.%2520Recently%252C%2520several%2520techniques%250Ahave%2520been%2520proposed%2520for%2520connecting%2520these%2520low-level%2520representations%2520with%2520the%250Ahigh-level%2520semantics%2520understanding%2520embodied%2520within%2520the%2520scene.%2520These%2520methods%250Aelevate%2520the%2520rich%2520semantic%2520understanding%2520from%25202D%2520imagery%2520to%25203D%2520representations%252C%250Adistilling%2520high-dimensional%2520spatial%2520features%2520onto%25203D%2520space.%2520In%2520our%2520work%252C%2520we%2520are%250Ainterested%2520in%2520connecting%2520language%2520with%2520a%2520dynamic%2520modeling%2520of%2520the%2520world.%2520We%2520show%250Ahow%2520to%2520lift%2520spatio-temporal%2520features%2520to%2520a%25204D%2520representation%2520based%2520on%25203D%250AGaussian%2520Splatting.%2520%2525%252C%2520%255Cgal%257Bwhile%2520introducing%2520a%2520feature-proximity%2520attention%250Amechanism%2520that%2520allows%2520for%2520neighboring%2520features%2520in%25203D%2520space%2520to%2520interact%257D.%2520This%250Aenables%2520an%2520interactive%2520interface%2520where%2520the%2520user%2520can%2520spatiotemporally%2520localize%250Aevents%2520in%2520the%2520video%2520from%2520text%2520prompts.%2520We%2520demonstrate%2520our%2520system%2520on%2520public%25203D%250Avideo%2520datasets%2520of%2520people%2520and%2520animals%2520performing%2520various%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting&entry.906535625=Gal%20Fiebelman%20and%20Tamir%20Cohen%20and%20Ayellet%20Morgenstern%20and%20Peter%20Hedman%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20The%20emergence%20of%20neural%20representations%20has%20revolutionized%20our%20means%20for%0Adigitally%20viewing%20a%20wide%20range%20of%203D%20scenes%2C%20enabling%20the%20synthesis%20of%0Aphotorealistic%20images%20rendered%20from%20novel%20views.%20Recently%2C%20several%20techniques%0Ahave%20been%20proposed%20for%20connecting%20these%20low-level%20representations%20with%20the%0Ahigh-level%20semantics%20understanding%20embodied%20within%20the%20scene.%20These%20methods%0Aelevate%20the%20rich%20semantic%20understanding%20from%202D%20imagery%20to%203D%20representations%2C%0Adistilling%20high-dimensional%20spatial%20features%20onto%203D%20space.%20In%20our%20work%2C%20we%20are%0Ainterested%20in%20connecting%20language%20with%20a%20dynamic%20modeling%20of%20the%20world.%20We%20show%0Ahow%20to%20lift%20spatio-temporal%20features%20to%20a%204D%20representation%20based%20on%203D%0AGaussian%20Splatting.%20%25%2C%20%5Cgal%7Bwhile%20introducing%20a%20feature-proximity%20attention%0Amechanism%20that%20allows%20for%20neighboring%20features%20in%203D%20space%20to%20interact%7D.%20This%0Aenables%20an%20interactive%20interface%20where%20the%20user%20can%20spatiotemporally%20localize%0Aevents%20in%20the%20video%20from%20text%20prompts.%20We%20demonstrate%20our%20system%20on%20public%203D%0Avideo%20datasets%20of%20people%20and%20animals%20performing%20various%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10719v1&entry.124074799=Read"},
{"title": "SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream", "author": "Jinze Yu and Xin Peng and Zhengda Lu and Laurent Kneip and Yiqun Wang", "abstract": "  A spike camera is a specialized high-speed visual sensor that offers\nadvantages such as high temporal resolution and high dynamic range compared to\nconventional frame cameras. These features provide the camera with significant\nadvantages in many computer vision tasks. However, the tasks of novel view\nsynthesis based on spike cameras remain underdeveloped. Although there are\nexisting methods for learning neural radiance fields from spike stream, they\neither lack robustness in extremely noisy, low-quality lighting conditions or\nsuffer from high computational complexity due to the deep fully connected\nneural networks and ray marching rendering strategies used in neural radiance\nfields, making it difficult to recover fine texture details. In contrast, the\nlatest advancements in 3DGS have achieved high-quality real-time rendering by\noptimizing the point cloud representation into Gaussian ellipsoids. Building on\nthis, we introduce SpikeGS, the method to learn 3D Gaussian fields solely from\nspike stream. We designed a differentiable spike stream rendering framework\nbased on 3DGS, incorporating noise embedding and spiking neurons. By leveraging\nthe multi-view consistency of 3DGS and the tile-based multi-threaded parallel\nrendering mechanism, we achieved high-quality real-time rendering results.\nAdditionally, we introduced a spike rendering loss function that generalizes\nunder varying illumination conditions. Our method can reconstruct view\nsynthesis results with fine texture details from a continuous spike stream\ncaptured by a moving spike camera, while demonstrating high robustness in\nextremely noisy low-light scenarios. Experimental results on both real and\nsynthetic datasets demonstrate that our method surpasses existing approaches in\nterms of rendering quality and speed. Our code will be available at\nhttps://github.com/520jz/SpikeGS.\n", "link": "http://arxiv.org/abs/2409.15176v5", "date": "2024-10-14", "relevancy": 3.2434, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6811}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6568}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeGS%3A%20Learning%203D%20Gaussian%20Fields%20from%20Continuous%20Spike%20Stream&body=Title%3A%20SpikeGS%3A%20Learning%203D%20Gaussian%20Fields%20from%20Continuous%20Spike%20Stream%0AAuthor%3A%20Jinze%20Yu%20and%20Xin%20Peng%20and%20Zhengda%20Lu%20and%20Laurent%20Kneip%20and%20Yiqun%20Wang%0AAbstract%3A%20%20%20A%20spike%20camera%20is%20a%20specialized%20high-speed%20visual%20sensor%20that%20offers%0Aadvantages%20such%20as%20high%20temporal%20resolution%20and%20high%20dynamic%20range%20compared%20to%0Aconventional%20frame%20cameras.%20These%20features%20provide%20the%20camera%20with%20significant%0Aadvantages%20in%20many%20computer%20vision%20tasks.%20However%2C%20the%20tasks%20of%20novel%20view%0Asynthesis%20based%20on%20spike%20cameras%20remain%20underdeveloped.%20Although%20there%20are%0Aexisting%20methods%20for%20learning%20neural%20radiance%20fields%20from%20spike%20stream%2C%20they%0Aeither%20lack%20robustness%20in%20extremely%20noisy%2C%20low-quality%20lighting%20conditions%20or%0Asuffer%20from%20high%20computational%20complexity%20due%20to%20the%20deep%20fully%20connected%0Aneural%20networks%20and%20ray%20marching%20rendering%20strategies%20used%20in%20neural%20radiance%0Afields%2C%20making%20it%20difficult%20to%20recover%20fine%20texture%20details.%20In%20contrast%2C%20the%0Alatest%20advancements%20in%203DGS%20have%20achieved%20high-quality%20real-time%20rendering%20by%0Aoptimizing%20the%20point%20cloud%20representation%20into%20Gaussian%20ellipsoids.%20Building%20on%0Athis%2C%20we%20introduce%20SpikeGS%2C%20the%20method%20to%20learn%203D%20Gaussian%20fields%20solely%20from%0Aspike%20stream.%20We%20designed%20a%20differentiable%20spike%20stream%20rendering%20framework%0Abased%20on%203DGS%2C%20incorporating%20noise%20embedding%20and%20spiking%20neurons.%20By%20leveraging%0Athe%20multi-view%20consistency%20of%203DGS%20and%20the%20tile-based%20multi-threaded%20parallel%0Arendering%20mechanism%2C%20we%20achieved%20high-quality%20real-time%20rendering%20results.%0AAdditionally%2C%20we%20introduced%20a%20spike%20rendering%20loss%20function%20that%20generalizes%0Aunder%20varying%20illumination%20conditions.%20Our%20method%20can%20reconstruct%20view%0Asynthesis%20results%20with%20fine%20texture%20details%20from%20a%20continuous%20spike%20stream%0Acaptured%20by%20a%20moving%20spike%20camera%2C%20while%20demonstrating%20high%20robustness%20in%0Aextremely%20noisy%20low-light%20scenarios.%20Experimental%20results%20on%20both%20real%20and%0Asynthetic%20datasets%20demonstrate%20that%20our%20method%20surpasses%20existing%20approaches%20in%0Aterms%20of%20rendering%20quality%20and%20speed.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/520jz/SpikeGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15176v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeGS%253A%2520Learning%25203D%2520Gaussian%2520Fields%2520from%2520Continuous%2520Spike%2520Stream%26entry.906535625%3DJinze%2520Yu%2520and%2520Xin%2520Peng%2520and%2520Zhengda%2520Lu%2520and%2520Laurent%2520Kneip%2520and%2520Yiqun%2520Wang%26entry.1292438233%3D%2520%2520A%2520spike%2520camera%2520is%2520a%2520specialized%2520high-speed%2520visual%2520sensor%2520that%2520offers%250Aadvantages%2520such%2520as%2520high%2520temporal%2520resolution%2520and%2520high%2520dynamic%2520range%2520compared%2520to%250Aconventional%2520frame%2520cameras.%2520These%2520features%2520provide%2520the%2520camera%2520with%2520significant%250Aadvantages%2520in%2520many%2520computer%2520vision%2520tasks.%2520However%252C%2520the%2520tasks%2520of%2520novel%2520view%250Asynthesis%2520based%2520on%2520spike%2520cameras%2520remain%2520underdeveloped.%2520Although%2520there%2520are%250Aexisting%2520methods%2520for%2520learning%2520neural%2520radiance%2520fields%2520from%2520spike%2520stream%252C%2520they%250Aeither%2520lack%2520robustness%2520in%2520extremely%2520noisy%252C%2520low-quality%2520lighting%2520conditions%2520or%250Asuffer%2520from%2520high%2520computational%2520complexity%2520due%2520to%2520the%2520deep%2520fully%2520connected%250Aneural%2520networks%2520and%2520ray%2520marching%2520rendering%2520strategies%2520used%2520in%2520neural%2520radiance%250Afields%252C%2520making%2520it%2520difficult%2520to%2520recover%2520fine%2520texture%2520details.%2520In%2520contrast%252C%2520the%250Alatest%2520advancements%2520in%25203DGS%2520have%2520achieved%2520high-quality%2520real-time%2520rendering%2520by%250Aoptimizing%2520the%2520point%2520cloud%2520representation%2520into%2520Gaussian%2520ellipsoids.%2520Building%2520on%250Athis%252C%2520we%2520introduce%2520SpikeGS%252C%2520the%2520method%2520to%2520learn%25203D%2520Gaussian%2520fields%2520solely%2520from%250Aspike%2520stream.%2520We%2520designed%2520a%2520differentiable%2520spike%2520stream%2520rendering%2520framework%250Abased%2520on%25203DGS%252C%2520incorporating%2520noise%2520embedding%2520and%2520spiking%2520neurons.%2520By%2520leveraging%250Athe%2520multi-view%2520consistency%2520of%25203DGS%2520and%2520the%2520tile-based%2520multi-threaded%2520parallel%250Arendering%2520mechanism%252C%2520we%2520achieved%2520high-quality%2520real-time%2520rendering%2520results.%250AAdditionally%252C%2520we%2520introduced%2520a%2520spike%2520rendering%2520loss%2520function%2520that%2520generalizes%250Aunder%2520varying%2520illumination%2520conditions.%2520Our%2520method%2520can%2520reconstruct%2520view%250Asynthesis%2520results%2520with%2520fine%2520texture%2520details%2520from%2520a%2520continuous%2520spike%2520stream%250Acaptured%2520by%2520a%2520moving%2520spike%2520camera%252C%2520while%2520demonstrating%2520high%2520robustness%2520in%250Aextremely%2520noisy%2520low-light%2520scenarios.%2520Experimental%2520results%2520on%2520both%2520real%2520and%250Asynthetic%2520datasets%2520demonstrate%2520that%2520our%2520method%2520surpasses%2520existing%2520approaches%2520in%250Aterms%2520of%2520rendering%2520quality%2520and%2520speed.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/520jz/SpikeGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15176v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeGS%3A%20Learning%203D%20Gaussian%20Fields%20from%20Continuous%20Spike%20Stream&entry.906535625=Jinze%20Yu%20and%20Xin%20Peng%20and%20Zhengda%20Lu%20and%20Laurent%20Kneip%20and%20Yiqun%20Wang&entry.1292438233=%20%20A%20spike%20camera%20is%20a%20specialized%20high-speed%20visual%20sensor%20that%20offers%0Aadvantages%20such%20as%20high%20temporal%20resolution%20and%20high%20dynamic%20range%20compared%20to%0Aconventional%20frame%20cameras.%20These%20features%20provide%20the%20camera%20with%20significant%0Aadvantages%20in%20many%20computer%20vision%20tasks.%20However%2C%20the%20tasks%20of%20novel%20view%0Asynthesis%20based%20on%20spike%20cameras%20remain%20underdeveloped.%20Although%20there%20are%0Aexisting%20methods%20for%20learning%20neural%20radiance%20fields%20from%20spike%20stream%2C%20they%0Aeither%20lack%20robustness%20in%20extremely%20noisy%2C%20low-quality%20lighting%20conditions%20or%0Asuffer%20from%20high%20computational%20complexity%20due%20to%20the%20deep%20fully%20connected%0Aneural%20networks%20and%20ray%20marching%20rendering%20strategies%20used%20in%20neural%20radiance%0Afields%2C%20making%20it%20difficult%20to%20recover%20fine%20texture%20details.%20In%20contrast%2C%20the%0Alatest%20advancements%20in%203DGS%20have%20achieved%20high-quality%20real-time%20rendering%20by%0Aoptimizing%20the%20point%20cloud%20representation%20into%20Gaussian%20ellipsoids.%20Building%20on%0Athis%2C%20we%20introduce%20SpikeGS%2C%20the%20method%20to%20learn%203D%20Gaussian%20fields%20solely%20from%0Aspike%20stream.%20We%20designed%20a%20differentiable%20spike%20stream%20rendering%20framework%0Abased%20on%203DGS%2C%20incorporating%20noise%20embedding%20and%20spiking%20neurons.%20By%20leveraging%0Athe%20multi-view%20consistency%20of%203DGS%20and%20the%20tile-based%20multi-threaded%20parallel%0Arendering%20mechanism%2C%20we%20achieved%20high-quality%20real-time%20rendering%20results.%0AAdditionally%2C%20we%20introduced%20a%20spike%20rendering%20loss%20function%20that%20generalizes%0Aunder%20varying%20illumination%20conditions.%20Our%20method%20can%20reconstruct%20view%0Asynthesis%20results%20with%20fine%20texture%20details%20from%20a%20continuous%20spike%20stream%0Acaptured%20by%20a%20moving%20spike%20camera%2C%20while%20demonstrating%20high%20robustness%20in%0Aextremely%20noisy%20low-light%20scenarios.%20Experimental%20results%20on%20both%20real%20and%0Asynthetic%20datasets%20demonstrate%20that%20our%20method%20surpasses%20existing%20approaches%20in%0Aterms%20of%20rendering%20quality%20and%20speed.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/520jz/SpikeGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15176v5&entry.124074799=Read"},
{"title": "FlexGen: Flexible Multi-View Generation from Text and Image Inputs", "author": "Xinli Xu and Wenhang Ge and Jiantao Lin and Jiawei Feng and Lie Xu and HanFeng Zhao and Shunsi Zhang and Ying-Cong Chen", "abstract": "  In this work, we introduce FlexGen, a flexible framework designed to generate\ncontrollable and consistent multi-view images, conditioned on a single-view\nimage, or a text prompt, or both. FlexGen tackles the challenges of\ncontrollable multi-view synthesis through additional conditioning on 3D-aware\ntext annotations. We utilize the strong reasoning capabilities of GPT-4V to\ngenerate 3D-aware text annotations. By analyzing four orthogonal views of an\nobject arranged as tiled multi-view images, GPT-4V can produce text annotations\nthat include 3D-aware information with spatial relationship. By integrating the\ncontrol signal with proposed adaptive dual-control module, our model can\ngenerate multi-view images that correspond to the specified text. FlexGen\nsupports multiple controllable capabilities, allowing users to modify text\nprompts to generate reasonable and corresponding unseen parts. Additionally,\nusers can influence attributes such as appearance and material properties,\nincluding metallic and roughness. Extensive experiments demonstrate that our\napproach offers enhanced multiple controllability, marking a significant\nadvancement over existing multi-view diffusion models. This work has\nsubstantial implications for fields requiring rapid and flexible 3D content\ncreation, including game development, animation, and virtual reality. Project\npage: https://xxu068.github.io/flexgen.github.io/.\n", "link": "http://arxiv.org/abs/2410.10745v1", "date": "2024-10-14", "relevancy": 3.1549, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6423}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6263}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexGen%3A%20Flexible%20Multi-View%20Generation%20from%20Text%20and%20Image%20Inputs&body=Title%3A%20FlexGen%3A%20Flexible%20Multi-View%20Generation%20from%20Text%20and%20Image%20Inputs%0AAuthor%3A%20Xinli%20Xu%20and%20Wenhang%20Ge%20and%20Jiantao%20Lin%20and%20Jiawei%20Feng%20and%20Lie%20Xu%20and%20HanFeng%20Zhao%20and%20Shunsi%20Zhang%20and%20Ying-Cong%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20FlexGen%2C%20a%20flexible%20framework%20designed%20to%20generate%0Acontrollable%20and%20consistent%20multi-view%20images%2C%20conditioned%20on%20a%20single-view%0Aimage%2C%20or%20a%20text%20prompt%2C%20or%20both.%20FlexGen%20tackles%20the%20challenges%20of%0Acontrollable%20multi-view%20synthesis%20through%20additional%20conditioning%20on%203D-aware%0Atext%20annotations.%20We%20utilize%20the%20strong%20reasoning%20capabilities%20of%20GPT-4V%20to%0Agenerate%203D-aware%20text%20annotations.%20By%20analyzing%20four%20orthogonal%20views%20of%20an%0Aobject%20arranged%20as%20tiled%20multi-view%20images%2C%20GPT-4V%20can%20produce%20text%20annotations%0Athat%20include%203D-aware%20information%20with%20spatial%20relationship.%20By%20integrating%20the%0Acontrol%20signal%20with%20proposed%20adaptive%20dual-control%20module%2C%20our%20model%20can%0Agenerate%20multi-view%20images%20that%20correspond%20to%20the%20specified%20text.%20FlexGen%0Asupports%20multiple%20controllable%20capabilities%2C%20allowing%20users%20to%20modify%20text%0Aprompts%20to%20generate%20reasonable%20and%20corresponding%20unseen%20parts.%20Additionally%2C%0Ausers%20can%20influence%20attributes%20such%20as%20appearance%20and%20material%20properties%2C%0Aincluding%20metallic%20and%20roughness.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20offers%20enhanced%20multiple%20controllability%2C%20marking%20a%20significant%0Aadvancement%20over%20existing%20multi-view%20diffusion%20models.%20This%20work%20has%0Asubstantial%20implications%20for%20fields%20requiring%20rapid%20and%20flexible%203D%20content%0Acreation%2C%20including%20game%20development%2C%20animation%2C%20and%20virtual%20reality.%20Project%0Apage%3A%20https%3A//xxu068.github.io/flexgen.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexGen%253A%2520Flexible%2520Multi-View%2520Generation%2520from%2520Text%2520and%2520Image%2520Inputs%26entry.906535625%3DXinli%2520Xu%2520and%2520Wenhang%2520Ge%2520and%2520Jiantao%2520Lin%2520and%2520Jiawei%2520Feng%2520and%2520Lie%2520Xu%2520and%2520HanFeng%2520Zhao%2520and%2520Shunsi%2520Zhang%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520FlexGen%252C%2520a%2520flexible%2520framework%2520designed%2520to%2520generate%250Acontrollable%2520and%2520consistent%2520multi-view%2520images%252C%2520conditioned%2520on%2520a%2520single-view%250Aimage%252C%2520or%2520a%2520text%2520prompt%252C%2520or%2520both.%2520FlexGen%2520tackles%2520the%2520challenges%2520of%250Acontrollable%2520multi-view%2520synthesis%2520through%2520additional%2520conditioning%2520on%25203D-aware%250Atext%2520annotations.%2520We%2520utilize%2520the%2520strong%2520reasoning%2520capabilities%2520of%2520GPT-4V%2520to%250Agenerate%25203D-aware%2520text%2520annotations.%2520By%2520analyzing%2520four%2520orthogonal%2520views%2520of%2520an%250Aobject%2520arranged%2520as%2520tiled%2520multi-view%2520images%252C%2520GPT-4V%2520can%2520produce%2520text%2520annotations%250Athat%2520include%25203D-aware%2520information%2520with%2520spatial%2520relationship.%2520By%2520integrating%2520the%250Acontrol%2520signal%2520with%2520proposed%2520adaptive%2520dual-control%2520module%252C%2520our%2520model%2520can%250Agenerate%2520multi-view%2520images%2520that%2520correspond%2520to%2520the%2520specified%2520text.%2520FlexGen%250Asupports%2520multiple%2520controllable%2520capabilities%252C%2520allowing%2520users%2520to%2520modify%2520text%250Aprompts%2520to%2520generate%2520reasonable%2520and%2520corresponding%2520unseen%2520parts.%2520Additionally%252C%250Ausers%2520can%2520influence%2520attributes%2520such%2520as%2520appearance%2520and%2520material%2520properties%252C%250Aincluding%2520metallic%2520and%2520roughness.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520offers%2520enhanced%2520multiple%2520controllability%252C%2520marking%2520a%2520significant%250Aadvancement%2520over%2520existing%2520multi-view%2520diffusion%2520models.%2520This%2520work%2520has%250Asubstantial%2520implications%2520for%2520fields%2520requiring%2520rapid%2520and%2520flexible%25203D%2520content%250Acreation%252C%2520including%2520game%2520development%252C%2520animation%252C%2520and%2520virtual%2520reality.%2520Project%250Apage%253A%2520https%253A//xxu068.github.io/flexgen.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexGen%3A%20Flexible%20Multi-View%20Generation%20from%20Text%20and%20Image%20Inputs&entry.906535625=Xinli%20Xu%20and%20Wenhang%20Ge%20and%20Jiantao%20Lin%20and%20Jiawei%20Feng%20and%20Lie%20Xu%20and%20HanFeng%20Zhao%20and%20Shunsi%20Zhang%20and%20Ying-Cong%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20FlexGen%2C%20a%20flexible%20framework%20designed%20to%20generate%0Acontrollable%20and%20consistent%20multi-view%20images%2C%20conditioned%20on%20a%20single-view%0Aimage%2C%20or%20a%20text%20prompt%2C%20or%20both.%20FlexGen%20tackles%20the%20challenges%20of%0Acontrollable%20multi-view%20synthesis%20through%20additional%20conditioning%20on%203D-aware%0Atext%20annotations.%20We%20utilize%20the%20strong%20reasoning%20capabilities%20of%20GPT-4V%20to%0Agenerate%203D-aware%20text%20annotations.%20By%20analyzing%20four%20orthogonal%20views%20of%20an%0Aobject%20arranged%20as%20tiled%20multi-view%20images%2C%20GPT-4V%20can%20produce%20text%20annotations%0Athat%20include%203D-aware%20information%20with%20spatial%20relationship.%20By%20integrating%20the%0Acontrol%20signal%20with%20proposed%20adaptive%20dual-control%20module%2C%20our%20model%20can%0Agenerate%20multi-view%20images%20that%20correspond%20to%20the%20specified%20text.%20FlexGen%0Asupports%20multiple%20controllable%20capabilities%2C%20allowing%20users%20to%20modify%20text%0Aprompts%20to%20generate%20reasonable%20and%20corresponding%20unseen%20parts.%20Additionally%2C%0Ausers%20can%20influence%20attributes%20such%20as%20appearance%20and%20material%20properties%2C%0Aincluding%20metallic%20and%20roughness.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20offers%20enhanced%20multiple%20controllability%2C%20marking%20a%20significant%0Aadvancement%20over%20existing%20multi-view%20diffusion%20models.%20This%20work%20has%0Asubstantial%20implications%20for%20fields%20requiring%20rapid%20and%20flexible%203D%20content%0Acreation%2C%20including%20game%20development%2C%20animation%2C%20and%20virtual%20reality.%20Project%0Apage%3A%20https%3A//xxu068.github.io/flexgen.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10745v1&entry.124074799=Read"},
{"title": "4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting", "author": "Wanlin Liang and Hongbin Xu and Weitao Chen and Feng Xiao and Wenxiong Kang", "abstract": "  3D neural style transfer has gained significant attention for its potential\nto provide user-friendly stylization with spatial consistency. However,\nexisting 3D style transfer methods often fall short in terms of inference\nefficiency, generalization ability, and struggle to handle dynamic scenes with\ntemporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D\nstyle transfer framework designed to achieve real-time stylization of arbitrary\nstyle references while maintaining reasonable content affinity, multi-view\nconsistency, and temporal coherence. Our approach leverages an embedded 4D\nGaussian Splatting technique, which is trained using a reversible neural\nnetwork for reducing content loss in the feature distillation process.\nUtilizing the 4D embedded Gaussians, we predict a 4D style transformation\nmatrix that facilitates spatially and temporally consistent style transfer with\nGaussian Splatting. Experiments demonstrate that our method can achieve\nhigh-quality and zero-shot stylization for 4D scenarios with enhanced\nefficiency and spatial-temporal consistency.\n", "link": "http://arxiv.org/abs/2410.10412v1", "date": "2024-10-14", "relevancy": 3.1083, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.65}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6119}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DStyleGaussian%3A%20Zero-shot%204D%20Style%20Transfer%20with%20Gaussian%20Splatting&body=Title%3A%204DStyleGaussian%3A%20Zero-shot%204D%20Style%20Transfer%20with%20Gaussian%20Splatting%0AAuthor%3A%20Wanlin%20Liang%20and%20Hongbin%20Xu%20and%20Weitao%20Chen%20and%20Feng%20Xiao%20and%20Wenxiong%20Kang%0AAbstract%3A%20%20%203D%20neural%20style%20transfer%20has%20gained%20significant%20attention%20for%20its%20potential%0Ato%20provide%20user-friendly%20stylization%20with%20spatial%20consistency.%20However%2C%0Aexisting%203D%20style%20transfer%20methods%20often%20fall%20short%20in%20terms%20of%20inference%0Aefficiency%2C%20generalization%20ability%2C%20and%20struggle%20to%20handle%20dynamic%20scenes%20with%0Atemporal%20consistency.%20In%20this%20paper%2C%20we%20introduce%204DStyleGaussian%2C%20a%20novel%204D%0Astyle%20transfer%20framework%20designed%20to%20achieve%20real-time%20stylization%20of%20arbitrary%0Astyle%20references%20while%20maintaining%20reasonable%20content%20affinity%2C%20multi-view%0Aconsistency%2C%20and%20temporal%20coherence.%20Our%20approach%20leverages%20an%20embedded%204D%0AGaussian%20Splatting%20technique%2C%20which%20is%20trained%20using%20a%20reversible%20neural%0Anetwork%20for%20reducing%20content%20loss%20in%20the%20feature%20distillation%20process.%0AUtilizing%20the%204D%20embedded%20Gaussians%2C%20we%20predict%20a%204D%20style%20transformation%0Amatrix%20that%20facilitates%20spatially%20and%20temporally%20consistent%20style%20transfer%20with%0AGaussian%20Splatting.%20Experiments%20demonstrate%20that%20our%20method%20can%20achieve%0Ahigh-quality%20and%20zero-shot%20stylization%20for%204D%20scenarios%20with%20enhanced%0Aefficiency%20and%20spatial-temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DStyleGaussian%253A%2520Zero-shot%25204D%2520Style%2520Transfer%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DWanlin%2520Liang%2520and%2520Hongbin%2520Xu%2520and%2520Weitao%2520Chen%2520and%2520Feng%2520Xiao%2520and%2520Wenxiong%2520Kang%26entry.1292438233%3D%2520%25203D%2520neural%2520style%2520transfer%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520potential%250Ato%2520provide%2520user-friendly%2520stylization%2520with%2520spatial%2520consistency.%2520However%252C%250Aexisting%25203D%2520style%2520transfer%2520methods%2520often%2520fall%2520short%2520in%2520terms%2520of%2520inference%250Aefficiency%252C%2520generalization%2520ability%252C%2520and%2520struggle%2520to%2520handle%2520dynamic%2520scenes%2520with%250Atemporal%2520consistency.%2520In%2520this%2520paper%252C%2520we%2520introduce%25204DStyleGaussian%252C%2520a%2520novel%25204D%250Astyle%2520transfer%2520framework%2520designed%2520to%2520achieve%2520real-time%2520stylization%2520of%2520arbitrary%250Astyle%2520references%2520while%2520maintaining%2520reasonable%2520content%2520affinity%252C%2520multi-view%250Aconsistency%252C%2520and%2520temporal%2520coherence.%2520Our%2520approach%2520leverages%2520an%2520embedded%25204D%250AGaussian%2520Splatting%2520technique%252C%2520which%2520is%2520trained%2520using%2520a%2520reversible%2520neural%250Anetwork%2520for%2520reducing%2520content%2520loss%2520in%2520the%2520feature%2520distillation%2520process.%250AUtilizing%2520the%25204D%2520embedded%2520Gaussians%252C%2520we%2520predict%2520a%25204D%2520style%2520transformation%250Amatrix%2520that%2520facilitates%2520spatially%2520and%2520temporally%2520consistent%2520style%2520transfer%2520with%250AGaussian%2520Splatting.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%250Ahigh-quality%2520and%2520zero-shot%2520stylization%2520for%25204D%2520scenarios%2520with%2520enhanced%250Aefficiency%2520and%2520spatial-temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DStyleGaussian%3A%20Zero-shot%204D%20Style%20Transfer%20with%20Gaussian%20Splatting&entry.906535625=Wanlin%20Liang%20and%20Hongbin%20Xu%20and%20Weitao%20Chen%20and%20Feng%20Xiao%20and%20Wenxiong%20Kang&entry.1292438233=%20%203D%20neural%20style%20transfer%20has%20gained%20significant%20attention%20for%20its%20potential%0Ato%20provide%20user-friendly%20stylization%20with%20spatial%20consistency.%20However%2C%0Aexisting%203D%20style%20transfer%20methods%20often%20fall%20short%20in%20terms%20of%20inference%0Aefficiency%2C%20generalization%20ability%2C%20and%20struggle%20to%20handle%20dynamic%20scenes%20with%0Atemporal%20consistency.%20In%20this%20paper%2C%20we%20introduce%204DStyleGaussian%2C%20a%20novel%204D%0Astyle%20transfer%20framework%20designed%20to%20achieve%20real-time%20stylization%20of%20arbitrary%0Astyle%20references%20while%20maintaining%20reasonable%20content%20affinity%2C%20multi-view%0Aconsistency%2C%20and%20temporal%20coherence.%20Our%20approach%20leverages%20an%20embedded%204D%0AGaussian%20Splatting%20technique%2C%20which%20is%20trained%20using%20a%20reversible%20neural%0Anetwork%20for%20reducing%20content%20loss%20in%20the%20feature%20distillation%20process.%0AUtilizing%20the%204D%20embedded%20Gaussians%2C%20we%20predict%20a%204D%20style%20transformation%0Amatrix%20that%20facilitates%20spatially%20and%20temporally%20consistent%20style%20transfer%20with%0AGaussian%20Splatting.%20Experiments%20demonstrate%20that%20our%20method%20can%20achieve%0Ahigh-quality%20and%20zero-shot%20stylization%20for%204D%20scenarios%20with%20enhanced%0Aefficiency%20and%20spatial-temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10412v1&entry.124074799=Read"},
{"title": "Words to Wheels: Vision-Based Autonomous Driving Understanding Human\n  Language Instructions Using Foundation Models", "author": "Chanhoe Ryu and Hyunki Seong and Daegyu Lee and Seongwoo Moon and Sungjae Min and D. Hyunchul Shim", "abstract": "  This paper introduces an innovative application of foundation models,\nenabling Unmanned Ground Vehicles (UGVs) equipped with an RGB-D camera to\nnavigate to designated destinations based on human language instructions.\nUnlike learning-based methods, this approach does not require prior training\nbut instead leverages existing foundation models, thus facilitating\ngeneralization to novel environments. Upon receiving human language\ninstructions, these are transformed into a 'cognitive route description' using\na large language model (LLM)-a detailed navigation route expressed in human\nlanguage. The vehicle then decomposes this description into landmarks and\nnavigation maneuvers. The vehicle also determines elevation costs and\nidentifies navigability levels of different regions through a terrain\nsegmentation model, GANav, trained on open datasets. Semantic elevation costs,\nwhich take both elevation and navigability levels into account, are estimated\nand provided to the Model Predictive Path Integral (MPPI) planner, responsible\nfor local path planning. Concurrently, the vehicle searches for target\nlandmarks using foundation models, including YOLO-World and EfficientViT-SAM.\nUltimately, the vehicle executes the navigation commands to reach the\ndesignated destination, the final landmark. Our experiments demonstrate that\nthis application successfully guides UGVs to their destinations following human\nlanguage instructions in novel environments, such as unfamiliar terrain or\nurban settings.\n", "link": "http://arxiv.org/abs/2410.10577v1", "date": "2024-10-14", "relevancy": 3.0775, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Words%20to%20Wheels%3A%20Vision-Based%20Autonomous%20Driving%20Understanding%20Human%0A%20%20Language%20Instructions%20Using%20Foundation%20Models&body=Title%3A%20Words%20to%20Wheels%3A%20Vision-Based%20Autonomous%20Driving%20Understanding%20Human%0A%20%20Language%20Instructions%20Using%20Foundation%20Models%0AAuthor%3A%20Chanhoe%20Ryu%20and%20Hyunki%20Seong%20and%20Daegyu%20Lee%20and%20Seongwoo%20Moon%20and%20Sungjae%20Min%20and%20D.%20Hyunchul%20Shim%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20application%20of%20foundation%20models%2C%0Aenabling%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20equipped%20with%20an%20RGB-D%20camera%20to%0Anavigate%20to%20designated%20destinations%20based%20on%20human%20language%20instructions.%0AUnlike%20learning-based%20methods%2C%20this%20approach%20does%20not%20require%20prior%20training%0Abut%20instead%20leverages%20existing%20foundation%20models%2C%20thus%20facilitating%0Ageneralization%20to%20novel%20environments.%20Upon%20receiving%20human%20language%0Ainstructions%2C%20these%20are%20transformed%20into%20a%20%27cognitive%20route%20description%27%20using%0Aa%20large%20language%20model%20%28LLM%29-a%20detailed%20navigation%20route%20expressed%20in%20human%0Alanguage.%20The%20vehicle%20then%20decomposes%20this%20description%20into%20landmarks%20and%0Anavigation%20maneuvers.%20The%20vehicle%20also%20determines%20elevation%20costs%20and%0Aidentifies%20navigability%20levels%20of%20different%20regions%20through%20a%20terrain%0Asegmentation%20model%2C%20GANav%2C%20trained%20on%20open%20datasets.%20Semantic%20elevation%20costs%2C%0Awhich%20take%20both%20elevation%20and%20navigability%20levels%20into%20account%2C%20are%20estimated%0Aand%20provided%20to%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20planner%2C%20responsible%0Afor%20local%20path%20planning.%20Concurrently%2C%20the%20vehicle%20searches%20for%20target%0Alandmarks%20using%20foundation%20models%2C%20including%20YOLO-World%20and%20EfficientViT-SAM.%0AUltimately%2C%20the%20vehicle%20executes%20the%20navigation%20commands%20to%20reach%20the%0Adesignated%20destination%2C%20the%20final%20landmark.%20Our%20experiments%20demonstrate%20that%0Athis%20application%20successfully%20guides%20UGVs%20to%20their%20destinations%20following%20human%0Alanguage%20instructions%20in%20novel%20environments%2C%20such%20as%20unfamiliar%20terrain%20or%0Aurban%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWords%2520to%2520Wheels%253A%2520Vision-Based%2520Autonomous%2520Driving%2520Understanding%2520Human%250A%2520%2520Language%2520Instructions%2520Using%2520Foundation%2520Models%26entry.906535625%3DChanhoe%2520Ryu%2520and%2520Hyunki%2520Seong%2520and%2520Daegyu%2520Lee%2520and%2520Seongwoo%2520Moon%2520and%2520Sungjae%2520Min%2520and%2520D.%2520Hyunchul%2520Shim%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520application%2520of%2520foundation%2520models%252C%250Aenabling%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520equipped%2520with%2520an%2520RGB-D%2520camera%2520to%250Anavigate%2520to%2520designated%2520destinations%2520based%2520on%2520human%2520language%2520instructions.%250AUnlike%2520learning-based%2520methods%252C%2520this%2520approach%2520does%2520not%2520require%2520prior%2520training%250Abut%2520instead%2520leverages%2520existing%2520foundation%2520models%252C%2520thus%2520facilitating%250Ageneralization%2520to%2520novel%2520environments.%2520Upon%2520receiving%2520human%2520language%250Ainstructions%252C%2520these%2520are%2520transformed%2520into%2520a%2520%2527cognitive%2520route%2520description%2527%2520using%250Aa%2520large%2520language%2520model%2520%2528LLM%2529-a%2520detailed%2520navigation%2520route%2520expressed%2520in%2520human%250Alanguage.%2520The%2520vehicle%2520then%2520decomposes%2520this%2520description%2520into%2520landmarks%2520and%250Anavigation%2520maneuvers.%2520The%2520vehicle%2520also%2520determines%2520elevation%2520costs%2520and%250Aidentifies%2520navigability%2520levels%2520of%2520different%2520regions%2520through%2520a%2520terrain%250Asegmentation%2520model%252C%2520GANav%252C%2520trained%2520on%2520open%2520datasets.%2520Semantic%2520elevation%2520costs%252C%250Awhich%2520take%2520both%2520elevation%2520and%2520navigability%2520levels%2520into%2520account%252C%2520are%2520estimated%250Aand%2520provided%2520to%2520the%2520Model%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520planner%252C%2520responsible%250Afor%2520local%2520path%2520planning.%2520Concurrently%252C%2520the%2520vehicle%2520searches%2520for%2520target%250Alandmarks%2520using%2520foundation%2520models%252C%2520including%2520YOLO-World%2520and%2520EfficientViT-SAM.%250AUltimately%252C%2520the%2520vehicle%2520executes%2520the%2520navigation%2520commands%2520to%2520reach%2520the%250Adesignated%2520destination%252C%2520the%2520final%2520landmark.%2520Our%2520experiments%2520demonstrate%2520that%250Athis%2520application%2520successfully%2520guides%2520UGVs%2520to%2520their%2520destinations%2520following%2520human%250Alanguage%2520instructions%2520in%2520novel%2520environments%252C%2520such%2520as%2520unfamiliar%2520terrain%2520or%250Aurban%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Words%20to%20Wheels%3A%20Vision-Based%20Autonomous%20Driving%20Understanding%20Human%0A%20%20Language%20Instructions%20Using%20Foundation%20Models&entry.906535625=Chanhoe%20Ryu%20and%20Hyunki%20Seong%20and%20Daegyu%20Lee%20and%20Seongwoo%20Moon%20and%20Sungjae%20Min%20and%20D.%20Hyunchul%20Shim&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20application%20of%20foundation%20models%2C%0Aenabling%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20equipped%20with%20an%20RGB-D%20camera%20to%0Anavigate%20to%20designated%20destinations%20based%20on%20human%20language%20instructions.%0AUnlike%20learning-based%20methods%2C%20this%20approach%20does%20not%20require%20prior%20training%0Abut%20instead%20leverages%20existing%20foundation%20models%2C%20thus%20facilitating%0Ageneralization%20to%20novel%20environments.%20Upon%20receiving%20human%20language%0Ainstructions%2C%20these%20are%20transformed%20into%20a%20%27cognitive%20route%20description%27%20using%0Aa%20large%20language%20model%20%28LLM%29-a%20detailed%20navigation%20route%20expressed%20in%20human%0Alanguage.%20The%20vehicle%20then%20decomposes%20this%20description%20into%20landmarks%20and%0Anavigation%20maneuvers.%20The%20vehicle%20also%20determines%20elevation%20costs%20and%0Aidentifies%20navigability%20levels%20of%20different%20regions%20through%20a%20terrain%0Asegmentation%20model%2C%20GANav%2C%20trained%20on%20open%20datasets.%20Semantic%20elevation%20costs%2C%0Awhich%20take%20both%20elevation%20and%20navigability%20levels%20into%20account%2C%20are%20estimated%0Aand%20provided%20to%20the%20Model%20Predictive%20Path%20Integral%20%28MPPI%29%20planner%2C%20responsible%0Afor%20local%20path%20planning.%20Concurrently%2C%20the%20vehicle%20searches%20for%20target%0Alandmarks%20using%20foundation%20models%2C%20including%20YOLO-World%20and%20EfficientViT-SAM.%0AUltimately%2C%20the%20vehicle%20executes%20the%20navigation%20commands%20to%20reach%20the%0Adesignated%20destination%2C%20the%20final%20landmark.%20Our%20experiments%20demonstrate%20that%0Athis%20application%20successfully%20guides%20UGVs%20to%20their%20destinations%20following%20human%0Alanguage%20instructions%20in%20novel%20environments%2C%20such%20as%20unfamiliar%20terrain%20or%0Aurban%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10577v1&entry.124074799=Read"},
{"title": "Free Video-LLM: Prompt-guided Visual Perception for Efficient\n  Training-free Video LLMs", "author": "Kai Han and Jianyuan Guo and Yehui Tang and Wei He and Enhua Wu and Yunhe Wang", "abstract": "  Vision-language large models have achieved remarkable success in various\nmulti-modal tasks, yet applying them to video understanding remains challenging\ndue to the inherent complexity and computational demands of video data. While\ntraining-based video-LLMs deliver high performance, they often require\nsubstantial resources for training and inference. Conversely, training-free\napproaches offer a more efficient alternative by adapting pre-trained\nimage-LLMs models for video tasks without additional training, but they face\ninference efficiency bottlenecks due to the large number of visual tokens\ngenerated from video frames. In this work, we present a novel prompt-guided\nvisual perception framework (abbreviated as \\emph{Free Video-LLM}) for\nefficient inference of training-free video LLMs. The proposed framework\ndecouples spatial-temporal dimension and performs temporal frame sampling and\nspatial RoI cropping respectively based on task-specific prompts. Our method\neffectively reduces the number of visual tokens while maintaining high\nperformance across multiple video question-answering benchmarks. Extensive\nexperiments demonstrate that our approach achieves competitive results with\nsignificantly fewer tokens, offering an optimal trade-off between accuracy and\ncomputational efficiency compared to state-of-the-art video LLMs. The code will\nbe available at \\url{https://github.com/contrastive/FreeVideoLLM}.\n", "link": "http://arxiv.org/abs/2410.10441v1", "date": "2024-10-14", "relevancy": 3.0311, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6228}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20Video-LLM%3A%20Prompt-guided%20Visual%20Perception%20for%20Efficient%0A%20%20Training-free%20Video%20LLMs&body=Title%3A%20Free%20Video-LLM%3A%20Prompt-guided%20Visual%20Perception%20for%20Efficient%0A%20%20Training-free%20Video%20LLMs%0AAuthor%3A%20Kai%20Han%20and%20Jianyuan%20Guo%20and%20Yehui%20Tang%20and%20Wei%20He%20and%20Enhua%20Wu%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Vision-language%20large%20models%20have%20achieved%20remarkable%20success%20in%20various%0Amulti-modal%20tasks%2C%20yet%20applying%20them%20to%20video%20understanding%20remains%20challenging%0Adue%20to%20the%20inherent%20complexity%20and%20computational%20demands%20of%20video%20data.%20While%0Atraining-based%20video-LLMs%20deliver%20high%20performance%2C%20they%20often%20require%0Asubstantial%20resources%20for%20training%20and%20inference.%20Conversely%2C%20training-free%0Aapproaches%20offer%20a%20more%20efficient%20alternative%20by%20adapting%20pre-trained%0Aimage-LLMs%20models%20for%20video%20tasks%20without%20additional%20training%2C%20but%20they%20face%0Ainference%20efficiency%20bottlenecks%20due%20to%20the%20large%20number%20of%20visual%20tokens%0Agenerated%20from%20video%20frames.%20In%20this%20work%2C%20we%20present%20a%20novel%20prompt-guided%0Avisual%20perception%20framework%20%28abbreviated%20as%20%5Cemph%7BFree%20Video-LLM%7D%29%20for%0Aefficient%20inference%20of%20training-free%20video%20LLMs.%20The%20proposed%20framework%0Adecouples%20spatial-temporal%20dimension%20and%20performs%20temporal%20frame%20sampling%20and%0Aspatial%20RoI%20cropping%20respectively%20based%20on%20task-specific%20prompts.%20Our%20method%0Aeffectively%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20high%0Aperformance%20across%20multiple%20video%20question-answering%20benchmarks.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%20with%0Asignificantly%20fewer%20tokens%2C%20offering%20an%20optimal%20trade-off%20between%20accuracy%20and%0Acomputational%20efficiency%20compared%20to%20state-of-the-art%20video%20LLMs.%20The%20code%20will%0Abe%20available%20at%20%5Curl%7Bhttps%3A//github.com/contrastive/FreeVideoLLM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520Video-LLM%253A%2520Prompt-guided%2520Visual%2520Perception%2520for%2520Efficient%250A%2520%2520Training-free%2520Video%2520LLMs%26entry.906535625%3DKai%2520Han%2520and%2520Jianyuan%2520Guo%2520and%2520Yehui%2520Tang%2520and%2520Wei%2520He%2520and%2520Enhua%2520Wu%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Vision-language%2520large%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520various%250Amulti-modal%2520tasks%252C%2520yet%2520applying%2520them%2520to%2520video%2520understanding%2520remains%2520challenging%250Adue%2520to%2520the%2520inherent%2520complexity%2520and%2520computational%2520demands%2520of%2520video%2520data.%2520While%250Atraining-based%2520video-LLMs%2520deliver%2520high%2520performance%252C%2520they%2520often%2520require%250Asubstantial%2520resources%2520for%2520training%2520and%2520inference.%2520Conversely%252C%2520training-free%250Aapproaches%2520offer%2520a%2520more%2520efficient%2520alternative%2520by%2520adapting%2520pre-trained%250Aimage-LLMs%2520models%2520for%2520video%2520tasks%2520without%2520additional%2520training%252C%2520but%2520they%2520face%250Ainference%2520efficiency%2520bottlenecks%2520due%2520to%2520the%2520large%2520number%2520of%2520visual%2520tokens%250Agenerated%2520from%2520video%2520frames.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520prompt-guided%250Avisual%2520perception%2520framework%2520%2528abbreviated%2520as%2520%255Cemph%257BFree%2520Video-LLM%257D%2529%2520for%250Aefficient%2520inference%2520of%2520training-free%2520video%2520LLMs.%2520The%2520proposed%2520framework%250Adecouples%2520spatial-temporal%2520dimension%2520and%2520performs%2520temporal%2520frame%2520sampling%2520and%250Aspatial%2520RoI%2520cropping%2520respectively%2520based%2520on%2520task-specific%2520prompts.%2520Our%2520method%250Aeffectively%2520reduces%2520the%2520number%2520of%2520visual%2520tokens%2520while%2520maintaining%2520high%250Aperformance%2520across%2520multiple%2520video%2520question-answering%2520benchmarks.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520results%2520with%250Asignificantly%2520fewer%2520tokens%252C%2520offering%2520an%2520optimal%2520trade-off%2520between%2520accuracy%2520and%250Acomputational%2520efficiency%2520compared%2520to%2520state-of-the-art%2520video%2520LLMs.%2520The%2520code%2520will%250Abe%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/contrastive/FreeVideoLLM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20Video-LLM%3A%20Prompt-guided%20Visual%20Perception%20for%20Efficient%0A%20%20Training-free%20Video%20LLMs&entry.906535625=Kai%20Han%20and%20Jianyuan%20Guo%20and%20Yehui%20Tang%20and%20Wei%20He%20and%20Enhua%20Wu%20and%20Yunhe%20Wang&entry.1292438233=%20%20Vision-language%20large%20models%20have%20achieved%20remarkable%20success%20in%20various%0Amulti-modal%20tasks%2C%20yet%20applying%20them%20to%20video%20understanding%20remains%20challenging%0Adue%20to%20the%20inherent%20complexity%20and%20computational%20demands%20of%20video%20data.%20While%0Atraining-based%20video-LLMs%20deliver%20high%20performance%2C%20they%20often%20require%0Asubstantial%20resources%20for%20training%20and%20inference.%20Conversely%2C%20training-free%0Aapproaches%20offer%20a%20more%20efficient%20alternative%20by%20adapting%20pre-trained%0Aimage-LLMs%20models%20for%20video%20tasks%20without%20additional%20training%2C%20but%20they%20face%0Ainference%20efficiency%20bottlenecks%20due%20to%20the%20large%20number%20of%20visual%20tokens%0Agenerated%20from%20video%20frames.%20In%20this%20work%2C%20we%20present%20a%20novel%20prompt-guided%0Avisual%20perception%20framework%20%28abbreviated%20as%20%5Cemph%7BFree%20Video-LLM%7D%29%20for%0Aefficient%20inference%20of%20training-free%20video%20LLMs.%20The%20proposed%20framework%0Adecouples%20spatial-temporal%20dimension%20and%20performs%20temporal%20frame%20sampling%20and%0Aspatial%20RoI%20cropping%20respectively%20based%20on%20task-specific%20prompts.%20Our%20method%0Aeffectively%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20high%0Aperformance%20across%20multiple%20video%20question-answering%20benchmarks.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%20with%0Asignificantly%20fewer%20tokens%2C%20offering%20an%20optimal%20trade-off%20between%20accuracy%20and%0Acomputational%20efficiency%20compared%20to%20state-of-the-art%20video%20LLMs.%20The%20code%20will%0Abe%20available%20at%20%5Curl%7Bhttps%3A//github.com/contrastive/FreeVideoLLM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10441v1&entry.124074799=Read"},
{"title": "3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for\n  Human-Object Interaction (HOI) and Autonomous Driving Applications", "author": "Eduardo R. Corral-Soto and Yang Liu and Tongtong Cao and Yuan Ren and Liu Bingbing", "abstract": "  Human-object interaction (HOI) and human-scene interaction (HSI) are crucial\nfor human-centric scene understanding applications in Embodied Artificial\nIntelligence (EAI), robotics, and augmented reality (AR). A common limitation\nfaced in these research areas is the data scarcity problem: insufficient\nlabeled human-scene object pairs on the input images, and limited interaction\ncomplexity and granularity between them. Recent HOI and HSI methods have\naddressed this issue by generating dynamic interactions with rigid objects. But\nmore complex dynamic interactions such as a human rider pedaling an articulated\nbicycle have been unexplored. To address this limitation, and to enable\nresearch on complex dynamic human-articulated object interactions, in this\npaper we propose a method to generate simulated 3D dynamic cyclist assets and\ninteractions. We designed a methodology for creating a new part-based\nmulti-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes\nthat can be used to train NeRF and 3DGS-based 3D reconstruction methods. We\nthen propose a 3DGS-based parametric bicycle composition model to assemble\n8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from\ncyclist videos, we build a complete synthetic dynamic 3D cyclist (rider\npedaling a bicycle) by re-posing a selectable synthetic 3D person while\nautomatically placing the rider onto one of our new articulated 3D bicycles\nusing a proposed 3D Keypoint optimization-based Inverse Kinematics pose\nrefinement. We present both, qualitative and quantitative results where we\ncompare our generated cyclists against those from a recent stable\ndiffusion-based method.\n", "link": "http://arxiv.org/abs/2410.10782v1", "date": "2024-10-14", "relevancy": 2.9803, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5967}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5967}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DArticCyclists%3A%20Generating%20Simulated%20Dynamic%203D%20Cyclists%20for%0A%20%20Human-Object%20Interaction%20%28HOI%29%20and%20Autonomous%20Driving%20Applications&body=Title%3A%203DArticCyclists%3A%20Generating%20Simulated%20Dynamic%203D%20Cyclists%20for%0A%20%20Human-Object%20Interaction%20%28HOI%29%20and%20Autonomous%20Driving%20Applications%0AAuthor%3A%20Eduardo%20R.%20Corral-Soto%20and%20Yang%20Liu%20and%20Tongtong%20Cao%20and%20Yuan%20Ren%20and%20Liu%20Bingbing%0AAbstract%3A%20%20%20Human-object%20interaction%20%28HOI%29%20and%20human-scene%20interaction%20%28HSI%29%20are%20crucial%0Afor%20human-centric%20scene%20understanding%20applications%20in%20Embodied%20Artificial%0AIntelligence%20%28EAI%29%2C%20robotics%2C%20and%20augmented%20reality%20%28AR%29.%20A%20common%20limitation%0Afaced%20in%20these%20research%20areas%20is%20the%20data%20scarcity%20problem%3A%20insufficient%0Alabeled%20human-scene%20object%20pairs%20on%20the%20input%20images%2C%20and%20limited%20interaction%0Acomplexity%20and%20granularity%20between%20them.%20Recent%20HOI%20and%20HSI%20methods%20have%0Aaddressed%20this%20issue%20by%20generating%20dynamic%20interactions%20with%20rigid%20objects.%20But%0Amore%20complex%20dynamic%20interactions%20such%20as%20a%20human%20rider%20pedaling%20an%20articulated%0Abicycle%20have%20been%20unexplored.%20To%20address%20this%20limitation%2C%20and%20to%20enable%0Aresearch%20on%20complex%20dynamic%20human-articulated%20object%20interactions%2C%20in%20this%0Apaper%20we%20propose%20a%20method%20to%20generate%20simulated%203D%20dynamic%20cyclist%20assets%20and%0Ainteractions.%20We%20designed%20a%20methodology%20for%20creating%20a%20new%20part-based%0Amulti-view%20articulated%20synthetic%203D%20bicycle%20dataset%20that%20we%20call%203DArticBikes%0Athat%20can%20be%20used%20to%20train%20NeRF%20and%203DGS-based%203D%20reconstruction%20methods.%20We%0Athen%20propose%20a%203DGS-based%20parametric%20bicycle%20composition%20model%20to%20assemble%0A8-DoF%20pose-controllable%203D%20bicycles.%20Finally%2C%20using%20dynamic%20information%20from%0Acyclist%20videos%2C%20we%20build%20a%20complete%20synthetic%20dynamic%203D%20cyclist%20%28rider%0Apedaling%20a%20bicycle%29%20by%20re-posing%20a%20selectable%20synthetic%203D%20person%20while%0Aautomatically%20placing%20the%20rider%20onto%20one%20of%20our%20new%20articulated%203D%20bicycles%0Ausing%20a%20proposed%203D%20Keypoint%20optimization-based%20Inverse%20Kinematics%20pose%0Arefinement.%20We%20present%20both%2C%20qualitative%20and%20quantitative%20results%20where%20we%0Acompare%20our%20generated%20cyclists%20against%20those%20from%20a%20recent%20stable%0Adiffusion-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DArticCyclists%253A%2520Generating%2520Simulated%2520Dynamic%25203D%2520Cyclists%2520for%250A%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520and%2520Autonomous%2520Driving%2520Applications%26entry.906535625%3DEduardo%2520R.%2520Corral-Soto%2520and%2520Yang%2520Liu%2520and%2520Tongtong%2520Cao%2520and%2520Yuan%2520Ren%2520and%2520Liu%2520Bingbing%26entry.1292438233%3D%2520%2520Human-object%2520interaction%2520%2528HOI%2529%2520and%2520human-scene%2520interaction%2520%2528HSI%2529%2520are%2520crucial%250Afor%2520human-centric%2520scene%2520understanding%2520applications%2520in%2520Embodied%2520Artificial%250AIntelligence%2520%2528EAI%2529%252C%2520robotics%252C%2520and%2520augmented%2520reality%2520%2528AR%2529.%2520A%2520common%2520limitation%250Afaced%2520in%2520these%2520research%2520areas%2520is%2520the%2520data%2520scarcity%2520problem%253A%2520insufficient%250Alabeled%2520human-scene%2520object%2520pairs%2520on%2520the%2520input%2520images%252C%2520and%2520limited%2520interaction%250Acomplexity%2520and%2520granularity%2520between%2520them.%2520Recent%2520HOI%2520and%2520HSI%2520methods%2520have%250Aaddressed%2520this%2520issue%2520by%2520generating%2520dynamic%2520interactions%2520with%2520rigid%2520objects.%2520But%250Amore%2520complex%2520dynamic%2520interactions%2520such%2520as%2520a%2520human%2520rider%2520pedaling%2520an%2520articulated%250Abicycle%2520have%2520been%2520unexplored.%2520To%2520address%2520this%2520limitation%252C%2520and%2520to%2520enable%250Aresearch%2520on%2520complex%2520dynamic%2520human-articulated%2520object%2520interactions%252C%2520in%2520this%250Apaper%2520we%2520propose%2520a%2520method%2520to%2520generate%2520simulated%25203D%2520dynamic%2520cyclist%2520assets%2520and%250Ainteractions.%2520We%2520designed%2520a%2520methodology%2520for%2520creating%2520a%2520new%2520part-based%250Amulti-view%2520articulated%2520synthetic%25203D%2520bicycle%2520dataset%2520that%2520we%2520call%25203DArticBikes%250Athat%2520can%2520be%2520used%2520to%2520train%2520NeRF%2520and%25203DGS-based%25203D%2520reconstruction%2520methods.%2520We%250Athen%2520propose%2520a%25203DGS-based%2520parametric%2520bicycle%2520composition%2520model%2520to%2520assemble%250A8-DoF%2520pose-controllable%25203D%2520bicycles.%2520Finally%252C%2520using%2520dynamic%2520information%2520from%250Acyclist%2520videos%252C%2520we%2520build%2520a%2520complete%2520synthetic%2520dynamic%25203D%2520cyclist%2520%2528rider%250Apedaling%2520a%2520bicycle%2529%2520by%2520re-posing%2520a%2520selectable%2520synthetic%25203D%2520person%2520while%250Aautomatically%2520placing%2520the%2520rider%2520onto%2520one%2520of%2520our%2520new%2520articulated%25203D%2520bicycles%250Ausing%2520a%2520proposed%25203D%2520Keypoint%2520optimization-based%2520Inverse%2520Kinematics%2520pose%250Arefinement.%2520We%2520present%2520both%252C%2520qualitative%2520and%2520quantitative%2520results%2520where%2520we%250Acompare%2520our%2520generated%2520cyclists%2520against%2520those%2520from%2520a%2520recent%2520stable%250Adiffusion-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DArticCyclists%3A%20Generating%20Simulated%20Dynamic%203D%20Cyclists%20for%0A%20%20Human-Object%20Interaction%20%28HOI%29%20and%20Autonomous%20Driving%20Applications&entry.906535625=Eduardo%20R.%20Corral-Soto%20and%20Yang%20Liu%20and%20Tongtong%20Cao%20and%20Yuan%20Ren%20and%20Liu%20Bingbing&entry.1292438233=%20%20Human-object%20interaction%20%28HOI%29%20and%20human-scene%20interaction%20%28HSI%29%20are%20crucial%0Afor%20human-centric%20scene%20understanding%20applications%20in%20Embodied%20Artificial%0AIntelligence%20%28EAI%29%2C%20robotics%2C%20and%20augmented%20reality%20%28AR%29.%20A%20common%20limitation%0Afaced%20in%20these%20research%20areas%20is%20the%20data%20scarcity%20problem%3A%20insufficient%0Alabeled%20human-scene%20object%20pairs%20on%20the%20input%20images%2C%20and%20limited%20interaction%0Acomplexity%20and%20granularity%20between%20them.%20Recent%20HOI%20and%20HSI%20methods%20have%0Aaddressed%20this%20issue%20by%20generating%20dynamic%20interactions%20with%20rigid%20objects.%20But%0Amore%20complex%20dynamic%20interactions%20such%20as%20a%20human%20rider%20pedaling%20an%20articulated%0Abicycle%20have%20been%20unexplored.%20To%20address%20this%20limitation%2C%20and%20to%20enable%0Aresearch%20on%20complex%20dynamic%20human-articulated%20object%20interactions%2C%20in%20this%0Apaper%20we%20propose%20a%20method%20to%20generate%20simulated%203D%20dynamic%20cyclist%20assets%20and%0Ainteractions.%20We%20designed%20a%20methodology%20for%20creating%20a%20new%20part-based%0Amulti-view%20articulated%20synthetic%203D%20bicycle%20dataset%20that%20we%20call%203DArticBikes%0Athat%20can%20be%20used%20to%20train%20NeRF%20and%203DGS-based%203D%20reconstruction%20methods.%20We%0Athen%20propose%20a%203DGS-based%20parametric%20bicycle%20composition%20model%20to%20assemble%0A8-DoF%20pose-controllable%203D%20bicycles.%20Finally%2C%20using%20dynamic%20information%20from%0Acyclist%20videos%2C%20we%20build%20a%20complete%20synthetic%20dynamic%203D%20cyclist%20%28rider%0Apedaling%20a%20bicycle%29%20by%20re-posing%20a%20selectable%20synthetic%203D%20person%20while%0Aautomatically%20placing%20the%20rider%20onto%20one%20of%20our%20new%20articulated%203D%20bicycles%0Ausing%20a%20proposed%203D%20Keypoint%20optimization-based%20Inverse%20Kinematics%20pose%0Arefinement.%20We%20present%20both%2C%20qualitative%20and%20quantitative%20results%20where%20we%0Acompare%20our%20generated%20cyclists%20against%20those%20from%20a%20recent%20stable%0Adiffusion-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10782v1&entry.124074799=Read"},
{"title": "PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion", "author": "Runsong Zhu and Shi Qiu and Qianyi Wu and Ka-Hei Hui and Pheng-Ann Heng and Chi-Wing Fu", "abstract": "  Panoptic lifting is an effective technique to address the 3D panoptic\nsegmentation task by unprojecting 2D panoptic segmentations from multi-views to\n3D scene. However, the quality of its results largely depends on the 2D\nsegmentations, which could be noisy and error-prone, so its performance often\ndrops significantly for complex scenes. In this work, we design a new pipeline\ncoined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn\nand embed probabilistic features throughout our pipeline to actively consider\ninaccurate segmentations and inconsistent instance IDs. Technical-wise, we\nfirst model the probabilistic feature embeddings through multivariate Gaussian\ndistributions. To fuse the probabilistic features, we incorporate the\nprobability product kernel into the contrastive loss formulation and design a\ncross-view constraint to enhance the feature consistency across different\nviews. For the inference, we introduce a new probabilistic clustering method to\neffectively associate prototype features with the underlying 3D object\ninstances for the generation of consistent panoptic segmentation results.\nFurther, we provide a theoretical analysis to justify the superiority of the\nproposed probabilistic solution. By conducting extensive experiments, our\nPCF-lift not only significantly outperforms the state-of-the-art methods on\nwidely used benchmarks including the ScanNet dataset and the challenging Messy\nRoom dataset (4.4% improvement of scene-level PQ), but also demonstrates strong\nrobustness when incorporating various 2D segmentation models or different\nlevels of hand-crafted noise.\n", "link": "http://arxiv.org/abs/2410.10659v1", "date": "2024-10-14", "relevancy": 2.9714, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCF-Lift%3A%20Panoptic%20Lifting%20by%20Probabilistic%20Contrastive%20Fusion&body=Title%3A%20PCF-Lift%3A%20Panoptic%20Lifting%20by%20Probabilistic%20Contrastive%20Fusion%0AAuthor%3A%20Runsong%20Zhu%20and%20Shi%20Qiu%20and%20Qianyi%20Wu%20and%20Ka-Hei%20Hui%20and%20Pheng-Ann%20Heng%20and%20Chi-Wing%20Fu%0AAbstract%3A%20%20%20Panoptic%20lifting%20is%20an%20effective%20technique%20to%20address%20the%203D%20panoptic%0Asegmentation%20task%20by%20unprojecting%202D%20panoptic%20segmentations%20from%20multi-views%20to%0A3D%20scene.%20However%2C%20the%20quality%20of%20its%20results%20largely%20depends%20on%20the%202D%0Asegmentations%2C%20which%20could%20be%20noisy%20and%20error-prone%2C%20so%20its%20performance%20often%0Adrops%20significantly%20for%20complex%20scenes.%20In%20this%20work%2C%20we%20design%20a%20new%20pipeline%0Acoined%20PCF-Lift%20based%20on%20our%20Probabilis-tic%20Contrastive%20Fusion%20%28PCF%29%20to%20learn%0Aand%20embed%20probabilistic%20features%20throughout%20our%20pipeline%20to%20actively%20consider%0Ainaccurate%20segmentations%20and%20inconsistent%20instance%20IDs.%20Technical-wise%2C%20we%0Afirst%20model%20the%20probabilistic%20feature%20embeddings%20through%20multivariate%20Gaussian%0Adistributions.%20To%20fuse%20the%20probabilistic%20features%2C%20we%20incorporate%20the%0Aprobability%20product%20kernel%20into%20the%20contrastive%20loss%20formulation%20and%20design%20a%0Across-view%20constraint%20to%20enhance%20the%20feature%20consistency%20across%20different%0Aviews.%20For%20the%20inference%2C%20we%20introduce%20a%20new%20probabilistic%20clustering%20method%20to%0Aeffectively%20associate%20prototype%20features%20with%20the%20underlying%203D%20object%0Ainstances%20for%20the%20generation%20of%20consistent%20panoptic%20segmentation%20results.%0AFurther%2C%20we%20provide%20a%20theoretical%20analysis%20to%20justify%20the%20superiority%20of%20the%0Aproposed%20probabilistic%20solution.%20By%20conducting%20extensive%20experiments%2C%20our%0APCF-lift%20not%20only%20significantly%20outperforms%20the%20state-of-the-art%20methods%20on%0Awidely%20used%20benchmarks%20including%20the%20ScanNet%20dataset%20and%20the%20challenging%20Messy%0ARoom%20dataset%20%284.4%25%20improvement%20of%20scene-level%20PQ%29%2C%20but%20also%20demonstrates%20strong%0Arobustness%20when%20incorporating%20various%202D%20segmentation%20models%20or%20different%0Alevels%20of%20hand-crafted%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCF-Lift%253A%2520Panoptic%2520Lifting%2520by%2520Probabilistic%2520Contrastive%2520Fusion%26entry.906535625%3DRunsong%2520Zhu%2520and%2520Shi%2520Qiu%2520and%2520Qianyi%2520Wu%2520and%2520Ka-Hei%2520Hui%2520and%2520Pheng-Ann%2520Heng%2520and%2520Chi-Wing%2520Fu%26entry.1292438233%3D%2520%2520Panoptic%2520lifting%2520is%2520an%2520effective%2520technique%2520to%2520address%2520the%25203D%2520panoptic%250Asegmentation%2520task%2520by%2520unprojecting%25202D%2520panoptic%2520segmentations%2520from%2520multi-views%2520to%250A3D%2520scene.%2520However%252C%2520the%2520quality%2520of%2520its%2520results%2520largely%2520depends%2520on%2520the%25202D%250Asegmentations%252C%2520which%2520could%2520be%2520noisy%2520and%2520error-prone%252C%2520so%2520its%2520performance%2520often%250Adrops%2520significantly%2520for%2520complex%2520scenes.%2520In%2520this%2520work%252C%2520we%2520design%2520a%2520new%2520pipeline%250Acoined%2520PCF-Lift%2520based%2520on%2520our%2520Probabilis-tic%2520Contrastive%2520Fusion%2520%2528PCF%2529%2520to%2520learn%250Aand%2520embed%2520probabilistic%2520features%2520throughout%2520our%2520pipeline%2520to%2520actively%2520consider%250Ainaccurate%2520segmentations%2520and%2520inconsistent%2520instance%2520IDs.%2520Technical-wise%252C%2520we%250Afirst%2520model%2520the%2520probabilistic%2520feature%2520embeddings%2520through%2520multivariate%2520Gaussian%250Adistributions.%2520To%2520fuse%2520the%2520probabilistic%2520features%252C%2520we%2520incorporate%2520the%250Aprobability%2520product%2520kernel%2520into%2520the%2520contrastive%2520loss%2520formulation%2520and%2520design%2520a%250Across-view%2520constraint%2520to%2520enhance%2520the%2520feature%2520consistency%2520across%2520different%250Aviews.%2520For%2520the%2520inference%252C%2520we%2520introduce%2520a%2520new%2520probabilistic%2520clustering%2520method%2520to%250Aeffectively%2520associate%2520prototype%2520features%2520with%2520the%2520underlying%25203D%2520object%250Ainstances%2520for%2520the%2520generation%2520of%2520consistent%2520panoptic%2520segmentation%2520results.%250AFurther%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520to%2520justify%2520the%2520superiority%2520of%2520the%250Aproposed%2520probabilistic%2520solution.%2520By%2520conducting%2520extensive%2520experiments%252C%2520our%250APCF-lift%2520not%2520only%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%250Awidely%2520used%2520benchmarks%2520including%2520the%2520ScanNet%2520dataset%2520and%2520the%2520challenging%2520Messy%250ARoom%2520dataset%2520%25284.4%2525%2520improvement%2520of%2520scene-level%2520PQ%2529%252C%2520but%2520also%2520demonstrates%2520strong%250Arobustness%2520when%2520incorporating%2520various%25202D%2520segmentation%2520models%2520or%2520different%250Alevels%2520of%2520hand-crafted%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCF-Lift%3A%20Panoptic%20Lifting%20by%20Probabilistic%20Contrastive%20Fusion&entry.906535625=Runsong%20Zhu%20and%20Shi%20Qiu%20and%20Qianyi%20Wu%20and%20Ka-Hei%20Hui%20and%20Pheng-Ann%20Heng%20and%20Chi-Wing%20Fu&entry.1292438233=%20%20Panoptic%20lifting%20is%20an%20effective%20technique%20to%20address%20the%203D%20panoptic%0Asegmentation%20task%20by%20unprojecting%202D%20panoptic%20segmentations%20from%20multi-views%20to%0A3D%20scene.%20However%2C%20the%20quality%20of%20its%20results%20largely%20depends%20on%20the%202D%0Asegmentations%2C%20which%20could%20be%20noisy%20and%20error-prone%2C%20so%20its%20performance%20often%0Adrops%20significantly%20for%20complex%20scenes.%20In%20this%20work%2C%20we%20design%20a%20new%20pipeline%0Acoined%20PCF-Lift%20based%20on%20our%20Probabilis-tic%20Contrastive%20Fusion%20%28PCF%29%20to%20learn%0Aand%20embed%20probabilistic%20features%20throughout%20our%20pipeline%20to%20actively%20consider%0Ainaccurate%20segmentations%20and%20inconsistent%20instance%20IDs.%20Technical-wise%2C%20we%0Afirst%20model%20the%20probabilistic%20feature%20embeddings%20through%20multivariate%20Gaussian%0Adistributions.%20To%20fuse%20the%20probabilistic%20features%2C%20we%20incorporate%20the%0Aprobability%20product%20kernel%20into%20the%20contrastive%20loss%20formulation%20and%20design%20a%0Across-view%20constraint%20to%20enhance%20the%20feature%20consistency%20across%20different%0Aviews.%20For%20the%20inference%2C%20we%20introduce%20a%20new%20probabilistic%20clustering%20method%20to%0Aeffectively%20associate%20prototype%20features%20with%20the%20underlying%203D%20object%0Ainstances%20for%20the%20generation%20of%20consistent%20panoptic%20segmentation%20results.%0AFurther%2C%20we%20provide%20a%20theoretical%20analysis%20to%20justify%20the%20superiority%20of%20the%0Aproposed%20probabilistic%20solution.%20By%20conducting%20extensive%20experiments%2C%20our%0APCF-lift%20not%20only%20significantly%20outperforms%20the%20state-of-the-art%20methods%20on%0Awidely%20used%20benchmarks%20including%20the%20ScanNet%20dataset%20and%20the%20challenging%20Messy%0ARoom%20dataset%20%284.4%25%20improvement%20of%20scene-level%20PQ%29%2C%20but%20also%20demonstrates%20strong%0Arobustness%20when%20incorporating%20various%202D%20segmentation%20models%20or%20different%0Alevels%20of%20hand-crafted%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10659v1&entry.124074799=Read"},
{"title": "Learning to Ground VLMs without Forgetting", "author": "Aritra Bhowmik and Mohammad Mahdi Derakhshani and Dennis Koelma and Martin R. Oswald and Yuki M. Asano and Cees G. M. Snoek", "abstract": "  Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Visual Language Models\n(VLMs) struggle at this task. In this paper, we introduce LynX, a framework\nthat equips pretrained VLMs with visual grounding ability without forgetting\ntheir existing image and language understanding skills. To this end, we propose\na Dual Mixture of Experts module that modifies only the decoder layer of the\nlanguage model, using one frozen Mixture of Experts (MoE) pre-trained on image\nand language understanding and another learnable MoE for new grounding\ncapabilities. This allows the VLM to retain previously learned knowledge and\nskills, while acquiring what is missing. To train the model effectively, we\ngenerate a high-quality synthetic dataset we call SCouT, which mimics human\nreasoning in visual grounding. This dataset provides rich supervision signals,\ndescribing a step-by-step multimodal reasoning process, thereby simplifying the\ntask of visual grounding. We evaluate LynX on several object detection and\nvisual grounding datasets, demonstrating strong performance in object\ndetection, zero-shot localization and grounded reasoning while maintaining its\noriginal image and language understanding capabilities on seven standard\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2410.10491v1", "date": "2024-10-14", "relevancy": 2.9713, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Ground%20VLMs%20without%20Forgetting&body=Title%3A%20Learning%20to%20Ground%20VLMs%20without%20Forgetting%0AAuthor%3A%20Aritra%20Bhowmik%20and%20Mohammad%20Mahdi%20Derakhshani%20and%20Dennis%20Koelma%20and%20Martin%20R.%20Oswald%20and%20Yuki%20M.%20Asano%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20Spatial%20awareness%20is%20key%20to%20enable%20embodied%20multimodal%20AI%20systems.%20Yet%2C%0Awithout%20vast%20amounts%20of%20spatial%20supervision%2C%20current%20Visual%20Language%20Models%0A%28VLMs%29%20struggle%20at%20this%20task.%20In%20this%20paper%2C%20we%20introduce%20LynX%2C%20a%20framework%0Athat%20equips%20pretrained%20VLMs%20with%20visual%20grounding%20ability%20without%20forgetting%0Atheir%20existing%20image%20and%20language%20understanding%20skills.%20To%20this%20end%2C%20we%20propose%0Aa%20Dual%20Mixture%20of%20Experts%20module%20that%20modifies%20only%20the%20decoder%20layer%20of%20the%0Alanguage%20model%2C%20using%20one%20frozen%20Mixture%20of%20Experts%20%28MoE%29%20pre-trained%20on%20image%0Aand%20language%20understanding%20and%20another%20learnable%20MoE%20for%20new%20grounding%0Acapabilities.%20This%20allows%20the%20VLM%20to%20retain%20previously%20learned%20knowledge%20and%0Askills%2C%20while%20acquiring%20what%20is%20missing.%20To%20train%20the%20model%20effectively%2C%20we%0Agenerate%20a%20high-quality%20synthetic%20dataset%20we%20call%20SCouT%2C%20which%20mimics%20human%0Areasoning%20in%20visual%20grounding.%20This%20dataset%20provides%20rich%20supervision%20signals%2C%0Adescribing%20a%20step-by-step%20multimodal%20reasoning%20process%2C%20thereby%20simplifying%20the%0Atask%20of%20visual%20grounding.%20We%20evaluate%20LynX%20on%20several%20object%20detection%20and%0Avisual%20grounding%20datasets%2C%20demonstrating%20strong%20performance%20in%20object%0Adetection%2C%20zero-shot%20localization%20and%20grounded%20reasoning%20while%20maintaining%20its%0Aoriginal%20image%20and%20language%20understanding%20capabilities%20on%20seven%20standard%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Ground%2520VLMs%2520without%2520Forgetting%26entry.906535625%3DAritra%2520Bhowmik%2520and%2520Mohammad%2520Mahdi%2520Derakhshani%2520and%2520Dennis%2520Koelma%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Yuki%2520M.%2520Asano%2520and%2520Cees%2520G.%2520M.%2520Snoek%26entry.1292438233%3D%2520%2520Spatial%2520awareness%2520is%2520key%2520to%2520enable%2520embodied%2520multimodal%2520AI%2520systems.%2520Yet%252C%250Awithout%2520vast%2520amounts%2520of%2520spatial%2520supervision%252C%2520current%2520Visual%2520Language%2520Models%250A%2528VLMs%2529%2520struggle%2520at%2520this%2520task.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LynX%252C%2520a%2520framework%250Athat%2520equips%2520pretrained%2520VLMs%2520with%2520visual%2520grounding%2520ability%2520without%2520forgetting%250Atheir%2520existing%2520image%2520and%2520language%2520understanding%2520skills.%2520To%2520this%2520end%252C%2520we%2520propose%250Aa%2520Dual%2520Mixture%2520of%2520Experts%2520module%2520that%2520modifies%2520only%2520the%2520decoder%2520layer%2520of%2520the%250Alanguage%2520model%252C%2520using%2520one%2520frozen%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520pre-trained%2520on%2520image%250Aand%2520language%2520understanding%2520and%2520another%2520learnable%2520MoE%2520for%2520new%2520grounding%250Acapabilities.%2520This%2520allows%2520the%2520VLM%2520to%2520retain%2520previously%2520learned%2520knowledge%2520and%250Askills%252C%2520while%2520acquiring%2520what%2520is%2520missing.%2520To%2520train%2520the%2520model%2520effectively%252C%2520we%250Agenerate%2520a%2520high-quality%2520synthetic%2520dataset%2520we%2520call%2520SCouT%252C%2520which%2520mimics%2520human%250Areasoning%2520in%2520visual%2520grounding.%2520This%2520dataset%2520provides%2520rich%2520supervision%2520signals%252C%250Adescribing%2520a%2520step-by-step%2520multimodal%2520reasoning%2520process%252C%2520thereby%2520simplifying%2520the%250Atask%2520of%2520visual%2520grounding.%2520We%2520evaluate%2520LynX%2520on%2520several%2520object%2520detection%2520and%250Avisual%2520grounding%2520datasets%252C%2520demonstrating%2520strong%2520performance%2520in%2520object%250Adetection%252C%2520zero-shot%2520localization%2520and%2520grounded%2520reasoning%2520while%2520maintaining%2520its%250Aoriginal%2520image%2520and%2520language%2520understanding%2520capabilities%2520on%2520seven%2520standard%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Ground%20VLMs%20without%20Forgetting&entry.906535625=Aritra%20Bhowmik%20and%20Mohammad%20Mahdi%20Derakhshani%20and%20Dennis%20Koelma%20and%20Martin%20R.%20Oswald%20and%20Yuki%20M.%20Asano%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20Spatial%20awareness%20is%20key%20to%20enable%20embodied%20multimodal%20AI%20systems.%20Yet%2C%0Awithout%20vast%20amounts%20of%20spatial%20supervision%2C%20current%20Visual%20Language%20Models%0A%28VLMs%29%20struggle%20at%20this%20task.%20In%20this%20paper%2C%20we%20introduce%20LynX%2C%20a%20framework%0Athat%20equips%20pretrained%20VLMs%20with%20visual%20grounding%20ability%20without%20forgetting%0Atheir%20existing%20image%20and%20language%20understanding%20skills.%20To%20this%20end%2C%20we%20propose%0Aa%20Dual%20Mixture%20of%20Experts%20module%20that%20modifies%20only%20the%20decoder%20layer%20of%20the%0Alanguage%20model%2C%20using%20one%20frozen%20Mixture%20of%20Experts%20%28MoE%29%20pre-trained%20on%20image%0Aand%20language%20understanding%20and%20another%20learnable%20MoE%20for%20new%20grounding%0Acapabilities.%20This%20allows%20the%20VLM%20to%20retain%20previously%20learned%20knowledge%20and%0Askills%2C%20while%20acquiring%20what%20is%20missing.%20To%20train%20the%20model%20effectively%2C%20we%0Agenerate%20a%20high-quality%20synthetic%20dataset%20we%20call%20SCouT%2C%20which%20mimics%20human%0Areasoning%20in%20visual%20grounding.%20This%20dataset%20provides%20rich%20supervision%20signals%2C%0Adescribing%20a%20step-by-step%20multimodal%20reasoning%20process%2C%20thereby%20simplifying%20the%0Atask%20of%20visual%20grounding.%20We%20evaluate%20LynX%20on%20several%20object%20detection%20and%0Avisual%20grounding%20datasets%2C%20demonstrating%20strong%20performance%20in%20object%0Adetection%2C%20zero-shot%20localization%20and%20grounded%20reasoning%20while%20maintaining%20its%0Aoriginal%20image%20and%20language%20understanding%20capabilities%20on%20seven%20standard%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10491v1&entry.124074799=Read"},
{"title": "Benchmarking Vision Language Models for Cultural Understanding", "author": "Shravan Nayak and Kanishk Jain and Rabiul Awal and Siva Reddy and Sjoerd van Steenkiste and Lisa Anne Hendricks and Karolina Sta\u0144czak and Aishwarya Agrawal", "abstract": "  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n", "link": "http://arxiv.org/abs/2407.10920v3", "date": "2024-10-14", "relevancy": 2.9008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding&body=Title%3A%20Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding%0AAuthor%3A%20Shravan%20Nayak%20and%20Kanishk%20Jain%20and%20Rabiul%20Awal%20and%20Siva%20Reddy%20and%20Sjoerd%20van%20Steenkiste%20and%20Lisa%20Anne%20Hendricks%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20Foundation%20models%20and%20vision-language%20pre-training%20have%20notably%20advanced%0AVision%20Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%0Alinguistic%20data.%20However%2C%20their%20performance%20has%20been%20typically%20assessed%20on%0Ageneral%20scene%20understanding%20-%20recognizing%20objects%2C%20attributes%2C%20and%20actions%20-%0Arather%20than%20cultural%20comprehension.%20This%20study%20introduces%20CulturalVQA%2C%20a%20visual%0Aquestion-answering%20benchmark%20aimed%20at%20assessing%20VLM%27s%20geo-diverse%20cultural%0Aunderstanding.%20We%20curate%20a%20collection%20of%202%2C378%20image-question%20pairs%20with%201-5%0Aanswers%20per%20question%20representing%20cultures%20from%2011%20countries%20across%205%0Acontinents.%20The%20questions%20probe%20understanding%20of%20various%20facets%20of%20culture%20such%0Aas%20clothing%2C%20food%2C%20drinks%2C%20rituals%2C%20and%20traditions.%20Benchmarking%20VLMs%20on%0ACulturalVQA%2C%20including%20GPT-4V%20and%20Gemini%2C%20reveals%20disparity%20in%20their%20level%20of%0Acultural%20understanding%20across%20regions%2C%20with%20strong%20cultural%20understanding%0Acapabilities%20for%20North%20America%20while%20significantly%20lower%20performance%20for%0AAfrica.%20We%20observe%20disparity%20in%20their%20performance%20across%20cultural%20facets%20too%2C%0Awith%20clothing%2C%20rituals%2C%20and%20traditions%20seeing%20higher%20performances%20than%20food%20and%0Adrink.%20These%20disparities%20help%20us%20identify%20areas%20where%20VLMs%20lack%20cultural%0Aunderstanding%20and%20demonstrate%20the%20potential%20of%20CulturalVQA%20as%20a%20comprehensive%0Aevaluation%20set%20for%20gauging%20VLM%20progress%20in%20understanding%20diverse%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10920v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision%2520Language%2520Models%2520for%2520Cultural%2520Understanding%26entry.906535625%3DShravan%2520Nayak%2520and%2520Kanishk%2520Jain%2520and%2520Rabiul%2520Awal%2520and%2520Siva%2520Reddy%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Karolina%2520Sta%25C5%2584czak%2520and%2520Aishwarya%2520Agrawal%26entry.1292438233%3D%2520%2520Foundation%2520models%2520and%2520vision-language%2520pre-training%2520have%2520notably%2520advanced%250AVision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520enabling%2520multimodal%2520processing%2520of%2520visual%2520and%250Alinguistic%2520data.%2520However%252C%2520their%2520performance%2520has%2520been%2520typically%2520assessed%2520on%250Ageneral%2520scene%2520understanding%2520-%2520recognizing%2520objects%252C%2520attributes%252C%2520and%2520actions%2520-%250Arather%2520than%2520cultural%2520comprehension.%2520This%2520study%2520introduces%2520CulturalVQA%252C%2520a%2520visual%250Aquestion-answering%2520benchmark%2520aimed%2520at%2520assessing%2520VLM%2527s%2520geo-diverse%2520cultural%250Aunderstanding.%2520We%2520curate%2520a%2520collection%2520of%25202%252C378%2520image-question%2520pairs%2520with%25201-5%250Aanswers%2520per%2520question%2520representing%2520cultures%2520from%252011%2520countries%2520across%25205%250Acontinents.%2520The%2520questions%2520probe%2520understanding%2520of%2520various%2520facets%2520of%2520culture%2520such%250Aas%2520clothing%252C%2520food%252C%2520drinks%252C%2520rituals%252C%2520and%2520traditions.%2520Benchmarking%2520VLMs%2520on%250ACulturalVQA%252C%2520including%2520GPT-4V%2520and%2520Gemini%252C%2520reveals%2520disparity%2520in%2520their%2520level%2520of%250Acultural%2520understanding%2520across%2520regions%252C%2520with%2520strong%2520cultural%2520understanding%250Acapabilities%2520for%2520North%2520America%2520while%2520significantly%2520lower%2520performance%2520for%250AAfrica.%2520We%2520observe%2520disparity%2520in%2520their%2520performance%2520across%2520cultural%2520facets%2520too%252C%250Awith%2520clothing%252C%2520rituals%252C%2520and%2520traditions%2520seeing%2520higher%2520performances%2520than%2520food%2520and%250Adrink.%2520These%2520disparities%2520help%2520us%2520identify%2520areas%2520where%2520VLMs%2520lack%2520cultural%250Aunderstanding%2520and%2520demonstrate%2520the%2520potential%2520of%2520CulturalVQA%2520as%2520a%2520comprehensive%250Aevaluation%2520set%2520for%2520gauging%2520VLM%2520progress%2520in%2520understanding%2520diverse%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10920v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision%20Language%20Models%20for%20Cultural%20Understanding&entry.906535625=Shravan%20Nayak%20and%20Kanishk%20Jain%20and%20Rabiul%20Awal%20and%20Siva%20Reddy%20and%20Sjoerd%20van%20Steenkiste%20and%20Lisa%20Anne%20Hendricks%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20Foundation%20models%20and%20vision-language%20pre-training%20have%20notably%20advanced%0AVision%20Language%20Models%20%28VLMs%29%2C%20enabling%20multimodal%20processing%20of%20visual%20and%0Alinguistic%20data.%20However%2C%20their%20performance%20has%20been%20typically%20assessed%20on%0Ageneral%20scene%20understanding%20-%20recognizing%20objects%2C%20attributes%2C%20and%20actions%20-%0Arather%20than%20cultural%20comprehension.%20This%20study%20introduces%20CulturalVQA%2C%20a%20visual%0Aquestion-answering%20benchmark%20aimed%20at%20assessing%20VLM%27s%20geo-diverse%20cultural%0Aunderstanding.%20We%20curate%20a%20collection%20of%202%2C378%20image-question%20pairs%20with%201-5%0Aanswers%20per%20question%20representing%20cultures%20from%2011%20countries%20across%205%0Acontinents.%20The%20questions%20probe%20understanding%20of%20various%20facets%20of%20culture%20such%0Aas%20clothing%2C%20food%2C%20drinks%2C%20rituals%2C%20and%20traditions.%20Benchmarking%20VLMs%20on%0ACulturalVQA%2C%20including%20GPT-4V%20and%20Gemini%2C%20reveals%20disparity%20in%20their%20level%20of%0Acultural%20understanding%20across%20regions%2C%20with%20strong%20cultural%20understanding%0Acapabilities%20for%20North%20America%20while%20significantly%20lower%20performance%20for%0AAfrica.%20We%20observe%20disparity%20in%20their%20performance%20across%20cultural%20facets%20too%2C%0Awith%20clothing%2C%20rituals%2C%20and%20traditions%20seeing%20higher%20performances%20than%20food%20and%0Adrink.%20These%20disparities%20help%20us%20identify%20areas%20where%20VLMs%20lack%20cultural%0Aunderstanding%20and%20demonstrate%20the%20potential%20of%20CulturalVQA%20as%20a%20comprehensive%0Aevaluation%20set%20for%20gauging%20VLM%20progress%20in%20understanding%20diverse%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10920v3&entry.124074799=Read"},
{"title": "BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using\n  Multi-parametric MRI", "author": "Shaohao Rui and Lingzhi Chen and Zhenyu Tang and Lilong Wang and Mianxin Liu and Shaoting Zhang and Xiaosong Wang", "abstract": "  Accurate diagnosis of brain abnormalities is greatly enhanced by the\ninclusion of complementary multi-parametric MRI imaging data. There is\nsignificant potential to develop a universal pre-training model that can be\nquickly adapted for image modalities and various clinical scenarios. However,\ncurrent models often rely on uni-modal image data, neglecting the cross-modal\ncorrelations among different image modalities or struggling to scale up\npre-training in the presence of missing modality data. In this paper, we\npropose BrainMVP, a multi-modal vision pre-training framework for brain image\nanalysis using multi-parametric MRI scans. First, we collect 16,022 brain MRI\nscans (over 2.4 million images), encompassing eight MRI modalities sourced from\na diverse range of centers and devices. Then, a novel pre-training paradigm is\nproposed for the multi-modal MRI data, addressing the issue of missing\nmodalities and achieving multi-modal information fusion. Cross-modal\nreconstruction is explored to learn distinctive brain image embeddings and\nefficient modality fusion capabilities. A modality-wise data distillation\nmodule is proposed to extract the essence representation of each MR image\nmodality for both the pre-training and downstream application purposes.\nFurthermore, we introduce a modality-aware contrastive learning module to\nenhance the cross-modality association within a study. Extensive experiments on\ndownstream tasks demonstrate superior performance compared to state-of-the-art\npre-training methods in the medical domain, with Dice Score improvement of\n0.28%-14.47% across six segmentation benchmarks and a consistent accuracy\nimprovement of 0.65%-18.07% in four individual classification tasks.\n", "link": "http://arxiv.org/abs/2410.10604v1", "date": "2024-10-14", "relevancy": 2.8585, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainMVP%3A%20Multi-modal%20Vision%20Pre-training%20for%20Brain%20Image%20Analysis%20using%0A%20%20Multi-parametric%20MRI&body=Title%3A%20BrainMVP%3A%20Multi-modal%20Vision%20Pre-training%20for%20Brain%20Image%20Analysis%20using%0A%20%20Multi-parametric%20MRI%0AAuthor%3A%20Shaohao%20Rui%20and%20Lingzhi%20Chen%20and%20Zhenyu%20Tang%20and%20Lilong%20Wang%20and%20Mianxin%20Liu%20and%20Shaoting%20Zhang%20and%20Xiaosong%20Wang%0AAbstract%3A%20%20%20Accurate%20diagnosis%20of%20brain%20abnormalities%20is%20greatly%20enhanced%20by%20the%0Ainclusion%20of%20complementary%20multi-parametric%20MRI%20imaging%20data.%20There%20is%0Asignificant%20potential%20to%20develop%20a%20universal%20pre-training%20model%20that%20can%20be%0Aquickly%20adapted%20for%20image%20modalities%20and%20various%20clinical%20scenarios.%20However%2C%0Acurrent%20models%20often%20rely%20on%20uni-modal%20image%20data%2C%20neglecting%20the%20cross-modal%0Acorrelations%20among%20different%20image%20modalities%20or%20struggling%20to%20scale%20up%0Apre-training%20in%20the%20presence%20of%20missing%20modality%20data.%20In%20this%20paper%2C%20we%0Apropose%20BrainMVP%2C%20a%20multi-modal%20vision%20pre-training%20framework%20for%20brain%20image%0Aanalysis%20using%20multi-parametric%20MRI%20scans.%20First%2C%20we%20collect%2016%2C022%20brain%20MRI%0Ascans%20%28over%202.4%20million%20images%29%2C%20encompassing%20eight%20MRI%20modalities%20sourced%20from%0Aa%20diverse%20range%20of%20centers%20and%20devices.%20Then%2C%20a%20novel%20pre-training%20paradigm%20is%0Aproposed%20for%20the%20multi-modal%20MRI%20data%2C%20addressing%20the%20issue%20of%20missing%0Amodalities%20and%20achieving%20multi-modal%20information%20fusion.%20Cross-modal%0Areconstruction%20is%20explored%20to%20learn%20distinctive%20brain%20image%20embeddings%20and%0Aefficient%20modality%20fusion%20capabilities.%20A%20modality-wise%20data%20distillation%0Amodule%20is%20proposed%20to%20extract%20the%20essence%20representation%20of%20each%20MR%20image%0Amodality%20for%20both%20the%20pre-training%20and%20downstream%20application%20purposes.%0AFurthermore%2C%20we%20introduce%20a%20modality-aware%20contrastive%20learning%20module%20to%0Aenhance%20the%20cross-modality%20association%20within%20a%20study.%20Extensive%20experiments%20on%0Adownstream%20tasks%20demonstrate%20superior%20performance%20compared%20to%20state-of-the-art%0Apre-training%20methods%20in%20the%20medical%20domain%2C%20with%20Dice%20Score%20improvement%20of%0A0.28%25-14.47%25%20across%20six%20segmentation%20benchmarks%20and%20a%20consistent%20accuracy%0Aimprovement%20of%200.65%25-18.07%25%20in%20four%20individual%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainMVP%253A%2520Multi-modal%2520Vision%2520Pre-training%2520for%2520Brain%2520Image%2520Analysis%2520using%250A%2520%2520Multi-parametric%2520MRI%26entry.906535625%3DShaohao%2520Rui%2520and%2520Lingzhi%2520Chen%2520and%2520Zhenyu%2520Tang%2520and%2520Lilong%2520Wang%2520and%2520Mianxin%2520Liu%2520and%2520Shaoting%2520Zhang%2520and%2520Xiaosong%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520diagnosis%2520of%2520brain%2520abnormalities%2520is%2520greatly%2520enhanced%2520by%2520the%250Ainclusion%2520of%2520complementary%2520multi-parametric%2520MRI%2520imaging%2520data.%2520There%2520is%250Asignificant%2520potential%2520to%2520develop%2520a%2520universal%2520pre-training%2520model%2520that%2520can%2520be%250Aquickly%2520adapted%2520for%2520image%2520modalities%2520and%2520various%2520clinical%2520scenarios.%2520However%252C%250Acurrent%2520models%2520often%2520rely%2520on%2520uni-modal%2520image%2520data%252C%2520neglecting%2520the%2520cross-modal%250Acorrelations%2520among%2520different%2520image%2520modalities%2520or%2520struggling%2520to%2520scale%2520up%250Apre-training%2520in%2520the%2520presence%2520of%2520missing%2520modality%2520data.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520BrainMVP%252C%2520a%2520multi-modal%2520vision%2520pre-training%2520framework%2520for%2520brain%2520image%250Aanalysis%2520using%2520multi-parametric%2520MRI%2520scans.%2520First%252C%2520we%2520collect%252016%252C022%2520brain%2520MRI%250Ascans%2520%2528over%25202.4%2520million%2520images%2529%252C%2520encompassing%2520eight%2520MRI%2520modalities%2520sourced%2520from%250Aa%2520diverse%2520range%2520of%2520centers%2520and%2520devices.%2520Then%252C%2520a%2520novel%2520pre-training%2520paradigm%2520is%250Aproposed%2520for%2520the%2520multi-modal%2520MRI%2520data%252C%2520addressing%2520the%2520issue%2520of%2520missing%250Amodalities%2520and%2520achieving%2520multi-modal%2520information%2520fusion.%2520Cross-modal%250Areconstruction%2520is%2520explored%2520to%2520learn%2520distinctive%2520brain%2520image%2520embeddings%2520and%250Aefficient%2520modality%2520fusion%2520capabilities.%2520A%2520modality-wise%2520data%2520distillation%250Amodule%2520is%2520proposed%2520to%2520extract%2520the%2520essence%2520representation%2520of%2520each%2520MR%2520image%250Amodality%2520for%2520both%2520the%2520pre-training%2520and%2520downstream%2520application%2520purposes.%250AFurthermore%252C%2520we%2520introduce%2520a%2520modality-aware%2520contrastive%2520learning%2520module%2520to%250Aenhance%2520the%2520cross-modality%2520association%2520within%2520a%2520study.%2520Extensive%2520experiments%2520on%250Adownstream%2520tasks%2520demonstrate%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%250Apre-training%2520methods%2520in%2520the%2520medical%2520domain%252C%2520with%2520Dice%2520Score%2520improvement%2520of%250A0.28%2525-14.47%2525%2520across%2520six%2520segmentation%2520benchmarks%2520and%2520a%2520consistent%2520accuracy%250Aimprovement%2520of%25200.65%2525-18.07%2525%2520in%2520four%2520individual%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainMVP%3A%20Multi-modal%20Vision%20Pre-training%20for%20Brain%20Image%20Analysis%20using%0A%20%20Multi-parametric%20MRI&entry.906535625=Shaohao%20Rui%20and%20Lingzhi%20Chen%20and%20Zhenyu%20Tang%20and%20Lilong%20Wang%20and%20Mianxin%20Liu%20and%20Shaoting%20Zhang%20and%20Xiaosong%20Wang&entry.1292438233=%20%20Accurate%20diagnosis%20of%20brain%20abnormalities%20is%20greatly%20enhanced%20by%20the%0Ainclusion%20of%20complementary%20multi-parametric%20MRI%20imaging%20data.%20There%20is%0Asignificant%20potential%20to%20develop%20a%20universal%20pre-training%20model%20that%20can%20be%0Aquickly%20adapted%20for%20image%20modalities%20and%20various%20clinical%20scenarios.%20However%2C%0Acurrent%20models%20often%20rely%20on%20uni-modal%20image%20data%2C%20neglecting%20the%20cross-modal%0Acorrelations%20among%20different%20image%20modalities%20or%20struggling%20to%20scale%20up%0Apre-training%20in%20the%20presence%20of%20missing%20modality%20data.%20In%20this%20paper%2C%20we%0Apropose%20BrainMVP%2C%20a%20multi-modal%20vision%20pre-training%20framework%20for%20brain%20image%0Aanalysis%20using%20multi-parametric%20MRI%20scans.%20First%2C%20we%20collect%2016%2C022%20brain%20MRI%0Ascans%20%28over%202.4%20million%20images%29%2C%20encompassing%20eight%20MRI%20modalities%20sourced%20from%0Aa%20diverse%20range%20of%20centers%20and%20devices.%20Then%2C%20a%20novel%20pre-training%20paradigm%20is%0Aproposed%20for%20the%20multi-modal%20MRI%20data%2C%20addressing%20the%20issue%20of%20missing%0Amodalities%20and%20achieving%20multi-modal%20information%20fusion.%20Cross-modal%0Areconstruction%20is%20explored%20to%20learn%20distinctive%20brain%20image%20embeddings%20and%0Aefficient%20modality%20fusion%20capabilities.%20A%20modality-wise%20data%20distillation%0Amodule%20is%20proposed%20to%20extract%20the%20essence%20representation%20of%20each%20MR%20image%0Amodality%20for%20both%20the%20pre-training%20and%20downstream%20application%20purposes.%0AFurthermore%2C%20we%20introduce%20a%20modality-aware%20contrastive%20learning%20module%20to%0Aenhance%20the%20cross-modality%20association%20within%20a%20study.%20Extensive%20experiments%20on%0Adownstream%20tasks%20demonstrate%20superior%20performance%20compared%20to%20state-of-the-art%0Apre-training%20methods%20in%20the%20medical%20domain%2C%20with%20Dice%20Score%20improvement%20of%0A0.28%25-14.47%25%20across%20six%20segmentation%20benchmarks%20and%20a%20consistent%20accuracy%0Aimprovement%20of%200.65%25-18.07%25%20in%20four%20individual%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10604v1&entry.124074799=Read"},
{"title": "DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy\n  World Model", "author": "Songen Gu and Wei Yin and Bu Jin and Xiaoyang Guo and Junming Wang and Haodong Li and Qian Zhang and Xiaoxiao Long", "abstract": "  We propose DOME, a diffusion-based world model that predicts future occupancy\nframes based on past occupancy observations. The ability of this world model to\ncapture the evolution of the environment is crucial for planning in autonomous\ndriving. Compared to 2D video-based world models, the occupancy world model\nutilizes a native 3D representation, which features easily obtainable\nannotations and is modality-agnostic. This flexibility has the potential to\nfacilitate the development of more advanced world models. Existing occupancy\nworld models either suffer from detail loss due to discrete tokenization or\nrely on simplistic diffusion architectures, leading to inefficiencies and\ndifficulties in predicting future occupancy with controllability. Our DOME\nexhibits two key features:(1) High-Fidelity and Long-Duration Generation. We\nadopt a spatial-temporal diffusion transformer to predict future occupancy\nframes based on historical context. This architecture efficiently captures\nspatial-temporal information, enabling high-fidelity details and the ability to\ngenerate predictions over long durations. (2)Fine-grained Controllability. We\naddress the challenge of controllability in predictions by introducing a\ntrajectory resampling method, which significantly enhances the model's ability\nto generate controlled predictions. Extensive experiments on the widely used\nnuScenes dataset demonstrate that our method surpasses existing baselines in\nboth qualitative and quantitative evaluations, establishing a new\nstate-of-the-art performance on nuScenes. Specifically, our approach surpasses\nthe baseline by 10.5% in mIoU and 21.2% in IoU for occupancy reconstruction and\nby 36.0% in mIoU and 24.6% in IoU for 4D occupancy forecasting.\n", "link": "http://arxiv.org/abs/2410.10429v1", "date": "2024-10-14", "relevancy": 2.8524, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5812}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5812}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOME%3A%20Taming%20Diffusion%20Model%20into%20High-Fidelity%20Controllable%20Occupancy%0A%20%20World%20Model&body=Title%3A%20DOME%3A%20Taming%20Diffusion%20Model%20into%20High-Fidelity%20Controllable%20Occupancy%0A%20%20World%20Model%0AAuthor%3A%20Songen%20Gu%20and%20Wei%20Yin%20and%20Bu%20Jin%20and%20Xiaoyang%20Guo%20and%20Junming%20Wang%20and%20Haodong%20Li%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long%0AAbstract%3A%20%20%20We%20propose%20DOME%2C%20a%20diffusion-based%20world%20model%20that%20predicts%20future%20occupancy%0Aframes%20based%20on%20past%20occupancy%20observations.%20The%20ability%20of%20this%20world%20model%20to%0Acapture%20the%20evolution%20of%20the%20environment%20is%20crucial%20for%20planning%20in%20autonomous%0Adriving.%20Compared%20to%202D%20video-based%20world%20models%2C%20the%20occupancy%20world%20model%0Autilizes%20a%20native%203D%20representation%2C%20which%20features%20easily%20obtainable%0Aannotations%20and%20is%20modality-agnostic.%20This%20flexibility%20has%20the%20potential%20to%0Afacilitate%20the%20development%20of%20more%20advanced%20world%20models.%20Existing%20occupancy%0Aworld%20models%20either%20suffer%20from%20detail%20loss%20due%20to%20discrete%20tokenization%20or%0Arely%20on%20simplistic%20diffusion%20architectures%2C%20leading%20to%20inefficiencies%20and%0Adifficulties%20in%20predicting%20future%20occupancy%20with%20controllability.%20Our%20DOME%0Aexhibits%20two%20key%20features%3A%281%29%20High-Fidelity%20and%20Long-Duration%20Generation.%20We%0Aadopt%20a%20spatial-temporal%20diffusion%20transformer%20to%20predict%20future%20occupancy%0Aframes%20based%20on%20historical%20context.%20This%20architecture%20efficiently%20captures%0Aspatial-temporal%20information%2C%20enabling%20high-fidelity%20details%20and%20the%20ability%20to%0Agenerate%20predictions%20over%20long%20durations.%20%282%29Fine-grained%20Controllability.%20We%0Aaddress%20the%20challenge%20of%20controllability%20in%20predictions%20by%20introducing%20a%0Atrajectory%20resampling%20method%2C%20which%20significantly%20enhances%20the%20model%27s%20ability%0Ato%20generate%20controlled%20predictions.%20Extensive%20experiments%20on%20the%20widely%20used%0AnuScenes%20dataset%20demonstrate%20that%20our%20method%20surpasses%20existing%20baselines%20in%0Aboth%20qualitative%20and%20quantitative%20evaluations%2C%20establishing%20a%20new%0Astate-of-the-art%20performance%20on%20nuScenes.%20Specifically%2C%20our%20approach%20surpasses%0Athe%20baseline%20by%2010.5%25%20in%20mIoU%20and%2021.2%25%20in%20IoU%20for%20occupancy%20reconstruction%20and%0Aby%2036.0%25%20in%20mIoU%20and%2024.6%25%20in%20IoU%20for%204D%20occupancy%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOME%253A%2520Taming%2520Diffusion%2520Model%2520into%2520High-Fidelity%2520Controllable%2520Occupancy%250A%2520%2520World%2520Model%26entry.906535625%3DSongen%2520Gu%2520and%2520Wei%2520Yin%2520and%2520Bu%2520Jin%2520and%2520Xiaoyang%2520Guo%2520and%2520Junming%2520Wang%2520and%2520Haodong%2520Li%2520and%2520Qian%2520Zhang%2520and%2520Xiaoxiao%2520Long%26entry.1292438233%3D%2520%2520We%2520propose%2520DOME%252C%2520a%2520diffusion-based%2520world%2520model%2520that%2520predicts%2520future%2520occupancy%250Aframes%2520based%2520on%2520past%2520occupancy%2520observations.%2520The%2520ability%2520of%2520this%2520world%2520model%2520to%250Acapture%2520the%2520evolution%2520of%2520the%2520environment%2520is%2520crucial%2520for%2520planning%2520in%2520autonomous%250Adriving.%2520Compared%2520to%25202D%2520video-based%2520world%2520models%252C%2520the%2520occupancy%2520world%2520model%250Autilizes%2520a%2520native%25203D%2520representation%252C%2520which%2520features%2520easily%2520obtainable%250Aannotations%2520and%2520is%2520modality-agnostic.%2520This%2520flexibility%2520has%2520the%2520potential%2520to%250Afacilitate%2520the%2520development%2520of%2520more%2520advanced%2520world%2520models.%2520Existing%2520occupancy%250Aworld%2520models%2520either%2520suffer%2520from%2520detail%2520loss%2520due%2520to%2520discrete%2520tokenization%2520or%250Arely%2520on%2520simplistic%2520diffusion%2520architectures%252C%2520leading%2520to%2520inefficiencies%2520and%250Adifficulties%2520in%2520predicting%2520future%2520occupancy%2520with%2520controllability.%2520Our%2520DOME%250Aexhibits%2520two%2520key%2520features%253A%25281%2529%2520High-Fidelity%2520and%2520Long-Duration%2520Generation.%2520We%250Aadopt%2520a%2520spatial-temporal%2520diffusion%2520transformer%2520to%2520predict%2520future%2520occupancy%250Aframes%2520based%2520on%2520historical%2520context.%2520This%2520architecture%2520efficiently%2520captures%250Aspatial-temporal%2520information%252C%2520enabling%2520high-fidelity%2520details%2520and%2520the%2520ability%2520to%250Agenerate%2520predictions%2520over%2520long%2520durations.%2520%25282%2529Fine-grained%2520Controllability.%2520We%250Aaddress%2520the%2520challenge%2520of%2520controllability%2520in%2520predictions%2520by%2520introducing%2520a%250Atrajectory%2520resampling%2520method%252C%2520which%2520significantly%2520enhances%2520the%2520model%2527s%2520ability%250Ato%2520generate%2520controlled%2520predictions.%2520Extensive%2520experiments%2520on%2520the%2520widely%2520used%250AnuScenes%2520dataset%2520demonstrate%2520that%2520our%2520method%2520surpasses%2520existing%2520baselines%2520in%250Aboth%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520performance%2520on%2520nuScenes.%2520Specifically%252C%2520our%2520approach%2520surpasses%250Athe%2520baseline%2520by%252010.5%2525%2520in%2520mIoU%2520and%252021.2%2525%2520in%2520IoU%2520for%2520occupancy%2520reconstruction%2520and%250Aby%252036.0%2525%2520in%2520mIoU%2520and%252024.6%2525%2520in%2520IoU%2520for%25204D%2520occupancy%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOME%3A%20Taming%20Diffusion%20Model%20into%20High-Fidelity%20Controllable%20Occupancy%0A%20%20World%20Model&entry.906535625=Songen%20Gu%20and%20Wei%20Yin%20and%20Bu%20Jin%20and%20Xiaoyang%20Guo%20and%20Junming%20Wang%20and%20Haodong%20Li%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long&entry.1292438233=%20%20We%20propose%20DOME%2C%20a%20diffusion-based%20world%20model%20that%20predicts%20future%20occupancy%0Aframes%20based%20on%20past%20occupancy%20observations.%20The%20ability%20of%20this%20world%20model%20to%0Acapture%20the%20evolution%20of%20the%20environment%20is%20crucial%20for%20planning%20in%20autonomous%0Adriving.%20Compared%20to%202D%20video-based%20world%20models%2C%20the%20occupancy%20world%20model%0Autilizes%20a%20native%203D%20representation%2C%20which%20features%20easily%20obtainable%0Aannotations%20and%20is%20modality-agnostic.%20This%20flexibility%20has%20the%20potential%20to%0Afacilitate%20the%20development%20of%20more%20advanced%20world%20models.%20Existing%20occupancy%0Aworld%20models%20either%20suffer%20from%20detail%20loss%20due%20to%20discrete%20tokenization%20or%0Arely%20on%20simplistic%20diffusion%20architectures%2C%20leading%20to%20inefficiencies%20and%0Adifficulties%20in%20predicting%20future%20occupancy%20with%20controllability.%20Our%20DOME%0Aexhibits%20two%20key%20features%3A%281%29%20High-Fidelity%20and%20Long-Duration%20Generation.%20We%0Aadopt%20a%20spatial-temporal%20diffusion%20transformer%20to%20predict%20future%20occupancy%0Aframes%20based%20on%20historical%20context.%20This%20architecture%20efficiently%20captures%0Aspatial-temporal%20information%2C%20enabling%20high-fidelity%20details%20and%20the%20ability%20to%0Agenerate%20predictions%20over%20long%20durations.%20%282%29Fine-grained%20Controllability.%20We%0Aaddress%20the%20challenge%20of%20controllability%20in%20predictions%20by%20introducing%20a%0Atrajectory%20resampling%20method%2C%20which%20significantly%20enhances%20the%20model%27s%20ability%0Ato%20generate%20controlled%20predictions.%20Extensive%20experiments%20on%20the%20widely%20used%0AnuScenes%20dataset%20demonstrate%20that%20our%20method%20surpasses%20existing%20baselines%20in%0Aboth%20qualitative%20and%20quantitative%20evaluations%2C%20establishing%20a%20new%0Astate-of-the-art%20performance%20on%20nuScenes.%20Specifically%2C%20our%20approach%20surpasses%0Athe%20baseline%20by%2010.5%25%20in%20mIoU%20and%2021.2%25%20in%20IoU%20for%20occupancy%20reconstruction%20and%0Aby%2036.0%25%20in%20mIoU%20and%2024.6%25%20in%20IoU%20for%204D%20occupancy%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10429v1&entry.124074799=Read"},
{"title": "Revisiting Few-Shot Object Detection with Vision-Language Models", "author": "Anish Madan and Neehar Peri and Shu Kong and Deva Ramanan", "abstract": "  The era of vision-language models (VLMs) trained on web-scale datasets\nchallenges conventional formulations of \"open-world\" perception. In this work,\nwe revisit the task of few-shot object detection (FSOD) in the context of\nrecent foundational VLMs. First, we point out that zero-shot predictions from\nVLMs such as GroundingDINO significantly outperform state-of-the-art few-shot\ndetectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance,\nsuch foundation models may still be sub-optimal. For example, trucks on the web\nmay be defined differently from trucks for a target application such as\nautonomous vehicle perception. We argue that the task of few-shot recognition\ncan be reformulated as aligning foundation models to target concepts using a\nfew examples. Interestingly, such examples can be multi-modal, using both text\nand visual cues, mimicking instructions that are often given to human\nannotators when defining a target concept of interest. Concretely, we propose\nFoundational FSOD, a new benchmark protocol that evaluates detectors\npre-trained on any external data and fine-tuned on multi-modal (text and\nvisual) K-shot examples per target class. We repurpose nuImages for\nFoundational FSOD, benchmark several popular open-source VLMs, and provide an\nempirical analysis of state-of-the-art methods. Lastly, we discuss our recent\nCVPR 2024 Foundational FSOD competition and share insights from the community.\nNotably, the winning team significantly outperforms our baseline by 23.3 mAP!\nOur code and dataset splits are available at\nhttps://github.com/anishmadan23/foundational_fsod\n", "link": "http://arxiv.org/abs/2312.14494v4", "date": "2024-10-14", "relevancy": 2.8386, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.599}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models&body=Title%3A%20Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models%0AAuthor%3A%20Anish%20Madan%20and%20Neehar%20Peri%20and%20Shu%20Kong%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20The%20era%20of%20vision-language%20models%20%28VLMs%29%20trained%20on%20web-scale%20datasets%0Achallenges%20conventional%20formulations%20of%20%22open-world%22%20perception.%20In%20this%20work%2C%0Awe%20revisit%20the%20task%20of%20few-shot%20object%20detection%20%28FSOD%29%20in%20the%20context%20of%0Arecent%20foundational%20VLMs.%20First%2C%20we%20point%20out%20that%20zero-shot%20predictions%20from%0AVLMs%20such%20as%20GroundingDINO%20significantly%20outperform%20state-of-the-art%20few-shot%0Adetectors%20%2848%20vs.%2033%20AP%29%20on%20COCO.%20Despite%20their%20strong%20zero-shot%20performance%2C%0Asuch%20foundation%20models%20may%20still%20be%20sub-optimal.%20For%20example%2C%20trucks%20on%20the%20web%0Amay%20be%20defined%20differently%20from%20trucks%20for%20a%20target%20application%20such%20as%0Aautonomous%20vehicle%20perception.%20We%20argue%20that%20the%20task%20of%20few-shot%20recognition%0Acan%20be%20reformulated%20as%20aligning%20foundation%20models%20to%20target%20concepts%20using%20a%0Afew%20examples.%20Interestingly%2C%20such%20examples%20can%20be%20multi-modal%2C%20using%20both%20text%0Aand%20visual%20cues%2C%20mimicking%20instructions%20that%20are%20often%20given%20to%20human%0Aannotators%20when%20defining%20a%20target%20concept%20of%20interest.%20Concretely%2C%20we%20propose%0AFoundational%20FSOD%2C%20a%20new%20benchmark%20protocol%20that%20evaluates%20detectors%0Apre-trained%20on%20any%20external%20data%20and%20fine-tuned%20on%20multi-modal%20%28text%20and%0Avisual%29%20K-shot%20examples%20per%20target%20class.%20We%20repurpose%20nuImages%20for%0AFoundational%20FSOD%2C%20benchmark%20several%20popular%20open-source%20VLMs%2C%20and%20provide%20an%0Aempirical%20analysis%20of%20state-of-the-art%20methods.%20Lastly%2C%20we%20discuss%20our%20recent%0ACVPR%202024%20Foundational%20FSOD%20competition%20and%20share%20insights%20from%20the%20community.%0ANotably%2C%20the%20winning%20team%20significantly%20outperforms%20our%20baseline%20by%2023.3%20mAP%21%0AOur%20code%20and%20dataset%20splits%20are%20available%20at%0Ahttps%3A//github.com/anishmadan23/foundational_fsod%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14494v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Few-Shot%2520Object%2520Detection%2520with%2520Vision-Language%2520Models%26entry.906535625%3DAnish%2520Madan%2520and%2520Neehar%2520Peri%2520and%2520Shu%2520Kong%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520The%2520era%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520trained%2520on%2520web-scale%2520datasets%250Achallenges%2520conventional%2520formulations%2520of%2520%2522open-world%2522%2520perception.%2520In%2520this%2520work%252C%250Awe%2520revisit%2520the%2520task%2520of%2520few-shot%2520object%2520detection%2520%2528FSOD%2529%2520in%2520the%2520context%2520of%250Arecent%2520foundational%2520VLMs.%2520First%252C%2520we%2520point%2520out%2520that%2520zero-shot%2520predictions%2520from%250AVLMs%2520such%2520as%2520GroundingDINO%2520significantly%2520outperform%2520state-of-the-art%2520few-shot%250Adetectors%2520%252848%2520vs.%252033%2520AP%2529%2520on%2520COCO.%2520Despite%2520their%2520strong%2520zero-shot%2520performance%252C%250Asuch%2520foundation%2520models%2520may%2520still%2520be%2520sub-optimal.%2520For%2520example%252C%2520trucks%2520on%2520the%2520web%250Amay%2520be%2520defined%2520differently%2520from%2520trucks%2520for%2520a%2520target%2520application%2520such%2520as%250Aautonomous%2520vehicle%2520perception.%2520We%2520argue%2520that%2520the%2520task%2520of%2520few-shot%2520recognition%250Acan%2520be%2520reformulated%2520as%2520aligning%2520foundation%2520models%2520to%2520target%2520concepts%2520using%2520a%250Afew%2520examples.%2520Interestingly%252C%2520such%2520examples%2520can%2520be%2520multi-modal%252C%2520using%2520both%2520text%250Aand%2520visual%2520cues%252C%2520mimicking%2520instructions%2520that%2520are%2520often%2520given%2520to%2520human%250Aannotators%2520when%2520defining%2520a%2520target%2520concept%2520of%2520interest.%2520Concretely%252C%2520we%2520propose%250AFoundational%2520FSOD%252C%2520a%2520new%2520benchmark%2520protocol%2520that%2520evaluates%2520detectors%250Apre-trained%2520on%2520any%2520external%2520data%2520and%2520fine-tuned%2520on%2520multi-modal%2520%2528text%2520and%250Avisual%2529%2520K-shot%2520examples%2520per%2520target%2520class.%2520We%2520repurpose%2520nuImages%2520for%250AFoundational%2520FSOD%252C%2520benchmark%2520several%2520popular%2520open-source%2520VLMs%252C%2520and%2520provide%2520an%250Aempirical%2520analysis%2520of%2520state-of-the-art%2520methods.%2520Lastly%252C%2520we%2520discuss%2520our%2520recent%250ACVPR%25202024%2520Foundational%2520FSOD%2520competition%2520and%2520share%2520insights%2520from%2520the%2520community.%250ANotably%252C%2520the%2520winning%2520team%2520significantly%2520outperforms%2520our%2520baseline%2520by%252023.3%2520mAP%2521%250AOur%2520code%2520and%2520dataset%2520splits%2520are%2520available%2520at%250Ahttps%253A//github.com/anishmadan23/foundational_fsod%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14494v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models&entry.906535625=Anish%20Madan%20and%20Neehar%20Peri%20and%20Shu%20Kong%20and%20Deva%20Ramanan&entry.1292438233=%20%20The%20era%20of%20vision-language%20models%20%28VLMs%29%20trained%20on%20web-scale%20datasets%0Achallenges%20conventional%20formulations%20of%20%22open-world%22%20perception.%20In%20this%20work%2C%0Awe%20revisit%20the%20task%20of%20few-shot%20object%20detection%20%28FSOD%29%20in%20the%20context%20of%0Arecent%20foundational%20VLMs.%20First%2C%20we%20point%20out%20that%20zero-shot%20predictions%20from%0AVLMs%20such%20as%20GroundingDINO%20significantly%20outperform%20state-of-the-art%20few-shot%0Adetectors%20%2848%20vs.%2033%20AP%29%20on%20COCO.%20Despite%20their%20strong%20zero-shot%20performance%2C%0Asuch%20foundation%20models%20may%20still%20be%20sub-optimal.%20For%20example%2C%20trucks%20on%20the%20web%0Amay%20be%20defined%20differently%20from%20trucks%20for%20a%20target%20application%20such%20as%0Aautonomous%20vehicle%20perception.%20We%20argue%20that%20the%20task%20of%20few-shot%20recognition%0Acan%20be%20reformulated%20as%20aligning%20foundation%20models%20to%20target%20concepts%20using%20a%0Afew%20examples.%20Interestingly%2C%20such%20examples%20can%20be%20multi-modal%2C%20using%20both%20text%0Aand%20visual%20cues%2C%20mimicking%20instructions%20that%20are%20often%20given%20to%20human%0Aannotators%20when%20defining%20a%20target%20concept%20of%20interest.%20Concretely%2C%20we%20propose%0AFoundational%20FSOD%2C%20a%20new%20benchmark%20protocol%20that%20evaluates%20detectors%0Apre-trained%20on%20any%20external%20data%20and%20fine-tuned%20on%20multi-modal%20%28text%20and%0Avisual%29%20K-shot%20examples%20per%20target%20class.%20We%20repurpose%20nuImages%20for%0AFoundational%20FSOD%2C%20benchmark%20several%20popular%20open-source%20VLMs%2C%20and%20provide%20an%0Aempirical%20analysis%20of%20state-of-the-art%20methods.%20Lastly%2C%20we%20discuss%20our%20recent%0ACVPR%202024%20Foundational%20FSOD%20competition%20and%20share%20insights%20from%20the%20community.%0ANotably%2C%20the%20winning%20team%20significantly%20outperforms%20our%20baseline%20by%2023.3%20mAP%21%0AOur%20code%20and%20dataset%20splits%20are%20available%20at%0Ahttps%3A//github.com/anishmadan23/foundational_fsod%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14494v4&entry.124074799=Read"},
{"title": "ReLayout: Towards Real-World Document Understanding via Layout-enhanced\n  Pre-training", "author": "Zhouqiang Jiang and Bowen Wang and Junhao Chen and Yuta Nakashima", "abstract": "  Recent approaches for visually-rich document understanding (VrDU) uses\nmanually annotated semantic groups, where a semantic group encompasses all\nsemantically relevant but not obviously grouped words. As OCR tools are unable\nto automatically identify such grouping, we argue that current VrDU approaches\nare unrealistic. We thus introduce a new variant of the VrDU task, real-world\nvisually-rich document understanding (ReVrDU), that does not allow for using\nmanually annotated semantic groups. We also propose a new method, ReLayout,\ncompliant with the ReVrDU scenario, which learns to capture semantic grouping\nthrough arranging words and bringing the representations of words that belong\nto the potential same semantic group closer together. Our experimental results\ndemonstrate the performance of existing methods is deteriorated with the ReVrDU\ntask, while ReLayout shows superiour performance.\n", "link": "http://arxiv.org/abs/2410.10471v1", "date": "2024-10-14", "relevancy": 2.8334, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLayout%3A%20Towards%20Real-World%20Document%20Understanding%20via%20Layout-enhanced%0A%20%20Pre-training&body=Title%3A%20ReLayout%3A%20Towards%20Real-World%20Document%20Understanding%20via%20Layout-enhanced%0A%20%20Pre-training%0AAuthor%3A%20Zhouqiang%20Jiang%20and%20Bowen%20Wang%20and%20Junhao%20Chen%20and%20Yuta%20Nakashima%0AAbstract%3A%20%20%20Recent%20approaches%20for%20visually-rich%20document%20understanding%20%28VrDU%29%20uses%0Amanually%20annotated%20semantic%20groups%2C%20where%20a%20semantic%20group%20encompasses%20all%0Asemantically%20relevant%20but%20not%20obviously%20grouped%20words.%20As%20OCR%20tools%20are%20unable%0Ato%20automatically%20identify%20such%20grouping%2C%20we%20argue%20that%20current%20VrDU%20approaches%0Aare%20unrealistic.%20We%20thus%20introduce%20a%20new%20variant%20of%20the%20VrDU%20task%2C%20real-world%0Avisually-rich%20document%20understanding%20%28ReVrDU%29%2C%20that%20does%20not%20allow%20for%20using%0Amanually%20annotated%20semantic%20groups.%20We%20also%20propose%20a%20new%20method%2C%20ReLayout%2C%0Acompliant%20with%20the%20ReVrDU%20scenario%2C%20which%20learns%20to%20capture%20semantic%20grouping%0Athrough%20arranging%20words%20and%20bringing%20the%20representations%20of%20words%20that%20belong%0Ato%20the%20potential%20same%20semantic%20group%20closer%20together.%20Our%20experimental%20results%0Ademonstrate%20the%20performance%20of%20existing%20methods%20is%20deteriorated%20with%20the%20ReVrDU%0Atask%2C%20while%20ReLayout%20shows%20superiour%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLayout%253A%2520Towards%2520Real-World%2520Document%2520Understanding%2520via%2520Layout-enhanced%250A%2520%2520Pre-training%26entry.906535625%3DZhouqiang%2520Jiang%2520and%2520Bowen%2520Wang%2520and%2520Junhao%2520Chen%2520and%2520Yuta%2520Nakashima%26entry.1292438233%3D%2520%2520Recent%2520approaches%2520for%2520visually-rich%2520document%2520understanding%2520%2528VrDU%2529%2520uses%250Amanually%2520annotated%2520semantic%2520groups%252C%2520where%2520a%2520semantic%2520group%2520encompasses%2520all%250Asemantically%2520relevant%2520but%2520not%2520obviously%2520grouped%2520words.%2520As%2520OCR%2520tools%2520are%2520unable%250Ato%2520automatically%2520identify%2520such%2520grouping%252C%2520we%2520argue%2520that%2520current%2520VrDU%2520approaches%250Aare%2520unrealistic.%2520We%2520thus%2520introduce%2520a%2520new%2520variant%2520of%2520the%2520VrDU%2520task%252C%2520real-world%250Avisually-rich%2520document%2520understanding%2520%2528ReVrDU%2529%252C%2520that%2520does%2520not%2520allow%2520for%2520using%250Amanually%2520annotated%2520semantic%2520groups.%2520We%2520also%2520propose%2520a%2520new%2520method%252C%2520ReLayout%252C%250Acompliant%2520with%2520the%2520ReVrDU%2520scenario%252C%2520which%2520learns%2520to%2520capture%2520semantic%2520grouping%250Athrough%2520arranging%2520words%2520and%2520bringing%2520the%2520representations%2520of%2520words%2520that%2520belong%250Ato%2520the%2520potential%2520same%2520semantic%2520group%2520closer%2520together.%2520Our%2520experimental%2520results%250Ademonstrate%2520the%2520performance%2520of%2520existing%2520methods%2520is%2520deteriorated%2520with%2520the%2520ReVrDU%250Atask%252C%2520while%2520ReLayout%2520shows%2520superiour%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLayout%3A%20Towards%20Real-World%20Document%20Understanding%20via%20Layout-enhanced%0A%20%20Pre-training&entry.906535625=Zhouqiang%20Jiang%20and%20Bowen%20Wang%20and%20Junhao%20Chen%20and%20Yuta%20Nakashima&entry.1292438233=%20%20Recent%20approaches%20for%20visually-rich%20document%20understanding%20%28VrDU%29%20uses%0Amanually%20annotated%20semantic%20groups%2C%20where%20a%20semantic%20group%20encompasses%20all%0Asemantically%20relevant%20but%20not%20obviously%20grouped%20words.%20As%20OCR%20tools%20are%20unable%0Ato%20automatically%20identify%20such%20grouping%2C%20we%20argue%20that%20current%20VrDU%20approaches%0Aare%20unrealistic.%20We%20thus%20introduce%20a%20new%20variant%20of%20the%20VrDU%20task%2C%20real-world%0Avisually-rich%20document%20understanding%20%28ReVrDU%29%2C%20that%20does%20not%20allow%20for%20using%0Amanually%20annotated%20semantic%20groups.%20We%20also%20propose%20a%20new%20method%2C%20ReLayout%2C%0Acompliant%20with%20the%20ReVrDU%20scenario%2C%20which%20learns%20to%20capture%20semantic%20grouping%0Athrough%20arranging%20words%20and%20bringing%20the%20representations%20of%20words%20that%20belong%0Ato%20the%20potential%20same%20semantic%20group%20closer%20together.%20Our%20experimental%20results%0Ademonstrate%20the%20performance%20of%20existing%20methods%20is%20deteriorated%20with%20the%20ReVrDU%0Atask%2C%20while%20ReLayout%20shows%20superiour%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10471v1&entry.124074799=Read"},
{"title": "DragEntity: Trajectory Guided Video Generation using Entity and\n  Positional Relationships", "author": "Zhang Wan and Sheng Tang and Jiawei Wei and Ruize Zhang and Juan Cao", "abstract": "  In recent years, diffusion models have achieved tremendous success in the\nfield of video generation, with controllable video generation receiving\nsignificant attention. However, existing control methods still face two\nlimitations: Firstly, control conditions (such as depth maps, 3D Mesh) are\ndifficult for ordinary users to obtain directly. Secondly, it's challenging to\ndrive multiple objects through complex motions with multiple trajectories\nsimultaneously. In this paper, we introduce DragEntity, a video generation\nmodel that utilizes entity representation for controlling the motion of\nmultiple objects. Compared to previous methods, DragEntity offers two main\nadvantages: 1) Our method is more user-friendly for interaction because it\nallows users to drag entities within the image rather than individual pixels.\n2) We use entity representation to represent any object in the image, and\nmultiple objects can maintain relative spatial relationships. Therefore, we\nallow multiple trajectories to control multiple objects in the image with\ndifferent levels of complexity simultaneously. Our experiments validate the\neffectiveness of DragEntity, demonstrating its excellent performance in\nfine-grained control in video generation.\n", "link": "http://arxiv.org/abs/2410.10751v1", "date": "2024-10-14", "relevancy": 2.803, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5817}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5527}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragEntity%3A%20Trajectory%20Guided%20Video%20Generation%20using%20Entity%20and%0A%20%20Positional%20Relationships&body=Title%3A%20DragEntity%3A%20Trajectory%20Guided%20Video%20Generation%20using%20Entity%20and%0A%20%20Positional%20Relationships%0AAuthor%3A%20Zhang%20Wan%20and%20Sheng%20Tang%20and%20Jiawei%20Wei%20and%20Ruize%20Zhang%20and%20Juan%20Cao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20diffusion%20models%20have%20achieved%20tremendous%20success%20in%20the%0Afield%20of%20video%20generation%2C%20with%20controllable%20video%20generation%20receiving%0Asignificant%20attention.%20However%2C%20existing%20control%20methods%20still%20face%20two%0Alimitations%3A%20Firstly%2C%20control%20conditions%20%28such%20as%20depth%20maps%2C%203D%20Mesh%29%20are%0Adifficult%20for%20ordinary%20users%20to%20obtain%20directly.%20Secondly%2C%20it%27s%20challenging%20to%0Adrive%20multiple%20objects%20through%20complex%20motions%20with%20multiple%20trajectories%0Asimultaneously.%20In%20this%20paper%2C%20we%20introduce%20DragEntity%2C%20a%20video%20generation%0Amodel%20that%20utilizes%20entity%20representation%20for%20controlling%20the%20motion%20of%0Amultiple%20objects.%20Compared%20to%20previous%20methods%2C%20DragEntity%20offers%20two%20main%0Aadvantages%3A%201%29%20Our%20method%20is%20more%20user-friendly%20for%20interaction%20because%20it%0Aallows%20users%20to%20drag%20entities%20within%20the%20image%20rather%20than%20individual%20pixels.%0A2%29%20We%20use%20entity%20representation%20to%20represent%20any%20object%20in%20the%20image%2C%20and%0Amultiple%20objects%20can%20maintain%20relative%20spatial%20relationships.%20Therefore%2C%20we%0Aallow%20multiple%20trajectories%20to%20control%20multiple%20objects%20in%20the%20image%20with%0Adifferent%20levels%20of%20complexity%20simultaneously.%20Our%20experiments%20validate%20the%0Aeffectiveness%20of%20DragEntity%2C%20demonstrating%20its%20excellent%20performance%20in%0Afine-grained%20control%20in%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragEntity%253A%2520Trajectory%2520Guided%2520Video%2520Generation%2520using%2520Entity%2520and%250A%2520%2520Positional%2520Relationships%26entry.906535625%3DZhang%2520Wan%2520and%2520Sheng%2520Tang%2520and%2520Jiawei%2520Wei%2520and%2520Ruize%2520Zhang%2520and%2520Juan%2520Cao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520diffusion%2520models%2520have%2520achieved%2520tremendous%2520success%2520in%2520the%250Afield%2520of%2520video%2520generation%252C%2520with%2520controllable%2520video%2520generation%2520receiving%250Asignificant%2520attention.%2520However%252C%2520existing%2520control%2520methods%2520still%2520face%2520two%250Alimitations%253A%2520Firstly%252C%2520control%2520conditions%2520%2528such%2520as%2520depth%2520maps%252C%25203D%2520Mesh%2529%2520are%250Adifficult%2520for%2520ordinary%2520users%2520to%2520obtain%2520directly.%2520Secondly%252C%2520it%2527s%2520challenging%2520to%250Adrive%2520multiple%2520objects%2520through%2520complex%2520motions%2520with%2520multiple%2520trajectories%250Asimultaneously.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DragEntity%252C%2520a%2520video%2520generation%250Amodel%2520that%2520utilizes%2520entity%2520representation%2520for%2520controlling%2520the%2520motion%2520of%250Amultiple%2520objects.%2520Compared%2520to%2520previous%2520methods%252C%2520DragEntity%2520offers%2520two%2520main%250Aadvantages%253A%25201%2529%2520Our%2520method%2520is%2520more%2520user-friendly%2520for%2520interaction%2520because%2520it%250Aallows%2520users%2520to%2520drag%2520entities%2520within%2520the%2520image%2520rather%2520than%2520individual%2520pixels.%250A2%2529%2520We%2520use%2520entity%2520representation%2520to%2520represent%2520any%2520object%2520in%2520the%2520image%252C%2520and%250Amultiple%2520objects%2520can%2520maintain%2520relative%2520spatial%2520relationships.%2520Therefore%252C%2520we%250Aallow%2520multiple%2520trajectories%2520to%2520control%2520multiple%2520objects%2520in%2520the%2520image%2520with%250Adifferent%2520levels%2520of%2520complexity%2520simultaneously.%2520Our%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520DragEntity%252C%2520demonstrating%2520its%2520excellent%2520performance%2520in%250Afine-grained%2520control%2520in%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragEntity%3A%20Trajectory%20Guided%20Video%20Generation%20using%20Entity%20and%0A%20%20Positional%20Relationships&entry.906535625=Zhang%20Wan%20and%20Sheng%20Tang%20and%20Jiawei%20Wei%20and%20Ruize%20Zhang%20and%20Juan%20Cao&entry.1292438233=%20%20In%20recent%20years%2C%20diffusion%20models%20have%20achieved%20tremendous%20success%20in%20the%0Afield%20of%20video%20generation%2C%20with%20controllable%20video%20generation%20receiving%0Asignificant%20attention.%20However%2C%20existing%20control%20methods%20still%20face%20two%0Alimitations%3A%20Firstly%2C%20control%20conditions%20%28such%20as%20depth%20maps%2C%203D%20Mesh%29%20are%0Adifficult%20for%20ordinary%20users%20to%20obtain%20directly.%20Secondly%2C%20it%27s%20challenging%20to%0Adrive%20multiple%20objects%20through%20complex%20motions%20with%20multiple%20trajectories%0Asimultaneously.%20In%20this%20paper%2C%20we%20introduce%20DragEntity%2C%20a%20video%20generation%0Amodel%20that%20utilizes%20entity%20representation%20for%20controlling%20the%20motion%20of%0Amultiple%20objects.%20Compared%20to%20previous%20methods%2C%20DragEntity%20offers%20two%20main%0Aadvantages%3A%201%29%20Our%20method%20is%20more%20user-friendly%20for%20interaction%20because%20it%0Aallows%20users%20to%20drag%20entities%20within%20the%20image%20rather%20than%20individual%20pixels.%0A2%29%20We%20use%20entity%20representation%20to%20represent%20any%20object%20in%20the%20image%2C%20and%0Amultiple%20objects%20can%20maintain%20relative%20spatial%20relationships.%20Therefore%2C%20we%0Aallow%20multiple%20trajectories%20to%20control%20multiple%20objects%20in%20the%20image%20with%0Adifferent%20levels%20of%20complexity%20simultaneously.%20Our%20experiments%20validate%20the%0Aeffectiveness%20of%20DragEntity%2C%20demonstrating%20its%20excellent%20performance%20in%0Afine-grained%20control%20in%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10751v1&entry.124074799=Read"},
{"title": "RICASSO: Reinforced Imbalance Learning with Class-Aware Self-Supervised\n  Outliers Exposure", "author": "Xuan Zhang and Sin Chee Chin and Tingxuan Gao and Wenming Yang", "abstract": "  In real-world scenarios, deep learning models often face challenges from both\nimbalanced (long-tailed) and out-of-distribution (OOD) data. However, existing\njoint methods rely on real OOD data, which leads to unnecessary trade-offs. In\ncontrast, our research shows that data mixing, a potent augmentation technique\nfor long-tailed recognition, can generate pseudo-OOD data that exhibit the\nfeatures of both in-distribution (ID) data and OOD data. Therefore, by using\nmixed data instead of real OOD data, we can address long-tailed recognition and\nOOD detection holistically. We propose a unified framework called Reinforced\nImbalance Learning with Class-Aware Self-Supervised Outliers Exposure\n(RICASSO), where \"self-supervised\" denotes that we only use ID data for outlier\nexposure. RICASSO includes three main strategies: Norm-Odd-Duality-Based\nOutlier Exposure: Uses mixed data as pseudo-OOD data, enabling simultaneous ID\ndata rebalancing and outlier exposure through a single loss function.\nAmbiguity-Aware Logits Adjustment: Utilizes the ambiguity of ID data to\nadaptively recalibrate logits. Contrastive Boundary-Center Learning: Combines\nVirtual Boundary Learning and Dual-Entropy Center Learning to use mixed data\nfor better feature separation and clustering, with Representation Consistency\nLearning for robustness. Extensive experiments demonstrate that RICASSO\nachieves state-of-the-art performance in long-tailed recognition and\nsignificantly improves OOD detection compared to our baseline (27% improvement\nin AUROC and 61% reduction in FPR on the iNaturalist2018 dataset). On\niNaturalist2018, we even outperforms methods using real OOD data. The code will\nbe made public soon.\n", "link": "http://arxiv.org/abs/2410.10548v1", "date": "2024-10-14", "relevancy": 2.7894, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6141}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RICASSO%3A%20Reinforced%20Imbalance%20Learning%20with%20Class-Aware%20Self-Supervised%0A%20%20Outliers%20Exposure&body=Title%3A%20RICASSO%3A%20Reinforced%20Imbalance%20Learning%20with%20Class-Aware%20Self-Supervised%0A%20%20Outliers%20Exposure%0AAuthor%3A%20Xuan%20Zhang%20and%20Sin%20Chee%20Chin%20and%20Tingxuan%20Gao%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20deep%20learning%20models%20often%20face%20challenges%20from%20both%0Aimbalanced%20%28long-tailed%29%20and%20out-of-distribution%20%28OOD%29%20data.%20However%2C%20existing%0Ajoint%20methods%20rely%20on%20real%20OOD%20data%2C%20which%20leads%20to%20unnecessary%20trade-offs.%20In%0Acontrast%2C%20our%20research%20shows%20that%20data%20mixing%2C%20a%20potent%20augmentation%20technique%0Afor%20long-tailed%20recognition%2C%20can%20generate%20pseudo-OOD%20data%20that%20exhibit%20the%0Afeatures%20of%20both%20in-distribution%20%28ID%29%20data%20and%20OOD%20data.%20Therefore%2C%20by%20using%0Amixed%20data%20instead%20of%20real%20OOD%20data%2C%20we%20can%20address%20long-tailed%20recognition%20and%0AOOD%20detection%20holistically.%20We%20propose%20a%20unified%20framework%20called%20Reinforced%0AImbalance%20Learning%20with%20Class-Aware%20Self-Supervised%20Outliers%20Exposure%0A%28RICASSO%29%2C%20where%20%22self-supervised%22%20denotes%20that%20we%20only%20use%20ID%20data%20for%20outlier%0Aexposure.%20RICASSO%20includes%20three%20main%20strategies%3A%20Norm-Odd-Duality-Based%0AOutlier%20Exposure%3A%20Uses%20mixed%20data%20as%20pseudo-OOD%20data%2C%20enabling%20simultaneous%20ID%0Adata%20rebalancing%20and%20outlier%20exposure%20through%20a%20single%20loss%20function.%0AAmbiguity-Aware%20Logits%20Adjustment%3A%20Utilizes%20the%20ambiguity%20of%20ID%20data%20to%0Aadaptively%20recalibrate%20logits.%20Contrastive%20Boundary-Center%20Learning%3A%20Combines%0AVirtual%20Boundary%20Learning%20and%20Dual-Entropy%20Center%20Learning%20to%20use%20mixed%20data%0Afor%20better%20feature%20separation%20and%20clustering%2C%20with%20Representation%20Consistency%0ALearning%20for%20robustness.%20Extensive%20experiments%20demonstrate%20that%20RICASSO%0Aachieves%20state-of-the-art%20performance%20in%20long-tailed%20recognition%20and%0Asignificantly%20improves%20OOD%20detection%20compared%20to%20our%20baseline%20%2827%25%20improvement%0Ain%20AUROC%20and%2061%25%20reduction%20in%20FPR%20on%20the%20iNaturalist2018%20dataset%29.%20On%0AiNaturalist2018%2C%20we%20even%20outperforms%20methods%20using%20real%20OOD%20data.%20The%20code%20will%0Abe%20made%20public%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRICASSO%253A%2520Reinforced%2520Imbalance%2520Learning%2520with%2520Class-Aware%2520Self-Supervised%250A%2520%2520Outliers%2520Exposure%26entry.906535625%3DXuan%2520Zhang%2520and%2520Sin%2520Chee%2520Chin%2520and%2520Tingxuan%2520Gao%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520deep%2520learning%2520models%2520often%2520face%2520challenges%2520from%2520both%250Aimbalanced%2520%2528long-tailed%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520However%252C%2520existing%250Ajoint%2520methods%2520rely%2520on%2520real%2520OOD%2520data%252C%2520which%2520leads%2520to%2520unnecessary%2520trade-offs.%2520In%250Acontrast%252C%2520our%2520research%2520shows%2520that%2520data%2520mixing%252C%2520a%2520potent%2520augmentation%2520technique%250Afor%2520long-tailed%2520recognition%252C%2520can%2520generate%2520pseudo-OOD%2520data%2520that%2520exhibit%2520the%250Afeatures%2520of%2520both%2520in-distribution%2520%2528ID%2529%2520data%2520and%2520OOD%2520data.%2520Therefore%252C%2520by%2520using%250Amixed%2520data%2520instead%2520of%2520real%2520OOD%2520data%252C%2520we%2520can%2520address%2520long-tailed%2520recognition%2520and%250AOOD%2520detection%2520holistically.%2520We%2520propose%2520a%2520unified%2520framework%2520called%2520Reinforced%250AImbalance%2520Learning%2520with%2520Class-Aware%2520Self-Supervised%2520Outliers%2520Exposure%250A%2528RICASSO%2529%252C%2520where%2520%2522self-supervised%2522%2520denotes%2520that%2520we%2520only%2520use%2520ID%2520data%2520for%2520outlier%250Aexposure.%2520RICASSO%2520includes%2520three%2520main%2520strategies%253A%2520Norm-Odd-Duality-Based%250AOutlier%2520Exposure%253A%2520Uses%2520mixed%2520data%2520as%2520pseudo-OOD%2520data%252C%2520enabling%2520simultaneous%2520ID%250Adata%2520rebalancing%2520and%2520outlier%2520exposure%2520through%2520a%2520single%2520loss%2520function.%250AAmbiguity-Aware%2520Logits%2520Adjustment%253A%2520Utilizes%2520the%2520ambiguity%2520of%2520ID%2520data%2520to%250Aadaptively%2520recalibrate%2520logits.%2520Contrastive%2520Boundary-Center%2520Learning%253A%2520Combines%250AVirtual%2520Boundary%2520Learning%2520and%2520Dual-Entropy%2520Center%2520Learning%2520to%2520use%2520mixed%2520data%250Afor%2520better%2520feature%2520separation%2520and%2520clustering%252C%2520with%2520Representation%2520Consistency%250ALearning%2520for%2520robustness.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RICASSO%250Aachieves%2520state-of-the-art%2520performance%2520in%2520long-tailed%2520recognition%2520and%250Asignificantly%2520improves%2520OOD%2520detection%2520compared%2520to%2520our%2520baseline%2520%252827%2525%2520improvement%250Ain%2520AUROC%2520and%252061%2525%2520reduction%2520in%2520FPR%2520on%2520the%2520iNaturalist2018%2520dataset%2529.%2520On%250AiNaturalist2018%252C%2520we%2520even%2520outperforms%2520methods%2520using%2520real%2520OOD%2520data.%2520The%2520code%2520will%250Abe%2520made%2520public%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RICASSO%3A%20Reinforced%20Imbalance%20Learning%20with%20Class-Aware%20Self-Supervised%0A%20%20Outliers%20Exposure&entry.906535625=Xuan%20Zhang%20and%20Sin%20Chee%20Chin%20and%20Tingxuan%20Gao%20and%20Wenming%20Yang&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20deep%20learning%20models%20often%20face%20challenges%20from%20both%0Aimbalanced%20%28long-tailed%29%20and%20out-of-distribution%20%28OOD%29%20data.%20However%2C%20existing%0Ajoint%20methods%20rely%20on%20real%20OOD%20data%2C%20which%20leads%20to%20unnecessary%20trade-offs.%20In%0Acontrast%2C%20our%20research%20shows%20that%20data%20mixing%2C%20a%20potent%20augmentation%20technique%0Afor%20long-tailed%20recognition%2C%20can%20generate%20pseudo-OOD%20data%20that%20exhibit%20the%0Afeatures%20of%20both%20in-distribution%20%28ID%29%20data%20and%20OOD%20data.%20Therefore%2C%20by%20using%0Amixed%20data%20instead%20of%20real%20OOD%20data%2C%20we%20can%20address%20long-tailed%20recognition%20and%0AOOD%20detection%20holistically.%20We%20propose%20a%20unified%20framework%20called%20Reinforced%0AImbalance%20Learning%20with%20Class-Aware%20Self-Supervised%20Outliers%20Exposure%0A%28RICASSO%29%2C%20where%20%22self-supervised%22%20denotes%20that%20we%20only%20use%20ID%20data%20for%20outlier%0Aexposure.%20RICASSO%20includes%20three%20main%20strategies%3A%20Norm-Odd-Duality-Based%0AOutlier%20Exposure%3A%20Uses%20mixed%20data%20as%20pseudo-OOD%20data%2C%20enabling%20simultaneous%20ID%0Adata%20rebalancing%20and%20outlier%20exposure%20through%20a%20single%20loss%20function.%0AAmbiguity-Aware%20Logits%20Adjustment%3A%20Utilizes%20the%20ambiguity%20of%20ID%20data%20to%0Aadaptively%20recalibrate%20logits.%20Contrastive%20Boundary-Center%20Learning%3A%20Combines%0AVirtual%20Boundary%20Learning%20and%20Dual-Entropy%20Center%20Learning%20to%20use%20mixed%20data%0Afor%20better%20feature%20separation%20and%20clustering%2C%20with%20Representation%20Consistency%0ALearning%20for%20robustness.%20Extensive%20experiments%20demonstrate%20that%20RICASSO%0Aachieves%20state-of-the-art%20performance%20in%20long-tailed%20recognition%20and%0Asignificantly%20improves%20OOD%20detection%20compared%20to%20our%20baseline%20%2827%25%20improvement%0Ain%20AUROC%20and%2061%25%20reduction%20in%20FPR%20on%20the%20iNaturalist2018%20dataset%29.%20On%0AiNaturalist2018%2C%20we%20even%20outperforms%20methods%20using%20real%20OOD%20data.%20The%20code%20will%0Abe%20made%20public%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10548v1&entry.124074799=Read"},
{"title": "Parameterize Structure with Differentiable Template for 3D Shape\n  Generation", "author": "Changfeng Ma and Pengxiao Guo and Shuangyu Yang and Yinuo Chen and Jie Guo and Chongjun Wang and Yanwen Guo and Wenping Wang", "abstract": "  Structural representation is crucial for reconstructing and generating\neditable 3D shapes with part semantics. Recent 3D shape generation works employ\ncomplicated networks and structure definitions relying on hierarchical\nannotations and pay less attention to the details inside parts. In this paper,\nwe propose the method that parameterizes the shared structure in the same\ncategory using a differentiable template and corresponding fixed-length\nparameters. Specific parameters are fed into the template to calculate cuboids\nthat indicate a concrete shape. We utilize the boundaries of three-view\ndrawings of each cuboid to further describe the inside details. Shapes are\nrepresented with the parameters and three-view details inside cuboids, from\nwhich the SDF can be calculated to recover the object. Benefiting from our\nfixed-length parameters and three-view details, our networks for reconstruction\nand generation are simple and effective to learn the latent space. Our method\ncan reconstruct or generate diverse shapes with complicated details, and\ninterpolate them smoothly. Extensive evaluations demonstrate the superiority of\nour method on reconstruction from point cloud, generation, and interpolation.\n", "link": "http://arxiv.org/abs/2410.10399v1", "date": "2024-10-14", "relevancy": 2.783, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5667}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameterize%20Structure%20with%20Differentiable%20Template%20for%203D%20Shape%0A%20%20Generation&body=Title%3A%20Parameterize%20Structure%20with%20Differentiable%20Template%20for%203D%20Shape%0A%20%20Generation%0AAuthor%3A%20Changfeng%20Ma%20and%20Pengxiao%20Guo%20and%20Shuangyu%20Yang%20and%20Yinuo%20Chen%20and%20Jie%20Guo%20and%20Chongjun%20Wang%20and%20Yanwen%20Guo%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20Structural%20representation%20is%20crucial%20for%20reconstructing%20and%20generating%0Aeditable%203D%20shapes%20with%20part%20semantics.%20Recent%203D%20shape%20generation%20works%20employ%0Acomplicated%20networks%20and%20structure%20definitions%20relying%20on%20hierarchical%0Aannotations%20and%20pay%20less%20attention%20to%20the%20details%20inside%20parts.%20In%20this%20paper%2C%0Awe%20propose%20the%20method%20that%20parameterizes%20the%20shared%20structure%20in%20the%20same%0Acategory%20using%20a%20differentiable%20template%20and%20corresponding%20fixed-length%0Aparameters.%20Specific%20parameters%20are%20fed%20into%20the%20template%20to%20calculate%20cuboids%0Athat%20indicate%20a%20concrete%20shape.%20We%20utilize%20the%20boundaries%20of%20three-view%0Adrawings%20of%20each%20cuboid%20to%20further%20describe%20the%20inside%20details.%20Shapes%20are%0Arepresented%20with%20the%20parameters%20and%20three-view%20details%20inside%20cuboids%2C%20from%0Awhich%20the%20SDF%20can%20be%20calculated%20to%20recover%20the%20object.%20Benefiting%20from%20our%0Afixed-length%20parameters%20and%20three-view%20details%2C%20our%20networks%20for%20reconstruction%0Aand%20generation%20are%20simple%20and%20effective%20to%20learn%20the%20latent%20space.%20Our%20method%0Acan%20reconstruct%20or%20generate%20diverse%20shapes%20with%20complicated%20details%2C%20and%0Ainterpolate%20them%20smoothly.%20Extensive%20evaluations%20demonstrate%20the%20superiority%20of%0Aour%20method%20on%20reconstruction%20from%20point%20cloud%2C%20generation%2C%20and%20interpolation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameterize%2520Structure%2520with%2520Differentiable%2520Template%2520for%25203D%2520Shape%250A%2520%2520Generation%26entry.906535625%3DChangfeng%2520Ma%2520and%2520Pengxiao%2520Guo%2520and%2520Shuangyu%2520Yang%2520and%2520Yinuo%2520Chen%2520and%2520Jie%2520Guo%2520and%2520Chongjun%2520Wang%2520and%2520Yanwen%2520Guo%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520Structural%2520representation%2520is%2520crucial%2520for%2520reconstructing%2520and%2520generating%250Aeditable%25203D%2520shapes%2520with%2520part%2520semantics.%2520Recent%25203D%2520shape%2520generation%2520works%2520employ%250Acomplicated%2520networks%2520and%2520structure%2520definitions%2520relying%2520on%2520hierarchical%250Aannotations%2520and%2520pay%2520less%2520attention%2520to%2520the%2520details%2520inside%2520parts.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520method%2520that%2520parameterizes%2520the%2520shared%2520structure%2520in%2520the%2520same%250Acategory%2520using%2520a%2520differentiable%2520template%2520and%2520corresponding%2520fixed-length%250Aparameters.%2520Specific%2520parameters%2520are%2520fed%2520into%2520the%2520template%2520to%2520calculate%2520cuboids%250Athat%2520indicate%2520a%2520concrete%2520shape.%2520We%2520utilize%2520the%2520boundaries%2520of%2520three-view%250Adrawings%2520of%2520each%2520cuboid%2520to%2520further%2520describe%2520the%2520inside%2520details.%2520Shapes%2520are%250Arepresented%2520with%2520the%2520parameters%2520and%2520three-view%2520details%2520inside%2520cuboids%252C%2520from%250Awhich%2520the%2520SDF%2520can%2520be%2520calculated%2520to%2520recover%2520the%2520object.%2520Benefiting%2520from%2520our%250Afixed-length%2520parameters%2520and%2520three-view%2520details%252C%2520our%2520networks%2520for%2520reconstruction%250Aand%2520generation%2520are%2520simple%2520and%2520effective%2520to%2520learn%2520the%2520latent%2520space.%2520Our%2520method%250Acan%2520reconstruct%2520or%2520generate%2520diverse%2520shapes%2520with%2520complicated%2520details%252C%2520and%250Ainterpolate%2520them%2520smoothly.%2520Extensive%2520evaluations%2520demonstrate%2520the%2520superiority%2520of%250Aour%2520method%2520on%2520reconstruction%2520from%2520point%2520cloud%252C%2520generation%252C%2520and%2520interpolation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameterize%20Structure%20with%20Differentiable%20Template%20for%203D%20Shape%0A%20%20Generation&entry.906535625=Changfeng%20Ma%20and%20Pengxiao%20Guo%20and%20Shuangyu%20Yang%20and%20Yinuo%20Chen%20and%20Jie%20Guo%20and%20Chongjun%20Wang%20and%20Yanwen%20Guo%20and%20Wenping%20Wang&entry.1292438233=%20%20Structural%20representation%20is%20crucial%20for%20reconstructing%20and%20generating%0Aeditable%203D%20shapes%20with%20part%20semantics.%20Recent%203D%20shape%20generation%20works%20employ%0Acomplicated%20networks%20and%20structure%20definitions%20relying%20on%20hierarchical%0Aannotations%20and%20pay%20less%20attention%20to%20the%20details%20inside%20parts.%20In%20this%20paper%2C%0Awe%20propose%20the%20method%20that%20parameterizes%20the%20shared%20structure%20in%20the%20same%0Acategory%20using%20a%20differentiable%20template%20and%20corresponding%20fixed-length%0Aparameters.%20Specific%20parameters%20are%20fed%20into%20the%20template%20to%20calculate%20cuboids%0Athat%20indicate%20a%20concrete%20shape.%20We%20utilize%20the%20boundaries%20of%20three-view%0Adrawings%20of%20each%20cuboid%20to%20further%20describe%20the%20inside%20details.%20Shapes%20are%0Arepresented%20with%20the%20parameters%20and%20three-view%20details%20inside%20cuboids%2C%20from%0Awhich%20the%20SDF%20can%20be%20calculated%20to%20recover%20the%20object.%20Benefiting%20from%20our%0Afixed-length%20parameters%20and%20three-view%20details%2C%20our%20networks%20for%20reconstruction%0Aand%20generation%20are%20simple%20and%20effective%20to%20learn%20the%20latent%20space.%20Our%20method%0Acan%20reconstruct%20or%20generate%20diverse%20shapes%20with%20complicated%20details%2C%20and%0Ainterpolate%20them%20smoothly.%20Extensive%20evaluations%20demonstrate%20the%20superiority%20of%0Aour%20method%20on%20reconstruction%20from%20point%20cloud%2C%20generation%2C%20and%20interpolation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10399v1&entry.124074799=Read"},
{"title": "TV-SAM: Increasing Zero-Shot Segmentation Performance on Multimodal\n  Medical Images Using GPT-4 Generated Descriptive Prompts Without Human\n  Annotation", "author": "Zekun Jiang and Dongjie Cheng and Ziyuan Qin and Jun Gao and Qicheng Lao and Abdullaev Bakhrom Ismoilovich and Urazboev Gayrat and Yuldashov Elyorbek and Bekchanov Habibullo and Defu Tang and LinJing Wei and Kang Li and Le Zhang", "abstract": "  This study presents a novel multimodal medical image zero-shot segmentation\nalgorithm named the text-visual-prompt segment anything model (TV-SAM) without\nany manual annotations. The TV-SAM incorporates and integrates the large\nlanguage model GPT-4, the vision language model GLIP, and the SAM to\nautonomously generate descriptive text prompts and visual bounding box prompts\nfrom medical images, thereby enhancing the SAM's capability for zero-shot\nsegmentation. Comprehensive evaluations are implemented on seven public\ndatasets encompassing eight imaging modalities to demonstrate that TV-SAM can\neffectively segment unseen targets across various modalities without additional\ntraining. TV-SAM significantly outperforms SAM AUTO and GSAM, closely matching\nthe performance of SAM BBOX with gold standard bounding box prompts and\nsurpasses the state-of-the-art methods on specific datasets such as ISIC and\nWBC. The study indicates that TV-SAM serves as an effective multimodal medical\nimage zero-shot segmentation algorithm, highlighting the significant\ncontribution of GPT-4 to zero-shot segmentation. By integrating foundational\nmodels such as GPT-4, GLIP, and SAM, the ability to address complex problems in\nspecialized domains can be enhanced.\n", "link": "http://arxiv.org/abs/2402.15759v2", "date": "2024-10-14", "relevancy": 2.7615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5505}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV-SAM%3A%20Increasing%20Zero-Shot%20Segmentation%20Performance%20on%20Multimodal%0A%20%20Medical%20Images%20Using%20GPT-4%20Generated%20Descriptive%20Prompts%20Without%20Human%0A%20%20Annotation&body=Title%3A%20TV-SAM%3A%20Increasing%20Zero-Shot%20Segmentation%20Performance%20on%20Multimodal%0A%20%20Medical%20Images%20Using%20GPT-4%20Generated%20Descriptive%20Prompts%20Without%20Human%0A%20%20Annotation%0AAuthor%3A%20Zekun%20Jiang%20and%20Dongjie%20Cheng%20and%20Ziyuan%20Qin%20and%20Jun%20Gao%20and%20Qicheng%20Lao%20and%20Abdullaev%20Bakhrom%20Ismoilovich%20and%20Urazboev%20Gayrat%20and%20Yuldashov%20Elyorbek%20and%20Bekchanov%20Habibullo%20and%20Defu%20Tang%20and%20LinJing%20Wei%20and%20Kang%20Li%20and%20Le%20Zhang%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20multimodal%20medical%20image%20zero-shot%20segmentation%0Aalgorithm%20named%20the%20text-visual-prompt%20segment%20anything%20model%20%28TV-SAM%29%20without%0Aany%20manual%20annotations.%20The%20TV-SAM%20incorporates%20and%20integrates%20the%20large%0Alanguage%20model%20GPT-4%2C%20the%20vision%20language%20model%20GLIP%2C%20and%20the%20SAM%20to%0Aautonomously%20generate%20descriptive%20text%20prompts%20and%20visual%20bounding%20box%20prompts%0Afrom%20medical%20images%2C%20thereby%20enhancing%20the%20SAM%27s%20capability%20for%20zero-shot%0Asegmentation.%20Comprehensive%20evaluations%20are%20implemented%20on%20seven%20public%0Adatasets%20encompassing%20eight%20imaging%20modalities%20to%20demonstrate%20that%20TV-SAM%20can%0Aeffectively%20segment%20unseen%20targets%20across%20various%20modalities%20without%20additional%0Atraining.%20TV-SAM%20significantly%20outperforms%20SAM%20AUTO%20and%20GSAM%2C%20closely%20matching%0Athe%20performance%20of%20SAM%20BBOX%20with%20gold%20standard%20bounding%20box%20prompts%20and%0Asurpasses%20the%20state-of-the-art%20methods%20on%20specific%20datasets%20such%20as%20ISIC%20and%0AWBC.%20The%20study%20indicates%20that%20TV-SAM%20serves%20as%20an%20effective%20multimodal%20medical%0Aimage%20zero-shot%20segmentation%20algorithm%2C%20highlighting%20the%20significant%0Acontribution%20of%20GPT-4%20to%20zero-shot%20segmentation.%20By%20integrating%20foundational%0Amodels%20such%20as%20GPT-4%2C%20GLIP%2C%20and%20SAM%2C%20the%20ability%20to%20address%20complex%20problems%20in%0Aspecialized%20domains%20can%20be%20enhanced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV-SAM%253A%2520Increasing%2520Zero-Shot%2520Segmentation%2520Performance%2520on%2520Multimodal%250A%2520%2520Medical%2520Images%2520Using%2520GPT-4%2520Generated%2520Descriptive%2520Prompts%2520Without%2520Human%250A%2520%2520Annotation%26entry.906535625%3DZekun%2520Jiang%2520and%2520Dongjie%2520Cheng%2520and%2520Ziyuan%2520Qin%2520and%2520Jun%2520Gao%2520and%2520Qicheng%2520Lao%2520and%2520Abdullaev%2520Bakhrom%2520Ismoilovich%2520and%2520Urazboev%2520Gayrat%2520and%2520Yuldashov%2520Elyorbek%2520and%2520Bekchanov%2520Habibullo%2520and%2520Defu%2520Tang%2520and%2520LinJing%2520Wei%2520and%2520Kang%2520Li%2520and%2520Le%2520Zhang%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520multimodal%2520medical%2520image%2520zero-shot%2520segmentation%250Aalgorithm%2520named%2520the%2520text-visual-prompt%2520segment%2520anything%2520model%2520%2528TV-SAM%2529%2520without%250Aany%2520manual%2520annotations.%2520The%2520TV-SAM%2520incorporates%2520and%2520integrates%2520the%2520large%250Alanguage%2520model%2520GPT-4%252C%2520the%2520vision%2520language%2520model%2520GLIP%252C%2520and%2520the%2520SAM%2520to%250Aautonomously%2520generate%2520descriptive%2520text%2520prompts%2520and%2520visual%2520bounding%2520box%2520prompts%250Afrom%2520medical%2520images%252C%2520thereby%2520enhancing%2520the%2520SAM%2527s%2520capability%2520for%2520zero-shot%250Asegmentation.%2520Comprehensive%2520evaluations%2520are%2520implemented%2520on%2520seven%2520public%250Adatasets%2520encompassing%2520eight%2520imaging%2520modalities%2520to%2520demonstrate%2520that%2520TV-SAM%2520can%250Aeffectively%2520segment%2520unseen%2520targets%2520across%2520various%2520modalities%2520without%2520additional%250Atraining.%2520TV-SAM%2520significantly%2520outperforms%2520SAM%2520AUTO%2520and%2520GSAM%252C%2520closely%2520matching%250Athe%2520performance%2520of%2520SAM%2520BBOX%2520with%2520gold%2520standard%2520bounding%2520box%2520prompts%2520and%250Asurpasses%2520the%2520state-of-the-art%2520methods%2520on%2520specific%2520datasets%2520such%2520as%2520ISIC%2520and%250AWBC.%2520The%2520study%2520indicates%2520that%2520TV-SAM%2520serves%2520as%2520an%2520effective%2520multimodal%2520medical%250Aimage%2520zero-shot%2520segmentation%2520algorithm%252C%2520highlighting%2520the%2520significant%250Acontribution%2520of%2520GPT-4%2520to%2520zero-shot%2520segmentation.%2520By%2520integrating%2520foundational%250Amodels%2520such%2520as%2520GPT-4%252C%2520GLIP%252C%2520and%2520SAM%252C%2520the%2520ability%2520to%2520address%2520complex%2520problems%2520in%250Aspecialized%2520domains%2520can%2520be%2520enhanced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV-SAM%3A%20Increasing%20Zero-Shot%20Segmentation%20Performance%20on%20Multimodal%0A%20%20Medical%20Images%20Using%20GPT-4%20Generated%20Descriptive%20Prompts%20Without%20Human%0A%20%20Annotation&entry.906535625=Zekun%20Jiang%20and%20Dongjie%20Cheng%20and%20Ziyuan%20Qin%20and%20Jun%20Gao%20and%20Qicheng%20Lao%20and%20Abdullaev%20Bakhrom%20Ismoilovich%20and%20Urazboev%20Gayrat%20and%20Yuldashov%20Elyorbek%20and%20Bekchanov%20Habibullo%20and%20Defu%20Tang%20and%20LinJing%20Wei%20and%20Kang%20Li%20and%20Le%20Zhang&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20multimodal%20medical%20image%20zero-shot%20segmentation%0Aalgorithm%20named%20the%20text-visual-prompt%20segment%20anything%20model%20%28TV-SAM%29%20without%0Aany%20manual%20annotations.%20The%20TV-SAM%20incorporates%20and%20integrates%20the%20large%0Alanguage%20model%20GPT-4%2C%20the%20vision%20language%20model%20GLIP%2C%20and%20the%20SAM%20to%0Aautonomously%20generate%20descriptive%20text%20prompts%20and%20visual%20bounding%20box%20prompts%0Afrom%20medical%20images%2C%20thereby%20enhancing%20the%20SAM%27s%20capability%20for%20zero-shot%0Asegmentation.%20Comprehensive%20evaluations%20are%20implemented%20on%20seven%20public%0Adatasets%20encompassing%20eight%20imaging%20modalities%20to%20demonstrate%20that%20TV-SAM%20can%0Aeffectively%20segment%20unseen%20targets%20across%20various%20modalities%20without%20additional%0Atraining.%20TV-SAM%20significantly%20outperforms%20SAM%20AUTO%20and%20GSAM%2C%20closely%20matching%0Athe%20performance%20of%20SAM%20BBOX%20with%20gold%20standard%20bounding%20box%20prompts%20and%0Asurpasses%20the%20state-of-the-art%20methods%20on%20specific%20datasets%20such%20as%20ISIC%20and%0AWBC.%20The%20study%20indicates%20that%20TV-SAM%20serves%20as%20an%20effective%20multimodal%20medical%0Aimage%20zero-shot%20segmentation%20algorithm%2C%20highlighting%20the%20significant%0Acontribution%20of%20GPT-4%20to%20zero-shot%20segmentation.%20By%20integrating%20foundational%0Amodels%20such%20as%20GPT-4%2C%20GLIP%2C%20and%20SAM%2C%20the%20ability%20to%20address%20complex%20problems%20in%0Aspecialized%20domains%20can%20be%20enhanced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15759v2&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Networks", "author": "Arturs Berzins and Andreas Radler and Eric Volkmann and Sebastian Sanokowski and Sepp Hochreiter and Johannes Brandstetter", "abstract": "  Geometry is a ubiquitous tool in computer graphics, design, and engineering.\nHowever, the lack of large shape datasets limits the application of\nstate-of-the-art supervised learning methods and motivates the exploration of\nalternative learning strategies. To this end, we introduce geometry-informed\nneural networks (GINNs) -- a framework for training shape-generative neural\nfields without data by leveraging user-specified design requirements in the\nform of objectives and constraints. By adding diversity as an explicit\nconstraint, GINNs avoid mode-collapse and can generate multiple diverse\nsolutions, often required in geometry tasks. Experimentally, we apply GINNs to\nseveral validation problems and a realistic 3D engineering design problem,\nshowing control over geometrical and topological properties, such as surface\nsmoothness or the number of holes. These results demonstrate the potential of\ntraining shape-generative models without data, paving the way for new\ngenerative design approaches without large datasets.\n", "link": "http://arxiv.org/abs/2402.14009v3", "date": "2024-10-14", "relevancy": 2.7349, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5583}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Networks&body=Title%3A%20Geometry-Informed%20Neural%20Networks%0AAuthor%3A%20Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Eric%20Volkmann%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Geometry%20is%20a%20ubiquitous%20tool%20in%20computer%20graphics%2C%20design%2C%20and%20engineering.%0AHowever%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%20of%0Astate-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%20of%0Aalternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20--%20a%20framework%20for%20training%20shape-generative%20neural%0Afields%20without%20data%20by%20leveraging%20user-specified%20design%20requirements%20in%20the%0Aform%20of%20objectives%20and%20constraints.%20By%20adding%20diversity%20as%20an%20explicit%0Aconstraint%2C%20GINNs%20avoid%20mode-collapse%20and%20can%20generate%20multiple%20diverse%0Asolutions%2C%20often%20required%20in%20geometry%20tasks.%20Experimentally%2C%20we%20apply%20GINNs%20to%0Aseveral%20validation%20problems%20and%20a%20realistic%203D%20engineering%20design%20problem%2C%0Ashowing%20control%20over%20geometrical%20and%20topological%20properties%2C%20such%20as%20surface%0Asmoothness%20or%20the%20number%20of%20holes.%20These%20results%20demonstrate%20the%20potential%20of%0Atraining%20shape-generative%20models%20without%20data%2C%20paving%20the%20way%20for%20new%0Agenerative%20design%20approaches%20without%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Networks%26entry.906535625%3DArturs%2520Berzins%2520and%2520Andreas%2520Radler%2520and%2520Eric%2520Volkmann%2520and%2520Sebastian%2520Sanokowski%2520and%2520Sepp%2520Hochreiter%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Geometry%2520is%2520a%2520ubiquitous%2520tool%2520in%2520computer%2520graphics%252C%2520design%252C%2520and%2520engineering.%250AHowever%252C%2520the%2520lack%2520of%2520large%2520shape%2520datasets%2520limits%2520the%2520application%2520of%250Astate-of-the-art%2520supervised%2520learning%2520methods%2520and%2520motivates%2520the%2520exploration%2520of%250Aalternative%2520learning%2520strategies.%2520To%2520this%2520end%252C%2520we%2520introduce%2520geometry-informed%250Aneural%2520networks%2520%2528GINNs%2529%2520--%2520a%2520framework%2520for%2520training%2520shape-generative%2520neural%250Afields%2520without%2520data%2520by%2520leveraging%2520user-specified%2520design%2520requirements%2520in%2520the%250Aform%2520of%2520objectives%2520and%2520constraints.%2520By%2520adding%2520diversity%2520as%2520an%2520explicit%250Aconstraint%252C%2520GINNs%2520avoid%2520mode-collapse%2520and%2520can%2520generate%2520multiple%2520diverse%250Asolutions%252C%2520often%2520required%2520in%2520geometry%2520tasks.%2520Experimentally%252C%2520we%2520apply%2520GINNs%2520to%250Aseveral%2520validation%2520problems%2520and%2520a%2520realistic%25203D%2520engineering%2520design%2520problem%252C%250Ashowing%2520control%2520over%2520geometrical%2520and%2520topological%2520properties%252C%2520such%2520as%2520surface%250Asmoothness%2520or%2520the%2520number%2520of%2520holes.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%250Atraining%2520shape-generative%2520models%2520without%2520data%252C%2520paving%2520the%2520way%2520for%2520new%250Agenerative%2520design%2520approaches%2520without%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Networks&entry.906535625=Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Eric%20Volkmann%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Geometry%20is%20a%20ubiquitous%20tool%20in%20computer%20graphics%2C%20design%2C%20and%20engineering.%0AHowever%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%20of%0Astate-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%20of%0Aalternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20--%20a%20framework%20for%20training%20shape-generative%20neural%0Afields%20without%20data%20by%20leveraging%20user-specified%20design%20requirements%20in%20the%0Aform%20of%20objectives%20and%20constraints.%20By%20adding%20diversity%20as%20an%20explicit%0Aconstraint%2C%20GINNs%20avoid%20mode-collapse%20and%20can%20generate%20multiple%20diverse%0Asolutions%2C%20often%20required%20in%20geometry%20tasks.%20Experimentally%2C%20we%20apply%20GINNs%20to%0Aseveral%20validation%20problems%20and%20a%20realistic%203D%20engineering%20design%20problem%2C%0Ashowing%20control%20over%20geometrical%20and%20topological%20properties%2C%20such%20as%20surface%0Asmoothness%20or%20the%20number%20of%20holes.%20These%20results%20demonstrate%20the%20potential%20of%0Atraining%20shape-generative%20models%20without%20data%2C%20paving%20the%20way%20for%20new%0Agenerative%20design%20approaches%20without%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14009v3&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Networks", "author": "Arturs Berzins and Andreas Radler and Eric Volkmann and Sebastian Sanokowski and Sepp Hochreiter and Johannes Brandstetter", "abstract": "  Geometry is a ubiquitous tool in computer graphics, design, and engineering.\nHowever, the lack of large shape datasets limits the application of\nstate-of-the-art supervised learning methods and motivates the exploration of\nalternative learning strategies. To this end, we introduce geometry-informed\nneural networks (GINNs) -- a framework for training shape-generative neural\nfields without data by leveraging user-specified design requirements in the\nform of objectives and constraints. By adding diversity as an explicit\nconstraint, GINNs avoid mode-collapse and can generate multiple diverse\nsolutions, often required in geometry tasks. Experimentally, we apply GINNs to\nseveral validation problems and a realistic 3D engineering design problem,\nshowing control over geometrical and topological properties, such as surface\nsmoothness or the number of holes. These results demonstrate the potential of\ntraining shape-generative models without data, paving the way for new\ngenerative design approaches without large datasets.\n", "link": "http://arxiv.org/abs/2402.14009v3", "date": "2024-10-14", "relevancy": 2.7349, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5583}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Networks&body=Title%3A%20Geometry-Informed%20Neural%20Networks%0AAuthor%3A%20Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Eric%20Volkmann%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Geometry%20is%20a%20ubiquitous%20tool%20in%20computer%20graphics%2C%20design%2C%20and%20engineering.%0AHowever%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%20of%0Astate-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%20of%0Aalternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20--%20a%20framework%20for%20training%20shape-generative%20neural%0Afields%20without%20data%20by%20leveraging%20user-specified%20design%20requirements%20in%20the%0Aform%20of%20objectives%20and%20constraints.%20By%20adding%20diversity%20as%20an%20explicit%0Aconstraint%2C%20GINNs%20avoid%20mode-collapse%20and%20can%20generate%20multiple%20diverse%0Asolutions%2C%20often%20required%20in%20geometry%20tasks.%20Experimentally%2C%20we%20apply%20GINNs%20to%0Aseveral%20validation%20problems%20and%20a%20realistic%203D%20engineering%20design%20problem%2C%0Ashowing%20control%20over%20geometrical%20and%20topological%20properties%2C%20such%20as%20surface%0Asmoothness%20or%20the%20number%20of%20holes.%20These%20results%20demonstrate%20the%20potential%20of%0Atraining%20shape-generative%20models%20without%20data%2C%20paving%20the%20way%20for%20new%0Agenerative%20design%20approaches%20without%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Networks%26entry.906535625%3DArturs%2520Berzins%2520and%2520Andreas%2520Radler%2520and%2520Eric%2520Volkmann%2520and%2520Sebastian%2520Sanokowski%2520and%2520Sepp%2520Hochreiter%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Geometry%2520is%2520a%2520ubiquitous%2520tool%2520in%2520computer%2520graphics%252C%2520design%252C%2520and%2520engineering.%250AHowever%252C%2520the%2520lack%2520of%2520large%2520shape%2520datasets%2520limits%2520the%2520application%2520of%250Astate-of-the-art%2520supervised%2520learning%2520methods%2520and%2520motivates%2520the%2520exploration%2520of%250Aalternative%2520learning%2520strategies.%2520To%2520this%2520end%252C%2520we%2520introduce%2520geometry-informed%250Aneural%2520networks%2520%2528GINNs%2529%2520--%2520a%2520framework%2520for%2520training%2520shape-generative%2520neural%250Afields%2520without%2520data%2520by%2520leveraging%2520user-specified%2520design%2520requirements%2520in%2520the%250Aform%2520of%2520objectives%2520and%2520constraints.%2520By%2520adding%2520diversity%2520as%2520an%2520explicit%250Aconstraint%252C%2520GINNs%2520avoid%2520mode-collapse%2520and%2520can%2520generate%2520multiple%2520diverse%250Asolutions%252C%2520often%2520required%2520in%2520geometry%2520tasks.%2520Experimentally%252C%2520we%2520apply%2520GINNs%2520to%250Aseveral%2520validation%2520problems%2520and%2520a%2520realistic%25203D%2520engineering%2520design%2520problem%252C%250Ashowing%2520control%2520over%2520geometrical%2520and%2520topological%2520properties%252C%2520such%2520as%2520surface%250Asmoothness%2520or%2520the%2520number%2520of%2520holes.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%250Atraining%2520shape-generative%2520models%2520without%2520data%252C%2520paving%2520the%2520way%2520for%2520new%250Agenerative%2520design%2520approaches%2520without%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Networks&entry.906535625=Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Eric%20Volkmann%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Geometry%20is%20a%20ubiquitous%20tool%20in%20computer%20graphics%2C%20design%2C%20and%20engineering.%0AHowever%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%20of%0Astate-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%20of%0Aalternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20--%20a%20framework%20for%20training%20shape-generative%20neural%0Afields%20without%20data%20by%20leveraging%20user-specified%20design%20requirements%20in%20the%0Aform%20of%20objectives%20and%20constraints.%20By%20adding%20diversity%20as%20an%20explicit%0Aconstraint%2C%20GINNs%20avoid%20mode-collapse%20and%20can%20generate%20multiple%20diverse%0Asolutions%2C%20often%20required%20in%20geometry%20tasks.%20Experimentally%2C%20we%20apply%20GINNs%20to%0Aseveral%20validation%20problems%20and%20a%20realistic%203D%20engineering%20design%20problem%2C%0Ashowing%20control%20over%20geometrical%20and%20topological%20properties%2C%20such%20as%20surface%0Asmoothness%20or%20the%20number%20of%20holes.%20These%20results%20demonstrate%20the%20potential%20of%0Atraining%20shape-generative%20models%20without%20data%2C%20paving%20the%20way%20for%20new%0Agenerative%20design%20approaches%20without%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14009v3&entry.124074799=Read"},
{"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?", "author": "Gabriel Roccabruna and Massimo Rizzoli and Giuseppe Riccardi", "abstract": "  The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.\n", "link": "http://arxiv.org/abs/2410.10476v1", "date": "2024-10-14", "relevancy": 2.6801, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F&body=Title%3A%20Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F%0AAuthor%3A%20Gabriel%20Roccabruna%20and%20Massimo%20Rizzoli%20and%20Giuseppe%20Riccardi%0AAbstract%3A%20%20%20The%20automatic%20detection%20of%20temporal%20relations%20among%20events%20has%20been%20mainly%0Ainvestigated%20with%20encoder-only%20models%20such%20as%20RoBERTa.%20Large%20Language%20Models%0A%28LLM%29%20have%20recently%20shown%20promising%20performance%20in%20temporal%20reasoning%20tasks%0Asuch%20as%20temporal%20question%20answering.%20Nevertheless%2C%20recent%20studies%20have%20tested%0Athe%20LLMs%27%20performance%20in%20detecting%20temporal%20relations%20of%20closed-source%20models%0Aonly%2C%20limiting%20the%20interpretability%20of%20those%20results.%20In%20this%20work%2C%20we%0Ainvestigate%20LLMs%27%20performance%20and%20decision%20process%20in%20the%20Temporal%20Relation%0AClassification%20task.%20First%2C%20we%20assess%20the%20performance%20of%20seven%20open%20and%0Aclosed-sourced%20LLMs%20experimenting%20with%20in-context%20learning%20and%20lightweight%0Afine-tuning%20approaches.%20Results%20show%20that%20LLMs%20with%20in-context%20learning%0Asignificantly%20underperform%20smaller%20encoder-only%20models%20based%20on%20RoBERTa.%20Then%2C%0Awe%20delve%20into%20the%20possible%20reasons%20for%20this%20gap%20by%20applying%20explainable%0Amethods.%20The%20outcome%20suggests%20a%20limitation%20of%20LLMs%20in%20this%20task%20due%20to%20their%0Aautoregressive%20nature%2C%20which%20causes%20them%20to%20focus%20only%20on%20the%20last%20part%20of%20the%0Asequence.%20Additionally%2C%20we%20evaluate%20the%20word%20embeddings%20of%20these%20two%20models%20to%0Abetter%20understand%20their%20pre-training%20differences.%20The%20code%20and%20the%20fine-tuned%0Amodels%20can%20be%20found%20respectively%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWill%2520LLMs%2520Replace%2520the%2520Encoder-Only%2520Models%2520in%2520Temporal%2520Relation%250A%2520%2520Classification%253F%26entry.906535625%3DGabriel%2520Roccabruna%2520and%2520Massimo%2520Rizzoli%2520and%2520Giuseppe%2520Riccardi%26entry.1292438233%3D%2520%2520The%2520automatic%2520detection%2520of%2520temporal%2520relations%2520among%2520events%2520has%2520been%2520mainly%250Ainvestigated%2520with%2520encoder-only%2520models%2520such%2520as%2520RoBERTa.%2520Large%2520Language%2520Models%250A%2528LLM%2529%2520have%2520recently%2520shown%2520promising%2520performance%2520in%2520temporal%2520reasoning%2520tasks%250Asuch%2520as%2520temporal%2520question%2520answering.%2520Nevertheless%252C%2520recent%2520studies%2520have%2520tested%250Athe%2520LLMs%2527%2520performance%2520in%2520detecting%2520temporal%2520relations%2520of%2520closed-source%2520models%250Aonly%252C%2520limiting%2520the%2520interpretability%2520of%2520those%2520results.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520LLMs%2527%2520performance%2520and%2520decision%2520process%2520in%2520the%2520Temporal%2520Relation%250AClassification%2520task.%2520First%252C%2520we%2520assess%2520the%2520performance%2520of%2520seven%2520open%2520and%250Aclosed-sourced%2520LLMs%2520experimenting%2520with%2520in-context%2520learning%2520and%2520lightweight%250Afine-tuning%2520approaches.%2520Results%2520show%2520that%2520LLMs%2520with%2520in-context%2520learning%250Asignificantly%2520underperform%2520smaller%2520encoder-only%2520models%2520based%2520on%2520RoBERTa.%2520Then%252C%250Awe%2520delve%2520into%2520the%2520possible%2520reasons%2520for%2520this%2520gap%2520by%2520applying%2520explainable%250Amethods.%2520The%2520outcome%2520suggests%2520a%2520limitation%2520of%2520LLMs%2520in%2520this%2520task%2520due%2520to%2520their%250Aautoregressive%2520nature%252C%2520which%2520causes%2520them%2520to%2520focus%2520only%2520on%2520the%2520last%2520part%2520of%2520the%250Asequence.%2520Additionally%252C%2520we%2520evaluate%2520the%2520word%2520embeddings%2520of%2520these%2520two%2520models%2520to%250Abetter%2520understand%2520their%2520pre-training%2520differences.%2520The%2520code%2520and%2520the%2520fine-tuned%250Amodels%2520can%2520be%2520found%2520respectively%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F&entry.906535625=Gabriel%20Roccabruna%20and%20Massimo%20Rizzoli%20and%20Giuseppe%20Riccardi&entry.1292438233=%20%20The%20automatic%20detection%20of%20temporal%20relations%20among%20events%20has%20been%20mainly%0Ainvestigated%20with%20encoder-only%20models%20such%20as%20RoBERTa.%20Large%20Language%20Models%0A%28LLM%29%20have%20recently%20shown%20promising%20performance%20in%20temporal%20reasoning%20tasks%0Asuch%20as%20temporal%20question%20answering.%20Nevertheless%2C%20recent%20studies%20have%20tested%0Athe%20LLMs%27%20performance%20in%20detecting%20temporal%20relations%20of%20closed-source%20models%0Aonly%2C%20limiting%20the%20interpretability%20of%20those%20results.%20In%20this%20work%2C%20we%0Ainvestigate%20LLMs%27%20performance%20and%20decision%20process%20in%20the%20Temporal%20Relation%0AClassification%20task.%20First%2C%20we%20assess%20the%20performance%20of%20seven%20open%20and%0Aclosed-sourced%20LLMs%20experimenting%20with%20in-context%20learning%20and%20lightweight%0Afine-tuning%20approaches.%20Results%20show%20that%20LLMs%20with%20in-context%20learning%0Asignificantly%20underperform%20smaller%20encoder-only%20models%20based%20on%20RoBERTa.%20Then%2C%0Awe%20delve%20into%20the%20possible%20reasons%20for%20this%20gap%20by%20applying%20explainable%0Amethods.%20The%20outcome%20suggests%20a%20limitation%20of%20LLMs%20in%20this%20task%20due%20to%20their%0Aautoregressive%20nature%2C%20which%20causes%20them%20to%20focus%20only%20on%20the%20last%20part%20of%20the%0Asequence.%20Additionally%2C%20we%20evaluate%20the%20word%20embeddings%20of%20these%20two%20models%20to%0Abetter%20understand%20their%20pre-training%20differences.%20The%20code%20and%20the%20fine-tuned%0Amodels%20can%20be%20found%20respectively%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10476v1&entry.124074799=Read"},
{"title": "Queryable Prototype Multiple Instance Learning with Vision-Language\n  Models for Incremental Whole Slide Image Classification", "author": "Jiaxiang Gou and Luping Ji and Pei Liu and Mao Ye", "abstract": "  Whole Slide Image (WSI) classification has very significant applications in\nclinical pathology, e.g., tumor identification and cancer diagnosis. Currently,\nmost research attention is focused on Multiple Instance Learning (MIL) using\nstatic datasets. One of the most obvious weaknesses of these methods is that\nthey cannot efficiently preserve and utilize previously learned knowledge. With\nany new data arriving, classification models are required to be re-trained on\nboth previous and current new data. To overcome this shortcoming and break\nthrough traditional vision modality, this paper proposes the first\nVision-Language-based framework with Queryable Prototype Multiple Instance\nLearning (QPMIL-VL) specially designed for incremental WSI classification. This\nframework mainly consists of two information processing branches. One is for\ngenerating the bag-level feature by prototype-guided aggregating on the\ninstance features. While the other is for enhancing the class feature through\nclass ensemble, tunable vector and class similarity loss. The experiments on\nfour TCGA datasets demonstrate that our QPMIL-VL framework is effective for\nincremental WSI classification and often significantly outperforms other\ncompared methods, achieving state-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2410.10573v1", "date": "2024-10-14", "relevancy": 2.6663, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Queryable%20Prototype%20Multiple%20Instance%20Learning%20with%20Vision-Language%0A%20%20Models%20for%20Incremental%20Whole%20Slide%20Image%20Classification&body=Title%3A%20Queryable%20Prototype%20Multiple%20Instance%20Learning%20with%20Vision-Language%0A%20%20Models%20for%20Incremental%20Whole%20Slide%20Image%20Classification%0AAuthor%3A%20Jiaxiang%20Gou%20and%20Luping%20Ji%20and%20Pei%20Liu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20has%20very%20significant%20applications%20in%0Aclinical%20pathology%2C%20e.g.%2C%20tumor%20identification%20and%20cancer%20diagnosis.%20Currently%2C%0Amost%20research%20attention%20is%20focused%20on%20Multiple%20Instance%20Learning%20%28MIL%29%20using%0Astatic%20datasets.%20One%20of%20the%20most%20obvious%20weaknesses%20of%20these%20methods%20is%20that%0Athey%20cannot%20efficiently%20preserve%20and%20utilize%20previously%20learned%20knowledge.%20With%0Aany%20new%20data%20arriving%2C%20classification%20models%20are%20required%20to%20be%20re-trained%20on%0Aboth%20previous%20and%20current%20new%20data.%20To%20overcome%20this%20shortcoming%20and%20break%0Athrough%20traditional%20vision%20modality%2C%20this%20paper%20proposes%20the%20first%0AVision-Language-based%20framework%20with%20Queryable%20Prototype%20Multiple%20Instance%0ALearning%20%28QPMIL-VL%29%20specially%20designed%20for%20incremental%20WSI%20classification.%20This%0Aframework%20mainly%20consists%20of%20two%20information%20processing%20branches.%20One%20is%20for%0Agenerating%20the%20bag-level%20feature%20by%20prototype-guided%20aggregating%20on%20the%0Ainstance%20features.%20While%20the%20other%20is%20for%20enhancing%20the%20class%20feature%20through%0Aclass%20ensemble%2C%20tunable%20vector%20and%20class%20similarity%20loss.%20The%20experiments%20on%0Afour%20TCGA%20datasets%20demonstrate%20that%20our%20QPMIL-VL%20framework%20is%20effective%20for%0Aincremental%20WSI%20classification%20and%20often%20significantly%20outperforms%20other%0Acompared%20methods%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueryable%2520Prototype%2520Multiple%2520Instance%2520Learning%2520with%2520Vision-Language%250A%2520%2520Models%2520for%2520Incremental%2520Whole%2520Slide%2520Image%2520Classification%26entry.906535625%3DJiaxiang%2520Gou%2520and%2520Luping%2520Ji%2520and%2520Pei%2520Liu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520classification%2520has%2520very%2520significant%2520applications%2520in%250Aclinical%2520pathology%252C%2520e.g.%252C%2520tumor%2520identification%2520and%2520cancer%2520diagnosis.%2520Currently%252C%250Amost%2520research%2520attention%2520is%2520focused%2520on%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520using%250Astatic%2520datasets.%2520One%2520of%2520the%2520most%2520obvious%2520weaknesses%2520of%2520these%2520methods%2520is%2520that%250Athey%2520cannot%2520efficiently%2520preserve%2520and%2520utilize%2520previously%2520learned%2520knowledge.%2520With%250Aany%2520new%2520data%2520arriving%252C%2520classification%2520models%2520are%2520required%2520to%2520be%2520re-trained%2520on%250Aboth%2520previous%2520and%2520current%2520new%2520data.%2520To%2520overcome%2520this%2520shortcoming%2520and%2520break%250Athrough%2520traditional%2520vision%2520modality%252C%2520this%2520paper%2520proposes%2520the%2520first%250AVision-Language-based%2520framework%2520with%2520Queryable%2520Prototype%2520Multiple%2520Instance%250ALearning%2520%2528QPMIL-VL%2529%2520specially%2520designed%2520for%2520incremental%2520WSI%2520classification.%2520This%250Aframework%2520mainly%2520consists%2520of%2520two%2520information%2520processing%2520branches.%2520One%2520is%2520for%250Agenerating%2520the%2520bag-level%2520feature%2520by%2520prototype-guided%2520aggregating%2520on%2520the%250Ainstance%2520features.%2520While%2520the%2520other%2520is%2520for%2520enhancing%2520the%2520class%2520feature%2520through%250Aclass%2520ensemble%252C%2520tunable%2520vector%2520and%2520class%2520similarity%2520loss.%2520The%2520experiments%2520on%250Afour%2520TCGA%2520datasets%2520demonstrate%2520that%2520our%2520QPMIL-VL%2520framework%2520is%2520effective%2520for%250Aincremental%2520WSI%2520classification%2520and%2520often%2520significantly%2520outperforms%2520other%250Acompared%2520methods%252C%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Queryable%20Prototype%20Multiple%20Instance%20Learning%20with%20Vision-Language%0A%20%20Models%20for%20Incremental%20Whole%20Slide%20Image%20Classification&entry.906535625=Jiaxiang%20Gou%20and%20Luping%20Ji%20and%20Pei%20Liu%20and%20Mao%20Ye&entry.1292438233=%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20has%20very%20significant%20applications%20in%0Aclinical%20pathology%2C%20e.g.%2C%20tumor%20identification%20and%20cancer%20diagnosis.%20Currently%2C%0Amost%20research%20attention%20is%20focused%20on%20Multiple%20Instance%20Learning%20%28MIL%29%20using%0Astatic%20datasets.%20One%20of%20the%20most%20obvious%20weaknesses%20of%20these%20methods%20is%20that%0Athey%20cannot%20efficiently%20preserve%20and%20utilize%20previously%20learned%20knowledge.%20With%0Aany%20new%20data%20arriving%2C%20classification%20models%20are%20required%20to%20be%20re-trained%20on%0Aboth%20previous%20and%20current%20new%20data.%20To%20overcome%20this%20shortcoming%20and%20break%0Athrough%20traditional%20vision%20modality%2C%20this%20paper%20proposes%20the%20first%0AVision-Language-based%20framework%20with%20Queryable%20Prototype%20Multiple%20Instance%0ALearning%20%28QPMIL-VL%29%20specially%20designed%20for%20incremental%20WSI%20classification.%20This%0Aframework%20mainly%20consists%20of%20two%20information%20processing%20branches.%20One%20is%20for%0Agenerating%20the%20bag-level%20feature%20by%20prototype-guided%20aggregating%20on%20the%0Ainstance%20features.%20While%20the%20other%20is%20for%20enhancing%20the%20class%20feature%20through%0Aclass%20ensemble%2C%20tunable%20vector%20and%20class%20similarity%20loss.%20The%20experiments%20on%0Afour%20TCGA%20datasets%20demonstrate%20that%20our%20QPMIL-VL%20framework%20is%20effective%20for%0Aincremental%20WSI%20classification%20and%20often%20significantly%20outperforms%20other%0Acompared%20methods%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10573v1&entry.124074799=Read"},
{"title": "On Representation of 3D Rotation in the Context of Deep Learning", "author": "Vikt\u00f3ria Pravdov\u00e1 and Luk\u00e1\u0161 Gajdo\u0161ech and Hassan Ali and Viktor Kocur", "abstract": "  This paper investigates various methods of representing 3D rotations and\ntheir impact on the learning process of deep neural networks. We evaluated the\nperformance of ResNet18 networks for 3D rotation estimation using several\nrotation representations and loss functions on both synthetic and real data.\nThe real datasets contained 3D scans of industrial bins, while the synthetic\ndatasets included views of a simple asymmetric object rendered under different\nrotations. On synthetic data, we also assessed the effects of different\nrotation distributions within the training and test sets, as well as the impact\nof the object's texture. In line with previous research, we found that networks\nusing the continuous 5D and 6D representations performed better than the\ndiscontinuous ones.\n", "link": "http://arxiv.org/abs/2410.10350v1", "date": "2024-10-14", "relevancy": 2.6656, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Representation%20of%203D%20Rotation%20in%20the%20Context%20of%20Deep%20Learning&body=Title%3A%20On%20Representation%20of%203D%20Rotation%20in%20the%20Context%20of%20Deep%20Learning%0AAuthor%3A%20Vikt%C3%B3ria%20Pravdov%C3%A1%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Hassan%20Ali%20and%20Viktor%20Kocur%0AAbstract%3A%20%20%20This%20paper%20investigates%20various%20methods%20of%20representing%203D%20rotations%20and%0Atheir%20impact%20on%20the%20learning%20process%20of%20deep%20neural%20networks.%20We%20evaluated%20the%0Aperformance%20of%20ResNet18%20networks%20for%203D%20rotation%20estimation%20using%20several%0Arotation%20representations%20and%20loss%20functions%20on%20both%20synthetic%20and%20real%20data.%0AThe%20real%20datasets%20contained%203D%20scans%20of%20industrial%20bins%2C%20while%20the%20synthetic%0Adatasets%20included%20views%20of%20a%20simple%20asymmetric%20object%20rendered%20under%20different%0Arotations.%20On%20synthetic%20data%2C%20we%20also%20assessed%20the%20effects%20of%20different%0Arotation%20distributions%20within%20the%20training%20and%20test%20sets%2C%20as%20well%20as%20the%20impact%0Aof%20the%20object%27s%20texture.%20In%20line%20with%20previous%20research%2C%20we%20found%20that%20networks%0Ausing%20the%20continuous%205D%20and%206D%20representations%20performed%20better%20than%20the%0Adiscontinuous%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Representation%2520of%25203D%2520Rotation%2520in%2520the%2520Context%2520of%2520Deep%2520Learning%26entry.906535625%3DVikt%25C3%25B3ria%2520Pravdov%25C3%25A1%2520and%2520Luk%25C3%25A1%25C5%25A1%2520Gajdo%25C5%25A1ech%2520and%2520Hassan%2520Ali%2520and%2520Viktor%2520Kocur%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520various%2520methods%2520of%2520representing%25203D%2520rotations%2520and%250Atheir%2520impact%2520on%2520the%2520learning%2520process%2520of%2520deep%2520neural%2520networks.%2520We%2520evaluated%2520the%250Aperformance%2520of%2520ResNet18%2520networks%2520for%25203D%2520rotation%2520estimation%2520using%2520several%250Arotation%2520representations%2520and%2520loss%2520functions%2520on%2520both%2520synthetic%2520and%2520real%2520data.%250AThe%2520real%2520datasets%2520contained%25203D%2520scans%2520of%2520industrial%2520bins%252C%2520while%2520the%2520synthetic%250Adatasets%2520included%2520views%2520of%2520a%2520simple%2520asymmetric%2520object%2520rendered%2520under%2520different%250Arotations.%2520On%2520synthetic%2520data%252C%2520we%2520also%2520assessed%2520the%2520effects%2520of%2520different%250Arotation%2520distributions%2520within%2520the%2520training%2520and%2520test%2520sets%252C%2520as%2520well%2520as%2520the%2520impact%250Aof%2520the%2520object%2527s%2520texture.%2520In%2520line%2520with%2520previous%2520research%252C%2520we%2520found%2520that%2520networks%250Ausing%2520the%2520continuous%25205D%2520and%25206D%2520representations%2520performed%2520better%2520than%2520the%250Adiscontinuous%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Representation%20of%203D%20Rotation%20in%20the%20Context%20of%20Deep%20Learning&entry.906535625=Vikt%C3%B3ria%20Pravdov%C3%A1%20and%20Luk%C3%A1%C5%A1%20Gajdo%C5%A1ech%20and%20Hassan%20Ali%20and%20Viktor%20Kocur&entry.1292438233=%20%20This%20paper%20investigates%20various%20methods%20of%20representing%203D%20rotations%20and%0Atheir%20impact%20on%20the%20learning%20process%20of%20deep%20neural%20networks.%20We%20evaluated%20the%0Aperformance%20of%20ResNet18%20networks%20for%203D%20rotation%20estimation%20using%20several%0Arotation%20representations%20and%20loss%20functions%20on%20both%20synthetic%20and%20real%20data.%0AThe%20real%20datasets%20contained%203D%20scans%20of%20industrial%20bins%2C%20while%20the%20synthetic%0Adatasets%20included%20views%20of%20a%20simple%20asymmetric%20object%20rendered%20under%20different%0Arotations.%20On%20synthetic%20data%2C%20we%20also%20assessed%20the%20effects%20of%20different%0Arotation%20distributions%20within%20the%20training%20and%20test%20sets%2C%20as%20well%20as%20the%20impact%0Aof%20the%20object%27s%20texture.%20In%20line%20with%20previous%20research%2C%20we%20found%20that%20networks%0Ausing%20the%20continuous%205D%20and%206D%20representations%20performed%20better%20than%20the%0Adiscontinuous%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10350v1&entry.124074799=Read"},
{"title": "LKASeg:Remote-Sensing Image Semantic Segmentation with Large Kernel\n  Attention and Full-Scale Skip Connections", "author": "Xuezhi Xiang and Yibo Ning and Lei Zhang and Denis Ombati and Himaloy Himu and Xiantong Zhen", "abstract": "  Semantic segmentation of remote sensing images is a fundamental task in\ngeospatial research. However, widely used Convolutional Neural Networks (CNNs)\nand Transformers have notable drawbacks: CNNs may be limited by insufficient\nremote sensing modeling capability, while Transformers face challenges due to\ncomputational complexity. In this paper, we propose a remote-sensing image\nsemantic segmentation network named LKASeg, which combines Large Kernel\nAttention(LSKA) and Full-Scale Skip Connections(FSC). Specifically, we propose\na decoder based on Large Kernel Attention (LKA), which extract global features\nwhile avoiding the computational overhead of self-attention and providing\nchannel adaptability. To achieve full-scale feature learning and fusion, we\napply Full-Scale Skip Connections (FSC) between the encoder and decoder. We\nconducted experiments by combining the LKA-based decoder with FSC. On the ISPRS\nVaihingen dataset, the mF1 and mIoU scores achieved 90.33% and 82.77%.\n", "link": "http://arxiv.org/abs/2410.10433v1", "date": "2024-10-14", "relevancy": 2.6627, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LKASeg%3ARemote-Sensing%20Image%20Semantic%20Segmentation%20with%20Large%20Kernel%0A%20%20Attention%20and%20Full-Scale%20Skip%20Connections&body=Title%3A%20LKASeg%3ARemote-Sensing%20Image%20Semantic%20Segmentation%20with%20Large%20Kernel%0A%20%20Attention%20and%20Full-Scale%20Skip%20Connections%0AAuthor%3A%20Xuezhi%20Xiang%20and%20Yibo%20Ning%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20a%20fundamental%20task%20in%0Ageospatial%20research.%20However%2C%20widely%20used%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Transformers%20have%20notable%20drawbacks%3A%20CNNs%20may%20be%20limited%20by%20insufficient%0Aremote%20sensing%20modeling%20capability%2C%20while%20Transformers%20face%20challenges%20due%20to%0Acomputational%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%20remote-sensing%20image%0Asemantic%20segmentation%20network%20named%20LKASeg%2C%20which%20combines%20Large%20Kernel%0AAttention%28LSKA%29%20and%20Full-Scale%20Skip%20Connections%28FSC%29.%20Specifically%2C%20we%20propose%0Aa%20decoder%20based%20on%20Large%20Kernel%20Attention%20%28LKA%29%2C%20which%20extract%20global%20features%0Awhile%20avoiding%20the%20computational%20overhead%20of%20self-attention%20and%20providing%0Achannel%20adaptability.%20To%20achieve%20full-scale%20feature%20learning%20and%20fusion%2C%20we%0Aapply%20Full-Scale%20Skip%20Connections%20%28FSC%29%20between%20the%20encoder%20and%20decoder.%20We%0Aconducted%20experiments%20by%20combining%20the%20LKA-based%20decoder%20with%20FSC.%20On%20the%20ISPRS%0AVaihingen%20dataset%2C%20the%20mF1%20and%20mIoU%20scores%20achieved%2090.33%25%20and%2082.77%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLKASeg%253ARemote-Sensing%2520Image%2520Semantic%2520Segmentation%2520with%2520Large%2520Kernel%250A%2520%2520Attention%2520and%2520Full-Scale%2520Skip%2520Connections%26entry.906535625%3DXuezhi%2520Xiang%2520and%2520Yibo%2520Ning%2520and%2520Lei%2520Zhang%2520and%2520Denis%2520Ombati%2520and%2520Himaloy%2520Himu%2520and%2520Xiantong%2520Zhen%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520remote%2520sensing%2520images%2520is%2520a%2520fundamental%2520task%2520in%250Ageospatial%2520research.%2520However%252C%2520widely%2520used%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%250Aand%2520Transformers%2520have%2520notable%2520drawbacks%253A%2520CNNs%2520may%2520be%2520limited%2520by%2520insufficient%250Aremote%2520sensing%2520modeling%2520capability%252C%2520while%2520Transformers%2520face%2520challenges%2520due%2520to%250Acomputational%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520remote-sensing%2520image%250Asemantic%2520segmentation%2520network%2520named%2520LKASeg%252C%2520which%2520combines%2520Large%2520Kernel%250AAttention%2528LSKA%2529%2520and%2520Full-Scale%2520Skip%2520Connections%2528FSC%2529.%2520Specifically%252C%2520we%2520propose%250Aa%2520decoder%2520based%2520on%2520Large%2520Kernel%2520Attention%2520%2528LKA%2529%252C%2520which%2520extract%2520global%2520features%250Awhile%2520avoiding%2520the%2520computational%2520overhead%2520of%2520self-attention%2520and%2520providing%250Achannel%2520adaptability.%2520To%2520achieve%2520full-scale%2520feature%2520learning%2520and%2520fusion%252C%2520we%250Aapply%2520Full-Scale%2520Skip%2520Connections%2520%2528FSC%2529%2520between%2520the%2520encoder%2520and%2520decoder.%2520We%250Aconducted%2520experiments%2520by%2520combining%2520the%2520LKA-based%2520decoder%2520with%2520FSC.%2520On%2520the%2520ISPRS%250AVaihingen%2520dataset%252C%2520the%2520mF1%2520and%2520mIoU%2520scores%2520achieved%252090.33%2525%2520and%252082.77%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LKASeg%3ARemote-Sensing%20Image%20Semantic%20Segmentation%20with%20Large%20Kernel%0A%20%20Attention%20and%20Full-Scale%20Skip%20Connections&entry.906535625=Xuezhi%20Xiang%20and%20Yibo%20Ning%20and%20Lei%20Zhang%20and%20Denis%20Ombati%20and%20Himaloy%20Himu%20and%20Xiantong%20Zhen&entry.1292438233=%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20a%20fundamental%20task%20in%0Ageospatial%20research.%20However%2C%20widely%20used%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Transformers%20have%20notable%20drawbacks%3A%20CNNs%20may%20be%20limited%20by%20insufficient%0Aremote%20sensing%20modeling%20capability%2C%20while%20Transformers%20face%20challenges%20due%20to%0Acomputational%20complexity.%20In%20this%20paper%2C%20we%20propose%20a%20remote-sensing%20image%0Asemantic%20segmentation%20network%20named%20LKASeg%2C%20which%20combines%20Large%20Kernel%0AAttention%28LSKA%29%20and%20Full-Scale%20Skip%20Connections%28FSC%29.%20Specifically%2C%20we%20propose%0Aa%20decoder%20based%20on%20Large%20Kernel%20Attention%20%28LKA%29%2C%20which%20extract%20global%20features%0Awhile%20avoiding%20the%20computational%20overhead%20of%20self-attention%20and%20providing%0Achannel%20adaptability.%20To%20achieve%20full-scale%20feature%20learning%20and%20fusion%2C%20we%0Aapply%20Full-Scale%20Skip%20Connections%20%28FSC%29%20between%20the%20encoder%20and%20decoder.%20We%0Aconducted%20experiments%20by%20combining%20the%20LKA-based%20decoder%20with%20FSC.%20On%20the%20ISPRS%0AVaihingen%20dataset%2C%20the%20mF1%20and%20mIoU%20scores%20achieved%2090.33%25%20and%2082.77%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10433v1&entry.124074799=Read"},
{"title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents", "author": "Shi Yu and Chaoyue Tang and Bokai Xu and Junbo Cui and Junhao Ran and Yukun Yan and Zhenghao Liu and Shuo Wang and Xu Han and Zhiyuan Liu and Maosong Sun", "abstract": "  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .\n", "link": "http://arxiv.org/abs/2410.10594v1", "date": "2024-10-14", "relevancy": 2.6568, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5311}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisRAG%3A%20Vision-based%20Retrieval-augmented%20Generation%20on%20Multi-modality%0A%20%20Documents&body=Title%3A%20VisRAG%3A%20Vision-based%20Retrieval-augmented%20Generation%20on%20Multi-modality%0A%20%20Documents%0AAuthor%3A%20Shi%20Yu%20and%20Chaoyue%20Tang%20and%20Bokai%20Xu%20and%20Junbo%20Cui%20and%20Junhao%20Ran%20and%20Yukun%20Yan%20and%20Zhenghao%20Liu%20and%20Shuo%20Wang%20and%20Xu%20Han%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20is%20an%20effective%20technique%20that%20enables%0Alarge%20language%20models%20%28LLMs%29%20to%20utilize%20external%20knowledge%20sources%20for%0Ageneration.%20However%2C%20current%20RAG%20systems%20are%20solely%20based%20on%20text%2C%20rendering%20it%0Aimpossible%20to%20utilize%20vision%20information%20like%20layout%20and%20images%20that%20play%0Acrucial%20roles%20in%20real-world%20multi-modality%20documents.%20In%20this%20paper%2C%20we%0Aintroduce%20VisRAG%2C%20which%20tackles%20this%20issue%20by%20establishing%20a%20vision-language%0Amodel%20%28VLM%29-based%20RAG%20pipeline.%20In%20this%20pipeline%2C%20instead%20of%20first%20parsing%20the%0Adocument%20to%20obtain%20text%2C%20the%20document%20is%20directly%20embedded%20using%20a%20VLM%20as%20an%0Aimage%20and%20then%20retrieved%20to%20enhance%20the%20generation%20of%20a%20VLM.%20Compared%20to%0Atraditional%20text-based%20RAG%2C%20VisRAG%20maximizes%20the%20retention%20and%20utilization%20of%0Athe%20data%20information%20in%20the%20original%20documents%2C%20eliminating%20the%20information%0Aloss%20introduced%20during%20the%20parsing%20process.%20We%20collect%20both%20open-source%20and%0Asynthetic%20data%20to%20train%20the%20retriever%20in%20VisRAG%20and%20explore%20a%20variety%20of%0Ageneration%20methods.%20Experiments%20demonstrate%20that%20VisRAG%20outperforms%20traditional%0ARAG%20in%20both%20the%20retrieval%20and%20generation%20stages%2C%20achieving%20a%2025--39%5C%25%0Aend-to-end%20performance%20gain%20over%20traditional%20text-based%20RAG%20pipeline.%20Further%0Aanalysis%20reveals%20that%20VisRAG%20is%20effective%20in%20utilizing%20training%20data%20and%0Ademonstrates%20strong%20generalization%20capability%2C%20positioning%20it%20as%20a%20promising%0Asolution%20for%20RAG%20on%20multi-modality%20documents.%20Our%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/openbmb/visrag%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisRAG%253A%2520Vision-based%2520Retrieval-augmented%2520Generation%2520on%2520Multi-modality%250A%2520%2520Documents%26entry.906535625%3DShi%2520Yu%2520and%2520Chaoyue%2520Tang%2520and%2520Bokai%2520Xu%2520and%2520Junbo%2520Cui%2520and%2520Junhao%2520Ran%2520and%2520Yukun%2520Yan%2520and%2520Zhenghao%2520Liu%2520and%2520Shuo%2520Wang%2520and%2520Xu%2520Han%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520is%2520an%2520effective%2520technique%2520that%2520enables%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520utilize%2520external%2520knowledge%2520sources%2520for%250Ageneration.%2520However%252C%2520current%2520RAG%2520systems%2520are%2520solely%2520based%2520on%2520text%252C%2520rendering%2520it%250Aimpossible%2520to%2520utilize%2520vision%2520information%2520like%2520layout%2520and%2520images%2520that%2520play%250Acrucial%2520roles%2520in%2520real-world%2520multi-modality%2520documents.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520VisRAG%252C%2520which%2520tackles%2520this%2520issue%2520by%2520establishing%2520a%2520vision-language%250Amodel%2520%2528VLM%2529-based%2520RAG%2520pipeline.%2520In%2520this%2520pipeline%252C%2520instead%2520of%2520first%2520parsing%2520the%250Adocument%2520to%2520obtain%2520text%252C%2520the%2520document%2520is%2520directly%2520embedded%2520using%2520a%2520VLM%2520as%2520an%250Aimage%2520and%2520then%2520retrieved%2520to%2520enhance%2520the%2520generation%2520of%2520a%2520VLM.%2520Compared%2520to%250Atraditional%2520text-based%2520RAG%252C%2520VisRAG%2520maximizes%2520the%2520retention%2520and%2520utilization%2520of%250Athe%2520data%2520information%2520in%2520the%2520original%2520documents%252C%2520eliminating%2520the%2520information%250Aloss%2520introduced%2520during%2520the%2520parsing%2520process.%2520We%2520collect%2520both%2520open-source%2520and%250Asynthetic%2520data%2520to%2520train%2520the%2520retriever%2520in%2520VisRAG%2520and%2520explore%2520a%2520variety%2520of%250Ageneration%2520methods.%2520Experiments%2520demonstrate%2520that%2520VisRAG%2520outperforms%2520traditional%250ARAG%2520in%2520both%2520the%2520retrieval%2520and%2520generation%2520stages%252C%2520achieving%2520a%252025--39%255C%2525%250Aend-to-end%2520performance%2520gain%2520over%2520traditional%2520text-based%2520RAG%2520pipeline.%2520Further%250Aanalysis%2520reveals%2520that%2520VisRAG%2520is%2520effective%2520in%2520utilizing%2520training%2520data%2520and%250Ademonstrates%2520strong%2520generalization%2520capability%252C%2520positioning%2520it%2520as%2520a%2520promising%250Asolution%2520for%2520RAG%2520on%2520multi-modality%2520documents.%2520Our%2520code%2520and%2520data%2520are%2520available%250Aat%2520https%253A//github.com/openbmb/visrag%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisRAG%3A%20Vision-based%20Retrieval-augmented%20Generation%20on%20Multi-modality%0A%20%20Documents&entry.906535625=Shi%20Yu%20and%20Chaoyue%20Tang%20and%20Bokai%20Xu%20and%20Junbo%20Cui%20and%20Junhao%20Ran%20and%20Yukun%20Yan%20and%20Zhenghao%20Liu%20and%20Shuo%20Wang%20and%20Xu%20Han%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20is%20an%20effective%20technique%20that%20enables%0Alarge%20language%20models%20%28LLMs%29%20to%20utilize%20external%20knowledge%20sources%20for%0Ageneration.%20However%2C%20current%20RAG%20systems%20are%20solely%20based%20on%20text%2C%20rendering%20it%0Aimpossible%20to%20utilize%20vision%20information%20like%20layout%20and%20images%20that%20play%0Acrucial%20roles%20in%20real-world%20multi-modality%20documents.%20In%20this%20paper%2C%20we%0Aintroduce%20VisRAG%2C%20which%20tackles%20this%20issue%20by%20establishing%20a%20vision-language%0Amodel%20%28VLM%29-based%20RAG%20pipeline.%20In%20this%20pipeline%2C%20instead%20of%20first%20parsing%20the%0Adocument%20to%20obtain%20text%2C%20the%20document%20is%20directly%20embedded%20using%20a%20VLM%20as%20an%0Aimage%20and%20then%20retrieved%20to%20enhance%20the%20generation%20of%20a%20VLM.%20Compared%20to%0Atraditional%20text-based%20RAG%2C%20VisRAG%20maximizes%20the%20retention%20and%20utilization%20of%0Athe%20data%20information%20in%20the%20original%20documents%2C%20eliminating%20the%20information%0Aloss%20introduced%20during%20the%20parsing%20process.%20We%20collect%20both%20open-source%20and%0Asynthetic%20data%20to%20train%20the%20retriever%20in%20VisRAG%20and%20explore%20a%20variety%20of%0Ageneration%20methods.%20Experiments%20demonstrate%20that%20VisRAG%20outperforms%20traditional%0ARAG%20in%20both%20the%20retrieval%20and%20generation%20stages%2C%20achieving%20a%2025--39%5C%25%0Aend-to-end%20performance%20gain%20over%20traditional%20text-based%20RAG%20pipeline.%20Further%0Aanalysis%20reveals%20that%20VisRAG%20is%20effective%20in%20utilizing%20training%20data%20and%0Ademonstrates%20strong%20generalization%20capability%2C%20positioning%20it%20as%20a%20promising%0Asolution%20for%20RAG%20on%20multi-modality%20documents.%20Our%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/openbmb/visrag%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10594v1&entry.124074799=Read"},
{"title": "Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models", "author": "Jingzhi Bao and Xueting Li and Ming-Hsuan Yang", "abstract": "  3D meshes are widely used in computer vision and graphics for their\nefficiency in animation and minimal memory use, playing a crucial role in\nmovies, games, AR, and VR. However, creating temporally consistent and\nrealistic textures for mesh sequences remains labor-intensive for professional\nartists. On the other hand, while video diffusion models excel at text-driven\nvideo generation, they often lack 3D geometry awareness and struggle with\nachieving multi-view consistent texturing for 3D meshes. In this work, we\npresent Tex4D, a zero-shot approach that integrates inherent 3D geometry\nknowledge from mesh sequences with the expressiveness of video diffusion models\nto produce multi-view and temporally consistent 4D textures. Given an\nuntextured mesh sequence and a text prompt as inputs, our method enhances\nmulti-view consistency by synchronizing the diffusion process across different\nviews through latent aggregation in the UV space. To ensure temporal\nconsistency, we leverage prior knowledge from a conditional video generation\nmodel for texture synthesis. However, straightforwardly combining the video\ndiffusion model and the UV texture aggregation leads to blurry results. We\nanalyze the underlying causes and propose a simple yet effective modification\nto the DDIM sampling process to address this issue. Additionally, we introduce\na reference latent texture to strengthen the correlation between frames during\nthe denoising process. To the best of our knowledge, Tex4D is the first method\nspecifically designed for 4D scene texturing. Extensive experiments demonstrate\nits superiority in producing multi-view and multi-frame consistent videos based\non untextured mesh sequences.\n", "link": "http://arxiv.org/abs/2410.10821v1", "date": "2024-10-14", "relevancy": 2.628, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.659}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.659}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tex4D%3A%20Zero-shot%204D%20Scene%20Texturing%20with%20Video%20Diffusion%20Models&body=Title%3A%20Tex4D%3A%20Zero-shot%204D%20Scene%20Texturing%20with%20Video%20Diffusion%20Models%0AAuthor%3A%20Jingzhi%20Bao%20and%20Xueting%20Li%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%203D%20meshes%20are%20widely%20used%20in%20computer%20vision%20and%20graphics%20for%20their%0Aefficiency%20in%20animation%20and%20minimal%20memory%20use%2C%20playing%20a%20crucial%20role%20in%0Amovies%2C%20games%2C%20AR%2C%20and%20VR.%20However%2C%20creating%20temporally%20consistent%20and%0Arealistic%20textures%20for%20mesh%20sequences%20remains%20labor-intensive%20for%20professional%0Aartists.%20On%20the%20other%20hand%2C%20while%20video%20diffusion%20models%20excel%20at%20text-driven%0Avideo%20generation%2C%20they%20often%20lack%203D%20geometry%20awareness%20and%20struggle%20with%0Aachieving%20multi-view%20consistent%20texturing%20for%203D%20meshes.%20In%20this%20work%2C%20we%0Apresent%20Tex4D%2C%20a%20zero-shot%20approach%20that%20integrates%20inherent%203D%20geometry%0Aknowledge%20from%20mesh%20sequences%20with%20the%20expressiveness%20of%20video%20diffusion%20models%0Ato%20produce%20multi-view%20and%20temporally%20consistent%204D%20textures.%20Given%20an%0Auntextured%20mesh%20sequence%20and%20a%20text%20prompt%20as%20inputs%2C%20our%20method%20enhances%0Amulti-view%20consistency%20by%20synchronizing%20the%20diffusion%20process%20across%20different%0Aviews%20through%20latent%20aggregation%20in%20the%20UV%20space.%20To%20ensure%20temporal%0Aconsistency%2C%20we%20leverage%20prior%20knowledge%20from%20a%20conditional%20video%20generation%0Amodel%20for%20texture%20synthesis.%20However%2C%20straightforwardly%20combining%20the%20video%0Adiffusion%20model%20and%20the%20UV%20texture%20aggregation%20leads%20to%20blurry%20results.%20We%0Aanalyze%20the%20underlying%20causes%20and%20propose%20a%20simple%20yet%20effective%20modification%0Ato%20the%20DDIM%20sampling%20process%20to%20address%20this%20issue.%20Additionally%2C%20we%20introduce%0Aa%20reference%20latent%20texture%20to%20strengthen%20the%20correlation%20between%20frames%20during%0Athe%20denoising%20process.%20To%20the%20best%20of%20our%20knowledge%2C%20Tex4D%20is%20the%20first%20method%0Aspecifically%20designed%20for%204D%20scene%20texturing.%20Extensive%20experiments%20demonstrate%0Aits%20superiority%20in%20producing%20multi-view%20and%20multi-frame%20consistent%20videos%20based%0Aon%20untextured%20mesh%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTex4D%253A%2520Zero-shot%25204D%2520Scene%2520Texturing%2520with%2520Video%2520Diffusion%2520Models%26entry.906535625%3DJingzhi%2520Bao%2520and%2520Xueting%2520Li%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%25203D%2520meshes%2520are%2520widely%2520used%2520in%2520computer%2520vision%2520and%2520graphics%2520for%2520their%250Aefficiency%2520in%2520animation%2520and%2520minimal%2520memory%2520use%252C%2520playing%2520a%2520crucial%2520role%2520in%250Amovies%252C%2520games%252C%2520AR%252C%2520and%2520VR.%2520However%252C%2520creating%2520temporally%2520consistent%2520and%250Arealistic%2520textures%2520for%2520mesh%2520sequences%2520remains%2520labor-intensive%2520for%2520professional%250Aartists.%2520On%2520the%2520other%2520hand%252C%2520while%2520video%2520diffusion%2520models%2520excel%2520at%2520text-driven%250Avideo%2520generation%252C%2520they%2520often%2520lack%25203D%2520geometry%2520awareness%2520and%2520struggle%2520with%250Aachieving%2520multi-view%2520consistent%2520texturing%2520for%25203D%2520meshes.%2520In%2520this%2520work%252C%2520we%250Apresent%2520Tex4D%252C%2520a%2520zero-shot%2520approach%2520that%2520integrates%2520inherent%25203D%2520geometry%250Aknowledge%2520from%2520mesh%2520sequences%2520with%2520the%2520expressiveness%2520of%2520video%2520diffusion%2520models%250Ato%2520produce%2520multi-view%2520and%2520temporally%2520consistent%25204D%2520textures.%2520Given%2520an%250Auntextured%2520mesh%2520sequence%2520and%2520a%2520text%2520prompt%2520as%2520inputs%252C%2520our%2520method%2520enhances%250Amulti-view%2520consistency%2520by%2520synchronizing%2520the%2520diffusion%2520process%2520across%2520different%250Aviews%2520through%2520latent%2520aggregation%2520in%2520the%2520UV%2520space.%2520To%2520ensure%2520temporal%250Aconsistency%252C%2520we%2520leverage%2520prior%2520knowledge%2520from%2520a%2520conditional%2520video%2520generation%250Amodel%2520for%2520texture%2520synthesis.%2520However%252C%2520straightforwardly%2520combining%2520the%2520video%250Adiffusion%2520model%2520and%2520the%2520UV%2520texture%2520aggregation%2520leads%2520to%2520blurry%2520results.%2520We%250Aanalyze%2520the%2520underlying%2520causes%2520and%2520propose%2520a%2520simple%2520yet%2520effective%2520modification%250Ato%2520the%2520DDIM%2520sampling%2520process%2520to%2520address%2520this%2520issue.%2520Additionally%252C%2520we%2520introduce%250Aa%2520reference%2520latent%2520texture%2520to%2520strengthen%2520the%2520correlation%2520between%2520frames%2520during%250Athe%2520denoising%2520process.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520Tex4D%2520is%2520the%2520first%2520method%250Aspecifically%2520designed%2520for%25204D%2520scene%2520texturing.%2520Extensive%2520experiments%2520demonstrate%250Aits%2520superiority%2520in%2520producing%2520multi-view%2520and%2520multi-frame%2520consistent%2520videos%2520based%250Aon%2520untextured%2520mesh%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tex4D%3A%20Zero-shot%204D%20Scene%20Texturing%20with%20Video%20Diffusion%20Models&entry.906535625=Jingzhi%20Bao%20and%20Xueting%20Li%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%203D%20meshes%20are%20widely%20used%20in%20computer%20vision%20and%20graphics%20for%20their%0Aefficiency%20in%20animation%20and%20minimal%20memory%20use%2C%20playing%20a%20crucial%20role%20in%0Amovies%2C%20games%2C%20AR%2C%20and%20VR.%20However%2C%20creating%20temporally%20consistent%20and%0Arealistic%20textures%20for%20mesh%20sequences%20remains%20labor-intensive%20for%20professional%0Aartists.%20On%20the%20other%20hand%2C%20while%20video%20diffusion%20models%20excel%20at%20text-driven%0Avideo%20generation%2C%20they%20often%20lack%203D%20geometry%20awareness%20and%20struggle%20with%0Aachieving%20multi-view%20consistent%20texturing%20for%203D%20meshes.%20In%20this%20work%2C%20we%0Apresent%20Tex4D%2C%20a%20zero-shot%20approach%20that%20integrates%20inherent%203D%20geometry%0Aknowledge%20from%20mesh%20sequences%20with%20the%20expressiveness%20of%20video%20diffusion%20models%0Ato%20produce%20multi-view%20and%20temporally%20consistent%204D%20textures.%20Given%20an%0Auntextured%20mesh%20sequence%20and%20a%20text%20prompt%20as%20inputs%2C%20our%20method%20enhances%0Amulti-view%20consistency%20by%20synchronizing%20the%20diffusion%20process%20across%20different%0Aviews%20through%20latent%20aggregation%20in%20the%20UV%20space.%20To%20ensure%20temporal%0Aconsistency%2C%20we%20leverage%20prior%20knowledge%20from%20a%20conditional%20video%20generation%0Amodel%20for%20texture%20synthesis.%20However%2C%20straightforwardly%20combining%20the%20video%0Adiffusion%20model%20and%20the%20UV%20texture%20aggregation%20leads%20to%20blurry%20results.%20We%0Aanalyze%20the%20underlying%20causes%20and%20propose%20a%20simple%20yet%20effective%20modification%0Ato%20the%20DDIM%20sampling%20process%20to%20address%20this%20issue.%20Additionally%2C%20we%20introduce%0Aa%20reference%20latent%20texture%20to%20strengthen%20the%20correlation%20between%20frames%20during%0Athe%20denoising%20process.%20To%20the%20best%20of%20our%20knowledge%2C%20Tex4D%20is%20the%20first%20method%0Aspecifically%20designed%20for%204D%20scene%20texturing.%20Extensive%20experiments%20demonstrate%0Aits%20superiority%20in%20producing%20multi-view%20and%20multi-frame%20consistent%20videos%20based%0Aon%20untextured%20mesh%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10821v1&entry.124074799=Read"},
{"title": "HAMMR: HierArchical MultiModal React agents for generic VQA", "author": "Lluis Castrejon and Thomas Mensink and Howard Zhou and Vittorio Ferrari and Andre Araujo and Jasper Uijlings", "abstract": "  Combining Large Language Models (LLMs) with external specialized tools\n(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual\nQuestion Answering (VQA). While this approach was demonstrated to work well\nwhen optimized and evaluated for each individual benchmark, in practice it is\ncrucial for the next generation of real-world AI systems to handle a broad\nrange of multimodal problems. Therefore we pose the VQA problem from a unified\nperspective and evaluate a single system on a varied suite of VQA tasks\nincluding counting, spatial reasoning, OCR-based reasoning, visual pointing,\nexternal knowledge, and more. In this setting, we demonstrate that naively\napplying the LLM+tools approach using the combined set of all tools leads to\npoor results. This motivates us to introduce HAMMR: HierArchical MultiModal\nReact. We start from a multimodal ReAct-based system and make it hierarchical\nby enabling our HAMMR agents to call upon other specialized agents. This\nenhances the compositionality of the LLM+tools approach, which we show to be\ncritical for obtaining high accuracy on generic VQA. Concretely, on our generic\nVQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.\nAdditionally, HAMMR achieves state-of-the-art results on this task,\noutperforming the generic standalone PaLI-X VQA model by 5.0%.\n", "link": "http://arxiv.org/abs/2404.05465v2", "date": "2024-10-14", "relevancy": 2.6259, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMMR%3A%20HierArchical%20MultiModal%20React%20agents%20for%20generic%20VQA&body=Title%3A%20HAMMR%3A%20HierArchical%20MultiModal%20React%20agents%20for%20generic%20VQA%0AAuthor%3A%20Lluis%20Castrejon%20and%20Thomas%20Mensink%20and%20Howard%20Zhou%20and%20Vittorio%20Ferrari%20and%20Andre%20Araujo%20and%20Jasper%20Uijlings%0AAbstract%3A%20%20%20Combining%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20specialized%20tools%0A%28LLMs%2Btools%29%20is%20a%20recent%20paradigm%20to%20solve%20multimodal%20tasks%20such%20as%20Visual%0AQuestion%20Answering%20%28VQA%29.%20While%20this%20approach%20was%20demonstrated%20to%20work%20well%0Awhen%20optimized%20and%20evaluated%20for%20each%20individual%20benchmark%2C%20in%20practice%20it%20is%0Acrucial%20for%20the%20next%20generation%20of%20real-world%20AI%20systems%20to%20handle%20a%20broad%0Arange%20of%20multimodal%20problems.%20Therefore%20we%20pose%20the%20VQA%20problem%20from%20a%20unified%0Aperspective%20and%20evaluate%20a%20single%20system%20on%20a%20varied%20suite%20of%20VQA%20tasks%0Aincluding%20counting%2C%20spatial%20reasoning%2C%20OCR-based%20reasoning%2C%20visual%20pointing%2C%0Aexternal%20knowledge%2C%20and%20more.%20In%20this%20setting%2C%20we%20demonstrate%20that%20naively%0Aapplying%20the%20LLM%2Btools%20approach%20using%20the%20combined%20set%20of%20all%20tools%20leads%20to%0Apoor%20results.%20This%20motivates%20us%20to%20introduce%20HAMMR%3A%20HierArchical%20MultiModal%0AReact.%20We%20start%20from%20a%20multimodal%20ReAct-based%20system%20and%20make%20it%20hierarchical%0Aby%20enabling%20our%20HAMMR%20agents%20to%20call%20upon%20other%20specialized%20agents.%20This%0Aenhances%20the%20compositionality%20of%20the%20LLM%2Btools%20approach%2C%20which%20we%20show%20to%20be%0Acritical%20for%20obtaining%20high%20accuracy%20on%20generic%20VQA.%20Concretely%2C%20on%20our%20generic%0AVQA%20suite%2C%20HAMMR%20outperforms%20the%20naive%20LLM%2Btools%20approach%20by%2019.5%25.%0AAdditionally%2C%20HAMMR%20achieves%20state-of-the-art%20results%20on%20this%20task%2C%0Aoutperforming%20the%20generic%20standalone%20PaLI-X%20VQA%20model%20by%205.0%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMMR%253A%2520HierArchical%2520MultiModal%2520React%2520agents%2520for%2520generic%2520VQA%26entry.906535625%3DLluis%2520Castrejon%2520and%2520Thomas%2520Mensink%2520and%2520Howard%2520Zhou%2520and%2520Vittorio%2520Ferrari%2520and%2520Andre%2520Araujo%2520and%2520Jasper%2520Uijlings%26entry.1292438233%3D%2520%2520Combining%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520external%2520specialized%2520tools%250A%2528LLMs%252Btools%2529%2520is%2520a%2520recent%2520paradigm%2520to%2520solve%2520multimodal%2520tasks%2520such%2520as%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529.%2520While%2520this%2520approach%2520was%2520demonstrated%2520to%2520work%2520well%250Awhen%2520optimized%2520and%2520evaluated%2520for%2520each%2520individual%2520benchmark%252C%2520in%2520practice%2520it%2520is%250Acrucial%2520for%2520the%2520next%2520generation%2520of%2520real-world%2520AI%2520systems%2520to%2520handle%2520a%2520broad%250Arange%2520of%2520multimodal%2520problems.%2520Therefore%2520we%2520pose%2520the%2520VQA%2520problem%2520from%2520a%2520unified%250Aperspective%2520and%2520evaluate%2520a%2520single%2520system%2520on%2520a%2520varied%2520suite%2520of%2520VQA%2520tasks%250Aincluding%2520counting%252C%2520spatial%2520reasoning%252C%2520OCR-based%2520reasoning%252C%2520visual%2520pointing%252C%250Aexternal%2520knowledge%252C%2520and%2520more.%2520In%2520this%2520setting%252C%2520we%2520demonstrate%2520that%2520naively%250Aapplying%2520the%2520LLM%252Btools%2520approach%2520using%2520the%2520combined%2520set%2520of%2520all%2520tools%2520leads%2520to%250Apoor%2520results.%2520This%2520motivates%2520us%2520to%2520introduce%2520HAMMR%253A%2520HierArchical%2520MultiModal%250AReact.%2520We%2520start%2520from%2520a%2520multimodal%2520ReAct-based%2520system%2520and%2520make%2520it%2520hierarchical%250Aby%2520enabling%2520our%2520HAMMR%2520agents%2520to%2520call%2520upon%2520other%2520specialized%2520agents.%2520This%250Aenhances%2520the%2520compositionality%2520of%2520the%2520LLM%252Btools%2520approach%252C%2520which%2520we%2520show%2520to%2520be%250Acritical%2520for%2520obtaining%2520high%2520accuracy%2520on%2520generic%2520VQA.%2520Concretely%252C%2520on%2520our%2520generic%250AVQA%2520suite%252C%2520HAMMR%2520outperforms%2520the%2520naive%2520LLM%252Btools%2520approach%2520by%252019.5%2525.%250AAdditionally%252C%2520HAMMR%2520achieves%2520state-of-the-art%2520results%2520on%2520this%2520task%252C%250Aoutperforming%2520the%2520generic%2520standalone%2520PaLI-X%2520VQA%2520model%2520by%25205.0%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMMR%3A%20HierArchical%20MultiModal%20React%20agents%20for%20generic%20VQA&entry.906535625=Lluis%20Castrejon%20and%20Thomas%20Mensink%20and%20Howard%20Zhou%20and%20Vittorio%20Ferrari%20and%20Andre%20Araujo%20and%20Jasper%20Uijlings&entry.1292438233=%20%20Combining%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20specialized%20tools%0A%28LLMs%2Btools%29%20is%20a%20recent%20paradigm%20to%20solve%20multimodal%20tasks%20such%20as%20Visual%0AQuestion%20Answering%20%28VQA%29.%20While%20this%20approach%20was%20demonstrated%20to%20work%20well%0Awhen%20optimized%20and%20evaluated%20for%20each%20individual%20benchmark%2C%20in%20practice%20it%20is%0Acrucial%20for%20the%20next%20generation%20of%20real-world%20AI%20systems%20to%20handle%20a%20broad%0Arange%20of%20multimodal%20problems.%20Therefore%20we%20pose%20the%20VQA%20problem%20from%20a%20unified%0Aperspective%20and%20evaluate%20a%20single%20system%20on%20a%20varied%20suite%20of%20VQA%20tasks%0Aincluding%20counting%2C%20spatial%20reasoning%2C%20OCR-based%20reasoning%2C%20visual%20pointing%2C%0Aexternal%20knowledge%2C%20and%20more.%20In%20this%20setting%2C%20we%20demonstrate%20that%20naively%0Aapplying%20the%20LLM%2Btools%20approach%20using%20the%20combined%20set%20of%20all%20tools%20leads%20to%0Apoor%20results.%20This%20motivates%20us%20to%20introduce%20HAMMR%3A%20HierArchical%20MultiModal%0AReact.%20We%20start%20from%20a%20multimodal%20ReAct-based%20system%20and%20make%20it%20hierarchical%0Aby%20enabling%20our%20HAMMR%20agents%20to%20call%20upon%20other%20specialized%20agents.%20This%0Aenhances%20the%20compositionality%20of%20the%20LLM%2Btools%20approach%2C%20which%20we%20show%20to%20be%0Acritical%20for%20obtaining%20high%20accuracy%20on%20generic%20VQA.%20Concretely%2C%20on%20our%20generic%0AVQA%20suite%2C%20HAMMR%20outperforms%20the%20naive%20LLM%2Btools%20approach%20by%2019.5%25.%0AAdditionally%2C%20HAMMR%20achieves%20state-of-the-art%20results%20on%20this%20task%2C%0Aoutperforming%20the%20generic%20standalone%20PaLI-X%20VQA%20model%20by%205.0%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05465v2&entry.124074799=Read"},
{"title": "Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification\n  with View-Based Aggregation", "author": "Yosuke Yamagishi and SHouhei Hanaoka", "abstract": "  In this work, we present our solution for the MICCAI 2024 CXR-LT challenge,\nachieving 4th place in Subtask 2 and 5th in Subtask 1. We leveraged an ensemble\nof ConvNeXt V2 and MaxViT models, pretrained on an external chest X-ray\ndataset, to address the long-tailed distribution of chest findings. The\nproposed method combines state-of-the-art image classification techniques,\nasymmetric loss for handling class imbalance, and view-based prediction\naggregation to enhance classification performance. Through experiments, we\ndemonstrate the advantages of our approach in improving both detection accuracy\nand the handling of the long-tailed distribution in CXR findings. The code is\navailable at \\url{https://github.com/yamagishi0824/cxrlt24-multiview-pp}.\n", "link": "http://arxiv.org/abs/2410.10710v1", "date": "2024-10-14", "relevancy": 2.6143, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5238}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5238}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20of%20ConvNeXt%20V2%20and%20MaxViT%20for%20Long-Tailed%20CXR%20Classification%0A%20%20with%20View-Based%20Aggregation&body=Title%3A%20Ensemble%20of%20ConvNeXt%20V2%20and%20MaxViT%20for%20Long-Tailed%20CXR%20Classification%0A%20%20with%20View-Based%20Aggregation%0AAuthor%3A%20Yosuke%20Yamagishi%20and%20SHouhei%20Hanaoka%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20our%20solution%20for%20the%20MICCAI%202024%20CXR-LT%20challenge%2C%0Aachieving%204th%20place%20in%20Subtask%202%20and%205th%20in%20Subtask%201.%20We%20leveraged%20an%20ensemble%0Aof%20ConvNeXt%20V2%20and%20MaxViT%20models%2C%20pretrained%20on%20an%20external%20chest%20X-ray%0Adataset%2C%20to%20address%20the%20long-tailed%20distribution%20of%20chest%20findings.%20The%0Aproposed%20method%20combines%20state-of-the-art%20image%20classification%20techniques%2C%0Aasymmetric%20loss%20for%20handling%20class%20imbalance%2C%20and%20view-based%20prediction%0Aaggregation%20to%20enhance%20classification%20performance.%20Through%20experiments%2C%20we%0Ademonstrate%20the%20advantages%20of%20our%20approach%20in%20improving%20both%20detection%20accuracy%0Aand%20the%20handling%20of%20the%20long-tailed%20distribution%20in%20CXR%20findings.%20The%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/yamagishi0824/cxrlt24-multiview-pp%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520of%2520ConvNeXt%2520V2%2520and%2520MaxViT%2520for%2520Long-Tailed%2520CXR%2520Classification%250A%2520%2520with%2520View-Based%2520Aggregation%26entry.906535625%3DYosuke%2520Yamagishi%2520and%2520SHouhei%2520Hanaoka%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520our%2520solution%2520for%2520the%2520MICCAI%25202024%2520CXR-LT%2520challenge%252C%250Aachieving%25204th%2520place%2520in%2520Subtask%25202%2520and%25205th%2520in%2520Subtask%25201.%2520We%2520leveraged%2520an%2520ensemble%250Aof%2520ConvNeXt%2520V2%2520and%2520MaxViT%2520models%252C%2520pretrained%2520on%2520an%2520external%2520chest%2520X-ray%250Adataset%252C%2520to%2520address%2520the%2520long-tailed%2520distribution%2520of%2520chest%2520findings.%2520The%250Aproposed%2520method%2520combines%2520state-of-the-art%2520image%2520classification%2520techniques%252C%250Aasymmetric%2520loss%2520for%2520handling%2520class%2520imbalance%252C%2520and%2520view-based%2520prediction%250Aaggregation%2520to%2520enhance%2520classification%2520performance.%2520Through%2520experiments%252C%2520we%250Ademonstrate%2520the%2520advantages%2520of%2520our%2520approach%2520in%2520improving%2520both%2520detection%2520accuracy%250Aand%2520the%2520handling%2520of%2520the%2520long-tailed%2520distribution%2520in%2520CXR%2520findings.%2520The%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/yamagishi0824/cxrlt24-multiview-pp%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20of%20ConvNeXt%20V2%20and%20MaxViT%20for%20Long-Tailed%20CXR%20Classification%0A%20%20with%20View-Based%20Aggregation&entry.906535625=Yosuke%20Yamagishi%20and%20SHouhei%20Hanaoka&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20our%20solution%20for%20the%20MICCAI%202024%20CXR-LT%20challenge%2C%0Aachieving%204th%20place%20in%20Subtask%202%20and%205th%20in%20Subtask%201.%20We%20leveraged%20an%20ensemble%0Aof%20ConvNeXt%20V2%20and%20MaxViT%20models%2C%20pretrained%20on%20an%20external%20chest%20X-ray%0Adataset%2C%20to%20address%20the%20long-tailed%20distribution%20of%20chest%20findings.%20The%0Aproposed%20method%20combines%20state-of-the-art%20image%20classification%20techniques%2C%0Aasymmetric%20loss%20for%20handling%20class%20imbalance%2C%20and%20view-based%20prediction%0Aaggregation%20to%20enhance%20classification%20performance.%20Through%20experiments%2C%20we%0Ademonstrate%20the%20advantages%20of%20our%20approach%20in%20improving%20both%20detection%20accuracy%0Aand%20the%20handling%20of%20the%20long-tailed%20distribution%20in%20CXR%20findings.%20The%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/yamagishi0824/cxrlt24-multiview-pp%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10710v1&entry.124074799=Read"},
{"title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content", "author": "Nimrod Shabtay and Felipe Maia Polo and Sivan Doveh and Wei Lin and M. Jehanzeb Mirza and Leshem Chosen and Mikhail Yurochkin and Yuekai Sun and Assaf Arbelle and Leonid Karlinsky and Raja Giryes", "abstract": "  The large-scale training of multi-modal models on data scraped from the web\nhas shown outstanding utility in infusing these models with the required world\nknowledge to perform effectively on multiple downstream tasks. However, one\ndownside of scraping data from the web can be the potential sacrifice of the\nbenchmarks on which the abilities of these models are often evaluated. To\nsafeguard against test data contamination and to truly test the abilities of\nthese foundation models we propose LiveXiv: A scalable evolving live benchmark\nbased on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts\nat any given timestamp and proposes to automatically generate visual\nquestion-answer pairs (VQA). This is done without any human-in-the-loop, using\nthe multi-modal content in the manuscripts, like graphs, charts, and tables.\nMoreover, we introduce an efficient evaluation approach that estimates the\nperformance of all models on the evolving benchmark using evaluations of only a\nsubset of models. This significantly reduces the overall evaluation cost. We\nbenchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the\nfirst version of our benchmark, showing its challenging nature and exposing the\nmodels true abilities, avoiding contamination. Lastly, in our commitment to\nhigh quality, we have collected and evaluated a manually verified subset. By\ncomparing its overall results to our automatic annotations, we have found that\nthe performance variance is indeed minimal (<2.5%). Our dataset is available\nonline on HuggingFace, and our code will be available here.\n", "link": "http://arxiv.org/abs/2410.10783v1", "date": "2024-10-14", "relevancy": 2.6116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiveXiv%20--%20A%20Multi-Modal%20Live%20Benchmark%20Based%20on%20Arxiv%20Papers%20Content&body=Title%3A%20LiveXiv%20--%20A%20Multi-Modal%20Live%20Benchmark%20Based%20on%20Arxiv%20Papers%20Content%0AAuthor%3A%20Nimrod%20Shabtay%20and%20Felipe%20Maia%20Polo%20and%20Sivan%20Doveh%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Leshem%20Chosen%20and%20Mikhail%20Yurochkin%20and%20Yuekai%20Sun%20and%20Assaf%20Arbelle%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20The%20large-scale%20training%20of%20multi-modal%20models%20on%20data%20scraped%20from%20the%20web%0Ahas%20shown%20outstanding%20utility%20in%20infusing%20these%20models%20with%20the%20required%20world%0Aknowledge%20to%20perform%20effectively%20on%20multiple%20downstream%20tasks.%20However%2C%20one%0Adownside%20of%20scraping%20data%20from%20the%20web%20can%20be%20the%20potential%20sacrifice%20of%20the%0Abenchmarks%20on%20which%20the%20abilities%20of%20these%20models%20are%20often%20evaluated.%20To%0Asafeguard%20against%20test%20data%20contamination%20and%20to%20truly%20test%20the%20abilities%20of%0Athese%20foundation%20models%20we%20propose%20LiveXiv%3A%20A%20scalable%20evolving%20live%20benchmark%0Abased%20on%20scientific%20ArXiv%20papers.%20LiveXiv%20accesses%20domain-specific%20manuscripts%0Aat%20any%20given%20timestamp%20and%20proposes%20to%20automatically%20generate%20visual%0Aquestion-answer%20pairs%20%28VQA%29.%20This%20is%20done%20without%20any%20human-in-the-loop%2C%20using%0Athe%20multi-modal%20content%20in%20the%20manuscripts%2C%20like%20graphs%2C%20charts%2C%20and%20tables.%0AMoreover%2C%20we%20introduce%20an%20efficient%20evaluation%20approach%20that%20estimates%20the%0Aperformance%20of%20all%20models%20on%20the%20evolving%20benchmark%20using%20evaluations%20of%20only%20a%0Asubset%20of%20models.%20This%20significantly%20reduces%20the%20overall%20evaluation%20cost.%20We%0Abenchmark%20multiple%20open%20and%20proprietary%20Large%20Multi-modal%20Models%20%28LMMs%29%20on%20the%0Afirst%20version%20of%20our%20benchmark%2C%20showing%20its%20challenging%20nature%20and%20exposing%20the%0Amodels%20true%20abilities%2C%20avoiding%20contamination.%20Lastly%2C%20in%20our%20commitment%20to%0Ahigh%20quality%2C%20we%20have%20collected%20and%20evaluated%20a%20manually%20verified%20subset.%20By%0Acomparing%20its%20overall%20results%20to%20our%20automatic%20annotations%2C%20we%20have%20found%20that%0Athe%20performance%20variance%20is%20indeed%20minimal%20%28%3C2.5%25%29.%20Our%20dataset%20is%20available%0Aonline%20on%20HuggingFace%2C%20and%20our%20code%20will%20be%20available%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiveXiv%2520--%2520A%2520Multi-Modal%2520Live%2520Benchmark%2520Based%2520on%2520Arxiv%2520Papers%2520Content%26entry.906535625%3DNimrod%2520Shabtay%2520and%2520Felipe%2520Maia%2520Polo%2520and%2520Sivan%2520Doveh%2520and%2520Wei%2520Lin%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520Leshem%2520Chosen%2520and%2520Mikhail%2520Yurochkin%2520and%2520Yuekai%2520Sun%2520and%2520Assaf%2520Arbelle%2520and%2520Leonid%2520Karlinsky%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520The%2520large-scale%2520training%2520of%2520multi-modal%2520models%2520on%2520data%2520scraped%2520from%2520the%2520web%250Ahas%2520shown%2520outstanding%2520utility%2520in%2520infusing%2520these%2520models%2520with%2520the%2520required%2520world%250Aknowledge%2520to%2520perform%2520effectively%2520on%2520multiple%2520downstream%2520tasks.%2520However%252C%2520one%250Adownside%2520of%2520scraping%2520data%2520from%2520the%2520web%2520can%2520be%2520the%2520potential%2520sacrifice%2520of%2520the%250Abenchmarks%2520on%2520which%2520the%2520abilities%2520of%2520these%2520models%2520are%2520often%2520evaluated.%2520To%250Asafeguard%2520against%2520test%2520data%2520contamination%2520and%2520to%2520truly%2520test%2520the%2520abilities%2520of%250Athese%2520foundation%2520models%2520we%2520propose%2520LiveXiv%253A%2520A%2520scalable%2520evolving%2520live%2520benchmark%250Abased%2520on%2520scientific%2520ArXiv%2520papers.%2520LiveXiv%2520accesses%2520domain-specific%2520manuscripts%250Aat%2520any%2520given%2520timestamp%2520and%2520proposes%2520to%2520automatically%2520generate%2520visual%250Aquestion-answer%2520pairs%2520%2528VQA%2529.%2520This%2520is%2520done%2520without%2520any%2520human-in-the-loop%252C%2520using%250Athe%2520multi-modal%2520content%2520in%2520the%2520manuscripts%252C%2520like%2520graphs%252C%2520charts%252C%2520and%2520tables.%250AMoreover%252C%2520we%2520introduce%2520an%2520efficient%2520evaluation%2520approach%2520that%2520estimates%2520the%250Aperformance%2520of%2520all%2520models%2520on%2520the%2520evolving%2520benchmark%2520using%2520evaluations%2520of%2520only%2520a%250Asubset%2520of%2520models.%2520This%2520significantly%2520reduces%2520the%2520overall%2520evaluation%2520cost.%2520We%250Abenchmark%2520multiple%2520open%2520and%2520proprietary%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%2520on%2520the%250Afirst%2520version%2520of%2520our%2520benchmark%252C%2520showing%2520its%2520challenging%2520nature%2520and%2520exposing%2520the%250Amodels%2520true%2520abilities%252C%2520avoiding%2520contamination.%2520Lastly%252C%2520in%2520our%2520commitment%2520to%250Ahigh%2520quality%252C%2520we%2520have%2520collected%2520and%2520evaluated%2520a%2520manually%2520verified%2520subset.%2520By%250Acomparing%2520its%2520overall%2520results%2520to%2520our%2520automatic%2520annotations%252C%2520we%2520have%2520found%2520that%250Athe%2520performance%2520variance%2520is%2520indeed%2520minimal%2520%2528%253C2.5%2525%2529.%2520Our%2520dataset%2520is%2520available%250Aonline%2520on%2520HuggingFace%252C%2520and%2520our%2520code%2520will%2520be%2520available%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiveXiv%20--%20A%20Multi-Modal%20Live%20Benchmark%20Based%20on%20Arxiv%20Papers%20Content&entry.906535625=Nimrod%20Shabtay%20and%20Felipe%20Maia%20Polo%20and%20Sivan%20Doveh%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Leshem%20Chosen%20and%20Mikhail%20Yurochkin%20and%20Yuekai%20Sun%20and%20Assaf%20Arbelle%20and%20Leonid%20Karlinsky%20and%20Raja%20Giryes&entry.1292438233=%20%20The%20large-scale%20training%20of%20multi-modal%20models%20on%20data%20scraped%20from%20the%20web%0Ahas%20shown%20outstanding%20utility%20in%20infusing%20these%20models%20with%20the%20required%20world%0Aknowledge%20to%20perform%20effectively%20on%20multiple%20downstream%20tasks.%20However%2C%20one%0Adownside%20of%20scraping%20data%20from%20the%20web%20can%20be%20the%20potential%20sacrifice%20of%20the%0Abenchmarks%20on%20which%20the%20abilities%20of%20these%20models%20are%20often%20evaluated.%20To%0Asafeguard%20against%20test%20data%20contamination%20and%20to%20truly%20test%20the%20abilities%20of%0Athese%20foundation%20models%20we%20propose%20LiveXiv%3A%20A%20scalable%20evolving%20live%20benchmark%0Abased%20on%20scientific%20ArXiv%20papers.%20LiveXiv%20accesses%20domain-specific%20manuscripts%0Aat%20any%20given%20timestamp%20and%20proposes%20to%20automatically%20generate%20visual%0Aquestion-answer%20pairs%20%28VQA%29.%20This%20is%20done%20without%20any%20human-in-the-loop%2C%20using%0Athe%20multi-modal%20content%20in%20the%20manuscripts%2C%20like%20graphs%2C%20charts%2C%20and%20tables.%0AMoreover%2C%20we%20introduce%20an%20efficient%20evaluation%20approach%20that%20estimates%20the%0Aperformance%20of%20all%20models%20on%20the%20evolving%20benchmark%20using%20evaluations%20of%20only%20a%0Asubset%20of%20models.%20This%20significantly%20reduces%20the%20overall%20evaluation%20cost.%20We%0Abenchmark%20multiple%20open%20and%20proprietary%20Large%20Multi-modal%20Models%20%28LMMs%29%20on%20the%0Afirst%20version%20of%20our%20benchmark%2C%20showing%20its%20challenging%20nature%20and%20exposing%20the%0Amodels%20true%20abilities%2C%20avoiding%20contamination.%20Lastly%2C%20in%20our%20commitment%20to%0Ahigh%20quality%2C%20we%20have%20collected%20and%20evaluated%20a%20manually%20verified%20subset.%20By%0Acomparing%20its%20overall%20results%20to%20our%20automatic%20annotations%2C%20we%20have%20found%20that%0Athe%20performance%20variance%20is%20indeed%20minimal%20%28%3C2.5%25%29.%20Our%20dataset%20is%20available%0Aonline%20on%20HuggingFace%2C%20and%20our%20code%20will%20be%20available%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10783v1&entry.124074799=Read"},
{"title": "Reverse Refinement Network for Narrow Rural Road Detection in\n  High-Resolution Satellite Imagery", "author": "Ningjing Wang and Xinyu Wang and Yang Pan and Wanqiang Yao and Yanfei Zhong", "abstract": "  The automated extraction of rural roads is pivotal for rural development and\ntransportation planning, serving as a cornerstone for socio-economic progress.\nCurrent research primarily focuses on road extraction in urban areas. However,\nrural roads present unique challenges due to their narrow and irregular nature,\nposing significant difficulties for road extraction. In this article, a reverse\nrefinement network (R2-Net) is proposed to extract narrow rural roads,\nenhancing their connectivity and distinctiveness from the background.\nSpecifically, to preserve the fine details of roads within high-resolution\nfeature maps, R2-Net utilizes an axis context aware module (ACAM) to capture\nthe long-distance spatial context information in various layers. Subsequently,\nthe multi-level features are aggregated through a global aggregation module\n(GAM). Moreover, in the decoder stage, R2-Net employs a reverse-aware module\n(RAM) to direct the attention of the network to the complex background, thus\namplifying its separability. In experiments, we compare R2-Net with several\nstate-of-the-art methods using the DeepGlobe road extraction dataset and the\nWHU-RuR+ global large-scale rural road dataset. R2-Net achieved superior\nperformance and especially excelled in accurately detecting narrow roads.\nFurthermore, we explored the applicability of R2-Net for large-scale rural road\nmapping. The results show that the proposed R2-Net has significant performance\nadvantages for large-scale rural road mapping applications.\n", "link": "http://arxiv.org/abs/2410.10389v1", "date": "2024-10-14", "relevancy": 2.6064, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5284}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reverse%20Refinement%20Network%20for%20Narrow%20Rural%20Road%20Detection%20in%0A%20%20High-Resolution%20Satellite%20Imagery&body=Title%3A%20Reverse%20Refinement%20Network%20for%20Narrow%20Rural%20Road%20Detection%20in%0A%20%20High-Resolution%20Satellite%20Imagery%0AAuthor%3A%20Ningjing%20Wang%20and%20Xinyu%20Wang%20and%20Yang%20Pan%20and%20Wanqiang%20Yao%20and%20Yanfei%20Zhong%0AAbstract%3A%20%20%20The%20automated%20extraction%20of%20rural%20roads%20is%20pivotal%20for%20rural%20development%20and%0Atransportation%20planning%2C%20serving%20as%20a%20cornerstone%20for%20socio-economic%20progress.%0ACurrent%20research%20primarily%20focuses%20on%20road%20extraction%20in%20urban%20areas.%20However%2C%0Arural%20roads%20present%20unique%20challenges%20due%20to%20their%20narrow%20and%20irregular%20nature%2C%0Aposing%20significant%20difficulties%20for%20road%20extraction.%20In%20this%20article%2C%20a%20reverse%0Arefinement%20network%20%28R2-Net%29%20is%20proposed%20to%20extract%20narrow%20rural%20roads%2C%0Aenhancing%20their%20connectivity%20and%20distinctiveness%20from%20the%20background.%0ASpecifically%2C%20to%20preserve%20the%20fine%20details%20of%20roads%20within%20high-resolution%0Afeature%20maps%2C%20R2-Net%20utilizes%20an%20axis%20context%20aware%20module%20%28ACAM%29%20to%20capture%0Athe%20long-distance%20spatial%20context%20information%20in%20various%20layers.%20Subsequently%2C%0Athe%20multi-level%20features%20are%20aggregated%20through%20a%20global%20aggregation%20module%0A%28GAM%29.%20Moreover%2C%20in%20the%20decoder%20stage%2C%20R2-Net%20employs%20a%20reverse-aware%20module%0A%28RAM%29%20to%20direct%20the%20attention%20of%20the%20network%20to%20the%20complex%20background%2C%20thus%0Aamplifying%20its%20separability.%20In%20experiments%2C%20we%20compare%20R2-Net%20with%20several%0Astate-of-the-art%20methods%20using%20the%20DeepGlobe%20road%20extraction%20dataset%20and%20the%0AWHU-RuR%2B%20global%20large-scale%20rural%20road%20dataset.%20R2-Net%20achieved%20superior%0Aperformance%20and%20especially%20excelled%20in%20accurately%20detecting%20narrow%20roads.%0AFurthermore%2C%20we%20explored%20the%20applicability%20of%20R2-Net%20for%20large-scale%20rural%20road%0Amapping.%20The%20results%20show%20that%20the%20proposed%20R2-Net%20has%20significant%20performance%0Aadvantages%20for%20large-scale%20rural%20road%20mapping%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReverse%2520Refinement%2520Network%2520for%2520Narrow%2520Rural%2520Road%2520Detection%2520in%250A%2520%2520High-Resolution%2520Satellite%2520Imagery%26entry.906535625%3DNingjing%2520Wang%2520and%2520Xinyu%2520Wang%2520and%2520Yang%2520Pan%2520and%2520Wanqiang%2520Yao%2520and%2520Yanfei%2520Zhong%26entry.1292438233%3D%2520%2520The%2520automated%2520extraction%2520of%2520rural%2520roads%2520is%2520pivotal%2520for%2520rural%2520development%2520and%250Atransportation%2520planning%252C%2520serving%2520as%2520a%2520cornerstone%2520for%2520socio-economic%2520progress.%250ACurrent%2520research%2520primarily%2520focuses%2520on%2520road%2520extraction%2520in%2520urban%2520areas.%2520However%252C%250Arural%2520roads%2520present%2520unique%2520challenges%2520due%2520to%2520their%2520narrow%2520and%2520irregular%2520nature%252C%250Aposing%2520significant%2520difficulties%2520for%2520road%2520extraction.%2520In%2520this%2520article%252C%2520a%2520reverse%250Arefinement%2520network%2520%2528R2-Net%2529%2520is%2520proposed%2520to%2520extract%2520narrow%2520rural%2520roads%252C%250Aenhancing%2520their%2520connectivity%2520and%2520distinctiveness%2520from%2520the%2520background.%250ASpecifically%252C%2520to%2520preserve%2520the%2520fine%2520details%2520of%2520roads%2520within%2520high-resolution%250Afeature%2520maps%252C%2520R2-Net%2520utilizes%2520an%2520axis%2520context%2520aware%2520module%2520%2528ACAM%2529%2520to%2520capture%250Athe%2520long-distance%2520spatial%2520context%2520information%2520in%2520various%2520layers.%2520Subsequently%252C%250Athe%2520multi-level%2520features%2520are%2520aggregated%2520through%2520a%2520global%2520aggregation%2520module%250A%2528GAM%2529.%2520Moreover%252C%2520in%2520the%2520decoder%2520stage%252C%2520R2-Net%2520employs%2520a%2520reverse-aware%2520module%250A%2528RAM%2529%2520to%2520direct%2520the%2520attention%2520of%2520the%2520network%2520to%2520the%2520complex%2520background%252C%2520thus%250Aamplifying%2520its%2520separability.%2520In%2520experiments%252C%2520we%2520compare%2520R2-Net%2520with%2520several%250Astate-of-the-art%2520methods%2520using%2520the%2520DeepGlobe%2520road%2520extraction%2520dataset%2520and%2520the%250AWHU-RuR%252B%2520global%2520large-scale%2520rural%2520road%2520dataset.%2520R2-Net%2520achieved%2520superior%250Aperformance%2520and%2520especially%2520excelled%2520in%2520accurately%2520detecting%2520narrow%2520roads.%250AFurthermore%252C%2520we%2520explored%2520the%2520applicability%2520of%2520R2-Net%2520for%2520large-scale%2520rural%2520road%250Amapping.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520R2-Net%2520has%2520significant%2520performance%250Aadvantages%2520for%2520large-scale%2520rural%2520road%2520mapping%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reverse%20Refinement%20Network%20for%20Narrow%20Rural%20Road%20Detection%20in%0A%20%20High-Resolution%20Satellite%20Imagery&entry.906535625=Ningjing%20Wang%20and%20Xinyu%20Wang%20and%20Yang%20Pan%20and%20Wanqiang%20Yao%20and%20Yanfei%20Zhong&entry.1292438233=%20%20The%20automated%20extraction%20of%20rural%20roads%20is%20pivotal%20for%20rural%20development%20and%0Atransportation%20planning%2C%20serving%20as%20a%20cornerstone%20for%20socio-economic%20progress.%0ACurrent%20research%20primarily%20focuses%20on%20road%20extraction%20in%20urban%20areas.%20However%2C%0Arural%20roads%20present%20unique%20challenges%20due%20to%20their%20narrow%20and%20irregular%20nature%2C%0Aposing%20significant%20difficulties%20for%20road%20extraction.%20In%20this%20article%2C%20a%20reverse%0Arefinement%20network%20%28R2-Net%29%20is%20proposed%20to%20extract%20narrow%20rural%20roads%2C%0Aenhancing%20their%20connectivity%20and%20distinctiveness%20from%20the%20background.%0ASpecifically%2C%20to%20preserve%20the%20fine%20details%20of%20roads%20within%20high-resolution%0Afeature%20maps%2C%20R2-Net%20utilizes%20an%20axis%20context%20aware%20module%20%28ACAM%29%20to%20capture%0Athe%20long-distance%20spatial%20context%20information%20in%20various%20layers.%20Subsequently%2C%0Athe%20multi-level%20features%20are%20aggregated%20through%20a%20global%20aggregation%20module%0A%28GAM%29.%20Moreover%2C%20in%20the%20decoder%20stage%2C%20R2-Net%20employs%20a%20reverse-aware%20module%0A%28RAM%29%20to%20direct%20the%20attention%20of%20the%20network%20to%20the%20complex%20background%2C%20thus%0Aamplifying%20its%20separability.%20In%20experiments%2C%20we%20compare%20R2-Net%20with%20several%0Astate-of-the-art%20methods%20using%20the%20DeepGlobe%20road%20extraction%20dataset%20and%20the%0AWHU-RuR%2B%20global%20large-scale%20rural%20road%20dataset.%20R2-Net%20achieved%20superior%0Aperformance%20and%20especially%20excelled%20in%20accurately%20detecting%20narrow%20roads.%0AFurthermore%2C%20we%20explored%20the%20applicability%20of%20R2-Net%20for%20large-scale%20rural%20road%0Amapping.%20The%20results%20show%20that%20the%20proposed%20R2-Net%20has%20significant%20performance%0Aadvantages%20for%20large-scale%20rural%20road%20mapping%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10389v1&entry.124074799=Read"},
{"title": "Pubic Symphysis-Fetal Head Segmentation Network Using BiFormer Attention\n  Mechanism and Multipath Dilated Convolution", "author": "Pengzhou Cai and Lu Jiang and Yanxin Li and Xiaojuan Liu and Libin Lan", "abstract": "  Pubic symphysis-fetal head segmentation in transperineal ultrasound images\nplays a critical role for the assessment of fetal head descent and progression.\nExisting transformer \\iffalse-based\\fi segmentation methods based on sparse\nattention mechanism use handcrafted static patterns, which leads to great\ndifferences \\iffalse in \\fi in terms of segmentation performance on specific\ndatasets. To address this issue, we introduce a dynamic, query-aware sparse\nattention mechanism for ultrasound image segmentation. Specifically, we propose\na novel method, named BRAU-Net to solve the pubic symphysis-fetal head\nsegmentation task in this paper. The method adopts a U-Net-like encoder-decoder\narchitecture with bi-level routing attention and skip connections, which\neffectively learns local-global semantic information. In addition, we propose\nan inverted bottleneck patch expanding (IBPE) module to reduce information loss\nwhile performing up-sampling operations. The proposed BRAU-Net is evaluated on\nFH-PS-AoP and HC18 datasets. The results demonstrate that our method could\nachieve excellent segmentation results. The code is available on GitHub.\n", "link": "http://arxiv.org/abs/2410.10352v1", "date": "2024-10-14", "relevancy": 2.6027, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5395}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5136}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pubic%20Symphysis-Fetal%20Head%20Segmentation%20Network%20Using%20BiFormer%20Attention%0A%20%20Mechanism%20and%20Multipath%20Dilated%20Convolution&body=Title%3A%20Pubic%20Symphysis-Fetal%20Head%20Segmentation%20Network%20Using%20BiFormer%20Attention%0A%20%20Mechanism%20and%20Multipath%20Dilated%20Convolution%0AAuthor%3A%20Pengzhou%20Cai%20and%20Lu%20Jiang%20and%20Yanxin%20Li%20and%20Xiaojuan%20Liu%20and%20Libin%20Lan%0AAbstract%3A%20%20%20Pubic%20symphysis-fetal%20head%20segmentation%20in%20transperineal%20ultrasound%20images%0Aplays%20a%20critical%20role%20for%20the%20assessment%20of%20fetal%20head%20descent%20and%20progression.%0AExisting%20transformer%20%5Ciffalse-based%5Cfi%20segmentation%20methods%20based%20on%20sparse%0Aattention%20mechanism%20use%20handcrafted%20static%20patterns%2C%20which%20leads%20to%20great%0Adifferences%20%5Ciffalse%20in%20%5Cfi%20in%20terms%20of%20segmentation%20performance%20on%20specific%0Adatasets.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20dynamic%2C%20query-aware%20sparse%0Aattention%20mechanism%20for%20ultrasound%20image%20segmentation.%20Specifically%2C%20we%20propose%0Aa%20novel%20method%2C%20named%20BRAU-Net%20to%20solve%20the%20pubic%20symphysis-fetal%20head%0Asegmentation%20task%20in%20this%20paper.%20The%20method%20adopts%20a%20U-Net-like%20encoder-decoder%0Aarchitecture%20with%20bi-level%20routing%20attention%20and%20skip%20connections%2C%20which%0Aeffectively%20learns%20local-global%20semantic%20information.%20In%20addition%2C%20we%20propose%0Aan%20inverted%20bottleneck%20patch%20expanding%20%28IBPE%29%20module%20to%20reduce%20information%20loss%0Awhile%20performing%20up-sampling%20operations.%20The%20proposed%20BRAU-Net%20is%20evaluated%20on%0AFH-PS-AoP%20and%20HC18%20datasets.%20The%20results%20demonstrate%20that%20our%20method%20could%0Aachieve%20excellent%20segmentation%20results.%20The%20code%20is%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPubic%2520Symphysis-Fetal%2520Head%2520Segmentation%2520Network%2520Using%2520BiFormer%2520Attention%250A%2520%2520Mechanism%2520and%2520Multipath%2520Dilated%2520Convolution%26entry.906535625%3DPengzhou%2520Cai%2520and%2520Lu%2520Jiang%2520and%2520Yanxin%2520Li%2520and%2520Xiaojuan%2520Liu%2520and%2520Libin%2520Lan%26entry.1292438233%3D%2520%2520Pubic%2520symphysis-fetal%2520head%2520segmentation%2520in%2520transperineal%2520ultrasound%2520images%250Aplays%2520a%2520critical%2520role%2520for%2520the%2520assessment%2520of%2520fetal%2520head%2520descent%2520and%2520progression.%250AExisting%2520transformer%2520%255Ciffalse-based%255Cfi%2520segmentation%2520methods%2520based%2520on%2520sparse%250Aattention%2520mechanism%2520use%2520handcrafted%2520static%2520patterns%252C%2520which%2520leads%2520to%2520great%250Adifferences%2520%255Ciffalse%2520in%2520%255Cfi%2520in%2520terms%2520of%2520segmentation%2520performance%2520on%2520specific%250Adatasets.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520dynamic%252C%2520query-aware%2520sparse%250Aattention%2520mechanism%2520for%2520ultrasound%2520image%2520segmentation.%2520Specifically%252C%2520we%2520propose%250Aa%2520novel%2520method%252C%2520named%2520BRAU-Net%2520to%2520solve%2520the%2520pubic%2520symphysis-fetal%2520head%250Asegmentation%2520task%2520in%2520this%2520paper.%2520The%2520method%2520adopts%2520a%2520U-Net-like%2520encoder-decoder%250Aarchitecture%2520with%2520bi-level%2520routing%2520attention%2520and%2520skip%2520connections%252C%2520which%250Aeffectively%2520learns%2520local-global%2520semantic%2520information.%2520In%2520addition%252C%2520we%2520propose%250Aan%2520inverted%2520bottleneck%2520patch%2520expanding%2520%2528IBPE%2529%2520module%2520to%2520reduce%2520information%2520loss%250Awhile%2520performing%2520up-sampling%2520operations.%2520The%2520proposed%2520BRAU-Net%2520is%2520evaluated%2520on%250AFH-PS-AoP%2520and%2520HC18%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520our%2520method%2520could%250Aachieve%2520excellent%2520segmentation%2520results.%2520The%2520code%2520is%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pubic%20Symphysis-Fetal%20Head%20Segmentation%20Network%20Using%20BiFormer%20Attention%0A%20%20Mechanism%20and%20Multipath%20Dilated%20Convolution&entry.906535625=Pengzhou%20Cai%20and%20Lu%20Jiang%20and%20Yanxin%20Li%20and%20Xiaojuan%20Liu%20and%20Libin%20Lan&entry.1292438233=%20%20Pubic%20symphysis-fetal%20head%20segmentation%20in%20transperineal%20ultrasound%20images%0Aplays%20a%20critical%20role%20for%20the%20assessment%20of%20fetal%20head%20descent%20and%20progression.%0AExisting%20transformer%20%5Ciffalse-based%5Cfi%20segmentation%20methods%20based%20on%20sparse%0Aattention%20mechanism%20use%20handcrafted%20static%20patterns%2C%20which%20leads%20to%20great%0Adifferences%20%5Ciffalse%20in%20%5Cfi%20in%20terms%20of%20segmentation%20performance%20on%20specific%0Adatasets.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20dynamic%2C%20query-aware%20sparse%0Aattention%20mechanism%20for%20ultrasound%20image%20segmentation.%20Specifically%2C%20we%20propose%0Aa%20novel%20method%2C%20named%20BRAU-Net%20to%20solve%20the%20pubic%20symphysis-fetal%20head%0Asegmentation%20task%20in%20this%20paper.%20The%20method%20adopts%20a%20U-Net-like%20encoder-decoder%0Aarchitecture%20with%20bi-level%20routing%20attention%20and%20skip%20connections%2C%20which%0Aeffectively%20learns%20local-global%20semantic%20information.%20In%20addition%2C%20we%20propose%0Aan%20inverted%20bottleneck%20patch%20expanding%20%28IBPE%29%20module%20to%20reduce%20information%20loss%0Awhile%20performing%20up-sampling%20operations.%20The%20proposed%20BRAU-Net%20is%20evaluated%20on%0AFH-PS-AoP%20and%20HC18%20datasets.%20The%20results%20demonstrate%20that%20our%20method%20could%0Aachieve%20excellent%20segmentation%20results.%20The%20code%20is%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10352v1&entry.124074799=Read"},
{"title": "Continual Learning Improves Zero-Shot Action Recognition", "author": "Shreyank N Gowda and Davide Moltisanti and Laura Sevilla-Lara", "abstract": "  Zero-shot action recognition requires a strong ability to generalize from\npre-training and seen classes to novel unseen classes. Similarly, continual\nlearning aims to develop models that can generalize effectively and learn new\ntasks without forgetting the ones previously learned. The generalization goals\nof zero-shot and continual learning are closely aligned, however techniques\nfrom continual learning have not been applied to zero-shot action recognition.\nIn this paper, we propose a novel method based on continual learning to address\nzero-shot action recognition. This model, which we call {\\em Generative\nIterative Learning} (GIL) uses a memory of synthesized features of past\nclasses, and combines these synthetic features with real ones from novel\nclasses. The memory is used to train a classification model, ensuring a\nbalanced exposure to both old and new classes. Experiments demonstrate that\n{\\em GIL} improves generalization in unseen classes, achieving a new\nstate-of-the-art in zero-shot recognition across multiple benchmarks.\nImportantly, {\\em GIL} also boosts performance in the more challenging\ngeneralized zero-shot setting, where models need to retain knowledge about\nclasses seen before fine-tuning.\n", "link": "http://arxiv.org/abs/2410.10497v1", "date": "2024-10-14", "relevancy": 2.5947, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5521}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20Improves%20Zero-Shot%20Action%20Recognition&body=Title%3A%20Continual%20Learning%20Improves%20Zero-Shot%20Action%20Recognition%0AAuthor%3A%20Shreyank%20N%20Gowda%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara%0AAbstract%3A%20%20%20Zero-shot%20action%20recognition%20requires%20a%20strong%20ability%20to%20generalize%20from%0Apre-training%20and%20seen%20classes%20to%20novel%20unseen%20classes.%20Similarly%2C%20continual%0Alearning%20aims%20to%20develop%20models%20that%20can%20generalize%20effectively%20and%20learn%20new%0Atasks%20without%20forgetting%20the%20ones%20previously%20learned.%20The%20generalization%20goals%0Aof%20zero-shot%20and%20continual%20learning%20are%20closely%20aligned%2C%20however%20techniques%0Afrom%20continual%20learning%20have%20not%20been%20applied%20to%20zero-shot%20action%20recognition.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20method%20based%20on%20continual%20learning%20to%20address%0Azero-shot%20action%20recognition.%20This%20model%2C%20which%20we%20call%20%7B%5Cem%20Generative%0AIterative%20Learning%7D%20%28GIL%29%20uses%20a%20memory%20of%20synthesized%20features%20of%20past%0Aclasses%2C%20and%20combines%20these%20synthetic%20features%20with%20real%20ones%20from%20novel%0Aclasses.%20The%20memory%20is%20used%20to%20train%20a%20classification%20model%2C%20ensuring%20a%0Abalanced%20exposure%20to%20both%20old%20and%20new%20classes.%20Experiments%20demonstrate%20that%0A%7B%5Cem%20GIL%7D%20improves%20generalization%20in%20unseen%20classes%2C%20achieving%20a%20new%0Astate-of-the-art%20in%20zero-shot%20recognition%20across%20multiple%20benchmarks.%0AImportantly%2C%20%7B%5Cem%20GIL%7D%20also%20boosts%20performance%20in%20the%20more%20challenging%0Ageneralized%20zero-shot%20setting%2C%20where%20models%20need%20to%20retain%20knowledge%20about%0Aclasses%20seen%20before%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520Improves%2520Zero-Shot%2520Action%2520Recognition%26entry.906535625%3DShreyank%2520N%2520Gowda%2520and%2520Davide%2520Moltisanti%2520and%2520Laura%2520Sevilla-Lara%26entry.1292438233%3D%2520%2520Zero-shot%2520action%2520recognition%2520requires%2520a%2520strong%2520ability%2520to%2520generalize%2520from%250Apre-training%2520and%2520seen%2520classes%2520to%2520novel%2520unseen%2520classes.%2520Similarly%252C%2520continual%250Alearning%2520aims%2520to%2520develop%2520models%2520that%2520can%2520generalize%2520effectively%2520and%2520learn%2520new%250Atasks%2520without%2520forgetting%2520the%2520ones%2520previously%2520learned.%2520The%2520generalization%2520goals%250Aof%2520zero-shot%2520and%2520continual%2520learning%2520are%2520closely%2520aligned%252C%2520however%2520techniques%250Afrom%2520continual%2520learning%2520have%2520not%2520been%2520applied%2520to%2520zero-shot%2520action%2520recognition.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520based%2520on%2520continual%2520learning%2520to%2520address%250Azero-shot%2520action%2520recognition.%2520This%2520model%252C%2520which%2520we%2520call%2520%257B%255Cem%2520Generative%250AIterative%2520Learning%257D%2520%2528GIL%2529%2520uses%2520a%2520memory%2520of%2520synthesized%2520features%2520of%2520past%250Aclasses%252C%2520and%2520combines%2520these%2520synthetic%2520features%2520with%2520real%2520ones%2520from%2520novel%250Aclasses.%2520The%2520memory%2520is%2520used%2520to%2520train%2520a%2520classification%2520model%252C%2520ensuring%2520a%250Abalanced%2520exposure%2520to%2520both%2520old%2520and%2520new%2520classes.%2520Experiments%2520demonstrate%2520that%250A%257B%255Cem%2520GIL%257D%2520improves%2520generalization%2520in%2520unseen%2520classes%252C%2520achieving%2520a%2520new%250Astate-of-the-art%2520in%2520zero-shot%2520recognition%2520across%2520multiple%2520benchmarks.%250AImportantly%252C%2520%257B%255Cem%2520GIL%257D%2520also%2520boosts%2520performance%2520in%2520the%2520more%2520challenging%250Ageneralized%2520zero-shot%2520setting%252C%2520where%2520models%2520need%2520to%2520retain%2520knowledge%2520about%250Aclasses%2520seen%2520before%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20Improves%20Zero-Shot%20Action%20Recognition&entry.906535625=Shreyank%20N%20Gowda%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara&entry.1292438233=%20%20Zero-shot%20action%20recognition%20requires%20a%20strong%20ability%20to%20generalize%20from%0Apre-training%20and%20seen%20classes%20to%20novel%20unseen%20classes.%20Similarly%2C%20continual%0Alearning%20aims%20to%20develop%20models%20that%20can%20generalize%20effectively%20and%20learn%20new%0Atasks%20without%20forgetting%20the%20ones%20previously%20learned.%20The%20generalization%20goals%0Aof%20zero-shot%20and%20continual%20learning%20are%20closely%20aligned%2C%20however%20techniques%0Afrom%20continual%20learning%20have%20not%20been%20applied%20to%20zero-shot%20action%20recognition.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20method%20based%20on%20continual%20learning%20to%20address%0Azero-shot%20action%20recognition.%20This%20model%2C%20which%20we%20call%20%7B%5Cem%20Generative%0AIterative%20Learning%7D%20%28GIL%29%20uses%20a%20memory%20of%20synthesized%20features%20of%20past%0Aclasses%2C%20and%20combines%20these%20synthetic%20features%20with%20real%20ones%20from%20novel%0Aclasses.%20The%20memory%20is%20used%20to%20train%20a%20classification%20model%2C%20ensuring%20a%0Abalanced%20exposure%20to%20both%20old%20and%20new%20classes.%20Experiments%20demonstrate%20that%0A%7B%5Cem%20GIL%7D%20improves%20generalization%20in%20unseen%20classes%2C%20achieving%20a%20new%0Astate-of-the-art%20in%20zero-shot%20recognition%20across%20multiple%20benchmarks.%0AImportantly%2C%20%7B%5Cem%20GIL%7D%20also%20boosts%20performance%20in%20the%20more%20challenging%0Ageneralized%20zero-shot%20setting%2C%20where%20models%20need%20to%20retain%20knowledge%20about%0Aclasses%20seen%20before%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10497v1&entry.124074799=Read"},
{"title": "DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched\n  Driving World Model", "author": "Yuqi Wang and Ke Cheng and Jiawei He and Qitai Wang and Hengchen Dai and Yuntao Chen and Fei Xia and Zhaoxiang Zhang", "abstract": "  Driving world models have gained increasing attention due to their ability to\nmodel complex physical dynamics. However, their superb modeling capability is\nyet to be fully unleashed due to the limited video diversity in current driving\ndatasets. We introduce DrivingDojo, the first dataset tailor-made for training\ninteractive world models with complex driving dynamics. Our dataset features\nvideo clips with a complete set of driving maneuvers, diverse multi-agent\ninterplay, and rich open-world driving knowledge, laying a stepping stone for\nfuture world model development. We further define an action instruction\nfollowing (AIF) benchmark for world models and demonstrate the superiority of\nthe proposed dataset for generating action-controlled future predictions.\n", "link": "http://arxiv.org/abs/2410.10738v1", "date": "2024-10-14", "relevancy": 2.5794, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrivingDojo%20Dataset%3A%20Advancing%20Interactive%20and%20Knowledge-Enriched%0A%20%20Driving%20World%20Model&body=Title%3A%20DrivingDojo%20Dataset%3A%20Advancing%20Interactive%20and%20Knowledge-Enriched%0A%20%20Driving%20World%20Model%0AAuthor%3A%20Yuqi%20Wang%20and%20Ke%20Cheng%20and%20Jiawei%20He%20and%20Qitai%20Wang%20and%20Hengchen%20Dai%20and%20Yuntao%20Chen%20and%20Fei%20Xia%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Driving%20world%20models%20have%20gained%20increasing%20attention%20due%20to%20their%20ability%20to%0Amodel%20complex%20physical%20dynamics.%20However%2C%20their%20superb%20modeling%20capability%20is%0Ayet%20to%20be%20fully%20unleashed%20due%20to%20the%20limited%20video%20diversity%20in%20current%20driving%0Adatasets.%20We%20introduce%20DrivingDojo%2C%20the%20first%20dataset%20tailor-made%20for%20training%0Ainteractive%20world%20models%20with%20complex%20driving%20dynamics.%20Our%20dataset%20features%0Avideo%20clips%20with%20a%20complete%20set%20of%20driving%20maneuvers%2C%20diverse%20multi-agent%0Ainterplay%2C%20and%20rich%20open-world%20driving%20knowledge%2C%20laying%20a%20stepping%20stone%20for%0Afuture%20world%20model%20development.%20We%20further%20define%20an%20action%20instruction%0Afollowing%20%28AIF%29%20benchmark%20for%20world%20models%20and%20demonstrate%20the%20superiority%20of%0Athe%20proposed%20dataset%20for%20generating%20action-controlled%20future%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrivingDojo%2520Dataset%253A%2520Advancing%2520Interactive%2520and%2520Knowledge-Enriched%250A%2520%2520Driving%2520World%2520Model%26entry.906535625%3DYuqi%2520Wang%2520and%2520Ke%2520Cheng%2520and%2520Jiawei%2520He%2520and%2520Qitai%2520Wang%2520and%2520Hengchen%2520Dai%2520and%2520Yuntao%2520Chen%2520and%2520Fei%2520Xia%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Driving%2520world%2520models%2520have%2520gained%2520increasing%2520attention%2520due%2520to%2520their%2520ability%2520to%250Amodel%2520complex%2520physical%2520dynamics.%2520However%252C%2520their%2520superb%2520modeling%2520capability%2520is%250Ayet%2520to%2520be%2520fully%2520unleashed%2520due%2520to%2520the%2520limited%2520video%2520diversity%2520in%2520current%2520driving%250Adatasets.%2520We%2520introduce%2520DrivingDojo%252C%2520the%2520first%2520dataset%2520tailor-made%2520for%2520training%250Ainteractive%2520world%2520models%2520with%2520complex%2520driving%2520dynamics.%2520Our%2520dataset%2520features%250Avideo%2520clips%2520with%2520a%2520complete%2520set%2520of%2520driving%2520maneuvers%252C%2520diverse%2520multi-agent%250Ainterplay%252C%2520and%2520rich%2520open-world%2520driving%2520knowledge%252C%2520laying%2520a%2520stepping%2520stone%2520for%250Afuture%2520world%2520model%2520development.%2520We%2520further%2520define%2520an%2520action%2520instruction%250Afollowing%2520%2528AIF%2529%2520benchmark%2520for%2520world%2520models%2520and%2520demonstrate%2520the%2520superiority%2520of%250Athe%2520proposed%2520dataset%2520for%2520generating%2520action-controlled%2520future%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrivingDojo%20Dataset%3A%20Advancing%20Interactive%20and%20Knowledge-Enriched%0A%20%20Driving%20World%20Model&entry.906535625=Yuqi%20Wang%20and%20Ke%20Cheng%20and%20Jiawei%20He%20and%20Qitai%20Wang%20and%20Hengchen%20Dai%20and%20Yuntao%20Chen%20and%20Fei%20Xia%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Driving%20world%20models%20have%20gained%20increasing%20attention%20due%20to%20their%20ability%20to%0Amodel%20complex%20physical%20dynamics.%20However%2C%20their%20superb%20modeling%20capability%20is%0Ayet%20to%20be%20fully%20unleashed%20due%20to%20the%20limited%20video%20diversity%20in%20current%20driving%0Adatasets.%20We%20introduce%20DrivingDojo%2C%20the%20first%20dataset%20tailor-made%20for%20training%0Ainteractive%20world%20models%20with%20complex%20driving%20dynamics.%20Our%20dataset%20features%0Avideo%20clips%20with%20a%20complete%20set%20of%20driving%20maneuvers%2C%20diverse%20multi-agent%0Ainterplay%2C%20and%20rich%20open-world%20driving%20knowledge%2C%20laying%20a%20stepping%20stone%20for%0Afuture%20world%20model%20development.%20We%20further%20define%20an%20action%20instruction%0Afollowing%20%28AIF%29%20benchmark%20for%20world%20models%20and%20demonstrate%20the%20superiority%20of%0Athe%20proposed%20dataset%20for%20generating%20action-controlled%20future%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10738v1&entry.124074799=Read"},
{"title": "Autoencoded Image Compression for Secure and Fast Transmission", "author": "Aryan Kashyap Naveen and Sunil Thunga and Anuhya Murki and Mahati A Kalale and Shriya Anil", "abstract": "  With exponential growth in the use of digital image data, the need for\nefficient transmission methods has become imperative. Traditional image\ncompression techniques often sacrifice image fidelity for reduced file sizes,\nchallenging maintaining quality and efficiency. They also compromise security,\nleaving images vulnerable to threats such as man-in-the-middle attacks. This\npaper proposes an autoencoder architecture for image compression to not only\nhelp in dimensionality reduction but also inherently encrypt the images. The\npaper also introduces a composite loss function that combines reconstruction\nloss and residual loss for improved performance. The autoencoder architecture\nis designed to achieve optimal dimensionality reduction and regeneration\naccuracy while safeguarding the compressed data during transmission or storage.\nImages regenerated by the autoencoder are evaluated against three key metrics:\nreconstruction quality, compression ratio, and one-way delay during image\ntransfer. The experiments reveal that the proposed architecture achieves an\nSSIM of 97.5% over the regenerated images and an average latency reduction of\n87.5%, indicating its effectiveness as a secure and efficient solution for\ncompressed image transfer.\n", "link": "http://arxiv.org/abs/2407.03990v2", "date": "2024-10-14", "relevancy": 2.564, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5225}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.521}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoded%20Image%20Compression%20for%20Secure%20and%20Fast%20Transmission&body=Title%3A%20Autoencoded%20Image%20Compression%20for%20Secure%20and%20Fast%20Transmission%0AAuthor%3A%20Aryan%20Kashyap%20Naveen%20and%20Sunil%20Thunga%20and%20Anuhya%20Murki%20and%20Mahati%20A%20Kalale%20and%20Shriya%20Anil%0AAbstract%3A%20%20%20With%20exponential%20growth%20in%20the%20use%20of%20digital%20image%20data%2C%20the%20need%20for%0Aefficient%20transmission%20methods%20has%20become%20imperative.%20Traditional%20image%0Acompression%20techniques%20often%20sacrifice%20image%20fidelity%20for%20reduced%20file%20sizes%2C%0Achallenging%20maintaining%20quality%20and%20efficiency.%20They%20also%20compromise%20security%2C%0Aleaving%20images%20vulnerable%20to%20threats%20such%20as%20man-in-the-middle%20attacks.%20This%0Apaper%20proposes%20an%20autoencoder%20architecture%20for%20image%20compression%20to%20not%20only%0Ahelp%20in%20dimensionality%20reduction%20but%20also%20inherently%20encrypt%20the%20images.%20The%0Apaper%20also%20introduces%20a%20composite%20loss%20function%20that%20combines%20reconstruction%0Aloss%20and%20residual%20loss%20for%20improved%20performance.%20The%20autoencoder%20architecture%0Ais%20designed%20to%20achieve%20optimal%20dimensionality%20reduction%20and%20regeneration%0Aaccuracy%20while%20safeguarding%20the%20compressed%20data%20during%20transmission%20or%20storage.%0AImages%20regenerated%20by%20the%20autoencoder%20are%20evaluated%20against%20three%20key%20metrics%3A%0Areconstruction%20quality%2C%20compression%20ratio%2C%20and%20one-way%20delay%20during%20image%0Atransfer.%20The%20experiments%20reveal%20that%20the%20proposed%20architecture%20achieves%20an%0ASSIM%20of%2097.5%25%20over%20the%20regenerated%20images%20and%20an%20average%20latency%20reduction%20of%0A87.5%25%2C%20indicating%20its%20effectiveness%20as%20a%20secure%20and%20efficient%20solution%20for%0Acompressed%20image%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoded%2520Image%2520Compression%2520for%2520Secure%2520and%2520Fast%2520Transmission%26entry.906535625%3DAryan%2520Kashyap%2520Naveen%2520and%2520Sunil%2520Thunga%2520and%2520Anuhya%2520Murki%2520and%2520Mahati%2520A%2520Kalale%2520and%2520Shriya%2520Anil%26entry.1292438233%3D%2520%2520With%2520exponential%2520growth%2520in%2520the%2520use%2520of%2520digital%2520image%2520data%252C%2520the%2520need%2520for%250Aefficient%2520transmission%2520methods%2520has%2520become%2520imperative.%2520Traditional%2520image%250Acompression%2520techniques%2520often%2520sacrifice%2520image%2520fidelity%2520for%2520reduced%2520file%2520sizes%252C%250Achallenging%2520maintaining%2520quality%2520and%2520efficiency.%2520They%2520also%2520compromise%2520security%252C%250Aleaving%2520images%2520vulnerable%2520to%2520threats%2520such%2520as%2520man-in-the-middle%2520attacks.%2520This%250Apaper%2520proposes%2520an%2520autoencoder%2520architecture%2520for%2520image%2520compression%2520to%2520not%2520only%250Ahelp%2520in%2520dimensionality%2520reduction%2520but%2520also%2520inherently%2520encrypt%2520the%2520images.%2520The%250Apaper%2520also%2520introduces%2520a%2520composite%2520loss%2520function%2520that%2520combines%2520reconstruction%250Aloss%2520and%2520residual%2520loss%2520for%2520improved%2520performance.%2520The%2520autoencoder%2520architecture%250Ais%2520designed%2520to%2520achieve%2520optimal%2520dimensionality%2520reduction%2520and%2520regeneration%250Aaccuracy%2520while%2520safeguarding%2520the%2520compressed%2520data%2520during%2520transmission%2520or%2520storage.%250AImages%2520regenerated%2520by%2520the%2520autoencoder%2520are%2520evaluated%2520against%2520three%2520key%2520metrics%253A%250Areconstruction%2520quality%252C%2520compression%2520ratio%252C%2520and%2520one-way%2520delay%2520during%2520image%250Atransfer.%2520The%2520experiments%2520reveal%2520that%2520the%2520proposed%2520architecture%2520achieves%2520an%250ASSIM%2520of%252097.5%2525%2520over%2520the%2520regenerated%2520images%2520and%2520an%2520average%2520latency%2520reduction%2520of%250A87.5%2525%252C%2520indicating%2520its%2520effectiveness%2520as%2520a%2520secure%2520and%2520efficient%2520solution%2520for%250Acompressed%2520image%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoded%20Image%20Compression%20for%20Secure%20and%20Fast%20Transmission&entry.906535625=Aryan%20Kashyap%20Naveen%20and%20Sunil%20Thunga%20and%20Anuhya%20Murki%20and%20Mahati%20A%20Kalale%20and%20Shriya%20Anil&entry.1292438233=%20%20With%20exponential%20growth%20in%20the%20use%20of%20digital%20image%20data%2C%20the%20need%20for%0Aefficient%20transmission%20methods%20has%20become%20imperative.%20Traditional%20image%0Acompression%20techniques%20often%20sacrifice%20image%20fidelity%20for%20reduced%20file%20sizes%2C%0Achallenging%20maintaining%20quality%20and%20efficiency.%20They%20also%20compromise%20security%2C%0Aleaving%20images%20vulnerable%20to%20threats%20such%20as%20man-in-the-middle%20attacks.%20This%0Apaper%20proposes%20an%20autoencoder%20architecture%20for%20image%20compression%20to%20not%20only%0Ahelp%20in%20dimensionality%20reduction%20but%20also%20inherently%20encrypt%20the%20images.%20The%0Apaper%20also%20introduces%20a%20composite%20loss%20function%20that%20combines%20reconstruction%0Aloss%20and%20residual%20loss%20for%20improved%20performance.%20The%20autoencoder%20architecture%0Ais%20designed%20to%20achieve%20optimal%20dimensionality%20reduction%20and%20regeneration%0Aaccuracy%20while%20safeguarding%20the%20compressed%20data%20during%20transmission%20or%20storage.%0AImages%20regenerated%20by%20the%20autoencoder%20are%20evaluated%20against%20three%20key%20metrics%3A%0Areconstruction%20quality%2C%20compression%20ratio%2C%20and%20one-way%20delay%20during%20image%0Atransfer.%20The%20experiments%20reveal%20that%20the%20proposed%20architecture%20achieves%20an%0ASSIM%20of%2097.5%25%20over%20the%20regenerated%20images%20and%20an%20average%20latency%20reduction%20of%0A87.5%25%2C%20indicating%20its%20effectiveness%20as%20a%20secure%20and%20efficient%20solution%20for%0Acompressed%20image%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03990v2&entry.124074799=Read"},
{"title": "TRESTLE: A Model of Concept Formation in Structured Domains", "author": "Christopher J. MacLellan and Erik Harpstead and Vincent Aleven and Kenneth R. Koedinger", "abstract": "  The literature on concept formation has demonstrated that humans are capable\nof learning concepts incrementally, with a variety of attribute types, and in\nboth supervised and unsupervised settings. Many models of concept formation\nfocus on a subset of these characteristics, but none account for all of them.\nIn this paper, we present TRESTLE, an incremental account of probabilistic\nconcept formation in structured domains that unifies prior concept learning\nmodels. TRESTLE works by creating a hierarchical categorization tree that can\nbe used to predict missing attribute values and cluster sets of examples into\nconceptually meaningful groups. It updates its knowledge by partially matching\nnovel structures and sorting them into its categorization tree. Finally, the\nsystem supports mixed-data representations, including nominal, numeric,\nrelational, and component attributes. We evaluate TRESTLE's performance on a\nsupervised learning task and an unsupervised clustering task. For both tasks,\nwe compare it to a nonincremental model and to human participants. We find that\nthis new categorization model is competitive with the nonincremental approach\nand more closely approximates human behavior on both tasks. These results serve\nas an initial demonstration of TRESTLE's capabilities and show that, by taking\nkey characteristics of human learning into account, it can better model\nbehavior than approaches that ignore them.\n", "link": "http://arxiv.org/abs/2410.10588v1", "date": "2024-10-14", "relevancy": 2.5621, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRESTLE%3A%20A%20Model%20of%20Concept%20Formation%20in%20Structured%20Domains&body=Title%3A%20TRESTLE%3A%20A%20Model%20of%20Concept%20Formation%20in%20Structured%20Domains%0AAuthor%3A%20Christopher%20J.%20MacLellan%20and%20Erik%20Harpstead%20and%20Vincent%20Aleven%20and%20Kenneth%20R.%20Koedinger%0AAbstract%3A%20%20%20The%20literature%20on%20concept%20formation%20has%20demonstrated%20that%20humans%20are%20capable%0Aof%20learning%20concepts%20incrementally%2C%20with%20a%20variety%20of%20attribute%20types%2C%20and%20in%0Aboth%20supervised%20and%20unsupervised%20settings.%20Many%20models%20of%20concept%20formation%0Afocus%20on%20a%20subset%20of%20these%20characteristics%2C%20but%20none%20account%20for%20all%20of%20them.%0AIn%20this%20paper%2C%20we%20present%20TRESTLE%2C%20an%20incremental%20account%20of%20probabilistic%0Aconcept%20formation%20in%20structured%20domains%20that%20unifies%20prior%20concept%20learning%0Amodels.%20TRESTLE%20works%20by%20creating%20a%20hierarchical%20categorization%20tree%20that%20can%0Abe%20used%20to%20predict%20missing%20attribute%20values%20and%20cluster%20sets%20of%20examples%20into%0Aconceptually%20meaningful%20groups.%20It%20updates%20its%20knowledge%20by%20partially%20matching%0Anovel%20structures%20and%20sorting%20them%20into%20its%20categorization%20tree.%20Finally%2C%20the%0Asystem%20supports%20mixed-data%20representations%2C%20including%20nominal%2C%20numeric%2C%0Arelational%2C%20and%20component%20attributes.%20We%20evaluate%20TRESTLE%27s%20performance%20on%20a%0Asupervised%20learning%20task%20and%20an%20unsupervised%20clustering%20task.%20For%20both%20tasks%2C%0Awe%20compare%20it%20to%20a%20nonincremental%20model%20and%20to%20human%20participants.%20We%20find%20that%0Athis%20new%20categorization%20model%20is%20competitive%20with%20the%20nonincremental%20approach%0Aand%20more%20closely%20approximates%20human%20behavior%20on%20both%20tasks.%20These%20results%20serve%0Aas%20an%20initial%20demonstration%20of%20TRESTLE%27s%20capabilities%20and%20show%20that%2C%20by%20taking%0Akey%20characteristics%20of%20human%20learning%20into%20account%2C%20it%20can%20better%20model%0Abehavior%20than%20approaches%20that%20ignore%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRESTLE%253A%2520A%2520Model%2520of%2520Concept%2520Formation%2520in%2520Structured%2520Domains%26entry.906535625%3DChristopher%2520J.%2520MacLellan%2520and%2520Erik%2520Harpstead%2520and%2520Vincent%2520Aleven%2520and%2520Kenneth%2520R.%2520Koedinger%26entry.1292438233%3D%2520%2520The%2520literature%2520on%2520concept%2520formation%2520has%2520demonstrated%2520that%2520humans%2520are%2520capable%250Aof%2520learning%2520concepts%2520incrementally%252C%2520with%2520a%2520variety%2520of%2520attribute%2520types%252C%2520and%2520in%250Aboth%2520supervised%2520and%2520unsupervised%2520settings.%2520Many%2520models%2520of%2520concept%2520formation%250Afocus%2520on%2520a%2520subset%2520of%2520these%2520characteristics%252C%2520but%2520none%2520account%2520for%2520all%2520of%2520them.%250AIn%2520this%2520paper%252C%2520we%2520present%2520TRESTLE%252C%2520an%2520incremental%2520account%2520of%2520probabilistic%250Aconcept%2520formation%2520in%2520structured%2520domains%2520that%2520unifies%2520prior%2520concept%2520learning%250Amodels.%2520TRESTLE%2520works%2520by%2520creating%2520a%2520hierarchical%2520categorization%2520tree%2520that%2520can%250Abe%2520used%2520to%2520predict%2520missing%2520attribute%2520values%2520and%2520cluster%2520sets%2520of%2520examples%2520into%250Aconceptually%2520meaningful%2520groups.%2520It%2520updates%2520its%2520knowledge%2520by%2520partially%2520matching%250Anovel%2520structures%2520and%2520sorting%2520them%2520into%2520its%2520categorization%2520tree.%2520Finally%252C%2520the%250Asystem%2520supports%2520mixed-data%2520representations%252C%2520including%2520nominal%252C%2520numeric%252C%250Arelational%252C%2520and%2520component%2520attributes.%2520We%2520evaluate%2520TRESTLE%2527s%2520performance%2520on%2520a%250Asupervised%2520learning%2520task%2520and%2520an%2520unsupervised%2520clustering%2520task.%2520For%2520both%2520tasks%252C%250Awe%2520compare%2520it%2520to%2520a%2520nonincremental%2520model%2520and%2520to%2520human%2520participants.%2520We%2520find%2520that%250Athis%2520new%2520categorization%2520model%2520is%2520competitive%2520with%2520the%2520nonincremental%2520approach%250Aand%2520more%2520closely%2520approximates%2520human%2520behavior%2520on%2520both%2520tasks.%2520These%2520results%2520serve%250Aas%2520an%2520initial%2520demonstration%2520of%2520TRESTLE%2527s%2520capabilities%2520and%2520show%2520that%252C%2520by%2520taking%250Akey%2520characteristics%2520of%2520human%2520learning%2520into%2520account%252C%2520it%2520can%2520better%2520model%250Abehavior%2520than%2520approaches%2520that%2520ignore%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRESTLE%3A%20A%20Model%20of%20Concept%20Formation%20in%20Structured%20Domains&entry.906535625=Christopher%20J.%20MacLellan%20and%20Erik%20Harpstead%20and%20Vincent%20Aleven%20and%20Kenneth%20R.%20Koedinger&entry.1292438233=%20%20The%20literature%20on%20concept%20formation%20has%20demonstrated%20that%20humans%20are%20capable%0Aof%20learning%20concepts%20incrementally%2C%20with%20a%20variety%20of%20attribute%20types%2C%20and%20in%0Aboth%20supervised%20and%20unsupervised%20settings.%20Many%20models%20of%20concept%20formation%0Afocus%20on%20a%20subset%20of%20these%20characteristics%2C%20but%20none%20account%20for%20all%20of%20them.%0AIn%20this%20paper%2C%20we%20present%20TRESTLE%2C%20an%20incremental%20account%20of%20probabilistic%0Aconcept%20formation%20in%20structured%20domains%20that%20unifies%20prior%20concept%20learning%0Amodels.%20TRESTLE%20works%20by%20creating%20a%20hierarchical%20categorization%20tree%20that%20can%0Abe%20used%20to%20predict%20missing%20attribute%20values%20and%20cluster%20sets%20of%20examples%20into%0Aconceptually%20meaningful%20groups.%20It%20updates%20its%20knowledge%20by%20partially%20matching%0Anovel%20structures%20and%20sorting%20them%20into%20its%20categorization%20tree.%20Finally%2C%20the%0Asystem%20supports%20mixed-data%20representations%2C%20including%20nominal%2C%20numeric%2C%0Arelational%2C%20and%20component%20attributes.%20We%20evaluate%20TRESTLE%27s%20performance%20on%20a%0Asupervised%20learning%20task%20and%20an%20unsupervised%20clustering%20task.%20For%20both%20tasks%2C%0Awe%20compare%20it%20to%20a%20nonincremental%20model%20and%20to%20human%20participants.%20We%20find%20that%0Athis%20new%20categorization%20model%20is%20competitive%20with%20the%20nonincremental%20approach%0Aand%20more%20closely%20approximates%20human%20behavior%20on%20both%20tasks.%20These%20results%20serve%0Aas%20an%20initial%20demonstration%20of%20TRESTLE%27s%20capabilities%20and%20show%20that%2C%20by%20taking%0Akey%20characteristics%20of%20human%20learning%20into%20account%2C%20it%20can%20better%20model%0Abehavior%20than%20approaches%20that%20ignore%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10588v1&entry.124074799=Read"},
{"title": "Deep Linear Probe Generators for Weight Space Learning", "author": "Jonathan Kahana and Eliahu Horwitz and Imri Shuval and Yedid Hoshen", "abstract": "  Weight space learning aims to extract information about a neural network,\nsuch as its training dataset or generalization error. Recent approaches learn\ndirectly from model weights, but this presents many challenges as weights are\nhigh-dimensional and include permutation symmetries between neurons. An\nalternative approach, Probing, represents a model by passing a set of learned\ninputs (probes) through the model, and training a predictor on top of the\ncorresponding outputs. Although probing is typically not used as a stand alone\napproach, our preliminary experiment found that a vanilla probing baseline\nworked surprisingly well. However, we discover that current probe learning\nstrategies are ineffective. We therefore propose Deep Linear Probe Generators\n(ProbeGen), a simple and effective modification to probing approaches. ProbeGen\nadds a shared generator module with a deep linear architecture, providing an\ninductive bias towards structured probes thus reducing overfitting. While\nsimple, ProbeGen performs significantly better than the state-of-the-art and is\nvery efficient, requiring between 30 to 1000 times fewer FLOPs than other top\napproaches.\n", "link": "http://arxiv.org/abs/2410.10811v1", "date": "2024-10-14", "relevancy": 2.5583, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Linear%20Probe%20Generators%20for%20Weight%20Space%20Learning&body=Title%3A%20Deep%20Linear%20Probe%20Generators%20for%20Weight%20Space%20Learning%0AAuthor%3A%20Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Imri%20Shuval%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20Weight%20space%20learning%20aims%20to%20extract%20information%20about%20a%20neural%20network%2C%0Asuch%20as%20its%20training%20dataset%20or%20generalization%20error.%20Recent%20approaches%20learn%0Adirectly%20from%20model%20weights%2C%20but%20this%20presents%20many%20challenges%20as%20weights%20are%0Ahigh-dimensional%20and%20include%20permutation%20symmetries%20between%20neurons.%20An%0Aalternative%20approach%2C%20Probing%2C%20represents%20a%20model%20by%20passing%20a%20set%20of%20learned%0Ainputs%20%28probes%29%20through%20the%20model%2C%20and%20training%20a%20predictor%20on%20top%20of%20the%0Acorresponding%20outputs.%20Although%20probing%20is%20typically%20not%20used%20as%20a%20stand%20alone%0Aapproach%2C%20our%20preliminary%20experiment%20found%20that%20a%20vanilla%20probing%20baseline%0Aworked%20surprisingly%20well.%20However%2C%20we%20discover%20that%20current%20probe%20learning%0Astrategies%20are%20ineffective.%20We%20therefore%20propose%20Deep%20Linear%20Probe%20Generators%0A%28ProbeGen%29%2C%20a%20simple%20and%20effective%20modification%20to%20probing%20approaches.%20ProbeGen%0Aadds%20a%20shared%20generator%20module%20with%20a%20deep%20linear%20architecture%2C%20providing%20an%0Ainductive%20bias%20towards%20structured%20probes%20thus%20reducing%20overfitting.%20While%0Asimple%2C%20ProbeGen%20performs%20significantly%20better%20than%20the%20state-of-the-art%20and%20is%0Avery%20efficient%2C%20requiring%20between%2030%20to%201000%20times%20fewer%20FLOPs%20than%20other%20top%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Linear%2520Probe%2520Generators%2520for%2520Weight%2520Space%2520Learning%26entry.906535625%3DJonathan%2520Kahana%2520and%2520Eliahu%2520Horwitz%2520and%2520Imri%2520Shuval%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520Weight%2520space%2520learning%2520aims%2520to%2520extract%2520information%2520about%2520a%2520neural%2520network%252C%250Asuch%2520as%2520its%2520training%2520dataset%2520or%2520generalization%2520error.%2520Recent%2520approaches%2520learn%250Adirectly%2520from%2520model%2520weights%252C%2520but%2520this%2520presents%2520many%2520challenges%2520as%2520weights%2520are%250Ahigh-dimensional%2520and%2520include%2520permutation%2520symmetries%2520between%2520neurons.%2520An%250Aalternative%2520approach%252C%2520Probing%252C%2520represents%2520a%2520model%2520by%2520passing%2520a%2520set%2520of%2520learned%250Ainputs%2520%2528probes%2529%2520through%2520the%2520model%252C%2520and%2520training%2520a%2520predictor%2520on%2520top%2520of%2520the%250Acorresponding%2520outputs.%2520Although%2520probing%2520is%2520typically%2520not%2520used%2520as%2520a%2520stand%2520alone%250Aapproach%252C%2520our%2520preliminary%2520experiment%2520found%2520that%2520a%2520vanilla%2520probing%2520baseline%250Aworked%2520surprisingly%2520well.%2520However%252C%2520we%2520discover%2520that%2520current%2520probe%2520learning%250Astrategies%2520are%2520ineffective.%2520We%2520therefore%2520propose%2520Deep%2520Linear%2520Probe%2520Generators%250A%2528ProbeGen%2529%252C%2520a%2520simple%2520and%2520effective%2520modification%2520to%2520probing%2520approaches.%2520ProbeGen%250Aadds%2520a%2520shared%2520generator%2520module%2520with%2520a%2520deep%2520linear%2520architecture%252C%2520providing%2520an%250Ainductive%2520bias%2520towards%2520structured%2520probes%2520thus%2520reducing%2520overfitting.%2520While%250Asimple%252C%2520ProbeGen%2520performs%2520significantly%2520better%2520than%2520the%2520state-of-the-art%2520and%2520is%250Avery%2520efficient%252C%2520requiring%2520between%252030%2520to%25201000%2520times%2520fewer%2520FLOPs%2520than%2520other%2520top%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Linear%20Probe%20Generators%20for%20Weight%20Space%20Learning&entry.906535625=Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Imri%20Shuval%20and%20Yedid%20Hoshen&entry.1292438233=%20%20Weight%20space%20learning%20aims%20to%20extract%20information%20about%20a%20neural%20network%2C%0Asuch%20as%20its%20training%20dataset%20or%20generalization%20error.%20Recent%20approaches%20learn%0Adirectly%20from%20model%20weights%2C%20but%20this%20presents%20many%20challenges%20as%20weights%20are%0Ahigh-dimensional%20and%20include%20permutation%20symmetries%20between%20neurons.%20An%0Aalternative%20approach%2C%20Probing%2C%20represents%20a%20model%20by%20passing%20a%20set%20of%20learned%0Ainputs%20%28probes%29%20through%20the%20model%2C%20and%20training%20a%20predictor%20on%20top%20of%20the%0Acorresponding%20outputs.%20Although%20probing%20is%20typically%20not%20used%20as%20a%20stand%20alone%0Aapproach%2C%20our%20preliminary%20experiment%20found%20that%20a%20vanilla%20probing%20baseline%0Aworked%20surprisingly%20well.%20However%2C%20we%20discover%20that%20current%20probe%20learning%0Astrategies%20are%20ineffective.%20We%20therefore%20propose%20Deep%20Linear%20Probe%20Generators%0A%28ProbeGen%29%2C%20a%20simple%20and%20effective%20modification%20to%20probing%20approaches.%20ProbeGen%0Aadds%20a%20shared%20generator%20module%20with%20a%20deep%20linear%20architecture%2C%20providing%20an%0Ainductive%20bias%20towards%20structured%20probes%20thus%20reducing%20overfitting.%20While%0Asimple%2C%20ProbeGen%20performs%20significantly%20better%20than%20the%20state-of-the-art%20and%20is%0Avery%20efficient%2C%20requiring%20between%2030%20to%201000%20times%20fewer%20FLOPs%20than%20other%20top%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10811v1&entry.124074799=Read"},
{"title": "Sign Stitching: A Novel Approach to Sign Language Production", "author": "Harry Walsh and Ben Saunders and Richard Bowden", "abstract": "  Sign Language Production (SLP) is a challenging task, given the limited\nresources available and the inherent diversity within sign data. As a result,\nprevious works have suffered from the problem of regression to the mean,\nleading to under-articulated and incomprehensible signing. In this paper, we\npropose using dictionary examples to create expressive sign language sequences.\nHowever, simply concatenating the signs would create robotic and unnatural\nsequences. Therefore, we present a 7-step approach to effectively stitch the\nsigns together. First, by normalising each sign into a canonical pose, cropping\nand stitching we create a continuous sequence. Then by applying filtering in\nthe frequency domain and resampling each sign we create cohesive natural\nsequences, that mimic the prosody found in the original data. We leverage the\nSignGAN model to map the output to a photo-realistic signer and present a\ncomplete Text-to-Sign (T2S) SLP pipeline. Our evaluation demonstrates the\neffectiveness of this approach, showcasing state-of-the-art performance across\nall datasets.\n", "link": "http://arxiv.org/abs/2405.07663v2", "date": "2024-10-14", "relevancy": 2.554, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production&body=Title%3A%20Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production%0AAuthor%3A%20Harry%20Walsh%20and%20Ben%20Saunders%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Production%20%28SLP%29%20is%20a%20challenging%20task%2C%20given%20the%20limited%0Aresources%20available%20and%20the%20inherent%20diversity%20within%20sign%20data.%20As%20a%20result%2C%0Aprevious%20works%20have%20suffered%20from%20the%20problem%20of%20regression%20to%20the%20mean%2C%0Aleading%20to%20under-articulated%20and%20incomprehensible%20signing.%20In%20this%20paper%2C%20we%0Apropose%20using%20dictionary%20examples%20to%20create%20expressive%20sign%20language%20sequences.%0AHowever%2C%20simply%20concatenating%20the%20signs%20would%20create%20robotic%20and%20unnatural%0Asequences.%20Therefore%2C%20we%20present%20a%207-step%20approach%20to%20effectively%20stitch%20the%0Asigns%20together.%20First%2C%20by%20normalising%20each%20sign%20into%20a%20canonical%20pose%2C%20cropping%0Aand%20stitching%20we%20create%20a%20continuous%20sequence.%20Then%20by%20applying%20filtering%20in%0Athe%20frequency%20domain%20and%20resampling%20each%20sign%20we%20create%20cohesive%20natural%0Asequences%2C%20that%20mimic%20the%20prosody%20found%20in%20the%20original%20data.%20We%20leverage%20the%0ASignGAN%20model%20to%20map%20the%20output%20to%20a%20photo-realistic%20signer%20and%20present%20a%0Acomplete%20Text-to-Sign%20%28T2S%29%20SLP%20pipeline.%20Our%20evaluation%20demonstrates%20the%0Aeffectiveness%20of%20this%20approach%2C%20showcasing%20state-of-the-art%20performance%20across%0Aall%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign%2520Stitching%253A%2520A%2520Novel%2520Approach%2520to%2520Sign%2520Language%2520Production%26entry.906535625%3DHarry%2520Walsh%2520and%2520Ben%2520Saunders%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Production%2520%2528SLP%2529%2520is%2520a%2520challenging%2520task%252C%2520given%2520the%2520limited%250Aresources%2520available%2520and%2520the%2520inherent%2520diversity%2520within%2520sign%2520data.%2520As%2520a%2520result%252C%250Aprevious%2520works%2520have%2520suffered%2520from%2520the%2520problem%2520of%2520regression%2520to%2520the%2520mean%252C%250Aleading%2520to%2520under-articulated%2520and%2520incomprehensible%2520signing.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520using%2520dictionary%2520examples%2520to%2520create%2520expressive%2520sign%2520language%2520sequences.%250AHowever%252C%2520simply%2520concatenating%2520the%2520signs%2520would%2520create%2520robotic%2520and%2520unnatural%250Asequences.%2520Therefore%252C%2520we%2520present%2520a%25207-step%2520approach%2520to%2520effectively%2520stitch%2520the%250Asigns%2520together.%2520First%252C%2520by%2520normalising%2520each%2520sign%2520into%2520a%2520canonical%2520pose%252C%2520cropping%250Aand%2520stitching%2520we%2520create%2520a%2520continuous%2520sequence.%2520Then%2520by%2520applying%2520filtering%2520in%250Athe%2520frequency%2520domain%2520and%2520resampling%2520each%2520sign%2520we%2520create%2520cohesive%2520natural%250Asequences%252C%2520that%2520mimic%2520the%2520prosody%2520found%2520in%2520the%2520original%2520data.%2520We%2520leverage%2520the%250ASignGAN%2520model%2520to%2520map%2520the%2520output%2520to%2520a%2520photo-realistic%2520signer%2520and%2520present%2520a%250Acomplete%2520Text-to-Sign%2520%2528T2S%2529%2520SLP%2520pipeline.%2520Our%2520evaluation%2520demonstrates%2520the%250Aeffectiveness%2520of%2520this%2520approach%252C%2520showcasing%2520state-of-the-art%2520performance%2520across%250Aall%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production&entry.906535625=Harry%20Walsh%20and%20Ben%20Saunders%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Production%20%28SLP%29%20is%20a%20challenging%20task%2C%20given%20the%20limited%0Aresources%20available%20and%20the%20inherent%20diversity%20within%20sign%20data.%20As%20a%20result%2C%0Aprevious%20works%20have%20suffered%20from%20the%20problem%20of%20regression%20to%20the%20mean%2C%0Aleading%20to%20under-articulated%20and%20incomprehensible%20signing.%20In%20this%20paper%2C%20we%0Apropose%20using%20dictionary%20examples%20to%20create%20expressive%20sign%20language%20sequences.%0AHowever%2C%20simply%20concatenating%20the%20signs%20would%20create%20robotic%20and%20unnatural%0Asequences.%20Therefore%2C%20we%20present%20a%207-step%20approach%20to%20effectively%20stitch%20the%0Asigns%20together.%20First%2C%20by%20normalising%20each%20sign%20into%20a%20canonical%20pose%2C%20cropping%0Aand%20stitching%20we%20create%20a%20continuous%20sequence.%20Then%20by%20applying%20filtering%20in%0Athe%20frequency%20domain%20and%20resampling%20each%20sign%20we%20create%20cohesive%20natural%0Asequences%2C%20that%20mimic%20the%20prosody%20found%20in%20the%20original%20data.%20We%20leverage%20the%0ASignGAN%20model%20to%20map%20the%20output%20to%20a%20photo-realistic%20signer%20and%20present%20a%0Acomplete%20Text-to-Sign%20%28T2S%29%20SLP%20pipeline.%20Our%20evaluation%20demonstrates%20the%0Aeffectiveness%20of%20this%20approach%2C%20showcasing%20state-of-the-art%20performance%20across%0Aall%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07663v2&entry.124074799=Read"},
{"title": "TopoFR: A Closer Look at Topology Alignment on Face Recognition", "author": "Jun Dan and Yang Liu and Jiankang Deng and Haoyu Xie and Siyuan Li and Baigui Sun and Shan Luo", "abstract": "  The field of face recognition (FR) has undergone significant advancements\nwith the rise of deep learning. Recently, the success of unsupervised learning\nand graph neural networks has demonstrated the effectiveness of data structure\ninformation. Considering that the FR task can leverage large-scale training\ndata, which intrinsically contains significant structure information, we aim to\ninvestigate how to encode such critical structure information into the latent\nspace. As revealed from our observations, directly aligning the structure\ninformation between the input and latent spaces inevitably suffers from an\noverfitting problem, leading to a structure collapse phenomenon in the latent\nspace. To address this problem, we propose TopoFR, a novel FR model that\nleverages a topological structure alignment strategy called PTSA and a hard\nsample mining strategy named SDE. Concretely, PTSA uses persistent homology to\nalign the topological structures of the input and latent spaces, effectively\npreserving the structure information and improving the generalization\nperformance of FR model. To mitigate the impact of hard samples on the latent\nspace structure, SDE accurately identifies hard samples by automatically\ncomputing structure damage score (SDS) for each sample, and directs the model\nto prioritize optimizing these samples. Experimental results on popular face\nbenchmarks demonstrate the superiority of our TopoFR over the state-of-the-art\nmethods. Code and models are available at:\nhttps://github.com/modelscope/facechain/tree/main/face_module/TopoFR.\n", "link": "http://arxiv.org/abs/2410.10587v1", "date": "2024-10-14", "relevancy": 2.5457, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoFR%3A%20A%20Closer%20Look%20at%20Topology%20Alignment%20on%20Face%20Recognition&body=Title%3A%20TopoFR%3A%20A%20Closer%20Look%20at%20Topology%20Alignment%20on%20Face%20Recognition%0AAuthor%3A%20Jun%20Dan%20and%20Yang%20Liu%20and%20Jiankang%20Deng%20and%20Haoyu%20Xie%20and%20Siyuan%20Li%20and%20Baigui%20Sun%20and%20Shan%20Luo%0AAbstract%3A%20%20%20The%20field%20of%20face%20recognition%20%28FR%29%20has%20undergone%20significant%20advancements%0Awith%20the%20rise%20of%20deep%20learning.%20Recently%2C%20the%20success%20of%20unsupervised%20learning%0Aand%20graph%20neural%20networks%20has%20demonstrated%20the%20effectiveness%20of%20data%20structure%0Ainformation.%20Considering%20that%20the%20FR%20task%20can%20leverage%20large-scale%20training%0Adata%2C%20which%20intrinsically%20contains%20significant%20structure%20information%2C%20we%20aim%20to%0Ainvestigate%20how%20to%20encode%20such%20critical%20structure%20information%20into%20the%20latent%0Aspace.%20As%20revealed%20from%20our%20observations%2C%20directly%20aligning%20the%20structure%0Ainformation%20between%20the%20input%20and%20latent%20spaces%20inevitably%20suffers%20from%20an%0Aoverfitting%20problem%2C%20leading%20to%20a%20structure%20collapse%20phenomenon%20in%20the%20latent%0Aspace.%20To%20address%20this%20problem%2C%20we%20propose%20TopoFR%2C%20a%20novel%20FR%20model%20that%0Aleverages%20a%20topological%20structure%20alignment%20strategy%20called%20PTSA%20and%20a%20hard%0Asample%20mining%20strategy%20named%20SDE.%20Concretely%2C%20PTSA%20uses%20persistent%20homology%20to%0Aalign%20the%20topological%20structures%20of%20the%20input%20and%20latent%20spaces%2C%20effectively%0Apreserving%20the%20structure%20information%20and%20improving%20the%20generalization%0Aperformance%20of%20FR%20model.%20To%20mitigate%20the%20impact%20of%20hard%20samples%20on%20the%20latent%0Aspace%20structure%2C%20SDE%20accurately%20identifies%20hard%20samples%20by%20automatically%0Acomputing%20structure%20damage%20score%20%28SDS%29%20for%20each%20sample%2C%20and%20directs%20the%20model%0Ato%20prioritize%20optimizing%20these%20samples.%20Experimental%20results%20on%20popular%20face%0Abenchmarks%20demonstrate%20the%20superiority%20of%20our%20TopoFR%20over%20the%20state-of-the-art%0Amethods.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/modelscope/facechain/tree/main/face_module/TopoFR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoFR%253A%2520A%2520Closer%2520Look%2520at%2520Topology%2520Alignment%2520on%2520Face%2520Recognition%26entry.906535625%3DJun%2520Dan%2520and%2520Yang%2520Liu%2520and%2520Jiankang%2520Deng%2520and%2520Haoyu%2520Xie%2520and%2520Siyuan%2520Li%2520and%2520Baigui%2520Sun%2520and%2520Shan%2520Luo%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520face%2520recognition%2520%2528FR%2529%2520has%2520undergone%2520significant%2520advancements%250Awith%2520the%2520rise%2520of%2520deep%2520learning.%2520Recently%252C%2520the%2520success%2520of%2520unsupervised%2520learning%250Aand%2520graph%2520neural%2520networks%2520has%2520demonstrated%2520the%2520effectiveness%2520of%2520data%2520structure%250Ainformation.%2520Considering%2520that%2520the%2520FR%2520task%2520can%2520leverage%2520large-scale%2520training%250Adata%252C%2520which%2520intrinsically%2520contains%2520significant%2520structure%2520information%252C%2520we%2520aim%2520to%250Ainvestigate%2520how%2520to%2520encode%2520such%2520critical%2520structure%2520information%2520into%2520the%2520latent%250Aspace.%2520As%2520revealed%2520from%2520our%2520observations%252C%2520directly%2520aligning%2520the%2520structure%250Ainformation%2520between%2520the%2520input%2520and%2520latent%2520spaces%2520inevitably%2520suffers%2520from%2520an%250Aoverfitting%2520problem%252C%2520leading%2520to%2520a%2520structure%2520collapse%2520phenomenon%2520in%2520the%2520latent%250Aspace.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520TopoFR%252C%2520a%2520novel%2520FR%2520model%2520that%250Aleverages%2520a%2520topological%2520structure%2520alignment%2520strategy%2520called%2520PTSA%2520and%2520a%2520hard%250Asample%2520mining%2520strategy%2520named%2520SDE.%2520Concretely%252C%2520PTSA%2520uses%2520persistent%2520homology%2520to%250Aalign%2520the%2520topological%2520structures%2520of%2520the%2520input%2520and%2520latent%2520spaces%252C%2520effectively%250Apreserving%2520the%2520structure%2520information%2520and%2520improving%2520the%2520generalization%250Aperformance%2520of%2520FR%2520model.%2520To%2520mitigate%2520the%2520impact%2520of%2520hard%2520samples%2520on%2520the%2520latent%250Aspace%2520structure%252C%2520SDE%2520accurately%2520identifies%2520hard%2520samples%2520by%2520automatically%250Acomputing%2520structure%2520damage%2520score%2520%2528SDS%2529%2520for%2520each%2520sample%252C%2520and%2520directs%2520the%2520model%250Ato%2520prioritize%2520optimizing%2520these%2520samples.%2520Experimental%2520results%2520on%2520popular%2520face%250Abenchmarks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520TopoFR%2520over%2520the%2520state-of-the-art%250Amethods.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/modelscope/facechain/tree/main/face_module/TopoFR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoFR%3A%20A%20Closer%20Look%20at%20Topology%20Alignment%20on%20Face%20Recognition&entry.906535625=Jun%20Dan%20and%20Yang%20Liu%20and%20Jiankang%20Deng%20and%20Haoyu%20Xie%20and%20Siyuan%20Li%20and%20Baigui%20Sun%20and%20Shan%20Luo&entry.1292438233=%20%20The%20field%20of%20face%20recognition%20%28FR%29%20has%20undergone%20significant%20advancements%0Awith%20the%20rise%20of%20deep%20learning.%20Recently%2C%20the%20success%20of%20unsupervised%20learning%0Aand%20graph%20neural%20networks%20has%20demonstrated%20the%20effectiveness%20of%20data%20structure%0Ainformation.%20Considering%20that%20the%20FR%20task%20can%20leverage%20large-scale%20training%0Adata%2C%20which%20intrinsically%20contains%20significant%20structure%20information%2C%20we%20aim%20to%0Ainvestigate%20how%20to%20encode%20such%20critical%20structure%20information%20into%20the%20latent%0Aspace.%20As%20revealed%20from%20our%20observations%2C%20directly%20aligning%20the%20structure%0Ainformation%20between%20the%20input%20and%20latent%20spaces%20inevitably%20suffers%20from%20an%0Aoverfitting%20problem%2C%20leading%20to%20a%20structure%20collapse%20phenomenon%20in%20the%20latent%0Aspace.%20To%20address%20this%20problem%2C%20we%20propose%20TopoFR%2C%20a%20novel%20FR%20model%20that%0Aleverages%20a%20topological%20structure%20alignment%20strategy%20called%20PTSA%20and%20a%20hard%0Asample%20mining%20strategy%20named%20SDE.%20Concretely%2C%20PTSA%20uses%20persistent%20homology%20to%0Aalign%20the%20topological%20structures%20of%20the%20input%20and%20latent%20spaces%2C%20effectively%0Apreserving%20the%20structure%20information%20and%20improving%20the%20generalization%0Aperformance%20of%20FR%20model.%20To%20mitigate%20the%20impact%20of%20hard%20samples%20on%20the%20latent%0Aspace%20structure%2C%20SDE%20accurately%20identifies%20hard%20samples%20by%20automatically%0Acomputing%20structure%20damage%20score%20%28SDS%29%20for%20each%20sample%2C%20and%20directs%20the%20model%0Ato%20prioritize%20optimizing%20these%20samples.%20Experimental%20results%20on%20popular%20face%0Abenchmarks%20demonstrate%20the%20superiority%20of%20our%20TopoFR%20over%20the%20state-of-the-art%0Amethods.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/modelscope/facechain/tree/main/face_module/TopoFR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10587v1&entry.124074799=Read"},
{"title": "GAIA: Rethinking Action Quality Assessment for AI-Generated Videos", "author": "Zijian Chen and Wei Sun and Yuan Tian and Jun Jia and Zicheng Zhang and Jiarui Wang and Ru Huang and Xiongkuo Min and Guangtao Zhai and Wenjun Zhang", "abstract": "  Assessing action quality is both imperative and challenging due to its\nsignificant impact on the quality of AI-generated videos, further complicated\nby the inherently ambiguous nature of actions within AI-generated video (AIGV).\nCurrent action quality assessment (AQA) algorithms predominantly focus on\nactions from real specific scenarios and are pre-trained with normative action\nfeatures, thus rendering them inapplicable in AIGVs. To address these problems,\nwe construct GAIA, a Generic AI-generated Action dataset, by conducting a\nlarge-scale subjective evaluation from a novel causal reasoning-based\nperspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based\non GAIA, we evaluate a suite of popular text-to-video (T2V) models on their\nability to generate visually rational actions, revealing their pros and cons on\ndifferent categories of actions. We also extend GAIA as a testbed to benchmark\nthe AQA capacity of existing automatic evaluation methods. Results show that\ntraditional AQA methods, action-related metrics in recent T2V benchmarks, and\nmainstream video quality methods perform poorly with an average SRCC of 0.454,\n0.191, and 0.519, respectively, indicating a sizable gap between current models\nand human action perception patterns in AIGVs. Our findings underscore the\nsignificance of action quality as a unique perspective for studying AIGVs and\ncan catalyze progress towards methods with enhanced capacities for AQA in\nAIGVs.\n", "link": "http://arxiv.org/abs/2406.06087v2", "date": "2024-10-14", "relevancy": 2.514, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5243}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAIA%3A%20Rethinking%20Action%20Quality%20Assessment%20for%20AI-Generated%20Videos&body=Title%3A%20GAIA%3A%20Rethinking%20Action%20Quality%20Assessment%20for%20AI-Generated%20Videos%0AAuthor%3A%20Zijian%20Chen%20and%20Wei%20Sun%20and%20Yuan%20Tian%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Jiarui%20Wang%20and%20Ru%20Huang%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Wenjun%20Zhang%0AAbstract%3A%20%20%20Assessing%20action%20quality%20is%20both%20imperative%20and%20challenging%20due%20to%20its%0Asignificant%20impact%20on%20the%20quality%20of%20AI-generated%20videos%2C%20further%20complicated%0Aby%20the%20inherently%20ambiguous%20nature%20of%20actions%20within%20AI-generated%20video%20%28AIGV%29.%0ACurrent%20action%20quality%20assessment%20%28AQA%29%20algorithms%20predominantly%20focus%20on%0Aactions%20from%20real%20specific%20scenarios%20and%20are%20pre-trained%20with%20normative%20action%0Afeatures%2C%20thus%20rendering%20them%20inapplicable%20in%20AIGVs.%20To%20address%20these%20problems%2C%0Awe%20construct%20GAIA%2C%20a%20Generic%20AI-generated%20Action%20dataset%2C%20by%20conducting%20a%0Alarge-scale%20subjective%20evaluation%20from%20a%20novel%20causal%20reasoning-based%0Aperspective%2C%20resulting%20in%20971%2C244%20ratings%20among%209%2C180%20video-action%20pairs.%20Based%0Aon%20GAIA%2C%20we%20evaluate%20a%20suite%20of%20popular%20text-to-video%20%28T2V%29%20models%20on%20their%0Aability%20to%20generate%20visually%20rational%20actions%2C%20revealing%20their%20pros%20and%20cons%20on%0Adifferent%20categories%20of%20actions.%20We%20also%20extend%20GAIA%20as%20a%20testbed%20to%20benchmark%0Athe%20AQA%20capacity%20of%20existing%20automatic%20evaluation%20methods.%20Results%20show%20that%0Atraditional%20AQA%20methods%2C%20action-related%20metrics%20in%20recent%20T2V%20benchmarks%2C%20and%0Amainstream%20video%20quality%20methods%20perform%20poorly%20with%20an%20average%20SRCC%20of%200.454%2C%0A0.191%2C%20and%200.519%2C%20respectively%2C%20indicating%20a%20sizable%20gap%20between%20current%20models%0Aand%20human%20action%20perception%20patterns%20in%20AIGVs.%20Our%20findings%20underscore%20the%0Asignificance%20of%20action%20quality%20as%20a%20unique%20perspective%20for%20studying%20AIGVs%20and%0Acan%20catalyze%20progress%20towards%20methods%20with%20enhanced%20capacities%20for%20AQA%20in%0AAIGVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAIA%253A%2520Rethinking%2520Action%2520Quality%2520Assessment%2520for%2520AI-Generated%2520Videos%26entry.906535625%3DZijian%2520Chen%2520and%2520Wei%2520Sun%2520and%2520Yuan%2520Tian%2520and%2520Jun%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Jiarui%2520Wang%2520and%2520Ru%2520Huang%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Wenjun%2520Zhang%26entry.1292438233%3D%2520%2520Assessing%2520action%2520quality%2520is%2520both%2520imperative%2520and%2520challenging%2520due%2520to%2520its%250Asignificant%2520impact%2520on%2520the%2520quality%2520of%2520AI-generated%2520videos%252C%2520further%2520complicated%250Aby%2520the%2520inherently%2520ambiguous%2520nature%2520of%2520actions%2520within%2520AI-generated%2520video%2520%2528AIGV%2529.%250ACurrent%2520action%2520quality%2520assessment%2520%2528AQA%2529%2520algorithms%2520predominantly%2520focus%2520on%250Aactions%2520from%2520real%2520specific%2520scenarios%2520and%2520are%2520pre-trained%2520with%2520normative%2520action%250Afeatures%252C%2520thus%2520rendering%2520them%2520inapplicable%2520in%2520AIGVs.%2520To%2520address%2520these%2520problems%252C%250Awe%2520construct%2520GAIA%252C%2520a%2520Generic%2520AI-generated%2520Action%2520dataset%252C%2520by%2520conducting%2520a%250Alarge-scale%2520subjective%2520evaluation%2520from%2520a%2520novel%2520causal%2520reasoning-based%250Aperspective%252C%2520resulting%2520in%2520971%252C244%2520ratings%2520among%25209%252C180%2520video-action%2520pairs.%2520Based%250Aon%2520GAIA%252C%2520we%2520evaluate%2520a%2520suite%2520of%2520popular%2520text-to-video%2520%2528T2V%2529%2520models%2520on%2520their%250Aability%2520to%2520generate%2520visually%2520rational%2520actions%252C%2520revealing%2520their%2520pros%2520and%2520cons%2520on%250Adifferent%2520categories%2520of%2520actions.%2520We%2520also%2520extend%2520GAIA%2520as%2520a%2520testbed%2520to%2520benchmark%250Athe%2520AQA%2520capacity%2520of%2520existing%2520automatic%2520evaluation%2520methods.%2520Results%2520show%2520that%250Atraditional%2520AQA%2520methods%252C%2520action-related%2520metrics%2520in%2520recent%2520T2V%2520benchmarks%252C%2520and%250Amainstream%2520video%2520quality%2520methods%2520perform%2520poorly%2520with%2520an%2520average%2520SRCC%2520of%25200.454%252C%250A0.191%252C%2520and%25200.519%252C%2520respectively%252C%2520indicating%2520a%2520sizable%2520gap%2520between%2520current%2520models%250Aand%2520human%2520action%2520perception%2520patterns%2520in%2520AIGVs.%2520Our%2520findings%2520underscore%2520the%250Asignificance%2520of%2520action%2520quality%2520as%2520a%2520unique%2520perspective%2520for%2520studying%2520AIGVs%2520and%250Acan%2520catalyze%2520progress%2520towards%2520methods%2520with%2520enhanced%2520capacities%2520for%2520AQA%2520in%250AAIGVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAIA%3A%20Rethinking%20Action%20Quality%20Assessment%20for%20AI-Generated%20Videos&entry.906535625=Zijian%20Chen%20and%20Wei%20Sun%20and%20Yuan%20Tian%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Jiarui%20Wang%20and%20Ru%20Huang%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Wenjun%20Zhang&entry.1292438233=%20%20Assessing%20action%20quality%20is%20both%20imperative%20and%20challenging%20due%20to%20its%0Asignificant%20impact%20on%20the%20quality%20of%20AI-generated%20videos%2C%20further%20complicated%0Aby%20the%20inherently%20ambiguous%20nature%20of%20actions%20within%20AI-generated%20video%20%28AIGV%29.%0ACurrent%20action%20quality%20assessment%20%28AQA%29%20algorithms%20predominantly%20focus%20on%0Aactions%20from%20real%20specific%20scenarios%20and%20are%20pre-trained%20with%20normative%20action%0Afeatures%2C%20thus%20rendering%20them%20inapplicable%20in%20AIGVs.%20To%20address%20these%20problems%2C%0Awe%20construct%20GAIA%2C%20a%20Generic%20AI-generated%20Action%20dataset%2C%20by%20conducting%20a%0Alarge-scale%20subjective%20evaluation%20from%20a%20novel%20causal%20reasoning-based%0Aperspective%2C%20resulting%20in%20971%2C244%20ratings%20among%209%2C180%20video-action%20pairs.%20Based%0Aon%20GAIA%2C%20we%20evaluate%20a%20suite%20of%20popular%20text-to-video%20%28T2V%29%20models%20on%20their%0Aability%20to%20generate%20visually%20rational%20actions%2C%20revealing%20their%20pros%20and%20cons%20on%0Adifferent%20categories%20of%20actions.%20We%20also%20extend%20GAIA%20as%20a%20testbed%20to%20benchmark%0Athe%20AQA%20capacity%20of%20existing%20automatic%20evaluation%20methods.%20Results%20show%20that%0Atraditional%20AQA%20methods%2C%20action-related%20metrics%20in%20recent%20T2V%20benchmarks%2C%20and%0Amainstream%20video%20quality%20methods%20perform%20poorly%20with%20an%20average%20SRCC%20of%200.454%2C%0A0.191%2C%20and%200.519%2C%20respectively%2C%20indicating%20a%20sizable%20gap%20between%20current%20models%0Aand%20human%20action%20perception%20patterns%20in%20AIGVs.%20Our%20findings%20underscore%20the%0Asignificance%20of%20action%20quality%20as%20a%20unique%20perspective%20for%20studying%20AIGVs%20and%0Acan%20catalyze%20progress%20towards%20methods%20with%20enhanced%20capacities%20for%20AQA%20in%0AAIGVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06087v2&entry.124074799=Read"},
{"title": "Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture\n  of Experts", "author": "Xu Liu and Juncheng Liu and Gerald Woo and Taha Aksu and Yuxuan Liang and Roger Zimmermann and Chenghao Liu and Silvio Savarese and Caiming Xiong and Doyen Sahoo", "abstract": "  Time series foundation models have demonstrated impressive performance as\nzero-shot forecasters. However, achieving effectively unified training on time\nseries remains an open challenge. Existing approaches introduce some level of\nmodel specialization to account for the highly heterogeneous nature of time\nseries data. For instance, Moirai pursues unified training by employing\nmultiple input/output projection layers, each tailored to handle time series at\na specific frequency. Similarly, TimesFM maintains a frequency embedding\ndictionary for this purpose. We identify two major drawbacks to this\nhuman-imposed frequency-level model specialization: (1) Frequency is not a\nreliable indicator of the underlying patterns in time series. For example, time\nseries with different frequencies can display similar patterns, while those\nwith the same frequency may exhibit varied patterns. (2) Non-stationarity is an\ninherent property of real-world time series, leading to varied distributions\neven within a short context window of a single time series. Frequency-level\nspecialization is too coarse-grained to capture this level of diversity. To\naddress these limitations, this paper introduces Moirai-MoE, using a single\ninput/output projection layer while delegating the modeling of diverse time\nseries patterns to the sparse mixture of experts (MoE) within Transformers.\nWith these designs, Moirai-MoE reduces reliance on human-defined heuristics and\nenables automatic token-level specialization. Extensive experiments on 39\ndatasets demonstrate the superiority of Moirai-MoE over existing foundation\nmodels in both in-distribution and zero-shot scenarios. Furthermore, this study\nconducts comprehensive model analyses to explore the inner workings of time\nseries MoE foundation models and provides valuable insights for future\nresearch.\n", "link": "http://arxiv.org/abs/2410.10469v1", "date": "2024-10-14", "relevancy": 2.5127, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moirai-MoE%3A%20Empowering%20Time%20Series%20Foundation%20Models%20with%20Sparse%20Mixture%0A%20%20of%20Experts&body=Title%3A%20Moirai-MoE%3A%20Empowering%20Time%20Series%20Foundation%20Models%20with%20Sparse%20Mixture%0A%20%20of%20Experts%0AAuthor%3A%20Xu%20Liu%20and%20Juncheng%20Liu%20and%20Gerald%20Woo%20and%20Taha%20Aksu%20and%20Yuxuan%20Liang%20and%20Roger%20Zimmermann%20and%20Chenghao%20Liu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Doyen%20Sahoo%0AAbstract%3A%20%20%20Time%20series%20foundation%20models%20have%20demonstrated%20impressive%20performance%20as%0Azero-shot%20forecasters.%20However%2C%20achieving%20effectively%20unified%20training%20on%20time%0Aseries%20remains%20an%20open%20challenge.%20Existing%20approaches%20introduce%20some%20level%20of%0Amodel%20specialization%20to%20account%20for%20the%20highly%20heterogeneous%20nature%20of%20time%0Aseries%20data.%20For%20instance%2C%20Moirai%20pursues%20unified%20training%20by%20employing%0Amultiple%20input/output%20projection%20layers%2C%20each%20tailored%20to%20handle%20time%20series%20at%0Aa%20specific%20frequency.%20Similarly%2C%20TimesFM%20maintains%20a%20frequency%20embedding%0Adictionary%20for%20this%20purpose.%20We%20identify%20two%20major%20drawbacks%20to%20this%0Ahuman-imposed%20frequency-level%20model%20specialization%3A%20%281%29%20Frequency%20is%20not%20a%0Areliable%20indicator%20of%20the%20underlying%20patterns%20in%20time%20series.%20For%20example%2C%20time%0Aseries%20with%20different%20frequencies%20can%20display%20similar%20patterns%2C%20while%20those%0Awith%20the%20same%20frequency%20may%20exhibit%20varied%20patterns.%20%282%29%20Non-stationarity%20is%20an%0Ainherent%20property%20of%20real-world%20time%20series%2C%20leading%20to%20varied%20distributions%0Aeven%20within%20a%20short%20context%20window%20of%20a%20single%20time%20series.%20Frequency-level%0Aspecialization%20is%20too%20coarse-grained%20to%20capture%20this%20level%20of%20diversity.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20introduces%20Moirai-MoE%2C%20using%20a%20single%0Ainput/output%20projection%20layer%20while%20delegating%20the%20modeling%20of%20diverse%20time%0Aseries%20patterns%20to%20the%20sparse%20mixture%20of%20experts%20%28MoE%29%20within%20Transformers.%0AWith%20these%20designs%2C%20Moirai-MoE%20reduces%20reliance%20on%20human-defined%20heuristics%20and%0Aenables%20automatic%20token-level%20specialization.%20Extensive%20experiments%20on%2039%0Adatasets%20demonstrate%20the%20superiority%20of%20Moirai-MoE%20over%20existing%20foundation%0Amodels%20in%20both%20in-distribution%20and%20zero-shot%20scenarios.%20Furthermore%2C%20this%20study%0Aconducts%20comprehensive%20model%20analyses%20to%20explore%20the%20inner%20workings%20of%20time%0Aseries%20MoE%20foundation%20models%20and%20provides%20valuable%20insights%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoirai-MoE%253A%2520Empowering%2520Time%2520Series%2520Foundation%2520Models%2520with%2520Sparse%2520Mixture%250A%2520%2520of%2520Experts%26entry.906535625%3DXu%2520Liu%2520and%2520Juncheng%2520Liu%2520and%2520Gerald%2520Woo%2520and%2520Taha%2520Aksu%2520and%2520Yuxuan%2520Liang%2520and%2520Roger%2520Zimmermann%2520and%2520Chenghao%2520Liu%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Doyen%2520Sahoo%26entry.1292438233%3D%2520%2520Time%2520series%2520foundation%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520as%250Azero-shot%2520forecasters.%2520However%252C%2520achieving%2520effectively%2520unified%2520training%2520on%2520time%250Aseries%2520remains%2520an%2520open%2520challenge.%2520Existing%2520approaches%2520introduce%2520some%2520level%2520of%250Amodel%2520specialization%2520to%2520account%2520for%2520the%2520highly%2520heterogeneous%2520nature%2520of%2520time%250Aseries%2520data.%2520For%2520instance%252C%2520Moirai%2520pursues%2520unified%2520training%2520by%2520employing%250Amultiple%2520input/output%2520projection%2520layers%252C%2520each%2520tailored%2520to%2520handle%2520time%2520series%2520at%250Aa%2520specific%2520frequency.%2520Similarly%252C%2520TimesFM%2520maintains%2520a%2520frequency%2520embedding%250Adictionary%2520for%2520this%2520purpose.%2520We%2520identify%2520two%2520major%2520drawbacks%2520to%2520this%250Ahuman-imposed%2520frequency-level%2520model%2520specialization%253A%2520%25281%2529%2520Frequency%2520is%2520not%2520a%250Areliable%2520indicator%2520of%2520the%2520underlying%2520patterns%2520in%2520time%2520series.%2520For%2520example%252C%2520time%250Aseries%2520with%2520different%2520frequencies%2520can%2520display%2520similar%2520patterns%252C%2520while%2520those%250Awith%2520the%2520same%2520frequency%2520may%2520exhibit%2520varied%2520patterns.%2520%25282%2529%2520Non-stationarity%2520is%2520an%250Ainherent%2520property%2520of%2520real-world%2520time%2520series%252C%2520leading%2520to%2520varied%2520distributions%250Aeven%2520within%2520a%2520short%2520context%2520window%2520of%2520a%2520single%2520time%2520series.%2520Frequency-level%250Aspecialization%2520is%2520too%2520coarse-grained%2520to%2520capture%2520this%2520level%2520of%2520diversity.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520Moirai-MoE%252C%2520using%2520a%2520single%250Ainput/output%2520projection%2520layer%2520while%2520delegating%2520the%2520modeling%2520of%2520diverse%2520time%250Aseries%2520patterns%2520to%2520the%2520sparse%2520mixture%2520of%2520experts%2520%2528MoE%2529%2520within%2520Transformers.%250AWith%2520these%2520designs%252C%2520Moirai-MoE%2520reduces%2520reliance%2520on%2520human-defined%2520heuristics%2520and%250Aenables%2520automatic%2520token-level%2520specialization.%2520Extensive%2520experiments%2520on%252039%250Adatasets%2520demonstrate%2520the%2520superiority%2520of%2520Moirai-MoE%2520over%2520existing%2520foundation%250Amodels%2520in%2520both%2520in-distribution%2520and%2520zero-shot%2520scenarios.%2520Furthermore%252C%2520this%2520study%250Aconducts%2520comprehensive%2520model%2520analyses%2520to%2520explore%2520the%2520inner%2520workings%2520of%2520time%250Aseries%2520MoE%2520foundation%2520models%2520and%2520provides%2520valuable%2520insights%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moirai-MoE%3A%20Empowering%20Time%20Series%20Foundation%20Models%20with%20Sparse%20Mixture%0A%20%20of%20Experts&entry.906535625=Xu%20Liu%20and%20Juncheng%20Liu%20and%20Gerald%20Woo%20and%20Taha%20Aksu%20and%20Yuxuan%20Liang%20and%20Roger%20Zimmermann%20and%20Chenghao%20Liu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Doyen%20Sahoo&entry.1292438233=%20%20Time%20series%20foundation%20models%20have%20demonstrated%20impressive%20performance%20as%0Azero-shot%20forecasters.%20However%2C%20achieving%20effectively%20unified%20training%20on%20time%0Aseries%20remains%20an%20open%20challenge.%20Existing%20approaches%20introduce%20some%20level%20of%0Amodel%20specialization%20to%20account%20for%20the%20highly%20heterogeneous%20nature%20of%20time%0Aseries%20data.%20For%20instance%2C%20Moirai%20pursues%20unified%20training%20by%20employing%0Amultiple%20input/output%20projection%20layers%2C%20each%20tailored%20to%20handle%20time%20series%20at%0Aa%20specific%20frequency.%20Similarly%2C%20TimesFM%20maintains%20a%20frequency%20embedding%0Adictionary%20for%20this%20purpose.%20We%20identify%20two%20major%20drawbacks%20to%20this%0Ahuman-imposed%20frequency-level%20model%20specialization%3A%20%281%29%20Frequency%20is%20not%20a%0Areliable%20indicator%20of%20the%20underlying%20patterns%20in%20time%20series.%20For%20example%2C%20time%0Aseries%20with%20different%20frequencies%20can%20display%20similar%20patterns%2C%20while%20those%0Awith%20the%20same%20frequency%20may%20exhibit%20varied%20patterns.%20%282%29%20Non-stationarity%20is%20an%0Ainherent%20property%20of%20real-world%20time%20series%2C%20leading%20to%20varied%20distributions%0Aeven%20within%20a%20short%20context%20window%20of%20a%20single%20time%20series.%20Frequency-level%0Aspecialization%20is%20too%20coarse-grained%20to%20capture%20this%20level%20of%20diversity.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20introduces%20Moirai-MoE%2C%20using%20a%20single%0Ainput/output%20projection%20layer%20while%20delegating%20the%20modeling%20of%20diverse%20time%0Aseries%20patterns%20to%20the%20sparse%20mixture%20of%20experts%20%28MoE%29%20within%20Transformers.%0AWith%20these%20designs%2C%20Moirai-MoE%20reduces%20reliance%20on%20human-defined%20heuristics%20and%0Aenables%20automatic%20token-level%20specialization.%20Extensive%20experiments%20on%2039%0Adatasets%20demonstrate%20the%20superiority%20of%20Moirai-MoE%20over%20existing%20foundation%0Amodels%20in%20both%20in-distribution%20and%20zero-shot%20scenarios.%20Furthermore%2C%20this%20study%0Aconducts%20comprehensive%20model%20analyses%20to%20explore%20the%20inner%20workings%20of%20time%0Aseries%20MoE%20foundation%20models%20and%20provides%20valuable%20insights%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10469v1&entry.124074799=Read"},
{"title": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into\n  Large Language Models", "author": "Yanbiao Ji and Chang Liu and Xin Chen and Yue Ding and Dan Luo and Mei Li and Wenqing Lin and Hongtao Lu", "abstract": "  Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks.\n", "link": "http://arxiv.org/abs/2410.10743v1", "date": "2024-10-14", "relevancy": 2.5009, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NT-LLM%3A%20A%20Novel%20Node%20Tokenizer%20for%20Integrating%20Graph%20Structure%20into%0A%20%20Large%20Language%20Models&body=Title%3A%20NT-LLM%3A%20A%20Novel%20Node%20Tokenizer%20for%20Integrating%20Graph%20Structure%20into%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yanbiao%20Ji%20and%20Chang%20Liu%20and%20Xin%20Chen%20and%20Yue%20Ding%20and%20Dan%20Luo%20and%20Mei%20Li%20and%20Wenqing%20Lin%20and%20Hongtao%20Lu%0AAbstract%3A%20%20%20Graphs%20are%20a%20fundamental%20data%20structure%20for%20representing%20relationships%20in%0Areal-world%20scenarios.%20With%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20across%0Avarious%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20there%20has%20been%20growing%0Ainterest%20in%20integrating%20LLMs%20for%20graph%20learning.%20However%2C%20applying%20LLMs%20to%0Agraph-related%20tasks%20poses%20significant%20challenges%2C%20as%20these%20models%20are%20not%0Ainherently%20designed%20to%20capture%20the%20complex%20structural%20information%20present%20in%0Agraphs.%20Existing%20approaches%20address%20this%20challenge%20through%20two%20strategies%3A%20the%0Achain%20of%20tasks%20approach%2C%20which%20uses%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20encode%20the%0Agraph%20structure%20so%20that%20LLMs%20are%20relieved%20from%20understanding%20spatial%20positions%3B%0Aand%20Graph-to-Text%20Conversion%2C%20which%20translates%20graph%20structures%20into%20semantic%0Atext%20representations%20that%20LLMs%20can%20process.%20Despite%20their%20progress%2C%20these%0Amethods%20often%20struggle%20to%20fully%20preserve%20the%20topological%20information%20of%20graphs%0Aor%20require%20extensive%20computational%20resources%2C%20limiting%20their%20practical%0Aapplicability.%0A%20%20In%20this%20work%2C%20we%20introduce%20Node%20Tokenizer%20for%20Large%20Language%20Models%20%28NT-LLM%29%2C%0Aa%20novel%20framework%20that%20efficiently%20encodes%20graph%20structures%20by%20selecting%20key%0Anodes%20as%20anchors%20and%20representing%20each%20node%20based%20on%20its%20relative%20distance%20to%0Athese%20anchors.%20This%20position-anchored%20encoding%20effectively%20captures%20the%20graph%0Atopology%2C%20enabling%20enhanced%20reasoning%20capabilities%20in%20LLMs%20over%20graph%20data.%0AAdditionally%2C%20we%20implement%20a%20task-specific%20tuning%20procedure%20to%20further%20improve%0Astructural%20understanding%20within%20LLMs.%20Through%20extensive%20empirical%20evaluations%2C%0ANT-LLM%20demonstrates%20significant%20performance%20improvements%20across%20a%20variety%20of%0Agraph-related%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNT-LLM%253A%2520A%2520Novel%2520Node%2520Tokenizer%2520for%2520Integrating%2520Graph%2520Structure%2520into%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYanbiao%2520Ji%2520and%2520Chang%2520Liu%2520and%2520Xin%2520Chen%2520and%2520Yue%2520Ding%2520and%2520Dan%2520Luo%2520and%2520Mei%2520Li%2520and%2520Wenqing%2520Lin%2520and%2520Hongtao%2520Lu%26entry.1292438233%3D%2520%2520Graphs%2520are%2520a%2520fundamental%2520data%2520structure%2520for%2520representing%2520relationships%2520in%250Areal-world%2520scenarios.%2520With%2520the%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%250Avarious%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks%252C%2520there%2520has%2520been%2520growing%250Ainterest%2520in%2520integrating%2520LLMs%2520for%2520graph%2520learning.%2520However%252C%2520applying%2520LLMs%2520to%250Agraph-related%2520tasks%2520poses%2520significant%2520challenges%252C%2520as%2520these%2520models%2520are%2520not%250Ainherently%2520designed%2520to%2520capture%2520the%2520complex%2520structural%2520information%2520present%2520in%250Agraphs.%2520Existing%2520approaches%2520address%2520this%2520challenge%2520through%2520two%2520strategies%253A%2520the%250Achain%2520of%2520tasks%2520approach%252C%2520which%2520uses%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520encode%2520the%250Agraph%2520structure%2520so%2520that%2520LLMs%2520are%2520relieved%2520from%2520understanding%2520spatial%2520positions%253B%250Aand%2520Graph-to-Text%2520Conversion%252C%2520which%2520translates%2520graph%2520structures%2520into%2520semantic%250Atext%2520representations%2520that%2520LLMs%2520can%2520process.%2520Despite%2520their%2520progress%252C%2520these%250Amethods%2520often%2520struggle%2520to%2520fully%2520preserve%2520the%2520topological%2520information%2520of%2520graphs%250Aor%2520require%2520extensive%2520computational%2520resources%252C%2520limiting%2520their%2520practical%250Aapplicability.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Node%2520Tokenizer%2520for%2520Large%2520Language%2520Models%2520%2528NT-LLM%2529%252C%250Aa%2520novel%2520framework%2520that%2520efficiently%2520encodes%2520graph%2520structures%2520by%2520selecting%2520key%250Anodes%2520as%2520anchors%2520and%2520representing%2520each%2520node%2520based%2520on%2520its%2520relative%2520distance%2520to%250Athese%2520anchors.%2520This%2520position-anchored%2520encoding%2520effectively%2520captures%2520the%2520graph%250Atopology%252C%2520enabling%2520enhanced%2520reasoning%2520capabilities%2520in%2520LLMs%2520over%2520graph%2520data.%250AAdditionally%252C%2520we%2520implement%2520a%2520task-specific%2520tuning%2520procedure%2520to%2520further%2520improve%250Astructural%2520understanding%2520within%2520LLMs.%2520Through%2520extensive%2520empirical%2520evaluations%252C%250ANT-LLM%2520demonstrates%2520significant%2520performance%2520improvements%2520across%2520a%2520variety%2520of%250Agraph-related%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NT-LLM%3A%20A%20Novel%20Node%20Tokenizer%20for%20Integrating%20Graph%20Structure%20into%0A%20%20Large%20Language%20Models&entry.906535625=Yanbiao%20Ji%20and%20Chang%20Liu%20and%20Xin%20Chen%20and%20Yue%20Ding%20and%20Dan%20Luo%20and%20Mei%20Li%20and%20Wenqing%20Lin%20and%20Hongtao%20Lu&entry.1292438233=%20%20Graphs%20are%20a%20fundamental%20data%20structure%20for%20representing%20relationships%20in%0Areal-world%20scenarios.%20With%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20across%0Avarious%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20there%20has%20been%20growing%0Ainterest%20in%20integrating%20LLMs%20for%20graph%20learning.%20However%2C%20applying%20LLMs%20to%0Agraph-related%20tasks%20poses%20significant%20challenges%2C%20as%20these%20models%20are%20not%0Ainherently%20designed%20to%20capture%20the%20complex%20structural%20information%20present%20in%0Agraphs.%20Existing%20approaches%20address%20this%20challenge%20through%20two%20strategies%3A%20the%0Achain%20of%20tasks%20approach%2C%20which%20uses%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20encode%20the%0Agraph%20structure%20so%20that%20LLMs%20are%20relieved%20from%20understanding%20spatial%20positions%3B%0Aand%20Graph-to-Text%20Conversion%2C%20which%20translates%20graph%20structures%20into%20semantic%0Atext%20representations%20that%20LLMs%20can%20process.%20Despite%20their%20progress%2C%20these%0Amethods%20often%20struggle%20to%20fully%20preserve%20the%20topological%20information%20of%20graphs%0Aor%20require%20extensive%20computational%20resources%2C%20limiting%20their%20practical%0Aapplicability.%0A%20%20In%20this%20work%2C%20we%20introduce%20Node%20Tokenizer%20for%20Large%20Language%20Models%20%28NT-LLM%29%2C%0Aa%20novel%20framework%20that%20efficiently%20encodes%20graph%20structures%20by%20selecting%20key%0Anodes%20as%20anchors%20and%20representing%20each%20node%20based%20on%20its%20relative%20distance%20to%0Athese%20anchors.%20This%20position-anchored%20encoding%20effectively%20captures%20the%20graph%0Atopology%2C%20enabling%20enhanced%20reasoning%20capabilities%20in%20LLMs%20over%20graph%20data.%0AAdditionally%2C%20we%20implement%20a%20task-specific%20tuning%20procedure%20to%20further%20improve%0Astructural%20understanding%20within%20LLMs.%20Through%20extensive%20empirical%20evaluations%2C%0ANT-LLM%20demonstrates%20significant%20performance%20improvements%20across%20a%20variety%20of%0Agraph-related%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10743v1&entry.124074799=Read"},
{"title": "ConML: A Universal Meta-Learning Framework with Task-Level Contrastive\n  Learning", "author": "Shiguang Wu and Yaqing Wang and Yatao Bian and Quanming Yao", "abstract": "  Meta-learning enables learning systems to adapt quickly to new tasks, similar\nto humans. To emulate this human-like rapid learning and enhance alignment and\ndiscrimination abilities, we propose ConML, a universal meta-learning framework\nthat can be applied to various meta-learning algorithms without relying on\nspecific model architectures nor target models. The core of ConML is task-level\ncontrastive learning, which extends contrastive learning from the\nrepresentation space in unsupervised learning to the model space in\nmeta-learning. By leveraging task identity as an additional supervision signal\nduring meta-training, we contrast the outputs of the meta-learner in the model\nspace, minimizing inner-task distance (between models trained on different\nsubsets of the same task) and maximizing inter-task distance (between models\nfrom different tasks). We demonstrate that ConML integrates seamlessly with\noptimization-based, metric-based, and amortization-based meta-learning\nalgorithms, as well as in-context learning, resulting in performance\nimprovements across diverse few-shot learning tasks.\n", "link": "http://arxiv.org/abs/2410.05975v2", "date": "2024-10-14", "relevancy": 2.4882, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConML%3A%20A%20Universal%20Meta-Learning%20Framework%20with%20Task-Level%20Contrastive%0A%20%20Learning&body=Title%3A%20ConML%3A%20A%20Universal%20Meta-Learning%20Framework%20with%20Task-Level%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Yatao%20Bian%20and%20Quanming%20Yao%0AAbstract%3A%20%20%20Meta-learning%20enables%20learning%20systems%20to%20adapt%20quickly%20to%20new%20tasks%2C%20similar%0Ato%20humans.%20To%20emulate%20this%20human-like%20rapid%20learning%20and%20enhance%20alignment%20and%0Adiscrimination%20abilities%2C%20we%20propose%20ConML%2C%20a%20universal%20meta-learning%20framework%0Athat%20can%20be%20applied%20to%20various%20meta-learning%20algorithms%20without%20relying%20on%0Aspecific%20model%20architectures%20nor%20target%20models.%20The%20core%20of%20ConML%20is%20task-level%0Acontrastive%20learning%2C%20which%20extends%20contrastive%20learning%20from%20the%0Arepresentation%20space%20in%20unsupervised%20learning%20to%20the%20model%20space%20in%0Ameta-learning.%20By%20leveraging%20task%20identity%20as%20an%20additional%20supervision%20signal%0Aduring%20meta-training%2C%20we%20contrast%20the%20outputs%20of%20the%20meta-learner%20in%20the%20model%0Aspace%2C%20minimizing%20inner-task%20distance%20%28between%20models%20trained%20on%20different%0Asubsets%20of%20the%20same%20task%29%20and%20maximizing%20inter-task%20distance%20%28between%20models%0Afrom%20different%20tasks%29.%20We%20demonstrate%20that%20ConML%20integrates%20seamlessly%20with%0Aoptimization-based%2C%20metric-based%2C%20and%20amortization-based%20meta-learning%0Aalgorithms%2C%20as%20well%20as%20in-context%20learning%2C%20resulting%20in%20performance%0Aimprovements%20across%20diverse%20few-shot%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConML%253A%2520A%2520Universal%2520Meta-Learning%2520Framework%2520with%2520Task-Level%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DShiguang%2520Wu%2520and%2520Yaqing%2520Wang%2520and%2520Yatao%2520Bian%2520and%2520Quanming%2520Yao%26entry.1292438233%3D%2520%2520Meta-learning%2520enables%2520learning%2520systems%2520to%2520adapt%2520quickly%2520to%2520new%2520tasks%252C%2520similar%250Ato%2520humans.%2520To%2520emulate%2520this%2520human-like%2520rapid%2520learning%2520and%2520enhance%2520alignment%2520and%250Adiscrimination%2520abilities%252C%2520we%2520propose%2520ConML%252C%2520a%2520universal%2520meta-learning%2520framework%250Athat%2520can%2520be%2520applied%2520to%2520various%2520meta-learning%2520algorithms%2520without%2520relying%2520on%250Aspecific%2520model%2520architectures%2520nor%2520target%2520models.%2520The%2520core%2520of%2520ConML%2520is%2520task-level%250Acontrastive%2520learning%252C%2520which%2520extends%2520contrastive%2520learning%2520from%2520the%250Arepresentation%2520space%2520in%2520unsupervised%2520learning%2520to%2520the%2520model%2520space%2520in%250Ameta-learning.%2520By%2520leveraging%2520task%2520identity%2520as%2520an%2520additional%2520supervision%2520signal%250Aduring%2520meta-training%252C%2520we%2520contrast%2520the%2520outputs%2520of%2520the%2520meta-learner%2520in%2520the%2520model%250Aspace%252C%2520minimizing%2520inner-task%2520distance%2520%2528between%2520models%2520trained%2520on%2520different%250Asubsets%2520of%2520the%2520same%2520task%2529%2520and%2520maximizing%2520inter-task%2520distance%2520%2528between%2520models%250Afrom%2520different%2520tasks%2529.%2520We%2520demonstrate%2520that%2520ConML%2520integrates%2520seamlessly%2520with%250Aoptimization-based%252C%2520metric-based%252C%2520and%2520amortization-based%2520meta-learning%250Aalgorithms%252C%2520as%2520well%2520as%2520in-context%2520learning%252C%2520resulting%2520in%2520performance%250Aimprovements%2520across%2520diverse%2520few-shot%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConML%3A%20A%20Universal%20Meta-Learning%20Framework%20with%20Task-Level%20Contrastive%0A%20%20Learning&entry.906535625=Shiguang%20Wu%20and%20Yaqing%20Wang%20and%20Yatao%20Bian%20and%20Quanming%20Yao&entry.1292438233=%20%20Meta-learning%20enables%20learning%20systems%20to%20adapt%20quickly%20to%20new%20tasks%2C%20similar%0Ato%20humans.%20To%20emulate%20this%20human-like%20rapid%20learning%20and%20enhance%20alignment%20and%0Adiscrimination%20abilities%2C%20we%20propose%20ConML%2C%20a%20universal%20meta-learning%20framework%0Athat%20can%20be%20applied%20to%20various%20meta-learning%20algorithms%20without%20relying%20on%0Aspecific%20model%20architectures%20nor%20target%20models.%20The%20core%20of%20ConML%20is%20task-level%0Acontrastive%20learning%2C%20which%20extends%20contrastive%20learning%20from%20the%0Arepresentation%20space%20in%20unsupervised%20learning%20to%20the%20model%20space%20in%0Ameta-learning.%20By%20leveraging%20task%20identity%20as%20an%20additional%20supervision%20signal%0Aduring%20meta-training%2C%20we%20contrast%20the%20outputs%20of%20the%20meta-learner%20in%20the%20model%0Aspace%2C%20minimizing%20inner-task%20distance%20%28between%20models%20trained%20on%20different%0Asubsets%20of%20the%20same%20task%29%20and%20maximizing%20inter-task%20distance%20%28between%20models%0Afrom%20different%20tasks%29.%20We%20demonstrate%20that%20ConML%20integrates%20seamlessly%20with%0Aoptimization-based%2C%20metric-based%2C%20and%20amortization-based%20meta-learning%0Aalgorithms%2C%20as%20well%20as%20in-context%20learning%2C%20resulting%20in%20performance%0Aimprovements%20across%20diverse%20few-shot%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05975v2&entry.124074799=Read"},
{"title": "Neural networks that overcome classic challenges through practice", "author": "Kazuki Irie and Brenden M. Lake", "abstract": "  Since the earliest proposals for neural network models of the mind and brain,\ncritics have pointed out key weaknesses in these models compared to human\ncognitive abilities. Here we review recent work that has used metalearning to\nhelp overcome some of these challenges. We characterize their successes as\naddressing an important developmental problem: they provide machines with an\nincentive to improve X (where X represents the desired capability) and\nopportunities to practice it, through explicit optimization for X; unlike\nconventional approaches that hope for achieving X through generalization from\nrelated but different objectives. We review applications of this principle to\nfour classic challenges: systematicity, catastrophic forgetting, few-shot\nlearning and multi-step reasoning; we also discuss related aspects of human\ndevelopment in natural environments.\n", "link": "http://arxiv.org/abs/2410.10596v1", "date": "2024-10-14", "relevancy": 2.4719, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20networks%20that%20overcome%20classic%20challenges%20through%20practice&body=Title%3A%20Neural%20networks%20that%20overcome%20classic%20challenges%20through%20practice%0AAuthor%3A%20Kazuki%20Irie%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Since%20the%20earliest%20proposals%20for%20neural%20network%20models%20of%20the%20mind%20and%20brain%2C%0Acritics%20have%20pointed%20out%20key%20weaknesses%20in%20these%20models%20compared%20to%20human%0Acognitive%20abilities.%20Here%20we%20review%20recent%20work%20that%20has%20used%20metalearning%20to%0Ahelp%20overcome%20some%20of%20these%20challenges.%20We%20characterize%20their%20successes%20as%0Aaddressing%20an%20important%20developmental%20problem%3A%20they%20provide%20machines%20with%20an%0Aincentive%20to%20improve%20X%20%28where%20X%20represents%20the%20desired%20capability%29%20and%0Aopportunities%20to%20practice%20it%2C%20through%20explicit%20optimization%20for%20X%3B%20unlike%0Aconventional%20approaches%20that%20hope%20for%20achieving%20X%20through%20generalization%20from%0Arelated%20but%20different%20objectives.%20We%20review%20applications%20of%20this%20principle%20to%0Afour%20classic%20challenges%3A%20systematicity%2C%20catastrophic%20forgetting%2C%20few-shot%0Alearning%20and%20multi-step%20reasoning%3B%20we%20also%20discuss%20related%20aspects%20of%20human%0Adevelopment%20in%20natural%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520networks%2520that%2520overcome%2520classic%2520challenges%2520through%2520practice%26entry.906535625%3DKazuki%2520Irie%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Since%2520the%2520earliest%2520proposals%2520for%2520neural%2520network%2520models%2520of%2520the%2520mind%2520and%2520brain%252C%250Acritics%2520have%2520pointed%2520out%2520key%2520weaknesses%2520in%2520these%2520models%2520compared%2520to%2520human%250Acognitive%2520abilities.%2520Here%2520we%2520review%2520recent%2520work%2520that%2520has%2520used%2520metalearning%2520to%250Ahelp%2520overcome%2520some%2520of%2520these%2520challenges.%2520We%2520characterize%2520their%2520successes%2520as%250Aaddressing%2520an%2520important%2520developmental%2520problem%253A%2520they%2520provide%2520machines%2520with%2520an%250Aincentive%2520to%2520improve%2520X%2520%2528where%2520X%2520represents%2520the%2520desired%2520capability%2529%2520and%250Aopportunities%2520to%2520practice%2520it%252C%2520through%2520explicit%2520optimization%2520for%2520X%253B%2520unlike%250Aconventional%2520approaches%2520that%2520hope%2520for%2520achieving%2520X%2520through%2520generalization%2520from%250Arelated%2520but%2520different%2520objectives.%2520We%2520review%2520applications%2520of%2520this%2520principle%2520to%250Afour%2520classic%2520challenges%253A%2520systematicity%252C%2520catastrophic%2520forgetting%252C%2520few-shot%250Alearning%2520and%2520multi-step%2520reasoning%253B%2520we%2520also%2520discuss%2520related%2520aspects%2520of%2520human%250Adevelopment%2520in%2520natural%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20networks%20that%20overcome%20classic%20challenges%20through%20practice&entry.906535625=Kazuki%20Irie%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Since%20the%20earliest%20proposals%20for%20neural%20network%20models%20of%20the%20mind%20and%20brain%2C%0Acritics%20have%20pointed%20out%20key%20weaknesses%20in%20these%20models%20compared%20to%20human%0Acognitive%20abilities.%20Here%20we%20review%20recent%20work%20that%20has%20used%20metalearning%20to%0Ahelp%20overcome%20some%20of%20these%20challenges.%20We%20characterize%20their%20successes%20as%0Aaddressing%20an%20important%20developmental%20problem%3A%20they%20provide%20machines%20with%20an%0Aincentive%20to%20improve%20X%20%28where%20X%20represents%20the%20desired%20capability%29%20and%0Aopportunities%20to%20practice%20it%2C%20through%20explicit%20optimization%20for%20X%3B%20unlike%0Aconventional%20approaches%20that%20hope%20for%20achieving%20X%20through%20generalization%20from%0Arelated%20but%20different%20objectives.%20We%20review%20applications%20of%20this%20principle%20to%0Afour%20classic%20challenges%3A%20systematicity%2C%20catastrophic%20forgetting%2C%20few-shot%0Alearning%20and%20multi-step%20reasoning%3B%20we%20also%20discuss%20related%20aspects%20of%20human%0Adevelopment%20in%20natural%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10596v1&entry.124074799=Read"},
{"title": "Vision-guided and Mask-enhanced Adaptive Denoising for Prompt-based\n  Image Editing", "author": "Kejie Wang and Xuemeng Song and Meng Liu and Weili Guan and Liqiang Nie", "abstract": "  Text-to-image diffusion models have demonstrated remarkable progress in\nsynthesizing high-quality images from text prompts, which boosts researches on\nprompt-based image editing that edits a source image according to a target\nprompt. Despite their advances, existing methods still encounter three key\nissues: 1) limited capacity of the text prompt in guiding target image\ngeneration, 2) insufficient mining of word-to-patch and patch-to-patch\nrelationships for grounding editing areas, and 3) unified editing strength for\nall regions during each denoising step. To address these issues, we present a\nVision-guided and Mask-enhanced Adaptive Editing (ViMAEdit) method with three\nkey novel designs. First, we propose to leverage image embeddings as explicit\nguidance to enhance the conventional textual prompt-based denoising process,\nwhere a CLIP-based target image embedding estimation strategy is introduced.\nSecond, we devise a self-attention-guided iterative editing area grounding\nstrategy, which iteratively exploits patch-to-patch relationships conveyed by\nself-attention maps to refine those word-to-patch relationships contained in\ncross-attention maps. Last, we present a spatially adaptive variance-guided\nsampling, which highlights sampling variances for critical image regions to\npromote the editing capability. Experimental results demonstrate the superior\nediting capacity of ViMAEdit over all existing methods.\n", "link": "http://arxiv.org/abs/2410.10496v1", "date": "2024-10-14", "relevancy": 2.4547, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6834}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6003}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-guided%20and%20Mask-enhanced%20Adaptive%20Denoising%20for%20Prompt-based%0A%20%20Image%20Editing&body=Title%3A%20Vision-guided%20and%20Mask-enhanced%20Adaptive%20Denoising%20for%20Prompt-based%0A%20%20Image%20Editing%0AAuthor%3A%20Kejie%20Wang%20and%20Xuemeng%20Song%20and%20Meng%20Liu%20and%20Weili%20Guan%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20demonstrated%20remarkable%20progress%20in%0Asynthesizing%20high-quality%20images%20from%20text%20prompts%2C%20which%20boosts%20researches%20on%0Aprompt-based%20image%20editing%20that%20edits%20a%20source%20image%20according%20to%20a%20target%0Aprompt.%20Despite%20their%20advances%2C%20existing%20methods%20still%20encounter%20three%20key%0Aissues%3A%201%29%20limited%20capacity%20of%20the%20text%20prompt%20in%20guiding%20target%20image%0Ageneration%2C%202%29%20insufficient%20mining%20of%20word-to-patch%20and%20patch-to-patch%0Arelationships%20for%20grounding%20editing%20areas%2C%20and%203%29%20unified%20editing%20strength%20for%0Aall%20regions%20during%20each%20denoising%20step.%20To%20address%20these%20issues%2C%20we%20present%20a%0AVision-guided%20and%20Mask-enhanced%20Adaptive%20Editing%20%28ViMAEdit%29%20method%20with%20three%0Akey%20novel%20designs.%20First%2C%20we%20propose%20to%20leverage%20image%20embeddings%20as%20explicit%0Aguidance%20to%20enhance%20the%20conventional%20textual%20prompt-based%20denoising%20process%2C%0Awhere%20a%20CLIP-based%20target%20image%20embedding%20estimation%20strategy%20is%20introduced.%0ASecond%2C%20we%20devise%20a%20self-attention-guided%20iterative%20editing%20area%20grounding%0Astrategy%2C%20which%20iteratively%20exploits%20patch-to-patch%20relationships%20conveyed%20by%0Aself-attention%20maps%20to%20refine%20those%20word-to-patch%20relationships%20contained%20in%0Across-attention%20maps.%20Last%2C%20we%20present%20a%20spatially%20adaptive%20variance-guided%0Asampling%2C%20which%20highlights%20sampling%20variances%20for%20critical%20image%20regions%20to%0Apromote%20the%20editing%20capability.%20Experimental%20results%20demonstrate%20the%20superior%0Aediting%20capacity%20of%20ViMAEdit%20over%20all%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-guided%2520and%2520Mask-enhanced%2520Adaptive%2520Denoising%2520for%2520Prompt-based%250A%2520%2520Image%2520Editing%26entry.906535625%3DKejie%2520Wang%2520and%2520Xuemeng%2520Song%2520and%2520Meng%2520Liu%2520and%2520Weili%2520Guan%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520demonstrated%2520remarkable%2520progress%2520in%250Asynthesizing%2520high-quality%2520images%2520from%2520text%2520prompts%252C%2520which%2520boosts%2520researches%2520on%250Aprompt-based%2520image%2520editing%2520that%2520edits%2520a%2520source%2520image%2520according%2520to%2520a%2520target%250Aprompt.%2520Despite%2520their%2520advances%252C%2520existing%2520methods%2520still%2520encounter%2520three%2520key%250Aissues%253A%25201%2529%2520limited%2520capacity%2520of%2520the%2520text%2520prompt%2520in%2520guiding%2520target%2520image%250Ageneration%252C%25202%2529%2520insufficient%2520mining%2520of%2520word-to-patch%2520and%2520patch-to-patch%250Arelationships%2520for%2520grounding%2520editing%2520areas%252C%2520and%25203%2529%2520unified%2520editing%2520strength%2520for%250Aall%2520regions%2520during%2520each%2520denoising%2520step.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520a%250AVision-guided%2520and%2520Mask-enhanced%2520Adaptive%2520Editing%2520%2528ViMAEdit%2529%2520method%2520with%2520three%250Akey%2520novel%2520designs.%2520First%252C%2520we%2520propose%2520to%2520leverage%2520image%2520embeddings%2520as%2520explicit%250Aguidance%2520to%2520enhance%2520the%2520conventional%2520textual%2520prompt-based%2520denoising%2520process%252C%250Awhere%2520a%2520CLIP-based%2520target%2520image%2520embedding%2520estimation%2520strategy%2520is%2520introduced.%250ASecond%252C%2520we%2520devise%2520a%2520self-attention-guided%2520iterative%2520editing%2520area%2520grounding%250Astrategy%252C%2520which%2520iteratively%2520exploits%2520patch-to-patch%2520relationships%2520conveyed%2520by%250Aself-attention%2520maps%2520to%2520refine%2520those%2520word-to-patch%2520relationships%2520contained%2520in%250Across-attention%2520maps.%2520Last%252C%2520we%2520present%2520a%2520spatially%2520adaptive%2520variance-guided%250Asampling%252C%2520which%2520highlights%2520sampling%2520variances%2520for%2520critical%2520image%2520regions%2520to%250Apromote%2520the%2520editing%2520capability.%2520Experimental%2520results%2520demonstrate%2520the%2520superior%250Aediting%2520capacity%2520of%2520ViMAEdit%2520over%2520all%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-guided%20and%20Mask-enhanced%20Adaptive%20Denoising%20for%20Prompt-based%0A%20%20Image%20Editing&entry.906535625=Kejie%20Wang%20and%20Xuemeng%20Song%20and%20Meng%20Liu%20and%20Weili%20Guan%20and%20Liqiang%20Nie&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20demonstrated%20remarkable%20progress%20in%0Asynthesizing%20high-quality%20images%20from%20text%20prompts%2C%20which%20boosts%20researches%20on%0Aprompt-based%20image%20editing%20that%20edits%20a%20source%20image%20according%20to%20a%20target%0Aprompt.%20Despite%20their%20advances%2C%20existing%20methods%20still%20encounter%20three%20key%0Aissues%3A%201%29%20limited%20capacity%20of%20the%20text%20prompt%20in%20guiding%20target%20image%0Ageneration%2C%202%29%20insufficient%20mining%20of%20word-to-patch%20and%20patch-to-patch%0Arelationships%20for%20grounding%20editing%20areas%2C%20and%203%29%20unified%20editing%20strength%20for%0Aall%20regions%20during%20each%20denoising%20step.%20To%20address%20these%20issues%2C%20we%20present%20a%0AVision-guided%20and%20Mask-enhanced%20Adaptive%20Editing%20%28ViMAEdit%29%20method%20with%20three%0Akey%20novel%20designs.%20First%2C%20we%20propose%20to%20leverage%20image%20embeddings%20as%20explicit%0Aguidance%20to%20enhance%20the%20conventional%20textual%20prompt-based%20denoising%20process%2C%0Awhere%20a%20CLIP-based%20target%20image%20embedding%20estimation%20strategy%20is%20introduced.%0ASecond%2C%20we%20devise%20a%20self-attention-guided%20iterative%20editing%20area%20grounding%0Astrategy%2C%20which%20iteratively%20exploits%20patch-to-patch%20relationships%20conveyed%20by%0Aself-attention%20maps%20to%20refine%20those%20word-to-patch%20relationships%20contained%20in%0Across-attention%20maps.%20Last%2C%20we%20present%20a%20spatially%20adaptive%20variance-guided%0Asampling%2C%20which%20highlights%20sampling%20variances%20for%20critical%20image%20regions%20to%0Apromote%20the%20editing%20capability.%20Experimental%20results%20demonstrate%20the%20superior%0Aediting%20capacity%20of%20ViMAEdit%20over%20all%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10496v1&entry.124074799=Read"},
{"title": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems", "author": "Subhankar Maity and Aniket Deroy", "abstract": "  Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences.\n", "link": "http://arxiv.org/abs/2410.10650v1", "date": "2024-10-14", "relevancy": 2.446, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5459}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4717}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20and%20Its%20Impact%20on%20Personalized%20Intelligent%20Tutoring%0A%20%20Systems&body=Title%3A%20Generative%20AI%20and%20Its%20Impact%20on%20Personalized%20Intelligent%20Tutoring%0A%20%20Systems%0AAuthor%3A%20Subhankar%20Maity%20and%20Aniket%20Deroy%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28AI%29%20is%20revolutionizing%20educational%0Atechnology%20by%20enabling%20highly%20personalized%20and%20adaptive%20learning%20environments%0Awithin%20Intelligent%20Tutoring%20Systems%20%28ITS%29.%20This%20report%20delves%20into%20the%0Aintegration%20of%20Generative%20AI%2C%20particularly%20large%20language%20models%20%28LLMs%29%20like%0AGPT-4%2C%20into%20ITS%20to%20enhance%20personalized%20education%20through%20dynamic%20content%0Ageneration%2C%20real-time%20feedback%2C%20and%20adaptive%20learning%20pathways.%20We%20explore%20key%0Aapplications%20such%20as%20automated%20question%20generation%2C%20customized%20feedback%0Amechanisms%2C%20and%20interactive%20dialogue%20systems%20that%20respond%20to%20individual%20learner%0Aneeds.%20The%20report%20also%20addresses%20significant%20challenges%2C%20including%20ensuring%0Apedagogical%20accuracy%2C%20mitigating%20inherent%20biases%20in%20AI%20models%2C%20and%20maintaining%0Alearner%20engagement.%20Future%20directions%20highlight%20the%20potential%20advancements%20in%0Amultimodal%20AI%20integration%2C%20emotional%20intelligence%20in%20tutoring%20systems%2C%20and%20the%0Aethical%20implications%20of%20AI-driven%20education.%20By%20synthesizing%20current%20research%0Aand%20practical%20implementations%2C%20this%20report%20underscores%20the%20transformative%0Apotential%20of%20Generative%20AI%20in%20creating%20more%20effective%2C%20equitable%2C%20and%20engaging%0Aeducational%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520and%2520Its%2520Impact%2520on%2520Personalized%2520Intelligent%2520Tutoring%250A%2520%2520Systems%26entry.906535625%3DSubhankar%2520Maity%2520and%2520Aniket%2520Deroy%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520is%2520revolutionizing%2520educational%250Atechnology%2520by%2520enabling%2520highly%2520personalized%2520and%2520adaptive%2520learning%2520environments%250Awithin%2520Intelligent%2520Tutoring%2520Systems%2520%2528ITS%2529.%2520This%2520report%2520delves%2520into%2520the%250Aintegration%2520of%2520Generative%2520AI%252C%2520particularly%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%250AGPT-4%252C%2520into%2520ITS%2520to%2520enhance%2520personalized%2520education%2520through%2520dynamic%2520content%250Ageneration%252C%2520real-time%2520feedback%252C%2520and%2520adaptive%2520learning%2520pathways.%2520We%2520explore%2520key%250Aapplications%2520such%2520as%2520automated%2520question%2520generation%252C%2520customized%2520feedback%250Amechanisms%252C%2520and%2520interactive%2520dialogue%2520systems%2520that%2520respond%2520to%2520individual%2520learner%250Aneeds.%2520The%2520report%2520also%2520addresses%2520significant%2520challenges%252C%2520including%2520ensuring%250Apedagogical%2520accuracy%252C%2520mitigating%2520inherent%2520biases%2520in%2520AI%2520models%252C%2520and%2520maintaining%250Alearner%2520engagement.%2520Future%2520directions%2520highlight%2520the%2520potential%2520advancements%2520in%250Amultimodal%2520AI%2520integration%252C%2520emotional%2520intelligence%2520in%2520tutoring%2520systems%252C%2520and%2520the%250Aethical%2520implications%2520of%2520AI-driven%2520education.%2520By%2520synthesizing%2520current%2520research%250Aand%2520practical%2520implementations%252C%2520this%2520report%2520underscores%2520the%2520transformative%250Apotential%2520of%2520Generative%2520AI%2520in%2520creating%2520more%2520effective%252C%2520equitable%252C%2520and%2520engaging%250Aeducational%2520experiences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20and%20Its%20Impact%20on%20Personalized%20Intelligent%20Tutoring%0A%20%20Systems&entry.906535625=Subhankar%20Maity%20and%20Aniket%20Deroy&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28AI%29%20is%20revolutionizing%20educational%0Atechnology%20by%20enabling%20highly%20personalized%20and%20adaptive%20learning%20environments%0Awithin%20Intelligent%20Tutoring%20Systems%20%28ITS%29.%20This%20report%20delves%20into%20the%0Aintegration%20of%20Generative%20AI%2C%20particularly%20large%20language%20models%20%28LLMs%29%20like%0AGPT-4%2C%20into%20ITS%20to%20enhance%20personalized%20education%20through%20dynamic%20content%0Ageneration%2C%20real-time%20feedback%2C%20and%20adaptive%20learning%20pathways.%20We%20explore%20key%0Aapplications%20such%20as%20automated%20question%20generation%2C%20customized%20feedback%0Amechanisms%2C%20and%20interactive%20dialogue%20systems%20that%20respond%20to%20individual%20learner%0Aneeds.%20The%20report%20also%20addresses%20significant%20challenges%2C%20including%20ensuring%0Apedagogical%20accuracy%2C%20mitigating%20inherent%20biases%20in%20AI%20models%2C%20and%20maintaining%0Alearner%20engagement.%20Future%20directions%20highlight%20the%20potential%20advancements%20in%0Amultimodal%20AI%20integration%2C%20emotional%20intelligence%20in%20tutoring%20systems%2C%20and%20the%0Aethical%20implications%20of%20AI-driven%20education.%20By%20synthesizing%20current%20research%0Aand%20practical%20implementations%2C%20this%20report%20underscores%20the%20transformative%0Apotential%20of%20Generative%20AI%20in%20creating%20more%20effective%2C%20equitable%2C%20and%20engaging%0Aeducational%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10650v1&entry.124074799=Read"},
{"title": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image\n  Animation", "author": "Jiahao Cui and Hui Li and Yao Yao and Hao Zhu and Hanlin Shang and Kaihui Cheng and Hang Zhou and Siyu Zhu and Jingdong Wang", "abstract": "  Recent advances in latent diffusion-based generative models for portrait\nimage animation, such as Hallo, have achieved impressive results in\nshort-duration video synthesis. In this paper, we present updates to Hallo,\nintroducing several design enhancements to extend its capabilities. First, we\nextend the method to produce long-duration videos. To address substantial\nchallenges such as appearance drift and temporal artifacts, we investigate\naugmentation strategies within the image space of conditional motion frames.\nSpecifically, we introduce a patch-drop technique augmented with Gaussian noise\nto enhance visual consistency and temporal coherence over long duration.\nSecond, we achieve 4K resolution portrait video generation. To accomplish this,\nwe implement vector quantization of latent codes and apply temporal alignment\ntechniques to maintain coherence across the temporal dimension. By integrating\na high-quality decoder, we realize visual synthesis at 4K resolution. Third, we\nincorporate adjustable semantic textual labels for portrait expressions as\nconditional inputs. This extends beyond traditional audio cues to improve\ncontrollability and increase the diversity of the generated content. To the\nbest of our knowledge, Hallo2, proposed in this paper, is the first method to\nachieve 4K resolution and generate hour-long, audio-driven portrait image\nanimations enhanced with textual prompts. We have conducted extensive\nexperiments to evaluate our method on publicly available datasets, including\nHDTF, CelebV, and our introduced \"Wild\" dataset. The experimental results\ndemonstrate that our approach achieves state-of-the-art performance in\nlong-duration portrait video animation, successfully generating rich and\ncontrollable content at 4K resolution for duration extending up to tens of\nminutes. Project page https://fudan-generative-vision.github.io/hallo2\n", "link": "http://arxiv.org/abs/2410.07718v2", "date": "2024-10-14", "relevancy": 2.4454, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6195}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6128}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallo2%3A%20Long-Duration%20and%20High-Resolution%20Audio-Driven%20Portrait%20Image%0A%20%20Animation&body=Title%3A%20Hallo2%3A%20Long-Duration%20and%20High-Resolution%20Audio-Driven%20Portrait%20Image%0A%20%20Animation%0AAuthor%3A%20Jiahao%20Cui%20and%20Hui%20Li%20and%20Yao%20Yao%20and%20Hao%20Zhu%20and%20Hanlin%20Shang%20and%20Kaihui%20Cheng%20and%20Hang%20Zhou%20and%20Siyu%20Zhu%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20latent%20diffusion-based%20generative%20models%20for%20portrait%0Aimage%20animation%2C%20such%20as%20Hallo%2C%20have%20achieved%20impressive%20results%20in%0Ashort-duration%20video%20synthesis.%20In%20this%20paper%2C%20we%20present%20updates%20to%20Hallo%2C%0Aintroducing%20several%20design%20enhancements%20to%20extend%20its%20capabilities.%20First%2C%20we%0Aextend%20the%20method%20to%20produce%20long-duration%20videos.%20To%20address%20substantial%0Achallenges%20such%20as%20appearance%20drift%20and%20temporal%20artifacts%2C%20we%20investigate%0Aaugmentation%20strategies%20within%20the%20image%20space%20of%20conditional%20motion%20frames.%0ASpecifically%2C%20we%20introduce%20a%20patch-drop%20technique%20augmented%20with%20Gaussian%20noise%0Ato%20enhance%20visual%20consistency%20and%20temporal%20coherence%20over%20long%20duration.%0ASecond%2C%20we%20achieve%204K%20resolution%20portrait%20video%20generation.%20To%20accomplish%20this%2C%0Awe%20implement%20vector%20quantization%20of%20latent%20codes%20and%20apply%20temporal%20alignment%0Atechniques%20to%20maintain%20coherence%20across%20the%20temporal%20dimension.%20By%20integrating%0Aa%20high-quality%20decoder%2C%20we%20realize%20visual%20synthesis%20at%204K%20resolution.%20Third%2C%20we%0Aincorporate%20adjustable%20semantic%20textual%20labels%20for%20portrait%20expressions%20as%0Aconditional%20inputs.%20This%20extends%20beyond%20traditional%20audio%20cues%20to%20improve%0Acontrollability%20and%20increase%20the%20diversity%20of%20the%20generated%20content.%20To%20the%0Abest%20of%20our%20knowledge%2C%20Hallo2%2C%20proposed%20in%20this%20paper%2C%20is%20the%20first%20method%20to%0Aachieve%204K%20resolution%20and%20generate%20hour-long%2C%20audio-driven%20portrait%20image%0Aanimations%20enhanced%20with%20textual%20prompts.%20We%20have%20conducted%20extensive%0Aexperiments%20to%20evaluate%20our%20method%20on%20publicly%20available%20datasets%2C%20including%0AHDTF%2C%20CelebV%2C%20and%20our%20introduced%20%22Wild%22%20dataset.%20The%20experimental%20results%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%0Along-duration%20portrait%20video%20animation%2C%20successfully%20generating%20rich%20and%0Acontrollable%20content%20at%204K%20resolution%20for%20duration%20extending%20up%20to%20tens%20of%0Aminutes.%20Project%20page%20https%3A//fudan-generative-vision.github.io/hallo2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallo2%253A%2520Long-Duration%2520and%2520High-Resolution%2520Audio-Driven%2520Portrait%2520Image%250A%2520%2520Animation%26entry.906535625%3DJiahao%2520Cui%2520and%2520Hui%2520Li%2520and%2520Yao%2520Yao%2520and%2520Hao%2520Zhu%2520and%2520Hanlin%2520Shang%2520and%2520Kaihui%2520Cheng%2520and%2520Hang%2520Zhou%2520and%2520Siyu%2520Zhu%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520latent%2520diffusion-based%2520generative%2520models%2520for%2520portrait%250Aimage%2520animation%252C%2520such%2520as%2520Hallo%252C%2520have%2520achieved%2520impressive%2520results%2520in%250Ashort-duration%2520video%2520synthesis.%2520In%2520this%2520paper%252C%2520we%2520present%2520updates%2520to%2520Hallo%252C%250Aintroducing%2520several%2520design%2520enhancements%2520to%2520extend%2520its%2520capabilities.%2520First%252C%2520we%250Aextend%2520the%2520method%2520to%2520produce%2520long-duration%2520videos.%2520To%2520address%2520substantial%250Achallenges%2520such%2520as%2520appearance%2520drift%2520and%2520temporal%2520artifacts%252C%2520we%2520investigate%250Aaugmentation%2520strategies%2520within%2520the%2520image%2520space%2520of%2520conditional%2520motion%2520frames.%250ASpecifically%252C%2520we%2520introduce%2520a%2520patch-drop%2520technique%2520augmented%2520with%2520Gaussian%2520noise%250Ato%2520enhance%2520visual%2520consistency%2520and%2520temporal%2520coherence%2520over%2520long%2520duration.%250ASecond%252C%2520we%2520achieve%25204K%2520resolution%2520portrait%2520video%2520generation.%2520To%2520accomplish%2520this%252C%250Awe%2520implement%2520vector%2520quantization%2520of%2520latent%2520codes%2520and%2520apply%2520temporal%2520alignment%250Atechniques%2520to%2520maintain%2520coherence%2520across%2520the%2520temporal%2520dimension.%2520By%2520integrating%250Aa%2520high-quality%2520decoder%252C%2520we%2520realize%2520visual%2520synthesis%2520at%25204K%2520resolution.%2520Third%252C%2520we%250Aincorporate%2520adjustable%2520semantic%2520textual%2520labels%2520for%2520portrait%2520expressions%2520as%250Aconditional%2520inputs.%2520This%2520extends%2520beyond%2520traditional%2520audio%2520cues%2520to%2520improve%250Acontrollability%2520and%2520increase%2520the%2520diversity%2520of%2520the%2520generated%2520content.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520Hallo2%252C%2520proposed%2520in%2520this%2520paper%252C%2520is%2520the%2520first%2520method%2520to%250Aachieve%25204K%2520resolution%2520and%2520generate%2520hour-long%252C%2520audio-driven%2520portrait%2520image%250Aanimations%2520enhanced%2520with%2520textual%2520prompts.%2520We%2520have%2520conducted%2520extensive%250Aexperiments%2520to%2520evaluate%2520our%2520method%2520on%2520publicly%2520available%2520datasets%252C%2520including%250AHDTF%252C%2520CelebV%252C%2520and%2520our%2520introduced%2520%2522Wild%2522%2520dataset.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%250Along-duration%2520portrait%2520video%2520animation%252C%2520successfully%2520generating%2520rich%2520and%250Acontrollable%2520content%2520at%25204K%2520resolution%2520for%2520duration%2520extending%2520up%2520to%2520tens%2520of%250Aminutes.%2520Project%2520page%2520https%253A//fudan-generative-vision.github.io/hallo2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallo2%3A%20Long-Duration%20and%20High-Resolution%20Audio-Driven%20Portrait%20Image%0A%20%20Animation&entry.906535625=Jiahao%20Cui%20and%20Hui%20Li%20and%20Yao%20Yao%20and%20Hao%20Zhu%20and%20Hanlin%20Shang%20and%20Kaihui%20Cheng%20and%20Hang%20Zhou%20and%20Siyu%20Zhu%20and%20Jingdong%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20latent%20diffusion-based%20generative%20models%20for%20portrait%0Aimage%20animation%2C%20such%20as%20Hallo%2C%20have%20achieved%20impressive%20results%20in%0Ashort-duration%20video%20synthesis.%20In%20this%20paper%2C%20we%20present%20updates%20to%20Hallo%2C%0Aintroducing%20several%20design%20enhancements%20to%20extend%20its%20capabilities.%20First%2C%20we%0Aextend%20the%20method%20to%20produce%20long-duration%20videos.%20To%20address%20substantial%0Achallenges%20such%20as%20appearance%20drift%20and%20temporal%20artifacts%2C%20we%20investigate%0Aaugmentation%20strategies%20within%20the%20image%20space%20of%20conditional%20motion%20frames.%0ASpecifically%2C%20we%20introduce%20a%20patch-drop%20technique%20augmented%20with%20Gaussian%20noise%0Ato%20enhance%20visual%20consistency%20and%20temporal%20coherence%20over%20long%20duration.%0ASecond%2C%20we%20achieve%204K%20resolution%20portrait%20video%20generation.%20To%20accomplish%20this%2C%0Awe%20implement%20vector%20quantization%20of%20latent%20codes%20and%20apply%20temporal%20alignment%0Atechniques%20to%20maintain%20coherence%20across%20the%20temporal%20dimension.%20By%20integrating%0Aa%20high-quality%20decoder%2C%20we%20realize%20visual%20synthesis%20at%204K%20resolution.%20Third%2C%20we%0Aincorporate%20adjustable%20semantic%20textual%20labels%20for%20portrait%20expressions%20as%0Aconditional%20inputs.%20This%20extends%20beyond%20traditional%20audio%20cues%20to%20improve%0Acontrollability%20and%20increase%20the%20diversity%20of%20the%20generated%20content.%20To%20the%0Abest%20of%20our%20knowledge%2C%20Hallo2%2C%20proposed%20in%20this%20paper%2C%20is%20the%20first%20method%20to%0Aachieve%204K%20resolution%20and%20generate%20hour-long%2C%20audio-driven%20portrait%20image%0Aanimations%20enhanced%20with%20textual%20prompts.%20We%20have%20conducted%20extensive%0Aexperiments%20to%20evaluate%20our%20method%20on%20publicly%20available%20datasets%2C%20including%0AHDTF%2C%20CelebV%2C%20and%20our%20introduced%20%22Wild%22%20dataset.%20The%20experimental%20results%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%0Along-duration%20portrait%20video%20animation%2C%20successfully%20generating%20rich%20and%0Acontrollable%20content%20at%204K%20resolution%20for%20duration%20extending%20up%20to%20tens%20of%0Aminutes.%20Project%20page%20https%3A//fudan-generative-vision.github.io/hallo2%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07718v2&entry.124074799=Read"},
{"title": "Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation", "author": "Chenyu Zhang and Wenxue Guan and Xiaodan Xing and Guan Yang", "abstract": "  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n", "link": "http://arxiv.org/abs/2410.10551v1", "date": "2024-10-14", "relevancy": 2.4335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation&body=Title%3A%20Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation%0AAuthor%3A%20Chenyu%20Zhang%20and%20Wenxue%20Guan%20and%20Xiaodan%20Xing%20and%20Guan%20Yang%0AAbstract%3A%20%20%20Whole%20heart%20segmentation%20%28WHS%29%20supports%20cardiovascular%20disease%20%28CVD%29%0Adiagnosis%2C%20disease%20monitoring%2C%20treatment%20planning%2C%20and%20prognosis.%20Deep%20learning%0Ahas%20become%20the%20most%20widely%20used%20method%20for%20WHS%20applications%20in%20recent%20years.%0AHowever%2C%20segmentation%20of%20whole-heart%20structures%20faces%20numerous%20challenges%0Aincluding%20heart%20shape%20variability%20during%20the%20cardiac%20cycle%2C%20clinical%20artifacts%0Alike%20motion%20and%20poor%20contrast-to-noise%20ratio%2C%20domain%20shifts%20in%20multi-center%0Adata%2C%20and%20the%20distinct%20modalities%20of%20CT%20and%20MRI.%20To%20address%20these%20limitations%0Aand%20improve%20segmentation%20quality%2C%20this%20paper%20introduces%20a%20new%0Atopology-preserving%20module%20that%20is%20integrated%20into%20deep%20neural%20networks.%20The%0Aimplementation%20achieves%20anatomically%20plausible%20segmentation%20by%20using%20learned%0Atopology-preserving%20fields%2C%20which%20are%20based%20entirely%20on%203D%20convolution%20and%20are%0Atherefore%20very%20effective%20for%203D%20voxel%20data.%20We%20incorporate%20natural%20constraints%0Abetween%20structures%20into%20the%20end-to-end%20training%20and%20enrich%20the%20feature%0Arepresentation%20of%20the%20neural%20network.%20The%20effectiveness%20of%20the%20proposed%20method%0Ais%20validated%20on%20an%20open-source%20medical%20heart%20dataset%2C%20specifically%20using%20the%0AWHS%2B%2B%20data.%20The%20results%20demonstrate%20that%20the%20architecture%20performs%0Aexceptionally%20well%2C%20achieving%20a%20Dice%20coefficient%20of%200.939%20during%20testing.%20This%0Aindicates%20full%20topology%20preservation%20for%20individual%20structures%20and%0Asignificantly%20outperforms%20other%20baselines%20in%20preserving%20the%20overall%20scene%0Atopology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Cardiac%2520Integrity%253A%2520A%2520Topology-Infused%2520Approach%2520to%2520Whole%2520Heart%250A%2520%2520Segmentation%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Wenxue%2520Guan%2520and%2520Xiaodan%2520Xing%2520and%2520Guan%2520Yang%26entry.1292438233%3D%2520%2520Whole%2520heart%2520segmentation%2520%2528WHS%2529%2520supports%2520cardiovascular%2520disease%2520%2528CVD%2529%250Adiagnosis%252C%2520disease%2520monitoring%252C%2520treatment%2520planning%252C%2520and%2520prognosis.%2520Deep%2520learning%250Ahas%2520become%2520the%2520most%2520widely%2520used%2520method%2520for%2520WHS%2520applications%2520in%2520recent%2520years.%250AHowever%252C%2520segmentation%2520of%2520whole-heart%2520structures%2520faces%2520numerous%2520challenges%250Aincluding%2520heart%2520shape%2520variability%2520during%2520the%2520cardiac%2520cycle%252C%2520clinical%2520artifacts%250Alike%2520motion%2520and%2520poor%2520contrast-to-noise%2520ratio%252C%2520domain%2520shifts%2520in%2520multi-center%250Adata%252C%2520and%2520the%2520distinct%2520modalities%2520of%2520CT%2520and%2520MRI.%2520To%2520address%2520these%2520limitations%250Aand%2520improve%2520segmentation%2520quality%252C%2520this%2520paper%2520introduces%2520a%2520new%250Atopology-preserving%2520module%2520that%2520is%2520integrated%2520into%2520deep%2520neural%2520networks.%2520The%250Aimplementation%2520achieves%2520anatomically%2520plausible%2520segmentation%2520by%2520using%2520learned%250Atopology-preserving%2520fields%252C%2520which%2520are%2520based%2520entirely%2520on%25203D%2520convolution%2520and%2520are%250Atherefore%2520very%2520effective%2520for%25203D%2520voxel%2520data.%2520We%2520incorporate%2520natural%2520constraints%250Abetween%2520structures%2520into%2520the%2520end-to-end%2520training%2520and%2520enrich%2520the%2520feature%250Arepresentation%2520of%2520the%2520neural%2520network.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%250Ais%2520validated%2520on%2520an%2520open-source%2520medical%2520heart%2520dataset%252C%2520specifically%2520using%2520the%250AWHS%252B%252B%2520data.%2520The%2520results%2520demonstrate%2520that%2520the%2520architecture%2520performs%250Aexceptionally%2520well%252C%2520achieving%2520a%2520Dice%2520coefficient%2520of%25200.939%2520during%2520testing.%2520This%250Aindicates%2520full%2520topology%2520preservation%2520for%2520individual%2520structures%2520and%250Asignificantly%2520outperforms%2520other%2520baselines%2520in%2520preserving%2520the%2520overall%2520scene%250Atopology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation&entry.906535625=Chenyu%20Zhang%20and%20Wenxue%20Guan%20and%20Xiaodan%20Xing%20and%20Guan%20Yang&entry.1292438233=%20%20Whole%20heart%20segmentation%20%28WHS%29%20supports%20cardiovascular%20disease%20%28CVD%29%0Adiagnosis%2C%20disease%20monitoring%2C%20treatment%20planning%2C%20and%20prognosis.%20Deep%20learning%0Ahas%20become%20the%20most%20widely%20used%20method%20for%20WHS%20applications%20in%20recent%20years.%0AHowever%2C%20segmentation%20of%20whole-heart%20structures%20faces%20numerous%20challenges%0Aincluding%20heart%20shape%20variability%20during%20the%20cardiac%20cycle%2C%20clinical%20artifacts%0Alike%20motion%20and%20poor%20contrast-to-noise%20ratio%2C%20domain%20shifts%20in%20multi-center%0Adata%2C%20and%20the%20distinct%20modalities%20of%20CT%20and%20MRI.%20To%20address%20these%20limitations%0Aand%20improve%20segmentation%20quality%2C%20this%20paper%20introduces%20a%20new%0Atopology-preserving%20module%20that%20is%20integrated%20into%20deep%20neural%20networks.%20The%0Aimplementation%20achieves%20anatomically%20plausible%20segmentation%20by%20using%20learned%0Atopology-preserving%20fields%2C%20which%20are%20based%20entirely%20on%203D%20convolution%20and%20are%0Atherefore%20very%20effective%20for%203D%20voxel%20data.%20We%20incorporate%20natural%20constraints%0Abetween%20structures%20into%20the%20end-to-end%20training%20and%20enrich%20the%20feature%0Arepresentation%20of%20the%20neural%20network.%20The%20effectiveness%20of%20the%20proposed%20method%0Ais%20validated%20on%20an%20open-source%20medical%20heart%20dataset%2C%20specifically%20using%20the%0AWHS%2B%2B%20data.%20The%20results%20demonstrate%20that%20the%20architecture%20performs%0Aexceptionally%20well%2C%20achieving%20a%20Dice%20coefficient%20of%200.939%20during%20testing.%20This%0Aindicates%20full%20topology%20preservation%20for%20individual%20structures%20and%0Asignificantly%20outperforms%20other%20baselines%20in%20preserving%20the%20overall%20scene%0Atopology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10551v1&entry.124074799=Read"},
{"title": "MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and\n  Mapping With a Dynamic and Static Object Discriminator", "author": "Taozhe Li and Wei Sun", "abstract": "  The Visual Simultaneous Localization and Mapping (V-SLAM) system has seen\nsignificant development in recent years, demonstrating high precision in\nenvironments with limited dynamic objects. However, their performance\nsignificantly deteriorates when deployed in settings with a higher presence of\nmovable objects, such as environments with pedestrians, cars, and buses, which\nare common in outdoor scenes. To address this issue, we propose a Multilayer\nPerceptron (MLP)-based real-time stereo SLAM system that leverages complete\ngeometry information to avoid information loss. Moreover, there is currently no\npublicly available dataset for directly evaluating the effectiveness of dynamic\nand static feature classification methods, and to bridge this gap, we have\ncreated a publicly available dataset containing over 50,000 feature points.\nExperimental results demonstrate that our MLP-based dynamic and static feature\npoint discriminator has achieved superior performance compared to other methods\non this dataset. Furthermore, the MLP-based real-time stereo SLAM system has\nshown the highest average precision and fastest speed on the outdoor KITTI\ntracking datasets compared to other dynamic SLAM systems.The open-source code\nand datasets are available at https://github.com/TaozheLi/MLP-SLAM.\n", "link": "http://arxiv.org/abs/2410.10669v1", "date": "2024-10-14", "relevancy": 2.4233, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6003}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLP-SLAM%3A%20Multilayer%20Perceptron-Based%20Simultaneous%20Localization%20and%0A%20%20Mapping%20With%20a%20Dynamic%20and%20Static%20Object%20Discriminator&body=Title%3A%20MLP-SLAM%3A%20Multilayer%20Perceptron-Based%20Simultaneous%20Localization%20and%0A%20%20Mapping%20With%20a%20Dynamic%20and%20Static%20Object%20Discriminator%0AAuthor%3A%20Taozhe%20Li%20and%20Wei%20Sun%0AAbstract%3A%20%20%20The%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28V-SLAM%29%20system%20has%20seen%0Asignificant%20development%20in%20recent%20years%2C%20demonstrating%20high%20precision%20in%0Aenvironments%20with%20limited%20dynamic%20objects.%20However%2C%20their%20performance%0Asignificantly%20deteriorates%20when%20deployed%20in%20settings%20with%20a%20higher%20presence%20of%0Amovable%20objects%2C%20such%20as%20environments%20with%20pedestrians%2C%20cars%2C%20and%20buses%2C%20which%0Aare%20common%20in%20outdoor%20scenes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Multilayer%0APerceptron%20%28MLP%29-based%20real-time%20stereo%20SLAM%20system%20that%20leverages%20complete%0Ageometry%20information%20to%20avoid%20information%20loss.%20Moreover%2C%20there%20is%20currently%20no%0Apublicly%20available%20dataset%20for%20directly%20evaluating%20the%20effectiveness%20of%20dynamic%0Aand%20static%20feature%20classification%20methods%2C%20and%20to%20bridge%20this%20gap%2C%20we%20have%0Acreated%20a%20publicly%20available%20dataset%20containing%20over%2050%2C000%20feature%20points.%0AExperimental%20results%20demonstrate%20that%20our%20MLP-based%20dynamic%20and%20static%20feature%0Apoint%20discriminator%20has%20achieved%20superior%20performance%20compared%20to%20other%20methods%0Aon%20this%20dataset.%20Furthermore%2C%20the%20MLP-based%20real-time%20stereo%20SLAM%20system%20has%0Ashown%20the%20highest%20average%20precision%20and%20fastest%20speed%20on%20the%20outdoor%20KITTI%0Atracking%20datasets%20compared%20to%20other%20dynamic%20SLAM%20systems.The%20open-source%20code%0Aand%20datasets%20are%20available%20at%20https%3A//github.com/TaozheLi/MLP-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLP-SLAM%253A%2520Multilayer%2520Perceptron-Based%2520Simultaneous%2520Localization%2520and%250A%2520%2520Mapping%2520With%2520a%2520Dynamic%2520and%2520Static%2520Object%2520Discriminator%26entry.906535625%3DTaozhe%2520Li%2520and%2520Wei%2520Sun%26entry.1292438233%3D%2520%2520The%2520Visual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528V-SLAM%2529%2520system%2520has%2520seen%250Asignificant%2520development%2520in%2520recent%2520years%252C%2520demonstrating%2520high%2520precision%2520in%250Aenvironments%2520with%2520limited%2520dynamic%2520objects.%2520However%252C%2520their%2520performance%250Asignificantly%2520deteriorates%2520when%2520deployed%2520in%2520settings%2520with%2520a%2520higher%2520presence%2520of%250Amovable%2520objects%252C%2520such%2520as%2520environments%2520with%2520pedestrians%252C%2520cars%252C%2520and%2520buses%252C%2520which%250Aare%2520common%2520in%2520outdoor%2520scenes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Multilayer%250APerceptron%2520%2528MLP%2529-based%2520real-time%2520stereo%2520SLAM%2520system%2520that%2520leverages%2520complete%250Ageometry%2520information%2520to%2520avoid%2520information%2520loss.%2520Moreover%252C%2520there%2520is%2520currently%2520no%250Apublicly%2520available%2520dataset%2520for%2520directly%2520evaluating%2520the%2520effectiveness%2520of%2520dynamic%250Aand%2520static%2520feature%2520classification%2520methods%252C%2520and%2520to%2520bridge%2520this%2520gap%252C%2520we%2520have%250Acreated%2520a%2520publicly%2520available%2520dataset%2520containing%2520over%252050%252C000%2520feature%2520points.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520MLP-based%2520dynamic%2520and%2520static%2520feature%250Apoint%2520discriminator%2520has%2520achieved%2520superior%2520performance%2520compared%2520to%2520other%2520methods%250Aon%2520this%2520dataset.%2520Furthermore%252C%2520the%2520MLP-based%2520real-time%2520stereo%2520SLAM%2520system%2520has%250Ashown%2520the%2520highest%2520average%2520precision%2520and%2520fastest%2520speed%2520on%2520the%2520outdoor%2520KITTI%250Atracking%2520datasets%2520compared%2520to%2520other%2520dynamic%2520SLAM%2520systems.The%2520open-source%2520code%250Aand%2520datasets%2520are%2520available%2520at%2520https%253A//github.com/TaozheLi/MLP-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLP-SLAM%3A%20Multilayer%20Perceptron-Based%20Simultaneous%20Localization%20and%0A%20%20Mapping%20With%20a%20Dynamic%20and%20Static%20Object%20Discriminator&entry.906535625=Taozhe%20Li%20and%20Wei%20Sun&entry.1292438233=%20%20The%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28V-SLAM%29%20system%20has%20seen%0Asignificant%20development%20in%20recent%20years%2C%20demonstrating%20high%20precision%20in%0Aenvironments%20with%20limited%20dynamic%20objects.%20However%2C%20their%20performance%0Asignificantly%20deteriorates%20when%20deployed%20in%20settings%20with%20a%20higher%20presence%20of%0Amovable%20objects%2C%20such%20as%20environments%20with%20pedestrians%2C%20cars%2C%20and%20buses%2C%20which%0Aare%20common%20in%20outdoor%20scenes.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Multilayer%0APerceptron%20%28MLP%29-based%20real-time%20stereo%20SLAM%20system%20that%20leverages%20complete%0Ageometry%20information%20to%20avoid%20information%20loss.%20Moreover%2C%20there%20is%20currently%20no%0Apublicly%20available%20dataset%20for%20directly%20evaluating%20the%20effectiveness%20of%20dynamic%0Aand%20static%20feature%20classification%20methods%2C%20and%20to%20bridge%20this%20gap%2C%20we%20have%0Acreated%20a%20publicly%20available%20dataset%20containing%20over%2050%2C000%20feature%20points.%0AExperimental%20results%20demonstrate%20that%20our%20MLP-based%20dynamic%20and%20static%20feature%0Apoint%20discriminator%20has%20achieved%20superior%20performance%20compared%20to%20other%20methods%0Aon%20this%20dataset.%20Furthermore%2C%20the%20MLP-based%20real-time%20stereo%20SLAM%20system%20has%0Ashown%20the%20highest%20average%20precision%20and%20fastest%20speed%20on%20the%20outdoor%20KITTI%0Atracking%20datasets%20compared%20to%20other%20dynamic%20SLAM%20systems.The%20open-source%20code%0Aand%20datasets%20are%20available%20at%20https%3A//github.com/TaozheLi/MLP-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10669v1&entry.124074799=Read"},
{"title": "Localized Schr\u00f6dinger Bridge Sampler", "author": "Georg A. Gottwald and Sebastian Reich", "abstract": "  We consider the generative problem of sampling from an unknown distribution\nfor which only a sufficiently large number of training samples are available.\nIn this paper, we build on previous work combining Schr\\\"odinger bridges and\nLangevin dynamics. A key bottleneck of this approach is the exponential\ndependence of the required training samples on the dimension, $d$, of the\nambient state space. We propose a localization strategy which exploits\nconditional independence of conditional expectation values. Localization thus\nreplaces a single high-dimensional Schr\\\"odinger bridge problem by $d$\nlow-dimensional Schr\\\"odinger bridge problems over the available training\nsamples. In this context, a connection to multi-head self attention transformer\narchitectures is established. As for the original Schr\\\"odinger bridge sampling\napproach, the localized sampler is stable and geometric ergodic. The sampler\nalso naturally extends to conditional sampling and to Bayesian inference. We\ndemonstrate the performance of our proposed scheme through experiments on a\nGaussian problem with increasing dimensions, on a temporal stochastic process,\nand on a stochastic subgrid-scale parametrization conditional sampling problem.\n", "link": "http://arxiv.org/abs/2409.07968v2", "date": "2024-10-14", "relevancy": 2.4228, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4931}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4834}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localized%20Schr%C3%B6dinger%20Bridge%20Sampler&body=Title%3A%20Localized%20Schr%C3%B6dinger%20Bridge%20Sampler%0AAuthor%3A%20Georg%20A.%20Gottwald%20and%20Sebastian%20Reich%0AAbstract%3A%20%20%20We%20consider%20the%20generative%20problem%20of%20sampling%20from%20an%20unknown%20distribution%0Afor%20which%20only%20a%20sufficiently%20large%20number%20of%20training%20samples%20are%20available.%0AIn%20this%20paper%2C%20we%20build%20on%20previous%20work%20combining%20Schr%5C%22odinger%20bridges%20and%0ALangevin%20dynamics.%20A%20key%20bottleneck%20of%20this%20approach%20is%20the%20exponential%0Adependence%20of%20the%20required%20training%20samples%20on%20the%20dimension%2C%20%24d%24%2C%20of%20the%0Aambient%20state%20space.%20We%20propose%20a%20localization%20strategy%20which%20exploits%0Aconditional%20independence%20of%20conditional%20expectation%20values.%20Localization%20thus%0Areplaces%20a%20single%20high-dimensional%20Schr%5C%22odinger%20bridge%20problem%20by%20%24d%24%0Alow-dimensional%20Schr%5C%22odinger%20bridge%20problems%20over%20the%20available%20training%0Asamples.%20In%20this%20context%2C%20a%20connection%20to%20multi-head%20self%20attention%20transformer%0Aarchitectures%20is%20established.%20As%20for%20the%20original%20Schr%5C%22odinger%20bridge%20sampling%0Aapproach%2C%20the%20localized%20sampler%20is%20stable%20and%20geometric%20ergodic.%20The%20sampler%0Aalso%20naturally%20extends%20to%20conditional%20sampling%20and%20to%20Bayesian%20inference.%20We%0Ademonstrate%20the%20performance%20of%20our%20proposed%20scheme%20through%20experiments%20on%20a%0AGaussian%20problem%20with%20increasing%20dimensions%2C%20on%20a%20temporal%20stochastic%20process%2C%0Aand%20on%20a%20stochastic%20subgrid-scale%20parametrization%20conditional%20sampling%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalized%2520Schr%25C3%25B6dinger%2520Bridge%2520Sampler%26entry.906535625%3DGeorg%2520A.%2520Gottwald%2520and%2520Sebastian%2520Reich%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520generative%2520problem%2520of%2520sampling%2520from%2520an%2520unknown%2520distribution%250Afor%2520which%2520only%2520a%2520sufficiently%2520large%2520number%2520of%2520training%2520samples%2520are%2520available.%250AIn%2520this%2520paper%252C%2520we%2520build%2520on%2520previous%2520work%2520combining%2520Schr%255C%2522odinger%2520bridges%2520and%250ALangevin%2520dynamics.%2520A%2520key%2520bottleneck%2520of%2520this%2520approach%2520is%2520the%2520exponential%250Adependence%2520of%2520the%2520required%2520training%2520samples%2520on%2520the%2520dimension%252C%2520%2524d%2524%252C%2520of%2520the%250Aambient%2520state%2520space.%2520We%2520propose%2520a%2520localization%2520strategy%2520which%2520exploits%250Aconditional%2520independence%2520of%2520conditional%2520expectation%2520values.%2520Localization%2520thus%250Areplaces%2520a%2520single%2520high-dimensional%2520Schr%255C%2522odinger%2520bridge%2520problem%2520by%2520%2524d%2524%250Alow-dimensional%2520Schr%255C%2522odinger%2520bridge%2520problems%2520over%2520the%2520available%2520training%250Asamples.%2520In%2520this%2520context%252C%2520a%2520connection%2520to%2520multi-head%2520self%2520attention%2520transformer%250Aarchitectures%2520is%2520established.%2520As%2520for%2520the%2520original%2520Schr%255C%2522odinger%2520bridge%2520sampling%250Aapproach%252C%2520the%2520localized%2520sampler%2520is%2520stable%2520and%2520geometric%2520ergodic.%2520The%2520sampler%250Aalso%2520naturally%2520extends%2520to%2520conditional%2520sampling%2520and%2520to%2520Bayesian%2520inference.%2520We%250Ademonstrate%2520the%2520performance%2520of%2520our%2520proposed%2520scheme%2520through%2520experiments%2520on%2520a%250AGaussian%2520problem%2520with%2520increasing%2520dimensions%252C%2520on%2520a%2520temporal%2520stochastic%2520process%252C%250Aand%2520on%2520a%2520stochastic%2520subgrid-scale%2520parametrization%2520conditional%2520sampling%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Schr%C3%B6dinger%20Bridge%20Sampler&entry.906535625=Georg%20A.%20Gottwald%20and%20Sebastian%20Reich&entry.1292438233=%20%20We%20consider%20the%20generative%20problem%20of%20sampling%20from%20an%20unknown%20distribution%0Afor%20which%20only%20a%20sufficiently%20large%20number%20of%20training%20samples%20are%20available.%0AIn%20this%20paper%2C%20we%20build%20on%20previous%20work%20combining%20Schr%5C%22odinger%20bridges%20and%0ALangevin%20dynamics.%20A%20key%20bottleneck%20of%20this%20approach%20is%20the%20exponential%0Adependence%20of%20the%20required%20training%20samples%20on%20the%20dimension%2C%20%24d%24%2C%20of%20the%0Aambient%20state%20space.%20We%20propose%20a%20localization%20strategy%20which%20exploits%0Aconditional%20independence%20of%20conditional%20expectation%20values.%20Localization%20thus%0Areplaces%20a%20single%20high-dimensional%20Schr%5C%22odinger%20bridge%20problem%20by%20%24d%24%0Alow-dimensional%20Schr%5C%22odinger%20bridge%20problems%20over%20the%20available%20training%0Asamples.%20In%20this%20context%2C%20a%20connection%20to%20multi-head%20self%20attention%20transformer%0Aarchitectures%20is%20established.%20As%20for%20the%20original%20Schr%5C%22odinger%20bridge%20sampling%0Aapproach%2C%20the%20localized%20sampler%20is%20stable%20and%20geometric%20ergodic.%20The%20sampler%0Aalso%20naturally%20extends%20to%20conditional%20sampling%20and%20to%20Bayesian%20inference.%20We%0Ademonstrate%20the%20performance%20of%20our%20proposed%20scheme%20through%20experiments%20on%20a%0AGaussian%20problem%20with%20increasing%20dimensions%2C%20on%20a%20temporal%20stochastic%20process%2C%0Aand%20on%20a%20stochastic%20subgrid-scale%20parametrization%20conditional%20sampling%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07968v2&entry.124074799=Read"},
{"title": "SimpleStrat: Diversifying Language Model Generation with Stratification", "author": "Justin Wong and Yury Orlovskiy and Michael Luo and Sanjit A. Seshia and Joseph E. Gonzalez", "abstract": "  Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\nSimpleStrat, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.\n", "link": "http://arxiv.org/abs/2410.09038v2", "date": "2024-10-14", "relevancy": 2.4196, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleStrat%3A%20Diversifying%20Language%20Model%20Generation%20with%20Stratification&body=Title%3A%20SimpleStrat%3A%20Diversifying%20Language%20Model%20Generation%20with%20Stratification%0AAuthor%3A%20Justin%20Wong%20and%20Yury%20Orlovskiy%20and%20Michael%20Luo%20and%20Sanjit%20A.%20Seshia%20and%20Joseph%20E.%20Gonzalez%0AAbstract%3A%20%20%20Generating%20diverse%20responses%20from%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Aapplications%20such%20as%20planning/search%20and%20synthetic%20data%20generation%2C%20where%0Adiversity%20provides%20distinct%20answers%20across%20generations.%20Prior%20approaches%20rely%0Aon%20increasing%20temperature%20to%20increase%20diversity.%20However%2C%20contrary%20to%20popular%0Abelief%2C%20we%20show%20not%20only%20does%20this%20approach%20produce%20lower%20quality%20individual%0Agenerations%20as%20temperature%20increases%2C%20but%20it%20depends%20on%20model%27s%20next-token%0Aprobabilities%20being%20similar%20to%20the%20true%20distribution%20of%20answers.%20We%20propose%0ASimpleStrat%2C%20an%20alternative%20approach%20that%20uses%20the%20language%20model%20itself%20to%0Apartition%20the%20space%20into%20strata.%20At%20inference%2C%20a%20random%20stratum%20is%20selected%20and%0Aa%20sample%20drawn%20from%20within%20the%20strata.%20To%20measure%20diversity%2C%20we%20introduce%0ACoverageQA%2C%20a%20dataset%20of%20underspecified%20questions%20with%20multiple%20equally%0Aplausible%20answers%2C%20and%20assess%20diversity%20by%20measuring%20KL%20Divergence%20between%20the%0Aoutput%20distribution%20and%20uniform%20distribution%20over%20valid%20ground%20truth%20answers.%0AAs%20computing%20probability%20per%20response/solution%20for%20proprietary%20models%20is%0Ainfeasible%2C%20we%20measure%20recall%20on%20ground%20truth%20solutions.%20Our%20evaluation%20show%0Ausing%20SimpleStrat%20achieves%20higher%20recall%20by%200.05%20compared%20to%20GPT-4o%20and%200.36%0Aaverage%20reduction%20in%20KL%20Divergence%20compared%20to%20Llama%203.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleStrat%253A%2520Diversifying%2520Language%2520Model%2520Generation%2520with%2520Stratification%26entry.906535625%3DJustin%2520Wong%2520and%2520Yury%2520Orlovskiy%2520and%2520Michael%2520Luo%2520and%2520Sanjit%2520A.%2520Seshia%2520and%2520Joseph%2520E.%2520Gonzalez%26entry.1292438233%3D%2520%2520Generating%2520diverse%2520responses%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%2520for%250Aapplications%2520such%2520as%2520planning/search%2520and%2520synthetic%2520data%2520generation%252C%2520where%250Adiversity%2520provides%2520distinct%2520answers%2520across%2520generations.%2520Prior%2520approaches%2520rely%250Aon%2520increasing%2520temperature%2520to%2520increase%2520diversity.%2520However%252C%2520contrary%2520to%2520popular%250Abelief%252C%2520we%2520show%2520not%2520only%2520does%2520this%2520approach%2520produce%2520lower%2520quality%2520individual%250Agenerations%2520as%2520temperature%2520increases%252C%2520but%2520it%2520depends%2520on%2520model%2527s%2520next-token%250Aprobabilities%2520being%2520similar%2520to%2520the%2520true%2520distribution%2520of%2520answers.%2520We%2520propose%250ASimpleStrat%252C%2520an%2520alternative%2520approach%2520that%2520uses%2520the%2520language%2520model%2520itself%2520to%250Apartition%2520the%2520space%2520into%2520strata.%2520At%2520inference%252C%2520a%2520random%2520stratum%2520is%2520selected%2520and%250Aa%2520sample%2520drawn%2520from%2520within%2520the%2520strata.%2520To%2520measure%2520diversity%252C%2520we%2520introduce%250ACoverageQA%252C%2520a%2520dataset%2520of%2520underspecified%2520questions%2520with%2520multiple%2520equally%250Aplausible%2520answers%252C%2520and%2520assess%2520diversity%2520by%2520measuring%2520KL%2520Divergence%2520between%2520the%250Aoutput%2520distribution%2520and%2520uniform%2520distribution%2520over%2520valid%2520ground%2520truth%2520answers.%250AAs%2520computing%2520probability%2520per%2520response/solution%2520for%2520proprietary%2520models%2520is%250Ainfeasible%252C%2520we%2520measure%2520recall%2520on%2520ground%2520truth%2520solutions.%2520Our%2520evaluation%2520show%250Ausing%2520SimpleStrat%2520achieves%2520higher%2520recall%2520by%25200.05%2520compared%2520to%2520GPT-4o%2520and%25200.36%250Aaverage%2520reduction%2520in%2520KL%2520Divergence%2520compared%2520to%2520Llama%25203.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleStrat%3A%20Diversifying%20Language%20Model%20Generation%20with%20Stratification&entry.906535625=Justin%20Wong%20and%20Yury%20Orlovskiy%20and%20Michael%20Luo%20and%20Sanjit%20A.%20Seshia%20and%20Joseph%20E.%20Gonzalez&entry.1292438233=%20%20Generating%20diverse%20responses%20from%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Aapplications%20such%20as%20planning/search%20and%20synthetic%20data%20generation%2C%20where%0Adiversity%20provides%20distinct%20answers%20across%20generations.%20Prior%20approaches%20rely%0Aon%20increasing%20temperature%20to%20increase%20diversity.%20However%2C%20contrary%20to%20popular%0Abelief%2C%20we%20show%20not%20only%20does%20this%20approach%20produce%20lower%20quality%20individual%0Agenerations%20as%20temperature%20increases%2C%20but%20it%20depends%20on%20model%27s%20next-token%0Aprobabilities%20being%20similar%20to%20the%20true%20distribution%20of%20answers.%20We%20propose%0ASimpleStrat%2C%20an%20alternative%20approach%20that%20uses%20the%20language%20model%20itself%20to%0Apartition%20the%20space%20into%20strata.%20At%20inference%2C%20a%20random%20stratum%20is%20selected%20and%0Aa%20sample%20drawn%20from%20within%20the%20strata.%20To%20measure%20diversity%2C%20we%20introduce%0ACoverageQA%2C%20a%20dataset%20of%20underspecified%20questions%20with%20multiple%20equally%0Aplausible%20answers%2C%20and%20assess%20diversity%20by%20measuring%20KL%20Divergence%20between%20the%0Aoutput%20distribution%20and%20uniform%20distribution%20over%20valid%20ground%20truth%20answers.%0AAs%20computing%20probability%20per%20response/solution%20for%20proprietary%20models%20is%0Ainfeasible%2C%20we%20measure%20recall%20on%20ground%20truth%20solutions.%20Our%20evaluation%20show%0Ausing%20SimpleStrat%20achieves%20higher%20recall%20by%200.05%20compared%20to%20GPT-4o%20and%200.36%0Aaverage%20reduction%20in%20KL%20Divergence%20compared%20to%20Llama%203.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09038v2&entry.124074799=Read"},
{"title": "KBLaM: Knowledge Base augmented Language Model", "author": "Xi Wang and Liana Mikaelyan and Taketomo Isazawa and James Hensman", "abstract": "  In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a\nnew method for augmenting Large Language Models (LLMs) with external knowledge.\nKBLaM works with a knowledge base (KB) constructed from a corpus of documents,\ntransforming each piece of knowledge in the KB into continuous key-value vector\npairs via pre-trained sentence encoders with linear adapters and integrating\nthem into pre-trained LLMs via a specialized rectangular attention mechanism.\nUnlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval\nmodules, and unlike in-context learning, its computational overhead scales\nlinearly with KB size rather than quadratically. Our approach enables\nintegrating a large KB of more than 10K triples into an 8B pre-trained LLM of\nonly 8K context window on one single A100 80GB GPU and allows for dynamic\nupdates without model fine-tuning or retraining. Experiments demonstrate\nKBLaM's effectiveness in various tasks, including question-answering and\nopen-ended reasoning, while providing interpretable insights into its use of\nthe augmented knowledge.\n", "link": "http://arxiv.org/abs/2410.10450v1", "date": "2024-10-14", "relevancy": 2.4186, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KBLaM%3A%20Knowledge%20Base%20augmented%20Language%20Model&body=Title%3A%20KBLaM%3A%20Knowledge%20Base%20augmented%20Language%20Model%0AAuthor%3A%20Xi%20Wang%20and%20Liana%20Mikaelyan%20and%20Taketomo%20Isazawa%20and%20James%20Hensman%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Knowledge%20Base%20augmented%20Language%20Model%20%28KBLaM%29%2C%20a%0Anew%20method%20for%20augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20knowledge.%0AKBLaM%20works%20with%20a%20knowledge%20base%20%28KB%29%20constructed%20from%20a%20corpus%20of%20documents%2C%0Atransforming%20each%20piece%20of%20knowledge%20in%20the%20KB%20into%20continuous%20key-value%20vector%0Apairs%20via%20pre-trained%20sentence%20encoders%20with%20linear%20adapters%20and%20integrating%0Athem%20into%20pre-trained%20LLMs%20via%20a%20specialized%20rectangular%20attention%20mechanism.%0AUnlike%20Retrieval-Augmented%20Generation%2C%20KBLaM%20eliminates%20external%20retrieval%0Amodules%2C%20and%20unlike%20in-context%20learning%2C%20its%20computational%20overhead%20scales%0Alinearly%20with%20KB%20size%20rather%20than%20quadratically.%20Our%20approach%20enables%0Aintegrating%20a%20large%20KB%20of%20more%20than%2010K%20triples%20into%20an%208B%20pre-trained%20LLM%20of%0Aonly%208K%20context%20window%20on%20one%20single%20A100%2080GB%20GPU%20and%20allows%20for%20dynamic%0Aupdates%20without%20model%20fine-tuning%20or%20retraining.%20Experiments%20demonstrate%0AKBLaM%27s%20effectiveness%20in%20various%20tasks%2C%20including%20question-answering%20and%0Aopen-ended%20reasoning%2C%20while%20providing%20interpretable%20insights%20into%20its%20use%20of%0Athe%20augmented%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKBLaM%253A%2520Knowledge%2520Base%2520augmented%2520Language%2520Model%26entry.906535625%3DXi%2520Wang%2520and%2520Liana%2520Mikaelyan%2520and%2520Taketomo%2520Isazawa%2520and%2520James%2520Hensman%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Knowledge%2520Base%2520augmented%2520Language%2520Model%2520%2528KBLaM%2529%252C%2520a%250Anew%2520method%2520for%2520augmenting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520external%2520knowledge.%250AKBLaM%2520works%2520with%2520a%2520knowledge%2520base%2520%2528KB%2529%2520constructed%2520from%2520a%2520corpus%2520of%2520documents%252C%250Atransforming%2520each%2520piece%2520of%2520knowledge%2520in%2520the%2520KB%2520into%2520continuous%2520key-value%2520vector%250Apairs%2520via%2520pre-trained%2520sentence%2520encoders%2520with%2520linear%2520adapters%2520and%2520integrating%250Athem%2520into%2520pre-trained%2520LLMs%2520via%2520a%2520specialized%2520rectangular%2520attention%2520mechanism.%250AUnlike%2520Retrieval-Augmented%2520Generation%252C%2520KBLaM%2520eliminates%2520external%2520retrieval%250Amodules%252C%2520and%2520unlike%2520in-context%2520learning%252C%2520its%2520computational%2520overhead%2520scales%250Alinearly%2520with%2520KB%2520size%2520rather%2520than%2520quadratically.%2520Our%2520approach%2520enables%250Aintegrating%2520a%2520large%2520KB%2520of%2520more%2520than%252010K%2520triples%2520into%2520an%25208B%2520pre-trained%2520LLM%2520of%250Aonly%25208K%2520context%2520window%2520on%2520one%2520single%2520A100%252080GB%2520GPU%2520and%2520allows%2520for%2520dynamic%250Aupdates%2520without%2520model%2520fine-tuning%2520or%2520retraining.%2520Experiments%2520demonstrate%250AKBLaM%2527s%2520effectiveness%2520in%2520various%2520tasks%252C%2520including%2520question-answering%2520and%250Aopen-ended%2520reasoning%252C%2520while%2520providing%2520interpretable%2520insights%2520into%2520its%2520use%2520of%250Athe%2520augmented%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KBLaM%3A%20Knowledge%20Base%20augmented%20Language%20Model&entry.906535625=Xi%20Wang%20and%20Liana%20Mikaelyan%20and%20Taketomo%20Isazawa%20and%20James%20Hensman&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Knowledge%20Base%20augmented%20Language%20Model%20%28KBLaM%29%2C%20a%0Anew%20method%20for%20augmenting%20Large%20Language%20Models%20%28LLMs%29%20with%20external%20knowledge.%0AKBLaM%20works%20with%20a%20knowledge%20base%20%28KB%29%20constructed%20from%20a%20corpus%20of%20documents%2C%0Atransforming%20each%20piece%20of%20knowledge%20in%20the%20KB%20into%20continuous%20key-value%20vector%0Apairs%20via%20pre-trained%20sentence%20encoders%20with%20linear%20adapters%20and%20integrating%0Athem%20into%20pre-trained%20LLMs%20via%20a%20specialized%20rectangular%20attention%20mechanism.%0AUnlike%20Retrieval-Augmented%20Generation%2C%20KBLaM%20eliminates%20external%20retrieval%0Amodules%2C%20and%20unlike%20in-context%20learning%2C%20its%20computational%20overhead%20scales%0Alinearly%20with%20KB%20size%20rather%20than%20quadratically.%20Our%20approach%20enables%0Aintegrating%20a%20large%20KB%20of%20more%20than%2010K%20triples%20into%20an%208B%20pre-trained%20LLM%20of%0Aonly%208K%20context%20window%20on%20one%20single%20A100%2080GB%20GPU%20and%20allows%20for%20dynamic%0Aupdates%20without%20model%20fine-tuning%20or%20retraining.%20Experiments%20demonstrate%0AKBLaM%27s%20effectiveness%20in%20various%20tasks%2C%20including%20question-answering%20and%0Aopen-ended%20reasoning%2C%20while%20providing%20interpretable%20insights%20into%20its%20use%20of%0Athe%20augmented%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10450v1&entry.124074799=Read"},
{"title": "MoTE: Reconciling Generalization with Specialization for Visual-Language\n  to Video Knowledge Transfer", "author": "Minghao Zhu and Zhengpu Wang and Mengxian Hu and Ronghao Dang and Xiao Lin and Xun Zhou and Chengju Liu and Qijun Chen", "abstract": "  Transferring visual-language knowledge from large-scale foundation models for\nvideo recognition has proved to be effective. To bridge the domain gap,\nadditional parametric modules are added to capture the temporal information.\nHowever, zero-shot generalization diminishes with the increase in the number of\nspecialized parameters, making existing works a trade-off between zero-shot and\nclose-set performance. In this paper, we present MoTE, a novel framework that\nenables generalization and specialization to be balanced in one unified model.\nOur approach tunes a mixture of temporal experts to learn multiple task views\nwith various degrees of data fitting. To maximally preserve the knowledge of\neach expert, we propose \\emph{Weight Merging Regularization}, which regularizes\nthe merging process of experts in weight space. Additionally with temporal\nfeature modulation to regularize the contribution of temporal feature during\ntest. We achieve a sound balance between zero-shot and close-set video\nrecognition tasks and obtain state-of-the-art or competitive results on various\ndatasets, including Kinetics-400 \\& 600, UCF, and HMDB. Code is available at\n\\url{https://github.com/ZMHH-H/MoTE}.\n", "link": "http://arxiv.org/abs/2410.10589v1", "date": "2024-10-14", "relevancy": 2.4161, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6328}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoTE%3A%20Reconciling%20Generalization%20with%20Specialization%20for%20Visual-Language%0A%20%20to%20Video%20Knowledge%20Transfer&body=Title%3A%20MoTE%3A%20Reconciling%20Generalization%20with%20Specialization%20for%20Visual-Language%0A%20%20to%20Video%20Knowledge%20Transfer%0AAuthor%3A%20Minghao%20Zhu%20and%20Zhengpu%20Wang%20and%20Mengxian%20Hu%20and%20Ronghao%20Dang%20and%20Xiao%20Lin%20and%20Xun%20Zhou%20and%20Chengju%20Liu%20and%20Qijun%20Chen%0AAbstract%3A%20%20%20Transferring%20visual-language%20knowledge%20from%20large-scale%20foundation%20models%20for%0Avideo%20recognition%20has%20proved%20to%20be%20effective.%20To%20bridge%20the%20domain%20gap%2C%0Aadditional%20parametric%20modules%20are%20added%20to%20capture%20the%20temporal%20information.%0AHowever%2C%20zero-shot%20generalization%20diminishes%20with%20the%20increase%20in%20the%20number%20of%0Aspecialized%20parameters%2C%20making%20existing%20works%20a%20trade-off%20between%20zero-shot%20and%0Aclose-set%20performance.%20In%20this%20paper%2C%20we%20present%20MoTE%2C%20a%20novel%20framework%20that%0Aenables%20generalization%20and%20specialization%20to%20be%20balanced%20in%20one%20unified%20model.%0AOur%20approach%20tunes%20a%20mixture%20of%20temporal%20experts%20to%20learn%20multiple%20task%20views%0Awith%20various%20degrees%20of%20data%20fitting.%20To%20maximally%20preserve%20the%20knowledge%20of%0Aeach%20expert%2C%20we%20propose%20%5Cemph%7BWeight%20Merging%20Regularization%7D%2C%20which%20regularizes%0Athe%20merging%20process%20of%20experts%20in%20weight%20space.%20Additionally%20with%20temporal%0Afeature%20modulation%20to%20regularize%20the%20contribution%20of%20temporal%20feature%20during%0Atest.%20We%20achieve%20a%20sound%20balance%20between%20zero-shot%20and%20close-set%20video%0Arecognition%20tasks%20and%20obtain%20state-of-the-art%20or%20competitive%20results%20on%20various%0Adatasets%2C%20including%20Kinetics-400%20%5C%26%20600%2C%20UCF%2C%20and%20HMDB.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ZMHH-H/MoTE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoTE%253A%2520Reconciling%2520Generalization%2520with%2520Specialization%2520for%2520Visual-Language%250A%2520%2520to%2520Video%2520Knowledge%2520Transfer%26entry.906535625%3DMinghao%2520Zhu%2520and%2520Zhengpu%2520Wang%2520and%2520Mengxian%2520Hu%2520and%2520Ronghao%2520Dang%2520and%2520Xiao%2520Lin%2520and%2520Xun%2520Zhou%2520and%2520Chengju%2520Liu%2520and%2520Qijun%2520Chen%26entry.1292438233%3D%2520%2520Transferring%2520visual-language%2520knowledge%2520from%2520large-scale%2520foundation%2520models%2520for%250Avideo%2520recognition%2520has%2520proved%2520to%2520be%2520effective.%2520To%2520bridge%2520the%2520domain%2520gap%252C%250Aadditional%2520parametric%2520modules%2520are%2520added%2520to%2520capture%2520the%2520temporal%2520information.%250AHowever%252C%2520zero-shot%2520generalization%2520diminishes%2520with%2520the%2520increase%2520in%2520the%2520number%2520of%250Aspecialized%2520parameters%252C%2520making%2520existing%2520works%2520a%2520trade-off%2520between%2520zero-shot%2520and%250Aclose-set%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%2520MoTE%252C%2520a%2520novel%2520framework%2520that%250Aenables%2520generalization%2520and%2520specialization%2520to%2520be%2520balanced%2520in%2520one%2520unified%2520model.%250AOur%2520approach%2520tunes%2520a%2520mixture%2520of%2520temporal%2520experts%2520to%2520learn%2520multiple%2520task%2520views%250Awith%2520various%2520degrees%2520of%2520data%2520fitting.%2520To%2520maximally%2520preserve%2520the%2520knowledge%2520of%250Aeach%2520expert%252C%2520we%2520propose%2520%255Cemph%257BWeight%2520Merging%2520Regularization%257D%252C%2520which%2520regularizes%250Athe%2520merging%2520process%2520of%2520experts%2520in%2520weight%2520space.%2520Additionally%2520with%2520temporal%250Afeature%2520modulation%2520to%2520regularize%2520the%2520contribution%2520of%2520temporal%2520feature%2520during%250Atest.%2520We%2520achieve%2520a%2520sound%2520balance%2520between%2520zero-shot%2520and%2520close-set%2520video%250Arecognition%2520tasks%2520and%2520obtain%2520state-of-the-art%2520or%2520competitive%2520results%2520on%2520various%250Adatasets%252C%2520including%2520Kinetics-400%2520%255C%2526%2520600%252C%2520UCF%252C%2520and%2520HMDB.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/ZMHH-H/MoTE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoTE%3A%20Reconciling%20Generalization%20with%20Specialization%20for%20Visual-Language%0A%20%20to%20Video%20Knowledge%20Transfer&entry.906535625=Minghao%20Zhu%20and%20Zhengpu%20Wang%20and%20Mengxian%20Hu%20and%20Ronghao%20Dang%20and%20Xiao%20Lin%20and%20Xun%20Zhou%20and%20Chengju%20Liu%20and%20Qijun%20Chen&entry.1292438233=%20%20Transferring%20visual-language%20knowledge%20from%20large-scale%20foundation%20models%20for%0Avideo%20recognition%20has%20proved%20to%20be%20effective.%20To%20bridge%20the%20domain%20gap%2C%0Aadditional%20parametric%20modules%20are%20added%20to%20capture%20the%20temporal%20information.%0AHowever%2C%20zero-shot%20generalization%20diminishes%20with%20the%20increase%20in%20the%20number%20of%0Aspecialized%20parameters%2C%20making%20existing%20works%20a%20trade-off%20between%20zero-shot%20and%0Aclose-set%20performance.%20In%20this%20paper%2C%20we%20present%20MoTE%2C%20a%20novel%20framework%20that%0Aenables%20generalization%20and%20specialization%20to%20be%20balanced%20in%20one%20unified%20model.%0AOur%20approach%20tunes%20a%20mixture%20of%20temporal%20experts%20to%20learn%20multiple%20task%20views%0Awith%20various%20degrees%20of%20data%20fitting.%20To%20maximally%20preserve%20the%20knowledge%20of%0Aeach%20expert%2C%20we%20propose%20%5Cemph%7BWeight%20Merging%20Regularization%7D%2C%20which%20regularizes%0Athe%20merging%20process%20of%20experts%20in%20weight%20space.%20Additionally%20with%20temporal%0Afeature%20modulation%20to%20regularize%20the%20contribution%20of%20temporal%20feature%20during%0Atest.%20We%20achieve%20a%20sound%20balance%20between%20zero-shot%20and%20close-set%20video%0Arecognition%20tasks%20and%20obtain%20state-of-the-art%20or%20competitive%20results%20on%20various%0Adatasets%2C%20including%20Kinetics-400%20%5C%26%20600%2C%20UCF%2C%20and%20HMDB.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ZMHH-H/MoTE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10589v1&entry.124074799=Read"},
{"title": "Exploiting Local Features and Range Images for Small Data Real-Time\n  Point Cloud Semantic Segmentation", "author": "Daniel Fusaro and Simone Mosco and Emanuele Menegatti and Alberto Pretto", "abstract": "  Semantic segmentation of point clouds is an essential task for understanding\nthe environment in autonomous driving and robotics. Recent range-based works\nachieve real-time efficiency, while point- and voxel-based methods produce\nbetter results but are affected by high computational complexity. Moreover,\nhighly complex deep learning models are often not suited to efficiently learn\nfrom small datasets. Their generalization capabilities can easily be driven by\nthe abundance of data rather than the architecture design. In this paper, we\nharness the information from the three-dimensional representation to\nproficiently capture local features, while introducing the range image\nrepresentation to incorporate additional information and facilitate fast\ncomputation. A GPU-based KDTree allows for rapid building, querying, and\nenhancing projection with straightforward operations. Extensive experiments on\nSemanticKITTI and nuScenes datasets demonstrate the benefits of our\nmodification in a ``small data'' setup, in which only one sequence of the\ndataset is used to train the models, but also in the conventional setup, where\nall sequences except one are used for training. We show that a reduced version\nof our model not only demonstrates strong competitiveness against full-scale\nstate-of-the-art models but also operates in real-time, making it a viable\nchoice for real-world case applications. The code of our method is available at\nhttps://github.com/Bender97/WaffleAndRange.\n", "link": "http://arxiv.org/abs/2410.10510v1", "date": "2024-10-14", "relevancy": 2.4024, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6272}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Local%20Features%20and%20Range%20Images%20for%20Small%20Data%20Real-Time%0A%20%20Point%20Cloud%20Semantic%20Segmentation&body=Title%3A%20Exploiting%20Local%20Features%20and%20Range%20Images%20for%20Small%20Data%20Real-Time%0A%20%20Point%20Cloud%20Semantic%20Segmentation%0AAuthor%3A%20Daniel%20Fusaro%20and%20Simone%20Mosco%20and%20Emanuele%20Menegatti%20and%20Alberto%20Pretto%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20point%20clouds%20is%20an%20essential%20task%20for%20understanding%0Athe%20environment%20in%20autonomous%20driving%20and%20robotics.%20Recent%20range-based%20works%0Aachieve%20real-time%20efficiency%2C%20while%20point-%20and%20voxel-based%20methods%20produce%0Abetter%20results%20but%20are%20affected%20by%20high%20computational%20complexity.%20Moreover%2C%0Ahighly%20complex%20deep%20learning%20models%20are%20often%20not%20suited%20to%20efficiently%20learn%0Afrom%20small%20datasets.%20Their%20generalization%20capabilities%20can%20easily%20be%20driven%20by%0Athe%20abundance%20of%20data%20rather%20than%20the%20architecture%20design.%20In%20this%20paper%2C%20we%0Aharness%20the%20information%20from%20the%20three-dimensional%20representation%20to%0Aproficiently%20capture%20local%20features%2C%20while%20introducing%20the%20range%20image%0Arepresentation%20to%20incorporate%20additional%20information%20and%20facilitate%20fast%0Acomputation.%20A%20GPU-based%20KDTree%20allows%20for%20rapid%20building%2C%20querying%2C%20and%0Aenhancing%20projection%20with%20straightforward%20operations.%20Extensive%20experiments%20on%0ASemanticKITTI%20and%20nuScenes%20datasets%20demonstrate%20the%20benefits%20of%20our%0Amodification%20in%20a%20%60%60small%20data%27%27%20setup%2C%20in%20which%20only%20one%20sequence%20of%20the%0Adataset%20is%20used%20to%20train%20the%20models%2C%20but%20also%20in%20the%20conventional%20setup%2C%20where%0Aall%20sequences%20except%20one%20are%20used%20for%20training.%20We%20show%20that%20a%20reduced%20version%0Aof%20our%20model%20not%20only%20demonstrates%20strong%20competitiveness%20against%20full-scale%0Astate-of-the-art%20models%20but%20also%20operates%20in%20real-time%2C%20making%20it%20a%20viable%0Achoice%20for%20real-world%20case%20applications.%20The%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/Bender97/WaffleAndRange.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Local%2520Features%2520and%2520Range%2520Images%2520for%2520Small%2520Data%2520Real-Time%250A%2520%2520Point%2520Cloud%2520Semantic%2520Segmentation%26entry.906535625%3DDaniel%2520Fusaro%2520and%2520Simone%2520Mosco%2520and%2520Emanuele%2520Menegatti%2520and%2520Alberto%2520Pretto%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520point%2520clouds%2520is%2520an%2520essential%2520task%2520for%2520understanding%250Athe%2520environment%2520in%2520autonomous%2520driving%2520and%2520robotics.%2520Recent%2520range-based%2520works%250Aachieve%2520real-time%2520efficiency%252C%2520while%2520point-%2520and%2520voxel-based%2520methods%2520produce%250Abetter%2520results%2520but%2520are%2520affected%2520by%2520high%2520computational%2520complexity.%2520Moreover%252C%250Ahighly%2520complex%2520deep%2520learning%2520models%2520are%2520often%2520not%2520suited%2520to%2520efficiently%2520learn%250Afrom%2520small%2520datasets.%2520Their%2520generalization%2520capabilities%2520can%2520easily%2520be%2520driven%2520by%250Athe%2520abundance%2520of%2520data%2520rather%2520than%2520the%2520architecture%2520design.%2520In%2520this%2520paper%252C%2520we%250Aharness%2520the%2520information%2520from%2520the%2520three-dimensional%2520representation%2520to%250Aproficiently%2520capture%2520local%2520features%252C%2520while%2520introducing%2520the%2520range%2520image%250Arepresentation%2520to%2520incorporate%2520additional%2520information%2520and%2520facilitate%2520fast%250Acomputation.%2520A%2520GPU-based%2520KDTree%2520allows%2520for%2520rapid%2520building%252C%2520querying%252C%2520and%250Aenhancing%2520projection%2520with%2520straightforward%2520operations.%2520Extensive%2520experiments%2520on%250ASemanticKITTI%2520and%2520nuScenes%2520datasets%2520demonstrate%2520the%2520benefits%2520of%2520our%250Amodification%2520in%2520a%2520%2560%2560small%2520data%2527%2527%2520setup%252C%2520in%2520which%2520only%2520one%2520sequence%2520of%2520the%250Adataset%2520is%2520used%2520to%2520train%2520the%2520models%252C%2520but%2520also%2520in%2520the%2520conventional%2520setup%252C%2520where%250Aall%2520sequences%2520except%2520one%2520are%2520used%2520for%2520training.%2520We%2520show%2520that%2520a%2520reduced%2520version%250Aof%2520our%2520model%2520not%2520only%2520demonstrates%2520strong%2520competitiveness%2520against%2520full-scale%250Astate-of-the-art%2520models%2520but%2520also%2520operates%2520in%2520real-time%252C%2520making%2520it%2520a%2520viable%250Achoice%2520for%2520real-world%2520case%2520applications.%2520The%2520code%2520of%2520our%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/Bender97/WaffleAndRange.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Local%20Features%20and%20Range%20Images%20for%20Small%20Data%20Real-Time%0A%20%20Point%20Cloud%20Semantic%20Segmentation&entry.906535625=Daniel%20Fusaro%20and%20Simone%20Mosco%20and%20Emanuele%20Menegatti%20and%20Alberto%20Pretto&entry.1292438233=%20%20Semantic%20segmentation%20of%20point%20clouds%20is%20an%20essential%20task%20for%20understanding%0Athe%20environment%20in%20autonomous%20driving%20and%20robotics.%20Recent%20range-based%20works%0Aachieve%20real-time%20efficiency%2C%20while%20point-%20and%20voxel-based%20methods%20produce%0Abetter%20results%20but%20are%20affected%20by%20high%20computational%20complexity.%20Moreover%2C%0Ahighly%20complex%20deep%20learning%20models%20are%20often%20not%20suited%20to%20efficiently%20learn%0Afrom%20small%20datasets.%20Their%20generalization%20capabilities%20can%20easily%20be%20driven%20by%0Athe%20abundance%20of%20data%20rather%20than%20the%20architecture%20design.%20In%20this%20paper%2C%20we%0Aharness%20the%20information%20from%20the%20three-dimensional%20representation%20to%0Aproficiently%20capture%20local%20features%2C%20while%20introducing%20the%20range%20image%0Arepresentation%20to%20incorporate%20additional%20information%20and%20facilitate%20fast%0Acomputation.%20A%20GPU-based%20KDTree%20allows%20for%20rapid%20building%2C%20querying%2C%20and%0Aenhancing%20projection%20with%20straightforward%20operations.%20Extensive%20experiments%20on%0ASemanticKITTI%20and%20nuScenes%20datasets%20demonstrate%20the%20benefits%20of%20our%0Amodification%20in%20a%20%60%60small%20data%27%27%20setup%2C%20in%20which%20only%20one%20sequence%20of%20the%0Adataset%20is%20used%20to%20train%20the%20models%2C%20but%20also%20in%20the%20conventional%20setup%2C%20where%0Aall%20sequences%20except%20one%20are%20used%20for%20training.%20We%20show%20that%20a%20reduced%20version%0Aof%20our%20model%20not%20only%20demonstrates%20strong%20competitiveness%20against%20full-scale%0Astate-of-the-art%20models%20but%20also%20operates%20in%20real-time%2C%20making%20it%20a%20viable%0Achoice%20for%20real-world%20case%20applications.%20The%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/Bender97/WaffleAndRange.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10510v1&entry.124074799=Read"},
{"title": "TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment\n  with Diffusion Model", "author": "Jiazhi Guan and Quanwei Yang and Kaisiyuan Wang and Hang Zhou and Shengyi He and Zhiliang Xu and Haocheng Feng and Errui Ding and Jingdong Wang and Hongtao Xie and Youjian Zhao and Ziwei Liu", "abstract": "  Recently, 2D speaking avatars have increasingly participated in everyday\nscenarios due to the fast development of facial animation techniques. However,\nmost existing works neglect the explicit control of human bodies. In this\npaper, we propose to drive not only the faces but also the torso and gesture\nmovements of a speaking figure. Inspired by recent advances in diffusion\nmodels, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing\nAvatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar\nreenactment from only short footage of monocular video. Our key idea is to\nenhance the textural awareness with explicit motion guidance in diffusion\nmodeling. Specifically, we carefully construct 2D and 3D structural information\nas intermediate guidance. While recent diffusion models adopt a side network\nfor control information injection, they fail to synthesize temporally stable\nresults even with person-specific fine-tuning. We propose a Motion-Enhanced\nTextural Alignment module to enhance the bond between driving and target\nsignals. Moreover, we build a Memory-based Hand-Recovering module to help with\nthe difficulties in hand-shape preserving. After pre-training, our model can\nachieve high-fidelity 2D avatar reenactment with only 30 seconds of\nperson-specific data. Extensive experiments demonstrate the effectiveness and\nsuperiority of our proposed framework. Resources can be found at\nhttps://guanjz20.github.io/projects/TALK-Act.\n", "link": "http://arxiv.org/abs/2410.10696v1", "date": "2024-10-14", "relevancy": 2.4017, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5923}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TALK-Act%3A%20Enhance%20Textural-Awareness%20for%202D%20Speaking%20Avatar%20Reenactment%0A%20%20with%20Diffusion%20Model&body=Title%3A%20TALK-Act%3A%20Enhance%20Textural-Awareness%20for%202D%20Speaking%20Avatar%20Reenactment%0A%20%20with%20Diffusion%20Model%0AAuthor%3A%20Jiazhi%20Guan%20and%20Quanwei%20Yang%20and%20Kaisiyuan%20Wang%20and%20Hang%20Zhou%20and%20Shengyi%20He%20and%20Zhiliang%20Xu%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Hongtao%20Xie%20and%20Youjian%20Zhao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recently%2C%202D%20speaking%20avatars%20have%20increasingly%20participated%20in%20everyday%0Ascenarios%20due%20to%20the%20fast%20development%20of%20facial%20animation%20techniques.%20However%2C%0Amost%20existing%20works%20neglect%20the%20explicit%20control%20of%20human%20bodies.%20In%20this%0Apaper%2C%20we%20propose%20to%20drive%20not%20only%20the%20faces%20but%20also%20the%20torso%20and%20gesture%0Amovements%20of%20a%20speaking%20figure.%20Inspired%20by%20recent%20advances%20in%20diffusion%0Amodels%2C%20we%20propose%20the%20Motion-Enhanced%20Textural-Aware%20ModeLing%20for%20SpeaKing%0AAvatar%20Reenactment%20%28TALK-Act%29%20framework%2C%20which%20enables%20high-fidelity%20avatar%0Areenactment%20from%20only%20short%20footage%20of%20monocular%20video.%20Our%20key%20idea%20is%20to%0Aenhance%20the%20textural%20awareness%20with%20explicit%20motion%20guidance%20in%20diffusion%0Amodeling.%20Specifically%2C%20we%20carefully%20construct%202D%20and%203D%20structural%20information%0Aas%20intermediate%20guidance.%20While%20recent%20diffusion%20models%20adopt%20a%20side%20network%0Afor%20control%20information%20injection%2C%20they%20fail%20to%20synthesize%20temporally%20stable%0Aresults%20even%20with%20person-specific%20fine-tuning.%20We%20propose%20a%20Motion-Enhanced%0ATextural%20Alignment%20module%20to%20enhance%20the%20bond%20between%20driving%20and%20target%0Asignals.%20Moreover%2C%20we%20build%20a%20Memory-based%20Hand-Recovering%20module%20to%20help%20with%0Athe%20difficulties%20in%20hand-shape%20preserving.%20After%20pre-training%2C%20our%20model%20can%0Aachieve%20high-fidelity%202D%20avatar%20reenactment%20with%20only%2030%20seconds%20of%0Aperson-specific%20data.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20proposed%20framework.%20Resources%20can%20be%20found%20at%0Ahttps%3A//guanjz20.github.io/projects/TALK-Act.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTALK-Act%253A%2520Enhance%2520Textural-Awareness%2520for%25202D%2520Speaking%2520Avatar%2520Reenactment%250A%2520%2520with%2520Diffusion%2520Model%26entry.906535625%3DJiazhi%2520Guan%2520and%2520Quanwei%2520Yang%2520and%2520Kaisiyuan%2520Wang%2520and%2520Hang%2520Zhou%2520and%2520Shengyi%2520He%2520and%2520Zhiliang%2520Xu%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%2520and%2520Hongtao%2520Xie%2520and%2520Youjian%2520Zhao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%25202D%2520speaking%2520avatars%2520have%2520increasingly%2520participated%2520in%2520everyday%250Ascenarios%2520due%2520to%2520the%2520fast%2520development%2520of%2520facial%2520animation%2520techniques.%2520However%252C%250Amost%2520existing%2520works%2520neglect%2520the%2520explicit%2520control%2520of%2520human%2520bodies.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520to%2520drive%2520not%2520only%2520the%2520faces%2520but%2520also%2520the%2520torso%2520and%2520gesture%250Amovements%2520of%2520a%2520speaking%2520figure.%2520Inspired%2520by%2520recent%2520advances%2520in%2520diffusion%250Amodels%252C%2520we%2520propose%2520the%2520Motion-Enhanced%2520Textural-Aware%2520ModeLing%2520for%2520SpeaKing%250AAvatar%2520Reenactment%2520%2528TALK-Act%2529%2520framework%252C%2520which%2520enables%2520high-fidelity%2520avatar%250Areenactment%2520from%2520only%2520short%2520footage%2520of%2520monocular%2520video.%2520Our%2520key%2520idea%2520is%2520to%250Aenhance%2520the%2520textural%2520awareness%2520with%2520explicit%2520motion%2520guidance%2520in%2520diffusion%250Amodeling.%2520Specifically%252C%2520we%2520carefully%2520construct%25202D%2520and%25203D%2520structural%2520information%250Aas%2520intermediate%2520guidance.%2520While%2520recent%2520diffusion%2520models%2520adopt%2520a%2520side%2520network%250Afor%2520control%2520information%2520injection%252C%2520they%2520fail%2520to%2520synthesize%2520temporally%2520stable%250Aresults%2520even%2520with%2520person-specific%2520fine-tuning.%2520We%2520propose%2520a%2520Motion-Enhanced%250ATextural%2520Alignment%2520module%2520to%2520enhance%2520the%2520bond%2520between%2520driving%2520and%2520target%250Asignals.%2520Moreover%252C%2520we%2520build%2520a%2520Memory-based%2520Hand-Recovering%2520module%2520to%2520help%2520with%250Athe%2520difficulties%2520in%2520hand-shape%2520preserving.%2520After%2520pre-training%252C%2520our%2520model%2520can%250Aachieve%2520high-fidelity%25202D%2520avatar%2520reenactment%2520with%2520only%252030%2520seconds%2520of%250Aperson-specific%2520data.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%250Asuperiority%2520of%2520our%2520proposed%2520framework.%2520Resources%2520can%2520be%2520found%2520at%250Ahttps%253A//guanjz20.github.io/projects/TALK-Act.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TALK-Act%3A%20Enhance%20Textural-Awareness%20for%202D%20Speaking%20Avatar%20Reenactment%0A%20%20with%20Diffusion%20Model&entry.906535625=Jiazhi%20Guan%20and%20Quanwei%20Yang%20and%20Kaisiyuan%20Wang%20and%20Hang%20Zhou%20and%20Shengyi%20He%20and%20Zhiliang%20Xu%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Hongtao%20Xie%20and%20Youjian%20Zhao%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recently%2C%202D%20speaking%20avatars%20have%20increasingly%20participated%20in%20everyday%0Ascenarios%20due%20to%20the%20fast%20development%20of%20facial%20animation%20techniques.%20However%2C%0Amost%20existing%20works%20neglect%20the%20explicit%20control%20of%20human%20bodies.%20In%20this%0Apaper%2C%20we%20propose%20to%20drive%20not%20only%20the%20faces%20but%20also%20the%20torso%20and%20gesture%0Amovements%20of%20a%20speaking%20figure.%20Inspired%20by%20recent%20advances%20in%20diffusion%0Amodels%2C%20we%20propose%20the%20Motion-Enhanced%20Textural-Aware%20ModeLing%20for%20SpeaKing%0AAvatar%20Reenactment%20%28TALK-Act%29%20framework%2C%20which%20enables%20high-fidelity%20avatar%0Areenactment%20from%20only%20short%20footage%20of%20monocular%20video.%20Our%20key%20idea%20is%20to%0Aenhance%20the%20textural%20awareness%20with%20explicit%20motion%20guidance%20in%20diffusion%0Amodeling.%20Specifically%2C%20we%20carefully%20construct%202D%20and%203D%20structural%20information%0Aas%20intermediate%20guidance.%20While%20recent%20diffusion%20models%20adopt%20a%20side%20network%0Afor%20control%20information%20injection%2C%20they%20fail%20to%20synthesize%20temporally%20stable%0Aresults%20even%20with%20person-specific%20fine-tuning.%20We%20propose%20a%20Motion-Enhanced%0ATextural%20Alignment%20module%20to%20enhance%20the%20bond%20between%20driving%20and%20target%0Asignals.%20Moreover%2C%20we%20build%20a%20Memory-based%20Hand-Recovering%20module%20to%20help%20with%0Athe%20difficulties%20in%20hand-shape%20preserving.%20After%20pre-training%2C%20our%20model%20can%0Aachieve%20high-fidelity%202D%20avatar%20reenactment%20with%20only%2030%20seconds%20of%0Aperson-specific%20data.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%0Asuperiority%20of%20our%20proposed%20framework.%20Resources%20can%20be%20found%20at%0Ahttps%3A//guanjz20.github.io/projects/TALK-Act.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10696v1&entry.124074799=Read"},
{"title": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning", "author": "Kuofeng Gao and Huanqia Cai and Qingyao Shuai and Dihong Gong and Zhifeng Li", "abstract": "  Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.\n", "link": "http://arxiv.org/abs/2410.10735v1", "date": "2024-10-14", "relevancy": 2.4004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning&body=Title%3A%20Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning%0AAuthor%3A%20Kuofeng%20Gao%20and%20Huanqia%20Cai%20and%20Qingyao%20Shuai%20and%20Dihong%20Gong%20and%20Zhifeng%20Li%0AAbstract%3A%20%20%20Accurate%20mathematical%20reasoning%20with%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%0Ain%20revolutionizing%20domains%20that%20heavily%20rely%20on%20such%20reasoning.%20However%2C%20LLMs%0Aoften%20encounter%20difficulties%20in%20certain%20aspects%20of%20mathematical%20reasoning%2C%0Aleading%20to%20flawed%20reasoning%20and%20erroneous%20results.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20mechanism%2C%20the%20Chain%20of%20Self-Correction%20%28CoSC%29%2C%20specifically%0Adesigned%20to%20embed%20self-correction%20as%20an%20inherent%20ability%20in%20LLMs%2C%20enabling%20them%0Ato%20validate%20and%20rectify%20their%20own%20results.%20The%20CoSC%20mechanism%20operates%20through%0Aa%20sequence%20of%20self-correction%20stages.%20In%20each%20stage%2C%20the%20LLMs%20generate%20a%0Aprogram%20to%20address%20a%20given%20problem%2C%20execute%20this%20program%20using%20program-based%0Atools%20to%20obtain%20an%20output%2C%20subsequently%20verify%20this%20output.%20Based%20on%20the%0Averification%2C%20the%20LLMs%20either%20proceed%20to%20the%20next%20correction%20stage%20or%20finalize%0Athe%20answer.%20This%20iterative%20self-correction%20process%20allows%20the%20LLMs%20to%20refine%0Atheir%20reasoning%20steps%20and%20improve%20the%20accuracy%20of%20their%20mathematical%20reasoning.%0ATo%20enable%20the%20CoSC%20mechanism%20at%20a%20low%20cost%2C%20we%20employ%20a%20two-phase%20finetuning%0Aapproach.%20In%20the%20first%20phase%2C%20the%20LLMs%20are%20trained%20with%20a%20relatively%20small%0Avolume%20of%20seeding%20data%20generated%20from%20GPT-4%2C%20establishing%20an%20initial%20CoSC%0Acapability.%20In%20the%20second%20phase%2C%20the%20CoSC%20capability%20is%20further%20enhanced%20by%0Atraining%20with%20a%20larger%20volume%20of%20self-generated%20data%20using%20the%20trained%20model%20in%0Athe%20first%20phase%2C%20without%20relying%20on%20the%20paid%20GPT-4.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20CoSC%20significantly%20improves%20performance%20on%0Atraditional%20mathematical%20datasets%20among%20existing%20open-source%20LLMs.%20Notably%2C%20our%0ACoSC-Code-34B%20model%20achieved%20a%2053.5%25%20score%20on%20MATH%2C%20the%20most%20challenging%0Amathematical%20reasoning%20dataset%20in%20the%20public%20domain%2C%20surpassing%20the%20performance%0Aof%20well-established%20models%20such%20as%20ChatGPT%2C%20GPT-4%2C%20and%20even%20multi-modal%20LLMs%0Alike%20GPT-4V%2C%20Gemini-1.0%20Pro%2C%20and%20Gemini-1.0%20Ultra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Self-Correction%2520as%2520an%2520Inherent%2520Ability%2520in%2520Large%2520Language%250A%2520%2520Models%2520for%2520Enhanced%2520Mathematical%2520Reasoning%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Huanqia%2520Cai%2520and%2520Qingyao%2520Shuai%2520and%2520Dihong%2520Gong%2520and%2520Zhifeng%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520mathematical%2520reasoning%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520crucial%250Ain%2520revolutionizing%2520domains%2520that%2520heavily%2520rely%2520on%2520such%2520reasoning.%2520However%252C%2520LLMs%250Aoften%2520encounter%2520difficulties%2520in%2520certain%2520aspects%2520of%2520mathematical%2520reasoning%252C%250Aleading%2520to%2520flawed%2520reasoning%2520and%2520erroneous%2520results.%2520To%2520mitigate%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520novel%2520mechanism%252C%2520the%2520Chain%2520of%2520Self-Correction%2520%2528CoSC%2529%252C%2520specifically%250Adesigned%2520to%2520embed%2520self-correction%2520as%2520an%2520inherent%2520ability%2520in%2520LLMs%252C%2520enabling%2520them%250Ato%2520validate%2520and%2520rectify%2520their%2520own%2520results.%2520The%2520CoSC%2520mechanism%2520operates%2520through%250Aa%2520sequence%2520of%2520self-correction%2520stages.%2520In%2520each%2520stage%252C%2520the%2520LLMs%2520generate%2520a%250Aprogram%2520to%2520address%2520a%2520given%2520problem%252C%2520execute%2520this%2520program%2520using%2520program-based%250Atools%2520to%2520obtain%2520an%2520output%252C%2520subsequently%2520verify%2520this%2520output.%2520Based%2520on%2520the%250Averification%252C%2520the%2520LLMs%2520either%2520proceed%2520to%2520the%2520next%2520correction%2520stage%2520or%2520finalize%250Athe%2520answer.%2520This%2520iterative%2520self-correction%2520process%2520allows%2520the%2520LLMs%2520to%2520refine%250Atheir%2520reasoning%2520steps%2520and%2520improve%2520the%2520accuracy%2520of%2520their%2520mathematical%2520reasoning.%250ATo%2520enable%2520the%2520CoSC%2520mechanism%2520at%2520a%2520low%2520cost%252C%2520we%2520employ%2520a%2520two-phase%2520finetuning%250Aapproach.%2520In%2520the%2520first%2520phase%252C%2520the%2520LLMs%2520are%2520trained%2520with%2520a%2520relatively%2520small%250Avolume%2520of%2520seeding%2520data%2520generated%2520from%2520GPT-4%252C%2520establishing%2520an%2520initial%2520CoSC%250Acapability.%2520In%2520the%2520second%2520phase%252C%2520the%2520CoSC%2520capability%2520is%2520further%2520enhanced%2520by%250Atraining%2520with%2520a%2520larger%2520volume%2520of%2520self-generated%2520data%2520using%2520the%2520trained%2520model%2520in%250Athe%2520first%2520phase%252C%2520without%2520relying%2520on%2520the%2520paid%2520GPT-4.%2520Our%2520comprehensive%250Aexperiments%2520demonstrate%2520that%2520CoSC%2520significantly%2520improves%2520performance%2520on%250Atraditional%2520mathematical%2520datasets%2520among%2520existing%2520open-source%2520LLMs.%2520Notably%252C%2520our%250ACoSC-Code-34B%2520model%2520achieved%2520a%252053.5%2525%2520score%2520on%2520MATH%252C%2520the%2520most%2520challenging%250Amathematical%2520reasoning%2520dataset%2520in%2520the%2520public%2520domain%252C%2520surpassing%2520the%2520performance%250Aof%2520well-established%2520models%2520such%2520as%2520ChatGPT%252C%2520GPT-4%252C%2520and%2520even%2520multi-modal%2520LLMs%250Alike%2520GPT-4V%252C%2520Gemini-1.0%2520Pro%252C%2520and%2520Gemini-1.0%2520Ultra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning&entry.906535625=Kuofeng%20Gao%20and%20Huanqia%20Cai%20and%20Qingyao%20Shuai%20and%20Dihong%20Gong%20and%20Zhifeng%20Li&entry.1292438233=%20%20Accurate%20mathematical%20reasoning%20with%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%0Ain%20revolutionizing%20domains%20that%20heavily%20rely%20on%20such%20reasoning.%20However%2C%20LLMs%0Aoften%20encounter%20difficulties%20in%20certain%20aspects%20of%20mathematical%20reasoning%2C%0Aleading%20to%20flawed%20reasoning%20and%20erroneous%20results.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20mechanism%2C%20the%20Chain%20of%20Self-Correction%20%28CoSC%29%2C%20specifically%0Adesigned%20to%20embed%20self-correction%20as%20an%20inherent%20ability%20in%20LLMs%2C%20enabling%20them%0Ato%20validate%20and%20rectify%20their%20own%20results.%20The%20CoSC%20mechanism%20operates%20through%0Aa%20sequence%20of%20self-correction%20stages.%20In%20each%20stage%2C%20the%20LLMs%20generate%20a%0Aprogram%20to%20address%20a%20given%20problem%2C%20execute%20this%20program%20using%20program-based%0Atools%20to%20obtain%20an%20output%2C%20subsequently%20verify%20this%20output.%20Based%20on%20the%0Averification%2C%20the%20LLMs%20either%20proceed%20to%20the%20next%20correction%20stage%20or%20finalize%0Athe%20answer.%20This%20iterative%20self-correction%20process%20allows%20the%20LLMs%20to%20refine%0Atheir%20reasoning%20steps%20and%20improve%20the%20accuracy%20of%20their%20mathematical%20reasoning.%0ATo%20enable%20the%20CoSC%20mechanism%20at%20a%20low%20cost%2C%20we%20employ%20a%20two-phase%20finetuning%0Aapproach.%20In%20the%20first%20phase%2C%20the%20LLMs%20are%20trained%20with%20a%20relatively%20small%0Avolume%20of%20seeding%20data%20generated%20from%20GPT-4%2C%20establishing%20an%20initial%20CoSC%0Acapability.%20In%20the%20second%20phase%2C%20the%20CoSC%20capability%20is%20further%20enhanced%20by%0Atraining%20with%20a%20larger%20volume%20of%20self-generated%20data%20using%20the%20trained%20model%20in%0Athe%20first%20phase%2C%20without%20relying%20on%20the%20paid%20GPT-4.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20CoSC%20significantly%20improves%20performance%20on%0Atraditional%20mathematical%20datasets%20among%20existing%20open-source%20LLMs.%20Notably%2C%20our%0ACoSC-Code-34B%20model%20achieved%20a%2053.5%25%20score%20on%20MATH%2C%20the%20most%20challenging%0Amathematical%20reasoning%20dataset%20in%20the%20public%20domain%2C%20surpassing%20the%20performance%0Aof%20well-established%20models%20such%20as%20ChatGPT%2C%20GPT-4%2C%20and%20even%20multi-modal%20LLMs%0Alike%20GPT-4V%2C%20Gemini-1.0%20Pro%2C%20and%20Gemini-1.0%20Ultra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10735v1&entry.124074799=Read"},
{"title": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning", "author": "Kuofeng Gao and Huanqia Cai and Qingyao Shuai and Dihong Gong and Zhifeng Li", "abstract": "  Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.\n", "link": "http://arxiv.org/abs/2410.10735v1", "date": "2024-10-14", "relevancy": 2.4004, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning&body=Title%3A%20Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning%0AAuthor%3A%20Kuofeng%20Gao%20and%20Huanqia%20Cai%20and%20Qingyao%20Shuai%20and%20Dihong%20Gong%20and%20Zhifeng%20Li%0AAbstract%3A%20%20%20Accurate%20mathematical%20reasoning%20with%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%0Ain%20revolutionizing%20domains%20that%20heavily%20rely%20on%20such%20reasoning.%20However%2C%20LLMs%0Aoften%20encounter%20difficulties%20in%20certain%20aspects%20of%20mathematical%20reasoning%2C%0Aleading%20to%20flawed%20reasoning%20and%20erroneous%20results.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20mechanism%2C%20the%20Chain%20of%20Self-Correction%20%28CoSC%29%2C%20specifically%0Adesigned%20to%20embed%20self-correction%20as%20an%20inherent%20ability%20in%20LLMs%2C%20enabling%20them%0Ato%20validate%20and%20rectify%20their%20own%20results.%20The%20CoSC%20mechanism%20operates%20through%0Aa%20sequence%20of%20self-correction%20stages.%20In%20each%20stage%2C%20the%20LLMs%20generate%20a%0Aprogram%20to%20address%20a%20given%20problem%2C%20execute%20this%20program%20using%20program-based%0Atools%20to%20obtain%20an%20output%2C%20subsequently%20verify%20this%20output.%20Based%20on%20the%0Averification%2C%20the%20LLMs%20either%20proceed%20to%20the%20next%20correction%20stage%20or%20finalize%0Athe%20answer.%20This%20iterative%20self-correction%20process%20allows%20the%20LLMs%20to%20refine%0Atheir%20reasoning%20steps%20and%20improve%20the%20accuracy%20of%20their%20mathematical%20reasoning.%0ATo%20enable%20the%20CoSC%20mechanism%20at%20a%20low%20cost%2C%20we%20employ%20a%20two-phase%20finetuning%0Aapproach.%20In%20the%20first%20phase%2C%20the%20LLMs%20are%20trained%20with%20a%20relatively%20small%0Avolume%20of%20seeding%20data%20generated%20from%20GPT-4%2C%20establishing%20an%20initial%20CoSC%0Acapability.%20In%20the%20second%20phase%2C%20the%20CoSC%20capability%20is%20further%20enhanced%20by%0Atraining%20with%20a%20larger%20volume%20of%20self-generated%20data%20using%20the%20trained%20model%20in%0Athe%20first%20phase%2C%20without%20relying%20on%20the%20paid%20GPT-4.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20CoSC%20significantly%20improves%20performance%20on%0Atraditional%20mathematical%20datasets%20among%20existing%20open-source%20LLMs.%20Notably%2C%20our%0ACoSC-Code-34B%20model%20achieved%20a%2053.5%25%20score%20on%20MATH%2C%20the%20most%20challenging%0Amathematical%20reasoning%20dataset%20in%20the%20public%20domain%2C%20surpassing%20the%20performance%0Aof%20well-established%20models%20such%20as%20ChatGPT%2C%20GPT-4%2C%20and%20even%20multi-modal%20LLMs%0Alike%20GPT-4V%2C%20Gemini-1.0%20Pro%2C%20and%20Gemini-1.0%20Ultra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Self-Correction%2520as%2520an%2520Inherent%2520Ability%2520in%2520Large%2520Language%250A%2520%2520Models%2520for%2520Enhanced%2520Mathematical%2520Reasoning%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Huanqia%2520Cai%2520and%2520Qingyao%2520Shuai%2520and%2520Dihong%2520Gong%2520and%2520Zhifeng%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520mathematical%2520reasoning%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520crucial%250Ain%2520revolutionizing%2520domains%2520that%2520heavily%2520rely%2520on%2520such%2520reasoning.%2520However%252C%2520LLMs%250Aoften%2520encounter%2520difficulties%2520in%2520certain%2520aspects%2520of%2520mathematical%2520reasoning%252C%250Aleading%2520to%2520flawed%2520reasoning%2520and%2520erroneous%2520results.%2520To%2520mitigate%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520novel%2520mechanism%252C%2520the%2520Chain%2520of%2520Self-Correction%2520%2528CoSC%2529%252C%2520specifically%250Adesigned%2520to%2520embed%2520self-correction%2520as%2520an%2520inherent%2520ability%2520in%2520LLMs%252C%2520enabling%2520them%250Ato%2520validate%2520and%2520rectify%2520their%2520own%2520results.%2520The%2520CoSC%2520mechanism%2520operates%2520through%250Aa%2520sequence%2520of%2520self-correction%2520stages.%2520In%2520each%2520stage%252C%2520the%2520LLMs%2520generate%2520a%250Aprogram%2520to%2520address%2520a%2520given%2520problem%252C%2520execute%2520this%2520program%2520using%2520program-based%250Atools%2520to%2520obtain%2520an%2520output%252C%2520subsequently%2520verify%2520this%2520output.%2520Based%2520on%2520the%250Averification%252C%2520the%2520LLMs%2520either%2520proceed%2520to%2520the%2520next%2520correction%2520stage%2520or%2520finalize%250Athe%2520answer.%2520This%2520iterative%2520self-correction%2520process%2520allows%2520the%2520LLMs%2520to%2520refine%250Atheir%2520reasoning%2520steps%2520and%2520improve%2520the%2520accuracy%2520of%2520their%2520mathematical%2520reasoning.%250ATo%2520enable%2520the%2520CoSC%2520mechanism%2520at%2520a%2520low%2520cost%252C%2520we%2520employ%2520a%2520two-phase%2520finetuning%250Aapproach.%2520In%2520the%2520first%2520phase%252C%2520the%2520LLMs%2520are%2520trained%2520with%2520a%2520relatively%2520small%250Avolume%2520of%2520seeding%2520data%2520generated%2520from%2520GPT-4%252C%2520establishing%2520an%2520initial%2520CoSC%250Acapability.%2520In%2520the%2520second%2520phase%252C%2520the%2520CoSC%2520capability%2520is%2520further%2520enhanced%2520by%250Atraining%2520with%2520a%2520larger%2520volume%2520of%2520self-generated%2520data%2520using%2520the%2520trained%2520model%2520in%250Athe%2520first%2520phase%252C%2520without%2520relying%2520on%2520the%2520paid%2520GPT-4.%2520Our%2520comprehensive%250Aexperiments%2520demonstrate%2520that%2520CoSC%2520significantly%2520improves%2520performance%2520on%250Atraditional%2520mathematical%2520datasets%2520among%2520existing%2520open-source%2520LLMs.%2520Notably%252C%2520our%250ACoSC-Code-34B%2520model%2520achieved%2520a%252053.5%2525%2520score%2520on%2520MATH%252C%2520the%2520most%2520challenging%250Amathematical%2520reasoning%2520dataset%2520in%2520the%2520public%2520domain%252C%2520surpassing%2520the%2520performance%250Aof%2520well-established%2520models%2520such%2520as%2520ChatGPT%252C%2520GPT-4%252C%2520and%2520even%2520multi-modal%2520LLMs%250Alike%2520GPT-4V%252C%2520Gemini-1.0%2520Pro%252C%2520and%2520Gemini-1.0%2520Ultra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Self-Correction%20as%20an%20Inherent%20Ability%20in%20Large%20Language%0A%20%20Models%20for%20Enhanced%20Mathematical%20Reasoning&entry.906535625=Kuofeng%20Gao%20and%20Huanqia%20Cai%20and%20Qingyao%20Shuai%20and%20Dihong%20Gong%20and%20Zhifeng%20Li&entry.1292438233=%20%20Accurate%20mathematical%20reasoning%20with%20Large%20Language%20Models%20%28LLMs%29%20is%20crucial%0Ain%20revolutionizing%20domains%20that%20heavily%20rely%20on%20such%20reasoning.%20However%2C%20LLMs%0Aoften%20encounter%20difficulties%20in%20certain%20aspects%20of%20mathematical%20reasoning%2C%0Aleading%20to%20flawed%20reasoning%20and%20erroneous%20results.%20To%20mitigate%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20mechanism%2C%20the%20Chain%20of%20Self-Correction%20%28CoSC%29%2C%20specifically%0Adesigned%20to%20embed%20self-correction%20as%20an%20inherent%20ability%20in%20LLMs%2C%20enabling%20them%0Ato%20validate%20and%20rectify%20their%20own%20results.%20The%20CoSC%20mechanism%20operates%20through%0Aa%20sequence%20of%20self-correction%20stages.%20In%20each%20stage%2C%20the%20LLMs%20generate%20a%0Aprogram%20to%20address%20a%20given%20problem%2C%20execute%20this%20program%20using%20program-based%0Atools%20to%20obtain%20an%20output%2C%20subsequently%20verify%20this%20output.%20Based%20on%20the%0Averification%2C%20the%20LLMs%20either%20proceed%20to%20the%20next%20correction%20stage%20or%20finalize%0Athe%20answer.%20This%20iterative%20self-correction%20process%20allows%20the%20LLMs%20to%20refine%0Atheir%20reasoning%20steps%20and%20improve%20the%20accuracy%20of%20their%20mathematical%20reasoning.%0ATo%20enable%20the%20CoSC%20mechanism%20at%20a%20low%20cost%2C%20we%20employ%20a%20two-phase%20finetuning%0Aapproach.%20In%20the%20first%20phase%2C%20the%20LLMs%20are%20trained%20with%20a%20relatively%20small%0Avolume%20of%20seeding%20data%20generated%20from%20GPT-4%2C%20establishing%20an%20initial%20CoSC%0Acapability.%20In%20the%20second%20phase%2C%20the%20CoSC%20capability%20is%20further%20enhanced%20by%0Atraining%20with%20a%20larger%20volume%20of%20self-generated%20data%20using%20the%20trained%20model%20in%0Athe%20first%20phase%2C%20without%20relying%20on%20the%20paid%20GPT-4.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20CoSC%20significantly%20improves%20performance%20on%0Atraditional%20mathematical%20datasets%20among%20existing%20open-source%20LLMs.%20Notably%2C%20our%0ACoSC-Code-34B%20model%20achieved%20a%2053.5%25%20score%20on%20MATH%2C%20the%20most%20challenging%0Amathematical%20reasoning%20dataset%20in%20the%20public%20domain%2C%20surpassing%20the%20performance%0Aof%20well-established%20models%20such%20as%20ChatGPT%2C%20GPT-4%2C%20and%20even%20multi-modal%20LLMs%0Alike%20GPT-4V%2C%20Gemini-1.0%20Pro%2C%20and%20Gemini-1.0%20Ultra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10735v1&entry.124074799=Read"},
{"title": "Depth Any Video with Scalable Synthetic Data", "author": "Honghui Yang and Di Huang and Wei Yin and Chunhua Shen and Haifeng Liu and Xiaofei He and Binbin Lin and Wanli Ouyang and Tong He", "abstract": "  Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse synthetic\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency.\n", "link": "http://arxiv.org/abs/2410.10815v1", "date": "2024-10-14", "relevancy": 2.3962, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6161}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5958}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Any%20Video%20with%20Scalable%20Synthetic%20Data&body=Title%3A%20Depth%20Any%20Video%20with%20Scalable%20Synthetic%20Data%0AAuthor%3A%20Honghui%20Yang%20and%20Di%20Huang%20and%20Wei%20Yin%20and%20Chunhua%20Shen%20and%20Haifeng%20Liu%20and%20Xiaofei%20He%20and%20Binbin%20Lin%20and%20Wanli%20Ouyang%20and%20Tong%20He%0AAbstract%3A%20%20%20Video%20depth%20estimation%20has%20long%20been%20hindered%20by%20the%20scarcity%20of%20consistent%0Aand%20scalable%20ground%20truth%20data%2C%20leading%20to%20inconsistent%20and%20unreliable%20results.%0AIn%20this%20paper%2C%20we%20introduce%20Depth%20Any%20Video%2C%20a%20model%20that%20tackles%20the%20challenge%0Athrough%20two%20key%20innovations.%20First%2C%20we%20develop%20a%20scalable%20synthetic%20data%0Apipeline%2C%20capturing%20real-time%20video%20depth%20data%20from%20diverse%20synthetic%0Aenvironments%2C%20yielding%2040%2C000%20video%20clips%20of%205-second%20duration%2C%20each%20with%0Aprecise%20depth%20annotations.%20Second%2C%20we%20leverage%20the%20powerful%20priors%20of%0Agenerative%20video%20diffusion%20models%20to%20handle%20real-world%20videos%20effectively%2C%0Aintegrating%20advanced%20techniques%20such%20as%20rotary%20position%20encoding%20and%20flow%0Amatching%20to%20further%20enhance%20flexibility%20and%20efficiency.%20Unlike%20previous%20models%2C%0Awhich%20are%20limited%20to%20fixed-length%20video%20sequences%2C%20our%20approach%20introduces%20a%0Anovel%20mixed-duration%20training%20strategy%20that%20handles%20videos%20of%20varying%20lengths%0Aand%20performs%20robustly%20across%20different%20frame%20rates-even%20on%20single%20frames.%20At%0Ainference%2C%20we%20propose%20a%20depth%20interpolation%20method%20that%20enables%20our%20model%20to%0Ainfer%20high-resolution%20video%20depth%20across%20sequences%20of%20up%20to%20150%20frames.%20Our%0Amodel%20outperforms%20all%20previous%20generative%20depth%20models%20in%20terms%20of%20spatial%0Aaccuracy%20and%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Any%2520Video%2520with%2520Scalable%2520Synthetic%2520Data%26entry.906535625%3DHonghui%2520Yang%2520and%2520Di%2520Huang%2520and%2520Wei%2520Yin%2520and%2520Chunhua%2520Shen%2520and%2520Haifeng%2520Liu%2520and%2520Xiaofei%2520He%2520and%2520Binbin%2520Lin%2520and%2520Wanli%2520Ouyang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520Video%2520depth%2520estimation%2520has%2520long%2520been%2520hindered%2520by%2520the%2520scarcity%2520of%2520consistent%250Aand%2520scalable%2520ground%2520truth%2520data%252C%2520leading%2520to%2520inconsistent%2520and%2520unreliable%2520results.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Depth%2520Any%2520Video%252C%2520a%2520model%2520that%2520tackles%2520the%2520challenge%250Athrough%2520two%2520key%2520innovations.%2520First%252C%2520we%2520develop%2520a%2520scalable%2520synthetic%2520data%250Apipeline%252C%2520capturing%2520real-time%2520video%2520depth%2520data%2520from%2520diverse%2520synthetic%250Aenvironments%252C%2520yielding%252040%252C000%2520video%2520clips%2520of%25205-second%2520duration%252C%2520each%2520with%250Aprecise%2520depth%2520annotations.%2520Second%252C%2520we%2520leverage%2520the%2520powerful%2520priors%2520of%250Agenerative%2520video%2520diffusion%2520models%2520to%2520handle%2520real-world%2520videos%2520effectively%252C%250Aintegrating%2520advanced%2520techniques%2520such%2520as%2520rotary%2520position%2520encoding%2520and%2520flow%250Amatching%2520to%2520further%2520enhance%2520flexibility%2520and%2520efficiency.%2520Unlike%2520previous%2520models%252C%250Awhich%2520are%2520limited%2520to%2520fixed-length%2520video%2520sequences%252C%2520our%2520approach%2520introduces%2520a%250Anovel%2520mixed-duration%2520training%2520strategy%2520that%2520handles%2520videos%2520of%2520varying%2520lengths%250Aand%2520performs%2520robustly%2520across%2520different%2520frame%2520rates-even%2520on%2520single%2520frames.%2520At%250Ainference%252C%2520we%2520propose%2520a%2520depth%2520interpolation%2520method%2520that%2520enables%2520our%2520model%2520to%250Ainfer%2520high-resolution%2520video%2520depth%2520across%2520sequences%2520of%2520up%2520to%2520150%2520frames.%2520Our%250Amodel%2520outperforms%2520all%2520previous%2520generative%2520depth%2520models%2520in%2520terms%2520of%2520spatial%250Aaccuracy%2520and%2520temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Any%20Video%20with%20Scalable%20Synthetic%20Data&entry.906535625=Honghui%20Yang%20and%20Di%20Huang%20and%20Wei%20Yin%20and%20Chunhua%20Shen%20and%20Haifeng%20Liu%20and%20Xiaofei%20He%20and%20Binbin%20Lin%20and%20Wanli%20Ouyang%20and%20Tong%20He&entry.1292438233=%20%20Video%20depth%20estimation%20has%20long%20been%20hindered%20by%20the%20scarcity%20of%20consistent%0Aand%20scalable%20ground%20truth%20data%2C%20leading%20to%20inconsistent%20and%20unreliable%20results.%0AIn%20this%20paper%2C%20we%20introduce%20Depth%20Any%20Video%2C%20a%20model%20that%20tackles%20the%20challenge%0Athrough%20two%20key%20innovations.%20First%2C%20we%20develop%20a%20scalable%20synthetic%20data%0Apipeline%2C%20capturing%20real-time%20video%20depth%20data%20from%20diverse%20synthetic%0Aenvironments%2C%20yielding%2040%2C000%20video%20clips%20of%205-second%20duration%2C%20each%20with%0Aprecise%20depth%20annotations.%20Second%2C%20we%20leverage%20the%20powerful%20priors%20of%0Agenerative%20video%20diffusion%20models%20to%20handle%20real-world%20videos%20effectively%2C%0Aintegrating%20advanced%20techniques%20such%20as%20rotary%20position%20encoding%20and%20flow%0Amatching%20to%20further%20enhance%20flexibility%20and%20efficiency.%20Unlike%20previous%20models%2C%0Awhich%20are%20limited%20to%20fixed-length%20video%20sequences%2C%20our%20approach%20introduces%20a%0Anovel%20mixed-duration%20training%20strategy%20that%20handles%20videos%20of%20varying%20lengths%0Aand%20performs%20robustly%20across%20different%20frame%20rates-even%20on%20single%20frames.%20At%0Ainference%2C%20we%20propose%20a%20depth%20interpolation%20method%20that%20enables%20our%20model%20to%0Ainfer%20high-resolution%20video%20depth%20across%20sequences%20of%20up%20to%20150%20frames.%20Our%0Amodel%20outperforms%20all%20previous%20generative%20depth%20models%20in%20terms%20of%20spatial%0Aaccuracy%20and%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10815v1&entry.124074799=Read"},
{"title": "QueST: Querying Functional and Structural Niches on Spatial\n  Transcriptomics Data via Contrastive Subgraph Embedding", "author": "Mo Chen and Minsheng Hao and Xuegong Zhang and Lei Wei", "abstract": "  The functional or structural spatial regions within tissues, referred to as\nspatial niches, are elements for illustrating the spatial contexts of\nmulticellular organisms. A key challenge is querying shared niches across\ndiverse tissues, which is crucial for achieving a comprehensive understanding\nof the organization and phenotypes of cell populations. However, current data\nanalysis methods predominantly focus on creating spatial-aware embeddings for\ncells, neglecting the development of niche-level representations for effective\nquerying. To address this gap, we introduce QueST, a novel niche representation\nlearning model designed for querying spatial niches across multiple samples.\nQueST utilizes a novel subgraph contrastive learning approach to explicitly\ncapture niche-level characteristics and incorporates adversarial training to\nmitigate batch effects. We evaluate QueST on established benchmarks using human\nand mouse datasets, demonstrating its superiority over state-of-the-art graph\nrepresentation learning methods in accurate niche queries. Overall, QueST\noffers a specialized model for spatial niche queries, paving the way for deeper\ninsights into the patterns and mechanisms of cell spatial organization across\ntissues. Source code can be found at https://github.com/cmhimself/QueST.\n", "link": "http://arxiv.org/abs/2410.10652v1", "date": "2024-10-14", "relevancy": 2.3839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueST%3A%20Querying%20Functional%20and%20Structural%20Niches%20on%20Spatial%0A%20%20Transcriptomics%20Data%20via%20Contrastive%20Subgraph%20Embedding&body=Title%3A%20QueST%3A%20Querying%20Functional%20and%20Structural%20Niches%20on%20Spatial%0A%20%20Transcriptomics%20Data%20via%20Contrastive%20Subgraph%20Embedding%0AAuthor%3A%20Mo%20Chen%20and%20Minsheng%20Hao%20and%20Xuegong%20Zhang%20and%20Lei%20Wei%0AAbstract%3A%20%20%20The%20functional%20or%20structural%20spatial%20regions%20within%20tissues%2C%20referred%20to%20as%0Aspatial%20niches%2C%20are%20elements%20for%20illustrating%20the%20spatial%20contexts%20of%0Amulticellular%20organisms.%20A%20key%20challenge%20is%20querying%20shared%20niches%20across%0Adiverse%20tissues%2C%20which%20is%20crucial%20for%20achieving%20a%20comprehensive%20understanding%0Aof%20the%20organization%20and%20phenotypes%20of%20cell%20populations.%20However%2C%20current%20data%0Aanalysis%20methods%20predominantly%20focus%20on%20creating%20spatial-aware%20embeddings%20for%0Acells%2C%20neglecting%20the%20development%20of%20niche-level%20representations%20for%20effective%0Aquerying.%20To%20address%20this%20gap%2C%20we%20introduce%20QueST%2C%20a%20novel%20niche%20representation%0Alearning%20model%20designed%20for%20querying%20spatial%20niches%20across%20multiple%20samples.%0AQueST%20utilizes%20a%20novel%20subgraph%20contrastive%20learning%20approach%20to%20explicitly%0Acapture%20niche-level%20characteristics%20and%20incorporates%20adversarial%20training%20to%0Amitigate%20batch%20effects.%20We%20evaluate%20QueST%20on%20established%20benchmarks%20using%20human%0Aand%20mouse%20datasets%2C%20demonstrating%20its%20superiority%20over%20state-of-the-art%20graph%0Arepresentation%20learning%20methods%20in%20accurate%20niche%20queries.%20Overall%2C%20QueST%0Aoffers%20a%20specialized%20model%20for%20spatial%20niche%20queries%2C%20paving%20the%20way%20for%20deeper%0Ainsights%20into%20the%20patterns%20and%20mechanisms%20of%20cell%20spatial%20organization%20across%0Atissues.%20Source%20code%20can%20be%20found%20at%20https%3A//github.com/cmhimself/QueST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueST%253A%2520Querying%2520Functional%2520and%2520Structural%2520Niches%2520on%2520Spatial%250A%2520%2520Transcriptomics%2520Data%2520via%2520Contrastive%2520Subgraph%2520Embedding%26entry.906535625%3DMo%2520Chen%2520and%2520Minsheng%2520Hao%2520and%2520Xuegong%2520Zhang%2520and%2520Lei%2520Wei%26entry.1292438233%3D%2520%2520The%2520functional%2520or%2520structural%2520spatial%2520regions%2520within%2520tissues%252C%2520referred%2520to%2520as%250Aspatial%2520niches%252C%2520are%2520elements%2520for%2520illustrating%2520the%2520spatial%2520contexts%2520of%250Amulticellular%2520organisms.%2520A%2520key%2520challenge%2520is%2520querying%2520shared%2520niches%2520across%250Adiverse%2520tissues%252C%2520which%2520is%2520crucial%2520for%2520achieving%2520a%2520comprehensive%2520understanding%250Aof%2520the%2520organization%2520and%2520phenotypes%2520of%2520cell%2520populations.%2520However%252C%2520current%2520data%250Aanalysis%2520methods%2520predominantly%2520focus%2520on%2520creating%2520spatial-aware%2520embeddings%2520for%250Acells%252C%2520neglecting%2520the%2520development%2520of%2520niche-level%2520representations%2520for%2520effective%250Aquerying.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520QueST%252C%2520a%2520novel%2520niche%2520representation%250Alearning%2520model%2520designed%2520for%2520querying%2520spatial%2520niches%2520across%2520multiple%2520samples.%250AQueST%2520utilizes%2520a%2520novel%2520subgraph%2520contrastive%2520learning%2520approach%2520to%2520explicitly%250Acapture%2520niche-level%2520characteristics%2520and%2520incorporates%2520adversarial%2520training%2520to%250Amitigate%2520batch%2520effects.%2520We%2520evaluate%2520QueST%2520on%2520established%2520benchmarks%2520using%2520human%250Aand%2520mouse%2520datasets%252C%2520demonstrating%2520its%2520superiority%2520over%2520state-of-the-art%2520graph%250Arepresentation%2520learning%2520methods%2520in%2520accurate%2520niche%2520queries.%2520Overall%252C%2520QueST%250Aoffers%2520a%2520specialized%2520model%2520for%2520spatial%2520niche%2520queries%252C%2520paving%2520the%2520way%2520for%2520deeper%250Ainsights%2520into%2520the%2520patterns%2520and%2520mechanisms%2520of%2520cell%2520spatial%2520organization%2520across%250Atissues.%2520Source%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/cmhimself/QueST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueST%3A%20Querying%20Functional%20and%20Structural%20Niches%20on%20Spatial%0A%20%20Transcriptomics%20Data%20via%20Contrastive%20Subgraph%20Embedding&entry.906535625=Mo%20Chen%20and%20Minsheng%20Hao%20and%20Xuegong%20Zhang%20and%20Lei%20Wei&entry.1292438233=%20%20The%20functional%20or%20structural%20spatial%20regions%20within%20tissues%2C%20referred%20to%20as%0Aspatial%20niches%2C%20are%20elements%20for%20illustrating%20the%20spatial%20contexts%20of%0Amulticellular%20organisms.%20A%20key%20challenge%20is%20querying%20shared%20niches%20across%0Adiverse%20tissues%2C%20which%20is%20crucial%20for%20achieving%20a%20comprehensive%20understanding%0Aof%20the%20organization%20and%20phenotypes%20of%20cell%20populations.%20However%2C%20current%20data%0Aanalysis%20methods%20predominantly%20focus%20on%20creating%20spatial-aware%20embeddings%20for%0Acells%2C%20neglecting%20the%20development%20of%20niche-level%20representations%20for%20effective%0Aquerying.%20To%20address%20this%20gap%2C%20we%20introduce%20QueST%2C%20a%20novel%20niche%20representation%0Alearning%20model%20designed%20for%20querying%20spatial%20niches%20across%20multiple%20samples.%0AQueST%20utilizes%20a%20novel%20subgraph%20contrastive%20learning%20approach%20to%20explicitly%0Acapture%20niche-level%20characteristics%20and%20incorporates%20adversarial%20training%20to%0Amitigate%20batch%20effects.%20We%20evaluate%20QueST%20on%20established%20benchmarks%20using%20human%0Aand%20mouse%20datasets%2C%20demonstrating%20its%20superiority%20over%20state-of-the-art%20graph%0Arepresentation%20learning%20methods%20in%20accurate%20niche%20queries.%20Overall%2C%20QueST%0Aoffers%20a%20specialized%20model%20for%20spatial%20niche%20queries%2C%20paving%20the%20way%20for%20deeper%0Ainsights%20into%20the%20patterns%20and%20mechanisms%20of%20cell%20spatial%20organization%20across%0Atissues.%20Source%20code%20can%20be%20found%20at%20https%3A//github.com/cmhimself/QueST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10652v1&entry.124074799=Read"},
{"title": "TL-PCA: Transfer Learning of Principal Component Analysis", "author": "Sharon Hendy and Yehuda Dar", "abstract": "  Principal component analysis (PCA) can be significantly limited when there is\ntoo few examples of the target data of interest. We propose a transfer learning\napproach to PCA (TL-PCA) where knowledge from a related source task is used in\naddition to the scarce data of a target task. Our TL-PCA has two versions, one\nthat uses a pretrained PCA solution of the source task, and another that uses\nthe source data. Our proposed approach extends the PCA optimization objective\nwith a penalty on the proximity of the target subspace and the source subspace\nas given by the pretrained source model or the source data. This optimization\nis solved by eigendecomposition for which the number of data-dependent\neigenvectors (i.e., principal directions of TL-PCA) is not limited to the\nnumber of target data examples, which is a root cause that limits the standard\nPCA performance. Accordingly, our results for image datasets show that the\nrepresentation of test data is improved by TL-PCA for dimensionality reduction\nwhere the learned subspace dimension is lower or higher than the number of\ntarget data examples.\n", "link": "http://arxiv.org/abs/2410.10805v1", "date": "2024-10-14", "relevancy": 2.3826, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TL-PCA%3A%20Transfer%20Learning%20of%20Principal%20Component%20Analysis&body=Title%3A%20TL-PCA%3A%20Transfer%20Learning%20of%20Principal%20Component%20Analysis%0AAuthor%3A%20Sharon%20Hendy%20and%20Yehuda%20Dar%0AAbstract%3A%20%20%20Principal%20component%20analysis%20%28PCA%29%20can%20be%20significantly%20limited%20when%20there%20is%0Atoo%20few%20examples%20of%20the%20target%20data%20of%20interest.%20We%20propose%20a%20transfer%20learning%0Aapproach%20to%20PCA%20%28TL-PCA%29%20where%20knowledge%20from%20a%20related%20source%20task%20is%20used%20in%0Aaddition%20to%20the%20scarce%20data%20of%20a%20target%20task.%20Our%20TL-PCA%20has%20two%20versions%2C%20one%0Athat%20uses%20a%20pretrained%20PCA%20solution%20of%20the%20source%20task%2C%20and%20another%20that%20uses%0Athe%20source%20data.%20Our%20proposed%20approach%20extends%20the%20PCA%20optimization%20objective%0Awith%20a%20penalty%20on%20the%20proximity%20of%20the%20target%20subspace%20and%20the%20source%20subspace%0Aas%20given%20by%20the%20pretrained%20source%20model%20or%20the%20source%20data.%20This%20optimization%0Ais%20solved%20by%20eigendecomposition%20for%20which%20the%20number%20of%20data-dependent%0Aeigenvectors%20%28i.e.%2C%20principal%20directions%20of%20TL-PCA%29%20is%20not%20limited%20to%20the%0Anumber%20of%20target%20data%20examples%2C%20which%20is%20a%20root%20cause%20that%20limits%20the%20standard%0APCA%20performance.%20Accordingly%2C%20our%20results%20for%20image%20datasets%20show%20that%20the%0Arepresentation%20of%20test%20data%20is%20improved%20by%20TL-PCA%20for%20dimensionality%20reduction%0Awhere%20the%20learned%20subspace%20dimension%20is%20lower%20or%20higher%20than%20the%20number%20of%0Atarget%20data%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTL-PCA%253A%2520Transfer%2520Learning%2520of%2520Principal%2520Component%2520Analysis%26entry.906535625%3DSharon%2520Hendy%2520and%2520Yehuda%2520Dar%26entry.1292438233%3D%2520%2520Principal%2520component%2520analysis%2520%2528PCA%2529%2520can%2520be%2520significantly%2520limited%2520when%2520there%2520is%250Atoo%2520few%2520examples%2520of%2520the%2520target%2520data%2520of%2520interest.%2520We%2520propose%2520a%2520transfer%2520learning%250Aapproach%2520to%2520PCA%2520%2528TL-PCA%2529%2520where%2520knowledge%2520from%2520a%2520related%2520source%2520task%2520is%2520used%2520in%250Aaddition%2520to%2520the%2520scarce%2520data%2520of%2520a%2520target%2520task.%2520Our%2520TL-PCA%2520has%2520two%2520versions%252C%2520one%250Athat%2520uses%2520a%2520pretrained%2520PCA%2520solution%2520of%2520the%2520source%2520task%252C%2520and%2520another%2520that%2520uses%250Athe%2520source%2520data.%2520Our%2520proposed%2520approach%2520extends%2520the%2520PCA%2520optimization%2520objective%250Awith%2520a%2520penalty%2520on%2520the%2520proximity%2520of%2520the%2520target%2520subspace%2520and%2520the%2520source%2520subspace%250Aas%2520given%2520by%2520the%2520pretrained%2520source%2520model%2520or%2520the%2520source%2520data.%2520This%2520optimization%250Ais%2520solved%2520by%2520eigendecomposition%2520for%2520which%2520the%2520number%2520of%2520data-dependent%250Aeigenvectors%2520%2528i.e.%252C%2520principal%2520directions%2520of%2520TL-PCA%2529%2520is%2520not%2520limited%2520to%2520the%250Anumber%2520of%2520target%2520data%2520examples%252C%2520which%2520is%2520a%2520root%2520cause%2520that%2520limits%2520the%2520standard%250APCA%2520performance.%2520Accordingly%252C%2520our%2520results%2520for%2520image%2520datasets%2520show%2520that%2520the%250Arepresentation%2520of%2520test%2520data%2520is%2520improved%2520by%2520TL-PCA%2520for%2520dimensionality%2520reduction%250Awhere%2520the%2520learned%2520subspace%2520dimension%2520is%2520lower%2520or%2520higher%2520than%2520the%2520number%2520of%250Atarget%2520data%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TL-PCA%3A%20Transfer%20Learning%20of%20Principal%20Component%20Analysis&entry.906535625=Sharon%20Hendy%20and%20Yehuda%20Dar&entry.1292438233=%20%20Principal%20component%20analysis%20%28PCA%29%20can%20be%20significantly%20limited%20when%20there%20is%0Atoo%20few%20examples%20of%20the%20target%20data%20of%20interest.%20We%20propose%20a%20transfer%20learning%0Aapproach%20to%20PCA%20%28TL-PCA%29%20where%20knowledge%20from%20a%20related%20source%20task%20is%20used%20in%0Aaddition%20to%20the%20scarce%20data%20of%20a%20target%20task.%20Our%20TL-PCA%20has%20two%20versions%2C%20one%0Athat%20uses%20a%20pretrained%20PCA%20solution%20of%20the%20source%20task%2C%20and%20another%20that%20uses%0Athe%20source%20data.%20Our%20proposed%20approach%20extends%20the%20PCA%20optimization%20objective%0Awith%20a%20penalty%20on%20the%20proximity%20of%20the%20target%20subspace%20and%20the%20source%20subspace%0Aas%20given%20by%20the%20pretrained%20source%20model%20or%20the%20source%20data.%20This%20optimization%0Ais%20solved%20by%20eigendecomposition%20for%20which%20the%20number%20of%20data-dependent%0Aeigenvectors%20%28i.e.%2C%20principal%20directions%20of%20TL-PCA%29%20is%20not%20limited%20to%20the%0Anumber%20of%20target%20data%20examples%2C%20which%20is%20a%20root%20cause%20that%20limits%20the%20standard%0APCA%20performance.%20Accordingly%2C%20our%20results%20for%20image%20datasets%20show%20that%20the%0Arepresentation%20of%20test%20data%20is%20improved%20by%20TL-PCA%20for%20dimensionality%20reduction%0Awhere%20the%20learned%20subspace%20dimension%20is%20lower%20or%20higher%20than%20the%20number%20of%0Atarget%20data%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10805v1&entry.124074799=Read"},
{"title": "Active Learning of Robot Vision Using Adaptive Path Planning", "author": "Julius R\u00fcckin and Federico Magistri and Cyrill Stachniss and Marija Popovi\u0107", "abstract": "  Robots need robust and flexible vision systems to perceive and reason about\ntheir environments beyond geometry. Most of such systems build upon deep\nlearning approaches. As autonomous robots are commonly deployed in initially\nunknown environments, pre-training on static datasets cannot always capture the\nvariety of domains and limits the robot's vision performance during missions.\nRecently, self-supervised as well as fully supervised active learning methods\nemerged to improve robotic vision. These approaches rely on large in-domain\npre-training datasets or require substantial human labelling effort. To address\nthese issues, we present a recent adaptive planning framework for efficient\ntraining data collection to substantially reduce human labelling requirements\nin semantic terrain monitoring missions. To this end, we combine high-quality\nhuman labels with automatically generated pseudo labels. Experimental results\nshow that the framework reaches segmentation performance close to fully\nsupervised approaches with drastically reduced human labelling effort while\noutperforming purely self-supervised approaches. We discuss the advantages and\nlimitations of current methods and outline valuable future research avenues\ntowards more robust and flexible robotic vision systems in unknown\nenvironments.\n", "link": "http://arxiv.org/abs/2410.10684v1", "date": "2024-10-14", "relevancy": 2.3765, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6086}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5925}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20of%20Robot%20Vision%20Using%20Adaptive%20Path%20Planning&body=Title%3A%20Active%20Learning%20of%20Robot%20Vision%20Using%20Adaptive%20Path%20Planning%0AAuthor%3A%20Julius%20R%C3%BCckin%20and%20Federico%20Magistri%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87%0AAbstract%3A%20%20%20Robots%20need%20robust%20and%20flexible%20vision%20systems%20to%20perceive%20and%20reason%20about%0Atheir%20environments%20beyond%20geometry.%20Most%20of%20such%20systems%20build%20upon%20deep%0Alearning%20approaches.%20As%20autonomous%20robots%20are%20commonly%20deployed%20in%20initially%0Aunknown%20environments%2C%20pre-training%20on%20static%20datasets%20cannot%20always%20capture%20the%0Avariety%20of%20domains%20and%20limits%20the%20robot%27s%20vision%20performance%20during%20missions.%0ARecently%2C%20self-supervised%20as%20well%20as%20fully%20supervised%20active%20learning%20methods%0Aemerged%20to%20improve%20robotic%20vision.%20These%20approaches%20rely%20on%20large%20in-domain%0Apre-training%20datasets%20or%20require%20substantial%20human%20labelling%20effort.%20To%20address%0Athese%20issues%2C%20we%20present%20a%20recent%20adaptive%20planning%20framework%20for%20efficient%0Atraining%20data%20collection%20to%20substantially%20reduce%20human%20labelling%20requirements%0Ain%20semantic%20terrain%20monitoring%20missions.%20To%20this%20end%2C%20we%20combine%20high-quality%0Ahuman%20labels%20with%20automatically%20generated%20pseudo%20labels.%20Experimental%20results%0Ashow%20that%20the%20framework%20reaches%20segmentation%20performance%20close%20to%20fully%0Asupervised%20approaches%20with%20drastically%20reduced%20human%20labelling%20effort%20while%0Aoutperforming%20purely%20self-supervised%20approaches.%20We%20discuss%20the%20advantages%20and%0Alimitations%20of%20current%20methods%20and%20outline%20valuable%20future%20research%20avenues%0Atowards%20more%20robust%20and%20flexible%20robotic%20vision%20systems%20in%20unknown%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520of%2520Robot%2520Vision%2520Using%2520Adaptive%2520Path%2520Planning%26entry.906535625%3DJulius%2520R%25C3%25BCckin%2520and%2520Federico%2520Magistri%2520and%2520Cyrill%2520Stachniss%2520and%2520Marija%2520Popovi%25C4%2587%26entry.1292438233%3D%2520%2520Robots%2520need%2520robust%2520and%2520flexible%2520vision%2520systems%2520to%2520perceive%2520and%2520reason%2520about%250Atheir%2520environments%2520beyond%2520geometry.%2520Most%2520of%2520such%2520systems%2520build%2520upon%2520deep%250Alearning%2520approaches.%2520As%2520autonomous%2520robots%2520are%2520commonly%2520deployed%2520in%2520initially%250Aunknown%2520environments%252C%2520pre-training%2520on%2520static%2520datasets%2520cannot%2520always%2520capture%2520the%250Avariety%2520of%2520domains%2520and%2520limits%2520the%2520robot%2527s%2520vision%2520performance%2520during%2520missions.%250ARecently%252C%2520self-supervised%2520as%2520well%2520as%2520fully%2520supervised%2520active%2520learning%2520methods%250Aemerged%2520to%2520improve%2520robotic%2520vision.%2520These%2520approaches%2520rely%2520on%2520large%2520in-domain%250Apre-training%2520datasets%2520or%2520require%2520substantial%2520human%2520labelling%2520effort.%2520To%2520address%250Athese%2520issues%252C%2520we%2520present%2520a%2520recent%2520adaptive%2520planning%2520framework%2520for%2520efficient%250Atraining%2520data%2520collection%2520to%2520substantially%2520reduce%2520human%2520labelling%2520requirements%250Ain%2520semantic%2520terrain%2520monitoring%2520missions.%2520To%2520this%2520end%252C%2520we%2520combine%2520high-quality%250Ahuman%2520labels%2520with%2520automatically%2520generated%2520pseudo%2520labels.%2520Experimental%2520results%250Ashow%2520that%2520the%2520framework%2520reaches%2520segmentation%2520performance%2520close%2520to%2520fully%250Asupervised%2520approaches%2520with%2520drastically%2520reduced%2520human%2520labelling%2520effort%2520while%250Aoutperforming%2520purely%2520self-supervised%2520approaches.%2520We%2520discuss%2520the%2520advantages%2520and%250Alimitations%2520of%2520current%2520methods%2520and%2520outline%2520valuable%2520future%2520research%2520avenues%250Atowards%2520more%2520robust%2520and%2520flexible%2520robotic%2520vision%2520systems%2520in%2520unknown%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20of%20Robot%20Vision%20Using%20Adaptive%20Path%20Planning&entry.906535625=Julius%20R%C3%BCckin%20and%20Federico%20Magistri%20and%20Cyrill%20Stachniss%20and%20Marija%20Popovi%C4%87&entry.1292438233=%20%20Robots%20need%20robust%20and%20flexible%20vision%20systems%20to%20perceive%20and%20reason%20about%0Atheir%20environments%20beyond%20geometry.%20Most%20of%20such%20systems%20build%20upon%20deep%0Alearning%20approaches.%20As%20autonomous%20robots%20are%20commonly%20deployed%20in%20initially%0Aunknown%20environments%2C%20pre-training%20on%20static%20datasets%20cannot%20always%20capture%20the%0Avariety%20of%20domains%20and%20limits%20the%20robot%27s%20vision%20performance%20during%20missions.%0ARecently%2C%20self-supervised%20as%20well%20as%20fully%20supervised%20active%20learning%20methods%0Aemerged%20to%20improve%20robotic%20vision.%20These%20approaches%20rely%20on%20large%20in-domain%0Apre-training%20datasets%20or%20require%20substantial%20human%20labelling%20effort.%20To%20address%0Athese%20issues%2C%20we%20present%20a%20recent%20adaptive%20planning%20framework%20for%20efficient%0Atraining%20data%20collection%20to%20substantially%20reduce%20human%20labelling%20requirements%0Ain%20semantic%20terrain%20monitoring%20missions.%20To%20this%20end%2C%20we%20combine%20high-quality%0Ahuman%20labels%20with%20automatically%20generated%20pseudo%20labels.%20Experimental%20results%0Ashow%20that%20the%20framework%20reaches%20segmentation%20performance%20close%20to%20fully%0Asupervised%20approaches%20with%20drastically%20reduced%20human%20labelling%20effort%20while%0Aoutperforming%20purely%20self-supervised%20approaches.%20We%20discuss%20the%20advantages%20and%0Alimitations%20of%20current%20methods%20and%20outline%20valuable%20future%20research%20avenues%0Atowards%20more%20robust%20and%20flexible%20robotic%20vision%20systems%20in%20unknown%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10684v1&entry.124074799=Read"},
{"title": "TotalVibeSegmentator: Full Torso Segmentation for the NAKO and UK\n  Biobank in Volumetric Interpolated Breath-hold Examination Body Images", "author": "Robert Graf and Paul-S\u00f6ren Platzek and Evamaria Olga Riedel and Constanze Ramsch\u00fctz and Sophie Starck and Hendrik Kristian M\u00f6ller and Matan Atad and Henry V\u00f6lzke and Robin B\u00fclow and Carsten Oliver Schmidt and Julia R\u00fcdebusch and Matthias Jung and Marco Reisert and Jakob Weiss and Maximilian L\u00f6ffler and Fabian Bamberg and Bene Wiestler and Johannes C. Paetzold and Daniel Rueckert and Jan Stefan Kirschke", "abstract": "  Objectives: To present a publicly available torso segmentation network for\nlarge epidemiology datasets on volumetric interpolated breath-hold examination\n(VIBE) images. Materials & Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition networks for VIBE images, then\nimproved them iteratively and retrained a nnUNet network. Using subsets of NAKO\n(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a\nholdout set (12 subjects) and existing organ segmentation approach (1000\nsubjects), generating 71 semantic segmentation types for VIBE images. We\nprovide an additional network for the vertebra segments 22 individual vertebra\ntypes. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71\nsegmentation labels. We scored > 0.90 Dice-score on the abdominal organs except\nfor the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed\nand refined publicly available full torso segmentation on VIBE images.\n", "link": "http://arxiv.org/abs/2406.00125v2", "date": "2024-10-14", "relevancy": 2.3738, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TotalVibeSegmentator%3A%20Full%20Torso%20Segmentation%20for%20the%20NAKO%20and%20UK%0A%20%20Biobank%20in%20Volumetric%20Interpolated%20Breath-hold%20Examination%20Body%20Images&body=Title%3A%20TotalVibeSegmentator%3A%20Full%20Torso%20Segmentation%20for%20the%20NAKO%20and%20UK%0A%20%20Biobank%20in%20Volumetric%20Interpolated%20Breath-hold%20Examination%20Body%20Images%0AAuthor%3A%20Robert%20Graf%20and%20Paul-S%C3%B6ren%20Platzek%20and%20Evamaria%20Olga%20Riedel%20and%20Constanze%20Ramsch%C3%BCtz%20and%20Sophie%20Starck%20and%20Hendrik%20Kristian%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Carsten%20Oliver%20Schmidt%20and%20Julia%20R%C3%BCdebusch%20and%20Matthias%20Jung%20and%20Marco%20Reisert%20and%20Jakob%20Weiss%20and%20Maximilian%20L%C3%B6ffler%20and%20Fabian%20Bamberg%20and%20Bene%20Wiestler%20and%20Johannes%20C.%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20Stefan%20Kirschke%0AAbstract%3A%20%20%20Objectives%3A%20To%20present%20a%20publicly%20available%20torso%20segmentation%20network%20for%0Alarge%20epidemiology%20datasets%20on%20volumetric%20interpolated%20breath-hold%20examination%0A%28VIBE%29%20images.%20Materials%20%26%20Methods%3A%20We%20extracted%20preliminary%20segmentations%20from%0ATotalSegmentator%2C%20spine%2C%20and%20body%20composition%20networks%20for%20VIBE%20images%2C%20then%0Aimproved%20them%20iteratively%20and%20retrained%20a%20nnUNet%20network.%20Using%20subsets%20of%20NAKO%0A%2885%20subjects%29%20and%20UK%20Biobank%20%2816%20subjects%29%2C%20we%20evaluated%20with%20Dice-score%20on%20a%0Aholdout%20set%20%2812%20subjects%29%20and%20existing%20organ%20segmentation%20approach%20%281000%0Asubjects%29%2C%20generating%2071%20semantic%20segmentation%20types%20for%20VIBE%20images.%20We%0Aprovide%20an%20additional%20network%20for%20the%20vertebra%20segments%2022%20individual%20vertebra%0Atypes.%20Results%3A%20We%20achieved%20an%20average%20Dice%20score%20of%200.89%20%2B-%200.07%20overall%2071%0Asegmentation%20labels.%20We%20scored%20%3E%200.90%20Dice-score%20on%20the%20abdominal%20organs%20except%0Afor%20the%20pancreas%20with%20a%20Dice%20of%200.70.%20Conclusion%3A%20Our%20work%20offers%20a%20detailed%0Aand%20refined%20publicly%20available%20full%20torso%20segmentation%20on%20VIBE%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTotalVibeSegmentator%253A%2520Full%2520Torso%2520Segmentation%2520for%2520the%2520NAKO%2520and%2520UK%250A%2520%2520Biobank%2520in%2520Volumetric%2520Interpolated%2520Breath-hold%2520Examination%2520Body%2520Images%26entry.906535625%3DRobert%2520Graf%2520and%2520Paul-S%25C3%25B6ren%2520Platzek%2520and%2520Evamaria%2520Olga%2520Riedel%2520and%2520Constanze%2520Ramsch%25C3%25BCtz%2520and%2520Sophie%2520Starck%2520and%2520Hendrik%2520Kristian%2520M%25C3%25B6ller%2520and%2520Matan%2520Atad%2520and%2520Henry%2520V%25C3%25B6lzke%2520and%2520Robin%2520B%25C3%25BClow%2520and%2520Carsten%2520Oliver%2520Schmidt%2520and%2520Julia%2520R%25C3%25BCdebusch%2520and%2520Matthias%2520Jung%2520and%2520Marco%2520Reisert%2520and%2520Jakob%2520Weiss%2520and%2520Maximilian%2520L%25C3%25B6ffler%2520and%2520Fabian%2520Bamberg%2520and%2520Bene%2520Wiestler%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Daniel%2520Rueckert%2520and%2520Jan%2520Stefan%2520Kirschke%26entry.1292438233%3D%2520%2520Objectives%253A%2520To%2520present%2520a%2520publicly%2520available%2520torso%2520segmentation%2520network%2520for%250Alarge%2520epidemiology%2520datasets%2520on%2520volumetric%2520interpolated%2520breath-hold%2520examination%250A%2528VIBE%2529%2520images.%2520Materials%2520%2526%2520Methods%253A%2520We%2520extracted%2520preliminary%2520segmentations%2520from%250ATotalSegmentator%252C%2520spine%252C%2520and%2520body%2520composition%2520networks%2520for%2520VIBE%2520images%252C%2520then%250Aimproved%2520them%2520iteratively%2520and%2520retrained%2520a%2520nnUNet%2520network.%2520Using%2520subsets%2520of%2520NAKO%250A%252885%2520subjects%2529%2520and%2520UK%2520Biobank%2520%252816%2520subjects%2529%252C%2520we%2520evaluated%2520with%2520Dice-score%2520on%2520a%250Aholdout%2520set%2520%252812%2520subjects%2529%2520and%2520existing%2520organ%2520segmentation%2520approach%2520%25281000%250Asubjects%2529%252C%2520generating%252071%2520semantic%2520segmentation%2520types%2520for%2520VIBE%2520images.%2520We%250Aprovide%2520an%2520additional%2520network%2520for%2520the%2520vertebra%2520segments%252022%2520individual%2520vertebra%250Atypes.%2520Results%253A%2520We%2520achieved%2520an%2520average%2520Dice%2520score%2520of%25200.89%2520%252B-%25200.07%2520overall%252071%250Asegmentation%2520labels.%2520We%2520scored%2520%253E%25200.90%2520Dice-score%2520on%2520the%2520abdominal%2520organs%2520except%250Afor%2520the%2520pancreas%2520with%2520a%2520Dice%2520of%25200.70.%2520Conclusion%253A%2520Our%2520work%2520offers%2520a%2520detailed%250Aand%2520refined%2520publicly%2520available%2520full%2520torso%2520segmentation%2520on%2520VIBE%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TotalVibeSegmentator%3A%20Full%20Torso%20Segmentation%20for%20the%20NAKO%20and%20UK%0A%20%20Biobank%20in%20Volumetric%20Interpolated%20Breath-hold%20Examination%20Body%20Images&entry.906535625=Robert%20Graf%20and%20Paul-S%C3%B6ren%20Platzek%20and%20Evamaria%20Olga%20Riedel%20and%20Constanze%20Ramsch%C3%BCtz%20and%20Sophie%20Starck%20and%20Hendrik%20Kristian%20M%C3%B6ller%20and%20Matan%20Atad%20and%20Henry%20V%C3%B6lzke%20and%20Robin%20B%C3%BClow%20and%20Carsten%20Oliver%20Schmidt%20and%20Julia%20R%C3%BCdebusch%20and%20Matthias%20Jung%20and%20Marco%20Reisert%20and%20Jakob%20Weiss%20and%20Maximilian%20L%C3%B6ffler%20and%20Fabian%20Bamberg%20and%20Bene%20Wiestler%20and%20Johannes%20C.%20Paetzold%20and%20Daniel%20Rueckert%20and%20Jan%20Stefan%20Kirschke&entry.1292438233=%20%20Objectives%3A%20To%20present%20a%20publicly%20available%20torso%20segmentation%20network%20for%0Alarge%20epidemiology%20datasets%20on%20volumetric%20interpolated%20breath-hold%20examination%0A%28VIBE%29%20images.%20Materials%20%26%20Methods%3A%20We%20extracted%20preliminary%20segmentations%20from%0ATotalSegmentator%2C%20spine%2C%20and%20body%20composition%20networks%20for%20VIBE%20images%2C%20then%0Aimproved%20them%20iteratively%20and%20retrained%20a%20nnUNet%20network.%20Using%20subsets%20of%20NAKO%0A%2885%20subjects%29%20and%20UK%20Biobank%20%2816%20subjects%29%2C%20we%20evaluated%20with%20Dice-score%20on%20a%0Aholdout%20set%20%2812%20subjects%29%20and%20existing%20organ%20segmentation%20approach%20%281000%0Asubjects%29%2C%20generating%2071%20semantic%20segmentation%20types%20for%20VIBE%20images.%20We%0Aprovide%20an%20additional%20network%20for%20the%20vertebra%20segments%2022%20individual%20vertebra%0Atypes.%20Results%3A%20We%20achieved%20an%20average%20Dice%20score%20of%200.89%20%2B-%200.07%20overall%2071%0Asegmentation%20labels.%20We%20scored%20%3E%200.90%20Dice-score%20on%20the%20abdominal%20organs%20except%0Afor%20the%20pancreas%20with%20a%20Dice%20of%200.70.%20Conclusion%3A%20Our%20work%20offers%20a%20detailed%0Aand%20refined%20publicly%20available%20full%20torso%20segmentation%20on%20VIBE%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00125v2&entry.124074799=Read"},
{"title": "UniGEM: A Unified Approach to Generation and Property Prediction for\n  Molecules", "author": "Shikun Feng and Yuyan Ni and Yan Lu and Zhi-Ming Ma and Wei-Ying Ma and Yanyan Lan", "abstract": "  Molecular generation and molecular property prediction are both crucial for\ndrug discovery, but they are often developed independently. Inspired by recent\nstudies, which demonstrate that diffusion model, a prominent generative\napproach, can learn meaningful data representations that enhance predictive\ntasks, we explore the potential for developing a unified generative model in\nthe molecular domain that effectively addresses both molecular generation and\nproperty prediction tasks. However, the integration of these tasks is\nchallenging due to inherent inconsistencies, making simple multi-task learning\nineffective. To address this, we propose UniGEM, the first unified model to\nsuccessfully integrate molecular generation and property prediction, delivering\nsuperior performance in both tasks. Our key innovation lies in a novel\ntwo-phase generative process, where predictive tasks are activated in the later\nstages, after the molecular scaffold is formed. We further enhance task balance\nthrough innovative training strategies. Rigorous theoretical analysis and\ncomprehensive experiments demonstrate our significant improvements in both\ntasks. The principles behind UniGEM hold promise for broader applications,\nincluding natural language processing and computer vision.\n", "link": "http://arxiv.org/abs/2410.10516v1", "date": "2024-10-14", "relevancy": 2.3535, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGEM%3A%20A%20Unified%20Approach%20to%20Generation%20and%20Property%20Prediction%20for%0A%20%20Molecules&body=Title%3A%20UniGEM%3A%20A%20Unified%20Approach%20to%20Generation%20and%20Property%20Prediction%20for%0A%20%20Molecules%0AAuthor%3A%20Shikun%20Feng%20and%20Yuyan%20Ni%20and%20Yan%20Lu%20and%20Zhi-Ming%20Ma%20and%20Wei-Ying%20Ma%20and%20Yanyan%20Lan%0AAbstract%3A%20%20%20Molecular%20generation%20and%20molecular%20property%20prediction%20are%20both%20crucial%20for%0Adrug%20discovery%2C%20but%20they%20are%20often%20developed%20independently.%20Inspired%20by%20recent%0Astudies%2C%20which%20demonstrate%20that%20diffusion%20model%2C%20a%20prominent%20generative%0Aapproach%2C%20can%20learn%20meaningful%20data%20representations%20that%20enhance%20predictive%0Atasks%2C%20we%20explore%20the%20potential%20for%20developing%20a%20unified%20generative%20model%20in%0Athe%20molecular%20domain%20that%20effectively%20addresses%20both%20molecular%20generation%20and%0Aproperty%20prediction%20tasks.%20However%2C%20the%20integration%20of%20these%20tasks%20is%0Achallenging%20due%20to%20inherent%20inconsistencies%2C%20making%20simple%20multi-task%20learning%0Aineffective.%20To%20address%20this%2C%20we%20propose%20UniGEM%2C%20the%20first%20unified%20model%20to%0Asuccessfully%20integrate%20molecular%20generation%20and%20property%20prediction%2C%20delivering%0Asuperior%20performance%20in%20both%20tasks.%20Our%20key%20innovation%20lies%20in%20a%20novel%0Atwo-phase%20generative%20process%2C%20where%20predictive%20tasks%20are%20activated%20in%20the%20later%0Astages%2C%20after%20the%20molecular%20scaffold%20is%20formed.%20We%20further%20enhance%20task%20balance%0Athrough%20innovative%20training%20strategies.%20Rigorous%20theoretical%20analysis%20and%0Acomprehensive%20experiments%20demonstrate%20our%20significant%20improvements%20in%20both%0Atasks.%20The%20principles%20behind%20UniGEM%20hold%20promise%20for%20broader%20applications%2C%0Aincluding%20natural%20language%20processing%20and%20computer%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGEM%253A%2520A%2520Unified%2520Approach%2520to%2520Generation%2520and%2520Property%2520Prediction%2520for%250A%2520%2520Molecules%26entry.906535625%3DShikun%2520Feng%2520and%2520Yuyan%2520Ni%2520and%2520Yan%2520Lu%2520and%2520Zhi-Ming%2520Ma%2520and%2520Wei-Ying%2520Ma%2520and%2520Yanyan%2520Lan%26entry.1292438233%3D%2520%2520Molecular%2520generation%2520and%2520molecular%2520property%2520prediction%2520are%2520both%2520crucial%2520for%250Adrug%2520discovery%252C%2520but%2520they%2520are%2520often%2520developed%2520independently.%2520Inspired%2520by%2520recent%250Astudies%252C%2520which%2520demonstrate%2520that%2520diffusion%2520model%252C%2520a%2520prominent%2520generative%250Aapproach%252C%2520can%2520learn%2520meaningful%2520data%2520representations%2520that%2520enhance%2520predictive%250Atasks%252C%2520we%2520explore%2520the%2520potential%2520for%2520developing%2520a%2520unified%2520generative%2520model%2520in%250Athe%2520molecular%2520domain%2520that%2520effectively%2520addresses%2520both%2520molecular%2520generation%2520and%250Aproperty%2520prediction%2520tasks.%2520However%252C%2520the%2520integration%2520of%2520these%2520tasks%2520is%250Achallenging%2520due%2520to%2520inherent%2520inconsistencies%252C%2520making%2520simple%2520multi-task%2520learning%250Aineffective.%2520To%2520address%2520this%252C%2520we%2520propose%2520UniGEM%252C%2520the%2520first%2520unified%2520model%2520to%250Asuccessfully%2520integrate%2520molecular%2520generation%2520and%2520property%2520prediction%252C%2520delivering%250Asuperior%2520performance%2520in%2520both%2520tasks.%2520Our%2520key%2520innovation%2520lies%2520in%2520a%2520novel%250Atwo-phase%2520generative%2520process%252C%2520where%2520predictive%2520tasks%2520are%2520activated%2520in%2520the%2520later%250Astages%252C%2520after%2520the%2520molecular%2520scaffold%2520is%2520formed.%2520We%2520further%2520enhance%2520task%2520balance%250Athrough%2520innovative%2520training%2520strategies.%2520Rigorous%2520theoretical%2520analysis%2520and%250Acomprehensive%2520experiments%2520demonstrate%2520our%2520significant%2520improvements%2520in%2520both%250Atasks.%2520The%2520principles%2520behind%2520UniGEM%2520hold%2520promise%2520for%2520broader%2520applications%252C%250Aincluding%2520natural%2520language%2520processing%2520and%2520computer%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGEM%3A%20A%20Unified%20Approach%20to%20Generation%20and%20Property%20Prediction%20for%0A%20%20Molecules&entry.906535625=Shikun%20Feng%20and%20Yuyan%20Ni%20and%20Yan%20Lu%20and%20Zhi-Ming%20Ma%20and%20Wei-Ying%20Ma%20and%20Yanyan%20Lan&entry.1292438233=%20%20Molecular%20generation%20and%20molecular%20property%20prediction%20are%20both%20crucial%20for%0Adrug%20discovery%2C%20but%20they%20are%20often%20developed%20independently.%20Inspired%20by%20recent%0Astudies%2C%20which%20demonstrate%20that%20diffusion%20model%2C%20a%20prominent%20generative%0Aapproach%2C%20can%20learn%20meaningful%20data%20representations%20that%20enhance%20predictive%0Atasks%2C%20we%20explore%20the%20potential%20for%20developing%20a%20unified%20generative%20model%20in%0Athe%20molecular%20domain%20that%20effectively%20addresses%20both%20molecular%20generation%20and%0Aproperty%20prediction%20tasks.%20However%2C%20the%20integration%20of%20these%20tasks%20is%0Achallenging%20due%20to%20inherent%20inconsistencies%2C%20making%20simple%20multi-task%20learning%0Aineffective.%20To%20address%20this%2C%20we%20propose%20UniGEM%2C%20the%20first%20unified%20model%20to%0Asuccessfully%20integrate%20molecular%20generation%20and%20property%20prediction%2C%20delivering%0Asuperior%20performance%20in%20both%20tasks.%20Our%20key%20innovation%20lies%20in%20a%20novel%0Atwo-phase%20generative%20process%2C%20where%20predictive%20tasks%20are%20activated%20in%20the%20later%0Astages%2C%20after%20the%20molecular%20scaffold%20is%20formed.%20We%20further%20enhance%20task%20balance%0Athrough%20innovative%20training%20strategies.%20Rigorous%20theoretical%20analysis%20and%0Acomprehensive%20experiments%20demonstrate%20our%20significant%20improvements%20in%20both%0Atasks.%20The%20principles%20behind%20UniGEM%20hold%20promise%20for%20broader%20applications%2C%0Aincluding%20natural%20language%20processing%20and%20computer%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10516v1&entry.124074799=Read"},
{"title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions", "author": "Tianwei Xiong and Yuqing Wang and Daquan Zhou and Zhijie Lin and Jiashi Feng and Xihui Liu", "abstract": "  The efficacy of video generation models heavily depends on the quality of\ntheir training datasets. Most previous video generation models are trained on\nshort video clips, while recently there has been increasing interest in\ntraining long video generation models directly on longer videos. However, the\nlack of such high-quality long videos impedes the advancement of long video\ngeneration. To promote research in long video generation, we desire a new\ndataset with four key features essential for training long video generation\nmodels: (1) long videos covering at least 10 seconds, (2) long-take videos\nwithout cuts, (3) large motion and diverse contents, and (4) temporally dense\ncaptions. To achieve this, we introduce a new pipeline for selecting\nhigh-quality long-take videos and generating temporally dense captions.\nSpecifically, we define a set of metrics to quantitatively assess video quality\nincluding scene cuts, dynamic degrees, and semantic-level quality, enabling us\nto filter high-quality long-take videos from a large amount of source videos.\nSubsequently, we develop a hierarchical video captioning pipeline to annotate\nlong videos with temporally-dense captions. With this pipeline, we curate the\nfirst long-take video dataset, LVD-2M, comprising 2 million long-take videos,\neach covering more than 10 seconds and annotated with temporally dense\ncaptions. We further validate the effectiveness of LVD-2M by fine-tuning video\ngeneration models to generate long videos with dynamic motions. We believe our\nwork will significantly contribute to future research in long video generation.\n", "link": "http://arxiv.org/abs/2410.10816v1", "date": "2024-10-14", "relevancy": 2.3468, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6133}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5821}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVD-2M%3A%20A%20Long-take%20Video%20Dataset%20with%20Temporally%20Dense%20Captions&body=Title%3A%20LVD-2M%3A%20A%20Long-take%20Video%20Dataset%20with%20Temporally%20Dense%20Captions%0AAuthor%3A%20Tianwei%20Xiong%20and%20Yuqing%20Wang%20and%20Daquan%20Zhou%20and%20Zhijie%20Lin%20and%20Jiashi%20Feng%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20The%20efficacy%20of%20video%20generation%20models%20heavily%20depends%20on%20the%20quality%20of%0Atheir%20training%20datasets.%20Most%20previous%20video%20generation%20models%20are%20trained%20on%0Ashort%20video%20clips%2C%20while%20recently%20there%20has%20been%20increasing%20interest%20in%0Atraining%20long%20video%20generation%20models%20directly%20on%20longer%20videos.%20However%2C%20the%0Alack%20of%20such%20high-quality%20long%20videos%20impedes%20the%20advancement%20of%20long%20video%0Ageneration.%20To%20promote%20research%20in%20long%20video%20generation%2C%20we%20desire%20a%20new%0Adataset%20with%20four%20key%20features%20essential%20for%20training%20long%20video%20generation%0Amodels%3A%20%281%29%20long%20videos%20covering%20at%20least%2010%20seconds%2C%20%282%29%20long-take%20videos%0Awithout%20cuts%2C%20%283%29%20large%20motion%20and%20diverse%20contents%2C%20and%20%284%29%20temporally%20dense%0Acaptions.%20To%20achieve%20this%2C%20we%20introduce%20a%20new%20pipeline%20for%20selecting%0Ahigh-quality%20long-take%20videos%20and%20generating%20temporally%20dense%20captions.%0ASpecifically%2C%20we%20define%20a%20set%20of%20metrics%20to%20quantitatively%20assess%20video%20quality%0Aincluding%20scene%20cuts%2C%20dynamic%20degrees%2C%20and%20semantic-level%20quality%2C%20enabling%20us%0Ato%20filter%20high-quality%20long-take%20videos%20from%20a%20large%20amount%20of%20source%20videos.%0ASubsequently%2C%20we%20develop%20a%20hierarchical%20video%20captioning%20pipeline%20to%20annotate%0Along%20videos%20with%20temporally-dense%20captions.%20With%20this%20pipeline%2C%20we%20curate%20the%0Afirst%20long-take%20video%20dataset%2C%20LVD-2M%2C%20comprising%202%20million%20long-take%20videos%2C%0Aeach%20covering%20more%20than%2010%20seconds%20and%20annotated%20with%20temporally%20dense%0Acaptions.%20We%20further%20validate%20the%20effectiveness%20of%20LVD-2M%20by%20fine-tuning%20video%0Ageneration%20models%20to%20generate%20long%20videos%20with%20dynamic%20motions.%20We%20believe%20our%0Awork%20will%20significantly%20contribute%20to%20future%20research%20in%20long%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVD-2M%253A%2520A%2520Long-take%2520Video%2520Dataset%2520with%2520Temporally%2520Dense%2520Captions%26entry.906535625%3DTianwei%2520Xiong%2520and%2520Yuqing%2520Wang%2520and%2520Daquan%2520Zhou%2520and%2520Zhijie%2520Lin%2520and%2520Jiashi%2520Feng%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520The%2520efficacy%2520of%2520video%2520generation%2520models%2520heavily%2520depends%2520on%2520the%2520quality%2520of%250Atheir%2520training%2520datasets.%2520Most%2520previous%2520video%2520generation%2520models%2520are%2520trained%2520on%250Ashort%2520video%2520clips%252C%2520while%2520recently%2520there%2520has%2520been%2520increasing%2520interest%2520in%250Atraining%2520long%2520video%2520generation%2520models%2520directly%2520on%2520longer%2520videos.%2520However%252C%2520the%250Alack%2520of%2520such%2520high-quality%2520long%2520videos%2520impedes%2520the%2520advancement%2520of%2520long%2520video%250Ageneration.%2520To%2520promote%2520research%2520in%2520long%2520video%2520generation%252C%2520we%2520desire%2520a%2520new%250Adataset%2520with%2520four%2520key%2520features%2520essential%2520for%2520training%2520long%2520video%2520generation%250Amodels%253A%2520%25281%2529%2520long%2520videos%2520covering%2520at%2520least%252010%2520seconds%252C%2520%25282%2529%2520long-take%2520videos%250Awithout%2520cuts%252C%2520%25283%2529%2520large%2520motion%2520and%2520diverse%2520contents%252C%2520and%2520%25284%2529%2520temporally%2520dense%250Acaptions.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520new%2520pipeline%2520for%2520selecting%250Ahigh-quality%2520long-take%2520videos%2520and%2520generating%2520temporally%2520dense%2520captions.%250ASpecifically%252C%2520we%2520define%2520a%2520set%2520of%2520metrics%2520to%2520quantitatively%2520assess%2520video%2520quality%250Aincluding%2520scene%2520cuts%252C%2520dynamic%2520degrees%252C%2520and%2520semantic-level%2520quality%252C%2520enabling%2520us%250Ato%2520filter%2520high-quality%2520long-take%2520videos%2520from%2520a%2520large%2520amount%2520of%2520source%2520videos.%250ASubsequently%252C%2520we%2520develop%2520a%2520hierarchical%2520video%2520captioning%2520pipeline%2520to%2520annotate%250Along%2520videos%2520with%2520temporally-dense%2520captions.%2520With%2520this%2520pipeline%252C%2520we%2520curate%2520the%250Afirst%2520long-take%2520video%2520dataset%252C%2520LVD-2M%252C%2520comprising%25202%2520million%2520long-take%2520videos%252C%250Aeach%2520covering%2520more%2520than%252010%2520seconds%2520and%2520annotated%2520with%2520temporally%2520dense%250Acaptions.%2520We%2520further%2520validate%2520the%2520effectiveness%2520of%2520LVD-2M%2520by%2520fine-tuning%2520video%250Ageneration%2520models%2520to%2520generate%2520long%2520videos%2520with%2520dynamic%2520motions.%2520We%2520believe%2520our%250Awork%2520will%2520significantly%2520contribute%2520to%2520future%2520research%2520in%2520long%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVD-2M%3A%20A%20Long-take%20Video%20Dataset%20with%20Temporally%20Dense%20Captions&entry.906535625=Tianwei%20Xiong%20and%20Yuqing%20Wang%20and%20Daquan%20Zhou%20and%20Zhijie%20Lin%20and%20Jiashi%20Feng%20and%20Xihui%20Liu&entry.1292438233=%20%20The%20efficacy%20of%20video%20generation%20models%20heavily%20depends%20on%20the%20quality%20of%0Atheir%20training%20datasets.%20Most%20previous%20video%20generation%20models%20are%20trained%20on%0Ashort%20video%20clips%2C%20while%20recently%20there%20has%20been%20increasing%20interest%20in%0Atraining%20long%20video%20generation%20models%20directly%20on%20longer%20videos.%20However%2C%20the%0Alack%20of%20such%20high-quality%20long%20videos%20impedes%20the%20advancement%20of%20long%20video%0Ageneration.%20To%20promote%20research%20in%20long%20video%20generation%2C%20we%20desire%20a%20new%0Adataset%20with%20four%20key%20features%20essential%20for%20training%20long%20video%20generation%0Amodels%3A%20%281%29%20long%20videos%20covering%20at%20least%2010%20seconds%2C%20%282%29%20long-take%20videos%0Awithout%20cuts%2C%20%283%29%20large%20motion%20and%20diverse%20contents%2C%20and%20%284%29%20temporally%20dense%0Acaptions.%20To%20achieve%20this%2C%20we%20introduce%20a%20new%20pipeline%20for%20selecting%0Ahigh-quality%20long-take%20videos%20and%20generating%20temporally%20dense%20captions.%0ASpecifically%2C%20we%20define%20a%20set%20of%20metrics%20to%20quantitatively%20assess%20video%20quality%0Aincluding%20scene%20cuts%2C%20dynamic%20degrees%2C%20and%20semantic-level%20quality%2C%20enabling%20us%0Ato%20filter%20high-quality%20long-take%20videos%20from%20a%20large%20amount%20of%20source%20videos.%0ASubsequently%2C%20we%20develop%20a%20hierarchical%20video%20captioning%20pipeline%20to%20annotate%0Along%20videos%20with%20temporally-dense%20captions.%20With%20this%20pipeline%2C%20we%20curate%20the%0Afirst%20long-take%20video%20dataset%2C%20LVD-2M%2C%20comprising%202%20million%20long-take%20videos%2C%0Aeach%20covering%20more%20than%2010%20seconds%20and%20annotated%20with%20temporally%20dense%0Acaptions.%20We%20further%20validate%20the%20effectiveness%20of%20LVD-2M%20by%20fine-tuning%20video%0Ageneration%20models%20to%20generate%20long%20videos%20with%20dynamic%20motions.%20We%20believe%20our%0Awork%20will%20significantly%20contribute%20to%20future%20research%20in%20long%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10816v1&entry.124074799=Read"},
{"title": "A Novel No-Reference Image Quality Metric For Assessing Sharpness In\n  Satellite Imagery", "author": "Lucas Gonzalo Antonel", "abstract": "  This study introduces a novel no-reference image quality metric aimed at\nassessing image sharpness. Designed to be robust against variations in noise,\nexposure, contrast, and image content, it measures the normalized decay rate of\ngradients along pronounced edges, offering an objective method for sharpness\nevaluation without reference images. Primarily developed for satellite imagery\nto align with human visual perception of sharpness, this metric supports\nmonitoring and quality characterization of satellite fleets. It demonstrates\nsignificant utility and superior performance in consistency with human\nperception across various image types and operational conditions. Unlike\nconventional metrics, this heuristic approach provides a way to score images\nfrom lower to higher sharpness, making it a reliable and versatile tool for\nenhancing quality assessment processes without the need for pristine or ground\ntruth comparison. Additionally, this metric is computationally efficient\ncompared to deep learning analysis, ensuring faster and more resource-effective\nsharpness evaluations.\n", "link": "http://arxiv.org/abs/2410.10488v1", "date": "2024-10-14", "relevancy": 2.3425, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.485}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.464}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20No-Reference%20Image%20Quality%20Metric%20For%20Assessing%20Sharpness%20In%0A%20%20Satellite%20Imagery&body=Title%3A%20A%20Novel%20No-Reference%20Image%20Quality%20Metric%20For%20Assessing%20Sharpness%20In%0A%20%20Satellite%20Imagery%0AAuthor%3A%20Lucas%20Gonzalo%20Antonel%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20no-reference%20image%20quality%20metric%20aimed%20at%0Aassessing%20image%20sharpness.%20Designed%20to%20be%20robust%20against%20variations%20in%20noise%2C%0Aexposure%2C%20contrast%2C%20and%20image%20content%2C%20it%20measures%20the%20normalized%20decay%20rate%20of%0Agradients%20along%20pronounced%20edges%2C%20offering%20an%20objective%20method%20for%20sharpness%0Aevaluation%20without%20reference%20images.%20Primarily%20developed%20for%20satellite%20imagery%0Ato%20align%20with%20human%20visual%20perception%20of%20sharpness%2C%20this%20metric%20supports%0Amonitoring%20and%20quality%20characterization%20of%20satellite%20fleets.%20It%20demonstrates%0Asignificant%20utility%20and%20superior%20performance%20in%20consistency%20with%20human%0Aperception%20across%20various%20image%20types%20and%20operational%20conditions.%20Unlike%0Aconventional%20metrics%2C%20this%20heuristic%20approach%20provides%20a%20way%20to%20score%20images%0Afrom%20lower%20to%20higher%20sharpness%2C%20making%20it%20a%20reliable%20and%20versatile%20tool%20for%0Aenhancing%20quality%20assessment%20processes%20without%20the%20need%20for%20pristine%20or%20ground%0Atruth%20comparison.%20Additionally%2C%20this%20metric%20is%20computationally%20efficient%0Acompared%20to%20deep%20learning%20analysis%2C%20ensuring%20faster%20and%20more%20resource-effective%0Asharpness%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520No-Reference%2520Image%2520Quality%2520Metric%2520For%2520Assessing%2520Sharpness%2520In%250A%2520%2520Satellite%2520Imagery%26entry.906535625%3DLucas%2520Gonzalo%2520Antonel%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520no-reference%2520image%2520quality%2520metric%2520aimed%2520at%250Aassessing%2520image%2520sharpness.%2520Designed%2520to%2520be%2520robust%2520against%2520variations%2520in%2520noise%252C%250Aexposure%252C%2520contrast%252C%2520and%2520image%2520content%252C%2520it%2520measures%2520the%2520normalized%2520decay%2520rate%2520of%250Agradients%2520along%2520pronounced%2520edges%252C%2520offering%2520an%2520objective%2520method%2520for%2520sharpness%250Aevaluation%2520without%2520reference%2520images.%2520Primarily%2520developed%2520for%2520satellite%2520imagery%250Ato%2520align%2520with%2520human%2520visual%2520perception%2520of%2520sharpness%252C%2520this%2520metric%2520supports%250Amonitoring%2520and%2520quality%2520characterization%2520of%2520satellite%2520fleets.%2520It%2520demonstrates%250Asignificant%2520utility%2520and%2520superior%2520performance%2520in%2520consistency%2520with%2520human%250Aperception%2520across%2520various%2520image%2520types%2520and%2520operational%2520conditions.%2520Unlike%250Aconventional%2520metrics%252C%2520this%2520heuristic%2520approach%2520provides%2520a%2520way%2520to%2520score%2520images%250Afrom%2520lower%2520to%2520higher%2520sharpness%252C%2520making%2520it%2520a%2520reliable%2520and%2520versatile%2520tool%2520for%250Aenhancing%2520quality%2520assessment%2520processes%2520without%2520the%2520need%2520for%2520pristine%2520or%2520ground%250Atruth%2520comparison.%2520Additionally%252C%2520this%2520metric%2520is%2520computationally%2520efficient%250Acompared%2520to%2520deep%2520learning%2520analysis%252C%2520ensuring%2520faster%2520and%2520more%2520resource-effective%250Asharpness%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20No-Reference%20Image%20Quality%20Metric%20For%20Assessing%20Sharpness%20In%0A%20%20Satellite%20Imagery&entry.906535625=Lucas%20Gonzalo%20Antonel&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20no-reference%20image%20quality%20metric%20aimed%20at%0Aassessing%20image%20sharpness.%20Designed%20to%20be%20robust%20against%20variations%20in%20noise%2C%0Aexposure%2C%20contrast%2C%20and%20image%20content%2C%20it%20measures%20the%20normalized%20decay%20rate%20of%0Agradients%20along%20pronounced%20edges%2C%20offering%20an%20objective%20method%20for%20sharpness%0Aevaluation%20without%20reference%20images.%20Primarily%20developed%20for%20satellite%20imagery%0Ato%20align%20with%20human%20visual%20perception%20of%20sharpness%2C%20this%20metric%20supports%0Amonitoring%20and%20quality%20characterization%20of%20satellite%20fleets.%20It%20demonstrates%0Asignificant%20utility%20and%20superior%20performance%20in%20consistency%20with%20human%0Aperception%20across%20various%20image%20types%20and%20operational%20conditions.%20Unlike%0Aconventional%20metrics%2C%20this%20heuristic%20approach%20provides%20a%20way%20to%20score%20images%0Afrom%20lower%20to%20higher%20sharpness%2C%20making%20it%20a%20reliable%20and%20versatile%20tool%20for%0Aenhancing%20quality%20assessment%20processes%20without%20the%20need%20for%20pristine%20or%20ground%0Atruth%20comparison.%20Additionally%2C%20this%20metric%20is%20computationally%20efficient%0Acompared%20to%20deep%20learning%20analysis%2C%20ensuring%20faster%20and%20more%20resource-effective%0Asharpness%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10488v1&entry.124074799=Read"},
{"title": "When Does Perceptual Alignment Benefit Vision Representations?", "author": "Shobhita Sundaram and Stephanie Fu and Lukas Muttenthaler and Netanel Y. Tamir and Lucy Chai and Simon Kornblith and Trevor Darrell and Phillip Isola", "abstract": "  Humans judge perceptual similarity according to diverse visual attributes,\nincluding scene layout, subject location, and camera pose. Existing vision\nmodels understand a wide range of semantic abstractions but improperly weigh\nthese attributes and thus make inferences misaligned with human perception.\nWhile vision representations have previously benefited from alignment in\ncontexts like image generation, the utility of perceptually aligned\nrepresentations in more general-purpose settings remains unclear. Here, we\ninvestigate how aligning vision model representations to human perceptual\njudgments impacts their usability across diverse computer vision tasks. We\nfinetune state-of-the-art models on human similarity judgments for image\ntriplets and evaluate them across standard vision benchmarks. We find that\naligning models to perceptual judgments yields representations that improve\nupon the original backbones across many downstream tasks, including counting,\nsegmentation, depth estimation, instance retrieval, and retrieval-augmented\ngeneration. In addition, we find that performance is widely preserved on other\ntasks, including specialized out-of-distribution domains such as in medical\nimaging and 3D environment frames. Our results suggest that injecting an\ninductive bias about human perceptual knowledge into vision models can\ncontribute to better representations.\n", "link": "http://arxiv.org/abs/2410.10817v1", "date": "2024-10-14", "relevancy": 2.3409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Perceptual%20Alignment%20Benefit%20Vision%20Representations%3F&body=Title%3A%20When%20Does%20Perceptual%20Alignment%20Benefit%20Vision%20Representations%3F%0AAuthor%3A%20Shobhita%20Sundaram%20and%20Stephanie%20Fu%20and%20Lukas%20Muttenthaler%20and%20Netanel%20Y.%20Tamir%20and%20Lucy%20Chai%20and%20Simon%20Kornblith%20and%20Trevor%20Darrell%20and%20Phillip%20Isola%0AAbstract%3A%20%20%20Humans%20judge%20perceptual%20similarity%20according%20to%20diverse%20visual%20attributes%2C%0Aincluding%20scene%20layout%2C%20subject%20location%2C%20and%20camera%20pose.%20Existing%20vision%0Amodels%20understand%20a%20wide%20range%20of%20semantic%20abstractions%20but%20improperly%20weigh%0Athese%20attributes%20and%20thus%20make%20inferences%20misaligned%20with%20human%20perception.%0AWhile%20vision%20representations%20have%20previously%20benefited%20from%20alignment%20in%0Acontexts%20like%20image%20generation%2C%20the%20utility%20of%20perceptually%20aligned%0Arepresentations%20in%20more%20general-purpose%20settings%20remains%20unclear.%20Here%2C%20we%0Ainvestigate%20how%20aligning%20vision%20model%20representations%20to%20human%20perceptual%0Ajudgments%20impacts%20their%20usability%20across%20diverse%20computer%20vision%20tasks.%20We%0Afinetune%20state-of-the-art%20models%20on%20human%20similarity%20judgments%20for%20image%0Atriplets%20and%20evaluate%20them%20across%20standard%20vision%20benchmarks.%20We%20find%20that%0Aaligning%20models%20to%20perceptual%20judgments%20yields%20representations%20that%20improve%0Aupon%20the%20original%20backbones%20across%20many%20downstream%20tasks%2C%20including%20counting%2C%0Asegmentation%2C%20depth%20estimation%2C%20instance%20retrieval%2C%20and%20retrieval-augmented%0Ageneration.%20In%20addition%2C%20we%20find%20that%20performance%20is%20widely%20preserved%20on%20other%0Atasks%2C%20including%20specialized%20out-of-distribution%20domains%20such%20as%20in%20medical%0Aimaging%20and%203D%20environment%20frames.%20Our%20results%20suggest%20that%20injecting%20an%0Ainductive%20bias%20about%20human%20perceptual%20knowledge%20into%20vision%20models%20can%0Acontribute%20to%20better%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Perceptual%2520Alignment%2520Benefit%2520Vision%2520Representations%253F%26entry.906535625%3DShobhita%2520Sundaram%2520and%2520Stephanie%2520Fu%2520and%2520Lukas%2520Muttenthaler%2520and%2520Netanel%2520Y.%2520Tamir%2520and%2520Lucy%2520Chai%2520and%2520Simon%2520Kornblith%2520and%2520Trevor%2520Darrell%2520and%2520Phillip%2520Isola%26entry.1292438233%3D%2520%2520Humans%2520judge%2520perceptual%2520similarity%2520according%2520to%2520diverse%2520visual%2520attributes%252C%250Aincluding%2520scene%2520layout%252C%2520subject%2520location%252C%2520and%2520camera%2520pose.%2520Existing%2520vision%250Amodels%2520understand%2520a%2520wide%2520range%2520of%2520semantic%2520abstractions%2520but%2520improperly%2520weigh%250Athese%2520attributes%2520and%2520thus%2520make%2520inferences%2520misaligned%2520with%2520human%2520perception.%250AWhile%2520vision%2520representations%2520have%2520previously%2520benefited%2520from%2520alignment%2520in%250Acontexts%2520like%2520image%2520generation%252C%2520the%2520utility%2520of%2520perceptually%2520aligned%250Arepresentations%2520in%2520more%2520general-purpose%2520settings%2520remains%2520unclear.%2520Here%252C%2520we%250Ainvestigate%2520how%2520aligning%2520vision%2520model%2520representations%2520to%2520human%2520perceptual%250Ajudgments%2520impacts%2520their%2520usability%2520across%2520diverse%2520computer%2520vision%2520tasks.%2520We%250Afinetune%2520state-of-the-art%2520models%2520on%2520human%2520similarity%2520judgments%2520for%2520image%250Atriplets%2520and%2520evaluate%2520them%2520across%2520standard%2520vision%2520benchmarks.%2520We%2520find%2520that%250Aaligning%2520models%2520to%2520perceptual%2520judgments%2520yields%2520representations%2520that%2520improve%250Aupon%2520the%2520original%2520backbones%2520across%2520many%2520downstream%2520tasks%252C%2520including%2520counting%252C%250Asegmentation%252C%2520depth%2520estimation%252C%2520instance%2520retrieval%252C%2520and%2520retrieval-augmented%250Ageneration.%2520In%2520addition%252C%2520we%2520find%2520that%2520performance%2520is%2520widely%2520preserved%2520on%2520other%250Atasks%252C%2520including%2520specialized%2520out-of-distribution%2520domains%2520such%2520as%2520in%2520medical%250Aimaging%2520and%25203D%2520environment%2520frames.%2520Our%2520results%2520suggest%2520that%2520injecting%2520an%250Ainductive%2520bias%2520about%2520human%2520perceptual%2520knowledge%2520into%2520vision%2520models%2520can%250Acontribute%2520to%2520better%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Perceptual%20Alignment%20Benefit%20Vision%20Representations%3F&entry.906535625=Shobhita%20Sundaram%20and%20Stephanie%20Fu%20and%20Lukas%20Muttenthaler%20and%20Netanel%20Y.%20Tamir%20and%20Lucy%20Chai%20and%20Simon%20Kornblith%20and%20Trevor%20Darrell%20and%20Phillip%20Isola&entry.1292438233=%20%20Humans%20judge%20perceptual%20similarity%20according%20to%20diverse%20visual%20attributes%2C%0Aincluding%20scene%20layout%2C%20subject%20location%2C%20and%20camera%20pose.%20Existing%20vision%0Amodels%20understand%20a%20wide%20range%20of%20semantic%20abstractions%20but%20improperly%20weigh%0Athese%20attributes%20and%20thus%20make%20inferences%20misaligned%20with%20human%20perception.%0AWhile%20vision%20representations%20have%20previously%20benefited%20from%20alignment%20in%0Acontexts%20like%20image%20generation%2C%20the%20utility%20of%20perceptually%20aligned%0Arepresentations%20in%20more%20general-purpose%20settings%20remains%20unclear.%20Here%2C%20we%0Ainvestigate%20how%20aligning%20vision%20model%20representations%20to%20human%20perceptual%0Ajudgments%20impacts%20their%20usability%20across%20diverse%20computer%20vision%20tasks.%20We%0Afinetune%20state-of-the-art%20models%20on%20human%20similarity%20judgments%20for%20image%0Atriplets%20and%20evaluate%20them%20across%20standard%20vision%20benchmarks.%20We%20find%20that%0Aaligning%20models%20to%20perceptual%20judgments%20yields%20representations%20that%20improve%0Aupon%20the%20original%20backbones%20across%20many%20downstream%20tasks%2C%20including%20counting%2C%0Asegmentation%2C%20depth%20estimation%2C%20instance%20retrieval%2C%20and%20retrieval-augmented%0Ageneration.%20In%20addition%2C%20we%20find%20that%20performance%20is%20widely%20preserved%20on%20other%0Atasks%2C%20including%20specialized%20out-of-distribution%20domains%20such%20as%20in%20medical%0Aimaging%20and%203D%20environment%20frames.%20Our%20results%20suggest%20that%20injecting%20an%0Ainductive%20bias%20about%20human%20perceptual%20knowledge%20into%20vision%20models%20can%0Acontribute%20to%20better%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10817v1&entry.124074799=Read"},
{"title": "Navigation under uncertainty: Trajectory prediction and occlusion\n  reasoning with switching dynamical systems", "author": "Ran Wei and Joseph Lee and Shohei Wakayama and Alexander Tschantz and Conor Heins and Christopher Buckley and John Carenbauer and Hari Thiruvengada and Mahault Albarracin and Miguel de Prado and Petter Horling and Peter Winzell and Renjith Rajagopal", "abstract": "  Predicting future trajectories of nearby objects, especially under occlusion,\nis a crucial task in autonomous driving and safe robot navigation. Prior works\ntypically neglect to maintain uncertainty about occluded objects and only\npredict trajectories of observed objects using high-capacity models such as\nTransformers trained on large datasets. While these approaches are effective in\nstandard scenarios, they can struggle to generalize to the long-tail,\nsafety-critical scenarios. In this work, we explore a conceptual framework\nunifying trajectory prediction and occlusion reasoning under the same class of\nstructured probabilistic generative model, namely, switching dynamical systems.\nWe then present some initial experiments illustrating its capabilities using\nthe Waymo open dataset.\n", "link": "http://arxiv.org/abs/2410.10653v1", "date": "2024-10-14", "relevancy": 2.327, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6191}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5839}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigation%20under%20uncertainty%3A%20Trajectory%20prediction%20and%20occlusion%0A%20%20reasoning%20with%20switching%20dynamical%20systems&body=Title%3A%20Navigation%20under%20uncertainty%3A%20Trajectory%20prediction%20and%20occlusion%0A%20%20reasoning%20with%20switching%20dynamical%20systems%0AAuthor%3A%20Ran%20Wei%20and%20Joseph%20Lee%20and%20Shohei%20Wakayama%20and%20Alexander%20Tschantz%20and%20Conor%20Heins%20and%20Christopher%20Buckley%20and%20John%20Carenbauer%20and%20Hari%20Thiruvengada%20and%20Mahault%20Albarracin%20and%20Miguel%20de%20Prado%20and%20Petter%20Horling%20and%20Peter%20Winzell%20and%20Renjith%20Rajagopal%0AAbstract%3A%20%20%20Predicting%20future%20trajectories%20of%20nearby%20objects%2C%20especially%20under%20occlusion%2C%0Ais%20a%20crucial%20task%20in%20autonomous%20driving%20and%20safe%20robot%20navigation.%20Prior%20works%0Atypically%20neglect%20to%20maintain%20uncertainty%20about%20occluded%20objects%20and%20only%0Apredict%20trajectories%20of%20observed%20objects%20using%20high-capacity%20models%20such%20as%0ATransformers%20trained%20on%20large%20datasets.%20While%20these%20approaches%20are%20effective%20in%0Astandard%20scenarios%2C%20they%20can%20struggle%20to%20generalize%20to%20the%20long-tail%2C%0Asafety-critical%20scenarios.%20In%20this%20work%2C%20we%20explore%20a%20conceptual%20framework%0Aunifying%20trajectory%20prediction%20and%20occlusion%20reasoning%20under%20the%20same%20class%20of%0Astructured%20probabilistic%20generative%20model%2C%20namely%2C%20switching%20dynamical%20systems.%0AWe%20then%20present%20some%20initial%20experiments%20illustrating%20its%20capabilities%20using%0Athe%20Waymo%20open%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigation%2520under%2520uncertainty%253A%2520Trajectory%2520prediction%2520and%2520occlusion%250A%2520%2520reasoning%2520with%2520switching%2520dynamical%2520systems%26entry.906535625%3DRan%2520Wei%2520and%2520Joseph%2520Lee%2520and%2520Shohei%2520Wakayama%2520and%2520Alexander%2520Tschantz%2520and%2520Conor%2520Heins%2520and%2520Christopher%2520Buckley%2520and%2520John%2520Carenbauer%2520and%2520Hari%2520Thiruvengada%2520and%2520Mahault%2520Albarracin%2520and%2520Miguel%2520de%2520Prado%2520and%2520Petter%2520Horling%2520and%2520Peter%2520Winzell%2520and%2520Renjith%2520Rajagopal%26entry.1292438233%3D%2520%2520Predicting%2520future%2520trajectories%2520of%2520nearby%2520objects%252C%2520especially%2520under%2520occlusion%252C%250Ais%2520a%2520crucial%2520task%2520in%2520autonomous%2520driving%2520and%2520safe%2520robot%2520navigation.%2520Prior%2520works%250Atypically%2520neglect%2520to%2520maintain%2520uncertainty%2520about%2520occluded%2520objects%2520and%2520only%250Apredict%2520trajectories%2520of%2520observed%2520objects%2520using%2520high-capacity%2520models%2520such%2520as%250ATransformers%2520trained%2520on%2520large%2520datasets.%2520While%2520these%2520approaches%2520are%2520effective%2520in%250Astandard%2520scenarios%252C%2520they%2520can%2520struggle%2520to%2520generalize%2520to%2520the%2520long-tail%252C%250Asafety-critical%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520conceptual%2520framework%250Aunifying%2520trajectory%2520prediction%2520and%2520occlusion%2520reasoning%2520under%2520the%2520same%2520class%2520of%250Astructured%2520probabilistic%2520generative%2520model%252C%2520namely%252C%2520switching%2520dynamical%2520systems.%250AWe%2520then%2520present%2520some%2520initial%2520experiments%2520illustrating%2520its%2520capabilities%2520using%250Athe%2520Waymo%2520open%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigation%20under%20uncertainty%3A%20Trajectory%20prediction%20and%20occlusion%0A%20%20reasoning%20with%20switching%20dynamical%20systems&entry.906535625=Ran%20Wei%20and%20Joseph%20Lee%20and%20Shohei%20Wakayama%20and%20Alexander%20Tschantz%20and%20Conor%20Heins%20and%20Christopher%20Buckley%20and%20John%20Carenbauer%20and%20Hari%20Thiruvengada%20and%20Mahault%20Albarracin%20and%20Miguel%20de%20Prado%20and%20Petter%20Horling%20and%20Peter%20Winzell%20and%20Renjith%20Rajagopal&entry.1292438233=%20%20Predicting%20future%20trajectories%20of%20nearby%20objects%2C%20especially%20under%20occlusion%2C%0Ais%20a%20crucial%20task%20in%20autonomous%20driving%20and%20safe%20robot%20navigation.%20Prior%20works%0Atypically%20neglect%20to%20maintain%20uncertainty%20about%20occluded%20objects%20and%20only%0Apredict%20trajectories%20of%20observed%20objects%20using%20high-capacity%20models%20such%20as%0ATransformers%20trained%20on%20large%20datasets.%20While%20these%20approaches%20are%20effective%20in%0Astandard%20scenarios%2C%20they%20can%20struggle%20to%20generalize%20to%20the%20long-tail%2C%0Asafety-critical%20scenarios.%20In%20this%20work%2C%20we%20explore%20a%20conceptual%20framework%0Aunifying%20trajectory%20prediction%20and%20occlusion%20reasoning%20under%20the%20same%20class%20of%0Astructured%20probabilistic%20generative%20model%2C%20namely%2C%20switching%20dynamical%20systems.%0AWe%20then%20present%20some%20initial%20experiments%20illustrating%20its%20capabilities%20using%0Athe%20Waymo%20open%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10653v1&entry.124074799=Read"},
{"title": "Building a Multivariate Time Series Benchmarking Datasets Inspired by\n  Natural Language Processing (NLP)", "author": "Mohammad Asif Ibna Mustafa and Ferdinand Heinrich", "abstract": "  Time series analysis has become increasingly important in various domains,\nand developing effective models relies heavily on high-quality benchmark\ndatasets. Inspired by the success of Natural Language Processing (NLP)\nbenchmark datasets in advancing pre-trained models, we propose a new approach\nto create a comprehensive benchmark dataset for time series analysis. This\npaper explores the methodologies used in NLP benchmark dataset creation and\nadapts them to the unique challenges of time series data. We discuss the\nprocess of curating diverse, representative, and challenging time series\ndatasets, highlighting the importance of domain relevance and data complexity.\nAdditionally, we investigate multi-task learning strategies that leverage the\nbenchmark dataset to enhance the performance of time series models. This\nresearch contributes to the broader goal of advancing the state-of-the-art in\ntime series modeling by adopting successful strategies from the NLP domain.\n", "link": "http://arxiv.org/abs/2410.10687v1", "date": "2024-10-14", "relevancy": 2.3264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20a%20Multivariate%20Time%20Series%20Benchmarking%20Datasets%20Inspired%20by%0A%20%20Natural%20Language%20Processing%20%28NLP%29&body=Title%3A%20Building%20a%20Multivariate%20Time%20Series%20Benchmarking%20Datasets%20Inspired%20by%0A%20%20Natural%20Language%20Processing%20%28NLP%29%0AAuthor%3A%20Mohammad%20Asif%20Ibna%20Mustafa%20and%20Ferdinand%20Heinrich%0AAbstract%3A%20%20%20Time%20series%20analysis%20has%20become%20increasingly%20important%20in%20various%20domains%2C%0Aand%20developing%20effective%20models%20relies%20heavily%20on%20high-quality%20benchmark%0Adatasets.%20Inspired%20by%20the%20success%20of%20Natural%20Language%20Processing%20%28NLP%29%0Abenchmark%20datasets%20in%20advancing%20pre-trained%20models%2C%20we%20propose%20a%20new%20approach%0Ato%20create%20a%20comprehensive%20benchmark%20dataset%20for%20time%20series%20analysis.%20This%0Apaper%20explores%20the%20methodologies%20used%20in%20NLP%20benchmark%20dataset%20creation%20and%0Aadapts%20them%20to%20the%20unique%20challenges%20of%20time%20series%20data.%20We%20discuss%20the%0Aprocess%20of%20curating%20diverse%2C%20representative%2C%20and%20challenging%20time%20series%0Adatasets%2C%20highlighting%20the%20importance%20of%20domain%20relevance%20and%20data%20complexity.%0AAdditionally%2C%20we%20investigate%20multi-task%20learning%20strategies%20that%20leverage%20the%0Abenchmark%20dataset%20to%20enhance%20the%20performance%20of%20time%20series%20models.%20This%0Aresearch%20contributes%20to%20the%20broader%20goal%20of%20advancing%20the%20state-of-the-art%20in%0Atime%20series%20modeling%20by%20adopting%20successful%20strategies%20from%20the%20NLP%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520a%2520Multivariate%2520Time%2520Series%2520Benchmarking%2520Datasets%2520Inspired%2520by%250A%2520%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%26entry.906535625%3DMohammad%2520Asif%2520Ibna%2520Mustafa%2520and%2520Ferdinand%2520Heinrich%26entry.1292438233%3D%2520%2520Time%2520series%2520analysis%2520has%2520become%2520increasingly%2520important%2520in%2520various%2520domains%252C%250Aand%2520developing%2520effective%2520models%2520relies%2520heavily%2520on%2520high-quality%2520benchmark%250Adatasets.%2520Inspired%2520by%2520the%2520success%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%250Abenchmark%2520datasets%2520in%2520advancing%2520pre-trained%2520models%252C%2520we%2520propose%2520a%2520new%2520approach%250Ato%2520create%2520a%2520comprehensive%2520benchmark%2520dataset%2520for%2520time%2520series%2520analysis.%2520This%250Apaper%2520explores%2520the%2520methodologies%2520used%2520in%2520NLP%2520benchmark%2520dataset%2520creation%2520and%250Aadapts%2520them%2520to%2520the%2520unique%2520challenges%2520of%2520time%2520series%2520data.%2520We%2520discuss%2520the%250Aprocess%2520of%2520curating%2520diverse%252C%2520representative%252C%2520and%2520challenging%2520time%2520series%250Adatasets%252C%2520highlighting%2520the%2520importance%2520of%2520domain%2520relevance%2520and%2520data%2520complexity.%250AAdditionally%252C%2520we%2520investigate%2520multi-task%2520learning%2520strategies%2520that%2520leverage%2520the%250Abenchmark%2520dataset%2520to%2520enhance%2520the%2520performance%2520of%2520time%2520series%2520models.%2520This%250Aresearch%2520contributes%2520to%2520the%2520broader%2520goal%2520of%2520advancing%2520the%2520state-of-the-art%2520in%250Atime%2520series%2520modeling%2520by%2520adopting%2520successful%2520strategies%2520from%2520the%2520NLP%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20a%20Multivariate%20Time%20Series%20Benchmarking%20Datasets%20Inspired%20by%0A%20%20Natural%20Language%20Processing%20%28NLP%29&entry.906535625=Mohammad%20Asif%20Ibna%20Mustafa%20and%20Ferdinand%20Heinrich&entry.1292438233=%20%20Time%20series%20analysis%20has%20become%20increasingly%20important%20in%20various%20domains%2C%0Aand%20developing%20effective%20models%20relies%20heavily%20on%20high-quality%20benchmark%0Adatasets.%20Inspired%20by%20the%20success%20of%20Natural%20Language%20Processing%20%28NLP%29%0Abenchmark%20datasets%20in%20advancing%20pre-trained%20models%2C%20we%20propose%20a%20new%20approach%0Ato%20create%20a%20comprehensive%20benchmark%20dataset%20for%20time%20series%20analysis.%20This%0Apaper%20explores%20the%20methodologies%20used%20in%20NLP%20benchmark%20dataset%20creation%20and%0Aadapts%20them%20to%20the%20unique%20challenges%20of%20time%20series%20data.%20We%20discuss%20the%0Aprocess%20of%20curating%20diverse%2C%20representative%2C%20and%20challenging%20time%20series%0Adatasets%2C%20highlighting%20the%20importance%20of%20domain%20relevance%20and%20data%20complexity.%0AAdditionally%2C%20we%20investigate%20multi-task%20learning%20strategies%20that%20leverage%20the%0Abenchmark%20dataset%20to%20enhance%20the%20performance%20of%20time%20series%20models.%20This%0Aresearch%20contributes%20to%20the%20broader%20goal%20of%20advancing%20the%20state-of-the-art%20in%0Atime%20series%20modeling%20by%20adopting%20successful%20strategies%20from%20the%20NLP%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10687v1&entry.124074799=Read"},
{"title": "Thinking LLMs: General Instruction Following with Thought Generation", "author": "Tianhao Wu and Janice Lan and Weizhe Yuan and Jiantao Jiao and Jason Weston and Sainbayar Sukhbaatar", "abstract": "  LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.\n", "link": "http://arxiv.org/abs/2410.10630v1", "date": "2024-10-14", "relevancy": 2.3198, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20LLMs%3A%20General%20Instruction%20Following%20with%20Thought%20Generation&body=Title%3A%20Thinking%20LLMs%3A%20General%20Instruction%20Following%20with%20Thought%20Generation%0AAuthor%3A%20Tianhao%20Wu%20and%20Janice%20Lan%20and%20Weizhe%20Yuan%20and%20Jiantao%20Jiao%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar%0AAbstract%3A%20%20%20LLMs%20are%20typically%20trained%20to%20answer%20user%20questions%20or%20follow%20instructions%0Asimilarly%20to%20how%20human%20experts%20respond.%20However%2C%20in%20the%20standard%20alignment%0Aframework%20they%20lack%20the%20basic%20ability%20of%20explicit%20thinking%20before%20answering.%0AThinking%20is%20important%20for%20complex%20questions%20that%20require%20reasoning%20and%20planning%0A--%20but%20can%20be%20applied%20to%20any%20task.%20We%20propose%20a%20training%20method%20for%20equipping%0Aexisting%20LLMs%20with%20such%20thinking%20abilities%20for%20general%20instruction%20following%0Awithout%20use%20of%20additional%20human%20data.%20We%20achieve%20this%20by%20an%20iterative%20search%0Aand%20optimization%20procedure%20that%20explores%20the%20space%20of%20possible%20thought%0Agenerations%2C%20allowing%20the%20model%20to%20learn%20how%20to%20think%20without%20direct%0Asupervision.%20For%20each%20instruction%2C%20the%20thought%20candidates%20are%20scored%20using%20a%0Ajudge%20model%20to%20evaluate%20their%20responses%20only%2C%20and%20then%20optimized%20via%20preference%0Aoptimization.%20We%20show%20that%20this%20procedure%20leads%20to%20superior%20performance%20on%0AAlpacaEval%20and%20Arena-Hard%2C%20and%20shows%20gains%20from%20thinking%20on%20non-reasoning%0Acategories%20such%20as%20marketing%2C%20health%20and%20general%20knowledge%2C%20in%20addition%20to%20more%0Atraditional%20reasoning%20%26%20problem-solving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520LLMs%253A%2520General%2520Instruction%2520Following%2520with%2520Thought%2520Generation%26entry.906535625%3DTianhao%2520Wu%2520and%2520Janice%2520Lan%2520and%2520Weizhe%2520Yuan%2520and%2520Jiantao%2520Jiao%2520and%2520Jason%2520Weston%2520and%2520Sainbayar%2520Sukhbaatar%26entry.1292438233%3D%2520%2520LLMs%2520are%2520typically%2520trained%2520to%2520answer%2520user%2520questions%2520or%2520follow%2520instructions%250Asimilarly%2520to%2520how%2520human%2520experts%2520respond.%2520However%252C%2520in%2520the%2520standard%2520alignment%250Aframework%2520they%2520lack%2520the%2520basic%2520ability%2520of%2520explicit%2520thinking%2520before%2520answering.%250AThinking%2520is%2520important%2520for%2520complex%2520questions%2520that%2520require%2520reasoning%2520and%2520planning%250A--%2520but%2520can%2520be%2520applied%2520to%2520any%2520task.%2520We%2520propose%2520a%2520training%2520method%2520for%2520equipping%250Aexisting%2520LLMs%2520with%2520such%2520thinking%2520abilities%2520for%2520general%2520instruction%2520following%250Awithout%2520use%2520of%2520additional%2520human%2520data.%2520We%2520achieve%2520this%2520by%2520an%2520iterative%2520search%250Aand%2520optimization%2520procedure%2520that%2520explores%2520the%2520space%2520of%2520possible%2520thought%250Agenerations%252C%2520allowing%2520the%2520model%2520to%2520learn%2520how%2520to%2520think%2520without%2520direct%250Asupervision.%2520For%2520each%2520instruction%252C%2520the%2520thought%2520candidates%2520are%2520scored%2520using%2520a%250Ajudge%2520model%2520to%2520evaluate%2520their%2520responses%2520only%252C%2520and%2520then%2520optimized%2520via%2520preference%250Aoptimization.%2520We%2520show%2520that%2520this%2520procedure%2520leads%2520to%2520superior%2520performance%2520on%250AAlpacaEval%2520and%2520Arena-Hard%252C%2520and%2520shows%2520gains%2520from%2520thinking%2520on%2520non-reasoning%250Acategories%2520such%2520as%2520marketing%252C%2520health%2520and%2520general%2520knowledge%252C%2520in%2520addition%2520to%2520more%250Atraditional%2520reasoning%2520%2526%2520problem-solving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20LLMs%3A%20General%20Instruction%20Following%20with%20Thought%20Generation&entry.906535625=Tianhao%20Wu%20and%20Janice%20Lan%20and%20Weizhe%20Yuan%20and%20Jiantao%20Jiao%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar&entry.1292438233=%20%20LLMs%20are%20typically%20trained%20to%20answer%20user%20questions%20or%20follow%20instructions%0Asimilarly%20to%20how%20human%20experts%20respond.%20However%2C%20in%20the%20standard%20alignment%0Aframework%20they%20lack%20the%20basic%20ability%20of%20explicit%20thinking%20before%20answering.%0AThinking%20is%20important%20for%20complex%20questions%20that%20require%20reasoning%20and%20planning%0A--%20but%20can%20be%20applied%20to%20any%20task.%20We%20propose%20a%20training%20method%20for%20equipping%0Aexisting%20LLMs%20with%20such%20thinking%20abilities%20for%20general%20instruction%20following%0Awithout%20use%20of%20additional%20human%20data.%20We%20achieve%20this%20by%20an%20iterative%20search%0Aand%20optimization%20procedure%20that%20explores%20the%20space%20of%20possible%20thought%0Agenerations%2C%20allowing%20the%20model%20to%20learn%20how%20to%20think%20without%20direct%0Asupervision.%20For%20each%20instruction%2C%20the%20thought%20candidates%20are%20scored%20using%20a%0Ajudge%20model%20to%20evaluate%20their%20responses%20only%2C%20and%20then%20optimized%20via%20preference%0Aoptimization.%20We%20show%20that%20this%20procedure%20leads%20to%20superior%20performance%20on%0AAlpacaEval%20and%20Arena-Hard%2C%20and%20shows%20gains%20from%20thinking%20on%20non-reasoning%0Acategories%20such%20as%20marketing%2C%20health%20and%20general%20knowledge%2C%20in%20addition%20to%20more%0Atraditional%20reasoning%20%26%20problem-solving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10630v1&entry.124074799=Read"},
{"title": "Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D\n  Scenes", "author": "Jianqi Chen and Panwen Hu and Xiaojun Chang and Zhenwei Shi and Michael Christian Kampffmeyer and Xiaodan Liang", "abstract": "  Recent advancements in human motion synthesis have focused on specific types\nof motions, such as human-scene interaction, locomotion or human-human\ninteraction, however, there is a lack of a unified system capable of generating\na diverse combination of motion types. In response, we introduce\nSitcom-Crafter, a comprehensive and extendable system for human motion\ngeneration in 3D space, which can be guided by extensive plot contexts to\nenhance workflow efficiency for anime and game designers. The system is\ncomprised of eight modules, three of which are dedicated to motion generation,\nwhile the remaining five are augmentation modules that ensure consistent fusion\nof motion sequences and system functionality. Central to the generation modules\nis our novel 3D scene-aware human-human interaction module, which addresses\ncollision issues by synthesizing implicit 3D Signed Distance Function (SDF)\npoints around motion spaces, thereby minimizing human-scene collisions without\nadditional data collection costs. Complementing this, our locomotion and\nhuman-scene interaction modules leverage existing methods to enrich the\nsystem's motion generation capabilities. Augmentation modules encompass plot\ncomprehension for command generation, motion synchronization for seamless\nintegration of different motion types, hand pose retrieval to enhance motion\nrealism, motion collision revision to prevent human collisions, and 3D\nretargeting to ensure visual fidelity. Experimental evaluations validate the\nsystem's ability to generate high-quality, diverse, and physically realistic\nmotions, underscoring its potential for advancing creative workflows.\n", "link": "http://arxiv.org/abs/2410.10790v1", "date": "2024-10-14", "relevancy": 2.2859, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.58}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes&body=Title%3A%20Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes%0AAuthor%3A%20Jianqi%20Chen%20and%20Panwen%20Hu%20and%20Xiaojun%20Chang%20and%20Zhenwei%20Shi%20and%20Michael%20Christian%20Kampffmeyer%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20human%20motion%20synthesis%20have%20focused%20on%20specific%20types%0Aof%20motions%2C%20such%20as%20human-scene%20interaction%2C%20locomotion%20or%20human-human%0Ainteraction%2C%20however%2C%20there%20is%20a%20lack%20of%20a%20unified%20system%20capable%20of%20generating%0Aa%20diverse%20combination%20of%20motion%20types.%20In%20response%2C%20we%20introduce%0ASitcom-Crafter%2C%20a%20comprehensive%20and%20extendable%20system%20for%20human%20motion%0Ageneration%20in%203D%20space%2C%20which%20can%20be%20guided%20by%20extensive%20plot%20contexts%20to%0Aenhance%20workflow%20efficiency%20for%20anime%20and%20game%20designers.%20The%20system%20is%0Acomprised%20of%20eight%20modules%2C%20three%20of%20which%20are%20dedicated%20to%20motion%20generation%2C%0Awhile%20the%20remaining%20five%20are%20augmentation%20modules%20that%20ensure%20consistent%20fusion%0Aof%20motion%20sequences%20and%20system%20functionality.%20Central%20to%20the%20generation%20modules%0Ais%20our%20novel%203D%20scene-aware%20human-human%20interaction%20module%2C%20which%20addresses%0Acollision%20issues%20by%20synthesizing%20implicit%203D%20Signed%20Distance%20Function%20%28SDF%29%0Apoints%20around%20motion%20spaces%2C%20thereby%20minimizing%20human-scene%20collisions%20without%0Aadditional%20data%20collection%20costs.%20Complementing%20this%2C%20our%20locomotion%20and%0Ahuman-scene%20interaction%20modules%20leverage%20existing%20methods%20to%20enrich%20the%0Asystem%27s%20motion%20generation%20capabilities.%20Augmentation%20modules%20encompass%20plot%0Acomprehension%20for%20command%20generation%2C%20motion%20synchronization%20for%20seamless%0Aintegration%20of%20different%20motion%20types%2C%20hand%20pose%20retrieval%20to%20enhance%20motion%0Arealism%2C%20motion%20collision%20revision%20to%20prevent%20human%20collisions%2C%20and%203D%0Aretargeting%20to%20ensure%20visual%20fidelity.%20Experimental%20evaluations%20validate%20the%0Asystem%27s%20ability%20to%20generate%20high-quality%2C%20diverse%2C%20and%20physically%20realistic%0Amotions%2C%20underscoring%20its%20potential%20for%20advancing%20creative%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSitcom-Crafter%253A%2520A%2520Plot-Driven%2520Human%2520Motion%2520Generation%2520System%2520in%25203D%250A%2520%2520Scenes%26entry.906535625%3DJianqi%2520Chen%2520and%2520Panwen%2520Hu%2520and%2520Xiaojun%2520Chang%2520and%2520Zhenwei%2520Shi%2520and%2520Michael%2520Christian%2520Kampffmeyer%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520human%2520motion%2520synthesis%2520have%2520focused%2520on%2520specific%2520types%250Aof%2520motions%252C%2520such%2520as%2520human-scene%2520interaction%252C%2520locomotion%2520or%2520human-human%250Ainteraction%252C%2520however%252C%2520there%2520is%2520a%2520lack%2520of%2520a%2520unified%2520system%2520capable%2520of%2520generating%250Aa%2520diverse%2520combination%2520of%2520motion%2520types.%2520In%2520response%252C%2520we%2520introduce%250ASitcom-Crafter%252C%2520a%2520comprehensive%2520and%2520extendable%2520system%2520for%2520human%2520motion%250Ageneration%2520in%25203D%2520space%252C%2520which%2520can%2520be%2520guided%2520by%2520extensive%2520plot%2520contexts%2520to%250Aenhance%2520workflow%2520efficiency%2520for%2520anime%2520and%2520game%2520designers.%2520The%2520system%2520is%250Acomprised%2520of%2520eight%2520modules%252C%2520three%2520of%2520which%2520are%2520dedicated%2520to%2520motion%2520generation%252C%250Awhile%2520the%2520remaining%2520five%2520are%2520augmentation%2520modules%2520that%2520ensure%2520consistent%2520fusion%250Aof%2520motion%2520sequences%2520and%2520system%2520functionality.%2520Central%2520to%2520the%2520generation%2520modules%250Ais%2520our%2520novel%25203D%2520scene-aware%2520human-human%2520interaction%2520module%252C%2520which%2520addresses%250Acollision%2520issues%2520by%2520synthesizing%2520implicit%25203D%2520Signed%2520Distance%2520Function%2520%2528SDF%2529%250Apoints%2520around%2520motion%2520spaces%252C%2520thereby%2520minimizing%2520human-scene%2520collisions%2520without%250Aadditional%2520data%2520collection%2520costs.%2520Complementing%2520this%252C%2520our%2520locomotion%2520and%250Ahuman-scene%2520interaction%2520modules%2520leverage%2520existing%2520methods%2520to%2520enrich%2520the%250Asystem%2527s%2520motion%2520generation%2520capabilities.%2520Augmentation%2520modules%2520encompass%2520plot%250Acomprehension%2520for%2520command%2520generation%252C%2520motion%2520synchronization%2520for%2520seamless%250Aintegration%2520of%2520different%2520motion%2520types%252C%2520hand%2520pose%2520retrieval%2520to%2520enhance%2520motion%250Arealism%252C%2520motion%2520collision%2520revision%2520to%2520prevent%2520human%2520collisions%252C%2520and%25203D%250Aretargeting%2520to%2520ensure%2520visual%2520fidelity.%2520Experimental%2520evaluations%2520validate%2520the%250Asystem%2527s%2520ability%2520to%2520generate%2520high-quality%252C%2520diverse%252C%2520and%2520physically%2520realistic%250Amotions%252C%2520underscoring%2520its%2520potential%2520for%2520advancing%2520creative%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes&entry.906535625=Jianqi%20Chen%20and%20Panwen%20Hu%20and%20Xiaojun%20Chang%20and%20Zhenwei%20Shi%20and%20Michael%20Christian%20Kampffmeyer%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Recent%20advancements%20in%20human%20motion%20synthesis%20have%20focused%20on%20specific%20types%0Aof%20motions%2C%20such%20as%20human-scene%20interaction%2C%20locomotion%20or%20human-human%0Ainteraction%2C%20however%2C%20there%20is%20a%20lack%20of%20a%20unified%20system%20capable%20of%20generating%0Aa%20diverse%20combination%20of%20motion%20types.%20In%20response%2C%20we%20introduce%0ASitcom-Crafter%2C%20a%20comprehensive%20and%20extendable%20system%20for%20human%20motion%0Ageneration%20in%203D%20space%2C%20which%20can%20be%20guided%20by%20extensive%20plot%20contexts%20to%0Aenhance%20workflow%20efficiency%20for%20anime%20and%20game%20designers.%20The%20system%20is%0Acomprised%20of%20eight%20modules%2C%20three%20of%20which%20are%20dedicated%20to%20motion%20generation%2C%0Awhile%20the%20remaining%20five%20are%20augmentation%20modules%20that%20ensure%20consistent%20fusion%0Aof%20motion%20sequences%20and%20system%20functionality.%20Central%20to%20the%20generation%20modules%0Ais%20our%20novel%203D%20scene-aware%20human-human%20interaction%20module%2C%20which%20addresses%0Acollision%20issues%20by%20synthesizing%20implicit%203D%20Signed%20Distance%20Function%20%28SDF%29%0Apoints%20around%20motion%20spaces%2C%20thereby%20minimizing%20human-scene%20collisions%20without%0Aadditional%20data%20collection%20costs.%20Complementing%20this%2C%20our%20locomotion%20and%0Ahuman-scene%20interaction%20modules%20leverage%20existing%20methods%20to%20enrich%20the%0Asystem%27s%20motion%20generation%20capabilities.%20Augmentation%20modules%20encompass%20plot%0Acomprehension%20for%20command%20generation%2C%20motion%20synchronization%20for%20seamless%0Aintegration%20of%20different%20motion%20types%2C%20hand%20pose%20retrieval%20to%20enhance%20motion%0Arealism%2C%20motion%20collision%20revision%20to%20prevent%20human%20collisions%2C%20and%203D%0Aretargeting%20to%20ensure%20visual%20fidelity.%20Experimental%20evaluations%20validate%20the%0Asystem%27s%20ability%20to%20generate%20high-quality%2C%20diverse%2C%20and%20physically%20realistic%0Amotions%2C%20underscoring%20its%20potential%20for%20advancing%20creative%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10790v1&entry.124074799=Read"},
{"title": "MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous\n  Driving", "author": "Xiyang Wang and Shouzheng Qi and Jieyou Zhao and Hangning Zhou and Siyu Zhang and Guoan Wang and Kai Tu and Songlin Guo and Jianbo Zhao and Jian Li and Mu Yang", "abstract": "  This paper introduces MCTrack, a new 3D multi-object tracking method that\nachieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo\ndatasets. Addressing the gap in existing tracking paradigms, which often\nperform well on specific datasets but lack generalizability, MCTrack offers a\nunified solution. Additionally, we have standardized the format of perceptual\nresults across various datasets, termed BaseVersion, facilitating researchers\nin the field of multi-object tracking (MOT) to concentrate on the core\nalgorithmic development without the undue burden of data preprocessing.\nFinally, recognizing the limitations of current evaluation metrics, we propose\na novel set that assesses motion information output, such as velocity and\nacceleration, crucial for downstream tasks. The source codes of the proposed\nmethod are available at this link:\nhttps://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack\n", "link": "http://arxiv.org/abs/2409.16149v2", "date": "2024-10-14", "relevancy": 2.2772, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCTrack%3A%20A%20Unified%203D%20Multi-Object%20Tracking%20Framework%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20MCTrack%3A%20A%20Unified%203D%20Multi-Object%20Tracking%20Framework%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Xiyang%20Wang%20and%20Shouzheng%20Qi%20and%20Jieyou%20Zhao%20and%20Hangning%20Zhou%20and%20Siyu%20Zhang%20and%20Guoan%20Wang%20and%20Kai%20Tu%20and%20Songlin%20Guo%20and%20Jianbo%20Zhao%20and%20Jian%20Li%20and%20Mu%20Yang%0AAbstract%3A%20%20%20This%20paper%20introduces%20MCTrack%2C%20a%20new%203D%20multi-object%20tracking%20method%20that%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20KITTI%2C%20nuScenes%2C%20and%20Waymo%0Adatasets.%20Addressing%20the%20gap%20in%20existing%20tracking%20paradigms%2C%20which%20often%0Aperform%20well%20on%20specific%20datasets%20but%20lack%20generalizability%2C%20MCTrack%20offers%20a%0Aunified%20solution.%20Additionally%2C%20we%20have%20standardized%20the%20format%20of%20perceptual%0Aresults%20across%20various%20datasets%2C%20termed%20BaseVersion%2C%20facilitating%20researchers%0Ain%20the%20field%20of%20multi-object%20tracking%20%28MOT%29%20to%20concentrate%20on%20the%20core%0Aalgorithmic%20development%20without%20the%20undue%20burden%20of%20data%20preprocessing.%0AFinally%2C%20recognizing%20the%20limitations%20of%20current%20evaluation%20metrics%2C%20we%20propose%0Aa%20novel%20set%20that%20assesses%20motion%20information%20output%2C%20such%20as%20velocity%20and%0Aacceleration%2C%20crucial%20for%20downstream%20tasks.%20The%20source%20codes%20of%20the%20proposed%0Amethod%20are%20available%20at%20this%20link%3A%0Ahttps%3A//github.com/megvii-research/MCTrack%7D%7Bhttps%3A//github.com/megvii-research/MCTrack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCTrack%253A%2520A%2520Unified%25203D%2520Multi-Object%2520Tracking%2520Framework%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DXiyang%2520Wang%2520and%2520Shouzheng%2520Qi%2520and%2520Jieyou%2520Zhao%2520and%2520Hangning%2520Zhou%2520and%2520Siyu%2520Zhang%2520and%2520Guoan%2520Wang%2520and%2520Kai%2520Tu%2520and%2520Songlin%2520Guo%2520and%2520Jianbo%2520Zhao%2520and%2520Jian%2520Li%2520and%2520Mu%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MCTrack%252C%2520a%2520new%25203D%2520multi-object%2520tracking%2520method%2520that%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520KITTI%252C%2520nuScenes%252C%2520and%2520Waymo%250Adatasets.%2520Addressing%2520the%2520gap%2520in%2520existing%2520tracking%2520paradigms%252C%2520which%2520often%250Aperform%2520well%2520on%2520specific%2520datasets%2520but%2520lack%2520generalizability%252C%2520MCTrack%2520offers%2520a%250Aunified%2520solution.%2520Additionally%252C%2520we%2520have%2520standardized%2520the%2520format%2520of%2520perceptual%250Aresults%2520across%2520various%2520datasets%252C%2520termed%2520BaseVersion%252C%2520facilitating%2520researchers%250Ain%2520the%2520field%2520of%2520multi-object%2520tracking%2520%2528MOT%2529%2520to%2520concentrate%2520on%2520the%2520core%250Aalgorithmic%2520development%2520without%2520the%2520undue%2520burden%2520of%2520data%2520preprocessing.%250AFinally%252C%2520recognizing%2520the%2520limitations%2520of%2520current%2520evaluation%2520metrics%252C%2520we%2520propose%250Aa%2520novel%2520set%2520that%2520assesses%2520motion%2520information%2520output%252C%2520such%2520as%2520velocity%2520and%250Aacceleration%252C%2520crucial%2520for%2520downstream%2520tasks.%2520The%2520source%2520codes%2520of%2520the%2520proposed%250Amethod%2520are%2520available%2520at%2520this%2520link%253A%250Ahttps%253A//github.com/megvii-research/MCTrack%257D%257Bhttps%253A//github.com/megvii-research/MCTrack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCTrack%3A%20A%20Unified%203D%20Multi-Object%20Tracking%20Framework%20for%20Autonomous%0A%20%20Driving&entry.906535625=Xiyang%20Wang%20and%20Shouzheng%20Qi%20and%20Jieyou%20Zhao%20and%20Hangning%20Zhou%20and%20Siyu%20Zhang%20and%20Guoan%20Wang%20and%20Kai%20Tu%20and%20Songlin%20Guo%20and%20Jianbo%20Zhao%20and%20Jian%20Li%20and%20Mu%20Yang&entry.1292438233=%20%20This%20paper%20introduces%20MCTrack%2C%20a%20new%203D%20multi-object%20tracking%20method%20that%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20KITTI%2C%20nuScenes%2C%20and%20Waymo%0Adatasets.%20Addressing%20the%20gap%20in%20existing%20tracking%20paradigms%2C%20which%20often%0Aperform%20well%20on%20specific%20datasets%20but%20lack%20generalizability%2C%20MCTrack%20offers%20a%0Aunified%20solution.%20Additionally%2C%20we%20have%20standardized%20the%20format%20of%20perceptual%0Aresults%20across%20various%20datasets%2C%20termed%20BaseVersion%2C%20facilitating%20researchers%0Ain%20the%20field%20of%20multi-object%20tracking%20%28MOT%29%20to%20concentrate%20on%20the%20core%0Aalgorithmic%20development%20without%20the%20undue%20burden%20of%20data%20preprocessing.%0AFinally%2C%20recognizing%20the%20limitations%20of%20current%20evaluation%20metrics%2C%20we%20propose%0Aa%20novel%20set%20that%20assesses%20motion%20information%20output%2C%20such%20as%20velocity%20and%0Aacceleration%2C%20crucial%20for%20downstream%20tasks.%20The%20source%20codes%20of%20the%20proposed%0Amethod%20are%20available%20at%20this%20link%3A%0Ahttps%3A//github.com/megvii-research/MCTrack%7D%7Bhttps%3A//github.com/megvii-research/MCTrack%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16149v2&entry.124074799=Read"},
{"title": "Self-Distilled Depth Refinement with Noisy Poisson Fusion", "author": "Jiaqi Li and Yiran Wang and Jinghong Zheng and Zihao Huang and Ke Xian and Zhiguo Cao and Jianming Zhang", "abstract": "  Depth refinement aims to infer high-resolution depth with fine-grained edges\nand details, refining low-resolution results of depth estimation models. The\nprevailing methods adopt tile-based manners by merging numerous patches, which\nlacks efficiency and produces inconsistency. Besides, prior arts suffer from\nfuzzy depth boundaries and limited generalizability. Analyzing the fundamental\nreasons for these limitations, we model depth refinement as a noisy Poisson\nfusion problem with local inconsistency and edge deformation noises. We propose\nthe Self-distilled Depth Refinement (SDDR) framework to enforce robustness\nagainst the noises, which mainly consists of depth edge representation and\nedge-based guidance. With noisy depth predictions as input, SDDR generates\nlow-noise depth edge representations as pseudo-labels by coarse-to-fine\nself-distillation. Edge-based guidance with edge-guided gradient loss and\nedge-based fusion loss serves as the optimization objective equivalent to\nPoisson fusion. When depth maps are better refined, the labels also become more\nnoise-free. Our model can acquire strong robustness to the noises, achieving\nsignificant improvements in accuracy, edge quality, efficiency, and\ngeneralizability on five different benchmarks. Moreover, directly training\nanother model with edge labels produced by SDDR brings improvements, suggesting\nthat our method could help with training robust refinement models in future\nworks.\n", "link": "http://arxiv.org/abs/2409.17880v2", "date": "2024-10-14", "relevancy": 2.275, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5748}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5655}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion&body=Title%3A%20Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion%0AAuthor%3A%20Jiaqi%20Li%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Zihao%20Huang%20and%20Ke%20Xian%20and%20Zhiguo%20Cao%20and%20Jianming%20Zhang%0AAbstract%3A%20%20%20Depth%20refinement%20aims%20to%20infer%20high-resolution%20depth%20with%20fine-grained%20edges%0Aand%20details%2C%20refining%20low-resolution%20results%20of%20depth%20estimation%20models.%20The%0Aprevailing%20methods%20adopt%20tile-based%20manners%20by%20merging%20numerous%20patches%2C%20which%0Alacks%20efficiency%20and%20produces%20inconsistency.%20Besides%2C%20prior%20arts%20suffer%20from%0Afuzzy%20depth%20boundaries%20and%20limited%20generalizability.%20Analyzing%20the%20fundamental%0Areasons%20for%20these%20limitations%2C%20we%20model%20depth%20refinement%20as%20a%20noisy%20Poisson%0Afusion%20problem%20with%20local%20inconsistency%20and%20edge%20deformation%20noises.%20We%20propose%0Athe%20Self-distilled%20Depth%20Refinement%20%28SDDR%29%20framework%20to%20enforce%20robustness%0Aagainst%20the%20noises%2C%20which%20mainly%20consists%20of%20depth%20edge%20representation%20and%0Aedge-based%20guidance.%20With%20noisy%20depth%20predictions%20as%20input%2C%20SDDR%20generates%0Alow-noise%20depth%20edge%20representations%20as%20pseudo-labels%20by%20coarse-to-fine%0Aself-distillation.%20Edge-based%20guidance%20with%20edge-guided%20gradient%20loss%20and%0Aedge-based%20fusion%20loss%20serves%20as%20the%20optimization%20objective%20equivalent%20to%0APoisson%20fusion.%20When%20depth%20maps%20are%20better%20refined%2C%20the%20labels%20also%20become%20more%0Anoise-free.%20Our%20model%20can%20acquire%20strong%20robustness%20to%20the%20noises%2C%20achieving%0Asignificant%20improvements%20in%20accuracy%2C%20edge%20quality%2C%20efficiency%2C%20and%0Ageneralizability%20on%20five%20different%20benchmarks.%20Moreover%2C%20directly%20training%0Aanother%20model%20with%20edge%20labels%20produced%20by%20SDDR%20brings%20improvements%2C%20suggesting%0Athat%20our%20method%20could%20help%20with%20training%20robust%20refinement%20models%20in%20future%0Aworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Distilled%2520Depth%2520Refinement%2520with%2520Noisy%2520Poisson%2520Fusion%26entry.906535625%3DJiaqi%2520Li%2520and%2520Yiran%2520Wang%2520and%2520Jinghong%2520Zheng%2520and%2520Zihao%2520Huang%2520and%2520Ke%2520Xian%2520and%2520Zhiguo%2520Cao%2520and%2520Jianming%2520Zhang%26entry.1292438233%3D%2520%2520Depth%2520refinement%2520aims%2520to%2520infer%2520high-resolution%2520depth%2520with%2520fine-grained%2520edges%250Aand%2520details%252C%2520refining%2520low-resolution%2520results%2520of%2520depth%2520estimation%2520models.%2520The%250Aprevailing%2520methods%2520adopt%2520tile-based%2520manners%2520by%2520merging%2520numerous%2520patches%252C%2520which%250Alacks%2520efficiency%2520and%2520produces%2520inconsistency.%2520Besides%252C%2520prior%2520arts%2520suffer%2520from%250Afuzzy%2520depth%2520boundaries%2520and%2520limited%2520generalizability.%2520Analyzing%2520the%2520fundamental%250Areasons%2520for%2520these%2520limitations%252C%2520we%2520model%2520depth%2520refinement%2520as%2520a%2520noisy%2520Poisson%250Afusion%2520problem%2520with%2520local%2520inconsistency%2520and%2520edge%2520deformation%2520noises.%2520We%2520propose%250Athe%2520Self-distilled%2520Depth%2520Refinement%2520%2528SDDR%2529%2520framework%2520to%2520enforce%2520robustness%250Aagainst%2520the%2520noises%252C%2520which%2520mainly%2520consists%2520of%2520depth%2520edge%2520representation%2520and%250Aedge-based%2520guidance.%2520With%2520noisy%2520depth%2520predictions%2520as%2520input%252C%2520SDDR%2520generates%250Alow-noise%2520depth%2520edge%2520representations%2520as%2520pseudo-labels%2520by%2520coarse-to-fine%250Aself-distillation.%2520Edge-based%2520guidance%2520with%2520edge-guided%2520gradient%2520loss%2520and%250Aedge-based%2520fusion%2520loss%2520serves%2520as%2520the%2520optimization%2520objective%2520equivalent%2520to%250APoisson%2520fusion.%2520When%2520depth%2520maps%2520are%2520better%2520refined%252C%2520the%2520labels%2520also%2520become%2520more%250Anoise-free.%2520Our%2520model%2520can%2520acquire%2520strong%2520robustness%2520to%2520the%2520noises%252C%2520achieving%250Asignificant%2520improvements%2520in%2520accuracy%252C%2520edge%2520quality%252C%2520efficiency%252C%2520and%250Ageneralizability%2520on%2520five%2520different%2520benchmarks.%2520Moreover%252C%2520directly%2520training%250Aanother%2520model%2520with%2520edge%2520labels%2520produced%2520by%2520SDDR%2520brings%2520improvements%252C%2520suggesting%250Athat%2520our%2520method%2520could%2520help%2520with%2520training%2520robust%2520refinement%2520models%2520in%2520future%250Aworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Distilled%20Depth%20Refinement%20with%20Noisy%20Poisson%20Fusion&entry.906535625=Jiaqi%20Li%20and%20Yiran%20Wang%20and%20Jinghong%20Zheng%20and%20Zihao%20Huang%20and%20Ke%20Xian%20and%20Zhiguo%20Cao%20and%20Jianming%20Zhang&entry.1292438233=%20%20Depth%20refinement%20aims%20to%20infer%20high-resolution%20depth%20with%20fine-grained%20edges%0Aand%20details%2C%20refining%20low-resolution%20results%20of%20depth%20estimation%20models.%20The%0Aprevailing%20methods%20adopt%20tile-based%20manners%20by%20merging%20numerous%20patches%2C%20which%0Alacks%20efficiency%20and%20produces%20inconsistency.%20Besides%2C%20prior%20arts%20suffer%20from%0Afuzzy%20depth%20boundaries%20and%20limited%20generalizability.%20Analyzing%20the%20fundamental%0Areasons%20for%20these%20limitations%2C%20we%20model%20depth%20refinement%20as%20a%20noisy%20Poisson%0Afusion%20problem%20with%20local%20inconsistency%20and%20edge%20deformation%20noises.%20We%20propose%0Athe%20Self-distilled%20Depth%20Refinement%20%28SDDR%29%20framework%20to%20enforce%20robustness%0Aagainst%20the%20noises%2C%20which%20mainly%20consists%20of%20depth%20edge%20representation%20and%0Aedge-based%20guidance.%20With%20noisy%20depth%20predictions%20as%20input%2C%20SDDR%20generates%0Alow-noise%20depth%20edge%20representations%20as%20pseudo-labels%20by%20coarse-to-fine%0Aself-distillation.%20Edge-based%20guidance%20with%20edge-guided%20gradient%20loss%20and%0Aedge-based%20fusion%20loss%20serves%20as%20the%20optimization%20objective%20equivalent%20to%0APoisson%20fusion.%20When%20depth%20maps%20are%20better%20refined%2C%20the%20labels%20also%20become%20more%0Anoise-free.%20Our%20model%20can%20acquire%20strong%20robustness%20to%20the%20noises%2C%20achieving%0Asignificant%20improvements%20in%20accuracy%2C%20edge%20quality%2C%20efficiency%2C%20and%0Ageneralizability%20on%20five%20different%20benchmarks.%20Moreover%2C%20directly%20training%0Aanother%20model%20with%20edge%20labels%20produced%20by%20SDDR%20brings%20improvements%2C%20suggesting%0Athat%20our%20method%20could%20help%20with%20training%20robust%20refinement%20models%20in%20future%0Aworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17880v2&entry.124074799=Read"},
{"title": "SMART-TRACK: A Novel Kalman Filter-Guided Sensor Fusion For Robust UAV\n  Object Tracking in Dynamic Environments", "author": "Khaled Gabr and Mohamed Abdelkader and Imen Jarraya and Abdullah AlMusalami and Anis Koubaa", "abstract": "  In the field of sensor fusion and state estimation for object detection and\nlocalization, ensuring accurate tracking in dynamic environments poses\nsignificant challenges. Traditional methods like the Kalman Filter (KF) often\nfail when measurements are intermittent, leading to rapid divergence in state\nestimations. To address this, we introduce SMART (Sensor Measurement\nAugmentation and Reacquisition Tracker), a novel approach that leverages\nhigh-frequency state estimates from the KF to guide the search for new\nmeasurements, maintaining tracking continuity even when direct measurements\nfalter. This is crucial for dynamic environments where traditional methods\nstruggle. Our contributions include: 1) Versatile Measurement Augmentation\nUsing KF Feedback: We implement a versatile measurement augmentation system\nthat serves as a backup when primary object detectors fail intermittently. This\nsystem is adaptable to various sensors, demonstrated using depth cameras where\nKF's 3D predictions are projected into 2D depth image coordinates, integrating\nnonlinear covariance propagation techniques simplified to first-order\napproximations. 2) Open-source ROS2 Implementation: We provide an open-source\nROS2 implementation of the SMART-TRACK framework, validated in a realistic\nsimulation environment using Gazebo and ROS2, fostering broader adaptation and\nfurther research. Our results showcase significant enhancements in tracking\nstability, with estimation RMSE as low as 0.04 m during measurement\ndisruptions, advancing the robustness of UAV tracking and expanding the\npotential for reliable autonomous UAV operations in complex scenarios. The\nimplementation is available at https://github.com/mzahana/SMART-TRACK.\n", "link": "http://arxiv.org/abs/2410.10409v1", "date": "2024-10-14", "relevancy": 2.2695, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5934}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5929}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART-TRACK%3A%20A%20Novel%20Kalman%20Filter-Guided%20Sensor%20Fusion%20For%20Robust%20UAV%0A%20%20Object%20Tracking%20in%20Dynamic%20Environments&body=Title%3A%20SMART-TRACK%3A%20A%20Novel%20Kalman%20Filter-Guided%20Sensor%20Fusion%20For%20Robust%20UAV%0A%20%20Object%20Tracking%20in%20Dynamic%20Environments%0AAuthor%3A%20Khaled%20Gabr%20and%20Mohamed%20Abdelkader%20and%20Imen%20Jarraya%20and%20Abdullah%20AlMusalami%20and%20Anis%20Koubaa%0AAbstract%3A%20%20%20In%20the%20field%20of%20sensor%20fusion%20and%20state%20estimation%20for%20object%20detection%20and%0Alocalization%2C%20ensuring%20accurate%20tracking%20in%20dynamic%20environments%20poses%0Asignificant%20challenges.%20Traditional%20methods%20like%20the%20Kalman%20Filter%20%28KF%29%20often%0Afail%20when%20measurements%20are%20intermittent%2C%20leading%20to%20rapid%20divergence%20in%20state%0Aestimations.%20To%20address%20this%2C%20we%20introduce%20SMART%20%28Sensor%20Measurement%0AAugmentation%20and%20Reacquisition%20Tracker%29%2C%20a%20novel%20approach%20that%20leverages%0Ahigh-frequency%20state%20estimates%20from%20the%20KF%20to%20guide%20the%20search%20for%20new%0Ameasurements%2C%20maintaining%20tracking%20continuity%20even%20when%20direct%20measurements%0Afalter.%20This%20is%20crucial%20for%20dynamic%20environments%20where%20traditional%20methods%0Astruggle.%20Our%20contributions%20include%3A%201%29%20Versatile%20Measurement%20Augmentation%0AUsing%20KF%20Feedback%3A%20We%20implement%20a%20versatile%20measurement%20augmentation%20system%0Athat%20serves%20as%20a%20backup%20when%20primary%20object%20detectors%20fail%20intermittently.%20This%0Asystem%20is%20adaptable%20to%20various%20sensors%2C%20demonstrated%20using%20depth%20cameras%20where%0AKF%27s%203D%20predictions%20are%20projected%20into%202D%20depth%20image%20coordinates%2C%20integrating%0Anonlinear%20covariance%20propagation%20techniques%20simplified%20to%20first-order%0Aapproximations.%202%29%20Open-source%20ROS2%20Implementation%3A%20We%20provide%20an%20open-source%0AROS2%20implementation%20of%20the%20SMART-TRACK%20framework%2C%20validated%20in%20a%20realistic%0Asimulation%20environment%20using%20Gazebo%20and%20ROS2%2C%20fostering%20broader%20adaptation%20and%0Afurther%20research.%20Our%20results%20showcase%20significant%20enhancements%20in%20tracking%0Astability%2C%20with%20estimation%20RMSE%20as%20low%20as%200.04%20m%20during%20measurement%0Adisruptions%2C%20advancing%20the%20robustness%20of%20UAV%20tracking%20and%20expanding%20the%0Apotential%20for%20reliable%20autonomous%20UAV%20operations%20in%20complex%20scenarios.%20The%0Aimplementation%20is%20available%20at%20https%3A//github.com/mzahana/SMART-TRACK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART-TRACK%253A%2520A%2520Novel%2520Kalman%2520Filter-Guided%2520Sensor%2520Fusion%2520For%2520Robust%2520UAV%250A%2520%2520Object%2520Tracking%2520in%2520Dynamic%2520Environments%26entry.906535625%3DKhaled%2520Gabr%2520and%2520Mohamed%2520Abdelkader%2520and%2520Imen%2520Jarraya%2520and%2520Abdullah%2520AlMusalami%2520and%2520Anis%2520Koubaa%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520sensor%2520fusion%2520and%2520state%2520estimation%2520for%2520object%2520detection%2520and%250Alocalization%252C%2520ensuring%2520accurate%2520tracking%2520in%2520dynamic%2520environments%2520poses%250Asignificant%2520challenges.%2520Traditional%2520methods%2520like%2520the%2520Kalman%2520Filter%2520%2528KF%2529%2520often%250Afail%2520when%2520measurements%2520are%2520intermittent%252C%2520leading%2520to%2520rapid%2520divergence%2520in%2520state%250Aestimations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SMART%2520%2528Sensor%2520Measurement%250AAugmentation%2520and%2520Reacquisition%2520Tracker%2529%252C%2520a%2520novel%2520approach%2520that%2520leverages%250Ahigh-frequency%2520state%2520estimates%2520from%2520the%2520KF%2520to%2520guide%2520the%2520search%2520for%2520new%250Ameasurements%252C%2520maintaining%2520tracking%2520continuity%2520even%2520when%2520direct%2520measurements%250Afalter.%2520This%2520is%2520crucial%2520for%2520dynamic%2520environments%2520where%2520traditional%2520methods%250Astruggle.%2520Our%2520contributions%2520include%253A%25201%2529%2520Versatile%2520Measurement%2520Augmentation%250AUsing%2520KF%2520Feedback%253A%2520We%2520implement%2520a%2520versatile%2520measurement%2520augmentation%2520system%250Athat%2520serves%2520as%2520a%2520backup%2520when%2520primary%2520object%2520detectors%2520fail%2520intermittently.%2520This%250Asystem%2520is%2520adaptable%2520to%2520various%2520sensors%252C%2520demonstrated%2520using%2520depth%2520cameras%2520where%250AKF%2527s%25203D%2520predictions%2520are%2520projected%2520into%25202D%2520depth%2520image%2520coordinates%252C%2520integrating%250Anonlinear%2520covariance%2520propagation%2520techniques%2520simplified%2520to%2520first-order%250Aapproximations.%25202%2529%2520Open-source%2520ROS2%2520Implementation%253A%2520We%2520provide%2520an%2520open-source%250AROS2%2520implementation%2520of%2520the%2520SMART-TRACK%2520framework%252C%2520validated%2520in%2520a%2520realistic%250Asimulation%2520environment%2520using%2520Gazebo%2520and%2520ROS2%252C%2520fostering%2520broader%2520adaptation%2520and%250Afurther%2520research.%2520Our%2520results%2520showcase%2520significant%2520enhancements%2520in%2520tracking%250Astability%252C%2520with%2520estimation%2520RMSE%2520as%2520low%2520as%25200.04%2520m%2520during%2520measurement%250Adisruptions%252C%2520advancing%2520the%2520robustness%2520of%2520UAV%2520tracking%2520and%2520expanding%2520the%250Apotential%2520for%2520reliable%2520autonomous%2520UAV%2520operations%2520in%2520complex%2520scenarios.%2520The%250Aimplementation%2520is%2520available%2520at%2520https%253A//github.com/mzahana/SMART-TRACK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART-TRACK%3A%20A%20Novel%20Kalman%20Filter-Guided%20Sensor%20Fusion%20For%20Robust%20UAV%0A%20%20Object%20Tracking%20in%20Dynamic%20Environments&entry.906535625=Khaled%20Gabr%20and%20Mohamed%20Abdelkader%20and%20Imen%20Jarraya%20and%20Abdullah%20AlMusalami%20and%20Anis%20Koubaa&entry.1292438233=%20%20In%20the%20field%20of%20sensor%20fusion%20and%20state%20estimation%20for%20object%20detection%20and%0Alocalization%2C%20ensuring%20accurate%20tracking%20in%20dynamic%20environments%20poses%0Asignificant%20challenges.%20Traditional%20methods%20like%20the%20Kalman%20Filter%20%28KF%29%20often%0Afail%20when%20measurements%20are%20intermittent%2C%20leading%20to%20rapid%20divergence%20in%20state%0Aestimations.%20To%20address%20this%2C%20we%20introduce%20SMART%20%28Sensor%20Measurement%0AAugmentation%20and%20Reacquisition%20Tracker%29%2C%20a%20novel%20approach%20that%20leverages%0Ahigh-frequency%20state%20estimates%20from%20the%20KF%20to%20guide%20the%20search%20for%20new%0Ameasurements%2C%20maintaining%20tracking%20continuity%20even%20when%20direct%20measurements%0Afalter.%20This%20is%20crucial%20for%20dynamic%20environments%20where%20traditional%20methods%0Astruggle.%20Our%20contributions%20include%3A%201%29%20Versatile%20Measurement%20Augmentation%0AUsing%20KF%20Feedback%3A%20We%20implement%20a%20versatile%20measurement%20augmentation%20system%0Athat%20serves%20as%20a%20backup%20when%20primary%20object%20detectors%20fail%20intermittently.%20This%0Asystem%20is%20adaptable%20to%20various%20sensors%2C%20demonstrated%20using%20depth%20cameras%20where%0AKF%27s%203D%20predictions%20are%20projected%20into%202D%20depth%20image%20coordinates%2C%20integrating%0Anonlinear%20covariance%20propagation%20techniques%20simplified%20to%20first-order%0Aapproximations.%202%29%20Open-source%20ROS2%20Implementation%3A%20We%20provide%20an%20open-source%0AROS2%20implementation%20of%20the%20SMART-TRACK%20framework%2C%20validated%20in%20a%20realistic%0Asimulation%20environment%20using%20Gazebo%20and%20ROS2%2C%20fostering%20broader%20adaptation%20and%0Afurther%20research.%20Our%20results%20showcase%20significant%20enhancements%20in%20tracking%0Astability%2C%20with%20estimation%20RMSE%20as%20low%20as%200.04%20m%20during%20measurement%0Adisruptions%2C%20advancing%20the%20robustness%20of%20UAV%20tracking%20and%20expanding%20the%0Apotential%20for%20reliable%20autonomous%20UAV%20operations%20in%20complex%20scenarios.%20The%0Aimplementation%20is%20available%20at%20https%3A//github.com/mzahana/SMART-TRACK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10409v1&entry.124074799=Read"},
{"title": "BookWorm: A Dataset for Character Description and Analysis", "author": "Argyrios Papoudakis and Mirella Lapata and Frank Keller", "abstract": "  Characters are at the heart of every story, driving the plot and engaging\nreaders. In this study, we explore the understanding of characters in\nfull-length books, which contain complex narratives and numerous interacting\ncharacters. We define two tasks: character description, which generates a brief\nfactual profile, and character analysis, which offers an in-depth\ninterpretation, including character development, personality, and social\ncontext. We introduce the BookWorm dataset, pairing books from the Gutenberg\nProject with human-written descriptions and analyses. Using this dataset, we\nevaluate state-of-the-art long-context models in zero-shot and fine-tuning\nsettings, utilizing both retrieval-based and hierarchical processing for\nbook-length inputs. Our findings show that retrieval-based approaches\noutperform hierarchical ones in both tasks. Additionally, fine-tuned models\nusing coreference-based retrieval produce the most factual descriptions, as\nmeasured by fact- and entailment-based metrics. We hope our dataset,\nexperiments, and analysis will inspire further research in character-based\nnarrative understanding.\n", "link": "http://arxiv.org/abs/2410.10372v1", "date": "2024-10-14", "relevancy": 2.2675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BookWorm%3A%20A%20Dataset%20for%20Character%20Description%20and%20Analysis&body=Title%3A%20BookWorm%3A%20A%20Dataset%20for%20Character%20Description%20and%20Analysis%0AAuthor%3A%20Argyrios%20Papoudakis%20and%20Mirella%20Lapata%20and%20Frank%20Keller%0AAbstract%3A%20%20%20Characters%20are%20at%20the%20heart%20of%20every%20story%2C%20driving%20the%20plot%20and%20engaging%0Areaders.%20In%20this%20study%2C%20we%20explore%20the%20understanding%20of%20characters%20in%0Afull-length%20books%2C%20which%20contain%20complex%20narratives%20and%20numerous%20interacting%0Acharacters.%20We%20define%20two%20tasks%3A%20character%20description%2C%20which%20generates%20a%20brief%0Afactual%20profile%2C%20and%20character%20analysis%2C%20which%20offers%20an%20in-depth%0Ainterpretation%2C%20including%20character%20development%2C%20personality%2C%20and%20social%0Acontext.%20We%20introduce%20the%20BookWorm%20dataset%2C%20pairing%20books%20from%20the%20Gutenberg%0AProject%20with%20human-written%20descriptions%20and%20analyses.%20Using%20this%20dataset%2C%20we%0Aevaluate%20state-of-the-art%20long-context%20models%20in%20zero-shot%20and%20fine-tuning%0Asettings%2C%20utilizing%20both%20retrieval-based%20and%20hierarchical%20processing%20for%0Abook-length%20inputs.%20Our%20findings%20show%20that%20retrieval-based%20approaches%0Aoutperform%20hierarchical%20ones%20in%20both%20tasks.%20Additionally%2C%20fine-tuned%20models%0Ausing%20coreference-based%20retrieval%20produce%20the%20most%20factual%20descriptions%2C%20as%0Ameasured%20by%20fact-%20and%20entailment-based%20metrics.%20We%20hope%20our%20dataset%2C%0Aexperiments%2C%20and%20analysis%20will%20inspire%20further%20research%20in%20character-based%0Anarrative%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBookWorm%253A%2520A%2520Dataset%2520for%2520Character%2520Description%2520and%2520Analysis%26entry.906535625%3DArgyrios%2520Papoudakis%2520and%2520Mirella%2520Lapata%2520and%2520Frank%2520Keller%26entry.1292438233%3D%2520%2520Characters%2520are%2520at%2520the%2520heart%2520of%2520every%2520story%252C%2520driving%2520the%2520plot%2520and%2520engaging%250Areaders.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520understanding%2520of%2520characters%2520in%250Afull-length%2520books%252C%2520which%2520contain%2520complex%2520narratives%2520and%2520numerous%2520interacting%250Acharacters.%2520We%2520define%2520two%2520tasks%253A%2520character%2520description%252C%2520which%2520generates%2520a%2520brief%250Afactual%2520profile%252C%2520and%2520character%2520analysis%252C%2520which%2520offers%2520an%2520in-depth%250Ainterpretation%252C%2520including%2520character%2520development%252C%2520personality%252C%2520and%2520social%250Acontext.%2520We%2520introduce%2520the%2520BookWorm%2520dataset%252C%2520pairing%2520books%2520from%2520the%2520Gutenberg%250AProject%2520with%2520human-written%2520descriptions%2520and%2520analyses.%2520Using%2520this%2520dataset%252C%2520we%250Aevaluate%2520state-of-the-art%2520long-context%2520models%2520in%2520zero-shot%2520and%2520fine-tuning%250Asettings%252C%2520utilizing%2520both%2520retrieval-based%2520and%2520hierarchical%2520processing%2520for%250Abook-length%2520inputs.%2520Our%2520findings%2520show%2520that%2520retrieval-based%2520approaches%250Aoutperform%2520hierarchical%2520ones%2520in%2520both%2520tasks.%2520Additionally%252C%2520fine-tuned%2520models%250Ausing%2520coreference-based%2520retrieval%2520produce%2520the%2520most%2520factual%2520descriptions%252C%2520as%250Ameasured%2520by%2520fact-%2520and%2520entailment-based%2520metrics.%2520We%2520hope%2520our%2520dataset%252C%250Aexperiments%252C%2520and%2520analysis%2520will%2520inspire%2520further%2520research%2520in%2520character-based%250Anarrative%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BookWorm%3A%20A%20Dataset%20for%20Character%20Description%20and%20Analysis&entry.906535625=Argyrios%20Papoudakis%20and%20Mirella%20Lapata%20and%20Frank%20Keller&entry.1292438233=%20%20Characters%20are%20at%20the%20heart%20of%20every%20story%2C%20driving%20the%20plot%20and%20engaging%0Areaders.%20In%20this%20study%2C%20we%20explore%20the%20understanding%20of%20characters%20in%0Afull-length%20books%2C%20which%20contain%20complex%20narratives%20and%20numerous%20interacting%0Acharacters.%20We%20define%20two%20tasks%3A%20character%20description%2C%20which%20generates%20a%20brief%0Afactual%20profile%2C%20and%20character%20analysis%2C%20which%20offers%20an%20in-depth%0Ainterpretation%2C%20including%20character%20development%2C%20personality%2C%20and%20social%0Acontext.%20We%20introduce%20the%20BookWorm%20dataset%2C%20pairing%20books%20from%20the%20Gutenberg%0AProject%20with%20human-written%20descriptions%20and%20analyses.%20Using%20this%20dataset%2C%20we%0Aevaluate%20state-of-the-art%20long-context%20models%20in%20zero-shot%20and%20fine-tuning%0Asettings%2C%20utilizing%20both%20retrieval-based%20and%20hierarchical%20processing%20for%0Abook-length%20inputs.%20Our%20findings%20show%20that%20retrieval-based%20approaches%0Aoutperform%20hierarchical%20ones%20in%20both%20tasks.%20Additionally%2C%20fine-tuned%20models%0Ausing%20coreference-based%20retrieval%20produce%20the%20most%20factual%20descriptions%2C%20as%0Ameasured%20by%20fact-%20and%20entailment-based%20metrics.%20We%20hope%20our%20dataset%2C%0Aexperiments%2C%20and%20analysis%20will%20inspire%20further%20research%20in%20character-based%0Anarrative%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10372v1&entry.124074799=Read"},
{"title": "Enhancing JEPAs with Spatial Conditioning: Robust and Efficient\n  Representation Learning", "author": "Etai Littwin and Vimal Thilak and Anand Gopalakrishnan", "abstract": "  Image-based Joint-Embedding Predictive Architecture (IJEPA) offers an\nattractive alternative to Masked Autoencoder (MAE) for representation learning\nusing the Masked Image Modeling framework. IJEPA drives representations to\ncapture useful semantic information by predicting in latent rather than input\nspace. However, IJEPA relies on carefully designed context and target windows\nto avoid representational collapse. The encoder modules in IJEPA cannot\nadaptively modulate the type of predicted and/or target features based on the\nfeasibility of the masked prediction task as they are not given sufficient\ninformation of both context and targets. Based on the intuition that in natural\nimages, information has a strong spatial bias with spatially local regions\nbeing highly predictive of one another compared to distant ones. We condition\nthe target encoder and context encoder modules in IJEPA with positions of\ncontext and target windows respectively. Our \"conditional\" encoders show\nperformance gains on several image classification benchmark datasets, improved\nrobustness to context window size and sample-efficiency during pretraining.\n", "link": "http://arxiv.org/abs/2410.10773v1", "date": "2024-10-14", "relevancy": 2.2573, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6024}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20JEPAs%20with%20Spatial%20Conditioning%3A%20Robust%20and%20Efficient%0A%20%20Representation%20Learning&body=Title%3A%20Enhancing%20JEPAs%20with%20Spatial%20Conditioning%3A%20Robust%20and%20Efficient%0A%20%20Representation%20Learning%0AAuthor%3A%20Etai%20Littwin%20and%20Vimal%20Thilak%20and%20Anand%20Gopalakrishnan%0AAbstract%3A%20%20%20Image-based%20Joint-Embedding%20Predictive%20Architecture%20%28IJEPA%29%20offers%20an%0Aattractive%20alternative%20to%20Masked%20Autoencoder%20%28MAE%29%20for%20representation%20learning%0Ausing%20the%20Masked%20Image%20Modeling%20framework.%20IJEPA%20drives%20representations%20to%0Acapture%20useful%20semantic%20information%20by%20predicting%20in%20latent%20rather%20than%20input%0Aspace.%20However%2C%20IJEPA%20relies%20on%20carefully%20designed%20context%20and%20target%20windows%0Ato%20avoid%20representational%20collapse.%20The%20encoder%20modules%20in%20IJEPA%20cannot%0Aadaptively%20modulate%20the%20type%20of%20predicted%20and/or%20target%20features%20based%20on%20the%0Afeasibility%20of%20the%20masked%20prediction%20task%20as%20they%20are%20not%20given%20sufficient%0Ainformation%20of%20both%20context%20and%20targets.%20Based%20on%20the%20intuition%20that%20in%20natural%0Aimages%2C%20information%20has%20a%20strong%20spatial%20bias%20with%20spatially%20local%20regions%0Abeing%20highly%20predictive%20of%20one%20another%20compared%20to%20distant%20ones.%20We%20condition%0Athe%20target%20encoder%20and%20context%20encoder%20modules%20in%20IJEPA%20with%20positions%20of%0Acontext%20and%20target%20windows%20respectively.%20Our%20%22conditional%22%20encoders%20show%0Aperformance%20gains%20on%20several%20image%20classification%20benchmark%20datasets%2C%20improved%0Arobustness%20to%20context%20window%20size%20and%20sample-efficiency%20during%20pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520JEPAs%2520with%2520Spatial%2520Conditioning%253A%2520Robust%2520and%2520Efficient%250A%2520%2520Representation%2520Learning%26entry.906535625%3DEtai%2520Littwin%2520and%2520Vimal%2520Thilak%2520and%2520Anand%2520Gopalakrishnan%26entry.1292438233%3D%2520%2520Image-based%2520Joint-Embedding%2520Predictive%2520Architecture%2520%2528IJEPA%2529%2520offers%2520an%250Aattractive%2520alternative%2520to%2520Masked%2520Autoencoder%2520%2528MAE%2529%2520for%2520representation%2520learning%250Ausing%2520the%2520Masked%2520Image%2520Modeling%2520framework.%2520IJEPA%2520drives%2520representations%2520to%250Acapture%2520useful%2520semantic%2520information%2520by%2520predicting%2520in%2520latent%2520rather%2520than%2520input%250Aspace.%2520However%252C%2520IJEPA%2520relies%2520on%2520carefully%2520designed%2520context%2520and%2520target%2520windows%250Ato%2520avoid%2520representational%2520collapse.%2520The%2520encoder%2520modules%2520in%2520IJEPA%2520cannot%250Aadaptively%2520modulate%2520the%2520type%2520of%2520predicted%2520and/or%2520target%2520features%2520based%2520on%2520the%250Afeasibility%2520of%2520the%2520masked%2520prediction%2520task%2520as%2520they%2520are%2520not%2520given%2520sufficient%250Ainformation%2520of%2520both%2520context%2520and%2520targets.%2520Based%2520on%2520the%2520intuition%2520that%2520in%2520natural%250Aimages%252C%2520information%2520has%2520a%2520strong%2520spatial%2520bias%2520with%2520spatially%2520local%2520regions%250Abeing%2520highly%2520predictive%2520of%2520one%2520another%2520compared%2520to%2520distant%2520ones.%2520We%2520condition%250Athe%2520target%2520encoder%2520and%2520context%2520encoder%2520modules%2520in%2520IJEPA%2520with%2520positions%2520of%250Acontext%2520and%2520target%2520windows%2520respectively.%2520Our%2520%2522conditional%2522%2520encoders%2520show%250Aperformance%2520gains%2520on%2520several%2520image%2520classification%2520benchmark%2520datasets%252C%2520improved%250Arobustness%2520to%2520context%2520window%2520size%2520and%2520sample-efficiency%2520during%2520pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20JEPAs%20with%20Spatial%20Conditioning%3A%20Robust%20and%20Efficient%0A%20%20Representation%20Learning&entry.906535625=Etai%20Littwin%20and%20Vimal%20Thilak%20and%20Anand%20Gopalakrishnan&entry.1292438233=%20%20Image-based%20Joint-Embedding%20Predictive%20Architecture%20%28IJEPA%29%20offers%20an%0Aattractive%20alternative%20to%20Masked%20Autoencoder%20%28MAE%29%20for%20representation%20learning%0Ausing%20the%20Masked%20Image%20Modeling%20framework.%20IJEPA%20drives%20representations%20to%0Acapture%20useful%20semantic%20information%20by%20predicting%20in%20latent%20rather%20than%20input%0Aspace.%20However%2C%20IJEPA%20relies%20on%20carefully%20designed%20context%20and%20target%20windows%0Ato%20avoid%20representational%20collapse.%20The%20encoder%20modules%20in%20IJEPA%20cannot%0Aadaptively%20modulate%20the%20type%20of%20predicted%20and/or%20target%20features%20based%20on%20the%0Afeasibility%20of%20the%20masked%20prediction%20task%20as%20they%20are%20not%20given%20sufficient%0Ainformation%20of%20both%20context%20and%20targets.%20Based%20on%20the%20intuition%20that%20in%20natural%0Aimages%2C%20information%20has%20a%20strong%20spatial%20bias%20with%20spatially%20local%20regions%0Abeing%20highly%20predictive%20of%20one%20another%20compared%20to%20distant%20ones.%20We%20condition%0Athe%20target%20encoder%20and%20context%20encoder%20modules%20in%20IJEPA%20with%20positions%20of%0Acontext%20and%20target%20windows%20respectively.%20Our%20%22conditional%22%20encoders%20show%0Aperformance%20gains%20on%20several%20image%20classification%20benchmark%20datasets%2C%20improved%0Arobustness%20to%20context%20window%20size%20and%20sample-efficiency%20during%20pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10773v1&entry.124074799=Read"},
{"title": "UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation", "author": "Lihe Yang and Zhen Zhao and Hengshuang Zhao", "abstract": "  Semi-supervised semantic segmentation (SSS) aims at learning rich visual\nknowledge from cheap unlabeled images to enhance semantic segmentation\ncapability. Among recent works, UniMatch improves its precedents tremendously\nby amplifying the practice of weak-to-strong consistency regularization.\nSubsequent works typically follow similar pipelines and propose various\ndelicate designs. Despite the achieved progress, strangely, even in this\nflourishing era of numerous powerful vision models, almost all SSS works are\nstill sticking to 1) using outdated ResNet encoders with small-scale\nImageNet-1K pre-training, and 2) evaluation on simple Pascal and Cityscapes\ndatasets. In this work, we argue that, it is necessary to switch the baseline\nof SSS from ResNet-based encoders to more capable ViT-based encoders (e.g.,\nDINOv2) that are pre-trained on massive data. A simple update on the encoder\n(even using 2x fewer parameters) can bring more significant improvement than\ncareful method designs. Built on this competitive baseline, we present our\nupgraded and simplified UniMatch V2, inheriting the core spirit of\nweak-to-strong consistency from V1, but requiring less training cost and\nproviding consistently better results. Additionally, witnessing the gradually\nsaturated performance on Pascal and Cityscapes, we appeal that we should focus\non more challenging benchmarks with complex taxonomy, such as ADE20K and COCO\ndatasets. Code, models, and logs of all reported values, are available at\nhttps://github.com/LiheYoung/UniMatch-V2.\n", "link": "http://arxiv.org/abs/2410.10777v1", "date": "2024-10-14", "relevancy": 2.2544, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMatch%20V2%3A%20Pushing%20the%20Limit%20of%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20UniMatch%20V2%3A%20Pushing%20the%20Limit%20of%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Lihe%20Yang%20and%20Zhen%20Zhao%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Semi-supervised%20semantic%20segmentation%20%28SSS%29%20aims%20at%20learning%20rich%20visual%0Aknowledge%20from%20cheap%20unlabeled%20images%20to%20enhance%20semantic%20segmentation%0Acapability.%20Among%20recent%20works%2C%20UniMatch%20improves%20its%20precedents%20tremendously%0Aby%20amplifying%20the%20practice%20of%20weak-to-strong%20consistency%20regularization.%0ASubsequent%20works%20typically%20follow%20similar%20pipelines%20and%20propose%20various%0Adelicate%20designs.%20Despite%20the%20achieved%20progress%2C%20strangely%2C%20even%20in%20this%0Aflourishing%20era%20of%20numerous%20powerful%20vision%20models%2C%20almost%20all%20SSS%20works%20are%0Astill%20sticking%20to%201%29%20using%20outdated%20ResNet%20encoders%20with%20small-scale%0AImageNet-1K%20pre-training%2C%20and%202%29%20evaluation%20on%20simple%20Pascal%20and%20Cityscapes%0Adatasets.%20In%20this%20work%2C%20we%20argue%20that%2C%20it%20is%20necessary%20to%20switch%20the%20baseline%0Aof%20SSS%20from%20ResNet-based%20encoders%20to%20more%20capable%20ViT-based%20encoders%20%28e.g.%2C%0ADINOv2%29%20that%20are%20pre-trained%20on%20massive%20data.%20A%20simple%20update%20on%20the%20encoder%0A%28even%20using%202x%20fewer%20parameters%29%20can%20bring%20more%20significant%20improvement%20than%0Acareful%20method%20designs.%20Built%20on%20this%20competitive%20baseline%2C%20we%20present%20our%0Aupgraded%20and%20simplified%20UniMatch%20V2%2C%20inheriting%20the%20core%20spirit%20of%0Aweak-to-strong%20consistency%20from%20V1%2C%20but%20requiring%20less%20training%20cost%20and%0Aproviding%20consistently%20better%20results.%20Additionally%2C%20witnessing%20the%20gradually%0Asaturated%20performance%20on%20Pascal%20and%20Cityscapes%2C%20we%20appeal%20that%20we%20should%20focus%0Aon%20more%20challenging%20benchmarks%20with%20complex%20taxonomy%2C%20such%20as%20ADE20K%20and%20COCO%0Adatasets.%20Code%2C%20models%2C%20and%20logs%20of%20all%20reported%20values%2C%20are%20available%20at%0Ahttps%3A//github.com/LiheYoung/UniMatch-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMatch%2520V2%253A%2520Pushing%2520the%2520Limit%2520of%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DLihe%2520Yang%2520and%2520Zhen%2520Zhao%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Semi-supervised%2520semantic%2520segmentation%2520%2528SSS%2529%2520aims%2520at%2520learning%2520rich%2520visual%250Aknowledge%2520from%2520cheap%2520unlabeled%2520images%2520to%2520enhance%2520semantic%2520segmentation%250Acapability.%2520Among%2520recent%2520works%252C%2520UniMatch%2520improves%2520its%2520precedents%2520tremendously%250Aby%2520amplifying%2520the%2520practice%2520of%2520weak-to-strong%2520consistency%2520regularization.%250ASubsequent%2520works%2520typically%2520follow%2520similar%2520pipelines%2520and%2520propose%2520various%250Adelicate%2520designs.%2520Despite%2520the%2520achieved%2520progress%252C%2520strangely%252C%2520even%2520in%2520this%250Aflourishing%2520era%2520of%2520numerous%2520powerful%2520vision%2520models%252C%2520almost%2520all%2520SSS%2520works%2520are%250Astill%2520sticking%2520to%25201%2529%2520using%2520outdated%2520ResNet%2520encoders%2520with%2520small-scale%250AImageNet-1K%2520pre-training%252C%2520and%25202%2529%2520evaluation%2520on%2520simple%2520Pascal%2520and%2520Cityscapes%250Adatasets.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%252C%2520it%2520is%2520necessary%2520to%2520switch%2520the%2520baseline%250Aof%2520SSS%2520from%2520ResNet-based%2520encoders%2520to%2520more%2520capable%2520ViT-based%2520encoders%2520%2528e.g.%252C%250ADINOv2%2529%2520that%2520are%2520pre-trained%2520on%2520massive%2520data.%2520A%2520simple%2520update%2520on%2520the%2520encoder%250A%2528even%2520using%25202x%2520fewer%2520parameters%2529%2520can%2520bring%2520more%2520significant%2520improvement%2520than%250Acareful%2520method%2520designs.%2520Built%2520on%2520this%2520competitive%2520baseline%252C%2520we%2520present%2520our%250Aupgraded%2520and%2520simplified%2520UniMatch%2520V2%252C%2520inheriting%2520the%2520core%2520spirit%2520of%250Aweak-to-strong%2520consistency%2520from%2520V1%252C%2520but%2520requiring%2520less%2520training%2520cost%2520and%250Aproviding%2520consistently%2520better%2520results.%2520Additionally%252C%2520witnessing%2520the%2520gradually%250Asaturated%2520performance%2520on%2520Pascal%2520and%2520Cityscapes%252C%2520we%2520appeal%2520that%2520we%2520should%2520focus%250Aon%2520more%2520challenging%2520benchmarks%2520with%2520complex%2520taxonomy%252C%2520such%2520as%2520ADE20K%2520and%2520COCO%250Adatasets.%2520Code%252C%2520models%252C%2520and%2520logs%2520of%2520all%2520reported%2520values%252C%2520are%2520available%2520at%250Ahttps%253A//github.com/LiheYoung/UniMatch-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMatch%20V2%3A%20Pushing%20the%20Limit%20of%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Lihe%20Yang%20and%20Zhen%20Zhao%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Semi-supervised%20semantic%20segmentation%20%28SSS%29%20aims%20at%20learning%20rich%20visual%0Aknowledge%20from%20cheap%20unlabeled%20images%20to%20enhance%20semantic%20segmentation%0Acapability.%20Among%20recent%20works%2C%20UniMatch%20improves%20its%20precedents%20tremendously%0Aby%20amplifying%20the%20practice%20of%20weak-to-strong%20consistency%20regularization.%0ASubsequent%20works%20typically%20follow%20similar%20pipelines%20and%20propose%20various%0Adelicate%20designs.%20Despite%20the%20achieved%20progress%2C%20strangely%2C%20even%20in%20this%0Aflourishing%20era%20of%20numerous%20powerful%20vision%20models%2C%20almost%20all%20SSS%20works%20are%0Astill%20sticking%20to%201%29%20using%20outdated%20ResNet%20encoders%20with%20small-scale%0AImageNet-1K%20pre-training%2C%20and%202%29%20evaluation%20on%20simple%20Pascal%20and%20Cityscapes%0Adatasets.%20In%20this%20work%2C%20we%20argue%20that%2C%20it%20is%20necessary%20to%20switch%20the%20baseline%0Aof%20SSS%20from%20ResNet-based%20encoders%20to%20more%20capable%20ViT-based%20encoders%20%28e.g.%2C%0ADINOv2%29%20that%20are%20pre-trained%20on%20massive%20data.%20A%20simple%20update%20on%20the%20encoder%0A%28even%20using%202x%20fewer%20parameters%29%20can%20bring%20more%20significant%20improvement%20than%0Acareful%20method%20designs.%20Built%20on%20this%20competitive%20baseline%2C%20we%20present%20our%0Aupgraded%20and%20simplified%20UniMatch%20V2%2C%20inheriting%20the%20core%20spirit%20of%0Aweak-to-strong%20consistency%20from%20V1%2C%20but%20requiring%20less%20training%20cost%20and%0Aproviding%20consistently%20better%20results.%20Additionally%2C%20witnessing%20the%20gradually%0Asaturated%20performance%20on%20Pascal%20and%20Cityscapes%2C%20we%20appeal%20that%20we%20should%20focus%0Aon%20more%20challenging%20benchmarks%20with%20complex%20taxonomy%2C%20such%20as%20ADE20K%20and%20COCO%0Adatasets.%20Code%2C%20models%2C%20and%20logs%20of%20all%20reported%20values%2C%20are%20available%20at%0Ahttps%3A//github.com/LiheYoung/UniMatch-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10777v1&entry.124074799=Read"},
{"title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion\n  Model Training", "author": "Kai Wang and Mingjia Shi and Yukun Zhou and Zekai Li and Zhihang Yuan and Yuzhang Shang and Xiaojiang Peng and Hanwang Zhang and Yang You", "abstract": "  Training diffusion models is always a computation-intensive task. In this\npaper, we introduce a novel speed-up method for diffusion model training,\ncalled, which is based on a closer look at time steps. Our key findings are: i)\nTime steps can be empirically divided into acceleration, deceleration, and\nconvergence areas based on the process increment. ii) These time steps are\nimbalanced, with many concentrated in the convergence area. iii) The\nconcentrated steps provide limited benefits for diffusion training. To address\nthis, we design an asymmetric sampling strategy that reduces the frequency of\nsteps from the convergence area while increasing the sampling probability for\nsteps from other areas. Additionally, we propose a weighting strategy to\nemphasize the importance of time steps with rapid-change process increments. As\na plug-and-play and architecture-agnostic approach, SpeeD consistently achieves\n3-times acceleration across various diffusion architectures, datasets, and\ntasks. Notably, due to its simple design, our approach significantly reduces\nthe cost of diffusion model training with minimal overhead. Our research\nenables more researchers to train diffusion models at a lower cost.\n", "link": "http://arxiv.org/abs/2405.17403v2", "date": "2024-10-14", "relevancy": 2.2501, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6456}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training&body=Title%3A%20A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training%0AAuthor%3A%20Kai%20Wang%20and%20Mingjia%20Shi%20and%20Yukun%20Zhou%20and%20Zekai%20Li%20and%20Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Xiaojiang%20Peng%20and%20Hanwang%20Zhang%20and%20Yang%20You%0AAbstract%3A%20%20%20Training%20diffusion%20models%20is%20always%20a%20computation-intensive%20task.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20speed-up%20method%20for%20diffusion%20model%20training%2C%0Acalled%2C%20which%20is%20based%20on%20a%20closer%20look%20at%20time%20steps.%20Our%20key%20findings%20are%3A%20i%29%0ATime%20steps%20can%20be%20empirically%20divided%20into%20acceleration%2C%20deceleration%2C%20and%0Aconvergence%20areas%20based%20on%20the%20process%20increment.%20ii%29%20These%20time%20steps%20are%0Aimbalanced%2C%20with%20many%20concentrated%20in%20the%20convergence%20area.%20iii%29%20The%0Aconcentrated%20steps%20provide%20limited%20benefits%20for%20diffusion%20training.%20To%20address%0Athis%2C%20we%20design%20an%20asymmetric%20sampling%20strategy%20that%20reduces%20the%20frequency%20of%0Asteps%20from%20the%20convergence%20area%20while%20increasing%20the%20sampling%20probability%20for%0Asteps%20from%20other%20areas.%20Additionally%2C%20we%20propose%20a%20weighting%20strategy%20to%0Aemphasize%20the%20importance%20of%20time%20steps%20with%20rapid-change%20process%20increments.%20As%0Aa%20plug-and-play%20and%20architecture-agnostic%20approach%2C%20SpeeD%20consistently%20achieves%0A3-times%20acceleration%20across%20various%20diffusion%20architectures%2C%20datasets%2C%20and%0Atasks.%20Notably%2C%20due%20to%20its%20simple%20design%2C%20our%20approach%20significantly%20reduces%0Athe%20cost%20of%20diffusion%20model%20training%20with%20minimal%20overhead.%20Our%20research%0Aenables%20more%20researchers%20to%20train%20diffusion%20models%20at%20a%20lower%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520Time%2520Steps%2520is%2520Worthy%2520of%2520Triple%2520Speed-Up%2520for%2520Diffusion%250A%2520%2520Model%2520Training%26entry.906535625%3DKai%2520Wang%2520and%2520Mingjia%2520Shi%2520and%2520Yukun%2520Zhou%2520and%2520Zekai%2520Li%2520and%2520Zhihang%2520Yuan%2520and%2520Yuzhang%2520Shang%2520and%2520Xiaojiang%2520Peng%2520and%2520Hanwang%2520Zhang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Training%2520diffusion%2520models%2520is%2520always%2520a%2520computation-intensive%2520task.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520speed-up%2520method%2520for%2520diffusion%2520model%2520training%252C%250Acalled%252C%2520which%2520is%2520based%2520on%2520a%2520closer%2520look%2520at%2520time%2520steps.%2520Our%2520key%2520findings%2520are%253A%2520i%2529%250ATime%2520steps%2520can%2520be%2520empirically%2520divided%2520into%2520acceleration%252C%2520deceleration%252C%2520and%250Aconvergence%2520areas%2520based%2520on%2520the%2520process%2520increment.%2520ii%2529%2520These%2520time%2520steps%2520are%250Aimbalanced%252C%2520with%2520many%2520concentrated%2520in%2520the%2520convergence%2520area.%2520iii%2529%2520The%250Aconcentrated%2520steps%2520provide%2520limited%2520benefits%2520for%2520diffusion%2520training.%2520To%2520address%250Athis%252C%2520we%2520design%2520an%2520asymmetric%2520sampling%2520strategy%2520that%2520reduces%2520the%2520frequency%2520of%250Asteps%2520from%2520the%2520convergence%2520area%2520while%2520increasing%2520the%2520sampling%2520probability%2520for%250Asteps%2520from%2520other%2520areas.%2520Additionally%252C%2520we%2520propose%2520a%2520weighting%2520strategy%2520to%250Aemphasize%2520the%2520importance%2520of%2520time%2520steps%2520with%2520rapid-change%2520process%2520increments.%2520As%250Aa%2520plug-and-play%2520and%2520architecture-agnostic%2520approach%252C%2520SpeeD%2520consistently%2520achieves%250A3-times%2520acceleration%2520across%2520various%2520diffusion%2520architectures%252C%2520datasets%252C%2520and%250Atasks.%2520Notably%252C%2520due%2520to%2520its%2520simple%2520design%252C%2520our%2520approach%2520significantly%2520reduces%250Athe%2520cost%2520of%2520diffusion%2520model%2520training%2520with%2520minimal%2520overhead.%2520Our%2520research%250Aenables%2520more%2520researchers%2520to%2520train%2520diffusion%2520models%2520at%2520a%2520lower%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training&entry.906535625=Kai%20Wang%20and%20Mingjia%20Shi%20and%20Yukun%20Zhou%20and%20Zekai%20Li%20and%20Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Xiaojiang%20Peng%20and%20Hanwang%20Zhang%20and%20Yang%20You&entry.1292438233=%20%20Training%20diffusion%20models%20is%20always%20a%20computation-intensive%20task.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20speed-up%20method%20for%20diffusion%20model%20training%2C%0Acalled%2C%20which%20is%20based%20on%20a%20closer%20look%20at%20time%20steps.%20Our%20key%20findings%20are%3A%20i%29%0ATime%20steps%20can%20be%20empirically%20divided%20into%20acceleration%2C%20deceleration%2C%20and%0Aconvergence%20areas%20based%20on%20the%20process%20increment.%20ii%29%20These%20time%20steps%20are%0Aimbalanced%2C%20with%20many%20concentrated%20in%20the%20convergence%20area.%20iii%29%20The%0Aconcentrated%20steps%20provide%20limited%20benefits%20for%20diffusion%20training.%20To%20address%0Athis%2C%20we%20design%20an%20asymmetric%20sampling%20strategy%20that%20reduces%20the%20frequency%20of%0Asteps%20from%20the%20convergence%20area%20while%20increasing%20the%20sampling%20probability%20for%0Asteps%20from%20other%20areas.%20Additionally%2C%20we%20propose%20a%20weighting%20strategy%20to%0Aemphasize%20the%20importance%20of%20time%20steps%20with%20rapid-change%20process%20increments.%20As%0Aa%20plug-and-play%20and%20architecture-agnostic%20approach%2C%20SpeeD%20consistently%20achieves%0A3-times%20acceleration%20across%20various%20diffusion%20architectures%2C%20datasets%2C%20and%0Atasks.%20Notably%2C%20due%20to%20its%20simple%20design%2C%20our%20approach%20significantly%20reduces%0Athe%20cost%20of%20diffusion%20model%20training%20with%20minimal%20overhead.%20Our%20research%0Aenables%20more%20researchers%20to%20train%20diffusion%20models%20at%20a%20lower%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17403v2&entry.124074799=Read"},
{"title": "Self-Assessed Generation: Trustworthy Label Generation for Optical Flow\n  and Stereo Matching in Real-world", "author": "Han Ling and Yinghui Sun and Quansen Sun and Ivor Tsang and Yuhui Zheng", "abstract": "  A significant challenge facing current optical flow and stereo methods is the\ndifficulty in generalizing them well to the real world. This is mainly due to\nthe high costs required to produce datasets, and the limitations of existing\nself-supervised methods on fuzzy results and complex model training problems.\nTo address the above challenges, we propose a unified self-supervised\ngeneralization framework for optical flow and stereo tasks: Self-Assessed\nGeneration (SAG). Unlike previous self-supervised methods, SAG is data-driven,\nusing advanced reconstruction techniques to construct a reconstruction field\nfrom RGB images and generate datasets based on it. Afterward, we quantified the\nconfidence level of the generated results from multiple perspectives, such as\nreconstruction field distribution, geometric consistency, and structural\nsimilarity, to eliminate inevitable defects in the generation process. We also\ndesigned a 3D flight foreground automatic rendering pipeline in SAG to\nencourage the network to learn occlusion and motion foreground. Experimentally,\nbecause SAG does not involve changes to methods or loss functions, it can\ndirectly self-supervised train the state-of-the-art deep networks, greatly\nimproving the generalization performance of self-supervised methods on current\nmainstream optical flow and stereo-matching datasets. Compared to previous\ntraining modes, SAG is more generalized, cost-effective, and accurate.\n", "link": "http://arxiv.org/abs/2410.10453v1", "date": "2024-10-14", "relevancy": 2.2428, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5693}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Assessed%20Generation%3A%20Trustworthy%20Label%20Generation%20for%20Optical%20Flow%0A%20%20and%20Stereo%20Matching%20in%20Real-world&body=Title%3A%20Self-Assessed%20Generation%3A%20Trustworthy%20Label%20Generation%20for%20Optical%20Flow%0A%20%20and%20Stereo%20Matching%20in%20Real-world%0AAuthor%3A%20Han%20Ling%20and%20Yinghui%20Sun%20and%20Quansen%20Sun%20and%20Ivor%20Tsang%20and%20Yuhui%20Zheng%0AAbstract%3A%20%20%20A%20significant%20challenge%20facing%20current%20optical%20flow%20and%20stereo%20methods%20is%20the%0Adifficulty%20in%20generalizing%20them%20well%20to%20the%20real%20world.%20This%20is%20mainly%20due%20to%0Athe%20high%20costs%20required%20to%20produce%20datasets%2C%20and%20the%20limitations%20of%20existing%0Aself-supervised%20methods%20on%20fuzzy%20results%20and%20complex%20model%20training%20problems.%0ATo%20address%20the%20above%20challenges%2C%20we%20propose%20a%20unified%20self-supervised%0Ageneralization%20framework%20for%20optical%20flow%20and%20stereo%20tasks%3A%20Self-Assessed%0AGeneration%20%28SAG%29.%20Unlike%20previous%20self-supervised%20methods%2C%20SAG%20is%20data-driven%2C%0Ausing%20advanced%20reconstruction%20techniques%20to%20construct%20a%20reconstruction%20field%0Afrom%20RGB%20images%20and%20generate%20datasets%20based%20on%20it.%20Afterward%2C%20we%20quantified%20the%0Aconfidence%20level%20of%20the%20generated%20results%20from%20multiple%20perspectives%2C%20such%20as%0Areconstruction%20field%20distribution%2C%20geometric%20consistency%2C%20and%20structural%0Asimilarity%2C%20to%20eliminate%20inevitable%20defects%20in%20the%20generation%20process.%20We%20also%0Adesigned%20a%203D%20flight%20foreground%20automatic%20rendering%20pipeline%20in%20SAG%20to%0Aencourage%20the%20network%20to%20learn%20occlusion%20and%20motion%20foreground.%20Experimentally%2C%0Abecause%20SAG%20does%20not%20involve%20changes%20to%20methods%20or%20loss%20functions%2C%20it%20can%0Adirectly%20self-supervised%20train%20the%20state-of-the-art%20deep%20networks%2C%20greatly%0Aimproving%20the%20generalization%20performance%20of%20self-supervised%20methods%20on%20current%0Amainstream%20optical%20flow%20and%20stereo-matching%20datasets.%20Compared%20to%20previous%0Atraining%20modes%2C%20SAG%20is%20more%20generalized%2C%20cost-effective%2C%20and%20accurate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Assessed%2520Generation%253A%2520Trustworthy%2520Label%2520Generation%2520for%2520Optical%2520Flow%250A%2520%2520and%2520Stereo%2520Matching%2520in%2520Real-world%26entry.906535625%3DHan%2520Ling%2520and%2520Yinghui%2520Sun%2520and%2520Quansen%2520Sun%2520and%2520Ivor%2520Tsang%2520and%2520Yuhui%2520Zheng%26entry.1292438233%3D%2520%2520A%2520significant%2520challenge%2520facing%2520current%2520optical%2520flow%2520and%2520stereo%2520methods%2520is%2520the%250Adifficulty%2520in%2520generalizing%2520them%2520well%2520to%2520the%2520real%2520world.%2520This%2520is%2520mainly%2520due%2520to%250Athe%2520high%2520costs%2520required%2520to%2520produce%2520datasets%252C%2520and%2520the%2520limitations%2520of%2520existing%250Aself-supervised%2520methods%2520on%2520fuzzy%2520results%2520and%2520complex%2520model%2520training%2520problems.%250ATo%2520address%2520the%2520above%2520challenges%252C%2520we%2520propose%2520a%2520unified%2520self-supervised%250Ageneralization%2520framework%2520for%2520optical%2520flow%2520and%2520stereo%2520tasks%253A%2520Self-Assessed%250AGeneration%2520%2528SAG%2529.%2520Unlike%2520previous%2520self-supervised%2520methods%252C%2520SAG%2520is%2520data-driven%252C%250Ausing%2520advanced%2520reconstruction%2520techniques%2520to%2520construct%2520a%2520reconstruction%2520field%250Afrom%2520RGB%2520images%2520and%2520generate%2520datasets%2520based%2520on%2520it.%2520Afterward%252C%2520we%2520quantified%2520the%250Aconfidence%2520level%2520of%2520the%2520generated%2520results%2520from%2520multiple%2520perspectives%252C%2520such%2520as%250Areconstruction%2520field%2520distribution%252C%2520geometric%2520consistency%252C%2520and%2520structural%250Asimilarity%252C%2520to%2520eliminate%2520inevitable%2520defects%2520in%2520the%2520generation%2520process.%2520We%2520also%250Adesigned%2520a%25203D%2520flight%2520foreground%2520automatic%2520rendering%2520pipeline%2520in%2520SAG%2520to%250Aencourage%2520the%2520network%2520to%2520learn%2520occlusion%2520and%2520motion%2520foreground.%2520Experimentally%252C%250Abecause%2520SAG%2520does%2520not%2520involve%2520changes%2520to%2520methods%2520or%2520loss%2520functions%252C%2520it%2520can%250Adirectly%2520self-supervised%2520train%2520the%2520state-of-the-art%2520deep%2520networks%252C%2520greatly%250Aimproving%2520the%2520generalization%2520performance%2520of%2520self-supervised%2520methods%2520on%2520current%250Amainstream%2520optical%2520flow%2520and%2520stereo-matching%2520datasets.%2520Compared%2520to%2520previous%250Atraining%2520modes%252C%2520SAG%2520is%2520more%2520generalized%252C%2520cost-effective%252C%2520and%2520accurate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Assessed%20Generation%3A%20Trustworthy%20Label%20Generation%20for%20Optical%20Flow%0A%20%20and%20Stereo%20Matching%20in%20Real-world&entry.906535625=Han%20Ling%20and%20Yinghui%20Sun%20and%20Quansen%20Sun%20and%20Ivor%20Tsang%20and%20Yuhui%20Zheng&entry.1292438233=%20%20A%20significant%20challenge%20facing%20current%20optical%20flow%20and%20stereo%20methods%20is%20the%0Adifficulty%20in%20generalizing%20them%20well%20to%20the%20real%20world.%20This%20is%20mainly%20due%20to%0Athe%20high%20costs%20required%20to%20produce%20datasets%2C%20and%20the%20limitations%20of%20existing%0Aself-supervised%20methods%20on%20fuzzy%20results%20and%20complex%20model%20training%20problems.%0ATo%20address%20the%20above%20challenges%2C%20we%20propose%20a%20unified%20self-supervised%0Ageneralization%20framework%20for%20optical%20flow%20and%20stereo%20tasks%3A%20Self-Assessed%0AGeneration%20%28SAG%29.%20Unlike%20previous%20self-supervised%20methods%2C%20SAG%20is%20data-driven%2C%0Ausing%20advanced%20reconstruction%20techniques%20to%20construct%20a%20reconstruction%20field%0Afrom%20RGB%20images%20and%20generate%20datasets%20based%20on%20it.%20Afterward%2C%20we%20quantified%20the%0Aconfidence%20level%20of%20the%20generated%20results%20from%20multiple%20perspectives%2C%20such%20as%0Areconstruction%20field%20distribution%2C%20geometric%20consistency%2C%20and%20structural%0Asimilarity%2C%20to%20eliminate%20inevitable%20defects%20in%20the%20generation%20process.%20We%20also%0Adesigned%20a%203D%20flight%20foreground%20automatic%20rendering%20pipeline%20in%20SAG%20to%0Aencourage%20the%20network%20to%20learn%20occlusion%20and%20motion%20foreground.%20Experimentally%2C%0Abecause%20SAG%20does%20not%20involve%20changes%20to%20methods%20or%20loss%20functions%2C%20it%20can%0Adirectly%20self-supervised%20train%20the%20state-of-the-art%20deep%20networks%2C%20greatly%0Aimproving%20the%20generalization%20performance%20of%20self-supervised%20methods%20on%20current%0Amainstream%20optical%20flow%20and%20stereo-matching%20datasets.%20Compared%20to%20previous%0Atraining%20modes%2C%20SAG%20is%20more%20generalized%2C%20cost-effective%2C%20and%20accurate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10453v1&entry.124074799=Read"},
{"title": "Graph Classification Gaussian Processes via Hodgelet Spectral Features", "author": "Mathieu Alain and So Takao and Xiaowen Dong and Bastian Rieck and Emmanuel Noutahi", "abstract": "  The problem of classifying graphs is ubiquitous in machine learning. While it\nis standard to apply graph neural networks for such tasks, Gaussian processes\ncan also be used, by transforming graph features into the spectral domain, and\nusing the resulting spectral features as input points. However, this approach\nonly takes into account features on vertices, whereas some graph data also\nsupport features on edges. In this work, we present a Gaussian process-based\nclassification algorithm that can utilise vertex and/or edges features to help\nclassify graphs. Furthermore, we take advantage of the Hodge decomposition of\nvertex and edge features to increase the flexibility of the model, which can be\nbeneficial on some tasks.\n", "link": "http://arxiv.org/abs/2410.10546v1", "date": "2024-10-14", "relevancy": 2.2271, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4579}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4523}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Classification%20Gaussian%20Processes%20via%20Hodgelet%20Spectral%20Features&body=Title%3A%20Graph%20Classification%20Gaussian%20Processes%20via%20Hodgelet%20Spectral%20Features%0AAuthor%3A%20Mathieu%20Alain%20and%20So%20Takao%20and%20Xiaowen%20Dong%20and%20Bastian%20Rieck%20and%20Emmanuel%20Noutahi%0AAbstract%3A%20%20%20The%20problem%20of%20classifying%20graphs%20is%20ubiquitous%20in%20machine%20learning.%20While%20it%0Ais%20standard%20to%20apply%20graph%20neural%20networks%20for%20such%20tasks%2C%20Gaussian%20processes%0Acan%20also%20be%20used%2C%20by%20transforming%20graph%20features%20into%20the%20spectral%20domain%2C%20and%0Ausing%20the%20resulting%20spectral%20features%20as%20input%20points.%20However%2C%20this%20approach%0Aonly%20takes%20into%20account%20features%20on%20vertices%2C%20whereas%20some%20graph%20data%20also%0Asupport%20features%20on%20edges.%20In%20this%20work%2C%20we%20present%20a%20Gaussian%20process-based%0Aclassification%20algorithm%20that%20can%20utilise%20vertex%20and/or%20edges%20features%20to%20help%0Aclassify%20graphs.%20Furthermore%2C%20we%20take%20advantage%20of%20the%20Hodge%20decomposition%20of%0Avertex%20and%20edge%20features%20to%20increase%20the%20flexibility%20of%20the%20model%2C%20which%20can%20be%0Abeneficial%20on%20some%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Classification%2520Gaussian%2520Processes%2520via%2520Hodgelet%2520Spectral%2520Features%26entry.906535625%3DMathieu%2520Alain%2520and%2520So%2520Takao%2520and%2520Xiaowen%2520Dong%2520and%2520Bastian%2520Rieck%2520and%2520Emmanuel%2520Noutahi%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520classifying%2520graphs%2520is%2520ubiquitous%2520in%2520machine%2520learning.%2520While%2520it%250Ais%2520standard%2520to%2520apply%2520graph%2520neural%2520networks%2520for%2520such%2520tasks%252C%2520Gaussian%2520processes%250Acan%2520also%2520be%2520used%252C%2520by%2520transforming%2520graph%2520features%2520into%2520the%2520spectral%2520domain%252C%2520and%250Ausing%2520the%2520resulting%2520spectral%2520features%2520as%2520input%2520points.%2520However%252C%2520this%2520approach%250Aonly%2520takes%2520into%2520account%2520features%2520on%2520vertices%252C%2520whereas%2520some%2520graph%2520data%2520also%250Asupport%2520features%2520on%2520edges.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520Gaussian%2520process-based%250Aclassification%2520algorithm%2520that%2520can%2520utilise%2520vertex%2520and/or%2520edges%2520features%2520to%2520help%250Aclassify%2520graphs.%2520Furthermore%252C%2520we%2520take%2520advantage%2520of%2520the%2520Hodge%2520decomposition%2520of%250Avertex%2520and%2520edge%2520features%2520to%2520increase%2520the%2520flexibility%2520of%2520the%2520model%252C%2520which%2520can%2520be%250Abeneficial%2520on%2520some%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Classification%20Gaussian%20Processes%20via%20Hodgelet%20Spectral%20Features&entry.906535625=Mathieu%20Alain%20and%20So%20Takao%20and%20Xiaowen%20Dong%20and%20Bastian%20Rieck%20and%20Emmanuel%20Noutahi&entry.1292438233=%20%20The%20problem%20of%20classifying%20graphs%20is%20ubiquitous%20in%20machine%20learning.%20While%20it%0Ais%20standard%20to%20apply%20graph%20neural%20networks%20for%20such%20tasks%2C%20Gaussian%20processes%0Acan%20also%20be%20used%2C%20by%20transforming%20graph%20features%20into%20the%20spectral%20domain%2C%20and%0Ausing%20the%20resulting%20spectral%20features%20as%20input%20points.%20However%2C%20this%20approach%0Aonly%20takes%20into%20account%20features%20on%20vertices%2C%20whereas%20some%20graph%20data%20also%0Asupport%20features%20on%20edges.%20In%20this%20work%2C%20we%20present%20a%20Gaussian%20process-based%0Aclassification%20algorithm%20that%20can%20utilise%20vertex%20and/or%20edges%20features%20to%20help%0Aclassify%20graphs.%20Furthermore%2C%20we%20take%20advantage%20of%20the%20Hodge%20decomposition%20of%0Avertex%20and%20edge%20features%20to%20increase%20the%20flexibility%20of%20the%20model%2C%20which%20can%20be%0Abeneficial%20on%20some%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10546v1&entry.124074799=Read"},
{"title": "Towards Generalist Robot Learning from Internet Video: A Survey", "author": "Robert McCarthy and Daniel C. H. Tan and Dominik Schmidt and Fernando Acero and Nathan Herr and Yilun Du and Thomas G. Thuruthel and Zhibin Li", "abstract": "  Scaling deep learning to huge internet-scraped datasets has yielded\nremarkably general capabilities in natural language processing and visual\nunderstanding and generation. In contrast, data is scarce and expensive to\ncollect in robotics. This has seen robot learning struggle to match the\ngenerality of capabilities observed in other domains. Learning from Videos\n(LfV) methods seek to address this data bottleneck by augmenting traditional\nrobot data with large internet-scraped video datasets. Such video data may\nprovide the model with foundational information regarding physical behaviours\nand the physics of the world. This holds great promise for improving the\ngenerality of our robots.\n  In this survey, we present an overview of the emerging field of LfV. We\noutline fundamental concepts, including the benefits and challenges of LfV. We\nprovide a comprehensive review of current methods for: extracting knowledge\nfrom large-scale internet video; tackling key LfV challenges; and boosting\ndownstream reinforcement and robot learning via the use of video data. LfV\ndatasets and benchmarks are also reviewed. The survey closes with a critical\ndiscussion of challenges and opportunities. Here, we advocate for scalable\nfoundation model approaches that can leverage the full range of available\ninternet video to aid the learning of robot policies and dynamics models. We\nhope this survey can inform and catalyse further LfV research, facilitating\nprogress towards the development of general-purpose robots.\n", "link": "http://arxiv.org/abs/2404.19664v3", "date": "2024-10-14", "relevancy": 2.2146, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&body=Title%3A%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey%0AAuthor%3A%20Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Scaling%20deep%20learning%20to%20huge%20internet-scraped%20datasets%20has%20yielded%0Aremarkably%20general%20capabilities%20in%20natural%20language%20processing%20and%20visual%0Aunderstanding%20and%20generation.%20In%20contrast%2C%20data%20is%20scarce%20and%20expensive%20to%0Acollect%20in%20robotics.%20This%20has%20seen%20robot%20learning%20struggle%20to%20match%20the%0Agenerality%20of%20capabilities%20observed%20in%20other%20domains.%20Learning%20from%20Videos%0A%28LfV%29%20methods%20seek%20to%20address%20this%20data%20bottleneck%20by%20augmenting%20traditional%0Arobot%20data%20with%20large%20internet-scraped%20video%20datasets.%20Such%20video%20data%20may%0Aprovide%20the%20model%20with%20foundational%20information%20regarding%20physical%20behaviours%0Aand%20the%20physics%20of%20the%20world.%20This%20holds%20great%20promise%20for%20improving%20the%0Agenerality%20of%20our%20robots.%0A%20%20In%20this%20survey%2C%20we%20present%20an%20overview%20of%20the%20emerging%20field%20of%20LfV.%20We%0Aoutline%20fundamental%20concepts%2C%20including%20the%20benefits%20and%20challenges%20of%20LfV.%20We%0Aprovide%20a%20comprehensive%20review%20of%20current%20methods%20for%3A%20extracting%20knowledge%0Afrom%20large-scale%20internet%20video%3B%20tackling%20key%20LfV%20challenges%3B%20and%20boosting%0Adownstream%20reinforcement%20and%20robot%20learning%20via%20the%20use%20of%20video%20data.%20LfV%0Adatasets%20and%20benchmarks%20are%20also%20reviewed.%20The%20survey%20closes%20with%20a%20critical%0Adiscussion%20of%20challenges%20and%20opportunities.%20Here%2C%20we%20advocate%20for%20scalable%0Afoundation%20model%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%0Ainternet%20video%20to%20aid%20the%20learning%20of%20robot%20policies%20and%20dynamics%20models.%20We%0Ahope%20this%20survey%20can%20inform%20and%20catalyse%20further%20LfV%20research%2C%20facilitating%0Aprogress%20towards%20the%20development%20of%20general-purpose%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19664v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Robot%2520Learning%2520from%2520Internet%2520Video%253A%2520A%2520Survey%26entry.906535625%3DRobert%2520McCarthy%2520and%2520Daniel%2520C.%2520H.%2520Tan%2520and%2520Dominik%2520Schmidt%2520and%2520Fernando%2520Acero%2520and%2520Nathan%2520Herr%2520and%2520Yilun%2520Du%2520and%2520Thomas%2520G.%2520Thuruthel%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Scaling%2520deep%2520learning%2520to%2520huge%2520internet-scraped%2520datasets%2520has%2520yielded%250Aremarkably%2520general%2520capabilities%2520in%2520natural%2520language%2520processing%2520and%2520visual%250Aunderstanding%2520and%2520generation.%2520In%2520contrast%252C%2520data%2520is%2520scarce%2520and%2520expensive%2520to%250Acollect%2520in%2520robotics.%2520This%2520has%2520seen%2520robot%2520learning%2520struggle%2520to%2520match%2520the%250Agenerality%2520of%2520capabilities%2520observed%2520in%2520other%2520domains.%2520Learning%2520from%2520Videos%250A%2528LfV%2529%2520methods%2520seek%2520to%2520address%2520this%2520data%2520bottleneck%2520by%2520augmenting%2520traditional%250Arobot%2520data%2520with%2520large%2520internet-scraped%2520video%2520datasets.%2520Such%2520video%2520data%2520may%250Aprovide%2520the%2520model%2520with%2520foundational%2520information%2520regarding%2520physical%2520behaviours%250Aand%2520the%2520physics%2520of%2520the%2520world.%2520This%2520holds%2520great%2520promise%2520for%2520improving%2520the%250Agenerality%2520of%2520our%2520robots.%250A%2520%2520In%2520this%2520survey%252C%2520we%2520present%2520an%2520overview%2520of%2520the%2520emerging%2520field%2520of%2520LfV.%2520We%250Aoutline%2520fundamental%2520concepts%252C%2520including%2520the%2520benefits%2520and%2520challenges%2520of%2520LfV.%2520We%250Aprovide%2520a%2520comprehensive%2520review%2520of%2520current%2520methods%2520for%253A%2520extracting%2520knowledge%250Afrom%2520large-scale%2520internet%2520video%253B%2520tackling%2520key%2520LfV%2520challenges%253B%2520and%2520boosting%250Adownstream%2520reinforcement%2520and%2520robot%2520learning%2520via%2520the%2520use%2520of%2520video%2520data.%2520LfV%250Adatasets%2520and%2520benchmarks%2520are%2520also%2520reviewed.%2520The%2520survey%2520closes%2520with%2520a%2520critical%250Adiscussion%2520of%2520challenges%2520and%2520opportunities.%2520Here%252C%2520we%2520advocate%2520for%2520scalable%250Afoundation%2520model%2520approaches%2520that%2520can%2520leverage%2520the%2520full%2520range%2520of%2520available%250Ainternet%2520video%2520to%2520aid%2520the%2520learning%2520of%2520robot%2520policies%2520and%2520dynamics%2520models.%2520We%250Ahope%2520this%2520survey%2520can%2520inform%2520and%2520catalyse%2520further%2520LfV%2520research%252C%2520facilitating%250Aprogress%2520towards%2520the%2520development%2520of%2520general-purpose%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19664v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&entry.906535625=Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li&entry.1292438233=%20%20Scaling%20deep%20learning%20to%20huge%20internet-scraped%20datasets%20has%20yielded%0Aremarkably%20general%20capabilities%20in%20natural%20language%20processing%20and%20visual%0Aunderstanding%20and%20generation.%20In%20contrast%2C%20data%20is%20scarce%20and%20expensive%20to%0Acollect%20in%20robotics.%20This%20has%20seen%20robot%20learning%20struggle%20to%20match%20the%0Agenerality%20of%20capabilities%20observed%20in%20other%20domains.%20Learning%20from%20Videos%0A%28LfV%29%20methods%20seek%20to%20address%20this%20data%20bottleneck%20by%20augmenting%20traditional%0Arobot%20data%20with%20large%20internet-scraped%20video%20datasets.%20Such%20video%20data%20may%0Aprovide%20the%20model%20with%20foundational%20information%20regarding%20physical%20behaviours%0Aand%20the%20physics%20of%20the%20world.%20This%20holds%20great%20promise%20for%20improving%20the%0Agenerality%20of%20our%20robots.%0A%20%20In%20this%20survey%2C%20we%20present%20an%20overview%20of%20the%20emerging%20field%20of%20LfV.%20We%0Aoutline%20fundamental%20concepts%2C%20including%20the%20benefits%20and%20challenges%20of%20LfV.%20We%0Aprovide%20a%20comprehensive%20review%20of%20current%20methods%20for%3A%20extracting%20knowledge%0Afrom%20large-scale%20internet%20video%3B%20tackling%20key%20LfV%20challenges%3B%20and%20boosting%0Adownstream%20reinforcement%20and%20robot%20learning%20via%20the%20use%20of%20video%20data.%20LfV%0Adatasets%20and%20benchmarks%20are%20also%20reviewed.%20The%20survey%20closes%20with%20a%20critical%0Adiscussion%20of%20challenges%20and%20opportunities.%20Here%2C%20we%20advocate%20for%20scalable%0Afoundation%20model%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%0Ainternet%20video%20to%20aid%20the%20learning%20of%20robot%20policies%20and%20dynamics%20models.%20We%0Ahope%20this%20survey%20can%20inform%20and%20catalyse%20further%20LfV%20research%2C%20facilitating%0Aprogress%20towards%20the%20development%20of%20general-purpose%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19664v3&entry.124074799=Read"},
{"title": "Probabilistic Degeneracy Detection for Point-to-Plane Error Minimization", "author": "Johan Hatleskog and Kostas Alexis", "abstract": "  Degeneracies arising from uninformative geometry are known to deteriorate\nLiDAR-based localization and mapping. This work introduces a new probabilistic\nmethod to detect and mitigate the effect of degeneracies in point-to-plane\nerror minimization. The noise on the Hessian of the point-to-plane optimization\nproblem is characterized by the noise on points and surface normals used in its\nconstruction. We exploit this characterization to quantify the probability of a\ndirection being degenerate. The degeneracy-detection procedure is used in a new\nreal-time degeneracy-aware iterative closest point algorithm for LiDAR\nregistration, in which we smoothly attenuate updates in degenerate directions.\nThe method's parameters are selected based on the noise characteristics\nprovided in the LiDAR's datasheet. We validate the approach in four real-world\nexperiments, demonstrating that it outperforms state-of-the-art methods at\ndetecting and mitigating the adverse effects of degeneracies. For the benefit\nof the community, we release the code for the method at:\ngithub.com/ntnu-arl/drpm.\n", "link": "http://arxiv.org/abs/2410.10784v1", "date": "2024-10-14", "relevancy": 2.2093, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5696}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Degeneracy%20Detection%20for%20Point-to-Plane%20Error%20Minimization&body=Title%3A%20Probabilistic%20Degeneracy%20Detection%20for%20Point-to-Plane%20Error%20Minimization%0AAuthor%3A%20Johan%20Hatleskog%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20Degeneracies%20arising%20from%20uninformative%20geometry%20are%20known%20to%20deteriorate%0ALiDAR-based%20localization%20and%20mapping.%20This%20work%20introduces%20a%20new%20probabilistic%0Amethod%20to%20detect%20and%20mitigate%20the%20effect%20of%20degeneracies%20in%20point-to-plane%0Aerror%20minimization.%20The%20noise%20on%20the%20Hessian%20of%20the%20point-to-plane%20optimization%0Aproblem%20is%20characterized%20by%20the%20noise%20on%20points%20and%20surface%20normals%20used%20in%20its%0Aconstruction.%20We%20exploit%20this%20characterization%20to%20quantify%20the%20probability%20of%20a%0Adirection%20being%20degenerate.%20The%20degeneracy-detection%20procedure%20is%20used%20in%20a%20new%0Areal-time%20degeneracy-aware%20iterative%20closest%20point%20algorithm%20for%20LiDAR%0Aregistration%2C%20in%20which%20we%20smoothly%20attenuate%20updates%20in%20degenerate%20directions.%0AThe%20method%27s%20parameters%20are%20selected%20based%20on%20the%20noise%20characteristics%0Aprovided%20in%20the%20LiDAR%27s%20datasheet.%20We%20validate%20the%20approach%20in%20four%20real-world%0Aexperiments%2C%20demonstrating%20that%20it%20outperforms%20state-of-the-art%20methods%20at%0Adetecting%20and%20mitigating%20the%20adverse%20effects%20of%20degeneracies.%20For%20the%20benefit%0Aof%20the%20community%2C%20we%20release%20the%20code%20for%20the%20method%20at%3A%0Agithub.com/ntnu-arl/drpm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Degeneracy%2520Detection%2520for%2520Point-to-Plane%2520Error%2520Minimization%26entry.906535625%3DJohan%2520Hatleskog%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520Degeneracies%2520arising%2520from%2520uninformative%2520geometry%2520are%2520known%2520to%2520deteriorate%250ALiDAR-based%2520localization%2520and%2520mapping.%2520This%2520work%2520introduces%2520a%2520new%2520probabilistic%250Amethod%2520to%2520detect%2520and%2520mitigate%2520the%2520effect%2520of%2520degeneracies%2520in%2520point-to-plane%250Aerror%2520minimization.%2520The%2520noise%2520on%2520the%2520Hessian%2520of%2520the%2520point-to-plane%2520optimization%250Aproblem%2520is%2520characterized%2520by%2520the%2520noise%2520on%2520points%2520and%2520surface%2520normals%2520used%2520in%2520its%250Aconstruction.%2520We%2520exploit%2520this%2520characterization%2520to%2520quantify%2520the%2520probability%2520of%2520a%250Adirection%2520being%2520degenerate.%2520The%2520degeneracy-detection%2520procedure%2520is%2520used%2520in%2520a%2520new%250Areal-time%2520degeneracy-aware%2520iterative%2520closest%2520point%2520algorithm%2520for%2520LiDAR%250Aregistration%252C%2520in%2520which%2520we%2520smoothly%2520attenuate%2520updates%2520in%2520degenerate%2520directions.%250AThe%2520method%2527s%2520parameters%2520are%2520selected%2520based%2520on%2520the%2520noise%2520characteristics%250Aprovided%2520in%2520the%2520LiDAR%2527s%2520datasheet.%2520We%2520validate%2520the%2520approach%2520in%2520four%2520real-world%250Aexperiments%252C%2520demonstrating%2520that%2520it%2520outperforms%2520state-of-the-art%2520methods%2520at%250Adetecting%2520and%2520mitigating%2520the%2520adverse%2520effects%2520of%2520degeneracies.%2520For%2520the%2520benefit%250Aof%2520the%2520community%252C%2520we%2520release%2520the%2520code%2520for%2520the%2520method%2520at%253A%250Agithub.com/ntnu-arl/drpm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Degeneracy%20Detection%20for%20Point-to-Plane%20Error%20Minimization&entry.906535625=Johan%20Hatleskog%20and%20Kostas%20Alexis&entry.1292438233=%20%20Degeneracies%20arising%20from%20uninformative%20geometry%20are%20known%20to%20deteriorate%0ALiDAR-based%20localization%20and%20mapping.%20This%20work%20introduces%20a%20new%20probabilistic%0Amethod%20to%20detect%20and%20mitigate%20the%20effect%20of%20degeneracies%20in%20point-to-plane%0Aerror%20minimization.%20The%20noise%20on%20the%20Hessian%20of%20the%20point-to-plane%20optimization%0Aproblem%20is%20characterized%20by%20the%20noise%20on%20points%20and%20surface%20normals%20used%20in%20its%0Aconstruction.%20We%20exploit%20this%20characterization%20to%20quantify%20the%20probability%20of%20a%0Adirection%20being%20degenerate.%20The%20degeneracy-detection%20procedure%20is%20used%20in%20a%20new%0Areal-time%20degeneracy-aware%20iterative%20closest%20point%20algorithm%20for%20LiDAR%0Aregistration%2C%20in%20which%20we%20smoothly%20attenuate%20updates%20in%20degenerate%20directions.%0AThe%20method%27s%20parameters%20are%20selected%20based%20on%20the%20noise%20characteristics%0Aprovided%20in%20the%20LiDAR%27s%20datasheet.%20We%20validate%20the%20approach%20in%20four%20real-world%0Aexperiments%2C%20demonstrating%20that%20it%20outperforms%20state-of-the-art%20methods%20at%0Adetecting%20and%20mitigating%20the%20adverse%20effects%20of%20degeneracies.%20For%20the%20benefit%0Aof%20the%20community%2C%20we%20release%20the%20code%20for%20the%20method%20at%3A%0Agithub.com/ntnu-arl/drpm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10784v1&entry.124074799=Read"},
{"title": "Adapt-$\\infty$: Scalable Lifelong Multimodal Instruction Tuning via\n  Dynamic Data Selection", "author": "Adyasha Maharana and Jaehong Yoon and Tianlong Chen and Mohit Bansal", "abstract": "  Visual instruction datasets from various distributors are released at\ndifferent times and often contain a significant number of semantically\nredundant text-image pairs, depending on their task compositions (i.e., skills)\nor reference sources. This redundancy greatly limits the efficient deployment\nof lifelong adaptable multimodal large language models, hindering their ability\nto refine existing skills and acquire new competencies over time. To address\nthis, we reframe the problem of Lifelong Instruction Tuning (LiIT) via data\nselection, where the model automatically selects beneficial samples to learn\nfrom earlier and new datasets based on the current state of acquired knowledge\nin the model. Based on empirical analyses that show that selecting the best\ndata subset using a static importance measure is often ineffective for\nmulti-task datasets with evolving distributions, we propose Adapt-$\\infty$, a\nnew multi-way and adaptive data selection approach that dynamically balances\nsample efficiency and effectiveness during LiIT. We construct pseudo-skill\nclusters by grouping gradient-based sample vectors. Next, we select the\nbest-performing data selector for each skill cluster from a pool of selector\nexperts, including our newly proposed scoring function, Image Grounding score.\nThis data selector samples a subset of the most important samples from each\nskill cluster for training. To prevent the continuous increase in the size of\nthe dataset pool during LiIT, which would result in excessive computation, we\nfurther introduce a cluster-wise permanent data pruning strategy to remove the\nmost semantically redundant samples from each cluster, keeping computational\nrequirements manageable. Training with samples selected by Adapt-$\\infty$\nalleviates catastrophic forgetting, especially for rare tasks, and promotes\nforward transfer across the continuum using only a fraction of the original\ndatasets.\n", "link": "http://arxiv.org/abs/2410.10636v1", "date": "2024-10-14", "relevancy": 2.2084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5419}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt-%24%5Cinfty%24%3A%20Scalable%20Lifelong%20Multimodal%20Instruction%20Tuning%20via%0A%20%20Dynamic%20Data%20Selection&body=Title%3A%20Adapt-%24%5Cinfty%24%3A%20Scalable%20Lifelong%20Multimodal%20Instruction%20Tuning%20via%0A%20%20Dynamic%20Data%20Selection%0AAuthor%3A%20Adyasha%20Maharana%20and%20Jaehong%20Yoon%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Visual%20instruction%20datasets%20from%20various%20distributors%20are%20released%20at%0Adifferent%20times%20and%20often%20contain%20a%20significant%20number%20of%20semantically%0Aredundant%20text-image%20pairs%2C%20depending%20on%20their%20task%20compositions%20%28i.e.%2C%20skills%29%0Aor%20reference%20sources.%20This%20redundancy%20greatly%20limits%20the%20efficient%20deployment%0Aof%20lifelong%20adaptable%20multimodal%20large%20language%20models%2C%20hindering%20their%20ability%0Ato%20refine%20existing%20skills%20and%20acquire%20new%20competencies%20over%20time.%20To%20address%0Athis%2C%20we%20reframe%20the%20problem%20of%20Lifelong%20Instruction%20Tuning%20%28LiIT%29%20via%20data%0Aselection%2C%20where%20the%20model%20automatically%20selects%20beneficial%20samples%20to%20learn%0Afrom%20earlier%20and%20new%20datasets%20based%20on%20the%20current%20state%20of%20acquired%20knowledge%0Ain%20the%20model.%20Based%20on%20empirical%20analyses%20that%20show%20that%20selecting%20the%20best%0Adata%20subset%20using%20a%20static%20importance%20measure%20is%20often%20ineffective%20for%0Amulti-task%20datasets%20with%20evolving%20distributions%2C%20we%20propose%20Adapt-%24%5Cinfty%24%2C%20a%0Anew%20multi-way%20and%20adaptive%20data%20selection%20approach%20that%20dynamically%20balances%0Asample%20efficiency%20and%20effectiveness%20during%20LiIT.%20We%20construct%20pseudo-skill%0Aclusters%20by%20grouping%20gradient-based%20sample%20vectors.%20Next%2C%20we%20select%20the%0Abest-performing%20data%20selector%20for%20each%20skill%20cluster%20from%20a%20pool%20of%20selector%0Aexperts%2C%20including%20our%20newly%20proposed%20scoring%20function%2C%20Image%20Grounding%20score.%0AThis%20data%20selector%20samples%20a%20subset%20of%20the%20most%20important%20samples%20from%20each%0Askill%20cluster%20for%20training.%20To%20prevent%20the%20continuous%20increase%20in%20the%20size%20of%0Athe%20dataset%20pool%20during%20LiIT%2C%20which%20would%20result%20in%20excessive%20computation%2C%20we%0Afurther%20introduce%20a%20cluster-wise%20permanent%20data%20pruning%20strategy%20to%20remove%20the%0Amost%20semantically%20redundant%20samples%20from%20each%20cluster%2C%20keeping%20computational%0Arequirements%20manageable.%20Training%20with%20samples%20selected%20by%20Adapt-%24%5Cinfty%24%0Aalleviates%20catastrophic%20forgetting%2C%20especially%20for%20rare%20tasks%2C%20and%20promotes%0Aforward%20transfer%20across%20the%20continuum%20using%20only%20a%20fraction%20of%20the%20original%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt-%2524%255Cinfty%2524%253A%2520Scalable%2520Lifelong%2520Multimodal%2520Instruction%2520Tuning%2520via%250A%2520%2520Dynamic%2520Data%2520Selection%26entry.906535625%3DAdyasha%2520Maharana%2520and%2520Jaehong%2520Yoon%2520and%2520Tianlong%2520Chen%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Visual%2520instruction%2520datasets%2520from%2520various%2520distributors%2520are%2520released%2520at%250Adifferent%2520times%2520and%2520often%2520contain%2520a%2520significant%2520number%2520of%2520semantically%250Aredundant%2520text-image%2520pairs%252C%2520depending%2520on%2520their%2520task%2520compositions%2520%2528i.e.%252C%2520skills%2529%250Aor%2520reference%2520sources.%2520This%2520redundancy%2520greatly%2520limits%2520the%2520efficient%2520deployment%250Aof%2520lifelong%2520adaptable%2520multimodal%2520large%2520language%2520models%252C%2520hindering%2520their%2520ability%250Ato%2520refine%2520existing%2520skills%2520and%2520acquire%2520new%2520competencies%2520over%2520time.%2520To%2520address%250Athis%252C%2520we%2520reframe%2520the%2520problem%2520of%2520Lifelong%2520Instruction%2520Tuning%2520%2528LiIT%2529%2520via%2520data%250Aselection%252C%2520where%2520the%2520model%2520automatically%2520selects%2520beneficial%2520samples%2520to%2520learn%250Afrom%2520earlier%2520and%2520new%2520datasets%2520based%2520on%2520the%2520current%2520state%2520of%2520acquired%2520knowledge%250Ain%2520the%2520model.%2520Based%2520on%2520empirical%2520analyses%2520that%2520show%2520that%2520selecting%2520the%2520best%250Adata%2520subset%2520using%2520a%2520static%2520importance%2520measure%2520is%2520often%2520ineffective%2520for%250Amulti-task%2520datasets%2520with%2520evolving%2520distributions%252C%2520we%2520propose%2520Adapt-%2524%255Cinfty%2524%252C%2520a%250Anew%2520multi-way%2520and%2520adaptive%2520data%2520selection%2520approach%2520that%2520dynamically%2520balances%250Asample%2520efficiency%2520and%2520effectiveness%2520during%2520LiIT.%2520We%2520construct%2520pseudo-skill%250Aclusters%2520by%2520grouping%2520gradient-based%2520sample%2520vectors.%2520Next%252C%2520we%2520select%2520the%250Abest-performing%2520data%2520selector%2520for%2520each%2520skill%2520cluster%2520from%2520a%2520pool%2520of%2520selector%250Aexperts%252C%2520including%2520our%2520newly%2520proposed%2520scoring%2520function%252C%2520Image%2520Grounding%2520score.%250AThis%2520data%2520selector%2520samples%2520a%2520subset%2520of%2520the%2520most%2520important%2520samples%2520from%2520each%250Askill%2520cluster%2520for%2520training.%2520To%2520prevent%2520the%2520continuous%2520increase%2520in%2520the%2520size%2520of%250Athe%2520dataset%2520pool%2520during%2520LiIT%252C%2520which%2520would%2520result%2520in%2520excessive%2520computation%252C%2520we%250Afurther%2520introduce%2520a%2520cluster-wise%2520permanent%2520data%2520pruning%2520strategy%2520to%2520remove%2520the%250Amost%2520semantically%2520redundant%2520samples%2520from%2520each%2520cluster%252C%2520keeping%2520computational%250Arequirements%2520manageable.%2520Training%2520with%2520samples%2520selected%2520by%2520Adapt-%2524%255Cinfty%2524%250Aalleviates%2520catastrophic%2520forgetting%252C%2520especially%2520for%2520rare%2520tasks%252C%2520and%2520promotes%250Aforward%2520transfer%2520across%2520the%2520continuum%2520using%2520only%2520a%2520fraction%2520of%2520the%2520original%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt-%24%5Cinfty%24%3A%20Scalable%20Lifelong%20Multimodal%20Instruction%20Tuning%20via%0A%20%20Dynamic%20Data%20Selection&entry.906535625=Adyasha%20Maharana%20and%20Jaehong%20Yoon%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal&entry.1292438233=%20%20Visual%20instruction%20datasets%20from%20various%20distributors%20are%20released%20at%0Adifferent%20times%20and%20often%20contain%20a%20significant%20number%20of%20semantically%0Aredundant%20text-image%20pairs%2C%20depending%20on%20their%20task%20compositions%20%28i.e.%2C%20skills%29%0Aor%20reference%20sources.%20This%20redundancy%20greatly%20limits%20the%20efficient%20deployment%0Aof%20lifelong%20adaptable%20multimodal%20large%20language%20models%2C%20hindering%20their%20ability%0Ato%20refine%20existing%20skills%20and%20acquire%20new%20competencies%20over%20time.%20To%20address%0Athis%2C%20we%20reframe%20the%20problem%20of%20Lifelong%20Instruction%20Tuning%20%28LiIT%29%20via%20data%0Aselection%2C%20where%20the%20model%20automatically%20selects%20beneficial%20samples%20to%20learn%0Afrom%20earlier%20and%20new%20datasets%20based%20on%20the%20current%20state%20of%20acquired%20knowledge%0Ain%20the%20model.%20Based%20on%20empirical%20analyses%20that%20show%20that%20selecting%20the%20best%0Adata%20subset%20using%20a%20static%20importance%20measure%20is%20often%20ineffective%20for%0Amulti-task%20datasets%20with%20evolving%20distributions%2C%20we%20propose%20Adapt-%24%5Cinfty%24%2C%20a%0Anew%20multi-way%20and%20adaptive%20data%20selection%20approach%20that%20dynamically%20balances%0Asample%20efficiency%20and%20effectiveness%20during%20LiIT.%20We%20construct%20pseudo-skill%0Aclusters%20by%20grouping%20gradient-based%20sample%20vectors.%20Next%2C%20we%20select%20the%0Abest-performing%20data%20selector%20for%20each%20skill%20cluster%20from%20a%20pool%20of%20selector%0Aexperts%2C%20including%20our%20newly%20proposed%20scoring%20function%2C%20Image%20Grounding%20score.%0AThis%20data%20selector%20samples%20a%20subset%20of%20the%20most%20important%20samples%20from%20each%0Askill%20cluster%20for%20training.%20To%20prevent%20the%20continuous%20increase%20in%20the%20size%20of%0Athe%20dataset%20pool%20during%20LiIT%2C%20which%20would%20result%20in%20excessive%20computation%2C%20we%0Afurther%20introduce%20a%20cluster-wise%20permanent%20data%20pruning%20strategy%20to%20remove%20the%0Amost%20semantically%20redundant%20samples%20from%20each%20cluster%2C%20keeping%20computational%0Arequirements%20manageable.%20Training%20with%20samples%20selected%20by%20Adapt-%24%5Cinfty%24%0Aalleviates%20catastrophic%20forgetting%2C%20especially%20for%20rare%20tasks%2C%20and%20promotes%0Aforward%20transfer%20across%20the%20continuum%20using%20only%20a%20fraction%20of%20the%20original%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10636v1&entry.124074799=Read"},
{"title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models", "author": "Mu Cai and Reuben Tan and Jianrui Zhang and Bocheng Zou and Kai Zhang and Feng Yao and Fangrui Zhu and Jing Gu and Yiwu Zhong and Yuzhang Shang and Yao Dou and Jaden Park and Jianfeng Gao and Yong Jae Lee and Jianwei Yang", "abstract": "  Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.\n", "link": "http://arxiv.org/abs/2410.10818v1", "date": "2024-10-14", "relevancy": 2.2058, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models&body=Title%3A%20TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models%0AAuthor%3A%20Mu%20Cai%20and%20Reuben%20Tan%20and%20Jianrui%20Zhang%20and%20Bocheng%20Zou%20and%20Kai%20Zhang%20and%20Feng%20Yao%20and%20Fangrui%20Zhu%20and%20Jing%20Gu%20and%20Yiwu%20Zhong%20and%20Yuzhang%20Shang%20and%20Yao%20Dou%20and%20Jaden%20Park%20and%20Jianfeng%20Gao%20and%20Yong%20Jae%20Lee%20and%20Jianwei%20Yang%0AAbstract%3A%20%20%20Understanding%20fine-grained%20temporal%20dynamics%20is%20crucial%20for%20multimodal%20video%0Acomprehension%20and%20generation.%20Due%20to%20the%20lack%20of%20fine-grained%20temporal%0Aannotations%2C%20existing%20video%20benchmarks%20mostly%20resemble%20static%20image%20benchmarks%0Aand%20are%20incompetent%20at%20evaluating%20models%20for%20temporal%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20TemporalBench%2C%20a%20new%20benchmark%20dedicated%20to%20evaluating%0Afine-grained%20temporal%20understanding%20in%20videos.%20TemporalBench%20consists%20of%20~10K%0Avideo%20question-answer%20pairs%2C%20derived%20from%20~2K%20high-quality%20human%20annotations%0Adetailing%20the%20temporal%20dynamics%20in%20video%20clips.%20As%20a%20result%2C%20our%20benchmark%0Aprovides%20a%20unique%20testbed%20for%20evaluating%20various%20temporal%20understanding%20and%0Areasoning%20abilities%20such%20as%20action%20frequency%2C%20motion%20magnitude%2C%20event%20order%2C%0Aetc.%20Moreover%2C%20it%20enables%20evaluations%20on%20various%20tasks%20like%20both%20video%20question%0Aanswering%20and%20captioning%2C%20both%20short%20and%20long%20video%20understanding%2C%20as%20well%20as%0Adifferent%20models%20such%20as%20multimodal%20video%20embedding%20models%20and%20text%20generation%0Amodels.%20Results%20show%20that%20state-of-the-art%20models%20like%20GPT-4o%20achieve%20only%0A38.5%25%20question%20answering%20accuracy%20on%20TemporalBench%2C%20demonstrating%20a%20significant%0Agap%20%28~30%25%29%20between%20humans%20and%20AI%20in%20temporal%20understanding.%20Furthermore%2C%20we%0Anotice%20a%20critical%20pitfall%20for%20multi-choice%20QA%20where%20LLMs%20can%20detect%20the%20subtle%0Achanges%20in%20negative%20captions%20and%20find%20a%20centralized%20description%20as%20a%20cue%20for%0Aits%20prediction%2C%20where%20we%20propose%20Multiple%20Binary%20Accuracy%20%28MBA%29%20to%20correct%20such%0Abias.%20We%20hope%20that%20TemporalBench%20can%20foster%20research%20on%20improving%20models%27%0Atemporal%20reasoning%20capabilities.%20Both%20dataset%20and%20evaluation%20code%20will%20be%20made%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporalBench%253A%2520Benchmarking%2520Fine-grained%2520Temporal%2520Understanding%2520for%250A%2520%2520Multimodal%2520Video%2520Models%26entry.906535625%3DMu%2520Cai%2520and%2520Reuben%2520Tan%2520and%2520Jianrui%2520Zhang%2520and%2520Bocheng%2520Zou%2520and%2520Kai%2520Zhang%2520and%2520Feng%2520Yao%2520and%2520Fangrui%2520Zhu%2520and%2520Jing%2520Gu%2520and%2520Yiwu%2520Zhong%2520and%2520Yuzhang%2520Shang%2520and%2520Yao%2520Dou%2520and%2520Jaden%2520Park%2520and%2520Jianfeng%2520Gao%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Jianwei%2520Yang%26entry.1292438233%3D%2520%2520Understanding%2520fine-grained%2520temporal%2520dynamics%2520is%2520crucial%2520for%2520multimodal%2520video%250Acomprehension%2520and%2520generation.%2520Due%2520to%2520the%2520lack%2520of%2520fine-grained%2520temporal%250Aannotations%252C%2520existing%2520video%2520benchmarks%2520mostly%2520resemble%2520static%2520image%2520benchmarks%250Aand%2520are%2520incompetent%2520at%2520evaluating%2520models%2520for%2520temporal%2520understanding.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520TemporalBench%252C%2520a%2520new%2520benchmark%2520dedicated%2520to%2520evaluating%250Afine-grained%2520temporal%2520understanding%2520in%2520videos.%2520TemporalBench%2520consists%2520of%2520~10K%250Avideo%2520question-answer%2520pairs%252C%2520derived%2520from%2520~2K%2520high-quality%2520human%2520annotations%250Adetailing%2520the%2520temporal%2520dynamics%2520in%2520video%2520clips.%2520As%2520a%2520result%252C%2520our%2520benchmark%250Aprovides%2520a%2520unique%2520testbed%2520for%2520evaluating%2520various%2520temporal%2520understanding%2520and%250Areasoning%2520abilities%2520such%2520as%2520action%2520frequency%252C%2520motion%2520magnitude%252C%2520event%2520order%252C%250Aetc.%2520Moreover%252C%2520it%2520enables%2520evaluations%2520on%2520various%2520tasks%2520like%2520both%2520video%2520question%250Aanswering%2520and%2520captioning%252C%2520both%2520short%2520and%2520long%2520video%2520understanding%252C%2520as%2520well%2520as%250Adifferent%2520models%2520such%2520as%2520multimodal%2520video%2520embedding%2520models%2520and%2520text%2520generation%250Amodels.%2520Results%2520show%2520that%2520state-of-the-art%2520models%2520like%2520GPT-4o%2520achieve%2520only%250A38.5%2525%2520question%2520answering%2520accuracy%2520on%2520TemporalBench%252C%2520demonstrating%2520a%2520significant%250Agap%2520%2528~30%2525%2529%2520between%2520humans%2520and%2520AI%2520in%2520temporal%2520understanding.%2520Furthermore%252C%2520we%250Anotice%2520a%2520critical%2520pitfall%2520for%2520multi-choice%2520QA%2520where%2520LLMs%2520can%2520detect%2520the%2520subtle%250Achanges%2520in%2520negative%2520captions%2520and%2520find%2520a%2520centralized%2520description%2520as%2520a%2520cue%2520for%250Aits%2520prediction%252C%2520where%2520we%2520propose%2520Multiple%2520Binary%2520Accuracy%2520%2528MBA%2529%2520to%2520correct%2520such%250Abias.%2520We%2520hope%2520that%2520TemporalBench%2520can%2520foster%2520research%2520on%2520improving%2520models%2527%250Atemporal%2520reasoning%2520capabilities.%2520Both%2520dataset%2520and%2520evaluation%2520code%2520will%2520be%2520made%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TemporalBench%3A%20Benchmarking%20Fine-grained%20Temporal%20Understanding%20for%0A%20%20Multimodal%20Video%20Models&entry.906535625=Mu%20Cai%20and%20Reuben%20Tan%20and%20Jianrui%20Zhang%20and%20Bocheng%20Zou%20and%20Kai%20Zhang%20and%20Feng%20Yao%20and%20Fangrui%20Zhu%20and%20Jing%20Gu%20and%20Yiwu%20Zhong%20and%20Yuzhang%20Shang%20and%20Yao%20Dou%20and%20Jaden%20Park%20and%20Jianfeng%20Gao%20and%20Yong%20Jae%20Lee%20and%20Jianwei%20Yang&entry.1292438233=%20%20Understanding%20fine-grained%20temporal%20dynamics%20is%20crucial%20for%20multimodal%20video%0Acomprehension%20and%20generation.%20Due%20to%20the%20lack%20of%20fine-grained%20temporal%0Aannotations%2C%20existing%20video%20benchmarks%20mostly%20resemble%20static%20image%20benchmarks%0Aand%20are%20incompetent%20at%20evaluating%20models%20for%20temporal%20understanding.%20In%20this%0Apaper%2C%20we%20introduce%20TemporalBench%2C%20a%20new%20benchmark%20dedicated%20to%20evaluating%0Afine-grained%20temporal%20understanding%20in%20videos.%20TemporalBench%20consists%20of%20~10K%0Avideo%20question-answer%20pairs%2C%20derived%20from%20~2K%20high-quality%20human%20annotations%0Adetailing%20the%20temporal%20dynamics%20in%20video%20clips.%20As%20a%20result%2C%20our%20benchmark%0Aprovides%20a%20unique%20testbed%20for%20evaluating%20various%20temporal%20understanding%20and%0Areasoning%20abilities%20such%20as%20action%20frequency%2C%20motion%20magnitude%2C%20event%20order%2C%0Aetc.%20Moreover%2C%20it%20enables%20evaluations%20on%20various%20tasks%20like%20both%20video%20question%0Aanswering%20and%20captioning%2C%20both%20short%20and%20long%20video%20understanding%2C%20as%20well%20as%0Adifferent%20models%20such%20as%20multimodal%20video%20embedding%20models%20and%20text%20generation%0Amodels.%20Results%20show%20that%20state-of-the-art%20models%20like%20GPT-4o%20achieve%20only%0A38.5%25%20question%20answering%20accuracy%20on%20TemporalBench%2C%20demonstrating%20a%20significant%0Agap%20%28~30%25%29%20between%20humans%20and%20AI%20in%20temporal%20understanding.%20Furthermore%2C%20we%0Anotice%20a%20critical%20pitfall%20for%20multi-choice%20QA%20where%20LLMs%20can%20detect%20the%20subtle%0Achanges%20in%20negative%20captions%20and%20find%20a%20centralized%20description%20as%20a%20cue%20for%0Aits%20prediction%2C%20where%20we%20propose%20Multiple%20Binary%20Accuracy%20%28MBA%29%20to%20correct%20such%0Abias.%20We%20hope%20that%20TemporalBench%20can%20foster%20research%20on%20improving%20models%27%0Atemporal%20reasoning%20capabilities.%20Both%20dataset%20and%20evaluation%20code%20will%20be%20made%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10818v1&entry.124074799=Read"},
{"title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation", "author": "Peiwen Sun and Sitong Cheng and Xiangtai Li and Zhen Ye and Huadai Liu and Honggang Zhang and Wei Xue and Yike Guo", "abstract": "  Recently, diffusion models have achieved great success in mono-channel audio\ngeneration. However, when it comes to stereo audio generation, the soundscapes\noften have a complex scene of multiple objects and directions. Controlling\nstereo audio with spatial contexts remains challenging due to high data costs\nand unstable generative models. To the best of our knowledge, this work\nrepresents the first attempt to address these issues. We first construct a\nlarge-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant\nsoundscapes and descriptions even including moving and multiple sources. Beyond\ntext modality, we have also acquired a set of images and rationally paired\nstereo audios through retrieval to advance multimodal generation. Existing\naudio generation models tend to generate rather random and indistinct spatial\naudio. To provide accurate guidance for latent diffusion models, we introduce\nthe SpatialSonic model utilizing spatial-aware encoders and azimuth state\nmatrices to reveal reasonable spatial guidance. By leveraging spatial guidance,\nour unified model not only achieves the objective of generating immersive and\ncontrollable spatial audio from text and image but also enables interactive\naudio generation during inference. Finally, under fair settings, we conduct\nsubjective and objective evaluations on simulated and real-world data to\ncompare our approach with prevailing methods. The results demonstrate the\neffectiveness of our method, highlighting its capability to generate spatial\naudio that adheres to physical rules.\n", "link": "http://arxiv.org/abs/2410.10676v1", "date": "2024-10-14", "relevancy": 2.1984, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5908}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation&body=Title%3A%20Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation%0AAuthor%3A%20Peiwen%20Sun%20and%20Sitong%20Cheng%20and%20Xiangtai%20Li%20and%20Zhen%20Ye%20and%20Huadai%20Liu%20and%20Honggang%20Zhang%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Recently%2C%20diffusion%20models%20have%20achieved%20great%20success%20in%20mono-channel%20audio%0Ageneration.%20However%2C%20when%20it%20comes%20to%20stereo%20audio%20generation%2C%20the%20soundscapes%0Aoften%20have%20a%20complex%20scene%20of%20multiple%20objects%20and%20directions.%20Controlling%0Astereo%20audio%20with%20spatial%20contexts%20remains%20challenging%20due%20to%20high%20data%20costs%0Aand%20unstable%20generative%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Arepresents%20the%20first%20attempt%20to%20address%20these%20issues.%20We%20first%20construct%20a%0Alarge-scale%2C%20simulation-based%2C%20and%20GPT-assisted%20dataset%2C%20BEWO-1M%2C%20with%20abundant%0Asoundscapes%20and%20descriptions%20even%20including%20moving%20and%20multiple%20sources.%20Beyond%0Atext%20modality%2C%20we%20have%20also%20acquired%20a%20set%20of%20images%20and%20rationally%20paired%0Astereo%20audios%20through%20retrieval%20to%20advance%20multimodal%20generation.%20Existing%0Aaudio%20generation%20models%20tend%20to%20generate%20rather%20random%20and%20indistinct%20spatial%0Aaudio.%20To%20provide%20accurate%20guidance%20for%20latent%20diffusion%20models%2C%20we%20introduce%0Athe%20SpatialSonic%20model%20utilizing%20spatial-aware%20encoders%20and%20azimuth%20state%0Amatrices%20to%20reveal%20reasonable%20spatial%20guidance.%20By%20leveraging%20spatial%20guidance%2C%0Aour%20unified%20model%20not%20only%20achieves%20the%20objective%20of%20generating%20immersive%20and%0Acontrollable%20spatial%20audio%20from%20text%20and%20image%20but%20also%20enables%20interactive%0Aaudio%20generation%20during%20inference.%20Finally%2C%20under%20fair%20settings%2C%20we%20conduct%0Asubjective%20and%20objective%20evaluations%20on%20simulated%20and%20real-world%20data%20to%0Acompare%20our%20approach%20with%20prevailing%20methods.%20The%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20highlighting%20its%20capability%20to%20generate%20spatial%0Aaudio%20that%20adheres%20to%20physical%20rules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoth%2520Ears%2520Wide%2520Open%253A%2520Towards%2520Language-Driven%2520Spatial%2520Audio%2520Generation%26entry.906535625%3DPeiwen%2520Sun%2520and%2520Sitong%2520Cheng%2520and%2520Xiangtai%2520Li%2520and%2520Zhen%2520Ye%2520and%2520Huadai%2520Liu%2520and%2520Honggang%2520Zhang%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Recently%252C%2520diffusion%2520models%2520have%2520achieved%2520great%2520success%2520in%2520mono-channel%2520audio%250Ageneration.%2520However%252C%2520when%2520it%2520comes%2520to%2520stereo%2520audio%2520generation%252C%2520the%2520soundscapes%250Aoften%2520have%2520a%2520complex%2520scene%2520of%2520multiple%2520objects%2520and%2520directions.%2520Controlling%250Astereo%2520audio%2520with%2520spatial%2520contexts%2520remains%2520challenging%2520due%2520to%2520high%2520data%2520costs%250Aand%2520unstable%2520generative%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%250Arepresents%2520the%2520first%2520attempt%2520to%2520address%2520these%2520issues.%2520We%2520first%2520construct%2520a%250Alarge-scale%252C%2520simulation-based%252C%2520and%2520GPT-assisted%2520dataset%252C%2520BEWO-1M%252C%2520with%2520abundant%250Asoundscapes%2520and%2520descriptions%2520even%2520including%2520moving%2520and%2520multiple%2520sources.%2520Beyond%250Atext%2520modality%252C%2520we%2520have%2520also%2520acquired%2520a%2520set%2520of%2520images%2520and%2520rationally%2520paired%250Astereo%2520audios%2520through%2520retrieval%2520to%2520advance%2520multimodal%2520generation.%2520Existing%250Aaudio%2520generation%2520models%2520tend%2520to%2520generate%2520rather%2520random%2520and%2520indistinct%2520spatial%250Aaudio.%2520To%2520provide%2520accurate%2520guidance%2520for%2520latent%2520diffusion%2520models%252C%2520we%2520introduce%250Athe%2520SpatialSonic%2520model%2520utilizing%2520spatial-aware%2520encoders%2520and%2520azimuth%2520state%250Amatrices%2520to%2520reveal%2520reasonable%2520spatial%2520guidance.%2520By%2520leveraging%2520spatial%2520guidance%252C%250Aour%2520unified%2520model%2520not%2520only%2520achieves%2520the%2520objective%2520of%2520generating%2520immersive%2520and%250Acontrollable%2520spatial%2520audio%2520from%2520text%2520and%2520image%2520but%2520also%2520enables%2520interactive%250Aaudio%2520generation%2520during%2520inference.%2520Finally%252C%2520under%2520fair%2520settings%252C%2520we%2520conduct%250Asubjective%2520and%2520objective%2520evaluations%2520on%2520simulated%2520and%2520real-world%2520data%2520to%250Acompare%2520our%2520approach%2520with%2520prevailing%2520methods.%2520The%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%252C%2520highlighting%2520its%2520capability%2520to%2520generate%2520spatial%250Aaudio%2520that%2520adheres%2520to%2520physical%2520rules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation&entry.906535625=Peiwen%20Sun%20and%20Sitong%20Cheng%20and%20Xiangtai%20Li%20and%20Zhen%20Ye%20and%20Huadai%20Liu%20and%20Honggang%20Zhang%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20Recently%2C%20diffusion%20models%20have%20achieved%20great%20success%20in%20mono-channel%20audio%0Ageneration.%20However%2C%20when%20it%20comes%20to%20stereo%20audio%20generation%2C%20the%20soundscapes%0Aoften%20have%20a%20complex%20scene%20of%20multiple%20objects%20and%20directions.%20Controlling%0Astereo%20audio%20with%20spatial%20contexts%20remains%20challenging%20due%20to%20high%20data%20costs%0Aand%20unstable%20generative%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Arepresents%20the%20first%20attempt%20to%20address%20these%20issues.%20We%20first%20construct%20a%0Alarge-scale%2C%20simulation-based%2C%20and%20GPT-assisted%20dataset%2C%20BEWO-1M%2C%20with%20abundant%0Asoundscapes%20and%20descriptions%20even%20including%20moving%20and%20multiple%20sources.%20Beyond%0Atext%20modality%2C%20we%20have%20also%20acquired%20a%20set%20of%20images%20and%20rationally%20paired%0Astereo%20audios%20through%20retrieval%20to%20advance%20multimodal%20generation.%20Existing%0Aaudio%20generation%20models%20tend%20to%20generate%20rather%20random%20and%20indistinct%20spatial%0Aaudio.%20To%20provide%20accurate%20guidance%20for%20latent%20diffusion%20models%2C%20we%20introduce%0Athe%20SpatialSonic%20model%20utilizing%20spatial-aware%20encoders%20and%20azimuth%20state%0Amatrices%20to%20reveal%20reasonable%20spatial%20guidance.%20By%20leveraging%20spatial%20guidance%2C%0Aour%20unified%20model%20not%20only%20achieves%20the%20objective%20of%20generating%20immersive%20and%0Acontrollable%20spatial%20audio%20from%20text%20and%20image%20but%20also%20enables%20interactive%0Aaudio%20generation%20during%20inference.%20Finally%2C%20under%20fair%20settings%2C%20we%20conduct%0Asubjective%20and%20objective%20evaluations%20on%20simulated%20and%20real-world%20data%20to%0Acompare%20our%20approach%20with%20prevailing%20methods.%20The%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%2C%20highlighting%20its%20capability%20to%20generate%20spatial%0Aaudio%20that%20adheres%20to%20physical%20rules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10676v1&entry.124074799=Read"},
{"title": "SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing", "author": "Pengrui Quan and Xiaomin Ouyang and Jeya Vikranth Jeyakumar and Ziqi Wang and Yang Xing and Mani Srivastava", "abstract": "  Effective processing, interpretation, and management of sensor data have\nemerged as a critical component of cyber-physical systems. Traditionally,\nprocessing sensor data requires profound theoretical knowledge and proficiency\nin signal-processing tools. However, recent works show that Large Language\nModels (LLMs) have promising capabilities in processing sensory data,\nsuggesting their potential as copilots for developing sensing systems.\n  To explore this potential, we construct a comprehensive benchmark,\nSensorBench, to establish a quantifiable objective. The benchmark incorporates\ndiverse real-world sensor datasets for various tasks. The results show that\nwhile LLMs exhibit considerable proficiency in simpler tasks, they face\ninherent challenges in processing compositional tasks with parameter selections\ncompared to engineering experts. Additionally, we investigate four prompting\nstrategies for sensor processing and show that self-verification can outperform\nall other baselines in 48% of tasks. Our study provides a comprehensive\nbenchmark and prompting analysis for future developments, paving the way toward\nan LLM-based sensor processing copilot.\n", "link": "http://arxiv.org/abs/2410.10741v1", "date": "2024-10-14", "relevancy": 2.1936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SensorBench%3A%20Benchmarking%20LLMs%20in%20Coding-Based%20Sensor%20Processing&body=Title%3A%20SensorBench%3A%20Benchmarking%20LLMs%20in%20Coding-Based%20Sensor%20Processing%0AAuthor%3A%20Pengrui%20Quan%20and%20Xiaomin%20Ouyang%20and%20Jeya%20Vikranth%20Jeyakumar%20and%20Ziqi%20Wang%20and%20Yang%20Xing%20and%20Mani%20Srivastava%0AAbstract%3A%20%20%20Effective%20processing%2C%20interpretation%2C%20and%20management%20of%20sensor%20data%20have%0Aemerged%20as%20a%20critical%20component%20of%20cyber-physical%20systems.%20Traditionally%2C%0Aprocessing%20sensor%20data%20requires%20profound%20theoretical%20knowledge%20and%20proficiency%0Ain%20signal-processing%20tools.%20However%2C%20recent%20works%20show%20that%20Large%20Language%0AModels%20%28LLMs%29%20have%20promising%20capabilities%20in%20processing%20sensory%20data%2C%0Asuggesting%20their%20potential%20as%20copilots%20for%20developing%20sensing%20systems.%0A%20%20To%20explore%20this%20potential%2C%20we%20construct%20a%20comprehensive%20benchmark%2C%0ASensorBench%2C%20to%20establish%20a%20quantifiable%20objective.%20The%20benchmark%20incorporates%0Adiverse%20real-world%20sensor%20datasets%20for%20various%20tasks.%20The%20results%20show%20that%0Awhile%20LLMs%20exhibit%20considerable%20proficiency%20in%20simpler%20tasks%2C%20they%20face%0Ainherent%20challenges%20in%20processing%20compositional%20tasks%20with%20parameter%20selections%0Acompared%20to%20engineering%20experts.%20Additionally%2C%20we%20investigate%20four%20prompting%0Astrategies%20for%20sensor%20processing%20and%20show%20that%20self-verification%20can%20outperform%0Aall%20other%20baselines%20in%2048%25%20of%20tasks.%20Our%20study%20provides%20a%20comprehensive%0Abenchmark%20and%20prompting%20analysis%20for%20future%20developments%2C%20paving%20the%20way%20toward%0Aan%20LLM-based%20sensor%20processing%20copilot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSensorBench%253A%2520Benchmarking%2520LLMs%2520in%2520Coding-Based%2520Sensor%2520Processing%26entry.906535625%3DPengrui%2520Quan%2520and%2520Xiaomin%2520Ouyang%2520and%2520Jeya%2520Vikranth%2520Jeyakumar%2520and%2520Ziqi%2520Wang%2520and%2520Yang%2520Xing%2520and%2520Mani%2520Srivastava%26entry.1292438233%3D%2520%2520Effective%2520processing%252C%2520interpretation%252C%2520and%2520management%2520of%2520sensor%2520data%2520have%250Aemerged%2520as%2520a%2520critical%2520component%2520of%2520cyber-physical%2520systems.%2520Traditionally%252C%250Aprocessing%2520sensor%2520data%2520requires%2520profound%2520theoretical%2520knowledge%2520and%2520proficiency%250Ain%2520signal-processing%2520tools.%2520However%252C%2520recent%2520works%2520show%2520that%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520promising%2520capabilities%2520in%2520processing%2520sensory%2520data%252C%250Asuggesting%2520their%2520potential%2520as%2520copilots%2520for%2520developing%2520sensing%2520systems.%250A%2520%2520To%2520explore%2520this%2520potential%252C%2520we%2520construct%2520a%2520comprehensive%2520benchmark%252C%250ASensorBench%252C%2520to%2520establish%2520a%2520quantifiable%2520objective.%2520The%2520benchmark%2520incorporates%250Adiverse%2520real-world%2520sensor%2520datasets%2520for%2520various%2520tasks.%2520The%2520results%2520show%2520that%250Awhile%2520LLMs%2520exhibit%2520considerable%2520proficiency%2520in%2520simpler%2520tasks%252C%2520they%2520face%250Ainherent%2520challenges%2520in%2520processing%2520compositional%2520tasks%2520with%2520parameter%2520selections%250Acompared%2520to%2520engineering%2520experts.%2520Additionally%252C%2520we%2520investigate%2520four%2520prompting%250Astrategies%2520for%2520sensor%2520processing%2520and%2520show%2520that%2520self-verification%2520can%2520outperform%250Aall%2520other%2520baselines%2520in%252048%2525%2520of%2520tasks.%2520Our%2520study%2520provides%2520a%2520comprehensive%250Abenchmark%2520and%2520prompting%2520analysis%2520for%2520future%2520developments%252C%2520paving%2520the%2520way%2520toward%250Aan%2520LLM-based%2520sensor%2520processing%2520copilot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SensorBench%3A%20Benchmarking%20LLMs%20in%20Coding-Based%20Sensor%20Processing&entry.906535625=Pengrui%20Quan%20and%20Xiaomin%20Ouyang%20and%20Jeya%20Vikranth%20Jeyakumar%20and%20Ziqi%20Wang%20and%20Yang%20Xing%20and%20Mani%20Srivastava&entry.1292438233=%20%20Effective%20processing%2C%20interpretation%2C%20and%20management%20of%20sensor%20data%20have%0Aemerged%20as%20a%20critical%20component%20of%20cyber-physical%20systems.%20Traditionally%2C%0Aprocessing%20sensor%20data%20requires%20profound%20theoretical%20knowledge%20and%20proficiency%0Ain%20signal-processing%20tools.%20However%2C%20recent%20works%20show%20that%20Large%20Language%0AModels%20%28LLMs%29%20have%20promising%20capabilities%20in%20processing%20sensory%20data%2C%0Asuggesting%20their%20potential%20as%20copilots%20for%20developing%20sensing%20systems.%0A%20%20To%20explore%20this%20potential%2C%20we%20construct%20a%20comprehensive%20benchmark%2C%0ASensorBench%2C%20to%20establish%20a%20quantifiable%20objective.%20The%20benchmark%20incorporates%0Adiverse%20real-world%20sensor%20datasets%20for%20various%20tasks.%20The%20results%20show%20that%0Awhile%20LLMs%20exhibit%20considerable%20proficiency%20in%20simpler%20tasks%2C%20they%20face%0Ainherent%20challenges%20in%20processing%20compositional%20tasks%20with%20parameter%20selections%0Acompared%20to%20engineering%20experts.%20Additionally%2C%20we%20investigate%20four%20prompting%0Astrategies%20for%20sensor%20processing%20and%20show%20that%20self-verification%20can%20outperform%0Aall%20other%20baselines%20in%2048%25%20of%20tasks.%20Our%20study%20provides%20a%20comprehensive%0Abenchmark%20and%20prompting%20analysis%20for%20future%20developments%2C%20paving%20the%20way%20toward%0Aan%20LLM-based%20sensor%20processing%20copilot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10741v1&entry.124074799=Read"},
{"title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent\n  poLicy for InTerpretability", "author": "Shuang Ao and Simon Khan and Haris Aziz and Flora D. Salim", "abstract": "  Understanding the agent's learning process, particularly the factors that\ncontribute to its success or failure post-training, is crucial for\ncomprehending the rationale behind the agent's decision-making process. Prior\nmethods clarify the learning process by creating a structural causal model\n(SCM) or visually representing the distribution of value functions.\nNevertheless, these approaches have constraints as they exclusively function in\n2D-environments or with uncomplicated transition dynamics. Understanding the\nagent's learning process in complicated environments or tasks is more\nchallenging. In this paper, we propose REVEAL-IT, a novel framework for\nexplaining the learning process of an agent in complex environments. Initially,\nwe visualize the policy structure and the agent's learning process for various\ntraining tasks. By visualizing these findings, we can understand how much a\nparticular training task or stage affects the agent's performance in test.\nThen, a GNN-based explainer learns to highlight the most important section of\nthe policy, providing a more clear and robust explanation of the agent's\nlearning process. The experiments demonstrate that explanations derived from\nthis framework can effectively help in the optimization of the training tasks,\nresulting in improved learning efficiency and final performance.\n", "link": "http://arxiv.org/abs/2406.14214v6", "date": "2024-10-14", "relevancy": 2.192, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.551}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5487}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&body=Title%3A%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability%0AAuthor%3A%20Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim%0AAbstract%3A%20%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14214v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEAL-IT%253A%2520REinforcement%2520learning%2520with%2520Visibility%2520of%2520Evolving%2520Agent%250A%2520%2520poLicy%2520for%2520InTerpretability%26entry.906535625%3DShuang%2520Ao%2520and%2520Simon%2520Khan%2520and%2520Haris%2520Aziz%2520and%2520Flora%2520D.%2520Salim%26entry.1292438233%3D%2520%2520Understanding%2520the%2520agent%2527s%2520learning%2520process%252C%2520particularly%2520the%2520factors%2520that%250Acontribute%2520to%2520its%2520success%2520or%2520failure%2520post-training%252C%2520is%2520crucial%2520for%250Acomprehending%2520the%2520rationale%2520behind%2520the%2520agent%2527s%2520decision-making%2520process.%2520Prior%250Amethods%2520clarify%2520the%2520learning%2520process%2520by%2520creating%2520a%2520structural%2520causal%2520model%250A%2528SCM%2529%2520or%2520visually%2520representing%2520the%2520distribution%2520of%2520value%2520functions.%250ANevertheless%252C%2520these%2520approaches%2520have%2520constraints%2520as%2520they%2520exclusively%2520function%2520in%250A2D-environments%2520or%2520with%2520uncomplicated%2520transition%2520dynamics.%2520Understanding%2520the%250Aagent%2527s%2520learning%2520process%2520in%2520complicated%2520environments%2520or%2520tasks%2520is%2520more%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520REVEAL-IT%252C%2520a%2520novel%2520framework%2520for%250Aexplaining%2520the%2520learning%2520process%2520of%2520an%2520agent%2520in%2520complex%2520environments.%2520Initially%252C%250Awe%2520visualize%2520the%2520policy%2520structure%2520and%2520the%2520agent%2527s%2520learning%2520process%2520for%2520various%250Atraining%2520tasks.%2520By%2520visualizing%2520these%2520findings%252C%2520we%2520can%2520understand%2520how%2520much%2520a%250Aparticular%2520training%2520task%2520or%2520stage%2520affects%2520the%2520agent%2527s%2520performance%2520in%2520test.%250AThen%252C%2520a%2520GNN-based%2520explainer%2520learns%2520to%2520highlight%2520the%2520most%2520important%2520section%2520of%250Athe%2520policy%252C%2520providing%2520a%2520more%2520clear%2520and%2520robust%2520explanation%2520of%2520the%2520agent%2527s%250Alearning%2520process.%2520The%2520experiments%2520demonstrate%2520that%2520explanations%2520derived%2520from%250Athis%2520framework%2520can%2520effectively%2520help%2520in%2520the%2520optimization%2520of%2520the%2520training%2520tasks%252C%250Aresulting%2520in%2520improved%2520learning%2520efficiency%2520and%2520final%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14214v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&entry.906535625=Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim&entry.1292438233=%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%20training%20tasks%2C%0Aresulting%20in%20improved%20learning%20efficiency%20and%20final%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14214v6&entry.124074799=Read"},
{"title": "Flexible Heteroscedastic Count Regression with Deep Double Poisson\n  Networks", "author": "Spencer Young and Porter Jenkins and Lonchao Da and Jeff Dotson and Hua Wei", "abstract": "  Neural networks that can produce accurate, input-conditional uncertainty\nrepresentations are critical for real-world applications. Recent progress on\nheteroscedastic continuous regression has shown great promise for calibrated\nuncertainty quantification on complex tasks, like image regression. However,\nwhen these methods are applied to discrete regression tasks, such as crowd\ncounting, ratings prediction, or inventory estimation, they tend to produce\npredictive distributions with numerous pathologies. Moreover, discrete models\nbased on the Generalized Linear Model (GLM) framework either cannot process\ncomplex input or are not fully heterosedastic. To address these issues we\npropose the Deep Double Poisson Network (DDPN). In contrast to networks trained\nto minimize Gaussian negative log likelihood (NLL), discrete network\nparameterizations (i.e., Poisson, Negative binomial), and GLMs, DDPN can\nproduce discrete predictive distributions of arbitrary flexibility.\nAdditionally, we propose a technique to tune the prioritization of mean fit and\nprobabilistic calibration during training. We show DDPN 1) vastly outperforms\nexisting discrete models; 2) meets or exceeds the accuracy and flexibility of\nnetworks trained with Gaussian NLL; 3) produces proper predictive distributions\nover discrete counts; and 4) exhibits superior out-of-distribution detection.\nDDPN can easily be applied to a variety of count regression datasets including\ntabular, image, point cloud, and text data.\n", "link": "http://arxiv.org/abs/2406.09262v2", "date": "2024-10-14", "relevancy": 2.169, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5622}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5296}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks&body=Title%3A%20Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks%0AAuthor%3A%20Spencer%20Young%20and%20Porter%20Jenkins%20and%20Lonchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei%0AAbstract%3A%20%20%20Neural%20networks%20that%20can%20produce%20accurate%2C%20input-conditional%20uncertainty%0Arepresentations%20are%20critical%20for%20real-world%20applications.%20Recent%20progress%20on%0Aheteroscedastic%20continuous%20regression%20has%20shown%20great%20promise%20for%20calibrated%0Auncertainty%20quantification%20on%20complex%20tasks%2C%20like%20image%20regression.%20However%2C%0Awhen%20these%20methods%20are%20applied%20to%20discrete%20regression%20tasks%2C%20such%20as%20crowd%0Acounting%2C%20ratings%20prediction%2C%20or%20inventory%20estimation%2C%20they%20tend%20to%20produce%0Apredictive%20distributions%20with%20numerous%20pathologies.%20Moreover%2C%20discrete%20models%0Abased%20on%20the%20Generalized%20Linear%20Model%20%28GLM%29%20framework%20either%20cannot%20process%0Acomplex%20input%20or%20are%20not%20fully%20heterosedastic.%20To%20address%20these%20issues%20we%0Apropose%20the%20Deep%20Double%20Poisson%20Network%20%28DDPN%29.%20In%20contrast%20to%20networks%20trained%0Ato%20minimize%20Gaussian%20negative%20log%20likelihood%20%28NLL%29%2C%20discrete%20network%0Aparameterizations%20%28i.e.%2C%20Poisson%2C%20Negative%20binomial%29%2C%20and%20GLMs%2C%20DDPN%20can%0Aproduce%20discrete%20predictive%20distributions%20of%20arbitrary%20flexibility.%0AAdditionally%2C%20we%20propose%20a%20technique%20to%20tune%20the%20prioritization%20of%20mean%20fit%20and%0Aprobabilistic%20calibration%20during%20training.%20We%20show%20DDPN%201%29%20vastly%20outperforms%0Aexisting%20discrete%20models%3B%202%29%20meets%20or%20exceeds%20the%20accuracy%20and%20flexibility%20of%0Anetworks%20trained%20with%20Gaussian%20NLL%3B%203%29%20produces%20proper%20predictive%20distributions%0Aover%20discrete%20counts%3B%20and%204%29%20exhibits%20superior%20out-of-distribution%20detection.%0ADDPN%20can%20easily%20be%20applied%20to%20a%20variety%20of%20count%20regression%20datasets%20including%0Atabular%2C%20image%2C%20point%20cloud%2C%20and%20text%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520Heteroscedastic%2520Count%2520Regression%2520with%2520Deep%2520Double%2520Poisson%250A%2520%2520Networks%26entry.906535625%3DSpencer%2520Young%2520and%2520Porter%2520Jenkins%2520and%2520Lonchao%2520Da%2520and%2520Jeff%2520Dotson%2520and%2520Hua%2520Wei%26entry.1292438233%3D%2520%2520Neural%2520networks%2520that%2520can%2520produce%2520accurate%252C%2520input-conditional%2520uncertainty%250Arepresentations%2520are%2520critical%2520for%2520real-world%2520applications.%2520Recent%2520progress%2520on%250Aheteroscedastic%2520continuous%2520regression%2520has%2520shown%2520great%2520promise%2520for%2520calibrated%250Auncertainty%2520quantification%2520on%2520complex%2520tasks%252C%2520like%2520image%2520regression.%2520However%252C%250Awhen%2520these%2520methods%2520are%2520applied%2520to%2520discrete%2520regression%2520tasks%252C%2520such%2520as%2520crowd%250Acounting%252C%2520ratings%2520prediction%252C%2520or%2520inventory%2520estimation%252C%2520they%2520tend%2520to%2520produce%250Apredictive%2520distributions%2520with%2520numerous%2520pathologies.%2520Moreover%252C%2520discrete%2520models%250Abased%2520on%2520the%2520Generalized%2520Linear%2520Model%2520%2528GLM%2529%2520framework%2520either%2520cannot%2520process%250Acomplex%2520input%2520or%2520are%2520not%2520fully%2520heterosedastic.%2520To%2520address%2520these%2520issues%2520we%250Apropose%2520the%2520Deep%2520Double%2520Poisson%2520Network%2520%2528DDPN%2529.%2520In%2520contrast%2520to%2520networks%2520trained%250Ato%2520minimize%2520Gaussian%2520negative%2520log%2520likelihood%2520%2528NLL%2529%252C%2520discrete%2520network%250Aparameterizations%2520%2528i.e.%252C%2520Poisson%252C%2520Negative%2520binomial%2529%252C%2520and%2520GLMs%252C%2520DDPN%2520can%250Aproduce%2520discrete%2520predictive%2520distributions%2520of%2520arbitrary%2520flexibility.%250AAdditionally%252C%2520we%2520propose%2520a%2520technique%2520to%2520tune%2520the%2520prioritization%2520of%2520mean%2520fit%2520and%250Aprobabilistic%2520calibration%2520during%2520training.%2520We%2520show%2520DDPN%25201%2529%2520vastly%2520outperforms%250Aexisting%2520discrete%2520models%253B%25202%2529%2520meets%2520or%2520exceeds%2520the%2520accuracy%2520and%2520flexibility%2520of%250Anetworks%2520trained%2520with%2520Gaussian%2520NLL%253B%25203%2529%2520produces%2520proper%2520predictive%2520distributions%250Aover%2520discrete%2520counts%253B%2520and%25204%2529%2520exhibits%2520superior%2520out-of-distribution%2520detection.%250ADDPN%2520can%2520easily%2520be%2520applied%2520to%2520a%2520variety%2520of%2520count%2520regression%2520datasets%2520including%250Atabular%252C%2520image%252C%2520point%2520cloud%252C%2520and%2520text%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks&entry.906535625=Spencer%20Young%20and%20Porter%20Jenkins%20and%20Lonchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei&entry.1292438233=%20%20Neural%20networks%20that%20can%20produce%20accurate%2C%20input-conditional%20uncertainty%0Arepresentations%20are%20critical%20for%20real-world%20applications.%20Recent%20progress%20on%0Aheteroscedastic%20continuous%20regression%20has%20shown%20great%20promise%20for%20calibrated%0Auncertainty%20quantification%20on%20complex%20tasks%2C%20like%20image%20regression.%20However%2C%0Awhen%20these%20methods%20are%20applied%20to%20discrete%20regression%20tasks%2C%20such%20as%20crowd%0Acounting%2C%20ratings%20prediction%2C%20or%20inventory%20estimation%2C%20they%20tend%20to%20produce%0Apredictive%20distributions%20with%20numerous%20pathologies.%20Moreover%2C%20discrete%20models%0Abased%20on%20the%20Generalized%20Linear%20Model%20%28GLM%29%20framework%20either%20cannot%20process%0Acomplex%20input%20or%20are%20not%20fully%20heterosedastic.%20To%20address%20these%20issues%20we%0Apropose%20the%20Deep%20Double%20Poisson%20Network%20%28DDPN%29.%20In%20contrast%20to%20networks%20trained%0Ato%20minimize%20Gaussian%20negative%20log%20likelihood%20%28NLL%29%2C%20discrete%20network%0Aparameterizations%20%28i.e.%2C%20Poisson%2C%20Negative%20binomial%29%2C%20and%20GLMs%2C%20DDPN%20can%0Aproduce%20discrete%20predictive%20distributions%20of%20arbitrary%20flexibility.%0AAdditionally%2C%20we%20propose%20a%20technique%20to%20tune%20the%20prioritization%20of%20mean%20fit%20and%0Aprobabilistic%20calibration%20during%20training.%20We%20show%20DDPN%201%29%20vastly%20outperforms%0Aexisting%20discrete%20models%3B%202%29%20meets%20or%20exceeds%20the%20accuracy%20and%20flexibility%20of%0Anetworks%20trained%20with%20Gaussian%20NLL%3B%203%29%20produces%20proper%20predictive%20distributions%0Aover%20discrete%20counts%3B%20and%204%29%20exhibits%20superior%20out-of-distribution%20detection.%0ADDPN%20can%20easily%20be%20applied%20to%20a%20variety%20of%20count%20regression%20datasets%20including%0Atabular%2C%20image%2C%20point%20cloud%2C%20and%20text%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09262v2&entry.124074799=Read"},
{"title": "Affinity-Graph-Guided Contractive Learning for Pretext-Free Medical\n  Image Segmentation with Minimal Annotation", "author": "Zehua Cheng and Di Yuan and Thomas Lukasiewicz", "abstract": "  The combination of semi-supervised learning (SemiSL) and contrastive learning\n(CL) has been successful in medical image segmentation with limited\nannotations. However, these works often rely on pretext tasks that lack the\nspecificity required for pixel-level segmentation, and still face overfitting\nissues due to insufficient supervision signals resulting from too few\nannotations. Therefore, this paper proposes an affinity-graph-guided\nsemi-supervised contrastive learning framework (Semi-AGCL) by establishing\nadditional affinity-graph-based supervision signals between the student and\nteacher network, to achieve medical image segmentation with minimal annotations\nwithout pretext. The framework first designs an average-patch-entropy-driven\ninter-patch sampling method, which can provide a robust initial feature space\nwithout relying on pretext tasks. Furthermore, the framework designs an\naffinity-graph-guided loss function, which can improve the quality of the\nlearned representation and the model generalization ability by exploiting the\ninherent structure of the data, thus mitigating overfitting. Our experiments\nindicate that with merely 10% of the complete annotation set, our model\napproaches the accuracy of the fully annotated baseline, manifesting a marginal\ndeviation of only 2.52%. Under the stringent conditions where only 5% of the\nannotations are employed, our model exhibits a significant enhancement in\nperformance surpassing the second best baseline by 23.09% on the dice metric\nand achieving an improvement of 26.57% on the notably arduous CRAG and ACDC\ndatasets.\n", "link": "http://arxiv.org/abs/2410.10366v1", "date": "2024-10-14", "relevancy": 2.1628, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affinity-Graph-Guided%20Contractive%20Learning%20for%20Pretext-Free%20Medical%0A%20%20Image%20Segmentation%20with%20Minimal%20Annotation&body=Title%3A%20Affinity-Graph-Guided%20Contractive%20Learning%20for%20Pretext-Free%20Medical%0A%20%20Image%20Segmentation%20with%20Minimal%20Annotation%0AAuthor%3A%20Zehua%20Cheng%20and%20Di%20Yuan%20and%20Thomas%20Lukasiewicz%0AAbstract%3A%20%20%20The%20combination%20of%20semi-supervised%20learning%20%28SemiSL%29%20and%20contrastive%20learning%0A%28CL%29%20has%20been%20successful%20in%20medical%20image%20segmentation%20with%20limited%0Aannotations.%20However%2C%20these%20works%20often%20rely%20on%20pretext%20tasks%20that%20lack%20the%0Aspecificity%20required%20for%20pixel-level%20segmentation%2C%20and%20still%20face%20overfitting%0Aissues%20due%20to%20insufficient%20supervision%20signals%20resulting%20from%20too%20few%0Aannotations.%20Therefore%2C%20this%20paper%20proposes%20an%20affinity-graph-guided%0Asemi-supervised%20contrastive%20learning%20framework%20%28Semi-AGCL%29%20by%20establishing%0Aadditional%20affinity-graph-based%20supervision%20signals%20between%20the%20student%20and%0Ateacher%20network%2C%20to%20achieve%20medical%20image%20segmentation%20with%20minimal%20annotations%0Awithout%20pretext.%20The%20framework%20first%20designs%20an%20average-patch-entropy-driven%0Ainter-patch%20sampling%20method%2C%20which%20can%20provide%20a%20robust%20initial%20feature%20space%0Awithout%20relying%20on%20pretext%20tasks.%20Furthermore%2C%20the%20framework%20designs%20an%0Aaffinity-graph-guided%20loss%20function%2C%20which%20can%20improve%20the%20quality%20of%20the%0Alearned%20representation%20and%20the%20model%20generalization%20ability%20by%20exploiting%20the%0Ainherent%20structure%20of%20the%20data%2C%20thus%20mitigating%20overfitting.%20Our%20experiments%0Aindicate%20that%20with%20merely%2010%25%20of%20the%20complete%20annotation%20set%2C%20our%20model%0Aapproaches%20the%20accuracy%20of%20the%20fully%20annotated%20baseline%2C%20manifesting%20a%20marginal%0Adeviation%20of%20only%202.52%25.%20Under%20the%20stringent%20conditions%20where%20only%205%25%20of%20the%0Aannotations%20are%20employed%2C%20our%20model%20exhibits%20a%20significant%20enhancement%20in%0Aperformance%20surpassing%20the%20second%20best%20baseline%20by%2023.09%25%20on%20the%20dice%20metric%0Aand%20achieving%20an%20improvement%20of%2026.57%25%20on%20the%20notably%20arduous%20CRAG%20and%20ACDC%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffinity-Graph-Guided%2520Contractive%2520Learning%2520for%2520Pretext-Free%2520Medical%250A%2520%2520Image%2520Segmentation%2520with%2520Minimal%2520Annotation%26entry.906535625%3DZehua%2520Cheng%2520and%2520Di%2520Yuan%2520and%2520Thomas%2520Lukasiewicz%26entry.1292438233%3D%2520%2520The%2520combination%2520of%2520semi-supervised%2520learning%2520%2528SemiSL%2529%2520and%2520contrastive%2520learning%250A%2528CL%2529%2520has%2520been%2520successful%2520in%2520medical%2520image%2520segmentation%2520with%2520limited%250Aannotations.%2520However%252C%2520these%2520works%2520often%2520rely%2520on%2520pretext%2520tasks%2520that%2520lack%2520the%250Aspecificity%2520required%2520for%2520pixel-level%2520segmentation%252C%2520and%2520still%2520face%2520overfitting%250Aissues%2520due%2520to%2520insufficient%2520supervision%2520signals%2520resulting%2520from%2520too%2520few%250Aannotations.%2520Therefore%252C%2520this%2520paper%2520proposes%2520an%2520affinity-graph-guided%250Asemi-supervised%2520contrastive%2520learning%2520framework%2520%2528Semi-AGCL%2529%2520by%2520establishing%250Aadditional%2520affinity-graph-based%2520supervision%2520signals%2520between%2520the%2520student%2520and%250Ateacher%2520network%252C%2520to%2520achieve%2520medical%2520image%2520segmentation%2520with%2520minimal%2520annotations%250Awithout%2520pretext.%2520The%2520framework%2520first%2520designs%2520an%2520average-patch-entropy-driven%250Ainter-patch%2520sampling%2520method%252C%2520which%2520can%2520provide%2520a%2520robust%2520initial%2520feature%2520space%250Awithout%2520relying%2520on%2520pretext%2520tasks.%2520Furthermore%252C%2520the%2520framework%2520designs%2520an%250Aaffinity-graph-guided%2520loss%2520function%252C%2520which%2520can%2520improve%2520the%2520quality%2520of%2520the%250Alearned%2520representation%2520and%2520the%2520model%2520generalization%2520ability%2520by%2520exploiting%2520the%250Ainherent%2520structure%2520of%2520the%2520data%252C%2520thus%2520mitigating%2520overfitting.%2520Our%2520experiments%250Aindicate%2520that%2520with%2520merely%252010%2525%2520of%2520the%2520complete%2520annotation%2520set%252C%2520our%2520model%250Aapproaches%2520the%2520accuracy%2520of%2520the%2520fully%2520annotated%2520baseline%252C%2520manifesting%2520a%2520marginal%250Adeviation%2520of%2520only%25202.52%2525.%2520Under%2520the%2520stringent%2520conditions%2520where%2520only%25205%2525%2520of%2520the%250Aannotations%2520are%2520employed%252C%2520our%2520model%2520exhibits%2520a%2520significant%2520enhancement%2520in%250Aperformance%2520surpassing%2520the%2520second%2520best%2520baseline%2520by%252023.09%2525%2520on%2520the%2520dice%2520metric%250Aand%2520achieving%2520an%2520improvement%2520of%252026.57%2525%2520on%2520the%2520notably%2520arduous%2520CRAG%2520and%2520ACDC%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affinity-Graph-Guided%20Contractive%20Learning%20for%20Pretext-Free%20Medical%0A%20%20Image%20Segmentation%20with%20Minimal%20Annotation&entry.906535625=Zehua%20Cheng%20and%20Di%20Yuan%20and%20Thomas%20Lukasiewicz&entry.1292438233=%20%20The%20combination%20of%20semi-supervised%20learning%20%28SemiSL%29%20and%20contrastive%20learning%0A%28CL%29%20has%20been%20successful%20in%20medical%20image%20segmentation%20with%20limited%0Aannotations.%20However%2C%20these%20works%20often%20rely%20on%20pretext%20tasks%20that%20lack%20the%0Aspecificity%20required%20for%20pixel-level%20segmentation%2C%20and%20still%20face%20overfitting%0Aissues%20due%20to%20insufficient%20supervision%20signals%20resulting%20from%20too%20few%0Aannotations.%20Therefore%2C%20this%20paper%20proposes%20an%20affinity-graph-guided%0Asemi-supervised%20contrastive%20learning%20framework%20%28Semi-AGCL%29%20by%20establishing%0Aadditional%20affinity-graph-based%20supervision%20signals%20between%20the%20student%20and%0Ateacher%20network%2C%20to%20achieve%20medical%20image%20segmentation%20with%20minimal%20annotations%0Awithout%20pretext.%20The%20framework%20first%20designs%20an%20average-patch-entropy-driven%0Ainter-patch%20sampling%20method%2C%20which%20can%20provide%20a%20robust%20initial%20feature%20space%0Awithout%20relying%20on%20pretext%20tasks.%20Furthermore%2C%20the%20framework%20designs%20an%0Aaffinity-graph-guided%20loss%20function%2C%20which%20can%20improve%20the%20quality%20of%20the%0Alearned%20representation%20and%20the%20model%20generalization%20ability%20by%20exploiting%20the%0Ainherent%20structure%20of%20the%20data%2C%20thus%20mitigating%20overfitting.%20Our%20experiments%0Aindicate%20that%20with%20merely%2010%25%20of%20the%20complete%20annotation%20set%2C%20our%20model%0Aapproaches%20the%20accuracy%20of%20the%20fully%20annotated%20baseline%2C%20manifesting%20a%20marginal%0Adeviation%20of%20only%202.52%25.%20Under%20the%20stringent%20conditions%20where%20only%205%25%20of%20the%0Aannotations%20are%20employed%2C%20our%20model%20exhibits%20a%20significant%20enhancement%20in%0Aperformance%20surpassing%20the%20second%20best%20baseline%20by%2023.09%25%20on%20the%20dice%20metric%0Aand%20achieving%20an%20improvement%20of%2026.57%25%20on%20the%20notably%20arduous%20CRAG%20and%20ACDC%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10366v1&entry.124074799=Read"},
{"title": "V2M: Visual 2-Dimensional Mamba for Image Representation Learning", "author": "Chengkun Wang and Wenzhao Zheng and Yuanhui Huang and Jie Zhou and Jiwen Lu", "abstract": "  Mamba has garnered widespread attention due to its flexible design and\nefficient hardware performance to process 1D sequences based on the state space\nmodel (SSM). Recent studies have attempted to apply Mamba to the visual domain\nby flattening 2D images into patches and then regarding them as a 1D sequence.\nTo compensate for the 2D structure information loss (e.g., local similarity) of\nthe original image, most existing methods focus on designing different orders\nto sequentially process the tokens, which could only alleviate this issue to\nsome extent. In this paper, we propose a Visual 2-Dimensional Mamba (V2M) model\nas a complete solution, which directly processes image tokens in the 2D space.\nWe first generalize SSM to the 2-dimensional space which generates the next\nstate considering two adjacent states on both dimensions (e.g., columns and\nrows). We then construct our V2M based on the 2-dimensional SSM formulation and\nincorporate Mamba to achieve hardware-efficient parallel processing. The\nproposed V2M effectively incorporates the 2D locality prior yet inherits the\nefficiency and input-dependent scalability of Mamba. Extensive experimental\nresults on ImageNet classification and downstream visual tasks including object\ndetection and instance segmentation on COCO and semantic segmentation on ADE20K\ndemonstrate the effectiveness of our V2M compared with other visual backbones.\n", "link": "http://arxiv.org/abs/2410.10382v1", "date": "2024-10-14", "relevancy": 2.1603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5321}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2M%3A%20Visual%202-Dimensional%20Mamba%20for%20Image%20Representation%20Learning&body=Title%3A%20V2M%3A%20Visual%202-Dimensional%20Mamba%20for%20Image%20Representation%20Learning%0AAuthor%3A%20Chengkun%20Wang%20and%20Wenzhao%20Zheng%20and%20Yuanhui%20Huang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Mamba%20has%20garnered%20widespread%20attention%20due%20to%20its%20flexible%20design%20and%0Aefficient%20hardware%20performance%20to%20process%201D%20sequences%20based%20on%20the%20state%20space%0Amodel%20%28SSM%29.%20Recent%20studies%20have%20attempted%20to%20apply%20Mamba%20to%20the%20visual%20domain%0Aby%20flattening%202D%20images%20into%20patches%20and%20then%20regarding%20them%20as%20a%201D%20sequence.%0ATo%20compensate%20for%20the%202D%20structure%20information%20loss%20%28e.g.%2C%20local%20similarity%29%20of%0Athe%20original%20image%2C%20most%20existing%20methods%20focus%20on%20designing%20different%20orders%0Ato%20sequentially%20process%20the%20tokens%2C%20which%20could%20only%20alleviate%20this%20issue%20to%0Asome%20extent.%20In%20this%20paper%2C%20we%20propose%20a%20Visual%202-Dimensional%20Mamba%20%28V2M%29%20model%0Aas%20a%20complete%20solution%2C%20which%20directly%20processes%20image%20tokens%20in%20the%202D%20space.%0AWe%20first%20generalize%20SSM%20to%20the%202-dimensional%20space%20which%20generates%20the%20next%0Astate%20considering%20two%20adjacent%20states%20on%20both%20dimensions%20%28e.g.%2C%20columns%20and%0Arows%29.%20We%20then%20construct%20our%20V2M%20based%20on%20the%202-dimensional%20SSM%20formulation%20and%0Aincorporate%20Mamba%20to%20achieve%20hardware-efficient%20parallel%20processing.%20The%0Aproposed%20V2M%20effectively%20incorporates%20the%202D%20locality%20prior%20yet%20inherits%20the%0Aefficiency%20and%20input-dependent%20scalability%20of%20Mamba.%20Extensive%20experimental%0Aresults%20on%20ImageNet%20classification%20and%20downstream%20visual%20tasks%20including%20object%0Adetection%20and%20instance%20segmentation%20on%20COCO%20and%20semantic%20segmentation%20on%20ADE20K%0Ademonstrate%20the%20effectiveness%20of%20our%20V2M%20compared%20with%20other%20visual%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2M%253A%2520Visual%25202-Dimensional%2520Mamba%2520for%2520Image%2520Representation%2520Learning%26entry.906535625%3DChengkun%2520Wang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yuanhui%2520Huang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Mamba%2520has%2520garnered%2520widespread%2520attention%2520due%2520to%2520its%2520flexible%2520design%2520and%250Aefficient%2520hardware%2520performance%2520to%2520process%25201D%2520sequences%2520based%2520on%2520the%2520state%2520space%250Amodel%2520%2528SSM%2529.%2520Recent%2520studies%2520have%2520attempted%2520to%2520apply%2520Mamba%2520to%2520the%2520visual%2520domain%250Aby%2520flattening%25202D%2520images%2520into%2520patches%2520and%2520then%2520regarding%2520them%2520as%2520a%25201D%2520sequence.%250ATo%2520compensate%2520for%2520the%25202D%2520structure%2520information%2520loss%2520%2528e.g.%252C%2520local%2520similarity%2529%2520of%250Athe%2520original%2520image%252C%2520most%2520existing%2520methods%2520focus%2520on%2520designing%2520different%2520orders%250Ato%2520sequentially%2520process%2520the%2520tokens%252C%2520which%2520could%2520only%2520alleviate%2520this%2520issue%2520to%250Asome%2520extent.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Visual%25202-Dimensional%2520Mamba%2520%2528V2M%2529%2520model%250Aas%2520a%2520complete%2520solution%252C%2520which%2520directly%2520processes%2520image%2520tokens%2520in%2520the%25202D%2520space.%250AWe%2520first%2520generalize%2520SSM%2520to%2520the%25202-dimensional%2520space%2520which%2520generates%2520the%2520next%250Astate%2520considering%2520two%2520adjacent%2520states%2520on%2520both%2520dimensions%2520%2528e.g.%252C%2520columns%2520and%250Arows%2529.%2520We%2520then%2520construct%2520our%2520V2M%2520based%2520on%2520the%25202-dimensional%2520SSM%2520formulation%2520and%250Aincorporate%2520Mamba%2520to%2520achieve%2520hardware-efficient%2520parallel%2520processing.%2520The%250Aproposed%2520V2M%2520effectively%2520incorporates%2520the%25202D%2520locality%2520prior%2520yet%2520inherits%2520the%250Aefficiency%2520and%2520input-dependent%2520scalability%2520of%2520Mamba.%2520Extensive%2520experimental%250Aresults%2520on%2520ImageNet%2520classification%2520and%2520downstream%2520visual%2520tasks%2520including%2520object%250Adetection%2520and%2520instance%2520segmentation%2520on%2520COCO%2520and%2520semantic%2520segmentation%2520on%2520ADE20K%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520V2M%2520compared%2520with%2520other%2520visual%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2M%3A%20Visual%202-Dimensional%20Mamba%20for%20Image%20Representation%20Learning&entry.906535625=Chengkun%20Wang%20and%20Wenzhao%20Zheng%20and%20Yuanhui%20Huang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Mamba%20has%20garnered%20widespread%20attention%20due%20to%20its%20flexible%20design%20and%0Aefficient%20hardware%20performance%20to%20process%201D%20sequences%20based%20on%20the%20state%20space%0Amodel%20%28SSM%29.%20Recent%20studies%20have%20attempted%20to%20apply%20Mamba%20to%20the%20visual%20domain%0Aby%20flattening%202D%20images%20into%20patches%20and%20then%20regarding%20them%20as%20a%201D%20sequence.%0ATo%20compensate%20for%20the%202D%20structure%20information%20loss%20%28e.g.%2C%20local%20similarity%29%20of%0Athe%20original%20image%2C%20most%20existing%20methods%20focus%20on%20designing%20different%20orders%0Ato%20sequentially%20process%20the%20tokens%2C%20which%20could%20only%20alleviate%20this%20issue%20to%0Asome%20extent.%20In%20this%20paper%2C%20we%20propose%20a%20Visual%202-Dimensional%20Mamba%20%28V2M%29%20model%0Aas%20a%20complete%20solution%2C%20which%20directly%20processes%20image%20tokens%20in%20the%202D%20space.%0AWe%20first%20generalize%20SSM%20to%20the%202-dimensional%20space%20which%20generates%20the%20next%0Astate%20considering%20two%20adjacent%20states%20on%20both%20dimensions%20%28e.g.%2C%20columns%20and%0Arows%29.%20We%20then%20construct%20our%20V2M%20based%20on%20the%202-dimensional%20SSM%20formulation%20and%0Aincorporate%20Mamba%20to%20achieve%20hardware-efficient%20parallel%20processing.%20The%0Aproposed%20V2M%20effectively%20incorporates%20the%202D%20locality%20prior%20yet%20inherits%20the%0Aefficiency%20and%20input-dependent%20scalability%20of%20Mamba.%20Extensive%20experimental%0Aresults%20on%20ImageNet%20classification%20and%20downstream%20visual%20tasks%20including%20object%0Adetection%20and%20instance%20segmentation%20on%20COCO%20and%20semantic%20segmentation%20on%20ADE20K%0Ademonstrate%20the%20effectiveness%20of%20our%20V2M%20compared%20with%20other%20visual%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10382v1&entry.124074799=Read"},
{"title": "SLaNC: Static LayerNorm Calibration", "author": "Mahsa Salmani and Nikita Trukhanov and Ilya Soloveychik", "abstract": "  The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations.\n", "link": "http://arxiv.org/abs/2410.10553v1", "date": "2024-10-14", "relevancy": 1.5399, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5878}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5217}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLaNC%3A%20Static%20LayerNorm%20Calibration&body=Title%3A%20SLaNC%3A%20Static%20LayerNorm%20Calibration%0AAuthor%3A%20Mahsa%20Salmani%20and%20Nikita%20Trukhanov%20and%20Ilya%20Soloveychik%0AAbstract%3A%20%20%20The%20ever%20increasing%20sizes%20of%20Large%20Language%20Models%20%28LLMs%29%20beyond%20hundreds%20of%0Abillions%20of%20parameters%20have%20generated%20enormous%20pressure%20on%20the%20manufacturers%20of%0Adedicated%20hardware%20accelerators%20and%20made%20the%20innovative%20design%20of%20the%20latter%0Aone%20of%20the%20most%20rapidly%20expanding%20fields%20of%20the%20AI%20industry.%20Various%20approaches%0Ahave%20been%20explored%20to%20enable%20efficient%20and%20accurate%20processing%20of%20LLMs%20on%20the%0Aavailable%20accelerators%20given%20their%20computational%20and%20storage%20limitations.%20Among%0Athese%2C%20various%20quantization%20techniques%20have%20become%20the%20main%20focus%20of%20the%0Acommunity%20as%20a%20means%20of%20reducing%20the%20compute%2C%20communication%20and%20storage%0Arequirements.%20Quantization%20to%20lower%20precision%20formats%20naturally%20poses%20a%20number%0Aof%20challenges%20caused%20by%20the%20limited%20range%20of%20the%20available%20value%0Arepresentations.%20When%20it%20comes%20to%20processing%20the%20popular%20Transformer%20models%20on%0Ahardware%2C%20one%20of%20the%20main%20issues%20becomes%20calculation%20of%20the%20LayerNorm%20simply%0Abecause%20accumulation%20of%20the%20variance%20requires%20a%20much%20wider%20dynamic%20range%20than%0Athe%20hardware%20enables.%20In%20this%20article%2C%20we%20address%20this%20matter%20and%20propose%20a%0Acomputationally-efficient%20scaling%20technique%20that%20can%20be%20easily%20applied%20to%0ATransformer%20models%20during%20inference.%20Our%20method%20suggests%20a%20straightforward%20way%0Aof%20scaling%20the%20LayerNorm%20inputs%20based%20on%20the%20static%20weights%20of%20the%20immediately%0Apreceding%20linear%20layers.%20The%20scaling%20factors%20are%20computed%20offline%2C%20based%20solely%0Aon%20the%20linear%20layer%20weights%2C%20hence%20no%20latency%20or%20computational%20overhead%20is%0Aadded%20during%20inference.%20Most%20importantly%2C%20our%20technique%20ensures%20that%20no%0Anumerical%20issues%20such%20as%20overflow%20or%20underflow%20could%20happen%20during%20the%20compute.%0AThis%20approach%20offers%20smooth%2C%20accurate%20and%20resource-effective%20inference%20across%20a%0Awide%20range%20of%20hardware%20architectures.%20The%20article%20provides%20theoretical%0Ajustification%20as%20well%20as%20supporting%20numerical%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLaNC%253A%2520Static%2520LayerNorm%2520Calibration%26entry.906535625%3DMahsa%2520Salmani%2520and%2520Nikita%2520Trukhanov%2520and%2520Ilya%2520Soloveychik%26entry.1292438233%3D%2520%2520The%2520ever%2520increasing%2520sizes%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520beyond%2520hundreds%2520of%250Abillions%2520of%2520parameters%2520have%2520generated%2520enormous%2520pressure%2520on%2520the%2520manufacturers%2520of%250Adedicated%2520hardware%2520accelerators%2520and%2520made%2520the%2520innovative%2520design%2520of%2520the%2520latter%250Aone%2520of%2520the%2520most%2520rapidly%2520expanding%2520fields%2520of%2520the%2520AI%2520industry.%2520Various%2520approaches%250Ahave%2520been%2520explored%2520to%2520enable%2520efficient%2520and%2520accurate%2520processing%2520of%2520LLMs%2520on%2520the%250Aavailable%2520accelerators%2520given%2520their%2520computational%2520and%2520storage%2520limitations.%2520Among%250Athese%252C%2520various%2520quantization%2520techniques%2520have%2520become%2520the%2520main%2520focus%2520of%2520the%250Acommunity%2520as%2520a%2520means%2520of%2520reducing%2520the%2520compute%252C%2520communication%2520and%2520storage%250Arequirements.%2520Quantization%2520to%2520lower%2520precision%2520formats%2520naturally%2520poses%2520a%2520number%250Aof%2520challenges%2520caused%2520by%2520the%2520limited%2520range%2520of%2520the%2520available%2520value%250Arepresentations.%2520When%2520it%2520comes%2520to%2520processing%2520the%2520popular%2520Transformer%2520models%2520on%250Ahardware%252C%2520one%2520of%2520the%2520main%2520issues%2520becomes%2520calculation%2520of%2520the%2520LayerNorm%2520simply%250Abecause%2520accumulation%2520of%2520the%2520variance%2520requires%2520a%2520much%2520wider%2520dynamic%2520range%2520than%250Athe%2520hardware%2520enables.%2520In%2520this%2520article%252C%2520we%2520address%2520this%2520matter%2520and%2520propose%2520a%250Acomputationally-efficient%2520scaling%2520technique%2520that%2520can%2520be%2520easily%2520applied%2520to%250ATransformer%2520models%2520during%2520inference.%2520Our%2520method%2520suggests%2520a%2520straightforward%2520way%250Aof%2520scaling%2520the%2520LayerNorm%2520inputs%2520based%2520on%2520the%2520static%2520weights%2520of%2520the%2520immediately%250Apreceding%2520linear%2520layers.%2520The%2520scaling%2520factors%2520are%2520computed%2520offline%252C%2520based%2520solely%250Aon%2520the%2520linear%2520layer%2520weights%252C%2520hence%2520no%2520latency%2520or%2520computational%2520overhead%2520is%250Aadded%2520during%2520inference.%2520Most%2520importantly%252C%2520our%2520technique%2520ensures%2520that%2520no%250Anumerical%2520issues%2520such%2520as%2520overflow%2520or%2520underflow%2520could%2520happen%2520during%2520the%2520compute.%250AThis%2520approach%2520offers%2520smooth%252C%2520accurate%2520and%2520resource-effective%2520inference%2520across%2520a%250Awide%2520range%2520of%2520hardware%2520architectures.%2520The%2520article%2520provides%2520theoretical%250Ajustification%2520as%2520well%2520as%2520supporting%2520numerical%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLaNC%3A%20Static%20LayerNorm%20Calibration&entry.906535625=Mahsa%20Salmani%20and%20Nikita%20Trukhanov%20and%20Ilya%20Soloveychik&entry.1292438233=%20%20The%20ever%20increasing%20sizes%20of%20Large%20Language%20Models%20%28LLMs%29%20beyond%20hundreds%20of%0Abillions%20of%20parameters%20have%20generated%20enormous%20pressure%20on%20the%20manufacturers%20of%0Adedicated%20hardware%20accelerators%20and%20made%20the%20innovative%20design%20of%20the%20latter%0Aone%20of%20the%20most%20rapidly%20expanding%20fields%20of%20the%20AI%20industry.%20Various%20approaches%0Ahave%20been%20explored%20to%20enable%20efficient%20and%20accurate%20processing%20of%20LLMs%20on%20the%0Aavailable%20accelerators%20given%20their%20computational%20and%20storage%20limitations.%20Among%0Athese%2C%20various%20quantization%20techniques%20have%20become%20the%20main%20focus%20of%20the%0Acommunity%20as%20a%20means%20of%20reducing%20the%20compute%2C%20communication%20and%20storage%0Arequirements.%20Quantization%20to%20lower%20precision%20formats%20naturally%20poses%20a%20number%0Aof%20challenges%20caused%20by%20the%20limited%20range%20of%20the%20available%20value%0Arepresentations.%20When%20it%20comes%20to%20processing%20the%20popular%20Transformer%20models%20on%0Ahardware%2C%20one%20of%20the%20main%20issues%20becomes%20calculation%20of%20the%20LayerNorm%20simply%0Abecause%20accumulation%20of%20the%20variance%20requires%20a%20much%20wider%20dynamic%20range%20than%0Athe%20hardware%20enables.%20In%20this%20article%2C%20we%20address%20this%20matter%20and%20propose%20a%0Acomputationally-efficient%20scaling%20technique%20that%20can%20be%20easily%20applied%20to%0ATransformer%20models%20during%20inference.%20Our%20method%20suggests%20a%20straightforward%20way%0Aof%20scaling%20the%20LayerNorm%20inputs%20based%20on%20the%20static%20weights%20of%20the%20immediately%0Apreceding%20linear%20layers.%20The%20scaling%20factors%20are%20computed%20offline%2C%20based%20solely%0Aon%20the%20linear%20layer%20weights%2C%20hence%20no%20latency%20or%20computational%20overhead%20is%0Aadded%20during%20inference.%20Most%20importantly%2C%20our%20technique%20ensures%20that%20no%0Anumerical%20issues%20such%20as%20overflow%20or%20underflow%20could%20happen%20during%20the%20compute.%0AThis%20approach%20offers%20smooth%2C%20accurate%20and%20resource-effective%20inference%20across%20a%0Awide%20range%20of%20hardware%20architectures.%20The%20article%20provides%20theoretical%0Ajustification%20as%20well%20as%20supporting%20numerical%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10553v1&entry.124074799=Read"},
{"title": "Compositional Shielding and Reinforcement Learning for Multi-Agent\n  Systems", "author": "Asger Horn Brorholt and Kim Guldstrand Larsen and Christian Schilling", "abstract": "  Deep reinforcement learning has emerged as a powerful tool for obtaining\nhigh-performance policies. However, the safety of these policies has been a\nlong-standing issue. One promising paradigm to guarantee safety is a shield,\nwhich shields a policy from making unsafe actions. However, computing a shield\nscales exponentially in the number of state variables. This is a particular\nconcern in multi-agent systems with many agents. In this work, we propose a\nnovel approach for multi-agent shielding. We address scalability by computing\nindividual shields for each agent. The challenge is that typical safety\nspecifications are global properties, but the shields of individual agents only\nensure local properties. Our key to overcome this challenge is to apply\nassume-guarantee reasoning. Specifically, we present a sound proof rule that\ndecomposes a (global, complex) safety specification into (local, simple)\nobligations for the shields of the individual agents. Moreover, we show that\napplying the shields during reinforcement learning significantly improves the\nquality of the policies obtained for a given training budget. We demonstrate\nthe effectiveness and scalability of our multi-agent shielding framework in two\ncase studies, reducing the computation time from hours to seconds and achieving\nfast learning convergence.\n", "link": "http://arxiv.org/abs/2410.10460v1", "date": "2024-10-14", "relevancy": 1.6166, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5159}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Shielding%20and%20Reinforcement%20Learning%20for%20Multi-Agent%0A%20%20Systems&body=Title%3A%20Compositional%20Shielding%20and%20Reinforcement%20Learning%20for%20Multi-Agent%0A%20%20Systems%0AAuthor%3A%20Asger%20Horn%20Brorholt%20and%20Kim%20Guldstrand%20Larsen%20and%20Christian%20Schilling%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20obtaining%0Ahigh-performance%20policies.%20However%2C%20the%20safety%20of%20these%20policies%20has%20been%20a%0Along-standing%20issue.%20One%20promising%20paradigm%20to%20guarantee%20safety%20is%20a%20shield%2C%0Awhich%20shields%20a%20policy%20from%20making%20unsafe%20actions.%20However%2C%20computing%20a%20shield%0Ascales%20exponentially%20in%20the%20number%20of%20state%20variables.%20This%20is%20a%20particular%0Aconcern%20in%20multi-agent%20systems%20with%20many%20agents.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20approach%20for%20multi-agent%20shielding.%20We%20address%20scalability%20by%20computing%0Aindividual%20shields%20for%20each%20agent.%20The%20challenge%20is%20that%20typical%20safety%0Aspecifications%20are%20global%20properties%2C%20but%20the%20shields%20of%20individual%20agents%20only%0Aensure%20local%20properties.%20Our%20key%20to%20overcome%20this%20challenge%20is%20to%20apply%0Aassume-guarantee%20reasoning.%20Specifically%2C%20we%20present%20a%20sound%20proof%20rule%20that%0Adecomposes%20a%20%28global%2C%20complex%29%20safety%20specification%20into%20%28local%2C%20simple%29%0Aobligations%20for%20the%20shields%20of%20the%20individual%20agents.%20Moreover%2C%20we%20show%20that%0Aapplying%20the%20shields%20during%20reinforcement%20learning%20significantly%20improves%20the%0Aquality%20of%20the%20policies%20obtained%20for%20a%20given%20training%20budget.%20We%20demonstrate%0Athe%20effectiveness%20and%20scalability%20of%20our%20multi-agent%20shielding%20framework%20in%20two%0Acase%20studies%2C%20reducing%20the%20computation%20time%20from%20hours%20to%20seconds%20and%20achieving%0Afast%20learning%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Shielding%2520and%2520Reinforcement%2520Learning%2520for%2520Multi-Agent%250A%2520%2520Systems%26entry.906535625%3DAsger%2520Horn%2520Brorholt%2520and%2520Kim%2520Guldstrand%2520Larsen%2520and%2520Christian%2520Schilling%26entry.1292438233%3D%2520%2520Deep%2520reinforcement%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520obtaining%250Ahigh-performance%2520policies.%2520However%252C%2520the%2520safety%2520of%2520these%2520policies%2520has%2520been%2520a%250Along-standing%2520issue.%2520One%2520promising%2520paradigm%2520to%2520guarantee%2520safety%2520is%2520a%2520shield%252C%250Awhich%2520shields%2520a%2520policy%2520from%2520making%2520unsafe%2520actions.%2520However%252C%2520computing%2520a%2520shield%250Ascales%2520exponentially%2520in%2520the%2520number%2520of%2520state%2520variables.%2520This%2520is%2520a%2520particular%250Aconcern%2520in%2520multi-agent%2520systems%2520with%2520many%2520agents.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520for%2520multi-agent%2520shielding.%2520We%2520address%2520scalability%2520by%2520computing%250Aindividual%2520shields%2520for%2520each%2520agent.%2520The%2520challenge%2520is%2520that%2520typical%2520safety%250Aspecifications%2520are%2520global%2520properties%252C%2520but%2520the%2520shields%2520of%2520individual%2520agents%2520only%250Aensure%2520local%2520properties.%2520Our%2520key%2520to%2520overcome%2520this%2520challenge%2520is%2520to%2520apply%250Aassume-guarantee%2520reasoning.%2520Specifically%252C%2520we%2520present%2520a%2520sound%2520proof%2520rule%2520that%250Adecomposes%2520a%2520%2528global%252C%2520complex%2529%2520safety%2520specification%2520into%2520%2528local%252C%2520simple%2529%250Aobligations%2520for%2520the%2520shields%2520of%2520the%2520individual%2520agents.%2520Moreover%252C%2520we%2520show%2520that%250Aapplying%2520the%2520shields%2520during%2520reinforcement%2520learning%2520significantly%2520improves%2520the%250Aquality%2520of%2520the%2520policies%2520obtained%2520for%2520a%2520given%2520training%2520budget.%2520We%2520demonstrate%250Athe%2520effectiveness%2520and%2520scalability%2520of%2520our%2520multi-agent%2520shielding%2520framework%2520in%2520two%250Acase%2520studies%252C%2520reducing%2520the%2520computation%2520time%2520from%2520hours%2520to%2520seconds%2520and%2520achieving%250Afast%2520learning%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Shielding%20and%20Reinforcement%20Learning%20for%20Multi-Agent%0A%20%20Systems&entry.906535625=Asger%20Horn%20Brorholt%20and%20Kim%20Guldstrand%20Larsen%20and%20Christian%20Schilling&entry.1292438233=%20%20Deep%20reinforcement%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20obtaining%0Ahigh-performance%20policies.%20However%2C%20the%20safety%20of%20these%20policies%20has%20been%20a%0Along-standing%20issue.%20One%20promising%20paradigm%20to%20guarantee%20safety%20is%20a%20shield%2C%0Awhich%20shields%20a%20policy%20from%20making%20unsafe%20actions.%20However%2C%20computing%20a%20shield%0Ascales%20exponentially%20in%20the%20number%20of%20state%20variables.%20This%20is%20a%20particular%0Aconcern%20in%20multi-agent%20systems%20with%20many%20agents.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20approach%20for%20multi-agent%20shielding.%20We%20address%20scalability%20by%20computing%0Aindividual%20shields%20for%20each%20agent.%20The%20challenge%20is%20that%20typical%20safety%0Aspecifications%20are%20global%20properties%2C%20but%20the%20shields%20of%20individual%20agents%20only%0Aensure%20local%20properties.%20Our%20key%20to%20overcome%20this%20challenge%20is%20to%20apply%0Aassume-guarantee%20reasoning.%20Specifically%2C%20we%20present%20a%20sound%20proof%20rule%20that%0Adecomposes%20a%20%28global%2C%20complex%29%20safety%20specification%20into%20%28local%2C%20simple%29%0Aobligations%20for%20the%20shields%20of%20the%20individual%20agents.%20Moreover%2C%20we%20show%20that%0Aapplying%20the%20shields%20during%20reinforcement%20learning%20significantly%20improves%20the%0Aquality%20of%20the%20policies%20obtained%20for%20a%20given%20training%20budget.%20We%20demonstrate%0Athe%20effectiveness%20and%20scalability%20of%20our%20multi-agent%20shielding%20framework%20in%20two%0Acase%20studies%2C%20reducing%20the%20computation%20time%20from%20hours%20to%20seconds%20and%20achieving%0Afast%20learning%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10460v1&entry.124074799=Read"},
{"title": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion\n  Transformers", "author": "Enze Xie and Junsong Chen and Junyu Chen and Han Cai and Yujun Lin and Zhekai Zhang and Muyang Li and Yao Lu and Song Han", "abstract": "  We introduce \\model, a text-to-image framework that can efficiently generate\nimages up to 4096$\\times$4096 resolution. \\model can synthesize\nhigh-resolution, high-quality images with strong text-image alignment at a\nremarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep\ncompression autoencoder: unlike traditional AEs, which compress images only\n8$\\times$, we trained an AE that can compress images 32$\\times$, effectively\nreducing the number of latent tokens. (2) Linear DiT: we replace all vanilla\nattention in DiT with linear attention, which is more efficient at high\nresolutions without sacrificing quality. (3) Decoder-only text encoder: we\nreplaced T5 with modern decoder-only small LLM as the text encoder and designed\ncomplex human instruction with in-context learning to enhance the image-text\nalignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to\nreduce sampling steps, with efficient caption labeling and selection to\naccelerate convergence. As a result, \\model-0.6B is very competitive with\nmodern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+\ntimes faster in measured throughput. Moreover, \\model-0.6B can be deployed on a\n16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024\nresolution image. Sana enables content creation at low cost. Code and model\nwill be publicly released.\n", "link": "http://arxiv.org/abs/2410.10629v1", "date": "2024-10-14", "relevancy": 2.0437, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7309}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SANA%3A%20Efficient%20High-Resolution%20Image%20Synthesis%20with%20Linear%20Diffusion%0A%20%20Transformers&body=Title%3A%20SANA%3A%20Efficient%20High-Resolution%20Image%20Synthesis%20with%20Linear%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Enze%20Xie%20and%20Junsong%20Chen%20and%20Junyu%20Chen%20and%20Han%20Cai%20and%20Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20introduce%20%5Cmodel%2C%20a%20text-to-image%20framework%20that%20can%20efficiently%20generate%0Aimages%20up%20to%204096%24%5Ctimes%244096%20resolution.%20%5Cmodel%20can%20synthesize%0Ahigh-resolution%2C%20high-quality%20images%20with%20strong%20text-image%20alignment%20at%20a%0Aremarkably%20fast%20speed%2C%20deployable%20on%20laptop%20GPU.%20Core%20designs%20include%3A%20%281%29%20Deep%0Acompression%20autoencoder%3A%20unlike%20traditional%20AEs%2C%20which%20compress%20images%20only%0A8%24%5Ctimes%24%2C%20we%20trained%20an%20AE%20that%20can%20compress%20images%2032%24%5Ctimes%24%2C%20effectively%0Areducing%20the%20number%20of%20latent%20tokens.%20%282%29%20Linear%20DiT%3A%20we%20replace%20all%20vanilla%0Aattention%20in%20DiT%20with%20linear%20attention%2C%20which%20is%20more%20efficient%20at%20high%0Aresolutions%20without%20sacrificing%20quality.%20%283%29%20Decoder-only%20text%20encoder%3A%20we%0Areplaced%20T5%20with%20modern%20decoder-only%20small%20LLM%20as%20the%20text%20encoder%20and%20designed%0Acomplex%20human%20instruction%20with%20in-context%20learning%20to%20enhance%20the%20image-text%0Aalignment.%20%284%29%20Efficient%20training%20and%20sampling%3A%20we%20propose%20Flow-DPM-Solver%20to%0Areduce%20sampling%20steps%2C%20with%20efficient%20caption%20labeling%20and%20selection%20to%0Aaccelerate%20convergence.%20As%20a%20result%2C%20%5Cmodel-0.6B%20is%20very%20competitive%20with%0Amodern%20giant%20diffusion%20model%20%28e.g.%20Flux-12B%29%2C%20being%2020%20times%20smaller%20and%20100%2B%0Atimes%20faster%20in%20measured%20throughput.%20Moreover%2C%20%5Cmodel-0.6B%20can%20be%20deployed%20on%20a%0A16GB%20laptop%20GPU%2C%20taking%20less%20than%201%20second%20to%20generate%20a%201024%24%5Ctimes%241024%0Aresolution%20image.%20Sana%20enables%20content%20creation%20at%20low%20cost.%20Code%20and%20model%0Awill%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSANA%253A%2520Efficient%2520High-Resolution%2520Image%2520Synthesis%2520with%2520Linear%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DEnze%2520Xie%2520and%2520Junsong%2520Chen%2520and%2520Junyu%2520Chen%2520and%2520Han%2520Cai%2520and%2520Yujun%2520Lin%2520and%2520Zhekai%2520Zhang%2520and%2520Muyang%2520Li%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520introduce%2520%255Cmodel%252C%2520a%2520text-to-image%2520framework%2520that%2520can%2520efficiently%2520generate%250Aimages%2520up%2520to%25204096%2524%255Ctimes%25244096%2520resolution.%2520%255Cmodel%2520can%2520synthesize%250Ahigh-resolution%252C%2520high-quality%2520images%2520with%2520strong%2520text-image%2520alignment%2520at%2520a%250Aremarkably%2520fast%2520speed%252C%2520deployable%2520on%2520laptop%2520GPU.%2520Core%2520designs%2520include%253A%2520%25281%2529%2520Deep%250Acompression%2520autoencoder%253A%2520unlike%2520traditional%2520AEs%252C%2520which%2520compress%2520images%2520only%250A8%2524%255Ctimes%2524%252C%2520we%2520trained%2520an%2520AE%2520that%2520can%2520compress%2520images%252032%2524%255Ctimes%2524%252C%2520effectively%250Areducing%2520the%2520number%2520of%2520latent%2520tokens.%2520%25282%2529%2520Linear%2520DiT%253A%2520we%2520replace%2520all%2520vanilla%250Aattention%2520in%2520DiT%2520with%2520linear%2520attention%252C%2520which%2520is%2520more%2520efficient%2520at%2520high%250Aresolutions%2520without%2520sacrificing%2520quality.%2520%25283%2529%2520Decoder-only%2520text%2520encoder%253A%2520we%250Areplaced%2520T5%2520with%2520modern%2520decoder-only%2520small%2520LLM%2520as%2520the%2520text%2520encoder%2520and%2520designed%250Acomplex%2520human%2520instruction%2520with%2520in-context%2520learning%2520to%2520enhance%2520the%2520image-text%250Aalignment.%2520%25284%2529%2520Efficient%2520training%2520and%2520sampling%253A%2520we%2520propose%2520Flow-DPM-Solver%2520to%250Areduce%2520sampling%2520steps%252C%2520with%2520efficient%2520caption%2520labeling%2520and%2520selection%2520to%250Aaccelerate%2520convergence.%2520As%2520a%2520result%252C%2520%255Cmodel-0.6B%2520is%2520very%2520competitive%2520with%250Amodern%2520giant%2520diffusion%2520model%2520%2528e.g.%2520Flux-12B%2529%252C%2520being%252020%2520times%2520smaller%2520and%2520100%252B%250Atimes%2520faster%2520in%2520measured%2520throughput.%2520Moreover%252C%2520%255Cmodel-0.6B%2520can%2520be%2520deployed%2520on%2520a%250A16GB%2520laptop%2520GPU%252C%2520taking%2520less%2520than%25201%2520second%2520to%2520generate%2520a%25201024%2524%255Ctimes%25241024%250Aresolution%2520image.%2520Sana%2520enables%2520content%2520creation%2520at%2520low%2520cost.%2520Code%2520and%2520model%250Awill%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANA%3A%20Efficient%20High-Resolution%20Image%20Synthesis%20with%20Linear%20Diffusion%0A%20%20Transformers&entry.906535625=Enze%20Xie%20and%20Junsong%20Chen%20and%20Junyu%20Chen%20and%20Han%20Cai%20and%20Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20introduce%20%5Cmodel%2C%20a%20text-to-image%20framework%20that%20can%20efficiently%20generate%0Aimages%20up%20to%204096%24%5Ctimes%244096%20resolution.%20%5Cmodel%20can%20synthesize%0Ahigh-resolution%2C%20high-quality%20images%20with%20strong%20text-image%20alignment%20at%20a%0Aremarkably%20fast%20speed%2C%20deployable%20on%20laptop%20GPU.%20Core%20designs%20include%3A%20%281%29%20Deep%0Acompression%20autoencoder%3A%20unlike%20traditional%20AEs%2C%20which%20compress%20images%20only%0A8%24%5Ctimes%24%2C%20we%20trained%20an%20AE%20that%20can%20compress%20images%2032%24%5Ctimes%24%2C%20effectively%0Areducing%20the%20number%20of%20latent%20tokens.%20%282%29%20Linear%20DiT%3A%20we%20replace%20all%20vanilla%0Aattention%20in%20DiT%20with%20linear%20attention%2C%20which%20is%20more%20efficient%20at%20high%0Aresolutions%20without%20sacrificing%20quality.%20%283%29%20Decoder-only%20text%20encoder%3A%20we%0Areplaced%20T5%20with%20modern%20decoder-only%20small%20LLM%20as%20the%20text%20encoder%20and%20designed%0Acomplex%20human%20instruction%20with%20in-context%20learning%20to%20enhance%20the%20image-text%0Aalignment.%20%284%29%20Efficient%20training%20and%20sampling%3A%20we%20propose%20Flow-DPM-Solver%20to%0Areduce%20sampling%20steps%2C%20with%20efficient%20caption%20labeling%20and%20selection%20to%0Aaccelerate%20convergence.%20As%20a%20result%2C%20%5Cmodel-0.6B%20is%20very%20competitive%20with%0Amodern%20giant%20diffusion%20model%20%28e.g.%20Flux-12B%29%2C%20being%2020%20times%20smaller%20and%20100%2B%0Atimes%20faster%20in%20measured%20throughput.%20Moreover%2C%20%5Cmodel-0.6B%20can%20be%20deployed%20on%20a%0A16GB%20laptop%20GPU%2C%20taking%20less%20than%201%20second%20to%20generate%20a%201024%24%5Ctimes%241024%0Aresolution%20image.%20Sana%20enables%20content%20creation%20at%20low%20cost.%20Code%20and%20model%0Awill%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10629v1&entry.124074799=Read"},
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "author": "Maksym Andriushchenko and Alexandra Souly and Mateusz Dziemian and Derek Duenas and Maxwell Lin and Justin Wang and Dan Hendrycks and Andy Zou and Zico Kolter and Matt Fredrikson and Eric Winsor and Jerome Wynne and Yarin Gal and Xander Davies", "abstract": "  The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.\n", "link": "http://arxiv.org/abs/2410.09024v2", "date": "2024-10-14", "relevancy": 1.6504, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4198}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4076}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents&body=Title%3A%20AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents%0AAuthor%3A%20Maksym%20Andriushchenko%20and%20Alexandra%20Souly%20and%20Mateusz%20Dziemian%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Dan%20Hendrycks%20and%20Andy%20Zou%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Eric%20Winsor%20and%20Jerome%20Wynne%20and%20Yarin%20Gal%20and%20Xander%20Davies%0AAbstract%3A%20%20%20The%20robustness%20of%20LLMs%20to%20jailbreak%20attacks%2C%20where%20users%20design%20prompts%20to%0Acircumvent%20safety%20measures%20and%20misuse%20model%20capabilities%2C%20has%20been%20studied%0Aprimarily%20for%20LLMs%20acting%20as%20simple%20chatbots.%20Meanwhile%2C%20LLM%20agents%20--%20which%0Ause%20external%20tools%20and%20can%20execute%20multi-stage%20tasks%20--%20may%20pose%20a%20greater%20risk%0Aif%20misused%2C%20but%20their%20robustness%20remains%20underexplored.%20To%20facilitate%20research%0Aon%20LLM%20agent%20misuse%2C%20we%20propose%20a%20new%20benchmark%20called%20AgentHarm.%20The%20benchmark%0Aincludes%20a%20diverse%20set%20of%20110%20explicitly%20malicious%20agent%20tasks%20%28440%20with%0Aaugmentations%29%2C%20covering%2011%20harm%20categories%20including%20fraud%2C%20cybercrime%2C%20and%0Aharassment.%20In%20addition%20to%20measuring%20whether%20models%20refuse%20harmful%20agentic%0Arequests%2C%20scoring%20well%20on%20AgentHarm%20requires%20jailbroken%20agents%20to%20maintain%0Atheir%20capabilities%20following%20an%20attack%20to%20complete%20a%20multi-step%20task.%20We%0Aevaluate%20a%20range%20of%20leading%20LLMs%2C%20and%20find%20%281%29%20leading%20LLMs%20are%20surprisingly%0Acompliant%20with%20malicious%20agent%20requests%20without%20jailbreaking%2C%20%282%29%20simple%0Auniversal%20jailbreak%20templates%20can%20be%20adapted%20to%20effectively%20jailbreak%20agents%2C%0Aand%20%283%29%20these%20jailbreaks%20enable%20coherent%20and%20malicious%20multi-step%20agent%0Abehavior%20and%20retain%20model%20capabilities.%20To%20enable%20simple%20and%20reliable%0Aevaluation%20of%20attacks%20and%20defenses%20for%20LLM-based%20agents%2C%20we%20publicly%20release%0AAgentHarm%20at%20https%3A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentHarm%253A%2520A%2520Benchmark%2520for%2520Measuring%2520Harmfulness%2520of%2520LLM%2520Agents%26entry.906535625%3DMaksym%2520Andriushchenko%2520and%2520Alexandra%2520Souly%2520and%2520Mateusz%2520Dziemian%2520and%2520Derek%2520Duenas%2520and%2520Maxwell%2520Lin%2520and%2520Justin%2520Wang%2520and%2520Dan%2520Hendrycks%2520and%2520Andy%2520Zou%2520and%2520Zico%2520Kolter%2520and%2520Matt%2520Fredrikson%2520and%2520Eric%2520Winsor%2520and%2520Jerome%2520Wynne%2520and%2520Yarin%2520Gal%2520and%2520Xander%2520Davies%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520LLMs%2520to%2520jailbreak%2520attacks%252C%2520where%2520users%2520design%2520prompts%2520to%250Acircumvent%2520safety%2520measures%2520and%2520misuse%2520model%2520capabilities%252C%2520has%2520been%2520studied%250Aprimarily%2520for%2520LLMs%2520acting%2520as%2520simple%2520chatbots.%2520Meanwhile%252C%2520LLM%2520agents%2520--%2520which%250Ause%2520external%2520tools%2520and%2520can%2520execute%2520multi-stage%2520tasks%2520--%2520may%2520pose%2520a%2520greater%2520risk%250Aif%2520misused%252C%2520but%2520their%2520robustness%2520remains%2520underexplored.%2520To%2520facilitate%2520research%250Aon%2520LLM%2520agent%2520misuse%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520called%2520AgentHarm.%2520The%2520benchmark%250Aincludes%2520a%2520diverse%2520set%2520of%2520110%2520explicitly%2520malicious%2520agent%2520tasks%2520%2528440%2520with%250Aaugmentations%2529%252C%2520covering%252011%2520harm%2520categories%2520including%2520fraud%252C%2520cybercrime%252C%2520and%250Aharassment.%2520In%2520addition%2520to%2520measuring%2520whether%2520models%2520refuse%2520harmful%2520agentic%250Arequests%252C%2520scoring%2520well%2520on%2520AgentHarm%2520requires%2520jailbroken%2520agents%2520to%2520maintain%250Atheir%2520capabilities%2520following%2520an%2520attack%2520to%2520complete%2520a%2520multi-step%2520task.%2520We%250Aevaluate%2520a%2520range%2520of%2520leading%2520LLMs%252C%2520and%2520find%2520%25281%2529%2520leading%2520LLMs%2520are%2520surprisingly%250Acompliant%2520with%2520malicious%2520agent%2520requests%2520without%2520jailbreaking%252C%2520%25282%2529%2520simple%250Auniversal%2520jailbreak%2520templates%2520can%2520be%2520adapted%2520to%2520effectively%2520jailbreak%2520agents%252C%250Aand%2520%25283%2529%2520these%2520jailbreaks%2520enable%2520coherent%2520and%2520malicious%2520multi-step%2520agent%250Abehavior%2520and%2520retain%2520model%2520capabilities.%2520To%2520enable%2520simple%2520and%2520reliable%250Aevaluation%2520of%2520attacks%2520and%2520defenses%2520for%2520LLM-based%2520agents%252C%2520we%2520publicly%2520release%250AAgentHarm%2520at%2520https%253A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents&entry.906535625=Maksym%20Andriushchenko%20and%20Alexandra%20Souly%20and%20Mateusz%20Dziemian%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Dan%20Hendrycks%20and%20Andy%20Zou%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Eric%20Winsor%20and%20Jerome%20Wynne%20and%20Yarin%20Gal%20and%20Xander%20Davies&entry.1292438233=%20%20The%20robustness%20of%20LLMs%20to%20jailbreak%20attacks%2C%20where%20users%20design%20prompts%20to%0Acircumvent%20safety%20measures%20and%20misuse%20model%20capabilities%2C%20has%20been%20studied%0Aprimarily%20for%20LLMs%20acting%20as%20simple%20chatbots.%20Meanwhile%2C%20LLM%20agents%20--%20which%0Ause%20external%20tools%20and%20can%20execute%20multi-stage%20tasks%20--%20may%20pose%20a%20greater%20risk%0Aif%20misused%2C%20but%20their%20robustness%20remains%20underexplored.%20To%20facilitate%20research%0Aon%20LLM%20agent%20misuse%2C%20we%20propose%20a%20new%20benchmark%20called%20AgentHarm.%20The%20benchmark%0Aincludes%20a%20diverse%20set%20of%20110%20explicitly%20malicious%20agent%20tasks%20%28440%20with%0Aaugmentations%29%2C%20covering%2011%20harm%20categories%20including%20fraud%2C%20cybercrime%2C%20and%0Aharassment.%20In%20addition%20to%20measuring%20whether%20models%20refuse%20harmful%20agentic%0Arequests%2C%20scoring%20well%20on%20AgentHarm%20requires%20jailbroken%20agents%20to%20maintain%0Atheir%20capabilities%20following%20an%20attack%20to%20complete%20a%20multi-step%20task.%20We%0Aevaluate%20a%20range%20of%20leading%20LLMs%2C%20and%20find%20%281%29%20leading%20LLMs%20are%20surprisingly%0Acompliant%20with%20malicious%20agent%20requests%20without%20jailbreaking%2C%20%282%29%20simple%0Auniversal%20jailbreak%20templates%20can%20be%20adapted%20to%20effectively%20jailbreak%20agents%2C%0Aand%20%283%29%20these%20jailbreaks%20enable%20coherent%20and%20malicious%20multi-step%20agent%0Abehavior%20and%20retain%20model%20capabilities.%20To%20enable%20simple%20and%20reliable%0Aevaluation%20of%20attacks%20and%20defenses%20for%20LLM-based%20agents%2C%20we%20publicly%20release%0AAgentHarm%20at%20https%3A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09024v2&entry.124074799=Read"},
{"title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers", "author": "Alex Stein and Samuel Sharpe and Doron Bergman and Senthil Kumar and Bayan Bruss and John Dickerson and Tom Goldstein and Micah Goldblum", "abstract": "  Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.\n", "link": "http://arxiv.org/abs/2410.10648v1", "date": "2024-10-14", "relevancy": 1.4687, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4901}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Baseline%20for%20Predicting%20Events%20with%20Auto-Regressive%20Tabular%0A%20%20Transformers&body=Title%3A%20A%20Simple%20Baseline%20for%20Predicting%20Events%20with%20Auto-Regressive%20Tabular%0A%20%20Transformers%0AAuthor%3A%20Alex%20Stein%20and%20Samuel%20Sharpe%20and%20Doron%20Bergman%20and%20Senthil%20Kumar%20and%20Bayan%20Bruss%20and%20John%20Dickerson%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum%0AAbstract%3A%20%20%20Many%20real-world%20applications%20of%20tabular%20data%20involve%20using%20historic%20events%20to%0Apredict%20properties%20of%20new%20ones%2C%20for%20example%20whether%20a%20credit%20card%20transaction%0Ais%20fraudulent%20or%20what%20rating%20a%20customer%20will%20assign%20a%20product%20on%20a%20retail%0Aplatform.%20Existing%20approaches%20to%20event%20prediction%20include%20costly%2C%20brittle%2C%20and%0Aapplication-dependent%20techniques%20such%20as%20time-aware%20positional%20embeddings%2C%0Alearned%20row%20and%20field%20encodings%2C%20and%20oversampling%20methods%20for%20addressing%20class%0Aimbalance.%20Moreover%2C%20these%20approaches%20often%20assume%20specific%20use-cases%2C%20for%0Aexample%20that%20we%20know%20the%20labels%20of%20all%20historic%20events%20or%20that%20we%20only%20predict%0Aa%20pre-specified%20label%20and%20not%20the%20data%27s%20features%20themselves.%20In%20this%20work%2C%20we%0Apropose%20a%20simple%20but%20flexible%20baseline%20using%20standard%20autoregressive%20LLM-style%0Atransformers%20with%20elementary%20positional%20embeddings%20and%20a%20causal%20language%0Amodeling%20objective.%20Our%20baseline%20outperforms%20existing%20approaches%20across%20popular%0Adatasets%20and%20can%20be%20employed%20for%20various%20use-cases.%20We%20demonstrate%20that%20the%0Asame%20model%20can%20predict%20labels%2C%20impute%20missing%20values%2C%20or%20model%20event%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Baseline%2520for%2520Predicting%2520Events%2520with%2520Auto-Regressive%2520Tabular%250A%2520%2520Transformers%26entry.906535625%3DAlex%2520Stein%2520and%2520Samuel%2520Sharpe%2520and%2520Doron%2520Bergman%2520and%2520Senthil%2520Kumar%2520and%2520Bayan%2520Bruss%2520and%2520John%2520Dickerson%2520and%2520Tom%2520Goldstein%2520and%2520Micah%2520Goldblum%26entry.1292438233%3D%2520%2520Many%2520real-world%2520applications%2520of%2520tabular%2520data%2520involve%2520using%2520historic%2520events%2520to%250Apredict%2520properties%2520of%2520new%2520ones%252C%2520for%2520example%2520whether%2520a%2520credit%2520card%2520transaction%250Ais%2520fraudulent%2520or%2520what%2520rating%2520a%2520customer%2520will%2520assign%2520a%2520product%2520on%2520a%2520retail%250Aplatform.%2520Existing%2520approaches%2520to%2520event%2520prediction%2520include%2520costly%252C%2520brittle%252C%2520and%250Aapplication-dependent%2520techniques%2520such%2520as%2520time-aware%2520positional%2520embeddings%252C%250Alearned%2520row%2520and%2520field%2520encodings%252C%2520and%2520oversampling%2520methods%2520for%2520addressing%2520class%250Aimbalance.%2520Moreover%252C%2520these%2520approaches%2520often%2520assume%2520specific%2520use-cases%252C%2520for%250Aexample%2520that%2520we%2520know%2520the%2520labels%2520of%2520all%2520historic%2520events%2520or%2520that%2520we%2520only%2520predict%250Aa%2520pre-specified%2520label%2520and%2520not%2520the%2520data%2527s%2520features%2520themselves.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520simple%2520but%2520flexible%2520baseline%2520using%2520standard%2520autoregressive%2520LLM-style%250Atransformers%2520with%2520elementary%2520positional%2520embeddings%2520and%2520a%2520causal%2520language%250Amodeling%2520objective.%2520Our%2520baseline%2520outperforms%2520existing%2520approaches%2520across%2520popular%250Adatasets%2520and%2520can%2520be%2520employed%2520for%2520various%2520use-cases.%2520We%2520demonstrate%2520that%2520the%250Asame%2520model%2520can%2520predict%2520labels%252C%2520impute%2520missing%2520values%252C%2520or%2520model%2520event%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Baseline%20for%20Predicting%20Events%20with%20Auto-Regressive%20Tabular%0A%20%20Transformers&entry.906535625=Alex%20Stein%20and%20Samuel%20Sharpe%20and%20Doron%20Bergman%20and%20Senthil%20Kumar%20and%20Bayan%20Bruss%20and%20John%20Dickerson%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum&entry.1292438233=%20%20Many%20real-world%20applications%20of%20tabular%20data%20involve%20using%20historic%20events%20to%0Apredict%20properties%20of%20new%20ones%2C%20for%20example%20whether%20a%20credit%20card%20transaction%0Ais%20fraudulent%20or%20what%20rating%20a%20customer%20will%20assign%20a%20product%20on%20a%20retail%0Aplatform.%20Existing%20approaches%20to%20event%20prediction%20include%20costly%2C%20brittle%2C%20and%0Aapplication-dependent%20techniques%20such%20as%20time-aware%20positional%20embeddings%2C%0Alearned%20row%20and%20field%20encodings%2C%20and%20oversampling%20methods%20for%20addressing%20class%0Aimbalance.%20Moreover%2C%20these%20approaches%20often%20assume%20specific%20use-cases%2C%20for%0Aexample%20that%20we%20know%20the%20labels%20of%20all%20historic%20events%20or%20that%20we%20only%20predict%0Aa%20pre-specified%20label%20and%20not%20the%20data%27s%20features%20themselves.%20In%20this%20work%2C%20we%0Apropose%20a%20simple%20but%20flexible%20baseline%20using%20standard%20autoregressive%20LLM-style%0Atransformers%20with%20elementary%20positional%20embeddings%20and%20a%20causal%20language%0Amodeling%20objective.%20Our%20baseline%20outperforms%20existing%20approaches%20across%20popular%0Adatasets%20and%20can%20be%20employed%20for%20various%20use-cases.%20We%20demonstrate%20that%20the%0Asame%20model%20can%20predict%20labels%2C%20impute%20missing%20values%2C%20or%20model%20event%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10648v1&entry.124074799=Read"},
{"title": "Improved Depth Estimation of Bayesian Neural Networks", "author": "Bart van Erp and Bert de Vries", "abstract": "  This paper proposes improvements over earlier work by Nazareth and Blei\n(2022) for estimating the depth of Bayesian neural networks. Here, we propose a\ndiscrete truncated normal distribution over the network depth to independently\nlearn its mean and variance. Posterior distributions are inferred by minimizing\nthe variational free energy, which balances the model complexity and accuracy.\nOur method improves test accuracy in the spiral data set and reduces the\nvariance in posterior depth estimates.\n", "link": "http://arxiv.org/abs/2410.10395v1", "date": "2024-10-14", "relevancy": 1.9589, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Depth%20Estimation%20of%20Bayesian%20Neural%20Networks&body=Title%3A%20Improved%20Depth%20Estimation%20of%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Bart%20van%20Erp%20and%20Bert%20de%20Vries%0AAbstract%3A%20%20%20This%20paper%20proposes%20improvements%20over%20earlier%20work%20by%20Nazareth%20and%20Blei%0A%282022%29%20for%20estimating%20the%20depth%20of%20Bayesian%20neural%20networks.%20Here%2C%20we%20propose%20a%0Adiscrete%20truncated%20normal%20distribution%20over%20the%20network%20depth%20to%20independently%0Alearn%20its%20mean%20and%20variance.%20Posterior%20distributions%20are%20inferred%20by%20minimizing%0Athe%20variational%20free%20energy%2C%20which%20balances%20the%20model%20complexity%20and%20accuracy.%0AOur%20method%20improves%20test%20accuracy%20in%20the%20spiral%20data%20set%20and%20reduces%20the%0Avariance%20in%20posterior%20depth%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Depth%2520Estimation%2520of%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DBart%2520van%2520Erp%2520and%2520Bert%2520de%2520Vries%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520improvements%2520over%2520earlier%2520work%2520by%2520Nazareth%2520and%2520Blei%250A%25282022%2529%2520for%2520estimating%2520the%2520depth%2520of%2520Bayesian%2520neural%2520networks.%2520Here%252C%2520we%2520propose%2520a%250Adiscrete%2520truncated%2520normal%2520distribution%2520over%2520the%2520network%2520depth%2520to%2520independently%250Alearn%2520its%2520mean%2520and%2520variance.%2520Posterior%2520distributions%2520are%2520inferred%2520by%2520minimizing%250Athe%2520variational%2520free%2520energy%252C%2520which%2520balances%2520the%2520model%2520complexity%2520and%2520accuracy.%250AOur%2520method%2520improves%2520test%2520accuracy%2520in%2520the%2520spiral%2520data%2520set%2520and%2520reduces%2520the%250Avariance%2520in%2520posterior%2520depth%2520estimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Depth%20Estimation%20of%20Bayesian%20Neural%20Networks&entry.906535625=Bart%20van%20Erp%20and%20Bert%20de%20Vries&entry.1292438233=%20%20This%20paper%20proposes%20improvements%20over%20earlier%20work%20by%20Nazareth%20and%20Blei%0A%282022%29%20for%20estimating%20the%20depth%20of%20Bayesian%20neural%20networks.%20Here%2C%20we%20propose%20a%0Adiscrete%20truncated%20normal%20distribution%20over%20the%20network%20depth%20to%20independently%0Alearn%20its%20mean%20and%20variance.%20Posterior%20distributions%20are%20inferred%20by%20minimizing%0Athe%20variational%20free%20energy%2C%20which%20balances%20the%20model%20complexity%20and%20accuracy.%0AOur%20method%20improves%20test%20accuracy%20in%20the%20spiral%20data%20set%20and%20reduces%20the%0Avariance%20in%20posterior%20depth%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10395v1&entry.124074799=Read"},
{"title": "Synthetic Potential Outcomes and Causal Mixture Identifiability", "author": "Bijan Mazaheri and Chandler Squires and Caroline Uhler", "abstract": "  A mixture model consists of a latent class that exerts a discrete signal on\nthe observed data. Uncovering these latent classes is fundamental to\nunsupervised learning. In this paper, we consider the problem of recovering\nlatent classes defined with respect to causal responses. We allow overlapping\nsupport in the distributions of these classes, meaning individuals cannot be\nclustered into groups with a similar response. Instead, we build on a setting\nfrom proximal causal inference to develop a method of moments approach to\nsynthetically sample potential outcome distributions. This approach is the\nfirst known identifiability result for what we call Mixtures of Treatment\nEffects (MTEs). More broadly, we show how MTEs fit into a hierarchy of causal\nidentifiability that unifies a number of perspectives on latent class\nconfounding.\n", "link": "http://arxiv.org/abs/2405.19225v2", "date": "2024-10-14", "relevancy": 1.8021, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4399}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Potential%20Outcomes%20and%20Causal%20Mixture%20Identifiability&body=Title%3A%20Synthetic%20Potential%20Outcomes%20and%20Causal%20Mixture%20Identifiability%0AAuthor%3A%20Bijan%20Mazaheri%20and%20Chandler%20Squires%20and%20Caroline%20Uhler%0AAbstract%3A%20%20%20A%20mixture%20model%20consists%20of%20a%20latent%20class%20that%20exerts%20a%20discrete%20signal%20on%0Athe%20observed%20data.%20Uncovering%20these%20latent%20classes%20is%20fundamental%20to%0Aunsupervised%20learning.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20recovering%0Alatent%20classes%20defined%20with%20respect%20to%20causal%20responses.%20We%20allow%20overlapping%0Asupport%20in%20the%20distributions%20of%20these%20classes%2C%20meaning%20individuals%20cannot%20be%0Aclustered%20into%20groups%20with%20a%20similar%20response.%20Instead%2C%20we%20build%20on%20a%20setting%0Afrom%20proximal%20causal%20inference%20to%20develop%20a%20method%20of%20moments%20approach%20to%0Asynthetically%20sample%20potential%20outcome%20distributions.%20This%20approach%20is%20the%0Afirst%20known%20identifiability%20result%20for%20what%20we%20call%20Mixtures%20of%20Treatment%0AEffects%20%28MTEs%29.%20More%20broadly%2C%20we%20show%20how%20MTEs%20fit%20into%20a%20hierarchy%20of%20causal%0Aidentifiability%20that%20unifies%20a%20number%20of%20perspectives%20on%20latent%20class%0Aconfounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Potential%2520Outcomes%2520and%2520Causal%2520Mixture%2520Identifiability%26entry.906535625%3DBijan%2520Mazaheri%2520and%2520Chandler%2520Squires%2520and%2520Caroline%2520Uhler%26entry.1292438233%3D%2520%2520A%2520mixture%2520model%2520consists%2520of%2520a%2520latent%2520class%2520that%2520exerts%2520a%2520discrete%2520signal%2520on%250Athe%2520observed%2520data.%2520Uncovering%2520these%2520latent%2520classes%2520is%2520fundamental%2520to%250Aunsupervised%2520learning.%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520of%2520recovering%250Alatent%2520classes%2520defined%2520with%2520respect%2520to%2520causal%2520responses.%2520We%2520allow%2520overlapping%250Asupport%2520in%2520the%2520distributions%2520of%2520these%2520classes%252C%2520meaning%2520individuals%2520cannot%2520be%250Aclustered%2520into%2520groups%2520with%2520a%2520similar%2520response.%2520Instead%252C%2520we%2520build%2520on%2520a%2520setting%250Afrom%2520proximal%2520causal%2520inference%2520to%2520develop%2520a%2520method%2520of%2520moments%2520approach%2520to%250Asynthetically%2520sample%2520potential%2520outcome%2520distributions.%2520This%2520approach%2520is%2520the%250Afirst%2520known%2520identifiability%2520result%2520for%2520what%2520we%2520call%2520Mixtures%2520of%2520Treatment%250AEffects%2520%2528MTEs%2529.%2520More%2520broadly%252C%2520we%2520show%2520how%2520MTEs%2520fit%2520into%2520a%2520hierarchy%2520of%2520causal%250Aidentifiability%2520that%2520unifies%2520a%2520number%2520of%2520perspectives%2520on%2520latent%2520class%250Aconfounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Potential%20Outcomes%20and%20Causal%20Mixture%20Identifiability&entry.906535625=Bijan%20Mazaheri%20and%20Chandler%20Squires%20and%20Caroline%20Uhler&entry.1292438233=%20%20A%20mixture%20model%20consists%20of%20a%20latent%20class%20that%20exerts%20a%20discrete%20signal%20on%0Athe%20observed%20data.%20Uncovering%20these%20latent%20classes%20is%20fundamental%20to%0Aunsupervised%20learning.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20recovering%0Alatent%20classes%20defined%20with%20respect%20to%20causal%20responses.%20We%20allow%20overlapping%0Asupport%20in%20the%20distributions%20of%20these%20classes%2C%20meaning%20individuals%20cannot%20be%0Aclustered%20into%20groups%20with%20a%20similar%20response.%20Instead%2C%20we%20build%20on%20a%20setting%0Afrom%20proximal%20causal%20inference%20to%20develop%20a%20method%20of%20moments%20approach%20to%0Asynthetically%20sample%20potential%20outcome%20distributions.%20This%20approach%20is%20the%0Afirst%20known%20identifiability%20result%20for%20what%20we%20call%20Mixtures%20of%20Treatment%0AEffects%20%28MTEs%29.%20More%20broadly%2C%20we%20show%20how%20MTEs%20fit%20into%20a%20hierarchy%20of%20causal%0Aidentifiability%20that%20unifies%20a%20number%20of%20perspectives%20on%20latent%20class%0Aconfounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19225v2&entry.124074799=Read"},
{"title": "Separation of Neural Drives to Muscles from Transferred Polyfunctional\n  Nerves using Implanted Micro-electrode Arrays", "author": "Laura Ferrante and Anna Boesendorfer and Deren Yusuf Barsakcioglu and Benedikt Baumgartner and Yazan Al-Ajam and Alex Woollard and Norbert Venantius Kang and Oskar Aszmann and Dario Farina", "abstract": "  Following limb amputation, neural signals for limb functions persist in the\nresidual peripheral nerves. Targeted muscle reinnervation (TMR) allows to\nredirected these signals into spare muscles to recover the neural information\nthrough electromyography (EMG). However, a significant challenge arises in\nseparating distinct neural commands redirected from the transferred nerves to\nthe muscles. Disentangling overlapping signals from EMG recordings remains\ncomplex, as they can contain mixed neural information that complicates limb\nfunction interpretation. To address this challenge, Regenerative Peripheral\nNerve Interfaces (RPNIs) surgically partition the nerve into individual\nfascicles that reinnervate specific muscle grafts, isolating distinct neural\nsources for more precise control and interpretation of EMG signals. We\nintroduce a novel biointerface that combines TMR surgery of polyvalent nerves\nwith a high-density micro-electrode array implanted at a single site within a\nreinnervated muscle. Instead of surgically identifying distinct nerve\nfascicles, our approach separates all neural signals that are re-directed into\na single muscle, using the high spatio-temporal selectivity of the\nmicro-electrode array and mathematical source separation methods. We recorded\nEMG signals from four reinnervated muscles while volunteers performed phantom\nlimb tasks. The decomposition of these signals into motor unit activity\nrevealed distinct clusters of motor neurons associated with diverse functional\ntasks. Notably, our method enabled the extraction of multiple neural commands\nwithin a single reinnervated muscle, eliminating the need for surgical nerve\ndivision. This approach not only has the potential of enhancing prosthesis\ncontrol but also uncovers mechanisms of motor neuron synergies following TMR,\nproviding valuable insights into how the central nervous system encodes\nmovement after reinnervation.\n", "link": "http://arxiv.org/abs/2410.10694v1", "date": "2024-10-14", "relevancy": 1.7207, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separation%20of%20Neural%20Drives%20to%20Muscles%20from%20Transferred%20Polyfunctional%0A%20%20Nerves%20using%20Implanted%20Micro-electrode%20Arrays&body=Title%3A%20Separation%20of%20Neural%20Drives%20to%20Muscles%20from%20Transferred%20Polyfunctional%0A%20%20Nerves%20using%20Implanted%20Micro-electrode%20Arrays%0AAuthor%3A%20Laura%20Ferrante%20and%20Anna%20Boesendorfer%20and%20Deren%20Yusuf%20Barsakcioglu%20and%20Benedikt%20Baumgartner%20and%20Yazan%20Al-Ajam%20and%20Alex%20Woollard%20and%20Norbert%20Venantius%20Kang%20and%20Oskar%20Aszmann%20and%20Dario%20Farina%0AAbstract%3A%20%20%20Following%20limb%20amputation%2C%20neural%20signals%20for%20limb%20functions%20persist%20in%20the%0Aresidual%20peripheral%20nerves.%20Targeted%20muscle%20reinnervation%20%28TMR%29%20allows%20to%0Aredirected%20these%20signals%20into%20spare%20muscles%20to%20recover%20the%20neural%20information%0Athrough%20electromyography%20%28EMG%29.%20However%2C%20a%20significant%20challenge%20arises%20in%0Aseparating%20distinct%20neural%20commands%20redirected%20from%20the%20transferred%20nerves%20to%0Athe%20muscles.%20Disentangling%20overlapping%20signals%20from%20EMG%20recordings%20remains%0Acomplex%2C%20as%20they%20can%20contain%20mixed%20neural%20information%20that%20complicates%20limb%0Afunction%20interpretation.%20To%20address%20this%20challenge%2C%20Regenerative%20Peripheral%0ANerve%20Interfaces%20%28RPNIs%29%20surgically%20partition%20the%20nerve%20into%20individual%0Afascicles%20that%20reinnervate%20specific%20muscle%20grafts%2C%20isolating%20distinct%20neural%0Asources%20for%20more%20precise%20control%20and%20interpretation%20of%20EMG%20signals.%20We%0Aintroduce%20a%20novel%20biointerface%20that%20combines%20TMR%20surgery%20of%20polyvalent%20nerves%0Awith%20a%20high-density%20micro-electrode%20array%20implanted%20at%20a%20single%20site%20within%20a%0Areinnervated%20muscle.%20Instead%20of%20surgically%20identifying%20distinct%20nerve%0Afascicles%2C%20our%20approach%20separates%20all%20neural%20signals%20that%20are%20re-directed%20into%0Aa%20single%20muscle%2C%20using%20the%20high%20spatio-temporal%20selectivity%20of%20the%0Amicro-electrode%20array%20and%20mathematical%20source%20separation%20methods.%20We%20recorded%0AEMG%20signals%20from%20four%20reinnervated%20muscles%20while%20volunteers%20performed%20phantom%0Alimb%20tasks.%20The%20decomposition%20of%20these%20signals%20into%20motor%20unit%20activity%0Arevealed%20distinct%20clusters%20of%20motor%20neurons%20associated%20with%20diverse%20functional%0Atasks.%20Notably%2C%20our%20method%20enabled%20the%20extraction%20of%20multiple%20neural%20commands%0Awithin%20a%20single%20reinnervated%20muscle%2C%20eliminating%20the%20need%20for%20surgical%20nerve%0Adivision.%20This%20approach%20not%20only%20has%20the%20potential%20of%20enhancing%20prosthesis%0Acontrol%20but%20also%20uncovers%20mechanisms%20of%20motor%20neuron%20synergies%20following%20TMR%2C%0Aproviding%20valuable%20insights%20into%20how%20the%20central%20nervous%20system%20encodes%0Amovement%20after%20reinnervation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparation%2520of%2520Neural%2520Drives%2520to%2520Muscles%2520from%2520Transferred%2520Polyfunctional%250A%2520%2520Nerves%2520using%2520Implanted%2520Micro-electrode%2520Arrays%26entry.906535625%3DLaura%2520Ferrante%2520and%2520Anna%2520Boesendorfer%2520and%2520Deren%2520Yusuf%2520Barsakcioglu%2520and%2520Benedikt%2520Baumgartner%2520and%2520Yazan%2520Al-Ajam%2520and%2520Alex%2520Woollard%2520and%2520Norbert%2520Venantius%2520Kang%2520and%2520Oskar%2520Aszmann%2520and%2520Dario%2520Farina%26entry.1292438233%3D%2520%2520Following%2520limb%2520amputation%252C%2520neural%2520signals%2520for%2520limb%2520functions%2520persist%2520in%2520the%250Aresidual%2520peripheral%2520nerves.%2520Targeted%2520muscle%2520reinnervation%2520%2528TMR%2529%2520allows%2520to%250Aredirected%2520these%2520signals%2520into%2520spare%2520muscles%2520to%2520recover%2520the%2520neural%2520information%250Athrough%2520electromyography%2520%2528EMG%2529.%2520However%252C%2520a%2520significant%2520challenge%2520arises%2520in%250Aseparating%2520distinct%2520neural%2520commands%2520redirected%2520from%2520the%2520transferred%2520nerves%2520to%250Athe%2520muscles.%2520Disentangling%2520overlapping%2520signals%2520from%2520EMG%2520recordings%2520remains%250Acomplex%252C%2520as%2520they%2520can%2520contain%2520mixed%2520neural%2520information%2520that%2520complicates%2520limb%250Afunction%2520interpretation.%2520To%2520address%2520this%2520challenge%252C%2520Regenerative%2520Peripheral%250ANerve%2520Interfaces%2520%2528RPNIs%2529%2520surgically%2520partition%2520the%2520nerve%2520into%2520individual%250Afascicles%2520that%2520reinnervate%2520specific%2520muscle%2520grafts%252C%2520isolating%2520distinct%2520neural%250Asources%2520for%2520more%2520precise%2520control%2520and%2520interpretation%2520of%2520EMG%2520signals.%2520We%250Aintroduce%2520a%2520novel%2520biointerface%2520that%2520combines%2520TMR%2520surgery%2520of%2520polyvalent%2520nerves%250Awith%2520a%2520high-density%2520micro-electrode%2520array%2520implanted%2520at%2520a%2520single%2520site%2520within%2520a%250Areinnervated%2520muscle.%2520Instead%2520of%2520surgically%2520identifying%2520distinct%2520nerve%250Afascicles%252C%2520our%2520approach%2520separates%2520all%2520neural%2520signals%2520that%2520are%2520re-directed%2520into%250Aa%2520single%2520muscle%252C%2520using%2520the%2520high%2520spatio-temporal%2520selectivity%2520of%2520the%250Amicro-electrode%2520array%2520and%2520mathematical%2520source%2520separation%2520methods.%2520We%2520recorded%250AEMG%2520signals%2520from%2520four%2520reinnervated%2520muscles%2520while%2520volunteers%2520performed%2520phantom%250Alimb%2520tasks.%2520The%2520decomposition%2520of%2520these%2520signals%2520into%2520motor%2520unit%2520activity%250Arevealed%2520distinct%2520clusters%2520of%2520motor%2520neurons%2520associated%2520with%2520diverse%2520functional%250Atasks.%2520Notably%252C%2520our%2520method%2520enabled%2520the%2520extraction%2520of%2520multiple%2520neural%2520commands%250Awithin%2520a%2520single%2520reinnervated%2520muscle%252C%2520eliminating%2520the%2520need%2520for%2520surgical%2520nerve%250Adivision.%2520This%2520approach%2520not%2520only%2520has%2520the%2520potential%2520of%2520enhancing%2520prosthesis%250Acontrol%2520but%2520also%2520uncovers%2520mechanisms%2520of%2520motor%2520neuron%2520synergies%2520following%2520TMR%252C%250Aproviding%2520valuable%2520insights%2520into%2520how%2520the%2520central%2520nervous%2520system%2520encodes%250Amovement%2520after%2520reinnervation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separation%20of%20Neural%20Drives%20to%20Muscles%20from%20Transferred%20Polyfunctional%0A%20%20Nerves%20using%20Implanted%20Micro-electrode%20Arrays&entry.906535625=Laura%20Ferrante%20and%20Anna%20Boesendorfer%20and%20Deren%20Yusuf%20Barsakcioglu%20and%20Benedikt%20Baumgartner%20and%20Yazan%20Al-Ajam%20and%20Alex%20Woollard%20and%20Norbert%20Venantius%20Kang%20and%20Oskar%20Aszmann%20and%20Dario%20Farina&entry.1292438233=%20%20Following%20limb%20amputation%2C%20neural%20signals%20for%20limb%20functions%20persist%20in%20the%0Aresidual%20peripheral%20nerves.%20Targeted%20muscle%20reinnervation%20%28TMR%29%20allows%20to%0Aredirected%20these%20signals%20into%20spare%20muscles%20to%20recover%20the%20neural%20information%0Athrough%20electromyography%20%28EMG%29.%20However%2C%20a%20significant%20challenge%20arises%20in%0Aseparating%20distinct%20neural%20commands%20redirected%20from%20the%20transferred%20nerves%20to%0Athe%20muscles.%20Disentangling%20overlapping%20signals%20from%20EMG%20recordings%20remains%0Acomplex%2C%20as%20they%20can%20contain%20mixed%20neural%20information%20that%20complicates%20limb%0Afunction%20interpretation.%20To%20address%20this%20challenge%2C%20Regenerative%20Peripheral%0ANerve%20Interfaces%20%28RPNIs%29%20surgically%20partition%20the%20nerve%20into%20individual%0Afascicles%20that%20reinnervate%20specific%20muscle%20grafts%2C%20isolating%20distinct%20neural%0Asources%20for%20more%20precise%20control%20and%20interpretation%20of%20EMG%20signals.%20We%0Aintroduce%20a%20novel%20biointerface%20that%20combines%20TMR%20surgery%20of%20polyvalent%20nerves%0Awith%20a%20high-density%20micro-electrode%20array%20implanted%20at%20a%20single%20site%20within%20a%0Areinnervated%20muscle.%20Instead%20of%20surgically%20identifying%20distinct%20nerve%0Afascicles%2C%20our%20approach%20separates%20all%20neural%20signals%20that%20are%20re-directed%20into%0Aa%20single%20muscle%2C%20using%20the%20high%20spatio-temporal%20selectivity%20of%20the%0Amicro-electrode%20array%20and%20mathematical%20source%20separation%20methods.%20We%20recorded%0AEMG%20signals%20from%20four%20reinnervated%20muscles%20while%20volunteers%20performed%20phantom%0Alimb%20tasks.%20The%20decomposition%20of%20these%20signals%20into%20motor%20unit%20activity%0Arevealed%20distinct%20clusters%20of%20motor%20neurons%20associated%20with%20diverse%20functional%0Atasks.%20Notably%2C%20our%20method%20enabled%20the%20extraction%20of%20multiple%20neural%20commands%0Awithin%20a%20single%20reinnervated%20muscle%2C%20eliminating%20the%20need%20for%20surgical%20nerve%0Adivision.%20This%20approach%20not%20only%20has%20the%20potential%20of%20enhancing%20prosthesis%0Acontrol%20but%20also%20uncovers%20mechanisms%20of%20motor%20neuron%20synergies%20following%20TMR%2C%0Aproviding%20valuable%20insights%20into%20how%20the%20central%20nervous%20system%20encodes%0Amovement%20after%20reinnervation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10694v1&entry.124074799=Read"},
{"title": "Comparison of deep learning and conventional methods for disease onset\n  prediction", "author": "Luis H. John and Chungsoo Kim and Jan A. Kors and Junhyuk Chang and Hannah Morgan-Cooper and Priya Desai and Chao Pang and Peter R. Rijnbeek and Jenna M. Reps and Egill A. Fridgeirsson", "abstract": "  Background: Conventional prediction methods such as logistic regression and\ngradient boosting have been widely utilized for disease onset prediction for\ntheir reliability and interpretability. Deep learning methods promise enhanced\nprediction performance by extracting complex patterns from clinical data, but\nface challenges like data sparsity and high dimensionality.\n  Methods: This study compares conventional and deep learning approaches to\npredict lung cancer, dementia, and bipolar disorder using observational data\nfrom eleven databases from North America, Europe, and Asia. Models were\ndeveloped using logistic regression, gradient boosting, ResNet, and\nTransformer, and validated both internally and externally across the data\nsources. Discrimination performance was assessed using AUROC, and calibration\nwas evaluated using Eavg.\n  Findings: Across 11 datasets, conventional methods generally outperformed\ndeep learning methods in terms of discrimination performance, particularly\nduring external validation, highlighting their better transportability.\nLearning curves suggest that deep learning models require substantially larger\ndatasets to reach the same performance levels as conventional methods.\nCalibration performance was also better for conventional methods, with ResNet\nshowing the poorest calibration.\n  Interpretation: Despite the potential of deep learning models to capture\ncomplex patterns in structured observational healthcare data, conventional\nmodels remain highly competitive for disease onset prediction, especially in\nscenarios involving smaller datasets and if lengthy training times need to be\navoided. The study underscores the need for future research focused on\noptimizing deep learning models to handle the sparsity, high dimensionality,\nand heterogeneity inherent in healthcare datasets, and find new strategies to\nexploit the full capabilities of deep learning methods.\n", "link": "http://arxiv.org/abs/2410.10505v1", "date": "2024-10-14", "relevancy": 1.4378, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.463}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20deep%20learning%20and%20conventional%20methods%20for%20disease%20onset%0A%20%20prediction&body=Title%3A%20Comparison%20of%20deep%20learning%20and%20conventional%20methods%20for%20disease%20onset%0A%20%20prediction%0AAuthor%3A%20Luis%20H.%20John%20and%20Chungsoo%20Kim%20and%20Jan%20A.%20Kors%20and%20Junhyuk%20Chang%20and%20Hannah%20Morgan-Cooper%20and%20Priya%20Desai%20and%20Chao%20Pang%20and%20Peter%20R.%20Rijnbeek%20and%20Jenna%20M.%20Reps%20and%20Egill%20A.%20Fridgeirsson%0AAbstract%3A%20%20%20Background%3A%20Conventional%20prediction%20methods%20such%20as%20logistic%20regression%20and%0Agradient%20boosting%20have%20been%20widely%20utilized%20for%20disease%20onset%20prediction%20for%0Atheir%20reliability%20and%20interpretability.%20Deep%20learning%20methods%20promise%20enhanced%0Aprediction%20performance%20by%20extracting%20complex%20patterns%20from%20clinical%20data%2C%20but%0Aface%20challenges%20like%20data%20sparsity%20and%20high%20dimensionality.%0A%20%20Methods%3A%20This%20study%20compares%20conventional%20and%20deep%20learning%20approaches%20to%0Apredict%20lung%20cancer%2C%20dementia%2C%20and%20bipolar%20disorder%20using%20observational%20data%0Afrom%20eleven%20databases%20from%20North%20America%2C%20Europe%2C%20and%20Asia.%20Models%20were%0Adeveloped%20using%20logistic%20regression%2C%20gradient%20boosting%2C%20ResNet%2C%20and%0ATransformer%2C%20and%20validated%20both%20internally%20and%20externally%20across%20the%20data%0Asources.%20Discrimination%20performance%20was%20assessed%20using%20AUROC%2C%20and%20calibration%0Awas%20evaluated%20using%20Eavg.%0A%20%20Findings%3A%20Across%2011%20datasets%2C%20conventional%20methods%20generally%20outperformed%0Adeep%20learning%20methods%20in%20terms%20of%20discrimination%20performance%2C%20particularly%0Aduring%20external%20validation%2C%20highlighting%20their%20better%20transportability.%0ALearning%20curves%20suggest%20that%20deep%20learning%20models%20require%20substantially%20larger%0Adatasets%20to%20reach%20the%20same%20performance%20levels%20as%20conventional%20methods.%0ACalibration%20performance%20was%20also%20better%20for%20conventional%20methods%2C%20with%20ResNet%0Ashowing%20the%20poorest%20calibration.%0A%20%20Interpretation%3A%20Despite%20the%20potential%20of%20deep%20learning%20models%20to%20capture%0Acomplex%20patterns%20in%20structured%20observational%20healthcare%20data%2C%20conventional%0Amodels%20remain%20highly%20competitive%20for%20disease%20onset%20prediction%2C%20especially%20in%0Ascenarios%20involving%20smaller%20datasets%20and%20if%20lengthy%20training%20times%20need%20to%20be%0Aavoided.%20The%20study%20underscores%20the%20need%20for%20future%20research%20focused%20on%0Aoptimizing%20deep%20learning%20models%20to%20handle%20the%20sparsity%2C%20high%20dimensionality%2C%0Aand%20heterogeneity%20inherent%20in%20healthcare%20datasets%2C%20and%20find%20new%20strategies%20to%0Aexploit%20the%20full%20capabilities%20of%20deep%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520deep%2520learning%2520and%2520conventional%2520methods%2520for%2520disease%2520onset%250A%2520%2520prediction%26entry.906535625%3DLuis%2520H.%2520John%2520and%2520Chungsoo%2520Kim%2520and%2520Jan%2520A.%2520Kors%2520and%2520Junhyuk%2520Chang%2520and%2520Hannah%2520Morgan-Cooper%2520and%2520Priya%2520Desai%2520and%2520Chao%2520Pang%2520and%2520Peter%2520R.%2520Rijnbeek%2520and%2520Jenna%2520M.%2520Reps%2520and%2520Egill%2520A.%2520Fridgeirsson%26entry.1292438233%3D%2520%2520Background%253A%2520Conventional%2520prediction%2520methods%2520such%2520as%2520logistic%2520regression%2520and%250Agradient%2520boosting%2520have%2520been%2520widely%2520utilized%2520for%2520disease%2520onset%2520prediction%2520for%250Atheir%2520reliability%2520and%2520interpretability.%2520Deep%2520learning%2520methods%2520promise%2520enhanced%250Aprediction%2520performance%2520by%2520extracting%2520complex%2520patterns%2520from%2520clinical%2520data%252C%2520but%250Aface%2520challenges%2520like%2520data%2520sparsity%2520and%2520high%2520dimensionality.%250A%2520%2520Methods%253A%2520This%2520study%2520compares%2520conventional%2520and%2520deep%2520learning%2520approaches%2520to%250Apredict%2520lung%2520cancer%252C%2520dementia%252C%2520and%2520bipolar%2520disorder%2520using%2520observational%2520data%250Afrom%2520eleven%2520databases%2520from%2520North%2520America%252C%2520Europe%252C%2520and%2520Asia.%2520Models%2520were%250Adeveloped%2520using%2520logistic%2520regression%252C%2520gradient%2520boosting%252C%2520ResNet%252C%2520and%250ATransformer%252C%2520and%2520validated%2520both%2520internally%2520and%2520externally%2520across%2520the%2520data%250Asources.%2520Discrimination%2520performance%2520was%2520assessed%2520using%2520AUROC%252C%2520and%2520calibration%250Awas%2520evaluated%2520using%2520Eavg.%250A%2520%2520Findings%253A%2520Across%252011%2520datasets%252C%2520conventional%2520methods%2520generally%2520outperformed%250Adeep%2520learning%2520methods%2520in%2520terms%2520of%2520discrimination%2520performance%252C%2520particularly%250Aduring%2520external%2520validation%252C%2520highlighting%2520their%2520better%2520transportability.%250ALearning%2520curves%2520suggest%2520that%2520deep%2520learning%2520models%2520require%2520substantially%2520larger%250Adatasets%2520to%2520reach%2520the%2520same%2520performance%2520levels%2520as%2520conventional%2520methods.%250ACalibration%2520performance%2520was%2520also%2520better%2520for%2520conventional%2520methods%252C%2520with%2520ResNet%250Ashowing%2520the%2520poorest%2520calibration.%250A%2520%2520Interpretation%253A%2520Despite%2520the%2520potential%2520of%2520deep%2520learning%2520models%2520to%2520capture%250Acomplex%2520patterns%2520in%2520structured%2520observational%2520healthcare%2520data%252C%2520conventional%250Amodels%2520remain%2520highly%2520competitive%2520for%2520disease%2520onset%2520prediction%252C%2520especially%2520in%250Ascenarios%2520involving%2520smaller%2520datasets%2520and%2520if%2520lengthy%2520training%2520times%2520need%2520to%2520be%250Aavoided.%2520The%2520study%2520underscores%2520the%2520need%2520for%2520future%2520research%2520focused%2520on%250Aoptimizing%2520deep%2520learning%2520models%2520to%2520handle%2520the%2520sparsity%252C%2520high%2520dimensionality%252C%250Aand%2520heterogeneity%2520inherent%2520in%2520healthcare%2520datasets%252C%2520and%2520find%2520new%2520strategies%2520to%250Aexploit%2520the%2520full%2520capabilities%2520of%2520deep%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20deep%20learning%20and%20conventional%20methods%20for%20disease%20onset%0A%20%20prediction&entry.906535625=Luis%20H.%20John%20and%20Chungsoo%20Kim%20and%20Jan%20A.%20Kors%20and%20Junhyuk%20Chang%20and%20Hannah%20Morgan-Cooper%20and%20Priya%20Desai%20and%20Chao%20Pang%20and%20Peter%20R.%20Rijnbeek%20and%20Jenna%20M.%20Reps%20and%20Egill%20A.%20Fridgeirsson&entry.1292438233=%20%20Background%3A%20Conventional%20prediction%20methods%20such%20as%20logistic%20regression%20and%0Agradient%20boosting%20have%20been%20widely%20utilized%20for%20disease%20onset%20prediction%20for%0Atheir%20reliability%20and%20interpretability.%20Deep%20learning%20methods%20promise%20enhanced%0Aprediction%20performance%20by%20extracting%20complex%20patterns%20from%20clinical%20data%2C%20but%0Aface%20challenges%20like%20data%20sparsity%20and%20high%20dimensionality.%0A%20%20Methods%3A%20This%20study%20compares%20conventional%20and%20deep%20learning%20approaches%20to%0Apredict%20lung%20cancer%2C%20dementia%2C%20and%20bipolar%20disorder%20using%20observational%20data%0Afrom%20eleven%20databases%20from%20North%20America%2C%20Europe%2C%20and%20Asia.%20Models%20were%0Adeveloped%20using%20logistic%20regression%2C%20gradient%20boosting%2C%20ResNet%2C%20and%0ATransformer%2C%20and%20validated%20both%20internally%20and%20externally%20across%20the%20data%0Asources.%20Discrimination%20performance%20was%20assessed%20using%20AUROC%2C%20and%20calibration%0Awas%20evaluated%20using%20Eavg.%0A%20%20Findings%3A%20Across%2011%20datasets%2C%20conventional%20methods%20generally%20outperformed%0Adeep%20learning%20methods%20in%20terms%20of%20discrimination%20performance%2C%20particularly%0Aduring%20external%20validation%2C%20highlighting%20their%20better%20transportability.%0ALearning%20curves%20suggest%20that%20deep%20learning%20models%20require%20substantially%20larger%0Adatasets%20to%20reach%20the%20same%20performance%20levels%20as%20conventional%20methods.%0ACalibration%20performance%20was%20also%20better%20for%20conventional%20methods%2C%20with%20ResNet%0Ashowing%20the%20poorest%20calibration.%0A%20%20Interpretation%3A%20Despite%20the%20potential%20of%20deep%20learning%20models%20to%20capture%0Acomplex%20patterns%20in%20structured%20observational%20healthcare%20data%2C%20conventional%0Amodels%20remain%20highly%20competitive%20for%20disease%20onset%20prediction%2C%20especially%20in%0Ascenarios%20involving%20smaller%20datasets%20and%20if%20lengthy%20training%20times%20need%20to%20be%0Aavoided.%20The%20study%20underscores%20the%20need%20for%20future%20research%20focused%20on%0Aoptimizing%20deep%20learning%20models%20to%20handle%20the%20sparsity%2C%20high%20dimensionality%2C%0Aand%20heterogeneity%20inherent%20in%20healthcare%20datasets%2C%20and%20find%20new%20strategies%20to%0Aexploit%20the%20full%20capabilities%20of%20deep%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10505v1&entry.124074799=Read"},
{"title": "Feudal Graph Reinforcement Learning", "author": "Tommaso Marzi and Arshjot Khehra and Andrea Cini and Cesare Alippi", "abstract": "  Graph-based representations and message-passing modular policies constitute\nprominent approaches to tackling composable control problems in reinforcement\nlearning (RL). However, as shown by recent graph deep learning literature, such\nlocal message-passing operators can create information bottlenecks and hinder\nglobal coordination. The issue becomes more serious in tasks requiring\nhigh-level planning. In this work, we propose a novel methodology, named Feudal\nGraph Reinforcement Learning (FGRL), that addresses such challenges by relying\non hierarchical RL and a pyramidal message-passing architecture. In particular,\nFGRL defines a hierarchy of policies where high-level commands are propagated\nfrom the top of the hierarchy down through a layered graph structure. The\nbottom layers mimic the morphology of the physical system, while the upper\nlayers correspond to higher-order sub-modules. The resulting agents are then\ncharacterized by a committee of policies where actions at a certain level set\ngoals for the level below, thus implementing a hierarchical decision-making\nstructure that can naturally implement task decomposition. We evaluate the\nproposed framework on a graph clustering problem and MuJoCo locomotion tasks;\nsimulation results show that FGRL compares favorably against relevant\nbaselines. Furthermore, an in-depth analysis of the command propagation\nmechanism provides evidence that the introduced message-passing scheme favors\nlearning hierarchical decision-making policies.\n", "link": "http://arxiv.org/abs/2304.05099v5", "date": "2024-10-14", "relevancy": 1.4918, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feudal%20Graph%20Reinforcement%20Learning&body=Title%3A%20Feudal%20Graph%20Reinforcement%20Learning%0AAuthor%3A%20Tommaso%20Marzi%20and%20Arshjot%20Khehra%20and%20Andrea%20Cini%20and%20Cesare%20Alippi%0AAbstract%3A%20%20%20Graph-based%20representations%20and%20message-passing%20modular%20policies%20constitute%0Aprominent%20approaches%20to%20tackling%20composable%20control%20problems%20in%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20as%20shown%20by%20recent%20graph%20deep%20learning%20literature%2C%20such%0Alocal%20message-passing%20operators%20can%20create%20information%20bottlenecks%20and%20hinder%0Aglobal%20coordination.%20The%20issue%20becomes%20more%20serious%20in%20tasks%20requiring%0Ahigh-level%20planning.%20In%20this%20work%2C%20we%20propose%20a%20novel%20methodology%2C%20named%20Feudal%0AGraph%20Reinforcement%20Learning%20%28FGRL%29%2C%20that%20addresses%20such%20challenges%20by%20relying%0Aon%20hierarchical%20RL%20and%20a%20pyramidal%20message-passing%20architecture.%20In%20particular%2C%0AFGRL%20defines%20a%20hierarchy%20of%20policies%20where%20high-level%20commands%20are%20propagated%0Afrom%20the%20top%20of%20the%20hierarchy%20down%20through%20a%20layered%20graph%20structure.%20The%0Abottom%20layers%20mimic%20the%20morphology%20of%20the%20physical%20system%2C%20while%20the%20upper%0Alayers%20correspond%20to%20higher-order%20sub-modules.%20The%20resulting%20agents%20are%20then%0Acharacterized%20by%20a%20committee%20of%20policies%20where%20actions%20at%20a%20certain%20level%20set%0Agoals%20for%20the%20level%20below%2C%20thus%20implementing%20a%20hierarchical%20decision-making%0Astructure%20that%20can%20naturally%20implement%20task%20decomposition.%20We%20evaluate%20the%0Aproposed%20framework%20on%20a%20graph%20clustering%20problem%20and%20MuJoCo%20locomotion%20tasks%3B%0Asimulation%20results%20show%20that%20FGRL%20compares%20favorably%20against%20relevant%0Abaselines.%20Furthermore%2C%20an%20in-depth%20analysis%20of%20the%20command%20propagation%0Amechanism%20provides%20evidence%20that%20the%20introduced%20message-passing%20scheme%20favors%0Alearning%20hierarchical%20decision-making%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.05099v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeudal%2520Graph%2520Reinforcement%2520Learning%26entry.906535625%3DTommaso%2520Marzi%2520and%2520Arshjot%2520Khehra%2520and%2520Andrea%2520Cini%2520and%2520Cesare%2520Alippi%26entry.1292438233%3D%2520%2520Graph-based%2520representations%2520and%2520message-passing%2520modular%2520policies%2520constitute%250Aprominent%2520approaches%2520to%2520tackling%2520composable%2520control%2520problems%2520in%2520reinforcement%250Alearning%2520%2528RL%2529.%2520However%252C%2520as%2520shown%2520by%2520recent%2520graph%2520deep%2520learning%2520literature%252C%2520such%250Alocal%2520message-passing%2520operators%2520can%2520create%2520information%2520bottlenecks%2520and%2520hinder%250Aglobal%2520coordination.%2520The%2520issue%2520becomes%2520more%2520serious%2520in%2520tasks%2520requiring%250Ahigh-level%2520planning.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520methodology%252C%2520named%2520Feudal%250AGraph%2520Reinforcement%2520Learning%2520%2528FGRL%2529%252C%2520that%2520addresses%2520such%2520challenges%2520by%2520relying%250Aon%2520hierarchical%2520RL%2520and%2520a%2520pyramidal%2520message-passing%2520architecture.%2520In%2520particular%252C%250AFGRL%2520defines%2520a%2520hierarchy%2520of%2520policies%2520where%2520high-level%2520commands%2520are%2520propagated%250Afrom%2520the%2520top%2520of%2520the%2520hierarchy%2520down%2520through%2520a%2520layered%2520graph%2520structure.%2520The%250Abottom%2520layers%2520mimic%2520the%2520morphology%2520of%2520the%2520physical%2520system%252C%2520while%2520the%2520upper%250Alayers%2520correspond%2520to%2520higher-order%2520sub-modules.%2520The%2520resulting%2520agents%2520are%2520then%250Acharacterized%2520by%2520a%2520committee%2520of%2520policies%2520where%2520actions%2520at%2520a%2520certain%2520level%2520set%250Agoals%2520for%2520the%2520level%2520below%252C%2520thus%2520implementing%2520a%2520hierarchical%2520decision-making%250Astructure%2520that%2520can%2520naturally%2520implement%2520task%2520decomposition.%2520We%2520evaluate%2520the%250Aproposed%2520framework%2520on%2520a%2520graph%2520clustering%2520problem%2520and%2520MuJoCo%2520locomotion%2520tasks%253B%250Asimulation%2520results%2520show%2520that%2520FGRL%2520compares%2520favorably%2520against%2520relevant%250Abaselines.%2520Furthermore%252C%2520an%2520in-depth%2520analysis%2520of%2520the%2520command%2520propagation%250Amechanism%2520provides%2520evidence%2520that%2520the%2520introduced%2520message-passing%2520scheme%2520favors%250Alearning%2520hierarchical%2520decision-making%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.05099v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feudal%20Graph%20Reinforcement%20Learning&entry.906535625=Tommaso%20Marzi%20and%20Arshjot%20Khehra%20and%20Andrea%20Cini%20and%20Cesare%20Alippi&entry.1292438233=%20%20Graph-based%20representations%20and%20message-passing%20modular%20policies%20constitute%0Aprominent%20approaches%20to%20tackling%20composable%20control%20problems%20in%20reinforcement%0Alearning%20%28RL%29.%20However%2C%20as%20shown%20by%20recent%20graph%20deep%20learning%20literature%2C%20such%0Alocal%20message-passing%20operators%20can%20create%20information%20bottlenecks%20and%20hinder%0Aglobal%20coordination.%20The%20issue%20becomes%20more%20serious%20in%20tasks%20requiring%0Ahigh-level%20planning.%20In%20this%20work%2C%20we%20propose%20a%20novel%20methodology%2C%20named%20Feudal%0AGraph%20Reinforcement%20Learning%20%28FGRL%29%2C%20that%20addresses%20such%20challenges%20by%20relying%0Aon%20hierarchical%20RL%20and%20a%20pyramidal%20message-passing%20architecture.%20In%20particular%2C%0AFGRL%20defines%20a%20hierarchy%20of%20policies%20where%20high-level%20commands%20are%20propagated%0Afrom%20the%20top%20of%20the%20hierarchy%20down%20through%20a%20layered%20graph%20structure.%20The%0Abottom%20layers%20mimic%20the%20morphology%20of%20the%20physical%20system%2C%20while%20the%20upper%0Alayers%20correspond%20to%20higher-order%20sub-modules.%20The%20resulting%20agents%20are%20then%0Acharacterized%20by%20a%20committee%20of%20policies%20where%20actions%20at%20a%20certain%20level%20set%0Agoals%20for%20the%20level%20below%2C%20thus%20implementing%20a%20hierarchical%20decision-making%0Astructure%20that%20can%20naturally%20implement%20task%20decomposition.%20We%20evaluate%20the%0Aproposed%20framework%20on%20a%20graph%20clustering%20problem%20and%20MuJoCo%20locomotion%20tasks%3B%0Asimulation%20results%20show%20that%20FGRL%20compares%20favorably%20against%20relevant%0Abaselines.%20Furthermore%2C%20an%20in-depth%20analysis%20of%20the%20command%20propagation%0Amechanism%20provides%20evidence%20that%20the%20introduced%20message-passing%20scheme%20favors%0Alearning%20hierarchical%20decision-making%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05099v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


