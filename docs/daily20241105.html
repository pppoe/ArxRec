<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241104.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GenXD: Generating Any 3D and 4D Scenes", "author": "Yuyang Zhao and Chung-Ching Lin and Kevin Lin and Zhiwen Yan and Linjie Li and Zhengyuan Yang and Jianfeng Wang and Gim Hee Lee and Lijuan Wang", "abstract": "  Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.\n", "link": "http://arxiv.org/abs/2411.02319v1", "date": "2024-11-04", "relevancy": 3.3871, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6971}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6971}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenXD%3A%20Generating%20Any%203D%20and%204D%20Scenes&body=Title%3A%20GenXD%3A%20Generating%20Any%203D%20and%204D%20Scenes%0AAuthor%3A%20Yuyang%20Zhao%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Zhiwen%20Yan%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Jianfeng%20Wang%20and%20Gim%20Hee%20Lee%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20Recent%20developments%20in%202D%20visual%20generation%20have%20been%20remarkably%20successful.%0AHowever%2C%203D%20and%204D%20generation%20remain%20challenging%20in%20real-world%20applications%20due%0Ato%20the%20lack%20of%20large-scale%204D%20data%20and%20effective%20model%20design.%20In%20this%20paper%2C%0Awe%20propose%20to%20jointly%20investigate%20general%203D%20and%204D%20generation%20by%20leveraging%0Acamera%20and%20object%20movements%20commonly%20observed%20in%20daily%20life.%20Due%20to%20the%20lack%20of%0Areal-world%204D%20data%20in%20the%20community%2C%20we%20first%20propose%20a%20data%20curation%20pipeline%0Ato%20obtain%20camera%20poses%20and%20object%20motion%20strength%20from%20videos.%20Based%20on%20this%0Apipeline%2C%20we%20introduce%20a%20large-scale%20real-world%204D%20scene%20dataset%3A%20CamVid-30K.%0ABy%20leveraging%20all%20the%203D%20and%204D%20data%2C%20we%20develop%20our%20framework%2C%20GenXD%2C%20which%0Aallows%20us%20to%20produce%20any%203D%20or%204D%20scene.%20We%20propose%20multiview-temporal%20modules%2C%0Awhich%20disentangle%20camera%20and%20object%20movements%2C%20to%20seamlessly%20learn%20from%20both%203D%0Aand%204D%20data.%20Additionally%2C%20GenXD%20employs%20masked%20latent%20conditions%20to%20support%20a%0Avariety%20of%20conditioning%20views.%20GenXD%20can%20generate%20videos%20that%20follow%20the%20camera%0Atrajectory%20as%20well%20as%20consistent%203D%20views%20that%20can%20be%20lifted%20into%203D%0Arepresentations.%20We%20perform%20extensive%20evaluations%20across%20various%20real-world%20and%0Asynthetic%20datasets%2C%20demonstrating%20GenXD%27s%20effectiveness%20and%20versatility%0Acompared%20to%20previous%20methods%20in%203D%20and%204D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenXD%253A%2520Generating%2520Any%25203D%2520and%25204D%2520Scenes%26entry.906535625%3DYuyang%2520Zhao%2520and%2520Chung-Ching%2520Lin%2520and%2520Kevin%2520Lin%2520and%2520Zhiwen%2520Yan%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Jianfeng%2520Wang%2520and%2520Gim%2520Hee%2520Lee%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%25202D%2520visual%2520generation%2520have%2520been%2520remarkably%2520successful.%250AHowever%252C%25203D%2520and%25204D%2520generation%2520remain%2520challenging%2520in%2520real-world%2520applications%2520due%250Ato%2520the%2520lack%2520of%2520large-scale%25204D%2520data%2520and%2520effective%2520model%2520design.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520to%2520jointly%2520investigate%2520general%25203D%2520and%25204D%2520generation%2520by%2520leveraging%250Acamera%2520and%2520object%2520movements%2520commonly%2520observed%2520in%2520daily%2520life.%2520Due%2520to%2520the%2520lack%2520of%250Areal-world%25204D%2520data%2520in%2520the%2520community%252C%2520we%2520first%2520propose%2520a%2520data%2520curation%2520pipeline%250Ato%2520obtain%2520camera%2520poses%2520and%2520object%2520motion%2520strength%2520from%2520videos.%2520Based%2520on%2520this%250Apipeline%252C%2520we%2520introduce%2520a%2520large-scale%2520real-world%25204D%2520scene%2520dataset%253A%2520CamVid-30K.%250ABy%2520leveraging%2520all%2520the%25203D%2520and%25204D%2520data%252C%2520we%2520develop%2520our%2520framework%252C%2520GenXD%252C%2520which%250Aallows%2520us%2520to%2520produce%2520any%25203D%2520or%25204D%2520scene.%2520We%2520propose%2520multiview-temporal%2520modules%252C%250Awhich%2520disentangle%2520camera%2520and%2520object%2520movements%252C%2520to%2520seamlessly%2520learn%2520from%2520both%25203D%250Aand%25204D%2520data.%2520Additionally%252C%2520GenXD%2520employs%2520masked%2520latent%2520conditions%2520to%2520support%2520a%250Avariety%2520of%2520conditioning%2520views.%2520GenXD%2520can%2520generate%2520videos%2520that%2520follow%2520the%2520camera%250Atrajectory%2520as%2520well%2520as%2520consistent%25203D%2520views%2520that%2520can%2520be%2520lifted%2520into%25203D%250Arepresentations.%2520We%2520perform%2520extensive%2520evaluations%2520across%2520various%2520real-world%2520and%250Asynthetic%2520datasets%252C%2520demonstrating%2520GenXD%2527s%2520effectiveness%2520and%2520versatility%250Acompared%2520to%2520previous%2520methods%2520in%25203D%2520and%25204D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenXD%3A%20Generating%20Any%203D%20and%204D%20Scenes&entry.906535625=Yuyang%20Zhao%20and%20Chung-Ching%20Lin%20and%20Kevin%20Lin%20and%20Zhiwen%20Yan%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Jianfeng%20Wang%20and%20Gim%20Hee%20Lee%20and%20Lijuan%20Wang&entry.1292438233=%20%20Recent%20developments%20in%202D%20visual%20generation%20have%20been%20remarkably%20successful.%0AHowever%2C%203D%20and%204D%20generation%20remain%20challenging%20in%20real-world%20applications%20due%0Ato%20the%20lack%20of%20large-scale%204D%20data%20and%20effective%20model%20design.%20In%20this%20paper%2C%0Awe%20propose%20to%20jointly%20investigate%20general%203D%20and%204D%20generation%20by%20leveraging%0Acamera%20and%20object%20movements%20commonly%20observed%20in%20daily%20life.%20Due%20to%20the%20lack%20of%0Areal-world%204D%20data%20in%20the%20community%2C%20we%20first%20propose%20a%20data%20curation%20pipeline%0Ato%20obtain%20camera%20poses%20and%20object%20motion%20strength%20from%20videos.%20Based%20on%20this%0Apipeline%2C%20we%20introduce%20a%20large-scale%20real-world%204D%20scene%20dataset%3A%20CamVid-30K.%0ABy%20leveraging%20all%20the%203D%20and%204D%20data%2C%20we%20develop%20our%20framework%2C%20GenXD%2C%20which%0Aallows%20us%20to%20produce%20any%203D%20or%204D%20scene.%20We%20propose%20multiview-temporal%20modules%2C%0Awhich%20disentangle%20camera%20and%20object%20movements%2C%20to%20seamlessly%20learn%20from%20both%203D%0Aand%204D%20data.%20Additionally%2C%20GenXD%20employs%20masked%20latent%20conditions%20to%20support%20a%0Avariety%20of%20conditioning%20views.%20GenXD%20can%20generate%20videos%20that%20follow%20the%20camera%0Atrajectory%20as%20well%20as%20consistent%203D%20views%20that%20can%20be%20lifted%20into%203D%0Arepresentations.%20We%20perform%20extensive%20evaluations%20across%20various%20real-world%20and%0Asynthetic%20datasets%2C%20demonstrating%20GenXD%27s%20effectiveness%20and%20versatility%0Acompared%20to%20previous%20methods%20in%203D%20and%204D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02319v1&entry.124074799=Read"},
{"title": "FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage\n  Training", "author": "Ruihong Yin and Vladimir Yugay and Yue Li and Sezer Karaoglu and Theo Gevers", "abstract": "  The field of novel view synthesis from images has seen rapid advancements\nwith the introduction of Neural Radiance Fields (NeRF) and more recently with\n3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its\nefficiency and ability to render novel views accurately. While Gaussian\nSplatting performs well when a sufficient amount of training images are\navailable, its unstructured explicit representation tends to overfit in\nscenarios with sparse input images, resulting in poor rendering performance. To\naddress this, we present a 3D Gaussian-based novel view synthesis method using\nsparse input images that can accurately render the scene from the viewpoints\nnot covered by the training images. We propose a multi-stage training scheme\nwith matching-based consistency constraints imposed on the novel views without\nrelying on pre-trained depth estimation or diffusion models. This is achieved\nby using the matches of the available training images to supervise the\ngeneration of the novel views sampled between the training frames with color,\ngeometry, and semantic losses. In addition, we introduce a locality preserving\nregularization for 3D Gaussians which removes rendering artifacts by preserving\nthe local color structure of the scene. Evaluation on synthetic and real-world\ndatasets demonstrates competitive or superior performance of our method in\nfew-shot novel view synthesis compared to existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.02229v1", "date": "2024-11-04", "relevancy": 3.3034, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6878}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6557}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FewViewGS%3A%20Gaussian%20Splatting%20with%20Few%20View%20Matching%20and%20Multi-stage%0A%20%20Training&body=Title%3A%20FewViewGS%3A%20Gaussian%20Splatting%20with%20Few%20View%20Matching%20and%20Multi-stage%0A%20%20Training%0AAuthor%3A%20Ruihong%20Yin%20and%20Vladimir%20Yugay%20and%20Yue%20Li%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%20%20%20The%20field%20of%20novel%20view%20synthesis%20from%20images%20has%20seen%20rapid%20advancements%0Awith%20the%20introduction%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%20more%20recently%20with%0A3D%20Gaussian%20Splatting.%20Gaussian%20Splatting%20became%20widely%20adopted%20due%20to%20its%0Aefficiency%20and%20ability%20to%20render%20novel%20views%20accurately.%20While%20Gaussian%0ASplatting%20performs%20well%20when%20a%20sufficient%20amount%20of%20training%20images%20are%0Aavailable%2C%20its%20unstructured%20explicit%20representation%20tends%20to%20overfit%20in%0Ascenarios%20with%20sparse%20input%20images%2C%20resulting%20in%20poor%20rendering%20performance.%20To%0Aaddress%20this%2C%20we%20present%20a%203D%20Gaussian-based%20novel%20view%20synthesis%20method%20using%0Asparse%20input%20images%20that%20can%20accurately%20render%20the%20scene%20from%20the%20viewpoints%0Anot%20covered%20by%20the%20training%20images.%20We%20propose%20a%20multi-stage%20training%20scheme%0Awith%20matching-based%20consistency%20constraints%20imposed%20on%20the%20novel%20views%20without%0Arelying%20on%20pre-trained%20depth%20estimation%20or%20diffusion%20models.%20This%20is%20achieved%0Aby%20using%20the%20matches%20of%20the%20available%20training%20images%20to%20supervise%20the%0Ageneration%20of%20the%20novel%20views%20sampled%20between%20the%20training%20frames%20with%20color%2C%0Ageometry%2C%20and%20semantic%20losses.%20In%20addition%2C%20we%20introduce%20a%20locality%20preserving%0Aregularization%20for%203D%20Gaussians%20which%20removes%20rendering%20artifacts%20by%20preserving%0Athe%20local%20color%20structure%20of%20the%20scene.%20Evaluation%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrates%20competitive%20or%20superior%20performance%20of%20our%20method%20in%0Afew-shot%20novel%20view%20synthesis%20compared%20to%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFewViewGS%253A%2520Gaussian%2520Splatting%2520with%2520Few%2520View%2520Matching%2520and%2520Multi-stage%250A%2520%2520Training%26entry.906535625%3DRuihong%2520Yin%2520and%2520Vladimir%2520Yugay%2520and%2520Yue%2520Li%2520and%2520Sezer%2520Karaoglu%2520and%2520Theo%2520Gevers%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520novel%2520view%2520synthesis%2520from%2520images%2520has%2520seen%2520rapid%2520advancements%250Awith%2520the%2520introduction%2520of%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%2520more%2520recently%2520with%250A3D%2520Gaussian%2520Splatting.%2520Gaussian%2520Splatting%2520became%2520widely%2520adopted%2520due%2520to%2520its%250Aefficiency%2520and%2520ability%2520to%2520render%2520novel%2520views%2520accurately.%2520While%2520Gaussian%250ASplatting%2520performs%2520well%2520when%2520a%2520sufficient%2520amount%2520of%2520training%2520images%2520are%250Aavailable%252C%2520its%2520unstructured%2520explicit%2520representation%2520tends%2520to%2520overfit%2520in%250Ascenarios%2520with%2520sparse%2520input%2520images%252C%2520resulting%2520in%2520poor%2520rendering%2520performance.%2520To%250Aaddress%2520this%252C%2520we%2520present%2520a%25203D%2520Gaussian-based%2520novel%2520view%2520synthesis%2520method%2520using%250Asparse%2520input%2520images%2520that%2520can%2520accurately%2520render%2520the%2520scene%2520from%2520the%2520viewpoints%250Anot%2520covered%2520by%2520the%2520training%2520images.%2520We%2520propose%2520a%2520multi-stage%2520training%2520scheme%250Awith%2520matching-based%2520consistency%2520constraints%2520imposed%2520on%2520the%2520novel%2520views%2520without%250Arelying%2520on%2520pre-trained%2520depth%2520estimation%2520or%2520diffusion%2520models.%2520This%2520is%2520achieved%250Aby%2520using%2520the%2520matches%2520of%2520the%2520available%2520training%2520images%2520to%2520supervise%2520the%250Ageneration%2520of%2520the%2520novel%2520views%2520sampled%2520between%2520the%2520training%2520frames%2520with%2520color%252C%250Ageometry%252C%2520and%2520semantic%2520losses.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520locality%2520preserving%250Aregularization%2520for%25203D%2520Gaussians%2520which%2520removes%2520rendering%2520artifacts%2520by%2520preserving%250Athe%2520local%2520color%2520structure%2520of%2520the%2520scene.%2520Evaluation%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrates%2520competitive%2520or%2520superior%2520performance%2520of%2520our%2520method%2520in%250Afew-shot%2520novel%2520view%2520synthesis%2520compared%2520to%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FewViewGS%3A%20Gaussian%20Splatting%20with%20Few%20View%20Matching%20and%20Multi-stage%0A%20%20Training&entry.906535625=Ruihong%20Yin%20and%20Vladimir%20Yugay%20and%20Yue%20Li%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=%20%20The%20field%20of%20novel%20view%20synthesis%20from%20images%20has%20seen%20rapid%20advancements%0Awith%20the%20introduction%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%20more%20recently%20with%0A3D%20Gaussian%20Splatting.%20Gaussian%20Splatting%20became%20widely%20adopted%20due%20to%20its%0Aefficiency%20and%20ability%20to%20render%20novel%20views%20accurately.%20While%20Gaussian%0ASplatting%20performs%20well%20when%20a%20sufficient%20amount%20of%20training%20images%20are%0Aavailable%2C%20its%20unstructured%20explicit%20representation%20tends%20to%20overfit%20in%0Ascenarios%20with%20sparse%20input%20images%2C%20resulting%20in%20poor%20rendering%20performance.%20To%0Aaddress%20this%2C%20we%20present%20a%203D%20Gaussian-based%20novel%20view%20synthesis%20method%20using%0Asparse%20input%20images%20that%20can%20accurately%20render%20the%20scene%20from%20the%20viewpoints%0Anot%20covered%20by%20the%20training%20images.%20We%20propose%20a%20multi-stage%20training%20scheme%0Awith%20matching-based%20consistency%20constraints%20imposed%20on%20the%20novel%20views%20without%0Arelying%20on%20pre-trained%20depth%20estimation%20or%20diffusion%20models.%20This%20is%20achieved%0Aby%20using%20the%20matches%20of%20the%20available%20training%20images%20to%20supervise%20the%0Ageneration%20of%20the%20novel%20views%20sampled%20between%20the%20training%20frames%20with%20color%2C%0Ageometry%2C%20and%20semantic%20losses.%20In%20addition%2C%20we%20introduce%20a%20locality%20preserving%0Aregularization%20for%203D%20Gaussians%20which%20removes%20rendering%20artifacts%20by%20preserving%0Athe%20local%20color%20structure%20of%20the%20scene.%20Evaluation%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrates%20competitive%20or%20superior%20performance%20of%20our%20method%20in%0Afew-shot%20novel%20view%20synthesis%20compared%20to%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02229v1&entry.124074799=Read"},
{"title": "MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images", "author": "Eunji Hong and Minh Hieu Nguyen and Mikaela Angelina Uy and Minhyuk Sung", "abstract": "  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view\nimages, not merely as a field or raw geometry but as a sketch-extrude CAD\nmodel. Extracting extrusion cylinders from raw 3D geometry has been extensively\nresearched in computer vision, while the processing of 3D data through neural\nnetworks has remained a bottleneck. Since 3D scans are generally accompanied by\nmulti-view images, leveraging 2D convolutional neural networks allows these\nimages to be exploited as a rich source for extracting extrusion cylinder\ninformation. However, we observe that extracting only the surface information\nof the extrudes and utilizing it results in suboptimal outcomes due to the\nchallenges in the occlusion and surface segmentation. By synergizing with the\nextracted base curve information, we achieve the optimal reconstruction result\nwith the best accuracy in 2D sketch and extrude parameter estimation. Our\nexperiments, comparing our method with previous work that takes a raw 3D point\ncloud as input, demonstrate the effectiveness of our approach by taking\nadvantage of multi-view images.\n", "link": "http://arxiv.org/abs/2406.10853v2", "date": "2024-11-04", "relevancy": 3.0906, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6466}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images&body=Title%3A%20MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images%0AAuthor%3A%20Eunji%20Hong%20and%20Minh%20Hieu%20Nguyen%20and%20Mikaela%20Angelina%20Uy%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20present%20MV2Cyl%2C%20a%20novel%20method%20for%20reconstructing%203D%20from%202D%20multi-view%0Aimages%2C%20not%20merely%20as%20a%20field%20or%20raw%20geometry%20but%20as%20a%20sketch-extrude%20CAD%0Amodel.%20Extracting%20extrusion%20cylinders%20from%20raw%203D%20geometry%20has%20been%20extensively%0Aresearched%20in%20computer%20vision%2C%20while%20the%20processing%20of%203D%20data%20through%20neural%0Anetworks%20has%20remained%20a%20bottleneck.%20Since%203D%20scans%20are%20generally%20accompanied%20by%0Amulti-view%20images%2C%20leveraging%202D%20convolutional%20neural%20networks%20allows%20these%0Aimages%20to%20be%20exploited%20as%20a%20rich%20source%20for%20extracting%20extrusion%20cylinder%0Ainformation.%20However%2C%20we%20observe%20that%20extracting%20only%20the%20surface%20information%0Aof%20the%20extrudes%20and%20utilizing%20it%20results%20in%20suboptimal%20outcomes%20due%20to%20the%0Achallenges%20in%20the%20occlusion%20and%20surface%20segmentation.%20By%20synergizing%20with%20the%0Aextracted%20base%20curve%20information%2C%20we%20achieve%20the%20optimal%20reconstruction%20result%0Awith%20the%20best%20accuracy%20in%202D%20sketch%20and%20extrude%20parameter%20estimation.%20Our%0Aexperiments%2C%20comparing%20our%20method%20with%20previous%20work%20that%20takes%20a%20raw%203D%20point%0Acloud%20as%20input%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20taking%0Aadvantage%20of%20multi-view%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV2Cyl%253A%2520Reconstructing%25203D%2520Extrusion%2520Cylinders%2520from%2520Multi-View%2520Images%26entry.906535625%3DEunji%2520Hong%2520and%2520Minh%2520Hieu%2520Nguyen%2520and%2520Mikaela%2520Angelina%2520Uy%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3D%2520%2520We%2520present%2520MV2Cyl%252C%2520a%2520novel%2520method%2520for%2520reconstructing%25203D%2520from%25202D%2520multi-view%250Aimages%252C%2520not%2520merely%2520as%2520a%2520field%2520or%2520raw%2520geometry%2520but%2520as%2520a%2520sketch-extrude%2520CAD%250Amodel.%2520Extracting%2520extrusion%2520cylinders%2520from%2520raw%25203D%2520geometry%2520has%2520been%2520extensively%250Aresearched%2520in%2520computer%2520vision%252C%2520while%2520the%2520processing%2520of%25203D%2520data%2520through%2520neural%250Anetworks%2520has%2520remained%2520a%2520bottleneck.%2520Since%25203D%2520scans%2520are%2520generally%2520accompanied%2520by%250Amulti-view%2520images%252C%2520leveraging%25202D%2520convolutional%2520neural%2520networks%2520allows%2520these%250Aimages%2520to%2520be%2520exploited%2520as%2520a%2520rich%2520source%2520for%2520extracting%2520extrusion%2520cylinder%250Ainformation.%2520However%252C%2520we%2520observe%2520that%2520extracting%2520only%2520the%2520surface%2520information%250Aof%2520the%2520extrudes%2520and%2520utilizing%2520it%2520results%2520in%2520suboptimal%2520outcomes%2520due%2520to%2520the%250Achallenges%2520in%2520the%2520occlusion%2520and%2520surface%2520segmentation.%2520By%2520synergizing%2520with%2520the%250Aextracted%2520base%2520curve%2520information%252C%2520we%2520achieve%2520the%2520optimal%2520reconstruction%2520result%250Awith%2520the%2520best%2520accuracy%2520in%25202D%2520sketch%2520and%2520extrude%2520parameter%2520estimation.%2520Our%250Aexperiments%252C%2520comparing%2520our%2520method%2520with%2520previous%2520work%2520that%2520takes%2520a%2520raw%25203D%2520point%250Acloud%2520as%2520input%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520by%2520taking%250Aadvantage%2520of%2520multi-view%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images&entry.906535625=Eunji%20Hong%20and%20Minh%20Hieu%20Nguyen%20and%20Mikaela%20Angelina%20Uy%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20present%20MV2Cyl%2C%20a%20novel%20method%20for%20reconstructing%203D%20from%202D%20multi-view%0Aimages%2C%20not%20merely%20as%20a%20field%20or%20raw%20geometry%20but%20as%20a%20sketch-extrude%20CAD%0Amodel.%20Extracting%20extrusion%20cylinders%20from%20raw%203D%20geometry%20has%20been%20extensively%0Aresearched%20in%20computer%20vision%2C%20while%20the%20processing%20of%203D%20data%20through%20neural%0Anetworks%20has%20remained%20a%20bottleneck.%20Since%203D%20scans%20are%20generally%20accompanied%20by%0Amulti-view%20images%2C%20leveraging%202D%20convolutional%20neural%20networks%20allows%20these%0Aimages%20to%20be%20exploited%20as%20a%20rich%20source%20for%20extracting%20extrusion%20cylinder%0Ainformation.%20However%2C%20we%20observe%20that%20extracting%20only%20the%20surface%20information%0Aof%20the%20extrudes%20and%20utilizing%20it%20results%20in%20suboptimal%20outcomes%20due%20to%20the%0Achallenges%20in%20the%20occlusion%20and%20surface%20segmentation.%20By%20synergizing%20with%20the%0Aextracted%20base%20curve%20information%2C%20we%20achieve%20the%20optimal%20reconstruction%20result%0Awith%20the%20best%20accuracy%20in%202D%20sketch%20and%20extrude%20parameter%20estimation.%20Our%0Aexperiments%2C%20comparing%20our%20method%20with%20previous%20work%20that%20takes%20a%20raw%203D%20point%0Acloud%20as%20input%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20taking%0Aadvantage%20of%20multi-view%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10853v2&entry.124074799=Read"},
{"title": "3D Audio-Visual Segmentation", "author": "Artem Sokolov and Swapnil Bhosale and Xiatian Zhu", "abstract": "  Recognizing the sounding objects in scenes is a longstanding objective in\nembodied AI, with diverse applications in robotics and AR/VR/MR. To that end,\nAudio-Visual Segmentation (AVS), taking as condition an audio signal to\nidentify the masks of the target sounding objects in an input image with\nsynchronous camera and microphone sensors, has been recently advanced. However,\nthis paradigm is still insufficient for real-world operation, as the mapping\nfrom 2D images to 3D scenes is missing. To address this fundamental limitation,\nwe introduce a novel research problem, 3D Audio-Visual Segmentation, extending\nthe existing AVS to the 3D output space. This problem poses more challenges due\nto variations in camera extrinsics, audio scattering, occlusions, and diverse\nacoustics across sounding object categories. To facilitate this research, we\ncreate the very first simulation based benchmark, 3DAVS-S34-O7, providing\nphotorealistic 3D scene environments with grounded spatial audio under\nsingle-instance and multi-instance settings, across 34 scenes and 7 object\ncategories. This is made possible by re-purposing the Habitat simulator to\ngenerate comprehensive annotations of sounding object locations and\ncorresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,\ncharacterized by integrating the ready-to-use knowledge from pretrained 2D\naudio-visual foundation models synergistically with 3D visual scene\nrepresentation through spatial audio-aware mask alignment and refinement.\nExtensive experiments demonstrate that EchoSegnet can effectively segment\nsounding objects in 3D space on our new benchmark, representing a significant\nadvancement in the field of embodied AI. Project page:\nhttps://surrey-uplab.github.io/research/3d-audio-visual-segmentation/\n", "link": "http://arxiv.org/abs/2411.02236v1", "date": "2024-11-04", "relevancy": 3.0372, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6196}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Audio-Visual%20Segmentation&body=Title%3A%203D%20Audio-Visual%20Segmentation%0AAuthor%3A%20Artem%20Sokolov%20and%20Swapnil%20Bhosale%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Recognizing%20the%20sounding%20objects%20in%20scenes%20is%20a%20longstanding%20objective%20in%0Aembodied%20AI%2C%20with%20diverse%20applications%20in%20robotics%20and%20AR/VR/MR.%20To%20that%20end%2C%0AAudio-Visual%20Segmentation%20%28AVS%29%2C%20taking%20as%20condition%20an%20audio%20signal%20to%0Aidentify%20the%20masks%20of%20the%20target%20sounding%20objects%20in%20an%20input%20image%20with%0Asynchronous%20camera%20and%20microphone%20sensors%2C%20has%20been%20recently%20advanced.%20However%2C%0Athis%20paradigm%20is%20still%20insufficient%20for%20real-world%20operation%2C%20as%20the%20mapping%0Afrom%202D%20images%20to%203D%20scenes%20is%20missing.%20To%20address%20this%20fundamental%20limitation%2C%0Awe%20introduce%20a%20novel%20research%20problem%2C%203D%20Audio-Visual%20Segmentation%2C%20extending%0Athe%20existing%20AVS%20to%20the%203D%20output%20space.%20This%20problem%20poses%20more%20challenges%20due%0Ato%20variations%20in%20camera%20extrinsics%2C%20audio%20scattering%2C%20occlusions%2C%20and%20diverse%0Aacoustics%20across%20sounding%20object%20categories.%20To%20facilitate%20this%20research%2C%20we%0Acreate%20the%20very%20first%20simulation%20based%20benchmark%2C%203DAVS-S34-O7%2C%20providing%0Aphotorealistic%203D%20scene%20environments%20with%20grounded%20spatial%20audio%20under%0Asingle-instance%20and%20multi-instance%20settings%2C%20across%2034%20scenes%20and%207%20object%0Acategories.%20This%20is%20made%20possible%20by%20re-purposing%20the%20Habitat%20simulator%20to%0Agenerate%20comprehensive%20annotations%20of%20sounding%20object%20locations%20and%0Acorresponding%203D%20masks.%20Subsequently%2C%20we%20propose%20a%20new%20approach%2C%20EchoSegnet%2C%0Acharacterized%20by%20integrating%20the%20ready-to-use%20knowledge%20from%20pretrained%202D%0Aaudio-visual%20foundation%20models%20synergistically%20with%203D%20visual%20scene%0Arepresentation%20through%20spatial%20audio-aware%20mask%20alignment%20and%20refinement.%0AExtensive%20experiments%20demonstrate%20that%20EchoSegnet%20can%20effectively%20segment%0Asounding%20objects%20in%203D%20space%20on%20our%20new%20benchmark%2C%20representing%20a%20significant%0Aadvancement%20in%20the%20field%20of%20embodied%20AI.%20Project%20page%3A%0Ahttps%3A//surrey-uplab.github.io/research/3d-audio-visual-segmentation/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Audio-Visual%2520Segmentation%26entry.906535625%3DArtem%2520Sokolov%2520and%2520Swapnil%2520Bhosale%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Recognizing%2520the%2520sounding%2520objects%2520in%2520scenes%2520is%2520a%2520longstanding%2520objective%2520in%250Aembodied%2520AI%252C%2520with%2520diverse%2520applications%2520in%2520robotics%2520and%2520AR/VR/MR.%2520To%2520that%2520end%252C%250AAudio-Visual%2520Segmentation%2520%2528AVS%2529%252C%2520taking%2520as%2520condition%2520an%2520audio%2520signal%2520to%250Aidentify%2520the%2520masks%2520of%2520the%2520target%2520sounding%2520objects%2520in%2520an%2520input%2520image%2520with%250Asynchronous%2520camera%2520and%2520microphone%2520sensors%252C%2520has%2520been%2520recently%2520advanced.%2520However%252C%250Athis%2520paradigm%2520is%2520still%2520insufficient%2520for%2520real-world%2520operation%252C%2520as%2520the%2520mapping%250Afrom%25202D%2520images%2520to%25203D%2520scenes%2520is%2520missing.%2520To%2520address%2520this%2520fundamental%2520limitation%252C%250Awe%2520introduce%2520a%2520novel%2520research%2520problem%252C%25203D%2520Audio-Visual%2520Segmentation%252C%2520extending%250Athe%2520existing%2520AVS%2520to%2520the%25203D%2520output%2520space.%2520This%2520problem%2520poses%2520more%2520challenges%2520due%250Ato%2520variations%2520in%2520camera%2520extrinsics%252C%2520audio%2520scattering%252C%2520occlusions%252C%2520and%2520diverse%250Aacoustics%2520across%2520sounding%2520object%2520categories.%2520To%2520facilitate%2520this%2520research%252C%2520we%250Acreate%2520the%2520very%2520first%2520simulation%2520based%2520benchmark%252C%25203DAVS-S34-O7%252C%2520providing%250Aphotorealistic%25203D%2520scene%2520environments%2520with%2520grounded%2520spatial%2520audio%2520under%250Asingle-instance%2520and%2520multi-instance%2520settings%252C%2520across%252034%2520scenes%2520and%25207%2520object%250Acategories.%2520This%2520is%2520made%2520possible%2520by%2520re-purposing%2520the%2520Habitat%2520simulator%2520to%250Agenerate%2520comprehensive%2520annotations%2520of%2520sounding%2520object%2520locations%2520and%250Acorresponding%25203D%2520masks.%2520Subsequently%252C%2520we%2520propose%2520a%2520new%2520approach%252C%2520EchoSegnet%252C%250Acharacterized%2520by%2520integrating%2520the%2520ready-to-use%2520knowledge%2520from%2520pretrained%25202D%250Aaudio-visual%2520foundation%2520models%2520synergistically%2520with%25203D%2520visual%2520scene%250Arepresentation%2520through%2520spatial%2520audio-aware%2520mask%2520alignment%2520and%2520refinement.%250AExtensive%2520experiments%2520demonstrate%2520that%2520EchoSegnet%2520can%2520effectively%2520segment%250Asounding%2520objects%2520in%25203D%2520space%2520on%2520our%2520new%2520benchmark%252C%2520representing%2520a%2520significant%250Aadvancement%2520in%2520the%2520field%2520of%2520embodied%2520AI.%2520Project%2520page%253A%250Ahttps%253A//surrey-uplab.github.io/research/3d-audio-visual-segmentation/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Audio-Visual%20Segmentation&entry.906535625=Artem%20Sokolov%20and%20Swapnil%20Bhosale%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Recognizing%20the%20sounding%20objects%20in%20scenes%20is%20a%20longstanding%20objective%20in%0Aembodied%20AI%2C%20with%20diverse%20applications%20in%20robotics%20and%20AR/VR/MR.%20To%20that%20end%2C%0AAudio-Visual%20Segmentation%20%28AVS%29%2C%20taking%20as%20condition%20an%20audio%20signal%20to%0Aidentify%20the%20masks%20of%20the%20target%20sounding%20objects%20in%20an%20input%20image%20with%0Asynchronous%20camera%20and%20microphone%20sensors%2C%20has%20been%20recently%20advanced.%20However%2C%0Athis%20paradigm%20is%20still%20insufficient%20for%20real-world%20operation%2C%20as%20the%20mapping%0Afrom%202D%20images%20to%203D%20scenes%20is%20missing.%20To%20address%20this%20fundamental%20limitation%2C%0Awe%20introduce%20a%20novel%20research%20problem%2C%203D%20Audio-Visual%20Segmentation%2C%20extending%0Athe%20existing%20AVS%20to%20the%203D%20output%20space.%20This%20problem%20poses%20more%20challenges%20due%0Ato%20variations%20in%20camera%20extrinsics%2C%20audio%20scattering%2C%20occlusions%2C%20and%20diverse%0Aacoustics%20across%20sounding%20object%20categories.%20To%20facilitate%20this%20research%2C%20we%0Acreate%20the%20very%20first%20simulation%20based%20benchmark%2C%203DAVS-S34-O7%2C%20providing%0Aphotorealistic%203D%20scene%20environments%20with%20grounded%20spatial%20audio%20under%0Asingle-instance%20and%20multi-instance%20settings%2C%20across%2034%20scenes%20and%207%20object%0Acategories.%20This%20is%20made%20possible%20by%20re-purposing%20the%20Habitat%20simulator%20to%0Agenerate%20comprehensive%20annotations%20of%20sounding%20object%20locations%20and%0Acorresponding%203D%20masks.%20Subsequently%2C%20we%20propose%20a%20new%20approach%2C%20EchoSegnet%2C%0Acharacterized%20by%20integrating%20the%20ready-to-use%20knowledge%20from%20pretrained%202D%0Aaudio-visual%20foundation%20models%20synergistically%20with%203D%20visual%20scene%0Arepresentation%20through%20spatial%20audio-aware%20mask%20alignment%20and%20refinement.%0AExtensive%20experiments%20demonstrate%20that%20EchoSegnet%20can%20effectively%20segment%0Asounding%20objects%20in%203D%20space%20on%20our%20new%20benchmark%2C%20representing%20a%20significant%0Aadvancement%20in%20the%20field%20of%20embodied%20AI.%20Project%20page%3A%0Ahttps%3A//surrey-uplab.github.io/research/3d-audio-visual-segmentation/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02236v1&entry.124074799=Read"},
{"title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D", "author": "Wei Cheng and Juncheng Mu and Xianfang Zeng and Xin Chen and Anqi Pang and Chi Zhang and Zhibin Wang and Bin Fu and Gang Yu and Ziwei Liu and Liang Pan", "abstract": "  Texturing is a crucial step in the 3D asset production workflow, which\nenhances the visual appeal and diversity of 3D assets. Despite recent\nadvancements in Text-to-Texture (T2T) generation, existing methods often yield\nsubpar results, primarily due to local discontinuities, inconsistencies across\nmultiple views, and their heavy dependence on UV unwrapping outcomes. To tackle\nthese challenges, we propose a novel generation-refinement 3D texturing\nframework called MVPaint, which can generate high-resolution, seamless textures\nwhile emphasizing multi-view consistency. MVPaint mainly consists of three key\nmodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,\nMVPaint first simultaneously generates multi-view images by employing an SMG\nmodel, which leads to coarse texturing results with unpainted parts due to\nmissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete\n3D texturing, we introduce the S3I method, specifically designed to effectively\ntexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,\nMVPaint employs a UVR module to improve the texture quality in the UV space,\nwhich first performs a UV-space Super-Resolution, followed by a Spatial-aware\nSeam-Smoothing algorithm for revising spatial texturing discontinuities caused\nby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the\nObjaverse T2T benchmark and the GSO T2T benchmark, based on selected\nhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,\nrespectively. Extensive experimental results demonstrate that MVPaint surpasses\nexisting state-of-the-art methods. Notably, MVPaint could generate\nhigh-fidelity textures with minimal Janus issues and highly enhanced cross-view\nconsistency.\n", "link": "http://arxiv.org/abs/2411.02336v1", "date": "2024-11-04", "relevancy": 3.0303, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6172}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6172}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVPaint%3A%20Synchronized%20Multi-View%20Diffusion%20for%20Painting%20Anything%203D&body=Title%3A%20MVPaint%3A%20Synchronized%20Multi-View%20Diffusion%20for%20Painting%20Anything%203D%0AAuthor%3A%20Wei%20Cheng%20and%20Juncheng%20Mu%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Chi%20Zhang%20and%20Zhibin%20Wang%20and%20Bin%20Fu%20and%20Gang%20Yu%20and%20Ziwei%20Liu%20and%20Liang%20Pan%0AAbstract%3A%20%20%20Texturing%20is%20a%20crucial%20step%20in%20the%203D%20asset%20production%20workflow%2C%20which%0Aenhances%20the%20visual%20appeal%20and%20diversity%20of%203D%20assets.%20Despite%20recent%0Aadvancements%20in%20Text-to-Texture%20%28T2T%29%20generation%2C%20existing%20methods%20often%20yield%0Asubpar%20results%2C%20primarily%20due%20to%20local%20discontinuities%2C%20inconsistencies%20across%0Amultiple%20views%2C%20and%20their%20heavy%20dependence%20on%20UV%20unwrapping%20outcomes.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20a%20novel%20generation-refinement%203D%20texturing%0Aframework%20called%20MVPaint%2C%20which%20can%20generate%20high-resolution%2C%20seamless%20textures%0Awhile%20emphasizing%20multi-view%20consistency.%20MVPaint%20mainly%20consists%20of%20three%20key%0Amodules.%201%29%20Synchronized%20Multi-view%20Generation%20%28SMG%29.%20Given%20a%203D%20mesh%20model%2C%0AMVPaint%20first%20simultaneously%20generates%20multi-view%20images%20by%20employing%20an%20SMG%0Amodel%2C%20which%20leads%20to%20coarse%20texturing%20results%20with%20unpainted%20parts%20due%20to%0Amissing%20observations.%202%29%20Spatial-aware%203D%20Inpainting%20%28S3I%29.%20To%20ensure%20complete%0A3D%20texturing%2C%20we%20introduce%20the%20S3I%20method%2C%20specifically%20designed%20to%20effectively%0Atexture%20previously%20unobserved%20areas.%203%29%20UV%20Refinement%20%28UVR%29.%20Furthermore%2C%0AMVPaint%20employs%20a%20UVR%20module%20to%20improve%20the%20texture%20quality%20in%20the%20UV%20space%2C%0Awhich%20first%20performs%20a%20UV-space%20Super-Resolution%2C%20followed%20by%20a%20Spatial-aware%0ASeam-Smoothing%20algorithm%20for%20revising%20spatial%20texturing%20discontinuities%20caused%0Aby%20UV%20unwrapping.%20Moreover%2C%20we%20establish%20two%20T2T%20evaluation%20benchmarks%3A%20the%0AObjaverse%20T2T%20benchmark%20and%20the%20GSO%20T2T%20benchmark%2C%20based%20on%20selected%0Ahigh-quality%203D%20meshes%20from%20the%20Objaverse%20dataset%20and%20the%20entire%20GSO%20dataset%2C%0Arespectively.%20Extensive%20experimental%20results%20demonstrate%20that%20MVPaint%20surpasses%0Aexisting%20state-of-the-art%20methods.%20Notably%2C%20MVPaint%20could%20generate%0Ahigh-fidelity%20textures%20with%20minimal%20Janus%20issues%20and%20highly%20enhanced%20cross-view%0Aconsistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVPaint%253A%2520Synchronized%2520Multi-View%2520Diffusion%2520for%2520Painting%2520Anything%25203D%26entry.906535625%3DWei%2520Cheng%2520and%2520Juncheng%2520Mu%2520and%2520Xianfang%2520Zeng%2520and%2520Xin%2520Chen%2520and%2520Anqi%2520Pang%2520and%2520Chi%2520Zhang%2520and%2520Zhibin%2520Wang%2520and%2520Bin%2520Fu%2520and%2520Gang%2520Yu%2520and%2520Ziwei%2520Liu%2520and%2520Liang%2520Pan%26entry.1292438233%3D%2520%2520Texturing%2520is%2520a%2520crucial%2520step%2520in%2520the%25203D%2520asset%2520production%2520workflow%252C%2520which%250Aenhances%2520the%2520visual%2520appeal%2520and%2520diversity%2520of%25203D%2520assets.%2520Despite%2520recent%250Aadvancements%2520in%2520Text-to-Texture%2520%2528T2T%2529%2520generation%252C%2520existing%2520methods%2520often%2520yield%250Asubpar%2520results%252C%2520primarily%2520due%2520to%2520local%2520discontinuities%252C%2520inconsistencies%2520across%250Amultiple%2520views%252C%2520and%2520their%2520heavy%2520dependence%2520on%2520UV%2520unwrapping%2520outcomes.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520generation-refinement%25203D%2520texturing%250Aframework%2520called%2520MVPaint%252C%2520which%2520can%2520generate%2520high-resolution%252C%2520seamless%2520textures%250Awhile%2520emphasizing%2520multi-view%2520consistency.%2520MVPaint%2520mainly%2520consists%2520of%2520three%2520key%250Amodules.%25201%2529%2520Synchronized%2520Multi-view%2520Generation%2520%2528SMG%2529.%2520Given%2520a%25203D%2520mesh%2520model%252C%250AMVPaint%2520first%2520simultaneously%2520generates%2520multi-view%2520images%2520by%2520employing%2520an%2520SMG%250Amodel%252C%2520which%2520leads%2520to%2520coarse%2520texturing%2520results%2520with%2520unpainted%2520parts%2520due%2520to%250Amissing%2520observations.%25202%2529%2520Spatial-aware%25203D%2520Inpainting%2520%2528S3I%2529.%2520To%2520ensure%2520complete%250A3D%2520texturing%252C%2520we%2520introduce%2520the%2520S3I%2520method%252C%2520specifically%2520designed%2520to%2520effectively%250Atexture%2520previously%2520unobserved%2520areas.%25203%2529%2520UV%2520Refinement%2520%2528UVR%2529.%2520Furthermore%252C%250AMVPaint%2520employs%2520a%2520UVR%2520module%2520to%2520improve%2520the%2520texture%2520quality%2520in%2520the%2520UV%2520space%252C%250Awhich%2520first%2520performs%2520a%2520UV-space%2520Super-Resolution%252C%2520followed%2520by%2520a%2520Spatial-aware%250ASeam-Smoothing%2520algorithm%2520for%2520revising%2520spatial%2520texturing%2520discontinuities%2520caused%250Aby%2520UV%2520unwrapping.%2520Moreover%252C%2520we%2520establish%2520two%2520T2T%2520evaluation%2520benchmarks%253A%2520the%250AObjaverse%2520T2T%2520benchmark%2520and%2520the%2520GSO%2520T2T%2520benchmark%252C%2520based%2520on%2520selected%250Ahigh-quality%25203D%2520meshes%2520from%2520the%2520Objaverse%2520dataset%2520and%2520the%2520entire%2520GSO%2520dataset%252C%250Arespectively.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520MVPaint%2520surpasses%250Aexisting%2520state-of-the-art%2520methods.%2520Notably%252C%2520MVPaint%2520could%2520generate%250Ahigh-fidelity%2520textures%2520with%2520minimal%2520Janus%2520issues%2520and%2520highly%2520enhanced%2520cross-view%250Aconsistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVPaint%3A%20Synchronized%20Multi-View%20Diffusion%20for%20Painting%20Anything%203D&entry.906535625=Wei%20Cheng%20and%20Juncheng%20Mu%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Chi%20Zhang%20and%20Zhibin%20Wang%20and%20Bin%20Fu%20and%20Gang%20Yu%20and%20Ziwei%20Liu%20and%20Liang%20Pan&entry.1292438233=%20%20Texturing%20is%20a%20crucial%20step%20in%20the%203D%20asset%20production%20workflow%2C%20which%0Aenhances%20the%20visual%20appeal%20and%20diversity%20of%203D%20assets.%20Despite%20recent%0Aadvancements%20in%20Text-to-Texture%20%28T2T%29%20generation%2C%20existing%20methods%20often%20yield%0Asubpar%20results%2C%20primarily%20due%20to%20local%20discontinuities%2C%20inconsistencies%20across%0Amultiple%20views%2C%20and%20their%20heavy%20dependence%20on%20UV%20unwrapping%20outcomes.%20To%20tackle%0Athese%20challenges%2C%20we%20propose%20a%20novel%20generation-refinement%203D%20texturing%0Aframework%20called%20MVPaint%2C%20which%20can%20generate%20high-resolution%2C%20seamless%20textures%0Awhile%20emphasizing%20multi-view%20consistency.%20MVPaint%20mainly%20consists%20of%20three%20key%0Amodules.%201%29%20Synchronized%20Multi-view%20Generation%20%28SMG%29.%20Given%20a%203D%20mesh%20model%2C%0AMVPaint%20first%20simultaneously%20generates%20multi-view%20images%20by%20employing%20an%20SMG%0Amodel%2C%20which%20leads%20to%20coarse%20texturing%20results%20with%20unpainted%20parts%20due%20to%0Amissing%20observations.%202%29%20Spatial-aware%203D%20Inpainting%20%28S3I%29.%20To%20ensure%20complete%0A3D%20texturing%2C%20we%20introduce%20the%20S3I%20method%2C%20specifically%20designed%20to%20effectively%0Atexture%20previously%20unobserved%20areas.%203%29%20UV%20Refinement%20%28UVR%29.%20Furthermore%2C%0AMVPaint%20employs%20a%20UVR%20module%20to%20improve%20the%20texture%20quality%20in%20the%20UV%20space%2C%0Awhich%20first%20performs%20a%20UV-space%20Super-Resolution%2C%20followed%20by%20a%20Spatial-aware%0ASeam-Smoothing%20algorithm%20for%20revising%20spatial%20texturing%20discontinuities%20caused%0Aby%20UV%20unwrapping.%20Moreover%2C%20we%20establish%20two%20T2T%20evaluation%20benchmarks%3A%20the%0AObjaverse%20T2T%20benchmark%20and%20the%20GSO%20T2T%20benchmark%2C%20based%20on%20selected%0Ahigh-quality%203D%20meshes%20from%20the%20Objaverse%20dataset%20and%20the%20entire%20GSO%20dataset%2C%0Arespectively.%20Extensive%20experimental%20results%20demonstrate%20that%20MVPaint%20surpasses%0Aexisting%20state-of-the-art%20methods.%20Notably%2C%20MVPaint%20could%20generate%0Ahigh-fidelity%20textures%20with%20minimal%20Janus%20issues%20and%20highly%20enhanced%20cross-view%0Aconsistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02336v1&entry.124074799=Read"},
{"title": "RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps", "author": "Abhijeet Nayak and Daniele Cattaneo and Abhinav Valada", "abstract": "  Localization is paramount for autonomous robots. While camera and LiDAR-based\napproaches have been extensively investigated, they are affected by adverse\nillumination and weather conditions. Therefore, radar sensors have recently\ngained attention due to their intrinsic robustness to such conditions. In this\npaper, we propose RaLF, a novel deep neural network-based approach for\nlocalizing radar scans in a LiDAR map of the environment, by jointly learning\nto address both place recognition and metric localization. RaLF is composed of\nradar and LiDAR feature encoders, a place recognition head that generates\nglobal descriptors, and a metric localization head that predicts the 3-DoF\ntransformation between the radar scan and the map. We tackle the place\nrecognition task by learning a shared embedding space between the two\nmodalities via cross-modal metric learning. Additionally, we perform metric\nlocalization by predicting pixel-level flow vectors that align the query radar\nscan with the LiDAR map. We extensively evaluate our approach on multiple\nreal-world driving datasets and show that RaLF achieves state-of-the-art\nperformance for both place recognition and metric localization. Moreover, we\ndemonstrate that our approach can effectively generalize to different cities\nand sensor setups than the ones used during training. We make the code and\ntrained models publicly available at http://ralf.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2309.09875v2", "date": "2024-11-04", "relevancy": 3.0105, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6448}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5816}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaLF%3A%20Flow-based%20Global%20and%20Metric%20Radar%20Localization%20in%20LiDAR%20Maps&body=Title%3A%20RaLF%3A%20Flow-based%20Global%20and%20Metric%20Radar%20Localization%20in%20LiDAR%20Maps%0AAuthor%3A%20Abhijeet%20Nayak%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Localization%20is%20paramount%20for%20autonomous%20robots.%20While%20camera%20and%20LiDAR-based%0Aapproaches%20have%20been%20extensively%20investigated%2C%20they%20are%20affected%20by%20adverse%0Aillumination%20and%20weather%20conditions.%20Therefore%2C%20radar%20sensors%20have%20recently%0Agained%20attention%20due%20to%20their%20intrinsic%20robustness%20to%20such%20conditions.%20In%20this%0Apaper%2C%20we%20propose%20RaLF%2C%20a%20novel%20deep%20neural%20network-based%20approach%20for%0Alocalizing%20radar%20scans%20in%20a%20LiDAR%20map%20of%20the%20environment%2C%20by%20jointly%20learning%0Ato%20address%20both%20place%20recognition%20and%20metric%20localization.%20RaLF%20is%20composed%20of%0Aradar%20and%20LiDAR%20feature%20encoders%2C%20a%20place%20recognition%20head%20that%20generates%0Aglobal%20descriptors%2C%20and%20a%20metric%20localization%20head%20that%20predicts%20the%203-DoF%0Atransformation%20between%20the%20radar%20scan%20and%20the%20map.%20We%20tackle%20the%20place%0Arecognition%20task%20by%20learning%20a%20shared%20embedding%20space%20between%20the%20two%0Amodalities%20via%20cross-modal%20metric%20learning.%20Additionally%2C%20we%20perform%20metric%0Alocalization%20by%20predicting%20pixel-level%20flow%20vectors%20that%20align%20the%20query%20radar%0Ascan%20with%20the%20LiDAR%20map.%20We%20extensively%20evaluate%20our%20approach%20on%20multiple%0Areal-world%20driving%20datasets%20and%20show%20that%20RaLF%20achieves%20state-of-the-art%0Aperformance%20for%20both%20place%20recognition%20and%20metric%20localization.%20Moreover%2C%20we%0Ademonstrate%20that%20our%20approach%20can%20effectively%20generalize%20to%20different%20cities%0Aand%20sensor%20setups%20than%20the%20ones%20used%20during%20training.%20We%20make%20the%20code%20and%0Atrained%20models%20publicly%20available%20at%20http%3A//ralf.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaLF%253A%2520Flow-based%2520Global%2520and%2520Metric%2520Radar%2520Localization%2520in%2520LiDAR%2520Maps%26entry.906535625%3DAbhijeet%2520Nayak%2520and%2520Daniele%2520Cattaneo%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Localization%2520is%2520paramount%2520for%2520autonomous%2520robots.%2520While%2520camera%2520and%2520LiDAR-based%250Aapproaches%2520have%2520been%2520extensively%2520investigated%252C%2520they%2520are%2520affected%2520by%2520adverse%250Aillumination%2520and%2520weather%2520conditions.%2520Therefore%252C%2520radar%2520sensors%2520have%2520recently%250Agained%2520attention%2520due%2520to%2520their%2520intrinsic%2520robustness%2520to%2520such%2520conditions.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520RaLF%252C%2520a%2520novel%2520deep%2520neural%2520network-based%2520approach%2520for%250Alocalizing%2520radar%2520scans%2520in%2520a%2520LiDAR%2520map%2520of%2520the%2520environment%252C%2520by%2520jointly%2520learning%250Ato%2520address%2520both%2520place%2520recognition%2520and%2520metric%2520localization.%2520RaLF%2520is%2520composed%2520of%250Aradar%2520and%2520LiDAR%2520feature%2520encoders%252C%2520a%2520place%2520recognition%2520head%2520that%2520generates%250Aglobal%2520descriptors%252C%2520and%2520a%2520metric%2520localization%2520head%2520that%2520predicts%2520the%25203-DoF%250Atransformation%2520between%2520the%2520radar%2520scan%2520and%2520the%2520map.%2520We%2520tackle%2520the%2520place%250Arecognition%2520task%2520by%2520learning%2520a%2520shared%2520embedding%2520space%2520between%2520the%2520two%250Amodalities%2520via%2520cross-modal%2520metric%2520learning.%2520Additionally%252C%2520we%2520perform%2520metric%250Alocalization%2520by%2520predicting%2520pixel-level%2520flow%2520vectors%2520that%2520align%2520the%2520query%2520radar%250Ascan%2520with%2520the%2520LiDAR%2520map.%2520We%2520extensively%2520evaluate%2520our%2520approach%2520on%2520multiple%250Areal-world%2520driving%2520datasets%2520and%2520show%2520that%2520RaLF%2520achieves%2520state-of-the-art%250Aperformance%2520for%2520both%2520place%2520recognition%2520and%2520metric%2520localization.%2520Moreover%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520can%2520effectively%2520generalize%2520to%2520different%2520cities%250Aand%2520sensor%2520setups%2520than%2520the%2520ones%2520used%2520during%2520training.%2520We%2520make%2520the%2520code%2520and%250Atrained%2520models%2520publicly%2520available%2520at%2520http%253A//ralf.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaLF%3A%20Flow-based%20Global%20and%20Metric%20Radar%20Localization%20in%20LiDAR%20Maps&entry.906535625=Abhijeet%20Nayak%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada&entry.1292438233=%20%20Localization%20is%20paramount%20for%20autonomous%20robots.%20While%20camera%20and%20LiDAR-based%0Aapproaches%20have%20been%20extensively%20investigated%2C%20they%20are%20affected%20by%20adverse%0Aillumination%20and%20weather%20conditions.%20Therefore%2C%20radar%20sensors%20have%20recently%0Agained%20attention%20due%20to%20their%20intrinsic%20robustness%20to%20such%20conditions.%20In%20this%0Apaper%2C%20we%20propose%20RaLF%2C%20a%20novel%20deep%20neural%20network-based%20approach%20for%0Alocalizing%20radar%20scans%20in%20a%20LiDAR%20map%20of%20the%20environment%2C%20by%20jointly%20learning%0Ato%20address%20both%20place%20recognition%20and%20metric%20localization.%20RaLF%20is%20composed%20of%0Aradar%20and%20LiDAR%20feature%20encoders%2C%20a%20place%20recognition%20head%20that%20generates%0Aglobal%20descriptors%2C%20and%20a%20metric%20localization%20head%20that%20predicts%20the%203-DoF%0Atransformation%20between%20the%20radar%20scan%20and%20the%20map.%20We%20tackle%20the%20place%0Arecognition%20task%20by%20learning%20a%20shared%20embedding%20space%20between%20the%20two%0Amodalities%20via%20cross-modal%20metric%20learning.%20Additionally%2C%20we%20perform%20metric%0Alocalization%20by%20predicting%20pixel-level%20flow%20vectors%20that%20align%20the%20query%20radar%0Ascan%20with%20the%20LiDAR%20map.%20We%20extensively%20evaluate%20our%20approach%20on%20multiple%0Areal-world%20driving%20datasets%20and%20show%20that%20RaLF%20achieves%20state-of-the-art%0Aperformance%20for%20both%20place%20recognition%20and%20metric%20localization.%20Moreover%2C%20we%0Ademonstrate%20that%20our%20approach%20can%20effectively%20generalize%20to%20different%20cities%0Aand%20sensor%20setups%20than%20the%20ones%20used%20during%20training.%20We%20make%20the%20code%20and%0Atrained%20models%20publicly%20available%20at%20http%3A//ralf.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09875v2&entry.124074799=Read"},
{"title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning", "author": "Dake Bu and Wei Huang and Andi Han and Atsushi Nitanda and Taiji Suzuki and Qingfu Zhang and Hau-San Wong", "abstract": "  Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.\n", "link": "http://arxiv.org/abs/2411.02199v1", "date": "2024-11-04", "relevancy": 2.9519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&body=Title%3A%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning%0AAuthor%3A%20Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Transformers%2520Harness%2520Multi-Concept%2520Word%2520Semantics%2520for%2520Efficient%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DDake%2520Bu%2520and%2520Wei%2520Huang%2520and%2520Andi%2520Han%2520and%2520Atsushi%2520Nitanda%2520and%2520Taiji%2520Suzuki%2520and%2520Qingfu%2520Zhang%2520and%2520Hau-San%2520Wong%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520displayed%2520remarkable%250Acreative%2520prowess%2520and%2520emergence%2520capabilities.%2520Existing%2520empirical%2520studies%2520have%250Arevealed%2520a%2520strong%2520connection%2520between%2520these%2520LLMs%2527%2520impressive%2520emergence%2520abilities%250Aand%2520their%2520in-context%2520learning%2520%2528ICL%2529%2520capacity%252C%2520allowing%2520them%2520to%2520solve%2520new%2520tasks%250Ausing%2520only%2520task-specific%2520prompts%2520without%2520further%2520fine-tuning.%2520On%2520the%2520other%250Ahand%252C%2520existing%2520empirical%2520and%2520theoretical%2520studies%2520also%2520show%2520that%2520there%2520is%2520a%250Alinear%2520regularity%2520of%2520the%2520multi-concept%2520encoded%2520semantic%2520representation%2520behind%250Atransformer-based%2520LLMs.%2520However%252C%2520existing%2520theoretical%2520work%2520fail%2520to%2520build%2520up%2520an%250Aunderstanding%2520of%2520the%2520connection%2520between%2520this%2520regularity%2520and%2520the%2520innovative%250Apower%2520of%2520ICL.%2520Additionally%252C%2520prior%2520work%2520often%2520focuses%2520on%2520simplified%252C%2520unrealistic%250Ascenarios%2520involving%2520linear%2520transformers%2520or%2520unrealistic%2520loss%2520functions%252C%2520and%2520they%250Aachieve%2520only%2520linear%2520or%2520sub-linear%2520convergence%2520rates.%2520In%2520contrast%252C%2520this%2520work%250Aprovides%2520a%2520fine-grained%2520mathematical%2520analysis%2520to%2520show%2520how%2520transformers%2520leverage%250Athe%2520multi-concept%2520semantics%2520of%2520words%2520to%2520enable%2520powerful%2520ICL%2520and%2520excellent%250Aout-of-distribution%2520ICL%2520abilities%252C%2520offering%2520insights%2520into%2520how%2520transformers%250Ainnovate%2520solutions%2520for%2520certain%2520unseen%2520tasks%2520encoded%2520with%2520multiple%2520cross-concept%250Asemantics.%2520Inspired%2520by%2520empirical%2520studies%2520on%2520the%2520linear%2520latent%2520geometry%2520of%2520LLMs%252C%250Athe%2520analysis%2520is%2520based%2520on%2520a%2520concept-based%2520low-noise%2520sparse%2520coding%2520prompt%2520model.%250ALeveraging%2520advanced%2520techniques%252C%2520this%2520work%2520showcases%2520the%2520exponential%25200-1%2520loss%250Aconvergence%2520over%2520the%2520highly%2520non-convex%2520training%2520dynamics%252C%2520which%2520pioneeringly%250Aincorporates%2520the%2520challenges%2520of%2520softmax%2520self-attention%252C%2520ReLU-activated%2520MLPs%252C%2520and%250Across-entropy%2520loss.%2520Empirical%2520simulations%2520corroborate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&entry.906535625=Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02199v1&entry.124074799=Read"},
{"title": "SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation", "author": "Changpeng Cai and Guinan Guo and Jiao Li and Junhao Su and Fei Shen and Chenghao He and Jing Xiao and Yuanxu Chen and Lei Dai and Feiyu Zhu", "abstract": "  Most earlier researches on talking face generation have focused on the\nsynchronization of lip motion and speech content. However, head pose and facial\nemotions are equally important characteristics of natural faces. While\naudio-driven talking face generation has seen notable advancements, existing\nmethods either overlook facial emotions or are limited to specific individuals\nand cannot be applied to arbitrary subjects. In this paper, we propose a novel\none-shot Talking Head Generation framework (SPEAK) that distinguishes itself\nfrom the general Talking Face Generation by enabling emotional and postural\ncontrol. Specifically, we introduce Inter-Reconstructed Feature Disentanglement\n(IRFD) module to decouple facial features into three latent spaces. Then we\ndesign a face editing module that modifies speech content and facial latent\ncodes into a single latent space. Subsequently, we present a novel generator\nthat employs modified latent codes derived from the editing module to regulate\nemotional expression, head poses, and speech content in synthesizing facial\nanimations. Extensive trials demonstrate that our method ensures lip\nsynchronization with the audio while enabling decoupled control of facial\nfeatures, it can generate realistic talking head with coordinated lip motions,\nauthentic facial emotions, and smooth head movements. The demo video is\navailable: https://anonymous.4open.science/r/SPEAK-8A22\n", "link": "http://arxiv.org/abs/2405.07257v3", "date": "2024-11-04", "relevancy": 2.9513, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5963}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5963}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPEAK%3A%20Speech-Driven%20Pose%20and%20Emotion-Adjustable%20Talking%20Head%20Generation&body=Title%3A%20SPEAK%3A%20Speech-Driven%20Pose%20and%20Emotion-Adjustable%20Talking%20Head%20Generation%0AAuthor%3A%20Changpeng%20Cai%20and%20Guinan%20Guo%20and%20Jiao%20Li%20and%20Junhao%20Su%20and%20Fei%20Shen%20and%20Chenghao%20He%20and%20Jing%20Xiao%20and%20Yuanxu%20Chen%20and%20Lei%20Dai%20and%20Feiyu%20Zhu%0AAbstract%3A%20%20%20Most%20earlier%20researches%20on%20talking%20face%20generation%20have%20focused%20on%20the%0Asynchronization%20of%20lip%20motion%20and%20speech%20content.%20However%2C%20head%20pose%20and%20facial%0Aemotions%20are%20equally%20important%20characteristics%20of%20natural%20faces.%20While%0Aaudio-driven%20talking%20face%20generation%20has%20seen%20notable%20advancements%2C%20existing%0Amethods%20either%20overlook%20facial%20emotions%20or%20are%20limited%20to%20specific%20individuals%0Aand%20cannot%20be%20applied%20to%20arbitrary%20subjects.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aone-shot%20Talking%20Head%20Generation%20framework%20%28SPEAK%29%20that%20distinguishes%20itself%0Afrom%20the%20general%20Talking%20Face%20Generation%20by%20enabling%20emotional%20and%20postural%0Acontrol.%20Specifically%2C%20we%20introduce%20Inter-Reconstructed%20Feature%20Disentanglement%0A%28IRFD%29%20module%20to%20decouple%20facial%20features%20into%20three%20latent%20spaces.%20Then%20we%0Adesign%20a%20face%20editing%20module%20that%20modifies%20speech%20content%20and%20facial%20latent%0Acodes%20into%20a%20single%20latent%20space.%20Subsequently%2C%20we%20present%20a%20novel%20generator%0Athat%20employs%20modified%20latent%20codes%20derived%20from%20the%20editing%20module%20to%20regulate%0Aemotional%20expression%2C%20head%20poses%2C%20and%20speech%20content%20in%20synthesizing%20facial%0Aanimations.%20Extensive%20trials%20demonstrate%20that%20our%20method%20ensures%20lip%0Asynchronization%20with%20the%20audio%20while%20enabling%20decoupled%20control%20of%20facial%0Afeatures%2C%20it%20can%20generate%20realistic%20talking%20head%20with%20coordinated%20lip%20motions%2C%0Aauthentic%20facial%20emotions%2C%20and%20smooth%20head%20movements.%20The%20demo%20video%20is%0Aavailable%3A%20https%3A//anonymous.4open.science/r/SPEAK-8A22%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07257v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPEAK%253A%2520Speech-Driven%2520Pose%2520and%2520Emotion-Adjustable%2520Talking%2520Head%2520Generation%26entry.906535625%3DChangpeng%2520Cai%2520and%2520Guinan%2520Guo%2520and%2520Jiao%2520Li%2520and%2520Junhao%2520Su%2520and%2520Fei%2520Shen%2520and%2520Chenghao%2520He%2520and%2520Jing%2520Xiao%2520and%2520Yuanxu%2520Chen%2520and%2520Lei%2520Dai%2520and%2520Feiyu%2520Zhu%26entry.1292438233%3D%2520%2520Most%2520earlier%2520researches%2520on%2520talking%2520face%2520generation%2520have%2520focused%2520on%2520the%250Asynchronization%2520of%2520lip%2520motion%2520and%2520speech%2520content.%2520However%252C%2520head%2520pose%2520and%2520facial%250Aemotions%2520are%2520equally%2520important%2520characteristics%2520of%2520natural%2520faces.%2520While%250Aaudio-driven%2520talking%2520face%2520generation%2520has%2520seen%2520notable%2520advancements%252C%2520existing%250Amethods%2520either%2520overlook%2520facial%2520emotions%2520or%2520are%2520limited%2520to%2520specific%2520individuals%250Aand%2520cannot%2520be%2520applied%2520to%2520arbitrary%2520subjects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aone-shot%2520Talking%2520Head%2520Generation%2520framework%2520%2528SPEAK%2529%2520that%2520distinguishes%2520itself%250Afrom%2520the%2520general%2520Talking%2520Face%2520Generation%2520by%2520enabling%2520emotional%2520and%2520postural%250Acontrol.%2520Specifically%252C%2520we%2520introduce%2520Inter-Reconstructed%2520Feature%2520Disentanglement%250A%2528IRFD%2529%2520module%2520to%2520decouple%2520facial%2520features%2520into%2520three%2520latent%2520spaces.%2520Then%2520we%250Adesign%2520a%2520face%2520editing%2520module%2520that%2520modifies%2520speech%2520content%2520and%2520facial%2520latent%250Acodes%2520into%2520a%2520single%2520latent%2520space.%2520Subsequently%252C%2520we%2520present%2520a%2520novel%2520generator%250Athat%2520employs%2520modified%2520latent%2520codes%2520derived%2520from%2520the%2520editing%2520module%2520to%2520regulate%250Aemotional%2520expression%252C%2520head%2520poses%252C%2520and%2520speech%2520content%2520in%2520synthesizing%2520facial%250Aanimations.%2520Extensive%2520trials%2520demonstrate%2520that%2520our%2520method%2520ensures%2520lip%250Asynchronization%2520with%2520the%2520audio%2520while%2520enabling%2520decoupled%2520control%2520of%2520facial%250Afeatures%252C%2520it%2520can%2520generate%2520realistic%2520talking%2520head%2520with%2520coordinated%2520lip%2520motions%252C%250Aauthentic%2520facial%2520emotions%252C%2520and%2520smooth%2520head%2520movements.%2520The%2520demo%2520video%2520is%250Aavailable%253A%2520https%253A//anonymous.4open.science/r/SPEAK-8A22%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07257v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEAK%3A%20Speech-Driven%20Pose%20and%20Emotion-Adjustable%20Talking%20Head%20Generation&entry.906535625=Changpeng%20Cai%20and%20Guinan%20Guo%20and%20Jiao%20Li%20and%20Junhao%20Su%20and%20Fei%20Shen%20and%20Chenghao%20He%20and%20Jing%20Xiao%20and%20Yuanxu%20Chen%20and%20Lei%20Dai%20and%20Feiyu%20Zhu&entry.1292438233=%20%20Most%20earlier%20researches%20on%20talking%20face%20generation%20have%20focused%20on%20the%0Asynchronization%20of%20lip%20motion%20and%20speech%20content.%20However%2C%20head%20pose%20and%20facial%0Aemotions%20are%20equally%20important%20characteristics%20of%20natural%20faces.%20While%0Aaudio-driven%20talking%20face%20generation%20has%20seen%20notable%20advancements%2C%20existing%0Amethods%20either%20overlook%20facial%20emotions%20or%20are%20limited%20to%20specific%20individuals%0Aand%20cannot%20be%20applied%20to%20arbitrary%20subjects.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aone-shot%20Talking%20Head%20Generation%20framework%20%28SPEAK%29%20that%20distinguishes%20itself%0Afrom%20the%20general%20Talking%20Face%20Generation%20by%20enabling%20emotional%20and%20postural%0Acontrol.%20Specifically%2C%20we%20introduce%20Inter-Reconstructed%20Feature%20Disentanglement%0A%28IRFD%29%20module%20to%20decouple%20facial%20features%20into%20three%20latent%20spaces.%20Then%20we%0Adesign%20a%20face%20editing%20module%20that%20modifies%20speech%20content%20and%20facial%20latent%0Acodes%20into%20a%20single%20latent%20space.%20Subsequently%2C%20we%20present%20a%20novel%20generator%0Athat%20employs%20modified%20latent%20codes%20derived%20from%20the%20editing%20module%20to%20regulate%0Aemotional%20expression%2C%20head%20poses%2C%20and%20speech%20content%20in%20synthesizing%20facial%0Aanimations.%20Extensive%20trials%20demonstrate%20that%20our%20method%20ensures%20lip%0Asynchronization%20with%20the%20audio%20while%20enabling%20decoupled%20control%20of%20facial%0Afeatures%2C%20it%20can%20generate%20realistic%20talking%20head%20with%20coordinated%20lip%20motions%2C%0Aauthentic%20facial%20emotions%2C%20and%20smooth%20head%20movements.%20The%20demo%20video%20is%0Aavailable%3A%20https%3A//anonymous.4open.science/r/SPEAK-8A22%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07257v3&entry.124074799=Read"},
{"title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications", "author": "G. Manni and C. Lauretti and F. Prata and R. Papalia and L. Zollo and P. Soda", "abstract": "  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Monocular\nVisual Simultaneous Localization and Mapping (MVSLAM) has emerged as a\npromising solution, its implementation in endoscopic procedures faces\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents\nBodySLAM, a robust deep learning-based MVSLAM approach that addresses these\nchallenges through three key components: CycleVO, a novel unsupervised\nmonocular pose estimation module; the integration of the state-of-the-art Zoe\narchitecture for monocular depth estimation; and a 3D reconstruction module\ncreating a coherent surgical map. The approach is rigorously evaluated using\nthree publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning\nlaparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against\nfour state-of-the-art methods. Results demonstrate that CycleVO exhibited\ncompetitive performance with the lowest inference time among pose estimation\nmethods, while maintaining robust generalization capabilities, whereas Zoe\nsignificantly outperformed existing algorithms for depth estimation in\nendoscopy. BodySLAM's strong performance across diverse endoscopic scenarios\ndemonstrates its potential as a viable MVSLAM solution for endoscopic\napplications.\n", "link": "http://arxiv.org/abs/2408.03078v2", "date": "2024-11-04", "relevancy": 2.9387, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5892}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications&body=Title%3A%20BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications%0AAuthor%3A%20G.%20Manni%20and%20C.%20Lauretti%20and%20F.%20Prata%20and%20R.%20Papalia%20and%20L.%20Zollo%20and%20P.%20Soda%0AAbstract%3A%20%20%20Endoscopic%20surgery%20relies%20on%20two-dimensional%20views%2C%20posing%20challenges%20for%0Asurgeons%20in%20depth%20perception%20and%20instrument%20manipulation.%20While%20Monocular%0AVisual%20Simultaneous%20Localization%20and%20Mapping%20%28MVSLAM%29%20has%20emerged%20as%20a%0Apromising%20solution%2C%20its%20implementation%20in%20endoscopic%20procedures%20faces%0Asignificant%20challenges%20due%20to%20hardware%20limitations%2C%20such%20as%20the%20use%20of%20a%0Amonocular%20camera%20and%20the%20absence%20of%20odometry%20sensors.%20This%20study%20presents%0ABodySLAM%2C%20a%20robust%20deep%20learning-based%20MVSLAM%20approach%20that%20addresses%20these%0Achallenges%20through%20three%20key%20components%3A%20CycleVO%2C%20a%20novel%20unsupervised%0Amonocular%20pose%20estimation%20module%3B%20the%20integration%20of%20the%20state-of-the-art%20Zoe%0Aarchitecture%20for%20monocular%20depth%20estimation%3B%20and%20a%203D%20reconstruction%20module%0Acreating%20a%20coherent%20surgical%20map.%20The%20approach%20is%20rigorously%20evaluated%20using%0Athree%20publicly%20available%20datasets%20%28Hamlyn%2C%20EndoSLAM%2C%20and%20SCARED%29%20spanning%0Alaparoscopy%2C%20gastroscopy%2C%20and%20colonoscopy%20scenarios%2C%20and%20benchmarked%20against%0Afour%20state-of-the-art%20methods.%20Results%20demonstrate%20that%20CycleVO%20exhibited%0Acompetitive%20performance%20with%20the%20lowest%20inference%20time%20among%20pose%20estimation%0Amethods%2C%20while%20maintaining%20robust%20generalization%20capabilities%2C%20whereas%20Zoe%0Asignificantly%20outperformed%20existing%20algorithms%20for%20depth%20estimation%20in%0Aendoscopy.%20BodySLAM%27s%20strong%20performance%20across%20diverse%20endoscopic%20scenarios%0Ademonstrates%20its%20potential%20as%20a%20viable%20MVSLAM%20solution%20for%20endoscopic%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBodySLAM%253A%2520A%2520Generalized%2520Monocular%2520Visual%2520SLAM%2520Framework%2520for%2520Surgical%250A%2520%2520Applications%26entry.906535625%3DG.%2520Manni%2520and%2520C.%2520Lauretti%2520and%2520F.%2520Prata%2520and%2520R.%2520Papalia%2520and%2520L.%2520Zollo%2520and%2520P.%2520Soda%26entry.1292438233%3D%2520%2520Endoscopic%2520surgery%2520relies%2520on%2520two-dimensional%2520views%252C%2520posing%2520challenges%2520for%250Asurgeons%2520in%2520depth%2520perception%2520and%2520instrument%2520manipulation.%2520While%2520Monocular%250AVisual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528MVSLAM%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520solution%252C%2520its%2520implementation%2520in%2520endoscopic%2520procedures%2520faces%250Asignificant%2520challenges%2520due%2520to%2520hardware%2520limitations%252C%2520such%2520as%2520the%2520use%2520of%2520a%250Amonocular%2520camera%2520and%2520the%2520absence%2520of%2520odometry%2520sensors.%2520This%2520study%2520presents%250ABodySLAM%252C%2520a%2520robust%2520deep%2520learning-based%2520MVSLAM%2520approach%2520that%2520addresses%2520these%250Achallenges%2520through%2520three%2520key%2520components%253A%2520CycleVO%252C%2520a%2520novel%2520unsupervised%250Amonocular%2520pose%2520estimation%2520module%253B%2520the%2520integration%2520of%2520the%2520state-of-the-art%2520Zoe%250Aarchitecture%2520for%2520monocular%2520depth%2520estimation%253B%2520and%2520a%25203D%2520reconstruction%2520module%250Acreating%2520a%2520coherent%2520surgical%2520map.%2520The%2520approach%2520is%2520rigorously%2520evaluated%2520using%250Athree%2520publicly%2520available%2520datasets%2520%2528Hamlyn%252C%2520EndoSLAM%252C%2520and%2520SCARED%2529%2520spanning%250Alaparoscopy%252C%2520gastroscopy%252C%2520and%2520colonoscopy%2520scenarios%252C%2520and%2520benchmarked%2520against%250Afour%2520state-of-the-art%2520methods.%2520Results%2520demonstrate%2520that%2520CycleVO%2520exhibited%250Acompetitive%2520performance%2520with%2520the%2520lowest%2520inference%2520time%2520among%2520pose%2520estimation%250Amethods%252C%2520while%2520maintaining%2520robust%2520generalization%2520capabilities%252C%2520whereas%2520Zoe%250Asignificantly%2520outperformed%2520existing%2520algorithms%2520for%2520depth%2520estimation%2520in%250Aendoscopy.%2520BodySLAM%2527s%2520strong%2520performance%2520across%2520diverse%2520endoscopic%2520scenarios%250Ademonstrates%2520its%2520potential%2520as%2520a%2520viable%2520MVSLAM%2520solution%2520for%2520endoscopic%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BodySLAM%3A%20A%20Generalized%20Monocular%20Visual%20SLAM%20Framework%20for%20Surgical%0A%20%20Applications&entry.906535625=G.%20Manni%20and%20C.%20Lauretti%20and%20F.%20Prata%20and%20R.%20Papalia%20and%20L.%20Zollo%20and%20P.%20Soda&entry.1292438233=%20%20Endoscopic%20surgery%20relies%20on%20two-dimensional%20views%2C%20posing%20challenges%20for%0Asurgeons%20in%20depth%20perception%20and%20instrument%20manipulation.%20While%20Monocular%0AVisual%20Simultaneous%20Localization%20and%20Mapping%20%28MVSLAM%29%20has%20emerged%20as%20a%0Apromising%20solution%2C%20its%20implementation%20in%20endoscopic%20procedures%20faces%0Asignificant%20challenges%20due%20to%20hardware%20limitations%2C%20such%20as%20the%20use%20of%20a%0Amonocular%20camera%20and%20the%20absence%20of%20odometry%20sensors.%20This%20study%20presents%0ABodySLAM%2C%20a%20robust%20deep%20learning-based%20MVSLAM%20approach%20that%20addresses%20these%0Achallenges%20through%20three%20key%20components%3A%20CycleVO%2C%20a%20novel%20unsupervised%0Amonocular%20pose%20estimation%20module%3B%20the%20integration%20of%20the%20state-of-the-art%20Zoe%0Aarchitecture%20for%20monocular%20depth%20estimation%3B%20and%20a%203D%20reconstruction%20module%0Acreating%20a%20coherent%20surgical%20map.%20The%20approach%20is%20rigorously%20evaluated%20using%0Athree%20publicly%20available%20datasets%20%28Hamlyn%2C%20EndoSLAM%2C%20and%20SCARED%29%20spanning%0Alaparoscopy%2C%20gastroscopy%2C%20and%20colonoscopy%20scenarios%2C%20and%20benchmarked%20against%0Afour%20state-of-the-art%20methods.%20Results%20demonstrate%20that%20CycleVO%20exhibited%0Acompetitive%20performance%20with%20the%20lowest%20inference%20time%20among%20pose%20estimation%0Amethods%2C%20while%20maintaining%20robust%20generalization%20capabilities%2C%20whereas%20Zoe%0Asignificantly%20outperformed%20existing%20algorithms%20for%20depth%20estimation%20in%0Aendoscopy.%20BodySLAM%27s%20strong%20performance%20across%20diverse%20endoscopic%20scenarios%0Ademonstrates%20its%20potential%20as%20a%20viable%20MVSLAM%20solution%20for%20endoscopic%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03078v2&entry.124074799=Read"},
{"title": "Adaptive Length Image Tokenization via Recurrent Allocation", "author": "Shivam Duggal and Phillip Isola and Antonio Torralba and William T. Freeman", "abstract": "  Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.\n", "link": "http://arxiv.org/abs/2411.02393v1", "date": "2024-11-04", "relevancy": 2.9211, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.548}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Length%20Image%20Tokenization%20via%20Recurrent%20Allocation&body=Title%3A%20Adaptive%20Length%20Image%20Tokenization%20via%20Recurrent%20Allocation%0AAuthor%3A%20Shivam%20Duggal%20and%20Phillip%20Isola%20and%20Antonio%20Torralba%20and%20William%20T.%20Freeman%0AAbstract%3A%20%20%20Current%20vision%20systems%20typically%20assign%20fixed-length%20representations%20to%0Aimages%2C%20regardless%20of%20the%20information%20content.%20This%20contrasts%20with%20human%0Aintelligence%20-%20and%20even%20large%20language%20models%20-%20which%20allocate%20varying%0Arepresentational%20capacities%20based%20on%20entropy%2C%20context%20and%20familiarity.%20Inspired%0Aby%20this%2C%20we%20propose%20an%20approach%20to%20learn%20variable-length%20token%20representations%0Afor%202D%20images.%20Our%20encoder-decoder%20architecture%20recursively%20processes%202D%20image%0Atokens%2C%20distilling%20them%20into%201D%20latent%20tokens%20over%20multiple%20iterations%20of%0Arecurrent%20rollouts.%20Each%20iteration%20refines%20the%202D%20tokens%2C%20updates%20the%20existing%0A1D%20latent%20tokens%2C%20and%20adaptively%20increases%20representational%20capacity%20by%20adding%0Anew%20tokens.%20This%20enables%20compression%20of%20images%20into%20a%20variable%20number%20of%0Atokens%2C%20ranging%20from%2032%20to%20256.%20We%20validate%20our%20tokenizer%20using%20reconstruction%0Aloss%20and%20FID%20metrics%2C%20demonstrating%20that%20token%20count%20aligns%20with%20image%20entropy%2C%0Afamiliarity%20and%20downstream%20task%20requirements.%20Recurrent%20token%20processing%20with%0Aincreasing%20representational%20capacity%20in%20each%20iteration%20shows%20signs%20of%20token%0Aspecialization%2C%20revealing%20potential%20for%20object%20/%20part%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Length%2520Image%2520Tokenization%2520via%2520Recurrent%2520Allocation%26entry.906535625%3DShivam%2520Duggal%2520and%2520Phillip%2520Isola%2520and%2520Antonio%2520Torralba%2520and%2520William%2520T.%2520Freeman%26entry.1292438233%3D%2520%2520Current%2520vision%2520systems%2520typically%2520assign%2520fixed-length%2520representations%2520to%250Aimages%252C%2520regardless%2520of%2520the%2520information%2520content.%2520This%2520contrasts%2520with%2520human%250Aintelligence%2520-%2520and%2520even%2520large%2520language%2520models%2520-%2520which%2520allocate%2520varying%250Arepresentational%2520capacities%2520based%2520on%2520entropy%252C%2520context%2520and%2520familiarity.%2520Inspired%250Aby%2520this%252C%2520we%2520propose%2520an%2520approach%2520to%2520learn%2520variable-length%2520token%2520representations%250Afor%25202D%2520images.%2520Our%2520encoder-decoder%2520architecture%2520recursively%2520processes%25202D%2520image%250Atokens%252C%2520distilling%2520them%2520into%25201D%2520latent%2520tokens%2520over%2520multiple%2520iterations%2520of%250Arecurrent%2520rollouts.%2520Each%2520iteration%2520refines%2520the%25202D%2520tokens%252C%2520updates%2520the%2520existing%250A1D%2520latent%2520tokens%252C%2520and%2520adaptively%2520increases%2520representational%2520capacity%2520by%2520adding%250Anew%2520tokens.%2520This%2520enables%2520compression%2520of%2520images%2520into%2520a%2520variable%2520number%2520of%250Atokens%252C%2520ranging%2520from%252032%2520to%2520256.%2520We%2520validate%2520our%2520tokenizer%2520using%2520reconstruction%250Aloss%2520and%2520FID%2520metrics%252C%2520demonstrating%2520that%2520token%2520count%2520aligns%2520with%2520image%2520entropy%252C%250Afamiliarity%2520and%2520downstream%2520task%2520requirements.%2520Recurrent%2520token%2520processing%2520with%250Aincreasing%2520representational%2520capacity%2520in%2520each%2520iteration%2520shows%2520signs%2520of%2520token%250Aspecialization%252C%2520revealing%2520potential%2520for%2520object%2520/%2520part%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Length%20Image%20Tokenization%20via%20Recurrent%20Allocation&entry.906535625=Shivam%20Duggal%20and%20Phillip%20Isola%20and%20Antonio%20Torralba%20and%20William%20T.%20Freeman&entry.1292438233=%20%20Current%20vision%20systems%20typically%20assign%20fixed-length%20representations%20to%0Aimages%2C%20regardless%20of%20the%20information%20content.%20This%20contrasts%20with%20human%0Aintelligence%20-%20and%20even%20large%20language%20models%20-%20which%20allocate%20varying%0Arepresentational%20capacities%20based%20on%20entropy%2C%20context%20and%20familiarity.%20Inspired%0Aby%20this%2C%20we%20propose%20an%20approach%20to%20learn%20variable-length%20token%20representations%0Afor%202D%20images.%20Our%20encoder-decoder%20architecture%20recursively%20processes%202D%20image%0Atokens%2C%20distilling%20them%20into%201D%20latent%20tokens%20over%20multiple%20iterations%20of%0Arecurrent%20rollouts.%20Each%20iteration%20refines%20the%202D%20tokens%2C%20updates%20the%20existing%0A1D%20latent%20tokens%2C%20and%20adaptively%20increases%20representational%20capacity%20by%20adding%0Anew%20tokens.%20This%20enables%20compression%20of%20images%20into%20a%20variable%20number%20of%0Atokens%2C%20ranging%20from%2032%20to%20256.%20We%20validate%20our%20tokenizer%20using%20reconstruction%0Aloss%20and%20FID%20metrics%2C%20demonstrating%20that%20token%20count%20aligns%20with%20image%20entropy%2C%0Afamiliarity%20and%20downstream%20task%20requirements.%20Recurrent%20token%20processing%20with%0Aincreasing%20representational%20capacity%20in%20each%20iteration%20shows%20signs%20of%20token%0Aspecialization%2C%20revealing%20potential%20for%20object%20/%20part%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02393v1&entry.124074799=Read"},
{"title": "Local Concept Embeddings for Analysis of Concept Distributions in DNN\n  Feature Spaces", "author": "Georgii Mikriukov and Gesina Schwalbe and Korinna Bade", "abstract": "  Insights into the learned latent representations are imperative for verifying\ndeep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore,\nstate-of-the-art supervised Concept-based eXplainable Artificial Intelligence\n(C-XAI) methods associate user-defined concepts like ``car'' each with a single\nvector in the DNN latent space (concept embedding vector). In the case of\nconcept segmentation, these linearly separate between activation map pixels\nbelonging to a concept and those belonging to background. Existing methods for\nconcept segmentation, however, fall short of capturing sub-concepts (e.g.,\n``proximate car'' and ``distant car''), and concept overlap (e.g., between\n``bus'' and ``truck''). In other words, they do not capture the full\ndistribution of concept representatives in latent space. For the first time,\nthis work shows that these simplifications are frequently broken and that\ndistribution information can be particularly useful for understanding\nDNN-learned notions of sub-concepts, concept confusion, and concept outliers.\nTo allow exploration of learned concept distributions, we propose a novel local\nconcept analysis framework. Instead of optimizing a single global concept\nvector on the complete dataset, it generates a local concept embedding (LoCE)\nvector for each individual sample. We use the distribution formed by LoCEs to\nexplore the latent concept distribution by fitting Gaussian mixture models\n(GMMs), hierarchical clustering, and concept-level information retrieval and\noutlier detection. Despite its context sensitivity, our method's concept\nsegmentation performance is competitive to global baselines. Analysis results\nare obtained on two datasets and five diverse vision DNN architectures,\nincluding vision transformers (ViTs).\n", "link": "http://arxiv.org/abs/2311.14435v2", "date": "2024-11-04", "relevancy": 2.8666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Concept%20Embeddings%20for%20Analysis%20of%20Concept%20Distributions%20in%20DNN%0A%20%20Feature%20Spaces&body=Title%3A%20Local%20Concept%20Embeddings%20for%20Analysis%20of%20Concept%20Distributions%20in%20DNN%0A%20%20Feature%20Spaces%0AAuthor%3A%20Georgii%20Mikriukov%20and%20Gesina%20Schwalbe%20and%20Korinna%20Bade%0AAbstract%3A%20%20%20Insights%20into%20the%20learned%20latent%20representations%20are%20imperative%20for%20verifying%0Adeep%20neural%20networks%20%28DNNs%29%20in%20critical%20computer%20vision%20%28CV%29%20tasks.%20Therefore%2C%0Astate-of-the-art%20supervised%20Concept-based%20eXplainable%20Artificial%20Intelligence%0A%28C-XAI%29%20methods%20associate%20user-defined%20concepts%20like%20%60%60car%27%27%20each%20with%20a%20single%0Avector%20in%20the%20DNN%20latent%20space%20%28concept%20embedding%20vector%29.%20In%20the%20case%20of%0Aconcept%20segmentation%2C%20these%20linearly%20separate%20between%20activation%20map%20pixels%0Abelonging%20to%20a%20concept%20and%20those%20belonging%20to%20background.%20Existing%20methods%20for%0Aconcept%20segmentation%2C%20however%2C%20fall%20short%20of%20capturing%20sub-concepts%20%28e.g.%2C%0A%60%60proximate%20car%27%27%20and%20%60%60distant%20car%27%27%29%2C%20and%20concept%20overlap%20%28e.g.%2C%20between%0A%60%60bus%27%27%20and%20%60%60truck%27%27%29.%20In%20other%20words%2C%20they%20do%20not%20capture%20the%20full%0Adistribution%20of%20concept%20representatives%20in%20latent%20space.%20For%20the%20first%20time%2C%0Athis%20work%20shows%20that%20these%20simplifications%20are%20frequently%20broken%20and%20that%0Adistribution%20information%20can%20be%20particularly%20useful%20for%20understanding%0ADNN-learned%20notions%20of%20sub-concepts%2C%20concept%20confusion%2C%20and%20concept%20outliers.%0ATo%20allow%20exploration%20of%20learned%20concept%20distributions%2C%20we%20propose%20a%20novel%20local%0Aconcept%20analysis%20framework.%20Instead%20of%20optimizing%20a%20single%20global%20concept%0Avector%20on%20the%20complete%20dataset%2C%20it%20generates%20a%20local%20concept%20embedding%20%28LoCE%29%0Avector%20for%20each%20individual%20sample.%20We%20use%20the%20distribution%20formed%20by%20LoCEs%20to%0Aexplore%20the%20latent%20concept%20distribution%20by%20fitting%20Gaussian%20mixture%20models%0A%28GMMs%29%2C%20hierarchical%20clustering%2C%20and%20concept-level%20information%20retrieval%20and%0Aoutlier%20detection.%20Despite%20its%20context%20sensitivity%2C%20our%20method%27s%20concept%0Asegmentation%20performance%20is%20competitive%20to%20global%20baselines.%20Analysis%20results%0Aare%20obtained%20on%20two%20datasets%20and%20five%20diverse%20vision%20DNN%20architectures%2C%0Aincluding%20vision%20transformers%20%28ViTs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Concept%2520Embeddings%2520for%2520Analysis%2520of%2520Concept%2520Distributions%2520in%2520DNN%250A%2520%2520Feature%2520Spaces%26entry.906535625%3DGeorgii%2520Mikriukov%2520and%2520Gesina%2520Schwalbe%2520and%2520Korinna%2520Bade%26entry.1292438233%3D%2520%2520Insights%2520into%2520the%2520learned%2520latent%2520representations%2520are%2520imperative%2520for%2520verifying%250Adeep%2520neural%2520networks%2520%2528DNNs%2529%2520in%2520critical%2520computer%2520vision%2520%2528CV%2529%2520tasks.%2520Therefore%252C%250Astate-of-the-art%2520supervised%2520Concept-based%2520eXplainable%2520Artificial%2520Intelligence%250A%2528C-XAI%2529%2520methods%2520associate%2520user-defined%2520concepts%2520like%2520%2560%2560car%2527%2527%2520each%2520with%2520a%2520single%250Avector%2520in%2520the%2520DNN%2520latent%2520space%2520%2528concept%2520embedding%2520vector%2529.%2520In%2520the%2520case%2520of%250Aconcept%2520segmentation%252C%2520these%2520linearly%2520separate%2520between%2520activation%2520map%2520pixels%250Abelonging%2520to%2520a%2520concept%2520and%2520those%2520belonging%2520to%2520background.%2520Existing%2520methods%2520for%250Aconcept%2520segmentation%252C%2520however%252C%2520fall%2520short%2520of%2520capturing%2520sub-concepts%2520%2528e.g.%252C%250A%2560%2560proximate%2520car%2527%2527%2520and%2520%2560%2560distant%2520car%2527%2527%2529%252C%2520and%2520concept%2520overlap%2520%2528e.g.%252C%2520between%250A%2560%2560bus%2527%2527%2520and%2520%2560%2560truck%2527%2527%2529.%2520In%2520other%2520words%252C%2520they%2520do%2520not%2520capture%2520the%2520full%250Adistribution%2520of%2520concept%2520representatives%2520in%2520latent%2520space.%2520For%2520the%2520first%2520time%252C%250Athis%2520work%2520shows%2520that%2520these%2520simplifications%2520are%2520frequently%2520broken%2520and%2520that%250Adistribution%2520information%2520can%2520be%2520particularly%2520useful%2520for%2520understanding%250ADNN-learned%2520notions%2520of%2520sub-concepts%252C%2520concept%2520confusion%252C%2520and%2520concept%2520outliers.%250ATo%2520allow%2520exploration%2520of%2520learned%2520concept%2520distributions%252C%2520we%2520propose%2520a%2520novel%2520local%250Aconcept%2520analysis%2520framework.%2520Instead%2520of%2520optimizing%2520a%2520single%2520global%2520concept%250Avector%2520on%2520the%2520complete%2520dataset%252C%2520it%2520generates%2520a%2520local%2520concept%2520embedding%2520%2528LoCE%2529%250Avector%2520for%2520each%2520individual%2520sample.%2520We%2520use%2520the%2520distribution%2520formed%2520by%2520LoCEs%2520to%250Aexplore%2520the%2520latent%2520concept%2520distribution%2520by%2520fitting%2520Gaussian%2520mixture%2520models%250A%2528GMMs%2529%252C%2520hierarchical%2520clustering%252C%2520and%2520concept-level%2520information%2520retrieval%2520and%250Aoutlier%2520detection.%2520Despite%2520its%2520context%2520sensitivity%252C%2520our%2520method%2527s%2520concept%250Asegmentation%2520performance%2520is%2520competitive%2520to%2520global%2520baselines.%2520Analysis%2520results%250Aare%2520obtained%2520on%2520two%2520datasets%2520and%2520five%2520diverse%2520vision%2520DNN%2520architectures%252C%250Aincluding%2520vision%2520transformers%2520%2528ViTs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Concept%20Embeddings%20for%20Analysis%20of%20Concept%20Distributions%20in%20DNN%0A%20%20Feature%20Spaces&entry.906535625=Georgii%20Mikriukov%20and%20Gesina%20Schwalbe%20and%20Korinna%20Bade&entry.1292438233=%20%20Insights%20into%20the%20learned%20latent%20representations%20are%20imperative%20for%20verifying%0Adeep%20neural%20networks%20%28DNNs%29%20in%20critical%20computer%20vision%20%28CV%29%20tasks.%20Therefore%2C%0Astate-of-the-art%20supervised%20Concept-based%20eXplainable%20Artificial%20Intelligence%0A%28C-XAI%29%20methods%20associate%20user-defined%20concepts%20like%20%60%60car%27%27%20each%20with%20a%20single%0Avector%20in%20the%20DNN%20latent%20space%20%28concept%20embedding%20vector%29.%20In%20the%20case%20of%0Aconcept%20segmentation%2C%20these%20linearly%20separate%20between%20activation%20map%20pixels%0Abelonging%20to%20a%20concept%20and%20those%20belonging%20to%20background.%20Existing%20methods%20for%0Aconcept%20segmentation%2C%20however%2C%20fall%20short%20of%20capturing%20sub-concepts%20%28e.g.%2C%0A%60%60proximate%20car%27%27%20and%20%60%60distant%20car%27%27%29%2C%20and%20concept%20overlap%20%28e.g.%2C%20between%0A%60%60bus%27%27%20and%20%60%60truck%27%27%29.%20In%20other%20words%2C%20they%20do%20not%20capture%20the%20full%0Adistribution%20of%20concept%20representatives%20in%20latent%20space.%20For%20the%20first%20time%2C%0Athis%20work%20shows%20that%20these%20simplifications%20are%20frequently%20broken%20and%20that%0Adistribution%20information%20can%20be%20particularly%20useful%20for%20understanding%0ADNN-learned%20notions%20of%20sub-concepts%2C%20concept%20confusion%2C%20and%20concept%20outliers.%0ATo%20allow%20exploration%20of%20learned%20concept%20distributions%2C%20we%20propose%20a%20novel%20local%0Aconcept%20analysis%20framework.%20Instead%20of%20optimizing%20a%20single%20global%20concept%0Avector%20on%20the%20complete%20dataset%2C%20it%20generates%20a%20local%20concept%20embedding%20%28LoCE%29%0Avector%20for%20each%20individual%20sample.%20We%20use%20the%20distribution%20formed%20by%20LoCEs%20to%0Aexplore%20the%20latent%20concept%20distribution%20by%20fitting%20Gaussian%20mixture%20models%0A%28GMMs%29%2C%20hierarchical%20clustering%2C%20and%20concept-level%20information%20retrieval%20and%0Aoutlier%20detection.%20Despite%20its%20context%20sensitivity%2C%20our%20method%27s%20concept%0Asegmentation%20performance%20is%20competitive%20to%20global%20baselines.%20Analysis%20results%0Aare%20obtained%20on%20two%20datasets%20and%20five%20diverse%20vision%20DNN%20architectures%2C%0Aincluding%20vision%20transformers%20%28ViTs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14435v2&entry.124074799=Read"},
{"title": "S3PT: Scene Semantics and Structure Guided Clustering to Boost\n  Self-Supervised Pre-Training for Autonomous Driving", "author": "Maciej K. Wozniak and Hariprasath Govindarajan and Marvin Klingner and Camille Maurice and B Ravi Kiran and Senthil Yogamani", "abstract": "  Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.\n", "link": "http://arxiv.org/abs/2410.23085v2", "date": "2024-11-04", "relevancy": 2.8592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S3PT%3A%20Scene%20Semantics%20and%20Structure%20Guided%20Clustering%20to%20Boost%0A%20%20Self-Supervised%20Pre-Training%20for%20Autonomous%20Driving&body=Title%3A%20S3PT%3A%20Scene%20Semantics%20and%20Structure%20Guided%20Clustering%20to%20Boost%0A%20%20Self-Supervised%20Pre-Training%20for%20Autonomous%20Driving%0AAuthor%3A%20Maciej%20K.%20Wozniak%20and%20Hariprasath%20Govindarajan%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani%0AAbstract%3A%20%20%20Recent%20self-supervised%20clustering-based%20pre-training%20techniques%20like%20DINO%20and%0ACribo%20have%20shown%20impressive%20results%20for%20downstream%20detection%20and%20segmentation%0Atasks.%20However%2C%20real-world%20applications%20such%20as%20autonomous%20driving%20face%0Achallenges%20with%20imbalanced%20object%20class%20and%20size%20distributions%20and%20complex%0Ascene%20geometries.%20In%20this%20paper%2C%20we%20propose%20S3PT%20a%20novel%20scene%20semantics%20and%0Astructure%20guided%20clustering%20to%20provide%20more%20scene-consistent%20objectives%20for%0Aself-supervised%20training.%20Specifically%2C%20our%20contributions%20are%20threefold%3A%20First%2C%0Awe%20incorporate%20semantic%20distribution%20consistent%20clustering%20to%20encourage%20better%0Arepresentation%20of%20rare%20classes%20such%20as%20motorcycles%20or%20animals.%20Second%2C%20we%0Aintroduce%20object%20diversity%20consistent%20spatial%20clustering%2C%20to%20handle%20imbalanced%0Aand%20diverse%20object%20sizes%2C%20ranging%20from%20large%20background%20areas%20to%20small%20objects%0Asuch%20as%20pedestrians%20and%20traffic%20signs.%20Third%2C%20we%20propose%20a%20depth-guided%20spatial%0Aclustering%20to%20regularize%20learning%20based%20on%20geometric%20information%20of%20the%20scene%2C%0Athus%20further%20refining%20region%20separation%20on%20the%20feature%20level.%20Our%20learned%0Arepresentations%20significantly%20improve%20performance%20in%20downstream%20semantic%0Asegmentation%20and%203D%20object%20detection%20tasks%20on%20the%20nuScenes%2C%20nuImages%2C%20and%0ACityscapes%20datasets%20and%20show%20promising%20domain%20translation%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS3PT%253A%2520Scene%2520Semantics%2520and%2520Structure%2520Guided%2520Clustering%2520to%2520Boost%250A%2520%2520Self-Supervised%2520Pre-Training%2520for%2520Autonomous%2520Driving%26entry.906535625%3DMaciej%2520K.%2520Wozniak%2520and%2520Hariprasath%2520Govindarajan%2520and%2520Marvin%2520Klingner%2520and%2520Camille%2520Maurice%2520and%2520B%2520Ravi%2520Kiran%2520and%2520Senthil%2520Yogamani%26entry.1292438233%3D%2520%2520Recent%2520self-supervised%2520clustering-based%2520pre-training%2520techniques%2520like%2520DINO%2520and%250ACribo%2520have%2520shown%2520impressive%2520results%2520for%2520downstream%2520detection%2520and%2520segmentation%250Atasks.%2520However%252C%2520real-world%2520applications%2520such%2520as%2520autonomous%2520driving%2520face%250Achallenges%2520with%2520imbalanced%2520object%2520class%2520and%2520size%2520distributions%2520and%2520complex%250Ascene%2520geometries.%2520In%2520this%2520paper%252C%2520we%2520propose%2520S3PT%2520a%2520novel%2520scene%2520semantics%2520and%250Astructure%2520guided%2520clustering%2520to%2520provide%2520more%2520scene-consistent%2520objectives%2520for%250Aself-supervised%2520training.%2520Specifically%252C%2520our%2520contributions%2520are%2520threefold%253A%2520First%252C%250Awe%2520incorporate%2520semantic%2520distribution%2520consistent%2520clustering%2520to%2520encourage%2520better%250Arepresentation%2520of%2520rare%2520classes%2520such%2520as%2520motorcycles%2520or%2520animals.%2520Second%252C%2520we%250Aintroduce%2520object%2520diversity%2520consistent%2520spatial%2520clustering%252C%2520to%2520handle%2520imbalanced%250Aand%2520diverse%2520object%2520sizes%252C%2520ranging%2520from%2520large%2520background%2520areas%2520to%2520small%2520objects%250Asuch%2520as%2520pedestrians%2520and%2520traffic%2520signs.%2520Third%252C%2520we%2520propose%2520a%2520depth-guided%2520spatial%250Aclustering%2520to%2520regularize%2520learning%2520based%2520on%2520geometric%2520information%2520of%2520the%2520scene%252C%250Athus%2520further%2520refining%2520region%2520separation%2520on%2520the%2520feature%2520level.%2520Our%2520learned%250Arepresentations%2520significantly%2520improve%2520performance%2520in%2520downstream%2520semantic%250Asegmentation%2520and%25203D%2520object%2520detection%2520tasks%2520on%2520the%2520nuScenes%252C%2520nuImages%252C%2520and%250ACityscapes%2520datasets%2520and%2520show%2520promising%2520domain%2520translation%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S3PT%3A%20Scene%20Semantics%20and%20Structure%20Guided%20Clustering%20to%20Boost%0A%20%20Self-Supervised%20Pre-Training%20for%20Autonomous%20Driving&entry.906535625=Maciej%20K.%20Wozniak%20and%20Hariprasath%20Govindarajan%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani&entry.1292438233=%20%20Recent%20self-supervised%20clustering-based%20pre-training%20techniques%20like%20DINO%20and%0ACribo%20have%20shown%20impressive%20results%20for%20downstream%20detection%20and%20segmentation%0Atasks.%20However%2C%20real-world%20applications%20such%20as%20autonomous%20driving%20face%0Achallenges%20with%20imbalanced%20object%20class%20and%20size%20distributions%20and%20complex%0Ascene%20geometries.%20In%20this%20paper%2C%20we%20propose%20S3PT%20a%20novel%20scene%20semantics%20and%0Astructure%20guided%20clustering%20to%20provide%20more%20scene-consistent%20objectives%20for%0Aself-supervised%20training.%20Specifically%2C%20our%20contributions%20are%20threefold%3A%20First%2C%0Awe%20incorporate%20semantic%20distribution%20consistent%20clustering%20to%20encourage%20better%0Arepresentation%20of%20rare%20classes%20such%20as%20motorcycles%20or%20animals.%20Second%2C%20we%0Aintroduce%20object%20diversity%20consistent%20spatial%20clustering%2C%20to%20handle%20imbalanced%0Aand%20diverse%20object%20sizes%2C%20ranging%20from%20large%20background%20areas%20to%20small%20objects%0Asuch%20as%20pedestrians%20and%20traffic%20signs.%20Third%2C%20we%20propose%20a%20depth-guided%20spatial%0Aclustering%20to%20regularize%20learning%20based%20on%20geometric%20information%20of%20the%20scene%2C%0Athus%20further%20refining%20region%20separation%20on%20the%20feature%20level.%20Our%20learned%0Arepresentations%20significantly%20improve%20performance%20in%20downstream%20semantic%0Asegmentation%20and%203D%20object%20detection%20tasks%20on%20the%20nuScenes%2C%20nuImages%2C%20and%0ACityscapes%20datasets%20and%20show%20promising%20domain%20translation%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23085v2&entry.124074799=Read"},
{"title": "Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking\n  Across Diverse Vocabularies", "author": "Sai Koneru and Matthias Huck and Miriam Exel and Jan Niehues", "abstract": "  Recent advancements in NLP have resulted in models with specialized\nstrengths, such as processing multimodal inputs or excelling in specific\ndomains. However, real-world tasks, like multimodal translation, often require\na combination of these strengths, such as handling both translation and image\nprocessing. While individual translation and vision models are powerful, they\ntypically lack the ability to perform both tasks in a single system. Combining\nthese models poses challenges, particularly due to differences in their\nvocabularies, which limit the effectiveness of traditional ensemble methods to\npost-generation techniques like N-best list re-ranking. In this work, we\npropose a novel zero-shot ensembling strategy that allows for the integration\nof different models during the decoding phase without the need for additional\ntraining. Our approach re-ranks beams during decoding by combining scores at\nthe word level, using heuristics to predict when a word is completed. We\ndemonstrate the effectiveness of this method in machine translation scenarios,\nshowing that it enables the generation of translations that are both speech-\nand image-aware while also improving overall translation quality (We will\nrelease the code upon paper acceptance.).\n", "link": "http://arxiv.org/abs/2408.11327v2", "date": "2024-11-04", "relevancy": 2.8537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5863}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plug%2C%20Play%2C%20and%20Fuse%3A%20Zero-Shot%20Joint%20Decoding%20via%20Word-Level%20Re-ranking%0A%20%20Across%20Diverse%20Vocabularies&body=Title%3A%20Plug%2C%20Play%2C%20and%20Fuse%3A%20Zero-Shot%20Joint%20Decoding%20via%20Word-Level%20Re-ranking%0A%20%20Across%20Diverse%20Vocabularies%0AAuthor%3A%20Sai%20Koneru%20and%20Matthias%20Huck%20and%20Miriam%20Exel%20and%20Jan%20Niehues%0AAbstract%3A%20%20%20Recent%20advancements%20in%20NLP%20have%20resulted%20in%20models%20with%20specialized%0Astrengths%2C%20such%20as%20processing%20multimodal%20inputs%20or%20excelling%20in%20specific%0Adomains.%20However%2C%20real-world%20tasks%2C%20like%20multimodal%20translation%2C%20often%20require%0Aa%20combination%20of%20these%20strengths%2C%20such%20as%20handling%20both%20translation%20and%20image%0Aprocessing.%20While%20individual%20translation%20and%20vision%20models%20are%20powerful%2C%20they%0Atypically%20lack%20the%20ability%20to%20perform%20both%20tasks%20in%20a%20single%20system.%20Combining%0Athese%20models%20poses%20challenges%2C%20particularly%20due%20to%20differences%20in%20their%0Avocabularies%2C%20which%20limit%20the%20effectiveness%20of%20traditional%20ensemble%20methods%20to%0Apost-generation%20techniques%20like%20N-best%20list%20re-ranking.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20zero-shot%20ensembling%20strategy%20that%20allows%20for%20the%20integration%0Aof%20different%20models%20during%20the%20decoding%20phase%20without%20the%20need%20for%20additional%0Atraining.%20Our%20approach%20re-ranks%20beams%20during%20decoding%20by%20combining%20scores%20at%0Athe%20word%20level%2C%20using%20heuristics%20to%20predict%20when%20a%20word%20is%20completed.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20method%20in%20machine%20translation%20scenarios%2C%0Ashowing%20that%20it%20enables%20the%20generation%20of%20translations%20that%20are%20both%20speech-%0Aand%20image-aware%20while%20also%20improving%20overall%20translation%20quality%20%28We%20will%0Arelease%20the%20code%20upon%20paper%20acceptance.%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlug%252C%2520Play%252C%2520and%2520Fuse%253A%2520Zero-Shot%2520Joint%2520Decoding%2520via%2520Word-Level%2520Re-ranking%250A%2520%2520Across%2520Diverse%2520Vocabularies%26entry.906535625%3DSai%2520Koneru%2520and%2520Matthias%2520Huck%2520and%2520Miriam%2520Exel%2520and%2520Jan%2520Niehues%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520NLP%2520have%2520resulted%2520in%2520models%2520with%2520specialized%250Astrengths%252C%2520such%2520as%2520processing%2520multimodal%2520inputs%2520or%2520excelling%2520in%2520specific%250Adomains.%2520However%252C%2520real-world%2520tasks%252C%2520like%2520multimodal%2520translation%252C%2520often%2520require%250Aa%2520combination%2520of%2520these%2520strengths%252C%2520such%2520as%2520handling%2520both%2520translation%2520and%2520image%250Aprocessing.%2520While%2520individual%2520translation%2520and%2520vision%2520models%2520are%2520powerful%252C%2520they%250Atypically%2520lack%2520the%2520ability%2520to%2520perform%2520both%2520tasks%2520in%2520a%2520single%2520system.%2520Combining%250Athese%2520models%2520poses%2520challenges%252C%2520particularly%2520due%2520to%2520differences%2520in%2520their%250Avocabularies%252C%2520which%2520limit%2520the%2520effectiveness%2520of%2520traditional%2520ensemble%2520methods%2520to%250Apost-generation%2520techniques%2520like%2520N-best%2520list%2520re-ranking.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520zero-shot%2520ensembling%2520strategy%2520that%2520allows%2520for%2520the%2520integration%250Aof%2520different%2520models%2520during%2520the%2520decoding%2520phase%2520without%2520the%2520need%2520for%2520additional%250Atraining.%2520Our%2520approach%2520re-ranks%2520beams%2520during%2520decoding%2520by%2520combining%2520scores%2520at%250Athe%2520word%2520level%252C%2520using%2520heuristics%2520to%2520predict%2520when%2520a%2520word%2520is%2520completed.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520method%2520in%2520machine%2520translation%2520scenarios%252C%250Ashowing%2520that%2520it%2520enables%2520the%2520generation%2520of%2520translations%2520that%2520are%2520both%2520speech-%250Aand%2520image-aware%2520while%2520also%2520improving%2520overall%2520translation%2520quality%2520%2528We%2520will%250Arelease%2520the%2520code%2520upon%2520paper%2520acceptance.%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plug%2C%20Play%2C%20and%20Fuse%3A%20Zero-Shot%20Joint%20Decoding%20via%20Word-Level%20Re-ranking%0A%20%20Across%20Diverse%20Vocabularies&entry.906535625=Sai%20Koneru%20and%20Matthias%20Huck%20and%20Miriam%20Exel%20and%20Jan%20Niehues&entry.1292438233=%20%20Recent%20advancements%20in%20NLP%20have%20resulted%20in%20models%20with%20specialized%0Astrengths%2C%20such%20as%20processing%20multimodal%20inputs%20or%20excelling%20in%20specific%0Adomains.%20However%2C%20real-world%20tasks%2C%20like%20multimodal%20translation%2C%20often%20require%0Aa%20combination%20of%20these%20strengths%2C%20such%20as%20handling%20both%20translation%20and%20image%0Aprocessing.%20While%20individual%20translation%20and%20vision%20models%20are%20powerful%2C%20they%0Atypically%20lack%20the%20ability%20to%20perform%20both%20tasks%20in%20a%20single%20system.%20Combining%0Athese%20models%20poses%20challenges%2C%20particularly%20due%20to%20differences%20in%20their%0Avocabularies%2C%20which%20limit%20the%20effectiveness%20of%20traditional%20ensemble%20methods%20to%0Apost-generation%20techniques%20like%20N-best%20list%20re-ranking.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20zero-shot%20ensembling%20strategy%20that%20allows%20for%20the%20integration%0Aof%20different%20models%20during%20the%20decoding%20phase%20without%20the%20need%20for%20additional%0Atraining.%20Our%20approach%20re-ranks%20beams%20during%20decoding%20by%20combining%20scores%20at%0Athe%20word%20level%2C%20using%20heuristics%20to%20predict%20when%20a%20word%20is%20completed.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20method%20in%20machine%20translation%20scenarios%2C%0Ashowing%20that%20it%20enables%20the%20generation%20of%20translations%20that%20are%20both%20speech-%0Aand%20image-aware%20while%20also%20improving%20overall%20translation%20quality%20%28We%20will%0Arelease%20the%20code%20upon%20paper%20acceptance.%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11327v2&entry.124074799=Read"},
{"title": "UniRGB-IR: A Unified Framework for RGB-Infrared Semantic Tasks via\n  Adapter Tuning", "author": "Maoxun Yuan and Bo Cui and Tianyi Zhao and Jiayi Wang and Shan Fu and Xingxing Wei", "abstract": "  Semantic analysis on visible (RGB) and infrared (IR) images has gained\nattention for its ability to be more accurate and robust under low-illumination\nand complex weather conditions. Due to the lack of pre-trained foundation\nmodels on the large-scale infrared image datasets, existing methods prefer to\ndesign task-specific frameworks and directly fine-tune them with pre-trained\nfoundation models on their RGB-IR semantic relevance datasets, which results in\npoor scalability and limited generalization. In this work, we propose a general\nand efficient framework called UniRGB-IR to unify RGB-IR semantic tasks, in\nwhich a novel adapter is developed to efficiently introduce richer RGB-IR\nfeatures into the pre-trained RGB-based foundation model. Specifically, our\nframework consists of a RGB-based foundation model, a Multi-modal Feature Pool\n(MFP) module and a Supplementary Feature Injector (SFI) module. The MFP and SFI\nmodules cooperate with each other as an adapter to effectively complement the\nRGB-based features with the rich RGB-IR features. During training process, we\nfreeze the entire foundation model to inherit prior knowledge and only optimize\nthe proposed adapter. Furthermore, to verify the effectiveness of our\nframework, we utilize the vanilla vision transformer (ViT-Base) as the\npre-trained foundation model to perform extensive experiments. Experimental\nresults on various RGB-IR downstream tasks demonstrate that our method can\nachieve state-of-the-art performance. The source code and results are available\nat https://github.com/PoTsui99/UniRGB-IR.git.\n", "link": "http://arxiv.org/abs/2404.17360v2", "date": "2024-11-04", "relevancy": 2.8512, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniRGB-IR%3A%20A%20Unified%20Framework%20for%20RGB-Infrared%20Semantic%20Tasks%20via%0A%20%20Adapter%20Tuning&body=Title%3A%20UniRGB-IR%3A%20A%20Unified%20Framework%20for%20RGB-Infrared%20Semantic%20Tasks%20via%0A%20%20Adapter%20Tuning%0AAuthor%3A%20Maoxun%20Yuan%20and%20Bo%20Cui%20and%20Tianyi%20Zhao%20and%20Jiayi%20Wang%20and%20Shan%20Fu%20and%20Xingxing%20Wei%0AAbstract%3A%20%20%20Semantic%20analysis%20on%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20gained%0Aattention%20for%20its%20ability%20to%20be%20more%20accurate%20and%20robust%20under%20low-illumination%0Aand%20complex%20weather%20conditions.%20Due%20to%20the%20lack%20of%20pre-trained%20foundation%0Amodels%20on%20the%20large-scale%20infrared%20image%20datasets%2C%20existing%20methods%20prefer%20to%0Adesign%20task-specific%20frameworks%20and%20directly%20fine-tune%20them%20with%20pre-trained%0Afoundation%20models%20on%20their%20RGB-IR%20semantic%20relevance%20datasets%2C%20which%20results%20in%0Apoor%20scalability%20and%20limited%20generalization.%20In%20this%20work%2C%20we%20propose%20a%20general%0Aand%20efficient%20framework%20called%20UniRGB-IR%20to%20unify%20RGB-IR%20semantic%20tasks%2C%20in%0Awhich%20a%20novel%20adapter%20is%20developed%20to%20efficiently%20introduce%20richer%20RGB-IR%0Afeatures%20into%20the%20pre-trained%20RGB-based%20foundation%20model.%20Specifically%2C%20our%0Aframework%20consists%20of%20a%20RGB-based%20foundation%20model%2C%20a%20Multi-modal%20Feature%20Pool%0A%28MFP%29%20module%20and%20a%20Supplementary%20Feature%20Injector%20%28SFI%29%20module.%20The%20MFP%20and%20SFI%0Amodules%20cooperate%20with%20each%20other%20as%20an%20adapter%20to%20effectively%20complement%20the%0ARGB-based%20features%20with%20the%20rich%20RGB-IR%20features.%20During%20training%20process%2C%20we%0Afreeze%20the%20entire%20foundation%20model%20to%20inherit%20prior%20knowledge%20and%20only%20optimize%0Athe%20proposed%20adapter.%20Furthermore%2C%20to%20verify%20the%20effectiveness%20of%20our%0Aframework%2C%20we%20utilize%20the%20vanilla%20vision%20transformer%20%28ViT-Base%29%20as%20the%0Apre-trained%20foundation%20model%20to%20perform%20extensive%20experiments.%20Experimental%0Aresults%20on%20various%20RGB-IR%20downstream%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20state-of-the-art%20performance.%20The%20source%20code%20and%20results%20are%20available%0Aat%20https%3A//github.com/PoTsui99/UniRGB-IR.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniRGB-IR%253A%2520A%2520Unified%2520Framework%2520for%2520RGB-Infrared%2520Semantic%2520Tasks%2520via%250A%2520%2520Adapter%2520Tuning%26entry.906535625%3DMaoxun%2520Yuan%2520and%2520Bo%2520Cui%2520and%2520Tianyi%2520Zhao%2520and%2520Jiayi%2520Wang%2520and%2520Shan%2520Fu%2520and%2520Xingxing%2520Wei%26entry.1292438233%3D%2520%2520Semantic%2520analysis%2520on%2520visible%2520%2528RGB%2529%2520and%2520infrared%2520%2528IR%2529%2520images%2520has%2520gained%250Aattention%2520for%2520its%2520ability%2520to%2520be%2520more%2520accurate%2520and%2520robust%2520under%2520low-illumination%250Aand%2520complex%2520weather%2520conditions.%2520Due%2520to%2520the%2520lack%2520of%2520pre-trained%2520foundation%250Amodels%2520on%2520the%2520large-scale%2520infrared%2520image%2520datasets%252C%2520existing%2520methods%2520prefer%2520to%250Adesign%2520task-specific%2520frameworks%2520and%2520directly%2520fine-tune%2520them%2520with%2520pre-trained%250Afoundation%2520models%2520on%2520their%2520RGB-IR%2520semantic%2520relevance%2520datasets%252C%2520which%2520results%2520in%250Apoor%2520scalability%2520and%2520limited%2520generalization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%250Aand%2520efficient%2520framework%2520called%2520UniRGB-IR%2520to%2520unify%2520RGB-IR%2520semantic%2520tasks%252C%2520in%250Awhich%2520a%2520novel%2520adapter%2520is%2520developed%2520to%2520efficiently%2520introduce%2520richer%2520RGB-IR%250Afeatures%2520into%2520the%2520pre-trained%2520RGB-based%2520foundation%2520model.%2520Specifically%252C%2520our%250Aframework%2520consists%2520of%2520a%2520RGB-based%2520foundation%2520model%252C%2520a%2520Multi-modal%2520Feature%2520Pool%250A%2528MFP%2529%2520module%2520and%2520a%2520Supplementary%2520Feature%2520Injector%2520%2528SFI%2529%2520module.%2520The%2520MFP%2520and%2520SFI%250Amodules%2520cooperate%2520with%2520each%2520other%2520as%2520an%2520adapter%2520to%2520effectively%2520complement%2520the%250ARGB-based%2520features%2520with%2520the%2520rich%2520RGB-IR%2520features.%2520During%2520training%2520process%252C%2520we%250Afreeze%2520the%2520entire%2520foundation%2520model%2520to%2520inherit%2520prior%2520knowledge%2520and%2520only%2520optimize%250Athe%2520proposed%2520adapter.%2520Furthermore%252C%2520to%2520verify%2520the%2520effectiveness%2520of%2520our%250Aframework%252C%2520we%2520utilize%2520the%2520vanilla%2520vision%2520transformer%2520%2528ViT-Base%2529%2520as%2520the%250Apre-trained%2520foundation%2520model%2520to%2520perform%2520extensive%2520experiments.%2520Experimental%250Aresults%2520on%2520various%2520RGB-IR%2520downstream%2520tasks%2520demonstrate%2520that%2520our%2520method%2520can%250Aachieve%2520state-of-the-art%2520performance.%2520The%2520source%2520code%2520and%2520results%2520are%2520available%250Aat%2520https%253A//github.com/PoTsui99/UniRGB-IR.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniRGB-IR%3A%20A%20Unified%20Framework%20for%20RGB-Infrared%20Semantic%20Tasks%20via%0A%20%20Adapter%20Tuning&entry.906535625=Maoxun%20Yuan%20and%20Bo%20Cui%20and%20Tianyi%20Zhao%20and%20Jiayi%20Wang%20and%20Shan%20Fu%20and%20Xingxing%20Wei&entry.1292438233=%20%20Semantic%20analysis%20on%20visible%20%28RGB%29%20and%20infrared%20%28IR%29%20images%20has%20gained%0Aattention%20for%20its%20ability%20to%20be%20more%20accurate%20and%20robust%20under%20low-illumination%0Aand%20complex%20weather%20conditions.%20Due%20to%20the%20lack%20of%20pre-trained%20foundation%0Amodels%20on%20the%20large-scale%20infrared%20image%20datasets%2C%20existing%20methods%20prefer%20to%0Adesign%20task-specific%20frameworks%20and%20directly%20fine-tune%20them%20with%20pre-trained%0Afoundation%20models%20on%20their%20RGB-IR%20semantic%20relevance%20datasets%2C%20which%20results%20in%0Apoor%20scalability%20and%20limited%20generalization.%20In%20this%20work%2C%20we%20propose%20a%20general%0Aand%20efficient%20framework%20called%20UniRGB-IR%20to%20unify%20RGB-IR%20semantic%20tasks%2C%20in%0Awhich%20a%20novel%20adapter%20is%20developed%20to%20efficiently%20introduce%20richer%20RGB-IR%0Afeatures%20into%20the%20pre-trained%20RGB-based%20foundation%20model.%20Specifically%2C%20our%0Aframework%20consists%20of%20a%20RGB-based%20foundation%20model%2C%20a%20Multi-modal%20Feature%20Pool%0A%28MFP%29%20module%20and%20a%20Supplementary%20Feature%20Injector%20%28SFI%29%20module.%20The%20MFP%20and%20SFI%0Amodules%20cooperate%20with%20each%20other%20as%20an%20adapter%20to%20effectively%20complement%20the%0ARGB-based%20features%20with%20the%20rich%20RGB-IR%20features.%20During%20training%20process%2C%20we%0Afreeze%20the%20entire%20foundation%20model%20to%20inherit%20prior%20knowledge%20and%20only%20optimize%0Athe%20proposed%20adapter.%20Furthermore%2C%20to%20verify%20the%20effectiveness%20of%20our%0Aframework%2C%20we%20utilize%20the%20vanilla%20vision%20transformer%20%28ViT-Base%29%20as%20the%0Apre-trained%20foundation%20model%20to%20perform%20extensive%20experiments.%20Experimental%0Aresults%20on%20various%20RGB-IR%20downstream%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20state-of-the-art%20performance.%20The%20source%20code%20and%20results%20are%20available%0Aat%20https%3A//github.com/PoTsui99/UniRGB-IR.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17360v2&entry.124074799=Read"},
{"title": "BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval", "author": "Imanol Miranda and Ander Salaberria and Eneko Agirre and Gorka Azkune", "abstract": "  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work, we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts significantly improves over the base\nmodel in SugarCrepe and in BiVLC for both retrieval directions. The gap to\nhuman performance in BiVLC confirms that Vision-Language Compositionality is\nstill a challenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n", "link": "http://arxiv.org/abs/2406.09952v2", "date": "2024-11-04", "relevancy": 2.8479, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval&body=Title%3A%20BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval%0AAuthor%3A%20Imanol%20Miranda%20and%20Ander%20Salaberria%20and%20Eneko%20Agirre%20and%20Gorka%20Azkune%0AAbstract%3A%20%20%20Existing%20Vision-Language%20Compositionality%20%28VLC%29%20benchmarks%20like%20SugarCrepe%0Aare%20formulated%20as%20image-to-text%20retrieval%20problems%2C%20where%2C%20given%20an%20image%2C%20the%0Amodels%20need%20to%20select%20between%20the%20correct%20textual%20description%20and%20a%20synthetic%0Ahard%20negative%20text.%20In%20this%20work%2C%20we%20present%20the%20Bidirectional%20Vision-Language%0ACompositionality%20%28BiVLC%29%20dataset.%20The%20novelty%20of%20BiVLC%20is%20to%20add%20a%20synthetic%0Ahard%20negative%20image%20generated%20from%20the%20synthetic%20text%2C%20resulting%20in%20two%0Aimage-to-text%20retrieval%20examples%20%28one%20for%20each%20image%29%20and%2C%20more%20importantly%2C%0Atwo%20text-to-image%20retrieval%20examples%20%28one%20for%20each%20text%29.%20Human%20annotators%0Afilter%20out%20ill-formed%20examples%20ensuring%20the%20validity%20of%20the%20benchmark.%20The%0Aexperiments%20on%20BiVLC%20uncover%20a%20weakness%20of%20current%20multimodal%20models%2C%20as%20they%0Aperform%20poorly%20in%20the%20text-to-image%20direction.%20In%20fact%2C%20when%20considering%20both%0Aretrieval%20directions%2C%20the%20conclusions%20obtained%20in%20previous%20works%20change%0Asignificantly.%20In%20addition%20to%20the%20benchmark%2C%20we%20show%20that%20a%20contrastive%20model%0Atrained%20using%20synthetic%20images%20and%20texts%20significantly%20improves%20over%20the%20base%0Amodel%20in%20SugarCrepe%20and%20in%20BiVLC%20for%20both%20retrieval%20directions.%20The%20gap%20to%0Ahuman%20performance%20in%20BiVLC%20confirms%20that%20Vision-Language%20Compositionality%20is%0Astill%20a%20challenging%20problem.%20BiVLC%20and%20code%20are%20available%20at%0Ahttps%3A//imirandam.github.io/BiVLC_project_page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiVLC%253A%2520Extending%2520Vision-Language%2520Compositionality%2520Evaluation%2520with%250A%2520%2520Text-to-Image%2520Retrieval%26entry.906535625%3DImanol%2520Miranda%2520and%2520Ander%2520Salaberria%2520and%2520Eneko%2520Agirre%2520and%2520Gorka%2520Azkune%26entry.1292438233%3D%2520%2520Existing%2520Vision-Language%2520Compositionality%2520%2528VLC%2529%2520benchmarks%2520like%2520SugarCrepe%250Aare%2520formulated%2520as%2520image-to-text%2520retrieval%2520problems%252C%2520where%252C%2520given%2520an%2520image%252C%2520the%250Amodels%2520need%2520to%2520select%2520between%2520the%2520correct%2520textual%2520description%2520and%2520a%2520synthetic%250Ahard%2520negative%2520text.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520Bidirectional%2520Vision-Language%250ACompositionality%2520%2528BiVLC%2529%2520dataset.%2520The%2520novelty%2520of%2520BiVLC%2520is%2520to%2520add%2520a%2520synthetic%250Ahard%2520negative%2520image%2520generated%2520from%2520the%2520synthetic%2520text%252C%2520resulting%2520in%2520two%250Aimage-to-text%2520retrieval%2520examples%2520%2528one%2520for%2520each%2520image%2529%2520and%252C%2520more%2520importantly%252C%250Atwo%2520text-to-image%2520retrieval%2520examples%2520%2528one%2520for%2520each%2520text%2529.%2520Human%2520annotators%250Afilter%2520out%2520ill-formed%2520examples%2520ensuring%2520the%2520validity%2520of%2520the%2520benchmark.%2520The%250Aexperiments%2520on%2520BiVLC%2520uncover%2520a%2520weakness%2520of%2520current%2520multimodal%2520models%252C%2520as%2520they%250Aperform%2520poorly%2520in%2520the%2520text-to-image%2520direction.%2520In%2520fact%252C%2520when%2520considering%2520both%250Aretrieval%2520directions%252C%2520the%2520conclusions%2520obtained%2520in%2520previous%2520works%2520change%250Asignificantly.%2520In%2520addition%2520to%2520the%2520benchmark%252C%2520we%2520show%2520that%2520a%2520contrastive%2520model%250Atrained%2520using%2520synthetic%2520images%2520and%2520texts%2520significantly%2520improves%2520over%2520the%2520base%250Amodel%2520in%2520SugarCrepe%2520and%2520in%2520BiVLC%2520for%2520both%2520retrieval%2520directions.%2520The%2520gap%2520to%250Ahuman%2520performance%2520in%2520BiVLC%2520confirms%2520that%2520Vision-Language%2520Compositionality%2520is%250Astill%2520a%2520challenging%2520problem.%2520BiVLC%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//imirandam.github.io/BiVLC_project_page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval&entry.906535625=Imanol%20Miranda%20and%20Ander%20Salaberria%20and%20Eneko%20Agirre%20and%20Gorka%20Azkune&entry.1292438233=%20%20Existing%20Vision-Language%20Compositionality%20%28VLC%29%20benchmarks%20like%20SugarCrepe%0Aare%20formulated%20as%20image-to-text%20retrieval%20problems%2C%20where%2C%20given%20an%20image%2C%20the%0Amodels%20need%20to%20select%20between%20the%20correct%20textual%20description%20and%20a%20synthetic%0Ahard%20negative%20text.%20In%20this%20work%2C%20we%20present%20the%20Bidirectional%20Vision-Language%0ACompositionality%20%28BiVLC%29%20dataset.%20The%20novelty%20of%20BiVLC%20is%20to%20add%20a%20synthetic%0Ahard%20negative%20image%20generated%20from%20the%20synthetic%20text%2C%20resulting%20in%20two%0Aimage-to-text%20retrieval%20examples%20%28one%20for%20each%20image%29%20and%2C%20more%20importantly%2C%0Atwo%20text-to-image%20retrieval%20examples%20%28one%20for%20each%20text%29.%20Human%20annotators%0Afilter%20out%20ill-formed%20examples%20ensuring%20the%20validity%20of%20the%20benchmark.%20The%0Aexperiments%20on%20BiVLC%20uncover%20a%20weakness%20of%20current%20multimodal%20models%2C%20as%20they%0Aperform%20poorly%20in%20the%20text-to-image%20direction.%20In%20fact%2C%20when%20considering%20both%0Aretrieval%20directions%2C%20the%20conclusions%20obtained%20in%20previous%20works%20change%0Asignificantly.%20In%20addition%20to%20the%20benchmark%2C%20we%20show%20that%20a%20contrastive%20model%0Atrained%20using%20synthetic%20images%20and%20texts%20significantly%20improves%20over%20the%20base%0Amodel%20in%20SugarCrepe%20and%20in%20BiVLC%20for%20both%20retrieval%20directions.%20The%20gap%20to%0Ahuman%20performance%20in%20BiVLC%20confirms%20that%20Vision-Language%20Compositionality%20is%0Astill%20a%20challenging%20problem.%20BiVLC%20and%20code%20are%20available%20at%0Ahttps%3A//imirandam.github.io/BiVLC_project_page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09952v2&entry.124074799=Read"},
{"title": "Visual Self-supervised Learning Scheme for Dense Prediction Tasks on\n  X-ray Images", "author": "Shervin Halat and Mohammad Rahmati and Ehsan Nazerfard", "abstract": "  Recently, significant advancements in artificial intelligence have been\nattributed to the integration of self-supervised learning (SSL) scheme. While\nSSL has shown impressive achievements in natural language processing (NLP), its\nprogress in computer vision has comparatively lagged behind. However, the\nincorporation of contrastive learning into existing visual SSL models has led\nto considerable progress, often surpassing supervised counterparts.\nNonetheless, these improvements have been mostly limited to classification\ntasks. Moreover, few studies have evaluated visual SSL models in real-world\nscenarios, as most have focused on datasets with class-wise portrait images,\nnotably ImageNet. Here, we focus on dense prediction tasks using security\ninspection x-ray images to evaluate our proposed model, Segment Localization\n(SegLoc). Based upon the Instance Localization (InsLoc) model, SegLoc addresses\none of the key challenges of contrastive learning, i.e., false negative pairs\nof query embeddings. Our pre-training dataset is synthesized by cutting,\ntransforming, and pasting labeled segments from an existing labeled dataset\n(PIDray) as foregrounds onto instances from an unlabeled dataset (SIXray) as\nbackgrounds. Furthermore, we fully leverage the labeled data by incorporating\nthe concept, one queue per class, into the MoCo-v2 memory bank, thereby\navoiding false negative pairs. In our experiments, SegLoc outperformed random\ninitialization by 3% to 6% while underperformed supervised initialization, in\nterms of AR and AP metrics across different IoU values over 20 to 30\npre-training epochs.\n", "link": "http://arxiv.org/abs/2310.08421v4", "date": "2024-11-04", "relevancy": 2.8328, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5914}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Self-supervised%20Learning%20Scheme%20for%20Dense%20Prediction%20Tasks%20on%0A%20%20X-ray%20Images&body=Title%3A%20Visual%20Self-supervised%20Learning%20Scheme%20for%20Dense%20Prediction%20Tasks%20on%0A%20%20X-ray%20Images%0AAuthor%3A%20Shervin%20Halat%20and%20Mohammad%20Rahmati%20and%20Ehsan%20Nazerfard%0AAbstract%3A%20%20%20Recently%2C%20significant%20advancements%20in%20artificial%20intelligence%20have%20been%0Aattributed%20to%20the%20integration%20of%20self-supervised%20learning%20%28SSL%29%20scheme.%20While%0ASSL%20has%20shown%20impressive%20achievements%20in%20natural%20language%20processing%20%28NLP%29%2C%20its%0Aprogress%20in%20computer%20vision%20has%20comparatively%20lagged%20behind.%20However%2C%20the%0Aincorporation%20of%20contrastive%20learning%20into%20existing%20visual%20SSL%20models%20has%20led%0Ato%20considerable%20progress%2C%20often%20surpassing%20supervised%20counterparts.%0ANonetheless%2C%20these%20improvements%20have%20been%20mostly%20limited%20to%20classification%0Atasks.%20Moreover%2C%20few%20studies%20have%20evaluated%20visual%20SSL%20models%20in%20real-world%0Ascenarios%2C%20as%20most%20have%20focused%20on%20datasets%20with%20class-wise%20portrait%20images%2C%0Anotably%20ImageNet.%20Here%2C%20we%20focus%20on%20dense%20prediction%20tasks%20using%20security%0Ainspection%20x-ray%20images%20to%20evaluate%20our%20proposed%20model%2C%20Segment%20Localization%0A%28SegLoc%29.%20Based%20upon%20the%20Instance%20Localization%20%28InsLoc%29%20model%2C%20SegLoc%20addresses%0Aone%20of%20the%20key%20challenges%20of%20contrastive%20learning%2C%20i.e.%2C%20false%20negative%20pairs%0Aof%20query%20embeddings.%20Our%20pre-training%20dataset%20is%20synthesized%20by%20cutting%2C%0Atransforming%2C%20and%20pasting%20labeled%20segments%20from%20an%20existing%20labeled%20dataset%0A%28PIDray%29%20as%20foregrounds%20onto%20instances%20from%20an%20unlabeled%20dataset%20%28SIXray%29%20as%0Abackgrounds.%20Furthermore%2C%20we%20fully%20leverage%20the%20labeled%20data%20by%20incorporating%0Athe%20concept%2C%20one%20queue%20per%20class%2C%20into%20the%20MoCo-v2%20memory%20bank%2C%20thereby%0Aavoiding%20false%20negative%20pairs.%20In%20our%20experiments%2C%20SegLoc%20outperformed%20random%0Ainitialization%20by%203%25%20to%206%25%20while%20underperformed%20supervised%20initialization%2C%20in%0Aterms%20of%20AR%20and%20AP%20metrics%20across%20different%20IoU%20values%20over%2020%20to%2030%0Apre-training%20epochs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08421v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Self-supervised%2520Learning%2520Scheme%2520for%2520Dense%2520Prediction%2520Tasks%2520on%250A%2520%2520X-ray%2520Images%26entry.906535625%3DShervin%2520Halat%2520and%2520Mohammad%2520Rahmati%2520and%2520Ehsan%2520Nazerfard%26entry.1292438233%3D%2520%2520Recently%252C%2520significant%2520advancements%2520in%2520artificial%2520intelligence%2520have%2520been%250Aattributed%2520to%2520the%2520integration%2520of%2520self-supervised%2520learning%2520%2528SSL%2529%2520scheme.%2520While%250ASSL%2520has%2520shown%2520impressive%2520achievements%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520its%250Aprogress%2520in%2520computer%2520vision%2520has%2520comparatively%2520lagged%2520behind.%2520However%252C%2520the%250Aincorporation%2520of%2520contrastive%2520learning%2520into%2520existing%2520visual%2520SSL%2520models%2520has%2520led%250Ato%2520considerable%2520progress%252C%2520often%2520surpassing%2520supervised%2520counterparts.%250ANonetheless%252C%2520these%2520improvements%2520have%2520been%2520mostly%2520limited%2520to%2520classification%250Atasks.%2520Moreover%252C%2520few%2520studies%2520have%2520evaluated%2520visual%2520SSL%2520models%2520in%2520real-world%250Ascenarios%252C%2520as%2520most%2520have%2520focused%2520on%2520datasets%2520with%2520class-wise%2520portrait%2520images%252C%250Anotably%2520ImageNet.%2520Here%252C%2520we%2520focus%2520on%2520dense%2520prediction%2520tasks%2520using%2520security%250Ainspection%2520x-ray%2520images%2520to%2520evaluate%2520our%2520proposed%2520model%252C%2520Segment%2520Localization%250A%2528SegLoc%2529.%2520Based%2520upon%2520the%2520Instance%2520Localization%2520%2528InsLoc%2529%2520model%252C%2520SegLoc%2520addresses%250Aone%2520of%2520the%2520key%2520challenges%2520of%2520contrastive%2520learning%252C%2520i.e.%252C%2520false%2520negative%2520pairs%250Aof%2520query%2520embeddings.%2520Our%2520pre-training%2520dataset%2520is%2520synthesized%2520by%2520cutting%252C%250Atransforming%252C%2520and%2520pasting%2520labeled%2520segments%2520from%2520an%2520existing%2520labeled%2520dataset%250A%2528PIDray%2529%2520as%2520foregrounds%2520onto%2520instances%2520from%2520an%2520unlabeled%2520dataset%2520%2528SIXray%2529%2520as%250Abackgrounds.%2520Furthermore%252C%2520we%2520fully%2520leverage%2520the%2520labeled%2520data%2520by%2520incorporating%250Athe%2520concept%252C%2520one%2520queue%2520per%2520class%252C%2520into%2520the%2520MoCo-v2%2520memory%2520bank%252C%2520thereby%250Aavoiding%2520false%2520negative%2520pairs.%2520In%2520our%2520experiments%252C%2520SegLoc%2520outperformed%2520random%250Ainitialization%2520by%25203%2525%2520to%25206%2525%2520while%2520underperformed%2520supervised%2520initialization%252C%2520in%250Aterms%2520of%2520AR%2520and%2520AP%2520metrics%2520across%2520different%2520IoU%2520values%2520over%252020%2520to%252030%250Apre-training%2520epochs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08421v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Self-supervised%20Learning%20Scheme%20for%20Dense%20Prediction%20Tasks%20on%0A%20%20X-ray%20Images&entry.906535625=Shervin%20Halat%20and%20Mohammad%20Rahmati%20and%20Ehsan%20Nazerfard&entry.1292438233=%20%20Recently%2C%20significant%20advancements%20in%20artificial%20intelligence%20have%20been%0Aattributed%20to%20the%20integration%20of%20self-supervised%20learning%20%28SSL%29%20scheme.%20While%0ASSL%20has%20shown%20impressive%20achievements%20in%20natural%20language%20processing%20%28NLP%29%2C%20its%0Aprogress%20in%20computer%20vision%20has%20comparatively%20lagged%20behind.%20However%2C%20the%0Aincorporation%20of%20contrastive%20learning%20into%20existing%20visual%20SSL%20models%20has%20led%0Ato%20considerable%20progress%2C%20often%20surpassing%20supervised%20counterparts.%0ANonetheless%2C%20these%20improvements%20have%20been%20mostly%20limited%20to%20classification%0Atasks.%20Moreover%2C%20few%20studies%20have%20evaluated%20visual%20SSL%20models%20in%20real-world%0Ascenarios%2C%20as%20most%20have%20focused%20on%20datasets%20with%20class-wise%20portrait%20images%2C%0Anotably%20ImageNet.%20Here%2C%20we%20focus%20on%20dense%20prediction%20tasks%20using%20security%0Ainspection%20x-ray%20images%20to%20evaluate%20our%20proposed%20model%2C%20Segment%20Localization%0A%28SegLoc%29.%20Based%20upon%20the%20Instance%20Localization%20%28InsLoc%29%20model%2C%20SegLoc%20addresses%0Aone%20of%20the%20key%20challenges%20of%20contrastive%20learning%2C%20i.e.%2C%20false%20negative%20pairs%0Aof%20query%20embeddings.%20Our%20pre-training%20dataset%20is%20synthesized%20by%20cutting%2C%0Atransforming%2C%20and%20pasting%20labeled%20segments%20from%20an%20existing%20labeled%20dataset%0A%28PIDray%29%20as%20foregrounds%20onto%20instances%20from%20an%20unlabeled%20dataset%20%28SIXray%29%20as%0Abackgrounds.%20Furthermore%2C%20we%20fully%20leverage%20the%20labeled%20data%20by%20incorporating%0Athe%20concept%2C%20one%20queue%20per%20class%2C%20into%20the%20MoCo-v2%20memory%20bank%2C%20thereby%0Aavoiding%20false%20negative%20pairs.%20In%20our%20experiments%2C%20SegLoc%20outperformed%20random%0Ainitialization%20by%203%25%20to%206%25%20while%20underperformed%20supervised%20initialization%2C%20in%0Aterms%20of%20AR%20and%20AP%20metrics%20across%20different%20IoU%20values%20over%2020%20to%2030%0Apre-training%20epochs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08421v4&entry.124074799=Read"},
{"title": "Deep Learning on 3D Semantic Segmentation: A Detailed Review", "author": "Thodoris Betsas and Andreas Georgopoulos and Anastasios Doulamis and Pierre Grussenmeyer", "abstract": "  In this paper an exhaustive review and comprehensive analysis of recent and\nformer deep learning methods in 3D Semantic Segmentation (3DSS) is presented.\nIn the related literature, the taxonomy scheme used for the classification of\nthe 3DSS deep learning methods is ambiguous. Based on the taxonomy schemes of 9\nexisting review papers, a new taxonomy scheme of the 3DSS deep learning methods\nis proposed, aiming to standardize it and improve the comparability and clarity\nacross related studies. Furthermore, an extensive overview of the available\n3DSS indoor and outdoor datasets is provided along with their links. The core\npart of the review is the detailed presentation of recent and former 3DSS deep\nlearning methods and their classification using the proposed taxonomy scheme\nalong with their GitHub repositories. Additionally, a brief but informative\nanalysis of the evaluation metrics and loss functions used in 3DSS is included.\nFinally, a fruitful discussion of the examined 3DSS methods and datasets, is\npresented to foster new research directions and applications in the field of\n3DSS. Supplementary, to this review a GitHub repository is provided\n(https://github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-\nDetailed-Review) including a quick classification of over 400 3DSS methods,\nusing the proposed taxonomy scheme.\n", "link": "http://arxiv.org/abs/2411.02104v1", "date": "2024-11-04", "relevancy": 2.8303, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20on%203D%20Semantic%20Segmentation%3A%20A%20Detailed%20Review&body=Title%3A%20Deep%20Learning%20on%203D%20Semantic%20Segmentation%3A%20A%20Detailed%20Review%0AAuthor%3A%20Thodoris%20Betsas%20and%20Andreas%20Georgopoulos%20and%20Anastasios%20Doulamis%20and%20Pierre%20Grussenmeyer%0AAbstract%3A%20%20%20In%20this%20paper%20an%20exhaustive%20review%20and%20comprehensive%20analysis%20of%20recent%20and%0Aformer%20deep%20learning%20methods%20in%203D%20Semantic%20Segmentation%20%283DSS%29%20is%20presented.%0AIn%20the%20related%20literature%2C%20the%20taxonomy%20scheme%20used%20for%20the%20classification%20of%0Athe%203DSS%20deep%20learning%20methods%20is%20ambiguous.%20Based%20on%20the%20taxonomy%20schemes%20of%209%0Aexisting%20review%20papers%2C%20a%20new%20taxonomy%20scheme%20of%20the%203DSS%20deep%20learning%20methods%0Ais%20proposed%2C%20aiming%20to%20standardize%20it%20and%20improve%20the%20comparability%20and%20clarity%0Aacross%20related%20studies.%20Furthermore%2C%20an%20extensive%20overview%20of%20the%20available%0A3DSS%20indoor%20and%20outdoor%20datasets%20is%20provided%20along%20with%20their%20links.%20The%20core%0Apart%20of%20the%20review%20is%20the%20detailed%20presentation%20of%20recent%20and%20former%203DSS%20deep%0Alearning%20methods%20and%20their%20classification%20using%20the%20proposed%20taxonomy%20scheme%0Aalong%20with%20their%20GitHub%20repositories.%20Additionally%2C%20a%20brief%20but%20informative%0Aanalysis%20of%20the%20evaluation%20metrics%20and%20loss%20functions%20used%20in%203DSS%20is%20included.%0AFinally%2C%20a%20fruitful%20discussion%20of%20the%20examined%203DSS%20methods%20and%20datasets%2C%20is%0Apresented%20to%20foster%20new%20research%20directions%20and%20applications%20in%20the%20field%20of%0A3DSS.%20Supplementary%2C%20to%20this%20review%20a%20GitHub%20repository%20is%20provided%0A%28https%3A//github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-%0ADetailed-Review%29%20including%20a%20quick%20classification%20of%20over%20400%203DSS%20methods%2C%0Ausing%20the%20proposed%20taxonomy%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520on%25203D%2520Semantic%2520Segmentation%253A%2520A%2520Detailed%2520Review%26entry.906535625%3DThodoris%2520Betsas%2520and%2520Andreas%2520Georgopoulos%2520and%2520Anastasios%2520Doulamis%2520and%2520Pierre%2520Grussenmeyer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520an%2520exhaustive%2520review%2520and%2520comprehensive%2520analysis%2520of%2520recent%2520and%250Aformer%2520deep%2520learning%2520methods%2520in%25203D%2520Semantic%2520Segmentation%2520%25283DSS%2529%2520is%2520presented.%250AIn%2520the%2520related%2520literature%252C%2520the%2520taxonomy%2520scheme%2520used%2520for%2520the%2520classification%2520of%250Athe%25203DSS%2520deep%2520learning%2520methods%2520is%2520ambiguous.%2520Based%2520on%2520the%2520taxonomy%2520schemes%2520of%25209%250Aexisting%2520review%2520papers%252C%2520a%2520new%2520taxonomy%2520scheme%2520of%2520the%25203DSS%2520deep%2520learning%2520methods%250Ais%2520proposed%252C%2520aiming%2520to%2520standardize%2520it%2520and%2520improve%2520the%2520comparability%2520and%2520clarity%250Aacross%2520related%2520studies.%2520Furthermore%252C%2520an%2520extensive%2520overview%2520of%2520the%2520available%250A3DSS%2520indoor%2520and%2520outdoor%2520datasets%2520is%2520provided%2520along%2520with%2520their%2520links.%2520The%2520core%250Apart%2520of%2520the%2520review%2520is%2520the%2520detailed%2520presentation%2520of%2520recent%2520and%2520former%25203DSS%2520deep%250Alearning%2520methods%2520and%2520their%2520classification%2520using%2520the%2520proposed%2520taxonomy%2520scheme%250Aalong%2520with%2520their%2520GitHub%2520repositories.%2520Additionally%252C%2520a%2520brief%2520but%2520informative%250Aanalysis%2520of%2520the%2520evaluation%2520metrics%2520and%2520loss%2520functions%2520used%2520in%25203DSS%2520is%2520included.%250AFinally%252C%2520a%2520fruitful%2520discussion%2520of%2520the%2520examined%25203DSS%2520methods%2520and%2520datasets%252C%2520is%250Apresented%2520to%2520foster%2520new%2520research%2520directions%2520and%2520applications%2520in%2520the%2520field%2520of%250A3DSS.%2520Supplementary%252C%2520to%2520this%2520review%2520a%2520GitHub%2520repository%2520is%2520provided%250A%2528https%253A//github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-%250ADetailed-Review%2529%2520including%2520a%2520quick%2520classification%2520of%2520over%2520400%25203DSS%2520methods%252C%250Ausing%2520the%2520proposed%2520taxonomy%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20on%203D%20Semantic%20Segmentation%3A%20A%20Detailed%20Review&entry.906535625=Thodoris%20Betsas%20and%20Andreas%20Georgopoulos%20and%20Anastasios%20Doulamis%20and%20Pierre%20Grussenmeyer&entry.1292438233=%20%20In%20this%20paper%20an%20exhaustive%20review%20and%20comprehensive%20analysis%20of%20recent%20and%0Aformer%20deep%20learning%20methods%20in%203D%20Semantic%20Segmentation%20%283DSS%29%20is%20presented.%0AIn%20the%20related%20literature%2C%20the%20taxonomy%20scheme%20used%20for%20the%20classification%20of%0Athe%203DSS%20deep%20learning%20methods%20is%20ambiguous.%20Based%20on%20the%20taxonomy%20schemes%20of%209%0Aexisting%20review%20papers%2C%20a%20new%20taxonomy%20scheme%20of%20the%203DSS%20deep%20learning%20methods%0Ais%20proposed%2C%20aiming%20to%20standardize%20it%20and%20improve%20the%20comparability%20and%20clarity%0Aacross%20related%20studies.%20Furthermore%2C%20an%20extensive%20overview%20of%20the%20available%0A3DSS%20indoor%20and%20outdoor%20datasets%20is%20provided%20along%20with%20their%20links.%20The%20core%0Apart%20of%20the%20review%20is%20the%20detailed%20presentation%20of%20recent%20and%20former%203DSS%20deep%0Alearning%20methods%20and%20their%20classification%20using%20the%20proposed%20taxonomy%20scheme%0Aalong%20with%20their%20GitHub%20repositories.%20Additionally%2C%20a%20brief%20but%20informative%0Aanalysis%20of%20the%20evaluation%20metrics%20and%20loss%20functions%20used%20in%203DSS%20is%20included.%0AFinally%2C%20a%20fruitful%20discussion%20of%20the%20examined%203DSS%20methods%20and%20datasets%2C%20is%0Apresented%20to%20foster%20new%20research%20directions%20and%20applications%20in%20the%20field%20of%0A3DSS.%20Supplementary%2C%20to%20this%20review%20a%20GitHub%20repository%20is%20provided%0A%28https%3A//github.com/thobet/Deep-Learning-on-3D-Semantic-Segmentation-a-%0ADetailed-Review%29%20including%20a%20quick%20classification%20of%20over%20400%203DSS%20methods%2C%0Ausing%20the%20proposed%20taxonomy%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02104v1&entry.124074799=Read"},
{"title": "The evolution of volumetric video: A survey of smart transcoding and\n  compression approaches", "author": "Preetish Kakkar and Hariharan Ragothaman", "abstract": "  Volumetric video, the capture and display of three-dimensional (3D) imagery,\nhas emerged as a revolutionary technology poised to transform the media\nlandscape, enabling immersive experiences that transcend the limitations of\ntraditional 2D video. One of the key challenges in this domain is the efficient\ndelivery of these high-bandwidth, data-intensive volumetric video streams,\nwhich requires innovative transcoding and compression techniques. This research\npaper explores the state-of-the-art in volumetric video compression and\ndelivery, with a focus on the potential of AI-driven solutions to address the\nunique challenges posed by this emerging medium.\n", "link": "http://arxiv.org/abs/2411.02095v1", "date": "2024-11-04", "relevancy": 2.7983, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5687}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20evolution%20of%20volumetric%20video%3A%20A%20survey%20of%20smart%20transcoding%20and%0A%20%20compression%20approaches&body=Title%3A%20The%20evolution%20of%20volumetric%20video%3A%20A%20survey%20of%20smart%20transcoding%20and%0A%20%20compression%20approaches%0AAuthor%3A%20Preetish%20Kakkar%20and%20Hariharan%20Ragothaman%0AAbstract%3A%20%20%20Volumetric%20video%2C%20the%20capture%20and%20display%20of%20three-dimensional%20%283D%29%20imagery%2C%0Ahas%20emerged%20as%20a%20revolutionary%20technology%20poised%20to%20transform%20the%20media%0Alandscape%2C%20enabling%20immersive%20experiences%20that%20transcend%20the%20limitations%20of%0Atraditional%202D%20video.%20One%20of%20the%20key%20challenges%20in%20this%20domain%20is%20the%20efficient%0Adelivery%20of%20these%20high-bandwidth%2C%20data-intensive%20volumetric%20video%20streams%2C%0Awhich%20requires%20innovative%20transcoding%20and%20compression%20techniques.%20This%20research%0Apaper%20explores%20the%20state-of-the-art%20in%20volumetric%20video%20compression%20and%0Adelivery%2C%20with%20a%20focus%20on%20the%20potential%20of%20AI-driven%20solutions%20to%20address%20the%0Aunique%20challenges%20posed%20by%20this%20emerging%20medium.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520evolution%2520of%2520volumetric%2520video%253A%2520A%2520survey%2520of%2520smart%2520transcoding%2520and%250A%2520%2520compression%2520approaches%26entry.906535625%3DPreetish%2520Kakkar%2520and%2520Hariharan%2520Ragothaman%26entry.1292438233%3D%2520%2520Volumetric%2520video%252C%2520the%2520capture%2520and%2520display%2520of%2520three-dimensional%2520%25283D%2529%2520imagery%252C%250Ahas%2520emerged%2520as%2520a%2520revolutionary%2520technology%2520poised%2520to%2520transform%2520the%2520media%250Alandscape%252C%2520enabling%2520immersive%2520experiences%2520that%2520transcend%2520the%2520limitations%2520of%250Atraditional%25202D%2520video.%2520One%2520of%2520the%2520key%2520challenges%2520in%2520this%2520domain%2520is%2520the%2520efficient%250Adelivery%2520of%2520these%2520high-bandwidth%252C%2520data-intensive%2520volumetric%2520video%2520streams%252C%250Awhich%2520requires%2520innovative%2520transcoding%2520and%2520compression%2520techniques.%2520This%2520research%250Apaper%2520explores%2520the%2520state-of-the-art%2520in%2520volumetric%2520video%2520compression%2520and%250Adelivery%252C%2520with%2520a%2520focus%2520on%2520the%2520potential%2520of%2520AI-driven%2520solutions%2520to%2520address%2520the%250Aunique%2520challenges%2520posed%2520by%2520this%2520emerging%2520medium.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20evolution%20of%20volumetric%20video%3A%20A%20survey%20of%20smart%20transcoding%20and%0A%20%20compression%20approaches&entry.906535625=Preetish%20Kakkar%20and%20Hariharan%20Ragothaman&entry.1292438233=%20%20Volumetric%20video%2C%20the%20capture%20and%20display%20of%20three-dimensional%20%283D%29%20imagery%2C%0Ahas%20emerged%20as%20a%20revolutionary%20technology%20poised%20to%20transform%20the%20media%0Alandscape%2C%20enabling%20immersive%20experiences%20that%20transcend%20the%20limitations%20of%0Atraditional%202D%20video.%20One%20of%20the%20key%20challenges%20in%20this%20domain%20is%20the%20efficient%0Adelivery%20of%20these%20high-bandwidth%2C%20data-intensive%20volumetric%20video%20streams%2C%0Awhich%20requires%20innovative%20transcoding%20and%20compression%20techniques.%20This%20research%0Apaper%20explores%20the%20state-of-the-art%20in%20volumetric%20video%20compression%20and%0Adelivery%2C%20with%20a%20focus%20on%20the%20potential%20of%20AI-driven%20solutions%20to%20address%20the%0Aunique%20challenges%20posed%20by%20this%20emerging%20medium.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02095v1&entry.124074799=Read"},
{"title": "Unified Speech Recognition: A Single Model for Auditory, Visual, and\n  Audiovisual Inputs", "author": "Alexandros Haliassos and Rodrigo Mira and Honglie Chen and Zoe Landgraf and Stavros Petridis and Maja Pantic", "abstract": "  Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,\nand AVSR, respectively) has traditionally been conducted independently. Even\nrecent self-supervised studies addressing two or all three tasks simultaneously\ntend to yield separate models, leading to disjoint inference pipelines with\nincreased memory requirements and redundancies. This paper proposes unified\ntraining strategies for these systems. We demonstrate that training a single\nmodel for all three tasks enhances VSR and AVSR performance, overcoming typical\noptimisation challenges when training from scratch. Moreover, we introduce a\ngreedy pseudo-labelling approach to more effectively leverage unlabelled\nsamples, addressing shortcomings in related self-supervised methods. Finally,\nwe develop a self-supervised pre-training method within our framework, proving\nits effectiveness alongside our semi-supervised approach. Despite using a\nsingle model for all tasks, our unified approach achieves state-of-the-art\nperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,\nas well as on the newly released WildVSR dataset. Code and models are available\nat https://github.com/ahaliassos/usr.\n", "link": "http://arxiv.org/abs/2411.02256v1", "date": "2024-11-04", "relevancy": 2.7694, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Speech%20Recognition%3A%20A%20Single%20Model%20for%20Auditory%2C%20Visual%2C%20and%0A%20%20Audiovisual%20Inputs&body=Title%3A%20Unified%20Speech%20Recognition%3A%20A%20Single%20Model%20for%20Auditory%2C%20Visual%2C%20and%0A%20%20Audiovisual%20Inputs%0AAuthor%3A%20Alexandros%20Haliassos%20and%20Rodrigo%20Mira%20and%20Honglie%20Chen%20and%20Zoe%20Landgraf%20and%20Stavros%20Petridis%20and%20Maja%20Pantic%0AAbstract%3A%20%20%20Research%20in%20auditory%2C%20visual%2C%20and%20audiovisual%20speech%20recognition%20%28ASR%2C%20VSR%2C%0Aand%20AVSR%2C%20respectively%29%20has%20traditionally%20been%20conducted%20independently.%20Even%0Arecent%20self-supervised%20studies%20addressing%20two%20or%20all%20three%20tasks%20simultaneously%0Atend%20to%20yield%20separate%20models%2C%20leading%20to%20disjoint%20inference%20pipelines%20with%0Aincreased%20memory%20requirements%20and%20redundancies.%20This%20paper%20proposes%20unified%0Atraining%20strategies%20for%20these%20systems.%20We%20demonstrate%20that%20training%20a%20single%0Amodel%20for%20all%20three%20tasks%20enhances%20VSR%20and%20AVSR%20performance%2C%20overcoming%20typical%0Aoptimisation%20challenges%20when%20training%20from%20scratch.%20Moreover%2C%20we%20introduce%20a%0Agreedy%20pseudo-labelling%20approach%20to%20more%20effectively%20leverage%20unlabelled%0Asamples%2C%20addressing%20shortcomings%20in%20related%20self-supervised%20methods.%20Finally%2C%0Awe%20develop%20a%20self-supervised%20pre-training%20method%20within%20our%20framework%2C%20proving%0Aits%20effectiveness%20alongside%20our%20semi-supervised%20approach.%20Despite%20using%20a%0Asingle%20model%20for%20all%20tasks%2C%20our%20unified%20approach%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20recent%20methods%20on%20LRS3%20and%20LRS2%20for%20ASR%2C%20VSR%2C%20and%20AVSR%2C%0Aas%20well%20as%20on%20the%20newly%20released%20WildVSR%20dataset.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/ahaliassos/usr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Speech%2520Recognition%253A%2520A%2520Single%2520Model%2520for%2520Auditory%252C%2520Visual%252C%2520and%250A%2520%2520Audiovisual%2520Inputs%26entry.906535625%3DAlexandros%2520Haliassos%2520and%2520Rodrigo%2520Mira%2520and%2520Honglie%2520Chen%2520and%2520Zoe%2520Landgraf%2520and%2520Stavros%2520Petridis%2520and%2520Maja%2520Pantic%26entry.1292438233%3D%2520%2520Research%2520in%2520auditory%252C%2520visual%252C%2520and%2520audiovisual%2520speech%2520recognition%2520%2528ASR%252C%2520VSR%252C%250Aand%2520AVSR%252C%2520respectively%2529%2520has%2520traditionally%2520been%2520conducted%2520independently.%2520Even%250Arecent%2520self-supervised%2520studies%2520addressing%2520two%2520or%2520all%2520three%2520tasks%2520simultaneously%250Atend%2520to%2520yield%2520separate%2520models%252C%2520leading%2520to%2520disjoint%2520inference%2520pipelines%2520with%250Aincreased%2520memory%2520requirements%2520and%2520redundancies.%2520This%2520paper%2520proposes%2520unified%250Atraining%2520strategies%2520for%2520these%2520systems.%2520We%2520demonstrate%2520that%2520training%2520a%2520single%250Amodel%2520for%2520all%2520three%2520tasks%2520enhances%2520VSR%2520and%2520AVSR%2520performance%252C%2520overcoming%2520typical%250Aoptimisation%2520challenges%2520when%2520training%2520from%2520scratch.%2520Moreover%252C%2520we%2520introduce%2520a%250Agreedy%2520pseudo-labelling%2520approach%2520to%2520more%2520effectively%2520leverage%2520unlabelled%250Asamples%252C%2520addressing%2520shortcomings%2520in%2520related%2520self-supervised%2520methods.%2520Finally%252C%250Awe%2520develop%2520a%2520self-supervised%2520pre-training%2520method%2520within%2520our%2520framework%252C%2520proving%250Aits%2520effectiveness%2520alongside%2520our%2520semi-supervised%2520approach.%2520Despite%2520using%2520a%250Asingle%2520model%2520for%2520all%2520tasks%252C%2520our%2520unified%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520compared%2520to%2520recent%2520methods%2520on%2520LRS3%2520and%2520LRS2%2520for%2520ASR%252C%2520VSR%252C%2520and%2520AVSR%252C%250Aas%2520well%2520as%2520on%2520the%2520newly%2520released%2520WildVSR%2520dataset.%2520Code%2520and%2520models%2520are%2520available%250Aat%2520https%253A//github.com/ahaliassos/usr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Speech%20Recognition%3A%20A%20Single%20Model%20for%20Auditory%2C%20Visual%2C%20and%0A%20%20Audiovisual%20Inputs&entry.906535625=Alexandros%20Haliassos%20and%20Rodrigo%20Mira%20and%20Honglie%20Chen%20and%20Zoe%20Landgraf%20and%20Stavros%20Petridis%20and%20Maja%20Pantic&entry.1292438233=%20%20Research%20in%20auditory%2C%20visual%2C%20and%20audiovisual%20speech%20recognition%20%28ASR%2C%20VSR%2C%0Aand%20AVSR%2C%20respectively%29%20has%20traditionally%20been%20conducted%20independently.%20Even%0Arecent%20self-supervised%20studies%20addressing%20two%20or%20all%20three%20tasks%20simultaneously%0Atend%20to%20yield%20separate%20models%2C%20leading%20to%20disjoint%20inference%20pipelines%20with%0Aincreased%20memory%20requirements%20and%20redundancies.%20This%20paper%20proposes%20unified%0Atraining%20strategies%20for%20these%20systems.%20We%20demonstrate%20that%20training%20a%20single%0Amodel%20for%20all%20three%20tasks%20enhances%20VSR%20and%20AVSR%20performance%2C%20overcoming%20typical%0Aoptimisation%20challenges%20when%20training%20from%20scratch.%20Moreover%2C%20we%20introduce%20a%0Agreedy%20pseudo-labelling%20approach%20to%20more%20effectively%20leverage%20unlabelled%0Asamples%2C%20addressing%20shortcomings%20in%20related%20self-supervised%20methods.%20Finally%2C%0Awe%20develop%20a%20self-supervised%20pre-training%20method%20within%20our%20framework%2C%20proving%0Aits%20effectiveness%20alongside%20our%20semi-supervised%20approach.%20Despite%20using%20a%0Asingle%20model%20for%20all%20tasks%2C%20our%20unified%20approach%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20recent%20methods%20on%20LRS3%20and%20LRS2%20for%20ASR%2C%20VSR%2C%20and%20AVSR%2C%0Aas%20well%20as%20on%20the%20newly%20released%20WildVSR%20dataset.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/ahaliassos/usr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02256v1&entry.124074799=Read"},
{"title": "GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for\n  Generalized Class Discovery", "author": "Bhupendra Solanki and Ashwin Nair and Mainak Singha and Souradeep Mukhopadhyay and Ankit Jha and Biplab Banerjee", "abstract": "  Generalized Category Discovery (GCD) aims to cluster unlabeled images into\nknown and novel categories using labeled images from known classes. To address\nthe challenge of transferring features from known to unknown classes while\nmitigating model bias, we introduce GraphVL, a novel approach for\nvision-language modeling in GCD, leveraging CLIP. Our method integrates a graph\nconvolutional network (GCN) with CLIP's text encoder to preserve class\nneighborhood structure. We also employ a lightweight visual projector for image\ndata, ensuring discriminative features through margin-based contrastive losses\nfor image-text mapping. This neighborhood preservation criterion effectively\nregulates the semantic space, making it less sensitive to known classes.\nAdditionally, we learn textual prompts from known classes and align them to\ncreate a more contextually meaningful semantic feature space for the GCN layer\nusing a contextual similarity loss. Finally, we represent unlabeled samples\nbased on their semantic distance to class prompts from the GCN, enabling\nsemi-supervised clustering for class discovery and minimizing errors. Our\nexperiments on seven benchmark datasets consistently demonstrate the\nsuperiority of GraphVL when integrated with the CLIP backbone.\n", "link": "http://arxiv.org/abs/2411.02074v1", "date": "2024-11-04", "relevancy": 2.7661, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphVL%3A%20Graph-Enhanced%20Semantic%20Modeling%20via%20Vision-Language%20Models%20for%0A%20%20Generalized%20Class%20Discovery&body=Title%3A%20GraphVL%3A%20Graph-Enhanced%20Semantic%20Modeling%20via%20Vision-Language%20Models%20for%0A%20%20Generalized%20Class%20Discovery%0AAuthor%3A%20Bhupendra%20Solanki%20and%20Ashwin%20Nair%20and%20Mainak%20Singha%20and%20Souradeep%20Mukhopadhyay%20and%20Ankit%20Jha%20and%20Biplab%20Banerjee%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20cluster%20unlabeled%20images%20into%0Aknown%20and%20novel%20categories%20using%20labeled%20images%20from%20known%20classes.%20To%20address%0Athe%20challenge%20of%20transferring%20features%20from%20known%20to%20unknown%20classes%20while%0Amitigating%20model%20bias%2C%20we%20introduce%20GraphVL%2C%20a%20novel%20approach%20for%0Avision-language%20modeling%20in%20GCD%2C%20leveraging%20CLIP.%20Our%20method%20integrates%20a%20graph%0Aconvolutional%20network%20%28GCN%29%20with%20CLIP%27s%20text%20encoder%20to%20preserve%20class%0Aneighborhood%20structure.%20We%20also%20employ%20a%20lightweight%20visual%20projector%20for%20image%0Adata%2C%20ensuring%20discriminative%20features%20through%20margin-based%20contrastive%20losses%0Afor%20image-text%20mapping.%20This%20neighborhood%20preservation%20criterion%20effectively%0Aregulates%20the%20semantic%20space%2C%20making%20it%20less%20sensitive%20to%20known%20classes.%0AAdditionally%2C%20we%20learn%20textual%20prompts%20from%20known%20classes%20and%20align%20them%20to%0Acreate%20a%20more%20contextually%20meaningful%20semantic%20feature%20space%20for%20the%20GCN%20layer%0Ausing%20a%20contextual%20similarity%20loss.%20Finally%2C%20we%20represent%20unlabeled%20samples%0Abased%20on%20their%20semantic%20distance%20to%20class%20prompts%20from%20the%20GCN%2C%20enabling%0Asemi-supervised%20clustering%20for%20class%20discovery%20and%20minimizing%20errors.%20Our%0Aexperiments%20on%20seven%20benchmark%20datasets%20consistently%20demonstrate%20the%0Asuperiority%20of%20GraphVL%20when%20integrated%20with%20the%20CLIP%20backbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphVL%253A%2520Graph-Enhanced%2520Semantic%2520Modeling%2520via%2520Vision-Language%2520Models%2520for%250A%2520%2520Generalized%2520Class%2520Discovery%26entry.906535625%3DBhupendra%2520Solanki%2520and%2520Ashwin%2520Nair%2520and%2520Mainak%2520Singha%2520and%2520Souradeep%2520Mukhopadhyay%2520and%2520Ankit%2520Jha%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3D%2520%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520aims%2520to%2520cluster%2520unlabeled%2520images%2520into%250Aknown%2520and%2520novel%2520categories%2520using%2520labeled%2520images%2520from%2520known%2520classes.%2520To%2520address%250Athe%2520challenge%2520of%2520transferring%2520features%2520from%2520known%2520to%2520unknown%2520classes%2520while%250Amitigating%2520model%2520bias%252C%2520we%2520introduce%2520GraphVL%252C%2520a%2520novel%2520approach%2520for%250Avision-language%2520modeling%2520in%2520GCD%252C%2520leveraging%2520CLIP.%2520Our%2520method%2520integrates%2520a%2520graph%250Aconvolutional%2520network%2520%2528GCN%2529%2520with%2520CLIP%2527s%2520text%2520encoder%2520to%2520preserve%2520class%250Aneighborhood%2520structure.%2520We%2520also%2520employ%2520a%2520lightweight%2520visual%2520projector%2520for%2520image%250Adata%252C%2520ensuring%2520discriminative%2520features%2520through%2520margin-based%2520contrastive%2520losses%250Afor%2520image-text%2520mapping.%2520This%2520neighborhood%2520preservation%2520criterion%2520effectively%250Aregulates%2520the%2520semantic%2520space%252C%2520making%2520it%2520less%2520sensitive%2520to%2520known%2520classes.%250AAdditionally%252C%2520we%2520learn%2520textual%2520prompts%2520from%2520known%2520classes%2520and%2520align%2520them%2520to%250Acreate%2520a%2520more%2520contextually%2520meaningful%2520semantic%2520feature%2520space%2520for%2520the%2520GCN%2520layer%250Ausing%2520a%2520contextual%2520similarity%2520loss.%2520Finally%252C%2520we%2520represent%2520unlabeled%2520samples%250Abased%2520on%2520their%2520semantic%2520distance%2520to%2520class%2520prompts%2520from%2520the%2520GCN%252C%2520enabling%250Asemi-supervised%2520clustering%2520for%2520class%2520discovery%2520and%2520minimizing%2520errors.%2520Our%250Aexperiments%2520on%2520seven%2520benchmark%2520datasets%2520consistently%2520demonstrate%2520the%250Asuperiority%2520of%2520GraphVL%2520when%2520integrated%2520with%2520the%2520CLIP%2520backbone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphVL%3A%20Graph-Enhanced%20Semantic%20Modeling%20via%20Vision-Language%20Models%20for%0A%20%20Generalized%20Class%20Discovery&entry.906535625=Bhupendra%20Solanki%20and%20Ashwin%20Nair%20and%20Mainak%20Singha%20and%20Souradeep%20Mukhopadhyay%20and%20Ankit%20Jha%20and%20Biplab%20Banerjee&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20aims%20to%20cluster%20unlabeled%20images%20into%0Aknown%20and%20novel%20categories%20using%20labeled%20images%20from%20known%20classes.%20To%20address%0Athe%20challenge%20of%20transferring%20features%20from%20known%20to%20unknown%20classes%20while%0Amitigating%20model%20bias%2C%20we%20introduce%20GraphVL%2C%20a%20novel%20approach%20for%0Avision-language%20modeling%20in%20GCD%2C%20leveraging%20CLIP.%20Our%20method%20integrates%20a%20graph%0Aconvolutional%20network%20%28GCN%29%20with%20CLIP%27s%20text%20encoder%20to%20preserve%20class%0Aneighborhood%20structure.%20We%20also%20employ%20a%20lightweight%20visual%20projector%20for%20image%0Adata%2C%20ensuring%20discriminative%20features%20through%20margin-based%20contrastive%20losses%0Afor%20image-text%20mapping.%20This%20neighborhood%20preservation%20criterion%20effectively%0Aregulates%20the%20semantic%20space%2C%20making%20it%20less%20sensitive%20to%20known%20classes.%0AAdditionally%2C%20we%20learn%20textual%20prompts%20from%20known%20classes%20and%20align%20them%20to%0Acreate%20a%20more%20contextually%20meaningful%20semantic%20feature%20space%20for%20the%20GCN%20layer%0Ausing%20a%20contextual%20similarity%20loss.%20Finally%2C%20we%20represent%20unlabeled%20samples%0Abased%20on%20their%20semantic%20distance%20to%20class%20prompts%20from%20the%20GCN%2C%20enabling%0Asemi-supervised%20clustering%20for%20class%20discovery%20and%20minimizing%20errors.%20Our%0Aexperiments%20on%20seven%20benchmark%20datasets%20consistently%20demonstrate%20the%0Asuperiority%20of%20GraphVL%20when%20integrated%20with%20the%20CLIP%20backbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02074v1&entry.124074799=Read"},
{"title": "SPECTRUM: Semantic Processing and Emotion-informed video-Captioning\n  Through Retrieval and Understanding Modalities", "author": "Ehsan Faghihi and Mohammedreza Zarenejad and Ali-Asghar Beheshti Shirazi", "abstract": "  Capturing a video's meaning and critical concepts by analyzing the subtle\ndetails is a fundamental yet challenging task in video captioning. Identifying\nthe dominant emotional tone in a video significantly enhances the perception of\nits context. Despite a strong emphasis on video captioning, existing models\noften need to adequately address emotional themes, resulting in suboptimal\ncaptioning results. To address these limitations, this paper proposes a novel\nSemantic Processing and Emotion-informed video-Captioning Through Retrieval and\nUnderstanding Modalities (SPECTRUM) framework to empower the generation of\nemotionally and semantically credible captions. Leveraging our pioneering\nstructure, SPECTRUM discerns multimodal semantics and emotional themes using\nVisual Text Attribute Investigation (VTAI) and determines the orientation of\ndescriptive captions through a Holistic Concept-Oriented Theme (HCOT),\nexpressing emotionally-informed and field-acquainted references. They exploit\nvideo-to-text retrieval capabilities and the multifaceted nature of video\ncontent to estimate the emotional probabilities of candidate captions. Then,\nthe dominant theme of the video is determined by appropriately weighting\nembedded attribute vectors and applying coarse- and fine-grained emotional\nconcepts, which define the video's contextual alignment. Furthermore, using two\nloss functions, SPECTRUM is optimized to integrate emotional information and\nminimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and\nMSRVTT video captioning datasets demonstrate that our model significantly\nsurpasses state-of-the-art methods. Quantitative and qualitative evaluations\nhighlight the model's ability to accurately capture and convey video emotions\nand multimodal attributes.\n", "link": "http://arxiv.org/abs/2411.01975v1", "date": "2024-11-04", "relevancy": 2.7563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPECTRUM%3A%20Semantic%20Processing%20and%20Emotion-informed%20video-Captioning%0A%20%20Through%20Retrieval%20and%20Understanding%20Modalities&body=Title%3A%20SPECTRUM%3A%20Semantic%20Processing%20and%20Emotion-informed%20video-Captioning%0A%20%20Through%20Retrieval%20and%20Understanding%20Modalities%0AAuthor%3A%20Ehsan%20Faghihi%20and%20Mohammedreza%20Zarenejad%20and%20Ali-Asghar%20Beheshti%20Shirazi%0AAbstract%3A%20%20%20Capturing%20a%20video%27s%20meaning%20and%20critical%20concepts%20by%20analyzing%20the%20subtle%0Adetails%20is%20a%20fundamental%20yet%20challenging%20task%20in%20video%20captioning.%20Identifying%0Athe%20dominant%20emotional%20tone%20in%20a%20video%20significantly%20enhances%20the%20perception%20of%0Aits%20context.%20Despite%20a%20strong%20emphasis%20on%20video%20captioning%2C%20existing%20models%0Aoften%20need%20to%20adequately%20address%20emotional%20themes%2C%20resulting%20in%20suboptimal%0Acaptioning%20results.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%0ASemantic%20Processing%20and%20Emotion-informed%20video-Captioning%20Through%20Retrieval%20and%0AUnderstanding%20Modalities%20%28SPECTRUM%29%20framework%20to%20empower%20the%20generation%20of%0Aemotionally%20and%20semantically%20credible%20captions.%20Leveraging%20our%20pioneering%0Astructure%2C%20SPECTRUM%20discerns%20multimodal%20semantics%20and%20emotional%20themes%20using%0AVisual%20Text%20Attribute%20Investigation%20%28VTAI%29%20and%20determines%20the%20orientation%20of%0Adescriptive%20captions%20through%20a%20Holistic%20Concept-Oriented%20Theme%20%28HCOT%29%2C%0Aexpressing%20emotionally-informed%20and%20field-acquainted%20references.%20They%20exploit%0Avideo-to-text%20retrieval%20capabilities%20and%20the%20multifaceted%20nature%20of%20video%0Acontent%20to%20estimate%20the%20emotional%20probabilities%20of%20candidate%20captions.%20Then%2C%0Athe%20dominant%20theme%20of%20the%20video%20is%20determined%20by%20appropriately%20weighting%0Aembedded%20attribute%20vectors%20and%20applying%20coarse-%20and%20fine-grained%20emotional%0Aconcepts%2C%20which%20define%20the%20video%27s%20contextual%20alignment.%20Furthermore%2C%20using%20two%0Aloss%20functions%2C%20SPECTRUM%20is%20optimized%20to%20integrate%20emotional%20information%20and%0Aminimize%20prediction%20errors.%20Extensive%20experiments%20on%20the%20EmVidCap%2C%20MSVD%2C%20and%0AMSRVTT%20video%20captioning%20datasets%20demonstrate%20that%20our%20model%20significantly%0Asurpasses%20state-of-the-art%20methods.%20Quantitative%20and%20qualitative%20evaluations%0Ahighlight%20the%20model%27s%20ability%20to%20accurately%20capture%20and%20convey%20video%20emotions%0Aand%20multimodal%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPECTRUM%253A%2520Semantic%2520Processing%2520and%2520Emotion-informed%2520video-Captioning%250A%2520%2520Through%2520Retrieval%2520and%2520Understanding%2520Modalities%26entry.906535625%3DEhsan%2520Faghihi%2520and%2520Mohammedreza%2520Zarenejad%2520and%2520Ali-Asghar%2520Beheshti%2520Shirazi%26entry.1292438233%3D%2520%2520Capturing%2520a%2520video%2527s%2520meaning%2520and%2520critical%2520concepts%2520by%2520analyzing%2520the%2520subtle%250Adetails%2520is%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520video%2520captioning.%2520Identifying%250Athe%2520dominant%2520emotional%2520tone%2520in%2520a%2520video%2520significantly%2520enhances%2520the%2520perception%2520of%250Aits%2520context.%2520Despite%2520a%2520strong%2520emphasis%2520on%2520video%2520captioning%252C%2520existing%2520models%250Aoften%2520need%2520to%2520adequately%2520address%2520emotional%2520themes%252C%2520resulting%2520in%2520suboptimal%250Acaptioning%2520results.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%250ASemantic%2520Processing%2520and%2520Emotion-informed%2520video-Captioning%2520Through%2520Retrieval%2520and%250AUnderstanding%2520Modalities%2520%2528SPECTRUM%2529%2520framework%2520to%2520empower%2520the%2520generation%2520of%250Aemotionally%2520and%2520semantically%2520credible%2520captions.%2520Leveraging%2520our%2520pioneering%250Astructure%252C%2520SPECTRUM%2520discerns%2520multimodal%2520semantics%2520and%2520emotional%2520themes%2520using%250AVisual%2520Text%2520Attribute%2520Investigation%2520%2528VTAI%2529%2520and%2520determines%2520the%2520orientation%2520of%250Adescriptive%2520captions%2520through%2520a%2520Holistic%2520Concept-Oriented%2520Theme%2520%2528HCOT%2529%252C%250Aexpressing%2520emotionally-informed%2520and%2520field-acquainted%2520references.%2520They%2520exploit%250Avideo-to-text%2520retrieval%2520capabilities%2520and%2520the%2520multifaceted%2520nature%2520of%2520video%250Acontent%2520to%2520estimate%2520the%2520emotional%2520probabilities%2520of%2520candidate%2520captions.%2520Then%252C%250Athe%2520dominant%2520theme%2520of%2520the%2520video%2520is%2520determined%2520by%2520appropriately%2520weighting%250Aembedded%2520attribute%2520vectors%2520and%2520applying%2520coarse-%2520and%2520fine-grained%2520emotional%250Aconcepts%252C%2520which%2520define%2520the%2520video%2527s%2520contextual%2520alignment.%2520Furthermore%252C%2520using%2520two%250Aloss%2520functions%252C%2520SPECTRUM%2520is%2520optimized%2520to%2520integrate%2520emotional%2520information%2520and%250Aminimize%2520prediction%2520errors.%2520Extensive%2520experiments%2520on%2520the%2520EmVidCap%252C%2520MSVD%252C%2520and%250AMSRVTT%2520video%2520captioning%2520datasets%2520demonstrate%2520that%2520our%2520model%2520significantly%250Asurpasses%2520state-of-the-art%2520methods.%2520Quantitative%2520and%2520qualitative%2520evaluations%250Ahighlight%2520the%2520model%2527s%2520ability%2520to%2520accurately%2520capture%2520and%2520convey%2520video%2520emotions%250Aand%2520multimodal%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPECTRUM%3A%20Semantic%20Processing%20and%20Emotion-informed%20video-Captioning%0A%20%20Through%20Retrieval%20and%20Understanding%20Modalities&entry.906535625=Ehsan%20Faghihi%20and%20Mohammedreza%20Zarenejad%20and%20Ali-Asghar%20Beheshti%20Shirazi&entry.1292438233=%20%20Capturing%20a%20video%27s%20meaning%20and%20critical%20concepts%20by%20analyzing%20the%20subtle%0Adetails%20is%20a%20fundamental%20yet%20challenging%20task%20in%20video%20captioning.%20Identifying%0Athe%20dominant%20emotional%20tone%20in%20a%20video%20significantly%20enhances%20the%20perception%20of%0Aits%20context.%20Despite%20a%20strong%20emphasis%20on%20video%20captioning%2C%20existing%20models%0Aoften%20need%20to%20adequately%20address%20emotional%20themes%2C%20resulting%20in%20suboptimal%0Acaptioning%20results.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%0ASemantic%20Processing%20and%20Emotion-informed%20video-Captioning%20Through%20Retrieval%20and%0AUnderstanding%20Modalities%20%28SPECTRUM%29%20framework%20to%20empower%20the%20generation%20of%0Aemotionally%20and%20semantically%20credible%20captions.%20Leveraging%20our%20pioneering%0Astructure%2C%20SPECTRUM%20discerns%20multimodal%20semantics%20and%20emotional%20themes%20using%0AVisual%20Text%20Attribute%20Investigation%20%28VTAI%29%20and%20determines%20the%20orientation%20of%0Adescriptive%20captions%20through%20a%20Holistic%20Concept-Oriented%20Theme%20%28HCOT%29%2C%0Aexpressing%20emotionally-informed%20and%20field-acquainted%20references.%20They%20exploit%0Avideo-to-text%20retrieval%20capabilities%20and%20the%20multifaceted%20nature%20of%20video%0Acontent%20to%20estimate%20the%20emotional%20probabilities%20of%20candidate%20captions.%20Then%2C%0Athe%20dominant%20theme%20of%20the%20video%20is%20determined%20by%20appropriately%20weighting%0Aembedded%20attribute%20vectors%20and%20applying%20coarse-%20and%20fine-grained%20emotional%0Aconcepts%2C%20which%20define%20the%20video%27s%20contextual%20alignment.%20Furthermore%2C%20using%20two%0Aloss%20functions%2C%20SPECTRUM%20is%20optimized%20to%20integrate%20emotional%20information%20and%0Aminimize%20prediction%20errors.%20Extensive%20experiments%20on%20the%20EmVidCap%2C%20MSVD%2C%20and%0AMSRVTT%20video%20captioning%20datasets%20demonstrate%20that%20our%20model%20significantly%0Asurpasses%20state-of-the-art%20methods.%20Quantitative%20and%20qualitative%20evaluations%0Ahighlight%20the%20model%27s%20ability%20to%20accurately%20capture%20and%20convey%20video%20emotions%0Aand%20multimodal%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01975v1&entry.124074799=Read"},
{"title": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for\n  Remote Sensing Images", "author": "Kaiyu Li and Ruixun Liu and Xiangyong Cao and Xueru Bai and Feng Zhou and Deyu Meng and Zhi Wang", "abstract": "  Remote sensing image plays an irreplaceable role in fields such as\nagriculture, water resources, military, and disaster relief. Pixel-level\ninterpretation is a critical aspect of remote sensing image applications;\nhowever, a prevalent limitation remains the need for extensive manual\nannotation. For this, we try to introduce open-vocabulary semantic segmentation\n(OVSS) into the remote sensing context. However, due to the sensitivity of\nremote sensing images to low-resolution features, distorted target shapes and\nill-fitting boundaries are exhibited in the prediction mask. To tackle this\nissue, we propose a simple and general upsampler, SimFeatUp, to restore lost\nspatial information in deep features in a training-free style. Further, based\non the observation of the abnormal response of local patch tokens to [CLS]\ntoken in CLIP, we propose to execute a straightforward subtraction operation to\nalleviate the global bias in patch tokens. Extensive experiments are conducted\non 17 remote sensing datasets spanning semantic segmentation, building\nextraction, road detection, and flood detection tasks. Our method achieves an\naverage of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-art\nmethods on 4 tasks. All codes are released.\n\\url{https://earth-insights.github.io/SegEarth-OV}\n", "link": "http://arxiv.org/abs/2410.01768v2", "date": "2024-11-04", "relevancy": 2.7557, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegEarth-OV%3A%20Towards%20Training-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images&body=Title%3A%20SegEarth-OV%3A%20Towards%20Training-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images%0AAuthor%3A%20Kaiyu%20Li%20and%20Ruixun%20Liu%20and%20Xiangyong%20Cao%20and%20Xueru%20Bai%20and%20Feng%20Zhou%20and%20Deyu%20Meng%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Remote%20sensing%20image%20plays%20an%20irreplaceable%20role%20in%20fields%20such%20as%0Aagriculture%2C%20water%20resources%2C%20military%2C%20and%20disaster%20relief.%20Pixel-level%0Ainterpretation%20is%20a%20critical%20aspect%20of%20remote%20sensing%20image%20applications%3B%0Ahowever%2C%20a%20prevalent%20limitation%20remains%20the%20need%20for%20extensive%20manual%0Aannotation.%20For%20this%2C%20we%20try%20to%20introduce%20open-vocabulary%20semantic%20segmentation%0A%28OVSS%29%20into%20the%20remote%20sensing%20context.%20However%2C%20due%20to%20the%20sensitivity%20of%0Aremote%20sensing%20images%20to%20low-resolution%20features%2C%20distorted%20target%20shapes%20and%0Aill-fitting%20boundaries%20are%20exhibited%20in%20the%20prediction%20mask.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20and%20general%20upsampler%2C%20SimFeatUp%2C%20to%20restore%20lost%0Aspatial%20information%20in%20deep%20features%20in%20a%20training-free%20style.%20Further%2C%20based%0Aon%20the%20observation%20of%20the%20abnormal%20response%20of%20local%20patch%20tokens%20to%20%5BCLS%5D%0Atoken%20in%20CLIP%2C%20we%20propose%20to%20execute%20a%20straightforward%20subtraction%20operation%20to%0Aalleviate%20the%20global%20bias%20in%20patch%20tokens.%20Extensive%20experiments%20are%20conducted%0Aon%2017%20remote%20sensing%20datasets%20spanning%20semantic%20segmentation%2C%20building%0Aextraction%2C%20road%20detection%2C%20and%20flood%20detection%20tasks.%20Our%20method%20achieves%20an%0Aaverage%20of%205.8%25%2C%208.2%25%2C%204.0%25%2C%20and%2015.3%25%20improvement%20over%20state-of-the-art%0Amethods%20on%204%20tasks.%20All%20codes%20are%20released.%0A%5Curl%7Bhttps%3A//earth-insights.github.io/SegEarth-OV%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegEarth-OV%253A%2520Towards%2520Training-Free%2520Open-Vocabulary%2520Segmentation%2520for%250A%2520%2520Remote%2520Sensing%2520Images%26entry.906535625%3DKaiyu%2520Li%2520and%2520Ruixun%2520Liu%2520and%2520Xiangyong%2520Cao%2520and%2520Xueru%2520Bai%2520and%2520Feng%2520Zhou%2520and%2520Deyu%2520Meng%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520plays%2520an%2520irreplaceable%2520role%2520in%2520fields%2520such%2520as%250Aagriculture%252C%2520water%2520resources%252C%2520military%252C%2520and%2520disaster%2520relief.%2520Pixel-level%250Ainterpretation%2520is%2520a%2520critical%2520aspect%2520of%2520remote%2520sensing%2520image%2520applications%253B%250Ahowever%252C%2520a%2520prevalent%2520limitation%2520remains%2520the%2520need%2520for%2520extensive%2520manual%250Aannotation.%2520For%2520this%252C%2520we%2520try%2520to%2520introduce%2520open-vocabulary%2520semantic%2520segmentation%250A%2528OVSS%2529%2520into%2520the%2520remote%2520sensing%2520context.%2520However%252C%2520due%2520to%2520the%2520sensitivity%2520of%250Aremote%2520sensing%2520images%2520to%2520low-resolution%2520features%252C%2520distorted%2520target%2520shapes%2520and%250Aill-fitting%2520boundaries%2520are%2520exhibited%2520in%2520the%2520prediction%2520mask.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520a%2520simple%2520and%2520general%2520upsampler%252C%2520SimFeatUp%252C%2520to%2520restore%2520lost%250Aspatial%2520information%2520in%2520deep%2520features%2520in%2520a%2520training-free%2520style.%2520Further%252C%2520based%250Aon%2520the%2520observation%2520of%2520the%2520abnormal%2520response%2520of%2520local%2520patch%2520tokens%2520to%2520%255BCLS%255D%250Atoken%2520in%2520CLIP%252C%2520we%2520propose%2520to%2520execute%2520a%2520straightforward%2520subtraction%2520operation%2520to%250Aalleviate%2520the%2520global%2520bias%2520in%2520patch%2520tokens.%2520Extensive%2520experiments%2520are%2520conducted%250Aon%252017%2520remote%2520sensing%2520datasets%2520spanning%2520semantic%2520segmentation%252C%2520building%250Aextraction%252C%2520road%2520detection%252C%2520and%2520flood%2520detection%2520tasks.%2520Our%2520method%2520achieves%2520an%250Aaverage%2520of%25205.8%2525%252C%25208.2%2525%252C%25204.0%2525%252C%2520and%252015.3%2525%2520improvement%2520over%2520state-of-the-art%250Amethods%2520on%25204%2520tasks.%2520All%2520codes%2520are%2520released.%250A%255Curl%257Bhttps%253A//earth-insights.github.io/SegEarth-OV%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegEarth-OV%3A%20Towards%20Training-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images&entry.906535625=Kaiyu%20Li%20and%20Ruixun%20Liu%20and%20Xiangyong%20Cao%20and%20Xueru%20Bai%20and%20Feng%20Zhou%20and%20Deyu%20Meng%20and%20Zhi%20Wang&entry.1292438233=%20%20Remote%20sensing%20image%20plays%20an%20irreplaceable%20role%20in%20fields%20such%20as%0Aagriculture%2C%20water%20resources%2C%20military%2C%20and%20disaster%20relief.%20Pixel-level%0Ainterpretation%20is%20a%20critical%20aspect%20of%20remote%20sensing%20image%20applications%3B%0Ahowever%2C%20a%20prevalent%20limitation%20remains%20the%20need%20for%20extensive%20manual%0Aannotation.%20For%20this%2C%20we%20try%20to%20introduce%20open-vocabulary%20semantic%20segmentation%0A%28OVSS%29%20into%20the%20remote%20sensing%20context.%20However%2C%20due%20to%20the%20sensitivity%20of%0Aremote%20sensing%20images%20to%20low-resolution%20features%2C%20distorted%20target%20shapes%20and%0Aill-fitting%20boundaries%20are%20exhibited%20in%20the%20prediction%20mask.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20and%20general%20upsampler%2C%20SimFeatUp%2C%20to%20restore%20lost%0Aspatial%20information%20in%20deep%20features%20in%20a%20training-free%20style.%20Further%2C%20based%0Aon%20the%20observation%20of%20the%20abnormal%20response%20of%20local%20patch%20tokens%20to%20%5BCLS%5D%0Atoken%20in%20CLIP%2C%20we%20propose%20to%20execute%20a%20straightforward%20subtraction%20operation%20to%0Aalleviate%20the%20global%20bias%20in%20patch%20tokens.%20Extensive%20experiments%20are%20conducted%0Aon%2017%20remote%20sensing%20datasets%20spanning%20semantic%20segmentation%2C%20building%0Aextraction%2C%20road%20detection%2C%20and%20flood%20detection%20tasks.%20Our%20method%20achieves%20an%0Aaverage%20of%205.8%25%2C%208.2%25%2C%204.0%25%2C%20and%2015.3%25%20improvement%20over%20state-of-the-art%0Amethods%20on%204%20tasks.%20All%20codes%20are%20released.%0A%5Curl%7Bhttps%3A//earth-insights.github.io/SegEarth-OV%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01768v2&entry.124074799=Read"},
{"title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)", "author": "Usha Bhalla and Alex Oesterling and Suraj Srinivas and Flavio P. Calmon and Himabindu Lakkaraju", "abstract": "  CLIP embeddings have demonstrated remarkable performance across a wide range\nof multimodal applications. However, these high-dimensional, dense vector\nrepresentations are not easily interpretable, limiting our understanding of the\nrich structure of CLIP and its use in downstream applications that require\ntransparency. In this work, we show that the semantic structure of CLIP's\nlatent space can be leveraged to provide interpretability, allowing for the\ndecomposition of representations into semantic concepts. We formulate this\nproblem as one of sparse recovery and propose a novel method, Sparse Linear\nConcept Embeddings, for transforming CLIP representations into sparse linear\ncombinations of human-interpretable concepts. Distinct from previous work,\nSpLiCE is task-agnostic and can be used, without training, to explain and even\nreplace traditional dense CLIP representations, maintaining high downstream\nperformance while significantly improving their interpretability. We also\ndemonstrate significant use cases of SpLiCE representations including detecting\nspurious correlations and model editing.\n", "link": "http://arxiv.org/abs/2402.10376v2", "date": "2024-11-04", "relevancy": 2.7433, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20CLIP%20with%20Sparse%20Linear%20Concept%20Embeddings%20%28SpLiCE%29&body=Title%3A%20Interpreting%20CLIP%20with%20Sparse%20Linear%20Concept%20Embeddings%20%28SpLiCE%29%0AAuthor%3A%20Usha%20Bhalla%20and%20Alex%20Oesterling%20and%20Suraj%20Srinivas%20and%20Flavio%20P.%20Calmon%20and%20Himabindu%20Lakkaraju%0AAbstract%3A%20%20%20CLIP%20embeddings%20have%20demonstrated%20remarkable%20performance%20across%20a%20wide%20range%0Aof%20multimodal%20applications.%20However%2C%20these%20high-dimensional%2C%20dense%20vector%0Arepresentations%20are%20not%20easily%20interpretable%2C%20limiting%20our%20understanding%20of%20the%0Arich%20structure%20of%20CLIP%20and%20its%20use%20in%20downstream%20applications%20that%20require%0Atransparency.%20In%20this%20work%2C%20we%20show%20that%20the%20semantic%20structure%20of%20CLIP%27s%0Alatent%20space%20can%20be%20leveraged%20to%20provide%20interpretability%2C%20allowing%20for%20the%0Adecomposition%20of%20representations%20into%20semantic%20concepts.%20We%20formulate%20this%0Aproblem%20as%20one%20of%20sparse%20recovery%20and%20propose%20a%20novel%20method%2C%20Sparse%20Linear%0AConcept%20Embeddings%2C%20for%20transforming%20CLIP%20representations%20into%20sparse%20linear%0Acombinations%20of%20human-interpretable%20concepts.%20Distinct%20from%20previous%20work%2C%0ASpLiCE%20is%20task-agnostic%20and%20can%20be%20used%2C%20without%20training%2C%20to%20explain%20and%20even%0Areplace%20traditional%20dense%20CLIP%20representations%2C%20maintaining%20high%20downstream%0Aperformance%20while%20significantly%20improving%20their%20interpretability.%20We%20also%0Ademonstrate%20significant%20use%20cases%20of%20SpLiCE%20representations%20including%20detecting%0Aspurious%20correlations%20and%20model%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520CLIP%2520with%2520Sparse%2520Linear%2520Concept%2520Embeddings%2520%2528SpLiCE%2529%26entry.906535625%3DUsha%2520Bhalla%2520and%2520Alex%2520Oesterling%2520and%2520Suraj%2520Srinivas%2520and%2520Flavio%2520P.%2520Calmon%2520and%2520Himabindu%2520Lakkaraju%26entry.1292438233%3D%2520%2520CLIP%2520embeddings%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520a%2520wide%2520range%250Aof%2520multimodal%2520applications.%2520However%252C%2520these%2520high-dimensional%252C%2520dense%2520vector%250Arepresentations%2520are%2520not%2520easily%2520interpretable%252C%2520limiting%2520our%2520understanding%2520of%2520the%250Arich%2520structure%2520of%2520CLIP%2520and%2520its%2520use%2520in%2520downstream%2520applications%2520that%2520require%250Atransparency.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520semantic%2520structure%2520of%2520CLIP%2527s%250Alatent%2520space%2520can%2520be%2520leveraged%2520to%2520provide%2520interpretability%252C%2520allowing%2520for%2520the%250Adecomposition%2520of%2520representations%2520into%2520semantic%2520concepts.%2520We%2520formulate%2520this%250Aproblem%2520as%2520one%2520of%2520sparse%2520recovery%2520and%2520propose%2520a%2520novel%2520method%252C%2520Sparse%2520Linear%250AConcept%2520Embeddings%252C%2520for%2520transforming%2520CLIP%2520representations%2520into%2520sparse%2520linear%250Acombinations%2520of%2520human-interpretable%2520concepts.%2520Distinct%2520from%2520previous%2520work%252C%250ASpLiCE%2520is%2520task-agnostic%2520and%2520can%2520be%2520used%252C%2520without%2520training%252C%2520to%2520explain%2520and%2520even%250Areplace%2520traditional%2520dense%2520CLIP%2520representations%252C%2520maintaining%2520high%2520downstream%250Aperformance%2520while%2520significantly%2520improving%2520their%2520interpretability.%2520We%2520also%250Ademonstrate%2520significant%2520use%2520cases%2520of%2520SpLiCE%2520representations%2520including%2520detecting%250Aspurious%2520correlations%2520and%2520model%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20CLIP%20with%20Sparse%20Linear%20Concept%20Embeddings%20%28SpLiCE%29&entry.906535625=Usha%20Bhalla%20and%20Alex%20Oesterling%20and%20Suraj%20Srinivas%20and%20Flavio%20P.%20Calmon%20and%20Himabindu%20Lakkaraju&entry.1292438233=%20%20CLIP%20embeddings%20have%20demonstrated%20remarkable%20performance%20across%20a%20wide%20range%0Aof%20multimodal%20applications.%20However%2C%20these%20high-dimensional%2C%20dense%20vector%0Arepresentations%20are%20not%20easily%20interpretable%2C%20limiting%20our%20understanding%20of%20the%0Arich%20structure%20of%20CLIP%20and%20its%20use%20in%20downstream%20applications%20that%20require%0Atransparency.%20In%20this%20work%2C%20we%20show%20that%20the%20semantic%20structure%20of%20CLIP%27s%0Alatent%20space%20can%20be%20leveraged%20to%20provide%20interpretability%2C%20allowing%20for%20the%0Adecomposition%20of%20representations%20into%20semantic%20concepts.%20We%20formulate%20this%0Aproblem%20as%20one%20of%20sparse%20recovery%20and%20propose%20a%20novel%20method%2C%20Sparse%20Linear%0AConcept%20Embeddings%2C%20for%20transforming%20CLIP%20representations%20into%20sparse%20linear%0Acombinations%20of%20human-interpretable%20concepts.%20Distinct%20from%20previous%20work%2C%0ASpLiCE%20is%20task-agnostic%20and%20can%20be%20used%2C%20without%20training%2C%20to%20explain%20and%20even%0Areplace%20traditional%20dense%20CLIP%20representations%2C%20maintaining%20high%20downstream%0Aperformance%20while%20significantly%20improving%20their%20interpretability.%20We%20also%0Ademonstrate%20significant%20use%20cases%20of%20SpLiCE%20representations%20including%20detecting%0Aspurious%20correlations%20and%20model%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10376v2&entry.124074799=Read"},
{"title": "SpecRaGE: Robust and Generalizable Multi-view Spectral Representation\n  Learning", "author": "Amitai Yacobi and Ofir Lindenbaum and Uri Shaham", "abstract": "  Multi-view representation learning (MvRL) has garnered substantial attention\nin recent years, driven by the increasing demand for applications that can\neffectively process and analyze data from multiple sources. In this context,\ngraph Laplacian-based MvRL methods have demonstrated remarkable success in\nrepresenting multi-view data. However, these methods often struggle with\ngeneralization to new data and face challenges with scalability. Moreover, in\nmany practical scenarios, multi-view data is contaminated by noise or outliers.\nIn such cases, modern deep-learning-based MvRL approaches that rely on\nalignment or contrastive objectives can lead to misleading results, as they may\nimpose incorrect consistency between clear and corrupted data sources. We\nintroduce $\\textit{SpecRaGE}$, a novel fusion-based framework that integrates\nthe strengths of graph Laplacian methods with the power of deep learning to\novercome these challenges. SpecRage uses neural networks to learn parametric\nmapping that approximates a joint diagonalization of graph Laplacians. This\nsolution bypasses the need for alignment while enabling generalizable and\nscalable learning of informative and meaningful representations. Moreover, it\nincorporates a meta-learning fusion module that dynamically adapts to data\nquality, ensuring robustness against outliers and noisy views. Our extensive\nexperiments demonstrate that SpecRaGE outperforms state-of-the-art methods,\nparticularly in scenarios with data contamination, paving the way for more\nreliable and efficient multi-view learning. Our code will be made publicly\navailable upon acceptance.\n", "link": "http://arxiv.org/abs/2411.02138v1", "date": "2024-11-04", "relevancy": 2.741, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5652}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecRaGE%3A%20Robust%20and%20Generalizable%20Multi-view%20Spectral%20Representation%0A%20%20Learning&body=Title%3A%20SpecRaGE%3A%20Robust%20and%20Generalizable%20Multi-view%20Spectral%20Representation%0A%20%20Learning%0AAuthor%3A%20Amitai%20Yacobi%20and%20Ofir%20Lindenbaum%20and%20Uri%20Shaham%0AAbstract%3A%20%20%20Multi-view%20representation%20learning%20%28MvRL%29%20has%20garnered%20substantial%20attention%0Ain%20recent%20years%2C%20driven%20by%20the%20increasing%20demand%20for%20applications%20that%20can%0Aeffectively%20process%20and%20analyze%20data%20from%20multiple%20sources.%20In%20this%20context%2C%0Agraph%20Laplacian-based%20MvRL%20methods%20have%20demonstrated%20remarkable%20success%20in%0Arepresenting%20multi-view%20data.%20However%2C%20these%20methods%20often%20struggle%20with%0Ageneralization%20to%20new%20data%20and%20face%20challenges%20with%20scalability.%20Moreover%2C%20in%0Amany%20practical%20scenarios%2C%20multi-view%20data%20is%20contaminated%20by%20noise%20or%20outliers.%0AIn%20such%20cases%2C%20modern%20deep-learning-based%20MvRL%20approaches%20that%20rely%20on%0Aalignment%20or%20contrastive%20objectives%20can%20lead%20to%20misleading%20results%2C%20as%20they%20may%0Aimpose%20incorrect%20consistency%20between%20clear%20and%20corrupted%20data%20sources.%20We%0Aintroduce%20%24%5Ctextit%7BSpecRaGE%7D%24%2C%20a%20novel%20fusion-based%20framework%20that%20integrates%0Athe%20strengths%20of%20graph%20Laplacian%20methods%20with%20the%20power%20of%20deep%20learning%20to%0Aovercome%20these%20challenges.%20SpecRage%20uses%20neural%20networks%20to%20learn%20parametric%0Amapping%20that%20approximates%20a%20joint%20diagonalization%20of%20graph%20Laplacians.%20This%0Asolution%20bypasses%20the%20need%20for%20alignment%20while%20enabling%20generalizable%20and%0Ascalable%20learning%20of%20informative%20and%20meaningful%20representations.%20Moreover%2C%20it%0Aincorporates%20a%20meta-learning%20fusion%20module%20that%20dynamically%20adapts%20to%20data%0Aquality%2C%20ensuring%20robustness%20against%20outliers%20and%20noisy%20views.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20SpecRaGE%20outperforms%20state-of-the-art%20methods%2C%0Aparticularly%20in%20scenarios%20with%20data%20contamination%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20efficient%20multi-view%20learning.%20Our%20code%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecRaGE%253A%2520Robust%2520and%2520Generalizable%2520Multi-view%2520Spectral%2520Representation%250A%2520%2520Learning%26entry.906535625%3DAmitai%2520Yacobi%2520and%2520Ofir%2520Lindenbaum%2520and%2520Uri%2520Shaham%26entry.1292438233%3D%2520%2520Multi-view%2520representation%2520learning%2520%2528MvRL%2529%2520has%2520garnered%2520substantial%2520attention%250Ain%2520recent%2520years%252C%2520driven%2520by%2520the%2520increasing%2520demand%2520for%2520applications%2520that%2520can%250Aeffectively%2520process%2520and%2520analyze%2520data%2520from%2520multiple%2520sources.%2520In%2520this%2520context%252C%250Agraph%2520Laplacian-based%2520MvRL%2520methods%2520have%2520demonstrated%2520remarkable%2520success%2520in%250Arepresenting%2520multi-view%2520data.%2520However%252C%2520these%2520methods%2520often%2520struggle%2520with%250Ageneralization%2520to%2520new%2520data%2520and%2520face%2520challenges%2520with%2520scalability.%2520Moreover%252C%2520in%250Amany%2520practical%2520scenarios%252C%2520multi-view%2520data%2520is%2520contaminated%2520by%2520noise%2520or%2520outliers.%250AIn%2520such%2520cases%252C%2520modern%2520deep-learning-based%2520MvRL%2520approaches%2520that%2520rely%2520on%250Aalignment%2520or%2520contrastive%2520objectives%2520can%2520lead%2520to%2520misleading%2520results%252C%2520as%2520they%2520may%250Aimpose%2520incorrect%2520consistency%2520between%2520clear%2520and%2520corrupted%2520data%2520sources.%2520We%250Aintroduce%2520%2524%255Ctextit%257BSpecRaGE%257D%2524%252C%2520a%2520novel%2520fusion-based%2520framework%2520that%2520integrates%250Athe%2520strengths%2520of%2520graph%2520Laplacian%2520methods%2520with%2520the%2520power%2520of%2520deep%2520learning%2520to%250Aovercome%2520these%2520challenges.%2520SpecRage%2520uses%2520neural%2520networks%2520to%2520learn%2520parametric%250Amapping%2520that%2520approximates%2520a%2520joint%2520diagonalization%2520of%2520graph%2520Laplacians.%2520This%250Asolution%2520bypasses%2520the%2520need%2520for%2520alignment%2520while%2520enabling%2520generalizable%2520and%250Ascalable%2520learning%2520of%2520informative%2520and%2520meaningful%2520representations.%2520Moreover%252C%2520it%250Aincorporates%2520a%2520meta-learning%2520fusion%2520module%2520that%2520dynamically%2520adapts%2520to%2520data%250Aquality%252C%2520ensuring%2520robustness%2520against%2520outliers%2520and%2520noisy%2520views.%2520Our%2520extensive%250Aexperiments%2520demonstrate%2520that%2520SpecRaGE%2520outperforms%2520state-of-the-art%2520methods%252C%250Aparticularly%2520in%2520scenarios%2520with%2520data%2520contamination%252C%2520paving%2520the%2520way%2520for%2520more%250Areliable%2520and%2520efficient%2520multi-view%2520learning.%2520Our%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecRaGE%3A%20Robust%20and%20Generalizable%20Multi-view%20Spectral%20Representation%0A%20%20Learning&entry.906535625=Amitai%20Yacobi%20and%20Ofir%20Lindenbaum%20and%20Uri%20Shaham&entry.1292438233=%20%20Multi-view%20representation%20learning%20%28MvRL%29%20has%20garnered%20substantial%20attention%0Ain%20recent%20years%2C%20driven%20by%20the%20increasing%20demand%20for%20applications%20that%20can%0Aeffectively%20process%20and%20analyze%20data%20from%20multiple%20sources.%20In%20this%20context%2C%0Agraph%20Laplacian-based%20MvRL%20methods%20have%20demonstrated%20remarkable%20success%20in%0Arepresenting%20multi-view%20data.%20However%2C%20these%20methods%20often%20struggle%20with%0Ageneralization%20to%20new%20data%20and%20face%20challenges%20with%20scalability.%20Moreover%2C%20in%0Amany%20practical%20scenarios%2C%20multi-view%20data%20is%20contaminated%20by%20noise%20or%20outliers.%0AIn%20such%20cases%2C%20modern%20deep-learning-based%20MvRL%20approaches%20that%20rely%20on%0Aalignment%20or%20contrastive%20objectives%20can%20lead%20to%20misleading%20results%2C%20as%20they%20may%0Aimpose%20incorrect%20consistency%20between%20clear%20and%20corrupted%20data%20sources.%20We%0Aintroduce%20%24%5Ctextit%7BSpecRaGE%7D%24%2C%20a%20novel%20fusion-based%20framework%20that%20integrates%0Athe%20strengths%20of%20graph%20Laplacian%20methods%20with%20the%20power%20of%20deep%20learning%20to%0Aovercome%20these%20challenges.%20SpecRage%20uses%20neural%20networks%20to%20learn%20parametric%0Amapping%20that%20approximates%20a%20joint%20diagonalization%20of%20graph%20Laplacians.%20This%0Asolution%20bypasses%20the%20need%20for%20alignment%20while%20enabling%20generalizable%20and%0Ascalable%20learning%20of%20informative%20and%20meaningful%20representations.%20Moreover%2C%20it%0Aincorporates%20a%20meta-learning%20fusion%20module%20that%20dynamically%20adapts%20to%20data%0Aquality%2C%20ensuring%20robustness%20against%20outliers%20and%20noisy%20views.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20SpecRaGE%20outperforms%20state-of-the-art%20methods%2C%0Aparticularly%20in%20scenarios%20with%20data%20contamination%2C%20paving%20the%20way%20for%20more%0Areliable%20and%20efficient%20multi-view%20learning.%20Our%20code%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02138v1&entry.124074799=Read"},
{"title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs", "author": "Jiasheng Zhang and Jialin Chen and Menglin Yang and Aosong Feng and Shuang Liang and Jie Shao and Rex Ying", "abstract": "  Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB.\n", "link": "http://arxiv.org/abs/2406.12072v3", "date": "2024-11-04", "relevancy": 2.6374, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5449}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5208}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTGB%3A%20A%20Comprehensive%20Benchmark%20for%20Dynamic%20Text-Attributed%20Graphs&body=Title%3A%20DTGB%3A%20A%20Comprehensive%20Benchmark%20for%20Dynamic%20Text-Attributed%20Graphs%0AAuthor%3A%20Jiasheng%20Zhang%20and%20Jialin%20Chen%20and%20Menglin%20Yang%20and%20Aosong%20Feng%20and%20Shuang%20Liang%20and%20Jie%20Shao%20and%20Rex%20Ying%0AAbstract%3A%20%20%20Dynamic%20text-attributed%20graphs%20%28DyTAGs%29%20are%20prevalent%20in%20various%20real-world%0Ascenarios%2C%20where%20each%20node%20and%20edge%20are%20associated%20with%20text%20descriptions%2C%20and%0Aboth%20the%20graph%20structure%20and%20text%20descriptions%20evolve%20over%20time.%20Despite%20their%0Abroad%20applicability%2C%20there%20is%20a%20notable%20scarcity%20of%20benchmark%20datasets%20tailored%0Ato%20DyTAGs%2C%20which%20hinders%20the%20potential%20advancement%20in%20many%20research%20fields.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Dynamic%20Text-attributed%20Graph%20Benchmark%20%28DTGB%29%2C%0Aa%20collection%20of%20large-scale%2C%20time-evolving%20graphs%20from%20diverse%20domains%2C%20with%0Anodes%20and%20edges%20enriched%20by%20dynamically%20changing%20text%20attributes%20and%0Acategories.%20To%20facilitate%20the%20use%20of%20DTGB%2C%20we%20design%20standardized%20evaluation%0Aprocedures%20based%20on%20four%20real-world%20use%20cases%3A%20future%20link%20prediction%2C%0Adestination%20node%20retrieval%2C%20edge%20classification%2C%20and%20textual%20relation%0Ageneration.%20These%20tasks%20require%20models%20to%20understand%20both%20dynamic%20graph%0Astructures%20and%20natural%20language%2C%20highlighting%20the%20unique%20challenges%20posed%20by%0ADyTAGs.%20Moreover%2C%20we%20conduct%20extensive%20benchmark%20experiments%20on%20DTGB%2C%0Aevaluating%207%20popular%20dynamic%20graph%20learning%20algorithms%20and%20their%20variants%20of%0Aadapting%20to%20text%20attributes%20with%20LLM%20embeddings%2C%20along%20with%206%20powerful%20large%0Alanguage%20models%20%28LLMs%29.%20Our%20results%20show%20the%20limitations%20of%20existing%20models%20in%0Ahandling%20DyTAGs.%20Our%20analysis%20also%20demonstrates%20the%20utility%20of%20DTGB%20in%0Ainvestigating%20the%20incorporation%20of%20structural%20and%20textual%20dynamics.%20The%0Aproposed%20DTGB%20fosters%20research%20on%20DyTAGs%20and%20their%20broad%20applications.%20It%0Aoffers%20a%20comprehensive%20benchmark%20for%20evaluating%20and%20advancing%20models%20to%20handle%0Athe%20interplay%20between%20dynamic%20graph%20structures%20and%20natural%20language.%20The%0Adataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/zjs123/DTGB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12072v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTGB%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Dynamic%2520Text-Attributed%2520Graphs%26entry.906535625%3DJiasheng%2520Zhang%2520and%2520Jialin%2520Chen%2520and%2520Menglin%2520Yang%2520and%2520Aosong%2520Feng%2520and%2520Shuang%2520Liang%2520and%2520Jie%2520Shao%2520and%2520Rex%2520Ying%26entry.1292438233%3D%2520%2520Dynamic%2520text-attributed%2520graphs%2520%2528DyTAGs%2529%2520are%2520prevalent%2520in%2520various%2520real-world%250Ascenarios%252C%2520where%2520each%2520node%2520and%2520edge%2520are%2520associated%2520with%2520text%2520descriptions%252C%2520and%250Aboth%2520the%2520graph%2520structure%2520and%2520text%2520descriptions%2520evolve%2520over%2520time.%2520Despite%2520their%250Abroad%2520applicability%252C%2520there%2520is%2520a%2520notable%2520scarcity%2520of%2520benchmark%2520datasets%2520tailored%250Ato%2520DyTAGs%252C%2520which%2520hinders%2520the%2520potential%2520advancement%2520in%2520many%2520research%2520fields.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520Dynamic%2520Text-attributed%2520Graph%2520Benchmark%2520%2528DTGB%2529%252C%250Aa%2520collection%2520of%2520large-scale%252C%2520time-evolving%2520graphs%2520from%2520diverse%2520domains%252C%2520with%250Anodes%2520and%2520edges%2520enriched%2520by%2520dynamically%2520changing%2520text%2520attributes%2520and%250Acategories.%2520To%2520facilitate%2520the%2520use%2520of%2520DTGB%252C%2520we%2520design%2520standardized%2520evaluation%250Aprocedures%2520based%2520on%2520four%2520real-world%2520use%2520cases%253A%2520future%2520link%2520prediction%252C%250Adestination%2520node%2520retrieval%252C%2520edge%2520classification%252C%2520and%2520textual%2520relation%250Ageneration.%2520These%2520tasks%2520require%2520models%2520to%2520understand%2520both%2520dynamic%2520graph%250Astructures%2520and%2520natural%2520language%252C%2520highlighting%2520the%2520unique%2520challenges%2520posed%2520by%250ADyTAGs.%2520Moreover%252C%2520we%2520conduct%2520extensive%2520benchmark%2520experiments%2520on%2520DTGB%252C%250Aevaluating%25207%2520popular%2520dynamic%2520graph%2520learning%2520algorithms%2520and%2520their%2520variants%2520of%250Aadapting%2520to%2520text%2520attributes%2520with%2520LLM%2520embeddings%252C%2520along%2520with%25206%2520powerful%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Our%2520results%2520show%2520the%2520limitations%2520of%2520existing%2520models%2520in%250Ahandling%2520DyTAGs.%2520Our%2520analysis%2520also%2520demonstrates%2520the%2520utility%2520of%2520DTGB%2520in%250Ainvestigating%2520the%2520incorporation%2520of%2520structural%2520and%2520textual%2520dynamics.%2520The%250Aproposed%2520DTGB%2520fosters%2520research%2520on%2520DyTAGs%2520and%2520their%2520broad%2520applications.%2520It%250Aoffers%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520and%2520advancing%2520models%2520to%2520handle%250Athe%2520interplay%2520between%2520dynamic%2520graph%2520structures%2520and%2520natural%2520language.%2520The%250Adataset%2520and%2520source%2520code%2520are%2520available%2520at%2520https%253A//github.com/zjs123/DTGB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12072v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTGB%3A%20A%20Comprehensive%20Benchmark%20for%20Dynamic%20Text-Attributed%20Graphs&entry.906535625=Jiasheng%20Zhang%20and%20Jialin%20Chen%20and%20Menglin%20Yang%20and%20Aosong%20Feng%20and%20Shuang%20Liang%20and%20Jie%20Shao%20and%20Rex%20Ying&entry.1292438233=%20%20Dynamic%20text-attributed%20graphs%20%28DyTAGs%29%20are%20prevalent%20in%20various%20real-world%0Ascenarios%2C%20where%20each%20node%20and%20edge%20are%20associated%20with%20text%20descriptions%2C%20and%0Aboth%20the%20graph%20structure%20and%20text%20descriptions%20evolve%20over%20time.%20Despite%20their%0Abroad%20applicability%2C%20there%20is%20a%20notable%20scarcity%20of%20benchmark%20datasets%20tailored%0Ato%20DyTAGs%2C%20which%20hinders%20the%20potential%20advancement%20in%20many%20research%20fields.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Dynamic%20Text-attributed%20Graph%20Benchmark%20%28DTGB%29%2C%0Aa%20collection%20of%20large-scale%2C%20time-evolving%20graphs%20from%20diverse%20domains%2C%20with%0Anodes%20and%20edges%20enriched%20by%20dynamically%20changing%20text%20attributes%20and%0Acategories.%20To%20facilitate%20the%20use%20of%20DTGB%2C%20we%20design%20standardized%20evaluation%0Aprocedures%20based%20on%20four%20real-world%20use%20cases%3A%20future%20link%20prediction%2C%0Adestination%20node%20retrieval%2C%20edge%20classification%2C%20and%20textual%20relation%0Ageneration.%20These%20tasks%20require%20models%20to%20understand%20both%20dynamic%20graph%0Astructures%20and%20natural%20language%2C%20highlighting%20the%20unique%20challenges%20posed%20by%0ADyTAGs.%20Moreover%2C%20we%20conduct%20extensive%20benchmark%20experiments%20on%20DTGB%2C%0Aevaluating%207%20popular%20dynamic%20graph%20learning%20algorithms%20and%20their%20variants%20of%0Aadapting%20to%20text%20attributes%20with%20LLM%20embeddings%2C%20along%20with%206%20powerful%20large%0Alanguage%20models%20%28LLMs%29.%20Our%20results%20show%20the%20limitations%20of%20existing%20models%20in%0Ahandling%20DyTAGs.%20Our%20analysis%20also%20demonstrates%20the%20utility%20of%20DTGB%20in%0Ainvestigating%20the%20incorporation%20of%20structural%20and%20textual%20dynamics.%20The%0Aproposed%20DTGB%20fosters%20research%20on%20DyTAGs%20and%20their%20broad%20applications.%20It%0Aoffers%20a%20comprehensive%20benchmark%20for%20evaluating%20and%20advancing%20models%20to%20handle%0Athe%20interplay%20between%20dynamic%20graph%20structures%20and%20natural%20language.%20The%0Adataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/zjs123/DTGB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12072v3&entry.124074799=Read"},
{"title": "Robust plug-and-play methods for highly accelerated non-Cartesian MRI\n  reconstruction", "author": "Pierre-Antoine Comby and Benjamin Lapostolle and Matthieu Terris and Philippe Ciuciu", "abstract": "  Achieving high-quality Magnetic Resonance Imaging (MRI) reconstruction at\naccelerated acquisition rates remains challenging due to the inherent ill-posed\nnature of the inverse problem. Traditional Compressed Sensing (CS) methods,\nwhile robust across varying acquisition settings, struggle to maintain good\nreconstruction quality at high acceleration factors ($\\ge$ 8). Recent advances\nin deep learning have improved reconstruction quality, but purely data-driven\nmethods are prone to overfitting and hallucination effects, notably when the\nacquisition setting is varying. Plug-and-Play (PnP) approaches have been\nproposed to mitigate the pitfalls of both frameworks. In a nutshell, PnP\nalgorithms amount to replacing suboptimal handcrafted CS priors with powerful\ndenoising deep neural network (DNNs). However, in MRI reconstruction, existing\nPnP methods often yield suboptimal results due to instabilities in the proximal\ngradient descent (PGD) schemes and the lack of curated, noiseless datasets for\ntraining robust denoisers. In this work, we propose a fully unsupervised\npreprocessing pipeline to generate clean, noiseless complex MRI signals from\nmulticoil data, enabling training of a high-performance denoising DNN.\nFurthermore, we introduce an annealed Half-Quadratic Splitting (HQS) algorithm\nto address the instability issues, leading to significant improvements over\nexisting PnP algorithms. When combined with preconditioning techniques, our\napproach achieves state-of-the-art results, providing a robust and efficient\nsolution for high-quality MRI reconstruction.\n", "link": "http://arxiv.org/abs/2411.01955v1", "date": "2024-11-04", "relevancy": 2.6358, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5272}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20plug-and-play%20methods%20for%20highly%20accelerated%20non-Cartesian%20MRI%0A%20%20reconstruction&body=Title%3A%20Robust%20plug-and-play%20methods%20for%20highly%20accelerated%20non-Cartesian%20MRI%0A%20%20reconstruction%0AAuthor%3A%20Pierre-Antoine%20Comby%20and%20Benjamin%20Lapostolle%20and%20Matthieu%20Terris%20and%20Philippe%20Ciuciu%0AAbstract%3A%20%20%20Achieving%20high-quality%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20reconstruction%20at%0Aaccelerated%20acquisition%20rates%20remains%20challenging%20due%20to%20the%20inherent%20ill-posed%0Anature%20of%20the%20inverse%20problem.%20Traditional%20Compressed%20Sensing%20%28CS%29%20methods%2C%0Awhile%20robust%20across%20varying%20acquisition%20settings%2C%20struggle%20to%20maintain%20good%0Areconstruction%20quality%20at%20high%20acceleration%20factors%20%28%24%5Cge%24%208%29.%20Recent%20advances%0Ain%20deep%20learning%20have%20improved%20reconstruction%20quality%2C%20but%20purely%20data-driven%0Amethods%20are%20prone%20to%20overfitting%20and%20hallucination%20effects%2C%20notably%20when%20the%0Aacquisition%20setting%20is%20varying.%20Plug-and-Play%20%28PnP%29%20approaches%20have%20been%0Aproposed%20to%20mitigate%20the%20pitfalls%20of%20both%20frameworks.%20In%20a%20nutshell%2C%20PnP%0Aalgorithms%20amount%20to%20replacing%20suboptimal%20handcrafted%20CS%20priors%20with%20powerful%0Adenoising%20deep%20neural%20network%20%28DNNs%29.%20However%2C%20in%20MRI%20reconstruction%2C%20existing%0APnP%20methods%20often%20yield%20suboptimal%20results%20due%20to%20instabilities%20in%20the%20proximal%0Agradient%20descent%20%28PGD%29%20schemes%20and%20the%20lack%20of%20curated%2C%20noiseless%20datasets%20for%0Atraining%20robust%20denoisers.%20In%20this%20work%2C%20we%20propose%20a%20fully%20unsupervised%0Apreprocessing%20pipeline%20to%20generate%20clean%2C%20noiseless%20complex%20MRI%20signals%20from%0Amulticoil%20data%2C%20enabling%20training%20of%20a%20high-performance%20denoising%20DNN.%0AFurthermore%2C%20we%20introduce%20an%20annealed%20Half-Quadratic%20Splitting%20%28HQS%29%20algorithm%0Ato%20address%20the%20instability%20issues%2C%20leading%20to%20significant%20improvements%20over%0Aexisting%20PnP%20algorithms.%20When%20combined%20with%20preconditioning%20techniques%2C%20our%0Aapproach%20achieves%20state-of-the-art%20results%2C%20providing%20a%20robust%20and%20efficient%0Asolution%20for%20high-quality%20MRI%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520plug-and-play%2520methods%2520for%2520highly%2520accelerated%2520non-Cartesian%2520MRI%250A%2520%2520reconstruction%26entry.906535625%3DPierre-Antoine%2520Comby%2520and%2520Benjamin%2520Lapostolle%2520and%2520Matthieu%2520Terris%2520and%2520Philippe%2520Ciuciu%26entry.1292438233%3D%2520%2520Achieving%2520high-quality%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520reconstruction%2520at%250Aaccelerated%2520acquisition%2520rates%2520remains%2520challenging%2520due%2520to%2520the%2520inherent%2520ill-posed%250Anature%2520of%2520the%2520inverse%2520problem.%2520Traditional%2520Compressed%2520Sensing%2520%2528CS%2529%2520methods%252C%250Awhile%2520robust%2520across%2520varying%2520acquisition%2520settings%252C%2520struggle%2520to%2520maintain%2520good%250Areconstruction%2520quality%2520at%2520high%2520acceleration%2520factors%2520%2528%2524%255Cge%2524%25208%2529.%2520Recent%2520advances%250Ain%2520deep%2520learning%2520have%2520improved%2520reconstruction%2520quality%252C%2520but%2520purely%2520data-driven%250Amethods%2520are%2520prone%2520to%2520overfitting%2520and%2520hallucination%2520effects%252C%2520notably%2520when%2520the%250Aacquisition%2520setting%2520is%2520varying.%2520Plug-and-Play%2520%2528PnP%2529%2520approaches%2520have%2520been%250Aproposed%2520to%2520mitigate%2520the%2520pitfalls%2520of%2520both%2520frameworks.%2520In%2520a%2520nutshell%252C%2520PnP%250Aalgorithms%2520amount%2520to%2520replacing%2520suboptimal%2520handcrafted%2520CS%2520priors%2520with%2520powerful%250Adenoising%2520deep%2520neural%2520network%2520%2528DNNs%2529.%2520However%252C%2520in%2520MRI%2520reconstruction%252C%2520existing%250APnP%2520methods%2520often%2520yield%2520suboptimal%2520results%2520due%2520to%2520instabilities%2520in%2520the%2520proximal%250Agradient%2520descent%2520%2528PGD%2529%2520schemes%2520and%2520the%2520lack%2520of%2520curated%252C%2520noiseless%2520datasets%2520for%250Atraining%2520robust%2520denoisers.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fully%2520unsupervised%250Apreprocessing%2520pipeline%2520to%2520generate%2520clean%252C%2520noiseless%2520complex%2520MRI%2520signals%2520from%250Amulticoil%2520data%252C%2520enabling%2520training%2520of%2520a%2520high-performance%2520denoising%2520DNN.%250AFurthermore%252C%2520we%2520introduce%2520an%2520annealed%2520Half-Quadratic%2520Splitting%2520%2528HQS%2529%2520algorithm%250Ato%2520address%2520the%2520instability%2520issues%252C%2520leading%2520to%2520significant%2520improvements%2520over%250Aexisting%2520PnP%2520algorithms.%2520When%2520combined%2520with%2520preconditioning%2520techniques%252C%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520results%252C%2520providing%2520a%2520robust%2520and%2520efficient%250Asolution%2520for%2520high-quality%2520MRI%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20plug-and-play%20methods%20for%20highly%20accelerated%20non-Cartesian%20MRI%0A%20%20reconstruction&entry.906535625=Pierre-Antoine%20Comby%20and%20Benjamin%20Lapostolle%20and%20Matthieu%20Terris%20and%20Philippe%20Ciuciu&entry.1292438233=%20%20Achieving%20high-quality%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20reconstruction%20at%0Aaccelerated%20acquisition%20rates%20remains%20challenging%20due%20to%20the%20inherent%20ill-posed%0Anature%20of%20the%20inverse%20problem.%20Traditional%20Compressed%20Sensing%20%28CS%29%20methods%2C%0Awhile%20robust%20across%20varying%20acquisition%20settings%2C%20struggle%20to%20maintain%20good%0Areconstruction%20quality%20at%20high%20acceleration%20factors%20%28%24%5Cge%24%208%29.%20Recent%20advances%0Ain%20deep%20learning%20have%20improved%20reconstruction%20quality%2C%20but%20purely%20data-driven%0Amethods%20are%20prone%20to%20overfitting%20and%20hallucination%20effects%2C%20notably%20when%20the%0Aacquisition%20setting%20is%20varying.%20Plug-and-Play%20%28PnP%29%20approaches%20have%20been%0Aproposed%20to%20mitigate%20the%20pitfalls%20of%20both%20frameworks.%20In%20a%20nutshell%2C%20PnP%0Aalgorithms%20amount%20to%20replacing%20suboptimal%20handcrafted%20CS%20priors%20with%20powerful%0Adenoising%20deep%20neural%20network%20%28DNNs%29.%20However%2C%20in%20MRI%20reconstruction%2C%20existing%0APnP%20methods%20often%20yield%20suboptimal%20results%20due%20to%20instabilities%20in%20the%20proximal%0Agradient%20descent%20%28PGD%29%20schemes%20and%20the%20lack%20of%20curated%2C%20noiseless%20datasets%20for%0Atraining%20robust%20denoisers.%20In%20this%20work%2C%20we%20propose%20a%20fully%20unsupervised%0Apreprocessing%20pipeline%20to%20generate%20clean%2C%20noiseless%20complex%20MRI%20signals%20from%0Amulticoil%20data%2C%20enabling%20training%20of%20a%20high-performance%20denoising%20DNN.%0AFurthermore%2C%20we%20introduce%20an%20annealed%20Half-Quadratic%20Splitting%20%28HQS%29%20algorithm%0Ato%20address%20the%20instability%20issues%2C%20leading%20to%20significant%20improvements%20over%0Aexisting%20PnP%20algorithms.%20When%20combined%20with%20preconditioning%20techniques%2C%20our%0Aapproach%20achieves%20state-of-the-art%20results%2C%20providing%20a%20robust%20and%20efficient%0Asolution%20for%20high-quality%20MRI%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01955v1&entry.124074799=Read"},
{"title": "One VLM to Keep it Learning: Generation and Balancing for Data-free\n  Continual Visual Question Answering", "author": "Deepayan Das and Davide Talon and Massimiliano Mancini and Yiming Wang and Elisa Ricci", "abstract": "  Vision-Language Models (VLMs) have shown significant promise in Visual\nQuestion Answering (VQA) tasks by leveraging web-scale multimodal datasets.\nHowever, these models often struggle with continual learning due to\ncatastrophic forgetting when adapting to new tasks. As an effective remedy to\nmitigate catastrophic forgetting, rehearsal strategy uses the data of past\ntasks upon learning new task. However, such strategy incurs the need of storing\npast data, which might not be feasible due to hardware constraints or privacy\nconcerns. In this work, we propose the first data-free method that leverages\nthe language generation capability of a VLM, instead of relying on external\nmodels, to produce pseudo-rehearsal data for addressing continual VQA. Our\nproposal, named as GaB, generates pseudo-rehearsal data by posing previous task\nquestions on new task data. Yet, despite being effective, the distribution of\ngenerated questions skews towards the most frequently posed questions due to\nthe limited and task-specific training data. To mitigate this issue, we\nintroduce a pseudo-rehearsal balancing module that aligns the generated data\ntowards the ground-truth data distribution using either the question\nmeta-statistics or an unsupervised clustering method. We evaluate our proposed\nmethod on two recent benchmarks, \\ie VQACL-VQAv2 and CLOVE-function benchmarks.\nGaB outperforms all the data-free baselines with substantial improvement in\nmaintaining VQA performance across evolving tasks, while being on-par with\nmethods with access to the past data.\n", "link": "http://arxiv.org/abs/2411.02210v1", "date": "2024-11-04", "relevancy": 2.6291, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20VLM%20to%20Keep%20it%20Learning%3A%20Generation%20and%20Balancing%20for%20Data-free%0A%20%20Continual%20Visual%20Question%20Answering&body=Title%3A%20One%20VLM%20to%20Keep%20it%20Learning%3A%20Generation%20and%20Balancing%20for%20Data-free%0A%20%20Continual%20Visual%20Question%20Answering%0AAuthor%3A%20Deepayan%20Das%20and%20Davide%20Talon%20and%20Massimiliano%20Mancini%20and%20Yiming%20Wang%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20significant%20promise%20in%20Visual%0AQuestion%20Answering%20%28VQA%29%20tasks%20by%20leveraging%20web-scale%20multimodal%20datasets.%0AHowever%2C%20these%20models%20often%20struggle%20with%20continual%20learning%20due%20to%0Acatastrophic%20forgetting%20when%20adapting%20to%20new%20tasks.%20As%20an%20effective%20remedy%20to%0Amitigate%20catastrophic%20forgetting%2C%20rehearsal%20strategy%20uses%20the%20data%20of%20past%0Atasks%20upon%20learning%20new%20task.%20However%2C%20such%20strategy%20incurs%20the%20need%20of%20storing%0Apast%20data%2C%20which%20might%20not%20be%20feasible%20due%20to%20hardware%20constraints%20or%20privacy%0Aconcerns.%20In%20this%20work%2C%20we%20propose%20the%20first%20data-free%20method%20that%20leverages%0Athe%20language%20generation%20capability%20of%20a%20VLM%2C%20instead%20of%20relying%20on%20external%0Amodels%2C%20to%20produce%20pseudo-rehearsal%20data%20for%20addressing%20continual%20VQA.%20Our%0Aproposal%2C%20named%20as%20GaB%2C%20generates%20pseudo-rehearsal%20data%20by%20posing%20previous%20task%0Aquestions%20on%20new%20task%20data.%20Yet%2C%20despite%20being%20effective%2C%20the%20distribution%20of%0Agenerated%20questions%20skews%20towards%20the%20most%20frequently%20posed%20questions%20due%20to%0Athe%20limited%20and%20task-specific%20training%20data.%20To%20mitigate%20this%20issue%2C%20we%0Aintroduce%20a%20pseudo-rehearsal%20balancing%20module%20that%20aligns%20the%20generated%20data%0Atowards%20the%20ground-truth%20data%20distribution%20using%20either%20the%20question%0Ameta-statistics%20or%20an%20unsupervised%20clustering%20method.%20We%20evaluate%20our%20proposed%0Amethod%20on%20two%20recent%20benchmarks%2C%20%5Cie%20VQACL-VQAv2%20and%20CLOVE-function%20benchmarks.%0AGaB%20outperforms%20all%20the%20data-free%20baselines%20with%20substantial%20improvement%20in%0Amaintaining%20VQA%20performance%20across%20evolving%20tasks%2C%20while%20being%20on-par%20with%0Amethods%20with%20access%20to%20the%20past%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520VLM%2520to%2520Keep%2520it%2520Learning%253A%2520Generation%2520and%2520Balancing%2520for%2520Data-free%250A%2520%2520Continual%2520Visual%2520Question%2520Answering%26entry.906535625%3DDeepayan%2520Das%2520and%2520Davide%2520Talon%2520and%2520Massimiliano%2520Mancini%2520and%2520Yiming%2520Wang%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520significant%2520promise%2520in%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529%2520tasks%2520by%2520leveraging%2520web-scale%2520multimodal%2520datasets.%250AHowever%252C%2520these%2520models%2520often%2520struggle%2520with%2520continual%2520learning%2520due%2520to%250Acatastrophic%2520forgetting%2520when%2520adapting%2520to%2520new%2520tasks.%2520As%2520an%2520effective%2520remedy%2520to%250Amitigate%2520catastrophic%2520forgetting%252C%2520rehearsal%2520strategy%2520uses%2520the%2520data%2520of%2520past%250Atasks%2520upon%2520learning%2520new%2520task.%2520However%252C%2520such%2520strategy%2520incurs%2520the%2520need%2520of%2520storing%250Apast%2520data%252C%2520which%2520might%2520not%2520be%2520feasible%2520due%2520to%2520hardware%2520constraints%2520or%2520privacy%250Aconcerns.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%2520data-free%2520method%2520that%2520leverages%250Athe%2520language%2520generation%2520capability%2520of%2520a%2520VLM%252C%2520instead%2520of%2520relying%2520on%2520external%250Amodels%252C%2520to%2520produce%2520pseudo-rehearsal%2520data%2520for%2520addressing%2520continual%2520VQA.%2520Our%250Aproposal%252C%2520named%2520as%2520GaB%252C%2520generates%2520pseudo-rehearsal%2520data%2520by%2520posing%2520previous%2520task%250Aquestions%2520on%2520new%2520task%2520data.%2520Yet%252C%2520despite%2520being%2520effective%252C%2520the%2520distribution%2520of%250Agenerated%2520questions%2520skews%2520towards%2520the%2520most%2520frequently%2520posed%2520questions%2520due%2520to%250Athe%2520limited%2520and%2520task-specific%2520training%2520data.%2520To%2520mitigate%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520pseudo-rehearsal%2520balancing%2520module%2520that%2520aligns%2520the%2520generated%2520data%250Atowards%2520the%2520ground-truth%2520data%2520distribution%2520using%2520either%2520the%2520question%250Ameta-statistics%2520or%2520an%2520unsupervised%2520clustering%2520method.%2520We%2520evaluate%2520our%2520proposed%250Amethod%2520on%2520two%2520recent%2520benchmarks%252C%2520%255Cie%2520VQACL-VQAv2%2520and%2520CLOVE-function%2520benchmarks.%250AGaB%2520outperforms%2520all%2520the%2520data-free%2520baselines%2520with%2520substantial%2520improvement%2520in%250Amaintaining%2520VQA%2520performance%2520across%2520evolving%2520tasks%252C%2520while%2520being%2520on-par%2520with%250Amethods%2520with%2520access%2520to%2520the%2520past%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20VLM%20to%20Keep%20it%20Learning%3A%20Generation%20and%20Balancing%20for%20Data-free%0A%20%20Continual%20Visual%20Question%20Answering&entry.906535625=Deepayan%20Das%20and%20Davide%20Talon%20and%20Massimiliano%20Mancini%20and%20Yiming%20Wang%20and%20Elisa%20Ricci&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20significant%20promise%20in%20Visual%0AQuestion%20Answering%20%28VQA%29%20tasks%20by%20leveraging%20web-scale%20multimodal%20datasets.%0AHowever%2C%20these%20models%20often%20struggle%20with%20continual%20learning%20due%20to%0Acatastrophic%20forgetting%20when%20adapting%20to%20new%20tasks.%20As%20an%20effective%20remedy%20to%0Amitigate%20catastrophic%20forgetting%2C%20rehearsal%20strategy%20uses%20the%20data%20of%20past%0Atasks%20upon%20learning%20new%20task.%20However%2C%20such%20strategy%20incurs%20the%20need%20of%20storing%0Apast%20data%2C%20which%20might%20not%20be%20feasible%20due%20to%20hardware%20constraints%20or%20privacy%0Aconcerns.%20In%20this%20work%2C%20we%20propose%20the%20first%20data-free%20method%20that%20leverages%0Athe%20language%20generation%20capability%20of%20a%20VLM%2C%20instead%20of%20relying%20on%20external%0Amodels%2C%20to%20produce%20pseudo-rehearsal%20data%20for%20addressing%20continual%20VQA.%20Our%0Aproposal%2C%20named%20as%20GaB%2C%20generates%20pseudo-rehearsal%20data%20by%20posing%20previous%20task%0Aquestions%20on%20new%20task%20data.%20Yet%2C%20despite%20being%20effective%2C%20the%20distribution%20of%0Agenerated%20questions%20skews%20towards%20the%20most%20frequently%20posed%20questions%20due%20to%0Athe%20limited%20and%20task-specific%20training%20data.%20To%20mitigate%20this%20issue%2C%20we%0Aintroduce%20a%20pseudo-rehearsal%20balancing%20module%20that%20aligns%20the%20generated%20data%0Atowards%20the%20ground-truth%20data%20distribution%20using%20either%20the%20question%0Ameta-statistics%20or%20an%20unsupervised%20clustering%20method.%20We%20evaluate%20our%20proposed%0Amethod%20on%20two%20recent%20benchmarks%2C%20%5Cie%20VQACL-VQAv2%20and%20CLOVE-function%20benchmarks.%0AGaB%20outperforms%20all%20the%20data-free%20baselines%20with%20substantial%20improvement%20in%0Amaintaining%20VQA%20performance%20across%20evolving%20tasks%2C%20while%20being%20on-par%20with%0Amethods%20with%20access%20to%20the%20past%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02210v1&entry.124074799=Read"},
{"title": "Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D\n  Generation", "author": "Xianghui Yang and Huiwen Shi and Bowen Zhang and Fan Yang and Jiacheng Wang and Hongxu Zhao and Xinhai Liu and Xinzhou Wang and Qingxiang Lin and Jiaao Yu and Lifu Wang and Zhuo Chen and Sicong Liu and Yuhong Liu and Yong Yang and Di Wang and Jie Jiang and Chunchao Guo", "abstract": "  While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D-1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. % Extensive\nexperimental results demonstrate the effectiveness of Hunyuan3D-1.0 in\ngenerating high-quality 3D assets. Our framework involves the text-to-image\nmodel ~\\ie, Hunyuan-DiT, making it a unified framework to support both text-\nand image-conditioned 3D generation. Our standard version has $10\\times$ more\nparameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves\nan impressive balance between speed and quality, significantly reducing\ngeneration time while maintaining the quality and diversity of the produced\nassets.\n", "link": "http://arxiv.org/abs/2411.02293v1", "date": "2024-11-04", "relevancy": 2.6239, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6614}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%20Image-to-3D%0A%20%20Generation&body=Title%3A%20Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%20Image-to-3D%0A%20%20Generation%0AAuthor%3A%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Bowen%20Zhang%20and%20Fan%20Yang%20and%20Jiacheng%20Wang%20and%20Hongxu%20Zhao%20and%20Xinhai%20Liu%20and%20Xinzhou%20Wang%20and%20Qingxiang%20Lin%20and%20Jiaao%20Yu%20and%20Lifu%20Wang%20and%20Zhuo%20Chen%20and%20Sicong%20Liu%20and%20Yuhong%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Jie%20Jiang%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20While%203D%20generative%20models%20have%20greatly%20improved%20artists%27%20workflows%2C%20the%0Aexisting%20diffusion%20models%20for%203D%20generation%20suffer%20from%20slow%20generation%20and%0Apoor%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%20approach%0Anamed%20Hunyuan3D-1.0%20including%20a%20lite%20version%20and%20a%20standard%20version%2C%20that%20both%0Asupport%20text-%20and%20image-conditioned%20generation.%20In%20the%20first%20stage%2C%20we%20employ%20a%0Amulti-view%20diffusion%20model%20that%20efficiently%20generates%20multi-view%20RGB%20in%0Aapproximately%204%20seconds.%20These%20multi-view%20images%20capture%20rich%20details%20of%20the%203D%0Aasset%20from%20different%20viewpoints%2C%20relaxing%20the%20tasks%20from%20single-view%20to%0Amulti-view%20reconstruction.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20feed-forward%0Areconstruction%20model%20that%20rapidly%20and%20faithfully%20reconstructs%20the%203D%20asset%0Agiven%20the%20generated%20multi-view%20images%20in%20approximately%207%20seconds.%20The%0Areconstruction%20network%20learns%20to%20handle%20noises%20and%20in-consistency%20introduced%20by%0Athe%20multi-view%20diffusion%20and%20leverages%20the%20available%20information%20from%20the%0Acondition%20image%20to%20efficiently%20recover%20the%203D%20structure.%20%25%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20Hunyuan3D-1.0%20in%0Agenerating%20high-quality%203D%20assets.%20Our%20framework%20involves%20the%20text-to-image%0Amodel%20~%5Cie%2C%20Hunyuan-DiT%2C%20making%20it%20a%20unified%20framework%20to%20support%20both%20text-%0Aand%20image-conditioned%203D%20generation.%20Our%20standard%20version%20has%20%2410%5Ctimes%24%20more%0Aparameters%20than%20our%20lite%20and%20other%20existing%20model.%20Our%20Hunyuan3D-1.0%20achieves%0Aan%20impressive%20balance%20between%20speed%20and%20quality%2C%20significantly%20reducing%0Ageneration%20time%20while%20maintaining%20the%20quality%20and%20diversity%20of%20the%20produced%0Aassets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan3D-1.0%253A%2520A%2520Unified%2520Framework%2520for%2520Text-to-3D%2520and%2520Image-to-3D%250A%2520%2520Generation%26entry.906535625%3DXianghui%2520Yang%2520and%2520Huiwen%2520Shi%2520and%2520Bowen%2520Zhang%2520and%2520Fan%2520Yang%2520and%2520Jiacheng%2520Wang%2520and%2520Hongxu%2520Zhao%2520and%2520Xinhai%2520Liu%2520and%2520Xinzhou%2520Wang%2520and%2520Qingxiang%2520Lin%2520and%2520Jiaao%2520Yu%2520and%2520Lifu%2520Wang%2520and%2520Zhuo%2520Chen%2520and%2520Sicong%2520Liu%2520and%2520Yuhong%2520Liu%2520and%2520Yong%2520Yang%2520and%2520Di%2520Wang%2520and%2520Jie%2520Jiang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520While%25203D%2520generative%2520models%2520have%2520greatly%2520improved%2520artists%2527%2520workflows%252C%2520the%250Aexisting%2520diffusion%2520models%2520for%25203D%2520generation%2520suffer%2520from%2520slow%2520generation%2520and%250Apoor%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520two-stage%2520approach%250Anamed%2520Hunyuan3D-1.0%2520including%2520a%2520lite%2520version%2520and%2520a%2520standard%2520version%252C%2520that%2520both%250Asupport%2520text-%2520and%2520image-conditioned%2520generation.%2520In%2520the%2520first%2520stage%252C%2520we%2520employ%2520a%250Amulti-view%2520diffusion%2520model%2520that%2520efficiently%2520generates%2520multi-view%2520RGB%2520in%250Aapproximately%25204%2520seconds.%2520These%2520multi-view%2520images%2520capture%2520rich%2520details%2520of%2520the%25203D%250Aasset%2520from%2520different%2520viewpoints%252C%2520relaxing%2520the%2520tasks%2520from%2520single-view%2520to%250Amulti-view%2520reconstruction.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520feed-forward%250Areconstruction%2520model%2520that%2520rapidly%2520and%2520faithfully%2520reconstructs%2520the%25203D%2520asset%250Agiven%2520the%2520generated%2520multi-view%2520images%2520in%2520approximately%25207%2520seconds.%2520The%250Areconstruction%2520network%2520learns%2520to%2520handle%2520noises%2520and%2520in-consistency%2520introduced%2520by%250Athe%2520multi-view%2520diffusion%2520and%2520leverages%2520the%2520available%2520information%2520from%2520the%250Acondition%2520image%2520to%2520efficiently%2520recover%2520the%25203D%2520structure.%2520%2525%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520Hunyuan3D-1.0%2520in%250Agenerating%2520high-quality%25203D%2520assets.%2520Our%2520framework%2520involves%2520the%2520text-to-image%250Amodel%2520~%255Cie%252C%2520Hunyuan-DiT%252C%2520making%2520it%2520a%2520unified%2520framework%2520to%2520support%2520both%2520text-%250Aand%2520image-conditioned%25203D%2520generation.%2520Our%2520standard%2520version%2520has%2520%252410%255Ctimes%2524%2520more%250Aparameters%2520than%2520our%2520lite%2520and%2520other%2520existing%2520model.%2520Our%2520Hunyuan3D-1.0%2520achieves%250Aan%2520impressive%2520balance%2520between%2520speed%2520and%2520quality%252C%2520significantly%2520reducing%250Ageneration%2520time%2520while%2520maintaining%2520the%2520quality%2520and%2520diversity%2520of%2520the%2520produced%250Aassets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan3D-1.0%3A%20A%20Unified%20Framework%20for%20Text-to-3D%20and%20Image-to-3D%0A%20%20Generation&entry.906535625=Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Bowen%20Zhang%20and%20Fan%20Yang%20and%20Jiacheng%20Wang%20and%20Hongxu%20Zhao%20and%20Xinhai%20Liu%20and%20Xinzhou%20Wang%20and%20Qingxiang%20Lin%20and%20Jiaao%20Yu%20and%20Lifu%20Wang%20and%20Zhuo%20Chen%20and%20Sicong%20Liu%20and%20Yuhong%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Jie%20Jiang%20and%20Chunchao%20Guo&entry.1292438233=%20%20While%203D%20generative%20models%20have%20greatly%20improved%20artists%27%20workflows%2C%20the%0Aexisting%20diffusion%20models%20for%203D%20generation%20suffer%20from%20slow%20generation%20and%0Apoor%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20two-stage%20approach%0Anamed%20Hunyuan3D-1.0%20including%20a%20lite%20version%20and%20a%20standard%20version%2C%20that%20both%0Asupport%20text-%20and%20image-conditioned%20generation.%20In%20the%20first%20stage%2C%20we%20employ%20a%0Amulti-view%20diffusion%20model%20that%20efficiently%20generates%20multi-view%20RGB%20in%0Aapproximately%204%20seconds.%20These%20multi-view%20images%20capture%20rich%20details%20of%20the%203D%0Aasset%20from%20different%20viewpoints%2C%20relaxing%20the%20tasks%20from%20single-view%20to%0Amulti-view%20reconstruction.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20feed-forward%0Areconstruction%20model%20that%20rapidly%20and%20faithfully%20reconstructs%20the%203D%20asset%0Agiven%20the%20generated%20multi-view%20images%20in%20approximately%207%20seconds.%20The%0Areconstruction%20network%20learns%20to%20handle%20noises%20and%20in-consistency%20introduced%20by%0Athe%20multi-view%20diffusion%20and%20leverages%20the%20available%20information%20from%20the%0Acondition%20image%20to%20efficiently%20recover%20the%203D%20structure.%20%25%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20Hunyuan3D-1.0%20in%0Agenerating%20high-quality%203D%20assets.%20Our%20framework%20involves%20the%20text-to-image%0Amodel%20~%5Cie%2C%20Hunyuan-DiT%2C%20making%20it%20a%20unified%20framework%20to%20support%20both%20text-%0Aand%20image-conditioned%203D%20generation.%20Our%20standard%20version%20has%20%2410%5Ctimes%24%20more%0Aparameters%20than%20our%20lite%20and%20other%20existing%20model.%20Our%20Hunyuan3D-1.0%20achieves%0Aan%20impressive%20balance%20between%20speed%20and%20quality%2C%20significantly%20reducing%0Ageneration%20time%20while%20maintaining%20the%20quality%20and%20diversity%20of%20the%20produced%0Aassets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02293v1&entry.124074799=Read"},
{"title": "Local Loss Optimization in the Infinite Width: Stable Parameterization\n  of Predictive Coding Networks and Target Propagation", "author": "Satoki Ishikawa and Rio Yokota and Ryo Karakida", "abstract": "  Local learning, which trains a network through layer-wise local targets and\nlosses, has been studied as an alternative to backpropagation (BP) in neural\ncomputation. However, its algorithms often become more complex or require\nadditional hyperparameters because of the locality, making it challenging to\nidentify desirable settings in which the algorithm progresses in a stable\nmanner. To provide theoretical and quantitative insights, we introduce the\nmaximal update parameterization ($\\mu$P) in the infinite-width limit for two\nrepresentative designs of local targets: predictive coding (PC) and target\npropagation (TP). We verified that $\\mu$P enables hyperparameter transfer\nacross models of different widths. Furthermore, our analysis revealed unique\nand intriguing properties of $\\mu$P that are not present in conventional BP. By\nanalyzing deep linear networks, we found that PC's gradients interpolate\nbetween first-order and Gauss-Newton-like gradients, depending on the\nparameterization. We demonstrate that, in specific standard settings, PC in the\ninfinite-width limit behaves more similarly to the first-order gradient. For\nTP, even with the standard scaling of the last layer, which differs from\nclassical $\\mu$P, its local loss optimization favors the feature learning\nregime over the kernel regime.\n", "link": "http://arxiv.org/abs/2411.02001v1", "date": "2024-11-04", "relevancy": 2.6182, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5302}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Loss%20Optimization%20in%20the%20Infinite%20Width%3A%20Stable%20Parameterization%0A%20%20of%20Predictive%20Coding%20Networks%20and%20Target%20Propagation&body=Title%3A%20Local%20Loss%20Optimization%20in%20the%20Infinite%20Width%3A%20Stable%20Parameterization%0A%20%20of%20Predictive%20Coding%20Networks%20and%20Target%20Propagation%0AAuthor%3A%20Satoki%20Ishikawa%20and%20Rio%20Yokota%20and%20Ryo%20Karakida%0AAbstract%3A%20%20%20Local%20learning%2C%20which%20trains%20a%20network%20through%20layer-wise%20local%20targets%20and%0Alosses%2C%20has%20been%20studied%20as%20an%20alternative%20to%20backpropagation%20%28BP%29%20in%20neural%0Acomputation.%20However%2C%20its%20algorithms%20often%20become%20more%20complex%20or%20require%0Aadditional%20hyperparameters%20because%20of%20the%20locality%2C%20making%20it%20challenging%20to%0Aidentify%20desirable%20settings%20in%20which%20the%20algorithm%20progresses%20in%20a%20stable%0Amanner.%20To%20provide%20theoretical%20and%20quantitative%20insights%2C%20we%20introduce%20the%0Amaximal%20update%20parameterization%20%28%24%5Cmu%24P%29%20in%20the%20infinite-width%20limit%20for%20two%0Arepresentative%20designs%20of%20local%20targets%3A%20predictive%20coding%20%28PC%29%20and%20target%0Apropagation%20%28TP%29.%20We%20verified%20that%20%24%5Cmu%24P%20enables%20hyperparameter%20transfer%0Aacross%20models%20of%20different%20widths.%20Furthermore%2C%20our%20analysis%20revealed%20unique%0Aand%20intriguing%20properties%20of%20%24%5Cmu%24P%20that%20are%20not%20present%20in%20conventional%20BP.%20By%0Aanalyzing%20deep%20linear%20networks%2C%20we%20found%20that%20PC%27s%20gradients%20interpolate%0Abetween%20first-order%20and%20Gauss-Newton-like%20gradients%2C%20depending%20on%20the%0Aparameterization.%20We%20demonstrate%20that%2C%20in%20specific%20standard%20settings%2C%20PC%20in%20the%0Ainfinite-width%20limit%20behaves%20more%20similarly%20to%20the%20first-order%20gradient.%20For%0ATP%2C%20even%20with%20the%20standard%20scaling%20of%20the%20last%20layer%2C%20which%20differs%20from%0Aclassical%20%24%5Cmu%24P%2C%20its%20local%20loss%20optimization%20favors%20the%20feature%20learning%0Aregime%20over%20the%20kernel%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Loss%2520Optimization%2520in%2520the%2520Infinite%2520Width%253A%2520Stable%2520Parameterization%250A%2520%2520of%2520Predictive%2520Coding%2520Networks%2520and%2520Target%2520Propagation%26entry.906535625%3DSatoki%2520Ishikawa%2520and%2520Rio%2520Yokota%2520and%2520Ryo%2520Karakida%26entry.1292438233%3D%2520%2520Local%2520learning%252C%2520which%2520trains%2520a%2520network%2520through%2520layer-wise%2520local%2520targets%2520and%250Alosses%252C%2520has%2520been%2520studied%2520as%2520an%2520alternative%2520to%2520backpropagation%2520%2528BP%2529%2520in%2520neural%250Acomputation.%2520However%252C%2520its%2520algorithms%2520often%2520become%2520more%2520complex%2520or%2520require%250Aadditional%2520hyperparameters%2520because%2520of%2520the%2520locality%252C%2520making%2520it%2520challenging%2520to%250Aidentify%2520desirable%2520settings%2520in%2520which%2520the%2520algorithm%2520progresses%2520in%2520a%2520stable%250Amanner.%2520To%2520provide%2520theoretical%2520and%2520quantitative%2520insights%252C%2520we%2520introduce%2520the%250Amaximal%2520update%2520parameterization%2520%2528%2524%255Cmu%2524P%2529%2520in%2520the%2520infinite-width%2520limit%2520for%2520two%250Arepresentative%2520designs%2520of%2520local%2520targets%253A%2520predictive%2520coding%2520%2528PC%2529%2520and%2520target%250Apropagation%2520%2528TP%2529.%2520We%2520verified%2520that%2520%2524%255Cmu%2524P%2520enables%2520hyperparameter%2520transfer%250Aacross%2520models%2520of%2520different%2520widths.%2520Furthermore%252C%2520our%2520analysis%2520revealed%2520unique%250Aand%2520intriguing%2520properties%2520of%2520%2524%255Cmu%2524P%2520that%2520are%2520not%2520present%2520in%2520conventional%2520BP.%2520By%250Aanalyzing%2520deep%2520linear%2520networks%252C%2520we%2520found%2520that%2520PC%2527s%2520gradients%2520interpolate%250Abetween%2520first-order%2520and%2520Gauss-Newton-like%2520gradients%252C%2520depending%2520on%2520the%250Aparameterization.%2520We%2520demonstrate%2520that%252C%2520in%2520specific%2520standard%2520settings%252C%2520PC%2520in%2520the%250Ainfinite-width%2520limit%2520behaves%2520more%2520similarly%2520to%2520the%2520first-order%2520gradient.%2520For%250ATP%252C%2520even%2520with%2520the%2520standard%2520scaling%2520of%2520the%2520last%2520layer%252C%2520which%2520differs%2520from%250Aclassical%2520%2524%255Cmu%2524P%252C%2520its%2520local%2520loss%2520optimization%2520favors%2520the%2520feature%2520learning%250Aregime%2520over%2520the%2520kernel%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Loss%20Optimization%20in%20the%20Infinite%20Width%3A%20Stable%20Parameterization%0A%20%20of%20Predictive%20Coding%20Networks%20and%20Target%20Propagation&entry.906535625=Satoki%20Ishikawa%20and%20Rio%20Yokota%20and%20Ryo%20Karakida&entry.1292438233=%20%20Local%20learning%2C%20which%20trains%20a%20network%20through%20layer-wise%20local%20targets%20and%0Alosses%2C%20has%20been%20studied%20as%20an%20alternative%20to%20backpropagation%20%28BP%29%20in%20neural%0Acomputation.%20However%2C%20its%20algorithms%20often%20become%20more%20complex%20or%20require%0Aadditional%20hyperparameters%20because%20of%20the%20locality%2C%20making%20it%20challenging%20to%0Aidentify%20desirable%20settings%20in%20which%20the%20algorithm%20progresses%20in%20a%20stable%0Amanner.%20To%20provide%20theoretical%20and%20quantitative%20insights%2C%20we%20introduce%20the%0Amaximal%20update%20parameterization%20%28%24%5Cmu%24P%29%20in%20the%20infinite-width%20limit%20for%20two%0Arepresentative%20designs%20of%20local%20targets%3A%20predictive%20coding%20%28PC%29%20and%20target%0Apropagation%20%28TP%29.%20We%20verified%20that%20%24%5Cmu%24P%20enables%20hyperparameter%20transfer%0Aacross%20models%20of%20different%20widths.%20Furthermore%2C%20our%20analysis%20revealed%20unique%0Aand%20intriguing%20properties%20of%20%24%5Cmu%24P%20that%20are%20not%20present%20in%20conventional%20BP.%20By%0Aanalyzing%20deep%20linear%20networks%2C%20we%20found%20that%20PC%27s%20gradients%20interpolate%0Abetween%20first-order%20and%20Gauss-Newton-like%20gradients%2C%20depending%20on%20the%0Aparameterization.%20We%20demonstrate%20that%2C%20in%20specific%20standard%20settings%2C%20PC%20in%20the%0Ainfinite-width%20limit%20behaves%20more%20similarly%20to%20the%20first-order%20gradient.%20For%0ATP%2C%20even%20with%20the%20standard%20scaling%20of%20the%20last%20layer%2C%20which%20differs%20from%0Aclassical%20%24%5Cmu%24P%2C%20its%20local%20loss%20optimization%20favors%20the%20feature%20learning%0Aregime%20over%20the%20kernel%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02001v1&entry.124074799=Read"},
{"title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units", "author": "Badr AlKhamissi and Greta Tuckute and Antoine Bosselut and Martin Schrimpf", "abstract": "  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n", "link": "http://arxiv.org/abs/2411.02280v1", "date": "2024-11-04", "relevancy": 2.5948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units&body=Title%3A%20The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units%0AAuthor%3A%20Badr%20AlKhamissi%20and%20Greta%20Tuckute%20and%20Antoine%20Bosselut%20and%20Martin%20Schrimpf%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20on%20not%20just%0Alanguage%20tasks%2C%20but%20also%20various%20tasks%20that%20are%20not%20linguistic%20in%20nature%2C%20such%0Aas%20logical%20reasoning%20and%20social%20inference.%20In%20the%20human%20brain%2C%20neuroscience%20has%0Aidentified%20a%20core%20language%20system%20that%20selectively%20and%20causally%20supports%0Alanguage%20processing.%20We%20here%20ask%20whether%20similar%20specialization%20for%20language%0Aemerges%20in%20LLMs.%20We%20identify%20language-selective%20units%20within%2018%20popular%20LLMs%2C%0Ausing%20the%20same%20localization%20approach%20that%20is%20used%20in%20neuroscience.%20We%20then%0Aestablish%20the%20causal%20role%20of%20these%20units%20by%20demonstrating%20that%20ablating%20LLM%0Alanguage-selective%20units%20--%20but%20not%20random%20units%20--%20leads%20to%20drastic%20deficits%0Ain%20language%20tasks.%20Correspondingly%2C%20language-selective%20LLM%20units%20are%20more%0Aaligned%20to%20brain%20recordings%20from%20the%20human%20language%20system%20than%20random%20units.%0AFinally%2C%20we%20investigate%20whether%20our%20localization%20method%20extends%20to%20other%0Acognitive%20domains%3A%20while%20we%20find%20specialized%20networks%20in%20some%20LLMs%20for%0Areasoning%20and%20social%20capabilities%2C%20there%20are%20substantial%20differences%20among%0Amodels.%20These%20findings%20provide%20functional%20and%20causal%20evidence%20for%0Aspecialization%20in%20large%20language%20models%2C%20and%20highlight%20parallels%20with%20the%0Afunctional%20organization%20in%20the%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520LLM%2520Language%2520Network%253A%2520A%2520Neuroscientific%2520Approach%2520for%2520Identifying%250A%2520%2520Causally%2520Task-Relevant%2520Units%26entry.906535625%3DBadr%2520AlKhamissi%2520and%2520Greta%2520Tuckute%2520and%2520Antoine%2520Bosselut%2520and%2520Martin%2520Schrimpf%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520capabilities%2520on%2520not%2520just%250Alanguage%2520tasks%252C%2520but%2520also%2520various%2520tasks%2520that%2520are%2520not%2520linguistic%2520in%2520nature%252C%2520such%250Aas%2520logical%2520reasoning%2520and%2520social%2520inference.%2520In%2520the%2520human%2520brain%252C%2520neuroscience%2520has%250Aidentified%2520a%2520core%2520language%2520system%2520that%2520selectively%2520and%2520causally%2520supports%250Alanguage%2520processing.%2520We%2520here%2520ask%2520whether%2520similar%2520specialization%2520for%2520language%250Aemerges%2520in%2520LLMs.%2520We%2520identify%2520language-selective%2520units%2520within%252018%2520popular%2520LLMs%252C%250Ausing%2520the%2520same%2520localization%2520approach%2520that%2520is%2520used%2520in%2520neuroscience.%2520We%2520then%250Aestablish%2520the%2520causal%2520role%2520of%2520these%2520units%2520by%2520demonstrating%2520that%2520ablating%2520LLM%250Alanguage-selective%2520units%2520--%2520but%2520not%2520random%2520units%2520--%2520leads%2520to%2520drastic%2520deficits%250Ain%2520language%2520tasks.%2520Correspondingly%252C%2520language-selective%2520LLM%2520units%2520are%2520more%250Aaligned%2520to%2520brain%2520recordings%2520from%2520the%2520human%2520language%2520system%2520than%2520random%2520units.%250AFinally%252C%2520we%2520investigate%2520whether%2520our%2520localization%2520method%2520extends%2520to%2520other%250Acognitive%2520domains%253A%2520while%2520we%2520find%2520specialized%2520networks%2520in%2520some%2520LLMs%2520for%250Areasoning%2520and%2520social%2520capabilities%252C%2520there%2520are%2520substantial%2520differences%2520among%250Amodels.%2520These%2520findings%2520provide%2520functional%2520and%2520causal%2520evidence%2520for%250Aspecialization%2520in%2520large%2520language%2520models%252C%2520and%2520highlight%2520parallels%2520with%2520the%250Afunctional%2520organization%2520in%2520the%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units&entry.906535625=Badr%20AlKhamissi%20and%20Greta%20Tuckute%20and%20Antoine%20Bosselut%20and%20Martin%20Schrimpf&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20on%20not%20just%0Alanguage%20tasks%2C%20but%20also%20various%20tasks%20that%20are%20not%20linguistic%20in%20nature%2C%20such%0Aas%20logical%20reasoning%20and%20social%20inference.%20In%20the%20human%20brain%2C%20neuroscience%20has%0Aidentified%20a%20core%20language%20system%20that%20selectively%20and%20causally%20supports%0Alanguage%20processing.%20We%20here%20ask%20whether%20similar%20specialization%20for%20language%0Aemerges%20in%20LLMs.%20We%20identify%20language-selective%20units%20within%2018%20popular%20LLMs%2C%0Ausing%20the%20same%20localization%20approach%20that%20is%20used%20in%20neuroscience.%20We%20then%0Aestablish%20the%20causal%20role%20of%20these%20units%20by%20demonstrating%20that%20ablating%20LLM%0Alanguage-selective%20units%20--%20but%20not%20random%20units%20--%20leads%20to%20drastic%20deficits%0Ain%20language%20tasks.%20Correspondingly%2C%20language-selective%20LLM%20units%20are%20more%0Aaligned%20to%20brain%20recordings%20from%20the%20human%20language%20system%20than%20random%20units.%0AFinally%2C%20we%20investigate%20whether%20our%20localization%20method%20extends%20to%20other%0Acognitive%20domains%3A%20while%20we%20find%20specialized%20networks%20in%20some%20LLMs%20for%0Areasoning%20and%20social%20capabilities%2C%20there%20are%20substantial%20differences%20among%0Amodels.%20These%20findings%20provide%20functional%20and%20causal%20evidence%20for%0Aspecialization%20in%20large%20language%20models%2C%20and%20highlight%20parallels%20with%20the%0Afunctional%20organization%20in%20the%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02280v1&entry.124074799=Read"},
{"title": "Private Attribute Inference from Images with Vision-Language Models", "author": "Batuhan T\u00f6mek\u00e7e and Mark Vero and Robin Staab and Martin Vechev", "abstract": "  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.\n", "link": "http://arxiv.org/abs/2404.10618v2", "date": "2024-11-04", "relevancy": 2.5944, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Private%20Attribute%20Inference%20from%20Images%20with%20Vision-Language%20Models&body=Title%3A%20Private%20Attribute%20Inference%20from%20Images%20with%20Vision-Language%20Models%0AAuthor%3A%20Batuhan%20T%C3%B6mek%C3%A7e%20and%20Mark%20Vero%20and%20Robin%20Staab%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20ubiquitous%20in%20our%20daily%20tasks%20and%0Adigital%20interactions%2C%20associated%20privacy%20risks%20are%20increasingly%20in%20focus.%20While%0ALLM%20privacy%20research%20has%20primarily%20focused%20on%20the%20leakage%20of%20model%20training%0Adata%2C%20it%20has%20recently%20been%20shown%20that%20LLMs%20can%20make%20accurate%20privacy-infringing%0Ainferences%20from%20previously%20unseen%20texts.%20With%20the%20rise%20of%20vision-language%0Amodels%20%28VLMs%29%2C%20capable%20of%20understanding%20both%20images%20and%20text%2C%20a%20key%20question%20is%0Awhether%20this%20concern%20transfers%20to%20the%20previously%20unexplored%20domain%20of%20benign%0Aimages%20posted%20online.%20To%20answer%20this%20question%2C%20we%20compile%20an%20image%20dataset%20with%0Ahuman-annotated%20labels%20of%20the%20image%20owner%27s%20personal%20attributes.%20In%20order%20to%0Aunderstand%20the%20privacy%20risks%20posed%20by%20VLMs%20beyond%20traditional%20human%20attribute%0Arecognition%2C%20our%20dataset%20consists%20of%20images%20where%20the%20inferable%20private%0Aattributes%20do%20not%20stem%20from%20direct%20depictions%20of%20humans.%20On%20this%20dataset%2C%20we%0Aevaluate%207%20state-of-the-art%20VLMs%2C%20finding%20that%20they%20can%20infer%20various%20personal%0Aattributes%20at%20up%20to%2077.6%25%20accuracy.%20Concerningly%2C%20we%20observe%20that%20accuracy%0Ascales%20with%20the%20general%20capabilities%20of%20the%20models%2C%20implying%20that%20future%20models%0Acan%20be%20misused%20as%20stronger%20inferential%20adversaries%2C%20establishing%20an%20imperative%0Afor%20the%20development%20of%20adequate%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivate%2520Attribute%2520Inference%2520from%2520Images%2520with%2520Vision-Language%2520Models%26entry.906535625%3DBatuhan%2520T%25C3%25B6mek%25C3%25A7e%2520and%2520Mark%2520Vero%2520and%2520Robin%2520Staab%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520ubiquitous%2520in%2520our%2520daily%2520tasks%2520and%250Adigital%2520interactions%252C%2520associated%2520privacy%2520risks%2520are%2520increasingly%2520in%2520focus.%2520While%250ALLM%2520privacy%2520research%2520has%2520primarily%2520focused%2520on%2520the%2520leakage%2520of%2520model%2520training%250Adata%252C%2520it%2520has%2520recently%2520been%2520shown%2520that%2520LLMs%2520can%2520make%2520accurate%2520privacy-infringing%250Ainferences%2520from%2520previously%2520unseen%2520texts.%2520With%2520the%2520rise%2520of%2520vision-language%250Amodels%2520%2528VLMs%2529%252C%2520capable%2520of%2520understanding%2520both%2520images%2520and%2520text%252C%2520a%2520key%2520question%2520is%250Awhether%2520this%2520concern%2520transfers%2520to%2520the%2520previously%2520unexplored%2520domain%2520of%2520benign%250Aimages%2520posted%2520online.%2520To%2520answer%2520this%2520question%252C%2520we%2520compile%2520an%2520image%2520dataset%2520with%250Ahuman-annotated%2520labels%2520of%2520the%2520image%2520owner%2527s%2520personal%2520attributes.%2520In%2520order%2520to%250Aunderstand%2520the%2520privacy%2520risks%2520posed%2520by%2520VLMs%2520beyond%2520traditional%2520human%2520attribute%250Arecognition%252C%2520our%2520dataset%2520consists%2520of%2520images%2520where%2520the%2520inferable%2520private%250Aattributes%2520do%2520not%2520stem%2520from%2520direct%2520depictions%2520of%2520humans.%2520On%2520this%2520dataset%252C%2520we%250Aevaluate%25207%2520state-of-the-art%2520VLMs%252C%2520finding%2520that%2520they%2520can%2520infer%2520various%2520personal%250Aattributes%2520at%2520up%2520to%252077.6%2525%2520accuracy.%2520Concerningly%252C%2520we%2520observe%2520that%2520accuracy%250Ascales%2520with%2520the%2520general%2520capabilities%2520of%2520the%2520models%252C%2520implying%2520that%2520future%2520models%250Acan%2520be%2520misused%2520as%2520stronger%2520inferential%2520adversaries%252C%2520establishing%2520an%2520imperative%250Afor%2520the%2520development%2520of%2520adequate%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20Attribute%20Inference%20from%20Images%20with%20Vision-Language%20Models&entry.906535625=Batuhan%20T%C3%B6mek%C3%A7e%20and%20Mark%20Vero%20and%20Robin%20Staab%20and%20Martin%20Vechev&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20ubiquitous%20in%20our%20daily%20tasks%20and%0Adigital%20interactions%2C%20associated%20privacy%20risks%20are%20increasingly%20in%20focus.%20While%0ALLM%20privacy%20research%20has%20primarily%20focused%20on%20the%20leakage%20of%20model%20training%0Adata%2C%20it%20has%20recently%20been%20shown%20that%20LLMs%20can%20make%20accurate%20privacy-infringing%0Ainferences%20from%20previously%20unseen%20texts.%20With%20the%20rise%20of%20vision-language%0Amodels%20%28VLMs%29%2C%20capable%20of%20understanding%20both%20images%20and%20text%2C%20a%20key%20question%20is%0Awhether%20this%20concern%20transfers%20to%20the%20previously%20unexplored%20domain%20of%20benign%0Aimages%20posted%20online.%20To%20answer%20this%20question%2C%20we%20compile%20an%20image%20dataset%20with%0Ahuman-annotated%20labels%20of%20the%20image%20owner%27s%20personal%20attributes.%20In%20order%20to%0Aunderstand%20the%20privacy%20risks%20posed%20by%20VLMs%20beyond%20traditional%20human%20attribute%0Arecognition%2C%20our%20dataset%20consists%20of%20images%20where%20the%20inferable%20private%0Aattributes%20do%20not%20stem%20from%20direct%20depictions%20of%20humans.%20On%20this%20dataset%2C%20we%0Aevaluate%207%20state-of-the-art%20VLMs%2C%20finding%20that%20they%20can%20infer%20various%20personal%0Aattributes%20at%20up%20to%2077.6%25%20accuracy.%20Concerningly%2C%20we%20observe%20that%20accuracy%0Ascales%20with%20the%20general%20capabilities%20of%20the%20models%2C%20implying%20that%20future%20models%0Acan%20be%20misused%20as%20stronger%20inferential%20adversaries%2C%20establishing%20an%20imperative%0Afor%20the%20development%20of%20adequate%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10618v2&entry.124074799=Read"},
{"title": "UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph\n  Attention Networks Clustering", "author": "A. Mudit Adityaja and Saurabh J. Shigwan and Nitin Kumar", "abstract": "  The data-intensive nature of supervised classification drives the interest of\nthe researchers towards unsupervised approaches, especially for problems such\nas medical image segmentation, where labeled data is scarce. Building on the\nrecent advancements of Vision transformers (ViT) in computer vision, we propose\nan unsupervised segmentation framework using a pre-trained Dino-ViT. In the\nproposed method, we leverage the inherent graph structure within the image to\nrealize a significant performance gain for segmentation in medical images. For\nthis, we introduce a modularity-based loss function coupled with a Graph\nAttention Network (GAT) to effectively capture the inherent graph topology\nwithin the image. Our method achieves state-of-the-art performance, even\nsignificantly surpassing or matching that of existing (semi)supervised\ntechnique such as MedSAM which is a Segment Anything Model in medical images.\nWe demonstrate this using two challenging medical image datasets ISIC-2018 and\nCVC-ColonDB. This work underscores the potential of unsupervised approaches in\nadvancing medical image analysis in scenarios where labeled data is scarce. The\ngithub repository of the code is available on\n[https://github.com/mudit-adityaja/UnSegMedGAT].\n", "link": "http://arxiv.org/abs/2411.01966v1", "date": "2024-11-04", "relevancy": 2.5928, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5126}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnSegMedGAT%3A%20Unsupervised%20Medical%20Image%20Segmentation%20using%20Graph%0A%20%20Attention%20Networks%20Clustering&body=Title%3A%20UnSegMedGAT%3A%20Unsupervised%20Medical%20Image%20Segmentation%20using%20Graph%0A%20%20Attention%20Networks%20Clustering%0AAuthor%3A%20A.%20Mudit%20Adityaja%20and%20Saurabh%20J.%20Shigwan%20and%20Nitin%20Kumar%0AAbstract%3A%20%20%20The%20data-intensive%20nature%20of%20supervised%20classification%20drives%20the%20interest%20of%0Athe%20researchers%20towards%20unsupervised%20approaches%2C%20especially%20for%20problems%20such%0Aas%20medical%20image%20segmentation%2C%20where%20labeled%20data%20is%20scarce.%20Building%20on%20the%0Arecent%20advancements%20of%20Vision%20transformers%20%28ViT%29%20in%20computer%20vision%2C%20we%20propose%0Aan%20unsupervised%20segmentation%20framework%20using%20a%20pre-trained%20Dino-ViT.%20In%20the%0Aproposed%20method%2C%20we%20leverage%20the%20inherent%20graph%20structure%20within%20the%20image%20to%0Arealize%20a%20significant%20performance%20gain%20for%20segmentation%20in%20medical%20images.%20For%0Athis%2C%20we%20introduce%20a%20modularity-based%20loss%20function%20coupled%20with%20a%20Graph%0AAttention%20Network%20%28GAT%29%20to%20effectively%20capture%20the%20inherent%20graph%20topology%0Awithin%20the%20image.%20Our%20method%20achieves%20state-of-the-art%20performance%2C%20even%0Asignificantly%20surpassing%20or%20matching%20that%20of%20existing%20%28semi%29supervised%0Atechnique%20such%20as%20MedSAM%20which%20is%20a%20Segment%20Anything%20Model%20in%20medical%20images.%0AWe%20demonstrate%20this%20using%20two%20challenging%20medical%20image%20datasets%20ISIC-2018%20and%0ACVC-ColonDB.%20This%20work%20underscores%20the%20potential%20of%20unsupervised%20approaches%20in%0Aadvancing%20medical%20image%20analysis%20in%20scenarios%20where%20labeled%20data%20is%20scarce.%20The%0Agithub%20repository%20of%20the%20code%20is%20available%20on%0A%5Bhttps%3A//github.com/mudit-adityaja/UnSegMedGAT%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnSegMedGAT%253A%2520Unsupervised%2520Medical%2520Image%2520Segmentation%2520using%2520Graph%250A%2520%2520Attention%2520Networks%2520Clustering%26entry.906535625%3DA.%2520Mudit%2520Adityaja%2520and%2520Saurabh%2520J.%2520Shigwan%2520and%2520Nitin%2520Kumar%26entry.1292438233%3D%2520%2520The%2520data-intensive%2520nature%2520of%2520supervised%2520classification%2520drives%2520the%2520interest%2520of%250Athe%2520researchers%2520towards%2520unsupervised%2520approaches%252C%2520especially%2520for%2520problems%2520such%250Aas%2520medical%2520image%2520segmentation%252C%2520where%2520labeled%2520data%2520is%2520scarce.%2520Building%2520on%2520the%250Arecent%2520advancements%2520of%2520Vision%2520transformers%2520%2528ViT%2529%2520in%2520computer%2520vision%252C%2520we%2520propose%250Aan%2520unsupervised%2520segmentation%2520framework%2520using%2520a%2520pre-trained%2520Dino-ViT.%2520In%2520the%250Aproposed%2520method%252C%2520we%2520leverage%2520the%2520inherent%2520graph%2520structure%2520within%2520the%2520image%2520to%250Arealize%2520a%2520significant%2520performance%2520gain%2520for%2520segmentation%2520in%2520medical%2520images.%2520For%250Athis%252C%2520we%2520introduce%2520a%2520modularity-based%2520loss%2520function%2520coupled%2520with%2520a%2520Graph%250AAttention%2520Network%2520%2528GAT%2529%2520to%2520effectively%2520capture%2520the%2520inherent%2520graph%2520topology%250Awithin%2520the%2520image.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%2520even%250Asignificantly%2520surpassing%2520or%2520matching%2520that%2520of%2520existing%2520%2528semi%2529supervised%250Atechnique%2520such%2520as%2520MedSAM%2520which%2520is%2520a%2520Segment%2520Anything%2520Model%2520in%2520medical%2520images.%250AWe%2520demonstrate%2520this%2520using%2520two%2520challenging%2520medical%2520image%2520datasets%2520ISIC-2018%2520and%250ACVC-ColonDB.%2520This%2520work%2520underscores%2520the%2520potential%2520of%2520unsupervised%2520approaches%2520in%250Aadvancing%2520medical%2520image%2520analysis%2520in%2520scenarios%2520where%2520labeled%2520data%2520is%2520scarce.%2520The%250Agithub%2520repository%2520of%2520the%2520code%2520is%2520available%2520on%250A%255Bhttps%253A//github.com/mudit-adityaja/UnSegMedGAT%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnSegMedGAT%3A%20Unsupervised%20Medical%20Image%20Segmentation%20using%20Graph%0A%20%20Attention%20Networks%20Clustering&entry.906535625=A.%20Mudit%20Adityaja%20and%20Saurabh%20J.%20Shigwan%20and%20Nitin%20Kumar&entry.1292438233=%20%20The%20data-intensive%20nature%20of%20supervised%20classification%20drives%20the%20interest%20of%0Athe%20researchers%20towards%20unsupervised%20approaches%2C%20especially%20for%20problems%20such%0Aas%20medical%20image%20segmentation%2C%20where%20labeled%20data%20is%20scarce.%20Building%20on%20the%0Arecent%20advancements%20of%20Vision%20transformers%20%28ViT%29%20in%20computer%20vision%2C%20we%20propose%0Aan%20unsupervised%20segmentation%20framework%20using%20a%20pre-trained%20Dino-ViT.%20In%20the%0Aproposed%20method%2C%20we%20leverage%20the%20inherent%20graph%20structure%20within%20the%20image%20to%0Arealize%20a%20significant%20performance%20gain%20for%20segmentation%20in%20medical%20images.%20For%0Athis%2C%20we%20introduce%20a%20modularity-based%20loss%20function%20coupled%20with%20a%20Graph%0AAttention%20Network%20%28GAT%29%20to%20effectively%20capture%20the%20inherent%20graph%20topology%0Awithin%20the%20image.%20Our%20method%20achieves%20state-of-the-art%20performance%2C%20even%0Asignificantly%20surpassing%20or%20matching%20that%20of%20existing%20%28semi%29supervised%0Atechnique%20such%20as%20MedSAM%20which%20is%20a%20Segment%20Anything%20Model%20in%20medical%20images.%0AWe%20demonstrate%20this%20using%20two%20challenging%20medical%20image%20datasets%20ISIC-2018%20and%0ACVC-ColonDB.%20This%20work%20underscores%20the%20potential%20of%20unsupervised%20approaches%20in%0Aadvancing%20medical%20image%20analysis%20in%20scenarios%20where%20labeled%20data%20is%20scarce.%20The%0Agithub%20repository%20of%20the%20code%20is%20available%20on%0A%5Bhttps%3A//github.com/mudit-adityaja/UnSegMedGAT%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01966v1&entry.124074799=Read"},
{"title": "Deep Learning for Leopard Individual Identification: An Adaptive Angular\n  Margin Approach", "author": "David Colomer Matachana", "abstract": "  Accurate identification of individual leopards across camera trap images is\ncritical for population monitoring and ecological studies. This paper\nintroduces a deep learning framework to distinguish between individual leopards\nbased on their unique spot patterns. This approach employs a novel adaptive\nangular margin method in the form of a modified CosFace architecture. In\naddition, I propose a preprocessing pipeline that combines RGB channels with an\nedge detection channel to underscore the critical features learned by the\nmodel.\n  This approach significantly outperforms the Triplet Network baseline,\nachieving a Dynamic Top-5 Average Precision of 0.8814 and a Top-5 Rank Match\nDetection of 0.9533, demonstrating its potential for open-set learning in\nwildlife identification. While not surpassing the performance of the SIFT-based\nHotspotter algorithm, this method represents a substantial advancement in\napplying deep learning to patterned wildlife identification.\n  This research contributes to the field of computer vision and provides a\nvaluable tool for biologists aiming to study and protect leopard populations.\nIt also serves as a stepping stone for applying the power of deep learning in\nCapture-Recapture studies for other patterned species.\n", "link": "http://arxiv.org/abs/2411.01962v1", "date": "2024-11-04", "relevancy": 2.5895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5234}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Leopard%20Individual%20Identification%3A%20An%20Adaptive%20Angular%0A%20%20Margin%20Approach&body=Title%3A%20Deep%20Learning%20for%20Leopard%20Individual%20Identification%3A%20An%20Adaptive%20Angular%0A%20%20Margin%20Approach%0AAuthor%3A%20David%20Colomer%20Matachana%0AAbstract%3A%20%20%20Accurate%20identification%20of%20individual%20leopards%20across%20camera%20trap%20images%20is%0Acritical%20for%20population%20monitoring%20and%20ecological%20studies.%20This%20paper%0Aintroduces%20a%20deep%20learning%20framework%20to%20distinguish%20between%20individual%20leopards%0Abased%20on%20their%20unique%20spot%20patterns.%20This%20approach%20employs%20a%20novel%20adaptive%0Aangular%20margin%20method%20in%20the%20form%20of%20a%20modified%20CosFace%20architecture.%20In%0Aaddition%2C%20I%20propose%20a%20preprocessing%20pipeline%20that%20combines%20RGB%20channels%20with%20an%0Aedge%20detection%20channel%20to%20underscore%20the%20critical%20features%20learned%20by%20the%0Amodel.%0A%20%20This%20approach%20significantly%20outperforms%20the%20Triplet%20Network%20baseline%2C%0Aachieving%20a%20Dynamic%20Top-5%20Average%20Precision%20of%200.8814%20and%20a%20Top-5%20Rank%20Match%0ADetection%20of%200.9533%2C%20demonstrating%20its%20potential%20for%20open-set%20learning%20in%0Awildlife%20identification.%20While%20not%20surpassing%20the%20performance%20of%20the%20SIFT-based%0AHotspotter%20algorithm%2C%20this%20method%20represents%20a%20substantial%20advancement%20in%0Aapplying%20deep%20learning%20to%20patterned%20wildlife%20identification.%0A%20%20This%20research%20contributes%20to%20the%20field%20of%20computer%20vision%20and%20provides%20a%0Avaluable%20tool%20for%20biologists%20aiming%20to%20study%20and%20protect%20leopard%20populations.%0AIt%20also%20serves%20as%20a%20stepping%20stone%20for%20applying%20the%20power%20of%20deep%20learning%20in%0ACapture-Recapture%20studies%20for%20other%20patterned%20species.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Leopard%2520Individual%2520Identification%253A%2520An%2520Adaptive%2520Angular%250A%2520%2520Margin%2520Approach%26entry.906535625%3DDavid%2520Colomer%2520Matachana%26entry.1292438233%3D%2520%2520Accurate%2520identification%2520of%2520individual%2520leopards%2520across%2520camera%2520trap%2520images%2520is%250Acritical%2520for%2520population%2520monitoring%2520and%2520ecological%2520studies.%2520This%2520paper%250Aintroduces%2520a%2520deep%2520learning%2520framework%2520to%2520distinguish%2520between%2520individual%2520leopards%250Abased%2520on%2520their%2520unique%2520spot%2520patterns.%2520This%2520approach%2520employs%2520a%2520novel%2520adaptive%250Aangular%2520margin%2520method%2520in%2520the%2520form%2520of%2520a%2520modified%2520CosFace%2520architecture.%2520In%250Aaddition%252C%2520I%2520propose%2520a%2520preprocessing%2520pipeline%2520that%2520combines%2520RGB%2520channels%2520with%2520an%250Aedge%2520detection%2520channel%2520to%2520underscore%2520the%2520critical%2520features%2520learned%2520by%2520the%250Amodel.%250A%2520%2520This%2520approach%2520significantly%2520outperforms%2520the%2520Triplet%2520Network%2520baseline%252C%250Aachieving%2520a%2520Dynamic%2520Top-5%2520Average%2520Precision%2520of%25200.8814%2520and%2520a%2520Top-5%2520Rank%2520Match%250ADetection%2520of%25200.9533%252C%2520demonstrating%2520its%2520potential%2520for%2520open-set%2520learning%2520in%250Awildlife%2520identification.%2520While%2520not%2520surpassing%2520the%2520performance%2520of%2520the%2520SIFT-based%250AHotspotter%2520algorithm%252C%2520this%2520method%2520represents%2520a%2520substantial%2520advancement%2520in%250Aapplying%2520deep%2520learning%2520to%2520patterned%2520wildlife%2520identification.%250A%2520%2520This%2520research%2520contributes%2520to%2520the%2520field%2520of%2520computer%2520vision%2520and%2520provides%2520a%250Avaluable%2520tool%2520for%2520biologists%2520aiming%2520to%2520study%2520and%2520protect%2520leopard%2520populations.%250AIt%2520also%2520serves%2520as%2520a%2520stepping%2520stone%2520for%2520applying%2520the%2520power%2520of%2520deep%2520learning%2520in%250ACapture-Recapture%2520studies%2520for%2520other%2520patterned%2520species.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Leopard%20Individual%20Identification%3A%20An%20Adaptive%20Angular%0A%20%20Margin%20Approach&entry.906535625=David%20Colomer%20Matachana&entry.1292438233=%20%20Accurate%20identification%20of%20individual%20leopards%20across%20camera%20trap%20images%20is%0Acritical%20for%20population%20monitoring%20and%20ecological%20studies.%20This%20paper%0Aintroduces%20a%20deep%20learning%20framework%20to%20distinguish%20between%20individual%20leopards%0Abased%20on%20their%20unique%20spot%20patterns.%20This%20approach%20employs%20a%20novel%20adaptive%0Aangular%20margin%20method%20in%20the%20form%20of%20a%20modified%20CosFace%20architecture.%20In%0Aaddition%2C%20I%20propose%20a%20preprocessing%20pipeline%20that%20combines%20RGB%20channels%20with%20an%0Aedge%20detection%20channel%20to%20underscore%20the%20critical%20features%20learned%20by%20the%0Amodel.%0A%20%20This%20approach%20significantly%20outperforms%20the%20Triplet%20Network%20baseline%2C%0Aachieving%20a%20Dynamic%20Top-5%20Average%20Precision%20of%200.8814%20and%20a%20Top-5%20Rank%20Match%0ADetection%20of%200.9533%2C%20demonstrating%20its%20potential%20for%20open-set%20learning%20in%0Awildlife%20identification.%20While%20not%20surpassing%20the%20performance%20of%20the%20SIFT-based%0AHotspotter%20algorithm%2C%20this%20method%20represents%20a%20substantial%20advancement%20in%0Aapplying%20deep%20learning%20to%20patterned%20wildlife%20identification.%0A%20%20This%20research%20contributes%20to%20the%20field%20of%20computer%20vision%20and%20provides%20a%0Avaluable%20tool%20for%20biologists%20aiming%20to%20study%20and%20protect%20leopard%20populations.%0AIt%20also%20serves%20as%20a%20stepping%20stone%20for%20applying%20the%20power%20of%20deep%20learning%20in%0ACapture-Recapture%20studies%20for%20other%20patterned%20species.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01962v1&entry.124074799=Read"},
{"title": "3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction", "author": "Jongmin Lee and Minsu Cho", "abstract": "  Determining the 3D orientations of an object in an image, known as\nsingle-image pose estimation, is a crucial task in 3D vision applications.\nExisting methods typically learn 3D rotations parametrized in the spatial\ndomain using Euler angles or quaternions, but these representations often\nintroduce discontinuities and singularities. SO(3)-equivariant networks enable\nthe structured capture of pose patterns with data-efficient learning, but the\nparametrizations in spatial domain are incompatible with their architecture,\nparticularly spherical CNNs, which operate in the frequency domain to enhance\ncomputational efficiency. To overcome these issues, we propose a\nfrequency-domain approach that directly predicts Wigner-D coefficients for 3D\nrotation regression, aligning with the operations of spherical CNNs. Our\nSO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial\nparameterizations, ensuring consistent pose estimation under arbitrary\nrotations. Trained with a frequency-domain regression loss, our method achieves\nstate-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+,\nwith significant improvements in accuracy, robustness, and data efficiency.\n", "link": "http://arxiv.org/abs/2411.00543v2", "date": "2024-11-04", "relevancy": 2.579, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5471}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5016}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Equivariant%20Pose%20Regression%20via%20Direct%20Wigner-D%20Harmonics%20Prediction&body=Title%3A%203D%20Equivariant%20Pose%20Regression%20via%20Direct%20Wigner-D%20Harmonics%20Prediction%0AAuthor%3A%20Jongmin%20Lee%20and%20Minsu%20Cho%0AAbstract%3A%20%20%20Determining%20the%203D%20orientations%20of%20an%20object%20in%20an%20image%2C%20known%20as%0Asingle-image%20pose%20estimation%2C%20is%20a%20crucial%20task%20in%203D%20vision%20applications.%0AExisting%20methods%20typically%20learn%203D%20rotations%20parametrized%20in%20the%20spatial%0Adomain%20using%20Euler%20angles%20or%20quaternions%2C%20but%20these%20representations%20often%0Aintroduce%20discontinuities%20and%20singularities.%20SO%283%29-equivariant%20networks%20enable%0Athe%20structured%20capture%20of%20pose%20patterns%20with%20data-efficient%20learning%2C%20but%20the%0Aparametrizations%20in%20spatial%20domain%20are%20incompatible%20with%20their%20architecture%2C%0Aparticularly%20spherical%20CNNs%2C%20which%20operate%20in%20the%20frequency%20domain%20to%20enhance%0Acomputational%20efficiency.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Afrequency-domain%20approach%20that%20directly%20predicts%20Wigner-D%20coefficients%20for%203D%0Arotation%20regression%2C%20aligning%20with%20the%20operations%20of%20spherical%20CNNs.%20Our%0ASO%283%29-equivariant%20pose%20harmonics%20predictor%20overcomes%20the%20limitations%20of%20spatial%0Aparameterizations%2C%20ensuring%20consistent%20pose%20estimation%20under%20arbitrary%0Arotations.%20Trained%20with%20a%20frequency-domain%20regression%20loss%2C%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20benchmarks%20such%20as%20ModelNet10-SO%283%29%20and%20PASCAL3D%2B%2C%0Awith%20significant%20improvements%20in%20accuracy%2C%20robustness%2C%20and%20data%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Equivariant%2520Pose%2520Regression%2520via%2520Direct%2520Wigner-D%2520Harmonics%2520Prediction%26entry.906535625%3DJongmin%2520Lee%2520and%2520Minsu%2520Cho%26entry.1292438233%3D%2520%2520Determining%2520the%25203D%2520orientations%2520of%2520an%2520object%2520in%2520an%2520image%252C%2520known%2520as%250Asingle-image%2520pose%2520estimation%252C%2520is%2520a%2520crucial%2520task%2520in%25203D%2520vision%2520applications.%250AExisting%2520methods%2520typically%2520learn%25203D%2520rotations%2520parametrized%2520in%2520the%2520spatial%250Adomain%2520using%2520Euler%2520angles%2520or%2520quaternions%252C%2520but%2520these%2520representations%2520often%250Aintroduce%2520discontinuities%2520and%2520singularities.%2520SO%25283%2529-equivariant%2520networks%2520enable%250Athe%2520structured%2520capture%2520of%2520pose%2520patterns%2520with%2520data-efficient%2520learning%252C%2520but%2520the%250Aparametrizations%2520in%2520spatial%2520domain%2520are%2520incompatible%2520with%2520their%2520architecture%252C%250Aparticularly%2520spherical%2520CNNs%252C%2520which%2520operate%2520in%2520the%2520frequency%2520domain%2520to%2520enhance%250Acomputational%2520efficiency.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%250Afrequency-domain%2520approach%2520that%2520directly%2520predicts%2520Wigner-D%2520coefficients%2520for%25203D%250Arotation%2520regression%252C%2520aligning%2520with%2520the%2520operations%2520of%2520spherical%2520CNNs.%2520Our%250ASO%25283%2529-equivariant%2520pose%2520harmonics%2520predictor%2520overcomes%2520the%2520limitations%2520of%2520spatial%250Aparameterizations%252C%2520ensuring%2520consistent%2520pose%2520estimation%2520under%2520arbitrary%250Arotations.%2520Trained%2520with%2520a%2520frequency-domain%2520regression%2520loss%252C%2520our%2520method%2520achieves%250Astate-of-the-art%2520results%2520on%2520benchmarks%2520such%2520as%2520ModelNet10-SO%25283%2529%2520and%2520PASCAL3D%252B%252C%250Awith%2520significant%2520improvements%2520in%2520accuracy%252C%2520robustness%252C%2520and%2520data%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Equivariant%20Pose%20Regression%20via%20Direct%20Wigner-D%20Harmonics%20Prediction&entry.906535625=Jongmin%20Lee%20and%20Minsu%20Cho&entry.1292438233=%20%20Determining%20the%203D%20orientations%20of%20an%20object%20in%20an%20image%2C%20known%20as%0Asingle-image%20pose%20estimation%2C%20is%20a%20crucial%20task%20in%203D%20vision%20applications.%0AExisting%20methods%20typically%20learn%203D%20rotations%20parametrized%20in%20the%20spatial%0Adomain%20using%20Euler%20angles%20or%20quaternions%2C%20but%20these%20representations%20often%0Aintroduce%20discontinuities%20and%20singularities.%20SO%283%29-equivariant%20networks%20enable%0Athe%20structured%20capture%20of%20pose%20patterns%20with%20data-efficient%20learning%2C%20but%20the%0Aparametrizations%20in%20spatial%20domain%20are%20incompatible%20with%20their%20architecture%2C%0Aparticularly%20spherical%20CNNs%2C%20which%20operate%20in%20the%20frequency%20domain%20to%20enhance%0Acomputational%20efficiency.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Afrequency-domain%20approach%20that%20directly%20predicts%20Wigner-D%20coefficients%20for%203D%0Arotation%20regression%2C%20aligning%20with%20the%20operations%20of%20spherical%20CNNs.%20Our%0ASO%283%29-equivariant%20pose%20harmonics%20predictor%20overcomes%20the%20limitations%20of%20spatial%0Aparameterizations%2C%20ensuring%20consistent%20pose%20estimation%20under%20arbitrary%0Arotations.%20Trained%20with%20a%20frequency-domain%20regression%20loss%2C%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20benchmarks%20such%20as%20ModelNet10-SO%283%29%20and%20PASCAL3D%2B%2C%0Awith%20significant%20improvements%20in%20accuracy%2C%20robustness%2C%20and%20data%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00543v2&entry.124074799=Read"},
{"title": "PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images", "author": "Yiheng Xiong and Angela Dai", "abstract": "  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n", "link": "http://arxiv.org/abs/2405.11914v3", "date": "2024-11-04", "relevancy": 2.5744, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.63}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images&body=Title%3A%20PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images%0AAuthor%3A%20Yiheng%20Xiong%20and%20Angela%20Dai%0AAbstract%3A%20%20%20Generating%203D%20shapes%20from%20single%20RGB%20images%20is%20essential%20in%20various%0Aapplications%20such%20as%20robotics.%20Current%20approaches%20typically%20target%20images%0Acontaining%20clear%20and%20complete%20visual%20descriptions%20of%20the%20object%2C%20without%0Aconsidering%20common%20realistic%20cases%20where%20observations%20of%20objects%20that%20are%0Alargely%20occluded%20or%20truncated.%20We%20thus%20propose%20a%20transformer-based%0Aautoregressive%20model%20to%20generate%20the%20probabilistic%20distribution%20of%203D%20shapes%0Aconditioned%20on%20an%20RGB%20image%20containing%20potentially%20highly%20ambiguous%0Aobservations%20of%20the%20object.%20To%20handle%20realistic%20scenarios%20such%20as%20occlusion%20or%0Afield-of-view%20truncation%2C%20we%20create%20simulated%20image-to-shape%20training%20pairs%0Athat%20enable%20improved%20fine-tuning%20for%20real-world%20scenarios.%20We%20then%20adopt%0Across-attention%20to%20effectively%20identify%20the%20most%20relevant%20region%20of%20interest%0Afrom%20the%20input%20image%20for%20shape%20generation.%20This%20enables%20inference%20of%20sampled%0Ashapes%20with%20reasonable%20diversity%20and%20strong%20alignment%20with%20the%20input%20image.%20We%0Atrain%20and%20test%20our%20model%20on%20our%20synthetic%20data%20then%20fine-tune%20and%20test%20it%20on%0Areal-world%20data.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20state%20of%0Athe%20art%20in%20both%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11914v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPT43D%253A%2520A%2520Probabilistic%2520Transformer%2520for%2520Generating%25203D%2520Shapes%2520from%2520Single%250A%2520%2520Highly-Ambiguous%2520RGB%2520Images%26entry.906535625%3DYiheng%2520Xiong%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520Generating%25203D%2520shapes%2520from%2520single%2520RGB%2520images%2520is%2520essential%2520in%2520various%250Aapplications%2520such%2520as%2520robotics.%2520Current%2520approaches%2520typically%2520target%2520images%250Acontaining%2520clear%2520and%2520complete%2520visual%2520descriptions%2520of%2520the%2520object%252C%2520without%250Aconsidering%2520common%2520realistic%2520cases%2520where%2520observations%2520of%2520objects%2520that%2520are%250Alargely%2520occluded%2520or%2520truncated.%2520We%2520thus%2520propose%2520a%2520transformer-based%250Aautoregressive%2520model%2520to%2520generate%2520the%2520probabilistic%2520distribution%2520of%25203D%2520shapes%250Aconditioned%2520on%2520an%2520RGB%2520image%2520containing%2520potentially%2520highly%2520ambiguous%250Aobservations%2520of%2520the%2520object.%2520To%2520handle%2520realistic%2520scenarios%2520such%2520as%2520occlusion%2520or%250Afield-of-view%2520truncation%252C%2520we%2520create%2520simulated%2520image-to-shape%2520training%2520pairs%250Athat%2520enable%2520improved%2520fine-tuning%2520for%2520real-world%2520scenarios.%2520We%2520then%2520adopt%250Across-attention%2520to%2520effectively%2520identify%2520the%2520most%2520relevant%2520region%2520of%2520interest%250Afrom%2520the%2520input%2520image%2520for%2520shape%2520generation.%2520This%2520enables%2520inference%2520of%2520sampled%250Ashapes%2520with%2520reasonable%2520diversity%2520and%2520strong%2520alignment%2520with%2520the%2520input%2520image.%2520We%250Atrain%2520and%2520test%2520our%2520model%2520on%2520our%2520synthetic%2520data%2520then%2520fine-tune%2520and%2520test%2520it%2520on%250Areal-world%2520data.%2520Experiments%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520state%2520of%250Athe%2520art%2520in%2520both%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11914v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PT43D%3A%20A%20Probabilistic%20Transformer%20for%20Generating%203D%20Shapes%20from%20Single%0A%20%20Highly-Ambiguous%20RGB%20Images&entry.906535625=Yiheng%20Xiong%20and%20Angela%20Dai&entry.1292438233=%20%20Generating%203D%20shapes%20from%20single%20RGB%20images%20is%20essential%20in%20various%0Aapplications%20such%20as%20robotics.%20Current%20approaches%20typically%20target%20images%0Acontaining%20clear%20and%20complete%20visual%20descriptions%20of%20the%20object%2C%20without%0Aconsidering%20common%20realistic%20cases%20where%20observations%20of%20objects%20that%20are%0Alargely%20occluded%20or%20truncated.%20We%20thus%20propose%20a%20transformer-based%0Aautoregressive%20model%20to%20generate%20the%20probabilistic%20distribution%20of%203D%20shapes%0Aconditioned%20on%20an%20RGB%20image%20containing%20potentially%20highly%20ambiguous%0Aobservations%20of%20the%20object.%20To%20handle%20realistic%20scenarios%20such%20as%20occlusion%20or%0Afield-of-view%20truncation%2C%20we%20create%20simulated%20image-to-shape%20training%20pairs%0Athat%20enable%20improved%20fine-tuning%20for%20real-world%20scenarios.%20We%20then%20adopt%0Across-attention%20to%20effectively%20identify%20the%20most%20relevant%20region%20of%20interest%0Afrom%20the%20input%20image%20for%20shape%20generation.%20This%20enables%20inference%20of%20sampled%0Ashapes%20with%20reasonable%20diversity%20and%20strong%20alignment%20with%20the%20input%20image.%20We%0Atrain%20and%20test%20our%20model%20on%20our%20synthetic%20data%20then%20fine-tune%20and%20test%20it%20on%0Areal-world%20data.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%20state%20of%0Athe%20art%20in%20both%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11914v3&entry.124074799=Read"},
{"title": "Online Adaptation of Language Models with a Memory of Amortized Contexts", "author": "Jihoon Tack and Jaehyung Kim and Eric Mitchell and Jinwoo Shin and Yee Whye Teh and Jonathan Richard Schwarz", "abstract": "  Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.\n", "link": "http://arxiv.org/abs/2403.04317v2", "date": "2024-11-04", "relevancy": 2.5691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Adaptation%20of%20Language%20Models%20with%20a%20Memory%20of%20Amortized%20Contexts&body=Title%3A%20Online%20Adaptation%20of%20Language%20Models%20with%20a%20Memory%20of%20Amortized%20Contexts%0AAuthor%3A%20Jihoon%20Tack%20and%20Jaehyung%20Kim%20and%20Eric%20Mitchell%20and%20Jinwoo%20Shin%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%0AAbstract%3A%20%20%20Due%20to%20the%20rapid%20generation%20and%20dissemination%20of%20information%2C%20large%20language%0Amodels%20%28LLMs%29%20quickly%20run%20out%20of%20date%20despite%20enormous%20development%20costs.%20To%0Aaddress%20the%20crucial%20need%20to%20keep%20models%20updated%2C%20online%20learning%20has%20emerged%20as%0Aa%20critical%20tool%20when%20utilizing%20LLMs%20for%20real-world%20applications.%20However%2C%20given%0Athe%20ever-expanding%20corpus%20of%20unseen%20documents%20and%20the%20large%20parameter%20space%20of%0Amodern%20LLMs%2C%20efficient%20adaptation%20is%20essential.%20To%20address%20these%20challenges%2C%20we%0Apropose%20Memory%20of%20Amortized%20Contexts%20%28MAC%29%2C%20an%20efficient%20and%20effective%20online%0Aadaptation%20framework%20for%20LLMs%20with%20strong%20knowledge%20retention.%20We%20propose%20a%0Afeature%20extraction%20and%20memory-augmentation%20approach%20to%20compress%20and%20extract%0Ainformation%20from%20new%20documents%20into%20compact%20modulations%20stored%20in%20a%20memory%0Abank.%20When%20answering%20questions%2C%20our%20model%20attends%20to%20and%20extracts%20relevant%0Aknowledge%20from%20this%20memory%20bank.%20To%20learn%20informative%20modulations%20in%20an%0Aefficient%20manner%2C%20we%20utilize%20amortization-based%20meta-learning%2C%20which%0Asubstitutes%20an%20otherwise%20required%20optimization%20process%20with%20a%20single%20forward%0Apass%20of%20the%20encoder.%20Subsequently%2C%20we%20learn%20to%20choose%20from%20and%20aggregate%0Aselected%20documents%20into%20a%20single%20modulation%20by%20conditioning%20on%20the%20question%2C%0Aallowing%20us%20to%20adapt%20a%20frozen%20language%20model%20during%20test%20time%20without%20requiring%0Afurther%20gradient%20updates.%20Our%20experiment%20demonstrates%20the%20superiority%20of%20MAC%20in%0Amultiple%20aspects%2C%20including%20online%20adaptation%20performance%2C%20time%2C%20and%20memory%0Aefficiency.%20In%20addition%2C%20we%20show%20how%20MAC%20can%20be%20combined%20with%20and%20improve%20the%0Aperformance%20of%20popular%20alternatives%20such%20as%20retrieval%20augmented%20generations%0A%28RAGs%29.%20Code%20is%20available%20at%3A%20https%3A//github.com/jihoontack/MAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Adaptation%2520of%2520Language%2520Models%2520with%2520a%2520Memory%2520of%2520Amortized%2520Contexts%26entry.906535625%3DJihoon%2520Tack%2520and%2520Jaehyung%2520Kim%2520and%2520Eric%2520Mitchell%2520and%2520Jinwoo%2520Shin%2520and%2520Yee%2520Whye%2520Teh%2520and%2520Jonathan%2520Richard%2520Schwarz%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520rapid%2520generation%2520and%2520dissemination%2520of%2520information%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520quickly%2520run%2520out%2520of%2520date%2520despite%2520enormous%2520development%2520costs.%2520To%250Aaddress%2520the%2520crucial%2520need%2520to%2520keep%2520models%2520updated%252C%2520online%2520learning%2520has%2520emerged%2520as%250Aa%2520critical%2520tool%2520when%2520utilizing%2520LLMs%2520for%2520real-world%2520applications.%2520However%252C%2520given%250Athe%2520ever-expanding%2520corpus%2520of%2520unseen%2520documents%2520and%2520the%2520large%2520parameter%2520space%2520of%250Amodern%2520LLMs%252C%2520efficient%2520adaptation%2520is%2520essential.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520Memory%2520of%2520Amortized%2520Contexts%2520%2528MAC%2529%252C%2520an%2520efficient%2520and%2520effective%2520online%250Aadaptation%2520framework%2520for%2520LLMs%2520with%2520strong%2520knowledge%2520retention.%2520We%2520propose%2520a%250Afeature%2520extraction%2520and%2520memory-augmentation%2520approach%2520to%2520compress%2520and%2520extract%250Ainformation%2520from%2520new%2520documents%2520into%2520compact%2520modulations%2520stored%2520in%2520a%2520memory%250Abank.%2520When%2520answering%2520questions%252C%2520our%2520model%2520attends%2520to%2520and%2520extracts%2520relevant%250Aknowledge%2520from%2520this%2520memory%2520bank.%2520To%2520learn%2520informative%2520modulations%2520in%2520an%250Aefficient%2520manner%252C%2520we%2520utilize%2520amortization-based%2520meta-learning%252C%2520which%250Asubstitutes%2520an%2520otherwise%2520required%2520optimization%2520process%2520with%2520a%2520single%2520forward%250Apass%2520of%2520the%2520encoder.%2520Subsequently%252C%2520we%2520learn%2520to%2520choose%2520from%2520and%2520aggregate%250Aselected%2520documents%2520into%2520a%2520single%2520modulation%2520by%2520conditioning%2520on%2520the%2520question%252C%250Aallowing%2520us%2520to%2520adapt%2520a%2520frozen%2520language%2520model%2520during%2520test%2520time%2520without%2520requiring%250Afurther%2520gradient%2520updates.%2520Our%2520experiment%2520demonstrates%2520the%2520superiority%2520of%2520MAC%2520in%250Amultiple%2520aspects%252C%2520including%2520online%2520adaptation%2520performance%252C%2520time%252C%2520and%2520memory%250Aefficiency.%2520In%2520addition%252C%2520we%2520show%2520how%2520MAC%2520can%2520be%2520combined%2520with%2520and%2520improve%2520the%250Aperformance%2520of%2520popular%2520alternatives%2520such%2520as%2520retrieval%2520augmented%2520generations%250A%2528RAGs%2529.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/jihoontack/MAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Adaptation%20of%20Language%20Models%20with%20a%20Memory%20of%20Amortized%20Contexts&entry.906535625=Jihoon%20Tack%20and%20Jaehyung%20Kim%20and%20Eric%20Mitchell%20and%20Jinwoo%20Shin%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz&entry.1292438233=%20%20Due%20to%20the%20rapid%20generation%20and%20dissemination%20of%20information%2C%20large%20language%0Amodels%20%28LLMs%29%20quickly%20run%20out%20of%20date%20despite%20enormous%20development%20costs.%20To%0Aaddress%20the%20crucial%20need%20to%20keep%20models%20updated%2C%20online%20learning%20has%20emerged%20as%0Aa%20critical%20tool%20when%20utilizing%20LLMs%20for%20real-world%20applications.%20However%2C%20given%0Athe%20ever-expanding%20corpus%20of%20unseen%20documents%20and%20the%20large%20parameter%20space%20of%0Amodern%20LLMs%2C%20efficient%20adaptation%20is%20essential.%20To%20address%20these%20challenges%2C%20we%0Apropose%20Memory%20of%20Amortized%20Contexts%20%28MAC%29%2C%20an%20efficient%20and%20effective%20online%0Aadaptation%20framework%20for%20LLMs%20with%20strong%20knowledge%20retention.%20We%20propose%20a%0Afeature%20extraction%20and%20memory-augmentation%20approach%20to%20compress%20and%20extract%0Ainformation%20from%20new%20documents%20into%20compact%20modulations%20stored%20in%20a%20memory%0Abank.%20When%20answering%20questions%2C%20our%20model%20attends%20to%20and%20extracts%20relevant%0Aknowledge%20from%20this%20memory%20bank.%20To%20learn%20informative%20modulations%20in%20an%0Aefficient%20manner%2C%20we%20utilize%20amortization-based%20meta-learning%2C%20which%0Asubstitutes%20an%20otherwise%20required%20optimization%20process%20with%20a%20single%20forward%0Apass%20of%20the%20encoder.%20Subsequently%2C%20we%20learn%20to%20choose%20from%20and%20aggregate%0Aselected%20documents%20into%20a%20single%20modulation%20by%20conditioning%20on%20the%20question%2C%0Aallowing%20us%20to%20adapt%20a%20frozen%20language%20model%20during%20test%20time%20without%20requiring%0Afurther%20gradient%20updates.%20Our%20experiment%20demonstrates%20the%20superiority%20of%20MAC%20in%0Amultiple%20aspects%2C%20including%20online%20adaptation%20performance%2C%20time%2C%20and%20memory%0Aefficiency.%20In%20addition%2C%20we%20show%20how%20MAC%20can%20be%20combined%20with%20and%20improve%20the%0Aperformance%20of%20popular%20alternatives%20such%20as%20retrieval%20augmented%20generations%0A%28RAGs%29.%20Code%20is%20available%20at%3A%20https%3A//github.com/jihoontack/MAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04317v2&entry.124074799=Read"},
{"title": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages", "author": "Hoang Nguyen and Khyati Mahajan and Vikas Yadav and Philip S. Yu and Masoud Hashemi and Rishabh Maheshwary", "abstract": "  Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.\n", "link": "http://arxiv.org/abs/2411.02398v1", "date": "2024-11-04", "relevancy": 2.5685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20with%20Phonemes%3A%20Enhancing%20LLM%20Multilinguality%20for%20non-Latin%0A%20%20Script%20Languages&body=Title%3A%20Prompting%20with%20Phonemes%3A%20Enhancing%20LLM%20Multilinguality%20for%20non-Latin%0A%20%20Script%20Languages%0AAuthor%3A%20Hoang%20Nguyen%20and%20Khyati%20Mahajan%20and%20Vikas%20Yadav%20and%20Philip%20S.%20Yu%20and%20Masoud%20Hashemi%20and%20Rishabh%20Maheshwary%0AAbstract%3A%20%20%20Multilingual%20LLMs%20have%20achieved%20remarkable%20benchmark%20performance%2C%20but%20we%20find%0Athey%20continue%20to%20underperform%20on%20non-Latin%20script%20languages%20across%20contemporary%0ALLM%20families.%20This%20discrepancy%20arises%20from%20the%20fact%20that%20LLMs%20are%20pretrained%0Awith%20orthographic%20scripts%2C%20which%20are%20dominated%20by%20Latin%20characters%20that%20obscure%0Atheir%20shared%20phonology%20with%20non-Latin%20scripts.%20We%20propose%20leveraging%20phonemic%0Atranscriptions%20as%20complementary%20signals%20to%20induce%20script-invariant%0Arepresentations.%20Our%20study%20demonstrates%20that%20integrating%20phonemic%20signals%0Aimproves%20performance%20across%20both%20non-Latin%20and%20Latin%20languages%2C%20with%20a%0Aparticularly%20significant%20impact%20on%20closing%20the%20performance%20gap%20between%20the%20two.%0AThrough%20detailed%20experiments%2C%20we%20show%20that%20phonemic%20and%20orthographic%20scripts%0Aretrieve%20distinct%20examples%20for%20in-context%20learning%20%28ICL%29.%20This%20motivates%20our%0Aproposed%20Mixed-ICL%20retrieval%20strategy%2C%20where%20further%20aggregation%20leads%20to%20our%0Asignificant%20performance%20improvements%20for%20both%20Latin%20script%20languages%20%28up%20to%0A12.6%25%29%20and%20non-Latin%20script%20languages%20%28up%20to%2015.1%25%29%20compared%20to%20randomized%20ICL%0Aretrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520with%2520Phonemes%253A%2520Enhancing%2520LLM%2520Multilinguality%2520for%2520non-Latin%250A%2520%2520Script%2520Languages%26entry.906535625%3DHoang%2520Nguyen%2520and%2520Khyati%2520Mahajan%2520and%2520Vikas%2520Yadav%2520and%2520Philip%2520S.%2520Yu%2520and%2520Masoud%2520Hashemi%2520and%2520Rishabh%2520Maheshwary%26entry.1292438233%3D%2520%2520Multilingual%2520LLMs%2520have%2520achieved%2520remarkable%2520benchmark%2520performance%252C%2520but%2520we%2520find%250Athey%2520continue%2520to%2520underperform%2520on%2520non-Latin%2520script%2520languages%2520across%2520contemporary%250ALLM%2520families.%2520This%2520discrepancy%2520arises%2520from%2520the%2520fact%2520that%2520LLMs%2520are%2520pretrained%250Awith%2520orthographic%2520scripts%252C%2520which%2520are%2520dominated%2520by%2520Latin%2520characters%2520that%2520obscure%250Atheir%2520shared%2520phonology%2520with%2520non-Latin%2520scripts.%2520We%2520propose%2520leveraging%2520phonemic%250Atranscriptions%2520as%2520complementary%2520signals%2520to%2520induce%2520script-invariant%250Arepresentations.%2520Our%2520study%2520demonstrates%2520that%2520integrating%2520phonemic%2520signals%250Aimproves%2520performance%2520across%2520both%2520non-Latin%2520and%2520Latin%2520languages%252C%2520with%2520a%250Aparticularly%2520significant%2520impact%2520on%2520closing%2520the%2520performance%2520gap%2520between%2520the%2520two.%250AThrough%2520detailed%2520experiments%252C%2520we%2520show%2520that%2520phonemic%2520and%2520orthographic%2520scripts%250Aretrieve%2520distinct%2520examples%2520for%2520in-context%2520learning%2520%2528ICL%2529.%2520This%2520motivates%2520our%250Aproposed%2520Mixed-ICL%2520retrieval%2520strategy%252C%2520where%2520further%2520aggregation%2520leads%2520to%2520our%250Asignificant%2520performance%2520improvements%2520for%2520both%2520Latin%2520script%2520languages%2520%2528up%2520to%250A12.6%2525%2529%2520and%2520non-Latin%2520script%2520languages%2520%2528up%2520to%252015.1%2525%2529%2520compared%2520to%2520randomized%2520ICL%250Aretrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20with%20Phonemes%3A%20Enhancing%20LLM%20Multilinguality%20for%20non-Latin%0A%20%20Script%20Languages&entry.906535625=Hoang%20Nguyen%20and%20Khyati%20Mahajan%20and%20Vikas%20Yadav%20and%20Philip%20S.%20Yu%20and%20Masoud%20Hashemi%20and%20Rishabh%20Maheshwary&entry.1292438233=%20%20Multilingual%20LLMs%20have%20achieved%20remarkable%20benchmark%20performance%2C%20but%20we%20find%0Athey%20continue%20to%20underperform%20on%20non-Latin%20script%20languages%20across%20contemporary%0ALLM%20families.%20This%20discrepancy%20arises%20from%20the%20fact%20that%20LLMs%20are%20pretrained%0Awith%20orthographic%20scripts%2C%20which%20are%20dominated%20by%20Latin%20characters%20that%20obscure%0Atheir%20shared%20phonology%20with%20non-Latin%20scripts.%20We%20propose%20leveraging%20phonemic%0Atranscriptions%20as%20complementary%20signals%20to%20induce%20script-invariant%0Arepresentations.%20Our%20study%20demonstrates%20that%20integrating%20phonemic%20signals%0Aimproves%20performance%20across%20both%20non-Latin%20and%20Latin%20languages%2C%20with%20a%0Aparticularly%20significant%20impact%20on%20closing%20the%20performance%20gap%20between%20the%20two.%0AThrough%20detailed%20experiments%2C%20we%20show%20that%20phonemic%20and%20orthographic%20scripts%0Aretrieve%20distinct%20examples%20for%20in-context%20learning%20%28ICL%29.%20This%20motivates%20our%0Aproposed%20Mixed-ICL%20retrieval%20strategy%2C%20where%20further%20aggregation%20leads%20to%20our%0Asignificant%20performance%20improvements%20for%20both%20Latin%20script%20languages%20%28up%20to%0A12.6%25%29%20and%20non-Latin%20script%20languages%20%28up%20to%2015.1%25%29%20compared%20to%20randomized%20ICL%0Aretrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02398v1&entry.124074799=Read"},
{"title": "How Far is Video Generation from World Model: A Physical Law Perspective", "author": "Bingyi Kang and Yang Yue and Rui Lu and Zhijie Lin and Yang Zhao and Kaixin Wang and Gao Huang and Jiashi Feng", "abstract": "  OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io\n", "link": "http://arxiv.org/abs/2411.02385v1", "date": "2024-11-04", "relevancy": 2.5675, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6926}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20is%20Video%20Generation%20from%20World%20Model%3A%20A%20Physical%20Law%20Perspective&body=Title%3A%20How%20Far%20is%20Video%20Generation%20from%20World%20Model%3A%20A%20Physical%20Law%20Perspective%0AAuthor%3A%20Bingyi%20Kang%20and%20Yang%20Yue%20and%20Rui%20Lu%20and%20Zhijie%20Lin%20and%20Yang%20Zhao%20and%20Kaixin%20Wang%20and%20Gao%20Huang%20and%20Jiashi%20Feng%0AAbstract%3A%20%20%20OpenAI%27s%20Sora%20highlights%20the%20potential%20of%20video%20generation%20for%20developing%0Aworld%20models%20that%20adhere%20to%20fundamental%20physical%20laws.%20However%2C%20the%20ability%20of%0Avideo%20generation%20models%20to%20discover%20such%20laws%20purely%20from%20visual%20data%20without%0Ahuman%20priors%20can%20be%20questioned.%20A%20world%20model%20learning%20the%20true%20law%20should%20give%0Apredictions%20robust%20to%20nuances%20and%20correctly%20extrapolate%20on%20unseen%20scenarios.%20In%0Athis%20work%2C%20we%20evaluate%20across%20three%20key%20scenarios%3A%20in-distribution%2C%0Aout-of-distribution%2C%20and%20combinatorial%20generalization.%20We%20developed%20a%202D%0Asimulation%20testbed%20for%20object%20movement%20and%20collisions%20to%20generate%20videos%0Adeterministically%20governed%20by%20one%20or%20more%20classical%20mechanics%20laws.%20This%0Aprovides%20an%20unlimited%20supply%20of%20data%20for%20large-scale%20experimentation%20and%0Aenables%20quantitative%20evaluation%20of%20whether%20the%20generated%20videos%20adhere%20to%0Aphysical%20laws.%20We%20trained%20diffusion-based%20video%20generation%20models%20to%20predict%0Aobject%20movements%20based%20on%20initial%20frames.%20Our%20scaling%20experiments%20show%20perfect%0Ageneralization%20within%20the%20distribution%2C%20measurable%20scaling%20behavior%20for%0Acombinatorial%20generalization%2C%20but%20failure%20in%20out-of-distribution%20scenarios.%0AFurther%20experiments%20reveal%20two%20key%20insights%20about%20the%20generalization%20mechanisms%0Aof%20these%20models%3A%20%281%29%20the%20models%20fail%20to%20abstract%20general%20physical%20rules%20and%0Ainstead%20exhibit%20%22case-based%22%20generalization%20behavior%2C%20i.e.%2C%20mimicking%20the%0Aclosest%20training%20example%3B%20%282%29%20when%20generalizing%20to%20new%20cases%2C%20models%20are%0Aobserved%20to%20prioritize%20different%20factors%20when%20referencing%20training%20data%3A%20color%0A%3E%20size%20%3E%20velocity%20%3E%20shape.%20Our%20study%20suggests%20that%20scaling%20alone%20is%0Ainsufficient%20for%20video%20generation%20models%20to%20uncover%20fundamental%20physical%20laws%2C%0Adespite%20its%20role%20in%20Sora%27s%20broader%20success.%20See%20our%20project%20page%20at%0Ahttps%3A//phyworld.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520is%2520Video%2520Generation%2520from%2520World%2520Model%253A%2520A%2520Physical%2520Law%2520Perspective%26entry.906535625%3DBingyi%2520Kang%2520and%2520Yang%2520Yue%2520and%2520Rui%2520Lu%2520and%2520Zhijie%2520Lin%2520and%2520Yang%2520Zhao%2520and%2520Kaixin%2520Wang%2520and%2520Gao%2520Huang%2520and%2520Jiashi%2520Feng%26entry.1292438233%3D%2520%2520OpenAI%2527s%2520Sora%2520highlights%2520the%2520potential%2520of%2520video%2520generation%2520for%2520developing%250Aworld%2520models%2520that%2520adhere%2520to%2520fundamental%2520physical%2520laws.%2520However%252C%2520the%2520ability%2520of%250Avideo%2520generation%2520models%2520to%2520discover%2520such%2520laws%2520purely%2520from%2520visual%2520data%2520without%250Ahuman%2520priors%2520can%2520be%2520questioned.%2520A%2520world%2520model%2520learning%2520the%2520true%2520law%2520should%2520give%250Apredictions%2520robust%2520to%2520nuances%2520and%2520correctly%2520extrapolate%2520on%2520unseen%2520scenarios.%2520In%250Athis%2520work%252C%2520we%2520evaluate%2520across%2520three%2520key%2520scenarios%253A%2520in-distribution%252C%250Aout-of-distribution%252C%2520and%2520combinatorial%2520generalization.%2520We%2520developed%2520a%25202D%250Asimulation%2520testbed%2520for%2520object%2520movement%2520and%2520collisions%2520to%2520generate%2520videos%250Adeterministically%2520governed%2520by%2520one%2520or%2520more%2520classical%2520mechanics%2520laws.%2520This%250Aprovides%2520an%2520unlimited%2520supply%2520of%2520data%2520for%2520large-scale%2520experimentation%2520and%250Aenables%2520quantitative%2520evaluation%2520of%2520whether%2520the%2520generated%2520videos%2520adhere%2520to%250Aphysical%2520laws.%2520We%2520trained%2520diffusion-based%2520video%2520generation%2520models%2520to%2520predict%250Aobject%2520movements%2520based%2520on%2520initial%2520frames.%2520Our%2520scaling%2520experiments%2520show%2520perfect%250Ageneralization%2520within%2520the%2520distribution%252C%2520measurable%2520scaling%2520behavior%2520for%250Acombinatorial%2520generalization%252C%2520but%2520failure%2520in%2520out-of-distribution%2520scenarios.%250AFurther%2520experiments%2520reveal%2520two%2520key%2520insights%2520about%2520the%2520generalization%2520mechanisms%250Aof%2520these%2520models%253A%2520%25281%2529%2520the%2520models%2520fail%2520to%2520abstract%2520general%2520physical%2520rules%2520and%250Ainstead%2520exhibit%2520%2522case-based%2522%2520generalization%2520behavior%252C%2520i.e.%252C%2520mimicking%2520the%250Aclosest%2520training%2520example%253B%2520%25282%2529%2520when%2520generalizing%2520to%2520new%2520cases%252C%2520models%2520are%250Aobserved%2520to%2520prioritize%2520different%2520factors%2520when%2520referencing%2520training%2520data%253A%2520color%250A%253E%2520size%2520%253E%2520velocity%2520%253E%2520shape.%2520Our%2520study%2520suggests%2520that%2520scaling%2520alone%2520is%250Ainsufficient%2520for%2520video%2520generation%2520models%2520to%2520uncover%2520fundamental%2520physical%2520laws%252C%250Adespite%2520its%2520role%2520in%2520Sora%2527s%2520broader%2520success.%2520See%2520our%2520project%2520page%2520at%250Ahttps%253A//phyworld.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20is%20Video%20Generation%20from%20World%20Model%3A%20A%20Physical%20Law%20Perspective&entry.906535625=Bingyi%20Kang%20and%20Yang%20Yue%20and%20Rui%20Lu%20and%20Zhijie%20Lin%20and%20Yang%20Zhao%20and%20Kaixin%20Wang%20and%20Gao%20Huang%20and%20Jiashi%20Feng&entry.1292438233=%20%20OpenAI%27s%20Sora%20highlights%20the%20potential%20of%20video%20generation%20for%20developing%0Aworld%20models%20that%20adhere%20to%20fundamental%20physical%20laws.%20However%2C%20the%20ability%20of%0Avideo%20generation%20models%20to%20discover%20such%20laws%20purely%20from%20visual%20data%20without%0Ahuman%20priors%20can%20be%20questioned.%20A%20world%20model%20learning%20the%20true%20law%20should%20give%0Apredictions%20robust%20to%20nuances%20and%20correctly%20extrapolate%20on%20unseen%20scenarios.%20In%0Athis%20work%2C%20we%20evaluate%20across%20three%20key%20scenarios%3A%20in-distribution%2C%0Aout-of-distribution%2C%20and%20combinatorial%20generalization.%20We%20developed%20a%202D%0Asimulation%20testbed%20for%20object%20movement%20and%20collisions%20to%20generate%20videos%0Adeterministically%20governed%20by%20one%20or%20more%20classical%20mechanics%20laws.%20This%0Aprovides%20an%20unlimited%20supply%20of%20data%20for%20large-scale%20experimentation%20and%0Aenables%20quantitative%20evaluation%20of%20whether%20the%20generated%20videos%20adhere%20to%0Aphysical%20laws.%20We%20trained%20diffusion-based%20video%20generation%20models%20to%20predict%0Aobject%20movements%20based%20on%20initial%20frames.%20Our%20scaling%20experiments%20show%20perfect%0Ageneralization%20within%20the%20distribution%2C%20measurable%20scaling%20behavior%20for%0Acombinatorial%20generalization%2C%20but%20failure%20in%20out-of-distribution%20scenarios.%0AFurther%20experiments%20reveal%20two%20key%20insights%20about%20the%20generalization%20mechanisms%0Aof%20these%20models%3A%20%281%29%20the%20models%20fail%20to%20abstract%20general%20physical%20rules%20and%0Ainstead%20exhibit%20%22case-based%22%20generalization%20behavior%2C%20i.e.%2C%20mimicking%20the%0Aclosest%20training%20example%3B%20%282%29%20when%20generalizing%20to%20new%20cases%2C%20models%20are%0Aobserved%20to%20prioritize%20different%20factors%20when%20referencing%20training%20data%3A%20color%0A%3E%20size%20%3E%20velocity%20%3E%20shape.%20Our%20study%20suggests%20that%20scaling%20alone%20is%0Ainsufficient%20for%20video%20generation%20models%20to%20uncover%20fundamental%20physical%20laws%2C%0Adespite%20its%20role%20in%20Sora%27s%20broader%20success.%20See%20our%20project%20page%20at%0Ahttps%3A//phyworld.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02385v1&entry.124074799=Read"},
{"title": "SIRA: Scalable Inter-frame Relation and Association for Radar Perception", "author": "Ryoma Yataka and Pu Perry Wang and Petros Boufounos and Ryuhei Takahashi", "abstract": "  Conventional radar feature extraction faces limitations due to low spatial\nresolution, noise, multipath reflection, the presence of ghost targets, and\nmotion blur. Such limitations can be exacerbated by nonlinear object motion,\nparticularly from an ego-centric viewpoint. It becomes evident that to address\nthese challenges, the key lies in exploiting temporal feature relation over an\nextended horizon and enforcing spatial motion consistency for effective\nassociation. To this end, this paper proposes SIRA (Scalable Inter-frame\nRelation and Association) with two designs. First, inspired by Swin\nTransformer, we introduce extended temporal relation, generalizing the existing\ntemporal relation layer from two consecutive frames to multiple inter-frames\nwith temporally regrouped window attention for scalability. Second, we propose\nmotion consistency track with the concept of a pseudo-tracklet generated from\nobservational data for better trajectory prediction and subsequent object\nassociation. Our approach achieves 58.11 mAP@0.5 for oriented object detection\nand 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing\nprevious state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,\nrespectively.\n", "link": "http://arxiv.org/abs/2411.02220v1", "date": "2024-11-04", "relevancy": 2.5674, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIRA%3A%20Scalable%20Inter-frame%20Relation%20and%20Association%20for%20Radar%20Perception&body=Title%3A%20SIRA%3A%20Scalable%20Inter-frame%20Relation%20and%20Association%20for%20Radar%20Perception%0AAuthor%3A%20Ryoma%20Yataka%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi%0AAbstract%3A%20%20%20Conventional%20radar%20feature%20extraction%20faces%20limitations%20due%20to%20low%20spatial%0Aresolution%2C%20noise%2C%20multipath%20reflection%2C%20the%20presence%20of%20ghost%20targets%2C%20and%0Amotion%20blur.%20Such%20limitations%20can%20be%20exacerbated%20by%20nonlinear%20object%20motion%2C%0Aparticularly%20from%20an%20ego-centric%20viewpoint.%20It%20becomes%20evident%20that%20to%20address%0Athese%20challenges%2C%20the%20key%20lies%20in%20exploiting%20temporal%20feature%20relation%20over%20an%0Aextended%20horizon%20and%20enforcing%20spatial%20motion%20consistency%20for%20effective%0Aassociation.%20To%20this%20end%2C%20this%20paper%20proposes%20SIRA%20%28Scalable%20Inter-frame%0ARelation%20and%20Association%29%20with%20two%20designs.%20First%2C%20inspired%20by%20Swin%0ATransformer%2C%20we%20introduce%20extended%20temporal%20relation%2C%20generalizing%20the%20existing%0Atemporal%20relation%20layer%20from%20two%20consecutive%20frames%20to%20multiple%20inter-frames%0Awith%20temporally%20regrouped%20window%20attention%20for%20scalability.%20Second%2C%20we%20propose%0Amotion%20consistency%20track%20with%20the%20concept%20of%20a%20pseudo-tracklet%20generated%20from%0Aobservational%20data%20for%20better%20trajectory%20prediction%20and%20subsequent%20object%0Aassociation.%20Our%20approach%20achieves%2058.11%20mAP%400.5%20for%20oriented%20object%20detection%0Aand%2047.79%20MOTA%20for%20multiple%20object%20tracking%20on%20the%20Radiate%20dataset%2C%20surpassing%0Aprevious%20state-of-the-art%20by%20a%20margin%20of%20%2B4.11%20mAP%400.5%20and%20%2B9.94%20MOTA%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIRA%253A%2520Scalable%2520Inter-frame%2520Relation%2520and%2520Association%2520for%2520Radar%2520Perception%26entry.906535625%3DRyoma%2520Yataka%2520and%2520Pu%2520Perry%2520Wang%2520and%2520Petros%2520Boufounos%2520and%2520Ryuhei%2520Takahashi%26entry.1292438233%3D%2520%2520Conventional%2520radar%2520feature%2520extraction%2520faces%2520limitations%2520due%2520to%2520low%2520spatial%250Aresolution%252C%2520noise%252C%2520multipath%2520reflection%252C%2520the%2520presence%2520of%2520ghost%2520targets%252C%2520and%250Amotion%2520blur.%2520Such%2520limitations%2520can%2520be%2520exacerbated%2520by%2520nonlinear%2520object%2520motion%252C%250Aparticularly%2520from%2520an%2520ego-centric%2520viewpoint.%2520It%2520becomes%2520evident%2520that%2520to%2520address%250Athese%2520challenges%252C%2520the%2520key%2520lies%2520in%2520exploiting%2520temporal%2520feature%2520relation%2520over%2520an%250Aextended%2520horizon%2520and%2520enforcing%2520spatial%2520motion%2520consistency%2520for%2520effective%250Aassociation.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520SIRA%2520%2528Scalable%2520Inter-frame%250ARelation%2520and%2520Association%2529%2520with%2520two%2520designs.%2520First%252C%2520inspired%2520by%2520Swin%250ATransformer%252C%2520we%2520introduce%2520extended%2520temporal%2520relation%252C%2520generalizing%2520the%2520existing%250Atemporal%2520relation%2520layer%2520from%2520two%2520consecutive%2520frames%2520to%2520multiple%2520inter-frames%250Awith%2520temporally%2520regrouped%2520window%2520attention%2520for%2520scalability.%2520Second%252C%2520we%2520propose%250Amotion%2520consistency%2520track%2520with%2520the%2520concept%2520of%2520a%2520pseudo-tracklet%2520generated%2520from%250Aobservational%2520data%2520for%2520better%2520trajectory%2520prediction%2520and%2520subsequent%2520object%250Aassociation.%2520Our%2520approach%2520achieves%252058.11%2520mAP%25400.5%2520for%2520oriented%2520object%2520detection%250Aand%252047.79%2520MOTA%2520for%2520multiple%2520object%2520tracking%2520on%2520the%2520Radiate%2520dataset%252C%2520surpassing%250Aprevious%2520state-of-the-art%2520by%2520a%2520margin%2520of%2520%252B4.11%2520mAP%25400.5%2520and%2520%252B9.94%2520MOTA%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIRA%3A%20Scalable%20Inter-frame%20Relation%20and%20Association%20for%20Radar%20Perception&entry.906535625=Ryoma%20Yataka%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi&entry.1292438233=%20%20Conventional%20radar%20feature%20extraction%20faces%20limitations%20due%20to%20low%20spatial%0Aresolution%2C%20noise%2C%20multipath%20reflection%2C%20the%20presence%20of%20ghost%20targets%2C%20and%0Amotion%20blur.%20Such%20limitations%20can%20be%20exacerbated%20by%20nonlinear%20object%20motion%2C%0Aparticularly%20from%20an%20ego-centric%20viewpoint.%20It%20becomes%20evident%20that%20to%20address%0Athese%20challenges%2C%20the%20key%20lies%20in%20exploiting%20temporal%20feature%20relation%20over%20an%0Aextended%20horizon%20and%20enforcing%20spatial%20motion%20consistency%20for%20effective%0Aassociation.%20To%20this%20end%2C%20this%20paper%20proposes%20SIRA%20%28Scalable%20Inter-frame%0ARelation%20and%20Association%29%20with%20two%20designs.%20First%2C%20inspired%20by%20Swin%0ATransformer%2C%20we%20introduce%20extended%20temporal%20relation%2C%20generalizing%20the%20existing%0Atemporal%20relation%20layer%20from%20two%20consecutive%20frames%20to%20multiple%20inter-frames%0Awith%20temporally%20regrouped%20window%20attention%20for%20scalability.%20Second%2C%20we%20propose%0Amotion%20consistency%20track%20with%20the%20concept%20of%20a%20pseudo-tracklet%20generated%20from%0Aobservational%20data%20for%20better%20trajectory%20prediction%20and%20subsequent%20object%0Aassociation.%20Our%20approach%20achieves%2058.11%20mAP%400.5%20for%20oriented%20object%20detection%0Aand%2047.79%20MOTA%20for%20multiple%20object%20tracking%20on%20the%20Radiate%20dataset%2C%20surpassing%0Aprevious%20state-of-the-art%20by%20a%20margin%20of%20%2B4.11%20mAP%400.5%20and%20%2B9.94%20MOTA%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02220v1&entry.124074799=Read"},
{"title": "SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models", "author": "Linglan Zhao and Xuerui Zhang and Ke Yan and Shouhong Ding and Weiran Huang", "abstract": "  Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2411.02175v1", "date": "2024-11-04", "relevancy": 2.5517, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5175}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFE%3A%20Slow%20and%20Fast%20Parameter-Efficient%20Tuning%20for%20Continual%20Learning%0A%20%20with%20Pre-Trained%20Models&body=Title%3A%20SAFE%3A%20Slow%20and%20Fast%20Parameter-Efficient%20Tuning%20for%20Continual%20Learning%0A%20%20with%20Pre-Trained%20Models%0AAuthor%3A%20Linglan%20Zhao%20and%20Xuerui%20Zhang%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Weiran%20Huang%0AAbstract%3A%20%20%20Continual%20learning%20aims%20to%20incrementally%20acquire%20new%20concepts%20in%20data%20streams%0Awhile%20resisting%20forgetting%20previous%20knowledge.%20With%20the%20rise%20of%20powerful%0Apre-trained%20models%20%28PTMs%29%2C%20there%20is%20a%20growing%20interest%20in%20training%20incremental%0Alearning%20systems%20using%20these%20foundation%20models%2C%20rather%20than%20learning%20from%0Ascratch.%20Existing%20works%20often%20view%20PTMs%20as%20a%20strong%20initial%20point%20and%20directly%0Aapply%20parameter-efficient%20tuning%20%28PET%29%20in%20the%20first%20session%20for%20adapting%20to%0Adownstream%20tasks.%20In%20the%20following%20sessions%2C%20most%20methods%20freeze%20model%0Aparameters%20for%20tackling%20forgetting%20issues.%20However%2C%20applying%20PET%20directly%20to%0Adownstream%20data%20cannot%20fully%20explore%20the%20inherent%20knowledge%20in%20PTMs.%0AAdditionally%2C%20freezing%20the%20parameters%20in%20incremental%20sessions%20hinders%20models%27%0Aplasticity%20to%20novel%20concepts%20not%20covered%20in%20the%20first%20session.%20To%20solve%20the%0Aabove%20issues%2C%20we%20propose%20a%20Slow%20And%20Fast%20parameter-Efficient%20tuning%20%28SAFE%29%0Aframework.%20In%20particular%2C%20to%20inherit%20general%20knowledge%20from%20foundation%20models%2C%0Awe%20include%20a%20transfer%20loss%20function%20by%20measuring%20the%20correlation%20between%20the%0APTM%20and%20the%20PET-applied%20model.%20After%20calibrating%20in%20the%20first%20session%2C%20the%20slow%0Aefficient%20tuning%20parameters%20can%20capture%20more%20informative%20features%2C%20improving%0Ageneralization%20to%20incoming%20classes.%20Moreover%2C%20to%20further%20incorporate%20novel%0Aconcepts%2C%20we%20strike%20a%20balance%20between%20stability%20and%20plasticity%20by%20fixing%20slow%0Aefficient%20tuning%20parameters%20and%20continuously%20updating%20the%20fast%20ones.%0ASpecifically%2C%20a%20cross-classification%20loss%20with%20feature%20alignment%20is%20proposed%20to%0Acircumvent%20catastrophic%20forgetting.%20During%20inference%2C%20we%20introduce%20an%0Aentropy-based%20aggregation%20strategy%20to%20dynamically%20utilize%20the%20complementarity%0Ain%20the%20slow%20and%20fast%20learners.%20Extensive%20experiments%20on%20seven%20benchmark%0Adatasets%20verify%20the%20effectiveness%20of%20our%20method%20by%20significantly%20surpassing%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFE%253A%2520Slow%2520and%2520Fast%2520Parameter-Efficient%2520Tuning%2520for%2520Continual%2520Learning%250A%2520%2520with%2520Pre-Trained%2520Models%26entry.906535625%3DLinglan%2520Zhao%2520and%2520Xuerui%2520Zhang%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%2520and%2520Weiran%2520Huang%26entry.1292438233%3D%2520%2520Continual%2520learning%2520aims%2520to%2520incrementally%2520acquire%2520new%2520concepts%2520in%2520data%2520streams%250Awhile%2520resisting%2520forgetting%2520previous%2520knowledge.%2520With%2520the%2520rise%2520of%2520powerful%250Apre-trained%2520models%2520%2528PTMs%2529%252C%2520there%2520is%2520a%2520growing%2520interest%2520in%2520training%2520incremental%250Alearning%2520systems%2520using%2520these%2520foundation%2520models%252C%2520rather%2520than%2520learning%2520from%250Ascratch.%2520Existing%2520works%2520often%2520view%2520PTMs%2520as%2520a%2520strong%2520initial%2520point%2520and%2520directly%250Aapply%2520parameter-efficient%2520tuning%2520%2528PET%2529%2520in%2520the%2520first%2520session%2520for%2520adapting%2520to%250Adownstream%2520tasks.%2520In%2520the%2520following%2520sessions%252C%2520most%2520methods%2520freeze%2520model%250Aparameters%2520for%2520tackling%2520forgetting%2520issues.%2520However%252C%2520applying%2520PET%2520directly%2520to%250Adownstream%2520data%2520cannot%2520fully%2520explore%2520the%2520inherent%2520knowledge%2520in%2520PTMs.%250AAdditionally%252C%2520freezing%2520the%2520parameters%2520in%2520incremental%2520sessions%2520hinders%2520models%2527%250Aplasticity%2520to%2520novel%2520concepts%2520not%2520covered%2520in%2520the%2520first%2520session.%2520To%2520solve%2520the%250Aabove%2520issues%252C%2520we%2520propose%2520a%2520Slow%2520And%2520Fast%2520parameter-Efficient%2520tuning%2520%2528SAFE%2529%250Aframework.%2520In%2520particular%252C%2520to%2520inherit%2520general%2520knowledge%2520from%2520foundation%2520models%252C%250Awe%2520include%2520a%2520transfer%2520loss%2520function%2520by%2520measuring%2520the%2520correlation%2520between%2520the%250APTM%2520and%2520the%2520PET-applied%2520model.%2520After%2520calibrating%2520in%2520the%2520first%2520session%252C%2520the%2520slow%250Aefficient%2520tuning%2520parameters%2520can%2520capture%2520more%2520informative%2520features%252C%2520improving%250Ageneralization%2520to%2520incoming%2520classes.%2520Moreover%252C%2520to%2520further%2520incorporate%2520novel%250Aconcepts%252C%2520we%2520strike%2520a%2520balance%2520between%2520stability%2520and%2520plasticity%2520by%2520fixing%2520slow%250Aefficient%2520tuning%2520parameters%2520and%2520continuously%2520updating%2520the%2520fast%2520ones.%250ASpecifically%252C%2520a%2520cross-classification%2520loss%2520with%2520feature%2520alignment%2520is%2520proposed%2520to%250Acircumvent%2520catastrophic%2520forgetting.%2520During%2520inference%252C%2520we%2520introduce%2520an%250Aentropy-based%2520aggregation%2520strategy%2520to%2520dynamically%2520utilize%2520the%2520complementarity%250Ain%2520the%2520slow%2520and%2520fast%2520learners.%2520Extensive%2520experiments%2520on%2520seven%2520benchmark%250Adatasets%2520verify%2520the%2520effectiveness%2520of%2520our%2520method%2520by%2520significantly%2520surpassing%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFE%3A%20Slow%20and%20Fast%20Parameter-Efficient%20Tuning%20for%20Continual%20Learning%0A%20%20with%20Pre-Trained%20Models&entry.906535625=Linglan%20Zhao%20and%20Xuerui%20Zhang%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Weiran%20Huang&entry.1292438233=%20%20Continual%20learning%20aims%20to%20incrementally%20acquire%20new%20concepts%20in%20data%20streams%0Awhile%20resisting%20forgetting%20previous%20knowledge.%20With%20the%20rise%20of%20powerful%0Apre-trained%20models%20%28PTMs%29%2C%20there%20is%20a%20growing%20interest%20in%20training%20incremental%0Alearning%20systems%20using%20these%20foundation%20models%2C%20rather%20than%20learning%20from%0Ascratch.%20Existing%20works%20often%20view%20PTMs%20as%20a%20strong%20initial%20point%20and%20directly%0Aapply%20parameter-efficient%20tuning%20%28PET%29%20in%20the%20first%20session%20for%20adapting%20to%0Adownstream%20tasks.%20In%20the%20following%20sessions%2C%20most%20methods%20freeze%20model%0Aparameters%20for%20tackling%20forgetting%20issues.%20However%2C%20applying%20PET%20directly%20to%0Adownstream%20data%20cannot%20fully%20explore%20the%20inherent%20knowledge%20in%20PTMs.%0AAdditionally%2C%20freezing%20the%20parameters%20in%20incremental%20sessions%20hinders%20models%27%0Aplasticity%20to%20novel%20concepts%20not%20covered%20in%20the%20first%20session.%20To%20solve%20the%0Aabove%20issues%2C%20we%20propose%20a%20Slow%20And%20Fast%20parameter-Efficient%20tuning%20%28SAFE%29%0Aframework.%20In%20particular%2C%20to%20inherit%20general%20knowledge%20from%20foundation%20models%2C%0Awe%20include%20a%20transfer%20loss%20function%20by%20measuring%20the%20correlation%20between%20the%0APTM%20and%20the%20PET-applied%20model.%20After%20calibrating%20in%20the%20first%20session%2C%20the%20slow%0Aefficient%20tuning%20parameters%20can%20capture%20more%20informative%20features%2C%20improving%0Ageneralization%20to%20incoming%20classes.%20Moreover%2C%20to%20further%20incorporate%20novel%0Aconcepts%2C%20we%20strike%20a%20balance%20between%20stability%20and%20plasticity%20by%20fixing%20slow%0Aefficient%20tuning%20parameters%20and%20continuously%20updating%20the%20fast%20ones.%0ASpecifically%2C%20a%20cross-classification%20loss%20with%20feature%20alignment%20is%20proposed%20to%0Acircumvent%20catastrophic%20forgetting.%20During%20inference%2C%20we%20introduce%20an%0Aentropy-based%20aggregation%20strategy%20to%20dynamically%20utilize%20the%20complementarity%0Ain%20the%20slow%20and%20fast%20learners.%20Extensive%20experiments%20on%20seven%20benchmark%0Adatasets%20verify%20the%20effectiveness%20of%20our%20method%20by%20significantly%20surpassing%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02175v1&entry.124074799=Read"},
{"title": "A citizen science toolkit to collect human perceptions of urban\n  environments using open street view images", "author": "Matthew Danish and SM Labib and Britta Ricker and Marco Helbich", "abstract": "  Street View Imagery (SVI) is a valuable data source for studies (e.g.,\nenvironmental assessments, green space identification or land cover\nclassification). While commercial SVI is available, such providers commonly\nrestrict copying or reuse in ways necessary for research. Open SVI datasets are\nreadily available from less restrictive sources, such as Mapillary, but due to\nthe heterogeneity of the images, these require substantial preprocessing,\nfiltering, and careful quality checks. We present an efficient method for\nautomated downloading, processing, cropping, and filtering open SVI, to be used\nin a survey of human perceptions of the streets portrayed in these images. We\ndemonstrate our open-source reusable SVI preparation and smartphone-friendly\nperception-survey software with Amsterdam (Netherlands) as the case study.\nUsing a citizen science approach, we collected from 331 people 22,637 ratings\nabout their perceptions for various criteria. We have published our software in\na public repository for future re-use and reproducibility.\n", "link": "http://arxiv.org/abs/2403.00174v4", "date": "2024-11-04", "relevancy": 2.5412, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5119}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5119}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20citizen%20science%20toolkit%20to%20collect%20human%20perceptions%20of%20urban%0A%20%20environments%20using%20open%20street%20view%20images&body=Title%3A%20A%20citizen%20science%20toolkit%20to%20collect%20human%20perceptions%20of%20urban%0A%20%20environments%20using%20open%20street%20view%20images%0AAuthor%3A%20Matthew%20Danish%20and%20SM%20Labib%20and%20Britta%20Ricker%20and%20Marco%20Helbich%0AAbstract%3A%20%20%20Street%20View%20Imagery%20%28SVI%29%20is%20a%20valuable%20data%20source%20for%20studies%20%28e.g.%2C%0Aenvironmental%20assessments%2C%20green%20space%20identification%20or%20land%20cover%0Aclassification%29.%20While%20commercial%20SVI%20is%20available%2C%20such%20providers%20commonly%0Arestrict%20copying%20or%20reuse%20in%20ways%20necessary%20for%20research.%20Open%20SVI%20datasets%20are%0Areadily%20available%20from%20less%20restrictive%20sources%2C%20such%20as%20Mapillary%2C%20but%20due%20to%0Athe%20heterogeneity%20of%20the%20images%2C%20these%20require%20substantial%20preprocessing%2C%0Afiltering%2C%20and%20careful%20quality%20checks.%20We%20present%20an%20efficient%20method%20for%0Aautomated%20downloading%2C%20processing%2C%20cropping%2C%20and%20filtering%20open%20SVI%2C%20to%20be%20used%0Ain%20a%20survey%20of%20human%20perceptions%20of%20the%20streets%20portrayed%20in%20these%20images.%20We%0Ademonstrate%20our%20open-source%20reusable%20SVI%20preparation%20and%20smartphone-friendly%0Aperception-survey%20software%20with%20Amsterdam%20%28Netherlands%29%20as%20the%20case%20study.%0AUsing%20a%20citizen%20science%20approach%2C%20we%20collected%20from%20331%20people%2022%2C637%20ratings%0Aabout%20their%20perceptions%20for%20various%20criteria.%20We%20have%20published%20our%20software%20in%0Aa%20public%20repository%20for%20future%20re-use%20and%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00174v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520citizen%2520science%2520toolkit%2520to%2520collect%2520human%2520perceptions%2520of%2520urban%250A%2520%2520environments%2520using%2520open%2520street%2520view%2520images%26entry.906535625%3DMatthew%2520Danish%2520and%2520SM%2520Labib%2520and%2520Britta%2520Ricker%2520and%2520Marco%2520Helbich%26entry.1292438233%3D%2520%2520Street%2520View%2520Imagery%2520%2528SVI%2529%2520is%2520a%2520valuable%2520data%2520source%2520for%2520studies%2520%2528e.g.%252C%250Aenvironmental%2520assessments%252C%2520green%2520space%2520identification%2520or%2520land%2520cover%250Aclassification%2529.%2520While%2520commercial%2520SVI%2520is%2520available%252C%2520such%2520providers%2520commonly%250Arestrict%2520copying%2520or%2520reuse%2520in%2520ways%2520necessary%2520for%2520research.%2520Open%2520SVI%2520datasets%2520are%250Areadily%2520available%2520from%2520less%2520restrictive%2520sources%252C%2520such%2520as%2520Mapillary%252C%2520but%2520due%2520to%250Athe%2520heterogeneity%2520of%2520the%2520images%252C%2520these%2520require%2520substantial%2520preprocessing%252C%250Afiltering%252C%2520and%2520careful%2520quality%2520checks.%2520We%2520present%2520an%2520efficient%2520method%2520for%250Aautomated%2520downloading%252C%2520processing%252C%2520cropping%252C%2520and%2520filtering%2520open%2520SVI%252C%2520to%2520be%2520used%250Ain%2520a%2520survey%2520of%2520human%2520perceptions%2520of%2520the%2520streets%2520portrayed%2520in%2520these%2520images.%2520We%250Ademonstrate%2520our%2520open-source%2520reusable%2520SVI%2520preparation%2520and%2520smartphone-friendly%250Aperception-survey%2520software%2520with%2520Amsterdam%2520%2528Netherlands%2529%2520as%2520the%2520case%2520study.%250AUsing%2520a%2520citizen%2520science%2520approach%252C%2520we%2520collected%2520from%2520331%2520people%252022%252C637%2520ratings%250Aabout%2520their%2520perceptions%2520for%2520various%2520criteria.%2520We%2520have%2520published%2520our%2520software%2520in%250Aa%2520public%2520repository%2520for%2520future%2520re-use%2520and%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00174v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20citizen%20science%20toolkit%20to%20collect%20human%20perceptions%20of%20urban%0A%20%20environments%20using%20open%20street%20view%20images&entry.906535625=Matthew%20Danish%20and%20SM%20Labib%20and%20Britta%20Ricker%20and%20Marco%20Helbich&entry.1292438233=%20%20Street%20View%20Imagery%20%28SVI%29%20is%20a%20valuable%20data%20source%20for%20studies%20%28e.g.%2C%0Aenvironmental%20assessments%2C%20green%20space%20identification%20or%20land%20cover%0Aclassification%29.%20While%20commercial%20SVI%20is%20available%2C%20such%20providers%20commonly%0Arestrict%20copying%20or%20reuse%20in%20ways%20necessary%20for%20research.%20Open%20SVI%20datasets%20are%0Areadily%20available%20from%20less%20restrictive%20sources%2C%20such%20as%20Mapillary%2C%20but%20due%20to%0Athe%20heterogeneity%20of%20the%20images%2C%20these%20require%20substantial%20preprocessing%2C%0Afiltering%2C%20and%20careful%20quality%20checks.%20We%20present%20an%20efficient%20method%20for%0Aautomated%20downloading%2C%20processing%2C%20cropping%2C%20and%20filtering%20open%20SVI%2C%20to%20be%20used%0Ain%20a%20survey%20of%20human%20perceptions%20of%20the%20streets%20portrayed%20in%20these%20images.%20We%0Ademonstrate%20our%20open-source%20reusable%20SVI%20preparation%20and%20smartphone-friendly%0Aperception-survey%20software%20with%20Amsterdam%20%28Netherlands%29%20as%20the%20case%20study.%0AUsing%20a%20citizen%20science%20approach%2C%20we%20collected%20from%20331%20people%2022%2C637%20ratings%0Aabout%20their%20perceptions%20for%20various%20criteria.%20We%20have%20published%20our%20software%20in%0Aa%20public%20repository%20for%20future%20re-use%20and%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00174v4&entry.124074799=Read"},
{"title": "ELU-GCN: Effectively Label-Utilizing Graph Convolutional Network", "author": "Jincheng Huang and Yujie Mo and Xiaoshuang Shi and Lei Feng and Xiaofeng Zhu", "abstract": "  The message-passing mechanism of graph convolutional networks (i.e., GCNs)\nenables label information to be propagated to a broader range of neighbors,\nthereby increasing the utilization of labels. However, the label information is\nnot always effectively utilized in the traditional GCN framework. To address\nthis issue, we propose a new two-step framework called ELU-GCN. In the first\nstage, ELU-GCN conducts graph learning to learn a new graph structure (\\ie\nELU-graph), which enables GCNs to effectively utilize label information. In the\nsecond stage, we design a new graph contrastive learning on the GCN framework\nfor representation learning by exploring the consistency and mutually exclusive\ninformation between the learned ELU graph and the original graph. Moreover, we\ntheoretically demonstrate that the proposed method can ensure the\ngeneralization ability of GCNs. Extensive experiments validate the superiority\nof the proposed method.\n", "link": "http://arxiv.org/abs/2411.02279v1", "date": "2024-11-04", "relevancy": 2.5287, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5376}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4969}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELU-GCN%3A%20Effectively%20Label-Utilizing%20Graph%20Convolutional%20Network&body=Title%3A%20ELU-GCN%3A%20Effectively%20Label-Utilizing%20Graph%20Convolutional%20Network%0AAuthor%3A%20Jincheng%20Huang%20and%20Yujie%20Mo%20and%20Xiaoshuang%20Shi%20and%20Lei%20Feng%20and%20Xiaofeng%20Zhu%0AAbstract%3A%20%20%20The%20message-passing%20mechanism%20of%20graph%20convolutional%20networks%20%28i.e.%2C%20GCNs%29%0Aenables%20label%20information%20to%20be%20propagated%20to%20a%20broader%20range%20of%20neighbors%2C%0Athereby%20increasing%20the%20utilization%20of%20labels.%20However%2C%20the%20label%20information%20is%0Anot%20always%20effectively%20utilized%20in%20the%20traditional%20GCN%20framework.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20new%20two-step%20framework%20called%20ELU-GCN.%20In%20the%20first%0Astage%2C%20ELU-GCN%20conducts%20graph%20learning%20to%20learn%20a%20new%20graph%20structure%20%28%5Cie%0AELU-graph%29%2C%20which%20enables%20GCNs%20to%20effectively%20utilize%20label%20information.%20In%20the%0Asecond%20stage%2C%20we%20design%20a%20new%20graph%20contrastive%20learning%20on%20the%20GCN%20framework%0Afor%20representation%20learning%20by%20exploring%20the%20consistency%20and%20mutually%20exclusive%0Ainformation%20between%20the%20learned%20ELU%20graph%20and%20the%20original%20graph.%20Moreover%2C%20we%0Atheoretically%20demonstrate%20that%20the%20proposed%20method%20can%20ensure%20the%0Ageneralization%20ability%20of%20GCNs.%20Extensive%20experiments%20validate%20the%20superiority%0Aof%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELU-GCN%253A%2520Effectively%2520Label-Utilizing%2520Graph%2520Convolutional%2520Network%26entry.906535625%3DJincheng%2520Huang%2520and%2520Yujie%2520Mo%2520and%2520Xiaoshuang%2520Shi%2520and%2520Lei%2520Feng%2520and%2520Xiaofeng%2520Zhu%26entry.1292438233%3D%2520%2520The%2520message-passing%2520mechanism%2520of%2520graph%2520convolutional%2520networks%2520%2528i.e.%252C%2520GCNs%2529%250Aenables%2520label%2520information%2520to%2520be%2520propagated%2520to%2520a%2520broader%2520range%2520of%2520neighbors%252C%250Athereby%2520increasing%2520the%2520utilization%2520of%2520labels.%2520However%252C%2520the%2520label%2520information%2520is%250Anot%2520always%2520effectively%2520utilized%2520in%2520the%2520traditional%2520GCN%2520framework.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520new%2520two-step%2520framework%2520called%2520ELU-GCN.%2520In%2520the%2520first%250Astage%252C%2520ELU-GCN%2520conducts%2520graph%2520learning%2520to%2520learn%2520a%2520new%2520graph%2520structure%2520%2528%255Cie%250AELU-graph%2529%252C%2520which%2520enables%2520GCNs%2520to%2520effectively%2520utilize%2520label%2520information.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520design%2520a%2520new%2520graph%2520contrastive%2520learning%2520on%2520the%2520GCN%2520framework%250Afor%2520representation%2520learning%2520by%2520exploring%2520the%2520consistency%2520and%2520mutually%2520exclusive%250Ainformation%2520between%2520the%2520learned%2520ELU%2520graph%2520and%2520the%2520original%2520graph.%2520Moreover%252C%2520we%250Atheoretically%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520ensure%2520the%250Ageneralization%2520ability%2520of%2520GCNs.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%250Aof%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELU-GCN%3A%20Effectively%20Label-Utilizing%20Graph%20Convolutional%20Network&entry.906535625=Jincheng%20Huang%20and%20Yujie%20Mo%20and%20Xiaoshuang%20Shi%20and%20Lei%20Feng%20and%20Xiaofeng%20Zhu&entry.1292438233=%20%20The%20message-passing%20mechanism%20of%20graph%20convolutional%20networks%20%28i.e.%2C%20GCNs%29%0Aenables%20label%20information%20to%20be%20propagated%20to%20a%20broader%20range%20of%20neighbors%2C%0Athereby%20increasing%20the%20utilization%20of%20labels.%20However%2C%20the%20label%20information%20is%0Anot%20always%20effectively%20utilized%20in%20the%20traditional%20GCN%20framework.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20new%20two-step%20framework%20called%20ELU-GCN.%20In%20the%20first%0Astage%2C%20ELU-GCN%20conducts%20graph%20learning%20to%20learn%20a%20new%20graph%20structure%20%28%5Cie%0AELU-graph%29%2C%20which%20enables%20GCNs%20to%20effectively%20utilize%20label%20information.%20In%20the%0Asecond%20stage%2C%20we%20design%20a%20new%20graph%20contrastive%20learning%20on%20the%20GCN%20framework%0Afor%20representation%20learning%20by%20exploring%20the%20consistency%20and%20mutually%20exclusive%0Ainformation%20between%20the%20learned%20ELU%20graph%20and%20the%20original%20graph.%20Moreover%2C%20we%0Atheoretically%20demonstrate%20that%20the%20proposed%20method%20can%20ensure%20the%0Ageneralization%20ability%20of%20GCNs.%20Extensive%20experiments%20validate%20the%20superiority%0Aof%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02279v1&entry.124074799=Read"},
{"title": "Understanding Variational Autoencoders with Intrinsic Dimension and\n  Information Imbalance", "author": "Charles Camboulin and Diego Doimo and Aldo Glielmo", "abstract": "  This work presents an analysis of the hidden representations of Variational\nAutoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information\nImbalance (II). We show that VAEs undergo a transition in behaviour once the\nbottleneck size is larger than the ID of the data, manifesting in a double\nhunchback ID profile and a qualitative shift in information processing as\ncaptured by the II. Our results also highlight two distinct training phases for\narchitectures with sufficiently large bottleneck sizes, consisting of a rapid\nfit and a slower generalisation, as assessed by a differentiated behaviour of\nID, II, and KL loss. These insights demonstrate that II and ID could be\nvaluable tools for aiding architecture search, for diagnosing underfitting in\nVAEs, and, more broadly, they contribute to advancing a unified understanding\nof deep generative models through geometric analysis.\n", "link": "http://arxiv.org/abs/2411.01978v1", "date": "2024-11-04", "relevancy": 2.528, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5191}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Variational%20Autoencoders%20with%20Intrinsic%20Dimension%20and%0A%20%20Information%20Imbalance&body=Title%3A%20Understanding%20Variational%20Autoencoders%20with%20Intrinsic%20Dimension%20and%0A%20%20Information%20Imbalance%0AAuthor%3A%20Charles%20Camboulin%20and%20Diego%20Doimo%20and%20Aldo%20Glielmo%0AAbstract%3A%20%20%20This%20work%20presents%20an%20analysis%20of%20the%20hidden%20representations%20of%20Variational%0AAutoencoders%20%28VAEs%29%20using%20the%20Intrinsic%20Dimension%20%28ID%29%20and%20the%20Information%0AImbalance%20%28II%29.%20We%20show%20that%20VAEs%20undergo%20a%20transition%20in%20behaviour%20once%20the%0Abottleneck%20size%20is%20larger%20than%20the%20ID%20of%20the%20data%2C%20manifesting%20in%20a%20double%0Ahunchback%20ID%20profile%20and%20a%20qualitative%20shift%20in%20information%20processing%20as%0Acaptured%20by%20the%20II.%20Our%20results%20also%20highlight%20two%20distinct%20training%20phases%20for%0Aarchitectures%20with%20sufficiently%20large%20bottleneck%20sizes%2C%20consisting%20of%20a%20rapid%0Afit%20and%20a%20slower%20generalisation%2C%20as%20assessed%20by%20a%20differentiated%20behaviour%20of%0AID%2C%20II%2C%20and%20KL%20loss.%20These%20insights%20demonstrate%20that%20II%20and%20ID%20could%20be%0Avaluable%20tools%20for%20aiding%20architecture%20search%2C%20for%20diagnosing%20underfitting%20in%0AVAEs%2C%20and%2C%20more%20broadly%2C%20they%20contribute%20to%20advancing%20a%20unified%20understanding%0Aof%20deep%20generative%20models%20through%20geometric%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Variational%2520Autoencoders%2520with%2520Intrinsic%2520Dimension%2520and%250A%2520%2520Information%2520Imbalance%26entry.906535625%3DCharles%2520Camboulin%2520and%2520Diego%2520Doimo%2520and%2520Aldo%2520Glielmo%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520an%2520analysis%2520of%2520the%2520hidden%2520representations%2520of%2520Variational%250AAutoencoders%2520%2528VAEs%2529%2520using%2520the%2520Intrinsic%2520Dimension%2520%2528ID%2529%2520and%2520the%2520Information%250AImbalance%2520%2528II%2529.%2520We%2520show%2520that%2520VAEs%2520undergo%2520a%2520transition%2520in%2520behaviour%2520once%2520the%250Abottleneck%2520size%2520is%2520larger%2520than%2520the%2520ID%2520of%2520the%2520data%252C%2520manifesting%2520in%2520a%2520double%250Ahunchback%2520ID%2520profile%2520and%2520a%2520qualitative%2520shift%2520in%2520information%2520processing%2520as%250Acaptured%2520by%2520the%2520II.%2520Our%2520results%2520also%2520highlight%2520two%2520distinct%2520training%2520phases%2520for%250Aarchitectures%2520with%2520sufficiently%2520large%2520bottleneck%2520sizes%252C%2520consisting%2520of%2520a%2520rapid%250Afit%2520and%2520a%2520slower%2520generalisation%252C%2520as%2520assessed%2520by%2520a%2520differentiated%2520behaviour%2520of%250AID%252C%2520II%252C%2520and%2520KL%2520loss.%2520These%2520insights%2520demonstrate%2520that%2520II%2520and%2520ID%2520could%2520be%250Avaluable%2520tools%2520for%2520aiding%2520architecture%2520search%252C%2520for%2520diagnosing%2520underfitting%2520in%250AVAEs%252C%2520and%252C%2520more%2520broadly%252C%2520they%2520contribute%2520to%2520advancing%2520a%2520unified%2520understanding%250Aof%2520deep%2520generative%2520models%2520through%2520geometric%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Variational%20Autoencoders%20with%20Intrinsic%20Dimension%20and%0A%20%20Information%20Imbalance&entry.906535625=Charles%20Camboulin%20and%20Diego%20Doimo%20and%20Aldo%20Glielmo&entry.1292438233=%20%20This%20work%20presents%20an%20analysis%20of%20the%20hidden%20representations%20of%20Variational%0AAutoencoders%20%28VAEs%29%20using%20the%20Intrinsic%20Dimension%20%28ID%29%20and%20the%20Information%0AImbalance%20%28II%29.%20We%20show%20that%20VAEs%20undergo%20a%20transition%20in%20behaviour%20once%20the%0Abottleneck%20size%20is%20larger%20than%20the%20ID%20of%20the%20data%2C%20manifesting%20in%20a%20double%0Ahunchback%20ID%20profile%20and%20a%20qualitative%20shift%20in%20information%20processing%20as%0Acaptured%20by%20the%20II.%20Our%20results%20also%20highlight%20two%20distinct%20training%20phases%20for%0Aarchitectures%20with%20sufficiently%20large%20bottleneck%20sizes%2C%20consisting%20of%20a%20rapid%0Afit%20and%20a%20slower%20generalisation%2C%20as%20assessed%20by%20a%20differentiated%20behaviour%20of%0AID%2C%20II%2C%20and%20KL%20loss.%20These%20insights%20demonstrate%20that%20II%20and%20ID%20could%20be%0Avaluable%20tools%20for%20aiding%20architecture%20search%2C%20for%20diagnosing%20underfitting%20in%0AVAEs%2C%20and%2C%20more%20broadly%2C%20they%20contribute%20to%20advancing%20a%20unified%20understanding%0Aof%20deep%20generative%20models%20through%20geometric%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01978v1&entry.124074799=Read"},
{"title": "Advancements and limitations of LLMs in replicating human color-word\n  associations", "author": "Makoto Fukushima and Shusuke Eshita and Hiroshige Fukuhara", "abstract": "  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.\n", "link": "http://arxiv.org/abs/2411.02116v1", "date": "2024-11-04", "relevancy": 2.5184, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20and%20limitations%20of%20LLMs%20in%20replicating%20human%20color-word%0A%20%20associations&body=Title%3A%20Advancements%20and%20limitations%20of%20LLMs%20in%20replicating%20human%20color-word%0A%20%20associations%0AAuthor%3A%20Makoto%20Fukushima%20and%20Shusuke%20Eshita%20and%20Hiroshige%20Fukuhara%0AAbstract%3A%20%20%20Color-word%20associations%20play%20a%20fundamental%20role%20in%20human%20cognition%20and%20design%0Aapplications.%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20widely%20available%20and%0Ademonstrated%20intelligent%20behaviors%20in%20various%20benchmarks%20with%20natural%0Aconversation%20skills.%20However%2C%20their%20ability%20to%20replicate%20human%20color-word%0Aassociations%20remains%20understudied.%20We%20compared%20multiple%20generations%20of%20LLMs%0A%28from%20GPT-3%20to%20GPT-%204o%29%20against%20human%20color-word%20associations%20using%20data%0Acollected%20from%20over%2010%2C000%20Japanese%20participants%2C%20involving%2017%20colors%20and%20words%0Afrom%20eight%20categories%20in%20Japanese.%20Our%20findings%20reveal%20a%20clear%20progression%20in%0ALLM%20performance%20across%20generations%2C%20with%20GPT-4o%20achieving%20the%20highest%20accuracy%0Ain%20predicting%20the%20best%20voted%20word%20for%20each%20color%20and%20category%2C%20particularly%0Awhen%20using%20visual%20inputs%20rather%20than%20text-based%20color%20codes.%20However%2C%20the%0Ahighest%20median%20performance%20was%20approximately%2050%25%20even%20for%20GPT4-o%20with%20visual%0Ainputs%20%28chance%20level%20is%2010%25%29%2C%20and%20the%20performance%20levels%20varied%20significantly%0Aacross%20word%20categories%20and%20colors%2C%20indicating%20a%20failure%20to%20fully%20replicate%0Ahuman%20color-word%20associations.%20On%20the%20other%20hand%2C%20color%20discrimination%20ability%0Aestimated%20from%20our%20color-word%20association%20data%20showed%20that%20LLMs%20demonstrated%0Ahigh%20correlation%20with%20human%20color%20discrimination%20patterns%2C%20similarly%20to%0Aprevious%20studies.%20Our%20study%20highlights%20both%20the%20advancements%20in%20LLM%0Acapabilities%20and%20their%20persistent%20limitations%2C%20suggesting%20differences%20in%0Asemantic%20memory%20structures%20between%20humans%20and%20LLMs%20in%20representing%20color-word%0Aassociations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520and%2520limitations%2520of%2520LLMs%2520in%2520replicating%2520human%2520color-word%250A%2520%2520associations%26entry.906535625%3DMakoto%2520Fukushima%2520and%2520Shusuke%2520Eshita%2520and%2520Hiroshige%2520Fukuhara%26entry.1292438233%3D%2520%2520Color-word%2520associations%2520play%2520a%2520fundamental%2520role%2520in%2520human%2520cognition%2520and%2520design%250Aapplications.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520widely%2520available%2520and%250Ademonstrated%2520intelligent%2520behaviors%2520in%2520various%2520benchmarks%2520with%2520natural%250Aconversation%2520skills.%2520However%252C%2520their%2520ability%2520to%2520replicate%2520human%2520color-word%250Aassociations%2520remains%2520understudied.%2520We%2520compared%2520multiple%2520generations%2520of%2520LLMs%250A%2528from%2520GPT-3%2520to%2520GPT-%25204o%2529%2520against%2520human%2520color-word%2520associations%2520using%2520data%250Acollected%2520from%2520over%252010%252C000%2520Japanese%2520participants%252C%2520involving%252017%2520colors%2520and%2520words%250Afrom%2520eight%2520categories%2520in%2520Japanese.%2520Our%2520findings%2520reveal%2520a%2520clear%2520progression%2520in%250ALLM%2520performance%2520across%2520generations%252C%2520with%2520GPT-4o%2520achieving%2520the%2520highest%2520accuracy%250Ain%2520predicting%2520the%2520best%2520voted%2520word%2520for%2520each%2520color%2520and%2520category%252C%2520particularly%250Awhen%2520using%2520visual%2520inputs%2520rather%2520than%2520text-based%2520color%2520codes.%2520However%252C%2520the%250Ahighest%2520median%2520performance%2520was%2520approximately%252050%2525%2520even%2520for%2520GPT4-o%2520with%2520visual%250Ainputs%2520%2528chance%2520level%2520is%252010%2525%2529%252C%2520and%2520the%2520performance%2520levels%2520varied%2520significantly%250Aacross%2520word%2520categories%2520and%2520colors%252C%2520indicating%2520a%2520failure%2520to%2520fully%2520replicate%250Ahuman%2520color-word%2520associations.%2520On%2520the%2520other%2520hand%252C%2520color%2520discrimination%2520ability%250Aestimated%2520from%2520our%2520color-word%2520association%2520data%2520showed%2520that%2520LLMs%2520demonstrated%250Ahigh%2520correlation%2520with%2520human%2520color%2520discrimination%2520patterns%252C%2520similarly%2520to%250Aprevious%2520studies.%2520Our%2520study%2520highlights%2520both%2520the%2520advancements%2520in%2520LLM%250Acapabilities%2520and%2520their%2520persistent%2520limitations%252C%2520suggesting%2520differences%2520in%250Asemantic%2520memory%2520structures%2520between%2520humans%2520and%2520LLMs%2520in%2520representing%2520color-word%250Aassociations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20and%20limitations%20of%20LLMs%20in%20replicating%20human%20color-word%0A%20%20associations&entry.906535625=Makoto%20Fukushima%20and%20Shusuke%20Eshita%20and%20Hiroshige%20Fukuhara&entry.1292438233=%20%20Color-word%20associations%20play%20a%20fundamental%20role%20in%20human%20cognition%20and%20design%0Aapplications.%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20widely%20available%20and%0Ademonstrated%20intelligent%20behaviors%20in%20various%20benchmarks%20with%20natural%0Aconversation%20skills.%20However%2C%20their%20ability%20to%20replicate%20human%20color-word%0Aassociations%20remains%20understudied.%20We%20compared%20multiple%20generations%20of%20LLMs%0A%28from%20GPT-3%20to%20GPT-%204o%29%20against%20human%20color-word%20associations%20using%20data%0Acollected%20from%20over%2010%2C000%20Japanese%20participants%2C%20involving%2017%20colors%20and%20words%0Afrom%20eight%20categories%20in%20Japanese.%20Our%20findings%20reveal%20a%20clear%20progression%20in%0ALLM%20performance%20across%20generations%2C%20with%20GPT-4o%20achieving%20the%20highest%20accuracy%0Ain%20predicting%20the%20best%20voted%20word%20for%20each%20color%20and%20category%2C%20particularly%0Awhen%20using%20visual%20inputs%20rather%20than%20text-based%20color%20codes.%20However%2C%20the%0Ahighest%20median%20performance%20was%20approximately%2050%25%20even%20for%20GPT4-o%20with%20visual%0Ainputs%20%28chance%20level%20is%2010%25%29%2C%20and%20the%20performance%20levels%20varied%20significantly%0Aacross%20word%20categories%20and%20colors%2C%20indicating%20a%20failure%20to%20fully%20replicate%0Ahuman%20color-word%20associations.%20On%20the%20other%20hand%2C%20color%20discrimination%20ability%0Aestimated%20from%20our%20color-word%20association%20data%20showed%20that%20LLMs%20demonstrated%0Ahigh%20correlation%20with%20human%20color%20discrimination%20patterns%2C%20similarly%20to%0Aprevious%20studies.%20Our%20study%20highlights%20both%20the%20advancements%20in%20LLM%0Acapabilities%20and%20their%20persistent%20limitations%2C%20suggesting%20differences%20in%0Asemantic%20memory%20structures%20between%20humans%20and%20LLMs%20in%20representing%20color-word%0Aassociations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02116v1&entry.124074799=Read"},
{"title": "Learning to grok: Emergence of in-context learning and skill composition\n  in modular arithmetic tasks", "author": "Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov", "abstract": "  Large language models can solve tasks that were not present in the training\nset. This capability is believed to be due to in-context learning and skill\ncomposition. In this work, we study the emergence of in-context learning and\nskill composition in a collection of modular arithmetic tasks. Specifically, we\nconsider a finite collection of linear modular functions $z = a \\, x + b \\, y\n\\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use\nsome of these tasks for pre-training and the rest for out-of-distribution\ntesting. We empirically show that a GPT-style transformer exhibits a transition\nfrom in-distribution to out-of-distribution generalization as the number of\npre-training tasks increases. We find that the smallest model capable of\nout-of-distribution generalization requires two transformer blocks, while for\ndeeper models, the out-of-distribution generalization phase is\n\\emph{transient}, necessitating early stopping. Finally, we perform an\ninterpretability study of the pre-trained models, revealing highly structured\nrepresentations in both attention heads and MLPs; and discuss the learned\nalgorithms. Notably, we find an algorithmic shift in deeper models, as we go\nfrom few to many in-context examples.\n", "link": "http://arxiv.org/abs/2406.02550v2", "date": "2024-11-04", "relevancy": 2.5087, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20grok%3A%20Emergence%20of%20in-context%20learning%20and%20skill%20composition%0A%20%20in%20modular%20arithmetic%20tasks&body=Title%3A%20Learning%20to%20grok%3A%20Emergence%20of%20in-context%20learning%20and%20skill%20composition%0A%20%20in%20modular%20arithmetic%20tasks%0AAuthor%3A%20Tianyu%20He%20and%20Darshil%20Doshi%20and%20Aritra%20Das%20and%20Andrey%20Gromov%0AAbstract%3A%20%20%20Large%20language%20models%20can%20solve%20tasks%20that%20were%20not%20present%20in%20the%20training%0Aset.%20This%20capability%20is%20believed%20to%20be%20due%20to%20in-context%20learning%20and%20skill%0Acomposition.%20In%20this%20work%2C%20we%20study%20the%20emergence%20of%20in-context%20learning%20and%0Askill%20composition%20in%20a%20collection%20of%20modular%20arithmetic%20tasks.%20Specifically%2C%20we%0Aconsider%20a%20finite%20collection%20of%20linear%20modular%20functions%20%24z%20%3D%20a%20%5C%2C%20x%20%2B%20b%20%5C%2C%20y%0A%5C%3B%5Cmathrm%7Bmod%7D%5C%3B%20p%24%20labeled%20by%20the%20vector%20%24%28a%2C%20b%29%20%5Cin%20%5Cmathbb%7BZ%7D_p%5E2%24.%20We%20use%0Asome%20of%20these%20tasks%20for%20pre-training%20and%20the%20rest%20for%20out-of-distribution%0Atesting.%20We%20empirically%20show%20that%20a%20GPT-style%20transformer%20exhibits%20a%20transition%0Afrom%20in-distribution%20to%20out-of-distribution%20generalization%20as%20the%20number%20of%0Apre-training%20tasks%20increases.%20We%20find%20that%20the%20smallest%20model%20capable%20of%0Aout-of-distribution%20generalization%20requires%20two%20transformer%20blocks%2C%20while%20for%0Adeeper%20models%2C%20the%20out-of-distribution%20generalization%20phase%20is%0A%5Cemph%7Btransient%7D%2C%20necessitating%20early%20stopping.%20Finally%2C%20we%20perform%20an%0Ainterpretability%20study%20of%20the%20pre-trained%20models%2C%20revealing%20highly%20structured%0Arepresentations%20in%20both%20attention%20heads%20and%20MLPs%3B%20and%20discuss%20the%20learned%0Aalgorithms.%20Notably%2C%20we%20find%20an%20algorithmic%20shift%20in%20deeper%20models%2C%20as%20we%20go%0Afrom%20few%20to%20many%20in-context%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520grok%253A%2520Emergence%2520of%2520in-context%2520learning%2520and%2520skill%2520composition%250A%2520%2520in%2520modular%2520arithmetic%2520tasks%26entry.906535625%3DTianyu%2520He%2520and%2520Darshil%2520Doshi%2520and%2520Aritra%2520Das%2520and%2520Andrey%2520Gromov%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520can%2520solve%2520tasks%2520that%2520were%2520not%2520present%2520in%2520the%2520training%250Aset.%2520This%2520capability%2520is%2520believed%2520to%2520be%2520due%2520to%2520in-context%2520learning%2520and%2520skill%250Acomposition.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520emergence%2520of%2520in-context%2520learning%2520and%250Askill%2520composition%2520in%2520a%2520collection%2520of%2520modular%2520arithmetic%2520tasks.%2520Specifically%252C%2520we%250Aconsider%2520a%2520finite%2520collection%2520of%2520linear%2520modular%2520functions%2520%2524z%2520%253D%2520a%2520%255C%252C%2520x%2520%252B%2520b%2520%255C%252C%2520y%250A%255C%253B%255Cmathrm%257Bmod%257D%255C%253B%2520p%2524%2520labeled%2520by%2520the%2520vector%2520%2524%2528a%252C%2520b%2529%2520%255Cin%2520%255Cmathbb%257BZ%257D_p%255E2%2524.%2520We%2520use%250Asome%2520of%2520these%2520tasks%2520for%2520pre-training%2520and%2520the%2520rest%2520for%2520out-of-distribution%250Atesting.%2520We%2520empirically%2520show%2520that%2520a%2520GPT-style%2520transformer%2520exhibits%2520a%2520transition%250Afrom%2520in-distribution%2520to%2520out-of-distribution%2520generalization%2520as%2520the%2520number%2520of%250Apre-training%2520tasks%2520increases.%2520We%2520find%2520that%2520the%2520smallest%2520model%2520capable%2520of%250Aout-of-distribution%2520generalization%2520requires%2520two%2520transformer%2520blocks%252C%2520while%2520for%250Adeeper%2520models%252C%2520the%2520out-of-distribution%2520generalization%2520phase%2520is%250A%255Cemph%257Btransient%257D%252C%2520necessitating%2520early%2520stopping.%2520Finally%252C%2520we%2520perform%2520an%250Ainterpretability%2520study%2520of%2520the%2520pre-trained%2520models%252C%2520revealing%2520highly%2520structured%250Arepresentations%2520in%2520both%2520attention%2520heads%2520and%2520MLPs%253B%2520and%2520discuss%2520the%2520learned%250Aalgorithms.%2520Notably%252C%2520we%2520find%2520an%2520algorithmic%2520shift%2520in%2520deeper%2520models%252C%2520as%2520we%2520go%250Afrom%2520few%2520to%2520many%2520in-context%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20grok%3A%20Emergence%20of%20in-context%20learning%20and%20skill%20composition%0A%20%20in%20modular%20arithmetic%20tasks&entry.906535625=Tianyu%20He%20and%20Darshil%20Doshi%20and%20Aritra%20Das%20and%20Andrey%20Gromov&entry.1292438233=%20%20Large%20language%20models%20can%20solve%20tasks%20that%20were%20not%20present%20in%20the%20training%0Aset.%20This%20capability%20is%20believed%20to%20be%20due%20to%20in-context%20learning%20and%20skill%0Acomposition.%20In%20this%20work%2C%20we%20study%20the%20emergence%20of%20in-context%20learning%20and%0Askill%20composition%20in%20a%20collection%20of%20modular%20arithmetic%20tasks.%20Specifically%2C%20we%0Aconsider%20a%20finite%20collection%20of%20linear%20modular%20functions%20%24z%20%3D%20a%20%5C%2C%20x%20%2B%20b%20%5C%2C%20y%0A%5C%3B%5Cmathrm%7Bmod%7D%5C%3B%20p%24%20labeled%20by%20the%20vector%20%24%28a%2C%20b%29%20%5Cin%20%5Cmathbb%7BZ%7D_p%5E2%24.%20We%20use%0Asome%20of%20these%20tasks%20for%20pre-training%20and%20the%20rest%20for%20out-of-distribution%0Atesting.%20We%20empirically%20show%20that%20a%20GPT-style%20transformer%20exhibits%20a%20transition%0Afrom%20in-distribution%20to%20out-of-distribution%20generalization%20as%20the%20number%20of%0Apre-training%20tasks%20increases.%20We%20find%20that%20the%20smallest%20model%20capable%20of%0Aout-of-distribution%20generalization%20requires%20two%20transformer%20blocks%2C%20while%20for%0Adeeper%20models%2C%20the%20out-of-distribution%20generalization%20phase%20is%0A%5Cemph%7Btransient%7D%2C%20necessitating%20early%20stopping.%20Finally%2C%20we%20perform%20an%0Ainterpretability%20study%20of%20the%20pre-trained%20models%2C%20revealing%20highly%20structured%0Arepresentations%20in%20both%20attention%20heads%20and%20MLPs%3B%20and%20discuss%20the%20learned%0Aalgorithms.%20Notably%2C%20we%20find%20an%20algorithmic%20shift%20in%20deeper%20models%2C%20as%20we%20go%0Afrom%20few%20to%20many%20in-context%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02550v2&entry.124074799=Read"},
{"title": "CTEFM-VC: Zero-Shot Voice Conversion Based on Content-Aware Timbre\n  Ensemble Modeling and Flow Matching", "author": "Yu Pan and Yuguang Yang and Jixun Yao and Jianhao Ye and Hongbin Zhou and Lei Ma and Jianjun Zhao", "abstract": "  Zero-shot voice conversion (VC) aims to transform the timbre of a source\nspeaker into any previously unseen target speaker, while preserving the\noriginal linguistic content. Despite notable progress, attaining a degree of\nspeaker similarity and naturalness on par with ground truth recordings\ncontinues to pose great challenge. In this paper, we propose CTEFM-VC, a\nzero-shot VC framework that leverages Content-aware Timbre Ensemble modeling\nand Flow Matching. Specifically, CTEFM-VC disentangles utterances into\nlinguistic content and timbre representations, subsequently utilizing a\nconditional flow matching model and a vocoder to reconstruct the\nmel-spectrogram and waveform. To enhance its timbre modeling capability and the\nnaturalness of generated speech, we propose a context-aware timbre ensemble\nmodeling approach that adaptively integrates diverse speaker verification\nembeddings and enables the joint utilization of linguistic and timbre features\nthrough a cross-attention module. Experiments show that our CTEFM-VC system\nsurpasses state-of-the-art VC methods in both speaker similarity and\nnaturalness by at least 18.5% and 7.0%.\n", "link": "http://arxiv.org/abs/2411.02026v1", "date": "2024-11-04", "relevancy": 2.5086, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5088}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5016}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTEFM-VC%3A%20Zero-Shot%20Voice%20Conversion%20Based%20on%20Content-Aware%20Timbre%0A%20%20Ensemble%20Modeling%20and%20Flow%20Matching&body=Title%3A%20CTEFM-VC%3A%20Zero-Shot%20Voice%20Conversion%20Based%20on%20Content-Aware%20Timbre%0A%20%20Ensemble%20Modeling%20and%20Flow%20Matching%0AAuthor%3A%20Yu%20Pan%20and%20Yuguang%20Yang%20and%20Jixun%20Yao%20and%20Jianhao%20Ye%20and%20Hongbin%20Zhou%20and%20Lei%20Ma%20and%20Jianjun%20Zhao%0AAbstract%3A%20%20%20Zero-shot%20voice%20conversion%20%28VC%29%20aims%20to%20transform%20the%20timbre%20of%20a%20source%0Aspeaker%20into%20any%20previously%20unseen%20target%20speaker%2C%20while%20preserving%20the%0Aoriginal%20linguistic%20content.%20Despite%20notable%20progress%2C%20attaining%20a%20degree%20of%0Aspeaker%20similarity%20and%20naturalness%20on%20par%20with%20ground%20truth%20recordings%0Acontinues%20to%20pose%20great%20challenge.%20In%20this%20paper%2C%20we%20propose%20CTEFM-VC%2C%20a%0Azero-shot%20VC%20framework%20that%20leverages%20Content-aware%20Timbre%20Ensemble%20modeling%0Aand%20Flow%20Matching.%20Specifically%2C%20CTEFM-VC%20disentangles%20utterances%20into%0Alinguistic%20content%20and%20timbre%20representations%2C%20subsequently%20utilizing%20a%0Aconditional%20flow%20matching%20model%20and%20a%20vocoder%20to%20reconstruct%20the%0Amel-spectrogram%20and%20waveform.%20To%20enhance%20its%20timbre%20modeling%20capability%20and%20the%0Anaturalness%20of%20generated%20speech%2C%20we%20propose%20a%20context-aware%20timbre%20ensemble%0Amodeling%20approach%20that%20adaptively%20integrates%20diverse%20speaker%20verification%0Aembeddings%20and%20enables%20the%20joint%20utilization%20of%20linguistic%20and%20timbre%20features%0Athrough%20a%20cross-attention%20module.%20Experiments%20show%20that%20our%20CTEFM-VC%20system%0Asurpasses%20state-of-the-art%20VC%20methods%20in%20both%20speaker%20similarity%20and%0Anaturalness%20by%20at%20least%2018.5%25%20and%207.0%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTEFM-VC%253A%2520Zero-Shot%2520Voice%2520Conversion%2520Based%2520on%2520Content-Aware%2520Timbre%250A%2520%2520Ensemble%2520Modeling%2520and%2520Flow%2520Matching%26entry.906535625%3DYu%2520Pan%2520and%2520Yuguang%2520Yang%2520and%2520Jixun%2520Yao%2520and%2520Jianhao%2520Ye%2520and%2520Hongbin%2520Zhou%2520and%2520Lei%2520Ma%2520and%2520Jianjun%2520Zhao%26entry.1292438233%3D%2520%2520Zero-shot%2520voice%2520conversion%2520%2528VC%2529%2520aims%2520to%2520transform%2520the%2520timbre%2520of%2520a%2520source%250Aspeaker%2520into%2520any%2520previously%2520unseen%2520target%2520speaker%252C%2520while%2520preserving%2520the%250Aoriginal%2520linguistic%2520content.%2520Despite%2520notable%2520progress%252C%2520attaining%2520a%2520degree%2520of%250Aspeaker%2520similarity%2520and%2520naturalness%2520on%2520par%2520with%2520ground%2520truth%2520recordings%250Acontinues%2520to%2520pose%2520great%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CTEFM-VC%252C%2520a%250Azero-shot%2520VC%2520framework%2520that%2520leverages%2520Content-aware%2520Timbre%2520Ensemble%2520modeling%250Aand%2520Flow%2520Matching.%2520Specifically%252C%2520CTEFM-VC%2520disentangles%2520utterances%2520into%250Alinguistic%2520content%2520and%2520timbre%2520representations%252C%2520subsequently%2520utilizing%2520a%250Aconditional%2520flow%2520matching%2520model%2520and%2520a%2520vocoder%2520to%2520reconstruct%2520the%250Amel-spectrogram%2520and%2520waveform.%2520To%2520enhance%2520its%2520timbre%2520modeling%2520capability%2520and%2520the%250Anaturalness%2520of%2520generated%2520speech%252C%2520we%2520propose%2520a%2520context-aware%2520timbre%2520ensemble%250Amodeling%2520approach%2520that%2520adaptively%2520integrates%2520diverse%2520speaker%2520verification%250Aembeddings%2520and%2520enables%2520the%2520joint%2520utilization%2520of%2520linguistic%2520and%2520timbre%2520features%250Athrough%2520a%2520cross-attention%2520module.%2520Experiments%2520show%2520that%2520our%2520CTEFM-VC%2520system%250Asurpasses%2520state-of-the-art%2520VC%2520methods%2520in%2520both%2520speaker%2520similarity%2520and%250Anaturalness%2520by%2520at%2520least%252018.5%2525%2520and%25207.0%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTEFM-VC%3A%20Zero-Shot%20Voice%20Conversion%20Based%20on%20Content-Aware%20Timbre%0A%20%20Ensemble%20Modeling%20and%20Flow%20Matching&entry.906535625=Yu%20Pan%20and%20Yuguang%20Yang%20and%20Jixun%20Yao%20and%20Jianhao%20Ye%20and%20Hongbin%20Zhou%20and%20Lei%20Ma%20and%20Jianjun%20Zhao&entry.1292438233=%20%20Zero-shot%20voice%20conversion%20%28VC%29%20aims%20to%20transform%20the%20timbre%20of%20a%20source%0Aspeaker%20into%20any%20previously%20unseen%20target%20speaker%2C%20while%20preserving%20the%0Aoriginal%20linguistic%20content.%20Despite%20notable%20progress%2C%20attaining%20a%20degree%20of%0Aspeaker%20similarity%20and%20naturalness%20on%20par%20with%20ground%20truth%20recordings%0Acontinues%20to%20pose%20great%20challenge.%20In%20this%20paper%2C%20we%20propose%20CTEFM-VC%2C%20a%0Azero-shot%20VC%20framework%20that%20leverages%20Content-aware%20Timbre%20Ensemble%20modeling%0Aand%20Flow%20Matching.%20Specifically%2C%20CTEFM-VC%20disentangles%20utterances%20into%0Alinguistic%20content%20and%20timbre%20representations%2C%20subsequently%20utilizing%20a%0Aconditional%20flow%20matching%20model%20and%20a%20vocoder%20to%20reconstruct%20the%0Amel-spectrogram%20and%20waveform.%20To%20enhance%20its%20timbre%20modeling%20capability%20and%20the%0Anaturalness%20of%20generated%20speech%2C%20we%20propose%20a%20context-aware%20timbre%20ensemble%0Amodeling%20approach%20that%20adaptively%20integrates%20diverse%20speaker%20verification%0Aembeddings%20and%20enables%20the%20joint%20utilization%20of%20linguistic%20and%20timbre%20features%0Athrough%20a%20cross-attention%20module.%20Experiments%20show%20that%20our%20CTEFM-VC%20system%0Asurpasses%20state-of-the-art%20VC%20methods%20in%20both%20speaker%20similarity%20and%0Anaturalness%20by%20at%20least%2018.5%25%20and%207.0%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02026v1&entry.124074799=Read"},
{"title": "QIS : Interactive Segmentation via Quasi-Conformal Mappings", "author": "Han Zhang and Daoping Zhang and Lok Ming Lui", "abstract": "  Image segmentation plays a crucial role in extracting important objects of\ninterest from images, enabling various applications. While existing methods\nhave shown success in segmenting clean images, they often struggle to produce\naccurate segmentation results when dealing with degraded images, such as those\ncontaining noise or occlusions. To address this challenge, interactive\nsegmentation has emerged as a promising approach, allowing users to provide\nmeaningful input to guide the segmentation process. However, an important\nproblem in interactive segmentation lies in determining how to incorporate\nminimal yet meaningful user guidance into the segmentation model. In this\npaper, we propose the quasi-conformal interactive segmentation (QIS) model,\nwhich incorporates user input in the form of positive and negative clicks.\nUsers mark a few pixels belonging to the object region as positive clicks,\nindicating that the segmentation model should include a region around these\nclicks. Conversely, negative clicks are provided on pixels belonging to the\nbackground, instructing the model to exclude the region near these clicks from\nthe segmentation mask. Additionally, the segmentation mask is obtained by\ndeforming a template mask with the same topology as the object of interest\nusing an orientation-preserving quasiconformal mapping. This approach helps to\navoid topological errors in the segmentation results. We provide a thorough\nanalysis of the proposed model, including theoretical support for the ability\nof QIS to include or exclude regions of interest or disinterest based on the\nuser's indication. To evaluate the performance of QIS, we conduct experiments\non synthesized images, medical images, natural images and noisy natural images.\nThe results demonstrate the efficacy of our proposed method.\n", "link": "http://arxiv.org/abs/2402.14695v2", "date": "2024-11-04", "relevancy": 2.5086, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5099}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5047}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QIS%20%3A%20Interactive%20Segmentation%20via%20Quasi-Conformal%20Mappings&body=Title%3A%20QIS%20%3A%20Interactive%20Segmentation%20via%20Quasi-Conformal%20Mappings%0AAuthor%3A%20Han%20Zhang%20and%20Daoping%20Zhang%20and%20Lok%20Ming%20Lui%0AAbstract%3A%20%20%20Image%20segmentation%20plays%20a%20crucial%20role%20in%20extracting%20important%20objects%20of%0Ainterest%20from%20images%2C%20enabling%20various%20applications.%20While%20existing%20methods%0Ahave%20shown%20success%20in%20segmenting%20clean%20images%2C%20they%20often%20struggle%20to%20produce%0Aaccurate%20segmentation%20results%20when%20dealing%20with%20degraded%20images%2C%20such%20as%20those%0Acontaining%20noise%20or%20occlusions.%20To%20address%20this%20challenge%2C%20interactive%0Asegmentation%20has%20emerged%20as%20a%20promising%20approach%2C%20allowing%20users%20to%20provide%0Ameaningful%20input%20to%20guide%20the%20segmentation%20process.%20However%2C%20an%20important%0Aproblem%20in%20interactive%20segmentation%20lies%20in%20determining%20how%20to%20incorporate%0Aminimal%20yet%20meaningful%20user%20guidance%20into%20the%20segmentation%20model.%20In%20this%0Apaper%2C%20we%20propose%20the%20quasi-conformal%20interactive%20segmentation%20%28QIS%29%20model%2C%0Awhich%20incorporates%20user%20input%20in%20the%20form%20of%20positive%20and%20negative%20clicks.%0AUsers%20mark%20a%20few%20pixels%20belonging%20to%20the%20object%20region%20as%20positive%20clicks%2C%0Aindicating%20that%20the%20segmentation%20model%20should%20include%20a%20region%20around%20these%0Aclicks.%20Conversely%2C%20negative%20clicks%20are%20provided%20on%20pixels%20belonging%20to%20the%0Abackground%2C%20instructing%20the%20model%20to%20exclude%20the%20region%20near%20these%20clicks%20from%0Athe%20segmentation%20mask.%20Additionally%2C%20the%20segmentation%20mask%20is%20obtained%20by%0Adeforming%20a%20template%20mask%20with%20the%20same%20topology%20as%20the%20object%20of%20interest%0Ausing%20an%20orientation-preserving%20quasiconformal%20mapping.%20This%20approach%20helps%20to%0Aavoid%20topological%20errors%20in%20the%20segmentation%20results.%20We%20provide%20a%20thorough%0Aanalysis%20of%20the%20proposed%20model%2C%20including%20theoretical%20support%20for%20the%20ability%0Aof%20QIS%20to%20include%20or%20exclude%20regions%20of%20interest%20or%20disinterest%20based%20on%20the%0Auser%27s%20indication.%20To%20evaluate%20the%20performance%20of%20QIS%2C%20we%20conduct%20experiments%0Aon%20synthesized%20images%2C%20medical%20images%2C%20natural%20images%20and%20noisy%20natural%20images.%0AThe%20results%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQIS%2520%253A%2520Interactive%2520Segmentation%2520via%2520Quasi-Conformal%2520Mappings%26entry.906535625%3DHan%2520Zhang%2520and%2520Daoping%2520Zhang%2520and%2520Lok%2520Ming%2520Lui%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520plays%2520a%2520crucial%2520role%2520in%2520extracting%2520important%2520objects%2520of%250Ainterest%2520from%2520images%252C%2520enabling%2520various%2520applications.%2520While%2520existing%2520methods%250Ahave%2520shown%2520success%2520in%2520segmenting%2520clean%2520images%252C%2520they%2520often%2520struggle%2520to%2520produce%250Aaccurate%2520segmentation%2520results%2520when%2520dealing%2520with%2520degraded%2520images%252C%2520such%2520as%2520those%250Acontaining%2520noise%2520or%2520occlusions.%2520To%2520address%2520this%2520challenge%252C%2520interactive%250Asegmentation%2520has%2520emerged%2520as%2520a%2520promising%2520approach%252C%2520allowing%2520users%2520to%2520provide%250Ameaningful%2520input%2520to%2520guide%2520the%2520segmentation%2520process.%2520However%252C%2520an%2520important%250Aproblem%2520in%2520interactive%2520segmentation%2520lies%2520in%2520determining%2520how%2520to%2520incorporate%250Aminimal%2520yet%2520meaningful%2520user%2520guidance%2520into%2520the%2520segmentation%2520model.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520quasi-conformal%2520interactive%2520segmentation%2520%2528QIS%2529%2520model%252C%250Awhich%2520incorporates%2520user%2520input%2520in%2520the%2520form%2520of%2520positive%2520and%2520negative%2520clicks.%250AUsers%2520mark%2520a%2520few%2520pixels%2520belonging%2520to%2520the%2520object%2520region%2520as%2520positive%2520clicks%252C%250Aindicating%2520that%2520the%2520segmentation%2520model%2520should%2520include%2520a%2520region%2520around%2520these%250Aclicks.%2520Conversely%252C%2520negative%2520clicks%2520are%2520provided%2520on%2520pixels%2520belonging%2520to%2520the%250Abackground%252C%2520instructing%2520the%2520model%2520to%2520exclude%2520the%2520region%2520near%2520these%2520clicks%2520from%250Athe%2520segmentation%2520mask.%2520Additionally%252C%2520the%2520segmentation%2520mask%2520is%2520obtained%2520by%250Adeforming%2520a%2520template%2520mask%2520with%2520the%2520same%2520topology%2520as%2520the%2520object%2520of%2520interest%250Ausing%2520an%2520orientation-preserving%2520quasiconformal%2520mapping.%2520This%2520approach%2520helps%2520to%250Aavoid%2520topological%2520errors%2520in%2520the%2520segmentation%2520results.%2520We%2520provide%2520a%2520thorough%250Aanalysis%2520of%2520the%2520proposed%2520model%252C%2520including%2520theoretical%2520support%2520for%2520the%2520ability%250Aof%2520QIS%2520to%2520include%2520or%2520exclude%2520regions%2520of%2520interest%2520or%2520disinterest%2520based%2520on%2520the%250Auser%2527s%2520indication.%2520To%2520evaluate%2520the%2520performance%2520of%2520QIS%252C%2520we%2520conduct%2520experiments%250Aon%2520synthesized%2520images%252C%2520medical%2520images%252C%2520natural%2520images%2520and%2520noisy%2520natural%2520images.%250AThe%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QIS%20%3A%20Interactive%20Segmentation%20via%20Quasi-Conformal%20Mappings&entry.906535625=Han%20Zhang%20and%20Daoping%20Zhang%20and%20Lok%20Ming%20Lui&entry.1292438233=%20%20Image%20segmentation%20plays%20a%20crucial%20role%20in%20extracting%20important%20objects%20of%0Ainterest%20from%20images%2C%20enabling%20various%20applications.%20While%20existing%20methods%0Ahave%20shown%20success%20in%20segmenting%20clean%20images%2C%20they%20often%20struggle%20to%20produce%0Aaccurate%20segmentation%20results%20when%20dealing%20with%20degraded%20images%2C%20such%20as%20those%0Acontaining%20noise%20or%20occlusions.%20To%20address%20this%20challenge%2C%20interactive%0Asegmentation%20has%20emerged%20as%20a%20promising%20approach%2C%20allowing%20users%20to%20provide%0Ameaningful%20input%20to%20guide%20the%20segmentation%20process.%20However%2C%20an%20important%0Aproblem%20in%20interactive%20segmentation%20lies%20in%20determining%20how%20to%20incorporate%0Aminimal%20yet%20meaningful%20user%20guidance%20into%20the%20segmentation%20model.%20In%20this%0Apaper%2C%20we%20propose%20the%20quasi-conformal%20interactive%20segmentation%20%28QIS%29%20model%2C%0Awhich%20incorporates%20user%20input%20in%20the%20form%20of%20positive%20and%20negative%20clicks.%0AUsers%20mark%20a%20few%20pixels%20belonging%20to%20the%20object%20region%20as%20positive%20clicks%2C%0Aindicating%20that%20the%20segmentation%20model%20should%20include%20a%20region%20around%20these%0Aclicks.%20Conversely%2C%20negative%20clicks%20are%20provided%20on%20pixels%20belonging%20to%20the%0Abackground%2C%20instructing%20the%20model%20to%20exclude%20the%20region%20near%20these%20clicks%20from%0Athe%20segmentation%20mask.%20Additionally%2C%20the%20segmentation%20mask%20is%20obtained%20by%0Adeforming%20a%20template%20mask%20with%20the%20same%20topology%20as%20the%20object%20of%20interest%0Ausing%20an%20orientation-preserving%20quasiconformal%20mapping.%20This%20approach%20helps%20to%0Aavoid%20topological%20errors%20in%20the%20segmentation%20results.%20We%20provide%20a%20thorough%0Aanalysis%20of%20the%20proposed%20model%2C%20including%20theoretical%20support%20for%20the%20ability%0Aof%20QIS%20to%20include%20or%20exclude%20regions%20of%20interest%20or%20disinterest%20based%20on%20the%0Auser%27s%20indication.%20To%20evaluate%20the%20performance%20of%20QIS%2C%20we%20conduct%20experiments%0Aon%20synthesized%20images%2C%20medical%20images%2C%20natural%20images%20and%20noisy%20natural%20images.%0AThe%20results%20demonstrate%20the%20efficacy%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14695v2&entry.124074799=Read"},
{"title": "Adaptive Sparse Allocation with Mutual Choice & Feature Choice Sparse\n  Autoencoders", "author": "Kola Ayonrinde", "abstract": "  Sparse autoencoders (SAEs) are a promising approach to extracting features\nfrom neural networks, enabling model interpretability as well as causal\ninterventions on model internals. SAEs generate sparse feature representations\nusing a sparsifying activation function that implicitly defines a set of\ntoken-feature matches. We frame the token-feature matching as a resource\nallocation problem constrained by a total sparsity upper bound. For example,\nTopK SAEs solve this allocation problem with the additional constraint that\neach token matches with at most $k$ features. In TopK SAEs, the $k$ active\nfeatures per token constraint is the same across tokens, despite some tokens\nbeing more difficult to reconstruct than others. To address this limitation, we\npropose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs,\nwhich each allow for a variable number of active features per token. Feature\nChoice SAEs solve the sparsity allocation problem under the additional\nconstraint that each feature matches with at most $m$ tokens. Mutual Choice\nSAEs solve the unrestricted allocation problem where the total sparsity budget\ncan be allocated freely between tokens and features. Additionally, we introduce\na new auxiliary loss function, $\\mathtt{aux\\_zipf\\_loss}$, which generalises\nthe $\\mathtt{aux\\_k\\_loss}$ to mitigate dead and underutilised features. Our\nmethods result in SAEs with fewer dead features and improved reconstruction\nloss at equivalent sparsity levels as a result of the inherent adaptive\ncomputation. More accurate and scalable feature extraction methods provide a\npath towards better understanding and more precise control of foundation\nmodels.\n", "link": "http://arxiv.org/abs/2411.02124v1", "date": "2024-11-04", "relevancy": 2.4955, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5429}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Sparse%20Allocation%20with%20Mutual%20Choice%20%26%20Feature%20Choice%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Adaptive%20Sparse%20Allocation%20with%20Mutual%20Choice%20%26%20Feature%20Choice%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Kola%20Ayonrinde%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20to%20extracting%20features%0Afrom%20neural%20networks%2C%20enabling%20model%20interpretability%20as%20well%20as%20causal%0Ainterventions%20on%20model%20internals.%20SAEs%20generate%20sparse%20feature%20representations%0Ausing%20a%20sparsifying%20activation%20function%20that%20implicitly%20defines%20a%20set%20of%0Atoken-feature%20matches.%20We%20frame%20the%20token-feature%20matching%20as%20a%20resource%0Aallocation%20problem%20constrained%20by%20a%20total%20sparsity%20upper%20bound.%20For%20example%2C%0ATopK%20SAEs%20solve%20this%20allocation%20problem%20with%20the%20additional%20constraint%20that%0Aeach%20token%20matches%20with%20at%20most%20%24k%24%20features.%20In%20TopK%20SAEs%2C%20the%20%24k%24%20active%0Afeatures%20per%20token%20constraint%20is%20the%20same%20across%20tokens%2C%20despite%20some%20tokens%0Abeing%20more%20difficult%20to%20reconstruct%20than%20others.%20To%20address%20this%20limitation%2C%20we%0Apropose%20two%20novel%20SAE%20variants%2C%20Feature%20Choice%20SAEs%20and%20Mutual%20Choice%20SAEs%2C%0Awhich%20each%20allow%20for%20a%20variable%20number%20of%20active%20features%20per%20token.%20Feature%0AChoice%20SAEs%20solve%20the%20sparsity%20allocation%20problem%20under%20the%20additional%0Aconstraint%20that%20each%20feature%20matches%20with%20at%20most%20%24m%24%20tokens.%20Mutual%20Choice%0ASAEs%20solve%20the%20unrestricted%20allocation%20problem%20where%20the%20total%20sparsity%20budget%0Acan%20be%20allocated%20freely%20between%20tokens%20and%20features.%20Additionally%2C%20we%20introduce%0Aa%20new%20auxiliary%20loss%20function%2C%20%24%5Cmathtt%7Baux%5C_zipf%5C_loss%7D%24%2C%20which%20generalises%0Athe%20%24%5Cmathtt%7Baux%5C_k%5C_loss%7D%24%20to%20mitigate%20dead%20and%20underutilised%20features.%20Our%0Amethods%20result%20in%20SAEs%20with%20fewer%20dead%20features%20and%20improved%20reconstruction%0Aloss%20at%20equivalent%20sparsity%20levels%20as%20a%20result%20of%20the%20inherent%20adaptive%0Acomputation.%20More%20accurate%20and%20scalable%20feature%20extraction%20methods%20provide%20a%0Apath%20towards%20better%20understanding%20and%20more%20precise%20control%20of%20foundation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Sparse%2520Allocation%2520with%2520Mutual%2520Choice%2520%2526%2520Feature%2520Choice%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DKola%2520Ayonrinde%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520approach%2520to%2520extracting%2520features%250Afrom%2520neural%2520networks%252C%2520enabling%2520model%2520interpretability%2520as%2520well%2520as%2520causal%250Ainterventions%2520on%2520model%2520internals.%2520SAEs%2520generate%2520sparse%2520feature%2520representations%250Ausing%2520a%2520sparsifying%2520activation%2520function%2520that%2520implicitly%2520defines%2520a%2520set%2520of%250Atoken-feature%2520matches.%2520We%2520frame%2520the%2520token-feature%2520matching%2520as%2520a%2520resource%250Aallocation%2520problem%2520constrained%2520by%2520a%2520total%2520sparsity%2520upper%2520bound.%2520For%2520example%252C%250ATopK%2520SAEs%2520solve%2520this%2520allocation%2520problem%2520with%2520the%2520additional%2520constraint%2520that%250Aeach%2520token%2520matches%2520with%2520at%2520most%2520%2524k%2524%2520features.%2520In%2520TopK%2520SAEs%252C%2520the%2520%2524k%2524%2520active%250Afeatures%2520per%2520token%2520constraint%2520is%2520the%2520same%2520across%2520tokens%252C%2520despite%2520some%2520tokens%250Abeing%2520more%2520difficult%2520to%2520reconstruct%2520than%2520others.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520two%2520novel%2520SAE%2520variants%252C%2520Feature%2520Choice%2520SAEs%2520and%2520Mutual%2520Choice%2520SAEs%252C%250Awhich%2520each%2520allow%2520for%2520a%2520variable%2520number%2520of%2520active%2520features%2520per%2520token.%2520Feature%250AChoice%2520SAEs%2520solve%2520the%2520sparsity%2520allocation%2520problem%2520under%2520the%2520additional%250Aconstraint%2520that%2520each%2520feature%2520matches%2520with%2520at%2520most%2520%2524m%2524%2520tokens.%2520Mutual%2520Choice%250ASAEs%2520solve%2520the%2520unrestricted%2520allocation%2520problem%2520where%2520the%2520total%2520sparsity%2520budget%250Acan%2520be%2520allocated%2520freely%2520between%2520tokens%2520and%2520features.%2520Additionally%252C%2520we%2520introduce%250Aa%2520new%2520auxiliary%2520loss%2520function%252C%2520%2524%255Cmathtt%257Baux%255C_zipf%255C_loss%257D%2524%252C%2520which%2520generalises%250Athe%2520%2524%255Cmathtt%257Baux%255C_k%255C_loss%257D%2524%2520to%2520mitigate%2520dead%2520and%2520underutilised%2520features.%2520Our%250Amethods%2520result%2520in%2520SAEs%2520with%2520fewer%2520dead%2520features%2520and%2520improved%2520reconstruction%250Aloss%2520at%2520equivalent%2520sparsity%2520levels%2520as%2520a%2520result%2520of%2520the%2520inherent%2520adaptive%250Acomputation.%2520More%2520accurate%2520and%2520scalable%2520feature%2520extraction%2520methods%2520provide%2520a%250Apath%2520towards%2520better%2520understanding%2520and%2520more%2520precise%2520control%2520of%2520foundation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Sparse%20Allocation%20with%20Mutual%20Choice%20%26%20Feature%20Choice%20Sparse%0A%20%20Autoencoders&entry.906535625=Kola%20Ayonrinde&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20approach%20to%20extracting%20features%0Afrom%20neural%20networks%2C%20enabling%20model%20interpretability%20as%20well%20as%20causal%0Ainterventions%20on%20model%20internals.%20SAEs%20generate%20sparse%20feature%20representations%0Ausing%20a%20sparsifying%20activation%20function%20that%20implicitly%20defines%20a%20set%20of%0Atoken-feature%20matches.%20We%20frame%20the%20token-feature%20matching%20as%20a%20resource%0Aallocation%20problem%20constrained%20by%20a%20total%20sparsity%20upper%20bound.%20For%20example%2C%0ATopK%20SAEs%20solve%20this%20allocation%20problem%20with%20the%20additional%20constraint%20that%0Aeach%20token%20matches%20with%20at%20most%20%24k%24%20features.%20In%20TopK%20SAEs%2C%20the%20%24k%24%20active%0Afeatures%20per%20token%20constraint%20is%20the%20same%20across%20tokens%2C%20despite%20some%20tokens%0Abeing%20more%20difficult%20to%20reconstruct%20than%20others.%20To%20address%20this%20limitation%2C%20we%0Apropose%20two%20novel%20SAE%20variants%2C%20Feature%20Choice%20SAEs%20and%20Mutual%20Choice%20SAEs%2C%0Awhich%20each%20allow%20for%20a%20variable%20number%20of%20active%20features%20per%20token.%20Feature%0AChoice%20SAEs%20solve%20the%20sparsity%20allocation%20problem%20under%20the%20additional%0Aconstraint%20that%20each%20feature%20matches%20with%20at%20most%20%24m%24%20tokens.%20Mutual%20Choice%0ASAEs%20solve%20the%20unrestricted%20allocation%20problem%20where%20the%20total%20sparsity%20budget%0Acan%20be%20allocated%20freely%20between%20tokens%20and%20features.%20Additionally%2C%20we%20introduce%0Aa%20new%20auxiliary%20loss%20function%2C%20%24%5Cmathtt%7Baux%5C_zipf%5C_loss%7D%24%2C%20which%20generalises%0Athe%20%24%5Cmathtt%7Baux%5C_k%5C_loss%7D%24%20to%20mitigate%20dead%20and%20underutilised%20features.%20Our%0Amethods%20result%20in%20SAEs%20with%20fewer%20dead%20features%20and%20improved%20reconstruction%0Aloss%20at%20equivalent%20sparsity%20levels%20as%20a%20result%20of%20the%20inherent%20adaptive%0Acomputation.%20More%20accurate%20and%20scalable%20feature%20extraction%20methods%20provide%20a%0Apath%20towards%20better%20understanding%20and%20more%20precise%20control%20of%20foundation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02124v1&entry.124074799=Read"},
{"title": "PointNCBW: Towards Dataset Ownership Verification for Point Clouds via\n  Negative Clean-label Backdoor Watermark", "author": "Cheng Wei and Yang Wang and Kuofeng Gao and Shuo Shao and Yiming Li and Zhibo Wang and Zhan Qin", "abstract": "  Recently, point clouds have been widely used in computer vision, whereas\ntheir collection is time-consuming and expensive. As such, point cloud datasets\nare the valuable intellectual property of their owners and deserve protection.\nTo detect and prevent unauthorized use of these datasets, especially for\ncommercial or open-sourced ones that cannot be sold again or used commercially\nwithout permission, we intend to identify whether a suspicious third-party\nmodel is trained on our protected dataset under the black-box setting. We\nachieve this goal by designing a scalable clean-label backdoor-based dataset\nwatermark for point clouds that ensures both effectiveness and stealthiness.\nUnlike existing clean-label watermark schemes, which are susceptible to the\nnumber of categories, our method could watermark samples from all classes\ninstead of only from the target one. Accordingly, it can still preserve high\neffectiveness even on large-scale datasets with many classes. Specifically, we\nperturb selected point clouds with non-target categories in both shape-wise and\npoint-wise manners before inserting trigger patterns without changing their\nlabels. The features of perturbed samples are similar to those of benign\nsamples from the target class. As such, models trained on the watermarked\ndataset will have a distinctive yet stealthy backdoor behavior, i.e.,\nmisclassifying samples from the target class whenever triggers appear, since\nthe trained DNNs will treat the inserted trigger pattern as a signal to deny\npredicting the target label. We also design a hypothesis-test-guided dataset\nownership verification based on the proposed watermark. Extensive experiments\non benchmark datasets are conducted, verifying the effectiveness of our method\nand its resistance to potential removal methods.\n", "link": "http://arxiv.org/abs/2408.05500v2", "date": "2024-11-04", "relevancy": 2.4832, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointNCBW%3A%20Towards%20Dataset%20Ownership%20Verification%20for%20Point%20Clouds%20via%0A%20%20Negative%20Clean-label%20Backdoor%20Watermark&body=Title%3A%20PointNCBW%3A%20Towards%20Dataset%20Ownership%20Verification%20for%20Point%20Clouds%20via%0A%20%20Negative%20Clean-label%20Backdoor%20Watermark%0AAuthor%3A%20Cheng%20Wei%20and%20Yang%20Wang%20and%20Kuofeng%20Gao%20and%20Shuo%20Shao%20and%20Yiming%20Li%20and%20Zhibo%20Wang%20and%20Zhan%20Qin%0AAbstract%3A%20%20%20Recently%2C%20point%20clouds%20have%20been%20widely%20used%20in%20computer%20vision%2C%20whereas%0Atheir%20collection%20is%20time-consuming%20and%20expensive.%20As%20such%2C%20point%20cloud%20datasets%0Aare%20the%20valuable%20intellectual%20property%20of%20their%20owners%20and%20deserve%20protection.%0ATo%20detect%20and%20prevent%20unauthorized%20use%20of%20these%20datasets%2C%20especially%20for%0Acommercial%20or%20open-sourced%20ones%20that%20cannot%20be%20sold%20again%20or%20used%20commercially%0Awithout%20permission%2C%20we%20intend%20to%20identify%20whether%20a%20suspicious%20third-party%0Amodel%20is%20trained%20on%20our%20protected%20dataset%20under%20the%20black-box%20setting.%20We%0Aachieve%20this%20goal%20by%20designing%20a%20scalable%20clean-label%20backdoor-based%20dataset%0Awatermark%20for%20point%20clouds%20that%20ensures%20both%20effectiveness%20and%20stealthiness.%0AUnlike%20existing%20clean-label%20watermark%20schemes%2C%20which%20are%20susceptible%20to%20the%0Anumber%20of%20categories%2C%20our%20method%20could%20watermark%20samples%20from%20all%20classes%0Ainstead%20of%20only%20from%20the%20target%20one.%20Accordingly%2C%20it%20can%20still%20preserve%20high%0Aeffectiveness%20even%20on%20large-scale%20datasets%20with%20many%20classes.%20Specifically%2C%20we%0Aperturb%20selected%20point%20clouds%20with%20non-target%20categories%20in%20both%20shape-wise%20and%0Apoint-wise%20manners%20before%20inserting%20trigger%20patterns%20without%20changing%20their%0Alabels.%20The%20features%20of%20perturbed%20samples%20are%20similar%20to%20those%20of%20benign%0Asamples%20from%20the%20target%20class.%20As%20such%2C%20models%20trained%20on%20the%20watermarked%0Adataset%20will%20have%20a%20distinctive%20yet%20stealthy%20backdoor%20behavior%2C%20i.e.%2C%0Amisclassifying%20samples%20from%20the%20target%20class%20whenever%20triggers%20appear%2C%20since%0Athe%20trained%20DNNs%20will%20treat%20the%20inserted%20trigger%20pattern%20as%20a%20signal%20to%20deny%0Apredicting%20the%20target%20label.%20We%20also%20design%20a%20hypothesis-test-guided%20dataset%0Aownership%20verification%20based%20on%20the%20proposed%20watermark.%20Extensive%20experiments%0Aon%20benchmark%20datasets%20are%20conducted%2C%20verifying%20the%20effectiveness%20of%20our%20method%0Aand%20its%20resistance%20to%20potential%20removal%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointNCBW%253A%2520Towards%2520Dataset%2520Ownership%2520Verification%2520for%2520Point%2520Clouds%2520via%250A%2520%2520Negative%2520Clean-label%2520Backdoor%2520Watermark%26entry.906535625%3DCheng%2520Wei%2520and%2520Yang%2520Wang%2520and%2520Kuofeng%2520Gao%2520and%2520Shuo%2520Shao%2520and%2520Yiming%2520Li%2520and%2520Zhibo%2520Wang%2520and%2520Zhan%2520Qin%26entry.1292438233%3D%2520%2520Recently%252C%2520point%2520clouds%2520have%2520been%2520widely%2520used%2520in%2520computer%2520vision%252C%2520whereas%250Atheir%2520collection%2520is%2520time-consuming%2520and%2520expensive.%2520As%2520such%252C%2520point%2520cloud%2520datasets%250Aare%2520the%2520valuable%2520intellectual%2520property%2520of%2520their%2520owners%2520and%2520deserve%2520protection.%250ATo%2520detect%2520and%2520prevent%2520unauthorized%2520use%2520of%2520these%2520datasets%252C%2520especially%2520for%250Acommercial%2520or%2520open-sourced%2520ones%2520that%2520cannot%2520be%2520sold%2520again%2520or%2520used%2520commercially%250Awithout%2520permission%252C%2520we%2520intend%2520to%2520identify%2520whether%2520a%2520suspicious%2520third-party%250Amodel%2520is%2520trained%2520on%2520our%2520protected%2520dataset%2520under%2520the%2520black-box%2520setting.%2520We%250Aachieve%2520this%2520goal%2520by%2520designing%2520a%2520scalable%2520clean-label%2520backdoor-based%2520dataset%250Awatermark%2520for%2520point%2520clouds%2520that%2520ensures%2520both%2520effectiveness%2520and%2520stealthiness.%250AUnlike%2520existing%2520clean-label%2520watermark%2520schemes%252C%2520which%2520are%2520susceptible%2520to%2520the%250Anumber%2520of%2520categories%252C%2520our%2520method%2520could%2520watermark%2520samples%2520from%2520all%2520classes%250Ainstead%2520of%2520only%2520from%2520the%2520target%2520one.%2520Accordingly%252C%2520it%2520can%2520still%2520preserve%2520high%250Aeffectiveness%2520even%2520on%2520large-scale%2520datasets%2520with%2520many%2520classes.%2520Specifically%252C%2520we%250Aperturb%2520selected%2520point%2520clouds%2520with%2520non-target%2520categories%2520in%2520both%2520shape-wise%2520and%250Apoint-wise%2520manners%2520before%2520inserting%2520trigger%2520patterns%2520without%2520changing%2520their%250Alabels.%2520The%2520features%2520of%2520perturbed%2520samples%2520are%2520similar%2520to%2520those%2520of%2520benign%250Asamples%2520from%2520the%2520target%2520class.%2520As%2520such%252C%2520models%2520trained%2520on%2520the%2520watermarked%250Adataset%2520will%2520have%2520a%2520distinctive%2520yet%2520stealthy%2520backdoor%2520behavior%252C%2520i.e.%252C%250Amisclassifying%2520samples%2520from%2520the%2520target%2520class%2520whenever%2520triggers%2520appear%252C%2520since%250Athe%2520trained%2520DNNs%2520will%2520treat%2520the%2520inserted%2520trigger%2520pattern%2520as%2520a%2520signal%2520to%2520deny%250Apredicting%2520the%2520target%2520label.%2520We%2520also%2520design%2520a%2520hypothesis-test-guided%2520dataset%250Aownership%2520verification%2520based%2520on%2520the%2520proposed%2520watermark.%2520Extensive%2520experiments%250Aon%2520benchmark%2520datasets%2520are%2520conducted%252C%2520verifying%2520the%2520effectiveness%2520of%2520our%2520method%250Aand%2520its%2520resistance%2520to%2520potential%2520removal%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointNCBW%3A%20Towards%20Dataset%20Ownership%20Verification%20for%20Point%20Clouds%20via%0A%20%20Negative%20Clean-label%20Backdoor%20Watermark&entry.906535625=Cheng%20Wei%20and%20Yang%20Wang%20and%20Kuofeng%20Gao%20and%20Shuo%20Shao%20and%20Yiming%20Li%20and%20Zhibo%20Wang%20and%20Zhan%20Qin&entry.1292438233=%20%20Recently%2C%20point%20clouds%20have%20been%20widely%20used%20in%20computer%20vision%2C%20whereas%0Atheir%20collection%20is%20time-consuming%20and%20expensive.%20As%20such%2C%20point%20cloud%20datasets%0Aare%20the%20valuable%20intellectual%20property%20of%20their%20owners%20and%20deserve%20protection.%0ATo%20detect%20and%20prevent%20unauthorized%20use%20of%20these%20datasets%2C%20especially%20for%0Acommercial%20or%20open-sourced%20ones%20that%20cannot%20be%20sold%20again%20or%20used%20commercially%0Awithout%20permission%2C%20we%20intend%20to%20identify%20whether%20a%20suspicious%20third-party%0Amodel%20is%20trained%20on%20our%20protected%20dataset%20under%20the%20black-box%20setting.%20We%0Aachieve%20this%20goal%20by%20designing%20a%20scalable%20clean-label%20backdoor-based%20dataset%0Awatermark%20for%20point%20clouds%20that%20ensures%20both%20effectiveness%20and%20stealthiness.%0AUnlike%20existing%20clean-label%20watermark%20schemes%2C%20which%20are%20susceptible%20to%20the%0Anumber%20of%20categories%2C%20our%20method%20could%20watermark%20samples%20from%20all%20classes%0Ainstead%20of%20only%20from%20the%20target%20one.%20Accordingly%2C%20it%20can%20still%20preserve%20high%0Aeffectiveness%20even%20on%20large-scale%20datasets%20with%20many%20classes.%20Specifically%2C%20we%0Aperturb%20selected%20point%20clouds%20with%20non-target%20categories%20in%20both%20shape-wise%20and%0Apoint-wise%20manners%20before%20inserting%20trigger%20patterns%20without%20changing%20their%0Alabels.%20The%20features%20of%20perturbed%20samples%20are%20similar%20to%20those%20of%20benign%0Asamples%20from%20the%20target%20class.%20As%20such%2C%20models%20trained%20on%20the%20watermarked%0Adataset%20will%20have%20a%20distinctive%20yet%20stealthy%20backdoor%20behavior%2C%20i.e.%2C%0Amisclassifying%20samples%20from%20the%20target%20class%20whenever%20triggers%20appear%2C%20since%0Athe%20trained%20DNNs%20will%20treat%20the%20inserted%20trigger%20pattern%20as%20a%20signal%20to%20deny%0Apredicting%20the%20target%20label.%20We%20also%20design%20a%20hypothesis-test-guided%20dataset%0Aownership%20verification%20based%20on%20the%20proposed%20watermark.%20Extensive%20experiments%0Aon%20benchmark%20datasets%20are%20conducted%2C%20verifying%20the%20effectiveness%20of%20our%20method%0Aand%20its%20resistance%20to%20potential%20removal%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05500v2&entry.124074799=Read"},
{"title": "Can Large Language Models generalize analogy solving like people can?", "author": "Claire E. Stevenson and Alexandra Pafford and Han L. J. van der Maas and Melanie Mitchell", "abstract": "  When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.\n", "link": "http://arxiv.org/abs/2411.02348v1", "date": "2024-11-04", "relevancy": 2.4777, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20generalize%20analogy%20solving%20like%20people%20can%3F&body=Title%3A%20Can%20Large%20Language%20Models%20generalize%20analogy%20solving%20like%20people%20can%3F%0AAuthor%3A%20Claire%20E.%20Stevenson%20and%20Alexandra%20Pafford%20and%20Han%20L.%20J.%20van%20der%20Maas%20and%20Melanie%20Mitchell%0AAbstract%3A%20%20%20When%20we%20solve%20an%20analogy%20we%20transfer%20information%20from%20a%20known%20context%20to%20a%0Anew%20one%20through%20abstract%20rules%20and%20relational%20similarity.%20In%20people%2C%20the%0Aability%20to%20solve%20analogies%20such%20as%20%22body%20%3A%20feet%20%3A%3A%20table%20%3A%20%3F%22%20emerges%20in%0Achildhood%2C%20and%20appears%20to%20transfer%20easily%20to%20other%20domains%2C%20such%20as%20the%20visual%0Adomain%20%22%28%20%3A%20%29%20%3A%3A%20%3C%20%3A%20%3F%22.%20Recent%20research%20shows%20that%20large%20language%20models%0A%28LLMs%29%20can%20solve%20various%20forms%20of%20analogies.%20However%2C%20can%20LLMs%20generalize%0Aanalogy%20solving%20to%20new%20domains%20like%20people%20can%3F%20To%20investigate%20this%2C%20we%20had%0Achildren%2C%20adults%2C%20and%20LLMs%20solve%20a%20series%20of%20letter-string%20analogies%20%28e.g.%2C%20a%20b%0A%3A%20a%20c%20%3A%3A%20j%20k%20%3A%20%3F%29%20in%20the%20Latin%20alphabet%2C%20in%20a%20near%20transfer%20domain%20%28Greek%0Aalphabet%29%2C%20and%20a%20far%20transfer%20domain%20%28list%20of%20symbols%29.%20As%20expected%2C%20children%0Aand%20adults%20easily%20generalized%20their%20knowledge%20to%20unfamiliar%20domains%2C%20whereas%0ALLMs%20did%20not.%20This%20key%20difference%20between%20human%20and%20AI%20performance%20is%20evidence%0Athat%20these%20LLMs%20still%20struggle%20with%20robust%20human-like%20analogical%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520generalize%2520analogy%2520solving%2520like%2520people%2520can%253F%26entry.906535625%3DClaire%2520E.%2520Stevenson%2520and%2520Alexandra%2520Pafford%2520and%2520Han%2520L.%2520J.%2520van%2520der%2520Maas%2520and%2520Melanie%2520Mitchell%26entry.1292438233%3D%2520%2520When%2520we%2520solve%2520an%2520analogy%2520we%2520transfer%2520information%2520from%2520a%2520known%2520context%2520to%2520a%250Anew%2520one%2520through%2520abstract%2520rules%2520and%2520relational%2520similarity.%2520In%2520people%252C%2520the%250Aability%2520to%2520solve%2520analogies%2520such%2520as%2520%2522body%2520%253A%2520feet%2520%253A%253A%2520table%2520%253A%2520%253F%2522%2520emerges%2520in%250Achildhood%252C%2520and%2520appears%2520to%2520transfer%2520easily%2520to%2520other%2520domains%252C%2520such%2520as%2520the%2520visual%250Adomain%2520%2522%2528%2520%253A%2520%2529%2520%253A%253A%2520%253C%2520%253A%2520%253F%2522.%2520Recent%2520research%2520shows%2520that%2520large%2520language%2520models%250A%2528LLMs%2529%2520can%2520solve%2520various%2520forms%2520of%2520analogies.%2520However%252C%2520can%2520LLMs%2520generalize%250Aanalogy%2520solving%2520to%2520new%2520domains%2520like%2520people%2520can%253F%2520To%2520investigate%2520this%252C%2520we%2520had%250Achildren%252C%2520adults%252C%2520and%2520LLMs%2520solve%2520a%2520series%2520of%2520letter-string%2520analogies%2520%2528e.g.%252C%2520a%2520b%250A%253A%2520a%2520c%2520%253A%253A%2520j%2520k%2520%253A%2520%253F%2529%2520in%2520the%2520Latin%2520alphabet%252C%2520in%2520a%2520near%2520transfer%2520domain%2520%2528Greek%250Aalphabet%2529%252C%2520and%2520a%2520far%2520transfer%2520domain%2520%2528list%2520of%2520symbols%2529.%2520As%2520expected%252C%2520children%250Aand%2520adults%2520easily%2520generalized%2520their%2520knowledge%2520to%2520unfamiliar%2520domains%252C%2520whereas%250ALLMs%2520did%2520not.%2520This%2520key%2520difference%2520between%2520human%2520and%2520AI%2520performance%2520is%2520evidence%250Athat%2520these%2520LLMs%2520still%2520struggle%2520with%2520robust%2520human-like%2520analogical%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20generalize%20analogy%20solving%20like%20people%20can%3F&entry.906535625=Claire%20E.%20Stevenson%20and%20Alexandra%20Pafford%20and%20Han%20L.%20J.%20van%20der%20Maas%20and%20Melanie%20Mitchell&entry.1292438233=%20%20When%20we%20solve%20an%20analogy%20we%20transfer%20information%20from%20a%20known%20context%20to%20a%0Anew%20one%20through%20abstract%20rules%20and%20relational%20similarity.%20In%20people%2C%20the%0Aability%20to%20solve%20analogies%20such%20as%20%22body%20%3A%20feet%20%3A%3A%20table%20%3A%20%3F%22%20emerges%20in%0Achildhood%2C%20and%20appears%20to%20transfer%20easily%20to%20other%20domains%2C%20such%20as%20the%20visual%0Adomain%20%22%28%20%3A%20%29%20%3A%3A%20%3C%20%3A%20%3F%22.%20Recent%20research%20shows%20that%20large%20language%20models%0A%28LLMs%29%20can%20solve%20various%20forms%20of%20analogies.%20However%2C%20can%20LLMs%20generalize%0Aanalogy%20solving%20to%20new%20domains%20like%20people%20can%3F%20To%20investigate%20this%2C%20we%20had%0Achildren%2C%20adults%2C%20and%20LLMs%20solve%20a%20series%20of%20letter-string%20analogies%20%28e.g.%2C%20a%20b%0A%3A%20a%20c%20%3A%3A%20j%20k%20%3A%20%3F%29%20in%20the%20Latin%20alphabet%2C%20in%20a%20near%20transfer%20domain%20%28Greek%0Aalphabet%29%2C%20and%20a%20far%20transfer%20domain%20%28list%20of%20symbols%29.%20As%20expected%2C%20children%0Aand%20adults%20easily%20generalized%20their%20knowledge%20to%20unfamiliar%20domains%2C%20whereas%0ALLMs%20did%20not.%20This%20key%20difference%20between%20human%20and%20AI%20performance%20is%20evidence%0Athat%20these%20LLMs%20still%20struggle%20with%20robust%20human-like%20analogical%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02348v1&entry.124074799=Read"},
{"title": "QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition", "author": "Chengpeng Wang and Li Chen and Lili Wang and Zhaofan Li and Xuebin Lv", "abstract": "  On facial expression datasets with complex and numerous feature types, where\nthe significance and dominance of labeled features are difficult to predict,\nfacial expression recognition(FER) encounters the challenges of inter-class\nsimilarity and intra-class variances, making it difficult to mine effective\nfeatures. We aim to solely leverage the feature similarity among facial samples\nto address this. We introduce the Cross Similarity Attention (CSA), an\ninput-output position-sensitive attention mechanism that harnesses feature\nsimilarity across different images to compute the corresponding global spatial\nattention. Based on this, we propose a four-branch circular framework, called\nQuadruplet Cross Similarity (QCS), to extract discriminative features from the\nsame class and eliminate redundant ones from different classes synchronously to\nrefine cleaner features. The symmetry of the network ensures balanced and\nstable training and reduces the amount of CSA interaction matrix. Contrastive\nresidual distillation is utilized to transfer the information learned in the\ncross module back to the base network. The cross-attention module exists during\ntraining, and only one base branch is retained during inference. our proposed\nQCS model outperforms state-of-the-art methods on several popular FER datasets,\nwithout requiring additional landmark information or other extra training data.\nThe code is available at https://github.com/birdwcp/QCS.\n", "link": "http://arxiv.org/abs/2411.01988v1", "date": "2024-11-04", "relevancy": 2.4714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QCS%3AFeature%20Refining%20from%20Quadruplet%20Cross%20Similarity%20for%20Facial%0A%20%20Expression%20Recognition&body=Title%3A%20QCS%3AFeature%20Refining%20from%20Quadruplet%20Cross%20Similarity%20for%20Facial%0A%20%20Expression%20Recognition%0AAuthor%3A%20Chengpeng%20Wang%20and%20Li%20Chen%20and%20Lili%20Wang%20and%20Zhaofan%20Li%20and%20Xuebin%20Lv%0AAbstract%3A%20%20%20On%20facial%20expression%20datasets%20with%20complex%20and%20numerous%20feature%20types%2C%20where%0Athe%20significance%20and%20dominance%20of%20labeled%20features%20are%20difficult%20to%20predict%2C%0Afacial%20expression%20recognition%28FER%29%20encounters%20the%20challenges%20of%20inter-class%0Asimilarity%20and%20intra-class%20variances%2C%20making%20it%20difficult%20to%20mine%20effective%0Afeatures.%20We%20aim%20to%20solely%20leverage%20the%20feature%20similarity%20among%20facial%20samples%0Ato%20address%20this.%20We%20introduce%20the%20Cross%20Similarity%20Attention%20%28CSA%29%2C%20an%0Ainput-output%20position-sensitive%20attention%20mechanism%20that%20harnesses%20feature%0Asimilarity%20across%20different%20images%20to%20compute%20the%20corresponding%20global%20spatial%0Aattention.%20Based%20on%20this%2C%20we%20propose%20a%20four-branch%20circular%20framework%2C%20called%0AQuadruplet%20Cross%20Similarity%20%28QCS%29%2C%20to%20extract%20discriminative%20features%20from%20the%0Asame%20class%20and%20eliminate%20redundant%20ones%20from%20different%20classes%20synchronously%20to%0Arefine%20cleaner%20features.%20The%20symmetry%20of%20the%20network%20ensures%20balanced%20and%0Astable%20training%20and%20reduces%20the%20amount%20of%20CSA%20interaction%20matrix.%20Contrastive%0Aresidual%20distillation%20is%20utilized%20to%20transfer%20the%20information%20learned%20in%20the%0Across%20module%20back%20to%20the%20base%20network.%20The%20cross-attention%20module%20exists%20during%0Atraining%2C%20and%20only%20one%20base%20branch%20is%20retained%20during%20inference.%20our%20proposed%0AQCS%20model%20outperforms%20state-of-the-art%20methods%20on%20several%20popular%20FER%20datasets%2C%0Awithout%20requiring%20additional%20landmark%20information%20or%20other%20extra%20training%20data.%0AThe%20code%20is%20available%20at%20https%3A//github.com/birdwcp/QCS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQCS%253AFeature%2520Refining%2520from%2520Quadruplet%2520Cross%2520Similarity%2520for%2520Facial%250A%2520%2520Expression%2520Recognition%26entry.906535625%3DChengpeng%2520Wang%2520and%2520Li%2520Chen%2520and%2520Lili%2520Wang%2520and%2520Zhaofan%2520Li%2520and%2520Xuebin%2520Lv%26entry.1292438233%3D%2520%2520On%2520facial%2520expression%2520datasets%2520with%2520complex%2520and%2520numerous%2520feature%2520types%252C%2520where%250Athe%2520significance%2520and%2520dominance%2520of%2520labeled%2520features%2520are%2520difficult%2520to%2520predict%252C%250Afacial%2520expression%2520recognition%2528FER%2529%2520encounters%2520the%2520challenges%2520of%2520inter-class%250Asimilarity%2520and%2520intra-class%2520variances%252C%2520making%2520it%2520difficult%2520to%2520mine%2520effective%250Afeatures.%2520We%2520aim%2520to%2520solely%2520leverage%2520the%2520feature%2520similarity%2520among%2520facial%2520samples%250Ato%2520address%2520this.%2520We%2520introduce%2520the%2520Cross%2520Similarity%2520Attention%2520%2528CSA%2529%252C%2520an%250Ainput-output%2520position-sensitive%2520attention%2520mechanism%2520that%2520harnesses%2520feature%250Asimilarity%2520across%2520different%2520images%2520to%2520compute%2520the%2520corresponding%2520global%2520spatial%250Aattention.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520four-branch%2520circular%2520framework%252C%2520called%250AQuadruplet%2520Cross%2520Similarity%2520%2528QCS%2529%252C%2520to%2520extract%2520discriminative%2520features%2520from%2520the%250Asame%2520class%2520and%2520eliminate%2520redundant%2520ones%2520from%2520different%2520classes%2520synchronously%2520to%250Arefine%2520cleaner%2520features.%2520The%2520symmetry%2520of%2520the%2520network%2520ensures%2520balanced%2520and%250Astable%2520training%2520and%2520reduces%2520the%2520amount%2520of%2520CSA%2520interaction%2520matrix.%2520Contrastive%250Aresidual%2520distillation%2520is%2520utilized%2520to%2520transfer%2520the%2520information%2520learned%2520in%2520the%250Across%2520module%2520back%2520to%2520the%2520base%2520network.%2520The%2520cross-attention%2520module%2520exists%2520during%250Atraining%252C%2520and%2520only%2520one%2520base%2520branch%2520is%2520retained%2520during%2520inference.%2520our%2520proposed%250AQCS%2520model%2520outperforms%2520state-of-the-art%2520methods%2520on%2520several%2520popular%2520FER%2520datasets%252C%250Awithout%2520requiring%2520additional%2520landmark%2520information%2520or%2520other%2520extra%2520training%2520data.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/birdwcp/QCS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QCS%3AFeature%20Refining%20from%20Quadruplet%20Cross%20Similarity%20for%20Facial%0A%20%20Expression%20Recognition&entry.906535625=Chengpeng%20Wang%20and%20Li%20Chen%20and%20Lili%20Wang%20and%20Zhaofan%20Li%20and%20Xuebin%20Lv&entry.1292438233=%20%20On%20facial%20expression%20datasets%20with%20complex%20and%20numerous%20feature%20types%2C%20where%0Athe%20significance%20and%20dominance%20of%20labeled%20features%20are%20difficult%20to%20predict%2C%0Afacial%20expression%20recognition%28FER%29%20encounters%20the%20challenges%20of%20inter-class%0Asimilarity%20and%20intra-class%20variances%2C%20making%20it%20difficult%20to%20mine%20effective%0Afeatures.%20We%20aim%20to%20solely%20leverage%20the%20feature%20similarity%20among%20facial%20samples%0Ato%20address%20this.%20We%20introduce%20the%20Cross%20Similarity%20Attention%20%28CSA%29%2C%20an%0Ainput-output%20position-sensitive%20attention%20mechanism%20that%20harnesses%20feature%0Asimilarity%20across%20different%20images%20to%20compute%20the%20corresponding%20global%20spatial%0Aattention.%20Based%20on%20this%2C%20we%20propose%20a%20four-branch%20circular%20framework%2C%20called%0AQuadruplet%20Cross%20Similarity%20%28QCS%29%2C%20to%20extract%20discriminative%20features%20from%20the%0Asame%20class%20and%20eliminate%20redundant%20ones%20from%20different%20classes%20synchronously%20to%0Arefine%20cleaner%20features.%20The%20symmetry%20of%20the%20network%20ensures%20balanced%20and%0Astable%20training%20and%20reduces%20the%20amount%20of%20CSA%20interaction%20matrix.%20Contrastive%0Aresidual%20distillation%20is%20utilized%20to%20transfer%20the%20information%20learned%20in%20the%0Across%20module%20back%20to%20the%20base%20network.%20The%20cross-attention%20module%20exists%20during%0Atraining%2C%20and%20only%20one%20base%20branch%20is%20retained%20during%20inference.%20our%20proposed%0AQCS%20model%20outperforms%20state-of-the-art%20methods%20on%20several%20popular%20FER%20datasets%2C%0Awithout%20requiring%20additional%20landmark%20information%20or%20other%20extra%20training%20data.%0AThe%20code%20is%20available%20at%20https%3A//github.com/birdwcp/QCS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01988v1&entry.124074799=Read"},
{"title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration", "author": "Aofeng Su and Aowen Wang and Chao Ye and Chen Zhou and Ga Zhang and Guangcheng Zhu and Haobo Wang and Haokai Xu and Hao Chen and Haoze Li and Haoxuan Lan and Jiaming Tian and Jing Yuan and Junbo Zhao and Junlin Zhou and Kaizhe Shou and Liangyu Zha and Lin Long and Liyao Li and Pengzuo Wu and Qi Zhang and Qingyi Huang and Saisai Yang and Tao Zhang and Wentao Ye and Wufang Zhu and Xiaomeng Hu and Xijun Gu and Xinjie Sun and Xiang Li and Yuhang Yang and Zhiqing Xiao", "abstract": "  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.\n", "link": "http://arxiv.org/abs/2411.02059v1", "date": "2024-11-04", "relevancy": 2.4684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration&body=Title%3A%20TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration%0AAuthor%3A%20Aofeng%20Su%20and%20Aowen%20Wang%20and%20Chao%20Ye%20and%20Chen%20Zhou%20and%20Ga%20Zhang%20and%20Guangcheng%20Zhu%20and%20Haobo%20Wang%20and%20Haokai%20Xu%20and%20Hao%20Chen%20and%20Haoze%20Li%20and%20Haoxuan%20Lan%20and%20Jiaming%20Tian%20and%20Jing%20Yuan%20and%20Junbo%20Zhao%20and%20Junlin%20Zhou%20and%20Kaizhe%20Shou%20and%20Liangyu%20Zha%20and%20Lin%20Long%20and%20Liyao%20Li%20and%20Pengzuo%20Wu%20and%20Qi%20Zhang%20and%20Qingyi%20Huang%20and%20Saisai%20Yang%20and%20Tao%20Zhang%20and%20Wentao%20Ye%20and%20Wufang%20Zhu%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Xinjie%20Sun%20and%20Xiang%20Li%20and%20Yuhang%20Yang%20and%20Zhiqing%20Xiao%0AAbstract%3A%20%20%20The%20emergence%20of%20models%20like%20GPTs%2C%20Claude%2C%20LLaMA%2C%20and%20Qwen%20has%20reshaped%20AI%0Aapplications%2C%20presenting%20vast%20new%20opportunities%20across%20industries.%20Yet%2C%20the%0Aintegration%20of%20tabular%20data%20remains%20notably%20underdeveloped%2C%20despite%20its%0Afoundational%20role%20in%20numerous%20real-world%20domains.%0A%20%20This%20gap%20is%20critical%20for%20three%20main%20reasons.%20First%2C%20database%20or%20data%0Awarehouse%20data%20integration%20is%20essential%20for%20advanced%20applications%3B%20second%2C%20the%0Avast%20and%20largely%20untapped%20resource%20of%20tabular%20data%20offers%20immense%20potential%20for%0Aanalysis%3B%20and%20third%2C%20the%20business%20intelligence%20domain%20specifically%20demands%0Aadaptable%2C%20precise%20solutions%20that%20many%20current%20LLMs%20may%20struggle%20to%20provide.%0A%20%20In%20response%2C%20we%20introduce%20TableGPT2%2C%20a%20model%20rigorously%20pre-trained%20and%0Afine-tuned%20with%20over%20593.8K%20tables%20and%202.36M%20high-quality%20query-table-output%0Atuples%2C%20a%20scale%20of%20table-related%20data%20unprecedented%20in%20prior%20research.%20This%0Aextensive%20training%20enables%20TableGPT2%20to%20excel%20in%20table-centric%20tasks%20while%0Amaintaining%20strong%20general%20language%20and%20coding%20abilities.%0A%20%20One%20of%20TableGPT2%27s%20key%20innovations%20is%20its%20novel%20table%20encoder%2C%20specifically%0Adesigned%20to%20capture%20schema-level%20and%20cell-level%20information.%20This%20encoder%0Astrengthens%20the%20model%27s%20ability%20to%20handle%20ambiguous%20queries%2C%20missing%20column%0Anames%2C%20and%20irregular%20tables%20commonly%20encountered%20in%20real-world%20applications.%0ASimilar%20to%20visual%20language%20models%2C%20this%20pioneering%20approach%20integrates%20with%20the%0Adecoder%20to%20form%20a%20robust%20large%20multimodal%20model.%0A%20%20We%20believe%20the%20results%20are%20compelling%3A%20over%2023%20benchmarking%20metrics%2C%0ATableGPT2%20achieves%20an%20average%20performance%20improvement%20of%2035.20%25%20in%20the%207B%20model%0Aand%2049.32%25%20in%20the%2072B%20model%20over%20prior%20benchmark-neutral%20LLMs%2C%20with%20robust%0Ageneral-purpose%20capabilities%20intact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTableGPT2%253A%2520A%2520Large%2520Multimodal%2520Model%2520with%2520Tabular%2520Data%2520Integration%26entry.906535625%3DAofeng%2520Su%2520and%2520Aowen%2520Wang%2520and%2520Chao%2520Ye%2520and%2520Chen%2520Zhou%2520and%2520Ga%2520Zhang%2520and%2520Guangcheng%2520Zhu%2520and%2520Haobo%2520Wang%2520and%2520Haokai%2520Xu%2520and%2520Hao%2520Chen%2520and%2520Haoze%2520Li%2520and%2520Haoxuan%2520Lan%2520and%2520Jiaming%2520Tian%2520and%2520Jing%2520Yuan%2520and%2520Junbo%2520Zhao%2520and%2520Junlin%2520Zhou%2520and%2520Kaizhe%2520Shou%2520and%2520Liangyu%2520Zha%2520and%2520Lin%2520Long%2520and%2520Liyao%2520Li%2520and%2520Pengzuo%2520Wu%2520and%2520Qi%2520Zhang%2520and%2520Qingyi%2520Huang%2520and%2520Saisai%2520Yang%2520and%2520Tao%2520Zhang%2520and%2520Wentao%2520Ye%2520and%2520Wufang%2520Zhu%2520and%2520Xiaomeng%2520Hu%2520and%2520Xijun%2520Gu%2520and%2520Xinjie%2520Sun%2520and%2520Xiang%2520Li%2520and%2520Yuhang%2520Yang%2520and%2520Zhiqing%2520Xiao%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520models%2520like%2520GPTs%252C%2520Claude%252C%2520LLaMA%252C%2520and%2520Qwen%2520has%2520reshaped%2520AI%250Aapplications%252C%2520presenting%2520vast%2520new%2520opportunities%2520across%2520industries.%2520Yet%252C%2520the%250Aintegration%2520of%2520tabular%2520data%2520remains%2520notably%2520underdeveloped%252C%2520despite%2520its%250Afoundational%2520role%2520in%2520numerous%2520real-world%2520domains.%250A%2520%2520This%2520gap%2520is%2520critical%2520for%2520three%2520main%2520reasons.%2520First%252C%2520database%2520or%2520data%250Awarehouse%2520data%2520integration%2520is%2520essential%2520for%2520advanced%2520applications%253B%2520second%252C%2520the%250Avast%2520and%2520largely%2520untapped%2520resource%2520of%2520tabular%2520data%2520offers%2520immense%2520potential%2520for%250Aanalysis%253B%2520and%2520third%252C%2520the%2520business%2520intelligence%2520domain%2520specifically%2520demands%250Aadaptable%252C%2520precise%2520solutions%2520that%2520many%2520current%2520LLMs%2520may%2520struggle%2520to%2520provide.%250A%2520%2520In%2520response%252C%2520we%2520introduce%2520TableGPT2%252C%2520a%2520model%2520rigorously%2520pre-trained%2520and%250Afine-tuned%2520with%2520over%2520593.8K%2520tables%2520and%25202.36M%2520high-quality%2520query-table-output%250Atuples%252C%2520a%2520scale%2520of%2520table-related%2520data%2520unprecedented%2520in%2520prior%2520research.%2520This%250Aextensive%2520training%2520enables%2520TableGPT2%2520to%2520excel%2520in%2520table-centric%2520tasks%2520while%250Amaintaining%2520strong%2520general%2520language%2520and%2520coding%2520abilities.%250A%2520%2520One%2520of%2520TableGPT2%2527s%2520key%2520innovations%2520is%2520its%2520novel%2520table%2520encoder%252C%2520specifically%250Adesigned%2520to%2520capture%2520schema-level%2520and%2520cell-level%2520information.%2520This%2520encoder%250Astrengthens%2520the%2520model%2527s%2520ability%2520to%2520handle%2520ambiguous%2520queries%252C%2520missing%2520column%250Anames%252C%2520and%2520irregular%2520tables%2520commonly%2520encountered%2520in%2520real-world%2520applications.%250ASimilar%2520to%2520visual%2520language%2520models%252C%2520this%2520pioneering%2520approach%2520integrates%2520with%2520the%250Adecoder%2520to%2520form%2520a%2520robust%2520large%2520multimodal%2520model.%250A%2520%2520We%2520believe%2520the%2520results%2520are%2520compelling%253A%2520over%252023%2520benchmarking%2520metrics%252C%250ATableGPT2%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%252035.20%2525%2520in%2520the%25207B%2520model%250Aand%252049.32%2525%2520in%2520the%252072B%2520model%2520over%2520prior%2520benchmark-neutral%2520LLMs%252C%2520with%2520robust%250Ageneral-purpose%2520capabilities%2520intact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration&entry.906535625=Aofeng%20Su%20and%20Aowen%20Wang%20and%20Chao%20Ye%20and%20Chen%20Zhou%20and%20Ga%20Zhang%20and%20Guangcheng%20Zhu%20and%20Haobo%20Wang%20and%20Haokai%20Xu%20and%20Hao%20Chen%20and%20Haoze%20Li%20and%20Haoxuan%20Lan%20and%20Jiaming%20Tian%20and%20Jing%20Yuan%20and%20Junbo%20Zhao%20and%20Junlin%20Zhou%20and%20Kaizhe%20Shou%20and%20Liangyu%20Zha%20and%20Lin%20Long%20and%20Liyao%20Li%20and%20Pengzuo%20Wu%20and%20Qi%20Zhang%20and%20Qingyi%20Huang%20and%20Saisai%20Yang%20and%20Tao%20Zhang%20and%20Wentao%20Ye%20and%20Wufang%20Zhu%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Xinjie%20Sun%20and%20Xiang%20Li%20and%20Yuhang%20Yang%20and%20Zhiqing%20Xiao&entry.1292438233=%20%20The%20emergence%20of%20models%20like%20GPTs%2C%20Claude%2C%20LLaMA%2C%20and%20Qwen%20has%20reshaped%20AI%0Aapplications%2C%20presenting%20vast%20new%20opportunities%20across%20industries.%20Yet%2C%20the%0Aintegration%20of%20tabular%20data%20remains%20notably%20underdeveloped%2C%20despite%20its%0Afoundational%20role%20in%20numerous%20real-world%20domains.%0A%20%20This%20gap%20is%20critical%20for%20three%20main%20reasons.%20First%2C%20database%20or%20data%0Awarehouse%20data%20integration%20is%20essential%20for%20advanced%20applications%3B%20second%2C%20the%0Avast%20and%20largely%20untapped%20resource%20of%20tabular%20data%20offers%20immense%20potential%20for%0Aanalysis%3B%20and%20third%2C%20the%20business%20intelligence%20domain%20specifically%20demands%0Aadaptable%2C%20precise%20solutions%20that%20many%20current%20LLMs%20may%20struggle%20to%20provide.%0A%20%20In%20response%2C%20we%20introduce%20TableGPT2%2C%20a%20model%20rigorously%20pre-trained%20and%0Afine-tuned%20with%20over%20593.8K%20tables%20and%202.36M%20high-quality%20query-table-output%0Atuples%2C%20a%20scale%20of%20table-related%20data%20unprecedented%20in%20prior%20research.%20This%0Aextensive%20training%20enables%20TableGPT2%20to%20excel%20in%20table-centric%20tasks%20while%0Amaintaining%20strong%20general%20language%20and%20coding%20abilities.%0A%20%20One%20of%20TableGPT2%27s%20key%20innovations%20is%20its%20novel%20table%20encoder%2C%20specifically%0Adesigned%20to%20capture%20schema-level%20and%20cell-level%20information.%20This%20encoder%0Astrengthens%20the%20model%27s%20ability%20to%20handle%20ambiguous%20queries%2C%20missing%20column%0Anames%2C%20and%20irregular%20tables%20commonly%20encountered%20in%20real-world%20applications.%0ASimilar%20to%20visual%20language%20models%2C%20this%20pioneering%20approach%20integrates%20with%20the%0Adecoder%20to%20form%20a%20robust%20large%20multimodal%20model.%0A%20%20We%20believe%20the%20results%20are%20compelling%3A%20over%2023%20benchmarking%20metrics%2C%0ATableGPT2%20achieves%20an%20average%20performance%20improvement%20of%2035.20%25%20in%20the%207B%20model%0Aand%2049.32%25%20in%20the%2072B%20model%20over%20prior%20benchmark-neutral%20LLMs%2C%20with%20robust%0Ageneral-purpose%20capabilities%20intact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02059v1&entry.124074799=Read"},
{"title": "Addressing Representation Collapse in Vector Quantized Models with One\n  Linear Layer", "author": "Yongxin Zhu and Bocheng Li and Yifei Xin and Linli Xu", "abstract": "  Vector Quantization (VQ) is a widely used method for converting continuous\nrepresentations into discrete codes, which has become fundamental in\nunsupervised representation learning and latent generative models. However, VQ\nmodels are often hindered by the problem of representation collapse in the\nlatent space, which leads to low codebook utilization and limits the\nscalability of the codebook for large-scale training. Existing methods designed\nto mitigate representation collapse typically reduce the dimensionality of\nlatent space at the expense of model capacity, which do not fully resolve the\ncore issue. In this study, we conduct a theoretical analysis of representation\ncollapse in VQ models and identify its primary cause as the disjoint\noptimization of the codebook, where only a small subset of code vectors are\nupdated through gradient descent. To address this issue, we propose\n\\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a\nlinear transformation layer based on a learnable latent basis. This\ntransformation optimizes the \\textit{entire linear space} spanned by the\ncodebook, rather than merely updating \\textit{the code vector} selected by the\nnearest-neighbor search in vanilla VQ models. Although it is commonly\nunderstood that the multiplication of two linear matrices is equivalent to\napplying a single linear layer, our approach works surprisingly well in\nresolving the collapse issue in VQ models with just one linear layer. We\nvalidate the efficacy of SimVQ through extensive experiments across various\nmodalities, including image and audio data with different model architectures.\nOur code is available at \\url{https://github.com/youngsheen/SimVQ}.\n", "link": "http://arxiv.org/abs/2411.02038v1", "date": "2024-11-04", "relevancy": 2.4647, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5078}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Representation%20Collapse%20in%20Vector%20Quantized%20Models%20with%20One%0A%20%20Linear%20Layer&body=Title%3A%20Addressing%20Representation%20Collapse%20in%20Vector%20Quantized%20Models%20with%20One%0A%20%20Linear%20Layer%0AAuthor%3A%20Yongxin%20Zhu%20and%20Bocheng%20Li%20and%20Yifei%20Xin%20and%20Linli%20Xu%0AAbstract%3A%20%20%20Vector%20Quantization%20%28VQ%29%20is%20a%20widely%20used%20method%20for%20converting%20continuous%0Arepresentations%20into%20discrete%20codes%2C%20which%20has%20become%20fundamental%20in%0Aunsupervised%20representation%20learning%20and%20latent%20generative%20models.%20However%2C%20VQ%0Amodels%20are%20often%20hindered%20by%20the%20problem%20of%20representation%20collapse%20in%20the%0Alatent%20space%2C%20which%20leads%20to%20low%20codebook%20utilization%20and%20limits%20the%0Ascalability%20of%20the%20codebook%20for%20large-scale%20training.%20Existing%20methods%20designed%0Ato%20mitigate%20representation%20collapse%20typically%20reduce%20the%20dimensionality%20of%0Alatent%20space%20at%20the%20expense%20of%20model%20capacity%2C%20which%20do%20not%20fully%20resolve%20the%0Acore%20issue.%20In%20this%20study%2C%20we%20conduct%20a%20theoretical%20analysis%20of%20representation%0Acollapse%20in%20VQ%20models%20and%20identify%20its%20primary%20cause%20as%20the%20disjoint%0Aoptimization%20of%20the%20codebook%2C%20where%20only%20a%20small%20subset%20of%20code%20vectors%20are%0Aupdated%20through%20gradient%20descent.%20To%20address%20this%20issue%2C%20we%20propose%0A%5Ctextbf%7BSimVQ%7D%2C%20a%20novel%20method%20which%20reparameterizes%20the%20code%20vectors%20through%20a%0Alinear%20transformation%20layer%20based%20on%20a%20learnable%20latent%20basis.%20This%0Atransformation%20optimizes%20the%20%5Ctextit%7Bentire%20linear%20space%7D%20spanned%20by%20the%0Acodebook%2C%20rather%20than%20merely%20updating%20%5Ctextit%7Bthe%20code%20vector%7D%20selected%20by%20the%0Anearest-neighbor%20search%20in%20vanilla%20VQ%20models.%20Although%20it%20is%20commonly%0Aunderstood%20that%20the%20multiplication%20of%20two%20linear%20matrices%20is%20equivalent%20to%0Aapplying%20a%20single%20linear%20layer%2C%20our%20approach%20works%20surprisingly%20well%20in%0Aresolving%20the%20collapse%20issue%20in%20VQ%20models%20with%20just%20one%20linear%20layer.%20We%0Avalidate%20the%20efficacy%20of%20SimVQ%20through%20extensive%20experiments%20across%20various%0Amodalities%2C%20including%20image%20and%20audio%20data%20with%20different%20model%20architectures.%0AOur%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/youngsheen/SimVQ%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Representation%2520Collapse%2520in%2520Vector%2520Quantized%2520Models%2520with%2520One%250A%2520%2520Linear%2520Layer%26entry.906535625%3DYongxin%2520Zhu%2520and%2520Bocheng%2520Li%2520and%2520Yifei%2520Xin%2520and%2520Linli%2520Xu%26entry.1292438233%3D%2520%2520Vector%2520Quantization%2520%2528VQ%2529%2520is%2520a%2520widely%2520used%2520method%2520for%2520converting%2520continuous%250Arepresentations%2520into%2520discrete%2520codes%252C%2520which%2520has%2520become%2520fundamental%2520in%250Aunsupervised%2520representation%2520learning%2520and%2520latent%2520generative%2520models.%2520However%252C%2520VQ%250Amodels%2520are%2520often%2520hindered%2520by%2520the%2520problem%2520of%2520representation%2520collapse%2520in%2520the%250Alatent%2520space%252C%2520which%2520leads%2520to%2520low%2520codebook%2520utilization%2520and%2520limits%2520the%250Ascalability%2520of%2520the%2520codebook%2520for%2520large-scale%2520training.%2520Existing%2520methods%2520designed%250Ato%2520mitigate%2520representation%2520collapse%2520typically%2520reduce%2520the%2520dimensionality%2520of%250Alatent%2520space%2520at%2520the%2520expense%2520of%2520model%2520capacity%252C%2520which%2520do%2520not%2520fully%2520resolve%2520the%250Acore%2520issue.%2520In%2520this%2520study%252C%2520we%2520conduct%2520a%2520theoretical%2520analysis%2520of%2520representation%250Acollapse%2520in%2520VQ%2520models%2520and%2520identify%2520its%2520primary%2520cause%2520as%2520the%2520disjoint%250Aoptimization%2520of%2520the%2520codebook%252C%2520where%2520only%2520a%2520small%2520subset%2520of%2520code%2520vectors%2520are%250Aupdated%2520through%2520gradient%2520descent.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250A%255Ctextbf%257BSimVQ%257D%252C%2520a%2520novel%2520method%2520which%2520reparameterizes%2520the%2520code%2520vectors%2520through%2520a%250Alinear%2520transformation%2520layer%2520based%2520on%2520a%2520learnable%2520latent%2520basis.%2520This%250Atransformation%2520optimizes%2520the%2520%255Ctextit%257Bentire%2520linear%2520space%257D%2520spanned%2520by%2520the%250Acodebook%252C%2520rather%2520than%2520merely%2520updating%2520%255Ctextit%257Bthe%2520code%2520vector%257D%2520selected%2520by%2520the%250Anearest-neighbor%2520search%2520in%2520vanilla%2520VQ%2520models.%2520Although%2520it%2520is%2520commonly%250Aunderstood%2520that%2520the%2520multiplication%2520of%2520two%2520linear%2520matrices%2520is%2520equivalent%2520to%250Aapplying%2520a%2520single%2520linear%2520layer%252C%2520our%2520approach%2520works%2520surprisingly%2520well%2520in%250Aresolving%2520the%2520collapse%2520issue%2520in%2520VQ%2520models%2520with%2520just%2520one%2520linear%2520layer.%2520We%250Avalidate%2520the%2520efficacy%2520of%2520SimVQ%2520through%2520extensive%2520experiments%2520across%2520various%250Amodalities%252C%2520including%2520image%2520and%2520audio%2520data%2520with%2520different%2520model%2520architectures.%250AOur%2520code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/youngsheen/SimVQ%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Representation%20Collapse%20in%20Vector%20Quantized%20Models%20with%20One%0A%20%20Linear%20Layer&entry.906535625=Yongxin%20Zhu%20and%20Bocheng%20Li%20and%20Yifei%20Xin%20and%20Linli%20Xu&entry.1292438233=%20%20Vector%20Quantization%20%28VQ%29%20is%20a%20widely%20used%20method%20for%20converting%20continuous%0Arepresentations%20into%20discrete%20codes%2C%20which%20has%20become%20fundamental%20in%0Aunsupervised%20representation%20learning%20and%20latent%20generative%20models.%20However%2C%20VQ%0Amodels%20are%20often%20hindered%20by%20the%20problem%20of%20representation%20collapse%20in%20the%0Alatent%20space%2C%20which%20leads%20to%20low%20codebook%20utilization%20and%20limits%20the%0Ascalability%20of%20the%20codebook%20for%20large-scale%20training.%20Existing%20methods%20designed%0Ato%20mitigate%20representation%20collapse%20typically%20reduce%20the%20dimensionality%20of%0Alatent%20space%20at%20the%20expense%20of%20model%20capacity%2C%20which%20do%20not%20fully%20resolve%20the%0Acore%20issue.%20In%20this%20study%2C%20we%20conduct%20a%20theoretical%20analysis%20of%20representation%0Acollapse%20in%20VQ%20models%20and%20identify%20its%20primary%20cause%20as%20the%20disjoint%0Aoptimization%20of%20the%20codebook%2C%20where%20only%20a%20small%20subset%20of%20code%20vectors%20are%0Aupdated%20through%20gradient%20descent.%20To%20address%20this%20issue%2C%20we%20propose%0A%5Ctextbf%7BSimVQ%7D%2C%20a%20novel%20method%20which%20reparameterizes%20the%20code%20vectors%20through%20a%0Alinear%20transformation%20layer%20based%20on%20a%20learnable%20latent%20basis.%20This%0Atransformation%20optimizes%20the%20%5Ctextit%7Bentire%20linear%20space%7D%20spanned%20by%20the%0Acodebook%2C%20rather%20than%20merely%20updating%20%5Ctextit%7Bthe%20code%20vector%7D%20selected%20by%20the%0Anearest-neighbor%20search%20in%20vanilla%20VQ%20models.%20Although%20it%20is%20commonly%0Aunderstood%20that%20the%20multiplication%20of%20two%20linear%20matrices%20is%20equivalent%20to%0Aapplying%20a%20single%20linear%20layer%2C%20our%20approach%20works%20surprisingly%20well%20in%0Aresolving%20the%20collapse%20issue%20in%20VQ%20models%20with%20just%20one%20linear%20layer.%20We%0Avalidate%20the%20efficacy%20of%20SimVQ%20through%20extensive%20experiments%20across%20various%0Amodalities%2C%20including%20image%20and%20audio%20data%20with%20different%20model%20architectures.%0AOur%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/youngsheen/SimVQ%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02038v1&entry.124074799=Read"},
{"title": "Federated GNNs for EEG-Based Stroke Assessment", "author": "Andrea Protani and Lorenzo Giusti and Albert Sund Aillet and Simona Sacco and Paolo Manganotti and Lucio Marinelli and Diogo Reis Santos and Pierpaolo Brutti and Pietro Caliandro and Luigi Serio", "abstract": "  Machine learning (ML) has the potential to become an essential tool in\nsupporting clinical decision-making processes, offering enhanced diagnostic\ncapabilities and personalized treatment plans. However, outsourcing medical\nrecords to train ML models using patient data raises legal, privacy, and\nsecurity concerns. Federated learning has emerged as a promising paradigm for\ncollaborative ML, meeting healthcare institutions' requirements for robust\nmodels without sharing sensitive data and compromising patient privacy. This\nstudy proposes a novel method that combines federated learning (FL) and Graph\nNeural Networks (GNNs) to predict stroke severity using electroencephalography\n(EEG) signals across multiple medical institutions. Our approach enables\nmultiple hospitals to jointly train a shared GNN model on their local EEG data\nwithout exchanging patient information. Specifically, we address a regression\nproblem by predicting the National Institutes of Health Stroke Scale (NIHSS), a\nkey indicator of stroke severity. The proposed model leverages a masked\nself-attention mechanism to capture salient brain connectivity patterns and\nemploys EdgeSHAP to provide post-hoc explanations of the neurological states\nafter a stroke. We evaluated our method on EEG recordings from four\ninstitutions, achieving a mean absolute error (MAE) of 3.23 in predicting\nNIHSS, close to the average error made by human experts (MAE $\\approx$ 3.0).\nThis demonstrates the method's effectiveness in providing accurate and\nexplainable predictions while maintaining data privacy.\n", "link": "http://arxiv.org/abs/2411.02286v1", "date": "2024-11-04", "relevancy": 2.4583, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4756}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20GNNs%20for%20EEG-Based%20Stroke%20Assessment&body=Title%3A%20Federated%20GNNs%20for%20EEG-Based%20Stroke%20Assessment%0AAuthor%3A%20Andrea%20Protani%20and%20Lorenzo%20Giusti%20and%20Albert%20Sund%20Aillet%20and%20Simona%20Sacco%20and%20Paolo%20Manganotti%20and%20Lucio%20Marinelli%20and%20Diogo%20Reis%20Santos%20and%20Pierpaolo%20Brutti%20and%20Pietro%20Caliandro%20and%20Luigi%20Serio%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20has%20the%20potential%20to%20become%20an%20essential%20tool%20in%0Asupporting%20clinical%20decision-making%20processes%2C%20offering%20enhanced%20diagnostic%0Acapabilities%20and%20personalized%20treatment%20plans.%20However%2C%20outsourcing%20medical%0Arecords%20to%20train%20ML%20models%20using%20patient%20data%20raises%20legal%2C%20privacy%2C%20and%0Asecurity%20concerns.%20Federated%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Acollaborative%20ML%2C%20meeting%20healthcare%20institutions%27%20requirements%20for%20robust%0Amodels%20without%20sharing%20sensitive%20data%20and%20compromising%20patient%20privacy.%20This%0Astudy%20proposes%20a%20novel%20method%20that%20combines%20federated%20learning%20%28FL%29%20and%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20predict%20stroke%20severity%20using%20electroencephalography%0A%28EEG%29%20signals%20across%20multiple%20medical%20institutions.%20Our%20approach%20enables%0Amultiple%20hospitals%20to%20jointly%20train%20a%20shared%20GNN%20model%20on%20their%20local%20EEG%20data%0Awithout%20exchanging%20patient%20information.%20Specifically%2C%20we%20address%20a%20regression%0Aproblem%20by%20predicting%20the%20National%20Institutes%20of%20Health%20Stroke%20Scale%20%28NIHSS%29%2C%20a%0Akey%20indicator%20of%20stroke%20severity.%20The%20proposed%20model%20leverages%20a%20masked%0Aself-attention%20mechanism%20to%20capture%20salient%20brain%20connectivity%20patterns%20and%0Aemploys%20EdgeSHAP%20to%20provide%20post-hoc%20explanations%20of%20the%20neurological%20states%0Aafter%20a%20stroke.%20We%20evaluated%20our%20method%20on%20EEG%20recordings%20from%20four%0Ainstitutions%2C%20achieving%20a%20mean%20absolute%20error%20%28MAE%29%20of%203.23%20in%20predicting%0ANIHSS%2C%20close%20to%20the%20average%20error%20made%20by%20human%20experts%20%28MAE%20%24%5Capprox%24%203.0%29.%0AThis%20demonstrates%20the%20method%27s%20effectiveness%20in%20providing%20accurate%20and%0Aexplainable%20predictions%20while%20maintaining%20data%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520GNNs%2520for%2520EEG-Based%2520Stroke%2520Assessment%26entry.906535625%3DAndrea%2520Protani%2520and%2520Lorenzo%2520Giusti%2520and%2520Albert%2520Sund%2520Aillet%2520and%2520Simona%2520Sacco%2520and%2520Paolo%2520Manganotti%2520and%2520Lucio%2520Marinelli%2520and%2520Diogo%2520Reis%2520Santos%2520and%2520Pierpaolo%2520Brutti%2520and%2520Pietro%2520Caliandro%2520and%2520Luigi%2520Serio%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520has%2520the%2520potential%2520to%2520become%2520an%2520essential%2520tool%2520in%250Asupporting%2520clinical%2520decision-making%2520processes%252C%2520offering%2520enhanced%2520diagnostic%250Acapabilities%2520and%2520personalized%2520treatment%2520plans.%2520However%252C%2520outsourcing%2520medical%250Arecords%2520to%2520train%2520ML%2520models%2520using%2520patient%2520data%2520raises%2520legal%252C%2520privacy%252C%2520and%250Asecurity%2520concerns.%2520Federated%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Acollaborative%2520ML%252C%2520meeting%2520healthcare%2520institutions%2527%2520requirements%2520for%2520robust%250Amodels%2520without%2520sharing%2520sensitive%2520data%2520and%2520compromising%2520patient%2520privacy.%2520This%250Astudy%2520proposes%2520a%2520novel%2520method%2520that%2520combines%2520federated%2520learning%2520%2528FL%2529%2520and%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520to%2520predict%2520stroke%2520severity%2520using%2520electroencephalography%250A%2528EEG%2529%2520signals%2520across%2520multiple%2520medical%2520institutions.%2520Our%2520approach%2520enables%250Amultiple%2520hospitals%2520to%2520jointly%2520train%2520a%2520shared%2520GNN%2520model%2520on%2520their%2520local%2520EEG%2520data%250Awithout%2520exchanging%2520patient%2520information.%2520Specifically%252C%2520we%2520address%2520a%2520regression%250Aproblem%2520by%2520predicting%2520the%2520National%2520Institutes%2520of%2520Health%2520Stroke%2520Scale%2520%2528NIHSS%2529%252C%2520a%250Akey%2520indicator%2520of%2520stroke%2520severity.%2520The%2520proposed%2520model%2520leverages%2520a%2520masked%250Aself-attention%2520mechanism%2520to%2520capture%2520salient%2520brain%2520connectivity%2520patterns%2520and%250Aemploys%2520EdgeSHAP%2520to%2520provide%2520post-hoc%2520explanations%2520of%2520the%2520neurological%2520states%250Aafter%2520a%2520stroke.%2520We%2520evaluated%2520our%2520method%2520on%2520EEG%2520recordings%2520from%2520four%250Ainstitutions%252C%2520achieving%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25203.23%2520in%2520predicting%250ANIHSS%252C%2520close%2520to%2520the%2520average%2520error%2520made%2520by%2520human%2520experts%2520%2528MAE%2520%2524%255Capprox%2524%25203.0%2529.%250AThis%2520demonstrates%2520the%2520method%2527s%2520effectiveness%2520in%2520providing%2520accurate%2520and%250Aexplainable%2520predictions%2520while%2520maintaining%2520data%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20GNNs%20for%20EEG-Based%20Stroke%20Assessment&entry.906535625=Andrea%20Protani%20and%20Lorenzo%20Giusti%20and%20Albert%20Sund%20Aillet%20and%20Simona%20Sacco%20and%20Paolo%20Manganotti%20and%20Lucio%20Marinelli%20and%20Diogo%20Reis%20Santos%20and%20Pierpaolo%20Brutti%20and%20Pietro%20Caliandro%20and%20Luigi%20Serio&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20has%20the%20potential%20to%20become%20an%20essential%20tool%20in%0Asupporting%20clinical%20decision-making%20processes%2C%20offering%20enhanced%20diagnostic%0Acapabilities%20and%20personalized%20treatment%20plans.%20However%2C%20outsourcing%20medical%0Arecords%20to%20train%20ML%20models%20using%20patient%20data%20raises%20legal%2C%20privacy%2C%20and%0Asecurity%20concerns.%20Federated%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Acollaborative%20ML%2C%20meeting%20healthcare%20institutions%27%20requirements%20for%20robust%0Amodels%20without%20sharing%20sensitive%20data%20and%20compromising%20patient%20privacy.%20This%0Astudy%20proposes%20a%20novel%20method%20that%20combines%20federated%20learning%20%28FL%29%20and%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20predict%20stroke%20severity%20using%20electroencephalography%0A%28EEG%29%20signals%20across%20multiple%20medical%20institutions.%20Our%20approach%20enables%0Amultiple%20hospitals%20to%20jointly%20train%20a%20shared%20GNN%20model%20on%20their%20local%20EEG%20data%0Awithout%20exchanging%20patient%20information.%20Specifically%2C%20we%20address%20a%20regression%0Aproblem%20by%20predicting%20the%20National%20Institutes%20of%20Health%20Stroke%20Scale%20%28NIHSS%29%2C%20a%0Akey%20indicator%20of%20stroke%20severity.%20The%20proposed%20model%20leverages%20a%20masked%0Aself-attention%20mechanism%20to%20capture%20salient%20brain%20connectivity%20patterns%20and%0Aemploys%20EdgeSHAP%20to%20provide%20post-hoc%20explanations%20of%20the%20neurological%20states%0Aafter%20a%20stroke.%20We%20evaluated%20our%20method%20on%20EEG%20recordings%20from%20four%0Ainstitutions%2C%20achieving%20a%20mean%20absolute%20error%20%28MAE%29%20of%203.23%20in%20predicting%0ANIHSS%2C%20close%20to%20the%20average%20error%20made%20by%20human%20experts%20%28MAE%20%24%5Capprox%24%203.0%29.%0AThis%20demonstrates%20the%20method%27s%20effectiveness%20in%20providing%20accurate%20and%0Aexplainable%20predictions%20while%20maintaining%20data%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02286v1&entry.124074799=Read"},
{"title": "Automatic Target-Less Camera-LiDAR Calibration From Motion and Deep\n  Point Correspondences", "author": "K\u00fcrsat Petek and Niclas V\u00f6disch and Johannes Meyer and Daniele Cattaneo and Abhinav Valada and Wolfram Burgard", "abstract": "  Sensor setups of robotic platforms commonly include both camera and LiDAR as\nthey provide complementary information. However, fusing these two modalities\ntypically requires a highly accurate calibration between them. In this paper,\nwe propose MDPCalib which is a novel method for camera-LiDAR calibration that\nrequires neither human supervision nor any specific target objects. Instead, we\nutilize sensor motion estimates from visual and LiDAR odometry as well as deep\nlearning-based 2D-pixel-to-3D-point correspondences that are obtained without\nin-domain retraining. We represent camera-LiDAR calibration as an optimization\nproblem and minimize the costs induced by constraints from sensor motion and\npoint correspondences. In extensive experiments, we demonstrate that our\napproach yields highly accurate extrinsic calibration parameters and is robust\nto random initialization. Additionally, our approach generalizes to a wide\nrange of sensor setups, which we demonstrate by employing it on various robotic\nplatforms including a self-driving perception car, a quadruped robot, and a\nUAV. To make our calibration method publicly accessible, we release the code on\nour project website at http://calibration.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2404.17298v3", "date": "2024-11-04", "relevancy": 2.4555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6198}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6179}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&body=Title%3A%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences%0AAuthor%3A%20K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20camera-LiDAR%20calibration%20as%20an%20optimization%0Aproblem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%20motion%20and%0Apoint%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%20robust%0Ato%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%20wide%0Arange%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%20robotic%0Aplatforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%20and%20a%0AUAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%20code%20on%0Aour%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17298v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Target-Less%2520Camera-LiDAR%2520Calibration%2520From%2520Motion%2520and%2520Deep%250A%2520%2520Point%2520Correspondences%26entry.906535625%3DK%25C3%25BCrsat%2520Petek%2520and%2520Niclas%2520V%25C3%25B6disch%2520and%2520Johannes%2520Meyer%2520and%2520Daniele%2520Cattaneo%2520and%2520Abhinav%2520Valada%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Sensor%2520setups%2520of%2520robotic%2520platforms%2520commonly%2520include%2520both%2520camera%2520and%2520LiDAR%2520as%250Athey%2520provide%2520complementary%2520information.%2520However%252C%2520fusing%2520these%2520two%2520modalities%250Atypically%2520requires%2520a%2520highly%2520accurate%2520calibration%2520between%2520them.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520MDPCalib%2520which%2520is%2520a%2520novel%2520method%2520for%2520camera-LiDAR%2520calibration%2520that%250Arequires%2520neither%2520human%2520supervision%2520nor%2520any%2520specific%2520target%2520objects.%2520Instead%252C%2520we%250Autilize%2520sensor%2520motion%2520estimates%2520from%2520visual%2520and%2520LiDAR%2520odometry%2520as%2520well%2520as%2520deep%250Alearning-based%25202D-pixel-to-3D-point%2520correspondences%2520that%2520are%2520obtained%2520without%250Ain-domain%2520retraining.%2520We%2520represent%2520camera-LiDAR%2520calibration%2520as%2520an%2520optimization%250Aproblem%2520and%2520minimize%2520the%2520costs%2520induced%2520by%2520constraints%2520from%2520sensor%2520motion%2520and%250Apoint%2520correspondences.%2520In%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%250Aapproach%2520yields%2520highly%2520accurate%2520extrinsic%2520calibration%2520parameters%2520and%2520is%2520robust%250Ato%2520random%2520initialization.%2520Additionally%252C%2520our%2520approach%2520generalizes%2520to%2520a%2520wide%250Arange%2520of%2520sensor%2520setups%252C%2520which%2520we%2520demonstrate%2520by%2520employing%2520it%2520on%2520various%2520robotic%250Aplatforms%2520including%2520a%2520self-driving%2520perception%2520car%252C%2520a%2520quadruped%2520robot%252C%2520and%2520a%250AUAV.%2520To%2520make%2520our%2520calibration%2520method%2520publicly%2520accessible%252C%2520we%2520release%2520the%2520code%2520on%250Aour%2520project%2520website%2520at%2520http%253A//calibration.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17298v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&entry.906535625=K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20camera-LiDAR%20calibration%20as%20an%20optimization%0Aproblem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%20motion%20and%0Apoint%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%20robust%0Ato%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%20wide%0Arange%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%20robotic%0Aplatforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%20and%20a%0AUAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%20code%20on%0Aour%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17298v3&entry.124074799=Read"},
{"title": "Collective Model Intelligence Requires Compatible Specialization", "author": "Jyothish Pari and Samy Jelassi and Pulkit Agrawal", "abstract": "  In this work, we explore the limitations of combining models by averaging\nintermediate features, referred to as model merging, and propose a new\ndirection for achieving collective model intelligence through what we call\ncompatible specialization. Current methods for model merging, such as parameter\nand feature averaging, struggle to effectively combine specialized models due\nto representational divergence during fine-tuning. As models specialize to\ntheir individual domains, their internal feature representations become\nincreasingly incompatible, leading to poor performance when attempting to merge\nthem for new tasks. We analyze this phenomenon using centered kernel alignment\n(CKA) and show that as models specialize, the similarity in their feature space\nstructure diminishes, hindering their capacity for collective use. To address\nthese challenges, we investigate routing-based merging strategies, which offer\nmore flexible methods for combining specialized models by dynamically routing\nacross different layers. This allows us to improve on existing methods by\ncombining features from multiple layers rather than relying on fixed,\nlayer-wise combinations. However, we find that these approaches still face\nlimitations when layers within models are representationally incompatible. Our\nfindings highlight the importance of designing new approaches for model merging\nthat operate on well-defined input and output spaces, similar to how humans\ncommunicate through language rather than intermediate neural activations.\n", "link": "http://arxiv.org/abs/2411.02207v1", "date": "2024-11-04", "relevancy": 2.4537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collective%20Model%20Intelligence%20Requires%20Compatible%20Specialization&body=Title%3A%20Collective%20Model%20Intelligence%20Requires%20Compatible%20Specialization%0AAuthor%3A%20Jyothish%20Pari%20and%20Samy%20Jelassi%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20the%20limitations%20of%20combining%20models%20by%20averaging%0Aintermediate%20features%2C%20referred%20to%20as%20model%20merging%2C%20and%20propose%20a%20new%0Adirection%20for%20achieving%20collective%20model%20intelligence%20through%20what%20we%20call%0Acompatible%20specialization.%20Current%20methods%20for%20model%20merging%2C%20such%20as%20parameter%0Aand%20feature%20averaging%2C%20struggle%20to%20effectively%20combine%20specialized%20models%20due%0Ato%20representational%20divergence%20during%20fine-tuning.%20As%20models%20specialize%20to%0Atheir%20individual%20domains%2C%20their%20internal%20feature%20representations%20become%0Aincreasingly%20incompatible%2C%20leading%20to%20poor%20performance%20when%20attempting%20to%20merge%0Athem%20for%20new%20tasks.%20We%20analyze%20this%20phenomenon%20using%20centered%20kernel%20alignment%0A%28CKA%29%20and%20show%20that%20as%20models%20specialize%2C%20the%20similarity%20in%20their%20feature%20space%0Astructure%20diminishes%2C%20hindering%20their%20capacity%20for%20collective%20use.%20To%20address%0Athese%20challenges%2C%20we%20investigate%20routing-based%20merging%20strategies%2C%20which%20offer%0Amore%20flexible%20methods%20for%20combining%20specialized%20models%20by%20dynamically%20routing%0Aacross%20different%20layers.%20This%20allows%20us%20to%20improve%20on%20existing%20methods%20by%0Acombining%20features%20from%20multiple%20layers%20rather%20than%20relying%20on%20fixed%2C%0Alayer-wise%20combinations.%20However%2C%20we%20find%20that%20these%20approaches%20still%20face%0Alimitations%20when%20layers%20within%20models%20are%20representationally%20incompatible.%20Our%0Afindings%20highlight%20the%20importance%20of%20designing%20new%20approaches%20for%20model%20merging%0Athat%20operate%20on%20well-defined%20input%20and%20output%20spaces%2C%20similar%20to%20how%20humans%0Acommunicate%20through%20language%20rather%20than%20intermediate%20neural%20activations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollective%2520Model%2520Intelligence%2520Requires%2520Compatible%2520Specialization%26entry.906535625%3DJyothish%2520Pari%2520and%2520Samy%2520Jelassi%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520limitations%2520of%2520combining%2520models%2520by%2520averaging%250Aintermediate%2520features%252C%2520referred%2520to%2520as%2520model%2520merging%252C%2520and%2520propose%2520a%2520new%250Adirection%2520for%2520achieving%2520collective%2520model%2520intelligence%2520through%2520what%2520we%2520call%250Acompatible%2520specialization.%2520Current%2520methods%2520for%2520model%2520merging%252C%2520such%2520as%2520parameter%250Aand%2520feature%2520averaging%252C%2520struggle%2520to%2520effectively%2520combine%2520specialized%2520models%2520due%250Ato%2520representational%2520divergence%2520during%2520fine-tuning.%2520As%2520models%2520specialize%2520to%250Atheir%2520individual%2520domains%252C%2520their%2520internal%2520feature%2520representations%2520become%250Aincreasingly%2520incompatible%252C%2520leading%2520to%2520poor%2520performance%2520when%2520attempting%2520to%2520merge%250Athem%2520for%2520new%2520tasks.%2520We%2520analyze%2520this%2520phenomenon%2520using%2520centered%2520kernel%2520alignment%250A%2528CKA%2529%2520and%2520show%2520that%2520as%2520models%2520specialize%252C%2520the%2520similarity%2520in%2520their%2520feature%2520space%250Astructure%2520diminishes%252C%2520hindering%2520their%2520capacity%2520for%2520collective%2520use.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520investigate%2520routing-based%2520merging%2520strategies%252C%2520which%2520offer%250Amore%2520flexible%2520methods%2520for%2520combining%2520specialized%2520models%2520by%2520dynamically%2520routing%250Aacross%2520different%2520layers.%2520This%2520allows%2520us%2520to%2520improve%2520on%2520existing%2520methods%2520by%250Acombining%2520features%2520from%2520multiple%2520layers%2520rather%2520than%2520relying%2520on%2520fixed%252C%250Alayer-wise%2520combinations.%2520However%252C%2520we%2520find%2520that%2520these%2520approaches%2520still%2520face%250Alimitations%2520when%2520layers%2520within%2520models%2520are%2520representationally%2520incompatible.%2520Our%250Afindings%2520highlight%2520the%2520importance%2520of%2520designing%2520new%2520approaches%2520for%2520model%2520merging%250Athat%2520operate%2520on%2520well-defined%2520input%2520and%2520output%2520spaces%252C%2520similar%2520to%2520how%2520humans%250Acommunicate%2520through%2520language%2520rather%2520than%2520intermediate%2520neural%2520activations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collective%20Model%20Intelligence%20Requires%20Compatible%20Specialization&entry.906535625=Jyothish%20Pari%20and%20Samy%20Jelassi%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20the%20limitations%20of%20combining%20models%20by%20averaging%0Aintermediate%20features%2C%20referred%20to%20as%20model%20merging%2C%20and%20propose%20a%20new%0Adirection%20for%20achieving%20collective%20model%20intelligence%20through%20what%20we%20call%0Acompatible%20specialization.%20Current%20methods%20for%20model%20merging%2C%20such%20as%20parameter%0Aand%20feature%20averaging%2C%20struggle%20to%20effectively%20combine%20specialized%20models%20due%0Ato%20representational%20divergence%20during%20fine-tuning.%20As%20models%20specialize%20to%0Atheir%20individual%20domains%2C%20their%20internal%20feature%20representations%20become%0Aincreasingly%20incompatible%2C%20leading%20to%20poor%20performance%20when%20attempting%20to%20merge%0Athem%20for%20new%20tasks.%20We%20analyze%20this%20phenomenon%20using%20centered%20kernel%20alignment%0A%28CKA%29%20and%20show%20that%20as%20models%20specialize%2C%20the%20similarity%20in%20their%20feature%20space%0Astructure%20diminishes%2C%20hindering%20their%20capacity%20for%20collective%20use.%20To%20address%0Athese%20challenges%2C%20we%20investigate%20routing-based%20merging%20strategies%2C%20which%20offer%0Amore%20flexible%20methods%20for%20combining%20specialized%20models%20by%20dynamically%20routing%0Aacross%20different%20layers.%20This%20allows%20us%20to%20improve%20on%20existing%20methods%20by%0Acombining%20features%20from%20multiple%20layers%20rather%20than%20relying%20on%20fixed%2C%0Alayer-wise%20combinations.%20However%2C%20we%20find%20that%20these%20approaches%20still%20face%0Alimitations%20when%20layers%20within%20models%20are%20representationally%20incompatible.%20Our%0Afindings%20highlight%20the%20importance%20of%20designing%20new%20approaches%20for%20model%20merging%0Athat%20operate%20on%20well-defined%20input%20and%20output%20spaces%2C%20similar%20to%20how%20humans%0Acommunicate%20through%20language%20rather%20than%20intermediate%20neural%20activations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02207v1&entry.124074799=Read"},
{"title": "EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast\n  Histopathology Classification: A Comprehensive Approach", "author": "Naren Sengodan", "abstract": "  Breast cancer histopathology image classification is crucial for early cancer\ndetection, offering the potential to reduce mortality rates through timely\ndiagnosis. This paper introduces a novel approach integrating Hybrid\nEfficientNet models with advanced attention mechanisms, including Convolutional\nBlock Attention Module (CBAM), Self-Attention, and Deformable Attention, to\nenhance feature extraction and focus on critical image regions. We evaluate the\nperformance of our models across multiple magnification scales using publicly\navailable histopathological datasets. Our method achieves significant\nimprovements, with accuracy reaching 98.42% at 400X magnification, surpassing\nseveral state-of-the-art models, including VGG and ResNet architectures. The\nresults are validated using metrics such as accuracy, F1-score, precision, and\nrecall, demonstrating the clinical potential of our model in improving\ndiagnostic accuracy. Furthermore, the proposed method shows increased\ncomputational efficiency, making it suitable for integration into real-time\ndiagnostic workflows.\n", "link": "http://arxiv.org/abs/2410.22392v2", "date": "2024-11-04", "relevancy": 2.4525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5108}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientNet%20with%20Hybrid%20Attention%20Mechanisms%20for%20Enhanced%20Breast%0A%20%20Histopathology%20Classification%3A%20A%20Comprehensive%20Approach&body=Title%3A%20EfficientNet%20with%20Hybrid%20Attention%20Mechanisms%20for%20Enhanced%20Breast%0A%20%20Histopathology%20Classification%3A%20A%20Comprehensive%20Approach%0AAuthor%3A%20Naren%20Sengodan%0AAbstract%3A%20%20%20Breast%20cancer%20histopathology%20image%20classification%20is%20crucial%20for%20early%20cancer%0Adetection%2C%20offering%20the%20potential%20to%20reduce%20mortality%20rates%20through%20timely%0Adiagnosis.%20This%20paper%20introduces%20a%20novel%20approach%20integrating%20Hybrid%0AEfficientNet%20models%20with%20advanced%20attention%20mechanisms%2C%20including%20Convolutional%0ABlock%20Attention%20Module%20%28CBAM%29%2C%20Self-Attention%2C%20and%20Deformable%20Attention%2C%20to%0Aenhance%20feature%20extraction%20and%20focus%20on%20critical%20image%20regions.%20We%20evaluate%20the%0Aperformance%20of%20our%20models%20across%20multiple%20magnification%20scales%20using%20publicly%0Aavailable%20histopathological%20datasets.%20Our%20method%20achieves%20significant%0Aimprovements%2C%20with%20accuracy%20reaching%2098.42%25%20at%20400X%20magnification%2C%20surpassing%0Aseveral%20state-of-the-art%20models%2C%20including%20VGG%20and%20ResNet%20architectures.%20The%0Aresults%20are%20validated%20using%20metrics%20such%20as%20accuracy%2C%20F1-score%2C%20precision%2C%20and%0Arecall%2C%20demonstrating%20the%20clinical%20potential%20of%20our%20model%20in%20improving%0Adiagnostic%20accuracy.%20Furthermore%2C%20the%20proposed%20method%20shows%20increased%0Acomputational%20efficiency%2C%20making%20it%20suitable%20for%20integration%20into%20real-time%0Adiagnostic%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientNet%2520with%2520Hybrid%2520Attention%2520Mechanisms%2520for%2520Enhanced%2520Breast%250A%2520%2520Histopathology%2520Classification%253A%2520A%2520Comprehensive%2520Approach%26entry.906535625%3DNaren%2520Sengodan%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520histopathology%2520image%2520classification%2520is%2520crucial%2520for%2520early%2520cancer%250Adetection%252C%2520offering%2520the%2520potential%2520to%2520reduce%2520mortality%2520rates%2520through%2520timely%250Adiagnosis.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520integrating%2520Hybrid%250AEfficientNet%2520models%2520with%2520advanced%2520attention%2520mechanisms%252C%2520including%2520Convolutional%250ABlock%2520Attention%2520Module%2520%2528CBAM%2529%252C%2520Self-Attention%252C%2520and%2520Deformable%2520Attention%252C%2520to%250Aenhance%2520feature%2520extraction%2520and%2520focus%2520on%2520critical%2520image%2520regions.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520our%2520models%2520across%2520multiple%2520magnification%2520scales%2520using%2520publicly%250Aavailable%2520histopathological%2520datasets.%2520Our%2520method%2520achieves%2520significant%250Aimprovements%252C%2520with%2520accuracy%2520reaching%252098.42%2525%2520at%2520400X%2520magnification%252C%2520surpassing%250Aseveral%2520state-of-the-art%2520models%252C%2520including%2520VGG%2520and%2520ResNet%2520architectures.%2520The%250Aresults%2520are%2520validated%2520using%2520metrics%2520such%2520as%2520accuracy%252C%2520F1-score%252C%2520precision%252C%2520and%250Arecall%252C%2520demonstrating%2520the%2520clinical%2520potential%2520of%2520our%2520model%2520in%2520improving%250Adiagnostic%2520accuracy.%2520Furthermore%252C%2520the%2520proposed%2520method%2520shows%2520increased%250Acomputational%2520efficiency%252C%2520making%2520it%2520suitable%2520for%2520integration%2520into%2520real-time%250Adiagnostic%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientNet%20with%20Hybrid%20Attention%20Mechanisms%20for%20Enhanced%20Breast%0A%20%20Histopathology%20Classification%3A%20A%20Comprehensive%20Approach&entry.906535625=Naren%20Sengodan&entry.1292438233=%20%20Breast%20cancer%20histopathology%20image%20classification%20is%20crucial%20for%20early%20cancer%0Adetection%2C%20offering%20the%20potential%20to%20reduce%20mortality%20rates%20through%20timely%0Adiagnosis.%20This%20paper%20introduces%20a%20novel%20approach%20integrating%20Hybrid%0AEfficientNet%20models%20with%20advanced%20attention%20mechanisms%2C%20including%20Convolutional%0ABlock%20Attention%20Module%20%28CBAM%29%2C%20Self-Attention%2C%20and%20Deformable%20Attention%2C%20to%0Aenhance%20feature%20extraction%20and%20focus%20on%20critical%20image%20regions.%20We%20evaluate%20the%0Aperformance%20of%20our%20models%20across%20multiple%20magnification%20scales%20using%20publicly%0Aavailable%20histopathological%20datasets.%20Our%20method%20achieves%20significant%0Aimprovements%2C%20with%20accuracy%20reaching%2098.42%25%20at%20400X%20magnification%2C%20surpassing%0Aseveral%20state-of-the-art%20models%2C%20including%20VGG%20and%20ResNet%20architectures.%20The%0Aresults%20are%20validated%20using%20metrics%20such%20as%20accuracy%2C%20F1-score%2C%20precision%2C%20and%0Arecall%2C%20demonstrating%20the%20clinical%20potential%20of%20our%20model%20in%20improving%0Adiagnostic%20accuracy.%20Furthermore%2C%20the%20proposed%20method%20shows%20increased%0Acomputational%20efficiency%2C%20making%20it%20suitable%20for%20integration%20into%20real-time%0Adiagnostic%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22392v2&entry.124074799=Read"},
{"title": "Improving Steering Vectors by Targeting Sparse Autoencoder Features", "author": "Sviatoslav Chalnev and Matthew Siu and Arthur Conmy", "abstract": "  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nalmost all existing methods, such as CAA (Panickssery et al., 2024) or the\ndirect use of SAE latents (Templeton et al., 2024). In our work, we address\nthis issue by using SAEs to measure the effects of steering vectors, giving us\na method that can be used to understand the causal effect of any steering\nvector intervention. We use this method for measuring causal effects to develop\nan improved steering method, SAE-Targeted Steering (SAE-TS), which finds\nsteering vectors to target specific SAE features while minimizing unintended\nside effects. We show that overall, SAE-TS balances steering effects with\ncoherence better than CAA and SAE feature steering, when evaluated on a range\nof tasks.\n", "link": "http://arxiv.org/abs/2411.02193v1", "date": "2024-11-04", "relevancy": 2.45, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features&body=Title%3A%20Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features%0AAuthor%3A%20Sviatoslav%20Chalnev%20and%20Matthew%20Siu%20and%20Arthur%20Conmy%0AAbstract%3A%20%20%20To%20control%20the%20behavior%20of%20language%20models%2C%20steering%20methods%20attempt%20to%0Aensure%20that%20outputs%20of%20the%20model%20satisfy%20specific%20pre-defined%20properties.%0AAdding%20steering%20vectors%20to%20the%20model%20is%20a%20promising%20method%20of%20model%20control%0Athat%20is%20easier%20than%20finetuning%2C%20and%20may%20be%20more%20robust%20than%20prompting.%20However%2C%0Ait%20can%20be%20difficult%20to%20anticipate%20the%20effects%20of%20steering%20vectors%20produced%20by%0Aalmost%20all%20existing%20methods%2C%20such%20as%20CAA%20%28Panickssery%20et%20al.%2C%202024%29%20or%20the%0Adirect%20use%20of%20SAE%20latents%20%28Templeton%20et%20al.%2C%202024%29.%20In%20our%20work%2C%20we%20address%0Athis%20issue%20by%20using%20SAEs%20to%20measure%20the%20effects%20of%20steering%20vectors%2C%20giving%20us%0Aa%20method%20that%20can%20be%20used%20to%20understand%20the%20causal%20effect%20of%20any%20steering%0Avector%20intervention.%20We%20use%20this%20method%20for%20measuring%20causal%20effects%20to%20develop%0Aan%20improved%20steering%20method%2C%20SAE-Targeted%20Steering%20%28SAE-TS%29%2C%20which%20finds%0Asteering%20vectors%20to%20target%20specific%20SAE%20features%20while%20minimizing%20unintended%0Aside%20effects.%20We%20show%20that%20overall%2C%20SAE-TS%20balances%20steering%20effects%20with%0Acoherence%20better%20than%20CAA%20and%20SAE%20feature%20steering%2C%20when%20evaluated%20on%20a%20range%0Aof%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Steering%2520Vectors%2520by%2520Targeting%2520Sparse%2520Autoencoder%2520Features%26entry.906535625%3DSviatoslav%2520Chalnev%2520and%2520Matthew%2520Siu%2520and%2520Arthur%2520Conmy%26entry.1292438233%3D%2520%2520To%2520control%2520the%2520behavior%2520of%2520language%2520models%252C%2520steering%2520methods%2520attempt%2520to%250Aensure%2520that%2520outputs%2520of%2520the%2520model%2520satisfy%2520specific%2520pre-defined%2520properties.%250AAdding%2520steering%2520vectors%2520to%2520the%2520model%2520is%2520a%2520promising%2520method%2520of%2520model%2520control%250Athat%2520is%2520easier%2520than%2520finetuning%252C%2520and%2520may%2520be%2520more%2520robust%2520than%2520prompting.%2520However%252C%250Ait%2520can%2520be%2520difficult%2520to%2520anticipate%2520the%2520effects%2520of%2520steering%2520vectors%2520produced%2520by%250Aalmost%2520all%2520existing%2520methods%252C%2520such%2520as%2520CAA%2520%2528Panickssery%2520et%2520al.%252C%25202024%2529%2520or%2520the%250Adirect%2520use%2520of%2520SAE%2520latents%2520%2528Templeton%2520et%2520al.%252C%25202024%2529.%2520In%2520our%2520work%252C%2520we%2520address%250Athis%2520issue%2520by%2520using%2520SAEs%2520to%2520measure%2520the%2520effects%2520of%2520steering%2520vectors%252C%2520giving%2520us%250Aa%2520method%2520that%2520can%2520be%2520used%2520to%2520understand%2520the%2520causal%2520effect%2520of%2520any%2520steering%250Avector%2520intervention.%2520We%2520use%2520this%2520method%2520for%2520measuring%2520causal%2520effects%2520to%2520develop%250Aan%2520improved%2520steering%2520method%252C%2520SAE-Targeted%2520Steering%2520%2528SAE-TS%2529%252C%2520which%2520finds%250Asteering%2520vectors%2520to%2520target%2520specific%2520SAE%2520features%2520while%2520minimizing%2520unintended%250Aside%2520effects.%2520We%2520show%2520that%2520overall%252C%2520SAE-TS%2520balances%2520steering%2520effects%2520with%250Acoherence%2520better%2520than%2520CAA%2520and%2520SAE%2520feature%2520steering%252C%2520when%2520evaluated%2520on%2520a%2520range%250Aof%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Steering%20Vectors%20by%20Targeting%20Sparse%20Autoencoder%20Features&entry.906535625=Sviatoslav%20Chalnev%20and%20Matthew%20Siu%20and%20Arthur%20Conmy&entry.1292438233=%20%20To%20control%20the%20behavior%20of%20language%20models%2C%20steering%20methods%20attempt%20to%0Aensure%20that%20outputs%20of%20the%20model%20satisfy%20specific%20pre-defined%20properties.%0AAdding%20steering%20vectors%20to%20the%20model%20is%20a%20promising%20method%20of%20model%20control%0Athat%20is%20easier%20than%20finetuning%2C%20and%20may%20be%20more%20robust%20than%20prompting.%20However%2C%0Ait%20can%20be%20difficult%20to%20anticipate%20the%20effects%20of%20steering%20vectors%20produced%20by%0Aalmost%20all%20existing%20methods%2C%20such%20as%20CAA%20%28Panickssery%20et%20al.%2C%202024%29%20or%20the%0Adirect%20use%20of%20SAE%20latents%20%28Templeton%20et%20al.%2C%202024%29.%20In%20our%20work%2C%20we%20address%0Athis%20issue%20by%20using%20SAEs%20to%20measure%20the%20effects%20of%20steering%20vectors%2C%20giving%20us%0Aa%20method%20that%20can%20be%20used%20to%20understand%20the%20causal%20effect%20of%20any%20steering%0Avector%20intervention.%20We%20use%20this%20method%20for%20measuring%20causal%20effects%20to%20develop%0Aan%20improved%20steering%20method%2C%20SAE-Targeted%20Steering%20%28SAE-TS%29%2C%20which%20finds%0Asteering%20vectors%20to%20target%20specific%20SAE%20features%20while%20minimizing%20unintended%0Aside%20effects.%20We%20show%20that%20overall%2C%20SAE-TS%20balances%20steering%20effects%20with%0Acoherence%20better%20than%20CAA%20and%20SAE%20feature%20steering%2C%20when%20evaluated%20on%20a%20range%0Aof%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02193v1&entry.124074799=Read"},
{"title": "Tool Learning with Large Language Models: A Survey", "author": "Changle Qu and Sunhao Dai and Xiaochi Wei and Hengyi Cai and Shuaiqiang Wang and Dawei Yin and Jun Xu and Ji-Rong Wen", "abstract": "  Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.\n", "link": "http://arxiv.org/abs/2405.17935v3", "date": "2024-11-04", "relevancy": 2.4411, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tool%20Learning%20with%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Tool%20Learning%20with%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Changle%20Qu%20and%20Sunhao%20Dai%20and%20Xiaochi%20Wei%20and%20Hengyi%20Cai%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%20and%20Jun%20Xu%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Recently%2C%20tool%20learning%20with%20large%20language%20models%20%28LLMs%29%20has%20emerged%20as%20a%0Apromising%20paradigm%20for%20augmenting%20the%20capabilities%20of%20LLMs%20to%20tackle%20highly%0Acomplex%20problems.%20Despite%20growing%20attention%20and%20rapid%20advancements%20in%20this%0Afield%2C%20the%20existing%20literature%20remains%20fragmented%20and%20lacks%20systematic%0Aorganization%2C%20posing%20barriers%20to%20entry%20for%20newcomers.%20This%20gap%20motivates%20us%20to%0Aconduct%20a%20comprehensive%20survey%20of%20existing%20works%20on%20tool%20learning%20with%20LLMs.%20In%0Athis%20survey%2C%20we%20focus%20on%20reviewing%20existing%20literature%20from%20the%20two%20primary%0Aaspects%20%281%29%20why%20tool%20learning%20is%20beneficial%20and%20%282%29%20how%20tool%20learning%20is%0Aimplemented%2C%20enabling%20a%20comprehensive%20understanding%20of%20tool%20learning%20with%20LLMs.%0AWe%20first%20explore%20the%20%22why%22%20by%20reviewing%20both%20the%20benefits%20of%20tool%20integration%0Aand%20the%20inherent%20benefits%20of%20the%20tool%20learning%20paradigm%20from%20six%20specific%0Aaspects.%20In%20terms%20of%20%22how%22%2C%20we%20systematically%20review%20the%20literature%20according%0Ato%20a%20taxonomy%20of%20four%20key%20stages%20in%20the%20tool%20learning%20workflow%3A%20task%20planning%2C%0Atool%20selection%2C%20tool%20calling%2C%20and%20response%20generation.%20Additionally%2C%20we%20provide%0Aa%20detailed%20summary%20of%20existing%20benchmarks%20and%20evaluation%20methods%2C%20categorizing%0Athem%20according%20to%20their%20relevance%20to%20different%20stages.%20Finally%2C%20we%20discuss%0Acurrent%20challenges%20and%20outline%20potential%20future%20directions%2C%20aiming%20to%20inspire%0Aboth%20researchers%20and%20industrial%20developers%20to%20further%20explore%20this%20emerging%20and%0Apromising%20area.%20We%20also%20maintain%20a%20GitHub%20repository%20to%20continually%20keep%20track%0Aof%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%20area%20at%0Ahttps%3A//github.com/quchangle1/LLM-Tool-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17935v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTool%2520Learning%2520with%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DChangle%2520Qu%2520and%2520Sunhao%2520Dai%2520and%2520Xiaochi%2520Wei%2520and%2520Hengyi%2520Cai%2520and%2520Shuaiqiang%2520Wang%2520and%2520Dawei%2520Yin%2520and%2520Jun%2520Xu%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520tool%2520learning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520paradigm%2520for%2520augmenting%2520the%2520capabilities%2520of%2520LLMs%2520to%2520tackle%2520highly%250Acomplex%2520problems.%2520Despite%2520growing%2520attention%2520and%2520rapid%2520advancements%2520in%2520this%250Afield%252C%2520the%2520existing%2520literature%2520remains%2520fragmented%2520and%2520lacks%2520systematic%250Aorganization%252C%2520posing%2520barriers%2520to%2520entry%2520for%2520newcomers.%2520This%2520gap%2520motivates%2520us%2520to%250Aconduct%2520a%2520comprehensive%2520survey%2520of%2520existing%2520works%2520on%2520tool%2520learning%2520with%2520LLMs.%2520In%250Athis%2520survey%252C%2520we%2520focus%2520on%2520reviewing%2520existing%2520literature%2520from%2520the%2520two%2520primary%250Aaspects%2520%25281%2529%2520why%2520tool%2520learning%2520is%2520beneficial%2520and%2520%25282%2529%2520how%2520tool%2520learning%2520is%250Aimplemented%252C%2520enabling%2520a%2520comprehensive%2520understanding%2520of%2520tool%2520learning%2520with%2520LLMs.%250AWe%2520first%2520explore%2520the%2520%2522why%2522%2520by%2520reviewing%2520both%2520the%2520benefits%2520of%2520tool%2520integration%250Aand%2520the%2520inherent%2520benefits%2520of%2520the%2520tool%2520learning%2520paradigm%2520from%2520six%2520specific%250Aaspects.%2520In%2520terms%2520of%2520%2522how%2522%252C%2520we%2520systematically%2520review%2520the%2520literature%2520according%250Ato%2520a%2520taxonomy%2520of%2520four%2520key%2520stages%2520in%2520the%2520tool%2520learning%2520workflow%253A%2520task%2520planning%252C%250Atool%2520selection%252C%2520tool%2520calling%252C%2520and%2520response%2520generation.%2520Additionally%252C%2520we%2520provide%250Aa%2520detailed%2520summary%2520of%2520existing%2520benchmarks%2520and%2520evaluation%2520methods%252C%2520categorizing%250Athem%2520according%2520to%2520their%2520relevance%2520to%2520different%2520stages.%2520Finally%252C%2520we%2520discuss%250Acurrent%2520challenges%2520and%2520outline%2520potential%2520future%2520directions%252C%2520aiming%2520to%2520inspire%250Aboth%2520researchers%2520and%2520industrial%2520developers%2520to%2520further%2520explore%2520this%2520emerging%2520and%250Apromising%2520area.%2520We%2520also%2520maintain%2520a%2520GitHub%2520repository%2520to%2520continually%2520keep%2520track%250Aof%2520the%2520relevant%2520papers%2520and%2520resources%2520in%2520this%2520rising%2520area%2520at%250Ahttps%253A//github.com/quchangle1/LLM-Tool-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17935v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tool%20Learning%20with%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Changle%20Qu%20and%20Sunhao%20Dai%20and%20Xiaochi%20Wei%20and%20Hengyi%20Cai%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%20and%20Jun%20Xu%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Recently%2C%20tool%20learning%20with%20large%20language%20models%20%28LLMs%29%20has%20emerged%20as%20a%0Apromising%20paradigm%20for%20augmenting%20the%20capabilities%20of%20LLMs%20to%20tackle%20highly%0Acomplex%20problems.%20Despite%20growing%20attention%20and%20rapid%20advancements%20in%20this%0Afield%2C%20the%20existing%20literature%20remains%20fragmented%20and%20lacks%20systematic%0Aorganization%2C%20posing%20barriers%20to%20entry%20for%20newcomers.%20This%20gap%20motivates%20us%20to%0Aconduct%20a%20comprehensive%20survey%20of%20existing%20works%20on%20tool%20learning%20with%20LLMs.%20In%0Athis%20survey%2C%20we%20focus%20on%20reviewing%20existing%20literature%20from%20the%20two%20primary%0Aaspects%20%281%29%20why%20tool%20learning%20is%20beneficial%20and%20%282%29%20how%20tool%20learning%20is%0Aimplemented%2C%20enabling%20a%20comprehensive%20understanding%20of%20tool%20learning%20with%20LLMs.%0AWe%20first%20explore%20the%20%22why%22%20by%20reviewing%20both%20the%20benefits%20of%20tool%20integration%0Aand%20the%20inherent%20benefits%20of%20the%20tool%20learning%20paradigm%20from%20six%20specific%0Aaspects.%20In%20terms%20of%20%22how%22%2C%20we%20systematically%20review%20the%20literature%20according%0Ato%20a%20taxonomy%20of%20four%20key%20stages%20in%20the%20tool%20learning%20workflow%3A%20task%20planning%2C%0Atool%20selection%2C%20tool%20calling%2C%20and%20response%20generation.%20Additionally%2C%20we%20provide%0Aa%20detailed%20summary%20of%20existing%20benchmarks%20and%20evaluation%20methods%2C%20categorizing%0Athem%20according%20to%20their%20relevance%20to%20different%20stages.%20Finally%2C%20we%20discuss%0Acurrent%20challenges%20and%20outline%20potential%20future%20directions%2C%20aiming%20to%20inspire%0Aboth%20researchers%20and%20industrial%20developers%20to%20further%20explore%20this%20emerging%20and%0Apromising%20area.%20We%20also%20maintain%20a%20GitHub%20repository%20to%20continually%20keep%20track%0Aof%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%20area%20at%0Ahttps%3A//github.com/quchangle1/LLM-Tool-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17935v3&entry.124074799=Read"},
{"title": "Collaborative Cognitive Diagnosis with Disentangled Representation\n  Learning for Learner Modeling", "author": "Weibo Gao and Qi Liu and Linan Yue and Fangzhou Yao and Hao Wang and Yin Gu and Zheng Zhang", "abstract": "  Learners sharing similar implicit cognitive states often display comparable\nobservable problem-solving performances. Leveraging collaborative connections\namong such similar learners proves valuable in comprehending human learning.\nMotivated by the success of collaborative modeling in various domains, such as\nrecommender systems, we aim to investigate how collaborative signals among\nlearners contribute to the diagnosis of human cognitive states (i.e., knowledge\nproficiency) in the context of intelligent education. The primary challenges\nlie in identifying implicit collaborative connections and disentangling the\nentangled cognitive factors of learners for improved explainability and\ncontrollability in learner Cognitive Diagnosis (CD). However, there has been no\nwork on CD capable of simultaneously modeling collaborative and disentangled\ncognitive states. To address this gap, we present Coral, a Collaborative\ncognitive diagnosis model with disentangled representation learning.\nSpecifically, Coral first introduces a disentangled state encoder to achieve\nthe initial disentanglement of learners' states. Subsequently, a meticulously\ndesigned collaborative representation learning procedure captures collaborative\nsignals. It dynamically constructs a collaborative graph of learners by\niteratively searching for optimal neighbors in a context-aware manner. Using\nthe constructed graph, collaborative information is extracted through node\nrepresentation learning. Finally, a decoding process aligns the initial\ncognitive states and collaborative states, achieving co-disentanglement with\npractice performance reconstructions. Extensive experiments demonstrate the\nsuperior performance of Coral, showcasing significant improvements over\nstate-of-the-art methods across several real-world datasets. Our code is\navailable at https://github.com/bigdata-ustc/Coral.\n", "link": "http://arxiv.org/abs/2411.02066v1", "date": "2024-11-04", "relevancy": 2.4381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Cognitive%20Diagnosis%20with%20Disentangled%20Representation%0A%20%20Learning%20for%20Learner%20Modeling&body=Title%3A%20Collaborative%20Cognitive%20Diagnosis%20with%20Disentangled%20Representation%0A%20%20Learning%20for%20Learner%20Modeling%0AAuthor%3A%20Weibo%20Gao%20and%20Qi%20Liu%20and%20Linan%20Yue%20and%20Fangzhou%20Yao%20and%20Hao%20Wang%20and%20Yin%20Gu%20and%20Zheng%20Zhang%0AAbstract%3A%20%20%20Learners%20sharing%20similar%20implicit%20cognitive%20states%20often%20display%20comparable%0Aobservable%20problem-solving%20performances.%20Leveraging%20collaborative%20connections%0Aamong%20such%20similar%20learners%20proves%20valuable%20in%20comprehending%20human%20learning.%0AMotivated%20by%20the%20success%20of%20collaborative%20modeling%20in%20various%20domains%2C%20such%20as%0Arecommender%20systems%2C%20we%20aim%20to%20investigate%20how%20collaborative%20signals%20among%0Alearners%20contribute%20to%20the%20diagnosis%20of%20human%20cognitive%20states%20%28i.e.%2C%20knowledge%0Aproficiency%29%20in%20the%20context%20of%20intelligent%20education.%20The%20primary%20challenges%0Alie%20in%20identifying%20implicit%20collaborative%20connections%20and%20disentangling%20the%0Aentangled%20cognitive%20factors%20of%20learners%20for%20improved%20explainability%20and%0Acontrollability%20in%20learner%20Cognitive%20Diagnosis%20%28CD%29.%20However%2C%20there%20has%20been%20no%0Awork%20on%20CD%20capable%20of%20simultaneously%20modeling%20collaborative%20and%20disentangled%0Acognitive%20states.%20To%20address%20this%20gap%2C%20we%20present%20Coral%2C%20a%20Collaborative%0Acognitive%20diagnosis%20model%20with%20disentangled%20representation%20learning.%0ASpecifically%2C%20Coral%20first%20introduces%20a%20disentangled%20state%20encoder%20to%20achieve%0Athe%20initial%20disentanglement%20of%20learners%27%20states.%20Subsequently%2C%20a%20meticulously%0Adesigned%20collaborative%20representation%20learning%20procedure%20captures%20collaborative%0Asignals.%20It%20dynamically%20constructs%20a%20collaborative%20graph%20of%20learners%20by%0Aiteratively%20searching%20for%20optimal%20neighbors%20in%20a%20context-aware%20manner.%20Using%0Athe%20constructed%20graph%2C%20collaborative%20information%20is%20extracted%20through%20node%0Arepresentation%20learning.%20Finally%2C%20a%20decoding%20process%20aligns%20the%20initial%0Acognitive%20states%20and%20collaborative%20states%2C%20achieving%20co-disentanglement%20with%0Apractice%20performance%20reconstructions.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20Coral%2C%20showcasing%20significant%20improvements%20over%0Astate-of-the-art%20methods%20across%20several%20real-world%20datasets.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bigdata-ustc/Coral.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Cognitive%2520Diagnosis%2520with%2520Disentangled%2520Representation%250A%2520%2520Learning%2520for%2520Learner%2520Modeling%26entry.906535625%3DWeibo%2520Gao%2520and%2520Qi%2520Liu%2520and%2520Linan%2520Yue%2520and%2520Fangzhou%2520Yao%2520and%2520Hao%2520Wang%2520and%2520Yin%2520Gu%2520and%2520Zheng%2520Zhang%26entry.1292438233%3D%2520%2520Learners%2520sharing%2520similar%2520implicit%2520cognitive%2520states%2520often%2520display%2520comparable%250Aobservable%2520problem-solving%2520performances.%2520Leveraging%2520collaborative%2520connections%250Aamong%2520such%2520similar%2520learners%2520proves%2520valuable%2520in%2520comprehending%2520human%2520learning.%250AMotivated%2520by%2520the%2520success%2520of%2520collaborative%2520modeling%2520in%2520various%2520domains%252C%2520such%2520as%250Arecommender%2520systems%252C%2520we%2520aim%2520to%2520investigate%2520how%2520collaborative%2520signals%2520among%250Alearners%2520contribute%2520to%2520the%2520diagnosis%2520of%2520human%2520cognitive%2520states%2520%2528i.e.%252C%2520knowledge%250Aproficiency%2529%2520in%2520the%2520context%2520of%2520intelligent%2520education.%2520The%2520primary%2520challenges%250Alie%2520in%2520identifying%2520implicit%2520collaborative%2520connections%2520and%2520disentangling%2520the%250Aentangled%2520cognitive%2520factors%2520of%2520learners%2520for%2520improved%2520explainability%2520and%250Acontrollability%2520in%2520learner%2520Cognitive%2520Diagnosis%2520%2528CD%2529.%2520However%252C%2520there%2520has%2520been%2520no%250Awork%2520on%2520CD%2520capable%2520of%2520simultaneously%2520modeling%2520collaborative%2520and%2520disentangled%250Acognitive%2520states.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520Coral%252C%2520a%2520Collaborative%250Acognitive%2520diagnosis%2520model%2520with%2520disentangled%2520representation%2520learning.%250ASpecifically%252C%2520Coral%2520first%2520introduces%2520a%2520disentangled%2520state%2520encoder%2520to%2520achieve%250Athe%2520initial%2520disentanglement%2520of%2520learners%2527%2520states.%2520Subsequently%252C%2520a%2520meticulously%250Adesigned%2520collaborative%2520representation%2520learning%2520procedure%2520captures%2520collaborative%250Asignals.%2520It%2520dynamically%2520constructs%2520a%2520collaborative%2520graph%2520of%2520learners%2520by%250Aiteratively%2520searching%2520for%2520optimal%2520neighbors%2520in%2520a%2520context-aware%2520manner.%2520Using%250Athe%2520constructed%2520graph%252C%2520collaborative%2520information%2520is%2520extracted%2520through%2520node%250Arepresentation%2520learning.%2520Finally%252C%2520a%2520decoding%2520process%2520aligns%2520the%2520initial%250Acognitive%2520states%2520and%2520collaborative%2520states%252C%2520achieving%2520co-disentanglement%2520with%250Apractice%2520performance%2520reconstructions.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520Coral%252C%2520showcasing%2520significant%2520improvements%2520over%250Astate-of-the-art%2520methods%2520across%2520several%2520real-world%2520datasets.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/bigdata-ustc/Coral.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Cognitive%20Diagnosis%20with%20Disentangled%20Representation%0A%20%20Learning%20for%20Learner%20Modeling&entry.906535625=Weibo%20Gao%20and%20Qi%20Liu%20and%20Linan%20Yue%20and%20Fangzhou%20Yao%20and%20Hao%20Wang%20and%20Yin%20Gu%20and%20Zheng%20Zhang&entry.1292438233=%20%20Learners%20sharing%20similar%20implicit%20cognitive%20states%20often%20display%20comparable%0Aobservable%20problem-solving%20performances.%20Leveraging%20collaborative%20connections%0Aamong%20such%20similar%20learners%20proves%20valuable%20in%20comprehending%20human%20learning.%0AMotivated%20by%20the%20success%20of%20collaborative%20modeling%20in%20various%20domains%2C%20such%20as%0Arecommender%20systems%2C%20we%20aim%20to%20investigate%20how%20collaborative%20signals%20among%0Alearners%20contribute%20to%20the%20diagnosis%20of%20human%20cognitive%20states%20%28i.e.%2C%20knowledge%0Aproficiency%29%20in%20the%20context%20of%20intelligent%20education.%20The%20primary%20challenges%0Alie%20in%20identifying%20implicit%20collaborative%20connections%20and%20disentangling%20the%0Aentangled%20cognitive%20factors%20of%20learners%20for%20improved%20explainability%20and%0Acontrollability%20in%20learner%20Cognitive%20Diagnosis%20%28CD%29.%20However%2C%20there%20has%20been%20no%0Awork%20on%20CD%20capable%20of%20simultaneously%20modeling%20collaborative%20and%20disentangled%0Acognitive%20states.%20To%20address%20this%20gap%2C%20we%20present%20Coral%2C%20a%20Collaborative%0Acognitive%20diagnosis%20model%20with%20disentangled%20representation%20learning.%0ASpecifically%2C%20Coral%20first%20introduces%20a%20disentangled%20state%20encoder%20to%20achieve%0Athe%20initial%20disentanglement%20of%20learners%27%20states.%20Subsequently%2C%20a%20meticulously%0Adesigned%20collaborative%20representation%20learning%20procedure%20captures%20collaborative%0Asignals.%20It%20dynamically%20constructs%20a%20collaborative%20graph%20of%20learners%20by%0Aiteratively%20searching%20for%20optimal%20neighbors%20in%20a%20context-aware%20manner.%20Using%0Athe%20constructed%20graph%2C%20collaborative%20information%20is%20extracted%20through%20node%0Arepresentation%20learning.%20Finally%2C%20a%20decoding%20process%20aligns%20the%20initial%0Acognitive%20states%20and%20collaborative%20states%2C%20achieving%20co-disentanglement%20with%0Apractice%20performance%20reconstructions.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20Coral%2C%20showcasing%20significant%20improvements%20over%0Astate-of-the-art%20methods%20across%20several%20real-world%20datasets.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bigdata-ustc/Coral.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02066v1&entry.124074799=Read"},
{"title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent", "author": "Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang", "abstract": "  In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large\n", "link": "http://arxiv.org/abs/2411.02265v1", "date": "2024-11-04", "relevancy": 2.4342, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan-Large%3A%20An%20Open-Source%20MoE%20Model%20with%2052%20Billion%20Activated%0A%20%20Parameters%20by%20Tencent&body=Title%3A%20Hunyuan-Large%3A%20An%20Open-Source%20MoE%20Model%20with%2052%20Billion%20Activated%0A%20%20Parameters%20by%20Tencent%0AAuthor%3A%20Xingwu%20Sun%20and%20Yanfeng%20Chen%20and%20Yiqing%20Huang%20and%20Ruobing%20Xie%20and%20Jiaqi%20Zhu%20and%20Kai%20Zhang%20and%20Shuaipeng%20Li%20and%20Zhen%20Yang%20and%20Jonny%20Han%20and%20Xiaobo%20Shu%20and%20Jiahao%20Bu%20and%20Zhongzhi%20Chen%20and%20Xuemeng%20Huang%20and%20Fengzong%20Lian%20and%20Saiyong%20Yang%20and%20Jianfeng%20Yan%20and%20Yuyuan%20Zeng%20and%20Xiaoqin%20Ren%20and%20Chao%20Yu%20and%20Lulu%20Wu%20and%20Yue%20Mao%20and%20Tao%20Yang%20and%20Suncong%20Zheng%20and%20Kan%20Wu%20and%20Dian%20Jiao%20and%20Jinbao%20Xue%20and%20Xipeng%20Zhang%20and%20Decheng%20Wu%20and%20Kai%20Liu%20and%20Dengpeng%20Wu%20and%20Guanghui%20Xu%20and%20Shaohua%20Chen%20and%20Shuang%20Chen%20and%20Xiao%20Feng%20and%20Yigeng%20Hong%20and%20Junqiang%20Zheng%20and%20Chengcheng%20Xu%20and%20Zongwei%20Li%20and%20Xiong%20Kuang%20and%20Jianglu%20Hu%20and%20Yiqi%20Chen%20and%20Yuchi%20Deng%20and%20Guiyang%20Li%20and%20Ao%20Liu%20and%20Chenchen%20Zhang%20and%20Shihui%20Hu%20and%20Zilong%20Zhao%20and%20Zifan%20Wu%20and%20Yao%20Ding%20and%20Weichao%20Wang%20and%20Han%20Liu%20and%20Roberts%20Wang%20and%20Hao%20Fei%20and%20Peijie%20She%20and%20Ze%20Zhao%20and%20Xun%20Cao%20and%20Hai%20Wang%20and%20Fusheng%20Xiang%20and%20Mengyuan%20Huang%20and%20Zhiyuan%20Xiong%20and%20Bin%20Hu%20and%20Xuebin%20Hou%20and%20Lei%20Jiang%20and%20Jiajia%20Wu%20and%20Yaping%20Deng%20and%20Yi%20Shen%20and%20Qian%20Wang%20and%20Weijie%20Liu%20and%20Jie%20Liu%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Weiwen%20Jia%20and%20Hu%20Chen%20and%20Feifei%20Liu%20and%20Rui%20Yuan%20and%20Huilin%20Xu%20and%20Zhenxiang%20Yan%20and%20Tengfei%20Cao%20and%20Zhichao%20Hu%20and%20Xinhua%20Feng%20and%20Dong%20Du%20and%20Tinghao%20She%20and%20Yangyu%20Tao%20and%20Feng%20Zhang%20and%20Jianchen%20Zhu%20and%20Chengzhong%20Xu%20and%20Xirui%20Li%20and%20Chong%20Zha%20and%20Wen%20Ouyang%20and%20Yinben%20Xia%20and%20Xiang%20Li%20and%20Zekun%20He%20and%20Rongpeng%20Chen%20and%20Jiawei%20Song%20and%20Ruibin%20Chen%20and%20Fan%20Jiang%20and%20Chongqing%20Zhao%20and%20Bo%20Wang%20and%20Hao%20Gong%20and%20Rong%20Gan%20and%20Winston%20Hu%20and%20Zhanhui%20Kang%20and%20Yong%20Yang%20and%20Yuhong%20Liu%20and%20Di%20Wang%20and%20Jie%20Jiang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Hunyuan-Large%2C%20which%20is%20currently%20the%20largest%0Aopen-source%20Transformer-based%20mixture%20of%20experts%20model%2C%20with%20a%20total%20of%20389%0Abillion%20parameters%20and%2052%20billion%20activation%20parameters%2C%20capable%20of%20handling%20up%0Ato%20256K%20tokens.%20We%20conduct%20a%20thorough%20evaluation%20of%20Hunyuan-Large%27s%20superior%0Aperformance%20across%20various%20benchmarks%20including%20language%20understanding%20and%0Ageneration%2C%20logical%20reasoning%2C%20mathematical%20problem-solving%2C%20coding%2C%0Along-context%2C%20and%20aggregated%20tasks%2C%20where%20it%20outperforms%20LLama3.1-70B%20and%0Aexhibits%20comparable%20performance%20when%20compared%20to%20the%20significantly%20larger%0ALLama3.1-405B%20model.%20Key%20practice%20of%20Hunyuan-Large%20include%20large-scale%0Asynthetic%20data%20that%20is%20orders%20larger%20than%20in%20previous%20literature%2C%20a%20mixed%0Aexpert%20routing%20strategy%2C%20a%20key-value%20cache%20compression%20technique%2C%20and%20an%0Aexpert-specific%20learning%20rate%20strategy.%20Additionally%2C%20we%20also%20investigate%20the%0Ascaling%20laws%20and%20learning%20rate%20schedule%20of%20mixture%20of%20experts%20models%2C%20providing%0Avaluable%20insights%20and%20guidances%20for%20future%20model%20development%20and%20optimization.%0AThe%20code%20and%20checkpoints%20of%20Hunyuan-Large%20are%20released%20to%20facilitate%20future%0Ainnovations%20and%20applications.%0A%20%20Codes%3A%20https%3A//github.com/Tencent/Hunyuan-Large%0A%20%20Models%3A%20https%3A//huggingface.co/tencent/Tencent-Hunyuan-Large%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan-Large%253A%2520An%2520Open-Source%2520MoE%2520Model%2520with%252052%2520Billion%2520Activated%250A%2520%2520Parameters%2520by%2520Tencent%26entry.906535625%3DXingwu%2520Sun%2520and%2520Yanfeng%2520Chen%2520and%2520Yiqing%2520Huang%2520and%2520Ruobing%2520Xie%2520and%2520Jiaqi%2520Zhu%2520and%2520Kai%2520Zhang%2520and%2520Shuaipeng%2520Li%2520and%2520Zhen%2520Yang%2520and%2520Jonny%2520Han%2520and%2520Xiaobo%2520Shu%2520and%2520Jiahao%2520Bu%2520and%2520Zhongzhi%2520Chen%2520and%2520Xuemeng%2520Huang%2520and%2520Fengzong%2520Lian%2520and%2520Saiyong%2520Yang%2520and%2520Jianfeng%2520Yan%2520and%2520Yuyuan%2520Zeng%2520and%2520Xiaoqin%2520Ren%2520and%2520Chao%2520Yu%2520and%2520Lulu%2520Wu%2520and%2520Yue%2520Mao%2520and%2520Tao%2520Yang%2520and%2520Suncong%2520Zheng%2520and%2520Kan%2520Wu%2520and%2520Dian%2520Jiao%2520and%2520Jinbao%2520Xue%2520and%2520Xipeng%2520Zhang%2520and%2520Decheng%2520Wu%2520and%2520Kai%2520Liu%2520and%2520Dengpeng%2520Wu%2520and%2520Guanghui%2520Xu%2520and%2520Shaohua%2520Chen%2520and%2520Shuang%2520Chen%2520and%2520Xiao%2520Feng%2520and%2520Yigeng%2520Hong%2520and%2520Junqiang%2520Zheng%2520and%2520Chengcheng%2520Xu%2520and%2520Zongwei%2520Li%2520and%2520Xiong%2520Kuang%2520and%2520Jianglu%2520Hu%2520and%2520Yiqi%2520Chen%2520and%2520Yuchi%2520Deng%2520and%2520Guiyang%2520Li%2520and%2520Ao%2520Liu%2520and%2520Chenchen%2520Zhang%2520and%2520Shihui%2520Hu%2520and%2520Zilong%2520Zhao%2520and%2520Zifan%2520Wu%2520and%2520Yao%2520Ding%2520and%2520Weichao%2520Wang%2520and%2520Han%2520Liu%2520and%2520Roberts%2520Wang%2520and%2520Hao%2520Fei%2520and%2520Peijie%2520She%2520and%2520Ze%2520Zhao%2520and%2520Xun%2520Cao%2520and%2520Hai%2520Wang%2520and%2520Fusheng%2520Xiang%2520and%2520Mengyuan%2520Huang%2520and%2520Zhiyuan%2520Xiong%2520and%2520Bin%2520Hu%2520and%2520Xuebin%2520Hou%2520and%2520Lei%2520Jiang%2520and%2520Jiajia%2520Wu%2520and%2520Yaping%2520Deng%2520and%2520Yi%2520Shen%2520and%2520Qian%2520Wang%2520and%2520Weijie%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Meng%2520Chen%2520and%2520Liang%2520Dong%2520and%2520Weiwen%2520Jia%2520and%2520Hu%2520Chen%2520and%2520Feifei%2520Liu%2520and%2520Rui%2520Yuan%2520and%2520Huilin%2520Xu%2520and%2520Zhenxiang%2520Yan%2520and%2520Tengfei%2520Cao%2520and%2520Zhichao%2520Hu%2520and%2520Xinhua%2520Feng%2520and%2520Dong%2520Du%2520and%2520Tinghao%2520She%2520and%2520Yangyu%2520Tao%2520and%2520Feng%2520Zhang%2520and%2520Jianchen%2520Zhu%2520and%2520Chengzhong%2520Xu%2520and%2520Xirui%2520Li%2520and%2520Chong%2520Zha%2520and%2520Wen%2520Ouyang%2520and%2520Yinben%2520Xia%2520and%2520Xiang%2520Li%2520and%2520Zekun%2520He%2520and%2520Rongpeng%2520Chen%2520and%2520Jiawei%2520Song%2520and%2520Ruibin%2520Chen%2520and%2520Fan%2520Jiang%2520and%2520Chongqing%2520Zhao%2520and%2520Bo%2520Wang%2520and%2520Hao%2520Gong%2520and%2520Rong%2520Gan%2520and%2520Winston%2520Hu%2520and%2520Zhanhui%2520Kang%2520and%2520Yong%2520Yang%2520and%2520Yuhong%2520Liu%2520and%2520Di%2520Wang%2520and%2520Jie%2520Jiang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Hunyuan-Large%252C%2520which%2520is%2520currently%2520the%2520largest%250Aopen-source%2520Transformer-based%2520mixture%2520of%2520experts%2520model%252C%2520with%2520a%2520total%2520of%2520389%250Abillion%2520parameters%2520and%252052%2520billion%2520activation%2520parameters%252C%2520capable%2520of%2520handling%2520up%250Ato%2520256K%2520tokens.%2520We%2520conduct%2520a%2520thorough%2520evaluation%2520of%2520Hunyuan-Large%2527s%2520superior%250Aperformance%2520across%2520various%2520benchmarks%2520including%2520language%2520understanding%2520and%250Ageneration%252C%2520logical%2520reasoning%252C%2520mathematical%2520problem-solving%252C%2520coding%252C%250Along-context%252C%2520and%2520aggregated%2520tasks%252C%2520where%2520it%2520outperforms%2520LLama3.1-70B%2520and%250Aexhibits%2520comparable%2520performance%2520when%2520compared%2520to%2520the%2520significantly%2520larger%250ALLama3.1-405B%2520model.%2520Key%2520practice%2520of%2520Hunyuan-Large%2520include%2520large-scale%250Asynthetic%2520data%2520that%2520is%2520orders%2520larger%2520than%2520in%2520previous%2520literature%252C%2520a%2520mixed%250Aexpert%2520routing%2520strategy%252C%2520a%2520key-value%2520cache%2520compression%2520technique%252C%2520and%2520an%250Aexpert-specific%2520learning%2520rate%2520strategy.%2520Additionally%252C%2520we%2520also%2520investigate%2520the%250Ascaling%2520laws%2520and%2520learning%2520rate%2520schedule%2520of%2520mixture%2520of%2520experts%2520models%252C%2520providing%250Avaluable%2520insights%2520and%2520guidances%2520for%2520future%2520model%2520development%2520and%2520optimization.%250AThe%2520code%2520and%2520checkpoints%2520of%2520Hunyuan-Large%2520are%2520released%2520to%2520facilitate%2520future%250Ainnovations%2520and%2520applications.%250A%2520%2520Codes%253A%2520https%253A//github.com/Tencent/Hunyuan-Large%250A%2520%2520Models%253A%2520https%253A//huggingface.co/tencent/Tencent-Hunyuan-Large%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan-Large%3A%20An%20Open-Source%20MoE%20Model%20with%2052%20Billion%20Activated%0A%20%20Parameters%20by%20Tencent&entry.906535625=Xingwu%20Sun%20and%20Yanfeng%20Chen%20and%20Yiqing%20Huang%20and%20Ruobing%20Xie%20and%20Jiaqi%20Zhu%20and%20Kai%20Zhang%20and%20Shuaipeng%20Li%20and%20Zhen%20Yang%20and%20Jonny%20Han%20and%20Xiaobo%20Shu%20and%20Jiahao%20Bu%20and%20Zhongzhi%20Chen%20and%20Xuemeng%20Huang%20and%20Fengzong%20Lian%20and%20Saiyong%20Yang%20and%20Jianfeng%20Yan%20and%20Yuyuan%20Zeng%20and%20Xiaoqin%20Ren%20and%20Chao%20Yu%20and%20Lulu%20Wu%20and%20Yue%20Mao%20and%20Tao%20Yang%20and%20Suncong%20Zheng%20and%20Kan%20Wu%20and%20Dian%20Jiao%20and%20Jinbao%20Xue%20and%20Xipeng%20Zhang%20and%20Decheng%20Wu%20and%20Kai%20Liu%20and%20Dengpeng%20Wu%20and%20Guanghui%20Xu%20and%20Shaohua%20Chen%20and%20Shuang%20Chen%20and%20Xiao%20Feng%20and%20Yigeng%20Hong%20and%20Junqiang%20Zheng%20and%20Chengcheng%20Xu%20and%20Zongwei%20Li%20and%20Xiong%20Kuang%20and%20Jianglu%20Hu%20and%20Yiqi%20Chen%20and%20Yuchi%20Deng%20and%20Guiyang%20Li%20and%20Ao%20Liu%20and%20Chenchen%20Zhang%20and%20Shihui%20Hu%20and%20Zilong%20Zhao%20and%20Zifan%20Wu%20and%20Yao%20Ding%20and%20Weichao%20Wang%20and%20Han%20Liu%20and%20Roberts%20Wang%20and%20Hao%20Fei%20and%20Peijie%20She%20and%20Ze%20Zhao%20and%20Xun%20Cao%20and%20Hai%20Wang%20and%20Fusheng%20Xiang%20and%20Mengyuan%20Huang%20and%20Zhiyuan%20Xiong%20and%20Bin%20Hu%20and%20Xuebin%20Hou%20and%20Lei%20Jiang%20and%20Jiajia%20Wu%20and%20Yaping%20Deng%20and%20Yi%20Shen%20and%20Qian%20Wang%20and%20Weijie%20Liu%20and%20Jie%20Liu%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Weiwen%20Jia%20and%20Hu%20Chen%20and%20Feifei%20Liu%20and%20Rui%20Yuan%20and%20Huilin%20Xu%20and%20Zhenxiang%20Yan%20and%20Tengfei%20Cao%20and%20Zhichao%20Hu%20and%20Xinhua%20Feng%20and%20Dong%20Du%20and%20Tinghao%20She%20and%20Yangyu%20Tao%20and%20Feng%20Zhang%20and%20Jianchen%20Zhu%20and%20Chengzhong%20Xu%20and%20Xirui%20Li%20and%20Chong%20Zha%20and%20Wen%20Ouyang%20and%20Yinben%20Xia%20and%20Xiang%20Li%20and%20Zekun%20He%20and%20Rongpeng%20Chen%20and%20Jiawei%20Song%20and%20Ruibin%20Chen%20and%20Fan%20Jiang%20and%20Chongqing%20Zhao%20and%20Bo%20Wang%20and%20Hao%20Gong%20and%20Rong%20Gan%20and%20Winston%20Hu%20and%20Zhanhui%20Kang%20and%20Yong%20Yang%20and%20Yuhong%20Liu%20and%20Di%20Wang%20and%20Jie%20Jiang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Hunyuan-Large%2C%20which%20is%20currently%20the%20largest%0Aopen-source%20Transformer-based%20mixture%20of%20experts%20model%2C%20with%20a%20total%20of%20389%0Abillion%20parameters%20and%2052%20billion%20activation%20parameters%2C%20capable%20of%20handling%20up%0Ato%20256K%20tokens.%20We%20conduct%20a%20thorough%20evaluation%20of%20Hunyuan-Large%27s%20superior%0Aperformance%20across%20various%20benchmarks%20including%20language%20understanding%20and%0Ageneration%2C%20logical%20reasoning%2C%20mathematical%20problem-solving%2C%20coding%2C%0Along-context%2C%20and%20aggregated%20tasks%2C%20where%20it%20outperforms%20LLama3.1-70B%20and%0Aexhibits%20comparable%20performance%20when%20compared%20to%20the%20significantly%20larger%0ALLama3.1-405B%20model.%20Key%20practice%20of%20Hunyuan-Large%20include%20large-scale%0Asynthetic%20data%20that%20is%20orders%20larger%20than%20in%20previous%20literature%2C%20a%20mixed%0Aexpert%20routing%20strategy%2C%20a%20key-value%20cache%20compression%20technique%2C%20and%20an%0Aexpert-specific%20learning%20rate%20strategy.%20Additionally%2C%20we%20also%20investigate%20the%0Ascaling%20laws%20and%20learning%20rate%20schedule%20of%20mixture%20of%20experts%20models%2C%20providing%0Avaluable%20insights%20and%20guidances%20for%20future%20model%20development%20and%20optimization.%0AThe%20code%20and%20checkpoints%20of%20Hunyuan-Large%20are%20released%20to%20facilitate%20future%0Ainnovations%20and%20applications.%0A%20%20Codes%3A%20https%3A//github.com/Tencent/Hunyuan-Large%0A%20%20Models%3A%20https%3A//huggingface.co/tencent/Tencent-Hunyuan-Large%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02265v1&entry.124074799=Read"},
{"title": "Enhancing ID-based Recommendation with Large Language Models", "author": "Lei Chen and Chen Gao and Xiaoyi Du and Hengliang Luo and Depeng Jin and Yong Li and Meng Wang", "abstract": "  Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data.\n", "link": "http://arxiv.org/abs/2411.02041v1", "date": "2024-11-04", "relevancy": 2.4248, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20ID-based%20Recommendation%20with%20Large%20Language%20Models&body=Title%3A%20Enhancing%20ID-based%20Recommendation%20with%20Large%20Language%20Models%0AAuthor%3A%20Lei%20Chen%20and%20Chen%20Gao%20and%20Xiaoyi%20Du%20and%20Hengliang%20Luo%20and%20Depeng%20Jin%20and%20Yong%20Li%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20garnered%20significant%20attention%20in%0Avarious%20domains%2C%20including%20recommendation%20systems.%20Recent%20research%20leverages%0Athe%20capabilities%20of%20LLMs%20to%20improve%20the%20performance%20and%20user%20modeling%20aspects%0Aof%20recommender%20systems.%20These%20studies%20primarily%20focus%20on%20utilizing%20LLMs%20to%0Ainterpret%20textual%20data%20in%20recommendation%20tasks.%20However%2C%20it%27s%20worth%20noting%20that%0Ain%20ID-based%20recommendations%2C%20textual%20data%20is%20absent%2C%20and%20only%20ID%20data%20is%0Aavailable.%20The%20untapped%20potential%20of%20LLMs%20for%20ID%20data%20within%20the%20ID-based%0Arecommendation%20paradigm%20remains%20relatively%20unexplored.%20To%20this%20end%2C%20we%0Aintroduce%20a%20pioneering%20approach%20called%20%22LLM%20for%20ID-based%20Recommendation%22%0A%28LLM4IDRec%29.%20This%20innovative%20approach%20integrates%20the%20capabilities%20of%20LLMs%20while%0Aexclusively%20relying%20on%20ID%20data%2C%20thus%20diverging%20from%20the%20previous%20reliance%20on%0Atextual%20data.%20The%20basic%20idea%20of%20LLM4IDRec%20is%20that%20by%20employing%20LLM%20to%20augment%0AID%20data%2C%20if%20augmented%20ID%20data%20can%20improve%20recommendation%20performance%2C%20it%0Ademonstrates%20the%20ability%20of%20LLM%20to%20interpret%20ID%20data%20effectively%2C%20exploring%20an%0Ainnovative%20way%20for%20the%20integration%20of%20LLM%20in%20ID-based%20recommendation.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20LLM4IDRec%20approach%20using%20three%20widely-used%0Adatasets.%20Our%20results%20demonstrate%20a%20notable%20improvement%20in%20recommendation%0Aperformance%2C%20with%20our%20approach%20consistently%20outperforming%20existing%20methods%20in%0AID-based%20recommendation%20by%20solely%20augmenting%20input%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520ID-based%2520Recommendation%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DLei%2520Chen%2520and%2520Chen%2520Gao%2520and%2520Xiaoyi%2520Du%2520and%2520Hengliang%2520Luo%2520and%2520Depeng%2520Jin%2520and%2520Yong%2520Li%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520garnered%2520significant%2520attention%2520in%250Avarious%2520domains%252C%2520including%2520recommendation%2520systems.%2520Recent%2520research%2520leverages%250Athe%2520capabilities%2520of%2520LLMs%2520to%2520improve%2520the%2520performance%2520and%2520user%2520modeling%2520aspects%250Aof%2520recommender%2520systems.%2520These%2520studies%2520primarily%2520focus%2520on%2520utilizing%2520LLMs%2520to%250Ainterpret%2520textual%2520data%2520in%2520recommendation%2520tasks.%2520However%252C%2520it%2527s%2520worth%2520noting%2520that%250Ain%2520ID-based%2520recommendations%252C%2520textual%2520data%2520is%2520absent%252C%2520and%2520only%2520ID%2520data%2520is%250Aavailable.%2520The%2520untapped%2520potential%2520of%2520LLMs%2520for%2520ID%2520data%2520within%2520the%2520ID-based%250Arecommendation%2520paradigm%2520remains%2520relatively%2520unexplored.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520pioneering%2520approach%2520called%2520%2522LLM%2520for%2520ID-based%2520Recommendation%2522%250A%2528LLM4IDRec%2529.%2520This%2520innovative%2520approach%2520integrates%2520the%2520capabilities%2520of%2520LLMs%2520while%250Aexclusively%2520relying%2520on%2520ID%2520data%252C%2520thus%2520diverging%2520from%2520the%2520previous%2520reliance%2520on%250Atextual%2520data.%2520The%2520basic%2520idea%2520of%2520LLM4IDRec%2520is%2520that%2520by%2520employing%2520LLM%2520to%2520augment%250AID%2520data%252C%2520if%2520augmented%2520ID%2520data%2520can%2520improve%2520recommendation%2520performance%252C%2520it%250Ademonstrates%2520the%2520ability%2520of%2520LLM%2520to%2520interpret%2520ID%2520data%2520effectively%252C%2520exploring%2520an%250Ainnovative%2520way%2520for%2520the%2520integration%2520of%2520LLM%2520in%2520ID-based%2520recommendation.%2520We%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520LLM4IDRec%2520approach%2520using%2520three%2520widely-used%250Adatasets.%2520Our%2520results%2520demonstrate%2520a%2520notable%2520improvement%2520in%2520recommendation%250Aperformance%252C%2520with%2520our%2520approach%2520consistently%2520outperforming%2520existing%2520methods%2520in%250AID-based%2520recommendation%2520by%2520solely%2520augmenting%2520input%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20ID-based%20Recommendation%20with%20Large%20Language%20Models&entry.906535625=Lei%20Chen%20and%20Chen%20Gao%20and%20Xiaoyi%20Du%20and%20Hengliang%20Luo%20and%20Depeng%20Jin%20and%20Yong%20Li%20and%20Meng%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20garnered%20significant%20attention%20in%0Avarious%20domains%2C%20including%20recommendation%20systems.%20Recent%20research%20leverages%0Athe%20capabilities%20of%20LLMs%20to%20improve%20the%20performance%20and%20user%20modeling%20aspects%0Aof%20recommender%20systems.%20These%20studies%20primarily%20focus%20on%20utilizing%20LLMs%20to%0Ainterpret%20textual%20data%20in%20recommendation%20tasks.%20However%2C%20it%27s%20worth%20noting%20that%0Ain%20ID-based%20recommendations%2C%20textual%20data%20is%20absent%2C%20and%20only%20ID%20data%20is%0Aavailable.%20The%20untapped%20potential%20of%20LLMs%20for%20ID%20data%20within%20the%20ID-based%0Arecommendation%20paradigm%20remains%20relatively%20unexplored.%20To%20this%20end%2C%20we%0Aintroduce%20a%20pioneering%20approach%20called%20%22LLM%20for%20ID-based%20Recommendation%22%0A%28LLM4IDRec%29.%20This%20innovative%20approach%20integrates%20the%20capabilities%20of%20LLMs%20while%0Aexclusively%20relying%20on%20ID%20data%2C%20thus%20diverging%20from%20the%20previous%20reliance%20on%0Atextual%20data.%20The%20basic%20idea%20of%20LLM4IDRec%20is%20that%20by%20employing%20LLM%20to%20augment%0AID%20data%2C%20if%20augmented%20ID%20data%20can%20improve%20recommendation%20performance%2C%20it%0Ademonstrates%20the%20ability%20of%20LLM%20to%20interpret%20ID%20data%20effectively%2C%20exploring%20an%0Ainnovative%20way%20for%20the%20integration%20of%20LLM%20in%20ID-based%20recommendation.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20LLM4IDRec%20approach%20using%20three%20widely-used%0Adatasets.%20Our%20results%20demonstrate%20a%20notable%20improvement%20in%20recommendation%0Aperformance%2C%20with%20our%20approach%20consistently%20outperforming%20existing%20methods%20in%0AID-based%20recommendation%20by%20solely%20augmenting%20input%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02041v1&entry.124074799=Read"},
{"title": "AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions", "author": "Hao-Yu Hsu and Zhi-Hao Lin and Albert Zhai and Hongchi Xia and Shenlong Wang", "abstract": "  Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.\n", "link": "http://arxiv.org/abs/2411.02394v1", "date": "2024-11-04", "relevancy": 2.4221, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6555}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5776}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoVFX%3A%20Physically%20Realistic%20Video%20Editing%20from%20Natural%20Language%0A%20%20Instructions&body=Title%3A%20AutoVFX%3A%20Physically%20Realistic%20Video%20Editing%20from%20Natural%20Language%0A%20%20Instructions%0AAuthor%3A%20Hao-Yu%20Hsu%20and%20Zhi-Hao%20Lin%20and%20Albert%20Zhai%20and%20Hongchi%20Xia%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20Modern%20visual%20effects%20%28VFX%29%20software%20has%20made%20it%20possible%20for%20skilled%20artists%0Ato%20create%20imagery%20of%20virtually%20anything.%20However%2C%20the%20creation%20process%20remains%0Alaborious%2C%20complex%2C%20and%20largely%20inaccessible%20to%20everyday%20users.%20In%20this%20work%2C%0Awe%20present%20AutoVFX%2C%20a%20framework%20that%20automatically%20creates%20realistic%20and%0Adynamic%20VFX%20videos%20from%20a%20single%20video%20and%20natural%20language%20instructions.%20By%0Acarefully%20integrating%20neural%20scene%20modeling%2C%20LLM-based%20code%20generation%2C%20and%0Aphysical%20simulation%2C%20AutoVFX%20is%20able%20to%20provide%20physically-grounded%2C%0Aphotorealistic%20editing%20effects%20that%20can%20be%20controlled%20directly%20using%20natural%0Alanguage%20instructions.%20We%20conduct%20extensive%20experiments%20to%20validate%20AutoVFX%27s%0Aefficacy%20across%20a%20diverse%20spectrum%20of%20videos%20and%20instructions.%20Quantitative%20and%0Aqualitative%20results%20suggest%20that%20AutoVFX%20outperforms%20all%20competing%20methods%20by%20a%0Alarge%20margin%20in%20generative%20quality%2C%20instruction%20alignment%2C%20editing%20versatility%2C%0Aand%20physical%20plausibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoVFX%253A%2520Physically%2520Realistic%2520Video%2520Editing%2520from%2520Natural%2520Language%250A%2520%2520Instructions%26entry.906535625%3DHao-Yu%2520Hsu%2520and%2520Zhi-Hao%2520Lin%2520and%2520Albert%2520Zhai%2520and%2520Hongchi%2520Xia%2520and%2520Shenlong%2520Wang%26entry.1292438233%3D%2520%2520Modern%2520visual%2520effects%2520%2528VFX%2529%2520software%2520has%2520made%2520it%2520possible%2520for%2520skilled%2520artists%250Ato%2520create%2520imagery%2520of%2520virtually%2520anything.%2520However%252C%2520the%2520creation%2520process%2520remains%250Alaborious%252C%2520complex%252C%2520and%2520largely%2520inaccessible%2520to%2520everyday%2520users.%2520In%2520this%2520work%252C%250Awe%2520present%2520AutoVFX%252C%2520a%2520framework%2520that%2520automatically%2520creates%2520realistic%2520and%250Adynamic%2520VFX%2520videos%2520from%2520a%2520single%2520video%2520and%2520natural%2520language%2520instructions.%2520By%250Acarefully%2520integrating%2520neural%2520scene%2520modeling%252C%2520LLM-based%2520code%2520generation%252C%2520and%250Aphysical%2520simulation%252C%2520AutoVFX%2520is%2520able%2520to%2520provide%2520physically-grounded%252C%250Aphotorealistic%2520editing%2520effects%2520that%2520can%2520be%2520controlled%2520directly%2520using%2520natural%250Alanguage%2520instructions.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520validate%2520AutoVFX%2527s%250Aefficacy%2520across%2520a%2520diverse%2520spectrum%2520of%2520videos%2520and%2520instructions.%2520Quantitative%2520and%250Aqualitative%2520results%2520suggest%2520that%2520AutoVFX%2520outperforms%2520all%2520competing%2520methods%2520by%2520a%250Alarge%2520margin%2520in%2520generative%2520quality%252C%2520instruction%2520alignment%252C%2520editing%2520versatility%252C%250Aand%2520physical%2520plausibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoVFX%3A%20Physically%20Realistic%20Video%20Editing%20from%20Natural%20Language%0A%20%20Instructions&entry.906535625=Hao-Yu%20Hsu%20and%20Zhi-Hao%20Lin%20and%20Albert%20Zhai%20and%20Hongchi%20Xia%20and%20Shenlong%20Wang&entry.1292438233=%20%20Modern%20visual%20effects%20%28VFX%29%20software%20has%20made%20it%20possible%20for%20skilled%20artists%0Ato%20create%20imagery%20of%20virtually%20anything.%20However%2C%20the%20creation%20process%20remains%0Alaborious%2C%20complex%2C%20and%20largely%20inaccessible%20to%20everyday%20users.%20In%20this%20work%2C%0Awe%20present%20AutoVFX%2C%20a%20framework%20that%20automatically%20creates%20realistic%20and%0Adynamic%20VFX%20videos%20from%20a%20single%20video%20and%20natural%20language%20instructions.%20By%0Acarefully%20integrating%20neural%20scene%20modeling%2C%20LLM-based%20code%20generation%2C%20and%0Aphysical%20simulation%2C%20AutoVFX%20is%20able%20to%20provide%20physically-grounded%2C%0Aphotorealistic%20editing%20effects%20that%20can%20be%20controlled%20directly%20using%20natural%0Alanguage%20instructions.%20We%20conduct%20extensive%20experiments%20to%20validate%20AutoVFX%27s%0Aefficacy%20across%20a%20diverse%20spectrum%20of%20videos%20and%20instructions.%20Quantitative%20and%0Aqualitative%20results%20suggest%20that%20AutoVFX%20outperforms%20all%20competing%20methods%20by%20a%0Alarge%20margin%20in%20generative%20quality%2C%20instruction%20alignment%2C%20editing%20versatility%2C%0Aand%20physical%20plausibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02394v1&entry.124074799=Read"},
{"title": "Shortcut Learning in In-Context Learning: A Survey", "author": "Rui Song and Yingji Li and Fausto Giunchiglia and Hao Xu", "abstract": "  Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.\n", "link": "http://arxiv.org/abs/2411.02018v1", "date": "2024-11-04", "relevancy": 2.3914, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shortcut%20Learning%20in%20In-Context%20Learning%3A%20A%20Survey&body=Title%3A%20Shortcut%20Learning%20in%20In-Context%20Learning%3A%20A%20Survey%0AAuthor%3A%20Rui%20Song%20and%20Yingji%20Li%20and%20Fausto%20Giunchiglia%20and%20Hao%20Xu%0AAbstract%3A%20%20%20Shortcut%20learning%20refers%20to%20the%20phenomenon%20where%20models%20employ%20simple%2C%0Anon-robust%20decision%20rules%20in%20practical%20tasks%2C%20which%20hinders%20their%0Ageneralization%20and%20robustness.%20With%20the%20rapid%20development%20of%20large%20language%0Amodels%20%28LLMs%29%20in%20recent%20years%2C%20an%20increasing%20number%20of%20studies%20have%20shown%20the%0Aimpact%20of%20shortcut%20learning%20on%20LLMs.%20This%20paper%20provides%20a%20novel%20perspective%20to%0Areview%20relevant%20research%20on%20shortcut%20learning%20in%20In-Context%20Learning%20%28ICL%29.%20It%0Aconducts%20a%20detailed%20exploration%20of%20the%20types%20of%20shortcuts%20in%20ICL%20tasks%2C%20their%0Acauses%2C%20available%20benchmarks%2C%20and%20strategies%20for%20mitigating%20shortcuts.%20Based%20on%0Acorresponding%20observations%2C%20it%20summarizes%20the%20unresolved%20issues%20in%20existing%0Aresearch%20and%20attempts%20to%20outline%20the%20future%20research%20landscape%20of%20shortcut%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShortcut%2520Learning%2520in%2520In-Context%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DRui%2520Song%2520and%2520Yingji%2520Li%2520and%2520Fausto%2520Giunchiglia%2520and%2520Hao%2520Xu%26entry.1292438233%3D%2520%2520Shortcut%2520learning%2520refers%2520to%2520the%2520phenomenon%2520where%2520models%2520employ%2520simple%252C%250Anon-robust%2520decision%2520rules%2520in%2520practical%2520tasks%252C%2520which%2520hinders%2520their%250Ageneralization%2520and%2520robustness.%2520With%2520the%2520rapid%2520development%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520in%2520recent%2520years%252C%2520an%2520increasing%2520number%2520of%2520studies%2520have%2520shown%2520the%250Aimpact%2520of%2520shortcut%2520learning%2520on%2520LLMs.%2520This%2520paper%2520provides%2520a%2520novel%2520perspective%2520to%250Areview%2520relevant%2520research%2520on%2520shortcut%2520learning%2520in%2520In-Context%2520Learning%2520%2528ICL%2529.%2520It%250Aconducts%2520a%2520detailed%2520exploration%2520of%2520the%2520types%2520of%2520shortcuts%2520in%2520ICL%2520tasks%252C%2520their%250Acauses%252C%2520available%2520benchmarks%252C%2520and%2520strategies%2520for%2520mitigating%2520shortcuts.%2520Based%2520on%250Acorresponding%2520observations%252C%2520it%2520summarizes%2520the%2520unresolved%2520issues%2520in%2520existing%250Aresearch%2520and%2520attempts%2520to%2520outline%2520the%2520future%2520research%2520landscape%2520of%2520shortcut%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shortcut%20Learning%20in%20In-Context%20Learning%3A%20A%20Survey&entry.906535625=Rui%20Song%20and%20Yingji%20Li%20and%20Fausto%20Giunchiglia%20and%20Hao%20Xu&entry.1292438233=%20%20Shortcut%20learning%20refers%20to%20the%20phenomenon%20where%20models%20employ%20simple%2C%0Anon-robust%20decision%20rules%20in%20practical%20tasks%2C%20which%20hinders%20their%0Ageneralization%20and%20robustness.%20With%20the%20rapid%20development%20of%20large%20language%0Amodels%20%28LLMs%29%20in%20recent%20years%2C%20an%20increasing%20number%20of%20studies%20have%20shown%20the%0Aimpact%20of%20shortcut%20learning%20on%20LLMs.%20This%20paper%20provides%20a%20novel%20perspective%20to%0Areview%20relevant%20research%20on%20shortcut%20learning%20in%20In-Context%20Learning%20%28ICL%29.%20It%0Aconducts%20a%20detailed%20exploration%20of%20the%20types%20of%20shortcuts%20in%20ICL%20tasks%2C%20their%0Acauses%2C%20available%20benchmarks%2C%20and%20strategies%20for%20mitigating%20shortcuts.%20Based%20on%0Acorresponding%20observations%2C%20it%20summarizes%20the%20unresolved%20issues%20in%20existing%0Aresearch%20and%20attempts%20to%20outline%20the%20future%20research%20landscape%20of%20shortcut%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02018v1&entry.124074799=Read"},
{"title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance", "author": "Ruyang Liu and Haoran Tang and Haibo Liu and Yixiao Ge and Ying Shan and Chen Li and Jiankun Yang", "abstract": "  The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.\n", "link": "http://arxiv.org/abs/2411.02327v1", "date": "2024-11-04", "relevancy": 2.3724, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPLLaVA%3A%20Varied%20Video%20Sequence%20Understanding%20With%20Prompt%20Guidance&body=Title%3A%20PPLLaVA%3A%20Varied%20Video%20Sequence%20Understanding%20With%20Prompt%20Guidance%0AAuthor%3A%20Ruyang%20Liu%20and%20Haoran%20Tang%20and%20Haibo%20Liu%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Chen%20Li%20and%20Jiankun%20Yang%0AAbstract%3A%20%20%20The%20past%20year%20has%20witnessed%20the%20significant%20advancement%20of%20video-based%20large%0Alanguage%20models.%20However%2C%20the%20challenge%20of%20developing%20a%20unified%20model%20for%20both%0Ashort%20and%20long%20video%20understanding%20remains%20unresolved.%20Most%20existing%20video%20LLMs%0Acannot%20handle%20hour-long%20videos%2C%20while%20methods%20custom%20for%20long%20videos%20tend%20to%20be%0Aineffective%20for%20shorter%20videos%20and%20images.%20In%20this%20paper%2C%20we%20identify%20the%20key%0Aissue%20as%20the%20redundant%20content%20in%20videos.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Apooling%20strategy%20that%20simultaneously%20achieves%20token%20compression%20and%0Ainstruction-aware%20visual%20feature%20aggregation.%20Our%20model%20is%20termed%20Prompt-guided%0APooling%20LLaVA%2C%20or%20PPLLaVA%20for%20short.%20Specifically%2C%20PPLLaVA%20consists%20of%20three%0Acore%20components%3A%20the%20CLIP-based%20visual-prompt%20alignment%20that%20extracts%20visual%0Ainformation%20relevant%20to%20the%20user%27s%20instructions%2C%20the%20prompt-guided%20pooling%20that%0Acompresses%20the%20visual%20sequence%20to%20arbitrary%20scales%20using%20convolution-style%0Apooling%2C%20and%20the%20clip%20context%20extension%20designed%20for%20lengthy%20prompt%20common%20in%0Avisual%20dialogue.%20Moreover%2C%20our%20codebase%20also%20integrates%20the%20most%20advanced%20video%0ADirect%20Preference%20Optimization%20%28DPO%29%20and%20visual%20interleave%20training.%20Extensive%0Aexperiments%20have%20validated%20the%20performance%20of%20our%20model.%20With%20superior%0Athroughput%20and%20only%201024%20visual%20context%2C%20PPLLaVA%20achieves%20better%20results%20on%0Aimage%20benchmarks%20as%20a%20video%20LLM%2C%20while%20achieving%20state-of-the-art%20performance%0Aacross%20various%20video%20benchmarks%2C%20excelling%20in%20tasks%20ranging%20from%20caption%0Ageneration%20to%20multiple-choice%20questions%2C%20and%20handling%20video%20lengths%20from%0Aseconds%20to%20hours.%20Codes%20have%20been%20available%20at%0Ahttps%3A//github.com/farewellthree/PPLLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPLLaVA%253A%2520Varied%2520Video%2520Sequence%2520Understanding%2520With%2520Prompt%2520Guidance%26entry.906535625%3DRuyang%2520Liu%2520and%2520Haoran%2520Tang%2520and%2520Haibo%2520Liu%2520and%2520Yixiao%2520Ge%2520and%2520Ying%2520Shan%2520and%2520Chen%2520Li%2520and%2520Jiankun%2520Yang%26entry.1292438233%3D%2520%2520The%2520past%2520year%2520has%2520witnessed%2520the%2520significant%2520advancement%2520of%2520video-based%2520large%250Alanguage%2520models.%2520However%252C%2520the%2520challenge%2520of%2520developing%2520a%2520unified%2520model%2520for%2520both%250Ashort%2520and%2520long%2520video%2520understanding%2520remains%2520unresolved.%2520Most%2520existing%2520video%2520LLMs%250Acannot%2520handle%2520hour-long%2520videos%252C%2520while%2520methods%2520custom%2520for%2520long%2520videos%2520tend%2520to%2520be%250Aineffective%2520for%2520shorter%2520videos%2520and%2520images.%2520In%2520this%2520paper%252C%2520we%2520identify%2520the%2520key%250Aissue%2520as%2520the%2520redundant%2520content%2520in%2520videos.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%250Apooling%2520strategy%2520that%2520simultaneously%2520achieves%2520token%2520compression%2520and%250Ainstruction-aware%2520visual%2520feature%2520aggregation.%2520Our%2520model%2520is%2520termed%2520Prompt-guided%250APooling%2520LLaVA%252C%2520or%2520PPLLaVA%2520for%2520short.%2520Specifically%252C%2520PPLLaVA%2520consists%2520of%2520three%250Acore%2520components%253A%2520the%2520CLIP-based%2520visual-prompt%2520alignment%2520that%2520extracts%2520visual%250Ainformation%2520relevant%2520to%2520the%2520user%2527s%2520instructions%252C%2520the%2520prompt-guided%2520pooling%2520that%250Acompresses%2520the%2520visual%2520sequence%2520to%2520arbitrary%2520scales%2520using%2520convolution-style%250Apooling%252C%2520and%2520the%2520clip%2520context%2520extension%2520designed%2520for%2520lengthy%2520prompt%2520common%2520in%250Avisual%2520dialogue.%2520Moreover%252C%2520our%2520codebase%2520also%2520integrates%2520the%2520most%2520advanced%2520video%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520visual%2520interleave%2520training.%2520Extensive%250Aexperiments%2520have%2520validated%2520the%2520performance%2520of%2520our%2520model.%2520With%2520superior%250Athroughput%2520and%2520only%25201024%2520visual%2520context%252C%2520PPLLaVA%2520achieves%2520better%2520results%2520on%250Aimage%2520benchmarks%2520as%2520a%2520video%2520LLM%252C%2520while%2520achieving%2520state-of-the-art%2520performance%250Aacross%2520various%2520video%2520benchmarks%252C%2520excelling%2520in%2520tasks%2520ranging%2520from%2520caption%250Ageneration%2520to%2520multiple-choice%2520questions%252C%2520and%2520handling%2520video%2520lengths%2520from%250Aseconds%2520to%2520hours.%2520Codes%2520have%2520been%2520available%2520at%250Ahttps%253A//github.com/farewellthree/PPLLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPLLaVA%3A%20Varied%20Video%20Sequence%20Understanding%20With%20Prompt%20Guidance&entry.906535625=Ruyang%20Liu%20and%20Haoran%20Tang%20and%20Haibo%20Liu%20and%20Yixiao%20Ge%20and%20Ying%20Shan%20and%20Chen%20Li%20and%20Jiankun%20Yang&entry.1292438233=%20%20The%20past%20year%20has%20witnessed%20the%20significant%20advancement%20of%20video-based%20large%0Alanguage%20models.%20However%2C%20the%20challenge%20of%20developing%20a%20unified%20model%20for%20both%0Ashort%20and%20long%20video%20understanding%20remains%20unresolved.%20Most%20existing%20video%20LLMs%0Acannot%20handle%20hour-long%20videos%2C%20while%20methods%20custom%20for%20long%20videos%20tend%20to%20be%0Aineffective%20for%20shorter%20videos%20and%20images.%20In%20this%20paper%2C%20we%20identify%20the%20key%0Aissue%20as%20the%20redundant%20content%20in%20videos.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Apooling%20strategy%20that%20simultaneously%20achieves%20token%20compression%20and%0Ainstruction-aware%20visual%20feature%20aggregation.%20Our%20model%20is%20termed%20Prompt-guided%0APooling%20LLaVA%2C%20or%20PPLLaVA%20for%20short.%20Specifically%2C%20PPLLaVA%20consists%20of%20three%0Acore%20components%3A%20the%20CLIP-based%20visual-prompt%20alignment%20that%20extracts%20visual%0Ainformation%20relevant%20to%20the%20user%27s%20instructions%2C%20the%20prompt-guided%20pooling%20that%0Acompresses%20the%20visual%20sequence%20to%20arbitrary%20scales%20using%20convolution-style%0Apooling%2C%20and%20the%20clip%20context%20extension%20designed%20for%20lengthy%20prompt%20common%20in%0Avisual%20dialogue.%20Moreover%2C%20our%20codebase%20also%20integrates%20the%20most%20advanced%20video%0ADirect%20Preference%20Optimization%20%28DPO%29%20and%20visual%20interleave%20training.%20Extensive%0Aexperiments%20have%20validated%20the%20performance%20of%20our%20model.%20With%20superior%0Athroughput%20and%20only%201024%20visual%20context%2C%20PPLLaVA%20achieves%20better%20results%20on%0Aimage%20benchmarks%20as%20a%20video%20LLM%2C%20while%20achieving%20state-of-the-art%20performance%0Aacross%20various%20video%20benchmarks%2C%20excelling%20in%20tasks%20ranging%20from%20caption%0Ageneration%20to%20multiple-choice%20questions%2C%20and%20handling%20video%20lengths%20from%0Aseconds%20to%20hours.%20Codes%20have%20been%20available%20at%0Ahttps%3A//github.com/farewellthree/PPLLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02327v1&entry.124074799=Read"},
{"title": "FilterViT and DropoutViT: Lightweight Vision Transformer Models for\n  Efficient Attention Mechanisms", "author": "Bohang Sun", "abstract": "  In this study, we introduce FilterViT, an enhanced version of MobileViT,\nwhich leverages an attention-based mechanism for early-stage downsampling.\nTraditional QKV operations on high-resolution feature maps are computationally\nintensive due to the abundance of tokens. To address this, we propose a filter\nattention mechanism using a convolutional neural network (CNN) to generate an\nimportance mask, focusing attention on key image regions. The method\nsignificantly reduces computational complexity while maintaining\ninterpretability, as it highlights essential image areas. Experimental results\nshow that FilterViT achieves substantial gains in both efficiency and accuracy\ncompared to other models. We also introduce DropoutViT, a variant that uses a\nstochastic approach for pixel selection, further enhancing robustness.\n", "link": "http://arxiv.org/abs/2410.22709v2", "date": "2024-11-04", "relevancy": 2.3469, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6502}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5441}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FilterViT%20and%20DropoutViT%3A%20Lightweight%20Vision%20Transformer%20Models%20for%0A%20%20Efficient%20Attention%20Mechanisms&body=Title%3A%20FilterViT%20and%20DropoutViT%3A%20Lightweight%20Vision%20Transformer%20Models%20for%0A%20%20Efficient%20Attention%20Mechanisms%0AAuthor%3A%20Bohang%20Sun%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20FilterViT%2C%20an%20enhanced%20version%20of%20MobileViT%2C%0Awhich%20leverages%20an%20attention-based%20mechanism%20for%20early-stage%20downsampling.%0ATraditional%20QKV%20operations%20on%20high-resolution%20feature%20maps%20are%20computationally%0Aintensive%20due%20to%20the%20abundance%20of%20tokens.%20To%20address%20this%2C%20we%20propose%20a%20filter%0Aattention%20mechanism%20using%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20generate%20an%0Aimportance%20mask%2C%20focusing%20attention%20on%20key%20image%20regions.%20The%20method%0Asignificantly%20reduces%20computational%20complexity%20while%20maintaining%0Ainterpretability%2C%20as%20it%20highlights%20essential%20image%20areas.%20Experimental%20results%0Ashow%20that%20FilterViT%20achieves%20substantial%20gains%20in%20both%20efficiency%20and%20accuracy%0Acompared%20to%20other%20models.%20We%20also%20introduce%20DropoutViT%2C%20a%20variant%20that%20uses%20a%0Astochastic%20approach%20for%20pixel%20selection%2C%20further%20enhancing%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFilterViT%2520and%2520DropoutViT%253A%2520Lightweight%2520Vision%2520Transformer%2520Models%2520for%250A%2520%2520Efficient%2520Attention%2520Mechanisms%26entry.906535625%3DBohang%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520FilterViT%252C%2520an%2520enhanced%2520version%2520of%2520MobileViT%252C%250Awhich%2520leverages%2520an%2520attention-based%2520mechanism%2520for%2520early-stage%2520downsampling.%250ATraditional%2520QKV%2520operations%2520on%2520high-resolution%2520feature%2520maps%2520are%2520computationally%250Aintensive%2520due%2520to%2520the%2520abundance%2520of%2520tokens.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520filter%250Aattention%2520mechanism%2520using%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520generate%2520an%250Aimportance%2520mask%252C%2520focusing%2520attention%2520on%2520key%2520image%2520regions.%2520The%2520method%250Asignificantly%2520reduces%2520computational%2520complexity%2520while%2520maintaining%250Ainterpretability%252C%2520as%2520it%2520highlights%2520essential%2520image%2520areas.%2520Experimental%2520results%250Ashow%2520that%2520FilterViT%2520achieves%2520substantial%2520gains%2520in%2520both%2520efficiency%2520and%2520accuracy%250Acompared%2520to%2520other%2520models.%2520We%2520also%2520introduce%2520DropoutViT%252C%2520a%2520variant%2520that%2520uses%2520a%250Astochastic%2520approach%2520for%2520pixel%2520selection%252C%2520further%2520enhancing%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FilterViT%20and%20DropoutViT%3A%20Lightweight%20Vision%20Transformer%20Models%20for%0A%20%20Efficient%20Attention%20Mechanisms&entry.906535625=Bohang%20Sun&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20FilterViT%2C%20an%20enhanced%20version%20of%20MobileViT%2C%0Awhich%20leverages%20an%20attention-based%20mechanism%20for%20early-stage%20downsampling.%0ATraditional%20QKV%20operations%20on%20high-resolution%20feature%20maps%20are%20computationally%0Aintensive%20due%20to%20the%20abundance%20of%20tokens.%20To%20address%20this%2C%20we%20propose%20a%20filter%0Aattention%20mechanism%20using%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20generate%20an%0Aimportance%20mask%2C%20focusing%20attention%20on%20key%20image%20regions.%20The%20method%0Asignificantly%20reduces%20computational%20complexity%20while%20maintaining%0Ainterpretability%2C%20as%20it%20highlights%20essential%20image%20areas.%20Experimental%20results%0Ashow%20that%20FilterViT%20achieves%20substantial%20gains%20in%20both%20efficiency%20and%20accuracy%0Acompared%20to%20other%20models.%20We%20also%20introduce%20DropoutViT%2C%20a%20variant%20that%20uses%20a%0Astochastic%20approach%20for%20pixel%20selection%2C%20further%20enhancing%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22709v2&entry.124074799=Read"},
{"title": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI", "author": "Ramneet Kaur and Colin Samplawski and Adam D. Cobb and Anirban Roy and Brian Matejek and Manoj Acharya and Daniel Elenius and Alexander M. Berenbeim and John A. Pavlik and Nathaniel D. Bastian and Susmit Jha", "abstract": "  In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline.\n", "link": "http://arxiv.org/abs/2411.02381v1", "date": "2024-11-04", "relevancy": 2.3453, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5943}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Uncertainty%20in%20LLMs%20to%20Enhance%20Reliability%20in%20Generative%20AI&body=Title%3A%20Addressing%20Uncertainty%20in%20LLMs%20to%20Enhance%20Reliability%20in%20Generative%20AI%0AAuthor%3A%20Ramneet%20Kaur%20and%20Colin%20Samplawski%20and%20Adam%20D.%20Cobb%20and%20Anirban%20Roy%20and%20Brian%20Matejek%20and%20Manoj%20Acharya%20and%20Daniel%20Elenius%20and%20Alexander%20M.%20Berenbeim%20and%20John%20A.%20Pavlik%20and%20Nathaniel%20D.%20Bastian%20and%20Susmit%20Jha%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20dynamic%20semantic%20clustering%20approach%20inspired%20by%0Athe%20Chinese%20Restaurant%20Process%2C%20aimed%20at%20addressing%20uncertainty%20in%20the%0Ainference%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20quantify%20uncertainty%20of%20an%20LLM%20on%0Aa%20given%20query%20by%20calculating%20entropy%20of%20the%20generated%20semantic%20clusters.%0AFurther%2C%20we%20propose%20leveraging%20the%20%28negative%29%20likelihood%20of%20these%20clusters%20as%0Athe%20%28non%29conformity%20score%20within%20Conformal%20Prediction%20framework%2C%20allowing%20the%0Amodel%20to%20predict%20a%20set%20of%20responses%20instead%20of%20a%20single%20output%2C%20thereby%0Aaccounting%20for%20uncertainty%20in%20its%20predictions.%20We%20demonstrate%20the%20effectiveness%0Aof%20our%20uncertainty%20quantification%20%28UQ%29%20technique%20on%20two%20well%20known%20question%0Aanswering%20benchmarks%2C%20COQA%20and%20TriviaQA%2C%20utilizing%20two%20LLMs%2C%20Llama2%20and%0AMistral.%20Our%20approach%20achieves%20SOTA%20performance%20in%20UQ%2C%20as%20assessed%20by%20metrics%0Asuch%20as%20AUROC%2C%20AUARC%2C%20and%20AURAC.%20The%20proposed%20conformal%20predictor%20is%20also%20shown%0Ato%20produce%20smaller%20prediction%20sets%20while%20maintaining%20the%20same%20probabilistic%0Aguarantee%20of%20including%20the%20correct%20response%2C%20in%20comparison%20to%20existing%20SOTA%0Aconformal%20prediction%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Uncertainty%2520in%2520LLMs%2520to%2520Enhance%2520Reliability%2520in%2520Generative%2520AI%26entry.906535625%3DRamneet%2520Kaur%2520and%2520Colin%2520Samplawski%2520and%2520Adam%2520D.%2520Cobb%2520and%2520Anirban%2520Roy%2520and%2520Brian%2520Matejek%2520and%2520Manoj%2520Acharya%2520and%2520Daniel%2520Elenius%2520and%2520Alexander%2520M.%2520Berenbeim%2520and%2520John%2520A.%2520Pavlik%2520and%2520Nathaniel%2520D.%2520Bastian%2520and%2520Susmit%2520Jha%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520dynamic%2520semantic%2520clustering%2520approach%2520inspired%2520by%250Athe%2520Chinese%2520Restaurant%2520Process%252C%2520aimed%2520at%2520addressing%2520uncertainty%2520in%2520the%250Ainference%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520quantify%2520uncertainty%2520of%2520an%2520LLM%2520on%250Aa%2520given%2520query%2520by%2520calculating%2520entropy%2520of%2520the%2520generated%2520semantic%2520clusters.%250AFurther%252C%2520we%2520propose%2520leveraging%2520the%2520%2528negative%2529%2520likelihood%2520of%2520these%2520clusters%2520as%250Athe%2520%2528non%2529conformity%2520score%2520within%2520Conformal%2520Prediction%2520framework%252C%2520allowing%2520the%250Amodel%2520to%2520predict%2520a%2520set%2520of%2520responses%2520instead%2520of%2520a%2520single%2520output%252C%2520thereby%250Aaccounting%2520for%2520uncertainty%2520in%2520its%2520predictions.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520uncertainty%2520quantification%2520%2528UQ%2529%2520technique%2520on%2520two%2520well%2520known%2520question%250Aanswering%2520benchmarks%252C%2520COQA%2520and%2520TriviaQA%252C%2520utilizing%2520two%2520LLMs%252C%2520Llama2%2520and%250AMistral.%2520Our%2520approach%2520achieves%2520SOTA%2520performance%2520in%2520UQ%252C%2520as%2520assessed%2520by%2520metrics%250Asuch%2520as%2520AUROC%252C%2520AUARC%252C%2520and%2520AURAC.%2520The%2520proposed%2520conformal%2520predictor%2520is%2520also%2520shown%250Ato%2520produce%2520smaller%2520prediction%2520sets%2520while%2520maintaining%2520the%2520same%2520probabilistic%250Aguarantee%2520of%2520including%2520the%2520correct%2520response%252C%2520in%2520comparison%2520to%2520existing%2520SOTA%250Aconformal%2520prediction%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Uncertainty%20in%20LLMs%20to%20Enhance%20Reliability%20in%20Generative%20AI&entry.906535625=Ramneet%20Kaur%20and%20Colin%20Samplawski%20and%20Adam%20D.%20Cobb%20and%20Anirban%20Roy%20and%20Brian%20Matejek%20and%20Manoj%20Acharya%20and%20Daniel%20Elenius%20and%20Alexander%20M.%20Berenbeim%20and%20John%20A.%20Pavlik%20and%20Nathaniel%20D.%20Bastian%20and%20Susmit%20Jha&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20dynamic%20semantic%20clustering%20approach%20inspired%20by%0Athe%20Chinese%20Restaurant%20Process%2C%20aimed%20at%20addressing%20uncertainty%20in%20the%0Ainference%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20quantify%20uncertainty%20of%20an%20LLM%20on%0Aa%20given%20query%20by%20calculating%20entropy%20of%20the%20generated%20semantic%20clusters.%0AFurther%2C%20we%20propose%20leveraging%20the%20%28negative%29%20likelihood%20of%20these%20clusters%20as%0Athe%20%28non%29conformity%20score%20within%20Conformal%20Prediction%20framework%2C%20allowing%20the%0Amodel%20to%20predict%20a%20set%20of%20responses%20instead%20of%20a%20single%20output%2C%20thereby%0Aaccounting%20for%20uncertainty%20in%20its%20predictions.%20We%20demonstrate%20the%20effectiveness%0Aof%20our%20uncertainty%20quantification%20%28UQ%29%20technique%20on%20two%20well%20known%20question%0Aanswering%20benchmarks%2C%20COQA%20and%20TriviaQA%2C%20utilizing%20two%20LLMs%2C%20Llama2%20and%0AMistral.%20Our%20approach%20achieves%20SOTA%20performance%20in%20UQ%2C%20as%20assessed%20by%20metrics%0Asuch%20as%20AUROC%2C%20AUARC%2C%20and%20AURAC.%20The%20proposed%20conformal%20predictor%20is%20also%20shown%0Ato%20produce%20smaller%20prediction%20sets%20while%20maintaining%20the%20same%20probabilistic%0Aguarantee%20of%20including%20the%20correct%20response%2C%20in%20comparison%20to%20existing%20SOTA%0Aconformal%20prediction%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02381v1&entry.124074799=Read"},
{"title": "Graph Edit Distance with General Costs Using Neural Set Divergence", "author": "Eeshaan Jain and Indradyumna Roy and Saswat Meher and Soumen Chakrabarti and Abir De", "abstract": "  Graph Edit Distance (GED) measures the (dis-)similarity between two given\ngraphs, in terms of the minimum-cost edit sequence that transforms one graph to\nthe other. However, the exact computation of GED is NP-Hard, which has recently\nmotivated the design of neural methods for GED estimation. However, they do not\nexplicitly account for edit operations with different costs. In response, we\npropose GRAPHEDX, a neural GED estimator that can work with general costs\nspecified for the four edit operations, viz., edge deletion, edge addition,\nnode deletion and node addition. We first present GED as a quadratic assignment\nproblem (QAP) that incorporates these four costs. Then, we represent each graph\nas a set of node and edge embeddings and use them to design a family of neural\nset divergence surrogates. We replace the QAP terms corresponding to each\noperation with their surrogates. Computing such neural set divergence require\naligning nodes and edges of the two graphs. We learn these alignments using a\nGumbel-Sinkhorn permutation generator, additionally ensuring that the node and\nedge alignments are consistent with each other. Moreover, these alignments are\ncognizant of both the presence and absence of edges between node-pairs.\nExperiments on several datasets, under a variety of edit cost settings, show\nthat GRAPHEDX consistently outperforms state-of-the-art methods and heuristics\nin terms of prediction error.\n", "link": "http://arxiv.org/abs/2409.17687v2", "date": "2024-11-04", "relevancy": 2.3377, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4886}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4661}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Edit%20Distance%20with%20General%20Costs%20Using%20Neural%20Set%20Divergence&body=Title%3A%20Graph%20Edit%20Distance%20with%20General%20Costs%20Using%20Neural%20Set%20Divergence%0AAuthor%3A%20Eeshaan%20Jain%20and%20Indradyumna%20Roy%20and%20Saswat%20Meher%20and%20Soumen%20Chakrabarti%20and%20Abir%20De%0AAbstract%3A%20%20%20Graph%20Edit%20Distance%20%28GED%29%20measures%20the%20%28dis-%29similarity%20between%20two%20given%0Agraphs%2C%20in%20terms%20of%20the%20minimum-cost%20edit%20sequence%20that%20transforms%20one%20graph%20to%0Athe%20other.%20However%2C%20the%20exact%20computation%20of%20GED%20is%20NP-Hard%2C%20which%20has%20recently%0Amotivated%20the%20design%20of%20neural%20methods%20for%20GED%20estimation.%20However%2C%20they%20do%20not%0Aexplicitly%20account%20for%20edit%20operations%20with%20different%20costs.%20In%20response%2C%20we%0Apropose%20GRAPHEDX%2C%20a%20neural%20GED%20estimator%20that%20can%20work%20with%20general%20costs%0Aspecified%20for%20the%20four%20edit%20operations%2C%20viz.%2C%20edge%20deletion%2C%20edge%20addition%2C%0Anode%20deletion%20and%20node%20addition.%20We%20first%20present%20GED%20as%20a%20quadratic%20assignment%0Aproblem%20%28QAP%29%20that%20incorporates%20these%20four%20costs.%20Then%2C%20we%20represent%20each%20graph%0Aas%20a%20set%20of%20node%20and%20edge%20embeddings%20and%20use%20them%20to%20design%20a%20family%20of%20neural%0Aset%20divergence%20surrogates.%20We%20replace%20the%20QAP%20terms%20corresponding%20to%20each%0Aoperation%20with%20their%20surrogates.%20Computing%20such%20neural%20set%20divergence%20require%0Aaligning%20nodes%20and%20edges%20of%20the%20two%20graphs.%20We%20learn%20these%20alignments%20using%20a%0AGumbel-Sinkhorn%20permutation%20generator%2C%20additionally%20ensuring%20that%20the%20node%20and%0Aedge%20alignments%20are%20consistent%20with%20each%20other.%20Moreover%2C%20these%20alignments%20are%0Acognizant%20of%20both%20the%20presence%20and%20absence%20of%20edges%20between%20node-pairs.%0AExperiments%20on%20several%20datasets%2C%20under%20a%20variety%20of%20edit%20cost%20settings%2C%20show%0Athat%20GRAPHEDX%20consistently%20outperforms%20state-of-the-art%20methods%20and%20heuristics%0Ain%20terms%20of%20prediction%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Edit%2520Distance%2520with%2520General%2520Costs%2520Using%2520Neural%2520Set%2520Divergence%26entry.906535625%3DEeshaan%2520Jain%2520and%2520Indradyumna%2520Roy%2520and%2520Saswat%2520Meher%2520and%2520Soumen%2520Chakrabarti%2520and%2520Abir%2520De%26entry.1292438233%3D%2520%2520Graph%2520Edit%2520Distance%2520%2528GED%2529%2520measures%2520the%2520%2528dis-%2529similarity%2520between%2520two%2520given%250Agraphs%252C%2520in%2520terms%2520of%2520the%2520minimum-cost%2520edit%2520sequence%2520that%2520transforms%2520one%2520graph%2520to%250Athe%2520other.%2520However%252C%2520the%2520exact%2520computation%2520of%2520GED%2520is%2520NP-Hard%252C%2520which%2520has%2520recently%250Amotivated%2520the%2520design%2520of%2520neural%2520methods%2520for%2520GED%2520estimation.%2520However%252C%2520they%2520do%2520not%250Aexplicitly%2520account%2520for%2520edit%2520operations%2520with%2520different%2520costs.%2520In%2520response%252C%2520we%250Apropose%2520GRAPHEDX%252C%2520a%2520neural%2520GED%2520estimator%2520that%2520can%2520work%2520with%2520general%2520costs%250Aspecified%2520for%2520the%2520four%2520edit%2520operations%252C%2520viz.%252C%2520edge%2520deletion%252C%2520edge%2520addition%252C%250Anode%2520deletion%2520and%2520node%2520addition.%2520We%2520first%2520present%2520GED%2520as%2520a%2520quadratic%2520assignment%250Aproblem%2520%2528QAP%2529%2520that%2520incorporates%2520these%2520four%2520costs.%2520Then%252C%2520we%2520represent%2520each%2520graph%250Aas%2520a%2520set%2520of%2520node%2520and%2520edge%2520embeddings%2520and%2520use%2520them%2520to%2520design%2520a%2520family%2520of%2520neural%250Aset%2520divergence%2520surrogates.%2520We%2520replace%2520the%2520QAP%2520terms%2520corresponding%2520to%2520each%250Aoperation%2520with%2520their%2520surrogates.%2520Computing%2520such%2520neural%2520set%2520divergence%2520require%250Aaligning%2520nodes%2520and%2520edges%2520of%2520the%2520two%2520graphs.%2520We%2520learn%2520these%2520alignments%2520using%2520a%250AGumbel-Sinkhorn%2520permutation%2520generator%252C%2520additionally%2520ensuring%2520that%2520the%2520node%2520and%250Aedge%2520alignments%2520are%2520consistent%2520with%2520each%2520other.%2520Moreover%252C%2520these%2520alignments%2520are%250Acognizant%2520of%2520both%2520the%2520presence%2520and%2520absence%2520of%2520edges%2520between%2520node-pairs.%250AExperiments%2520on%2520several%2520datasets%252C%2520under%2520a%2520variety%2520of%2520edit%2520cost%2520settings%252C%2520show%250Athat%2520GRAPHEDX%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520and%2520heuristics%250Ain%2520terms%2520of%2520prediction%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Edit%20Distance%20with%20General%20Costs%20Using%20Neural%20Set%20Divergence&entry.906535625=Eeshaan%20Jain%20and%20Indradyumna%20Roy%20and%20Saswat%20Meher%20and%20Soumen%20Chakrabarti%20and%20Abir%20De&entry.1292438233=%20%20Graph%20Edit%20Distance%20%28GED%29%20measures%20the%20%28dis-%29similarity%20between%20two%20given%0Agraphs%2C%20in%20terms%20of%20the%20minimum-cost%20edit%20sequence%20that%20transforms%20one%20graph%20to%0Athe%20other.%20However%2C%20the%20exact%20computation%20of%20GED%20is%20NP-Hard%2C%20which%20has%20recently%0Amotivated%20the%20design%20of%20neural%20methods%20for%20GED%20estimation.%20However%2C%20they%20do%20not%0Aexplicitly%20account%20for%20edit%20operations%20with%20different%20costs.%20In%20response%2C%20we%0Apropose%20GRAPHEDX%2C%20a%20neural%20GED%20estimator%20that%20can%20work%20with%20general%20costs%0Aspecified%20for%20the%20four%20edit%20operations%2C%20viz.%2C%20edge%20deletion%2C%20edge%20addition%2C%0Anode%20deletion%20and%20node%20addition.%20We%20first%20present%20GED%20as%20a%20quadratic%20assignment%0Aproblem%20%28QAP%29%20that%20incorporates%20these%20four%20costs.%20Then%2C%20we%20represent%20each%20graph%0Aas%20a%20set%20of%20node%20and%20edge%20embeddings%20and%20use%20them%20to%20design%20a%20family%20of%20neural%0Aset%20divergence%20surrogates.%20We%20replace%20the%20QAP%20terms%20corresponding%20to%20each%0Aoperation%20with%20their%20surrogates.%20Computing%20such%20neural%20set%20divergence%20require%0Aaligning%20nodes%20and%20edges%20of%20the%20two%20graphs.%20We%20learn%20these%20alignments%20using%20a%0AGumbel-Sinkhorn%20permutation%20generator%2C%20additionally%20ensuring%20that%20the%20node%20and%0Aedge%20alignments%20are%20consistent%20with%20each%20other.%20Moreover%2C%20these%20alignments%20are%0Acognizant%20of%20both%20the%20presence%20and%20absence%20of%20edges%20between%20node-pairs.%0AExperiments%20on%20several%20datasets%2C%20under%20a%20variety%20of%20edit%20cost%20settings%2C%20show%0Athat%20GRAPHEDX%20consistently%20outperforms%20state-of-the-art%20methods%20and%20heuristics%0Ain%20terms%20of%20prediction%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17687v2&entry.124074799=Read"},
{"title": "Revisiting K-mer Profile for Effective and Scalable Genome\n  Representation Learning", "author": "Abdulkadir Celikkanat and Andres R. Masegosa and Thomas D. Nielsen", "abstract": "  Obtaining effective representations of DNA sequences is crucial for genome\nanalysis. Metagenomic binning, for instance, relies on genome representations\nto cluster complex mixtures of DNA fragments from biological samples with the\naim of determining their microbial compositions. In this paper, we revisit\nk-mer-based representations of genomes and provide a theoretical analysis of\ntheir use in representation learning. Based on the analysis, we propose a\nlightweight and scalable model for performing metagenomic binning at the genome\nread level, relying only on the k-mer compositions of the DNA fragments. We\ncompare the model to recent genome foundation models and demonstrate that while\nthe models are comparable in performance, the proposed model is significantly\nmore effective in terms of scalability, a crucial aspect for performing\nmetagenomic binning of real-world datasets.\n", "link": "http://arxiv.org/abs/2411.02125v1", "date": "2024-11-04", "relevancy": 2.3228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20K-mer%20Profile%20for%20Effective%20and%20Scalable%20Genome%0A%20%20Representation%20Learning&body=Title%3A%20Revisiting%20K-mer%20Profile%20for%20Effective%20and%20Scalable%20Genome%0A%20%20Representation%20Learning%0AAuthor%3A%20Abdulkadir%20Celikkanat%20and%20Andres%20R.%20Masegosa%20and%20Thomas%20D.%20Nielsen%0AAbstract%3A%20%20%20Obtaining%20effective%20representations%20of%20DNA%20sequences%20is%20crucial%20for%20genome%0Aanalysis.%20Metagenomic%20binning%2C%20for%20instance%2C%20relies%20on%20genome%20representations%0Ato%20cluster%20complex%20mixtures%20of%20DNA%20fragments%20from%20biological%20samples%20with%20the%0Aaim%20of%20determining%20their%20microbial%20compositions.%20In%20this%20paper%2C%20we%20revisit%0Ak-mer-based%20representations%20of%20genomes%20and%20provide%20a%20theoretical%20analysis%20of%0Atheir%20use%20in%20representation%20learning.%20Based%20on%20the%20analysis%2C%20we%20propose%20a%0Alightweight%20and%20scalable%20model%20for%20performing%20metagenomic%20binning%20at%20the%20genome%0Aread%20level%2C%20relying%20only%20on%20the%20k-mer%20compositions%20of%20the%20DNA%20fragments.%20We%0Acompare%20the%20model%20to%20recent%20genome%20foundation%20models%20and%20demonstrate%20that%20while%0Athe%20models%20are%20comparable%20in%20performance%2C%20the%20proposed%20model%20is%20significantly%0Amore%20effective%20in%20terms%20of%20scalability%2C%20a%20crucial%20aspect%20for%20performing%0Ametagenomic%20binning%20of%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520K-mer%2520Profile%2520for%2520Effective%2520and%2520Scalable%2520Genome%250A%2520%2520Representation%2520Learning%26entry.906535625%3DAbdulkadir%2520Celikkanat%2520and%2520Andres%2520R.%2520Masegosa%2520and%2520Thomas%2520D.%2520Nielsen%26entry.1292438233%3D%2520%2520Obtaining%2520effective%2520representations%2520of%2520DNA%2520sequences%2520is%2520crucial%2520for%2520genome%250Aanalysis.%2520Metagenomic%2520binning%252C%2520for%2520instance%252C%2520relies%2520on%2520genome%2520representations%250Ato%2520cluster%2520complex%2520mixtures%2520of%2520DNA%2520fragments%2520from%2520biological%2520samples%2520with%2520the%250Aaim%2520of%2520determining%2520their%2520microbial%2520compositions.%2520In%2520this%2520paper%252C%2520we%2520revisit%250Ak-mer-based%2520representations%2520of%2520genomes%2520and%2520provide%2520a%2520theoretical%2520analysis%2520of%250Atheir%2520use%2520in%2520representation%2520learning.%2520Based%2520on%2520the%2520analysis%252C%2520we%2520propose%2520a%250Alightweight%2520and%2520scalable%2520model%2520for%2520performing%2520metagenomic%2520binning%2520at%2520the%2520genome%250Aread%2520level%252C%2520relying%2520only%2520on%2520the%2520k-mer%2520compositions%2520of%2520the%2520DNA%2520fragments.%2520We%250Acompare%2520the%2520model%2520to%2520recent%2520genome%2520foundation%2520models%2520and%2520demonstrate%2520that%2520while%250Athe%2520models%2520are%2520comparable%2520in%2520performance%252C%2520the%2520proposed%2520model%2520is%2520significantly%250Amore%2520effective%2520in%2520terms%2520of%2520scalability%252C%2520a%2520crucial%2520aspect%2520for%2520performing%250Ametagenomic%2520binning%2520of%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20K-mer%20Profile%20for%20Effective%20and%20Scalable%20Genome%0A%20%20Representation%20Learning&entry.906535625=Abdulkadir%20Celikkanat%20and%20Andres%20R.%20Masegosa%20and%20Thomas%20D.%20Nielsen&entry.1292438233=%20%20Obtaining%20effective%20representations%20of%20DNA%20sequences%20is%20crucial%20for%20genome%0Aanalysis.%20Metagenomic%20binning%2C%20for%20instance%2C%20relies%20on%20genome%20representations%0Ato%20cluster%20complex%20mixtures%20of%20DNA%20fragments%20from%20biological%20samples%20with%20the%0Aaim%20of%20determining%20their%20microbial%20compositions.%20In%20this%20paper%2C%20we%20revisit%0Ak-mer-based%20representations%20of%20genomes%20and%20provide%20a%20theoretical%20analysis%20of%0Atheir%20use%20in%20representation%20learning.%20Based%20on%20the%20analysis%2C%20we%20propose%20a%0Alightweight%20and%20scalable%20model%20for%20performing%20metagenomic%20binning%20at%20the%20genome%0Aread%20level%2C%20relying%20only%20on%20the%20k-mer%20compositions%20of%20the%20DNA%20fragments.%20We%0Acompare%20the%20model%20to%20recent%20genome%20foundation%20models%20and%20demonstrate%20that%20while%0Athe%20models%20are%20comparable%20in%20performance%2C%20the%20proposed%20model%20is%20significantly%0Amore%20effective%20in%20terms%20of%20scalability%2C%20a%20crucial%20aspect%20for%20performing%0Ametagenomic%20binning%20of%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02125v1&entry.124074799=Read"},
{"title": "Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis", "author": "Neel Dey and Benjamin Billot and Hallee E. Wong and Clinton J. Wang and Mengwei Ren and P. Ellen Grant and Adrian V. Dalca and Polina Golland", "abstract": "  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that enable generalization to\nnew biomedical contexts. To then train a single 3D network for any voxel-level\ntask, we develop a contrastive learning method that pretrains the network to be\nstable against nuisance imaging variation simulated by the data engine, a key\ninductive bias for generalization. This network's features can be used as\nrobust representations of input images for downstream tasks and its weights\nprovide a strong, dataset-agnostic initialization for finetuning on new\ndatasets. As a result, we set new standards across both multimodality\nregistration and few-shot segmentation, a first for any 3D biomedical vision\nmodel, all without (pre-)training on any existing dataset of real images.\n", "link": "http://arxiv.org/abs/2411.02372v1", "date": "2024-11-04", "relevancy": 2.2785, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5819}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20General-Purpose%20Biomedical%20Volume%20Representations%20using%0A%20%20Randomized%20Synthesis&body=Title%3A%20Learning%20General-Purpose%20Biomedical%20Volume%20Representations%20using%0A%20%20Randomized%20Synthesis%0AAuthor%3A%20Neel%20Dey%20and%20Benjamin%20Billot%20and%20Hallee%20E.%20Wong%20and%20Clinton%20J.%20Wang%20and%20Mengwei%20Ren%20and%20P.%20Ellen%20Grant%20and%20Adrian%20V.%20Dalca%20and%20Polina%20Golland%0AAbstract%3A%20%20%20Current%20volumetric%20biomedical%20foundation%20models%20struggle%20to%20generalize%20as%0Apublic%203D%20datasets%20are%20small%20and%20do%20not%20cover%20the%20broad%20diversity%20of%20medical%0Aprocedures%2C%20conditions%2C%20anatomical%20regions%2C%20and%20imaging%20protocols.%20We%20address%0Athis%20by%20creating%20a%20representation%20learning%20method%20that%20instead%20anticipates%0Astrong%20domain%20shifts%20at%20training%20time%20itself.%20We%20first%20propose%20a%20data%20engine%0Athat%20synthesizes%20highly%20variable%20training%20samples%20that%20enable%20generalization%20to%0Anew%20biomedical%20contexts.%20To%20then%20train%20a%20single%203D%20network%20for%20any%20voxel-level%0Atask%2C%20we%20develop%20a%20contrastive%20learning%20method%20that%20pretrains%20the%20network%20to%20be%0Astable%20against%20nuisance%20imaging%20variation%20simulated%20by%20the%20data%20engine%2C%20a%20key%0Ainductive%20bias%20for%20generalization.%20This%20network%27s%20features%20can%20be%20used%20as%0Arobust%20representations%20of%20input%20images%20for%20downstream%20tasks%20and%20its%20weights%0Aprovide%20a%20strong%2C%20dataset-agnostic%20initialization%20for%20finetuning%20on%20new%0Adatasets.%20As%20a%20result%2C%20we%20set%20new%20standards%20across%20both%20multimodality%0Aregistration%20and%20few-shot%20segmentation%2C%20a%20first%20for%20any%203D%20biomedical%20vision%0Amodel%2C%20all%20without%20%28pre-%29training%20on%20any%20existing%20dataset%20of%20real%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520General-Purpose%2520Biomedical%2520Volume%2520Representations%2520using%250A%2520%2520Randomized%2520Synthesis%26entry.906535625%3DNeel%2520Dey%2520and%2520Benjamin%2520Billot%2520and%2520Hallee%2520E.%2520Wong%2520and%2520Clinton%2520J.%2520Wang%2520and%2520Mengwei%2520Ren%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Adrian%2520V.%2520Dalca%2520and%2520Polina%2520Golland%26entry.1292438233%3D%2520%2520Current%2520volumetric%2520biomedical%2520foundation%2520models%2520struggle%2520to%2520generalize%2520as%250Apublic%25203D%2520datasets%2520are%2520small%2520and%2520do%2520not%2520cover%2520the%2520broad%2520diversity%2520of%2520medical%250Aprocedures%252C%2520conditions%252C%2520anatomical%2520regions%252C%2520and%2520imaging%2520protocols.%2520We%2520address%250Athis%2520by%2520creating%2520a%2520representation%2520learning%2520method%2520that%2520instead%2520anticipates%250Astrong%2520domain%2520shifts%2520at%2520training%2520time%2520itself.%2520We%2520first%2520propose%2520a%2520data%2520engine%250Athat%2520synthesizes%2520highly%2520variable%2520training%2520samples%2520that%2520enable%2520generalization%2520to%250Anew%2520biomedical%2520contexts.%2520To%2520then%2520train%2520a%2520single%25203D%2520network%2520for%2520any%2520voxel-level%250Atask%252C%2520we%2520develop%2520a%2520contrastive%2520learning%2520method%2520that%2520pretrains%2520the%2520network%2520to%2520be%250Astable%2520against%2520nuisance%2520imaging%2520variation%2520simulated%2520by%2520the%2520data%2520engine%252C%2520a%2520key%250Ainductive%2520bias%2520for%2520generalization.%2520This%2520network%2527s%2520features%2520can%2520be%2520used%2520as%250Arobust%2520representations%2520of%2520input%2520images%2520for%2520downstream%2520tasks%2520and%2520its%2520weights%250Aprovide%2520a%2520strong%252C%2520dataset-agnostic%2520initialization%2520for%2520finetuning%2520on%2520new%250Adatasets.%2520As%2520a%2520result%252C%2520we%2520set%2520new%2520standards%2520across%2520both%2520multimodality%250Aregistration%2520and%2520few-shot%2520segmentation%252C%2520a%2520first%2520for%2520any%25203D%2520biomedical%2520vision%250Amodel%252C%2520all%2520without%2520%2528pre-%2529training%2520on%2520any%2520existing%2520dataset%2520of%2520real%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20General-Purpose%20Biomedical%20Volume%20Representations%20using%0A%20%20Randomized%20Synthesis&entry.906535625=Neel%20Dey%20and%20Benjamin%20Billot%20and%20Hallee%20E.%20Wong%20and%20Clinton%20J.%20Wang%20and%20Mengwei%20Ren%20and%20P.%20Ellen%20Grant%20and%20Adrian%20V.%20Dalca%20and%20Polina%20Golland&entry.1292438233=%20%20Current%20volumetric%20biomedical%20foundation%20models%20struggle%20to%20generalize%20as%0Apublic%203D%20datasets%20are%20small%20and%20do%20not%20cover%20the%20broad%20diversity%20of%20medical%0Aprocedures%2C%20conditions%2C%20anatomical%20regions%2C%20and%20imaging%20protocols.%20We%20address%0Athis%20by%20creating%20a%20representation%20learning%20method%20that%20instead%20anticipates%0Astrong%20domain%20shifts%20at%20training%20time%20itself.%20We%20first%20propose%20a%20data%20engine%0Athat%20synthesizes%20highly%20variable%20training%20samples%20that%20enable%20generalization%20to%0Anew%20biomedical%20contexts.%20To%20then%20train%20a%20single%203D%20network%20for%20any%20voxel-level%0Atask%2C%20we%20develop%20a%20contrastive%20learning%20method%20that%20pretrains%20the%20network%20to%20be%0Astable%20against%20nuisance%20imaging%20variation%20simulated%20by%20the%20data%20engine%2C%20a%20key%0Ainductive%20bias%20for%20generalization.%20This%20network%27s%20features%20can%20be%20used%20as%0Arobust%20representations%20of%20input%20images%20for%20downstream%20tasks%20and%20its%20weights%0Aprovide%20a%20strong%2C%20dataset-agnostic%20initialization%20for%20finetuning%20on%20new%0Adatasets.%20As%20a%20result%2C%20we%20set%20new%20standards%20across%20both%20multimodality%0Aregistration%20and%20few-shot%20segmentation%2C%20a%20first%20for%20any%203D%20biomedical%20vision%0Amodel%2C%20all%20without%20%28pre-%29training%20on%20any%20existing%20dataset%20of%20real%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02372v1&entry.124074799=Read"},
{"title": "Touch-to-Touch Translation -- Learning the Mapping Between Heterogeneous\n  Tactile Sensing Technologies", "author": "Francesco Grella and Alessandro Albini and Giorgio Cannata and Perla Maiolino", "abstract": "  The use of data-driven techniques for tactile data processing and\nclassification has recently increased. However, collecting tactile data is a\ntime-expensive and sensor-specific procedure. Indeed, due to the lack of\nhardware standards in tactile sensing, data is required to be collected for\neach different sensor. This paper considers the problem of learning the mapping\nbetween two tactile sensor outputs with respect to the same physical stimulus\n-- we refer to this problem as touch-to-touch translation. In this respect, we\nproposed two data-driven approaches to address this task and we compared their\nperformance. The first one exploits a generative model developed for\nimage-to-image translation and adapted for this context. The second one uses a\nResNet model trained to perform a regression task. We validated both methods\nusing two completely different tactile sensors -- a camera-based, Digit and a\ncapacitance-based, CySkin. In particular, we used Digit images to generate the\ncorresponding CySkin data. We trained the models on a set of tactile features\nthat can be found in common larger objects and we performed the testing on a\npreviously unseen set of data. Experimental results show the possibility of\ntranslating Digit images into the CySkin output by preserving the contact shape\nand with an error of 15.18% in the magnitude of the sensor responses.\n", "link": "http://arxiv.org/abs/2411.02187v1", "date": "2024-11-04", "relevancy": 2.2734, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.574}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Touch-to-Touch%20Translation%20--%20Learning%20the%20Mapping%20Between%20Heterogeneous%0A%20%20Tactile%20Sensing%20Technologies&body=Title%3A%20Touch-to-Touch%20Translation%20--%20Learning%20the%20Mapping%20Between%20Heterogeneous%0A%20%20Tactile%20Sensing%20Technologies%0AAuthor%3A%20Francesco%20Grella%20and%20Alessandro%20Albini%20and%20Giorgio%20Cannata%20and%20Perla%20Maiolino%0AAbstract%3A%20%20%20The%20use%20of%20data-driven%20techniques%20for%20tactile%20data%20processing%20and%0Aclassification%20has%20recently%20increased.%20However%2C%20collecting%20tactile%20data%20is%20a%0Atime-expensive%20and%20sensor-specific%20procedure.%20Indeed%2C%20due%20to%20the%20lack%20of%0Ahardware%20standards%20in%20tactile%20sensing%2C%20data%20is%20required%20to%20be%20collected%20for%0Aeach%20different%20sensor.%20This%20paper%20considers%20the%20problem%20of%20learning%20the%20mapping%0Abetween%20two%20tactile%20sensor%20outputs%20with%20respect%20to%20the%20same%20physical%20stimulus%0A--%20we%20refer%20to%20this%20problem%20as%20touch-to-touch%20translation.%20In%20this%20respect%2C%20we%0Aproposed%20two%20data-driven%20approaches%20to%20address%20this%20task%20and%20we%20compared%20their%0Aperformance.%20The%20first%20one%20exploits%20a%20generative%20model%20developed%20for%0Aimage-to-image%20translation%20and%20adapted%20for%20this%20context.%20The%20second%20one%20uses%20a%0AResNet%20model%20trained%20to%20perform%20a%20regression%20task.%20We%20validated%20both%20methods%0Ausing%20two%20completely%20different%20tactile%20sensors%20--%20a%20camera-based%2C%20Digit%20and%20a%0Acapacitance-based%2C%20CySkin.%20In%20particular%2C%20we%20used%20Digit%20images%20to%20generate%20the%0Acorresponding%20CySkin%20data.%20We%20trained%20the%20models%20on%20a%20set%20of%20tactile%20features%0Athat%20can%20be%20found%20in%20common%20larger%20objects%20and%20we%20performed%20the%20testing%20on%20a%0Apreviously%20unseen%20set%20of%20data.%20Experimental%20results%20show%20the%20possibility%20of%0Atranslating%20Digit%20images%20into%20the%20CySkin%20output%20by%20preserving%20the%20contact%20shape%0Aand%20with%20an%20error%20of%2015.18%25%20in%20the%20magnitude%20of%20the%20sensor%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTouch-to-Touch%2520Translation%2520--%2520Learning%2520the%2520Mapping%2520Between%2520Heterogeneous%250A%2520%2520Tactile%2520Sensing%2520Technologies%26entry.906535625%3DFrancesco%2520Grella%2520and%2520Alessandro%2520Albini%2520and%2520Giorgio%2520Cannata%2520and%2520Perla%2520Maiolino%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520data-driven%2520techniques%2520for%2520tactile%2520data%2520processing%2520and%250Aclassification%2520has%2520recently%2520increased.%2520However%252C%2520collecting%2520tactile%2520data%2520is%2520a%250Atime-expensive%2520and%2520sensor-specific%2520procedure.%2520Indeed%252C%2520due%2520to%2520the%2520lack%2520of%250Ahardware%2520standards%2520in%2520tactile%2520sensing%252C%2520data%2520is%2520required%2520to%2520be%2520collected%2520for%250Aeach%2520different%2520sensor.%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520learning%2520the%2520mapping%250Abetween%2520two%2520tactile%2520sensor%2520outputs%2520with%2520respect%2520to%2520the%2520same%2520physical%2520stimulus%250A--%2520we%2520refer%2520to%2520this%2520problem%2520as%2520touch-to-touch%2520translation.%2520In%2520this%2520respect%252C%2520we%250Aproposed%2520two%2520data-driven%2520approaches%2520to%2520address%2520this%2520task%2520and%2520we%2520compared%2520their%250Aperformance.%2520The%2520first%2520one%2520exploits%2520a%2520generative%2520model%2520developed%2520for%250Aimage-to-image%2520translation%2520and%2520adapted%2520for%2520this%2520context.%2520The%2520second%2520one%2520uses%2520a%250AResNet%2520model%2520trained%2520to%2520perform%2520a%2520regression%2520task.%2520We%2520validated%2520both%2520methods%250Ausing%2520two%2520completely%2520different%2520tactile%2520sensors%2520--%2520a%2520camera-based%252C%2520Digit%2520and%2520a%250Acapacitance-based%252C%2520CySkin.%2520In%2520particular%252C%2520we%2520used%2520Digit%2520images%2520to%2520generate%2520the%250Acorresponding%2520CySkin%2520data.%2520We%2520trained%2520the%2520models%2520on%2520a%2520set%2520of%2520tactile%2520features%250Athat%2520can%2520be%2520found%2520in%2520common%2520larger%2520objects%2520and%2520we%2520performed%2520the%2520testing%2520on%2520a%250Apreviously%2520unseen%2520set%2520of%2520data.%2520Experimental%2520results%2520show%2520the%2520possibility%2520of%250Atranslating%2520Digit%2520images%2520into%2520the%2520CySkin%2520output%2520by%2520preserving%2520the%2520contact%2520shape%250Aand%2520with%2520an%2520error%2520of%252015.18%2525%2520in%2520the%2520magnitude%2520of%2520the%2520sensor%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Touch-to-Touch%20Translation%20--%20Learning%20the%20Mapping%20Between%20Heterogeneous%0A%20%20Tactile%20Sensing%20Technologies&entry.906535625=Francesco%20Grella%20and%20Alessandro%20Albini%20and%20Giorgio%20Cannata%20and%20Perla%20Maiolino&entry.1292438233=%20%20The%20use%20of%20data-driven%20techniques%20for%20tactile%20data%20processing%20and%0Aclassification%20has%20recently%20increased.%20However%2C%20collecting%20tactile%20data%20is%20a%0Atime-expensive%20and%20sensor-specific%20procedure.%20Indeed%2C%20due%20to%20the%20lack%20of%0Ahardware%20standards%20in%20tactile%20sensing%2C%20data%20is%20required%20to%20be%20collected%20for%0Aeach%20different%20sensor.%20This%20paper%20considers%20the%20problem%20of%20learning%20the%20mapping%0Abetween%20two%20tactile%20sensor%20outputs%20with%20respect%20to%20the%20same%20physical%20stimulus%0A--%20we%20refer%20to%20this%20problem%20as%20touch-to-touch%20translation.%20In%20this%20respect%2C%20we%0Aproposed%20two%20data-driven%20approaches%20to%20address%20this%20task%20and%20we%20compared%20their%0Aperformance.%20The%20first%20one%20exploits%20a%20generative%20model%20developed%20for%0Aimage-to-image%20translation%20and%20adapted%20for%20this%20context.%20The%20second%20one%20uses%20a%0AResNet%20model%20trained%20to%20perform%20a%20regression%20task.%20We%20validated%20both%20methods%0Ausing%20two%20completely%20different%20tactile%20sensors%20--%20a%20camera-based%2C%20Digit%20and%20a%0Acapacitance-based%2C%20CySkin.%20In%20particular%2C%20we%20used%20Digit%20images%20to%20generate%20the%0Acorresponding%20CySkin%20data.%20We%20trained%20the%20models%20on%20a%20set%20of%20tactile%20features%0Athat%20can%20be%20found%20in%20common%20larger%20objects%20and%20we%20performed%20the%20testing%20on%20a%0Apreviously%20unseen%20set%20of%20data.%20Experimental%20results%20show%20the%20possibility%20of%0Atranslating%20Digit%20images%20into%20the%20CySkin%20output%20by%20preserving%20the%20contact%20shape%0Aand%20with%20an%20error%20of%2015.18%25%20in%20the%20magnitude%20of%20the%20sensor%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02187v1&entry.124074799=Read"},
{"title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for\n  Open-World Perception", "author": "Julia Hindel and Daniele Cattaneo and Abhinav Valada", "abstract": "  Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2407.18145v2", "date": "2024-11-04", "relevancy": 2.2686, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception&body=Title%3A%20Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception%0AAuthor%3A%20Julia%20Hindel%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Semantic%20segmentation%20models%20are%20typically%20trained%20on%20a%20fixed%20set%20of%20classes%2C%0Alimiting%20their%20applicability%20in%20open-world%20scenarios.%20Class-incremental%0Asemantic%20segmentation%20aims%20to%20update%20models%20with%20emerging%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20of%20previously%20learned%20ones.%20However%2C%0Aexisting%20methods%20impose%20strict%20rigidity%20on%20old%20classes%2C%20reducing%20their%0Aeffectiveness%20in%20learning%20new%20incremental%20classes.%20In%20this%20work%2C%20we%20propose%0ATaxonomy-Oriented%20Poincar%5C%27e-regularized%20Incremental-Class%20Segmentation%0A%28TOPICS%29%20that%20learns%20feature%20embeddings%20in%20hyperbolic%20space%20following%20explicit%0Ataxonomy-tree%20structures.%20This%20supervision%20provides%20plasticity%20for%20old%20classes%2C%0Aupdating%20ancestors%20based%20on%20new%20classes%20while%20integrating%20new%20classes%20at%0Afitting%20positions.%20Additionally%2C%20we%20maintain%20implicit%20class%20relational%0Aconstraints%20on%20the%20geometric%20basis%20of%20the%20Poincar%5C%27e%20ball.%20This%20ensures%20that%0Athe%20latent%20space%20can%20continuously%20adapt%20to%20new%20constraints%20while%20maintaining%20a%0Arobust%20structure%20to%20combat%20catastrophic%20forgetting.%20We%20also%20establish%20eight%0Arealistic%20incremental%20learning%20protocols%20for%20autonomous%20driving%20scenarios%2C%0Awhere%20novel%20classes%20can%20originate%20from%20known%20classes%20or%20the%20background.%0AExtensive%20evaluations%20of%20TOPICS%20on%20the%20Cityscapes%20and%20Mapillary%20Vistas%202.0%0Abenchmarks%20demonstrate%20that%20it%20achieves%20state-of-the-art%20performance.%20We%20make%0Athe%20code%20and%20trained%20models%20publicly%20available%20at%0Ahttp%3A//topics.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaxonomy-Aware%2520Continual%2520Semantic%2520Segmentation%2520in%2520Hyperbolic%2520Spaces%2520for%250A%2520%2520Open-World%2520Perception%26entry.906535625%3DJulia%2520Hindel%2520and%2520Daniele%2520Cattaneo%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520models%2520are%2520typically%2520trained%2520on%2520a%2520fixed%2520set%2520of%2520classes%252C%250Alimiting%2520their%2520applicability%2520in%2520open-world%2520scenarios.%2520Class-incremental%250Asemantic%2520segmentation%2520aims%2520to%2520update%2520models%2520with%2520emerging%2520new%2520classes%2520while%250Apreventing%2520catastrophic%2520forgetting%2520of%2520previously%2520learned%2520ones.%2520However%252C%250Aexisting%2520methods%2520impose%2520strict%2520rigidity%2520on%2520old%2520classes%252C%2520reducing%2520their%250Aeffectiveness%2520in%2520learning%2520new%2520incremental%2520classes.%2520In%2520this%2520work%252C%2520we%2520propose%250ATaxonomy-Oriented%2520Poincar%255C%2527e-regularized%2520Incremental-Class%2520Segmentation%250A%2528TOPICS%2529%2520that%2520learns%2520feature%2520embeddings%2520in%2520hyperbolic%2520space%2520following%2520explicit%250Ataxonomy-tree%2520structures.%2520This%2520supervision%2520provides%2520plasticity%2520for%2520old%2520classes%252C%250Aupdating%2520ancestors%2520based%2520on%2520new%2520classes%2520while%2520integrating%2520new%2520classes%2520at%250Afitting%2520positions.%2520Additionally%252C%2520we%2520maintain%2520implicit%2520class%2520relational%250Aconstraints%2520on%2520the%2520geometric%2520basis%2520of%2520the%2520Poincar%255C%2527e%2520ball.%2520This%2520ensures%2520that%250Athe%2520latent%2520space%2520can%2520continuously%2520adapt%2520to%2520new%2520constraints%2520while%2520maintaining%2520a%250Arobust%2520structure%2520to%2520combat%2520catastrophic%2520forgetting.%2520We%2520also%2520establish%2520eight%250Arealistic%2520incremental%2520learning%2520protocols%2520for%2520autonomous%2520driving%2520scenarios%252C%250Awhere%2520novel%2520classes%2520can%2520originate%2520from%2520known%2520classes%2520or%2520the%2520background.%250AExtensive%2520evaluations%2520of%2520TOPICS%2520on%2520the%2520Cityscapes%2520and%2520Mapillary%2520Vistas%25202.0%250Abenchmarks%2520demonstrate%2520that%2520it%2520achieves%2520state-of-the-art%2520performance.%2520We%2520make%250Athe%2520code%2520and%2520trained%2520models%2520publicly%2520available%2520at%250Ahttp%253A//topics.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception&entry.906535625=Julia%20Hindel%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada&entry.1292438233=%20%20Semantic%20segmentation%20models%20are%20typically%20trained%20on%20a%20fixed%20set%20of%20classes%2C%0Alimiting%20their%20applicability%20in%20open-world%20scenarios.%20Class-incremental%0Asemantic%20segmentation%20aims%20to%20update%20models%20with%20emerging%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20of%20previously%20learned%20ones.%20However%2C%0Aexisting%20methods%20impose%20strict%20rigidity%20on%20old%20classes%2C%20reducing%20their%0Aeffectiveness%20in%20learning%20new%20incremental%20classes.%20In%20this%20work%2C%20we%20propose%0ATaxonomy-Oriented%20Poincar%5C%27e-regularized%20Incremental-Class%20Segmentation%0A%28TOPICS%29%20that%20learns%20feature%20embeddings%20in%20hyperbolic%20space%20following%20explicit%0Ataxonomy-tree%20structures.%20This%20supervision%20provides%20plasticity%20for%20old%20classes%2C%0Aupdating%20ancestors%20based%20on%20new%20classes%20while%20integrating%20new%20classes%20at%0Afitting%20positions.%20Additionally%2C%20we%20maintain%20implicit%20class%20relational%0Aconstraints%20on%20the%20geometric%20basis%20of%20the%20Poincar%5C%27e%20ball.%20This%20ensures%20that%0Athe%20latent%20space%20can%20continuously%20adapt%20to%20new%20constraints%20while%20maintaining%20a%0Arobust%20structure%20to%20combat%20catastrophic%20forgetting.%20We%20also%20establish%20eight%0Arealistic%20incremental%20learning%20protocols%20for%20autonomous%20driving%20scenarios%2C%0Awhere%20novel%20classes%20can%20originate%20from%20known%20classes%20or%20the%20background.%0AExtensive%20evaluations%20of%20TOPICS%20on%20the%20Cityscapes%20and%20Mapillary%20Vistas%202.0%0Abenchmarks%20demonstrate%20that%20it%20achieves%20state-of-the-art%20performance.%20We%20make%0Athe%20code%20and%20trained%20models%20publicly%20available%20at%0Ahttp%3A//topics.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18145v2&entry.124074799=Read"},
{"title": "Tree level change detection over Ahmedabad city using very high\n  resolution satellite images and Deep Learning", "author": "Jai G Singla and Gautam Jaiswal", "abstract": "  In this study, 0.5m high resolution satellite datasets over Indian urban\nregion was used to demonstrate the applicability of deep learning models over\nAhmedabad, India. Here, YOLOv7 instance segmentation model was trained on well\ncurated trees canopy dataset (6500 images) in order to carry out the change\ndetection. During training, evaluation metrics such as bounding box regression\nand mask regression loss, mean average precision (mAP) and stochastic gradient\ndescent algorithm were used for evaluating and optimizing the performance of\nmodel. After the 500 epochs, the mAP of 0.715 and 0.699 for individual tree\ndetection and tree canopy mask segmentation were obtained. However, by further\ntuning hyper parameters of the model, maximum accuracy of 80 % of trees\ndetection with false segmentation rate of 2% on data was obtained.\n", "link": "http://arxiv.org/abs/2411.02009v1", "date": "2024-11-04", "relevancy": 2.2592, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4577}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20level%20change%20detection%20over%20Ahmedabad%20city%20using%20very%20high%0A%20%20resolution%20satellite%20images%20and%20Deep%20Learning&body=Title%3A%20Tree%20level%20change%20detection%20over%20Ahmedabad%20city%20using%20very%20high%0A%20%20resolution%20satellite%20images%20and%20Deep%20Learning%0AAuthor%3A%20Jai%20G%20Singla%20and%20Gautam%20Jaiswal%0AAbstract%3A%20%20%20In%20this%20study%2C%200.5m%20high%20resolution%20satellite%20datasets%20over%20Indian%20urban%0Aregion%20was%20used%20to%20demonstrate%20the%20applicability%20of%20deep%20learning%20models%20over%0AAhmedabad%2C%20India.%20Here%2C%20YOLOv7%20instance%20segmentation%20model%20was%20trained%20on%20well%0Acurated%20trees%20canopy%20dataset%20%286500%20images%29%20in%20order%20to%20carry%20out%20the%20change%0Adetection.%20During%20training%2C%20evaluation%20metrics%20such%20as%20bounding%20box%20regression%0Aand%20mask%20regression%20loss%2C%20mean%20average%20precision%20%28mAP%29%20and%20stochastic%20gradient%0Adescent%20algorithm%20were%20used%20for%20evaluating%20and%20optimizing%20the%20performance%20of%0Amodel.%20After%20the%20500%20epochs%2C%20the%20mAP%20of%200.715%20and%200.699%20for%20individual%20tree%0Adetection%20and%20tree%20canopy%20mask%20segmentation%20were%20obtained.%20However%2C%20by%20further%0Atuning%20hyper%20parameters%20of%20the%20model%2C%20maximum%20accuracy%20of%2080%20%25%20of%20trees%0Adetection%20with%20false%20segmentation%20rate%20of%202%25%20on%20data%20was%20obtained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520level%2520change%2520detection%2520over%2520Ahmedabad%2520city%2520using%2520very%2520high%250A%2520%2520resolution%2520satellite%2520images%2520and%2520Deep%2520Learning%26entry.906535625%3DJai%2520G%2520Singla%2520and%2520Gautam%2520Jaiswal%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%25200.5m%2520high%2520resolution%2520satellite%2520datasets%2520over%2520Indian%2520urban%250Aregion%2520was%2520used%2520to%2520demonstrate%2520the%2520applicability%2520of%2520deep%2520learning%2520models%2520over%250AAhmedabad%252C%2520India.%2520Here%252C%2520YOLOv7%2520instance%2520segmentation%2520model%2520was%2520trained%2520on%2520well%250Acurated%2520trees%2520canopy%2520dataset%2520%25286500%2520images%2529%2520in%2520order%2520to%2520carry%2520out%2520the%2520change%250Adetection.%2520During%2520training%252C%2520evaluation%2520metrics%2520such%2520as%2520bounding%2520box%2520regression%250Aand%2520mask%2520regression%2520loss%252C%2520mean%2520average%2520precision%2520%2528mAP%2529%2520and%2520stochastic%2520gradient%250Adescent%2520algorithm%2520were%2520used%2520for%2520evaluating%2520and%2520optimizing%2520the%2520performance%2520of%250Amodel.%2520After%2520the%2520500%2520epochs%252C%2520the%2520mAP%2520of%25200.715%2520and%25200.699%2520for%2520individual%2520tree%250Adetection%2520and%2520tree%2520canopy%2520mask%2520segmentation%2520were%2520obtained.%2520However%252C%2520by%2520further%250Atuning%2520hyper%2520parameters%2520of%2520the%2520model%252C%2520maximum%2520accuracy%2520of%252080%2520%2525%2520of%2520trees%250Adetection%2520with%2520false%2520segmentation%2520rate%2520of%25202%2525%2520on%2520data%2520was%2520obtained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20level%20change%20detection%20over%20Ahmedabad%20city%20using%20very%20high%0A%20%20resolution%20satellite%20images%20and%20Deep%20Learning&entry.906535625=Jai%20G%20Singla%20and%20Gautam%20Jaiswal&entry.1292438233=%20%20In%20this%20study%2C%200.5m%20high%20resolution%20satellite%20datasets%20over%20Indian%20urban%0Aregion%20was%20used%20to%20demonstrate%20the%20applicability%20of%20deep%20learning%20models%20over%0AAhmedabad%2C%20India.%20Here%2C%20YOLOv7%20instance%20segmentation%20model%20was%20trained%20on%20well%0Acurated%20trees%20canopy%20dataset%20%286500%20images%29%20in%20order%20to%20carry%20out%20the%20change%0Adetection.%20During%20training%2C%20evaluation%20metrics%20such%20as%20bounding%20box%20regression%0Aand%20mask%20regression%20loss%2C%20mean%20average%20precision%20%28mAP%29%20and%20stochastic%20gradient%0Adescent%20algorithm%20were%20used%20for%20evaluating%20and%20optimizing%20the%20performance%20of%0Amodel.%20After%20the%20500%20epochs%2C%20the%20mAP%20of%200.715%20and%200.699%20for%20individual%20tree%0Adetection%20and%20tree%20canopy%20mask%20segmentation%20were%20obtained.%20However%2C%20by%20further%0Atuning%20hyper%20parameters%20of%20the%20model%2C%20maximum%20accuracy%20of%2080%20%25%20of%20trees%0Adetection%20with%20false%20segmentation%20rate%20of%202%25%20on%20data%20was%20obtained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02009v1&entry.124074799=Read"},
{"title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution", "author": "Yang Yue and Yulin Wang and Bingyi Kang and Yizeng Han and Shenzhi Wang and Shiji Song and Jiashi Feng and Gao Huang", "abstract": "  MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.\n", "link": "http://arxiv.org/abs/2411.02359v1", "date": "2024-11-04", "relevancy": 2.2514, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeeR-VLA%3A%20Dynamic%20Inference%20of%20Multimodal%20Large%20Language%20Models%20for%0A%20%20Efficient%20Robot%20Execution&body=Title%3A%20DeeR-VLA%3A%20Dynamic%20Inference%20of%20Multimodal%20Large%20Language%20Models%20for%0A%20%20Efficient%20Robot%20Execution%0AAuthor%3A%20Yang%20Yue%20and%20Yulin%20Wang%20and%20Bingyi%20Kang%20and%20Yizeng%20Han%20and%20Shenzhi%20Wang%20and%20Shiji%20Song%20and%20Jiashi%20Feng%20and%20Gao%20Huang%0AAbstract%3A%20%20%20MLLMs%20have%20demonstrated%20remarkable%20comprehension%20and%20reasoning%20capabilities%0Awith%20complex%20language%20and%20visual%20data.%20These%20advances%20have%20spurred%20the%20vision%0Aof%20establishing%20a%20generalist%20robotic%20MLLM%20proficient%20in%20understanding%20complex%0Ahuman%20instructions%20and%20accomplishing%20various%20embodied%20tasks.%20However%2C%0Adeveloping%20MLLMs%20for%20real-world%20robots%20is%20challenging%20due%20to%20the%20typically%0Alimited%20computation%20and%20memory%20capacities%20available%20on%20robotic%20platforms.%20In%0Acontrast%2C%20the%20inference%20of%20MLLMs%20involves%20storing%20billions%20of%20parameters%20and%0Aperforming%20tremendous%20computation%2C%20imposing%20significant%20hardware%20demands.%20In%0Aour%20paper%2C%20we%20propose%20a%20Dynamic%20Early-Exit%20Framework%20for%20Robotic%0AVision-Language-Action%20Model%20%28DeeR-VLA%2C%20or%20simply%20DeeR%29%20that%20automatically%0Aadjusts%20the%20size%20of%20the%20activated%20MLLM%20based%20on%20each%20situation%20at%20hand.%20The%0Aapproach%20leverages%20a%20multi-exit%20architecture%20in%20MLLMs%2C%20which%20allows%20the%20model%0Ato%20terminate%20processing%20once%20a%20proper%20size%20of%20the%20model%20has%20been%20activated%20for%0Aa%20specific%20situation%2C%20thus%20avoiding%20further%20redundant%20computation.%0AAdditionally%2C%20we%20develop%20novel%20algorithms%20that%20establish%20early-termination%0Acriteria%20for%20DeeR%2C%20conditioned%20on%20predefined%20demands%20such%20as%20average%0Acomputational%20cost%20%28i.e.%2C%20power%20consumption%29%2C%20as%20well%20as%20peak%20computational%0Aconsumption%20%28i.e.%2C%20latency%29%20and%20GPU%20memory%20usage.%20These%20enhancements%20ensure%0Athat%20DeeR%20operates%20efficiently%20under%20varying%20resource%20constraints%20while%0Amaintaining%20competitive%20performance.%20On%20the%20CALVIN%20robot%20manipulation%0Abenchmark%2C%20DeeR%20demonstrates%20significant%20reductions%20in%20computational%20costs%20of%0ALLM%20by%205.2-6.5x%20and%20GPU%20memory%20of%20LLM%20by%202-6x%20without%20compromising%20performance.%0ACode%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/yueyang130/DeeR-VLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeeR-VLA%253A%2520Dynamic%2520Inference%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520for%250A%2520%2520Efficient%2520Robot%2520Execution%26entry.906535625%3DYang%2520Yue%2520and%2520Yulin%2520Wang%2520and%2520Bingyi%2520Kang%2520and%2520Yizeng%2520Han%2520and%2520Shenzhi%2520Wang%2520and%2520Shiji%2520Song%2520and%2520Jiashi%2520Feng%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520MLLMs%2520have%2520demonstrated%2520remarkable%2520comprehension%2520and%2520reasoning%2520capabilities%250Awith%2520complex%2520language%2520and%2520visual%2520data.%2520These%2520advances%2520have%2520spurred%2520the%2520vision%250Aof%2520establishing%2520a%2520generalist%2520robotic%2520MLLM%2520proficient%2520in%2520understanding%2520complex%250Ahuman%2520instructions%2520and%2520accomplishing%2520various%2520embodied%2520tasks.%2520However%252C%250Adeveloping%2520MLLMs%2520for%2520real-world%2520robots%2520is%2520challenging%2520due%2520to%2520the%2520typically%250Alimited%2520computation%2520and%2520memory%2520capacities%2520available%2520on%2520robotic%2520platforms.%2520In%250Acontrast%252C%2520the%2520inference%2520of%2520MLLMs%2520involves%2520storing%2520billions%2520of%2520parameters%2520and%250Aperforming%2520tremendous%2520computation%252C%2520imposing%2520significant%2520hardware%2520demands.%2520In%250Aour%2520paper%252C%2520we%2520propose%2520a%2520Dynamic%2520Early-Exit%2520Framework%2520for%2520Robotic%250AVision-Language-Action%2520Model%2520%2528DeeR-VLA%252C%2520or%2520simply%2520DeeR%2529%2520that%2520automatically%250Aadjusts%2520the%2520size%2520of%2520the%2520activated%2520MLLM%2520based%2520on%2520each%2520situation%2520at%2520hand.%2520The%250Aapproach%2520leverages%2520a%2520multi-exit%2520architecture%2520in%2520MLLMs%252C%2520which%2520allows%2520the%2520model%250Ato%2520terminate%2520processing%2520once%2520a%2520proper%2520size%2520of%2520the%2520model%2520has%2520been%2520activated%2520for%250Aa%2520specific%2520situation%252C%2520thus%2520avoiding%2520further%2520redundant%2520computation.%250AAdditionally%252C%2520we%2520develop%2520novel%2520algorithms%2520that%2520establish%2520early-termination%250Acriteria%2520for%2520DeeR%252C%2520conditioned%2520on%2520predefined%2520demands%2520such%2520as%2520average%250Acomputational%2520cost%2520%2528i.e.%252C%2520power%2520consumption%2529%252C%2520as%2520well%2520as%2520peak%2520computational%250Aconsumption%2520%2528i.e.%252C%2520latency%2529%2520and%2520GPU%2520memory%2520usage.%2520These%2520enhancements%2520ensure%250Athat%2520DeeR%2520operates%2520efficiently%2520under%2520varying%2520resource%2520constraints%2520while%250Amaintaining%2520competitive%2520performance.%2520On%2520the%2520CALVIN%2520robot%2520manipulation%250Abenchmark%252C%2520DeeR%2520demonstrates%2520significant%2520reductions%2520in%2520computational%2520costs%2520of%250ALLM%2520by%25205.2-6.5x%2520and%2520GPU%2520memory%2520of%2520LLM%2520by%25202-6x%2520without%2520compromising%2520performance.%250ACode%2520and%2520checkpoints%2520are%2520available%2520at%2520https%253A//github.com/yueyang130/DeeR-VLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeeR-VLA%3A%20Dynamic%20Inference%20of%20Multimodal%20Large%20Language%20Models%20for%0A%20%20Efficient%20Robot%20Execution&entry.906535625=Yang%20Yue%20and%20Yulin%20Wang%20and%20Bingyi%20Kang%20and%20Yizeng%20Han%20and%20Shenzhi%20Wang%20and%20Shiji%20Song%20and%20Jiashi%20Feng%20and%20Gao%20Huang&entry.1292438233=%20%20MLLMs%20have%20demonstrated%20remarkable%20comprehension%20and%20reasoning%20capabilities%0Awith%20complex%20language%20and%20visual%20data.%20These%20advances%20have%20spurred%20the%20vision%0Aof%20establishing%20a%20generalist%20robotic%20MLLM%20proficient%20in%20understanding%20complex%0Ahuman%20instructions%20and%20accomplishing%20various%20embodied%20tasks.%20However%2C%0Adeveloping%20MLLMs%20for%20real-world%20robots%20is%20challenging%20due%20to%20the%20typically%0Alimited%20computation%20and%20memory%20capacities%20available%20on%20robotic%20platforms.%20In%0Acontrast%2C%20the%20inference%20of%20MLLMs%20involves%20storing%20billions%20of%20parameters%20and%0Aperforming%20tremendous%20computation%2C%20imposing%20significant%20hardware%20demands.%20In%0Aour%20paper%2C%20we%20propose%20a%20Dynamic%20Early-Exit%20Framework%20for%20Robotic%0AVision-Language-Action%20Model%20%28DeeR-VLA%2C%20or%20simply%20DeeR%29%20that%20automatically%0Aadjusts%20the%20size%20of%20the%20activated%20MLLM%20based%20on%20each%20situation%20at%20hand.%20The%0Aapproach%20leverages%20a%20multi-exit%20architecture%20in%20MLLMs%2C%20which%20allows%20the%20model%0Ato%20terminate%20processing%20once%20a%20proper%20size%20of%20the%20model%20has%20been%20activated%20for%0Aa%20specific%20situation%2C%20thus%20avoiding%20further%20redundant%20computation.%0AAdditionally%2C%20we%20develop%20novel%20algorithms%20that%20establish%20early-termination%0Acriteria%20for%20DeeR%2C%20conditioned%20on%20predefined%20demands%20such%20as%20average%0Acomputational%20cost%20%28i.e.%2C%20power%20consumption%29%2C%20as%20well%20as%20peak%20computational%0Aconsumption%20%28i.e.%2C%20latency%29%20and%20GPU%20memory%20usage.%20These%20enhancements%20ensure%0Athat%20DeeR%20operates%20efficiently%20under%20varying%20resource%20constraints%20while%0Amaintaining%20competitive%20performance.%20On%20the%20CALVIN%20robot%20manipulation%0Abenchmark%2C%20DeeR%20demonstrates%20significant%20reductions%20in%20computational%20costs%20of%0ALLM%20by%205.2-6.5x%20and%20GPU%20memory%20of%20LLM%20by%202-6x%20without%20compromising%20performance.%0ACode%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/yueyang130/DeeR-VLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02359v1&entry.124074799=Read"},
{"title": "Ask, and it shall be given: Turing completeness of prompting", "author": "Ruizhong Qiu and Zhe Xu and Wenxuan Bao and Hanghang Tong", "abstract": "  Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice.\n", "link": "http://arxiv.org/abs/2411.01992v1", "date": "2024-11-04", "relevancy": 2.2408, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ask%2C%20and%20it%20shall%20be%20given%3A%20Turing%20completeness%20of%20prompting&body=Title%3A%20Ask%2C%20and%20it%20shall%20be%20given%3A%20Turing%20completeness%20of%20prompting%0AAuthor%3A%20Ruizhong%20Qiu%20and%20Zhe%20Xu%20and%20Wenxuan%20Bao%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Since%20the%20success%20of%20GPT%2C%20large%20language%20models%20%28LLMs%29%20have%20been%0Arevolutionizing%20machine%20learning%20and%20have%20initiated%20the%20so-called%20LLM%20prompting%0Aparadigm.%20In%20the%20era%20of%20LLMs%2C%20people%20train%20a%20single%20general-purpose%20LLM%20and%0Aprovide%20the%20LLM%20with%20different%20prompts%20to%20perform%20different%20tasks.%20However%2C%0Asuch%20empirical%20success%20largely%20lacks%20theoretical%20understanding.%20Here%2C%20we%0Apresent%20the%20first%20theoretical%20study%20on%20the%20LLM%20prompting%20paradigm%20to%20the%20best%0Aof%20our%20knowledge.%20In%20this%20work%2C%20we%20show%20that%20prompting%20is%20in%20fact%0ATuring-complete%3A%20there%20exists%20a%20finite-size%20Transformer%20such%20that%20for%20any%0Acomputable%20function%2C%20there%20exists%20a%20corresponding%20prompt%20following%20which%20the%0ATransformer%20computes%20the%20function.%20Furthermore%2C%20we%20show%20that%20even%20though%20we%20use%0Aonly%20a%20single%20finite-size%20Transformer%2C%20it%20can%20still%20achieve%20nearly%20the%20same%0Acomplexity%20bounds%20as%20that%20of%20the%20class%20of%20all%20unbounded-size%20Transformers.%0AOverall%2C%20our%20result%20reveals%20that%20prompting%20can%20enable%20a%20single%20finite-size%0ATransformer%20to%20be%20efficiently%20universal%2C%20which%20establishes%20a%20theoretical%0Aunderpinning%20for%20prompt%20engineering%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsk%252C%2520and%2520it%2520shall%2520be%2520given%253A%2520Turing%2520completeness%2520of%2520prompting%26entry.906535625%3DRuizhong%2520Qiu%2520and%2520Zhe%2520Xu%2520and%2520Wenxuan%2520Bao%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Since%2520the%2520success%2520of%2520GPT%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%250Arevolutionizing%2520machine%2520learning%2520and%2520have%2520initiated%2520the%2520so-called%2520LLM%2520prompting%250Aparadigm.%2520In%2520the%2520era%2520of%2520LLMs%252C%2520people%2520train%2520a%2520single%2520general-purpose%2520LLM%2520and%250Aprovide%2520the%2520LLM%2520with%2520different%2520prompts%2520to%2520perform%2520different%2520tasks.%2520However%252C%250Asuch%2520empirical%2520success%2520largely%2520lacks%2520theoretical%2520understanding.%2520Here%252C%2520we%250Apresent%2520the%2520first%2520theoretical%2520study%2520on%2520the%2520LLM%2520prompting%2520paradigm%2520to%2520the%2520best%250Aof%2520our%2520knowledge.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520prompting%2520is%2520in%2520fact%250ATuring-complete%253A%2520there%2520exists%2520a%2520finite-size%2520Transformer%2520such%2520that%2520for%2520any%250Acomputable%2520function%252C%2520there%2520exists%2520a%2520corresponding%2520prompt%2520following%2520which%2520the%250ATransformer%2520computes%2520the%2520function.%2520Furthermore%252C%2520we%2520show%2520that%2520even%2520though%2520we%2520use%250Aonly%2520a%2520single%2520finite-size%2520Transformer%252C%2520it%2520can%2520still%2520achieve%2520nearly%2520the%2520same%250Acomplexity%2520bounds%2520as%2520that%2520of%2520the%2520class%2520of%2520all%2520unbounded-size%2520Transformers.%250AOverall%252C%2520our%2520result%2520reveals%2520that%2520prompting%2520can%2520enable%2520a%2520single%2520finite-size%250ATransformer%2520to%2520be%2520efficiently%2520universal%252C%2520which%2520establishes%2520a%2520theoretical%250Aunderpinning%2520for%2520prompt%2520engineering%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ask%2C%20and%20it%20shall%20be%20given%3A%20Turing%20completeness%20of%20prompting&entry.906535625=Ruizhong%20Qiu%20and%20Zhe%20Xu%20and%20Wenxuan%20Bao%20and%20Hanghang%20Tong&entry.1292438233=%20%20Since%20the%20success%20of%20GPT%2C%20large%20language%20models%20%28LLMs%29%20have%20been%0Arevolutionizing%20machine%20learning%20and%20have%20initiated%20the%20so-called%20LLM%20prompting%0Aparadigm.%20In%20the%20era%20of%20LLMs%2C%20people%20train%20a%20single%20general-purpose%20LLM%20and%0Aprovide%20the%20LLM%20with%20different%20prompts%20to%20perform%20different%20tasks.%20However%2C%0Asuch%20empirical%20success%20largely%20lacks%20theoretical%20understanding.%20Here%2C%20we%0Apresent%20the%20first%20theoretical%20study%20on%20the%20LLM%20prompting%20paradigm%20to%20the%20best%0Aof%20our%20knowledge.%20In%20this%20work%2C%20we%20show%20that%20prompting%20is%20in%20fact%0ATuring-complete%3A%20there%20exists%20a%20finite-size%20Transformer%20such%20that%20for%20any%0Acomputable%20function%2C%20there%20exists%20a%20corresponding%20prompt%20following%20which%20the%0ATransformer%20computes%20the%20function.%20Furthermore%2C%20we%20show%20that%20even%20though%20we%20use%0Aonly%20a%20single%20finite-size%20Transformer%2C%20it%20can%20still%20achieve%20nearly%20the%20same%0Acomplexity%20bounds%20as%20that%20of%20the%20class%20of%20all%20unbounded-size%20Transformers.%0AOverall%2C%20our%20result%20reveals%20that%20prompting%20can%20enable%20a%20single%20finite-size%0ATransformer%20to%20be%20efficiently%20universal%2C%20which%20establishes%20a%20theoretical%0Aunderpinning%20for%20prompt%20engineering%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01992v1&entry.124074799=Read"},
{"title": "Metric properties of partial and robust Gromov-Wasserstein distances", "author": "Jannatul Chhoa and Michael Ivanitskiy and Fushuai Jiang and Shiying Li and Daniel McBride and Tom Needham and Kaiying O'Hare", "abstract": "  The Gromov-Wasserstein (GW) distances define a family of metrics, based on\nideas from optimal transport, which enable comparisons between probability\nmeasures defined on distinct metric spaces. They are particularly useful in\nareas such as network analysis and geometry processing, as computation of a GW\ndistance involves solving for registration between the objects which minimizes\ngeometric distortion. Although GW distances have proven useful for various\napplications in the recent machine learning literature, it has been observed\nthat they are inherently sensitive to outlier noise and cannot accommodate\npartial matching. This has been addressed by various constructions building on\nthe GW framework; in this article, we focus specifically on a natural\nrelaxation of the GW optimization problem, introduced by Chapel et al., which\nis aimed at addressing exactly these shortcomings. Our goal is to understand\nthe theoretical properties of this relaxed optimization problem, from the\nviewpoint of metric geometry. While the relaxed problem fails to induce a\nmetric, we derive precise characterizations of how it fails the axioms of\nnon-degeneracy and triangle inequality. These observations lead us to define a\nnovel family of distances, whose construction is inspired by the Prokhorov and\nKy Fan distances, as well as by the recent work of Raghvendra et al.\\ on robust\nversions of classical Wasserstein distance. We show that our new distances\ndefine true metrics, that they induce the same topology as the GW distances,\nand that they enjoy additional robustness to perturbations. These results\nprovide a mathematically rigorous basis for using our robust partial GW\ndistances in applications where outliers and partial matching are concerns.\n", "link": "http://arxiv.org/abs/2411.02198v1", "date": "2024-11-04", "relevancy": 2.2374, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4737}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4356}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20properties%20of%20partial%20and%20robust%20Gromov-Wasserstein%20distances&body=Title%3A%20Metric%20properties%20of%20partial%20and%20robust%20Gromov-Wasserstein%20distances%0AAuthor%3A%20Jannatul%20Chhoa%20and%20Michael%20Ivanitskiy%20and%20Fushuai%20Jiang%20and%20Shiying%20Li%20and%20Daniel%20McBride%20and%20Tom%20Needham%20and%20Kaiying%20O%27Hare%0AAbstract%3A%20%20%20The%20Gromov-Wasserstein%20%28GW%29%20distances%20define%20a%20family%20of%20metrics%2C%20based%20on%0Aideas%20from%20optimal%20transport%2C%20which%20enable%20comparisons%20between%20probability%0Ameasures%20defined%20on%20distinct%20metric%20spaces.%20They%20are%20particularly%20useful%20in%0Aareas%20such%20as%20network%20analysis%20and%20geometry%20processing%2C%20as%20computation%20of%20a%20GW%0Adistance%20involves%20solving%20for%20registration%20between%20the%20objects%20which%20minimizes%0Ageometric%20distortion.%20Although%20GW%20distances%20have%20proven%20useful%20for%20various%0Aapplications%20in%20the%20recent%20machine%20learning%20literature%2C%20it%20has%20been%20observed%0Athat%20they%20are%20inherently%20sensitive%20to%20outlier%20noise%20and%20cannot%20accommodate%0Apartial%20matching.%20This%20has%20been%20addressed%20by%20various%20constructions%20building%20on%0Athe%20GW%20framework%3B%20in%20this%20article%2C%20we%20focus%20specifically%20on%20a%20natural%0Arelaxation%20of%20the%20GW%20optimization%20problem%2C%20introduced%20by%20Chapel%20et%20al.%2C%20which%0Ais%20aimed%20at%20addressing%20exactly%20these%20shortcomings.%20Our%20goal%20is%20to%20understand%0Athe%20theoretical%20properties%20of%20this%20relaxed%20optimization%20problem%2C%20from%20the%0Aviewpoint%20of%20metric%20geometry.%20While%20the%20relaxed%20problem%20fails%20to%20induce%20a%0Ametric%2C%20we%20derive%20precise%20characterizations%20of%20how%20it%20fails%20the%20axioms%20of%0Anon-degeneracy%20and%20triangle%20inequality.%20These%20observations%20lead%20us%20to%20define%20a%0Anovel%20family%20of%20distances%2C%20whose%20construction%20is%20inspired%20by%20the%20Prokhorov%20and%0AKy%20Fan%20distances%2C%20as%20well%20as%20by%20the%20recent%20work%20of%20Raghvendra%20et%20al.%5C%20on%20robust%0Aversions%20of%20classical%20Wasserstein%20distance.%20We%20show%20that%20our%20new%20distances%0Adefine%20true%20metrics%2C%20that%20they%20induce%20the%20same%20topology%20as%20the%20GW%20distances%2C%0Aand%20that%20they%20enjoy%20additional%20robustness%20to%20perturbations.%20These%20results%0Aprovide%20a%20mathematically%20rigorous%20basis%20for%20using%20our%20robust%20partial%20GW%0Adistances%20in%20applications%20where%20outliers%20and%20partial%20matching%20are%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520properties%2520of%2520partial%2520and%2520robust%2520Gromov-Wasserstein%2520distances%26entry.906535625%3DJannatul%2520Chhoa%2520and%2520Michael%2520Ivanitskiy%2520and%2520Fushuai%2520Jiang%2520and%2520Shiying%2520Li%2520and%2520Daniel%2520McBride%2520and%2520Tom%2520Needham%2520and%2520Kaiying%2520O%2527Hare%26entry.1292438233%3D%2520%2520The%2520Gromov-Wasserstein%2520%2528GW%2529%2520distances%2520define%2520a%2520family%2520of%2520metrics%252C%2520based%2520on%250Aideas%2520from%2520optimal%2520transport%252C%2520which%2520enable%2520comparisons%2520between%2520probability%250Ameasures%2520defined%2520on%2520distinct%2520metric%2520spaces.%2520They%2520are%2520particularly%2520useful%2520in%250Aareas%2520such%2520as%2520network%2520analysis%2520and%2520geometry%2520processing%252C%2520as%2520computation%2520of%2520a%2520GW%250Adistance%2520involves%2520solving%2520for%2520registration%2520between%2520the%2520objects%2520which%2520minimizes%250Ageometric%2520distortion.%2520Although%2520GW%2520distances%2520have%2520proven%2520useful%2520for%2520various%250Aapplications%2520in%2520the%2520recent%2520machine%2520learning%2520literature%252C%2520it%2520has%2520been%2520observed%250Athat%2520they%2520are%2520inherently%2520sensitive%2520to%2520outlier%2520noise%2520and%2520cannot%2520accommodate%250Apartial%2520matching.%2520This%2520has%2520been%2520addressed%2520by%2520various%2520constructions%2520building%2520on%250Athe%2520GW%2520framework%253B%2520in%2520this%2520article%252C%2520we%2520focus%2520specifically%2520on%2520a%2520natural%250Arelaxation%2520of%2520the%2520GW%2520optimization%2520problem%252C%2520introduced%2520by%2520Chapel%2520et%2520al.%252C%2520which%250Ais%2520aimed%2520at%2520addressing%2520exactly%2520these%2520shortcomings.%2520Our%2520goal%2520is%2520to%2520understand%250Athe%2520theoretical%2520properties%2520of%2520this%2520relaxed%2520optimization%2520problem%252C%2520from%2520the%250Aviewpoint%2520of%2520metric%2520geometry.%2520While%2520the%2520relaxed%2520problem%2520fails%2520to%2520induce%2520a%250Ametric%252C%2520we%2520derive%2520precise%2520characterizations%2520of%2520how%2520it%2520fails%2520the%2520axioms%2520of%250Anon-degeneracy%2520and%2520triangle%2520inequality.%2520These%2520observations%2520lead%2520us%2520to%2520define%2520a%250Anovel%2520family%2520of%2520distances%252C%2520whose%2520construction%2520is%2520inspired%2520by%2520the%2520Prokhorov%2520and%250AKy%2520Fan%2520distances%252C%2520as%2520well%2520as%2520by%2520the%2520recent%2520work%2520of%2520Raghvendra%2520et%2520al.%255C%2520on%2520robust%250Aversions%2520of%2520classical%2520Wasserstein%2520distance.%2520We%2520show%2520that%2520our%2520new%2520distances%250Adefine%2520true%2520metrics%252C%2520that%2520they%2520induce%2520the%2520same%2520topology%2520as%2520the%2520GW%2520distances%252C%250Aand%2520that%2520they%2520enjoy%2520additional%2520robustness%2520to%2520perturbations.%2520These%2520results%250Aprovide%2520a%2520mathematically%2520rigorous%2520basis%2520for%2520using%2520our%2520robust%2520partial%2520GW%250Adistances%2520in%2520applications%2520where%2520outliers%2520and%2520partial%2520matching%2520are%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20properties%20of%20partial%20and%20robust%20Gromov-Wasserstein%20distances&entry.906535625=Jannatul%20Chhoa%20and%20Michael%20Ivanitskiy%20and%20Fushuai%20Jiang%20and%20Shiying%20Li%20and%20Daniel%20McBride%20and%20Tom%20Needham%20and%20Kaiying%20O%27Hare&entry.1292438233=%20%20The%20Gromov-Wasserstein%20%28GW%29%20distances%20define%20a%20family%20of%20metrics%2C%20based%20on%0Aideas%20from%20optimal%20transport%2C%20which%20enable%20comparisons%20between%20probability%0Ameasures%20defined%20on%20distinct%20metric%20spaces.%20They%20are%20particularly%20useful%20in%0Aareas%20such%20as%20network%20analysis%20and%20geometry%20processing%2C%20as%20computation%20of%20a%20GW%0Adistance%20involves%20solving%20for%20registration%20between%20the%20objects%20which%20minimizes%0Ageometric%20distortion.%20Although%20GW%20distances%20have%20proven%20useful%20for%20various%0Aapplications%20in%20the%20recent%20machine%20learning%20literature%2C%20it%20has%20been%20observed%0Athat%20they%20are%20inherently%20sensitive%20to%20outlier%20noise%20and%20cannot%20accommodate%0Apartial%20matching.%20This%20has%20been%20addressed%20by%20various%20constructions%20building%20on%0Athe%20GW%20framework%3B%20in%20this%20article%2C%20we%20focus%20specifically%20on%20a%20natural%0Arelaxation%20of%20the%20GW%20optimization%20problem%2C%20introduced%20by%20Chapel%20et%20al.%2C%20which%0Ais%20aimed%20at%20addressing%20exactly%20these%20shortcomings.%20Our%20goal%20is%20to%20understand%0Athe%20theoretical%20properties%20of%20this%20relaxed%20optimization%20problem%2C%20from%20the%0Aviewpoint%20of%20metric%20geometry.%20While%20the%20relaxed%20problem%20fails%20to%20induce%20a%0Ametric%2C%20we%20derive%20precise%20characterizations%20of%20how%20it%20fails%20the%20axioms%20of%0Anon-degeneracy%20and%20triangle%20inequality.%20These%20observations%20lead%20us%20to%20define%20a%0Anovel%20family%20of%20distances%2C%20whose%20construction%20is%20inspired%20by%20the%20Prokhorov%20and%0AKy%20Fan%20distances%2C%20as%20well%20as%20by%20the%20recent%20work%20of%20Raghvendra%20et%20al.%5C%20on%20robust%0Aversions%20of%20classical%20Wasserstein%20distance.%20We%20show%20that%20our%20new%20distances%0Adefine%20true%20metrics%2C%20that%20they%20induce%20the%20same%20topology%20as%20the%20GW%20distances%2C%0Aand%20that%20they%20enjoy%20additional%20robustness%20to%20perturbations.%20These%20results%0Aprovide%20a%20mathematically%20rigorous%20basis%20for%20using%20our%20robust%20partial%20GW%0Adistances%20in%20applications%20where%20outliers%20and%20partial%20matching%20are%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02198v1&entry.124074799=Read"},
{"title": "The Enhancement of Software Delivery Performance through Enterprise\n  DevSecOps and Generative Artificial Intelligence in Chinese Technology Firms", "author": "Jun Cui", "abstract": "  This study investigates the impact of integrating DevSecOps and Generative\nArtificial Intelligence (GAI) on software delivery performance within\ntechnology firms. Utilizing a qualitative research methodology, the research\ninvolved semi-structured interviews with industry practitioners and analysis of\ncase studies from organizations that have successfully implemented these\nmethodologies. The findings reveal significant enhancements in research and\ndevelopment (R&D) efficiency, improved source code management, and heightened\nsoftware quality and security. The integration of GAI facilitated automation of\ncoding tasks and predictive analytics, while DevSecOps ensured that security\nmeasures were embedded throughout the development lifecycle. Despite the\npromising results, the study identifies gaps related to the generalizability of\nthe findings due to the limited sample size and the qualitative nature of the\nresearch. This paper contributes valuable insights into the practical\nimplementation of DevSecOps and GAI, highlighting their potential to transform\nsoftware delivery processes in technology firms. Future research directions\ninclude quantitative assessments of the impact on specific business outcomes\nand comparative studies across different industries.\n", "link": "http://arxiv.org/abs/2411.02255v1", "date": "2024-11-04", "relevancy": 2.2361, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4774}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4355}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Enhancement%20of%20Software%20Delivery%20Performance%20through%20Enterprise%0A%20%20DevSecOps%20and%20Generative%20Artificial%20Intelligence%20in%20Chinese%20Technology%20Firms&body=Title%3A%20The%20Enhancement%20of%20Software%20Delivery%20Performance%20through%20Enterprise%0A%20%20DevSecOps%20and%20Generative%20Artificial%20Intelligence%20in%20Chinese%20Technology%20Firms%0AAuthor%3A%20Jun%20Cui%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20impact%20of%20integrating%20DevSecOps%20and%20Generative%0AArtificial%20Intelligence%20%28GAI%29%20on%20software%20delivery%20performance%20within%0Atechnology%20firms.%20Utilizing%20a%20qualitative%20research%20methodology%2C%20the%20research%0Ainvolved%20semi-structured%20interviews%20with%20industry%20practitioners%20and%20analysis%20of%0Acase%20studies%20from%20organizations%20that%20have%20successfully%20implemented%20these%0Amethodologies.%20The%20findings%20reveal%20significant%20enhancements%20in%20research%20and%0Adevelopment%20%28R%26D%29%20efficiency%2C%20improved%20source%20code%20management%2C%20and%20heightened%0Asoftware%20quality%20and%20security.%20The%20integration%20of%20GAI%20facilitated%20automation%20of%0Acoding%20tasks%20and%20predictive%20analytics%2C%20while%20DevSecOps%20ensured%20that%20security%0Ameasures%20were%20embedded%20throughout%20the%20development%20lifecycle.%20Despite%20the%0Apromising%20results%2C%20the%20study%20identifies%20gaps%20related%20to%20the%20generalizability%20of%0Athe%20findings%20due%20to%20the%20limited%20sample%20size%20and%20the%20qualitative%20nature%20of%20the%0Aresearch.%20This%20paper%20contributes%20valuable%20insights%20into%20the%20practical%0Aimplementation%20of%20DevSecOps%20and%20GAI%2C%20highlighting%20their%20potential%20to%20transform%0Asoftware%20delivery%20processes%20in%20technology%20firms.%20Future%20research%20directions%0Ainclude%20quantitative%20assessments%20of%20the%20impact%20on%20specific%20business%20outcomes%0Aand%20comparative%20studies%20across%20different%20industries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Enhancement%2520of%2520Software%2520Delivery%2520Performance%2520through%2520Enterprise%250A%2520%2520DevSecOps%2520and%2520Generative%2520Artificial%2520Intelligence%2520in%2520Chinese%2520Technology%2520Firms%26entry.906535625%3DJun%2520Cui%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520impact%2520of%2520integrating%2520DevSecOps%2520and%2520Generative%250AArtificial%2520Intelligence%2520%2528GAI%2529%2520on%2520software%2520delivery%2520performance%2520within%250Atechnology%2520firms.%2520Utilizing%2520a%2520qualitative%2520research%2520methodology%252C%2520the%2520research%250Ainvolved%2520semi-structured%2520interviews%2520with%2520industry%2520practitioners%2520and%2520analysis%2520of%250Acase%2520studies%2520from%2520organizations%2520that%2520have%2520successfully%2520implemented%2520these%250Amethodologies.%2520The%2520findings%2520reveal%2520significant%2520enhancements%2520in%2520research%2520and%250Adevelopment%2520%2528R%2526D%2529%2520efficiency%252C%2520improved%2520source%2520code%2520management%252C%2520and%2520heightened%250Asoftware%2520quality%2520and%2520security.%2520The%2520integration%2520of%2520GAI%2520facilitated%2520automation%2520of%250Acoding%2520tasks%2520and%2520predictive%2520analytics%252C%2520while%2520DevSecOps%2520ensured%2520that%2520security%250Ameasures%2520were%2520embedded%2520throughout%2520the%2520development%2520lifecycle.%2520Despite%2520the%250Apromising%2520results%252C%2520the%2520study%2520identifies%2520gaps%2520related%2520to%2520the%2520generalizability%2520of%250Athe%2520findings%2520due%2520to%2520the%2520limited%2520sample%2520size%2520and%2520the%2520qualitative%2520nature%2520of%2520the%250Aresearch.%2520This%2520paper%2520contributes%2520valuable%2520insights%2520into%2520the%2520practical%250Aimplementation%2520of%2520DevSecOps%2520and%2520GAI%252C%2520highlighting%2520their%2520potential%2520to%2520transform%250Asoftware%2520delivery%2520processes%2520in%2520technology%2520firms.%2520Future%2520research%2520directions%250Ainclude%2520quantitative%2520assessments%2520of%2520the%2520impact%2520on%2520specific%2520business%2520outcomes%250Aand%2520comparative%2520studies%2520across%2520different%2520industries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Enhancement%20of%20Software%20Delivery%20Performance%20through%20Enterprise%0A%20%20DevSecOps%20and%20Generative%20Artificial%20Intelligence%20in%20Chinese%20Technology%20Firms&entry.906535625=Jun%20Cui&entry.1292438233=%20%20This%20study%20investigates%20the%20impact%20of%20integrating%20DevSecOps%20and%20Generative%0AArtificial%20Intelligence%20%28GAI%29%20on%20software%20delivery%20performance%20within%0Atechnology%20firms.%20Utilizing%20a%20qualitative%20research%20methodology%2C%20the%20research%0Ainvolved%20semi-structured%20interviews%20with%20industry%20practitioners%20and%20analysis%20of%0Acase%20studies%20from%20organizations%20that%20have%20successfully%20implemented%20these%0Amethodologies.%20The%20findings%20reveal%20significant%20enhancements%20in%20research%20and%0Adevelopment%20%28R%26D%29%20efficiency%2C%20improved%20source%20code%20management%2C%20and%20heightened%0Asoftware%20quality%20and%20security.%20The%20integration%20of%20GAI%20facilitated%20automation%20of%0Acoding%20tasks%20and%20predictive%20analytics%2C%20while%20DevSecOps%20ensured%20that%20security%0Ameasures%20were%20embedded%20throughout%20the%20development%20lifecycle.%20Despite%20the%0Apromising%20results%2C%20the%20study%20identifies%20gaps%20related%20to%20the%20generalizability%20of%0Athe%20findings%20due%20to%20the%20limited%20sample%20size%20and%20the%20qualitative%20nature%20of%20the%0Aresearch.%20This%20paper%20contributes%20valuable%20insights%20into%20the%20practical%0Aimplementation%20of%20DevSecOps%20and%20GAI%2C%20highlighting%20their%20potential%20to%20transform%0Asoftware%20delivery%20processes%20in%20technology%20firms.%20Future%20research%20directions%0Ainclude%20quantitative%20assessments%20of%20the%20impact%20on%20specific%20business%20outcomes%0Aand%20comparative%20studies%20across%20different%20industries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02255v1&entry.124074799=Read"},
{"title": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "author": "Jyh-Jing Hwang and Runsheng Xu and Hubert Lin and Wei-Chih Hung and Jingwei Ji and Kristy Choi and Di Huang and Tong He and Paul Covington and Benjamin Sapp and Yin Zhou and James Guo and Dragomir Anguelov and Mingxing Tan", "abstract": "  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n", "link": "http://arxiv.org/abs/2410.23262v2", "date": "2024-11-04", "relevancy": 2.2338, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving&body=Title%3A%20EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving%0AAuthor%3A%20Jyh-Jing%20Hwang%20and%20Runsheng%20Xu%20and%20Hubert%20Lin%20and%20Wei-Chih%20Hung%20and%20Jingwei%20Ji%20and%20Kristy%20Choi%20and%20Di%20Huang%20and%20Tong%20He%20and%20Paul%20Covington%20and%20Benjamin%20Sapp%20and%20Yin%20Zhou%20and%20James%20Guo%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan%0AAbstract%3A%20%20%20We%20introduce%20EMMA%2C%20an%20End-to-end%20Multimodal%20Model%20for%20Autonomous%20driving.%0ABuilt%20on%20a%20multi-modal%20large%20language%20model%20foundation%2C%20EMMA%20directly%20maps%20raw%0Acamera%20sensor%20data%20into%20various%20driving-specific%20outputs%2C%20including%20planner%0Atrajectories%2C%20perception%20objects%2C%20and%20road%20graph%20elements.%20EMMA%20maximizes%20the%0Autility%20of%20world%20knowledge%20from%20the%20pre-trained%20large%20language%20models%2C%20by%0Arepresenting%20all%20non-sensor%20inputs%20%28e.g.%20navigation%20instructions%20and%20ego%0Avehicle%20status%29%20and%20outputs%20%28e.g.%20trajectories%20and%203D%20locations%29%20as%20natural%0Alanguage%20text.%20This%20approach%20allows%20EMMA%20to%20jointly%20process%20various%20driving%0Atasks%20in%20a%20unified%20language%20space%2C%20and%20generate%20the%20outputs%20for%20each%20task%20using%0Atask-specific%20prompts.%20Empirically%2C%20we%20demonstrate%20EMMA%27s%20effectiveness%20by%0Aachieving%20state-of-the-art%20performance%20in%20motion%20planning%20on%20nuScenes%20as%20well%0Aas%20competitive%20results%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29.%20EMMA%20also%0Ayields%20competitive%20results%20for%20camera-primary%203D%20object%20detection%20on%20the%20Waymo%0AOpen%20Dataset%20%28WOD%29.%20We%20show%20that%20co-training%20EMMA%20with%20planner%20trajectories%2C%0Aobject%20detection%2C%20and%20road%20graph%20tasks%20yields%20improvements%20across%20all%20three%0Adomains%2C%20highlighting%20EMMA%27s%20potential%20as%20a%20generalist%20model%20for%20autonomous%0Adriving%20applications.%20However%2C%20EMMA%20also%20exhibits%20certain%20limitations%3A%20it%20can%0Aprocess%20only%20a%20small%20amount%20of%20image%20frames%2C%20does%20not%20incorporate%20accurate%203D%0Asensing%20modalities%20like%20LiDAR%20or%20radar%20and%20is%20computationally%20expensive.%20We%0Ahope%20that%20our%20results%20will%20inspire%20further%20research%20to%20mitigate%20these%20issues%0Aand%20to%20further%20evolve%20the%20state%20of%20the%20art%20in%20autonomous%20driving%20model%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520End-to-End%2520Multimodal%2520Model%2520for%2520Autonomous%2520Driving%26entry.906535625%3DJyh-Jing%2520Hwang%2520and%2520Runsheng%2520Xu%2520and%2520Hubert%2520Lin%2520and%2520Wei-Chih%2520Hung%2520and%2520Jingwei%2520Ji%2520and%2520Kristy%2520Choi%2520and%2520Di%2520Huang%2520and%2520Tong%2520He%2520and%2520Paul%2520Covington%2520and%2520Benjamin%2520Sapp%2520and%2520Yin%2520Zhou%2520and%2520James%2520Guo%2520and%2520Dragomir%2520Anguelov%2520and%2520Mingxing%2520Tan%26entry.1292438233%3D%2520%2520We%2520introduce%2520EMMA%252C%2520an%2520End-to-end%2520Multimodal%2520Model%2520for%2520Autonomous%2520driving.%250ABuilt%2520on%2520a%2520multi-modal%2520large%2520language%2520model%2520foundation%252C%2520EMMA%2520directly%2520maps%2520raw%250Acamera%2520sensor%2520data%2520into%2520various%2520driving-specific%2520outputs%252C%2520including%2520planner%250Atrajectories%252C%2520perception%2520objects%252C%2520and%2520road%2520graph%2520elements.%2520EMMA%2520maximizes%2520the%250Autility%2520of%2520world%2520knowledge%2520from%2520the%2520pre-trained%2520large%2520language%2520models%252C%2520by%250Arepresenting%2520all%2520non-sensor%2520inputs%2520%2528e.g.%2520navigation%2520instructions%2520and%2520ego%250Avehicle%2520status%2529%2520and%2520outputs%2520%2528e.g.%2520trajectories%2520and%25203D%2520locations%2529%2520as%2520natural%250Alanguage%2520text.%2520This%2520approach%2520allows%2520EMMA%2520to%2520jointly%2520process%2520various%2520driving%250Atasks%2520in%2520a%2520unified%2520language%2520space%252C%2520and%2520generate%2520the%2520outputs%2520for%2520each%2520task%2520using%250Atask-specific%2520prompts.%2520Empirically%252C%2520we%2520demonstrate%2520EMMA%2527s%2520effectiveness%2520by%250Aachieving%2520state-of-the-art%2520performance%2520in%2520motion%2520planning%2520on%2520nuScenes%2520as%2520well%250Aas%2520competitive%2520results%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528WOMD%2529.%2520EMMA%2520also%250Ayields%2520competitive%2520results%2520for%2520camera-primary%25203D%2520object%2520detection%2520on%2520the%2520Waymo%250AOpen%2520Dataset%2520%2528WOD%2529.%2520We%2520show%2520that%2520co-training%2520EMMA%2520with%2520planner%2520trajectories%252C%250Aobject%2520detection%252C%2520and%2520road%2520graph%2520tasks%2520yields%2520improvements%2520across%2520all%2520three%250Adomains%252C%2520highlighting%2520EMMA%2527s%2520potential%2520as%2520a%2520generalist%2520model%2520for%2520autonomous%250Adriving%2520applications.%2520However%252C%2520EMMA%2520also%2520exhibits%2520certain%2520limitations%253A%2520it%2520can%250Aprocess%2520only%2520a%2520small%2520amount%2520of%2520image%2520frames%252C%2520does%2520not%2520incorporate%2520accurate%25203D%250Asensing%2520modalities%2520like%2520LiDAR%2520or%2520radar%2520and%2520is%2520computationally%2520expensive.%2520We%250Ahope%2520that%2520our%2520results%2520will%2520inspire%2520further%2520research%2520to%2520mitigate%2520these%2520issues%250Aand%2520to%2520further%2520evolve%2520the%2520state%2520of%2520the%2520art%2520in%2520autonomous%2520driving%2520model%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20End-to-End%20Multimodal%20Model%20for%20Autonomous%20Driving&entry.906535625=Jyh-Jing%20Hwang%20and%20Runsheng%20Xu%20and%20Hubert%20Lin%20and%20Wei-Chih%20Hung%20and%20Jingwei%20Ji%20and%20Kristy%20Choi%20and%20Di%20Huang%20and%20Tong%20He%20and%20Paul%20Covington%20and%20Benjamin%20Sapp%20and%20Yin%20Zhou%20and%20James%20Guo%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan&entry.1292438233=%20%20We%20introduce%20EMMA%2C%20an%20End-to-end%20Multimodal%20Model%20for%20Autonomous%20driving.%0ABuilt%20on%20a%20multi-modal%20large%20language%20model%20foundation%2C%20EMMA%20directly%20maps%20raw%0Acamera%20sensor%20data%20into%20various%20driving-specific%20outputs%2C%20including%20planner%0Atrajectories%2C%20perception%20objects%2C%20and%20road%20graph%20elements.%20EMMA%20maximizes%20the%0Autility%20of%20world%20knowledge%20from%20the%20pre-trained%20large%20language%20models%2C%20by%0Arepresenting%20all%20non-sensor%20inputs%20%28e.g.%20navigation%20instructions%20and%20ego%0Avehicle%20status%29%20and%20outputs%20%28e.g.%20trajectories%20and%203D%20locations%29%20as%20natural%0Alanguage%20text.%20This%20approach%20allows%20EMMA%20to%20jointly%20process%20various%20driving%0Atasks%20in%20a%20unified%20language%20space%2C%20and%20generate%20the%20outputs%20for%20each%20task%20using%0Atask-specific%20prompts.%20Empirically%2C%20we%20demonstrate%20EMMA%27s%20effectiveness%20by%0Aachieving%20state-of-the-art%20performance%20in%20motion%20planning%20on%20nuScenes%20as%20well%0Aas%20competitive%20results%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29.%20EMMA%20also%0Ayields%20competitive%20results%20for%20camera-primary%203D%20object%20detection%20on%20the%20Waymo%0AOpen%20Dataset%20%28WOD%29.%20We%20show%20that%20co-training%20EMMA%20with%20planner%20trajectories%2C%0Aobject%20detection%2C%20and%20road%20graph%20tasks%20yields%20improvements%20across%20all%20three%0Adomains%2C%20highlighting%20EMMA%27s%20potential%20as%20a%20generalist%20model%20for%20autonomous%0Adriving%20applications.%20However%2C%20EMMA%20also%20exhibits%20certain%20limitations%3A%20it%20can%0Aprocess%20only%20a%20small%20amount%20of%20image%20frames%2C%20does%20not%20incorporate%20accurate%203D%0Asensing%20modalities%20like%20LiDAR%20or%20radar%20and%20is%20computationally%20expensive.%20We%0Ahope%20that%20our%20results%20will%20inspire%20further%20research%20to%20mitigate%20these%20issues%0Aand%20to%20further%20evolve%20the%20state%20of%20the%20art%20in%20autonomous%20driving%20model%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23262v2&entry.124074799=Read"},
{"title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept\n  Space", "author": "Core Francisco Park and Maya Okawa and Andrew Lee and Ekdeep Singh Lubana and Hidenori Tanaka", "abstract": "  Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.\n", "link": "http://arxiv.org/abs/2406.19370v2", "date": "2024-11-04", "relevancy": 2.2326, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5821}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space&body=Title%3A%20Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space%0AAuthor%3A%20Core%20Francisco%20Park%20and%20Maya%20Okawa%20and%20Andrew%20Lee%20and%20Ekdeep%20Singh%20Lubana%20and%20Hidenori%20Tanaka%0AAbstract%3A%20%20%20Modern%20generative%20models%20demonstrate%20impressive%20capabilities%2C%20likely%20stemming%0Afrom%20an%20ability%20to%20identify%20and%20manipulate%20abstract%20concepts%20underlying%20their%0Atraining%20data.%20However%2C%20fundamental%20questions%20remain%3A%20what%20determines%20the%0Aconcepts%20a%20model%20learns%2C%20the%20order%20in%20which%20it%20learns%20them%2C%20and%20its%20ability%20to%0Amanipulate%20those%20concepts%3F%20To%20address%20these%20questions%2C%20we%20propose%20analyzing%20a%0Amodel%27s%20learning%20dynamics%20via%20a%20framework%20we%20call%20the%20concept%20space%2C%20where%20each%0Aaxis%20represents%20an%20independent%20concept%20underlying%20the%20data%20generating%20process.%0ABy%20characterizing%20learning%20dynamics%20in%20this%20space%2C%20we%20identify%20how%20the%20speed%20at%0Awhich%20a%20concept%20is%20learned%2C%20and%20hence%20the%20order%20of%20concept%20learning%2C%20is%0Acontrolled%20by%20properties%20of%20the%20data%20we%20term%20concept%20signal.%20Further%2C%20we%0Aobserve%20moments%20of%20sudden%20turns%20in%20the%20direction%20of%20a%20model%27s%20learning%20dynamics%0Ain%20concept%20space.%20Surprisingly%2C%20these%20points%20precisely%20correspond%20to%20the%0Aemergence%20of%20hidden%20capabilities%2C%20i.e.%2C%20where%20latent%20interventions%20show%20the%0Amodel%20possesses%20the%20capability%20to%20manipulate%20a%20concept%2C%20but%20these%20capabilities%0Acannot%20yet%20be%20elicited%20via%20naive%20input%20prompting.%20While%20our%20results%20focus%20on%0Asynthetically%20defined%20toy%20datasets%2C%20we%20hypothesize%20a%20general%20claim%20on%20emergence%0Aof%20hidden%20capabilities%20may%20hold%3A%20generative%20models%20possess%20latent%20capabilities%0Athat%20emerge%20suddenly%20and%20consistently%20during%20training%2C%20though%20a%20model%20might%20not%0Aexhibit%20these%20capabilities%20under%20naive%20input%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Hidden%2520Capabilities%253A%2520Exploring%2520Learning%2520Dynamics%2520in%2520Concept%250A%2520%2520Space%26entry.906535625%3DCore%2520Francisco%2520Park%2520and%2520Maya%2520Okawa%2520and%2520Andrew%2520Lee%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Hidenori%2520Tanaka%26entry.1292438233%3D%2520%2520Modern%2520generative%2520models%2520demonstrate%2520impressive%2520capabilities%252C%2520likely%2520stemming%250Afrom%2520an%2520ability%2520to%2520identify%2520and%2520manipulate%2520abstract%2520concepts%2520underlying%2520their%250Atraining%2520data.%2520However%252C%2520fundamental%2520questions%2520remain%253A%2520what%2520determines%2520the%250Aconcepts%2520a%2520model%2520learns%252C%2520the%2520order%2520in%2520which%2520it%2520learns%2520them%252C%2520and%2520its%2520ability%2520to%250Amanipulate%2520those%2520concepts%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520propose%2520analyzing%2520a%250Amodel%2527s%2520learning%2520dynamics%2520via%2520a%2520framework%2520we%2520call%2520the%2520concept%2520space%252C%2520where%2520each%250Aaxis%2520represents%2520an%2520independent%2520concept%2520underlying%2520the%2520data%2520generating%2520process.%250ABy%2520characterizing%2520learning%2520dynamics%2520in%2520this%2520space%252C%2520we%2520identify%2520how%2520the%2520speed%2520at%250Awhich%2520a%2520concept%2520is%2520learned%252C%2520and%2520hence%2520the%2520order%2520of%2520concept%2520learning%252C%2520is%250Acontrolled%2520by%2520properties%2520of%2520the%2520data%2520we%2520term%2520concept%2520signal.%2520Further%252C%2520we%250Aobserve%2520moments%2520of%2520sudden%2520turns%2520in%2520the%2520direction%2520of%2520a%2520model%2527s%2520learning%2520dynamics%250Ain%2520concept%2520space.%2520Surprisingly%252C%2520these%2520points%2520precisely%2520correspond%2520to%2520the%250Aemergence%2520of%2520hidden%2520capabilities%252C%2520i.e.%252C%2520where%2520latent%2520interventions%2520show%2520the%250Amodel%2520possesses%2520the%2520capability%2520to%2520manipulate%2520a%2520concept%252C%2520but%2520these%2520capabilities%250Acannot%2520yet%2520be%2520elicited%2520via%2520naive%2520input%2520prompting.%2520While%2520our%2520results%2520focus%2520on%250Asynthetically%2520defined%2520toy%2520datasets%252C%2520we%2520hypothesize%2520a%2520general%2520claim%2520on%2520emergence%250Aof%2520hidden%2520capabilities%2520may%2520hold%253A%2520generative%2520models%2520possess%2520latent%2520capabilities%250Athat%2520emerge%2520suddenly%2520and%2520consistently%2520during%2520training%252C%2520though%2520a%2520model%2520might%2520not%250Aexhibit%2520these%2520capabilities%2520under%2520naive%2520input%2520prompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Hidden%20Capabilities%3A%20Exploring%20Learning%20Dynamics%20in%20Concept%0A%20%20Space&entry.906535625=Core%20Francisco%20Park%20and%20Maya%20Okawa%20and%20Andrew%20Lee%20and%20Ekdeep%20Singh%20Lubana%20and%20Hidenori%20Tanaka&entry.1292438233=%20%20Modern%20generative%20models%20demonstrate%20impressive%20capabilities%2C%20likely%20stemming%0Afrom%20an%20ability%20to%20identify%20and%20manipulate%20abstract%20concepts%20underlying%20their%0Atraining%20data.%20However%2C%20fundamental%20questions%20remain%3A%20what%20determines%20the%0Aconcepts%20a%20model%20learns%2C%20the%20order%20in%20which%20it%20learns%20them%2C%20and%20its%20ability%20to%0Amanipulate%20those%20concepts%3F%20To%20address%20these%20questions%2C%20we%20propose%20analyzing%20a%0Amodel%27s%20learning%20dynamics%20via%20a%20framework%20we%20call%20the%20concept%20space%2C%20where%20each%0Aaxis%20represents%20an%20independent%20concept%20underlying%20the%20data%20generating%20process.%0ABy%20characterizing%20learning%20dynamics%20in%20this%20space%2C%20we%20identify%20how%20the%20speed%20at%0Awhich%20a%20concept%20is%20learned%2C%20and%20hence%20the%20order%20of%20concept%20learning%2C%20is%0Acontrolled%20by%20properties%20of%20the%20data%20we%20term%20concept%20signal.%20Further%2C%20we%0Aobserve%20moments%20of%20sudden%20turns%20in%20the%20direction%20of%20a%20model%27s%20learning%20dynamics%0Ain%20concept%20space.%20Surprisingly%2C%20these%20points%20precisely%20correspond%20to%20the%0Aemergence%20of%20hidden%20capabilities%2C%20i.e.%2C%20where%20latent%20interventions%20show%20the%0Amodel%20possesses%20the%20capability%20to%20manipulate%20a%20concept%2C%20but%20these%20capabilities%0Acannot%20yet%20be%20elicited%20via%20naive%20input%20prompting.%20While%20our%20results%20focus%20on%0Asynthetically%20defined%20toy%20datasets%2C%20we%20hypothesize%20a%20general%20claim%20on%20emergence%0Aof%20hidden%20capabilities%20may%20hold%3A%20generative%20models%20possess%20latent%20capabilities%0Athat%20emerge%20suddenly%20and%20consistently%20during%20training%2C%20though%20a%20model%20might%20not%0Aexhibit%20these%20capabilities%20under%20naive%20input%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19370v2&entry.124074799=Read"},
{"title": "Physics-informed neural networks viewpoint for solving the\n  Dyson-Schwinger equations of quantum electrodynamics", "author": "Rodrigo Carmo Terin", "abstract": "  We employ physics-informed neural networks (PINNs) to solve fundamental\nDyson-Schwinger integral equations in the theory of quantum electrodynamics\n(QED) in Euclidean space. Our approach uses neural networks to approximate the\nfermion wave function renormalization, dynamical mass function, and photon\npropagator. By integrating the Dyson-Schwinger equations into the loss\nfunction, the networks learn and predict solutions over a range of momenta and\nultraviolet cutoff values. This method can be extended to other quantum field\ntheories (QFTs), potentially paving the way for forefront applications of\nmachine learning within high-level theoretical physics.\n", "link": "http://arxiv.org/abs/2411.02177v1", "date": "2024-11-04", "relevancy": 2.2189, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.452}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4435}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics&body=Title%3A%20Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics%0AAuthor%3A%20Rodrigo%20Carmo%20Terin%0AAbstract%3A%20%20%20We%20employ%20physics-informed%20neural%20networks%20%28PINNs%29%20to%20solve%20fundamental%0ADyson-Schwinger%20integral%20equations%20in%20the%20theory%20of%20quantum%20electrodynamics%0A%28QED%29%20in%20Euclidean%20space.%20Our%20approach%20uses%20neural%20networks%20to%20approximate%20the%0Afermion%20wave%20function%20renormalization%2C%20dynamical%20mass%20function%2C%20and%20photon%0Apropagator.%20By%20integrating%20the%20Dyson-Schwinger%20equations%20into%20the%20loss%0Afunction%2C%20the%20networks%20learn%20and%20predict%20solutions%20over%20a%20range%20of%20momenta%20and%0Aultraviolet%20cutoff%20values.%20This%20method%20can%20be%20extended%20to%20other%20quantum%20field%0Atheories%20%28QFTs%29%2C%20potentially%20paving%20the%20way%20for%20forefront%20applications%20of%0Amachine%20learning%20within%20high-level%20theoretical%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520neural%2520networks%2520viewpoint%2520for%2520solving%2520the%250A%2520%2520Dyson-Schwinger%2520equations%2520of%2520quantum%2520electrodynamics%26entry.906535625%3DRodrigo%2520Carmo%2520Terin%26entry.1292438233%3D%2520%2520We%2520employ%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520to%2520solve%2520fundamental%250ADyson-Schwinger%2520integral%2520equations%2520in%2520the%2520theory%2520of%2520quantum%2520electrodynamics%250A%2528QED%2529%2520in%2520Euclidean%2520space.%2520Our%2520approach%2520uses%2520neural%2520networks%2520to%2520approximate%2520the%250Afermion%2520wave%2520function%2520renormalization%252C%2520dynamical%2520mass%2520function%252C%2520and%2520photon%250Apropagator.%2520By%2520integrating%2520the%2520Dyson-Schwinger%2520equations%2520into%2520the%2520loss%250Afunction%252C%2520the%2520networks%2520learn%2520and%2520predict%2520solutions%2520over%2520a%2520range%2520of%2520momenta%2520and%250Aultraviolet%2520cutoff%2520values.%2520This%2520method%2520can%2520be%2520extended%2520to%2520other%2520quantum%2520field%250Atheories%2520%2528QFTs%2529%252C%2520potentially%2520paving%2520the%2520way%2520for%2520forefront%2520applications%2520of%250Amachine%2520learning%2520within%2520high-level%2520theoretical%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20neural%20networks%20viewpoint%20for%20solving%20the%0A%20%20Dyson-Schwinger%20equations%20of%20quantum%20electrodynamics&entry.906535625=Rodrigo%20Carmo%20Terin&entry.1292438233=%20%20We%20employ%20physics-informed%20neural%20networks%20%28PINNs%29%20to%20solve%20fundamental%0ADyson-Schwinger%20integral%20equations%20in%20the%20theory%20of%20quantum%20electrodynamics%0A%28QED%29%20in%20Euclidean%20space.%20Our%20approach%20uses%20neural%20networks%20to%20approximate%20the%0Afermion%20wave%20function%20renormalization%2C%20dynamical%20mass%20function%2C%20and%20photon%0Apropagator.%20By%20integrating%20the%20Dyson-Schwinger%20equations%20into%20the%20loss%0Afunction%2C%20the%20networks%20learn%20and%20predict%20solutions%20over%20a%20range%20of%20momenta%20and%0Aultraviolet%20cutoff%20values.%20This%20method%20can%20be%20extended%20to%20other%20quantum%20field%0Atheories%20%28QFTs%29%2C%20potentially%20paving%20the%20way%20for%20forefront%20applications%20of%0Amachine%20learning%20within%20high-level%20theoretical%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02177v1&entry.124074799=Read"},
{"title": "DualDn: Dual-domain Denoising via Differentiable ISP", "author": "Ruikang Li and Yujin Wang and Shiqi Chen and Fan Zhang and Jinwei Gu and Tianfan Xue", "abstract": "  Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/\n", "link": "http://arxiv.org/abs/2409.18783v2", "date": "2024-11-04", "relevancy": 2.2181, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5781}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.567}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP&body=Title%3A%20DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP%0AAuthor%3A%20Ruikang%20Li%20and%20Yujin%20Wang%20and%20Shiqi%20Chen%20and%20Fan%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Image%20denoising%20is%20a%20critical%20component%20in%20a%20camera%27s%20Image%20Signal%20Processing%0A%28ISP%29%20pipeline.%20There%20are%20two%20typical%20ways%20to%20inject%20a%20denoiser%20into%20the%20ISP%0Apipeline%3A%20applying%20a%20denoiser%20directly%20to%20captured%20raw%20frames%20%28raw%20domain%29%20or%0Ato%20the%20ISP%27s%20output%20sRGB%20images%20%28sRGB%20domain%29.%20However%2C%20both%20approaches%20have%0Atheir%20limitations.%20Residual%20noise%20from%20raw-domain%20denoising%20can%20be%20amplified%20by%0Athe%20subsequent%20ISP%20processing%2C%20and%20the%20sRGB%20domain%20struggles%20to%20handle%0Aspatially%20varying%20noise%20since%20it%20only%20sees%20noise%20distorted%20by%20the%20ISP.%0AConsequently%2C%20most%20raw%20or%20sRGB%20domain%20denoising%20works%20only%20for%20specific%20noise%0Adistributions%20and%20ISP%20configurations.%20To%20address%20these%20challenges%2C%20we%20propose%0ADualDn%2C%20a%20novel%20learning-based%20dual-domain%20denoising.%20Unlike%20previous%0Asingle-domain%20denoising%2C%20DualDn%20consists%20of%20two%20denoising%20networks%3A%20one%20in%20the%0Araw%20domain%20and%20one%20in%20the%20sRGB%20domain.%20The%20raw%20domain%20denoising%20adapts%20to%0Asensor-specific%20noise%20as%20well%20as%20spatially%20varying%20noise%20levels%2C%20while%20the%20sRGB%0Adomain%20denoising%20adapts%20to%20ISP%20variations%20and%20removes%20residual%20noise%20amplified%0Aby%20the%20ISP.%20Both%20denoising%20networks%20are%20connected%20with%20a%20differentiable%20ISP%2C%0Awhich%20is%20trained%20end-to-end%20and%20discarded%20during%20the%20inference%20stage.%20With%20this%0Adesign%2C%20DualDn%20achieves%20greater%20generalizability%20compared%20to%20most%0Alearning-based%20denoising%20methods%2C%20as%20it%20can%20adapt%20to%20different%20unseen%20noises%2C%0AISP%20parameters%2C%20and%20even%20novel%20ISP%20pipelines.%20Experiments%20show%20that%20DualDn%0Aachieves%20state-of-the-art%20performance%20and%20can%20adapt%20to%20different%20denoising%0Aarchitectures.%20Moreover%2C%20DualDn%20can%20be%20used%20as%20a%20plug-and-play%20denoising%20module%0Awith%20real%20cameras%20without%20retraining%2C%20and%20still%20demonstrate%20better%20performance%0Athan%20commercial%20on-camera%20denoising.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//openimaginglab.github.io/DualDn/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualDn%253A%2520Dual-domain%2520Denoising%2520via%2520Differentiable%2520ISP%26entry.906535625%3DRuikang%2520Li%2520and%2520Yujin%2520Wang%2520and%2520Shiqi%2520Chen%2520and%2520Fan%2520Zhang%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Image%2520denoising%2520is%2520a%2520critical%2520component%2520in%2520a%2520camera%2527s%2520Image%2520Signal%2520Processing%250A%2528ISP%2529%2520pipeline.%2520There%2520are%2520two%2520typical%2520ways%2520to%2520inject%2520a%2520denoiser%2520into%2520the%2520ISP%250Apipeline%253A%2520applying%2520a%2520denoiser%2520directly%2520to%2520captured%2520raw%2520frames%2520%2528raw%2520domain%2529%2520or%250Ato%2520the%2520ISP%2527s%2520output%2520sRGB%2520images%2520%2528sRGB%2520domain%2529.%2520However%252C%2520both%2520approaches%2520have%250Atheir%2520limitations.%2520Residual%2520noise%2520from%2520raw-domain%2520denoising%2520can%2520be%2520amplified%2520by%250Athe%2520subsequent%2520ISP%2520processing%252C%2520and%2520the%2520sRGB%2520domain%2520struggles%2520to%2520handle%250Aspatially%2520varying%2520noise%2520since%2520it%2520only%2520sees%2520noise%2520distorted%2520by%2520the%2520ISP.%250AConsequently%252C%2520most%2520raw%2520or%2520sRGB%2520domain%2520denoising%2520works%2520only%2520for%2520specific%2520noise%250Adistributions%2520and%2520ISP%2520configurations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250ADualDn%252C%2520a%2520novel%2520learning-based%2520dual-domain%2520denoising.%2520Unlike%2520previous%250Asingle-domain%2520denoising%252C%2520DualDn%2520consists%2520of%2520two%2520denoising%2520networks%253A%2520one%2520in%2520the%250Araw%2520domain%2520and%2520one%2520in%2520the%2520sRGB%2520domain.%2520The%2520raw%2520domain%2520denoising%2520adapts%2520to%250Asensor-specific%2520noise%2520as%2520well%2520as%2520spatially%2520varying%2520noise%2520levels%252C%2520while%2520the%2520sRGB%250Adomain%2520denoising%2520adapts%2520to%2520ISP%2520variations%2520and%2520removes%2520residual%2520noise%2520amplified%250Aby%2520the%2520ISP.%2520Both%2520denoising%2520networks%2520are%2520connected%2520with%2520a%2520differentiable%2520ISP%252C%250Awhich%2520is%2520trained%2520end-to-end%2520and%2520discarded%2520during%2520the%2520inference%2520stage.%2520With%2520this%250Adesign%252C%2520DualDn%2520achieves%2520greater%2520generalizability%2520compared%2520to%2520most%250Alearning-based%2520denoising%2520methods%252C%2520as%2520it%2520can%2520adapt%2520to%2520different%2520unseen%2520noises%252C%250AISP%2520parameters%252C%2520and%2520even%2520novel%2520ISP%2520pipelines.%2520Experiments%2520show%2520that%2520DualDn%250Aachieves%2520state-of-the-art%2520performance%2520and%2520can%2520adapt%2520to%2520different%2520denoising%250Aarchitectures.%2520Moreover%252C%2520DualDn%2520can%2520be%2520used%2520as%2520a%2520plug-and-play%2520denoising%2520module%250Awith%2520real%2520cameras%2520without%2520retraining%252C%2520and%2520still%2520demonstrate%2520better%2520performance%250Athan%2520commercial%2520on-camera%2520denoising.%2520The%2520project%2520website%2520is%2520available%2520at%253A%250Ahttps%253A//openimaginglab.github.io/DualDn/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualDn%3A%20Dual-domain%20Denoising%20via%20Differentiable%20ISP&entry.906535625=Ruikang%20Li%20and%20Yujin%20Wang%20and%20Shiqi%20Chen%20and%20Fan%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Image%20denoising%20is%20a%20critical%20component%20in%20a%20camera%27s%20Image%20Signal%20Processing%0A%28ISP%29%20pipeline.%20There%20are%20two%20typical%20ways%20to%20inject%20a%20denoiser%20into%20the%20ISP%0Apipeline%3A%20applying%20a%20denoiser%20directly%20to%20captured%20raw%20frames%20%28raw%20domain%29%20or%0Ato%20the%20ISP%27s%20output%20sRGB%20images%20%28sRGB%20domain%29.%20However%2C%20both%20approaches%20have%0Atheir%20limitations.%20Residual%20noise%20from%20raw-domain%20denoising%20can%20be%20amplified%20by%0Athe%20subsequent%20ISP%20processing%2C%20and%20the%20sRGB%20domain%20struggles%20to%20handle%0Aspatially%20varying%20noise%20since%20it%20only%20sees%20noise%20distorted%20by%20the%20ISP.%0AConsequently%2C%20most%20raw%20or%20sRGB%20domain%20denoising%20works%20only%20for%20specific%20noise%0Adistributions%20and%20ISP%20configurations.%20To%20address%20these%20challenges%2C%20we%20propose%0ADualDn%2C%20a%20novel%20learning-based%20dual-domain%20denoising.%20Unlike%20previous%0Asingle-domain%20denoising%2C%20DualDn%20consists%20of%20two%20denoising%20networks%3A%20one%20in%20the%0Araw%20domain%20and%20one%20in%20the%20sRGB%20domain.%20The%20raw%20domain%20denoising%20adapts%20to%0Asensor-specific%20noise%20as%20well%20as%20spatially%20varying%20noise%20levels%2C%20while%20the%20sRGB%0Adomain%20denoising%20adapts%20to%20ISP%20variations%20and%20removes%20residual%20noise%20amplified%0Aby%20the%20ISP.%20Both%20denoising%20networks%20are%20connected%20with%20a%20differentiable%20ISP%2C%0Awhich%20is%20trained%20end-to-end%20and%20discarded%20during%20the%20inference%20stage.%20With%20this%0Adesign%2C%20DualDn%20achieves%20greater%20generalizability%20compared%20to%20most%0Alearning-based%20denoising%20methods%2C%20as%20it%20can%20adapt%20to%20different%20unseen%20noises%2C%0AISP%20parameters%2C%20and%20even%20novel%20ISP%20pipelines.%20Experiments%20show%20that%20DualDn%0Aachieves%20state-of-the-art%20performance%20and%20can%20adapt%20to%20different%20denoising%0Aarchitectures.%20Moreover%2C%20DualDn%20can%20be%20used%20as%20a%20plug-and-play%20denoising%20module%0Awith%20real%20cameras%20without%20retraining%2C%20and%20still%20demonstrate%20better%20performance%0Athan%20commercial%20on-camera%20denoising.%20The%20project%20website%20is%20available%20at%3A%0Ahttps%3A//openimaginglab.github.io/DualDn/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18783v2&entry.124074799=Read"},
{"title": "Grouped Discrete Representation for Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Object-Centric Learning (OCL) can discover objects in images or videos by\nsimply reconstructing the input. For better object discovery, representative\nOCL methods reconstruct the input as its Variational Autoencoder (VAE)\nintermediate representation, which suppresses pixel noises and promotes object\nseparability by discretizing continuous super-pixels with template features.\nHowever, treating features as units overlooks their composing attributes, thus\nimpeding model generalization; indexing features with scalar numbers loses\nattribute-level similarities and differences, thus hindering model convergence.\nWe propose \\textit{Grouped Discrete Representation} (GDR) for OCL. We decompose\nfeatures into combinatorial attributes via organized channel grouping, and\ncompose these attributes into discrete representation via tuple indexes.\nExperiments show that our GDR improves both Transformer- and Diffusion-based\nOCL methods consistently on various datasets. Visualizations show that our GDR\ncaptures better object separability.\n", "link": "http://arxiv.org/abs/2411.02299v1", "date": "2024-11-04", "relevancy": 2.2151, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5635}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5602}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&body=Title%3A%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Object-Centric%20Learning%20%28OCL%29%20can%20discover%20objects%20in%20images%20or%20videos%20by%0Asimply%20reconstructing%20the%20input.%20For%20better%20object%20discovery%2C%20representative%0AOCL%20methods%20reconstruct%20the%20input%20as%20its%20Variational%20Autoencoder%20%28VAE%29%0Aintermediate%20representation%2C%20which%20suppresses%20pixel%20noises%20and%20promotes%20object%0Aseparability%20by%20discretizing%20continuous%20super-pixels%20with%20template%20features.%0AHowever%2C%20treating%20features%20as%20units%20overlooks%20their%20composing%20attributes%2C%20thus%0Aimpeding%20model%20generalization%3B%20indexing%20features%20with%20scalar%20numbers%20loses%0Aattribute-level%20similarities%20and%20differences%2C%20thus%20hindering%20model%20convergence.%0AWe%20propose%20%5Ctextit%7BGrouped%20Discrete%20Representation%7D%20%28GDR%29%20for%20OCL.%20We%20decompose%0Afeatures%20into%20combinatorial%20attributes%20via%20organized%20channel%20grouping%2C%20and%0Acompose%20these%20attributes%20into%20discrete%20representation%20via%20tuple%20indexes.%0AExperiments%20show%20that%20our%20GDR%20improves%20both%20Transformer-%20and%20Diffusion-based%0AOCL%20methods%20consistently%20on%20various%20datasets.%20Visualizations%20show%20that%20our%20GDR%0Acaptures%20better%20object%20separability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrouped%2520Discrete%2520Representation%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520can%2520discover%2520objects%2520in%2520images%2520or%2520videos%2520by%250Asimply%2520reconstructing%2520the%2520input.%2520For%2520better%2520object%2520discovery%252C%2520representative%250AOCL%2520methods%2520reconstruct%2520the%2520input%2520as%2520its%2520Variational%2520Autoencoder%2520%2528VAE%2529%250Aintermediate%2520representation%252C%2520which%2520suppresses%2520pixel%2520noises%2520and%2520promotes%2520object%250Aseparability%2520by%2520discretizing%2520continuous%2520super-pixels%2520with%2520template%2520features.%250AHowever%252C%2520treating%2520features%2520as%2520units%2520overlooks%2520their%2520composing%2520attributes%252C%2520thus%250Aimpeding%2520model%2520generalization%253B%2520indexing%2520features%2520with%2520scalar%2520numbers%2520loses%250Aattribute-level%2520similarities%2520and%2520differences%252C%2520thus%2520hindering%2520model%2520convergence.%250AWe%2520propose%2520%255Ctextit%257BGrouped%2520Discrete%2520Representation%257D%2520%2528GDR%2529%2520for%2520OCL.%2520We%2520decompose%250Afeatures%2520into%2520combinatorial%2520attributes%2520via%2520organized%2520channel%2520grouping%252C%2520and%250Acompose%2520these%2520attributes%2520into%2520discrete%2520representation%2520via%2520tuple%2520indexes.%250AExperiments%2520show%2520that%2520our%2520GDR%2520improves%2520both%2520Transformer-%2520and%2520Diffusion-based%250AOCL%2520methods%2520consistently%2520on%2520various%2520datasets.%2520Visualizations%2520show%2520that%2520our%2520GDR%250Acaptures%2520better%2520object%2520separability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Object-Centric%20Learning%20%28OCL%29%20can%20discover%20objects%20in%20images%20or%20videos%20by%0Asimply%20reconstructing%20the%20input.%20For%20better%20object%20discovery%2C%20representative%0AOCL%20methods%20reconstruct%20the%20input%20as%20its%20Variational%20Autoencoder%20%28VAE%29%0Aintermediate%20representation%2C%20which%20suppresses%20pixel%20noises%20and%20promotes%20object%0Aseparability%20by%20discretizing%20continuous%20super-pixels%20with%20template%20features.%0AHowever%2C%20treating%20features%20as%20units%20overlooks%20their%20composing%20attributes%2C%20thus%0Aimpeding%20model%20generalization%3B%20indexing%20features%20with%20scalar%20numbers%20loses%0Aattribute-level%20similarities%20and%20differences%2C%20thus%20hindering%20model%20convergence.%0AWe%20propose%20%5Ctextit%7BGrouped%20Discrete%20Representation%7D%20%28GDR%29%20for%20OCL.%20We%20decompose%0Afeatures%20into%20combinatorial%20attributes%20via%20organized%20channel%20grouping%2C%20and%0Acompose%20these%20attributes%20into%20discrete%20representation%20via%20tuple%20indexes.%0AExperiments%20show%20that%20our%20GDR%20improves%20both%20Transformer-%20and%20Diffusion-based%0AOCL%20methods%20consistently%20on%20various%20datasets.%20Visualizations%20show%20that%20our%20GDR%0Acaptures%20better%20object%20separability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02299v1&entry.124074799=Read"},
{"title": "Exploiting Unlabeled Data with Multiple Expert Teachers for Open\n  Vocabulary Aerial Object Detection and Its Orientation Adaptation", "author": "Yan Li and Weiwei Guo and Xue Yang and Ning Liao and Shaofeng Zhang and Yi Yu and Wenxian Yu and Junchi Yan", "abstract": "  In recent years, aerial object detection has been increasingly pivotal in\nvarious earth observation applications. However, current algorithms are limited\nto detecting a set of pre-defined object categories, demanding sufficient\nannotated training samples, and fail to detect novel object categories. In this\npaper, we put forth a novel formulation of the aerial object detection problem,\nnamely open-vocabulary aerial object detection (OVAD), which can detect objects\nbeyond training categories without costly collecting new labeled data. We\npropose CastDet, a CLIP-activated student-teacher detection framework that\nserves as the first OVAD detector specifically designed for the challenging\naerial scenario, where objects often exhibit weak appearance features and\narbitrary orientations. Our framework integrates a robust localization teacher\nalong with several box selection strategies to generate high-quality proposals\nfor novel objects. Additionally, the RemoteCLIP model is adopted as an\nomniscient teacher, which provides rich knowledge to enhance classification\ncapabilities for novel categories. A dynamic label queue is devised to maintain\nhigh-quality pseudo-labels during training. By doing so, the proposed CastDet\nboosts not only novel object proposals but also classification. Furthermore, we\nextend our approach from horizontal OVAD to oriented OVAD with tailored\nalgorithm designs to effectively manage bounding box representation and\npseudo-label generation. Extensive experiments for both tasks on multiple\nexisting aerial object detection datasets demonstrate the effectiveness of our\napproach. The code is available at https://github.com/lizzy8587/CastDet.\n", "link": "http://arxiv.org/abs/2411.02057v1", "date": "2024-11-04", "relevancy": 2.2145, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Unlabeled%20Data%20with%20Multiple%20Expert%20Teachers%20for%20Open%0A%20%20Vocabulary%20Aerial%20Object%20Detection%20and%20Its%20Orientation%20Adaptation&body=Title%3A%20Exploiting%20Unlabeled%20Data%20with%20Multiple%20Expert%20Teachers%20for%20Open%0A%20%20Vocabulary%20Aerial%20Object%20Detection%20and%20Its%20Orientation%20Adaptation%0AAuthor%3A%20Yan%20Li%20and%20Weiwei%20Guo%20and%20Xue%20Yang%20and%20Ning%20Liao%20and%20Shaofeng%20Zhang%20and%20Yi%20Yu%20and%20Wenxian%20Yu%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20In%20recent%20years%2C%20aerial%20object%20detection%20has%20been%20increasingly%20pivotal%20in%0Avarious%20earth%20observation%20applications.%20However%2C%20current%20algorithms%20are%20limited%0Ato%20detecting%20a%20set%20of%20pre-defined%20object%20categories%2C%20demanding%20sufficient%0Aannotated%20training%20samples%2C%20and%20fail%20to%20detect%20novel%20object%20categories.%20In%20this%0Apaper%2C%20we%20put%20forth%20a%20novel%20formulation%20of%20the%20aerial%20object%20detection%20problem%2C%0Anamely%20open-vocabulary%20aerial%20object%20detection%20%28OVAD%29%2C%20which%20can%20detect%20objects%0Abeyond%20training%20categories%20without%20costly%20collecting%20new%20labeled%20data.%20We%0Apropose%20CastDet%2C%20a%20CLIP-activated%20student-teacher%20detection%20framework%20that%0Aserves%20as%20the%20first%20OVAD%20detector%20specifically%20designed%20for%20the%20challenging%0Aaerial%20scenario%2C%20where%20objects%20often%20exhibit%20weak%20appearance%20features%20and%0Aarbitrary%20orientations.%20Our%20framework%20integrates%20a%20robust%20localization%20teacher%0Aalong%20with%20several%20box%20selection%20strategies%20to%20generate%20high-quality%20proposals%0Afor%20novel%20objects.%20Additionally%2C%20the%20RemoteCLIP%20model%20is%20adopted%20as%20an%0Aomniscient%20teacher%2C%20which%20provides%20rich%20knowledge%20to%20enhance%20classification%0Acapabilities%20for%20novel%20categories.%20A%20dynamic%20label%20queue%20is%20devised%20to%20maintain%0Ahigh-quality%20pseudo-labels%20during%20training.%20By%20doing%20so%2C%20the%20proposed%20CastDet%0Aboosts%20not%20only%20novel%20object%20proposals%20but%20also%20classification.%20Furthermore%2C%20we%0Aextend%20our%20approach%20from%20horizontal%20OVAD%20to%20oriented%20OVAD%20with%20tailored%0Aalgorithm%20designs%20to%20effectively%20manage%20bounding%20box%20representation%20and%0Apseudo-label%20generation.%20Extensive%20experiments%20for%20both%20tasks%20on%20multiple%0Aexisting%20aerial%20object%20detection%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach.%20The%20code%20is%20available%20at%20https%3A//github.com/lizzy8587/CastDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Unlabeled%2520Data%2520with%2520Multiple%2520Expert%2520Teachers%2520for%2520Open%250A%2520%2520Vocabulary%2520Aerial%2520Object%2520Detection%2520and%2520Its%2520Orientation%2520Adaptation%26entry.906535625%3DYan%2520Li%2520and%2520Weiwei%2520Guo%2520and%2520Xue%2520Yang%2520and%2520Ning%2520Liao%2520and%2520Shaofeng%2520Zhang%2520and%2520Yi%2520Yu%2520and%2520Wenxian%2520Yu%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520aerial%2520object%2520detection%2520has%2520been%2520increasingly%2520pivotal%2520in%250Avarious%2520earth%2520observation%2520applications.%2520However%252C%2520current%2520algorithms%2520are%2520limited%250Ato%2520detecting%2520a%2520set%2520of%2520pre-defined%2520object%2520categories%252C%2520demanding%2520sufficient%250Aannotated%2520training%2520samples%252C%2520and%2520fail%2520to%2520detect%2520novel%2520object%2520categories.%2520In%2520this%250Apaper%252C%2520we%2520put%2520forth%2520a%2520novel%2520formulation%2520of%2520the%2520aerial%2520object%2520detection%2520problem%252C%250Anamely%2520open-vocabulary%2520aerial%2520object%2520detection%2520%2528OVAD%2529%252C%2520which%2520can%2520detect%2520objects%250Abeyond%2520training%2520categories%2520without%2520costly%2520collecting%2520new%2520labeled%2520data.%2520We%250Apropose%2520CastDet%252C%2520a%2520CLIP-activated%2520student-teacher%2520detection%2520framework%2520that%250Aserves%2520as%2520the%2520first%2520OVAD%2520detector%2520specifically%2520designed%2520for%2520the%2520challenging%250Aaerial%2520scenario%252C%2520where%2520objects%2520often%2520exhibit%2520weak%2520appearance%2520features%2520and%250Aarbitrary%2520orientations.%2520Our%2520framework%2520integrates%2520a%2520robust%2520localization%2520teacher%250Aalong%2520with%2520several%2520box%2520selection%2520strategies%2520to%2520generate%2520high-quality%2520proposals%250Afor%2520novel%2520objects.%2520Additionally%252C%2520the%2520RemoteCLIP%2520model%2520is%2520adopted%2520as%2520an%250Aomniscient%2520teacher%252C%2520which%2520provides%2520rich%2520knowledge%2520to%2520enhance%2520classification%250Acapabilities%2520for%2520novel%2520categories.%2520A%2520dynamic%2520label%2520queue%2520is%2520devised%2520to%2520maintain%250Ahigh-quality%2520pseudo-labels%2520during%2520training.%2520By%2520doing%2520so%252C%2520the%2520proposed%2520CastDet%250Aboosts%2520not%2520only%2520novel%2520object%2520proposals%2520but%2520also%2520classification.%2520Furthermore%252C%2520we%250Aextend%2520our%2520approach%2520from%2520horizontal%2520OVAD%2520to%2520oriented%2520OVAD%2520with%2520tailored%250Aalgorithm%2520designs%2520to%2520effectively%2520manage%2520bounding%2520box%2520representation%2520and%250Apseudo-label%2520generation.%2520Extensive%2520experiments%2520for%2520both%2520tasks%2520on%2520multiple%250Aexisting%2520aerial%2520object%2520detection%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/lizzy8587/CastDet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Unlabeled%20Data%20with%20Multiple%20Expert%20Teachers%20for%20Open%0A%20%20Vocabulary%20Aerial%20Object%20Detection%20and%20Its%20Orientation%20Adaptation&entry.906535625=Yan%20Li%20and%20Weiwei%20Guo%20and%20Xue%20Yang%20and%20Ning%20Liao%20and%20Shaofeng%20Zhang%20and%20Yi%20Yu%20and%20Wenxian%20Yu%20and%20Junchi%20Yan&entry.1292438233=%20%20In%20recent%20years%2C%20aerial%20object%20detection%20has%20been%20increasingly%20pivotal%20in%0Avarious%20earth%20observation%20applications.%20However%2C%20current%20algorithms%20are%20limited%0Ato%20detecting%20a%20set%20of%20pre-defined%20object%20categories%2C%20demanding%20sufficient%0Aannotated%20training%20samples%2C%20and%20fail%20to%20detect%20novel%20object%20categories.%20In%20this%0Apaper%2C%20we%20put%20forth%20a%20novel%20formulation%20of%20the%20aerial%20object%20detection%20problem%2C%0Anamely%20open-vocabulary%20aerial%20object%20detection%20%28OVAD%29%2C%20which%20can%20detect%20objects%0Abeyond%20training%20categories%20without%20costly%20collecting%20new%20labeled%20data.%20We%0Apropose%20CastDet%2C%20a%20CLIP-activated%20student-teacher%20detection%20framework%20that%0Aserves%20as%20the%20first%20OVAD%20detector%20specifically%20designed%20for%20the%20challenging%0Aaerial%20scenario%2C%20where%20objects%20often%20exhibit%20weak%20appearance%20features%20and%0Aarbitrary%20orientations.%20Our%20framework%20integrates%20a%20robust%20localization%20teacher%0Aalong%20with%20several%20box%20selection%20strategies%20to%20generate%20high-quality%20proposals%0Afor%20novel%20objects.%20Additionally%2C%20the%20RemoteCLIP%20model%20is%20adopted%20as%20an%0Aomniscient%20teacher%2C%20which%20provides%20rich%20knowledge%20to%20enhance%20classification%0Acapabilities%20for%20novel%20categories.%20A%20dynamic%20label%20queue%20is%20devised%20to%20maintain%0Ahigh-quality%20pseudo-labels%20during%20training.%20By%20doing%20so%2C%20the%20proposed%20CastDet%0Aboosts%20not%20only%20novel%20object%20proposals%20but%20also%20classification.%20Furthermore%2C%20we%0Aextend%20our%20approach%20from%20horizontal%20OVAD%20to%20oriented%20OVAD%20with%20tailored%0Aalgorithm%20designs%20to%20effectively%20manage%20bounding%20box%20representation%20and%0Apseudo-label%20generation.%20Extensive%20experiments%20for%20both%20tasks%20on%20multiple%0Aexisting%20aerial%20object%20detection%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach.%20The%20code%20is%20available%20at%20https%3A//github.com/lizzy8587/CastDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02057v1&entry.124074799=Read"},
{"title": "Sample-Efficient Private Learning of Mixtures of Gaussians", "author": "Hassan Ashtiani and Mahbod Majid and Shyam Narayanan", "abstract": "  We study the problem of learning mixtures of Gaussians with approximate\ndifferential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$\nsamples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians\nup to low total variation distance, with differential privacy. Our work\nimproves over the previous best result [AAL24b] (which required roughly $k^2\nd^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.\nMoreover, we give the first optimal bound for privately learning mixtures of\n$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the\nsample complexity for privately learning mixtures of univariate Gaussians is\nlinear in the number of components $k$, whereas the previous best sample\ncomplexity [AAL21] was quadratic in $k$. Our algorithms utilize various\ntechniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],\nsample compression for distributions [ABDH+20], and methods for bounding\nvolumes of sumsets.\n", "link": "http://arxiv.org/abs/2411.02298v1", "date": "2024-11-04", "relevancy": 2.1919, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4357}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Efficient%20Private%20Learning%20of%20Mixtures%20of%20Gaussians&body=Title%3A%20Sample-Efficient%20Private%20Learning%20of%20Mixtures%20of%20Gaussians%0AAuthor%3A%20Hassan%20Ashtiani%20and%20Mahbod%20Majid%20and%20Shyam%20Narayanan%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20mixtures%20of%20Gaussians%20with%20approximate%0Adifferential%20privacy.%20We%20prove%20that%20roughly%20%24kd%5E2%20%2B%20k%5E%7B1.5%7D%20d%5E%7B1.75%7D%20%2B%20k%5E2%20d%24%0Asamples%20suffice%20to%20learn%20a%20mixture%20of%20%24k%24%20arbitrary%20%24d%24-dimensional%20Gaussians%0Aup%20to%20low%20total%20variation%20distance%2C%20with%20differential%20privacy.%20Our%20work%0Aimproves%20over%20the%20previous%20best%20result%20%5BAAL24b%5D%20%28which%20required%20roughly%20%24k%5E2%0Ad%5E4%24%20samples%29%20and%20is%20provably%20optimal%20when%20%24d%24%20is%20much%20larger%20than%20%24k%5E2%24.%0AMoreover%2C%20we%20give%20the%20first%20optimal%20bound%20for%20privately%20learning%20mixtures%20of%0A%24k%24%20univariate%20%28i.e.%2C%20%241%24-dimensional%29%20Gaussians.%20Importantly%2C%20we%20show%20that%20the%0Asample%20complexity%20for%20privately%20learning%20mixtures%20of%20univariate%20Gaussians%20is%0Alinear%20in%20the%20number%20of%20components%20%24k%24%2C%20whereas%20the%20previous%20best%20sample%0Acomplexity%20%5BAAL21%5D%20was%20quadratic%20in%20%24k%24.%20Our%20algorithms%20utilize%20various%0Atechniques%2C%20including%20the%20inverse%20sensitivity%20mechanism%20%5BAD20b%2C%20AD20a%2C%20HKMN23%5D%2C%0Asample%20compression%20for%20distributions%20%5BABDH%2B20%5D%2C%20and%20methods%20for%20bounding%0Avolumes%20of%20sumsets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Efficient%2520Private%2520Learning%2520of%2520Mixtures%2520of%2520Gaussians%26entry.906535625%3DHassan%2520Ashtiani%2520and%2520Mahbod%2520Majid%2520and%2520Shyam%2520Narayanan%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520mixtures%2520of%2520Gaussians%2520with%2520approximate%250Adifferential%2520privacy.%2520We%2520prove%2520that%2520roughly%2520%2524kd%255E2%2520%252B%2520k%255E%257B1.5%257D%2520d%255E%257B1.75%257D%2520%252B%2520k%255E2%2520d%2524%250Asamples%2520suffice%2520to%2520learn%2520a%2520mixture%2520of%2520%2524k%2524%2520arbitrary%2520%2524d%2524-dimensional%2520Gaussians%250Aup%2520to%2520low%2520total%2520variation%2520distance%252C%2520with%2520differential%2520privacy.%2520Our%2520work%250Aimproves%2520over%2520the%2520previous%2520best%2520result%2520%255BAAL24b%255D%2520%2528which%2520required%2520roughly%2520%2524k%255E2%250Ad%255E4%2524%2520samples%2529%2520and%2520is%2520provably%2520optimal%2520when%2520%2524d%2524%2520is%2520much%2520larger%2520than%2520%2524k%255E2%2524.%250AMoreover%252C%2520we%2520give%2520the%2520first%2520optimal%2520bound%2520for%2520privately%2520learning%2520mixtures%2520of%250A%2524k%2524%2520univariate%2520%2528i.e.%252C%2520%25241%2524-dimensional%2529%2520Gaussians.%2520Importantly%252C%2520we%2520show%2520that%2520the%250Asample%2520complexity%2520for%2520privately%2520learning%2520mixtures%2520of%2520univariate%2520Gaussians%2520is%250Alinear%2520in%2520the%2520number%2520of%2520components%2520%2524k%2524%252C%2520whereas%2520the%2520previous%2520best%2520sample%250Acomplexity%2520%255BAAL21%255D%2520was%2520quadratic%2520in%2520%2524k%2524.%2520Our%2520algorithms%2520utilize%2520various%250Atechniques%252C%2520including%2520the%2520inverse%2520sensitivity%2520mechanism%2520%255BAD20b%252C%2520AD20a%252C%2520HKMN23%255D%252C%250Asample%2520compression%2520for%2520distributions%2520%255BABDH%252B20%255D%252C%2520and%2520methods%2520for%2520bounding%250Avolumes%2520of%2520sumsets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Efficient%20Private%20Learning%20of%20Mixtures%20of%20Gaussians&entry.906535625=Hassan%20Ashtiani%20and%20Mahbod%20Majid%20and%20Shyam%20Narayanan&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20mixtures%20of%20Gaussians%20with%20approximate%0Adifferential%20privacy.%20We%20prove%20that%20roughly%20%24kd%5E2%20%2B%20k%5E%7B1.5%7D%20d%5E%7B1.75%7D%20%2B%20k%5E2%20d%24%0Asamples%20suffice%20to%20learn%20a%20mixture%20of%20%24k%24%20arbitrary%20%24d%24-dimensional%20Gaussians%0Aup%20to%20low%20total%20variation%20distance%2C%20with%20differential%20privacy.%20Our%20work%0Aimproves%20over%20the%20previous%20best%20result%20%5BAAL24b%5D%20%28which%20required%20roughly%20%24k%5E2%0Ad%5E4%24%20samples%29%20and%20is%20provably%20optimal%20when%20%24d%24%20is%20much%20larger%20than%20%24k%5E2%24.%0AMoreover%2C%20we%20give%20the%20first%20optimal%20bound%20for%20privately%20learning%20mixtures%20of%0A%24k%24%20univariate%20%28i.e.%2C%20%241%24-dimensional%29%20Gaussians.%20Importantly%2C%20we%20show%20that%20the%0Asample%20complexity%20for%20privately%20learning%20mixtures%20of%20univariate%20Gaussians%20is%0Alinear%20in%20the%20number%20of%20components%20%24k%24%2C%20whereas%20the%20previous%20best%20sample%0Acomplexity%20%5BAAL21%5D%20was%20quadratic%20in%20%24k%24.%20Our%20algorithms%20utilize%20various%0Atechniques%2C%20including%20the%20inverse%20sensitivity%20mechanism%20%5BAD20b%2C%20AD20a%2C%20HKMN23%5D%2C%0Asample%20compression%20for%20distributions%20%5BABDH%2B20%5D%2C%20and%20methods%20for%20bounding%0Avolumes%20of%20sumsets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02298v1&entry.124074799=Read"},
{"title": "Fast yet Safe: Early-Exiting with Risk Control", "author": "Metod Jazbec and Alexander Timans and Tin Had\u017ei Veljkovi\u0107 and Kaspar Sakmann and Dan Zhang and Christian A. Naesseth and Eric Nalisnick", "abstract": "  Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.\n", "link": "http://arxiv.org/abs/2405.20915v2", "date": "2024-11-04", "relevancy": 2.1875, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6089}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control&body=Title%3A%20Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control%0AAuthor%3A%20Metod%20Jazbec%20and%20Alexander%20Timans%20and%20Tin%20Had%C5%BEi%20Veljkovi%C4%87%20and%20Kaspar%20Sakmann%20and%20Dan%20Zhang%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick%0AAbstract%3A%20%20%20Scaling%20machine%20learning%20models%20significantly%20improves%20their%20performance.%0AHowever%2C%20such%20gains%20come%20at%20the%20cost%20of%20inference%20being%20slow%20and%0Aresource-intensive.%20Early-exit%20neural%20networks%20%28EENNs%29%20offer%20a%20promising%0Asolution%3A%20they%20accelerate%20inference%20by%20allowing%20intermediate%20layers%20to%20exit%20and%0Aproduce%20a%20prediction%20early.%20Yet%20a%20fundamental%20issue%20with%20EENNs%20is%20how%20to%0Adetermine%20when%20to%20exit%20without%20severely%20degrading%20performance.%20In%20other%20words%2C%0Awhen%20is%20it%20%27safe%27%20for%20an%20EENN%20to%20go%20%27fast%27%3F%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20how%20to%20adapt%20frameworks%20of%20risk%20control%20to%20EENNs.%20Risk%20control%0Aoffers%20a%20distribution-free%2C%20post-hoc%20solution%20that%20tunes%20the%20EENN%27s%20exiting%0Amechanism%20so%20that%20exits%20only%20occur%20when%20the%20output%20is%20of%20sufficient%20quality.%20We%0Aempirically%20validate%20our%20insights%20on%20a%20range%20of%20vision%20and%20language%20tasks%2C%0Ademonstrating%20that%20risk%20control%20can%20produce%20substantial%20computational%20savings%2C%0Aall%20the%20while%20preserving%20user-specified%20performance%20goals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520yet%2520Safe%253A%2520Early-Exiting%2520with%2520Risk%2520Control%26entry.906535625%3DMetod%2520Jazbec%2520and%2520Alexander%2520Timans%2520and%2520Tin%2520Had%25C5%25BEi%2520Veljkovi%25C4%2587%2520and%2520Kaspar%2520Sakmann%2520and%2520Dan%2520Zhang%2520and%2520Christian%2520A.%2520Naesseth%2520and%2520Eric%2520Nalisnick%26entry.1292438233%3D%2520%2520Scaling%2520machine%2520learning%2520models%2520significantly%2520improves%2520their%2520performance.%250AHowever%252C%2520such%2520gains%2520come%2520at%2520the%2520cost%2520of%2520inference%2520being%2520slow%2520and%250Aresource-intensive.%2520Early-exit%2520neural%2520networks%2520%2528EENNs%2529%2520offer%2520a%2520promising%250Asolution%253A%2520they%2520accelerate%2520inference%2520by%2520allowing%2520intermediate%2520layers%2520to%2520exit%2520and%250Aproduce%2520a%2520prediction%2520early.%2520Yet%2520a%2520fundamental%2520issue%2520with%2520EENNs%2520is%2520how%2520to%250Adetermine%2520when%2520to%2520exit%2520without%2520severely%2520degrading%2520performance.%2520In%2520other%2520words%252C%250Awhen%2520is%2520it%2520%2527safe%2527%2520for%2520an%2520EENN%2520to%2520go%2520%2527fast%2527%253F%2520To%2520address%2520this%2520issue%252C%2520we%250Ainvestigate%2520how%2520to%2520adapt%2520frameworks%2520of%2520risk%2520control%2520to%2520EENNs.%2520Risk%2520control%250Aoffers%2520a%2520distribution-free%252C%2520post-hoc%2520solution%2520that%2520tunes%2520the%2520EENN%2527s%2520exiting%250Amechanism%2520so%2520that%2520exits%2520only%2520occur%2520when%2520the%2520output%2520is%2520of%2520sufficient%2520quality.%2520We%250Aempirically%2520validate%2520our%2520insights%2520on%2520a%2520range%2520of%2520vision%2520and%2520language%2520tasks%252C%250Ademonstrating%2520that%2520risk%2520control%2520can%2520produce%2520substantial%2520computational%2520savings%252C%250Aall%2520the%2520while%2520preserving%2520user-specified%2520performance%2520goals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control&entry.906535625=Metod%20Jazbec%20and%20Alexander%20Timans%20and%20Tin%20Had%C5%BEi%20Veljkovi%C4%87%20and%20Kaspar%20Sakmann%20and%20Dan%20Zhang%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick&entry.1292438233=%20%20Scaling%20machine%20learning%20models%20significantly%20improves%20their%20performance.%0AHowever%2C%20such%20gains%20come%20at%20the%20cost%20of%20inference%20being%20slow%20and%0Aresource-intensive.%20Early-exit%20neural%20networks%20%28EENNs%29%20offer%20a%20promising%0Asolution%3A%20they%20accelerate%20inference%20by%20allowing%20intermediate%20layers%20to%20exit%20and%0Aproduce%20a%20prediction%20early.%20Yet%20a%20fundamental%20issue%20with%20EENNs%20is%20how%20to%0Adetermine%20when%20to%20exit%20without%20severely%20degrading%20performance.%20In%20other%20words%2C%0Awhen%20is%20it%20%27safe%27%20for%20an%20EENN%20to%20go%20%27fast%27%3F%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20how%20to%20adapt%20frameworks%20of%20risk%20control%20to%20EENNs.%20Risk%20control%0Aoffers%20a%20distribution-free%2C%20post-hoc%20solution%20that%20tunes%20the%20EENN%27s%20exiting%0Amechanism%20so%20that%20exits%20only%20occur%20when%20the%20output%20is%20of%20sufficient%20quality.%20We%0Aempirically%20validate%20our%20insights%20on%20a%20range%20of%20vision%20and%20language%20tasks%2C%0Ademonstrating%20that%20risk%20control%20can%20produce%20substantial%20computational%20savings%2C%0Aall%20the%20while%20preserving%20user-specified%20performance%20goals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20915v2&entry.124074799=Read"},
{"title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark", "author": "Ge Zhang and Xinrun Du and Bei Chen and Yiming Liang and Tongxu Luo and Tianyu Zheng and Kang Zhu and Yuyang Cheng and Chunpu Xu and Shuyue Guo and Haoran Zhang and Xingwei Qu and Junjie Wang and Ruibin Yuan and Yizhi Li and Zekun Wang and Yudong Liu and Yu-Hsuan Tsai and Fengji Zhang and Chenghua Lin and Wenhao Huang and Jie Fu", "abstract": "  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n", "link": "http://arxiv.org/abs/2401.11944v4", "date": "2024-11-04", "relevancy": 2.1869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&body=Title%3A%20CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark%0AAuthor%3A%20Ge%20Zhang%20and%20Xinrun%20Du%20and%20Bei%20Chen%20and%20Yiming%20Liang%20and%20Tongxu%20Luo%20and%20Tianyu%20Zheng%20and%20Kang%20Zhu%20and%20Yuyang%20Cheng%20and%20Chunpu%20Xu%20and%20Shuyue%20Guo%20and%20Haoran%20Zhang%20and%20Xingwei%20Qu%20and%20Junjie%20Wang%20and%20Ruibin%20Yuan%20and%20Yizhi%20Li%20and%20Zekun%20Wang%20and%20Yudong%20Liu%20and%20Yu-Hsuan%20Tsai%20and%20Fengji%20Zhang%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu%0AAbstract%3A%20%20%20As%20the%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29%20continue%20to%20advance%2C%0Aevaluating%20the%20performance%20of%20LMMs%20emerges%20as%20an%20increasing%20need.%20Additionally%2C%0Athere%20is%20an%20even%20larger%20gap%20in%20evaluating%20the%20advanced%20knowledge%20and%20reasoning%0Aabilities%20of%20LMMs%20in%20non-English%20contexts%20such%20as%20Chinese.%20We%20introduce%20CMMMU%2C%0Aa%20new%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%20benchmark%0Adesigned%20to%20evaluate%20LMMs%20on%20tasks%20demanding%20college-level%20subject%20knowledge%0Aand%20deliberate%20reasoning%20in%20a%20Chinese%20context.%20CMMMU%20is%20inspired%20by%20and%0Astrictly%20follows%20the%20annotation%20and%20analysis%20pattern%20of%20MMMU.%20CMMMU%20includes%0A12k%20manually%20collected%20multimodal%20questions%20from%20college%20exams%2C%20quizzes%2C%20and%0Atextbooks%2C%20covering%20six%20core%20disciplines%3A%20Art%20%26%20Design%2C%20Business%2C%20Science%2C%0AHealth%20%26%20Medicine%2C%20Humanities%20%26%20Social%20Science%2C%20and%20Tech%20%26%20Engineering%2C%20like%0Aits%20companion%2C%20MMMU.%20These%20questions%20span%2030%20subjects%20and%20comprise%2039%20highly%0Aheterogeneous%20image%20types%2C%20such%20as%20charts%2C%20diagrams%2C%20maps%2C%20tables%2C%20music%0Asheets%2C%20and%20chemical%20structures.%20CMMMU%20focuses%20on%20complex%20perception%20and%0Areasoning%20with%20domain-specific%20knowledge%20in%20the%20Chinese%20context.%20We%20evaluate%2011%0Aopen-source%20LLMs%20and%20one%20proprietary%20GPT-4V%28ision%29.%20Even%20GPT-4V%20only%20achieves%0Aaccuracies%20of%2042%25%2C%20indicating%20a%20large%20space%20for%20improvement.%20CMMMU%20will%20boost%0Athe%20community%20to%20build%20the%20next-generation%20LMMs%20towards%20expert%20artificial%0Aintelligence%20and%20promote%20the%20democratization%20of%20LMMs%20by%20providing%20diverse%0Alanguage%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11944v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMMMU%253A%2520A%2520Chinese%2520Massive%2520Multi-discipline%2520Multimodal%2520Understanding%250A%2520%2520Benchmark%26entry.906535625%3DGe%2520Zhang%2520and%2520Xinrun%2520Du%2520and%2520Bei%2520Chen%2520and%2520Yiming%2520Liang%2520and%2520Tongxu%2520Luo%2520and%2520Tianyu%2520Zheng%2520and%2520Kang%2520Zhu%2520and%2520Yuyang%2520Cheng%2520and%2520Chunpu%2520Xu%2520and%2520Shuyue%2520Guo%2520and%2520Haoran%2520Zhang%2520and%2520Xingwei%2520Qu%2520and%2520Junjie%2520Wang%2520and%2520Ruibin%2520Yuan%2520and%2520Yizhi%2520Li%2520and%2520Zekun%2520Wang%2520and%2520Yudong%2520Liu%2520and%2520Yu-Hsuan%2520Tsai%2520and%2520Fengji%2520Zhang%2520and%2520Chenghua%2520Lin%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520As%2520the%2520capabilities%2520of%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520continue%2520to%2520advance%252C%250Aevaluating%2520the%2520performance%2520of%2520LMMs%2520emerges%2520as%2520an%2520increasing%2520need.%2520Additionally%252C%250Athere%2520is%2520an%2520even%2520larger%2520gap%2520in%2520evaluating%2520the%2520advanced%2520knowledge%2520and%2520reasoning%250Aabilities%2520of%2520LMMs%2520in%2520non-English%2520contexts%2520such%2520as%2520Chinese.%2520We%2520introduce%2520CMMMU%252C%250Aa%2520new%2520Chinese%2520Massive%2520Multi-discipline%2520Multimodal%2520Understanding%2520benchmark%250Adesigned%2520to%2520evaluate%2520LMMs%2520on%2520tasks%2520demanding%2520college-level%2520subject%2520knowledge%250Aand%2520deliberate%2520reasoning%2520in%2520a%2520Chinese%2520context.%2520CMMMU%2520is%2520inspired%2520by%2520and%250Astrictly%2520follows%2520the%2520annotation%2520and%2520analysis%2520pattern%2520of%2520MMMU.%2520CMMMU%2520includes%250A12k%2520manually%2520collected%2520multimodal%2520questions%2520from%2520college%2520exams%252C%2520quizzes%252C%2520and%250Atextbooks%252C%2520covering%2520six%2520core%2520disciplines%253A%2520Art%2520%2526%2520Design%252C%2520Business%252C%2520Science%252C%250AHealth%2520%2526%2520Medicine%252C%2520Humanities%2520%2526%2520Social%2520Science%252C%2520and%2520Tech%2520%2526%2520Engineering%252C%2520like%250Aits%2520companion%252C%2520MMMU.%2520These%2520questions%2520span%252030%2520subjects%2520and%2520comprise%252039%2520highly%250Aheterogeneous%2520image%2520types%252C%2520such%2520as%2520charts%252C%2520diagrams%252C%2520maps%252C%2520tables%252C%2520music%250Asheets%252C%2520and%2520chemical%2520structures.%2520CMMMU%2520focuses%2520on%2520complex%2520perception%2520and%250Areasoning%2520with%2520domain-specific%2520knowledge%2520in%2520the%2520Chinese%2520context.%2520We%2520evaluate%252011%250Aopen-source%2520LLMs%2520and%2520one%2520proprietary%2520GPT-4V%2528ision%2529.%2520Even%2520GPT-4V%2520only%2520achieves%250Aaccuracies%2520of%252042%2525%252C%2520indicating%2520a%2520large%2520space%2520for%2520improvement.%2520CMMMU%2520will%2520boost%250Athe%2520community%2520to%2520build%2520the%2520next-generation%2520LMMs%2520towards%2520expert%2520artificial%250Aintelligence%2520and%2520promote%2520the%2520democratization%2520of%2520LMMs%2520by%2520providing%2520diverse%250Alanguage%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11944v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMMMU%3A%20A%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&entry.906535625=Ge%20Zhang%20and%20Xinrun%20Du%20and%20Bei%20Chen%20and%20Yiming%20Liang%20and%20Tongxu%20Luo%20and%20Tianyu%20Zheng%20and%20Kang%20Zhu%20and%20Yuyang%20Cheng%20and%20Chunpu%20Xu%20and%20Shuyue%20Guo%20and%20Haoran%20Zhang%20and%20Xingwei%20Qu%20and%20Junjie%20Wang%20and%20Ruibin%20Yuan%20and%20Yizhi%20Li%20and%20Zekun%20Wang%20and%20Yudong%20Liu%20and%20Yu-Hsuan%20Tsai%20and%20Fengji%20Zhang%20and%20Chenghua%20Lin%20and%20Wenhao%20Huang%20and%20Jie%20Fu&entry.1292438233=%20%20As%20the%20capabilities%20of%20large%20multimodal%20models%20%28LMMs%29%20continue%20to%20advance%2C%0Aevaluating%20the%20performance%20of%20LMMs%20emerges%20as%20an%20increasing%20need.%20Additionally%2C%0Athere%20is%20an%20even%20larger%20gap%20in%20evaluating%20the%20advanced%20knowledge%20and%20reasoning%0Aabilities%20of%20LMMs%20in%20non-English%20contexts%20such%20as%20Chinese.%20We%20introduce%20CMMMU%2C%0Aa%20new%20Chinese%20Massive%20Multi-discipline%20Multimodal%20Understanding%20benchmark%0Adesigned%20to%20evaluate%20LMMs%20on%20tasks%20demanding%20college-level%20subject%20knowledge%0Aand%20deliberate%20reasoning%20in%20a%20Chinese%20context.%20CMMMU%20is%20inspired%20by%20and%0Astrictly%20follows%20the%20annotation%20and%20analysis%20pattern%20of%20MMMU.%20CMMMU%20includes%0A12k%20manually%20collected%20multimodal%20questions%20from%20college%20exams%2C%20quizzes%2C%20and%0Atextbooks%2C%20covering%20six%20core%20disciplines%3A%20Art%20%26%20Design%2C%20Business%2C%20Science%2C%0AHealth%20%26%20Medicine%2C%20Humanities%20%26%20Social%20Science%2C%20and%20Tech%20%26%20Engineering%2C%20like%0Aits%20companion%2C%20MMMU.%20These%20questions%20span%2030%20subjects%20and%20comprise%2039%20highly%0Aheterogeneous%20image%20types%2C%20such%20as%20charts%2C%20diagrams%2C%20maps%2C%20tables%2C%20music%0Asheets%2C%20and%20chemical%20structures.%20CMMMU%20focuses%20on%20complex%20perception%20and%0Areasoning%20with%20domain-specific%20knowledge%20in%20the%20Chinese%20context.%20We%20evaluate%2011%0Aopen-source%20LLMs%20and%20one%20proprietary%20GPT-4V%28ision%29.%20Even%20GPT-4V%20only%20achieves%0Aaccuracies%20of%2042%25%2C%20indicating%20a%20large%20space%20for%20improvement.%20CMMMU%20will%20boost%0Athe%20community%20to%20build%20the%20next-generation%20LMMs%20towards%20expert%20artificial%0Aintelligence%20and%20promote%20the%20democratization%20of%20LMMs%20by%20providing%20diverse%0Alanguage%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11944v4&entry.124074799=Read"},
{"title": "Improving Domain Generalization in Self-supervised Monocular Depth\n  Estimation via Stabilized Adversarial Training", "author": "Yuanqi Yao and Gang Wu and Kui Jiang and Siao Liu and Jian Kuai and Xianming Liu and Junjun Jiang", "abstract": "  Learning a self-supervised Monocular Depth Estimation (MDE) model with great\ngeneralization remains significantly challenging. Despite the success of\nadversarial augmentation in the supervised learning generalization, naively\nincorporating it into self-supervised MDE models potentially causes\nover-regularization, suffering from severe performance degradation. In this\npaper, we conduct qualitative analysis and illuminate the main causes: (i)\ninherent sensitivity in the UNet-alike depth network and (ii) dual optimization\nconflict caused by over-regularization. To tackle these issues, we propose a\ngeneral adversarial training framework, named Stabilized Conflict-optimization\nAdversarial Training (SCAT), integrating adversarial data augmentation into\nself-supervised MDE methods to achieve a balance between stability and\ngeneralization. Specifically, we devise an effective scaling depth network that\ntunes the coefficients of long skip connection and effectively stabilizes the\ntraining process. Then, we propose a conflict gradient surgery strategy, which\nprogressively integrates the adversarial gradient and optimizes the model\ntoward a conflict-free direction. Extensive experiments on five benchmarks\ndemonstrate that SCAT can achieve state-of-the-art performance and\nsignificantly improve the generalization capability of existing self-supervised\nMDE methods.\n", "link": "http://arxiv.org/abs/2411.02149v1", "date": "2024-11-04", "relevancy": 2.1776, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Domain%20Generalization%20in%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation%20via%20Stabilized%20Adversarial%20Training&body=Title%3A%20Improving%20Domain%20Generalization%20in%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation%20via%20Stabilized%20Adversarial%20Training%0AAuthor%3A%20Yuanqi%20Yao%20and%20Gang%20Wu%20and%20Kui%20Jiang%20and%20Siao%20Liu%20and%20Jian%20Kuai%20and%20Xianming%20Liu%20and%20Junjun%20Jiang%0AAbstract%3A%20%20%20Learning%20a%20self-supervised%20Monocular%20Depth%20Estimation%20%28MDE%29%20model%20with%20great%0Ageneralization%20remains%20significantly%20challenging.%20Despite%20the%20success%20of%0Aadversarial%20augmentation%20in%20the%20supervised%20learning%20generalization%2C%20naively%0Aincorporating%20it%20into%20self-supervised%20MDE%20models%20potentially%20causes%0Aover-regularization%2C%20suffering%20from%20severe%20performance%20degradation.%20In%20this%0Apaper%2C%20we%20conduct%20qualitative%20analysis%20and%20illuminate%20the%20main%20causes%3A%20%28i%29%0Ainherent%20sensitivity%20in%20the%20UNet-alike%20depth%20network%20and%20%28ii%29%20dual%20optimization%0Aconflict%20caused%20by%20over-regularization.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%0Ageneral%20adversarial%20training%20framework%2C%20named%20Stabilized%20Conflict-optimization%0AAdversarial%20Training%20%28SCAT%29%2C%20integrating%20adversarial%20data%20augmentation%20into%0Aself-supervised%20MDE%20methods%20to%20achieve%20a%20balance%20between%20stability%20and%0Ageneralization.%20Specifically%2C%20we%20devise%20an%20effective%20scaling%20depth%20network%20that%0Atunes%20the%20coefficients%20of%20long%20skip%20connection%20and%20effectively%20stabilizes%20the%0Atraining%20process.%20Then%2C%20we%20propose%20a%20conflict%20gradient%20surgery%20strategy%2C%20which%0Aprogressively%20integrates%20the%20adversarial%20gradient%20and%20optimizes%20the%20model%0Atoward%20a%20conflict-free%20direction.%20Extensive%20experiments%20on%20five%20benchmarks%0Ademonstrate%20that%20SCAT%20can%20achieve%20state-of-the-art%20performance%20and%0Asignificantly%20improve%20the%20generalization%20capability%20of%20existing%20self-supervised%0AMDE%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Domain%2520Generalization%2520in%2520Self-supervised%2520Monocular%2520Depth%250A%2520%2520Estimation%2520via%2520Stabilized%2520Adversarial%2520Training%26entry.906535625%3DYuanqi%2520Yao%2520and%2520Gang%2520Wu%2520and%2520Kui%2520Jiang%2520and%2520Siao%2520Liu%2520and%2520Jian%2520Kuai%2520and%2520Xianming%2520Liu%2520and%2520Junjun%2520Jiang%26entry.1292438233%3D%2520%2520Learning%2520a%2520self-supervised%2520Monocular%2520Depth%2520Estimation%2520%2528MDE%2529%2520model%2520with%2520great%250Ageneralization%2520remains%2520significantly%2520challenging.%2520Despite%2520the%2520success%2520of%250Aadversarial%2520augmentation%2520in%2520the%2520supervised%2520learning%2520generalization%252C%2520naively%250Aincorporating%2520it%2520into%2520self-supervised%2520MDE%2520models%2520potentially%2520causes%250Aover-regularization%252C%2520suffering%2520from%2520severe%2520performance%2520degradation.%2520In%2520this%250Apaper%252C%2520we%2520conduct%2520qualitative%2520analysis%2520and%2520illuminate%2520the%2520main%2520causes%253A%2520%2528i%2529%250Ainherent%2520sensitivity%2520in%2520the%2520UNet-alike%2520depth%2520network%2520and%2520%2528ii%2529%2520dual%2520optimization%250Aconflict%2520caused%2520by%2520over-regularization.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520a%250Ageneral%2520adversarial%2520training%2520framework%252C%2520named%2520Stabilized%2520Conflict-optimization%250AAdversarial%2520Training%2520%2528SCAT%2529%252C%2520integrating%2520adversarial%2520data%2520augmentation%2520into%250Aself-supervised%2520MDE%2520methods%2520to%2520achieve%2520a%2520balance%2520between%2520stability%2520and%250Ageneralization.%2520Specifically%252C%2520we%2520devise%2520an%2520effective%2520scaling%2520depth%2520network%2520that%250Atunes%2520the%2520coefficients%2520of%2520long%2520skip%2520connection%2520and%2520effectively%2520stabilizes%2520the%250Atraining%2520process.%2520Then%252C%2520we%2520propose%2520a%2520conflict%2520gradient%2520surgery%2520strategy%252C%2520which%250Aprogressively%2520integrates%2520the%2520adversarial%2520gradient%2520and%2520optimizes%2520the%2520model%250Atoward%2520a%2520conflict-free%2520direction.%2520Extensive%2520experiments%2520on%2520five%2520benchmarks%250Ademonstrate%2520that%2520SCAT%2520can%2520achieve%2520state-of-the-art%2520performance%2520and%250Asignificantly%2520improve%2520the%2520generalization%2520capability%2520of%2520existing%2520self-supervised%250AMDE%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Domain%20Generalization%20in%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation%20via%20Stabilized%20Adversarial%20Training&entry.906535625=Yuanqi%20Yao%20and%20Gang%20Wu%20and%20Kui%20Jiang%20and%20Siao%20Liu%20and%20Jian%20Kuai%20and%20Xianming%20Liu%20and%20Junjun%20Jiang&entry.1292438233=%20%20Learning%20a%20self-supervised%20Monocular%20Depth%20Estimation%20%28MDE%29%20model%20with%20great%0Ageneralization%20remains%20significantly%20challenging.%20Despite%20the%20success%20of%0Aadversarial%20augmentation%20in%20the%20supervised%20learning%20generalization%2C%20naively%0Aincorporating%20it%20into%20self-supervised%20MDE%20models%20potentially%20causes%0Aover-regularization%2C%20suffering%20from%20severe%20performance%20degradation.%20In%20this%0Apaper%2C%20we%20conduct%20qualitative%20analysis%20and%20illuminate%20the%20main%20causes%3A%20%28i%29%0Ainherent%20sensitivity%20in%20the%20UNet-alike%20depth%20network%20and%20%28ii%29%20dual%20optimization%0Aconflict%20caused%20by%20over-regularization.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%0Ageneral%20adversarial%20training%20framework%2C%20named%20Stabilized%20Conflict-optimization%0AAdversarial%20Training%20%28SCAT%29%2C%20integrating%20adversarial%20data%20augmentation%20into%0Aself-supervised%20MDE%20methods%20to%20achieve%20a%20balance%20between%20stability%20and%0Ageneralization.%20Specifically%2C%20we%20devise%20an%20effective%20scaling%20depth%20network%20that%0Atunes%20the%20coefficients%20of%20long%20skip%20connection%20and%20effectively%20stabilizes%20the%0Atraining%20process.%20Then%2C%20we%20propose%20a%20conflict%20gradient%20surgery%20strategy%2C%20which%0Aprogressively%20integrates%20the%20adversarial%20gradient%20and%20optimizes%20the%20model%0Atoward%20a%20conflict-free%20direction.%20Extensive%20experiments%20on%20five%20benchmarks%0Ademonstrate%20that%20SCAT%20can%20achieve%20state-of-the-art%20performance%20and%0Asignificantly%20improve%20the%20generalization%20capability%20of%20existing%20self-supervised%0AMDE%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02149v1&entry.124074799=Read"},
{"title": "Framer: Interactive Frame Interpolation", "author": "Wen Wang and Qiuyu Wang and Kecheng Zheng and Hao Ouyang and Zhekai Chen and Biao Gong and Hao Chen and Yujun Shen and Chunhua Shen", "abstract": "  We propose Framer for interactive frame interpolation, which targets\nproducing smoothly transitioning frames between two images as per user\ncreativity. Concretely, besides taking the start and end frames as inputs, our\napproach supports customizing the transition process by tailoring the\ntrajectory of some selected keypoints. Such a design enjoys two clear benefits.\nFirst, incorporating human interaction mitigates the issue arising from\nnumerous possibilities of transforming one image to another, and in turn\nenables finer control of local motions. Second, as the most basic form of\ninteraction, keypoints help establish the correspondence across frames,\nenhancing the model to handle challenging cases (e.g., objects on the start and\nend frames are of different shapes and styles). It is noteworthy that our\nsystem also offers an \"autopilot\" mode, where we introduce a module to estimate\nthe keypoints and refine the trajectory automatically, to simplify the usage in\npractice. Extensive experimental results demonstrate the appealing performance\nof Framer on various applications, such as image morphing, time-lapse video\ngeneration, cartoon interpolation, etc. The code, the model, and the interface\nwill be released to facilitate further research.\n", "link": "http://arxiv.org/abs/2410.18978v2", "date": "2024-11-04", "relevancy": 2.1728, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5498}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5455}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framer%3A%20Interactive%20Frame%20Interpolation&body=Title%3A%20Framer%3A%20Interactive%20Frame%20Interpolation%0AAuthor%3A%20Wen%20Wang%20and%20Qiuyu%20Wang%20and%20Kecheng%20Zheng%20and%20Hao%20Ouyang%20and%20Zhekai%20Chen%20and%20Biao%20Gong%20and%20Hao%20Chen%20and%20Yujun%20Shen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20We%20propose%20Framer%20for%20interactive%20frame%20interpolation%2C%20which%20targets%0Aproducing%20smoothly%20transitioning%20frames%20between%20two%20images%20as%20per%20user%0Acreativity.%20Concretely%2C%20besides%20taking%20the%20start%20and%20end%20frames%20as%20inputs%2C%20our%0Aapproach%20supports%20customizing%20the%20transition%20process%20by%20tailoring%20the%0Atrajectory%20of%20some%20selected%20keypoints.%20Such%20a%20design%20enjoys%20two%20clear%20benefits.%0AFirst%2C%20incorporating%20human%20interaction%20mitigates%20the%20issue%20arising%20from%0Anumerous%20possibilities%20of%20transforming%20one%20image%20to%20another%2C%20and%20in%20turn%0Aenables%20finer%20control%20of%20local%20motions.%20Second%2C%20as%20the%20most%20basic%20form%20of%0Ainteraction%2C%20keypoints%20help%20establish%20the%20correspondence%20across%20frames%2C%0Aenhancing%20the%20model%20to%20handle%20challenging%20cases%20%28e.g.%2C%20objects%20on%20the%20start%20and%0Aend%20frames%20are%20of%20different%20shapes%20and%20styles%29.%20It%20is%20noteworthy%20that%20our%0Asystem%20also%20offers%20an%20%22autopilot%22%20mode%2C%20where%20we%20introduce%20a%20module%20to%20estimate%0Athe%20keypoints%20and%20refine%20the%20trajectory%20automatically%2C%20to%20simplify%20the%20usage%20in%0Apractice.%20Extensive%20experimental%20results%20demonstrate%20the%20appealing%20performance%0Aof%20Framer%20on%20various%20applications%2C%20such%20as%20image%20morphing%2C%20time-lapse%20video%0Ageneration%2C%20cartoon%20interpolation%2C%20etc.%20The%20code%2C%20the%20model%2C%20and%20the%20interface%0Awill%20be%20released%20to%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramer%253A%2520Interactive%2520Frame%2520Interpolation%26entry.906535625%3DWen%2520Wang%2520and%2520Qiuyu%2520Wang%2520and%2520Kecheng%2520Zheng%2520and%2520Hao%2520Ouyang%2520and%2520Zhekai%2520Chen%2520and%2520Biao%2520Gong%2520and%2520Hao%2520Chen%2520and%2520Yujun%2520Shen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520We%2520propose%2520Framer%2520for%2520interactive%2520frame%2520interpolation%252C%2520which%2520targets%250Aproducing%2520smoothly%2520transitioning%2520frames%2520between%2520two%2520images%2520as%2520per%2520user%250Acreativity.%2520Concretely%252C%2520besides%2520taking%2520the%2520start%2520and%2520end%2520frames%2520as%2520inputs%252C%2520our%250Aapproach%2520supports%2520customizing%2520the%2520transition%2520process%2520by%2520tailoring%2520the%250Atrajectory%2520of%2520some%2520selected%2520keypoints.%2520Such%2520a%2520design%2520enjoys%2520two%2520clear%2520benefits.%250AFirst%252C%2520incorporating%2520human%2520interaction%2520mitigates%2520the%2520issue%2520arising%2520from%250Anumerous%2520possibilities%2520of%2520transforming%2520one%2520image%2520to%2520another%252C%2520and%2520in%2520turn%250Aenables%2520finer%2520control%2520of%2520local%2520motions.%2520Second%252C%2520as%2520the%2520most%2520basic%2520form%2520of%250Ainteraction%252C%2520keypoints%2520help%2520establish%2520the%2520correspondence%2520across%2520frames%252C%250Aenhancing%2520the%2520model%2520to%2520handle%2520challenging%2520cases%2520%2528e.g.%252C%2520objects%2520on%2520the%2520start%2520and%250Aend%2520frames%2520are%2520of%2520different%2520shapes%2520and%2520styles%2529.%2520It%2520is%2520noteworthy%2520that%2520our%250Asystem%2520also%2520offers%2520an%2520%2522autopilot%2522%2520mode%252C%2520where%2520we%2520introduce%2520a%2520module%2520to%2520estimate%250Athe%2520keypoints%2520and%2520refine%2520the%2520trajectory%2520automatically%252C%2520to%2520simplify%2520the%2520usage%2520in%250Apractice.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520appealing%2520performance%250Aof%2520Framer%2520on%2520various%2520applications%252C%2520such%2520as%2520image%2520morphing%252C%2520time-lapse%2520video%250Ageneration%252C%2520cartoon%2520interpolation%252C%2520etc.%2520The%2520code%252C%2520the%2520model%252C%2520and%2520the%2520interface%250Awill%2520be%2520released%2520to%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framer%3A%20Interactive%20Frame%20Interpolation&entry.906535625=Wen%20Wang%20and%20Qiuyu%20Wang%20and%20Kecheng%20Zheng%20and%20Hao%20Ouyang%20and%20Zhekai%20Chen%20and%20Biao%20Gong%20and%20Hao%20Chen%20and%20Yujun%20Shen%20and%20Chunhua%20Shen&entry.1292438233=%20%20We%20propose%20Framer%20for%20interactive%20frame%20interpolation%2C%20which%20targets%0Aproducing%20smoothly%20transitioning%20frames%20between%20two%20images%20as%20per%20user%0Acreativity.%20Concretely%2C%20besides%20taking%20the%20start%20and%20end%20frames%20as%20inputs%2C%20our%0Aapproach%20supports%20customizing%20the%20transition%20process%20by%20tailoring%20the%0Atrajectory%20of%20some%20selected%20keypoints.%20Such%20a%20design%20enjoys%20two%20clear%20benefits.%0AFirst%2C%20incorporating%20human%20interaction%20mitigates%20the%20issue%20arising%20from%0Anumerous%20possibilities%20of%20transforming%20one%20image%20to%20another%2C%20and%20in%20turn%0Aenables%20finer%20control%20of%20local%20motions.%20Second%2C%20as%20the%20most%20basic%20form%20of%0Ainteraction%2C%20keypoints%20help%20establish%20the%20correspondence%20across%20frames%2C%0Aenhancing%20the%20model%20to%20handle%20challenging%20cases%20%28e.g.%2C%20objects%20on%20the%20start%20and%0Aend%20frames%20are%20of%20different%20shapes%20and%20styles%29.%20It%20is%20noteworthy%20that%20our%0Asystem%20also%20offers%20an%20%22autopilot%22%20mode%2C%20where%20we%20introduce%20a%20module%20to%20estimate%0Athe%20keypoints%20and%20refine%20the%20trajectory%20automatically%2C%20to%20simplify%20the%20usage%20in%0Apractice.%20Extensive%20experimental%20results%20demonstrate%20the%20appealing%20performance%0Aof%20Framer%20on%20various%20applications%2C%20such%20as%20image%20morphing%2C%20time-lapse%20video%0Ageneration%2C%20cartoon%20interpolation%2C%20etc.%20The%20code%2C%20the%20model%2C%20and%20the%20interface%0Awill%20be%20released%20to%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18978v2&entry.124074799=Read"},
{"title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality", "author": "Yiqin Zhao and Mallesham Dasari and Tian Guo", "abstract": "  High-quality environment lighting is the foundation of creating immersive\nuser experiences in mobile augmented reality (AR) applications. However,\nachieving visually coherent environment lighting estimation for Mobile AR is\nchallenging due to several key limitations associated with AR device sensing\ncapabilities, including limitations in device camera FoV and pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address their key limitations of\ngeneration hallucination and slow inference process. To do so, in this work, we\ndesign and implement a generative lighting estimation system called CleAR that\ncan produce high-quality and diverse environment maps in the format of\n360$^\\circ$ images. Specifically, we design a two-step generation pipeline\nguided by AR environment context data to ensure the results follow physical\nenvironment visual context and color appearances. To improve the estimation\nrobustness under different lighting conditions, we design a real-time\nrefinement component to adjust lighting estimation results on AR devices. To\ntrain and test our generative models, we curate a large-scale environment\nlighting estimation dataset with diverse lighting conditions. Through\nquantitative evaluation and user study, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy and\nrobustness. Moreover, CleAR supports real-time refinement of lighting\nestimation results, ensuring robust and timely environment lighting updates for\nAR applications. Our end-to-end generative estimation takes as fast as 3.2\nseconds, outperforming state-of-the-art methods by 110x.\n", "link": "http://arxiv.org/abs/2411.02179v1", "date": "2024-11-04", "relevancy": 2.1607, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5457}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality&body=Title%3A%20CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality%0AAuthor%3A%20Yiqin%20Zhao%20and%20Mallesham%20Dasari%20and%20Tian%20Guo%0AAbstract%3A%20%20%20High-quality%20environment%20lighting%20is%20the%20foundation%20of%20creating%20immersive%0Auser%20experiences%20in%20mobile%20augmented%20reality%20%28AR%29%20applications.%20However%2C%0Aachieving%20visually%20coherent%20environment%20lighting%20estimation%20for%20Mobile%20AR%20is%0Achallenging%20due%20to%20several%20key%20limitations%20associated%20with%20AR%20device%20sensing%0Acapabilities%2C%20including%20limitations%20in%20device%20camera%20FoV%20and%20pixel%20dynamic%0Aranges.%20Recent%20advancements%20in%20generative%20AI%2C%20which%20can%20generate%20high-quality%0Aimages%20from%20different%20types%20of%20prompts%2C%20including%20texts%20and%20images%2C%20present%20a%0Apotential%20solution%20for%20high-quality%20lighting%20estimation.%20Still%2C%20to%20effectively%0Ause%20generative%20image%20diffusion%20models%2C%20we%20must%20address%20their%20key%20limitations%20of%0Ageneration%20hallucination%20and%20slow%20inference%20process.%20To%20do%20so%2C%20in%20this%20work%2C%20we%0Adesign%20and%20implement%20a%20generative%20lighting%20estimation%20system%20called%20CleAR%20that%0Acan%20produce%20high-quality%20and%20diverse%20environment%20maps%20in%20the%20format%20of%0A360%24%5E%5Ccirc%24%20images.%20Specifically%2C%20we%20design%20a%20two-step%20generation%20pipeline%0Aguided%20by%20AR%20environment%20context%20data%20to%20ensure%20the%20results%20follow%20physical%0Aenvironment%20visual%20context%20and%20color%20appearances.%20To%20improve%20the%20estimation%0Arobustness%20under%20different%20lighting%20conditions%2C%20we%20design%20a%20real-time%0Arefinement%20component%20to%20adjust%20lighting%20estimation%20results%20on%20AR%20devices.%20To%0Atrain%20and%20test%20our%20generative%20models%2C%20we%20curate%20a%20large-scale%20environment%0Alighting%20estimation%20dataset%20with%20diverse%20lighting%20conditions.%20Through%0Aquantitative%20evaluation%20and%20user%20study%2C%20we%20show%20that%20CleAR%20outperforms%0Astate-of-the-art%20lighting%20estimation%20methods%20on%20both%20estimation%20accuracy%20and%0Arobustness.%20Moreover%2C%20CleAR%20supports%20real-time%20refinement%20of%20lighting%0Aestimation%20results%2C%20ensuring%20robust%20and%20timely%20environment%20lighting%20updates%20for%0AAR%20applications.%20Our%20end-to-end%20generative%20estimation%20takes%20as%20fast%20as%203.2%0Aseconds%2C%20outperforming%20state-of-the-art%20methods%20by%20110x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCleAR%253A%2520Robust%2520Context-Guided%2520Generative%2520Lighting%2520Estimation%2520for%2520Mobile%250A%2520%2520Augmented%2520Reality%26entry.906535625%3DYiqin%2520Zhao%2520and%2520Mallesham%2520Dasari%2520and%2520Tian%2520Guo%26entry.1292438233%3D%2520%2520High-quality%2520environment%2520lighting%2520is%2520the%2520foundation%2520of%2520creating%2520immersive%250Auser%2520experiences%2520in%2520mobile%2520augmented%2520reality%2520%2528AR%2529%2520applications.%2520However%252C%250Aachieving%2520visually%2520coherent%2520environment%2520lighting%2520estimation%2520for%2520Mobile%2520AR%2520is%250Achallenging%2520due%2520to%2520several%2520key%2520limitations%2520associated%2520with%2520AR%2520device%2520sensing%250Acapabilities%252C%2520including%2520limitations%2520in%2520device%2520camera%2520FoV%2520and%2520pixel%2520dynamic%250Aranges.%2520Recent%2520advancements%2520in%2520generative%2520AI%252C%2520which%2520can%2520generate%2520high-quality%250Aimages%2520from%2520different%2520types%2520of%2520prompts%252C%2520including%2520texts%2520and%2520images%252C%2520present%2520a%250Apotential%2520solution%2520for%2520high-quality%2520lighting%2520estimation.%2520Still%252C%2520to%2520effectively%250Ause%2520generative%2520image%2520diffusion%2520models%252C%2520we%2520must%2520address%2520their%2520key%2520limitations%2520of%250Ageneration%2520hallucination%2520and%2520slow%2520inference%2520process.%2520To%2520do%2520so%252C%2520in%2520this%2520work%252C%2520we%250Adesign%2520and%2520implement%2520a%2520generative%2520lighting%2520estimation%2520system%2520called%2520CleAR%2520that%250Acan%2520produce%2520high-quality%2520and%2520diverse%2520environment%2520maps%2520in%2520the%2520format%2520of%250A360%2524%255E%255Ccirc%2524%2520images.%2520Specifically%252C%2520we%2520design%2520a%2520two-step%2520generation%2520pipeline%250Aguided%2520by%2520AR%2520environment%2520context%2520data%2520to%2520ensure%2520the%2520results%2520follow%2520physical%250Aenvironment%2520visual%2520context%2520and%2520color%2520appearances.%2520To%2520improve%2520the%2520estimation%250Arobustness%2520under%2520different%2520lighting%2520conditions%252C%2520we%2520design%2520a%2520real-time%250Arefinement%2520component%2520to%2520adjust%2520lighting%2520estimation%2520results%2520on%2520AR%2520devices.%2520To%250Atrain%2520and%2520test%2520our%2520generative%2520models%252C%2520we%2520curate%2520a%2520large-scale%2520environment%250Alighting%2520estimation%2520dataset%2520with%2520diverse%2520lighting%2520conditions.%2520Through%250Aquantitative%2520evaluation%2520and%2520user%2520study%252C%2520we%2520show%2520that%2520CleAR%2520outperforms%250Astate-of-the-art%2520lighting%2520estimation%2520methods%2520on%2520both%2520estimation%2520accuracy%2520and%250Arobustness.%2520Moreover%252C%2520CleAR%2520supports%2520real-time%2520refinement%2520of%2520lighting%250Aestimation%2520results%252C%2520ensuring%2520robust%2520and%2520timely%2520environment%2520lighting%2520updates%2520for%250AAR%2520applications.%2520Our%2520end-to-end%2520generative%2520estimation%2520takes%2520as%2520fast%2520as%25203.2%250Aseconds%252C%2520outperforming%2520state-of-the-art%2520methods%2520by%2520110x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CleAR%3A%20Robust%20Context-Guided%20Generative%20Lighting%20Estimation%20for%20Mobile%0A%20%20Augmented%20Reality&entry.906535625=Yiqin%20Zhao%20and%20Mallesham%20Dasari%20and%20Tian%20Guo&entry.1292438233=%20%20High-quality%20environment%20lighting%20is%20the%20foundation%20of%20creating%20immersive%0Auser%20experiences%20in%20mobile%20augmented%20reality%20%28AR%29%20applications.%20However%2C%0Aachieving%20visually%20coherent%20environment%20lighting%20estimation%20for%20Mobile%20AR%20is%0Achallenging%20due%20to%20several%20key%20limitations%20associated%20with%20AR%20device%20sensing%0Acapabilities%2C%20including%20limitations%20in%20device%20camera%20FoV%20and%20pixel%20dynamic%0Aranges.%20Recent%20advancements%20in%20generative%20AI%2C%20which%20can%20generate%20high-quality%0Aimages%20from%20different%20types%20of%20prompts%2C%20including%20texts%20and%20images%2C%20present%20a%0Apotential%20solution%20for%20high-quality%20lighting%20estimation.%20Still%2C%20to%20effectively%0Ause%20generative%20image%20diffusion%20models%2C%20we%20must%20address%20their%20key%20limitations%20of%0Ageneration%20hallucination%20and%20slow%20inference%20process.%20To%20do%20so%2C%20in%20this%20work%2C%20we%0Adesign%20and%20implement%20a%20generative%20lighting%20estimation%20system%20called%20CleAR%20that%0Acan%20produce%20high-quality%20and%20diverse%20environment%20maps%20in%20the%20format%20of%0A360%24%5E%5Ccirc%24%20images.%20Specifically%2C%20we%20design%20a%20two-step%20generation%20pipeline%0Aguided%20by%20AR%20environment%20context%20data%20to%20ensure%20the%20results%20follow%20physical%0Aenvironment%20visual%20context%20and%20color%20appearances.%20To%20improve%20the%20estimation%0Arobustness%20under%20different%20lighting%20conditions%2C%20we%20design%20a%20real-time%0Arefinement%20component%20to%20adjust%20lighting%20estimation%20results%20on%20AR%20devices.%20To%0Atrain%20and%20test%20our%20generative%20models%2C%20we%20curate%20a%20large-scale%20environment%0Alighting%20estimation%20dataset%20with%20diverse%20lighting%20conditions.%20Through%0Aquantitative%20evaluation%20and%20user%20study%2C%20we%20show%20that%20CleAR%20outperforms%0Astate-of-the-art%20lighting%20estimation%20methods%20on%20both%20estimation%20accuracy%20and%0Arobustness.%20Moreover%2C%20CleAR%20supports%20real-time%20refinement%20of%20lighting%0Aestimation%20results%2C%20ensuring%20robust%20and%20timely%20environment%20lighting%20updates%20for%0AAR%20applications.%20Our%20end-to-end%20generative%20estimation%20takes%20as%20fast%20as%203.2%0Aseconds%2C%20outperforming%20state-of-the-art%20methods%20by%20110x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02179v1&entry.124074799=Read"},
{"title": "UniMAP: Universal SMILES-Graph Representation Learning", "author": "Shikun Feng and Lixin Yang and Yanwen Huang and Yuyan Ni and Weiying Ma and Yanyan Lan", "abstract": "  Molecular representation learning is fundamental for many drug related\napplications. Most existing molecular pre-training models are limited in using\nsingle molecular modality, either SMILES or graph representation. To\neffectively leverage both modalities, we argue that it is critical to capture\nthe fine-grained 'semantics' between SMILES and graph, because subtle\nsequence/graph differences may lead to contrary molecular properties. In this\npaper, we propose a universal SMILE-graph representation learning model, namely\nUniMAP. Firstly, an embedding layer is employed to obtain the token and\nnode/edge representation in SMILES and graph, respectively. A multi-layer\nTransformer is then utilized to conduct deep cross-modality fusion. Specially,\nfour kinds of pre-training tasks are designed for UniMAP, including Multi-Level\nCross-Modality Masking (CMM), SMILES-Graph Matching (SGM), Fragment-Level\nAlignment (FLA), and Domain Knowledge Learning (DKL). In this way, both global\n(i.e. SGM and DKL) and local (i.e. CMM and FLA) alignments are integrated to\nachieve comprehensive cross-modality fusion. We evaluate UniMAP on various\ndownstream tasks, i.e. molecular property prediction, drug-target affinity\nprediction and drug-drug interaction. Experimental results show that UniMAP\noutperforms current state-of-the-art pre-training methods.We also visualize the\nlearned representations to demonstrate the effect of multi-modality\nintegration.\n", "link": "http://arxiv.org/abs/2310.14216v2", "date": "2024-11-04", "relevancy": 2.1583, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMAP%3A%20Universal%20SMILES-Graph%20Representation%20Learning&body=Title%3A%20UniMAP%3A%20Universal%20SMILES-Graph%20Representation%20Learning%0AAuthor%3A%20Shikun%20Feng%20and%20Lixin%20Yang%20and%20Yanwen%20Huang%20and%20Yuyan%20Ni%20and%20Weiying%20Ma%20and%20Yanyan%20Lan%0AAbstract%3A%20%20%20Molecular%20representation%20learning%20is%20fundamental%20for%20many%20drug%20related%0Aapplications.%20Most%20existing%20molecular%20pre-training%20models%20are%20limited%20in%20using%0Asingle%20molecular%20modality%2C%20either%20SMILES%20or%20graph%20representation.%20To%0Aeffectively%20leverage%20both%20modalities%2C%20we%20argue%20that%20it%20is%20critical%20to%20capture%0Athe%20fine-grained%20%27semantics%27%20between%20SMILES%20and%20graph%2C%20because%20subtle%0Asequence/graph%20differences%20may%20lead%20to%20contrary%20molecular%20properties.%20In%20this%0Apaper%2C%20we%20propose%20a%20universal%20SMILE-graph%20representation%20learning%20model%2C%20namely%0AUniMAP.%20Firstly%2C%20an%20embedding%20layer%20is%20employed%20to%20obtain%20the%20token%20and%0Anode/edge%20representation%20in%20SMILES%20and%20graph%2C%20respectively.%20A%20multi-layer%0ATransformer%20is%20then%20utilized%20to%20conduct%20deep%20cross-modality%20fusion.%20Specially%2C%0Afour%20kinds%20of%20pre-training%20tasks%20are%20designed%20for%20UniMAP%2C%20including%20Multi-Level%0ACross-Modality%20Masking%20%28CMM%29%2C%20SMILES-Graph%20Matching%20%28SGM%29%2C%20Fragment-Level%0AAlignment%20%28FLA%29%2C%20and%20Domain%20Knowledge%20Learning%20%28DKL%29.%20In%20this%20way%2C%20both%20global%0A%28i.e.%20SGM%20and%20DKL%29%20and%20local%20%28i.e.%20CMM%20and%20FLA%29%20alignments%20are%20integrated%20to%0Aachieve%20comprehensive%20cross-modality%20fusion.%20We%20evaluate%20UniMAP%20on%20various%0Adownstream%20tasks%2C%20i.e.%20molecular%20property%20prediction%2C%20drug-target%20affinity%0Aprediction%20and%20drug-drug%20interaction.%20Experimental%20results%20show%20that%20UniMAP%0Aoutperforms%20current%20state-of-the-art%20pre-training%20methods.We%20also%20visualize%20the%0Alearned%20representations%20to%20demonstrate%20the%20effect%20of%20multi-modality%0Aintegration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMAP%253A%2520Universal%2520SMILES-Graph%2520Representation%2520Learning%26entry.906535625%3DShikun%2520Feng%2520and%2520Lixin%2520Yang%2520and%2520Yanwen%2520Huang%2520and%2520Yuyan%2520Ni%2520and%2520Weiying%2520Ma%2520and%2520Yanyan%2520Lan%26entry.1292438233%3D%2520%2520Molecular%2520representation%2520learning%2520is%2520fundamental%2520for%2520many%2520drug%2520related%250Aapplications.%2520Most%2520existing%2520molecular%2520pre-training%2520models%2520are%2520limited%2520in%2520using%250Asingle%2520molecular%2520modality%252C%2520either%2520SMILES%2520or%2520graph%2520representation.%2520To%250Aeffectively%2520leverage%2520both%2520modalities%252C%2520we%2520argue%2520that%2520it%2520is%2520critical%2520to%2520capture%250Athe%2520fine-grained%2520%2527semantics%2527%2520between%2520SMILES%2520and%2520graph%252C%2520because%2520subtle%250Asequence/graph%2520differences%2520may%2520lead%2520to%2520contrary%2520molecular%2520properties.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520universal%2520SMILE-graph%2520representation%2520learning%2520model%252C%2520namely%250AUniMAP.%2520Firstly%252C%2520an%2520embedding%2520layer%2520is%2520employed%2520to%2520obtain%2520the%2520token%2520and%250Anode/edge%2520representation%2520in%2520SMILES%2520and%2520graph%252C%2520respectively.%2520A%2520multi-layer%250ATransformer%2520is%2520then%2520utilized%2520to%2520conduct%2520deep%2520cross-modality%2520fusion.%2520Specially%252C%250Afour%2520kinds%2520of%2520pre-training%2520tasks%2520are%2520designed%2520for%2520UniMAP%252C%2520including%2520Multi-Level%250ACross-Modality%2520Masking%2520%2528CMM%2529%252C%2520SMILES-Graph%2520Matching%2520%2528SGM%2529%252C%2520Fragment-Level%250AAlignment%2520%2528FLA%2529%252C%2520and%2520Domain%2520Knowledge%2520Learning%2520%2528DKL%2529.%2520In%2520this%2520way%252C%2520both%2520global%250A%2528i.e.%2520SGM%2520and%2520DKL%2529%2520and%2520local%2520%2528i.e.%2520CMM%2520and%2520FLA%2529%2520alignments%2520are%2520integrated%2520to%250Aachieve%2520comprehensive%2520cross-modality%2520fusion.%2520We%2520evaluate%2520UniMAP%2520on%2520various%250Adownstream%2520tasks%252C%2520i.e.%2520molecular%2520property%2520prediction%252C%2520drug-target%2520affinity%250Aprediction%2520and%2520drug-drug%2520interaction.%2520Experimental%2520results%2520show%2520that%2520UniMAP%250Aoutperforms%2520current%2520state-of-the-art%2520pre-training%2520methods.We%2520also%2520visualize%2520the%250Alearned%2520representations%2520to%2520demonstrate%2520the%2520effect%2520of%2520multi-modality%250Aintegration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMAP%3A%20Universal%20SMILES-Graph%20Representation%20Learning&entry.906535625=Shikun%20Feng%20and%20Lixin%20Yang%20and%20Yanwen%20Huang%20and%20Yuyan%20Ni%20and%20Weiying%20Ma%20and%20Yanyan%20Lan&entry.1292438233=%20%20Molecular%20representation%20learning%20is%20fundamental%20for%20many%20drug%20related%0Aapplications.%20Most%20existing%20molecular%20pre-training%20models%20are%20limited%20in%20using%0Asingle%20molecular%20modality%2C%20either%20SMILES%20or%20graph%20representation.%20To%0Aeffectively%20leverage%20both%20modalities%2C%20we%20argue%20that%20it%20is%20critical%20to%20capture%0Athe%20fine-grained%20%27semantics%27%20between%20SMILES%20and%20graph%2C%20because%20subtle%0Asequence/graph%20differences%20may%20lead%20to%20contrary%20molecular%20properties.%20In%20this%0Apaper%2C%20we%20propose%20a%20universal%20SMILE-graph%20representation%20learning%20model%2C%20namely%0AUniMAP.%20Firstly%2C%20an%20embedding%20layer%20is%20employed%20to%20obtain%20the%20token%20and%0Anode/edge%20representation%20in%20SMILES%20and%20graph%2C%20respectively.%20A%20multi-layer%0ATransformer%20is%20then%20utilized%20to%20conduct%20deep%20cross-modality%20fusion.%20Specially%2C%0Afour%20kinds%20of%20pre-training%20tasks%20are%20designed%20for%20UniMAP%2C%20including%20Multi-Level%0ACross-Modality%20Masking%20%28CMM%29%2C%20SMILES-Graph%20Matching%20%28SGM%29%2C%20Fragment-Level%0AAlignment%20%28FLA%29%2C%20and%20Domain%20Knowledge%20Learning%20%28DKL%29.%20In%20this%20way%2C%20both%20global%0A%28i.e.%20SGM%20and%20DKL%29%20and%20local%20%28i.e.%20CMM%20and%20FLA%29%20alignments%20are%20integrated%20to%0Aachieve%20comprehensive%20cross-modality%20fusion.%20We%20evaluate%20UniMAP%20on%20various%0Adownstream%20tasks%2C%20i.e.%20molecular%20property%20prediction%2C%20drug-target%20affinity%0Aprediction%20and%20drug-drug%20interaction.%20Experimental%20results%20show%20that%20UniMAP%0Aoutperforms%20current%20state-of-the-art%20pre-training%20methods.We%20also%20visualize%20the%0Alearned%20representations%20to%20demonstrate%20the%20effect%20of%20multi-modality%0Aintegration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14216v2&entry.124074799=Read"},
{"title": "Do graph neural network states contain graph properties?", "author": "Tom Pelletreau-Duris and Ruud van Bakel and Michael Cochez", "abstract": "  Graph learning models achieve state-of-the-art performance on many tasks, but\nthis often requires increasingly large model sizes. Accordingly, the complexity\nof their representations increase. Explainability techniques (XAI) have made\nremarkable progress in the interpretability of ML models. However, the\nnon-relational nature of Graph Neural Networks (GNNs) make it difficult to\nreuse already existing XAI methods. While other works have focused on\ninstance-based explanation methods for GNNs, very few have investigated\nmodel-based methods and, to our knowledge, none have tried to probe the\nembedding of the GNNs for well-known structural graph properties. In this paper\nwe present a model agnostic explainability pipeline for Graph Neural Networks\n(GNNs) employing diagnostic classifiers. This pipeline aims to probe and\ninterpret the learned representations in GNNs across various architectures and\ndatasets, refining our understanding and trust in these models.\n", "link": "http://arxiv.org/abs/2411.02168v1", "date": "2024-11-04", "relevancy": 2.0634, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.416}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4157}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20graph%20neural%20network%20states%20contain%20graph%20properties%3F&body=Title%3A%20Do%20graph%20neural%20network%20states%20contain%20graph%20properties%3F%0AAuthor%3A%20Tom%20Pelletreau-Duris%20and%20Ruud%20van%20Bakel%20and%20Michael%20Cochez%0AAbstract%3A%20%20%20Graph%20learning%20models%20achieve%20state-of-the-art%20performance%20on%20many%20tasks%2C%20but%0Athis%20often%20requires%20increasingly%20large%20model%20sizes.%20Accordingly%2C%20the%20complexity%0Aof%20their%20representations%20increase.%20Explainability%20techniques%20%28XAI%29%20have%20made%0Aremarkable%20progress%20in%20the%20interpretability%20of%20ML%20models.%20However%2C%20the%0Anon-relational%20nature%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20make%20it%20difficult%20to%0Areuse%20already%20existing%20XAI%20methods.%20While%20other%20works%20have%20focused%20on%0Ainstance-based%20explanation%20methods%20for%20GNNs%2C%20very%20few%20have%20investigated%0Amodel-based%20methods%20and%2C%20to%20our%20knowledge%2C%20none%20have%20tried%20to%20probe%20the%0Aembedding%20of%20the%20GNNs%20for%20well-known%20structural%20graph%20properties.%20In%20this%20paper%0Awe%20present%20a%20model%20agnostic%20explainability%20pipeline%20for%20Graph%20Neural%20Networks%0A%28GNNs%29%20employing%20diagnostic%20classifiers.%20This%20pipeline%20aims%20to%20probe%20and%0Ainterpret%20the%20learned%20representations%20in%20GNNs%20across%20various%20architectures%20and%0Adatasets%2C%20refining%20our%20understanding%20and%20trust%20in%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520graph%2520neural%2520network%2520states%2520contain%2520graph%2520properties%253F%26entry.906535625%3DTom%2520Pelletreau-Duris%2520and%2520Ruud%2520van%2520Bakel%2520and%2520Michael%2520Cochez%26entry.1292438233%3D%2520%2520Graph%2520learning%2520models%2520achieve%2520state-of-the-art%2520performance%2520on%2520many%2520tasks%252C%2520but%250Athis%2520often%2520requires%2520increasingly%2520large%2520model%2520sizes.%2520Accordingly%252C%2520the%2520complexity%250Aof%2520their%2520representations%2520increase.%2520Explainability%2520techniques%2520%2528XAI%2529%2520have%2520made%250Aremarkable%2520progress%2520in%2520the%2520interpretability%2520of%2520ML%2520models.%2520However%252C%2520the%250Anon-relational%2520nature%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520make%2520it%2520difficult%2520to%250Areuse%2520already%2520existing%2520XAI%2520methods.%2520While%2520other%2520works%2520have%2520focused%2520on%250Ainstance-based%2520explanation%2520methods%2520for%2520GNNs%252C%2520very%2520few%2520have%2520investigated%250Amodel-based%2520methods%2520and%252C%2520to%2520our%2520knowledge%252C%2520none%2520have%2520tried%2520to%2520probe%2520the%250Aembedding%2520of%2520the%2520GNNs%2520for%2520well-known%2520structural%2520graph%2520properties.%2520In%2520this%2520paper%250Awe%2520present%2520a%2520model%2520agnostic%2520explainability%2520pipeline%2520for%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520employing%2520diagnostic%2520classifiers.%2520This%2520pipeline%2520aims%2520to%2520probe%2520and%250Ainterpret%2520the%2520learned%2520representations%2520in%2520GNNs%2520across%2520various%2520architectures%2520and%250Adatasets%252C%2520refining%2520our%2520understanding%2520and%2520trust%2520in%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20graph%20neural%20network%20states%20contain%20graph%20properties%3F&entry.906535625=Tom%20Pelletreau-Duris%20and%20Ruud%20van%20Bakel%20and%20Michael%20Cochez&entry.1292438233=%20%20Graph%20learning%20models%20achieve%20state-of-the-art%20performance%20on%20many%20tasks%2C%20but%0Athis%20often%20requires%20increasingly%20large%20model%20sizes.%20Accordingly%2C%20the%20complexity%0Aof%20their%20representations%20increase.%20Explainability%20techniques%20%28XAI%29%20have%20made%0Aremarkable%20progress%20in%20the%20interpretability%20of%20ML%20models.%20However%2C%20the%0Anon-relational%20nature%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20make%20it%20difficult%20to%0Areuse%20already%20existing%20XAI%20methods.%20While%20other%20works%20have%20focused%20on%0Ainstance-based%20explanation%20methods%20for%20GNNs%2C%20very%20few%20have%20investigated%0Amodel-based%20methods%20and%2C%20to%20our%20knowledge%2C%20none%20have%20tried%20to%20probe%20the%0Aembedding%20of%20the%20GNNs%20for%20well-known%20structural%20graph%20properties.%20In%20this%20paper%0Awe%20present%20a%20model%20agnostic%20explainability%20pipeline%20for%20Graph%20Neural%20Networks%0A%28GNNs%29%20employing%20diagnostic%20classifiers.%20This%20pipeline%20aims%20to%20probe%20and%0Ainterpret%20the%20learned%20representations%20in%20GNNs%20across%20various%20architectures%20and%0Adatasets%2C%20refining%20our%20understanding%20and%20trust%20in%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02168v1&entry.124074799=Read"},
{"title": "FedPID: An Aggregation Method for Federated Learning", "author": "Leon M\u00e4chler and Gustav Grimberg and Ivan Ezhov and Manuel Nickel and Suprosanna Shit and David Naccache and Johannes C. Paetzold", "abstract": "  This paper presents FedPID, our submission to the Federated Tumor\nSegmentation Challenge 2024 (FETS24). Inspired by FedCostWAvg and FedPIDAvg,\nour winning contributions to FETS21 and FETS2022, we propose an improved\naggregation strategy for federated and collaborative learning. FedCostWAvg is a\nmethod that averages results by considering both the number of training samples\nin each group and how much the cost function decreased in the last round of\ntraining. This is similar to how the derivative part of a PID controller works.\nIn FedPIDAvg, we also included the integral part that was missing. Another\nchallenge we faced were vastly differing dataset sizes at each center. We\nsolved this by assuming the sizes follow a Poisson distribution and adjusting\nthe training iterations for each center accordingly. Essentially, this part of\nthe method controls that outliers that require too much training time are less\nfrequently used. Based on these contributions we now adapted FedPIDAvg by\nchanging how the integral part is computed. Instead of integrating the loss\nfunction we measure the global drop in cost since the first round.\n", "link": "http://arxiv.org/abs/2411.02152v1", "date": "2024-11-04", "relevancy": 1.7219, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.439}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4351}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedPID%3A%20An%20Aggregation%20Method%20for%20Federated%20Learning&body=Title%3A%20FedPID%3A%20An%20Aggregation%20Method%20for%20Federated%20Learning%0AAuthor%3A%20Leon%20M%C3%A4chler%20and%20Gustav%20Grimberg%20and%20Ivan%20Ezhov%20and%20Manuel%20Nickel%20and%20Suprosanna%20Shit%20and%20David%20Naccache%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20%20%20This%20paper%20presents%20FedPID%2C%20our%20submission%20to%20the%20Federated%20Tumor%0ASegmentation%20Challenge%202024%20%28FETS24%29.%20Inspired%20by%20FedCostWAvg%20and%20FedPIDAvg%2C%0Aour%20winning%20contributions%20to%20FETS21%20and%20FETS2022%2C%20we%20propose%20an%20improved%0Aaggregation%20strategy%20for%20federated%20and%20collaborative%20learning.%20FedCostWAvg%20is%20a%0Amethod%20that%20averages%20results%20by%20considering%20both%20the%20number%20of%20training%20samples%0Ain%20each%20group%20and%20how%20much%20the%20cost%20function%20decreased%20in%20the%20last%20round%20of%0Atraining.%20This%20is%20similar%20to%20how%20the%20derivative%20part%20of%20a%20PID%20controller%20works.%0AIn%20FedPIDAvg%2C%20we%20also%20included%20the%20integral%20part%20that%20was%20missing.%20Another%0Achallenge%20we%20faced%20were%20vastly%20differing%20dataset%20sizes%20at%20each%20center.%20We%0Asolved%20this%20by%20assuming%20the%20sizes%20follow%20a%20Poisson%20distribution%20and%20adjusting%0Athe%20training%20iterations%20for%20each%20center%20accordingly.%20Essentially%2C%20this%20part%20of%0Athe%20method%20controls%20that%20outliers%20that%20require%20too%20much%20training%20time%20are%20less%0Afrequently%20used.%20Based%20on%20these%20contributions%20we%20now%20adapted%20FedPIDAvg%20by%0Achanging%20how%20the%20integral%20part%20is%20computed.%20Instead%20of%20integrating%20the%20loss%0Afunction%20we%20measure%20the%20global%20drop%20in%20cost%20since%20the%20first%20round.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedPID%253A%2520An%2520Aggregation%2520Method%2520for%2520Federated%2520Learning%26entry.906535625%3DLeon%2520M%25C3%25A4chler%2520and%2520Gustav%2520Grimberg%2520and%2520Ivan%2520Ezhov%2520and%2520Manuel%2520Nickel%2520and%2520Suprosanna%2520Shit%2520and%2520David%2520Naccache%2520and%2520Johannes%2520C.%2520Paetzold%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520FedPID%252C%2520our%2520submission%2520to%2520the%2520Federated%2520Tumor%250ASegmentation%2520Challenge%25202024%2520%2528FETS24%2529.%2520Inspired%2520by%2520FedCostWAvg%2520and%2520FedPIDAvg%252C%250Aour%2520winning%2520contributions%2520to%2520FETS21%2520and%2520FETS2022%252C%2520we%2520propose%2520an%2520improved%250Aaggregation%2520strategy%2520for%2520federated%2520and%2520collaborative%2520learning.%2520FedCostWAvg%2520is%2520a%250Amethod%2520that%2520averages%2520results%2520by%2520considering%2520both%2520the%2520number%2520of%2520training%2520samples%250Ain%2520each%2520group%2520and%2520how%2520much%2520the%2520cost%2520function%2520decreased%2520in%2520the%2520last%2520round%2520of%250Atraining.%2520This%2520is%2520similar%2520to%2520how%2520the%2520derivative%2520part%2520of%2520a%2520PID%2520controller%2520works.%250AIn%2520FedPIDAvg%252C%2520we%2520also%2520included%2520the%2520integral%2520part%2520that%2520was%2520missing.%2520Another%250Achallenge%2520we%2520faced%2520were%2520vastly%2520differing%2520dataset%2520sizes%2520at%2520each%2520center.%2520We%250Asolved%2520this%2520by%2520assuming%2520the%2520sizes%2520follow%2520a%2520Poisson%2520distribution%2520and%2520adjusting%250Athe%2520training%2520iterations%2520for%2520each%2520center%2520accordingly.%2520Essentially%252C%2520this%2520part%2520of%250Athe%2520method%2520controls%2520that%2520outliers%2520that%2520require%2520too%2520much%2520training%2520time%2520are%2520less%250Afrequently%2520used.%2520Based%2520on%2520these%2520contributions%2520we%2520now%2520adapted%2520FedPIDAvg%2520by%250Achanging%2520how%2520the%2520integral%2520part%2520is%2520computed.%2520Instead%2520of%2520integrating%2520the%2520loss%250Afunction%2520we%2520measure%2520the%2520global%2520drop%2520in%2520cost%2520since%2520the%2520first%2520round.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedPID%3A%20An%20Aggregation%20Method%20for%20Federated%20Learning&entry.906535625=Leon%20M%C3%A4chler%20and%20Gustav%20Grimberg%20and%20Ivan%20Ezhov%20and%20Manuel%20Nickel%20and%20Suprosanna%20Shit%20and%20David%20Naccache%20and%20Johannes%20C.%20Paetzold&entry.1292438233=%20%20This%20paper%20presents%20FedPID%2C%20our%20submission%20to%20the%20Federated%20Tumor%0ASegmentation%20Challenge%202024%20%28FETS24%29.%20Inspired%20by%20FedCostWAvg%20and%20FedPIDAvg%2C%0Aour%20winning%20contributions%20to%20FETS21%20and%20FETS2022%2C%20we%20propose%20an%20improved%0Aaggregation%20strategy%20for%20federated%20and%20collaborative%20learning.%20FedCostWAvg%20is%20a%0Amethod%20that%20averages%20results%20by%20considering%20both%20the%20number%20of%20training%20samples%0Ain%20each%20group%20and%20how%20much%20the%20cost%20function%20decreased%20in%20the%20last%20round%20of%0Atraining.%20This%20is%20similar%20to%20how%20the%20derivative%20part%20of%20a%20PID%20controller%20works.%0AIn%20FedPIDAvg%2C%20we%20also%20included%20the%20integral%20part%20that%20was%20missing.%20Another%0Achallenge%20we%20faced%20were%20vastly%20differing%20dataset%20sizes%20at%20each%20center.%20We%0Asolved%20this%20by%20assuming%20the%20sizes%20follow%20a%20Poisson%20distribution%20and%20adjusting%0Athe%20training%20iterations%20for%20each%20center%20accordingly.%20Essentially%2C%20this%20part%20of%0Athe%20method%20controls%20that%20outliers%20that%20require%20too%20much%20training%20time%20are%20less%0Afrequently%20used.%20Based%20on%20these%20contributions%20we%20now%20adapted%20FedPIDAvg%20by%0Achanging%20how%20the%20integral%20part%20is%20computed.%20Instead%20of%20integrating%20the%20loss%0Afunction%20we%20measure%20the%20global%20drop%20in%20cost%20since%20the%20first%20round.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02152v1&entry.124074799=Read"},
{"title": "The Certainty Ratio $C_\u03c1$: a novel metric for assessing the\n  reliability of classifier predictions", "author": "Jesus S. Aguilar-Ruiz", "abstract": "  Evaluating the performance of classifiers is critical in machine learning,\nparticularly in high-stakes applications where the reliability of predictions\ncan significantly impact decision-making. Traditional performance measures,\nsuch as accuracy and F-score, often fail to account for the uncertainty\ninherent in classifier predictions, leading to potentially misleading\nassessments. This paper introduces the Certainty Ratio ($C_\\rho$), a novel\nmetric designed to quantify the contribution of confident (certain) versus\nuncertain predictions to any classification performance measure. By integrating\nthe Probabilistic Confusion Matrix ($CM^\\star$) and decomposing predictions\ninto certainty and uncertainty components, $C_\\rho$ provides a more\ncomprehensive evaluation of classifier reliability. Experimental results across\n26 datasets and multiple classifiers, including Decision Trees, Naive-Bayes,\n3-Nearest Neighbors, and Random Forests, demonstrate that $C_\\rho$ reveals\ncritical insights that conventional metrics often overlook. These findings\nemphasize the importance of incorporating probabilistic information into\nclassifier evaluation, offering a robust tool for researchers and practitioners\nseeking to improve model trustworthiness in complex environments.\n", "link": "http://arxiv.org/abs/2411.01973v1", "date": "2024-11-04", "relevancy": 1.3738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4653}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Certainty%20Ratio%20%24C_%CF%81%24%3A%20a%20novel%20metric%20for%20assessing%20the%0A%20%20reliability%20of%20classifier%20predictions&body=Title%3A%20The%20Certainty%20Ratio%20%24C_%CF%81%24%3A%20a%20novel%20metric%20for%20assessing%20the%0A%20%20reliability%20of%20classifier%20predictions%0AAuthor%3A%20Jesus%20S.%20Aguilar-Ruiz%0AAbstract%3A%20%20%20Evaluating%20the%20performance%20of%20classifiers%20is%20critical%20in%20machine%20learning%2C%0Aparticularly%20in%20high-stakes%20applications%20where%20the%20reliability%20of%20predictions%0Acan%20significantly%20impact%20decision-making.%20Traditional%20performance%20measures%2C%0Asuch%20as%20accuracy%20and%20F-score%2C%20often%20fail%20to%20account%20for%20the%20uncertainty%0Ainherent%20in%20classifier%20predictions%2C%20leading%20to%20potentially%20misleading%0Aassessments.%20This%20paper%20introduces%20the%20Certainty%20Ratio%20%28%24C_%5Crho%24%29%2C%20a%20novel%0Ametric%20designed%20to%20quantify%20the%20contribution%20of%20confident%20%28certain%29%20versus%0Auncertain%20predictions%20to%20any%20classification%20performance%20measure.%20By%20integrating%0Athe%20Probabilistic%20Confusion%20Matrix%20%28%24CM%5E%5Cstar%24%29%20and%20decomposing%20predictions%0Ainto%20certainty%20and%20uncertainty%20components%2C%20%24C_%5Crho%24%20provides%20a%20more%0Acomprehensive%20evaluation%20of%20classifier%20reliability.%20Experimental%20results%20across%0A26%20datasets%20and%20multiple%20classifiers%2C%20including%20Decision%20Trees%2C%20Naive-Bayes%2C%0A3-Nearest%20Neighbors%2C%20and%20Random%20Forests%2C%20demonstrate%20that%20%24C_%5Crho%24%20reveals%0Acritical%20insights%20that%20conventional%20metrics%20often%20overlook.%20These%20findings%0Aemphasize%20the%20importance%20of%20incorporating%20probabilistic%20information%20into%0Aclassifier%20evaluation%2C%20offering%20a%20robust%20tool%20for%20researchers%20and%20practitioners%0Aseeking%20to%20improve%20model%20trustworthiness%20in%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Certainty%2520Ratio%2520%2524C_%25CF%2581%2524%253A%2520a%2520novel%2520metric%2520for%2520assessing%2520the%250A%2520%2520reliability%2520of%2520classifier%2520predictions%26entry.906535625%3DJesus%2520S.%2520Aguilar-Ruiz%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520performance%2520of%2520classifiers%2520is%2520critical%2520in%2520machine%2520learning%252C%250Aparticularly%2520in%2520high-stakes%2520applications%2520where%2520the%2520reliability%2520of%2520predictions%250Acan%2520significantly%2520impact%2520decision-making.%2520Traditional%2520performance%2520measures%252C%250Asuch%2520as%2520accuracy%2520and%2520F-score%252C%2520often%2520fail%2520to%2520account%2520for%2520the%2520uncertainty%250Ainherent%2520in%2520classifier%2520predictions%252C%2520leading%2520to%2520potentially%2520misleading%250Aassessments.%2520This%2520paper%2520introduces%2520the%2520Certainty%2520Ratio%2520%2528%2524C_%255Crho%2524%2529%252C%2520a%2520novel%250Ametric%2520designed%2520to%2520quantify%2520the%2520contribution%2520of%2520confident%2520%2528certain%2529%2520versus%250Auncertain%2520predictions%2520to%2520any%2520classification%2520performance%2520measure.%2520By%2520integrating%250Athe%2520Probabilistic%2520Confusion%2520Matrix%2520%2528%2524CM%255E%255Cstar%2524%2529%2520and%2520decomposing%2520predictions%250Ainto%2520certainty%2520and%2520uncertainty%2520components%252C%2520%2524C_%255Crho%2524%2520provides%2520a%2520more%250Acomprehensive%2520evaluation%2520of%2520classifier%2520reliability.%2520Experimental%2520results%2520across%250A26%2520datasets%2520and%2520multiple%2520classifiers%252C%2520including%2520Decision%2520Trees%252C%2520Naive-Bayes%252C%250A3-Nearest%2520Neighbors%252C%2520and%2520Random%2520Forests%252C%2520demonstrate%2520that%2520%2524C_%255Crho%2524%2520reveals%250Acritical%2520insights%2520that%2520conventional%2520metrics%2520often%2520overlook.%2520These%2520findings%250Aemphasize%2520the%2520importance%2520of%2520incorporating%2520probabilistic%2520information%2520into%250Aclassifier%2520evaluation%252C%2520offering%2520a%2520robust%2520tool%2520for%2520researchers%2520and%2520practitioners%250Aseeking%2520to%2520improve%2520model%2520trustworthiness%2520in%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Certainty%20Ratio%20%24C_%CF%81%24%3A%20a%20novel%20metric%20for%20assessing%20the%0A%20%20reliability%20of%20classifier%20predictions&entry.906535625=Jesus%20S.%20Aguilar-Ruiz&entry.1292438233=%20%20Evaluating%20the%20performance%20of%20classifiers%20is%20critical%20in%20machine%20learning%2C%0Aparticularly%20in%20high-stakes%20applications%20where%20the%20reliability%20of%20predictions%0Acan%20significantly%20impact%20decision-making.%20Traditional%20performance%20measures%2C%0Asuch%20as%20accuracy%20and%20F-score%2C%20often%20fail%20to%20account%20for%20the%20uncertainty%0Ainherent%20in%20classifier%20predictions%2C%20leading%20to%20potentially%20misleading%0Aassessments.%20This%20paper%20introduces%20the%20Certainty%20Ratio%20%28%24C_%5Crho%24%29%2C%20a%20novel%0Ametric%20designed%20to%20quantify%20the%20contribution%20of%20confident%20%28certain%29%20versus%0Auncertain%20predictions%20to%20any%20classification%20performance%20measure.%20By%20integrating%0Athe%20Probabilistic%20Confusion%20Matrix%20%28%24CM%5E%5Cstar%24%29%20and%20decomposing%20predictions%0Ainto%20certainty%20and%20uncertainty%20components%2C%20%24C_%5Crho%24%20provides%20a%20more%0Acomprehensive%20evaluation%20of%20classifier%20reliability.%20Experimental%20results%20across%0A26%20datasets%20and%20multiple%20classifiers%2C%20including%20Decision%20Trees%2C%20Naive-Bayes%2C%0A3-Nearest%20Neighbors%2C%20and%20Random%20Forests%2C%20demonstrate%20that%20%24C_%5Crho%24%20reveals%0Acritical%20insights%20that%20conventional%20metrics%20often%20overlook.%20These%20findings%0Aemphasize%20the%20importance%20of%20incorporating%20probabilistic%20information%20into%0Aclassifier%20evaluation%2C%20offering%20a%20robust%20tool%20for%20researchers%20and%20practitioners%0Aseeking%20to%20improve%20model%20trustworthiness%20in%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01973v1&entry.124074799=Read"},
{"title": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout\n  Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism", "author": "Fan Wu and Muhammad Bilal and Haolong Xiang and Heng Wang and Jinjun Yu and Xiaolong Xu", "abstract": "  Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts.\n", "link": "http://arxiv.org/abs/2411.02086v1", "date": "2024-11-04", "relevancy": 1.3743, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4745}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20and%20Downtime-tolerant%20Fault%20Diagnosis%20for%20Railway%20Turnout%0A%20%20Machines%20%28RTMs%29%20Empowered%20with%20Cloud-Edge%20Pipeline%20Parallelism&body=Title%3A%20Real-time%20and%20Downtime-tolerant%20Fault%20Diagnosis%20for%20Railway%20Turnout%0A%20%20Machines%20%28RTMs%29%20Empowered%20with%20Cloud-Edge%20Pipeline%20Parallelism%0AAuthor%3A%20Fan%20Wu%20and%20Muhammad%20Bilal%20and%20Haolong%20Xiang%20and%20Heng%20Wang%20and%20Jinjun%20Yu%20and%20Xiaolong%20Xu%0AAbstract%3A%20%20%20Railway%20Turnout%20Machines%20%28RTMs%29%20are%20mission-critical%20components%20of%20the%0Arailway%20transportation%20infrastructure%2C%20responsible%20for%20directing%20trains%20onto%0Adesired%20tracks.%20For%20safety%20assurance%20applications%2C%20especially%20in%20early-warning%0Ascenarios%2C%20RTM%20faults%20are%20expected%20to%20be%20detected%20as%20early%20as%20possible%20on%20a%0Acontinuous%207x24%20basis.%20However%2C%20limited%20emphasis%20has%20been%20placed%20on%20distributed%0Amodel%20inference%20frameworks%20that%20can%20meet%20the%20inference%20latency%20and%20reliability%0Arequirements%20of%20such%20mission%20critical%20fault%20diagnosis%20systems.%20In%20this%20paper%2C%0Aan%20edge-cloud%20collaborative%20early-warning%20system%20is%20proposed%20to%20enable%0Areal-time%20and%20downtime-tolerant%20fault%20diagnosis%20of%20RTMs%2C%20providing%20a%20new%0Aparadigm%20for%20the%20deployment%20of%20models%20in%20safety-critical%20scenarios.%20Firstly%2C%20a%0Amodular%20fault%20diagnosis%20model%20is%20designed%20specifically%20for%20distributed%0Adeployment%2C%20which%20utilizes%20a%20hierarchical%20architecture%20consisting%20of%20the%20prior%0Aknowledge%20module%2C%20subordinate%20classifiers%2C%20and%20a%20fusion%20layer%20for%20enhanced%0Aaccuracy%20and%20parallelism.%20Then%2C%20a%20cloud-edge%20collaborative%20framework%20leveraging%0Apipeline%20parallelism%2C%20namely%20CEC-PA%2C%20is%20developed%20to%20minimize%20the%20overhead%0Aresulting%20from%20distributed%20task%20execution%20and%20context%20exchange%20by%20strategically%0Apartitioning%20and%20offloading%20model%20components%20across%20cloud%20and%20edge.%0AAdditionally%2C%20an%20election%20consensus%20mechanism%20is%20implemented%20within%20CEC-PA%20to%0Aensure%20system%20robustness%20during%20coordinator%20node%20downtime.%20Comparative%0Aexperiments%20and%20ablation%20studies%20are%20conducted%20to%20validate%20the%20effectiveness%20of%0Athe%20proposed%20distributed%20fault%20diagnosis%20approach.%20Our%20ensemble-based%20fault%0Adiagnosis%20model%20achieves%20a%20remarkable%2097.4%25%20accuracy%20on%20a%20real-world%20dataset%0Acollected%20by%20Nanjing%20Metro%20in%20Jiangsu%20Province%2C%20China.%20Meanwhile%2C%20CEC-PA%0Ademonstrates%20superior%20recovery%20proficiency%20during%20node%20disruptions%20and%20speed-up%0Aranging%20from%201.98x%20to%207.93x%20in%20total%20inference%20time%20compared%20to%20its%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520and%2520Downtime-tolerant%2520Fault%2520Diagnosis%2520for%2520Railway%2520Turnout%250A%2520%2520Machines%2520%2528RTMs%2529%2520Empowered%2520with%2520Cloud-Edge%2520Pipeline%2520Parallelism%26entry.906535625%3DFan%2520Wu%2520and%2520Muhammad%2520Bilal%2520and%2520Haolong%2520Xiang%2520and%2520Heng%2520Wang%2520and%2520Jinjun%2520Yu%2520and%2520Xiaolong%2520Xu%26entry.1292438233%3D%2520%2520Railway%2520Turnout%2520Machines%2520%2528RTMs%2529%2520are%2520mission-critical%2520components%2520of%2520the%250Arailway%2520transportation%2520infrastructure%252C%2520responsible%2520for%2520directing%2520trains%2520onto%250Adesired%2520tracks.%2520For%2520safety%2520assurance%2520applications%252C%2520especially%2520in%2520early-warning%250Ascenarios%252C%2520RTM%2520faults%2520are%2520expected%2520to%2520be%2520detected%2520as%2520early%2520as%2520possible%2520on%2520a%250Acontinuous%25207x24%2520basis.%2520However%252C%2520limited%2520emphasis%2520has%2520been%2520placed%2520on%2520distributed%250Amodel%2520inference%2520frameworks%2520that%2520can%2520meet%2520the%2520inference%2520latency%2520and%2520reliability%250Arequirements%2520of%2520such%2520mission%2520critical%2520fault%2520diagnosis%2520systems.%2520In%2520this%2520paper%252C%250Aan%2520edge-cloud%2520collaborative%2520early-warning%2520system%2520is%2520proposed%2520to%2520enable%250Areal-time%2520and%2520downtime-tolerant%2520fault%2520diagnosis%2520of%2520RTMs%252C%2520providing%2520a%2520new%250Aparadigm%2520for%2520the%2520deployment%2520of%2520models%2520in%2520safety-critical%2520scenarios.%2520Firstly%252C%2520a%250Amodular%2520fault%2520diagnosis%2520model%2520is%2520designed%2520specifically%2520for%2520distributed%250Adeployment%252C%2520which%2520utilizes%2520a%2520hierarchical%2520architecture%2520consisting%2520of%2520the%2520prior%250Aknowledge%2520module%252C%2520subordinate%2520classifiers%252C%2520and%2520a%2520fusion%2520layer%2520for%2520enhanced%250Aaccuracy%2520and%2520parallelism.%2520Then%252C%2520a%2520cloud-edge%2520collaborative%2520framework%2520leveraging%250Apipeline%2520parallelism%252C%2520namely%2520CEC-PA%252C%2520is%2520developed%2520to%2520minimize%2520the%2520overhead%250Aresulting%2520from%2520distributed%2520task%2520execution%2520and%2520context%2520exchange%2520by%2520strategically%250Apartitioning%2520and%2520offloading%2520model%2520components%2520across%2520cloud%2520and%2520edge.%250AAdditionally%252C%2520an%2520election%2520consensus%2520mechanism%2520is%2520implemented%2520within%2520CEC-PA%2520to%250Aensure%2520system%2520robustness%2520during%2520coordinator%2520node%2520downtime.%2520Comparative%250Aexperiments%2520and%2520ablation%2520studies%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520distributed%2520fault%2520diagnosis%2520approach.%2520Our%2520ensemble-based%2520fault%250Adiagnosis%2520model%2520achieves%2520a%2520remarkable%252097.4%2525%2520accuracy%2520on%2520a%2520real-world%2520dataset%250Acollected%2520by%2520Nanjing%2520Metro%2520in%2520Jiangsu%2520Province%252C%2520China.%2520Meanwhile%252C%2520CEC-PA%250Ademonstrates%2520superior%2520recovery%2520proficiency%2520during%2520node%2520disruptions%2520and%2520speed-up%250Aranging%2520from%25201.98x%2520to%25207.93x%2520in%2520total%2520inference%2520time%2520compared%2520to%2520its%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20and%20Downtime-tolerant%20Fault%20Diagnosis%20for%20Railway%20Turnout%0A%20%20Machines%20%28RTMs%29%20Empowered%20with%20Cloud-Edge%20Pipeline%20Parallelism&entry.906535625=Fan%20Wu%20and%20Muhammad%20Bilal%20and%20Haolong%20Xiang%20and%20Heng%20Wang%20and%20Jinjun%20Yu%20and%20Xiaolong%20Xu&entry.1292438233=%20%20Railway%20Turnout%20Machines%20%28RTMs%29%20are%20mission-critical%20components%20of%20the%0Arailway%20transportation%20infrastructure%2C%20responsible%20for%20directing%20trains%20onto%0Adesired%20tracks.%20For%20safety%20assurance%20applications%2C%20especially%20in%20early-warning%0Ascenarios%2C%20RTM%20faults%20are%20expected%20to%20be%20detected%20as%20early%20as%20possible%20on%20a%0Acontinuous%207x24%20basis.%20However%2C%20limited%20emphasis%20has%20been%20placed%20on%20distributed%0Amodel%20inference%20frameworks%20that%20can%20meet%20the%20inference%20latency%20and%20reliability%0Arequirements%20of%20such%20mission%20critical%20fault%20diagnosis%20systems.%20In%20this%20paper%2C%0Aan%20edge-cloud%20collaborative%20early-warning%20system%20is%20proposed%20to%20enable%0Areal-time%20and%20downtime-tolerant%20fault%20diagnosis%20of%20RTMs%2C%20providing%20a%20new%0Aparadigm%20for%20the%20deployment%20of%20models%20in%20safety-critical%20scenarios.%20Firstly%2C%20a%0Amodular%20fault%20diagnosis%20model%20is%20designed%20specifically%20for%20distributed%0Adeployment%2C%20which%20utilizes%20a%20hierarchical%20architecture%20consisting%20of%20the%20prior%0Aknowledge%20module%2C%20subordinate%20classifiers%2C%20and%20a%20fusion%20layer%20for%20enhanced%0Aaccuracy%20and%20parallelism.%20Then%2C%20a%20cloud-edge%20collaborative%20framework%20leveraging%0Apipeline%20parallelism%2C%20namely%20CEC-PA%2C%20is%20developed%20to%20minimize%20the%20overhead%0Aresulting%20from%20distributed%20task%20execution%20and%20context%20exchange%20by%20strategically%0Apartitioning%20and%20offloading%20model%20components%20across%20cloud%20and%20edge.%0AAdditionally%2C%20an%20election%20consensus%20mechanism%20is%20implemented%20within%20CEC-PA%20to%0Aensure%20system%20robustness%20during%20coordinator%20node%20downtime.%20Comparative%0Aexperiments%20and%20ablation%20studies%20are%20conducted%20to%20validate%20the%20effectiveness%20of%0Athe%20proposed%20distributed%20fault%20diagnosis%20approach.%20Our%20ensemble-based%20fault%0Adiagnosis%20model%20achieves%20a%20remarkable%2097.4%25%20accuracy%20on%20a%20real-world%20dataset%0Acollected%20by%20Nanjing%20Metro%20in%20Jiangsu%20Province%2C%20China.%20Meanwhile%2C%20CEC-PA%0Ademonstrates%20superior%20recovery%20proficiency%20during%20node%20disruptions%20and%20speed-up%0Aranging%20from%201.98x%20to%207.93x%20in%20total%20inference%20time%20compared%20to%20its%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02086v1&entry.124074799=Read"},
{"title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed\n  Acyclic Graph Generation", "author": "Mufei Li and Viraj Shitole and Eli Chien and Changhai Man and Zhaodong Wang and Srinivas Sridharan and Ying Zhang and Tushar Krishna and Pan Li", "abstract": "  Directed acyclic graphs (DAGs) serve as crucial data representations in\ndomains such as hardware synthesis and compiler/program optimization for\ncomputing systems. DAG generative models facilitate the creation of synthetic\nDAGs, which can be used for benchmarking computing systems while preserving\nintellectual property. However, generating realistic DAGs is challenging due to\ntheir inherent directional and logical dependencies. This paper introduces\nLayerDAG, an autoregressive diffusion model, to address these challenges.\nLayerDAG decouples the strong node dependencies into manageable units that can\nbe processed sequentially. By interpreting the partial order of nodes as a\nsequence of bipartite graphs, LayerDAG leverages autoregressive generation to\nmodel directional dependencies and employs diffusion models to capture logical\ndependencies within each bipartite graph. Comparative analyses demonstrate that\nLayerDAG outperforms existing DAG generative models in both expressiveness and\ngeneralization, particularly for generating large-scale DAGs with up to 400\nnodes-a critical scenario for system benchmarking. Extensive experiments on\nboth synthetic and real-world flow graphs from various computing platforms show\nthat LayerDAG generates valid DAGs with superior statistical properties and\nbenchmarking performance. The synthetic DAGs generated by LayerDAG enhance the\ntraining of ML-based surrogate models, resulting in improved accuracy in\npredicting performance metrics of real-world DAGs across diverse computing\nplatforms.\n", "link": "http://arxiv.org/abs/2411.02322v1", "date": "2024-11-04", "relevancy": 1.5189, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5081}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerDAG%3A%20A%20Layerwise%20Autoregressive%20Diffusion%20Model%20for%20Directed%0A%20%20Acyclic%20Graph%20Generation&body=Title%3A%20LayerDAG%3A%20A%20Layerwise%20Autoregressive%20Diffusion%20Model%20for%20Directed%0A%20%20Acyclic%20Graph%20Generation%0AAuthor%3A%20Mufei%20Li%20and%20Viraj%20Shitole%20and%20Eli%20Chien%20and%20Changhai%20Man%20and%20Zhaodong%20Wang%20and%20Srinivas%20Sridharan%20and%20Ying%20Zhang%20and%20Tushar%20Krishna%20and%20Pan%20Li%0AAbstract%3A%20%20%20Directed%20acyclic%20graphs%20%28DAGs%29%20serve%20as%20crucial%20data%20representations%20in%0Adomains%20such%20as%20hardware%20synthesis%20and%20compiler/program%20optimization%20for%0Acomputing%20systems.%20DAG%20generative%20models%20facilitate%20the%20creation%20of%20synthetic%0ADAGs%2C%20which%20can%20be%20used%20for%20benchmarking%20computing%20systems%20while%20preserving%0Aintellectual%20property.%20However%2C%20generating%20realistic%20DAGs%20is%20challenging%20due%20to%0Atheir%20inherent%20directional%20and%20logical%20dependencies.%20This%20paper%20introduces%0ALayerDAG%2C%20an%20autoregressive%20diffusion%20model%2C%20to%20address%20these%20challenges.%0ALayerDAG%20decouples%20the%20strong%20node%20dependencies%20into%20manageable%20units%20that%20can%0Abe%20processed%20sequentially.%20By%20interpreting%20the%20partial%20order%20of%20nodes%20as%20a%0Asequence%20of%20bipartite%20graphs%2C%20LayerDAG%20leverages%20autoregressive%20generation%20to%0Amodel%20directional%20dependencies%20and%20employs%20diffusion%20models%20to%20capture%20logical%0Adependencies%20within%20each%20bipartite%20graph.%20Comparative%20analyses%20demonstrate%20that%0ALayerDAG%20outperforms%20existing%20DAG%20generative%20models%20in%20both%20expressiveness%20and%0Ageneralization%2C%20particularly%20for%20generating%20large-scale%20DAGs%20with%20up%20to%20400%0Anodes-a%20critical%20scenario%20for%20system%20benchmarking.%20Extensive%20experiments%20on%0Aboth%20synthetic%20and%20real-world%20flow%20graphs%20from%20various%20computing%20platforms%20show%0Athat%20LayerDAG%20generates%20valid%20DAGs%20with%20superior%20statistical%20properties%20and%0Abenchmarking%20performance.%20The%20synthetic%20DAGs%20generated%20by%20LayerDAG%20enhance%20the%0Atraining%20of%20ML-based%20surrogate%20models%2C%20resulting%20in%20improved%20accuracy%20in%0Apredicting%20performance%20metrics%20of%20real-world%20DAGs%20across%20diverse%20computing%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerDAG%253A%2520A%2520Layerwise%2520Autoregressive%2520Diffusion%2520Model%2520for%2520Directed%250A%2520%2520Acyclic%2520Graph%2520Generation%26entry.906535625%3DMufei%2520Li%2520and%2520Viraj%2520Shitole%2520and%2520Eli%2520Chien%2520and%2520Changhai%2520Man%2520and%2520Zhaodong%2520Wang%2520and%2520Srinivas%2520Sridharan%2520and%2520Ying%2520Zhang%2520and%2520Tushar%2520Krishna%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Directed%2520acyclic%2520graphs%2520%2528DAGs%2529%2520serve%2520as%2520crucial%2520data%2520representations%2520in%250Adomains%2520such%2520as%2520hardware%2520synthesis%2520and%2520compiler/program%2520optimization%2520for%250Acomputing%2520systems.%2520DAG%2520generative%2520models%2520facilitate%2520the%2520creation%2520of%2520synthetic%250ADAGs%252C%2520which%2520can%2520be%2520used%2520for%2520benchmarking%2520computing%2520systems%2520while%2520preserving%250Aintellectual%2520property.%2520However%252C%2520generating%2520realistic%2520DAGs%2520is%2520challenging%2520due%2520to%250Atheir%2520inherent%2520directional%2520and%2520logical%2520dependencies.%2520This%2520paper%2520introduces%250ALayerDAG%252C%2520an%2520autoregressive%2520diffusion%2520model%252C%2520to%2520address%2520these%2520challenges.%250ALayerDAG%2520decouples%2520the%2520strong%2520node%2520dependencies%2520into%2520manageable%2520units%2520that%2520can%250Abe%2520processed%2520sequentially.%2520By%2520interpreting%2520the%2520partial%2520order%2520of%2520nodes%2520as%2520a%250Asequence%2520of%2520bipartite%2520graphs%252C%2520LayerDAG%2520leverages%2520autoregressive%2520generation%2520to%250Amodel%2520directional%2520dependencies%2520and%2520employs%2520diffusion%2520models%2520to%2520capture%2520logical%250Adependencies%2520within%2520each%2520bipartite%2520graph.%2520Comparative%2520analyses%2520demonstrate%2520that%250ALayerDAG%2520outperforms%2520existing%2520DAG%2520generative%2520models%2520in%2520both%2520expressiveness%2520and%250Ageneralization%252C%2520particularly%2520for%2520generating%2520large-scale%2520DAGs%2520with%2520up%2520to%2520400%250Anodes-a%2520critical%2520scenario%2520for%2520system%2520benchmarking.%2520Extensive%2520experiments%2520on%250Aboth%2520synthetic%2520and%2520real-world%2520flow%2520graphs%2520from%2520various%2520computing%2520platforms%2520show%250Athat%2520LayerDAG%2520generates%2520valid%2520DAGs%2520with%2520superior%2520statistical%2520properties%2520and%250Abenchmarking%2520performance.%2520The%2520synthetic%2520DAGs%2520generated%2520by%2520LayerDAG%2520enhance%2520the%250Atraining%2520of%2520ML-based%2520surrogate%2520models%252C%2520resulting%2520in%2520improved%2520accuracy%2520in%250Apredicting%2520performance%2520metrics%2520of%2520real-world%2520DAGs%2520across%2520diverse%2520computing%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerDAG%3A%20A%20Layerwise%20Autoregressive%20Diffusion%20Model%20for%20Directed%0A%20%20Acyclic%20Graph%20Generation&entry.906535625=Mufei%20Li%20and%20Viraj%20Shitole%20and%20Eli%20Chien%20and%20Changhai%20Man%20and%20Zhaodong%20Wang%20and%20Srinivas%20Sridharan%20and%20Ying%20Zhang%20and%20Tushar%20Krishna%20and%20Pan%20Li&entry.1292438233=%20%20Directed%20acyclic%20graphs%20%28DAGs%29%20serve%20as%20crucial%20data%20representations%20in%0Adomains%20such%20as%20hardware%20synthesis%20and%20compiler/program%20optimization%20for%0Acomputing%20systems.%20DAG%20generative%20models%20facilitate%20the%20creation%20of%20synthetic%0ADAGs%2C%20which%20can%20be%20used%20for%20benchmarking%20computing%20systems%20while%20preserving%0Aintellectual%20property.%20However%2C%20generating%20realistic%20DAGs%20is%20challenging%20due%20to%0Atheir%20inherent%20directional%20and%20logical%20dependencies.%20This%20paper%20introduces%0ALayerDAG%2C%20an%20autoregressive%20diffusion%20model%2C%20to%20address%20these%20challenges.%0ALayerDAG%20decouples%20the%20strong%20node%20dependencies%20into%20manageable%20units%20that%20can%0Abe%20processed%20sequentially.%20By%20interpreting%20the%20partial%20order%20of%20nodes%20as%20a%0Asequence%20of%20bipartite%20graphs%2C%20LayerDAG%20leverages%20autoregressive%20generation%20to%0Amodel%20directional%20dependencies%20and%20employs%20diffusion%20models%20to%20capture%20logical%0Adependencies%20within%20each%20bipartite%20graph.%20Comparative%20analyses%20demonstrate%20that%0ALayerDAG%20outperforms%20existing%20DAG%20generative%20models%20in%20both%20expressiveness%20and%0Ageneralization%2C%20particularly%20for%20generating%20large-scale%20DAGs%20with%20up%20to%20400%0Anodes-a%20critical%20scenario%20for%20system%20benchmarking.%20Extensive%20experiments%20on%0Aboth%20synthetic%20and%20real-world%20flow%20graphs%20from%20various%20computing%20platforms%20show%0Athat%20LayerDAG%20generates%20valid%20DAGs%20with%20superior%20statistical%20properties%20and%0Abenchmarking%20performance.%20The%20synthetic%20DAGs%20generated%20by%20LayerDAG%20enhance%20the%0Atraining%20of%20ML-based%20surrogate%20models%2C%20resulting%20in%20improved%20accuracy%20in%0Apredicting%20performance%20metrics%20of%20real-world%20DAGs%20across%20diverse%20computing%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02322v1&entry.124074799=Read"},
{"title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers", "author": "Kumara Kahatapitiya and Haozhe Liu and Sen He and Ding Liu and Menglin Jia and Michael S. Ryoo and Tian Xie", "abstract": "  Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.\n", "link": "http://arxiv.org/abs/2411.02397v1", "date": "2024-11-04", "relevancy": 1.9549, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6573}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6539}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Caching%20for%20Faster%20Video%20Generation%20with%20Diffusion%20Transformers&body=Title%3A%20Adaptive%20Caching%20for%20Faster%20Video%20Generation%20with%20Diffusion%20Transformers%0AAuthor%3A%20Kumara%20Kahatapitiya%20and%20Haozhe%20Liu%20and%20Sen%20He%20and%20Ding%20Liu%20and%20Menglin%20Jia%20and%20Michael%20S.%20Ryoo%20and%20Tian%20Xie%0AAbstract%3A%20%20%20Generating%20temporally-consistent%20high-fidelity%20videos%20can%20be%20computationally%0Aexpensive%2C%20especially%20over%20longer%20temporal%20spans.%20More-recent%20Diffusion%0ATransformers%20%28DiTs%29%20--%20despite%20making%20significant%20headway%20in%20this%20context%20--%0Ahave%20only%20heightened%20such%20challenges%20as%20they%20rely%20on%20larger%20models%20and%20heavier%0Aattention%20mechanisms%2C%20resulting%20in%20slower%20inference%20speeds.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20training-free%20method%20to%20accelerate%20video%20DiTs%2C%20termed%20Adaptive%0ACaching%20%28AdaCache%29%2C%20which%20is%20motivated%20by%20the%20fact%20that%20%22not%20all%20videos%20are%0Acreated%20equal%22%3A%20meaning%2C%20some%20videos%20require%20fewer%20denoising%20steps%20to%20attain%20a%0Areasonable%20quality%20than%20others.%20Building%20on%20this%2C%20we%20not%20only%20cache%0Acomputations%20through%20the%20diffusion%20process%2C%20but%20also%20devise%20a%20caching%20schedule%0Atailored%20to%20each%20video%20generation%2C%20maximizing%20the%20quality-latency%20trade-off.%20We%0Afurther%20introduce%20a%20Motion%20Regularization%20%28MoReg%29%20scheme%20to%20utilize%20video%0Ainformation%20within%20AdaCache%2C%20essentially%20controlling%20the%20compute%20allocation%0Abased%20on%20motion%20content.%20Altogether%2C%20our%20plug-and-play%20contributions%20grant%0Asignificant%20inference%20speedups%20%28e.g.%20up%20to%204.7x%20on%20Open-Sora%20720p%20-%202s%20video%0Ageneration%29%20without%20sacrificing%20the%20generation%20quality%2C%20across%20multiple%20video%0ADiT%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Caching%2520for%2520Faster%2520Video%2520Generation%2520with%2520Diffusion%2520Transformers%26entry.906535625%3DKumara%2520Kahatapitiya%2520and%2520Haozhe%2520Liu%2520and%2520Sen%2520He%2520and%2520Ding%2520Liu%2520and%2520Menglin%2520Jia%2520and%2520Michael%2520S.%2520Ryoo%2520and%2520Tian%2520Xie%26entry.1292438233%3D%2520%2520Generating%2520temporally-consistent%2520high-fidelity%2520videos%2520can%2520be%2520computationally%250Aexpensive%252C%2520especially%2520over%2520longer%2520temporal%2520spans.%2520More-recent%2520Diffusion%250ATransformers%2520%2528DiTs%2529%2520--%2520despite%2520making%2520significant%2520headway%2520in%2520this%2520context%2520--%250Ahave%2520only%2520heightened%2520such%2520challenges%2520as%2520they%2520rely%2520on%2520larger%2520models%2520and%2520heavier%250Aattention%2520mechanisms%252C%2520resulting%2520in%2520slower%2520inference%2520speeds.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520training-free%2520method%2520to%2520accelerate%2520video%2520DiTs%252C%2520termed%2520Adaptive%250ACaching%2520%2528AdaCache%2529%252C%2520which%2520is%2520motivated%2520by%2520the%2520fact%2520that%2520%2522not%2520all%2520videos%2520are%250Acreated%2520equal%2522%253A%2520meaning%252C%2520some%2520videos%2520require%2520fewer%2520denoising%2520steps%2520to%2520attain%2520a%250Areasonable%2520quality%2520than%2520others.%2520Building%2520on%2520this%252C%2520we%2520not%2520only%2520cache%250Acomputations%2520through%2520the%2520diffusion%2520process%252C%2520but%2520also%2520devise%2520a%2520caching%2520schedule%250Atailored%2520to%2520each%2520video%2520generation%252C%2520maximizing%2520the%2520quality-latency%2520trade-off.%2520We%250Afurther%2520introduce%2520a%2520Motion%2520Regularization%2520%2528MoReg%2529%2520scheme%2520to%2520utilize%2520video%250Ainformation%2520within%2520AdaCache%252C%2520essentially%2520controlling%2520the%2520compute%2520allocation%250Abased%2520on%2520motion%2520content.%2520Altogether%252C%2520our%2520plug-and-play%2520contributions%2520grant%250Asignificant%2520inference%2520speedups%2520%2528e.g.%2520up%2520to%25204.7x%2520on%2520Open-Sora%2520720p%2520-%25202s%2520video%250Ageneration%2529%2520without%2520sacrificing%2520the%2520generation%2520quality%252C%2520across%2520multiple%2520video%250ADiT%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Caching%20for%20Faster%20Video%20Generation%20with%20Diffusion%20Transformers&entry.906535625=Kumara%20Kahatapitiya%20and%20Haozhe%20Liu%20and%20Sen%20He%20and%20Ding%20Liu%20and%20Menglin%20Jia%20and%20Michael%20S.%20Ryoo%20and%20Tian%20Xie&entry.1292438233=%20%20Generating%20temporally-consistent%20high-fidelity%20videos%20can%20be%20computationally%0Aexpensive%2C%20especially%20over%20longer%20temporal%20spans.%20More-recent%20Diffusion%0ATransformers%20%28DiTs%29%20--%20despite%20making%20significant%20headway%20in%20this%20context%20--%0Ahave%20only%20heightened%20such%20challenges%20as%20they%20rely%20on%20larger%20models%20and%20heavier%0Aattention%20mechanisms%2C%20resulting%20in%20slower%20inference%20speeds.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20training-free%20method%20to%20accelerate%20video%20DiTs%2C%20termed%20Adaptive%0ACaching%20%28AdaCache%29%2C%20which%20is%20motivated%20by%20the%20fact%20that%20%22not%20all%20videos%20are%0Acreated%20equal%22%3A%20meaning%2C%20some%20videos%20require%20fewer%20denoising%20steps%20to%20attain%20a%0Areasonable%20quality%20than%20others.%20Building%20on%20this%2C%20we%20not%20only%20cache%0Acomputations%20through%20the%20diffusion%20process%2C%20but%20also%20devise%20a%20caching%20schedule%0Atailored%20to%20each%20video%20generation%2C%20maximizing%20the%20quality-latency%20trade-off.%20We%0Afurther%20introduce%20a%20Motion%20Regularization%20%28MoReg%29%20scheme%20to%20utilize%20video%0Ainformation%20within%20AdaCache%2C%20essentially%20controlling%20the%20compute%20allocation%0Abased%20on%20motion%20content.%20Altogether%2C%20our%20plug-and-play%20contributions%20grant%0Asignificant%20inference%20speedups%20%28e.g.%20up%20to%204.7x%20on%20Open-Sora%20720p%20-%202s%20video%0Ageneration%29%20without%20sacrificing%20the%20generation%20quality%2C%20across%20multiple%20video%0ADiT%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02397v1&entry.124074799=Read"},
{"title": "Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention", "author": "Xingtai Lv and Ning Ding and Kaiyan Zhang and Ermo Hua and Ganqu Cui and Bowen Zhou", "abstract": "  Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.\n", "link": "http://arxiv.org/abs/2411.02063v1", "date": "2024-11-04", "relevancy": 2.1082, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5175}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Efficient%20Training%20of%20Large%20Language%20Models%20with%0A%20%20Low-dimensional%20Projected%20Attention&body=Title%3A%20Scalable%20Efficient%20Training%20of%20Large%20Language%20Models%20with%0A%20%20Low-dimensional%20Projected%20Attention%0AAuthor%3A%20Xingtai%20Lv%20and%20Ning%20Ding%20and%20Kaiyan%20Zhang%20and%20Ermo%20Hua%20and%20Ganqu%20Cui%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20Improving%20the%20effectiveness%20and%20efficiency%20of%20large%20language%20models%20%28LLMs%29%0Asimultaneously%20is%20a%20critical%20yet%20challenging%20research%20goal.%20In%20this%20paper%2C%20we%0Afind%20that%20low-rank%20pre-training%2C%20normally%20considered%20as%20efficient%20methods%20that%0Awill%20compromise%20performance%2C%20can%20be%20scalably%20effective%20when%20reduced%20parameters%0Aare%20precisely%20targeted.%20Specifically%2C%20applying%20the%20low-dimensional%20module%20only%0Ato%20the%20attention%20layer%20--%20resolves%20this%20issue%20and%20enhances%20both%20effectiveness%0Aand%20efficiency.%20We%20refer%20to%20this%20structure%20as%20Low-dimensional%20Projected%0AAttention%20%28LPA%29%20and%20provide%20an%20explanatory%20analysis.%20Through%20extensive%0Aexperimentation%20at%20parameter%20scales%20of%20130M%2C%20370M%2C%20and%20scaling%20up%20to%203B%2C%20we%0Ahave%20validated%20the%20effectiveness%20and%20scalability%20of%20LPA.%20Our%20results%20show%20that%0ALPA%20model%20can%20save%20up%20to%2012.4%25%20in%20time%20while%20achieving%20an%20approximate%205%25%0Aimprovement%20in%20test%20perplexity%20%28ppl%29%20and%20on%20downstream%20tasks%20compared%20with%20the%0Avanilla%20Transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Efficient%2520Training%2520of%2520Large%2520Language%2520Models%2520with%250A%2520%2520Low-dimensional%2520Projected%2520Attention%26entry.906535625%3DXingtai%2520Lv%2520and%2520Ning%2520Ding%2520and%2520Kaiyan%2520Zhang%2520and%2520Ermo%2520Hua%2520and%2520Ganqu%2520Cui%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520Improving%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Asimultaneously%2520is%2520a%2520critical%2520yet%2520challenging%2520research%2520goal.%2520In%2520this%2520paper%252C%2520we%250Afind%2520that%2520low-rank%2520pre-training%252C%2520normally%2520considered%2520as%2520efficient%2520methods%2520that%250Awill%2520compromise%2520performance%252C%2520can%2520be%2520scalably%2520effective%2520when%2520reduced%2520parameters%250Aare%2520precisely%2520targeted.%2520Specifically%252C%2520applying%2520the%2520low-dimensional%2520module%2520only%250Ato%2520the%2520attention%2520layer%2520--%2520resolves%2520this%2520issue%2520and%2520enhances%2520both%2520effectiveness%250Aand%2520efficiency.%2520We%2520refer%2520to%2520this%2520structure%2520as%2520Low-dimensional%2520Projected%250AAttention%2520%2528LPA%2529%2520and%2520provide%2520an%2520explanatory%2520analysis.%2520Through%2520extensive%250Aexperimentation%2520at%2520parameter%2520scales%2520of%2520130M%252C%2520370M%252C%2520and%2520scaling%2520up%2520to%25203B%252C%2520we%250Ahave%2520validated%2520the%2520effectiveness%2520and%2520scalability%2520of%2520LPA.%2520Our%2520results%2520show%2520that%250ALPA%2520model%2520can%2520save%2520up%2520to%252012.4%2525%2520in%2520time%2520while%2520achieving%2520an%2520approximate%25205%2525%250Aimprovement%2520in%2520test%2520perplexity%2520%2528ppl%2529%2520and%2520on%2520downstream%2520tasks%2520compared%2520with%2520the%250Avanilla%2520Transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Efficient%20Training%20of%20Large%20Language%20Models%20with%0A%20%20Low-dimensional%20Projected%20Attention&entry.906535625=Xingtai%20Lv%20and%20Ning%20Ding%20and%20Kaiyan%20Zhang%20and%20Ermo%20Hua%20and%20Ganqu%20Cui%20and%20Bowen%20Zhou&entry.1292438233=%20%20Improving%20the%20effectiveness%20and%20efficiency%20of%20large%20language%20models%20%28LLMs%29%0Asimultaneously%20is%20a%20critical%20yet%20challenging%20research%20goal.%20In%20this%20paper%2C%20we%0Afind%20that%20low-rank%20pre-training%2C%20normally%20considered%20as%20efficient%20methods%20that%0Awill%20compromise%20performance%2C%20can%20be%20scalably%20effective%20when%20reduced%20parameters%0Aare%20precisely%20targeted.%20Specifically%2C%20applying%20the%20low-dimensional%20module%20only%0Ato%20the%20attention%20layer%20--%20resolves%20this%20issue%20and%20enhances%20both%20effectiveness%0Aand%20efficiency.%20We%20refer%20to%20this%20structure%20as%20Low-dimensional%20Projected%0AAttention%20%28LPA%29%20and%20provide%20an%20explanatory%20analysis.%20Through%20extensive%0Aexperimentation%20at%20parameter%20scales%20of%20130M%2C%20370M%2C%20and%20scaling%20up%20to%203B%2C%20we%0Ahave%20validated%20the%20effectiveness%20and%20scalability%20of%20LPA.%20Our%20results%20show%20that%0ALPA%20model%20can%20save%20up%20to%2012.4%25%20in%20time%20while%20achieving%20an%20approximate%205%25%0Aimprovement%20in%20test%20perplexity%20%28ppl%29%20and%20on%20downstream%20tasks%20compared%20with%20the%0Avanilla%20Transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02063v1&entry.124074799=Read"},
{"title": "N-Gram Induction Heads for In-Context RL: Improving Stability and\n  Reducing Data Needs", "author": "Ilya Zisman and Alexander Nikulin and Andrei Polubarov and Nikita Lyubaykin and Vladislav Kurenkov", "abstract": "  In-context learning allows models like transformers to adapt to new tasks\nfrom a few examples without updating their weights, a desirable trait for\nreinforcement learning (RL). However, existing in-context RL methods, such as\nAlgorithm Distillation (AD), demand large, carefully curated datasets and can\nbe unstable and costly to train due to the transient nature of in-context\nlearning abilities. In this work we integrated the n-gram induction heads into\ntransformers for in-context RL. By incorporating these n-gram attention\npatterns, we significantly reduced the data required for generalization - up to\n27 times fewer transitions in the Key-to-Door environment - and eased the\ntraining process by making models less sensitive to hyperparameters. Our\napproach not only matches but often surpasses the performance of AD,\ndemonstrating the potential of n-gram induction heads to enhance the efficiency\nof in-context RL.\n", "link": "http://arxiv.org/abs/2411.01958v1", "date": "2024-11-04", "relevancy": 1.9224, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5192}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20N-Gram%20Induction%20Heads%20for%20In-Context%20RL%3A%20Improving%20Stability%20and%0A%20%20Reducing%20Data%20Needs&body=Title%3A%20N-Gram%20Induction%20Heads%20for%20In-Context%20RL%3A%20Improving%20Stability%20and%0A%20%20Reducing%20Data%20Needs%0AAuthor%3A%20Ilya%20Zisman%20and%20Alexander%20Nikulin%20and%20Andrei%20Polubarov%20and%20Nikita%20Lyubaykin%20and%20Vladislav%20Kurenkov%0AAbstract%3A%20%20%20In-context%20learning%20allows%20models%20like%20transformers%20to%20adapt%20to%20new%20tasks%0Afrom%20a%20few%20examples%20without%20updating%20their%20weights%2C%20a%20desirable%20trait%20for%0Areinforcement%20learning%20%28RL%29.%20However%2C%20existing%20in-context%20RL%20methods%2C%20such%20as%0AAlgorithm%20Distillation%20%28AD%29%2C%20demand%20large%2C%20carefully%20curated%20datasets%20and%20can%0Abe%20unstable%20and%20costly%20to%20train%20due%20to%20the%20transient%20nature%20of%20in-context%0Alearning%20abilities.%20In%20this%20work%20we%20integrated%20the%20n-gram%20induction%20heads%20into%0Atransformers%20for%20in-context%20RL.%20By%20incorporating%20these%20n-gram%20attention%0Apatterns%2C%20we%20significantly%20reduced%20the%20data%20required%20for%20generalization%20-%20up%20to%0A27%20times%20fewer%20transitions%20in%20the%20Key-to-Door%20environment%20-%20and%20eased%20the%0Atraining%20process%20by%20making%20models%20less%20sensitive%20to%20hyperparameters.%20Our%0Aapproach%20not%20only%20matches%20but%20often%20surpasses%20the%20performance%20of%20AD%2C%0Ademonstrating%20the%20potential%20of%20n-gram%20induction%20heads%20to%20enhance%20the%20efficiency%0Aof%20in-context%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DN-Gram%2520Induction%2520Heads%2520for%2520In-Context%2520RL%253A%2520Improving%2520Stability%2520and%250A%2520%2520Reducing%2520Data%2520Needs%26entry.906535625%3DIlya%2520Zisman%2520and%2520Alexander%2520Nikulin%2520and%2520Andrei%2520Polubarov%2520and%2520Nikita%2520Lyubaykin%2520and%2520Vladislav%2520Kurenkov%26entry.1292438233%3D%2520%2520In-context%2520learning%2520allows%2520models%2520like%2520transformers%2520to%2520adapt%2520to%2520new%2520tasks%250Afrom%2520a%2520few%2520examples%2520without%2520updating%2520their%2520weights%252C%2520a%2520desirable%2520trait%2520for%250Areinforcement%2520learning%2520%2528RL%2529.%2520However%252C%2520existing%2520in-context%2520RL%2520methods%252C%2520such%2520as%250AAlgorithm%2520Distillation%2520%2528AD%2529%252C%2520demand%2520large%252C%2520carefully%2520curated%2520datasets%2520and%2520can%250Abe%2520unstable%2520and%2520costly%2520to%2520train%2520due%2520to%2520the%2520transient%2520nature%2520of%2520in-context%250Alearning%2520abilities.%2520In%2520this%2520work%2520we%2520integrated%2520the%2520n-gram%2520induction%2520heads%2520into%250Atransformers%2520for%2520in-context%2520RL.%2520By%2520incorporating%2520these%2520n-gram%2520attention%250Apatterns%252C%2520we%2520significantly%2520reduced%2520the%2520data%2520required%2520for%2520generalization%2520-%2520up%2520to%250A27%2520times%2520fewer%2520transitions%2520in%2520the%2520Key-to-Door%2520environment%2520-%2520and%2520eased%2520the%250Atraining%2520process%2520by%2520making%2520models%2520less%2520sensitive%2520to%2520hyperparameters.%2520Our%250Aapproach%2520not%2520only%2520matches%2520but%2520often%2520surpasses%2520the%2520performance%2520of%2520AD%252C%250Ademonstrating%2520the%2520potential%2520of%2520n-gram%2520induction%2520heads%2520to%2520enhance%2520the%2520efficiency%250Aof%2520in-context%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N-Gram%20Induction%20Heads%20for%20In-Context%20RL%3A%20Improving%20Stability%20and%0A%20%20Reducing%20Data%20Needs&entry.906535625=Ilya%20Zisman%20and%20Alexander%20Nikulin%20and%20Andrei%20Polubarov%20and%20Nikita%20Lyubaykin%20and%20Vladislav%20Kurenkov&entry.1292438233=%20%20In-context%20learning%20allows%20models%20like%20transformers%20to%20adapt%20to%20new%20tasks%0Afrom%20a%20few%20examples%20without%20updating%20their%20weights%2C%20a%20desirable%20trait%20for%0Areinforcement%20learning%20%28RL%29.%20However%2C%20existing%20in-context%20RL%20methods%2C%20such%20as%0AAlgorithm%20Distillation%20%28AD%29%2C%20demand%20large%2C%20carefully%20curated%20datasets%20and%20can%0Abe%20unstable%20and%20costly%20to%20train%20due%20to%20the%20transient%20nature%20of%20in-context%0Alearning%20abilities.%20In%20this%20work%20we%20integrated%20the%20n-gram%20induction%20heads%20into%0Atransformers%20for%20in-context%20RL.%20By%20incorporating%20these%20n-gram%20attention%0Apatterns%2C%20we%20significantly%20reduced%20the%20data%20required%20for%20generalization%20-%20up%20to%0A27%20times%20fewer%20transitions%20in%20the%20Key-to-Door%20environment%20-%20and%20eased%20the%0Atraining%20process%20by%20making%20models%20less%20sensitive%20to%20hyperparameters.%20Our%0Aapproach%20not%20only%20matches%20but%20often%20surpasses%20the%20performance%20of%20AD%2C%0Ademonstrating%20the%20potential%20of%20n-gram%20induction%20heads%20to%20enhance%20the%20efficiency%0Aof%20in-context%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01958v1&entry.124074799=Read"},
{"title": "Grid-Based Projection of Spatial Data into Knowledge Graphs", "author": "Amin Anjomshoaa and Hannah Schuster and Axel Polleres", "abstract": "  The Spatial Knowledge Graphs (SKG) are experiencing growing adoption as a\nmeans to model real-world entities, proving especially invaluable in domains\nlike crisis management and urban planning. Considering that RDF specifications\noffer limited support for effectively managing spatial information, it's common\npractice to include text-based serializations of geometrical features, such as\npolygons and lines, as string literals in knowledge graphs. Consequently,\nSpatial Knowledge Graphs (SKGs) often rely on geo-enabled RDF Stores capable of\nparsing, interpreting, and indexing such serializations. In this paper, we\nleverage grid cells as the foundational element of SKGs and demonstrate how\nefficiently the spatial characteristics of real-world entities and their\nattributes can be encoded within knowledge graphs. Furthermore, we introduce a\nnovel methodology for representing street networks in knowledge graphs,\ndiverging from the conventional practice of individually capturing each street\nsegment. Instead, our approach is based on tessellating the street network\nusing grid cells and creating a simplified representation that could be\nutilized for various routing and navigation tasks, solely relying on RDF\nspecifications.\n", "link": "http://arxiv.org/abs/2411.02309v1", "date": "2024-11-04", "relevancy": 1.8252, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4766}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4557}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grid-Based%20Projection%20of%20Spatial%20Data%20into%20Knowledge%20Graphs&body=Title%3A%20Grid-Based%20Projection%20of%20Spatial%20Data%20into%20Knowledge%20Graphs%0AAuthor%3A%20Amin%20Anjomshoaa%20and%20Hannah%20Schuster%20and%20Axel%20Polleres%0AAbstract%3A%20%20%20The%20Spatial%20Knowledge%20Graphs%20%28SKG%29%20are%20experiencing%20growing%20adoption%20as%20a%0Ameans%20to%20model%20real-world%20entities%2C%20proving%20especially%20invaluable%20in%20domains%0Alike%20crisis%20management%20and%20urban%20planning.%20Considering%20that%20RDF%20specifications%0Aoffer%20limited%20support%20for%20effectively%20managing%20spatial%20information%2C%20it%27s%20common%0Apractice%20to%20include%20text-based%20serializations%20of%20geometrical%20features%2C%20such%20as%0Apolygons%20and%20lines%2C%20as%20string%20literals%20in%20knowledge%20graphs.%20Consequently%2C%0ASpatial%20Knowledge%20Graphs%20%28SKGs%29%20often%20rely%20on%20geo-enabled%20RDF%20Stores%20capable%20of%0Aparsing%2C%20interpreting%2C%20and%20indexing%20such%20serializations.%20In%20this%20paper%2C%20we%0Aleverage%20grid%20cells%20as%20the%20foundational%20element%20of%20SKGs%20and%20demonstrate%20how%0Aefficiently%20the%20spatial%20characteristics%20of%20real-world%20entities%20and%20their%0Aattributes%20can%20be%20encoded%20within%20knowledge%20graphs.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20methodology%20for%20representing%20street%20networks%20in%20knowledge%20graphs%2C%0Adiverging%20from%20the%20conventional%20practice%20of%20individually%20capturing%20each%20street%0Asegment.%20Instead%2C%20our%20approach%20is%20based%20on%20tessellating%20the%20street%20network%0Ausing%20grid%20cells%20and%20creating%20a%20simplified%20representation%20that%20could%20be%0Autilized%20for%20various%20routing%20and%20navigation%20tasks%2C%20solely%20relying%20on%20RDF%0Aspecifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrid-Based%2520Projection%2520of%2520Spatial%2520Data%2520into%2520Knowledge%2520Graphs%26entry.906535625%3DAmin%2520Anjomshoaa%2520and%2520Hannah%2520Schuster%2520and%2520Axel%2520Polleres%26entry.1292438233%3D%2520%2520The%2520Spatial%2520Knowledge%2520Graphs%2520%2528SKG%2529%2520are%2520experiencing%2520growing%2520adoption%2520as%2520a%250Ameans%2520to%2520model%2520real-world%2520entities%252C%2520proving%2520especially%2520invaluable%2520in%2520domains%250Alike%2520crisis%2520management%2520and%2520urban%2520planning.%2520Considering%2520that%2520RDF%2520specifications%250Aoffer%2520limited%2520support%2520for%2520effectively%2520managing%2520spatial%2520information%252C%2520it%2527s%2520common%250Apractice%2520to%2520include%2520text-based%2520serializations%2520of%2520geometrical%2520features%252C%2520such%2520as%250Apolygons%2520and%2520lines%252C%2520as%2520string%2520literals%2520in%2520knowledge%2520graphs.%2520Consequently%252C%250ASpatial%2520Knowledge%2520Graphs%2520%2528SKGs%2529%2520often%2520rely%2520on%2520geo-enabled%2520RDF%2520Stores%2520capable%2520of%250Aparsing%252C%2520interpreting%252C%2520and%2520indexing%2520such%2520serializations.%2520In%2520this%2520paper%252C%2520we%250Aleverage%2520grid%2520cells%2520as%2520the%2520foundational%2520element%2520of%2520SKGs%2520and%2520demonstrate%2520how%250Aefficiently%2520the%2520spatial%2520characteristics%2520of%2520real-world%2520entities%2520and%2520their%250Aattributes%2520can%2520be%2520encoded%2520within%2520knowledge%2520graphs.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anovel%2520methodology%2520for%2520representing%2520street%2520networks%2520in%2520knowledge%2520graphs%252C%250Adiverging%2520from%2520the%2520conventional%2520practice%2520of%2520individually%2520capturing%2520each%2520street%250Asegment.%2520Instead%252C%2520our%2520approach%2520is%2520based%2520on%2520tessellating%2520the%2520street%2520network%250Ausing%2520grid%2520cells%2520and%2520creating%2520a%2520simplified%2520representation%2520that%2520could%2520be%250Autilized%2520for%2520various%2520routing%2520and%2520navigation%2520tasks%252C%2520solely%2520relying%2520on%2520RDF%250Aspecifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid-Based%20Projection%20of%20Spatial%20Data%20into%20Knowledge%20Graphs&entry.906535625=Amin%20Anjomshoaa%20and%20Hannah%20Schuster%20and%20Axel%20Polleres&entry.1292438233=%20%20The%20Spatial%20Knowledge%20Graphs%20%28SKG%29%20are%20experiencing%20growing%20adoption%20as%20a%0Ameans%20to%20model%20real-world%20entities%2C%20proving%20especially%20invaluable%20in%20domains%0Alike%20crisis%20management%20and%20urban%20planning.%20Considering%20that%20RDF%20specifications%0Aoffer%20limited%20support%20for%20effectively%20managing%20spatial%20information%2C%20it%27s%20common%0Apractice%20to%20include%20text-based%20serializations%20of%20geometrical%20features%2C%20such%20as%0Apolygons%20and%20lines%2C%20as%20string%20literals%20in%20knowledge%20graphs.%20Consequently%2C%0ASpatial%20Knowledge%20Graphs%20%28SKGs%29%20often%20rely%20on%20geo-enabled%20RDF%20Stores%20capable%20of%0Aparsing%2C%20interpreting%2C%20and%20indexing%20such%20serializations.%20In%20this%20paper%2C%20we%0Aleverage%20grid%20cells%20as%20the%20foundational%20element%20of%20SKGs%20and%20demonstrate%20how%0Aefficiently%20the%20spatial%20characteristics%20of%20real-world%20entities%20and%20their%0Aattributes%20can%20be%20encoded%20within%20knowledge%20graphs.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20methodology%20for%20representing%20street%20networks%20in%20knowledge%20graphs%2C%0Adiverging%20from%20the%20conventional%20practice%20of%20individually%20capturing%20each%20street%0Asegment.%20Instead%2C%20our%20approach%20is%20based%20on%20tessellating%20the%20street%20network%0Ausing%20grid%20cells%20and%20creating%20a%20simplified%20representation%20that%20could%20be%0Autilized%20for%20various%20routing%20and%20navigation%20tasks%2C%20solely%20relying%20on%20RDF%0Aspecifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02309v1&entry.124074799=Read"},
{"title": "Detect an Object At Once without Fine-tuning", "author": "Junyu Hao and Jianheng Liu and Yongjia Zhao and Zuofan Chen and Qi Sun and Jinlong Chen and Jianguo Wei and Minghao Yang", "abstract": "  When presented with one or a few photos of a previously unseen object, humans\ncan instantly recognize it in different scenes. Although the human brain\nmechanism behind this phenomenon is still not fully understood, this work\nintroduces a novel technical realization of this task. It consists of two\nphases: (1) generating a Similarity Density Map (SDM) by convolving the scene\nimage with the given object image patch(es) so that the highlight areas in the\nSDM indicate the possible locations; (2) obtaining the object occupied areas in\nthe scene through a Region Alignment Network (RAN). The RAN is constructed on a\nbackbone of Deep Siamese Network (DSN), and different from the traditional\nDSNs, it aims to obtain the object accurate regions by regressing the location\nand area differences between the ground truths and the predicted ones indicated\nby the highlight areas in SDM. By pre-learning from labels annotated in\ntraditional datasets, the SDM-RAN can detect previously unknown objects without\nfine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.\nThe results indicate that the proposed method outperforms state-of-the-art\nmethods on the same task.\n", "link": "http://arxiv.org/abs/2411.02181v1", "date": "2024-11-04", "relevancy": 1.6839, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5663}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5584}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detect%20an%20Object%20At%20Once%20without%20Fine-tuning&body=Title%3A%20Detect%20an%20Object%20At%20Once%20without%20Fine-tuning%0AAuthor%3A%20Junyu%20Hao%20and%20Jianheng%20Liu%20and%20Yongjia%20Zhao%20and%20Zuofan%20Chen%20and%20Qi%20Sun%20and%20Jinlong%20Chen%20and%20Jianguo%20Wei%20and%20Minghao%20Yang%0AAbstract%3A%20%20%20When%20presented%20with%20one%20or%20a%20few%20photos%20of%20a%20previously%20unseen%20object%2C%20humans%0Acan%20instantly%20recognize%20it%20in%20different%20scenes.%20Although%20the%20human%20brain%0Amechanism%20behind%20this%20phenomenon%20is%20still%20not%20fully%20understood%2C%20this%20work%0Aintroduces%20a%20novel%20technical%20realization%20of%20this%20task.%20It%20consists%20of%20two%0Aphases%3A%20%281%29%20generating%20a%20Similarity%20Density%20Map%20%28SDM%29%20by%20convolving%20the%20scene%0Aimage%20with%20the%20given%20object%20image%20patch%28es%29%20so%20that%20the%20highlight%20areas%20in%20the%0ASDM%20indicate%20the%20possible%20locations%3B%20%282%29%20obtaining%20the%20object%20occupied%20areas%20in%0Athe%20scene%20through%20a%20Region%20Alignment%20Network%20%28RAN%29.%20The%20RAN%20is%20constructed%20on%20a%0Abackbone%20of%20Deep%20Siamese%20Network%20%28DSN%29%2C%20and%20different%20from%20the%20traditional%0ADSNs%2C%20it%20aims%20to%20obtain%20the%20object%20accurate%20regions%20by%20regressing%20the%20location%0Aand%20area%20differences%20between%20the%20ground%20truths%20and%20the%20predicted%20ones%20indicated%0Aby%20the%20highlight%20areas%20in%20SDM.%20By%20pre-learning%20from%20labels%20annotated%20in%0Atraditional%20datasets%2C%20the%20SDM-RAN%20can%20detect%20previously%20unknown%20objects%20without%0Afine-tuning.%20Experiments%20were%20conducted%20on%20the%20MS%20COCO%2C%20PASCAL%20VOC%20datasets.%0AThe%20results%20indicate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20the%20same%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetect%2520an%2520Object%2520At%2520Once%2520without%2520Fine-tuning%26entry.906535625%3DJunyu%2520Hao%2520and%2520Jianheng%2520Liu%2520and%2520Yongjia%2520Zhao%2520and%2520Zuofan%2520Chen%2520and%2520Qi%2520Sun%2520and%2520Jinlong%2520Chen%2520and%2520Jianguo%2520Wei%2520and%2520Minghao%2520Yang%26entry.1292438233%3D%2520%2520When%2520presented%2520with%2520one%2520or%2520a%2520few%2520photos%2520of%2520a%2520previously%2520unseen%2520object%252C%2520humans%250Acan%2520instantly%2520recognize%2520it%2520in%2520different%2520scenes.%2520Although%2520the%2520human%2520brain%250Amechanism%2520behind%2520this%2520phenomenon%2520is%2520still%2520not%2520fully%2520understood%252C%2520this%2520work%250Aintroduces%2520a%2520novel%2520technical%2520realization%2520of%2520this%2520task.%2520It%2520consists%2520of%2520two%250Aphases%253A%2520%25281%2529%2520generating%2520a%2520Similarity%2520Density%2520Map%2520%2528SDM%2529%2520by%2520convolving%2520the%2520scene%250Aimage%2520with%2520the%2520given%2520object%2520image%2520patch%2528es%2529%2520so%2520that%2520the%2520highlight%2520areas%2520in%2520the%250ASDM%2520indicate%2520the%2520possible%2520locations%253B%2520%25282%2529%2520obtaining%2520the%2520object%2520occupied%2520areas%2520in%250Athe%2520scene%2520through%2520a%2520Region%2520Alignment%2520Network%2520%2528RAN%2529.%2520The%2520RAN%2520is%2520constructed%2520on%2520a%250Abackbone%2520of%2520Deep%2520Siamese%2520Network%2520%2528DSN%2529%252C%2520and%2520different%2520from%2520the%2520traditional%250ADSNs%252C%2520it%2520aims%2520to%2520obtain%2520the%2520object%2520accurate%2520regions%2520by%2520regressing%2520the%2520location%250Aand%2520area%2520differences%2520between%2520the%2520ground%2520truths%2520and%2520the%2520predicted%2520ones%2520indicated%250Aby%2520the%2520highlight%2520areas%2520in%2520SDM.%2520By%2520pre-learning%2520from%2520labels%2520annotated%2520in%250Atraditional%2520datasets%252C%2520the%2520SDM-RAN%2520can%2520detect%2520previously%2520unknown%2520objects%2520without%250Afine-tuning.%2520Experiments%2520were%2520conducted%2520on%2520the%2520MS%2520COCO%252C%2520PASCAL%2520VOC%2520datasets.%250AThe%2520results%2520indicate%2520that%2520the%2520proposed%2520method%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520the%2520same%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20an%20Object%20At%20Once%20without%20Fine-tuning&entry.906535625=Junyu%20Hao%20and%20Jianheng%20Liu%20and%20Yongjia%20Zhao%20and%20Zuofan%20Chen%20and%20Qi%20Sun%20and%20Jinlong%20Chen%20and%20Jianguo%20Wei%20and%20Minghao%20Yang&entry.1292438233=%20%20When%20presented%20with%20one%20or%20a%20few%20photos%20of%20a%20previously%20unseen%20object%2C%20humans%0Acan%20instantly%20recognize%20it%20in%20different%20scenes.%20Although%20the%20human%20brain%0Amechanism%20behind%20this%20phenomenon%20is%20still%20not%20fully%20understood%2C%20this%20work%0Aintroduces%20a%20novel%20technical%20realization%20of%20this%20task.%20It%20consists%20of%20two%0Aphases%3A%20%281%29%20generating%20a%20Similarity%20Density%20Map%20%28SDM%29%20by%20convolving%20the%20scene%0Aimage%20with%20the%20given%20object%20image%20patch%28es%29%20so%20that%20the%20highlight%20areas%20in%20the%0ASDM%20indicate%20the%20possible%20locations%3B%20%282%29%20obtaining%20the%20object%20occupied%20areas%20in%0Athe%20scene%20through%20a%20Region%20Alignment%20Network%20%28RAN%29.%20The%20RAN%20is%20constructed%20on%20a%0Abackbone%20of%20Deep%20Siamese%20Network%20%28DSN%29%2C%20and%20different%20from%20the%20traditional%0ADSNs%2C%20it%20aims%20to%20obtain%20the%20object%20accurate%20regions%20by%20regressing%20the%20location%0Aand%20area%20differences%20between%20the%20ground%20truths%20and%20the%20predicted%20ones%20indicated%0Aby%20the%20highlight%20areas%20in%20SDM.%20By%20pre-learning%20from%20labels%20annotated%20in%0Atraditional%20datasets%2C%20the%20SDM-RAN%20can%20detect%20previously%20unknown%20objects%20without%0Afine-tuning.%20Experiments%20were%20conducted%20on%20the%20MS%20COCO%2C%20PASCAL%20VOC%20datasets.%0AThe%20results%20indicate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20the%20same%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02181v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


