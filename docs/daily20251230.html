<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251229.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction", "author": "Shuhong Liu and Chenyu Bao and Ziteng Cui and Yun Liu and Xuangeng Chu and Lin Gu and Marcos V. Conde and Ryo Umagami and Tomohiro Hashimoto and Zijian Hu and Tianhan Xu and Yuan Gan and Yusuke Kurose and Tatsuya Harada", "abstract": "We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.", "link": "http://arxiv.org/abs/2512.23437v1", "date": "2025-12-29", "relevancy": 3.1837, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction&body=Title%3A%20RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction%0AAuthor%3A%20Shuhong%20Liu%20and%20Chenyu%20Bao%20and%20Ziteng%20Cui%20and%20Yun%20Liu%20and%20Xuangeng%20Chu%20and%20Lin%20Gu%20and%20Marcos%20V.%20Conde%20and%20Ryo%20Umagami%20and%20Tomohiro%20Hashimoto%20and%20Zijian%20Hu%20and%20Tianhan%20Xu%20and%20Yuan%20Gan%20and%20Yusuke%20Kurose%20and%20Tatsuya%20Harada%0AAbstract%3A%20We%20introduce%20RealX3D%2C%20a%20real-capture%20benchmark%20for%20multi-view%20visual%20restoration%20and%203D%20reconstruction%20under%20diverse%20physical%20degradations.%20RealX3D%20groups%20corruptions%20into%20four%20families%2C%20including%20illumination%2C%20scattering%2C%20occlusion%2C%20and%20blurring%2C%20and%20captures%20each%20at%20multiple%20severity%20levels%20using%20a%20unified%20acquisition%20protocol%20that%20yields%20pixel-aligned%20LQ/GT%20views.%20Each%20scene%20includes%20high-resolution%20capture%2C%20RAW%20images%2C%20and%20dense%20laser%20scans%2C%20from%20which%20we%20derive%20world-scale%20meshes%20and%20metric%20depth.%20Benchmarking%20a%20broad%20range%20of%20optimization-based%20and%20feed-forward%20methods%20shows%20substantial%20degradation%20in%20reconstruction%20quality%20under%20physical%20corruptions%2C%20underscoring%20the%20fragility%20of%20current%20multi-view%20pipelines%20in%20real-world%20challenging%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealX3D%253A%2520A%2520Physically-Degraded%25203D%2520Benchmark%2520for%2520Multi-view%2520Visual%2520Restoration%2520and%2520Reconstruction%26entry.906535625%3DShuhong%2520Liu%2520and%2520Chenyu%2520Bao%2520and%2520Ziteng%2520Cui%2520and%2520Yun%2520Liu%2520and%2520Xuangeng%2520Chu%2520and%2520Lin%2520Gu%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Ryo%2520Umagami%2520and%2520Tomohiro%2520Hashimoto%2520and%2520Zijian%2520Hu%2520and%2520Tianhan%2520Xu%2520and%2520Yuan%2520Gan%2520and%2520Yusuke%2520Kurose%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3DWe%2520introduce%2520RealX3D%252C%2520a%2520real-capture%2520benchmark%2520for%2520multi-view%2520visual%2520restoration%2520and%25203D%2520reconstruction%2520under%2520diverse%2520physical%2520degradations.%2520RealX3D%2520groups%2520corruptions%2520into%2520four%2520families%252C%2520including%2520illumination%252C%2520scattering%252C%2520occlusion%252C%2520and%2520blurring%252C%2520and%2520captures%2520each%2520at%2520multiple%2520severity%2520levels%2520using%2520a%2520unified%2520acquisition%2520protocol%2520that%2520yields%2520pixel-aligned%2520LQ/GT%2520views.%2520Each%2520scene%2520includes%2520high-resolution%2520capture%252C%2520RAW%2520images%252C%2520and%2520dense%2520laser%2520scans%252C%2520from%2520which%2520we%2520derive%2520world-scale%2520meshes%2520and%2520metric%2520depth.%2520Benchmarking%2520a%2520broad%2520range%2520of%2520optimization-based%2520and%2520feed-forward%2520methods%2520shows%2520substantial%2520degradation%2520in%2520reconstruction%2520quality%2520under%2520physical%2520corruptions%252C%2520underscoring%2520the%2520fragility%2520of%2520current%2520multi-view%2520pipelines%2520in%2520real-world%2520challenging%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealX3D%3A%20A%20Physically-Degraded%203D%20Benchmark%20for%20Multi-view%20Visual%20Restoration%20and%20Reconstruction&entry.906535625=Shuhong%20Liu%20and%20Chenyu%20Bao%20and%20Ziteng%20Cui%20and%20Yun%20Liu%20and%20Xuangeng%20Chu%20and%20Lin%20Gu%20and%20Marcos%20V.%20Conde%20and%20Ryo%20Umagami%20and%20Tomohiro%20Hashimoto%20and%20Zijian%20Hu%20and%20Tianhan%20Xu%20and%20Yuan%20Gan%20and%20Yusuke%20Kurose%20and%20Tatsuya%20Harada&entry.1292438233=We%20introduce%20RealX3D%2C%20a%20real-capture%20benchmark%20for%20multi-view%20visual%20restoration%20and%203D%20reconstruction%20under%20diverse%20physical%20degradations.%20RealX3D%20groups%20corruptions%20into%20four%20families%2C%20including%20illumination%2C%20scattering%2C%20occlusion%2C%20and%20blurring%2C%20and%20captures%20each%20at%20multiple%20severity%20levels%20using%20a%20unified%20acquisition%20protocol%20that%20yields%20pixel-aligned%20LQ/GT%20views.%20Each%20scene%20includes%20high-resolution%20capture%2C%20RAW%20images%2C%20and%20dense%20laser%20scans%2C%20from%20which%20we%20derive%20world-scale%20meshes%20and%20metric%20depth.%20Benchmarking%20a%20broad%20range%20of%20optimization-based%20and%20feed-forward%20methods%20shows%20substantial%20degradation%20in%20reconstruction%20quality%20under%20physical%20corruptions%2C%20underscoring%20the%20fragility%20of%20current%20multi-view%20pipelines%20in%20real-world%20challenging%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.23437v1&entry.124074799=Read"},
{"title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos", "author": "Wenkang Zhang and Yan Zhao and Qiang Wang and Zhixin Xu and Li Song and Zhengxue Cheng", "abstract": "Free-Viewpoint Video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representation remains a major challenge. Existing dynamic 3D Gaussian Splatting methods couple reconstruction with optimization-dependent compression and customized motion formats, limiting generalization and standardization. To address this, we propose D-FCGS, a novel Feedforward Compression framework for Dynamic Gaussian Splatting. Key innovations include: (1) a standardized Group-of-Frames (GoF) structure with I-P coding, leveraging sparse control points to extract inter-frame motion tensors; (2) a dual prior-aware entropy model that fuses hyperprior and spatial-temporal priors for accurate rate estimation; (3) a control-point-guided motion compensation mechanism and refinement network to enhance view-consistent fidelity. Trained on Gaussian frames derived from multi-view videos, D-FCGS generalizes across diverse scenes in a zero-shot fashion. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 17 times compression compared to the baseline while preserving visual quality across viewpoints. This work advances feedforward compression of dynamic 3DGS, facilitating scalable FVV transmission and storage for immersive applications.", "link": "http://arxiv.org/abs/2507.05859v4", "date": "2025-12-29", "relevancy": 3.1606, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6573}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6379}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-FCGS%3A%20Feedforward%20Compression%20of%20Dynamic%20Gaussian%20Splatting%20for%20Free-Viewpoint%20Videos&body=Title%3A%20D-FCGS%3A%20Feedforward%20Compression%20of%20Dynamic%20Gaussian%20Splatting%20for%20Free-Viewpoint%20Videos%0AAuthor%3A%20Wenkang%20Zhang%20and%20Yan%20Zhao%20and%20Qiang%20Wang%20and%20Zhixin%20Xu%20and%20Li%20Song%20and%20Zhengxue%20Cheng%0AAbstract%3A%20Free-Viewpoint%20Video%20%28FVV%29%20enables%20immersive%203D%20experiences%2C%20but%20efficient%20compression%20of%20dynamic%203D%20representation%20remains%20a%20major%20challenge.%20Existing%20dynamic%203D%20Gaussian%20Splatting%20methods%20couple%20reconstruction%20with%20optimization-dependent%20compression%20and%20customized%20motion%20formats%2C%20limiting%20generalization%20and%20standardization.%20To%20address%20this%2C%20we%20propose%20D-FCGS%2C%20a%20novel%20Feedforward%20Compression%20framework%20for%20Dynamic%20Gaussian%20Splatting.%20Key%20innovations%20include%3A%20%281%29%20a%20standardized%20Group-of-Frames%20%28GoF%29%20structure%20with%20I-P%20coding%2C%20leveraging%20sparse%20control%20points%20to%20extract%20inter-frame%20motion%20tensors%3B%20%282%29%20a%20dual%20prior-aware%20entropy%20model%20that%20fuses%20hyperprior%20and%20spatial-temporal%20priors%20for%20accurate%20rate%20estimation%3B%20%283%29%20a%20control-point-guided%20motion%20compensation%20mechanism%20and%20refinement%20network%20to%20enhance%20view-consistent%20fidelity.%20Trained%20on%20Gaussian%20frames%20derived%20from%20multi-view%20videos%2C%20D-FCGS%20generalizes%20across%20diverse%20scenes%20in%20a%20zero-shot%20fashion.%20Experiments%20show%20that%20it%20matches%20the%20rate-distortion%20performance%20of%20optimization-based%20methods%2C%20achieving%20over%2017%20times%20compression%20compared%20to%20the%20baseline%20while%20preserving%20visual%20quality%20across%20viewpoints.%20This%20work%20advances%20feedforward%20compression%20of%20dynamic%203DGS%2C%20facilitating%20scalable%20FVV%20transmission%20and%20storage%20for%20immersive%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05859v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-FCGS%253A%2520Feedforward%2520Compression%2520of%2520Dynamic%2520Gaussian%2520Splatting%2520for%2520Free-Viewpoint%2520Videos%26entry.906535625%3DWenkang%2520Zhang%2520and%2520Yan%2520Zhao%2520and%2520Qiang%2520Wang%2520and%2520Zhixin%2520Xu%2520and%2520Li%2520Song%2520and%2520Zhengxue%2520Cheng%26entry.1292438233%3DFree-Viewpoint%2520Video%2520%2528FVV%2529%2520enables%2520immersive%25203D%2520experiences%252C%2520but%2520efficient%2520compression%2520of%2520dynamic%25203D%2520representation%2520remains%2520a%2520major%2520challenge.%2520Existing%2520dynamic%25203D%2520Gaussian%2520Splatting%2520methods%2520couple%2520reconstruction%2520with%2520optimization-dependent%2520compression%2520and%2520customized%2520motion%2520formats%252C%2520limiting%2520generalization%2520and%2520standardization.%2520To%2520address%2520this%252C%2520we%2520propose%2520D-FCGS%252C%2520a%2520novel%2520Feedforward%2520Compression%2520framework%2520for%2520Dynamic%2520Gaussian%2520Splatting.%2520Key%2520innovations%2520include%253A%2520%25281%2529%2520a%2520standardized%2520Group-of-Frames%2520%2528GoF%2529%2520structure%2520with%2520I-P%2520coding%252C%2520leveraging%2520sparse%2520control%2520points%2520to%2520extract%2520inter-frame%2520motion%2520tensors%253B%2520%25282%2529%2520a%2520dual%2520prior-aware%2520entropy%2520model%2520that%2520fuses%2520hyperprior%2520and%2520spatial-temporal%2520priors%2520for%2520accurate%2520rate%2520estimation%253B%2520%25283%2529%2520a%2520control-point-guided%2520motion%2520compensation%2520mechanism%2520and%2520refinement%2520network%2520to%2520enhance%2520view-consistent%2520fidelity.%2520Trained%2520on%2520Gaussian%2520frames%2520derived%2520from%2520multi-view%2520videos%252C%2520D-FCGS%2520generalizes%2520across%2520diverse%2520scenes%2520in%2520a%2520zero-shot%2520fashion.%2520Experiments%2520show%2520that%2520it%2520matches%2520the%2520rate-distortion%2520performance%2520of%2520optimization-based%2520methods%252C%2520achieving%2520over%252017%2520times%2520compression%2520compared%2520to%2520the%2520baseline%2520while%2520preserving%2520visual%2520quality%2520across%2520viewpoints.%2520This%2520work%2520advances%2520feedforward%2520compression%2520of%2520dynamic%25203DGS%252C%2520facilitating%2520scalable%2520FVV%2520transmission%2520and%2520storage%2520for%2520immersive%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05859v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-FCGS%3A%20Feedforward%20Compression%20of%20Dynamic%20Gaussian%20Splatting%20for%20Free-Viewpoint%20Videos&entry.906535625=Wenkang%20Zhang%20and%20Yan%20Zhao%20and%20Qiang%20Wang%20and%20Zhixin%20Xu%20and%20Li%20Song%20and%20Zhengxue%20Cheng&entry.1292438233=Free-Viewpoint%20Video%20%28FVV%29%20enables%20immersive%203D%20experiences%2C%20but%20efficient%20compression%20of%20dynamic%203D%20representation%20remains%20a%20major%20challenge.%20Existing%20dynamic%203D%20Gaussian%20Splatting%20methods%20couple%20reconstruction%20with%20optimization-dependent%20compression%20and%20customized%20motion%20formats%2C%20limiting%20generalization%20and%20standardization.%20To%20address%20this%2C%20we%20propose%20D-FCGS%2C%20a%20novel%20Feedforward%20Compression%20framework%20for%20Dynamic%20Gaussian%20Splatting.%20Key%20innovations%20include%3A%20%281%29%20a%20standardized%20Group-of-Frames%20%28GoF%29%20structure%20with%20I-P%20coding%2C%20leveraging%20sparse%20control%20points%20to%20extract%20inter-frame%20motion%20tensors%3B%20%282%29%20a%20dual%20prior-aware%20entropy%20model%20that%20fuses%20hyperprior%20and%20spatial-temporal%20priors%20for%20accurate%20rate%20estimation%3B%20%283%29%20a%20control-point-guided%20motion%20compensation%20mechanism%20and%20refinement%20network%20to%20enhance%20view-consistent%20fidelity.%20Trained%20on%20Gaussian%20frames%20derived%20from%20multi-view%20videos%2C%20D-FCGS%20generalizes%20across%20diverse%20scenes%20in%20a%20zero-shot%20fashion.%20Experiments%20show%20that%20it%20matches%20the%20rate-distortion%20performance%20of%20optimization-based%20methods%2C%20achieving%20over%2017%20times%20compression%20compared%20to%20the%20baseline%20while%20preserving%20visual%20quality%20across%20viewpoints.%20This%20work%20advances%20feedforward%20compression%20of%20dynamic%203DGS%2C%20facilitating%20scalable%20FVV%20transmission%20and%20storage%20for%20immersive%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2507.05859v4&entry.124074799=Read"},
{"title": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models", "author": "L'ea Dubois and Klaus Schmidt and Chengyu Wang and Ji-Hoon Park and Lin Wang and Santiago Munoz", "abstract": "Current video understanding models excel at recognizing \"what\" is happening but fall short in high-level cognitive tasks like causal reasoning and future prediction, a limitation rooted in their lack of commonsense world knowledge. To bridge this cognitive gap, we propose a novel framework that synergistically fuses a powerful Vision Foundation Model (VFM) for deep visual perception with a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our key technical innovation is a sophisticated fusion module, inspired by the Q-Former architecture, which distills complex spatiotemporal and object-centric visual features into a concise, language-aligned representation. This enables the LLM to effectively ground its inferential processes in direct visual evidence. The model is trained via a two-stage strategy, beginning with large-scale alignment pre-training on video-text data, followed by targeted instruction fine-tuning on a curated dataset designed to elicit advanced reasoning and prediction skills. Extensive experiments demonstrate that our model achieves state-of-the-art performance on multiple challenging benchmarks. Notably, it exhibits remarkable zero-shot generalization to unseen reasoning tasks, and our in-depth ablation studies validate the critical contribution of each architectural component. This work pushes the boundary of machine perception from simple recognition towards genuine cognitive understanding, paving the way for more intelligent and capable AI systems in robotics, human-computer interaction, and beyond.", "link": "http://arxiv.org/abs/2507.05822v3", "date": "2025-12-29", "relevancy": 3.1117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6349}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Event%20Reasoning%20and%20Prediction%20by%20Fusing%20World%20Knowledge%20from%20LLMs%20with%20Vision%20Foundation%20Models&body=Title%3A%20Video%20Event%20Reasoning%20and%20Prediction%20by%20Fusing%20World%20Knowledge%20from%20LLMs%20with%20Vision%20Foundation%20Models%0AAuthor%3A%20L%27ea%20Dubois%20and%20Klaus%20Schmidt%20and%20Chengyu%20Wang%20and%20Ji-Hoon%20Park%20and%20Lin%20Wang%20and%20Santiago%20Munoz%0AAbstract%3A%20Current%20video%20understanding%20models%20excel%20at%20recognizing%20%22what%22%20is%20happening%20but%20fall%20short%20in%20high-level%20cognitive%20tasks%20like%20causal%20reasoning%20and%20future%20prediction%2C%20a%20limitation%20rooted%20in%20their%20lack%20of%20commonsense%20world%20knowledge.%20To%20bridge%20this%20cognitive%20gap%2C%20we%20propose%20a%20novel%20framework%20that%20synergistically%20fuses%20a%20powerful%20Vision%20Foundation%20Model%20%28VFM%29%20for%20deep%20visual%20perception%20with%20a%20Large%20Language%20Model%20%28LLM%29%20serving%20as%20a%20knowledge-driven%20reasoning%20core.%20Our%20key%20technical%20innovation%20is%20a%20sophisticated%20fusion%20module%2C%20inspired%20by%20the%20Q-Former%20architecture%2C%20which%20distills%20complex%20spatiotemporal%20and%20object-centric%20visual%20features%20into%20a%20concise%2C%20language-aligned%20representation.%20This%20enables%20the%20LLM%20to%20effectively%20ground%20its%20inferential%20processes%20in%20direct%20visual%20evidence.%20The%20model%20is%20trained%20via%20a%20two-stage%20strategy%2C%20beginning%20with%20large-scale%20alignment%20pre-training%20on%20video-text%20data%2C%20followed%20by%20targeted%20instruction%20fine-tuning%20on%20a%20curated%20dataset%20designed%20to%20elicit%20advanced%20reasoning%20and%20prediction%20skills.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20performance%20on%20multiple%20challenging%20benchmarks.%20Notably%2C%20it%20exhibits%20remarkable%20zero-shot%20generalization%20to%20unseen%20reasoning%20tasks%2C%20and%20our%20in-depth%20ablation%20studies%20validate%20the%20critical%20contribution%20of%20each%20architectural%20component.%20This%20work%20pushes%20the%20boundary%20of%20machine%20perception%20from%20simple%20recognition%20towards%20genuine%20cognitive%20understanding%2C%20paving%20the%20way%20for%20more%20intelligent%20and%20capable%20AI%20systems%20in%20robotics%2C%20human-computer%20interaction%2C%20and%20beyond.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05822v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Event%2520Reasoning%2520and%2520Prediction%2520by%2520Fusing%2520World%2520Knowledge%2520from%2520LLMs%2520with%2520Vision%2520Foundation%2520Models%26entry.906535625%3DL%2527ea%2520Dubois%2520and%2520Klaus%2520Schmidt%2520and%2520Chengyu%2520Wang%2520and%2520Ji-Hoon%2520Park%2520and%2520Lin%2520Wang%2520and%2520Santiago%2520Munoz%26entry.1292438233%3DCurrent%2520video%2520understanding%2520models%2520excel%2520at%2520recognizing%2520%2522what%2522%2520is%2520happening%2520but%2520fall%2520short%2520in%2520high-level%2520cognitive%2520tasks%2520like%2520causal%2520reasoning%2520and%2520future%2520prediction%252C%2520a%2520limitation%2520rooted%2520in%2520their%2520lack%2520of%2520commonsense%2520world%2520knowledge.%2520To%2520bridge%2520this%2520cognitive%2520gap%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520synergistically%2520fuses%2520a%2520powerful%2520Vision%2520Foundation%2520Model%2520%2528VFM%2529%2520for%2520deep%2520visual%2520perception%2520with%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520serving%2520as%2520a%2520knowledge-driven%2520reasoning%2520core.%2520Our%2520key%2520technical%2520innovation%2520is%2520a%2520sophisticated%2520fusion%2520module%252C%2520inspired%2520by%2520the%2520Q-Former%2520architecture%252C%2520which%2520distills%2520complex%2520spatiotemporal%2520and%2520object-centric%2520visual%2520features%2520into%2520a%2520concise%252C%2520language-aligned%2520representation.%2520This%2520enables%2520the%2520LLM%2520to%2520effectively%2520ground%2520its%2520inferential%2520processes%2520in%2520direct%2520visual%2520evidence.%2520The%2520model%2520is%2520trained%2520via%2520a%2520two-stage%2520strategy%252C%2520beginning%2520with%2520large-scale%2520alignment%2520pre-training%2520on%2520video-text%2520data%252C%2520followed%2520by%2520targeted%2520instruction%2520fine-tuning%2520on%2520a%2520curated%2520dataset%2520designed%2520to%2520elicit%2520advanced%2520reasoning%2520and%2520prediction%2520skills.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520challenging%2520benchmarks.%2520Notably%252C%2520it%2520exhibits%2520remarkable%2520zero-shot%2520generalization%2520to%2520unseen%2520reasoning%2520tasks%252C%2520and%2520our%2520in-depth%2520ablation%2520studies%2520validate%2520the%2520critical%2520contribution%2520of%2520each%2520architectural%2520component.%2520This%2520work%2520pushes%2520the%2520boundary%2520of%2520machine%2520perception%2520from%2520simple%2520recognition%2520towards%2520genuine%2520cognitive%2520understanding%252C%2520paving%2520the%2520way%2520for%2520more%2520intelligent%2520and%2520capable%2520AI%2520systems%2520in%2520robotics%252C%2520human-computer%2520interaction%252C%2520and%2520beyond.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05822v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Event%20Reasoning%20and%20Prediction%20by%20Fusing%20World%20Knowledge%20from%20LLMs%20with%20Vision%20Foundation%20Models&entry.906535625=L%27ea%20Dubois%20and%20Klaus%20Schmidt%20and%20Chengyu%20Wang%20and%20Ji-Hoon%20Park%20and%20Lin%20Wang%20and%20Santiago%20Munoz&entry.1292438233=Current%20video%20understanding%20models%20excel%20at%20recognizing%20%22what%22%20is%20happening%20but%20fall%20short%20in%20high-level%20cognitive%20tasks%20like%20causal%20reasoning%20and%20future%20prediction%2C%20a%20limitation%20rooted%20in%20their%20lack%20of%20commonsense%20world%20knowledge.%20To%20bridge%20this%20cognitive%20gap%2C%20we%20propose%20a%20novel%20framework%20that%20synergistically%20fuses%20a%20powerful%20Vision%20Foundation%20Model%20%28VFM%29%20for%20deep%20visual%20perception%20with%20a%20Large%20Language%20Model%20%28LLM%29%20serving%20as%20a%20knowledge-driven%20reasoning%20core.%20Our%20key%20technical%20innovation%20is%20a%20sophisticated%20fusion%20module%2C%20inspired%20by%20the%20Q-Former%20architecture%2C%20which%20distills%20complex%20spatiotemporal%20and%20object-centric%20visual%20features%20into%20a%20concise%2C%20language-aligned%20representation.%20This%20enables%20the%20LLM%20to%20effectively%20ground%20its%20inferential%20processes%20in%20direct%20visual%20evidence.%20The%20model%20is%20trained%20via%20a%20two-stage%20strategy%2C%20beginning%20with%20large-scale%20alignment%20pre-training%20on%20video-text%20data%2C%20followed%20by%20targeted%20instruction%20fine-tuning%20on%20a%20curated%20dataset%20designed%20to%20elicit%20advanced%20reasoning%20and%20prediction%20skills.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20performance%20on%20multiple%20challenging%20benchmarks.%20Notably%2C%20it%20exhibits%20remarkable%20zero-shot%20generalization%20to%20unseen%20reasoning%20tasks%2C%20and%20our%20in-depth%20ablation%20studies%20validate%20the%20critical%20contribution%20of%20each%20architectural%20component.%20This%20work%20pushes%20the%20boundary%20of%20machine%20perception%20from%20simple%20recognition%20towards%20genuine%20cognitive%20understanding%2C%20paving%20the%20way%20for%20more%20intelligent%20and%20capable%20AI%20systems%20in%20robotics%2C%20human-computer%20interaction%2C%20and%20beyond.&entry.1838667208=http%3A//arxiv.org/abs/2507.05822v3&entry.124074799=Read"},
{"title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "author": "Shaocong Xu and Songlin Wei and Qizhe Wei and Zheng Geng and Hong Li and Licheng Shen and Qianpu Sun and Shu Han and Bin Ma and Bohan Li and Chongjie Ye and Yuhang Zheng and Nan Wang and Saining Zhang and Hao Zhao", "abstract": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "link": "http://arxiv.org/abs/2512.23705v1", "date": "2025-12-29", "relevancy": 3.0904, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6289}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6141}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Knows%20Transparency%3A%20Repurposing%20Video%20Diffusion%20for%20Transparent%20Object%20Depth%20and%20Normal%20Estimation&body=Title%3A%20Diffusion%20Knows%20Transparency%3A%20Repurposing%20Video%20Diffusion%20for%20Transparent%20Object%20Depth%20and%20Normal%20Estimation%0AAuthor%3A%20Shaocong%20Xu%20and%20Songlin%20Wei%20and%20Qizhe%20Wei%20and%20Zheng%20Geng%20and%20Hong%20Li%20and%20Licheng%20Shen%20and%20Qianpu%20Sun%20and%20Shu%20Han%20and%20Bin%20Ma%20and%20Bohan%20Li%20and%20Chongjie%20Ye%20and%20Yuhang%20Zheng%20and%20Nan%20Wang%20and%20Saining%20Zhang%20and%20Hao%20Zhao%0AAbstract%3A%20Transparent%20objects%20remain%20notoriously%20hard%20for%20perception%20systems%3A%20refraction%2C%20reflection%20and%20transmission%20break%20the%20assumptions%20behind%20stereo%2C%20ToF%20and%20purely%20discriminative%20monocular%20depth%2C%20causing%20holes%20and%20temporally%20unstable%20estimates.%20Our%20key%20observation%20is%20that%20modern%20video%20diffusion%20models%20already%20synthesize%20convincing%20transparent%20phenomena%2C%20suggesting%20they%20have%20internalized%20the%20optical%20rules.%20We%20build%20TransPhy3D%2C%20a%20synthetic%20video%20corpus%20of%20transparent/reflective%20scenes%3A%2011k%20sequences%20rendered%20with%20Blender/Cycles.%20Scenes%20are%20assembled%20from%20a%20curated%20bank%20of%20category-rich%20static%20assets%20and%20shape-rich%20procedural%20assets%20paired%20with%20glass/plastic/metal%20materials.%20We%20render%20RGB%20%2B%20depth%20%2B%20normals%20with%20physically%20based%20ray%20tracing%20and%20OptiX%20denoising.%20Starting%20from%20a%20large%20video%20diffusion%20model%2C%20we%20learn%20a%20video-to-video%20translator%20for%20depth%20%28and%20normals%29%20via%20lightweight%20LoRA%20adapters.%20During%20training%20we%20concatenate%20RGB%20and%20%28noisy%29%20depth%20latents%20in%20the%20DiT%20backbone%20and%20co-train%20on%20TransPhy3D%20and%20existing%20frame-wise%20synthetic%20datasets%2C%20yielding%20temporally%20consistent%20predictions%20for%20arbitrary-length%20input%20videos.%20The%20resulting%20model%2C%20DKT%2C%20achieves%20zero-shot%20SOTA%20on%20real%20and%20synthetic%20video%20benchmarks%20involving%20transparency%3A%20ClearPose%2C%20DREDS%20%28CatKnown/CatNovel%29%2C%20and%20TransPhy3D-Test.%20It%20improves%20accuracy%20and%20temporal%20consistency%20over%20strong%20image/video%20baselines%2C%20and%20a%20normal%20variant%20sets%20the%20best%20video%20normal%20estimation%20results%20on%20ClearPose.%20A%20compact%201.3B%20version%20runs%20at%20~0.17%20s/frame.%20Integrated%20into%20a%20grasping%20stack%2C%20DKT%27s%20depth%20boosts%20success%20rates%20across%20translucent%2C%20reflective%20and%20diffuse%20surfaces%2C%20outperforming%20prior%20estimators.%20Together%2C%20these%20results%20support%20a%20broader%20claim%3A%20%22Diffusion%20knows%20transparency.%22%20Generative%20video%20priors%20can%20be%20repurposed%2C%20efficiently%20and%20label-free%2C%20into%20robust%2C%20temporally%20coherent%20perception%20for%20challenging%20real-world%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Knows%2520Transparency%253A%2520Repurposing%2520Video%2520Diffusion%2520for%2520Transparent%2520Object%2520Depth%2520and%2520Normal%2520Estimation%26entry.906535625%3DShaocong%2520Xu%2520and%2520Songlin%2520Wei%2520and%2520Qizhe%2520Wei%2520and%2520Zheng%2520Geng%2520and%2520Hong%2520Li%2520and%2520Licheng%2520Shen%2520and%2520Qianpu%2520Sun%2520and%2520Shu%2520Han%2520and%2520Bin%2520Ma%2520and%2520Bohan%2520Li%2520and%2520Chongjie%2520Ye%2520and%2520Yuhang%2520Zheng%2520and%2520Nan%2520Wang%2520and%2520Saining%2520Zhang%2520and%2520Hao%2520Zhao%26entry.1292438233%3DTransparent%2520objects%2520remain%2520notoriously%2520hard%2520for%2520perception%2520systems%253A%2520refraction%252C%2520reflection%2520and%2520transmission%2520break%2520the%2520assumptions%2520behind%2520stereo%252C%2520ToF%2520and%2520purely%2520discriminative%2520monocular%2520depth%252C%2520causing%2520holes%2520and%2520temporally%2520unstable%2520estimates.%2520Our%2520key%2520observation%2520is%2520that%2520modern%2520video%2520diffusion%2520models%2520already%2520synthesize%2520convincing%2520transparent%2520phenomena%252C%2520suggesting%2520they%2520have%2520internalized%2520the%2520optical%2520rules.%2520We%2520build%2520TransPhy3D%252C%2520a%2520synthetic%2520video%2520corpus%2520of%2520transparent/reflective%2520scenes%253A%252011k%2520sequences%2520rendered%2520with%2520Blender/Cycles.%2520Scenes%2520are%2520assembled%2520from%2520a%2520curated%2520bank%2520of%2520category-rich%2520static%2520assets%2520and%2520shape-rich%2520procedural%2520assets%2520paired%2520with%2520glass/plastic/metal%2520materials.%2520We%2520render%2520RGB%2520%252B%2520depth%2520%252B%2520normals%2520with%2520physically%2520based%2520ray%2520tracing%2520and%2520OptiX%2520denoising.%2520Starting%2520from%2520a%2520large%2520video%2520diffusion%2520model%252C%2520we%2520learn%2520a%2520video-to-video%2520translator%2520for%2520depth%2520%2528and%2520normals%2529%2520via%2520lightweight%2520LoRA%2520adapters.%2520During%2520training%2520we%2520concatenate%2520RGB%2520and%2520%2528noisy%2529%2520depth%2520latents%2520in%2520the%2520DiT%2520backbone%2520and%2520co-train%2520on%2520TransPhy3D%2520and%2520existing%2520frame-wise%2520synthetic%2520datasets%252C%2520yielding%2520temporally%2520consistent%2520predictions%2520for%2520arbitrary-length%2520input%2520videos.%2520The%2520resulting%2520model%252C%2520DKT%252C%2520achieves%2520zero-shot%2520SOTA%2520on%2520real%2520and%2520synthetic%2520video%2520benchmarks%2520involving%2520transparency%253A%2520ClearPose%252C%2520DREDS%2520%2528CatKnown/CatNovel%2529%252C%2520and%2520TransPhy3D-Test.%2520It%2520improves%2520accuracy%2520and%2520temporal%2520consistency%2520over%2520strong%2520image/video%2520baselines%252C%2520and%2520a%2520normal%2520variant%2520sets%2520the%2520best%2520video%2520normal%2520estimation%2520results%2520on%2520ClearPose.%2520A%2520compact%25201.3B%2520version%2520runs%2520at%2520~0.17%2520s/frame.%2520Integrated%2520into%2520a%2520grasping%2520stack%252C%2520DKT%2527s%2520depth%2520boosts%2520success%2520rates%2520across%2520translucent%252C%2520reflective%2520and%2520diffuse%2520surfaces%252C%2520outperforming%2520prior%2520estimators.%2520Together%252C%2520these%2520results%2520support%2520a%2520broader%2520claim%253A%2520%2522Diffusion%2520knows%2520transparency.%2522%2520Generative%2520video%2520priors%2520can%2520be%2520repurposed%252C%2520efficiently%2520and%2520label-free%252C%2520into%2520robust%252C%2520temporally%2520coherent%2520perception%2520for%2520challenging%2520real-world%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Knows%20Transparency%3A%20Repurposing%20Video%20Diffusion%20for%20Transparent%20Object%20Depth%20and%20Normal%20Estimation&entry.906535625=Shaocong%20Xu%20and%20Songlin%20Wei%20and%20Qizhe%20Wei%20and%20Zheng%20Geng%20and%20Hong%20Li%20and%20Licheng%20Shen%20and%20Qianpu%20Sun%20and%20Shu%20Han%20and%20Bin%20Ma%20and%20Bohan%20Li%20and%20Chongjie%20Ye%20and%20Yuhang%20Zheng%20and%20Nan%20Wang%20and%20Saining%20Zhang%20and%20Hao%20Zhao&entry.1292438233=Transparent%20objects%20remain%20notoriously%20hard%20for%20perception%20systems%3A%20refraction%2C%20reflection%20and%20transmission%20break%20the%20assumptions%20behind%20stereo%2C%20ToF%20and%20purely%20discriminative%20monocular%20depth%2C%20causing%20holes%20and%20temporally%20unstable%20estimates.%20Our%20key%20observation%20is%20that%20modern%20video%20diffusion%20models%20already%20synthesize%20convincing%20transparent%20phenomena%2C%20suggesting%20they%20have%20internalized%20the%20optical%20rules.%20We%20build%20TransPhy3D%2C%20a%20synthetic%20video%20corpus%20of%20transparent/reflective%20scenes%3A%2011k%20sequences%20rendered%20with%20Blender/Cycles.%20Scenes%20are%20assembled%20from%20a%20curated%20bank%20of%20category-rich%20static%20assets%20and%20shape-rich%20procedural%20assets%20paired%20with%20glass/plastic/metal%20materials.%20We%20render%20RGB%20%2B%20depth%20%2B%20normals%20with%20physically%20based%20ray%20tracing%20and%20OptiX%20denoising.%20Starting%20from%20a%20large%20video%20diffusion%20model%2C%20we%20learn%20a%20video-to-video%20translator%20for%20depth%20%28and%20normals%29%20via%20lightweight%20LoRA%20adapters.%20During%20training%20we%20concatenate%20RGB%20and%20%28noisy%29%20depth%20latents%20in%20the%20DiT%20backbone%20and%20co-train%20on%20TransPhy3D%20and%20existing%20frame-wise%20synthetic%20datasets%2C%20yielding%20temporally%20consistent%20predictions%20for%20arbitrary-length%20input%20videos.%20The%20resulting%20model%2C%20DKT%2C%20achieves%20zero-shot%20SOTA%20on%20real%20and%20synthetic%20video%20benchmarks%20involving%20transparency%3A%20ClearPose%2C%20DREDS%20%28CatKnown/CatNovel%29%2C%20and%20TransPhy3D-Test.%20It%20improves%20accuracy%20and%20temporal%20consistency%20over%20strong%20image/video%20baselines%2C%20and%20a%20normal%20variant%20sets%20the%20best%20video%20normal%20estimation%20results%20on%20ClearPose.%20A%20compact%201.3B%20version%20runs%20at%20~0.17%20s/frame.%20Integrated%20into%20a%20grasping%20stack%2C%20DKT%27s%20depth%20boosts%20success%20rates%20across%20translucent%2C%20reflective%20and%20diffuse%20surfaces%2C%20outperforming%20prior%20estimators.%20Together%2C%20these%20results%20support%20a%20broader%20claim%3A%20%22Diffusion%20knows%20transparency.%22%20Generative%20video%20priors%20can%20be%20repurposed%2C%20efficiently%20and%20label-free%2C%20into%20robust%2C%20temporally%20coherent%20perception%20for%20challenging%20real-world%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.23705v1&entry.124074799=Read"},
{"title": "UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?", "author": "Fengjiao Chen and Minhao Jing and Weitao Lu and Yan Feng and Xiaoyu Li and Xuezhi Cao", "abstract": "Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.", "link": "http://arxiv.org/abs/2512.23512v1", "date": "2025-12-29", "relevancy": 2.9657, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5989}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniHetero%3A%20Could%20Generation%20Enhance%20Understanding%20for%20Vision-Language-Model%20at%20Large%20Data%20Scale%3F&body=Title%3A%20UniHetero%3A%20Could%20Generation%20Enhance%20Understanding%20for%20Vision-Language-Model%20at%20Large%20Data%20Scale%3F%0AAuthor%3A%20Fengjiao%20Chen%20and%20Minhao%20Jing%20and%20Weitao%20Lu%20and%20Yan%20Feng%20and%20Xiaoyu%20Li%20and%20Xuezhi%20Cao%0AAbstract%3A%20Vision-language%20large%20models%20are%20moving%20toward%20the%20unification%20of%20visual%20understanding%20and%20visual%20generation%20tasks.%20However%2C%20whether%20generation%20can%20enhance%20understanding%20is%20still%20under-explored%20on%20large%20data%20scale.%20In%20this%20work%2C%20we%20analysis%20the%20unified%20model%20with%20a%20concise%20structure%2C%20UniHetero%2C%20under%20large-scale%20pretraining%20%28%3E200M%20samples%29.%20Our%20key%20observations%20are%3A%20%281%29%20Generation%20can%20improve%20understanding%2C%20but%20Only%20if%20you%20generate%20Semantics%2C%20Not%20Pixels.%20%282%29%20Generation%20reveals%20a%20superior%20Data%20Scaling%20trend%20and%20higher%20Data%20Utilization.%20%283%29%20Autoregression%20on%20Input%20Embedding%20is%20effective%20to%20capture%20visual%20details.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniHetero%253A%2520Could%2520Generation%2520Enhance%2520Understanding%2520for%2520Vision-Language-Model%2520at%2520Large%2520Data%2520Scale%253F%26entry.906535625%3DFengjiao%2520Chen%2520and%2520Minhao%2520Jing%2520and%2520Weitao%2520Lu%2520and%2520Yan%2520Feng%2520and%2520Xiaoyu%2520Li%2520and%2520Xuezhi%2520Cao%26entry.1292438233%3DVision-language%2520large%2520models%2520are%2520moving%2520toward%2520the%2520unification%2520of%2520visual%2520understanding%2520and%2520visual%2520generation%2520tasks.%2520However%252C%2520whether%2520generation%2520can%2520enhance%2520understanding%2520is%2520still%2520under-explored%2520on%2520large%2520data%2520scale.%2520In%2520this%2520work%252C%2520we%2520analysis%2520the%2520unified%2520model%2520with%2520a%2520concise%2520structure%252C%2520UniHetero%252C%2520under%2520large-scale%2520pretraining%2520%2528%253E200M%2520samples%2529.%2520Our%2520key%2520observations%2520are%253A%2520%25281%2529%2520Generation%2520can%2520improve%2520understanding%252C%2520but%2520Only%2520if%2520you%2520generate%2520Semantics%252C%2520Not%2520Pixels.%2520%25282%2529%2520Generation%2520reveals%2520a%2520superior%2520Data%2520Scaling%2520trend%2520and%2520higher%2520Data%2520Utilization.%2520%25283%2529%2520Autoregression%2520on%2520Input%2520Embedding%2520is%2520effective%2520to%2520capture%2520visual%2520details.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniHetero%3A%20Could%20Generation%20Enhance%20Understanding%20for%20Vision-Language-Model%20at%20Large%20Data%20Scale%3F&entry.906535625=Fengjiao%20Chen%20and%20Minhao%20Jing%20and%20Weitao%20Lu%20and%20Yan%20Feng%20and%20Xiaoyu%20Li%20and%20Xuezhi%20Cao&entry.1292438233=Vision-language%20large%20models%20are%20moving%20toward%20the%20unification%20of%20visual%20understanding%20and%20visual%20generation%20tasks.%20However%2C%20whether%20generation%20can%20enhance%20understanding%20is%20still%20under-explored%20on%20large%20data%20scale.%20In%20this%20work%2C%20we%20analysis%20the%20unified%20model%20with%20a%20concise%20structure%2C%20UniHetero%2C%20under%20large-scale%20pretraining%20%28%3E200M%20samples%29.%20Our%20key%20observations%20are%3A%20%281%29%20Generation%20can%20improve%20understanding%2C%20but%20Only%20if%20you%20generate%20Semantics%2C%20Not%20Pixels.%20%282%29%20Generation%20reveals%20a%20superior%20Data%20Scaling%20trend%20and%20higher%20Data%20Utilization.%20%283%29%20Autoregression%20on%20Input%20Embedding%20is%20effective%20to%20capture%20visual%20details.&entry.1838667208=http%3A//arxiv.org/abs/2512.23512v1&entry.124074799=Read"},
{"title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility", "author": "Kanghee Lee and Injae Lee and Minseok Kwak and Kwonyoung Ryu and Jungi Hong and Jaesik Park", "abstract": "The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.", "link": "http://arxiv.org/abs/2512.23365v1", "date": "2025-12-29", "relevancy": 2.9603, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialMosaic%3A%20A%20Multiview%20VLM%20Dataset%20for%20Partial%20Visibility&body=Title%3A%20SpatialMosaic%3A%20A%20Multiview%20VLM%20Dataset%20for%20Partial%20Visibility%0AAuthor%3A%20Kanghee%20Lee%20and%20Injae%20Lee%20and%20Minseok%20Kwak%20and%20Kwonyoung%20Ryu%20and%20Jungi%20Hong%20and%20Jaesik%20Park%0AAbstract%3A%20The%20rapid%20progress%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20unlocked%20the%20potential%20for%20enhanced%203D%20scene%20understanding%20and%20spatial%20reasoning.%20However%2C%20existing%20approaches%20often%20rely%20on%20pre-constructed%203D%20representations%20or%20off-the-shelf%20reconstruction%20pipelines%2C%20which%20constrain%20scalability%20and%20real-world%20applicability.%20A%20recent%20line%20of%20work%20explores%20learning%20spatial%20reasoning%20directly%20from%20multi-view%20images%2C%20enabling%20Vision-Language%20Models%20%28VLMs%29%20to%20understand%203D%20scenes%20without%20explicit%203D%20reconstructions.%20Nevertheless%2C%20key%20challenges%20that%20frequently%20arise%20in%20real-world%20environments%2C%20such%20as%20partial%20visibility%2C%20occlusion%2C%20and%20low-overlap%20conditions%20that%20require%20spatial%20reasoning%20from%20fragmented%20visual%20cues%2C%20remain%20under-explored.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20scalable%20multi-view%20data%20generation%20and%20annotation%20pipeline%20that%20constructs%20realistic%20spatial%20reasoning%20QAs%2C%20resulting%20in%20SpatialMosaic%2C%20a%20comprehensive%20instruction-tuning%20dataset%20featuring%202M%20QA%20pairs.%20We%20further%20introduce%20SpatialMosaic-Bench%2C%20a%20challenging%20benchmark%20for%20evaluating%20multi-view%20spatial%20reasoning%20under%20realistic%20and%20challenging%20scenarios%2C%20consisting%20of%201M%20QA%20pairs%20across%206%20tasks.%20In%20addition%2C%20we%20present%20SpatialMosaicVLM%2C%20a%20hybrid%20framework%20that%20integrates%203D%20reconstruction%20models%20as%20geometry%20encoders%20within%20VLMs%20for%20robust%20spatial%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20dataset%20and%20VQA%20tasks%20effectively%20enhance%20spatial%20reasoning%20under%20challenging%20multi-view%20conditions%2C%20validating%20the%20effectiveness%20of%20our%20data%20generation%20pipeline%20in%20constructing%20realistic%20and%20diverse%20QA%20pairs.%20Code%20and%20dataset%20will%20be%20available%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialMosaic%253A%2520A%2520Multiview%2520VLM%2520Dataset%2520for%2520Partial%2520Visibility%26entry.906535625%3DKanghee%2520Lee%2520and%2520Injae%2520Lee%2520and%2520Minseok%2520Kwak%2520and%2520Kwonyoung%2520Ryu%2520and%2520Jungi%2520Hong%2520and%2520Jaesik%2520Park%26entry.1292438233%3DThe%2520rapid%2520progress%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520unlocked%2520the%2520potential%2520for%2520enhanced%25203D%2520scene%2520understanding%2520and%2520spatial%2520reasoning.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%2520pre-constructed%25203D%2520representations%2520or%2520off-the-shelf%2520reconstruction%2520pipelines%252C%2520which%2520constrain%2520scalability%2520and%2520real-world%2520applicability.%2520A%2520recent%2520line%2520of%2520work%2520explores%2520learning%2520spatial%2520reasoning%2520directly%2520from%2520multi-view%2520images%252C%2520enabling%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520understand%25203D%2520scenes%2520without%2520explicit%25203D%2520reconstructions.%2520Nevertheless%252C%2520key%2520challenges%2520that%2520frequently%2520arise%2520in%2520real-world%2520environments%252C%2520such%2520as%2520partial%2520visibility%252C%2520occlusion%252C%2520and%2520low-overlap%2520conditions%2520that%2520require%2520spatial%2520reasoning%2520from%2520fragmented%2520visual%2520cues%252C%2520remain%2520under-explored.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520scalable%2520multi-view%2520data%2520generation%2520and%2520annotation%2520pipeline%2520that%2520constructs%2520realistic%2520spatial%2520reasoning%2520QAs%252C%2520resulting%2520in%2520SpatialMosaic%252C%2520a%2520comprehensive%2520instruction-tuning%2520dataset%2520featuring%25202M%2520QA%2520pairs.%2520We%2520further%2520introduce%2520SpatialMosaic-Bench%252C%2520a%2520challenging%2520benchmark%2520for%2520evaluating%2520multi-view%2520spatial%2520reasoning%2520under%2520realistic%2520and%2520challenging%2520scenarios%252C%2520consisting%2520of%25201M%2520QA%2520pairs%2520across%25206%2520tasks.%2520In%2520addition%252C%2520we%2520present%2520SpatialMosaicVLM%252C%2520a%2520hybrid%2520framework%2520that%2520integrates%25203D%2520reconstruction%2520models%2520as%2520geometry%2520encoders%2520within%2520VLMs%2520for%2520robust%2520spatial%2520reasoning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520dataset%2520and%2520VQA%2520tasks%2520effectively%2520enhance%2520spatial%2520reasoning%2520under%2520challenging%2520multi-view%2520conditions%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520data%2520generation%2520pipeline%2520in%2520constructing%2520realistic%2520and%2520diverse%2520QA%2520pairs.%2520Code%2520and%2520dataset%2520will%2520be%2520available%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialMosaic%3A%20A%20Multiview%20VLM%20Dataset%20for%20Partial%20Visibility&entry.906535625=Kanghee%20Lee%20and%20Injae%20Lee%20and%20Minseok%20Kwak%20and%20Kwonyoung%20Ryu%20and%20Jungi%20Hong%20and%20Jaesik%20Park&entry.1292438233=The%20rapid%20progress%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20unlocked%20the%20potential%20for%20enhanced%203D%20scene%20understanding%20and%20spatial%20reasoning.%20However%2C%20existing%20approaches%20often%20rely%20on%20pre-constructed%203D%20representations%20or%20off-the-shelf%20reconstruction%20pipelines%2C%20which%20constrain%20scalability%20and%20real-world%20applicability.%20A%20recent%20line%20of%20work%20explores%20learning%20spatial%20reasoning%20directly%20from%20multi-view%20images%2C%20enabling%20Vision-Language%20Models%20%28VLMs%29%20to%20understand%203D%20scenes%20without%20explicit%203D%20reconstructions.%20Nevertheless%2C%20key%20challenges%20that%20frequently%20arise%20in%20real-world%20environments%2C%20such%20as%20partial%20visibility%2C%20occlusion%2C%20and%20low-overlap%20conditions%20that%20require%20spatial%20reasoning%20from%20fragmented%20visual%20cues%2C%20remain%20under-explored.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20scalable%20multi-view%20data%20generation%20and%20annotation%20pipeline%20that%20constructs%20realistic%20spatial%20reasoning%20QAs%2C%20resulting%20in%20SpatialMosaic%2C%20a%20comprehensive%20instruction-tuning%20dataset%20featuring%202M%20QA%20pairs.%20We%20further%20introduce%20SpatialMosaic-Bench%2C%20a%20challenging%20benchmark%20for%20evaluating%20multi-view%20spatial%20reasoning%20under%20realistic%20and%20challenging%20scenarios%2C%20consisting%20of%201M%20QA%20pairs%20across%206%20tasks.%20In%20addition%2C%20we%20present%20SpatialMosaicVLM%2C%20a%20hybrid%20framework%20that%20integrates%203D%20reconstruction%20models%20as%20geometry%20encoders%20within%20VLMs%20for%20robust%20spatial%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20dataset%20and%20VQA%20tasks%20effectively%20enhance%20spatial%20reasoning%20under%20challenging%20multi-view%20conditions%2C%20validating%20the%20effectiveness%20of%20our%20data%20generation%20pipeline%20in%20constructing%20realistic%20and%20diverse%20QA%20pairs.%20Code%20and%20dataset%20will%20be%20available%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2512.23365v1&entry.124074799=Read"},
{"title": "MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration", "author": "Shuyuan Lin and Wenwu Peng and Junjie Huang and Qiang Qi and Miaohui Wang and Jian Weng", "abstract": "Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\\% on 3DMatch. Source code is available at http://www.linshuyuan.com.", "link": "http://arxiv.org/abs/2512.23472v1", "date": "2025-12-29", "relevancy": 2.9446, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6594}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCI-Net%3A%20A%20Robust%20Multi-Domain%20Context%20Integration%20Network%20for%20Point%20Cloud%20Registration&body=Title%3A%20MCI-Net%3A%20A%20Robust%20Multi-Domain%20Context%20Integration%20Network%20for%20Point%20Cloud%20Registration%0AAuthor%3A%20Shuyuan%20Lin%20and%20Wenwu%20Peng%20and%20Junjie%20Huang%20and%20Qiang%20Qi%20and%20Miaohui%20Wang%20and%20Jian%20Weng%0AAbstract%3A%20Robust%20and%20discriminative%20feature%20learning%20is%20critical%20for%20high-quality%20point%20cloud%20registration.%20However%2C%20existing%20deep%20learning-based%20methods%20typically%20rely%20on%20Euclidean%20neighborhood-based%20strategies%20for%20feature%20extraction%2C%20which%20struggle%20to%20effectively%20capture%20the%20implicit%20semantics%20and%20structural%20consistency%20in%20point%20clouds.%20To%20address%20these%20issues%2C%20we%20propose%20a%20multi-domain%20context%20integration%20network%20%28MCI-Net%29%20that%20improves%20feature%20representation%20and%20registration%20performance%20by%20aggregating%20contextual%20cues%20from%20diverse%20domains.%20Specifically%2C%20we%20propose%20a%20graph%20neighborhood%20aggregation%20module%2C%20which%20constructs%20a%20global%20graph%20to%20capture%20the%20overall%20structural%20relationships%20within%20point%20clouds.%20We%20then%20propose%20a%20progressive%20context%20interaction%20module%20to%20enhance%20feature%20discriminability%20by%20performing%20intra-domain%20feature%20decoupling%20and%20inter-domain%20context%20interaction.%20Finally%2C%20we%20design%20a%20dynamic%20inlier%20selection%20method%20that%20optimizes%20inlier%20weights%20using%20residual%20information%20from%20multiple%20iterations%20of%20pose%20estimation%2C%20thereby%20improving%20the%20accuracy%20and%20robustness%20of%20registration.%20Extensive%20experiments%20on%20indoor%20RGB-D%20and%20outdoor%20LiDAR%20datasets%20show%20that%20the%20proposed%20MCI-Net%20significantly%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%20the%20highest%20registration%20recall%20of%2096.4%5C%25%20on%203DMatch.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCI-Net%253A%2520A%2520Robust%2520Multi-Domain%2520Context%2520Integration%2520Network%2520for%2520Point%2520Cloud%2520Registration%26entry.906535625%3DShuyuan%2520Lin%2520and%2520Wenwu%2520Peng%2520and%2520Junjie%2520Huang%2520and%2520Qiang%2520Qi%2520and%2520Miaohui%2520Wang%2520and%2520Jian%2520Weng%26entry.1292438233%3DRobust%2520and%2520discriminative%2520feature%2520learning%2520is%2520critical%2520for%2520high-quality%2520point%2520cloud%2520registration.%2520However%252C%2520existing%2520deep%2520learning-based%2520methods%2520typically%2520rely%2520on%2520Euclidean%2520neighborhood-based%2520strategies%2520for%2520feature%2520extraction%252C%2520which%2520struggle%2520to%2520effectively%2520capture%2520the%2520implicit%2520semantics%2520and%2520structural%2520consistency%2520in%2520point%2520clouds.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520multi-domain%2520context%2520integration%2520network%2520%2528MCI-Net%2529%2520that%2520improves%2520feature%2520representation%2520and%2520registration%2520performance%2520by%2520aggregating%2520contextual%2520cues%2520from%2520diverse%2520domains.%2520Specifically%252C%2520we%2520propose%2520a%2520graph%2520neighborhood%2520aggregation%2520module%252C%2520which%2520constructs%2520a%2520global%2520graph%2520to%2520capture%2520the%2520overall%2520structural%2520relationships%2520within%2520point%2520clouds.%2520We%2520then%2520propose%2520a%2520progressive%2520context%2520interaction%2520module%2520to%2520enhance%2520feature%2520discriminability%2520by%2520performing%2520intra-domain%2520feature%2520decoupling%2520and%2520inter-domain%2520context%2520interaction.%2520Finally%252C%2520we%2520design%2520a%2520dynamic%2520inlier%2520selection%2520method%2520that%2520optimizes%2520inlier%2520weights%2520using%2520residual%2520information%2520from%2520multiple%2520iterations%2520of%2520pose%2520estimation%252C%2520thereby%2520improving%2520the%2520accuracy%2520and%2520robustness%2520of%2520registration.%2520Extensive%2520experiments%2520on%2520indoor%2520RGB-D%2520and%2520outdoor%2520LiDAR%2520datasets%2520show%2520that%2520the%2520proposed%2520MCI-Net%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%252C%2520achieving%2520the%2520highest%2520registration%2520recall%2520of%252096.4%255C%2525%2520on%25203DMatch.%2520Source%2520code%2520is%2520available%2520at%2520http%253A//www.linshuyuan.com.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCI-Net%3A%20A%20Robust%20Multi-Domain%20Context%20Integration%20Network%20for%20Point%20Cloud%20Registration&entry.906535625=Shuyuan%20Lin%20and%20Wenwu%20Peng%20and%20Junjie%20Huang%20and%20Qiang%20Qi%20and%20Miaohui%20Wang%20and%20Jian%20Weng&entry.1292438233=Robust%20and%20discriminative%20feature%20learning%20is%20critical%20for%20high-quality%20point%20cloud%20registration.%20However%2C%20existing%20deep%20learning-based%20methods%20typically%20rely%20on%20Euclidean%20neighborhood-based%20strategies%20for%20feature%20extraction%2C%20which%20struggle%20to%20effectively%20capture%20the%20implicit%20semantics%20and%20structural%20consistency%20in%20point%20clouds.%20To%20address%20these%20issues%2C%20we%20propose%20a%20multi-domain%20context%20integration%20network%20%28MCI-Net%29%20that%20improves%20feature%20representation%20and%20registration%20performance%20by%20aggregating%20contextual%20cues%20from%20diverse%20domains.%20Specifically%2C%20we%20propose%20a%20graph%20neighborhood%20aggregation%20module%2C%20which%20constructs%20a%20global%20graph%20to%20capture%20the%20overall%20structural%20relationships%20within%20point%20clouds.%20We%20then%20propose%20a%20progressive%20context%20interaction%20module%20to%20enhance%20feature%20discriminability%20by%20performing%20intra-domain%20feature%20decoupling%20and%20inter-domain%20context%20interaction.%20Finally%2C%20we%20design%20a%20dynamic%20inlier%20selection%20method%20that%20optimizes%20inlier%20weights%20using%20residual%20information%20from%20multiple%20iterations%20of%20pose%20estimation%2C%20thereby%20improving%20the%20accuracy%20and%20robustness%20of%20registration.%20Extensive%20experiments%20on%20indoor%20RGB-D%20and%20outdoor%20LiDAR%20datasets%20show%20that%20the%20proposed%20MCI-Net%20significantly%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%20the%20highest%20registration%20recall%20of%2096.4%5C%25%20on%203DMatch.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.&entry.1838667208=http%3A//arxiv.org/abs/2512.23472v1&entry.124074799=Read"},
{"title": "Same or Not? Enhancing Visual Perception in Vision-Language Models", "author": "Damiano Marsili and Aditya Mehta and Ryan Y. Lin and Georgia Gkioxari", "abstract": "Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (\"Is it a cat or a dog?\") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/", "link": "http://arxiv.org/abs/2512.23592v1", "date": "2025-12-29", "relevancy": 2.9288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Same%20or%20Not%3F%20Enhancing%20Visual%20Perception%20in%20Vision-Language%20Models&body=Title%3A%20Same%20or%20Not%3F%20Enhancing%20Visual%20Perception%20in%20Vision-Language%20Models%0AAuthor%3A%20Damiano%20Marsili%20and%20Aditya%20Mehta%20and%20Ryan%20Y.%20Lin%20and%20Georgia%20Gkioxari%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20remain%20coarse-grained%2C%20exhibit%20visual%20biases%2C%20and%20miss%20subtle%20visual%20details.%20Existing%20training%20corpora%20reinforce%20this%20limitation%20by%20emphasizing%20general%20recognition%20%28%22Is%20it%20a%20cat%20or%20a%20dog%3F%22%29%20over%20fine-grained%20perception.%20To%20address%20this%2C%20we%20introduce%20a%20new%20training%20corpus%20and%20task%20designed%20to%20enhance%20the%20perceptual%20abilities%20of%20VLMs.%20TWIN%20is%20a%20large-scale%20dataset%20of%20561%2C000%20image-pair%20queries%20that%20task%20models%20to%20determine%20whether%20two%20visually%20similar%20images%20depict%20the%20same%20object%2C%20encouraging%20attention%20to%20nuanced%20visual%20cues.%20The%20dataset%20spans%20a%20diverse%20range%20of%20everyday%20objects%20across%20contexts%2C%20viewpoints%2C%20and%20appearances.%20Fine-tuning%20VLMs%20on%20TWIN%20yields%20notable%20gains%20in%20fine-grained%20recognition%2C%20even%20on%20unseen%20domains%20such%20as%20art%2C%20animals%2C%20plants%2C%20and%20landmarks.%20To%20quantify%20these%20gains%2C%20we%20introduce%20FGVQA%2C%20a%20benchmark%20suite%20of%2012%2C000%20queries%20that%20repurposes%20fine-grained%20recognition%20and%20retrieval%20datasets%20from%20multiple%20domains.%20While%20existing%20VLMs%20struggle%20on%20FGVQA%2C%20when%20fine-tuned%20on%20TWIN%20they%20improve%20by%20up%20to%2019.3%25%2C%20without%20compromising%20performance%20on%20general%20VQA%20benchmarks.%20Finally%2C%20our%20TWIN%20dataset%20scales%20favorably%20with%20object%20annotations%2C%20and%20our%20analysis%20shows%20that%20scale%20is%20key%20to%20performance.%20We%20envision%20TWIN%20as%20a%20drop-in%20addition%20to%20open-source%20VLM%20training%20corpora%2C%20advancing%20perceptual%20precision%20of%20future%20models.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/twin/%0ALink%3A%20http%3A//arxiv.org/abs/2512.23592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSame%2520or%2520Not%253F%2520Enhancing%2520Visual%2520Perception%2520in%2520Vision-Language%2520Models%26entry.906535625%3DDamiano%2520Marsili%2520and%2520Aditya%2520Mehta%2520and%2520Ryan%2520Y.%2520Lin%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520excel%2520at%2520broad%2520visual%2520understanding%2520but%2520remain%2520coarse-grained%252C%2520exhibit%2520visual%2520biases%252C%2520and%2520miss%2520subtle%2520visual%2520details.%2520Existing%2520training%2520corpora%2520reinforce%2520this%2520limitation%2520by%2520emphasizing%2520general%2520recognition%2520%2528%2522Is%2520it%2520a%2520cat%2520or%2520a%2520dog%253F%2522%2529%2520over%2520fine-grained%2520perception.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520new%2520training%2520corpus%2520and%2520task%2520designed%2520to%2520enhance%2520the%2520perceptual%2520abilities%2520of%2520VLMs.%2520TWIN%2520is%2520a%2520large-scale%2520dataset%2520of%2520561%252C000%2520image-pair%2520queries%2520that%2520task%2520models%2520to%2520determine%2520whether%2520two%2520visually%2520similar%2520images%2520depict%2520the%2520same%2520object%252C%2520encouraging%2520attention%2520to%2520nuanced%2520visual%2520cues.%2520The%2520dataset%2520spans%2520a%2520diverse%2520range%2520of%2520everyday%2520objects%2520across%2520contexts%252C%2520viewpoints%252C%2520and%2520appearances.%2520Fine-tuning%2520VLMs%2520on%2520TWIN%2520yields%2520notable%2520gains%2520in%2520fine-grained%2520recognition%252C%2520even%2520on%2520unseen%2520domains%2520such%2520as%2520art%252C%2520animals%252C%2520plants%252C%2520and%2520landmarks.%2520To%2520quantify%2520these%2520gains%252C%2520we%2520introduce%2520FGVQA%252C%2520a%2520benchmark%2520suite%2520of%252012%252C000%2520queries%2520that%2520repurposes%2520fine-grained%2520recognition%2520and%2520retrieval%2520datasets%2520from%2520multiple%2520domains.%2520While%2520existing%2520VLMs%2520struggle%2520on%2520FGVQA%252C%2520when%2520fine-tuned%2520on%2520TWIN%2520they%2520improve%2520by%2520up%2520to%252019.3%2525%252C%2520without%2520compromising%2520performance%2520on%2520general%2520VQA%2520benchmarks.%2520Finally%252C%2520our%2520TWIN%2520dataset%2520scales%2520favorably%2520with%2520object%2520annotations%252C%2520and%2520our%2520analysis%2520shows%2520that%2520scale%2520is%2520key%2520to%2520performance.%2520We%2520envision%2520TWIN%2520as%2520a%2520drop-in%2520addition%2520to%2520open-source%2520VLM%2520training%2520corpora%252C%2520advancing%2520perceptual%2520precision%2520of%2520future%2520models.%2520Project%2520webpage%253A%2520https%253A//glab-caltech.github.io/twin/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Same%20or%20Not%3F%20Enhancing%20Visual%20Perception%20in%20Vision-Language%20Models&entry.906535625=Damiano%20Marsili%20and%20Aditya%20Mehta%20and%20Ryan%20Y.%20Lin%20and%20Georgia%20Gkioxari&entry.1292438233=Vision-language%20models%20%28VLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20remain%20coarse-grained%2C%20exhibit%20visual%20biases%2C%20and%20miss%20subtle%20visual%20details.%20Existing%20training%20corpora%20reinforce%20this%20limitation%20by%20emphasizing%20general%20recognition%20%28%22Is%20it%20a%20cat%20or%20a%20dog%3F%22%29%20over%20fine-grained%20perception.%20To%20address%20this%2C%20we%20introduce%20a%20new%20training%20corpus%20and%20task%20designed%20to%20enhance%20the%20perceptual%20abilities%20of%20VLMs.%20TWIN%20is%20a%20large-scale%20dataset%20of%20561%2C000%20image-pair%20queries%20that%20task%20models%20to%20determine%20whether%20two%20visually%20similar%20images%20depict%20the%20same%20object%2C%20encouraging%20attention%20to%20nuanced%20visual%20cues.%20The%20dataset%20spans%20a%20diverse%20range%20of%20everyday%20objects%20across%20contexts%2C%20viewpoints%2C%20and%20appearances.%20Fine-tuning%20VLMs%20on%20TWIN%20yields%20notable%20gains%20in%20fine-grained%20recognition%2C%20even%20on%20unseen%20domains%20such%20as%20art%2C%20animals%2C%20plants%2C%20and%20landmarks.%20To%20quantify%20these%20gains%2C%20we%20introduce%20FGVQA%2C%20a%20benchmark%20suite%20of%2012%2C000%20queries%20that%20repurposes%20fine-grained%20recognition%20and%20retrieval%20datasets%20from%20multiple%20domains.%20While%20existing%20VLMs%20struggle%20on%20FGVQA%2C%20when%20fine-tuned%20on%20TWIN%20they%20improve%20by%20up%20to%2019.3%25%2C%20without%20compromising%20performance%20on%20general%20VQA%20benchmarks.%20Finally%2C%20our%20TWIN%20dataset%20scales%20favorably%20with%20object%20annotations%2C%20and%20our%20analysis%20shows%20that%20scale%20is%20key%20to%20performance.%20We%20envision%20TWIN%20as%20a%20drop-in%20addition%20to%20open-source%20VLM%20training%20corpora%2C%20advancing%20perceptual%20precision%20of%20future%20models.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/twin/&entry.1838667208=http%3A//arxiv.org/abs/2512.23592v1&entry.124074799=Read"},
{"title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding", "author": "Keda Tao and Wenjie Du and Bohan Yu and Weiqiang Wang and Jian Liu and Huan Wang", "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.", "link": "http://arxiv.org/abs/2512.23646v1", "date": "2025-12-29", "relevancy": 2.8996, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniAgent%3A%20Audio-Guided%20Active%20Perception%20Agent%20for%20Omnimodal%20Audio-Video%20Understanding&body=Title%3A%20OmniAgent%3A%20Audio-Guided%20Active%20Perception%20Agent%20for%20Omnimodal%20Audio-Video%20Understanding%0AAuthor%3A%20Keda%20Tao%20and%20Wenjie%20Du%20and%20Bohan%20Yu%20and%20Weiqiang%20Wang%20and%20Jian%20Liu%20and%20Huan%20Wang%0AAbstract%3A%20Omnimodal%20large%20language%20models%20have%20made%20significant%20strides%20in%20unifying%20audio%20and%20visual%20modalities%3B%20however%2C%20they%20often%20lack%20the%20fine-grained%20cross-modal%20understanding%20and%20have%20difficulty%20with%20multimodal%20alignment.%20To%20address%20these%20limitations%2C%20we%20introduce%20OmniAgent%2C%20a%20fully%20audio-guided%20active%20perception%20agent%20that%20dynamically%20orchestrates%20specialized%20tools%20to%20achieve%20more%20fine-grained%20audio-visual%20reasoning.%20Unlike%20previous%20works%20that%20rely%20on%20rigid%2C%20static%20workflows%20and%20dense%20frame-captioning%2C%20this%20paper%20demonstrates%20a%20paradigm%20shift%20from%20passive%20response%20generation%20to%20active%20multimodal%20inquiry.%20OmniAgent%20employs%20dynamic%20planning%20to%20autonomously%20orchestrate%20tool%20invocation%20on%20demand%2C%20strategically%20concentrating%20perceptual%20attention%20on%20task-relevant%20cues.%20Central%20to%20our%20approach%20is%20a%20novel%20coarse-to-fine%20audio-guided%20perception%20paradigm%2C%20which%20leverages%20audio%20cues%20to%20localize%20temporal%20events%20and%20guide%20subsequent%20reasoning.%20Extensive%20empirical%20evaluations%20on%20three%20audio-video%20understanding%20benchmarks%20demonstrate%20that%20OmniAgent%20achieves%20state-of-the-art%20performance%2C%20surpassing%20leading%20open-source%20and%20proprietary%20models%20by%20substantial%20margins%20of%2010%25%20-%2020%25%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniAgent%253A%2520Audio-Guided%2520Active%2520Perception%2520Agent%2520for%2520Omnimodal%2520Audio-Video%2520Understanding%26entry.906535625%3DKeda%2520Tao%2520and%2520Wenjie%2520Du%2520and%2520Bohan%2520Yu%2520and%2520Weiqiang%2520Wang%2520and%2520Jian%2520Liu%2520and%2520Huan%2520Wang%26entry.1292438233%3DOmnimodal%2520large%2520language%2520models%2520have%2520made%2520significant%2520strides%2520in%2520unifying%2520audio%2520and%2520visual%2520modalities%253B%2520however%252C%2520they%2520often%2520lack%2520the%2520fine-grained%2520cross-modal%2520understanding%2520and%2520have%2520difficulty%2520with%2520multimodal%2520alignment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520OmniAgent%252C%2520a%2520fully%2520audio-guided%2520active%2520perception%2520agent%2520that%2520dynamically%2520orchestrates%2520specialized%2520tools%2520to%2520achieve%2520more%2520fine-grained%2520audio-visual%2520reasoning.%2520Unlike%2520previous%2520works%2520that%2520rely%2520on%2520rigid%252C%2520static%2520workflows%2520and%2520dense%2520frame-captioning%252C%2520this%2520paper%2520demonstrates%2520a%2520paradigm%2520shift%2520from%2520passive%2520response%2520generation%2520to%2520active%2520multimodal%2520inquiry.%2520OmniAgent%2520employs%2520dynamic%2520planning%2520to%2520autonomously%2520orchestrate%2520tool%2520invocation%2520on%2520demand%252C%2520strategically%2520concentrating%2520perceptual%2520attention%2520on%2520task-relevant%2520cues.%2520Central%2520to%2520our%2520approach%2520is%2520a%2520novel%2520coarse-to-fine%2520audio-guided%2520perception%2520paradigm%252C%2520which%2520leverages%2520audio%2520cues%2520to%2520localize%2520temporal%2520events%2520and%2520guide%2520subsequent%2520reasoning.%2520Extensive%2520empirical%2520evaluations%2520on%2520three%2520audio-video%2520understanding%2520benchmarks%2520demonstrate%2520that%2520OmniAgent%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520leading%2520open-source%2520and%2520proprietary%2520models%2520by%2520substantial%2520margins%2520of%252010%2525%2520-%252020%2525%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniAgent%3A%20Audio-Guided%20Active%20Perception%20Agent%20for%20Omnimodal%20Audio-Video%20Understanding&entry.906535625=Keda%20Tao%20and%20Wenjie%20Du%20and%20Bohan%20Yu%20and%20Weiqiang%20Wang%20and%20Jian%20Liu%20and%20Huan%20Wang&entry.1292438233=Omnimodal%20large%20language%20models%20have%20made%20significant%20strides%20in%20unifying%20audio%20and%20visual%20modalities%3B%20however%2C%20they%20often%20lack%20the%20fine-grained%20cross-modal%20understanding%20and%20have%20difficulty%20with%20multimodal%20alignment.%20To%20address%20these%20limitations%2C%20we%20introduce%20OmniAgent%2C%20a%20fully%20audio-guided%20active%20perception%20agent%20that%20dynamically%20orchestrates%20specialized%20tools%20to%20achieve%20more%20fine-grained%20audio-visual%20reasoning.%20Unlike%20previous%20works%20that%20rely%20on%20rigid%2C%20static%20workflows%20and%20dense%20frame-captioning%2C%20this%20paper%20demonstrates%20a%20paradigm%20shift%20from%20passive%20response%20generation%20to%20active%20multimodal%20inquiry.%20OmniAgent%20employs%20dynamic%20planning%20to%20autonomously%20orchestrate%20tool%20invocation%20on%20demand%2C%20strategically%20concentrating%20perceptual%20attention%20on%20task-relevant%20cues.%20Central%20to%20our%20approach%20is%20a%20novel%20coarse-to-fine%20audio-guided%20perception%20paradigm%2C%20which%20leverages%20audio%20cues%20to%20localize%20temporal%20events%20and%20guide%20subsequent%20reasoning.%20Extensive%20empirical%20evaluations%20on%20three%20audio-video%20understanding%20benchmarks%20demonstrate%20that%20OmniAgent%20achieves%20state-of-the-art%20performance%2C%20surpassing%20leading%20open-source%20and%20proprietary%20models%20by%20substantial%20margins%20of%2010%25%20-%2020%25%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.23646v1&entry.124074799=Read"},
{"title": "CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models", "author": "Zongsheng Cao and Yangfan He and Anran Liu and Jun Xie and Feng Chen and Zepeng Wang", "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \\textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.", "link": "http://arxiv.org/abs/2512.23453v1", "date": "2025-12-29", "relevancy": 2.7988, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoFi-Dec%3A%20Hallucination-Resistant%20Decoding%20via%20Coarse-to-Fine%20Generative%20Feedback%20in%20Large%20Vision-Language%20Models&body=Title%3A%20CoFi-Dec%3A%20Hallucination-Resistant%20Decoding%20via%20Coarse-to-Fine%20Generative%20Feedback%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Jun%20Xie%20and%20Feng%20Chen%20and%20Zepeng%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20impressive%20progress%20in%20multi-modal%20understanding%20and%20generation.%20However%2C%20they%20still%20tend%20to%20produce%20hallucinated%20content%20that%20is%20inconsistent%20with%20the%20visual%20input%2C%20which%20limits%20their%20reliability%20in%20real-world%20applications.%20We%20propose%20%5Ctextbf%7BCoFi-Dec%7D%2C%20a%20training-free%20decoding%20framework%20that%20mitigates%20hallucinations%20by%20integrating%20generative%20self-feedback%20with%20coarse-to-fine%20visual%20conditioning.%20Inspired%20by%20the%20human%20visual%20process%20from%20global%20scene%20perception%20to%20detailed%20inspection%2C%20CoFi-Dec%20first%20generates%20two%20intermediate%20textual%20responses%20conditioned%20on%20coarse-%20and%20fine-grained%20views%20of%20the%20original%20image.%20These%20responses%20are%20then%20transformed%20into%20synthetic%20images%20using%20a%20text-to-image%20model%2C%20forming%20multi-level%20visual%20hypotheses%20that%20enrich%20grounding%20cues.%20To%20unify%20the%20predictions%20from%20these%20multiple%20visual%20conditions%2C%20we%20introduce%20a%20Wasserstein-based%20fusion%20mechanism%20that%20aligns%20their%20predictive%20distributions%20into%20a%20geometrically%20consistent%20decoding%20trajectory.%20This%20principled%20fusion%20reconciles%20high-level%20semantic%20consistency%20with%20fine-grained%20visual%20grounding%2C%20leading%20to%20more%20robust%20and%20faithful%20outputs.%20Extensive%20experiments%20on%20six%20hallucination-focused%20benchmarks%20show%20that%20CoFi-Dec%20substantially%20reduces%20both%20entity-level%20and%20semantic-level%20hallucinations%2C%20outperforming%20existing%20decoding%20strategies.%20The%20framework%20is%20model-agnostic%2C%20requires%20no%20additional%20training%2C%20and%20can%20be%20seamlessly%20applied%20to%20a%20wide%20range%20of%20LVLMs.%20The%20implementation%20is%20available%20at%20https%3A//github.com/AI-Researcher-Team/CoFi-Dec.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoFi-Dec%253A%2520Hallucination-Resistant%2520Decoding%2520via%2520Coarse-to-Fine%2520Generative%2520Feedback%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DZongsheng%2520Cao%2520and%2520Yangfan%2520He%2520and%2520Anran%2520Liu%2520and%2520Jun%2520Xie%2520and%2520Feng%2520Chen%2520and%2520Zepeng%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520impressive%2520progress%2520in%2520multi-modal%2520understanding%2520and%2520generation.%2520However%252C%2520they%2520still%2520tend%2520to%2520produce%2520hallucinated%2520content%2520that%2520is%2520inconsistent%2520with%2520the%2520visual%2520input%252C%2520which%2520limits%2520their%2520reliability%2520in%2520real-world%2520applications.%2520We%2520propose%2520%255Ctextbf%257BCoFi-Dec%257D%252C%2520a%2520training-free%2520decoding%2520framework%2520that%2520mitigates%2520hallucinations%2520by%2520integrating%2520generative%2520self-feedback%2520with%2520coarse-to-fine%2520visual%2520conditioning.%2520Inspired%2520by%2520the%2520human%2520visual%2520process%2520from%2520global%2520scene%2520perception%2520to%2520detailed%2520inspection%252C%2520CoFi-Dec%2520first%2520generates%2520two%2520intermediate%2520textual%2520responses%2520conditioned%2520on%2520coarse-%2520and%2520fine-grained%2520views%2520of%2520the%2520original%2520image.%2520These%2520responses%2520are%2520then%2520transformed%2520into%2520synthetic%2520images%2520using%2520a%2520text-to-image%2520model%252C%2520forming%2520multi-level%2520visual%2520hypotheses%2520that%2520enrich%2520grounding%2520cues.%2520To%2520unify%2520the%2520predictions%2520from%2520these%2520multiple%2520visual%2520conditions%252C%2520we%2520introduce%2520a%2520Wasserstein-based%2520fusion%2520mechanism%2520that%2520aligns%2520their%2520predictive%2520distributions%2520into%2520a%2520geometrically%2520consistent%2520decoding%2520trajectory.%2520This%2520principled%2520fusion%2520reconciles%2520high-level%2520semantic%2520consistency%2520with%2520fine-grained%2520visual%2520grounding%252C%2520leading%2520to%2520more%2520robust%2520and%2520faithful%2520outputs.%2520Extensive%2520experiments%2520on%2520six%2520hallucination-focused%2520benchmarks%2520show%2520that%2520CoFi-Dec%2520substantially%2520reduces%2520both%2520entity-level%2520and%2520semantic-level%2520hallucinations%252C%2520outperforming%2520existing%2520decoding%2520strategies.%2520The%2520framework%2520is%2520model-agnostic%252C%2520requires%2520no%2520additional%2520training%252C%2520and%2520can%2520be%2520seamlessly%2520applied%2520to%2520a%2520wide%2520range%2520of%2520LVLMs.%2520The%2520implementation%2520is%2520available%2520at%2520https%253A//github.com/AI-Researcher-Team/CoFi-Dec.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoFi-Dec%3A%20Hallucination-Resistant%20Decoding%20via%20Coarse-to-Fine%20Generative%20Feedback%20in%20Large%20Vision-Language%20Models&entry.906535625=Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Jun%20Xie%20and%20Feng%20Chen%20and%20Zepeng%20Wang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20impressive%20progress%20in%20multi-modal%20understanding%20and%20generation.%20However%2C%20they%20still%20tend%20to%20produce%20hallucinated%20content%20that%20is%20inconsistent%20with%20the%20visual%20input%2C%20which%20limits%20their%20reliability%20in%20real-world%20applications.%20We%20propose%20%5Ctextbf%7BCoFi-Dec%7D%2C%20a%20training-free%20decoding%20framework%20that%20mitigates%20hallucinations%20by%20integrating%20generative%20self-feedback%20with%20coarse-to-fine%20visual%20conditioning.%20Inspired%20by%20the%20human%20visual%20process%20from%20global%20scene%20perception%20to%20detailed%20inspection%2C%20CoFi-Dec%20first%20generates%20two%20intermediate%20textual%20responses%20conditioned%20on%20coarse-%20and%20fine-grained%20views%20of%20the%20original%20image.%20These%20responses%20are%20then%20transformed%20into%20synthetic%20images%20using%20a%20text-to-image%20model%2C%20forming%20multi-level%20visual%20hypotheses%20that%20enrich%20grounding%20cues.%20To%20unify%20the%20predictions%20from%20these%20multiple%20visual%20conditions%2C%20we%20introduce%20a%20Wasserstein-based%20fusion%20mechanism%20that%20aligns%20their%20predictive%20distributions%20into%20a%20geometrically%20consistent%20decoding%20trajectory.%20This%20principled%20fusion%20reconciles%20high-level%20semantic%20consistency%20with%20fine-grained%20visual%20grounding%2C%20leading%20to%20more%20robust%20and%20faithful%20outputs.%20Extensive%20experiments%20on%20six%20hallucination-focused%20benchmarks%20show%20that%20CoFi-Dec%20substantially%20reduces%20both%20entity-level%20and%20semantic-level%20hallucinations%2C%20outperforming%20existing%20decoding%20strategies.%20The%20framework%20is%20model-agnostic%2C%20requires%20no%20additional%20training%2C%20and%20can%20be%20seamlessly%20applied%20to%20a%20wide%20range%20of%20LVLMs.%20The%20implementation%20is%20available%20at%20https%3A//github.com/AI-Researcher-Team/CoFi-Dec.&entry.1838667208=http%3A//arxiv.org/abs/2512.23453v1&entry.124074799=Read"},
{"title": "When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework", "author": "Haoyu Liu and Chaoyu Gong and Mengke He and Jiate Li and Kai Han and Siqiang Luo", "abstract": "The proliferation of generative video models has made detecting AI-generated and manipulated videos an urgent challenge. Existing detection approaches often fail to generalize across diverse manipulation types due to their reliance on isolated spatial, temporal, or spectral information, and typically require large models to perform well. This paper introduces SSTGNN, a lightweight Spatial-Spectral-Temporal Graph Neural Network framework that represents videos as structured graphs, enabling joint reasoning over spatial inconsistencies, temporal artifacts, and spectral distortions. SSTGNN incorporates learnable spectral filters and spatial-temporal differential modeling into a unified graph-based architecture, capturing subtle manipulation traces more effectively. Extensive experiments on diverse benchmark datasets demonstrate that SSTGNN not only achieves superior performance in both in-domain and cross-domain settings, but also offers strong efficiency and resource allocation. Remarkably, SSTGNN accomplishes these results with up to 42$\\times$ fewer parameters than state-of-the-art models, making it highly lightweight and resource-friendly for real-world deployment.", "link": "http://arxiv.org/abs/2508.05526v2", "date": "2025-12-29", "relevancy": 2.7663, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5575}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5514}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%20Lightweight%20Learning%20Framework&body=Title%3A%20When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%20Lightweight%20Learning%20Framework%0AAuthor%3A%20Haoyu%20Liu%20and%20Chaoyu%20Gong%20and%20Mengke%20He%20and%20Jiate%20Li%20and%20Kai%20Han%20and%20Siqiang%20Luo%0AAbstract%3A%20The%20proliferation%20of%20generative%20video%20models%20has%20made%20detecting%20AI-generated%20and%20manipulated%20videos%20an%20urgent%20challenge.%20Existing%20detection%20approaches%20often%20fail%20to%20generalize%20across%20diverse%20manipulation%20types%20due%20to%20their%20reliance%20on%20isolated%20spatial%2C%20temporal%2C%20or%20spectral%20information%2C%20and%20typically%20require%20large%20models%20to%20perform%20well.%20This%20paper%20introduces%20SSTGNN%2C%20a%20lightweight%20Spatial-Spectral-Temporal%20Graph%20Neural%20Network%20framework%20that%20represents%20videos%20as%20structured%20graphs%2C%20enabling%20joint%20reasoning%20over%20spatial%20inconsistencies%2C%20temporal%20artifacts%2C%20and%20spectral%20distortions.%20SSTGNN%20incorporates%20learnable%20spectral%20filters%20and%20spatial-temporal%20differential%20modeling%20into%20a%20unified%20graph-based%20architecture%2C%20capturing%20subtle%20manipulation%20traces%20more%20effectively.%20Extensive%20experiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20SSTGNN%20not%20only%20achieves%20superior%20performance%20in%20both%20in-domain%20and%20cross-domain%20settings%2C%20but%20also%20offers%20strong%20efficiency%20and%20resource%20allocation.%20Remarkably%2C%20SSTGNN%20accomplishes%20these%20results%20with%20up%20to%2042%24%5Ctimes%24%20fewer%20parameters%20than%20state-of-the-art%20models%2C%20making%20it%20highly%20lightweight%20and%20resource-friendly%20for%20real-world%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Deepfake%2520Detection%2520Meets%2520Graph%2520Neural%2520Network%253Aa%2520Unified%2520and%2520Lightweight%2520Learning%2520Framework%26entry.906535625%3DHaoyu%2520Liu%2520and%2520Chaoyu%2520Gong%2520and%2520Mengke%2520He%2520and%2520Jiate%2520Li%2520and%2520Kai%2520Han%2520and%2520Siqiang%2520Luo%26entry.1292438233%3DThe%2520proliferation%2520of%2520generative%2520video%2520models%2520has%2520made%2520detecting%2520AI-generated%2520and%2520manipulated%2520videos%2520an%2520urgent%2520challenge.%2520Existing%2520detection%2520approaches%2520often%2520fail%2520to%2520generalize%2520across%2520diverse%2520manipulation%2520types%2520due%2520to%2520their%2520reliance%2520on%2520isolated%2520spatial%252C%2520temporal%252C%2520or%2520spectral%2520information%252C%2520and%2520typically%2520require%2520large%2520models%2520to%2520perform%2520well.%2520This%2520paper%2520introduces%2520SSTGNN%252C%2520a%2520lightweight%2520Spatial-Spectral-Temporal%2520Graph%2520Neural%2520Network%2520framework%2520that%2520represents%2520videos%2520as%2520structured%2520graphs%252C%2520enabling%2520joint%2520reasoning%2520over%2520spatial%2520inconsistencies%252C%2520temporal%2520artifacts%252C%2520and%2520spectral%2520distortions.%2520SSTGNN%2520incorporates%2520learnable%2520spectral%2520filters%2520and%2520spatial-temporal%2520differential%2520modeling%2520into%2520a%2520unified%2520graph-based%2520architecture%252C%2520capturing%2520subtle%2520manipulation%2520traces%2520more%2520effectively.%2520Extensive%2520experiments%2520on%2520diverse%2520benchmark%2520datasets%2520demonstrate%2520that%2520SSTGNN%2520not%2520only%2520achieves%2520superior%2520performance%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520settings%252C%2520but%2520also%2520offers%2520strong%2520efficiency%2520and%2520resource%2520allocation.%2520Remarkably%252C%2520SSTGNN%2520accomplishes%2520these%2520results%2520with%2520up%2520to%252042%2524%255Ctimes%2524%2520fewer%2520parameters%2520than%2520state-of-the-art%2520models%252C%2520making%2520it%2520highly%2520lightweight%2520and%2520resource-friendly%2520for%2520real-world%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Deepfake%20Detection%20Meets%20Graph%20Neural%20Network%3Aa%20Unified%20and%20Lightweight%20Learning%20Framework&entry.906535625=Haoyu%20Liu%20and%20Chaoyu%20Gong%20and%20Mengke%20He%20and%20Jiate%20Li%20and%20Kai%20Han%20and%20Siqiang%20Luo&entry.1292438233=The%20proliferation%20of%20generative%20video%20models%20has%20made%20detecting%20AI-generated%20and%20manipulated%20videos%20an%20urgent%20challenge.%20Existing%20detection%20approaches%20often%20fail%20to%20generalize%20across%20diverse%20manipulation%20types%20due%20to%20their%20reliance%20on%20isolated%20spatial%2C%20temporal%2C%20or%20spectral%20information%2C%20and%20typically%20require%20large%20models%20to%20perform%20well.%20This%20paper%20introduces%20SSTGNN%2C%20a%20lightweight%20Spatial-Spectral-Temporal%20Graph%20Neural%20Network%20framework%20that%20represents%20videos%20as%20structured%20graphs%2C%20enabling%20joint%20reasoning%20over%20spatial%20inconsistencies%2C%20temporal%20artifacts%2C%20and%20spectral%20distortions.%20SSTGNN%20incorporates%20learnable%20spectral%20filters%20and%20spatial-temporal%20differential%20modeling%20into%20a%20unified%20graph-based%20architecture%2C%20capturing%20subtle%20manipulation%20traces%20more%20effectively.%20Extensive%20experiments%20on%20diverse%20benchmark%20datasets%20demonstrate%20that%20SSTGNN%20not%20only%20achieves%20superior%20performance%20in%20both%20in-domain%20and%20cross-domain%20settings%2C%20but%20also%20offers%20strong%20efficiency%20and%20resource%20allocation.%20Remarkably%2C%20SSTGNN%20accomplishes%20these%20results%20with%20up%20to%2042%24%5Ctimes%24%20fewer%20parameters%20than%20state-of-the-art%20models%2C%20making%20it%20highly%20lightweight%20and%20resource-friendly%20for%20real-world%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2508.05526v2&entry.124074799=Read"},
{"title": "Instruction-Following Evaluation of Large Vision-Language Models", "author": "Daiki Shiono and Shumpei Miyawaki and Ryota Tanaka and Jun Suzuki", "abstract": "Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.", "link": "http://arxiv.org/abs/2512.23572v1", "date": "2025-12-29", "relevancy": 2.7585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Following%20Evaluation%20of%20Large%20Vision-Language%20Models&body=Title%3A%20Instruction-Following%20Evaluation%20of%20Large%20Vision-Language%20Models%0AAuthor%3A%20Daiki%20Shiono%20and%20Shumpei%20Miyawaki%20and%20Ryota%20Tanaka%20and%20Jun%20Suzuki%0AAbstract%3A%20Following%20the%20initial%20flourishing%20of%20large%20language%20models%20%28LLMs%29%2C%20there%20has%20been%20a%20surge%20in%20proposed%20large%20vision-language%20models%20%28LVLMs%29%20that%20integrate%20LLMs%20with%20vision%20capabilities.%20However%2C%20it%20has%20been%20observed%20that%20LVLMs%2C%20after%20tuning%20to%20visual%20instruction%20using%20commonly%20used%20training%20datasets%2C%20often%20fail%20to%20exhibit%20the%20instruction-following%20ability%20that%20was%20present%20in%20the%20LLM%20before%20integration%2C%20leading%20to%20results%20in%20which%20they%20do%20not%20follow%20task%20instructions%20as%20expected.%20This%20study%20quantitatively%20demonstrates%20that%20LVLMs%27%20instruction-following%20ability%20declines%20after%20fine-tuning%20and%20analyzes%20its%20underlying%20causes.%20In%20particular%2C%20we%20constructed%20new%20training%20datasets%20highlighting%20whether%20the%20output%20format%20is%20specified.%20Then%2C%20we%20investigated%20how%20explicitly%20indicating%20the%20output%20format%20during%20fine-tuning%20affects%20LVLMs%27%20instruction-following%20ability.%20Our%20quantitative%20evaluation%20confirmed%20that%20LVLMs%27%20instruction-following%20ability%20declines%20after%20fine-tuning%20with%20commonly%20used%20datasets.%20Furthermore%2C%20we%20found%20that%20LVLMs%20trained%20with%20datasets%2C%20including%20instructions%20on%20output%20format%2C%20tend%20to%20follow%20instructions%20more%20accurately%20than%20models%20that%20do%20not.%20These%20findings%20suggest%20that%20including%20samples%20with%20instructions%20on%20output%20format%20during%20%28visual%29%20instruction%20tuning%20may%20help%20mitigate%20the%20decline%20in%20instruction-following%20abilities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Following%2520Evaluation%2520of%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DDaiki%2520Shiono%2520and%2520Shumpei%2520Miyawaki%2520and%2520Ryota%2520Tanaka%2520and%2520Jun%2520Suzuki%26entry.1292438233%3DFollowing%2520the%2520initial%2520flourishing%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%2520has%2520been%2520a%2520surge%2520in%2520proposed%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520that%2520integrate%2520LLMs%2520with%2520vision%2520capabilities.%2520However%252C%2520it%2520has%2520been%2520observed%2520that%2520LVLMs%252C%2520after%2520tuning%2520to%2520visual%2520instruction%2520using%2520commonly%2520used%2520training%2520datasets%252C%2520often%2520fail%2520to%2520exhibit%2520the%2520instruction-following%2520ability%2520that%2520was%2520present%2520in%2520the%2520LLM%2520before%2520integration%252C%2520leading%2520to%2520results%2520in%2520which%2520they%2520do%2520not%2520follow%2520task%2520instructions%2520as%2520expected.%2520This%2520study%2520quantitatively%2520demonstrates%2520that%2520LVLMs%2527%2520instruction-following%2520ability%2520declines%2520after%2520fine-tuning%2520and%2520analyzes%2520its%2520underlying%2520causes.%2520In%2520particular%252C%2520we%2520constructed%2520new%2520training%2520datasets%2520highlighting%2520whether%2520the%2520output%2520format%2520is%2520specified.%2520Then%252C%2520we%2520investigated%2520how%2520explicitly%2520indicating%2520the%2520output%2520format%2520during%2520fine-tuning%2520affects%2520LVLMs%2527%2520instruction-following%2520ability.%2520Our%2520quantitative%2520evaluation%2520confirmed%2520that%2520LVLMs%2527%2520instruction-following%2520ability%2520declines%2520after%2520fine-tuning%2520with%2520commonly%2520used%2520datasets.%2520Furthermore%252C%2520we%2520found%2520that%2520LVLMs%2520trained%2520with%2520datasets%252C%2520including%2520instructions%2520on%2520output%2520format%252C%2520tend%2520to%2520follow%2520instructions%2520more%2520accurately%2520than%2520models%2520that%2520do%2520not.%2520These%2520findings%2520suggest%2520that%2520including%2520samples%2520with%2520instructions%2520on%2520output%2520format%2520during%2520%2528visual%2529%2520instruction%2520tuning%2520may%2520help%2520mitigate%2520the%2520decline%2520in%2520instruction-following%2520abilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Following%20Evaluation%20of%20Large%20Vision-Language%20Models&entry.906535625=Daiki%20Shiono%20and%20Shumpei%20Miyawaki%20and%20Ryota%20Tanaka%20and%20Jun%20Suzuki&entry.1292438233=Following%20the%20initial%20flourishing%20of%20large%20language%20models%20%28LLMs%29%2C%20there%20has%20been%20a%20surge%20in%20proposed%20large%20vision-language%20models%20%28LVLMs%29%20that%20integrate%20LLMs%20with%20vision%20capabilities.%20However%2C%20it%20has%20been%20observed%20that%20LVLMs%2C%20after%20tuning%20to%20visual%20instruction%20using%20commonly%20used%20training%20datasets%2C%20often%20fail%20to%20exhibit%20the%20instruction-following%20ability%20that%20was%20present%20in%20the%20LLM%20before%20integration%2C%20leading%20to%20results%20in%20which%20they%20do%20not%20follow%20task%20instructions%20as%20expected.%20This%20study%20quantitatively%20demonstrates%20that%20LVLMs%27%20instruction-following%20ability%20declines%20after%20fine-tuning%20and%20analyzes%20its%20underlying%20causes.%20In%20particular%2C%20we%20constructed%20new%20training%20datasets%20highlighting%20whether%20the%20output%20format%20is%20specified.%20Then%2C%20we%20investigated%20how%20explicitly%20indicating%20the%20output%20format%20during%20fine-tuning%20affects%20LVLMs%27%20instruction-following%20ability.%20Our%20quantitative%20evaluation%20confirmed%20that%20LVLMs%27%20instruction-following%20ability%20declines%20after%20fine-tuning%20with%20commonly%20used%20datasets.%20Furthermore%2C%20we%20found%20that%20LVLMs%20trained%20with%20datasets%2C%20including%20instructions%20on%20output%20format%2C%20tend%20to%20follow%20instructions%20more%20accurately%20than%20models%20that%20do%20not.%20These%20findings%20suggest%20that%20including%20samples%20with%20instructions%20on%20output%20format%20during%20%28visual%29%20instruction%20tuning%20may%20help%20mitigate%20the%20decline%20in%20instruction-following%20abilities.&entry.1838667208=http%3A//arxiv.org/abs/2512.23572v1&entry.124074799=Read"},
{"title": "Towards Generalisable Foundation Models for Brain MRI", "author": "Moona Mazher and Geoff J. M. Parker and Daniel C. Alexander", "abstract": "Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.", "link": "http://arxiv.org/abs/2510.23415v2", "date": "2025-12-29", "relevancy": 2.7543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalisable%20Foundation%20Models%20for%20Brain%20MRI&body=Title%3A%20Towards%20Generalisable%20Foundation%20Models%20for%20Brain%20MRI%0AAuthor%3A%20Moona%20Mazher%20and%20Geoff%20J.%20M.%20Parker%20and%20Daniel%20C.%20Alexander%0AAbstract%3A%20Foundation%20models%20in%20artificial%20intelligence%20%28AI%29%20are%20transforming%20medical%20imaging%20by%20enabling%20general-purpose%20feature%20learning%20from%20large-scale%2C%20unlabeled%20datasets.%20In%20this%20work%2C%20we%20introduce%20BrainFound%2C%20a%20self-supervised%20foundation%20model%20for%20brain%20MRI%2C%20built%20by%20extending%20DINO-v2%2C%20a%20vision%20transformer%20originally%20designed%20for%202D%20natural%20images.%20BrainFound%20adapts%20DINO-v2%20to%20model%20full%203D%20brain%20anatomy%20by%20incorporating%20volumetric%20information%20from%20sequential%20MRI%20slices%2C%20moving%20beyond%20conventional%20single-slice%20paradigms.%20It%20supports%20both%20single-%20and%20multimodal%20inputs%2C%20enabling%20a%20broad%20range%20of%20downstream%20tasks%2C%20including%20disease%20detection%20and%20image%20segmentation%2C%20while%20generalising%20across%20varied%20imaging%20protocols%20and%20clinical%20scenarios.%20We%20show%20that%20BrainFound%20consistently%20outperforms%20existing%20self-supervised%20pretraining%20strategies%20and%20supervised%20baselines%2C%20particularly%20in%20label-scarce%20and%20multi-contrast%20settings.%20By%20integrating%20information%20from%20diverse%203D%20MRI%20modalities%20%28e.g.%2C%20T1%2C%20T2%2C%20FLAIR%29%2C%20it%20enhances%20diagnostic%20accuracy%20and%20reduces%20dependency%20on%20extensive%20expert%20annotations.%20This%20flexibility%20makes%20BrainFound%20a%20scalable%20and%20practical%20solution%20for%203D%20neuroimaging%20pipelines%2C%20with%20significant%20potential%20for%20clinical%20deployment%20and%20research%20innovation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalisable%2520Foundation%2520Models%2520for%2520Brain%2520MRI%26entry.906535625%3DMoona%2520Mazher%2520and%2520Geoff%2520J.%2520M.%2520Parker%2520and%2520Daniel%2520C.%2520Alexander%26entry.1292438233%3DFoundation%2520models%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520are%2520transforming%2520medical%2520imaging%2520by%2520enabling%2520general-purpose%2520feature%2520learning%2520from%2520large-scale%252C%2520unlabeled%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520BrainFound%252C%2520a%2520self-supervised%2520foundation%2520model%2520for%2520brain%2520MRI%252C%2520built%2520by%2520extending%2520DINO-v2%252C%2520a%2520vision%2520transformer%2520originally%2520designed%2520for%25202D%2520natural%2520images.%2520BrainFound%2520adapts%2520DINO-v2%2520to%2520model%2520full%25203D%2520brain%2520anatomy%2520by%2520incorporating%2520volumetric%2520information%2520from%2520sequential%2520MRI%2520slices%252C%2520moving%2520beyond%2520conventional%2520single-slice%2520paradigms.%2520It%2520supports%2520both%2520single-%2520and%2520multimodal%2520inputs%252C%2520enabling%2520a%2520broad%2520range%2520of%2520downstream%2520tasks%252C%2520including%2520disease%2520detection%2520and%2520image%2520segmentation%252C%2520while%2520generalising%2520across%2520varied%2520imaging%2520protocols%2520and%2520clinical%2520scenarios.%2520We%2520show%2520that%2520BrainFound%2520consistently%2520outperforms%2520existing%2520self-supervised%2520pretraining%2520strategies%2520and%2520supervised%2520baselines%252C%2520particularly%2520in%2520label-scarce%2520and%2520multi-contrast%2520settings.%2520By%2520integrating%2520information%2520from%2520diverse%25203D%2520MRI%2520modalities%2520%2528e.g.%252C%2520T1%252C%2520T2%252C%2520FLAIR%2529%252C%2520it%2520enhances%2520diagnostic%2520accuracy%2520and%2520reduces%2520dependency%2520on%2520extensive%2520expert%2520annotations.%2520This%2520flexibility%2520makes%2520BrainFound%2520a%2520scalable%2520and%2520practical%2520solution%2520for%25203D%2520neuroimaging%2520pipelines%252C%2520with%2520significant%2520potential%2520for%2520clinical%2520deployment%2520and%2520research%2520innovation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalisable%20Foundation%20Models%20for%20Brain%20MRI&entry.906535625=Moona%20Mazher%20and%20Geoff%20J.%20M.%20Parker%20and%20Daniel%20C.%20Alexander&entry.1292438233=Foundation%20models%20in%20artificial%20intelligence%20%28AI%29%20are%20transforming%20medical%20imaging%20by%20enabling%20general-purpose%20feature%20learning%20from%20large-scale%2C%20unlabeled%20datasets.%20In%20this%20work%2C%20we%20introduce%20BrainFound%2C%20a%20self-supervised%20foundation%20model%20for%20brain%20MRI%2C%20built%20by%20extending%20DINO-v2%2C%20a%20vision%20transformer%20originally%20designed%20for%202D%20natural%20images.%20BrainFound%20adapts%20DINO-v2%20to%20model%20full%203D%20brain%20anatomy%20by%20incorporating%20volumetric%20information%20from%20sequential%20MRI%20slices%2C%20moving%20beyond%20conventional%20single-slice%20paradigms.%20It%20supports%20both%20single-%20and%20multimodal%20inputs%2C%20enabling%20a%20broad%20range%20of%20downstream%20tasks%2C%20including%20disease%20detection%20and%20image%20segmentation%2C%20while%20generalising%20across%20varied%20imaging%20protocols%20and%20clinical%20scenarios.%20We%20show%20that%20BrainFound%20consistently%20outperforms%20existing%20self-supervised%20pretraining%20strategies%20and%20supervised%20baselines%2C%20particularly%20in%20label-scarce%20and%20multi-contrast%20settings.%20By%20integrating%20information%20from%20diverse%203D%20MRI%20modalities%20%28e.g.%2C%20T1%2C%20T2%2C%20FLAIR%29%2C%20it%20enhances%20diagnostic%20accuracy%20and%20reduces%20dependency%20on%20extensive%20expert%20annotations.%20This%20flexibility%20makes%20BrainFound%20a%20scalable%20and%20practical%20solution%20for%203D%20neuroimaging%20pipelines%2C%20with%20significant%20potential%20for%20clinical%20deployment%20and%20research%20innovation.&entry.1838667208=http%3A//arxiv.org/abs/2510.23415v2&entry.124074799=Read"},
{"title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI", "author": "Shravya Kanchi and Neal Mangaokar and Aravind Cheruvu and Sifat Muhammad Abdullah and Shirin Nilizadeh and Atul Prakash and Bimal Viswanath", "abstract": "Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.", "link": "http://arxiv.org/abs/2507.06092v3", "date": "2025-12-29", "relevancy": 2.7263, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5675}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5445}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI&body=Title%3A%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI%0AAuthor%3A%20Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath%0AAbstract%3A%20Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%20tasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%20advancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%20performance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%20the%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%20address%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%20augmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%20techniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%20across%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%20introduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%20synthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%20performance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%20in%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%20Furthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%20concept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%20process.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%20initialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%20characteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%20distributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%20GenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%20tools%20designed%20for%20security%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06092v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Data%2520Challenges%2520in%2520ML-based%2520Security%2520Tasks%253A%2520Lessons%2520from%2520Integrating%2520Generative%2520AI%26entry.906535625%3DShravya%2520Kanchi%2520and%2520Neal%2520Mangaokar%2520and%2520Aravind%2520Cheruvu%2520and%2520Sifat%2520Muhammad%2520Abdullah%2520and%2520Shirin%2520Nilizadeh%2520and%2520Atul%2520Prakash%2520and%2520Bimal%2520Viswanath%26entry.1292438233%3DMachine%2520learning-based%2520supervised%2520classifiers%2520are%2520widely%2520used%2520for%2520security%2520tasks%252C%2520and%2520their%2520improvement%2520has%2520been%2520largely%2520focused%2520on%2520algorithmic%2520advancements.%2520We%2520argue%2520that%2520data%2520challenges%2520that%2520negatively%2520impact%2520the%2520performance%2520of%2520these%2520classifiers%2520have%2520received%2520limited%2520attention.%2520We%2520address%2520the%2520following%2520research%2520question%253A%2520Can%2520developments%2520in%2520Generative%2520AI%2520%2528GenAI%2529%2520address%2520these%2520data%2520challenges%2520and%2520improve%2520classifier%2520performance%253F%2520We%2520propose%2520augmenting%2520training%2520datasets%2520with%2520synthetic%2520data%2520generated%2520using%2520GenAI%2520techniques%2520to%2520improve%2520classifier%2520generalization.%2520We%2520evaluate%2520this%2520approach%2520across%25207%2520diverse%2520security%2520tasks%2520using%25206%2520state-of-the-art%2520GenAI%2520methods%2520and%2520introduce%2520a%2520novel%2520GenAI%2520scheme%2520called%2520Nimai%2520that%2520enables%2520highly%2520controlled%2520data%2520synthesis.%2520We%2520find%2520that%2520GenAI%2520techniques%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520security%2520classifiers%252C%2520achieving%2520improvements%2520of%2520up%2520to%252032.6%2525%2520even%2520in%2520severely%2520data-constrained%2520settings%2520%2528only%2520~180%2520training%2520samples%2529.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520GenAI%2520can%2520facilitate%2520rapid%2520adaptation%2520to%2520concept%2520drift%2520post-deployment%252C%2520requiring%2520minimal%2520labeling%2520in%2520the%2520adjustment%2520process.%2520Despite%2520successes%252C%2520our%2520study%2520finds%2520that%2520some%2520GenAI%2520schemes%2520struggle%2520to%2520initialize%2520%2528train%2520and%2520produce%2520data%2529%2520on%2520certain%2520security%2520tasks.%2520We%2520also%2520identify%2520characteristics%2520of%2520specific%2520tasks%252C%2520such%2520as%2520noisy%2520labels%252C%2520overlapping%2520class%2520distributions%252C%2520and%2520sparse%2520feature%2520vectors%252C%2520which%2520hinder%2520performance%2520boost%2520using%2520GenAI.%2520We%2520believe%2520that%2520our%2520study%2520will%2520drive%2520the%2520development%2520of%2520future%2520GenAI%2520tools%2520designed%2520for%2520security%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06092v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI&entry.906535625=Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath&entry.1292438233=Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%20tasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%20advancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%20performance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%20the%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%20address%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%20augmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%20techniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%20across%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%20introduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%20synthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%20performance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%20in%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%20Furthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%20concept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%20process.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%20initialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%20characteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%20distributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%20GenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%20tools%20designed%20for%20security%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2507.06092v3&entry.124074799=Read"},
{"title": "Fine-Tuned Vision Transformers Capture Complex Wheat Spike Morphology for Volume Estimation from RGB Images", "author": "Olivia Zumsteg and Nico Graf and Aaron Haeusler and Norbert Kirchgessner and Nicola Storni and Lukas Roth and Andreas Hund", "abstract": "Estimating three-dimensional morphological traits such as volume from two-dimensional RGB images presents inherent challenges due to the loss of depth information, projection distortions, and occlusions under field conditions. In this work, we explore multiple approaches for non-destructive volume estimation of wheat spikes using RGB images and structured-light 3D scans as ground truth references. Wheat spike volume is promising for phenotyping as it shows high correlation with spike dry weight, a key component of fruiting efficiency. Accounting for the complex geometry of the spikes, we compare different neural network approaches for volume estimation from 2D images and benchmark them against two conventional baselines: a 2D area-based projection and a geometric reconstruction using axis-aligned cross-sections. Fine-tuned Vision Transformers (DINOv2 and DINOv3) with MLPs achieve the lowest MAPE of 5.08\\% and 4.67\\% and the highest correlation of 0.96 and 0.97 on six-view indoor images, outperforming fine-tuned CNNs (ResNet18 and ResNet50), wheat-specific backbones, and both baselines. When using frozen DINO backbones, deep-supervised LSTMs outperform MLPs, whereas after fine-tuning, improved high-level representations allow simple MLPs to outperform LSTMs. We demonstrate that object shape significantly impacts volume estimation accuracy, with irregular geometries such as wheat spikes posing greater challenges for geometric methods than for deep learning approaches. Fine-tuning DINOv3 on field-based single side-view images yields a MAPE of 8.39\\% and a correlation of 0.90, providing a novel pipeline and a fast, accurate, and non-destructive approach for wheat spike volume phenotyping.", "link": "http://arxiv.org/abs/2506.18060v2", "date": "2025-12-29", "relevancy": 2.7082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuned%20Vision%20Transformers%20Capture%20Complex%20Wheat%20Spike%20Morphology%20for%20Volume%20Estimation%20from%20RGB%20Images&body=Title%3A%20Fine-Tuned%20Vision%20Transformers%20Capture%20Complex%20Wheat%20Spike%20Morphology%20for%20Volume%20Estimation%20from%20RGB%20Images%0AAuthor%3A%20Olivia%20Zumsteg%20and%20Nico%20Graf%20and%20Aaron%20Haeusler%20and%20Norbert%20Kirchgessner%20and%20Nicola%20Storni%20and%20Lukas%20Roth%20and%20Andreas%20Hund%0AAbstract%3A%20Estimating%20three-dimensional%20morphological%20traits%20such%20as%20volume%20from%20two-dimensional%20RGB%20images%20presents%20inherent%20challenges%20due%20to%20the%20loss%20of%20depth%20information%2C%20projection%20distortions%2C%20and%20occlusions%20under%20field%20conditions.%20In%20this%20work%2C%20we%20explore%20multiple%20approaches%20for%20non-destructive%20volume%20estimation%20of%20wheat%20spikes%20using%20RGB%20images%20and%20structured-light%203D%20scans%20as%20ground%20truth%20references.%20Wheat%20spike%20volume%20is%20promising%20for%20phenotyping%20as%20it%20shows%20high%20correlation%20with%20spike%20dry%20weight%2C%20a%20key%20component%20of%20fruiting%20efficiency.%20Accounting%20for%20the%20complex%20geometry%20of%20the%20spikes%2C%20we%20compare%20different%20neural%20network%20approaches%20for%20volume%20estimation%20from%202D%20images%20and%20benchmark%20them%20against%20two%20conventional%20baselines%3A%20a%202D%20area-based%20projection%20and%20a%20geometric%20reconstruction%20using%20axis-aligned%20cross-sections.%20Fine-tuned%20Vision%20Transformers%20%28DINOv2%20and%20DINOv3%29%20with%20MLPs%20achieve%20the%20lowest%20MAPE%20of%205.08%5C%25%20and%204.67%5C%25%20and%20the%20highest%20correlation%20of%200.96%20and%200.97%20on%20six-view%20indoor%20images%2C%20outperforming%20fine-tuned%20CNNs%20%28ResNet18%20and%20ResNet50%29%2C%20wheat-specific%20backbones%2C%20and%20both%20baselines.%20When%20using%20frozen%20DINO%20backbones%2C%20deep-supervised%20LSTMs%20outperform%20MLPs%2C%20whereas%20after%20fine-tuning%2C%20improved%20high-level%20representations%20allow%20simple%20MLPs%20to%20outperform%20LSTMs.%20We%20demonstrate%20that%20object%20shape%20significantly%20impacts%20volume%20estimation%20accuracy%2C%20with%20irregular%20geometries%20such%20as%20wheat%20spikes%20posing%20greater%20challenges%20for%20geometric%20methods%20than%20for%20deep%20learning%20approaches.%20Fine-tuning%20DINOv3%20on%20field-based%20single%20side-view%20images%20yields%20a%20MAPE%20of%208.39%5C%25%20and%20a%20correlation%20of%200.90%2C%20providing%20a%20novel%20pipeline%20and%20a%20fast%2C%20accurate%2C%20and%20non-destructive%20approach%20for%20wheat%20spike%20volume%20phenotyping.%0ALink%3A%20http%3A//arxiv.org/abs/2506.18060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuned%2520Vision%2520Transformers%2520Capture%2520Complex%2520Wheat%2520Spike%2520Morphology%2520for%2520Volume%2520Estimation%2520from%2520RGB%2520Images%26entry.906535625%3DOlivia%2520Zumsteg%2520and%2520Nico%2520Graf%2520and%2520Aaron%2520Haeusler%2520and%2520Norbert%2520Kirchgessner%2520and%2520Nicola%2520Storni%2520and%2520Lukas%2520Roth%2520and%2520Andreas%2520Hund%26entry.1292438233%3DEstimating%2520three-dimensional%2520morphological%2520traits%2520such%2520as%2520volume%2520from%2520two-dimensional%2520RGB%2520images%2520presents%2520inherent%2520challenges%2520due%2520to%2520the%2520loss%2520of%2520depth%2520information%252C%2520projection%2520distortions%252C%2520and%2520occlusions%2520under%2520field%2520conditions.%2520In%2520this%2520work%252C%2520we%2520explore%2520multiple%2520approaches%2520for%2520non-destructive%2520volume%2520estimation%2520of%2520wheat%2520spikes%2520using%2520RGB%2520images%2520and%2520structured-light%25203D%2520scans%2520as%2520ground%2520truth%2520references.%2520Wheat%2520spike%2520volume%2520is%2520promising%2520for%2520phenotyping%2520as%2520it%2520shows%2520high%2520correlation%2520with%2520spike%2520dry%2520weight%252C%2520a%2520key%2520component%2520of%2520fruiting%2520efficiency.%2520Accounting%2520for%2520the%2520complex%2520geometry%2520of%2520the%2520spikes%252C%2520we%2520compare%2520different%2520neural%2520network%2520approaches%2520for%2520volume%2520estimation%2520from%25202D%2520images%2520and%2520benchmark%2520them%2520against%2520two%2520conventional%2520baselines%253A%2520a%25202D%2520area-based%2520projection%2520and%2520a%2520geometric%2520reconstruction%2520using%2520axis-aligned%2520cross-sections.%2520Fine-tuned%2520Vision%2520Transformers%2520%2528DINOv2%2520and%2520DINOv3%2529%2520with%2520MLPs%2520achieve%2520the%2520lowest%2520MAPE%2520of%25205.08%255C%2525%2520and%25204.67%255C%2525%2520and%2520the%2520highest%2520correlation%2520of%25200.96%2520and%25200.97%2520on%2520six-view%2520indoor%2520images%252C%2520outperforming%2520fine-tuned%2520CNNs%2520%2528ResNet18%2520and%2520ResNet50%2529%252C%2520wheat-specific%2520backbones%252C%2520and%2520both%2520baselines.%2520When%2520using%2520frozen%2520DINO%2520backbones%252C%2520deep-supervised%2520LSTMs%2520outperform%2520MLPs%252C%2520whereas%2520after%2520fine-tuning%252C%2520improved%2520high-level%2520representations%2520allow%2520simple%2520MLPs%2520to%2520outperform%2520LSTMs.%2520We%2520demonstrate%2520that%2520object%2520shape%2520significantly%2520impacts%2520volume%2520estimation%2520accuracy%252C%2520with%2520irregular%2520geometries%2520such%2520as%2520wheat%2520spikes%2520posing%2520greater%2520challenges%2520for%2520geometric%2520methods%2520than%2520for%2520deep%2520learning%2520approaches.%2520Fine-tuning%2520DINOv3%2520on%2520field-based%2520single%2520side-view%2520images%2520yields%2520a%2520MAPE%2520of%25208.39%255C%2525%2520and%2520a%2520correlation%2520of%25200.90%252C%2520providing%2520a%2520novel%2520pipeline%2520and%2520a%2520fast%252C%2520accurate%252C%2520and%2520non-destructive%2520approach%2520for%2520wheat%2520spike%2520volume%2520phenotyping.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuned%20Vision%20Transformers%20Capture%20Complex%20Wheat%20Spike%20Morphology%20for%20Volume%20Estimation%20from%20RGB%20Images&entry.906535625=Olivia%20Zumsteg%20and%20Nico%20Graf%20and%20Aaron%20Haeusler%20and%20Norbert%20Kirchgessner%20and%20Nicola%20Storni%20and%20Lukas%20Roth%20and%20Andreas%20Hund&entry.1292438233=Estimating%20three-dimensional%20morphological%20traits%20such%20as%20volume%20from%20two-dimensional%20RGB%20images%20presents%20inherent%20challenges%20due%20to%20the%20loss%20of%20depth%20information%2C%20projection%20distortions%2C%20and%20occlusions%20under%20field%20conditions.%20In%20this%20work%2C%20we%20explore%20multiple%20approaches%20for%20non-destructive%20volume%20estimation%20of%20wheat%20spikes%20using%20RGB%20images%20and%20structured-light%203D%20scans%20as%20ground%20truth%20references.%20Wheat%20spike%20volume%20is%20promising%20for%20phenotyping%20as%20it%20shows%20high%20correlation%20with%20spike%20dry%20weight%2C%20a%20key%20component%20of%20fruiting%20efficiency.%20Accounting%20for%20the%20complex%20geometry%20of%20the%20spikes%2C%20we%20compare%20different%20neural%20network%20approaches%20for%20volume%20estimation%20from%202D%20images%20and%20benchmark%20them%20against%20two%20conventional%20baselines%3A%20a%202D%20area-based%20projection%20and%20a%20geometric%20reconstruction%20using%20axis-aligned%20cross-sections.%20Fine-tuned%20Vision%20Transformers%20%28DINOv2%20and%20DINOv3%29%20with%20MLPs%20achieve%20the%20lowest%20MAPE%20of%205.08%5C%25%20and%204.67%5C%25%20and%20the%20highest%20correlation%20of%200.96%20and%200.97%20on%20six-view%20indoor%20images%2C%20outperforming%20fine-tuned%20CNNs%20%28ResNet18%20and%20ResNet50%29%2C%20wheat-specific%20backbones%2C%20and%20both%20baselines.%20When%20using%20frozen%20DINO%20backbones%2C%20deep-supervised%20LSTMs%20outperform%20MLPs%2C%20whereas%20after%20fine-tuning%2C%20improved%20high-level%20representations%20allow%20simple%20MLPs%20to%20outperform%20LSTMs.%20We%20demonstrate%20that%20object%20shape%20significantly%20impacts%20volume%20estimation%20accuracy%2C%20with%20irregular%20geometries%20such%20as%20wheat%20spikes%20posing%20greater%20challenges%20for%20geometric%20methods%20than%20for%20deep%20learning%20approaches.%20Fine-tuning%20DINOv3%20on%20field-based%20single%20side-view%20images%20yields%20a%20MAPE%20of%208.39%5C%25%20and%20a%20correlation%20of%200.90%2C%20providing%20a%20novel%20pipeline%20and%20a%20fast%2C%20accurate%2C%20and%20non-destructive%20approach%20for%20wheat%20spike%20volume%20phenotyping.&entry.1838667208=http%3A//arxiv.org/abs/2506.18060v2&entry.124074799=Read"},
{"title": "Multi-label Classification with Panoptic Context Aggregation Networks", "author": "Mingyuan Jiu and Hailong Zhu and Wenchuan Wei and Hichem Sahbi and Rongrong Ji and Mingliang Xu", "abstract": "Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.", "link": "http://arxiv.org/abs/2512.23486v1", "date": "2025-12-29", "relevancy": 2.6809, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-label%20Classification%20with%20Panoptic%20Context%20Aggregation%20Networks&body=Title%3A%20Multi-label%20Classification%20with%20Panoptic%20Context%20Aggregation%20Networks%0AAuthor%3A%20Mingyuan%20Jiu%20and%20Hailong%20Zhu%20and%20Wenchuan%20Wei%20and%20Hichem%20Sahbi%20and%20Rongrong%20Ji%20and%20Mingliang%20Xu%0AAbstract%3A%20Context%20modeling%20is%20crucial%20for%20visual%20recognition%2C%20enabling%20highly%20discriminative%20image%20representations%20by%20integrating%20both%20intrinsic%20and%20extrinsic%20relationships%20between%20objects%20and%20labels%20in%20images.%20A%20limitation%20in%20current%20approaches%20is%20their%20focus%20on%20basic%20geometric%20relationships%20or%20localized%20features%2C%20often%20neglecting%20cross-scale%20contextual%20interactions%20between%20objects.%20This%20paper%20introduces%20the%20Deep%20Panoptic%20Context%20Aggregation%20Network%20%28PanCAN%29%2C%20a%20novel%20approach%20that%20hierarchically%20integrates%20multi-order%20geometric%20contexts%20through%20cross-scale%20feature%20aggregation%20in%20a%20high-dimensional%20Hilbert%20space.%20Specifically%2C%20PanCAN%20learns%20multi-order%20neighborhood%20relationships%20at%20each%20scale%20by%20combining%20random%20walks%20with%20an%20attention%20mechanism.%20Modules%20from%20different%20scales%20are%20cascaded%2C%20where%20salient%20anchors%20at%20a%20finer%20scale%20are%20selected%20and%20their%20neighborhood%20features%20are%20dynamically%20fused%20via%20attention.%20This%20enables%20effective%20cross-scale%20modeling%20that%20significantly%20enhances%20complex%20scene%20understanding%20by%20combining%20multi-order%20and%20cross-scale%20context-aware%20features.%20Extensive%20multi-label%20classification%20experiments%20on%20NUS-WIDE%2C%20PASCAL%20VOC2007%2C%20and%20MS-COCO%20benchmarks%20demonstrate%20that%20PanCAN%20consistently%20achieves%20competitive%20results%2C%20outperforming%20state-of-the-art%20techniques%20in%20both%20quantitative%20and%20qualitative%20evaluations%2C%20thereby%20substantially%20improving%20multi-label%20classification%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-label%2520Classification%2520with%2520Panoptic%2520Context%2520Aggregation%2520Networks%26entry.906535625%3DMingyuan%2520Jiu%2520and%2520Hailong%2520Zhu%2520and%2520Wenchuan%2520Wei%2520and%2520Hichem%2520Sahbi%2520and%2520Rongrong%2520Ji%2520and%2520Mingliang%2520Xu%26entry.1292438233%3DContext%2520modeling%2520is%2520crucial%2520for%2520visual%2520recognition%252C%2520enabling%2520highly%2520discriminative%2520image%2520representations%2520by%2520integrating%2520both%2520intrinsic%2520and%2520extrinsic%2520relationships%2520between%2520objects%2520and%2520labels%2520in%2520images.%2520A%2520limitation%2520in%2520current%2520approaches%2520is%2520their%2520focus%2520on%2520basic%2520geometric%2520relationships%2520or%2520localized%2520features%252C%2520often%2520neglecting%2520cross-scale%2520contextual%2520interactions%2520between%2520objects.%2520This%2520paper%2520introduces%2520the%2520Deep%2520Panoptic%2520Context%2520Aggregation%2520Network%2520%2528PanCAN%2529%252C%2520a%2520novel%2520approach%2520that%2520hierarchically%2520integrates%2520multi-order%2520geometric%2520contexts%2520through%2520cross-scale%2520feature%2520aggregation%2520in%2520a%2520high-dimensional%2520Hilbert%2520space.%2520Specifically%252C%2520PanCAN%2520learns%2520multi-order%2520neighborhood%2520relationships%2520at%2520each%2520scale%2520by%2520combining%2520random%2520walks%2520with%2520an%2520attention%2520mechanism.%2520Modules%2520from%2520different%2520scales%2520are%2520cascaded%252C%2520where%2520salient%2520anchors%2520at%2520a%2520finer%2520scale%2520are%2520selected%2520and%2520their%2520neighborhood%2520features%2520are%2520dynamically%2520fused%2520via%2520attention.%2520This%2520enables%2520effective%2520cross-scale%2520modeling%2520that%2520significantly%2520enhances%2520complex%2520scene%2520understanding%2520by%2520combining%2520multi-order%2520and%2520cross-scale%2520context-aware%2520features.%2520Extensive%2520multi-label%2520classification%2520experiments%2520on%2520NUS-WIDE%252C%2520PASCAL%2520VOC2007%252C%2520and%2520MS-COCO%2520benchmarks%2520demonstrate%2520that%2520PanCAN%2520consistently%2520achieves%2520competitive%2520results%252C%2520outperforming%2520state-of-the-art%2520techniques%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations%252C%2520thereby%2520substantially%2520improving%2520multi-label%2520classification%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-label%20Classification%20with%20Panoptic%20Context%20Aggregation%20Networks&entry.906535625=Mingyuan%20Jiu%20and%20Hailong%20Zhu%20and%20Wenchuan%20Wei%20and%20Hichem%20Sahbi%20and%20Rongrong%20Ji%20and%20Mingliang%20Xu&entry.1292438233=Context%20modeling%20is%20crucial%20for%20visual%20recognition%2C%20enabling%20highly%20discriminative%20image%20representations%20by%20integrating%20both%20intrinsic%20and%20extrinsic%20relationships%20between%20objects%20and%20labels%20in%20images.%20A%20limitation%20in%20current%20approaches%20is%20their%20focus%20on%20basic%20geometric%20relationships%20or%20localized%20features%2C%20often%20neglecting%20cross-scale%20contextual%20interactions%20between%20objects.%20This%20paper%20introduces%20the%20Deep%20Panoptic%20Context%20Aggregation%20Network%20%28PanCAN%29%2C%20a%20novel%20approach%20that%20hierarchically%20integrates%20multi-order%20geometric%20contexts%20through%20cross-scale%20feature%20aggregation%20in%20a%20high-dimensional%20Hilbert%20space.%20Specifically%2C%20PanCAN%20learns%20multi-order%20neighborhood%20relationships%20at%20each%20scale%20by%20combining%20random%20walks%20with%20an%20attention%20mechanism.%20Modules%20from%20different%20scales%20are%20cascaded%2C%20where%20salient%20anchors%20at%20a%20finer%20scale%20are%20selected%20and%20their%20neighborhood%20features%20are%20dynamically%20fused%20via%20attention.%20This%20enables%20effective%20cross-scale%20modeling%20that%20significantly%20enhances%20complex%20scene%20understanding%20by%20combining%20multi-order%20and%20cross-scale%20context-aware%20features.%20Extensive%20multi-label%20classification%20experiments%20on%20NUS-WIDE%2C%20PASCAL%20VOC2007%2C%20and%20MS-COCO%20benchmarks%20demonstrate%20that%20PanCAN%20consistently%20achieves%20competitive%20results%2C%20outperforming%20state-of-the-art%20techniques%20in%20both%20quantitative%20and%20qualitative%20evaluations%2C%20thereby%20substantially%20improving%20multi-label%20classification%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.23486v1&entry.124074799=Read"},
{"title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation", "author": "Zeteng Lin and Xingxing Li and Wen You and Xiaoyang Li and Zehan Lu and Yujun Cai and Jing Tang", "abstract": "Existing vision language models (VLMs), including GPT-4 and DALL.E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.", "link": "http://arxiv.org/abs/2510.10969v2", "date": "2025-12-29", "relevancy": 2.6464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IUT-Plug%3A%20A%20Plug-in%20tool%20for%20Interleaved%20Image-Text%20Generation&body=Title%3A%20IUT-Plug%3A%20A%20Plug-in%20tool%20for%20Interleaved%20Image-Text%20Generation%0AAuthor%3A%20Zeteng%20Lin%20and%20Xingxing%20Li%20and%20Wen%20You%20and%20Xiaoyang%20Li%20and%20Zehan%20Lu%20and%20Yujun%20Cai%20and%20Jing%20Tang%0AAbstract%3A%20Existing%20vision%20language%20models%20%28VLMs%29%2C%20including%20GPT-4%20and%20DALL.E%2C%20often%20struggle%20to%20preserve%20logic%2C%20object%20identity%2C%20and%20style%20in%20multimodal%20image-text%20generation.%20This%20limitation%20significantly%20hinders%20the%20generalization%20capability%20of%20VLMs%20in%20complex%20image-text%20input-output%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20IUT-Plug%2C%20a%20module%20grounded%20in%20an%20Image%20Understanding%20Tree%20%28IUT%29%2C%20which%20enhances%20existing%20interleaved%20VLMs%20through%20explicit%20structured%20reasoning%2C%20thereby%20mitigating%20context%20drift%20in%20logic%2C%20entity%20identity%2C%20and%20style.%20The%20proposed%20framework%20operates%20in%20two%20stages.%20%281%29%20A%20dynamic%20IUT-Plug%20extraction%20module%20parses%20visual%20scenes%20into%20hierarchical%20symbolic%20structures.%20%282%29%20A%20coordinated%20narrative-flow%20and%20image%20synthesis%20mechanism%20ensures%20cross-modal%20consistency.%20To%20evaluate%20our%20approach%2C%20we%20construct%20a%20novel%20benchmark%20based%20on%203%2C000%20real%20human-generated%20question-answer%20pairs%20over%20fine-tuned%20large%20models%2C%20introducing%20a%20dynamic%20evaluation%20protocol%20for%20quantifying%20context%20drift%20in%20interleaved%20VLMs.%20Experimental%20results%20demonstrate%20that%20IUT-Plug%20not%20only%20improves%20accuracy%20on%20established%20benchmarks%20but%20also%20effectively%20alleviates%20the%20three%20critical%20forms%20of%20context%20drift%20across%20diverse%20multimodal%20question%20answering%20%28QA%29%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIUT-Plug%253A%2520A%2520Plug-in%2520tool%2520for%2520Interleaved%2520Image-Text%2520Generation%26entry.906535625%3DZeteng%2520Lin%2520and%2520Xingxing%2520Li%2520and%2520Wen%2520You%2520and%2520Xiaoyang%2520Li%2520and%2520Zehan%2520Lu%2520and%2520Yujun%2520Cai%2520and%2520Jing%2520Tang%26entry.1292438233%3DExisting%2520vision%2520language%2520models%2520%2528VLMs%2529%252C%2520including%2520GPT-4%2520and%2520DALL.E%252C%2520often%2520struggle%2520to%2520preserve%2520logic%252C%2520object%2520identity%252C%2520and%2520style%2520in%2520multimodal%2520image-text%2520generation.%2520This%2520limitation%2520significantly%2520hinders%2520the%2520generalization%2520capability%2520of%2520VLMs%2520in%2520complex%2520image-text%2520input-output%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520IUT-Plug%252C%2520a%2520module%2520grounded%2520in%2520an%2520Image%2520Understanding%2520Tree%2520%2528IUT%2529%252C%2520which%2520enhances%2520existing%2520interleaved%2520VLMs%2520through%2520explicit%2520structured%2520reasoning%252C%2520thereby%2520mitigating%2520context%2520drift%2520in%2520logic%252C%2520entity%2520identity%252C%2520and%2520style.%2520The%2520proposed%2520framework%2520operates%2520in%2520two%2520stages.%2520%25281%2529%2520A%2520dynamic%2520IUT-Plug%2520extraction%2520module%2520parses%2520visual%2520scenes%2520into%2520hierarchical%2520symbolic%2520structures.%2520%25282%2529%2520A%2520coordinated%2520narrative-flow%2520and%2520image%2520synthesis%2520mechanism%2520ensures%2520cross-modal%2520consistency.%2520To%2520evaluate%2520our%2520approach%252C%2520we%2520construct%2520a%2520novel%2520benchmark%2520based%2520on%25203%252C000%2520real%2520human-generated%2520question-answer%2520pairs%2520over%2520fine-tuned%2520large%2520models%252C%2520introducing%2520a%2520dynamic%2520evaluation%2520protocol%2520for%2520quantifying%2520context%2520drift%2520in%2520interleaved%2520VLMs.%2520Experimental%2520results%2520demonstrate%2520that%2520IUT-Plug%2520not%2520only%2520improves%2520accuracy%2520on%2520established%2520benchmarks%2520but%2520also%2520effectively%2520alleviates%2520the%2520three%2520critical%2520forms%2520of%2520context%2520drift%2520across%2520diverse%2520multimodal%2520question%2520answering%2520%2528QA%2529%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IUT-Plug%3A%20A%20Plug-in%20tool%20for%20Interleaved%20Image-Text%20Generation&entry.906535625=Zeteng%20Lin%20and%20Xingxing%20Li%20and%20Wen%20You%20and%20Xiaoyang%20Li%20and%20Zehan%20Lu%20and%20Yujun%20Cai%20and%20Jing%20Tang&entry.1292438233=Existing%20vision%20language%20models%20%28VLMs%29%2C%20including%20GPT-4%20and%20DALL.E%2C%20often%20struggle%20to%20preserve%20logic%2C%20object%20identity%2C%20and%20style%20in%20multimodal%20image-text%20generation.%20This%20limitation%20significantly%20hinders%20the%20generalization%20capability%20of%20VLMs%20in%20complex%20image-text%20input-output%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20IUT-Plug%2C%20a%20module%20grounded%20in%20an%20Image%20Understanding%20Tree%20%28IUT%29%2C%20which%20enhances%20existing%20interleaved%20VLMs%20through%20explicit%20structured%20reasoning%2C%20thereby%20mitigating%20context%20drift%20in%20logic%2C%20entity%20identity%2C%20and%20style.%20The%20proposed%20framework%20operates%20in%20two%20stages.%20%281%29%20A%20dynamic%20IUT-Plug%20extraction%20module%20parses%20visual%20scenes%20into%20hierarchical%20symbolic%20structures.%20%282%29%20A%20coordinated%20narrative-flow%20and%20image%20synthesis%20mechanism%20ensures%20cross-modal%20consistency.%20To%20evaluate%20our%20approach%2C%20we%20construct%20a%20novel%20benchmark%20based%20on%203%2C000%20real%20human-generated%20question-answer%20pairs%20over%20fine-tuned%20large%20models%2C%20introducing%20a%20dynamic%20evaluation%20protocol%20for%20quantifying%20context%20drift%20in%20interleaved%20VLMs.%20Experimental%20results%20demonstrate%20that%20IUT-Plug%20not%20only%20improves%20accuracy%20on%20established%20benchmarks%20but%20also%20effectively%20alleviates%20the%20three%20critical%20forms%20of%20context%20drift%20across%20diverse%20multimodal%20question%20answering%20%28QA%29%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2510.10969v2&entry.124074799=Read"},
{"title": "Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging", "author": "Janani Annur Thiruvengadam and Kiran Mayee Nabigaru and Anusha Kovi", "abstract": "The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.", "link": "http://arxiv.org/abs/2512.23597v1", "date": "2025-12-29", "relevancy": 2.6218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5167}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Residual%20Feature%20Aggregation%20Framework%20with%20Hybrid%20Metaheuristic%20Optimization%20for%20Robust%20Early%20Pancreatic%20Neoplasm%20Detection%20in%20Multimodal%20CT%20Imaging&body=Title%3A%20Scalable%20Residual%20Feature%20Aggregation%20Framework%20with%20Hybrid%20Metaheuristic%20Optimization%20for%20Robust%20Early%20Pancreatic%20Neoplasm%20Detection%20in%20Multimodal%20CT%20Imaging%0AAuthor%3A%20Janani%20Annur%20Thiruvengadam%20and%20Kiran%20Mayee%20Nabigaru%20and%20Anusha%20Kovi%0AAbstract%3A%20The%20early%20detection%20of%20pancreatic%20neoplasm%20is%20a%20major%20clinical%20dilemma%2C%20and%20it%20is%20predominantly%20so%20because%20tumors%20are%20likely%20to%20occur%20with%20minimal%20contrast%20margins%20and%20a%20large%20spread%20anatomy-wide%20variation%20amongst%20patients%20on%20a%20CT%20scan.%20These%20complexities%20require%20to%20be%20addressed%20with%20an%20effective%20and%20scalable%20system%20that%20can%20assist%20in%20enhancing%20the%20salience%20of%20the%20subtle%20visual%20cues%20and%20provide%20a%20high%20level%20of%20the%20generalization%20on%20the%20multimodal%20imaging%20data.%20A%20Scalable%20Residual%20Feature%20Aggregation%20%28SRFA%29%20framework%20is%20proposed%20to%20be%20used%20to%20meet%20these%20conditions%20in%20this%20study.%20The%20framework%20integrates%20a%20pipeline%20of%20preprocessing%20followed%20by%20the%20segmentation%20using%20the%20MAGRes-UNet%20that%20is%20effective%20in%20making%20the%20pancreatic%20structures%20and%20isolating%20regions%20of%20interest%20more%20visible.%20DenseNet-121%20performed%20with%20residual%20feature%20storage%20is%20used%20to%20extract%20features%20to%20allow%20deep%20hierarchical%20features%20to%20be%20aggregated%20without%20properties%20loss.%20To%20go%20further%2C%20hybrid%20HHO-BA%20metaheuristic%20feature%20selection%20strategy%20is%20used%2C%20which%20guarantees%20the%20best%20feature%20subset%20refinement.%20To%20be%20classified%2C%20the%20system%20is%20trained%20based%20on%20a%20new%20hybrid%20model%20that%20integrates%20the%20ability%20to%20pay%20attention%20on%20the%20world%2C%20which%20is%20the%20Vision%20Transformer%20%28ViT%29%20with%20the%20high%20representational%20efficiency%20of%20EfficientNet-B3.%20A%20dual%20optimization%20mechanism%20incorporating%20SSA%20and%20GWO%20is%20used%20to%20fine-tune%20hyperparameters%20to%20enhance%20greater%20robustness%20and%20less%20overfitting.%20Experimental%20results%20support%20the%20significant%20improvement%20in%20performance%2C%20with%20the%20suggested%20model%20reaching%2096.23%25%20accuracy%2C%2095.58%25%20F1-score%20and%2094.83%25%20specificity%2C%20the%20model%20is%20significantly%20better%20than%20the%20traditional%20CNNs%20and%20contemporary%20transformer-based%20models.%20Such%20results%20highlight%20the%20possibility%20of%20the%20SRFA%20framework%20as%20a%20useful%20instrument%20in%20the%20early%20detection%20of%20pancreatic%20tumors.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Residual%2520Feature%2520Aggregation%2520Framework%2520with%2520Hybrid%2520Metaheuristic%2520Optimization%2520for%2520Robust%2520Early%2520Pancreatic%2520Neoplasm%2520Detection%2520in%2520Multimodal%2520CT%2520Imaging%26entry.906535625%3DJanani%2520Annur%2520Thiruvengadam%2520and%2520Kiran%2520Mayee%2520Nabigaru%2520and%2520Anusha%2520Kovi%26entry.1292438233%3DThe%2520early%2520detection%2520of%2520pancreatic%2520neoplasm%2520is%2520a%2520major%2520clinical%2520dilemma%252C%2520and%2520it%2520is%2520predominantly%2520so%2520because%2520tumors%2520are%2520likely%2520to%2520occur%2520with%2520minimal%2520contrast%2520margins%2520and%2520a%2520large%2520spread%2520anatomy-wide%2520variation%2520amongst%2520patients%2520on%2520a%2520CT%2520scan.%2520These%2520complexities%2520require%2520to%2520be%2520addressed%2520with%2520an%2520effective%2520and%2520scalable%2520system%2520that%2520can%2520assist%2520in%2520enhancing%2520the%2520salience%2520of%2520the%2520subtle%2520visual%2520cues%2520and%2520provide%2520a%2520high%2520level%2520of%2520the%2520generalization%2520on%2520the%2520multimodal%2520imaging%2520data.%2520A%2520Scalable%2520Residual%2520Feature%2520Aggregation%2520%2528SRFA%2529%2520framework%2520is%2520proposed%2520to%2520be%2520used%2520to%2520meet%2520these%2520conditions%2520in%2520this%2520study.%2520The%2520framework%2520integrates%2520a%2520pipeline%2520of%2520preprocessing%2520followed%2520by%2520the%2520segmentation%2520using%2520the%2520MAGRes-UNet%2520that%2520is%2520effective%2520in%2520making%2520the%2520pancreatic%2520structures%2520and%2520isolating%2520regions%2520of%2520interest%2520more%2520visible.%2520DenseNet-121%2520performed%2520with%2520residual%2520feature%2520storage%2520is%2520used%2520to%2520extract%2520features%2520to%2520allow%2520deep%2520hierarchical%2520features%2520to%2520be%2520aggregated%2520without%2520properties%2520loss.%2520To%2520go%2520further%252C%2520hybrid%2520HHO-BA%2520metaheuristic%2520feature%2520selection%2520strategy%2520is%2520used%252C%2520which%2520guarantees%2520the%2520best%2520feature%2520subset%2520refinement.%2520To%2520be%2520classified%252C%2520the%2520system%2520is%2520trained%2520based%2520on%2520a%2520new%2520hybrid%2520model%2520that%2520integrates%2520the%2520ability%2520to%2520pay%2520attention%2520on%2520the%2520world%252C%2520which%2520is%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520with%2520the%2520high%2520representational%2520efficiency%2520of%2520EfficientNet-B3.%2520A%2520dual%2520optimization%2520mechanism%2520incorporating%2520SSA%2520and%2520GWO%2520is%2520used%2520to%2520fine-tune%2520hyperparameters%2520to%2520enhance%2520greater%2520robustness%2520and%2520less%2520overfitting.%2520Experimental%2520results%2520support%2520the%2520significant%2520improvement%2520in%2520performance%252C%2520with%2520the%2520suggested%2520model%2520reaching%252096.23%2525%2520accuracy%252C%252095.58%2525%2520F1-score%2520and%252094.83%2525%2520specificity%252C%2520the%2520model%2520is%2520significantly%2520better%2520than%2520the%2520traditional%2520CNNs%2520and%2520contemporary%2520transformer-based%2520models.%2520Such%2520results%2520highlight%2520the%2520possibility%2520of%2520the%2520SRFA%2520framework%2520as%2520a%2520useful%2520instrument%2520in%2520the%2520early%2520detection%2520of%2520pancreatic%2520tumors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Residual%20Feature%20Aggregation%20Framework%20with%20Hybrid%20Metaheuristic%20Optimization%20for%20Robust%20Early%20Pancreatic%20Neoplasm%20Detection%20in%20Multimodal%20CT%20Imaging&entry.906535625=Janani%20Annur%20Thiruvengadam%20and%20Kiran%20Mayee%20Nabigaru%20and%20Anusha%20Kovi&entry.1292438233=The%20early%20detection%20of%20pancreatic%20neoplasm%20is%20a%20major%20clinical%20dilemma%2C%20and%20it%20is%20predominantly%20so%20because%20tumors%20are%20likely%20to%20occur%20with%20minimal%20contrast%20margins%20and%20a%20large%20spread%20anatomy-wide%20variation%20amongst%20patients%20on%20a%20CT%20scan.%20These%20complexities%20require%20to%20be%20addressed%20with%20an%20effective%20and%20scalable%20system%20that%20can%20assist%20in%20enhancing%20the%20salience%20of%20the%20subtle%20visual%20cues%20and%20provide%20a%20high%20level%20of%20the%20generalization%20on%20the%20multimodal%20imaging%20data.%20A%20Scalable%20Residual%20Feature%20Aggregation%20%28SRFA%29%20framework%20is%20proposed%20to%20be%20used%20to%20meet%20these%20conditions%20in%20this%20study.%20The%20framework%20integrates%20a%20pipeline%20of%20preprocessing%20followed%20by%20the%20segmentation%20using%20the%20MAGRes-UNet%20that%20is%20effective%20in%20making%20the%20pancreatic%20structures%20and%20isolating%20regions%20of%20interest%20more%20visible.%20DenseNet-121%20performed%20with%20residual%20feature%20storage%20is%20used%20to%20extract%20features%20to%20allow%20deep%20hierarchical%20features%20to%20be%20aggregated%20without%20properties%20loss.%20To%20go%20further%2C%20hybrid%20HHO-BA%20metaheuristic%20feature%20selection%20strategy%20is%20used%2C%20which%20guarantees%20the%20best%20feature%20subset%20refinement.%20To%20be%20classified%2C%20the%20system%20is%20trained%20based%20on%20a%20new%20hybrid%20model%20that%20integrates%20the%20ability%20to%20pay%20attention%20on%20the%20world%2C%20which%20is%20the%20Vision%20Transformer%20%28ViT%29%20with%20the%20high%20representational%20efficiency%20of%20EfficientNet-B3.%20A%20dual%20optimization%20mechanism%20incorporating%20SSA%20and%20GWO%20is%20used%20to%20fine-tune%20hyperparameters%20to%20enhance%20greater%20robustness%20and%20less%20overfitting.%20Experimental%20results%20support%20the%20significant%20improvement%20in%20performance%2C%20with%20the%20suggested%20model%20reaching%2096.23%25%20accuracy%2C%2095.58%25%20F1-score%20and%2094.83%25%20specificity%2C%20the%20model%20is%20significantly%20better%20than%20the%20traditional%20CNNs%20and%20contemporary%20transformer-based%20models.%20Such%20results%20highlight%20the%20possibility%20of%20the%20SRFA%20framework%20as%20a%20useful%20instrument%20in%20the%20early%20detection%20of%20pancreatic%20tumors.&entry.1838667208=http%3A//arxiv.org/abs/2512.23597v1&entry.124074799=Read"},
{"title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview", "author": "Jiatao Li and Yanheng Li and Xiaojun Wan", "abstract": "Large Language Models significantly influence social interactions, decision-making, and information dissemination, underscoring the need to understand the implicit socio-cognitive attitudes, referred to as \"worldviews\", encoded within these systems. Unlike previous studies predominantly addressing demographic and ethical biases as fixed attributes, our study explores deeper cognitive orientations toward authority, equality, autonomy, and fate, emphasizing their adaptability in dynamic social contexts. We introduce the Social Worldview Taxonomy (SWT), an evaluation framework grounded in Cultural Theory, operationalizing four canonical worldviews, namely Hierarchy, Egalitarianism, Individualism, and Fatalism, into quantifiable sub-dimensions. Through extensive analysis of 28 diverse LLMs, we identify distinct cognitive profiles reflecting intrinsic model-specific socio-cognitive structures. Leveraging principles from Social Referencing Theory, our experiments demonstrate that explicit social cues systematically modulate these profiles, revealing robust patterns of cognitive adaptability. Our findings provide insights into the latent cognitive flexibility of LLMs and offer computational scientists practical pathways toward developing more transparent, interpretable, and socially responsible AI systems", "link": "http://arxiv.org/abs/2505.01967v2", "date": "2025-12-29", "relevancy": 2.5542, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Cognitive%20Differences%20Among%20Large%20Language%20Models%20through%20the%20Lens%20of%20Social%20Worldview&body=Title%3A%20Analyzing%20Cognitive%20Differences%20Among%20Large%20Language%20Models%20through%20the%20Lens%20of%20Social%20Worldview%0AAuthor%3A%20Jiatao%20Li%20and%20Yanheng%20Li%20and%20Xiaojun%20Wan%0AAbstract%3A%20Large%20Language%20Models%20significantly%20influence%20social%20interactions%2C%20decision-making%2C%20and%20information%20dissemination%2C%20underscoring%20the%20need%20to%20understand%20the%20implicit%20socio-cognitive%20attitudes%2C%20referred%20to%20as%20%22worldviews%22%2C%20encoded%20within%20these%20systems.%20Unlike%20previous%20studies%20predominantly%20addressing%20demographic%20and%20ethical%20biases%20as%20fixed%20attributes%2C%20our%20study%20explores%20deeper%20cognitive%20orientations%20toward%20authority%2C%20equality%2C%20autonomy%2C%20and%20fate%2C%20emphasizing%20their%20adaptability%20in%20dynamic%20social%20contexts.%20We%20introduce%20the%20Social%20Worldview%20Taxonomy%20%28SWT%29%2C%20an%20evaluation%20framework%20grounded%20in%20Cultural%20Theory%2C%20operationalizing%20four%20canonical%20worldviews%2C%20namely%20Hierarchy%2C%20Egalitarianism%2C%20Individualism%2C%20and%20Fatalism%2C%20into%20quantifiable%20sub-dimensions.%20Through%20extensive%20analysis%20of%2028%20diverse%20LLMs%2C%20we%20identify%20distinct%20cognitive%20profiles%20reflecting%20intrinsic%20model-specific%20socio-cognitive%20structures.%20Leveraging%20principles%20from%20Social%20Referencing%20Theory%2C%20our%20experiments%20demonstrate%20that%20explicit%20social%20cues%20systematically%20modulate%20these%20profiles%2C%20revealing%20robust%20patterns%20of%20cognitive%20adaptability.%20Our%20findings%20provide%20insights%20into%20the%20latent%20cognitive%20flexibility%20of%20LLMs%20and%20offer%20computational%20scientists%20practical%20pathways%20toward%20developing%20more%20transparent%2C%20interpretable%2C%20and%20socially%20responsible%20AI%20systems%0ALink%3A%20http%3A//arxiv.org/abs/2505.01967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Cognitive%2520Differences%2520Among%2520Large%2520Language%2520Models%2520through%2520the%2520Lens%2520of%2520Social%2520Worldview%26entry.906535625%3DJiatao%2520Li%2520and%2520Yanheng%2520Li%2520and%2520Xiaojun%2520Wan%26entry.1292438233%3DLarge%2520Language%2520Models%2520significantly%2520influence%2520social%2520interactions%252C%2520decision-making%252C%2520and%2520information%2520dissemination%252C%2520underscoring%2520the%2520need%2520to%2520understand%2520the%2520implicit%2520socio-cognitive%2520attitudes%252C%2520referred%2520to%2520as%2520%2522worldviews%2522%252C%2520encoded%2520within%2520these%2520systems.%2520Unlike%2520previous%2520studies%2520predominantly%2520addressing%2520demographic%2520and%2520ethical%2520biases%2520as%2520fixed%2520attributes%252C%2520our%2520study%2520explores%2520deeper%2520cognitive%2520orientations%2520toward%2520authority%252C%2520equality%252C%2520autonomy%252C%2520and%2520fate%252C%2520emphasizing%2520their%2520adaptability%2520in%2520dynamic%2520social%2520contexts.%2520We%2520introduce%2520the%2520Social%2520Worldview%2520Taxonomy%2520%2528SWT%2529%252C%2520an%2520evaluation%2520framework%2520grounded%2520in%2520Cultural%2520Theory%252C%2520operationalizing%2520four%2520canonical%2520worldviews%252C%2520namely%2520Hierarchy%252C%2520Egalitarianism%252C%2520Individualism%252C%2520and%2520Fatalism%252C%2520into%2520quantifiable%2520sub-dimensions.%2520Through%2520extensive%2520analysis%2520of%252028%2520diverse%2520LLMs%252C%2520we%2520identify%2520distinct%2520cognitive%2520profiles%2520reflecting%2520intrinsic%2520model-specific%2520socio-cognitive%2520structures.%2520Leveraging%2520principles%2520from%2520Social%2520Referencing%2520Theory%252C%2520our%2520experiments%2520demonstrate%2520that%2520explicit%2520social%2520cues%2520systematically%2520modulate%2520these%2520profiles%252C%2520revealing%2520robust%2520patterns%2520of%2520cognitive%2520adaptability.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%2520latent%2520cognitive%2520flexibility%2520of%2520LLMs%2520and%2520offer%2520computational%2520scientists%2520practical%2520pathways%2520toward%2520developing%2520more%2520transparent%252C%2520interpretable%252C%2520and%2520socially%2520responsible%2520AI%2520systems%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Cognitive%20Differences%20Among%20Large%20Language%20Models%20through%20the%20Lens%20of%20Social%20Worldview&entry.906535625=Jiatao%20Li%20and%20Yanheng%20Li%20and%20Xiaojun%20Wan&entry.1292438233=Large%20Language%20Models%20significantly%20influence%20social%20interactions%2C%20decision-making%2C%20and%20information%20dissemination%2C%20underscoring%20the%20need%20to%20understand%20the%20implicit%20socio-cognitive%20attitudes%2C%20referred%20to%20as%20%22worldviews%22%2C%20encoded%20within%20these%20systems.%20Unlike%20previous%20studies%20predominantly%20addressing%20demographic%20and%20ethical%20biases%20as%20fixed%20attributes%2C%20our%20study%20explores%20deeper%20cognitive%20orientations%20toward%20authority%2C%20equality%2C%20autonomy%2C%20and%20fate%2C%20emphasizing%20their%20adaptability%20in%20dynamic%20social%20contexts.%20We%20introduce%20the%20Social%20Worldview%20Taxonomy%20%28SWT%29%2C%20an%20evaluation%20framework%20grounded%20in%20Cultural%20Theory%2C%20operationalizing%20four%20canonical%20worldviews%2C%20namely%20Hierarchy%2C%20Egalitarianism%2C%20Individualism%2C%20and%20Fatalism%2C%20into%20quantifiable%20sub-dimensions.%20Through%20extensive%20analysis%20of%2028%20diverse%20LLMs%2C%20we%20identify%20distinct%20cognitive%20profiles%20reflecting%20intrinsic%20model-specific%20socio-cognitive%20structures.%20Leveraging%20principles%20from%20Social%20Referencing%20Theory%2C%20our%20experiments%20demonstrate%20that%20explicit%20social%20cues%20systematically%20modulate%20these%20profiles%2C%20revealing%20robust%20patterns%20of%20cognitive%20adaptability.%20Our%20findings%20provide%20insights%20into%20the%20latent%20cognitive%20flexibility%20of%20LLMs%20and%20offer%20computational%20scientists%20practical%20pathways%20toward%20developing%20more%20transparent%2C%20interpretable%2C%20and%20socially%20responsible%20AI%20systems&entry.1838667208=http%3A//arxiv.org/abs/2505.01967v2&entry.124074799=Read"},
{"title": "Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face", "author": "Rui-qing Sun and Xingshan Yao and Tian Lan and Jia-Ling Shi and Chen-Hao Cui and Hui-Yang Zhao and Zhijing Wu and Chen Yang and Xian-Ling Mao", "abstract": "State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.", "link": "http://arxiv.org/abs/2512.21019v2", "date": "2025-12-29", "relevancy": 2.5244, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6564}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6201}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Robust%20Video%20Defense%20Framework%20against%203D-field%20Personalized%20Talking%20Face&body=Title%3A%20Efficient%20and%20Robust%20Video%20Defense%20Framework%20against%203D-field%20Personalized%20Talking%20Face%0AAuthor%3A%20Rui-qing%20Sun%20and%20Xingshan%20Yao%20and%20Tian%20Lan%20and%20Jia-Ling%20Shi%20and%20Chen-Hao%20Cui%20and%20Hui-Yang%20Zhao%20and%20Zhijing%20Wu%20and%20Chen%20Yang%20and%20Xian-Ling%20Mao%0AAbstract%3A%20State-of-the-art%203D-field%20video-referenced%20Talking%20Face%20Generation%20%28TFG%29%20methods%20synthesize%20high-fidelity%20personalized%20talking-face%20videos%20in%20real%20time%20by%20modeling%203D%20geometry%20and%20appearance%20from%20reference%20portrait%20video.%20This%20capability%20raises%20significant%20privacy%20concerns%20regarding%20malicious%20misuse%20of%20personal%20portraits.%20However%2C%20no%20efficient%20defense%20framework%20exists%20to%20protect%20such%20videos%20against%203D-field%20TFG%20methods.%20While%20image-based%20defenses%20could%20apply%20per-frame%202D%20perturbations%2C%20they%20incur%20prohibitive%20computational%20costs%2C%20severe%20video%20quality%20degradation%2C%20failing%20to%20disrupt%203D%20information%20for%20video%20protection.%20To%20address%20this%2C%20we%20propose%20a%20novel%20and%20efficient%20video%20defense%20framework%20against%203D-field%20TFG%20methods%2C%20which%20protects%20portrait%20video%20by%20perturbing%20the%203D%20information%20acquisition%20process%20while%20maintain%20high-fidelity%20video%20quality.%20Specifically%2C%20our%20method%20introduces%3A%20%281%29%20a%20similarity-guided%20parameter%20sharing%20mechanism%20for%20computational%20efficiency%2C%20and%20%282%29%20a%20multi-scale%20dual-domain%20attention%20module%20to%20jointly%20optimize%20spatial-frequency%20perturbations.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20framework%20exhibits%20strong%20defense%20capability%20and%20achieves%20a%2047x%20acceleration%20over%20the%20fastest%20baseline%20while%20maintaining%20high%20fidelity.%20Moreover%2C%20it%20remains%20robust%20against%20scaling%20operations%20and%20state-of-the-art%20purification%20attacks%2C%20and%20the%20effectiveness%20of%20our%20design%20choices%20is%20further%20validated%20through%20ablation%20studies.%20Our%20project%20is%20available%20at%20https%3A//github.com/Richen7418/VDF.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Robust%2520Video%2520Defense%2520Framework%2520against%25203D-field%2520Personalized%2520Talking%2520Face%26entry.906535625%3DRui-qing%2520Sun%2520and%2520Xingshan%2520Yao%2520and%2520Tian%2520Lan%2520and%2520Jia-Ling%2520Shi%2520and%2520Chen-Hao%2520Cui%2520and%2520Hui-Yang%2520Zhao%2520and%2520Zhijing%2520Wu%2520and%2520Chen%2520Yang%2520and%2520Xian-Ling%2520Mao%26entry.1292438233%3DState-of-the-art%25203D-field%2520video-referenced%2520Talking%2520Face%2520Generation%2520%2528TFG%2529%2520methods%2520synthesize%2520high-fidelity%2520personalized%2520talking-face%2520videos%2520in%2520real%2520time%2520by%2520modeling%25203D%2520geometry%2520and%2520appearance%2520from%2520reference%2520portrait%2520video.%2520This%2520capability%2520raises%2520significant%2520privacy%2520concerns%2520regarding%2520malicious%2520misuse%2520of%2520personal%2520portraits.%2520However%252C%2520no%2520efficient%2520defense%2520framework%2520exists%2520to%2520protect%2520such%2520videos%2520against%25203D-field%2520TFG%2520methods.%2520While%2520image-based%2520defenses%2520could%2520apply%2520per-frame%25202D%2520perturbations%252C%2520they%2520incur%2520prohibitive%2520computational%2520costs%252C%2520severe%2520video%2520quality%2520degradation%252C%2520failing%2520to%2520disrupt%25203D%2520information%2520for%2520video%2520protection.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520and%2520efficient%2520video%2520defense%2520framework%2520against%25203D-field%2520TFG%2520methods%252C%2520which%2520protects%2520portrait%2520video%2520by%2520perturbing%2520the%25203D%2520information%2520acquisition%2520process%2520while%2520maintain%2520high-fidelity%2520video%2520quality.%2520Specifically%252C%2520our%2520method%2520introduces%253A%2520%25281%2529%2520a%2520similarity-guided%2520parameter%2520sharing%2520mechanism%2520for%2520computational%2520efficiency%252C%2520and%2520%25282%2529%2520a%2520multi-scale%2520dual-domain%2520attention%2520module%2520to%2520jointly%2520optimize%2520spatial-frequency%2520perturbations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520exhibits%2520strong%2520defense%2520capability%2520and%2520achieves%2520a%252047x%2520acceleration%2520over%2520the%2520fastest%2520baseline%2520while%2520maintaining%2520high%2520fidelity.%2520Moreover%252C%2520it%2520remains%2520robust%2520against%2520scaling%2520operations%2520and%2520state-of-the-art%2520purification%2520attacks%252C%2520and%2520the%2520effectiveness%2520of%2520our%2520design%2520choices%2520is%2520further%2520validated%2520through%2520ablation%2520studies.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//github.com/Richen7418/VDF.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Robust%20Video%20Defense%20Framework%20against%203D-field%20Personalized%20Talking%20Face&entry.906535625=Rui-qing%20Sun%20and%20Xingshan%20Yao%20and%20Tian%20Lan%20and%20Jia-Ling%20Shi%20and%20Chen-Hao%20Cui%20and%20Hui-Yang%20Zhao%20and%20Zhijing%20Wu%20and%20Chen%20Yang%20and%20Xian-Ling%20Mao&entry.1292438233=State-of-the-art%203D-field%20video-referenced%20Talking%20Face%20Generation%20%28TFG%29%20methods%20synthesize%20high-fidelity%20personalized%20talking-face%20videos%20in%20real%20time%20by%20modeling%203D%20geometry%20and%20appearance%20from%20reference%20portrait%20video.%20This%20capability%20raises%20significant%20privacy%20concerns%20regarding%20malicious%20misuse%20of%20personal%20portraits.%20However%2C%20no%20efficient%20defense%20framework%20exists%20to%20protect%20such%20videos%20against%203D-field%20TFG%20methods.%20While%20image-based%20defenses%20could%20apply%20per-frame%202D%20perturbations%2C%20they%20incur%20prohibitive%20computational%20costs%2C%20severe%20video%20quality%20degradation%2C%20failing%20to%20disrupt%203D%20information%20for%20video%20protection.%20To%20address%20this%2C%20we%20propose%20a%20novel%20and%20efficient%20video%20defense%20framework%20against%203D-field%20TFG%20methods%2C%20which%20protects%20portrait%20video%20by%20perturbing%20the%203D%20information%20acquisition%20process%20while%20maintain%20high-fidelity%20video%20quality.%20Specifically%2C%20our%20method%20introduces%3A%20%281%29%20a%20similarity-guided%20parameter%20sharing%20mechanism%20for%20computational%20efficiency%2C%20and%20%282%29%20a%20multi-scale%20dual-domain%20attention%20module%20to%20jointly%20optimize%20spatial-frequency%20perturbations.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20framework%20exhibits%20strong%20defense%20capability%20and%20achieves%20a%2047x%20acceleration%20over%20the%20fastest%20baseline%20while%20maintaining%20high%20fidelity.%20Moreover%2C%20it%20remains%20robust%20against%20scaling%20operations%20and%20state-of-the-art%20purification%20attacks%2C%20and%20the%20effectiveness%20of%20our%20design%20choices%20is%20further%20validated%20through%20ablation%20studies.%20Our%20project%20is%20available%20at%20https%3A//github.com/Richen7418/VDF.&entry.1838667208=http%3A//arxiv.org/abs/2512.21019v2&entry.124074799=Read"},
{"title": "Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains", "author": "Jiada Huang and Hao Ma and Zhibin Shen and Yizhou Qiao and Haiyang Li", "abstract": "Local high strain in solid rocket motor grains is a primary cause of structural failure. However, traditional numerical simulations are computationally expensive, and existing surrogate models cannot explicitly establish geometric models and accurately capture high-strain regions. Therefore, this paper proposes an adaptive graph network, GrainGNet, which employs an adaptive pooling dynamic node selection mechanism to effectively preserve the key mechanical features of structurally critical regions, while concurrently utilising feature fusion to transmit deep features and enhance the model's representational capacity. In the joint prediction task involving four sequential conditions--curing and cooling, storage, overloading, and ignition--GrainGNet reduces the mean squared error by 62.8% compared to the baseline graph U-Net model, with only a 5.2% increase in parameter count and an approximately sevenfold improvement in training efficiency. Furthermore, in the high-strain regions of debonding seams, the prediction error is further reduced by 33% compared to the second-best method, offering a computationally efficient and high-fidelity approach to evaluate motor structural safety.", "link": "http://arxiv.org/abs/2512.23443v1", "date": "2025-12-29", "relevancy": 2.5151, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5189}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4972}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Fusion%20Graph%20Network%20for%203D%20Strain%20Field%20Prediction%20in%20Solid%20Rocket%20Motor%20Grains&body=Title%3A%20Adaptive%20Fusion%20Graph%20Network%20for%203D%20Strain%20Field%20Prediction%20in%20Solid%20Rocket%20Motor%20Grains%0AAuthor%3A%20Jiada%20Huang%20and%20Hao%20Ma%20and%20Zhibin%20Shen%20and%20Yizhou%20Qiao%20and%20Haiyang%20Li%0AAbstract%3A%20Local%20high%20strain%20in%20solid%20rocket%20motor%20grains%20is%20a%20primary%20cause%20of%20structural%20failure.%20However%2C%20traditional%20numerical%20simulations%20are%20computationally%20expensive%2C%20and%20existing%20surrogate%20models%20cannot%20explicitly%20establish%20geometric%20models%20and%20accurately%20capture%20high-strain%20regions.%20Therefore%2C%20this%20paper%20proposes%20an%20adaptive%20graph%20network%2C%20GrainGNet%2C%20which%20employs%20an%20adaptive%20pooling%20dynamic%20node%20selection%20mechanism%20to%20effectively%20preserve%20the%20key%20mechanical%20features%20of%20structurally%20critical%20regions%2C%20while%20concurrently%20utilising%20feature%20fusion%20to%20transmit%20deep%20features%20and%20enhance%20the%20model%27s%20representational%20capacity.%20In%20the%20joint%20prediction%20task%20involving%20four%20sequential%20conditions--curing%20and%20cooling%2C%20storage%2C%20overloading%2C%20and%20ignition--GrainGNet%20reduces%20the%20mean%20squared%20error%20by%2062.8%25%20compared%20to%20the%20baseline%20graph%20U-Net%20model%2C%20with%20only%20a%205.2%25%20increase%20in%20parameter%20count%20and%20an%20approximately%20sevenfold%20improvement%20in%20training%20efficiency.%20Furthermore%2C%20in%20the%20high-strain%20regions%20of%20debonding%20seams%2C%20the%20prediction%20error%20is%20further%20reduced%20by%2033%25%20compared%20to%20the%20second-best%20method%2C%20offering%20a%20computationally%20efficient%20and%20high-fidelity%20approach%20to%20evaluate%20motor%20structural%20safety.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Fusion%2520Graph%2520Network%2520for%25203D%2520Strain%2520Field%2520Prediction%2520in%2520Solid%2520Rocket%2520Motor%2520Grains%26entry.906535625%3DJiada%2520Huang%2520and%2520Hao%2520Ma%2520and%2520Zhibin%2520Shen%2520and%2520Yizhou%2520Qiao%2520and%2520Haiyang%2520Li%26entry.1292438233%3DLocal%2520high%2520strain%2520in%2520solid%2520rocket%2520motor%2520grains%2520is%2520a%2520primary%2520cause%2520of%2520structural%2520failure.%2520However%252C%2520traditional%2520numerical%2520simulations%2520are%2520computationally%2520expensive%252C%2520and%2520existing%2520surrogate%2520models%2520cannot%2520explicitly%2520establish%2520geometric%2520models%2520and%2520accurately%2520capture%2520high-strain%2520regions.%2520Therefore%252C%2520this%2520paper%2520proposes%2520an%2520adaptive%2520graph%2520network%252C%2520GrainGNet%252C%2520which%2520employs%2520an%2520adaptive%2520pooling%2520dynamic%2520node%2520selection%2520mechanism%2520to%2520effectively%2520preserve%2520the%2520key%2520mechanical%2520features%2520of%2520structurally%2520critical%2520regions%252C%2520while%2520concurrently%2520utilising%2520feature%2520fusion%2520to%2520transmit%2520deep%2520features%2520and%2520enhance%2520the%2520model%2527s%2520representational%2520capacity.%2520In%2520the%2520joint%2520prediction%2520task%2520involving%2520four%2520sequential%2520conditions--curing%2520and%2520cooling%252C%2520storage%252C%2520overloading%252C%2520and%2520ignition--GrainGNet%2520reduces%2520the%2520mean%2520squared%2520error%2520by%252062.8%2525%2520compared%2520to%2520the%2520baseline%2520graph%2520U-Net%2520model%252C%2520with%2520only%2520a%25205.2%2525%2520increase%2520in%2520parameter%2520count%2520and%2520an%2520approximately%2520sevenfold%2520improvement%2520in%2520training%2520efficiency.%2520Furthermore%252C%2520in%2520the%2520high-strain%2520regions%2520of%2520debonding%2520seams%252C%2520the%2520prediction%2520error%2520is%2520further%2520reduced%2520by%252033%2525%2520compared%2520to%2520the%2520second-best%2520method%252C%2520offering%2520a%2520computationally%2520efficient%2520and%2520high-fidelity%2520approach%2520to%2520evaluate%2520motor%2520structural%2520safety.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Fusion%20Graph%20Network%20for%203D%20Strain%20Field%20Prediction%20in%20Solid%20Rocket%20Motor%20Grains&entry.906535625=Jiada%20Huang%20and%20Hao%20Ma%20and%20Zhibin%20Shen%20and%20Yizhou%20Qiao%20and%20Haiyang%20Li&entry.1292438233=Local%20high%20strain%20in%20solid%20rocket%20motor%20grains%20is%20a%20primary%20cause%20of%20structural%20failure.%20However%2C%20traditional%20numerical%20simulations%20are%20computationally%20expensive%2C%20and%20existing%20surrogate%20models%20cannot%20explicitly%20establish%20geometric%20models%20and%20accurately%20capture%20high-strain%20regions.%20Therefore%2C%20this%20paper%20proposes%20an%20adaptive%20graph%20network%2C%20GrainGNet%2C%20which%20employs%20an%20adaptive%20pooling%20dynamic%20node%20selection%20mechanism%20to%20effectively%20preserve%20the%20key%20mechanical%20features%20of%20structurally%20critical%20regions%2C%20while%20concurrently%20utilising%20feature%20fusion%20to%20transmit%20deep%20features%20and%20enhance%20the%20model%27s%20representational%20capacity.%20In%20the%20joint%20prediction%20task%20involving%20four%20sequential%20conditions--curing%20and%20cooling%2C%20storage%2C%20overloading%2C%20and%20ignition--GrainGNet%20reduces%20the%20mean%20squared%20error%20by%2062.8%25%20compared%20to%20the%20baseline%20graph%20U-Net%20model%2C%20with%20only%20a%205.2%25%20increase%20in%20parameter%20count%20and%20an%20approximately%20sevenfold%20improvement%20in%20training%20efficiency.%20Furthermore%2C%20in%20the%20high-strain%20regions%20of%20debonding%20seams%2C%20the%20prediction%20error%20is%20further%20reduced%20by%2033%25%20compared%20to%20the%20second-best%20method%2C%20offering%20a%20computationally%20efficient%20and%20high-fidelity%20approach%20to%20evaluate%20motor%20structural%20safety.&entry.1838667208=http%3A//arxiv.org/abs/2512.23443v1&entry.124074799=Read"},
{"title": "ThinkGen: Generalized Thinking for Visual Generation", "author": "Siyu Jiao and Yiheng Lin and Yujie Zhong and Qi She and Wei Zhou and Xiaohan Lan and Zilong Huang and Fei Yu and Yingchen Yu and Yunqing Zhao and Yao Zhao and Yunchao Wei", "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen", "link": "http://arxiv.org/abs/2512.23568v1", "date": "2025-12-29", "relevancy": 2.5044, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.65}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6102}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThinkGen%3A%20Generalized%20Thinking%20for%20Visual%20Generation&body=Title%3A%20ThinkGen%3A%20Generalized%20Thinking%20for%20Visual%20Generation%0AAuthor%3A%20Siyu%20Jiao%20and%20Yiheng%20Lin%20and%20Yujie%20Zhong%20and%20Qi%20She%20and%20Wei%20Zhou%20and%20Xiaohan%20Lan%20and%20Zilong%20Huang%20and%20Fei%20Yu%20and%20Yingchen%20Yu%20and%20Yunqing%20Zhao%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrates%20that%20Chain-of-Thought%20%28CoT%29%20reasoning%20enables%20systematic%20solutions%20to%20complex%20understanding%20tasks.%20However%2C%20its%20extension%20to%20generation%20tasks%20remains%20nascent%20and%20limited%20by%20scenario-specific%20mechanisms%20that%20hinder%20generalization%20and%20adaptation.%20In%20this%20work%2C%20we%20present%20ThinkGen%2C%20the%20first%20think-driven%20visual%20generation%20framework%20that%20explicitly%20leverages%20MLLM%27s%20CoT%20reasoning%20in%20various%20generation%20scenarios.%20ThinkGen%20employs%20a%20decoupled%20architecture%20comprising%20a%20pretrained%20MLLM%20and%20a%20Diffusion%20Transformer%20%28DiT%29%2C%20wherein%20the%20MLLM%20generates%20tailored%20instructions%20based%20on%20user%20intent%2C%20and%20DiT%20produces%20high-quality%20images%20guided%20by%20these%20instructions.%20We%20further%20propose%20a%20separable%20GRPO-based%20training%20paradigm%20%28SepGRPO%29%2C%20alternating%20reinforcement%20learning%20between%20the%20MLLM%20and%20DiT%20modules.%20This%20flexible%20design%20enables%20joint%20training%20across%20diverse%20datasets%2C%20facilitating%20effective%20CoT%20reasoning%20for%20a%20wide%20range%20of%20generative%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20ThinkGen%20achieves%20robust%2C%20state-of-the-art%20performance%20across%20multiple%20generation%20benchmarks.%20Code%20is%20available%3A%20https%3A//github.com/jiaosiyuu/ThinkGen%0ALink%3A%20http%3A//arxiv.org/abs/2512.23568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinkGen%253A%2520Generalized%2520Thinking%2520for%2520Visual%2520Generation%26entry.906535625%3DSiyu%2520Jiao%2520and%2520Yiheng%2520Lin%2520and%2520Yujie%2520Zhong%2520and%2520Qi%2520She%2520and%2520Wei%2520Zhou%2520and%2520Xiaohan%2520Lan%2520and%2520Zilong%2520Huang%2520and%2520Fei%2520Yu%2520and%2520Yingchen%2520Yu%2520and%2520Yunqing%2520Zhao%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3DRecent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrates%2520that%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520enables%2520systematic%2520solutions%2520to%2520complex%2520understanding%2520tasks.%2520However%252C%2520its%2520extension%2520to%2520generation%2520tasks%2520remains%2520nascent%2520and%2520limited%2520by%2520scenario-specific%2520mechanisms%2520that%2520hinder%2520generalization%2520and%2520adaptation.%2520In%2520this%2520work%252C%2520we%2520present%2520ThinkGen%252C%2520the%2520first%2520think-driven%2520visual%2520generation%2520framework%2520that%2520explicitly%2520leverages%2520MLLM%2527s%2520CoT%2520reasoning%2520in%2520various%2520generation%2520scenarios.%2520ThinkGen%2520employs%2520a%2520decoupled%2520architecture%2520comprising%2520a%2520pretrained%2520MLLM%2520and%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529%252C%2520wherein%2520the%2520MLLM%2520generates%2520tailored%2520instructions%2520based%2520on%2520user%2520intent%252C%2520and%2520DiT%2520produces%2520high-quality%2520images%2520guided%2520by%2520these%2520instructions.%2520We%2520further%2520propose%2520a%2520separable%2520GRPO-based%2520training%2520paradigm%2520%2528SepGRPO%2529%252C%2520alternating%2520reinforcement%2520learning%2520between%2520the%2520MLLM%2520and%2520DiT%2520modules.%2520This%2520flexible%2520design%2520enables%2520joint%2520training%2520across%2520diverse%2520datasets%252C%2520facilitating%2520effective%2520CoT%2520reasoning%2520for%2520a%2520wide%2520range%2520of%2520generative%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ThinkGen%2520achieves%2520robust%252C%2520state-of-the-art%2520performance%2520across%2520multiple%2520generation%2520benchmarks.%2520Code%2520is%2520available%253A%2520https%253A//github.com/jiaosiyuu/ThinkGen%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThinkGen%3A%20Generalized%20Thinking%20for%20Visual%20Generation&entry.906535625=Siyu%20Jiao%20and%20Yiheng%20Lin%20and%20Yujie%20Zhong%20and%20Qi%20She%20and%20Wei%20Zhou%20and%20Xiaohan%20Lan%20and%20Zilong%20Huang%20and%20Fei%20Yu%20and%20Yingchen%20Yu%20and%20Yunqing%20Zhao%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrates%20that%20Chain-of-Thought%20%28CoT%29%20reasoning%20enables%20systematic%20solutions%20to%20complex%20understanding%20tasks.%20However%2C%20its%20extension%20to%20generation%20tasks%20remains%20nascent%20and%20limited%20by%20scenario-specific%20mechanisms%20that%20hinder%20generalization%20and%20adaptation.%20In%20this%20work%2C%20we%20present%20ThinkGen%2C%20the%20first%20think-driven%20visual%20generation%20framework%20that%20explicitly%20leverages%20MLLM%27s%20CoT%20reasoning%20in%20various%20generation%20scenarios.%20ThinkGen%20employs%20a%20decoupled%20architecture%20comprising%20a%20pretrained%20MLLM%20and%20a%20Diffusion%20Transformer%20%28DiT%29%2C%20wherein%20the%20MLLM%20generates%20tailored%20instructions%20based%20on%20user%20intent%2C%20and%20DiT%20produces%20high-quality%20images%20guided%20by%20these%20instructions.%20We%20further%20propose%20a%20separable%20GRPO-based%20training%20paradigm%20%28SepGRPO%29%2C%20alternating%20reinforcement%20learning%20between%20the%20MLLM%20and%20DiT%20modules.%20This%20flexible%20design%20enables%20joint%20training%20across%20diverse%20datasets%2C%20facilitating%20effective%20CoT%20reasoning%20for%20a%20wide%20range%20of%20generative%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20ThinkGen%20achieves%20robust%2C%20state-of-the-art%20performance%20across%20multiple%20generation%20benchmarks.%20Code%20is%20available%3A%20https%3A//github.com/jiaosiyuu/ThinkGen&entry.1838667208=http%3A//arxiv.org/abs/2512.23568v1&entry.124074799=Read"},
{"title": "SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation", "author": "Xiaolan Li and Wanquan Liu and Pengcheng Li and Pengyu Jie and Chenqiang Gao", "abstract": "Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.", "link": "http://arxiv.org/abs/2512.23411v1", "date": "2025-12-29", "relevancy": 2.4875, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOFTooth%3A%20Semantics-Enhanced%20Order-Aware%20Fusion%20for%20Tooth%20Instance%20Segmentation&body=Title%3A%20SOFTooth%3A%20Semantics-Enhanced%20Order-Aware%20Fusion%20for%20Tooth%20Instance%20Segmentation%0AAuthor%3A%20Xiaolan%20Li%20and%20Wanquan%20Liu%20and%20Pengcheng%20Li%20and%20Pengyu%20Jie%20and%20Chenqiang%20Gao%0AAbstract%3A%20Three-dimensional%20%283D%29%20tooth%20instance%20segmentation%20remains%20challenging%20due%20to%20crowded%20arches%2C%20ambiguous%20tooth-gingiva%20boundaries%2C%20missing%20teeth%2C%20and%20rare%20yet%20clinically%20important%20third%20molars.%20Native%203D%20methods%20relying%20on%20geometric%20cues%20often%20suffer%20from%20boundary%20leakage%2C%20center%20drift%2C%20and%20inconsistent%20tooth%20identities%2C%20especially%20for%20minority%20classes%20and%20complex%20anatomies.%20Meanwhile%2C%202D%20foundation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20provide%20strong%20boundary-aware%20semantics%2C%20but%20directly%20applying%20them%20in%203D%20is%20impractical%20in%20clinical%20workflows.%20To%20address%20these%20issues%2C%20we%20propose%20SOFTooth%2C%20a%20semantics-enhanced%2C%20order-aware%202D-3D%20fusion%20framework%20that%20leverages%20frozen%202D%20semantics%20without%20explicit%202D%20mask%20supervision.%20First%2C%20a%20point-wise%20residual%20gating%20module%20injects%20occlusal-view%20SAM%20embeddings%20into%203D%20point%20features%20to%20refine%20tooth-gingiva%20and%20inter-tooth%20boundaries.%20Second%2C%20a%20center-guided%20mask%20refinement%20regularizes%20consistency%20between%20instance%20masks%20and%20geometric%20centroids%2C%20reducing%20center%20drift.%20Furthermore%2C%20an%20order-aware%20Hungarian%20matching%20strategy%20integrates%20anatomical%20tooth%20order%20and%20center%20distance%20into%20similarity-based%20assignment%2C%20ensuring%20coherent%20labeling%20even%20under%20missing%20or%20crowded%20dentitions.%20On%203DTeethSeg%2722%2C%20SOFTooth%20achieves%20state-of-the-art%20overall%20accuracy%20and%20mean%20IoU%2C%20with%20clear%20gains%20on%20cases%20involving%20third%20molars%2C%20demonstrating%20that%20rich%202D%20semantics%20can%20be%20effectively%20transferred%20to%203D%20tooth%20instance%20segmentation%20without%202D%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOFTooth%253A%2520Semantics-Enhanced%2520Order-Aware%2520Fusion%2520for%2520Tooth%2520Instance%2520Segmentation%26entry.906535625%3DXiaolan%2520Li%2520and%2520Wanquan%2520Liu%2520and%2520Pengcheng%2520Li%2520and%2520Pengyu%2520Jie%2520and%2520Chenqiang%2520Gao%26entry.1292438233%3DThree-dimensional%2520%25283D%2529%2520tooth%2520instance%2520segmentation%2520remains%2520challenging%2520due%2520to%2520crowded%2520arches%252C%2520ambiguous%2520tooth-gingiva%2520boundaries%252C%2520missing%2520teeth%252C%2520and%2520rare%2520yet%2520clinically%2520important%2520third%2520molars.%2520Native%25203D%2520methods%2520relying%2520on%2520geometric%2520cues%2520often%2520suffer%2520from%2520boundary%2520leakage%252C%2520center%2520drift%252C%2520and%2520inconsistent%2520tooth%2520identities%252C%2520especially%2520for%2520minority%2520classes%2520and%2520complex%2520anatomies.%2520Meanwhile%252C%25202D%2520foundation%2520models%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520provide%2520strong%2520boundary-aware%2520semantics%252C%2520but%2520directly%2520applying%2520them%2520in%25203D%2520is%2520impractical%2520in%2520clinical%2520workflows.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520SOFTooth%252C%2520a%2520semantics-enhanced%252C%2520order-aware%25202D-3D%2520fusion%2520framework%2520that%2520leverages%2520frozen%25202D%2520semantics%2520without%2520explicit%25202D%2520mask%2520supervision.%2520First%252C%2520a%2520point-wise%2520residual%2520gating%2520module%2520injects%2520occlusal-view%2520SAM%2520embeddings%2520into%25203D%2520point%2520features%2520to%2520refine%2520tooth-gingiva%2520and%2520inter-tooth%2520boundaries.%2520Second%252C%2520a%2520center-guided%2520mask%2520refinement%2520regularizes%2520consistency%2520between%2520instance%2520masks%2520and%2520geometric%2520centroids%252C%2520reducing%2520center%2520drift.%2520Furthermore%252C%2520an%2520order-aware%2520Hungarian%2520matching%2520strategy%2520integrates%2520anatomical%2520tooth%2520order%2520and%2520center%2520distance%2520into%2520similarity-based%2520assignment%252C%2520ensuring%2520coherent%2520labeling%2520even%2520under%2520missing%2520or%2520crowded%2520dentitions.%2520On%25203DTeethSeg%252722%252C%2520SOFTooth%2520achieves%2520state-of-the-art%2520overall%2520accuracy%2520and%2520mean%2520IoU%252C%2520with%2520clear%2520gains%2520on%2520cases%2520involving%2520third%2520molars%252C%2520demonstrating%2520that%2520rich%25202D%2520semantics%2520can%2520be%2520effectively%2520transferred%2520to%25203D%2520tooth%2520instance%2520segmentation%2520without%25202D%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOFTooth%3A%20Semantics-Enhanced%20Order-Aware%20Fusion%20for%20Tooth%20Instance%20Segmentation&entry.906535625=Xiaolan%20Li%20and%20Wanquan%20Liu%20and%20Pengcheng%20Li%20and%20Pengyu%20Jie%20and%20Chenqiang%20Gao&entry.1292438233=Three-dimensional%20%283D%29%20tooth%20instance%20segmentation%20remains%20challenging%20due%20to%20crowded%20arches%2C%20ambiguous%20tooth-gingiva%20boundaries%2C%20missing%20teeth%2C%20and%20rare%20yet%20clinically%20important%20third%20molars.%20Native%203D%20methods%20relying%20on%20geometric%20cues%20often%20suffer%20from%20boundary%20leakage%2C%20center%20drift%2C%20and%20inconsistent%20tooth%20identities%2C%20especially%20for%20minority%20classes%20and%20complex%20anatomies.%20Meanwhile%2C%202D%20foundation%20models%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20provide%20strong%20boundary-aware%20semantics%2C%20but%20directly%20applying%20them%20in%203D%20is%20impractical%20in%20clinical%20workflows.%20To%20address%20these%20issues%2C%20we%20propose%20SOFTooth%2C%20a%20semantics-enhanced%2C%20order-aware%202D-3D%20fusion%20framework%20that%20leverages%20frozen%202D%20semantics%20without%20explicit%202D%20mask%20supervision.%20First%2C%20a%20point-wise%20residual%20gating%20module%20injects%20occlusal-view%20SAM%20embeddings%20into%203D%20point%20features%20to%20refine%20tooth-gingiva%20and%20inter-tooth%20boundaries.%20Second%2C%20a%20center-guided%20mask%20refinement%20regularizes%20consistency%20between%20instance%20masks%20and%20geometric%20centroids%2C%20reducing%20center%20drift.%20Furthermore%2C%20an%20order-aware%20Hungarian%20matching%20strategy%20integrates%20anatomical%20tooth%20order%20and%20center%20distance%20into%20similarity-based%20assignment%2C%20ensuring%20coherent%20labeling%20even%20under%20missing%20or%20crowded%20dentitions.%20On%203DTeethSeg%2722%2C%20SOFTooth%20achieves%20state-of-the-art%20overall%20accuracy%20and%20mean%20IoU%2C%20with%20clear%20gains%20on%20cases%20involving%20third%20molars%2C%20demonstrating%20that%20rich%202D%20semantics%20can%20be%20effectively%20transferred%20to%203D%20tooth%20instance%20segmentation%20without%202D%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2512.23411v1&entry.124074799=Read"},
{"title": "End-to-End Test-Time Training for Long Context", "author": "Arnuv Tandon and Karan Dalal and Xinhao Li and Daniel Koceja and Marcel R\u00f8d and Sam Buchanan and Xiaolong Wang and Jure Leskovec and Sanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin and Jed McCaleb and Yejin Choi and Yu Sun", "abstract": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.", "link": "http://arxiv.org/abs/2512.23675v1", "date": "2025-12-29", "relevancy": 2.4728, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Test-Time%20Training%20for%20Long%20Context&body=Title%3A%20End-to-End%20Test-Time%20Training%20for%20Long%20Context%0AAuthor%3A%20Arnuv%20Tandon%20and%20Karan%20Dalal%20and%20Xinhao%20Li%20and%20Daniel%20Koceja%20and%20Marcel%20R%C3%B8d%20and%20Sam%20Buchanan%20and%20Xiaolong%20Wang%20and%20Jure%20Leskovec%20and%20Sanmi%20Koyejo%20and%20Tatsunori%20Hashimoto%20and%20Carlos%20Guestrin%20and%20Jed%20McCaleb%20and%20Yejin%20Choi%20and%20Yu%20Sun%0AAbstract%3A%20We%20formulate%20long-context%20language%20modeling%20as%20a%20problem%20in%20continual%20learning%20rather%20than%20architecture%20design.%20Under%20this%20formulation%2C%20we%20only%20use%20a%20standard%20architecture%20--%20a%20Transformer%20with%20sliding-window%20attention.%20However%2C%20our%20model%20continues%20learning%20at%20test%20time%20via%20next-token%20prediction%20on%20the%20given%20context%2C%20compressing%20the%20context%20it%20reads%20into%20its%20weights.%20In%20addition%2C%20we%20improve%20the%20model%27s%20initialization%20for%20learning%20at%20test%20time%20via%20meta-learning%20at%20training%20time.%20Overall%2C%20our%20method%2C%20a%20form%20of%20Test-Time%20Training%20%28TTT%29%2C%20is%20End-to-End%20%28E2E%29%20both%20at%20test%20time%20%28via%20next-token%20prediction%29%20and%20training%20time%20%28via%20meta-learning%29%2C%20in%20contrast%20to%20previous%20forms.%20We%20conduct%20extensive%20experiments%20with%20a%20focus%20on%20scaling%20properties.%20In%20particular%2C%20for%203B%20models%20trained%20with%20164B%20tokens%2C%20our%20method%20%28TTT-E2E%29%20scales%20with%20context%20length%20in%20the%20same%20way%20as%20Transformer%20with%20full%20attention%2C%20while%20others%2C%20such%20as%20Mamba%202%20and%20Gated%20DeltaNet%2C%20do%20not.%20However%2C%20similar%20to%20RNNs%2C%20TTT-E2E%20has%20constant%20inference%20latency%20regardless%20of%20context%20length%2C%20making%20it%202.7%20times%20faster%20than%20full%20attention%20for%20128K%20context.%20Our%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Test-Time%2520Training%2520for%2520Long%2520Context%26entry.906535625%3DArnuv%2520Tandon%2520and%2520Karan%2520Dalal%2520and%2520Xinhao%2520Li%2520and%2520Daniel%2520Koceja%2520and%2520Marcel%2520R%25C3%25B8d%2520and%2520Sam%2520Buchanan%2520and%2520Xiaolong%2520Wang%2520and%2520Jure%2520Leskovec%2520and%2520Sanmi%2520Koyejo%2520and%2520Tatsunori%2520Hashimoto%2520and%2520Carlos%2520Guestrin%2520and%2520Jed%2520McCaleb%2520and%2520Yejin%2520Choi%2520and%2520Yu%2520Sun%26entry.1292438233%3DWe%2520formulate%2520long-context%2520language%2520modeling%2520as%2520a%2520problem%2520in%2520continual%2520learning%2520rather%2520than%2520architecture%2520design.%2520Under%2520this%2520formulation%252C%2520we%2520only%2520use%2520a%2520standard%2520architecture%2520--%2520a%2520Transformer%2520with%2520sliding-window%2520attention.%2520However%252C%2520our%2520model%2520continues%2520learning%2520at%2520test%2520time%2520via%2520next-token%2520prediction%2520on%2520the%2520given%2520context%252C%2520compressing%2520the%2520context%2520it%2520reads%2520into%2520its%2520weights.%2520In%2520addition%252C%2520we%2520improve%2520the%2520model%2527s%2520initialization%2520for%2520learning%2520at%2520test%2520time%2520via%2520meta-learning%2520at%2520training%2520time.%2520Overall%252C%2520our%2520method%252C%2520a%2520form%2520of%2520Test-Time%2520Training%2520%2528TTT%2529%252C%2520is%2520End-to-End%2520%2528E2E%2529%2520both%2520at%2520test%2520time%2520%2528via%2520next-token%2520prediction%2529%2520and%2520training%2520time%2520%2528via%2520meta-learning%2529%252C%2520in%2520contrast%2520to%2520previous%2520forms.%2520We%2520conduct%2520extensive%2520experiments%2520with%2520a%2520focus%2520on%2520scaling%2520properties.%2520In%2520particular%252C%2520for%25203B%2520models%2520trained%2520with%2520164B%2520tokens%252C%2520our%2520method%2520%2528TTT-E2E%2529%2520scales%2520with%2520context%2520length%2520in%2520the%2520same%2520way%2520as%2520Transformer%2520with%2520full%2520attention%252C%2520while%2520others%252C%2520such%2520as%2520Mamba%25202%2520and%2520Gated%2520DeltaNet%252C%2520do%2520not.%2520However%252C%2520similar%2520to%2520RNNs%252C%2520TTT-E2E%2520has%2520constant%2520inference%2520latency%2520regardless%2520of%2520context%2520length%252C%2520making%2520it%25202.7%2520times%2520faster%2520than%2520full%2520attention%2520for%2520128K%2520context.%2520Our%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Test-Time%20Training%20for%20Long%20Context&entry.906535625=Arnuv%20Tandon%20and%20Karan%20Dalal%20and%20Xinhao%20Li%20and%20Daniel%20Koceja%20and%20Marcel%20R%C3%B8d%20and%20Sam%20Buchanan%20and%20Xiaolong%20Wang%20and%20Jure%20Leskovec%20and%20Sanmi%20Koyejo%20and%20Tatsunori%20Hashimoto%20and%20Carlos%20Guestrin%20and%20Jed%20McCaleb%20and%20Yejin%20Choi%20and%20Yu%20Sun&entry.1292438233=We%20formulate%20long-context%20language%20modeling%20as%20a%20problem%20in%20continual%20learning%20rather%20than%20architecture%20design.%20Under%20this%20formulation%2C%20we%20only%20use%20a%20standard%20architecture%20--%20a%20Transformer%20with%20sliding-window%20attention.%20However%2C%20our%20model%20continues%20learning%20at%20test%20time%20via%20next-token%20prediction%20on%20the%20given%20context%2C%20compressing%20the%20context%20it%20reads%20into%20its%20weights.%20In%20addition%2C%20we%20improve%20the%20model%27s%20initialization%20for%20learning%20at%20test%20time%20via%20meta-learning%20at%20training%20time.%20Overall%2C%20our%20method%2C%20a%20form%20of%20Test-Time%20Training%20%28TTT%29%2C%20is%20End-to-End%20%28E2E%29%20both%20at%20test%20time%20%28via%20next-token%20prediction%29%20and%20training%20time%20%28via%20meta-learning%29%2C%20in%20contrast%20to%20previous%20forms.%20We%20conduct%20extensive%20experiments%20with%20a%20focus%20on%20scaling%20properties.%20In%20particular%2C%20for%203B%20models%20trained%20with%20164B%20tokens%2C%20our%20method%20%28TTT-E2E%29%20scales%20with%20context%20length%20in%20the%20same%20way%20as%20Transformer%20with%20full%20attention%2C%20while%20others%2C%20such%20as%20Mamba%202%20and%20Gated%20DeltaNet%2C%20do%20not.%20However%2C%20similar%20to%20RNNs%2C%20TTT-E2E%20has%20constant%20inference%20latency%20regardless%20of%20context%20length%2C%20making%20it%202.7%20times%20faster%20than%20full%20attention%20for%20128K%20context.%20Our%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.23675v1&entry.124074799=Read"},
{"title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion", "author": "Zhe Li and Cheng Chi and Yangyang Wei and Boan Zhu and Tao Huang and Zhenguo Sun and Yibo Peng and Pengwei Wang and Zhongyuan Wang and Fangzhou Liu and Chang Xu and Shanghang Zhang", "abstract": "Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying \"understand before you imitate\". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.", "link": "http://arxiv.org/abs/2512.23649v1", "date": "2025-12-29", "relevancy": 2.4675, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6289}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6241}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboMirror%3A%20Understand%20Before%20You%20Imitate%20for%20Video%20to%20Humanoid%20Locomotion&body=Title%3A%20RoboMirror%3A%20Understand%20Before%20You%20Imitate%20for%20Video%20to%20Humanoid%20Locomotion%0AAuthor%3A%20Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Tao%20Huang%20and%20Zhenguo%20Sun%20and%20Yibo%20Peng%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Fangzhou%20Liu%20and%20Chang%20Xu%20and%20Shanghang%20Zhang%0AAbstract%3A%20Humans%20learn%20locomotion%20through%20visual%20observation%2C%20interpreting%20visual%20content%20first%20before%20imitating%20actions.%20However%2C%20state-of-the-art%20humanoid%20locomotion%20systems%20rely%20on%20either%20curated%20motion%20capture%20trajectories%20or%20sparse%20text%20commands%2C%20leaving%20a%20critical%20gap%20between%20visual%20understanding%20and%20control.%20Text-to-motion%20methods%20suffer%20from%20semantic%20sparsity%20and%20staged%20pipeline%20errors%2C%20while%20video-based%20approaches%20only%20perform%20mechanical%20pose%20mimicry%20without%20genuine%20visual%20understanding.%20We%20propose%20RoboMirror%2C%20the%20first%20retargeting-free%20video-to-locomotion%20framework%20embodying%20%22understand%20before%20you%20imitate%22.%20Leveraging%20VLMs%2C%20it%20distills%20raw%20egocentric/third-person%20videos%20into%20visual%20motion%20intents%2C%20which%20directly%20condition%20a%20diffusion-based%20policy%20to%20generate%20physically%20plausible%2C%20semantically%20aligned%20locomotion%20without%20explicit%20pose%20reconstruction%20or%20retargeting.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20RoboMirror%2C%20it%20enables%20telepresence%20via%20egocentric%20videos%2C%20drastically%20reduces%20third-person%20control%20latency%20by%2080%25%2C%20and%20achieves%20a%203.7%25%20higher%20task%20success%20rate%20than%20baselines.%20By%20reframing%20humanoid%20control%20around%20video%20understanding%2C%20we%20bridge%20the%20visual%20understanding%20and%20action%20gap.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboMirror%253A%2520Understand%2520Before%2520You%2520Imitate%2520for%2520Video%2520to%2520Humanoid%2520Locomotion%26entry.906535625%3DZhe%2520Li%2520and%2520Cheng%2520Chi%2520and%2520Yangyang%2520Wei%2520and%2520Boan%2520Zhu%2520and%2520Tao%2520Huang%2520and%2520Zhenguo%2520Sun%2520and%2520Yibo%2520Peng%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Fangzhou%2520Liu%2520and%2520Chang%2520Xu%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DHumans%2520learn%2520locomotion%2520through%2520visual%2520observation%252C%2520interpreting%2520visual%2520content%2520first%2520before%2520imitating%2520actions.%2520However%252C%2520state-of-the-art%2520humanoid%2520locomotion%2520systems%2520rely%2520on%2520either%2520curated%2520motion%2520capture%2520trajectories%2520or%2520sparse%2520text%2520commands%252C%2520leaving%2520a%2520critical%2520gap%2520between%2520visual%2520understanding%2520and%2520control.%2520Text-to-motion%2520methods%2520suffer%2520from%2520semantic%2520sparsity%2520and%2520staged%2520pipeline%2520errors%252C%2520while%2520video-based%2520approaches%2520only%2520perform%2520mechanical%2520pose%2520mimicry%2520without%2520genuine%2520visual%2520understanding.%2520We%2520propose%2520RoboMirror%252C%2520the%2520first%2520retargeting-free%2520video-to-locomotion%2520framework%2520embodying%2520%2522understand%2520before%2520you%2520imitate%2522.%2520Leveraging%2520VLMs%252C%2520it%2520distills%2520raw%2520egocentric/third-person%2520videos%2520into%2520visual%2520motion%2520intents%252C%2520which%2520directly%2520condition%2520a%2520diffusion-based%2520policy%2520to%2520generate%2520physically%2520plausible%252C%2520semantically%2520aligned%2520locomotion%2520without%2520explicit%2520pose%2520reconstruction%2520or%2520retargeting.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520RoboMirror%252C%2520it%2520enables%2520telepresence%2520via%2520egocentric%2520videos%252C%2520drastically%2520reduces%2520third-person%2520control%2520latency%2520by%252080%2525%252C%2520and%2520achieves%2520a%25203.7%2525%2520higher%2520task%2520success%2520rate%2520than%2520baselines.%2520By%2520reframing%2520humanoid%2520control%2520around%2520video%2520understanding%252C%2520we%2520bridge%2520the%2520visual%2520understanding%2520and%2520action%2520gap.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboMirror%3A%20Understand%20Before%20You%20Imitate%20for%20Video%20to%20Humanoid%20Locomotion&entry.906535625=Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Tao%20Huang%20and%20Zhenguo%20Sun%20and%20Yibo%20Peng%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Fangzhou%20Liu%20and%20Chang%20Xu%20and%20Shanghang%20Zhang&entry.1292438233=Humans%20learn%20locomotion%20through%20visual%20observation%2C%20interpreting%20visual%20content%20first%20before%20imitating%20actions.%20However%2C%20state-of-the-art%20humanoid%20locomotion%20systems%20rely%20on%20either%20curated%20motion%20capture%20trajectories%20or%20sparse%20text%20commands%2C%20leaving%20a%20critical%20gap%20between%20visual%20understanding%20and%20control.%20Text-to-motion%20methods%20suffer%20from%20semantic%20sparsity%20and%20staged%20pipeline%20errors%2C%20while%20video-based%20approaches%20only%20perform%20mechanical%20pose%20mimicry%20without%20genuine%20visual%20understanding.%20We%20propose%20RoboMirror%2C%20the%20first%20retargeting-free%20video-to-locomotion%20framework%20embodying%20%22understand%20before%20you%20imitate%22.%20Leveraging%20VLMs%2C%20it%20distills%20raw%20egocentric/third-person%20videos%20into%20visual%20motion%20intents%2C%20which%20directly%20condition%20a%20diffusion-based%20policy%20to%20generate%20physically%20plausible%2C%20semantically%20aligned%20locomotion%20without%20explicit%20pose%20reconstruction%20or%20retargeting.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20RoboMirror%2C%20it%20enables%20telepresence%20via%20egocentric%20videos%2C%20drastically%20reduces%20third-person%20control%20latency%20by%2080%25%2C%20and%20achieves%20a%203.7%25%20higher%20task%20success%20rate%20than%20baselines.%20By%20reframing%20humanoid%20control%20around%20video%20understanding%2C%20we%20bridge%20the%20visual%20understanding%20and%20action%20gap.&entry.1838667208=http%3A//arxiv.org/abs/2512.23649v1&entry.124074799=Read"},
{"title": "ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling", "author": "Hai Duong Nguyen and Xuan-The Tran", "abstract": "Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.", "link": "http://arxiv.org/abs/2512.23347v1", "date": "2025-12-29", "relevancy": 2.4526, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5036}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECG-RAMBA%3A%20Zero-Shot%20ECG%20Generalization%20by%20Morphology-Rhythm%20Disentanglement%20and%20Long-Range%20Modeling&body=Title%3A%20ECG-RAMBA%3A%20Zero-Shot%20ECG%20Generalization%20by%20Morphology-Rhythm%20Disentanglement%20and%20Long-Range%20Modeling%0AAuthor%3A%20Hai%20Duong%20Nguyen%20and%20Xuan-The%20Tran%0AAbstract%3A%20Deep%20learning%20has%20achieved%20strong%20performance%20for%20electrocardiogram%20%28ECG%29%20classification%20within%20individual%20datasets%2C%20yet%20dependable%20generalization%20across%20heterogeneous%20acquisition%20settings%20remains%20a%20major%20obstacle%20to%20clinical%20deployment%20and%20longitudinal%20monitoring.%20A%20key%20limitation%20of%20many%20model%20architectures%20is%20the%20implicit%20entanglement%20of%20morphological%20waveform%20patterns%20and%20rhythm%20dynamics%2C%20which%20can%20promote%20shortcut%20learning%20and%20amplify%20sensitivity%20to%20distribution%20shifts.%20We%20propose%20ECG-RAMBA%2C%20a%20framework%20that%20separates%20morphology%20and%20rhythm%20and%20then%20re-integrates%20them%20through%20context-aware%20fusion.%20ECG-RAMBA%20combines%3A%20%28i%29%20deterministic%20morphological%20features%20extracted%20by%20MiniRocket%2C%20%28ii%29%20global%20rhythm%20descriptors%20computed%20from%20heart-rate%20variability%20%28HRV%29%2C%20and%20%28iii%29%20long-range%20contextual%20modeling%20via%20a%20bi-directional%20Mamba%20backbone.%20To%20improve%20sensitivity%20to%20transient%20abnormalities%20under%20windowed%20inference%2C%20we%20introduce%20a%20numerically%20stable%20Power%20Mean%20pooling%20operator%20%28%24Q%3D3%24%29%20that%20emphasizes%20high-evidence%20segments%20while%20avoiding%20the%20brittleness%20of%20max%20pooling%20and%20the%20dilution%20of%20averaging.%20We%20evaluate%20under%20a%20protocol-faithful%20setting%20with%20subject-level%20cross-validation%2C%20a%20fixed%20decision%20threshold%2C%20and%20no%20test-time%20adaptation.%20On%20the%20Chapman--Shaoxing%20dataset%2C%20ECG-RAMBA%20achieves%20a%20macro%20ROC-AUC%20%24%5Capprox%200.85%24.%20In%20zero-shot%20transfer%2C%20it%20attains%20PR-AUC%20%24%3D0.708%24%20for%20atrial%20fibrillation%20detection%20on%20the%20external%20CPSC-2021%20dataset%2C%20substantially%20outperforming%20a%20comparable%20raw-signal%20Mamba%20baseline%2C%20and%20shows%20consistent%20cross-dataset%20performance%20on%20PTB-XL.%20Ablation%20studies%20indicate%20that%20deterministic%20morphology%20provides%20a%20strong%20foundation%2C%20while%20explicit%20rhythm%20modeling%20and%20long-range%20context%20are%20critical%20drivers%20of%20cross-domain%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECG-RAMBA%253A%2520Zero-Shot%2520ECG%2520Generalization%2520by%2520Morphology-Rhythm%2520Disentanglement%2520and%2520Long-Range%2520Modeling%26entry.906535625%3DHai%2520Duong%2520Nguyen%2520and%2520Xuan-The%2520Tran%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520strong%2520performance%2520for%2520electrocardiogram%2520%2528ECG%2529%2520classification%2520within%2520individual%2520datasets%252C%2520yet%2520dependable%2520generalization%2520across%2520heterogeneous%2520acquisition%2520settings%2520remains%2520a%2520major%2520obstacle%2520to%2520clinical%2520deployment%2520and%2520longitudinal%2520monitoring.%2520A%2520key%2520limitation%2520of%2520many%2520model%2520architectures%2520is%2520the%2520implicit%2520entanglement%2520of%2520morphological%2520waveform%2520patterns%2520and%2520rhythm%2520dynamics%252C%2520which%2520can%2520promote%2520shortcut%2520learning%2520and%2520amplify%2520sensitivity%2520to%2520distribution%2520shifts.%2520We%2520propose%2520ECG-RAMBA%252C%2520a%2520framework%2520that%2520separates%2520morphology%2520and%2520rhythm%2520and%2520then%2520re-integrates%2520them%2520through%2520context-aware%2520fusion.%2520ECG-RAMBA%2520combines%253A%2520%2528i%2529%2520deterministic%2520morphological%2520features%2520extracted%2520by%2520MiniRocket%252C%2520%2528ii%2529%2520global%2520rhythm%2520descriptors%2520computed%2520from%2520heart-rate%2520variability%2520%2528HRV%2529%252C%2520and%2520%2528iii%2529%2520long-range%2520contextual%2520modeling%2520via%2520a%2520bi-directional%2520Mamba%2520backbone.%2520To%2520improve%2520sensitivity%2520to%2520transient%2520abnormalities%2520under%2520windowed%2520inference%252C%2520we%2520introduce%2520a%2520numerically%2520stable%2520Power%2520Mean%2520pooling%2520operator%2520%2528%2524Q%253D3%2524%2529%2520that%2520emphasizes%2520high-evidence%2520segments%2520while%2520avoiding%2520the%2520brittleness%2520of%2520max%2520pooling%2520and%2520the%2520dilution%2520of%2520averaging.%2520We%2520evaluate%2520under%2520a%2520protocol-faithful%2520setting%2520with%2520subject-level%2520cross-validation%252C%2520a%2520fixed%2520decision%2520threshold%252C%2520and%2520no%2520test-time%2520adaptation.%2520On%2520the%2520Chapman--Shaoxing%2520dataset%252C%2520ECG-RAMBA%2520achieves%2520a%2520macro%2520ROC-AUC%2520%2524%255Capprox%25200.85%2524.%2520In%2520zero-shot%2520transfer%252C%2520it%2520attains%2520PR-AUC%2520%2524%253D0.708%2524%2520for%2520atrial%2520fibrillation%2520detection%2520on%2520the%2520external%2520CPSC-2021%2520dataset%252C%2520substantially%2520outperforming%2520a%2520comparable%2520raw-signal%2520Mamba%2520baseline%252C%2520and%2520shows%2520consistent%2520cross-dataset%2520performance%2520on%2520PTB-XL.%2520Ablation%2520studies%2520indicate%2520that%2520deterministic%2520morphology%2520provides%2520a%2520strong%2520foundation%252C%2520while%2520explicit%2520rhythm%2520modeling%2520and%2520long-range%2520context%2520are%2520critical%2520drivers%2520of%2520cross-domain%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECG-RAMBA%3A%20Zero-Shot%20ECG%20Generalization%20by%20Morphology-Rhythm%20Disentanglement%20and%20Long-Range%20Modeling&entry.906535625=Hai%20Duong%20Nguyen%20and%20Xuan-The%20Tran&entry.1292438233=Deep%20learning%20has%20achieved%20strong%20performance%20for%20electrocardiogram%20%28ECG%29%20classification%20within%20individual%20datasets%2C%20yet%20dependable%20generalization%20across%20heterogeneous%20acquisition%20settings%20remains%20a%20major%20obstacle%20to%20clinical%20deployment%20and%20longitudinal%20monitoring.%20A%20key%20limitation%20of%20many%20model%20architectures%20is%20the%20implicit%20entanglement%20of%20morphological%20waveform%20patterns%20and%20rhythm%20dynamics%2C%20which%20can%20promote%20shortcut%20learning%20and%20amplify%20sensitivity%20to%20distribution%20shifts.%20We%20propose%20ECG-RAMBA%2C%20a%20framework%20that%20separates%20morphology%20and%20rhythm%20and%20then%20re-integrates%20them%20through%20context-aware%20fusion.%20ECG-RAMBA%20combines%3A%20%28i%29%20deterministic%20morphological%20features%20extracted%20by%20MiniRocket%2C%20%28ii%29%20global%20rhythm%20descriptors%20computed%20from%20heart-rate%20variability%20%28HRV%29%2C%20and%20%28iii%29%20long-range%20contextual%20modeling%20via%20a%20bi-directional%20Mamba%20backbone.%20To%20improve%20sensitivity%20to%20transient%20abnormalities%20under%20windowed%20inference%2C%20we%20introduce%20a%20numerically%20stable%20Power%20Mean%20pooling%20operator%20%28%24Q%3D3%24%29%20that%20emphasizes%20high-evidence%20segments%20while%20avoiding%20the%20brittleness%20of%20max%20pooling%20and%20the%20dilution%20of%20averaging.%20We%20evaluate%20under%20a%20protocol-faithful%20setting%20with%20subject-level%20cross-validation%2C%20a%20fixed%20decision%20threshold%2C%20and%20no%20test-time%20adaptation.%20On%20the%20Chapman--Shaoxing%20dataset%2C%20ECG-RAMBA%20achieves%20a%20macro%20ROC-AUC%20%24%5Capprox%200.85%24.%20In%20zero-shot%20transfer%2C%20it%20attains%20PR-AUC%20%24%3D0.708%24%20for%20atrial%20fibrillation%20detection%20on%20the%20external%20CPSC-2021%20dataset%2C%20substantially%20outperforming%20a%20comparable%20raw-signal%20Mamba%20baseline%2C%20and%20shows%20consistent%20cross-dataset%20performance%20on%20PTB-XL.%20Ablation%20studies%20indicate%20that%20deterministic%20morphology%20provides%20a%20strong%20foundation%2C%20while%20explicit%20rhythm%20modeling%20and%20long-range%20context%20are%20critical%20drivers%20of%20cross-domain%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2512.23347v1&entry.124074799=Read"},
{"title": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception", "author": "Xiaoyu Li and Peidong Li and Xian Wu and Long Shi and Dedong Liu and Yitao Wu and Jiajia Fu and Dixiao Cui and Lijun Zhao and Lining Sun", "abstract": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.", "link": "http://arxiv.org/abs/2512.23635v1", "date": "2025-12-29", "relevancy": 2.4371, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6076}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Spatio-Temporal%20Alignment%20of%20End-to-End%203D%20Perception&body=Title%3A%20Rethinking%20the%20Spatio-Temporal%20Alignment%20of%20End-to-End%203D%20Perception%0AAuthor%3A%20Xiaoyu%20Li%20and%20Peidong%20Li%20and%20Xian%20Wu%20and%20Long%20Shi%20and%20Dedong%20Liu%20and%20Yitao%20Wu%20and%20Jiajia%20Fu%20and%20Dixiao%20Cui%20and%20Lijun%20Zhao%20and%20Lining%20Sun%0AAbstract%3A%20Spatio-temporal%20alignment%20is%20crucial%20for%20temporal%20modeling%20of%20end-to-end%20%28E2E%29%20perception%20in%20autonomous%20driving%20%28AD%29%2C%20providing%20valuable%20structural%20and%20textural%20prior%20information.%20Existing%20methods%20typically%20rely%20on%20the%20attention%20mechanism%20to%20align%20objects%20across%20frames%2C%20simplifying%20the%20motion%20model%20with%20a%20unified%20explicit%20physical%20model%20%28constant%20velocity%2C%20etc.%29.%20These%20approaches%20prefer%20semantic%20features%20for%20implicit%20alignment%2C%20challenging%20the%20importance%20of%20explicit%20motion%20modeling%20in%20the%20traditional%20perception%20paradigm.%20However%2C%20variations%20in%20motion%20states%20and%20object%20features%20across%20categories%20and%20frames%20render%20this%20alignment%20suboptimal.%20To%20address%20this%2C%20we%20propose%20HAT%2C%20a%20spatio-temporal%20alignment%20module%20that%20allows%20each%20object%20to%20adaptively%20decode%20the%20optimal%20alignment%20proposal%20from%20multiple%20hypotheses%20without%20direct%20supervision.%20Specifically%2C%20HAT%20first%20utilizes%20multiple%20explicit%20motion%20models%20to%20generate%20spatial%20anchors%20and%20motion-aware%20feature%20proposals%20for%20historical%20instances.%20It%20then%20performs%20multi-hypothesis%20decoding%20by%20incorporating%20semantic%20and%20motion%20cues%20embedded%20in%20cached%20object%20queries%2C%20ultimately%20providing%20the%20optimal%20alignment%20proposal%20for%20the%20target%20frame.%20On%20nuScenes%2C%20HAT%20consistently%20improves%203D%20temporal%20detectors%20and%20trackers%20across%20diverse%20baselines.%20It%20achieves%20state-of-the-art%20tracking%20results%20with%2046.0%25%20AMOTA%20on%20the%20test%20set%20when%20paired%20with%20the%20DETR3D%20detector.%20In%20an%20object-centric%20E2E%20AD%20method%2C%20HAT%20enhances%20perception%20accuracy%20%28%2B1.3%25%20mAP%2C%20%2B3.1%25%20AMOTA%29%20and%20reduces%20the%20collision%20rate%20by%2032%25.%20When%20semantics%20are%20corrupted%20%28nuScenes-C%29%2C%20the%20enhancement%20of%20motion%20modeling%20by%20HAT%20enables%20more%20robust%20perception%20and%20planning%20in%20the%20E2E%20AD.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Spatio-Temporal%2520Alignment%2520of%2520End-to-End%25203D%2520Perception%26entry.906535625%3DXiaoyu%2520Li%2520and%2520Peidong%2520Li%2520and%2520Xian%2520Wu%2520and%2520Long%2520Shi%2520and%2520Dedong%2520Liu%2520and%2520Yitao%2520Wu%2520and%2520Jiajia%2520Fu%2520and%2520Dixiao%2520Cui%2520and%2520Lijun%2520Zhao%2520and%2520Lining%2520Sun%26entry.1292438233%3DSpatio-temporal%2520alignment%2520is%2520crucial%2520for%2520temporal%2520modeling%2520of%2520end-to-end%2520%2528E2E%2529%2520perception%2520in%2520autonomous%2520driving%2520%2528AD%2529%252C%2520providing%2520valuable%2520structural%2520and%2520textural%2520prior%2520information.%2520Existing%2520methods%2520typically%2520rely%2520on%2520the%2520attention%2520mechanism%2520to%2520align%2520objects%2520across%2520frames%252C%2520simplifying%2520the%2520motion%2520model%2520with%2520a%2520unified%2520explicit%2520physical%2520model%2520%2528constant%2520velocity%252C%2520etc.%2529.%2520These%2520approaches%2520prefer%2520semantic%2520features%2520for%2520implicit%2520alignment%252C%2520challenging%2520the%2520importance%2520of%2520explicit%2520motion%2520modeling%2520in%2520the%2520traditional%2520perception%2520paradigm.%2520However%252C%2520variations%2520in%2520motion%2520states%2520and%2520object%2520features%2520across%2520categories%2520and%2520frames%2520render%2520this%2520alignment%2520suboptimal.%2520To%2520address%2520this%252C%2520we%2520propose%2520HAT%252C%2520a%2520spatio-temporal%2520alignment%2520module%2520that%2520allows%2520each%2520object%2520to%2520adaptively%2520decode%2520the%2520optimal%2520alignment%2520proposal%2520from%2520multiple%2520hypotheses%2520without%2520direct%2520supervision.%2520Specifically%252C%2520HAT%2520first%2520utilizes%2520multiple%2520explicit%2520motion%2520models%2520to%2520generate%2520spatial%2520anchors%2520and%2520motion-aware%2520feature%2520proposals%2520for%2520historical%2520instances.%2520It%2520then%2520performs%2520multi-hypothesis%2520decoding%2520by%2520incorporating%2520semantic%2520and%2520motion%2520cues%2520embedded%2520in%2520cached%2520object%2520queries%252C%2520ultimately%2520providing%2520the%2520optimal%2520alignment%2520proposal%2520for%2520the%2520target%2520frame.%2520On%2520nuScenes%252C%2520HAT%2520consistently%2520improves%25203D%2520temporal%2520detectors%2520and%2520trackers%2520across%2520diverse%2520baselines.%2520It%2520achieves%2520state-of-the-art%2520tracking%2520results%2520with%252046.0%2525%2520AMOTA%2520on%2520the%2520test%2520set%2520when%2520paired%2520with%2520the%2520DETR3D%2520detector.%2520In%2520an%2520object-centric%2520E2E%2520AD%2520method%252C%2520HAT%2520enhances%2520perception%2520accuracy%2520%2528%252B1.3%2525%2520mAP%252C%2520%252B3.1%2525%2520AMOTA%2529%2520and%2520reduces%2520the%2520collision%2520rate%2520by%252032%2525.%2520When%2520semantics%2520are%2520corrupted%2520%2528nuScenes-C%2529%252C%2520the%2520enhancement%2520of%2520motion%2520modeling%2520by%2520HAT%2520enables%2520more%2520robust%2520perception%2520and%2520planning%2520in%2520the%2520E2E%2520AD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Spatio-Temporal%20Alignment%20of%20End-to-End%203D%20Perception&entry.906535625=Xiaoyu%20Li%20and%20Peidong%20Li%20and%20Xian%20Wu%20and%20Long%20Shi%20and%20Dedong%20Liu%20and%20Yitao%20Wu%20and%20Jiajia%20Fu%20and%20Dixiao%20Cui%20and%20Lijun%20Zhao%20and%20Lining%20Sun&entry.1292438233=Spatio-temporal%20alignment%20is%20crucial%20for%20temporal%20modeling%20of%20end-to-end%20%28E2E%29%20perception%20in%20autonomous%20driving%20%28AD%29%2C%20providing%20valuable%20structural%20and%20textural%20prior%20information.%20Existing%20methods%20typically%20rely%20on%20the%20attention%20mechanism%20to%20align%20objects%20across%20frames%2C%20simplifying%20the%20motion%20model%20with%20a%20unified%20explicit%20physical%20model%20%28constant%20velocity%2C%20etc.%29.%20These%20approaches%20prefer%20semantic%20features%20for%20implicit%20alignment%2C%20challenging%20the%20importance%20of%20explicit%20motion%20modeling%20in%20the%20traditional%20perception%20paradigm.%20However%2C%20variations%20in%20motion%20states%20and%20object%20features%20across%20categories%20and%20frames%20render%20this%20alignment%20suboptimal.%20To%20address%20this%2C%20we%20propose%20HAT%2C%20a%20spatio-temporal%20alignment%20module%20that%20allows%20each%20object%20to%20adaptively%20decode%20the%20optimal%20alignment%20proposal%20from%20multiple%20hypotheses%20without%20direct%20supervision.%20Specifically%2C%20HAT%20first%20utilizes%20multiple%20explicit%20motion%20models%20to%20generate%20spatial%20anchors%20and%20motion-aware%20feature%20proposals%20for%20historical%20instances.%20It%20then%20performs%20multi-hypothesis%20decoding%20by%20incorporating%20semantic%20and%20motion%20cues%20embedded%20in%20cached%20object%20queries%2C%20ultimately%20providing%20the%20optimal%20alignment%20proposal%20for%20the%20target%20frame.%20On%20nuScenes%2C%20HAT%20consistently%20improves%203D%20temporal%20detectors%20and%20trackers%20across%20diverse%20baselines.%20It%20achieves%20state-of-the-art%20tracking%20results%20with%2046.0%25%20AMOTA%20on%20the%20test%20set%20when%20paired%20with%20the%20DETR3D%20detector.%20In%20an%20object-centric%20E2E%20AD%20method%2C%20HAT%20enhances%20perception%20accuracy%20%28%2B1.3%25%20mAP%2C%20%2B3.1%25%20AMOTA%29%20and%20reduces%20the%20collision%20rate%20by%2032%25.%20When%20semantics%20are%20corrupted%20%28nuScenes-C%29%2C%20the%20enhancement%20of%20motion%20modeling%20by%20HAT%20enables%20more%20robust%20perception%20and%20planning%20in%20the%20E2E%20AD.&entry.1838667208=http%3A//arxiv.org/abs/2512.23635v1&entry.124074799=Read"},
{"title": "Task-driven Heterophilic Graph Structure Learning", "author": "Ayushman Raghuvanshi and Gonzalo Mateos and Sundeep Prabhakar Chepuri", "abstract": "Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.", "link": "http://arxiv.org/abs/2512.23406v1", "date": "2025-12-29", "relevancy": 2.395, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5029}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-driven%20Heterophilic%20Graph%20Structure%20Learning&body=Title%3A%20Task-driven%20Heterophilic%20Graph%20Structure%20Learning%0AAuthor%3A%20Ayushman%20Raghuvanshi%20and%20Gonzalo%20Mateos%20and%20Sundeep%20Prabhakar%20Chepuri%0AAbstract%3A%20Graph%20neural%20networks%20%28GNNs%29%20often%20struggle%20to%20learn%20discriminative%20node%20representations%20for%20heterophilic%20graphs%2C%20where%20connected%20nodes%20tend%20to%20have%20dissimilar%20labels%20and%20feature%20similarity%20provides%20weak%20structural%20cues.%20We%20propose%20frequency-guided%20graph%20structure%20learning%20%28FgGSL%29%2C%20an%20end-to-end%20graph%20inference%20framework%20that%20jointly%20learns%20homophilic%20and%20heterophilic%20graph%20structures%20along%20with%20a%20spectral%20encoder.%20FgGSL%20employs%20a%20learnable%2C%20symmetric%2C%20feature-driven%20masking%20function%20to%20infer%20said%20complementary%20graphs%2C%20which%20are%20processed%20using%20pre-designed%20low-%20and%20high-pass%20graph%20filter%20banks.%20A%20label-based%20structural%20loss%20explicitly%20promotes%20the%20recovery%20of%20homophilic%20and%20heterophilic%20edges%2C%20enabling%20task-driven%20graph%20structure%20learning.%20We%20derive%20stability%20bounds%20for%20the%20structural%20loss%20and%20establish%20robustness%20guarantees%20for%20the%20filter%20banks%20under%20graph%20perturbations.%20Experiments%20on%20six%20heterophilic%20benchmarks%20demonstrate%20that%20FgGSL%20consistently%20outperforms%20state-of-the-art%20GNNs%20and%20graph%20rewiring%20methods%2C%20highlighting%20the%20benefits%20of%20combining%20frequency%20information%20with%20supervised%20topology%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-driven%2520Heterophilic%2520Graph%2520Structure%2520Learning%26entry.906535625%3DAyushman%2520Raghuvanshi%2520and%2520Gonzalo%2520Mateos%2520and%2520Sundeep%2520Prabhakar%2520Chepuri%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNNs%2529%2520often%2520struggle%2520to%2520learn%2520discriminative%2520node%2520representations%2520for%2520heterophilic%2520graphs%252C%2520where%2520connected%2520nodes%2520tend%2520to%2520have%2520dissimilar%2520labels%2520and%2520feature%2520similarity%2520provides%2520weak%2520structural%2520cues.%2520We%2520propose%2520frequency-guided%2520graph%2520structure%2520learning%2520%2528FgGSL%2529%252C%2520an%2520end-to-end%2520graph%2520inference%2520framework%2520that%2520jointly%2520learns%2520homophilic%2520and%2520heterophilic%2520graph%2520structures%2520along%2520with%2520a%2520spectral%2520encoder.%2520FgGSL%2520employs%2520a%2520learnable%252C%2520symmetric%252C%2520feature-driven%2520masking%2520function%2520to%2520infer%2520said%2520complementary%2520graphs%252C%2520which%2520are%2520processed%2520using%2520pre-designed%2520low-%2520and%2520high-pass%2520graph%2520filter%2520banks.%2520A%2520label-based%2520structural%2520loss%2520explicitly%2520promotes%2520the%2520recovery%2520of%2520homophilic%2520and%2520heterophilic%2520edges%252C%2520enabling%2520task-driven%2520graph%2520structure%2520learning.%2520We%2520derive%2520stability%2520bounds%2520for%2520the%2520structural%2520loss%2520and%2520establish%2520robustness%2520guarantees%2520for%2520the%2520filter%2520banks%2520under%2520graph%2520perturbations.%2520Experiments%2520on%2520six%2520heterophilic%2520benchmarks%2520demonstrate%2520that%2520FgGSL%2520consistently%2520outperforms%2520state-of-the-art%2520GNNs%2520and%2520graph%2520rewiring%2520methods%252C%2520highlighting%2520the%2520benefits%2520of%2520combining%2520frequency%2520information%2520with%2520supervised%2520topology%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-driven%20Heterophilic%20Graph%20Structure%20Learning&entry.906535625=Ayushman%20Raghuvanshi%20and%20Gonzalo%20Mateos%20and%20Sundeep%20Prabhakar%20Chepuri&entry.1292438233=Graph%20neural%20networks%20%28GNNs%29%20often%20struggle%20to%20learn%20discriminative%20node%20representations%20for%20heterophilic%20graphs%2C%20where%20connected%20nodes%20tend%20to%20have%20dissimilar%20labels%20and%20feature%20similarity%20provides%20weak%20structural%20cues.%20We%20propose%20frequency-guided%20graph%20structure%20learning%20%28FgGSL%29%2C%20an%20end-to-end%20graph%20inference%20framework%20that%20jointly%20learns%20homophilic%20and%20heterophilic%20graph%20structures%20along%20with%20a%20spectral%20encoder.%20FgGSL%20employs%20a%20learnable%2C%20symmetric%2C%20feature-driven%20masking%20function%20to%20infer%20said%20complementary%20graphs%2C%20which%20are%20processed%20using%20pre-designed%20low-%20and%20high-pass%20graph%20filter%20banks.%20A%20label-based%20structural%20loss%20explicitly%20promotes%20the%20recovery%20of%20homophilic%20and%20heterophilic%20edges%2C%20enabling%20task-driven%20graph%20structure%20learning.%20We%20derive%20stability%20bounds%20for%20the%20structural%20loss%20and%20establish%20robustness%20guarantees%20for%20the%20filter%20banks%20under%20graph%20perturbations.%20Experiments%20on%20six%20heterophilic%20benchmarks%20demonstrate%20that%20FgGSL%20consistently%20outperforms%20state-of-the-art%20GNNs%20and%20graph%20rewiring%20methods%2C%20highlighting%20the%20benefits%20of%20combining%20frequency%20information%20with%20supervised%20topology%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2512.23406v1&entry.124074799=Read"},
{"title": "Detection Fire in Camera RGB-NIR", "author": "Nguyen Truong Khai and Luong Duc Vinh", "abstract": "Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.\n  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.", "link": "http://arxiv.org/abs/2512.23594v1", "date": "2025-12-29", "relevancy": 2.3874, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.48}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4774}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20Fire%20in%20Camera%20RGB-NIR&body=Title%3A%20Detection%20Fire%20in%20Camera%20RGB-NIR%0AAuthor%3A%20Nguyen%20Truong%20Khai%20and%20Luong%20Duc%20Vinh%0AAbstract%3A%20Improving%20the%20accuracy%20of%20fire%20detection%20using%20infrared%20night%20vision%20cameras%20remains%20a%20challenging%20task.%20Previous%20studies%20have%20reported%20strong%20performance%20with%20popular%20detection%20models.%20For%20example%2C%20YOLOv7%20achieved%20an%20mAP50-95%20of%200.51%20using%20an%20input%20image%20size%20of%20640%20x%201280%2C%20RT-DETR%20reached%20an%20mAP50-95%20of%200.65%20with%20an%20image%20size%20of%20640%20x%20640%2C%20and%20YOLOv9%20obtained%20an%20mAP50-95%20of%200.598%20at%20the%20same%20resolution.%20Despite%20these%20results%2C%20limitations%20in%20dataset%20construction%20continue%20to%20cause%20issues%2C%20particularly%20the%20frequent%20misclassification%20of%20bright%20artificial%20lights%20as%20fire.%0A%20%20This%20report%20presents%20three%20main%20contributions%3A%20an%20additional%20NIR%20dataset%2C%20a%20two-stage%20detection%20model%2C%20and%20Patched-YOLO.%20First%2C%20to%20address%20data%20scarcity%2C%20we%20explore%20and%20apply%20various%20data%20augmentation%20strategies%20for%20both%20the%20NIR%20dataset%20and%20the%20classification%20dataset.%20Second%2C%20to%20improve%20night-time%20fire%20detection%20accuracy%20while%20reducing%20false%20positives%20caused%20by%20artificial%20lights%2C%20we%20propose%20a%20two-stage%20pipeline%20combining%20YOLOv11%20and%20EfficientNetV2-B0.%20The%20proposed%20approach%20achieves%20higher%20detection%20accuracy%20compared%20to%20previous%20methods%2C%20particularly%20for%20night-time%20fire%20detection.%20Third%2C%20to%20improve%20fire%20detection%20in%20RGB%20images%2C%20especially%20for%20small%20and%20distant%20objects%2C%20we%20introduce%20Patched-YOLO%2C%20which%20enhances%20the%20model%27s%20detection%20capability%20through%20patch-based%20processing.%20Further%20details%20of%20these%20contributions%20are%20discussed%20in%20the%20following%20sections.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520Fire%2520in%2520Camera%2520RGB-NIR%26entry.906535625%3DNguyen%2520Truong%2520Khai%2520and%2520Luong%2520Duc%2520Vinh%26entry.1292438233%3DImproving%2520the%2520accuracy%2520of%2520fire%2520detection%2520using%2520infrared%2520night%2520vision%2520cameras%2520remains%2520a%2520challenging%2520task.%2520Previous%2520studies%2520have%2520reported%2520strong%2520performance%2520with%2520popular%2520detection%2520models.%2520For%2520example%252C%2520YOLOv7%2520achieved%2520an%2520mAP50-95%2520of%25200.51%2520using%2520an%2520input%2520image%2520size%2520of%2520640%2520x%25201280%252C%2520RT-DETR%2520reached%2520an%2520mAP50-95%2520of%25200.65%2520with%2520an%2520image%2520size%2520of%2520640%2520x%2520640%252C%2520and%2520YOLOv9%2520obtained%2520an%2520mAP50-95%2520of%25200.598%2520at%2520the%2520same%2520resolution.%2520Despite%2520these%2520results%252C%2520limitations%2520in%2520dataset%2520construction%2520continue%2520to%2520cause%2520issues%252C%2520particularly%2520the%2520frequent%2520misclassification%2520of%2520bright%2520artificial%2520lights%2520as%2520fire.%250A%2520%2520This%2520report%2520presents%2520three%2520main%2520contributions%253A%2520an%2520additional%2520NIR%2520dataset%252C%2520a%2520two-stage%2520detection%2520model%252C%2520and%2520Patched-YOLO.%2520First%252C%2520to%2520address%2520data%2520scarcity%252C%2520we%2520explore%2520and%2520apply%2520various%2520data%2520augmentation%2520strategies%2520for%2520both%2520the%2520NIR%2520dataset%2520and%2520the%2520classification%2520dataset.%2520Second%252C%2520to%2520improve%2520night-time%2520fire%2520detection%2520accuracy%2520while%2520reducing%2520false%2520positives%2520caused%2520by%2520artificial%2520lights%252C%2520we%2520propose%2520a%2520two-stage%2520pipeline%2520combining%2520YOLOv11%2520and%2520EfficientNetV2-B0.%2520The%2520proposed%2520approach%2520achieves%2520higher%2520detection%2520accuracy%2520compared%2520to%2520previous%2520methods%252C%2520particularly%2520for%2520night-time%2520fire%2520detection.%2520Third%252C%2520to%2520improve%2520fire%2520detection%2520in%2520RGB%2520images%252C%2520especially%2520for%2520small%2520and%2520distant%2520objects%252C%2520we%2520introduce%2520Patched-YOLO%252C%2520which%2520enhances%2520the%2520model%2527s%2520detection%2520capability%2520through%2520patch-based%2520processing.%2520Further%2520details%2520of%2520these%2520contributions%2520are%2520discussed%2520in%2520the%2520following%2520sections.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20Fire%20in%20Camera%20RGB-NIR&entry.906535625=Nguyen%20Truong%20Khai%20and%20Luong%20Duc%20Vinh&entry.1292438233=Improving%20the%20accuracy%20of%20fire%20detection%20using%20infrared%20night%20vision%20cameras%20remains%20a%20challenging%20task.%20Previous%20studies%20have%20reported%20strong%20performance%20with%20popular%20detection%20models.%20For%20example%2C%20YOLOv7%20achieved%20an%20mAP50-95%20of%200.51%20using%20an%20input%20image%20size%20of%20640%20x%201280%2C%20RT-DETR%20reached%20an%20mAP50-95%20of%200.65%20with%20an%20image%20size%20of%20640%20x%20640%2C%20and%20YOLOv9%20obtained%20an%20mAP50-95%20of%200.598%20at%20the%20same%20resolution.%20Despite%20these%20results%2C%20limitations%20in%20dataset%20construction%20continue%20to%20cause%20issues%2C%20particularly%20the%20frequent%20misclassification%20of%20bright%20artificial%20lights%20as%20fire.%0A%20%20This%20report%20presents%20three%20main%20contributions%3A%20an%20additional%20NIR%20dataset%2C%20a%20two-stage%20detection%20model%2C%20and%20Patched-YOLO.%20First%2C%20to%20address%20data%20scarcity%2C%20we%20explore%20and%20apply%20various%20data%20augmentation%20strategies%20for%20both%20the%20NIR%20dataset%20and%20the%20classification%20dataset.%20Second%2C%20to%20improve%20night-time%20fire%20detection%20accuracy%20while%20reducing%20false%20positives%20caused%20by%20artificial%20lights%2C%20we%20propose%20a%20two-stage%20pipeline%20combining%20YOLOv11%20and%20EfficientNetV2-B0.%20The%20proposed%20approach%20achieves%20higher%20detection%20accuracy%20compared%20to%20previous%20methods%2C%20particularly%20for%20night-time%20fire%20detection.%20Third%2C%20to%20improve%20fire%20detection%20in%20RGB%20images%2C%20especially%20for%20small%20and%20distant%20objects%2C%20we%20introduce%20Patched-YOLO%2C%20which%20enhances%20the%20model%27s%20detection%20capability%20through%20patch-based%20processing.%20Further%20details%20of%20these%20contributions%20are%20discussed%20in%20the%20following%20sections.&entry.1838667208=http%3A//arxiv.org/abs/2512.23594v1&entry.124074799=Read"},
{"title": "SoulX-LiveTalk Technical Report", "author": "Le Shen and Qiao Qian and Tan Yu and Ke Zhou and Tianhang Yu and Yu Zhan and Zhenjie Wang and Ming Tao and Shunshun Yin and Siyuan Liu", "abstract": "Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \\textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \\textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \\textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \\textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \\textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.", "link": "http://arxiv.org/abs/2512.23379v1", "date": "2025-12-29", "relevancy": 2.3771, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6195}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5779}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoulX-LiveTalk%20Technical%20Report&body=Title%3A%20SoulX-LiveTalk%20Technical%20Report%0AAuthor%3A%20Le%20Shen%20and%20Qiao%20Qian%20and%20Tan%20Yu%20and%20Ke%20Zhou%20and%20Tianhang%20Yu%20and%20Yu%20Zhan%20and%20Zhenjie%20Wang%20and%20Ming%20Tao%20and%20Shunshun%20Yin%20and%20Siyuan%20Liu%0AAbstract%3A%20Deploying%20massive%20diffusion%20models%20for%20real-time%2C%20infinite-duration%2C%20audio-driven%20avatar%20generation%20presents%20a%20significant%20engineering%20challenge%2C%20primarily%20due%20to%20the%20conflict%20between%20computational%20load%20and%20strict%20latency%20constraints.%20Existing%20approaches%20often%20compromise%20visual%20fidelity%20by%20enforcing%20strictly%20unidirectional%20attention%20mechanisms%20or%20reducing%20model%20capacity.%20To%20address%20this%20problem%2C%20we%20introduce%20%5Ctextbf%7BSoulX-LiveTalk%7D%2C%20a%2014B-parameter%20framework%20optimized%20for%20high-fidelity%20real-time%20streaming.%20Diverging%20from%20conventional%20unidirectional%20paradigms%2C%20we%20use%20a%20%5Ctextbf%7BSelf-correcting%20Bidirectional%20Distillation%7D%20strategy%20that%20retains%20bidirectional%20attention%20within%20video%20chunks.%20This%20design%20preserves%20critical%20spatiotemporal%20correlations%2C%20significantly%20enhancing%20motion%20coherence%20and%20visual%20detail.%20To%20ensure%20stability%20during%20infinite%20generation%2C%20we%20incorporate%20a%20%5Ctextbf%7BMulti-step%20Retrospective%20Self-Correction%20Mechanism%7D%2C%20enabling%20the%20model%20to%20autonomously%20recover%20from%20accumulated%20errors%20and%20preventing%20collapse.%20Furthermore%2C%20we%20engineered%20a%20full-stack%20inference%20acceleration%20suite%20incorporating%20hybrid%20sequence%20parallelism%2C%20Parallel%20VAE%2C%20and%20kernel-level%20optimizations.%20Extensive%20evaluations%20confirm%20that%20SoulX-LiveTalk%20is%20the%20first%2014B-scale%20system%20to%20achieve%20a%20%5Ctextbf%7Bsub-second%20start-up%20latency%20%280.87s%29%7D%20while%20reaching%20a%20real-time%20throughput%20of%20%5Ctextbf%7B32%20FPS%7D%2C%20setting%20a%20new%20standard%20for%20high-fidelity%20interactive%20digital%20human%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoulX-LiveTalk%2520Technical%2520Report%26entry.906535625%3DLe%2520Shen%2520and%2520Qiao%2520Qian%2520and%2520Tan%2520Yu%2520and%2520Ke%2520Zhou%2520and%2520Tianhang%2520Yu%2520and%2520Yu%2520Zhan%2520and%2520Zhenjie%2520Wang%2520and%2520Ming%2520Tao%2520and%2520Shunshun%2520Yin%2520and%2520Siyuan%2520Liu%26entry.1292438233%3DDeploying%2520massive%2520diffusion%2520models%2520for%2520real-time%252C%2520infinite-duration%252C%2520audio-driven%2520avatar%2520generation%2520presents%2520a%2520significant%2520engineering%2520challenge%252C%2520primarily%2520due%2520to%2520the%2520conflict%2520between%2520computational%2520load%2520and%2520strict%2520latency%2520constraints.%2520Existing%2520approaches%2520often%2520compromise%2520visual%2520fidelity%2520by%2520enforcing%2520strictly%2520unidirectional%2520attention%2520mechanisms%2520or%2520reducing%2520model%2520capacity.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520%255Ctextbf%257BSoulX-LiveTalk%257D%252C%2520a%252014B-parameter%2520framework%2520optimized%2520for%2520high-fidelity%2520real-time%2520streaming.%2520Diverging%2520from%2520conventional%2520unidirectional%2520paradigms%252C%2520we%2520use%2520a%2520%255Ctextbf%257BSelf-correcting%2520Bidirectional%2520Distillation%257D%2520strategy%2520that%2520retains%2520bidirectional%2520attention%2520within%2520video%2520chunks.%2520This%2520design%2520preserves%2520critical%2520spatiotemporal%2520correlations%252C%2520significantly%2520enhancing%2520motion%2520coherence%2520and%2520visual%2520detail.%2520To%2520ensure%2520stability%2520during%2520infinite%2520generation%252C%2520we%2520incorporate%2520a%2520%255Ctextbf%257BMulti-step%2520Retrospective%2520Self-Correction%2520Mechanism%257D%252C%2520enabling%2520the%2520model%2520to%2520autonomously%2520recover%2520from%2520accumulated%2520errors%2520and%2520preventing%2520collapse.%2520Furthermore%252C%2520we%2520engineered%2520a%2520full-stack%2520inference%2520acceleration%2520suite%2520incorporating%2520hybrid%2520sequence%2520parallelism%252C%2520Parallel%2520VAE%252C%2520and%2520kernel-level%2520optimizations.%2520Extensive%2520evaluations%2520confirm%2520that%2520SoulX-LiveTalk%2520is%2520the%2520first%252014B-scale%2520system%2520to%2520achieve%2520a%2520%255Ctextbf%257Bsub-second%2520start-up%2520latency%2520%25280.87s%2529%257D%2520while%2520reaching%2520a%2520real-time%2520throughput%2520of%2520%255Ctextbf%257B32%2520FPS%257D%252C%2520setting%2520a%2520new%2520standard%2520for%2520high-fidelity%2520interactive%2520digital%2520human%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoulX-LiveTalk%20Technical%20Report&entry.906535625=Le%20Shen%20and%20Qiao%20Qian%20and%20Tan%20Yu%20and%20Ke%20Zhou%20and%20Tianhang%20Yu%20and%20Yu%20Zhan%20and%20Zhenjie%20Wang%20and%20Ming%20Tao%20and%20Shunshun%20Yin%20and%20Siyuan%20Liu&entry.1292438233=Deploying%20massive%20diffusion%20models%20for%20real-time%2C%20infinite-duration%2C%20audio-driven%20avatar%20generation%20presents%20a%20significant%20engineering%20challenge%2C%20primarily%20due%20to%20the%20conflict%20between%20computational%20load%20and%20strict%20latency%20constraints.%20Existing%20approaches%20often%20compromise%20visual%20fidelity%20by%20enforcing%20strictly%20unidirectional%20attention%20mechanisms%20or%20reducing%20model%20capacity.%20To%20address%20this%20problem%2C%20we%20introduce%20%5Ctextbf%7BSoulX-LiveTalk%7D%2C%20a%2014B-parameter%20framework%20optimized%20for%20high-fidelity%20real-time%20streaming.%20Diverging%20from%20conventional%20unidirectional%20paradigms%2C%20we%20use%20a%20%5Ctextbf%7BSelf-correcting%20Bidirectional%20Distillation%7D%20strategy%20that%20retains%20bidirectional%20attention%20within%20video%20chunks.%20This%20design%20preserves%20critical%20spatiotemporal%20correlations%2C%20significantly%20enhancing%20motion%20coherence%20and%20visual%20detail.%20To%20ensure%20stability%20during%20infinite%20generation%2C%20we%20incorporate%20a%20%5Ctextbf%7BMulti-step%20Retrospective%20Self-Correction%20Mechanism%7D%2C%20enabling%20the%20model%20to%20autonomously%20recover%20from%20accumulated%20errors%20and%20preventing%20collapse.%20Furthermore%2C%20we%20engineered%20a%20full-stack%20inference%20acceleration%20suite%20incorporating%20hybrid%20sequence%20parallelism%2C%20Parallel%20VAE%2C%20and%20kernel-level%20optimizations.%20Extensive%20evaluations%20confirm%20that%20SoulX-LiveTalk%20is%20the%20first%2014B-scale%20system%20to%20achieve%20a%20%5Ctextbf%7Bsub-second%20start-up%20latency%20%280.87s%29%7D%20while%20reaching%20a%20real-time%20throughput%20of%20%5Ctextbf%7B32%20FPS%7D%2C%20setting%20a%20new%20standard%20for%20high-fidelity%20interactive%20digital%20human%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2512.23379v1&entry.124074799=Read"},
{"title": "SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context", "author": "Shuyuan Lin and Hailiang Liao and Qiang Qi and Junjie Huang and Taotao Lai and Jian Weng", "abstract": "Recent research has focused on using convolutional neural networks (CNNs) as the backbones in two-view correspondence learning, demonstrating significant superiority over methods based on multilayer perceptrons. However, CNN backbones that are not tailored to specific tasks may fail to effectively aggregate global context and oversmooth dense motion fields in scenes with large disparity. To address these problems, we propose a novel network named SC-Net, which effectively integrates bilateral context from both spatial and channel perspectives. Specifically, we design an adaptive focused regularization module (AFR) to enhance the model's position-awareness and robustness against spurious motion samples, thereby facilitating the generation of a more accurate motion field. We then propose a bilateral field adjustment module (BFA) to refine the motion field by simultaneously modeling long-range relationships and facilitating interaction across spatial and channel dimensions. Finally, we recover the motion vectors from the refined field using a position-aware recovery module (PAR) that ensures consistency and precision. Extensive experiments demonstrate that SC-Net outperforms state-of-the-art methods in relative pose estimation and outlier removal tasks on YFCC100M and SUN3D datasets. Source code is available at http://www.linshuyuan.com.", "link": "http://arxiv.org/abs/2512.23473v1", "date": "2025-12-29", "relevancy": 2.3476, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6204}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SC-Net%3A%20Robust%20Correspondence%20Learning%20via%20Spatial%20and%20Cross-Channel%20Context&body=Title%3A%20SC-Net%3A%20Robust%20Correspondence%20Learning%20via%20Spatial%20and%20Cross-Channel%20Context%0AAuthor%3A%20Shuyuan%20Lin%20and%20Hailiang%20Liao%20and%20Qiang%20Qi%20and%20Junjie%20Huang%20and%20Taotao%20Lai%20and%20Jian%20Weng%0AAbstract%3A%20Recent%20research%20has%20focused%20on%20using%20convolutional%20neural%20networks%20%28CNNs%29%20as%20the%20backbones%20in%20two-view%20correspondence%20learning%2C%20demonstrating%20significant%20superiority%20over%20methods%20based%20on%20multilayer%20perceptrons.%20However%2C%20CNN%20backbones%20that%20are%20not%20tailored%20to%20specific%20tasks%20may%20fail%20to%20effectively%20aggregate%20global%20context%20and%20oversmooth%20dense%20motion%20fields%20in%20scenes%20with%20large%20disparity.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20network%20named%20SC-Net%2C%20which%20effectively%20integrates%20bilateral%20context%20from%20both%20spatial%20and%20channel%20perspectives.%20Specifically%2C%20we%20design%20an%20adaptive%20focused%20regularization%20module%20%28AFR%29%20to%20enhance%20the%20model%27s%20position-awareness%20and%20robustness%20against%20spurious%20motion%20samples%2C%20thereby%20facilitating%20the%20generation%20of%20a%20more%20accurate%20motion%20field.%20We%20then%20propose%20a%20bilateral%20field%20adjustment%20module%20%28BFA%29%20to%20refine%20the%20motion%20field%20by%20simultaneously%20modeling%20long-range%20relationships%20and%20facilitating%20interaction%20across%20spatial%20and%20channel%20dimensions.%20Finally%2C%20we%20recover%20the%20motion%20vectors%20from%20the%20refined%20field%20using%20a%20position-aware%20recovery%20module%20%28PAR%29%20that%20ensures%20consistency%20and%20precision.%20Extensive%20experiments%20demonstrate%20that%20SC-Net%20outperforms%20state-of-the-art%20methods%20in%20relative%20pose%20estimation%20and%20outlier%20removal%20tasks%20on%20YFCC100M%20and%20SUN3D%20datasets.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSC-Net%253A%2520Robust%2520Correspondence%2520Learning%2520via%2520Spatial%2520and%2520Cross-Channel%2520Context%26entry.906535625%3DShuyuan%2520Lin%2520and%2520Hailiang%2520Liao%2520and%2520Qiang%2520Qi%2520and%2520Junjie%2520Huang%2520and%2520Taotao%2520Lai%2520and%2520Jian%2520Weng%26entry.1292438233%3DRecent%2520research%2520has%2520focused%2520on%2520using%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520as%2520the%2520backbones%2520in%2520two-view%2520correspondence%2520learning%252C%2520demonstrating%2520significant%2520superiority%2520over%2520methods%2520based%2520on%2520multilayer%2520perceptrons.%2520However%252C%2520CNN%2520backbones%2520that%2520are%2520not%2520tailored%2520to%2520specific%2520tasks%2520may%2520fail%2520to%2520effectively%2520aggregate%2520global%2520context%2520and%2520oversmooth%2520dense%2520motion%2520fields%2520in%2520scenes%2520with%2520large%2520disparity.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520network%2520named%2520SC-Net%252C%2520which%2520effectively%2520integrates%2520bilateral%2520context%2520from%2520both%2520spatial%2520and%2520channel%2520perspectives.%2520Specifically%252C%2520we%2520design%2520an%2520adaptive%2520focused%2520regularization%2520module%2520%2528AFR%2529%2520to%2520enhance%2520the%2520model%2527s%2520position-awareness%2520and%2520robustness%2520against%2520spurious%2520motion%2520samples%252C%2520thereby%2520facilitating%2520the%2520generation%2520of%2520a%2520more%2520accurate%2520motion%2520field.%2520We%2520then%2520propose%2520a%2520bilateral%2520field%2520adjustment%2520module%2520%2528BFA%2529%2520to%2520refine%2520the%2520motion%2520field%2520by%2520simultaneously%2520modeling%2520long-range%2520relationships%2520and%2520facilitating%2520interaction%2520across%2520spatial%2520and%2520channel%2520dimensions.%2520Finally%252C%2520we%2520recover%2520the%2520motion%2520vectors%2520from%2520the%2520refined%2520field%2520using%2520a%2520position-aware%2520recovery%2520module%2520%2528PAR%2529%2520that%2520ensures%2520consistency%2520and%2520precision.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SC-Net%2520outperforms%2520state-of-the-art%2520methods%2520in%2520relative%2520pose%2520estimation%2520and%2520outlier%2520removal%2520tasks%2520on%2520YFCC100M%2520and%2520SUN3D%2520datasets.%2520Source%2520code%2520is%2520available%2520at%2520http%253A//www.linshuyuan.com.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SC-Net%3A%20Robust%20Correspondence%20Learning%20via%20Spatial%20and%20Cross-Channel%20Context&entry.906535625=Shuyuan%20Lin%20and%20Hailiang%20Liao%20and%20Qiang%20Qi%20and%20Junjie%20Huang%20and%20Taotao%20Lai%20and%20Jian%20Weng&entry.1292438233=Recent%20research%20has%20focused%20on%20using%20convolutional%20neural%20networks%20%28CNNs%29%20as%20the%20backbones%20in%20two-view%20correspondence%20learning%2C%20demonstrating%20significant%20superiority%20over%20methods%20based%20on%20multilayer%20perceptrons.%20However%2C%20CNN%20backbones%20that%20are%20not%20tailored%20to%20specific%20tasks%20may%20fail%20to%20effectively%20aggregate%20global%20context%20and%20oversmooth%20dense%20motion%20fields%20in%20scenes%20with%20large%20disparity.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20network%20named%20SC-Net%2C%20which%20effectively%20integrates%20bilateral%20context%20from%20both%20spatial%20and%20channel%20perspectives.%20Specifically%2C%20we%20design%20an%20adaptive%20focused%20regularization%20module%20%28AFR%29%20to%20enhance%20the%20model%27s%20position-awareness%20and%20robustness%20against%20spurious%20motion%20samples%2C%20thereby%20facilitating%20the%20generation%20of%20a%20more%20accurate%20motion%20field.%20We%20then%20propose%20a%20bilateral%20field%20adjustment%20module%20%28BFA%29%20to%20refine%20the%20motion%20field%20by%20simultaneously%20modeling%20long-range%20relationships%20and%20facilitating%20interaction%20across%20spatial%20and%20channel%20dimensions.%20Finally%2C%20we%20recover%20the%20motion%20vectors%20from%20the%20refined%20field%20using%20a%20position-aware%20recovery%20module%20%28PAR%29%20that%20ensures%20consistency%20and%20precision.%20Extensive%20experiments%20demonstrate%20that%20SC-Net%20outperforms%20state-of-the-art%20methods%20in%20relative%20pose%20estimation%20and%20outlier%20removal%20tasks%20on%20YFCC100M%20and%20SUN3D%20datasets.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.&entry.1838667208=http%3A//arxiv.org/abs/2512.23473v1&entry.124074799=Read"},
{"title": "IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation", "author": "Donghao Zhou and Jingyu Lin and Guibao Shen and Quande Liu and Jialin Gao and Lihao Liu and Lan Du and Cunjian Chen and Chi-Wing Fu and Xiaowei Hu and Pheng-Ann Heng", "abstract": "Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.", "link": "http://arxiv.org/abs/2512.23519v1", "date": "2025-12-29", "relevancy": 2.3465, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6422}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5547}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IdentityStory%3A%20Taming%20Your%20Identity-Preserving%20Generator%20for%20Human-Centric%20Story%20Generation&body=Title%3A%20IdentityStory%3A%20Taming%20Your%20Identity-Preserving%20Generator%20for%20Human-Centric%20Story%20Generation%0AAuthor%3A%20Donghao%20Zhou%20and%20Jingyu%20Lin%20and%20Guibao%20Shen%20and%20Quande%20Liu%20and%20Jialin%20Gao%20and%20Lihao%20Liu%20and%20Lan%20Du%20and%20Cunjian%20Chen%20and%20Chi-Wing%20Fu%20and%20Xiaowei%20Hu%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20Recent%20visual%20generative%20models%20enable%20story%20generation%20with%20consistent%20characters%20from%20text%2C%20but%20human-centric%20story%20generation%20faces%20additional%20challenges%2C%20such%20as%20maintaining%20detailed%20and%20diverse%20human%20face%20consistency%20and%20coordinating%20multiple%20characters%20across%20different%20images.%20This%20paper%20presents%20IdentityStory%2C%20a%20framework%20for%20human-centric%20story%20generation%20that%20ensures%20consistent%20character%20identity%20across%20multiple%20sequential%20images.%20By%20taming%20identity-preserving%20generators%2C%20the%20framework%20features%20two%20key%20components%3A%20Iterative%20Identity%20Discovery%2C%20which%20extracts%20cohesive%20character%20identities%2C%20and%20Re-denoising%20Identity%20Injection%2C%20which%20re-denoises%20images%20to%20inject%20identities%20while%20preserving%20desired%20context.%20Experiments%20on%20the%20ConsiStory-Human%20benchmark%20demonstrate%20that%20IdentityStory%20outperforms%20existing%20methods%2C%20particularly%20in%20face%20consistency%2C%20and%20supports%20multi-character%20combinations.%20The%20framework%20also%20shows%20strong%20potential%20for%20applications%20such%20as%20infinite-length%20story%20generation%20and%20dynamic%20character%20composition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentityStory%253A%2520Taming%2520Your%2520Identity-Preserving%2520Generator%2520for%2520Human-Centric%2520Story%2520Generation%26entry.906535625%3DDonghao%2520Zhou%2520and%2520Jingyu%2520Lin%2520and%2520Guibao%2520Shen%2520and%2520Quande%2520Liu%2520and%2520Jialin%2520Gao%2520and%2520Lihao%2520Liu%2520and%2520Lan%2520Du%2520and%2520Cunjian%2520Chen%2520and%2520Chi-Wing%2520Fu%2520and%2520Xiaowei%2520Hu%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3DRecent%2520visual%2520generative%2520models%2520enable%2520story%2520generation%2520with%2520consistent%2520characters%2520from%2520text%252C%2520but%2520human-centric%2520story%2520generation%2520faces%2520additional%2520challenges%252C%2520such%2520as%2520maintaining%2520detailed%2520and%2520diverse%2520human%2520face%2520consistency%2520and%2520coordinating%2520multiple%2520characters%2520across%2520different%2520images.%2520This%2520paper%2520presents%2520IdentityStory%252C%2520a%2520framework%2520for%2520human-centric%2520story%2520generation%2520that%2520ensures%2520consistent%2520character%2520identity%2520across%2520multiple%2520sequential%2520images.%2520By%2520taming%2520identity-preserving%2520generators%252C%2520the%2520framework%2520features%2520two%2520key%2520components%253A%2520Iterative%2520Identity%2520Discovery%252C%2520which%2520extracts%2520cohesive%2520character%2520identities%252C%2520and%2520Re-denoising%2520Identity%2520Injection%252C%2520which%2520re-denoises%2520images%2520to%2520inject%2520identities%2520while%2520preserving%2520desired%2520context.%2520Experiments%2520on%2520the%2520ConsiStory-Human%2520benchmark%2520demonstrate%2520that%2520IdentityStory%2520outperforms%2520existing%2520methods%252C%2520particularly%2520in%2520face%2520consistency%252C%2520and%2520supports%2520multi-character%2520combinations.%2520The%2520framework%2520also%2520shows%2520strong%2520potential%2520for%2520applications%2520such%2520as%2520infinite-length%2520story%2520generation%2520and%2520dynamic%2520character%2520composition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IdentityStory%3A%20Taming%20Your%20Identity-Preserving%20Generator%20for%20Human-Centric%20Story%20Generation&entry.906535625=Donghao%20Zhou%20and%20Jingyu%20Lin%20and%20Guibao%20Shen%20and%20Quande%20Liu%20and%20Jialin%20Gao%20and%20Lihao%20Liu%20and%20Lan%20Du%20and%20Cunjian%20Chen%20and%20Chi-Wing%20Fu%20and%20Xiaowei%20Hu%20and%20Pheng-Ann%20Heng&entry.1292438233=Recent%20visual%20generative%20models%20enable%20story%20generation%20with%20consistent%20characters%20from%20text%2C%20but%20human-centric%20story%20generation%20faces%20additional%20challenges%2C%20such%20as%20maintaining%20detailed%20and%20diverse%20human%20face%20consistency%20and%20coordinating%20multiple%20characters%20across%20different%20images.%20This%20paper%20presents%20IdentityStory%2C%20a%20framework%20for%20human-centric%20story%20generation%20that%20ensures%20consistent%20character%20identity%20across%20multiple%20sequential%20images.%20By%20taming%20identity-preserving%20generators%2C%20the%20framework%20features%20two%20key%20components%3A%20Iterative%20Identity%20Discovery%2C%20which%20extracts%20cohesive%20character%20identities%2C%20and%20Re-denoising%20Identity%20Injection%2C%20which%20re-denoises%20images%20to%20inject%20identities%20while%20preserving%20desired%20context.%20Experiments%20on%20the%20ConsiStory-Human%20benchmark%20demonstrate%20that%20IdentityStory%20outperforms%20existing%20methods%2C%20particularly%20in%20face%20consistency%2C%20and%20supports%20multi-character%20combinations.%20The%20framework%20also%20shows%20strong%20potential%20for%20applications%20such%20as%20infinite-length%20story%20generation%20and%20dynamic%20character%20composition.&entry.1838667208=http%3A//arxiv.org/abs/2512.23519v1&entry.124074799=Read"},
{"title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving", "author": "Zhenguo Zhang and Haohan Zheng and Yishen Wang and Le Xu and Tianchen Deng and Xuefeng Chen and Qu Chen and Bo Zhang and Wuxiong Huang", "abstract": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning. While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels. Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.", "link": "http://arxiv.org/abs/2512.14044v2", "date": "2025-12-29", "relevancy": 2.3332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDrive-R1%3A%20Reinforcement-driven%20Interleaved%20Multi-modal%20Chain-of-Thought%20for%20Trustworthy%20Vision-Language%20Autonomous%20Driving&body=Title%3A%20OmniDrive-R1%3A%20Reinforcement-driven%20Interleaved%20Multi-modal%20Chain-of-Thought%20for%20Trustworthy%20Vision-Language%20Autonomous%20Driving%0AAuthor%3A%20Zhenguo%20Zhang%20and%20Haohan%20Zheng%20and%20Yishen%20Wang%20and%20Le%20Xu%20and%20Tianchen%20Deng%20and%20Xuefeng%20Chen%20and%20Qu%20Chen%20and%20Bo%20Zhang%20and%20Wuxiong%20Huang%0AAbstract%3A%20The%20deployment%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20safety-critical%20domains%20like%20autonomous%20driving%20%28AD%29%20is%20critically%20hindered%20by%20reliability%20failures%2C%20most%20notably%20object%20hallucination.%20This%20failure%20stems%20from%20their%20reliance%20on%20ungrounded%2C%20text-based%20Chain-of-Thought%20%28CoT%29%20reasoning.%20While%20existing%20multi-modal%20CoT%20approaches%20attempt%20mitigation%2C%20they%20suffer%20from%20two%20fundamental%20flaws%3A%20%281%29%20decoupled%20perception%20and%20reasoning%20stages%20that%20prevent%20end-to-end%20joint%20optimization%2C%20and%20%282%29%20reliance%20on%20expensive%2C%20dense%20localization%20labels.%20Thus%20we%20introduce%20OmniDrive-R1%2C%20an%20end-to-end%20VLM%20framework%20designed%20for%20autonomous%20driving%2C%20which%20unifies%20perception%20and%20reasoning%20through%20an%20interleaved%20Multi-modal%20Chain-of-Thought%20%28iMCoT%29%20mechanism.%20Our%20core%20innovation%20is%20an%20Reinforcement-driven%20visual%20grounding%20capability%2C%20enabling%20the%20model%20to%20autonomously%20direct%20its%20attention%20and%20%22zoom%20in%22%20on%20critical%20regions%20for%20fine-grained%20analysis.%20This%20capability%20is%20enabled%20by%20our%20pure%20two-stage%20reinforcement%20learning%20training%20pipeline%20and%20Clip-GRPO%20algorithm.%20Crucially%2C%20Clip-GRPO%20introduces%20an%20annotation-free%2C%20process-based%20grounding%20reward.%20This%20reward%20not%20only%20eliminates%20the%20need%20for%20dense%20labels%20but%20also%20circumvents%20the%20instability%20of%20external%20tool%20calls%20by%20enforcing%20real-time%20cross-modal%20consistency%20between%20the%20visual%20focus%20and%20the%20textual%20reasoning.%20Extensive%20experiments%20on%20DriveLMM-o1%20demonstrate%20our%20model%27s%20significant%20improvements.%20Compared%20to%20the%20baseline%20Qwen2.5VL-7B%2C%20OmniDrive-R1%20improves%20the%20overall%20reasoning%20score%20from%2051.77%25%20to%2080.35%25%2C%20and%20the%20final%20answer%20accuracy%20from%2037.81%25%20to%2073.62%25.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDrive-R1%253A%2520Reinforcement-driven%2520Interleaved%2520Multi-modal%2520Chain-of-Thought%2520for%2520Trustworthy%2520Vision-Language%2520Autonomous%2520Driving%26entry.906535625%3DZhenguo%2520Zhang%2520and%2520Haohan%2520Zheng%2520and%2520Yishen%2520Wang%2520and%2520Le%2520Xu%2520and%2520Tianchen%2520Deng%2520and%2520Xuefeng%2520Chen%2520and%2520Qu%2520Chen%2520and%2520Bo%2520Zhang%2520and%2520Wuxiong%2520Huang%26entry.1292438233%3DThe%2520deployment%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520safety-critical%2520domains%2520like%2520autonomous%2520driving%2520%2528AD%2529%2520is%2520critically%2520hindered%2520by%2520reliability%2520failures%252C%2520most%2520notably%2520object%2520hallucination.%2520This%2520failure%2520stems%2520from%2520their%2520reliance%2520on%2520ungrounded%252C%2520text-based%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning.%2520While%2520existing%2520multi-modal%2520CoT%2520approaches%2520attempt%2520mitigation%252C%2520they%2520suffer%2520from%2520two%2520fundamental%2520flaws%253A%2520%25281%2529%2520decoupled%2520perception%2520and%2520reasoning%2520stages%2520that%2520prevent%2520end-to-end%2520joint%2520optimization%252C%2520and%2520%25282%2529%2520reliance%2520on%2520expensive%252C%2520dense%2520localization%2520labels.%2520Thus%2520we%2520introduce%2520OmniDrive-R1%252C%2520an%2520end-to-end%2520VLM%2520framework%2520designed%2520for%2520autonomous%2520driving%252C%2520which%2520unifies%2520perception%2520and%2520reasoning%2520through%2520an%2520interleaved%2520Multi-modal%2520Chain-of-Thought%2520%2528iMCoT%2529%2520mechanism.%2520Our%2520core%2520innovation%2520is%2520an%2520Reinforcement-driven%2520visual%2520grounding%2520capability%252C%2520enabling%2520the%2520model%2520to%2520autonomously%2520direct%2520its%2520attention%2520and%2520%2522zoom%2520in%2522%2520on%2520critical%2520regions%2520for%2520fine-grained%2520analysis.%2520This%2520capability%2520is%2520enabled%2520by%2520our%2520pure%2520two-stage%2520reinforcement%2520learning%2520training%2520pipeline%2520and%2520Clip-GRPO%2520algorithm.%2520Crucially%252C%2520Clip-GRPO%2520introduces%2520an%2520annotation-free%252C%2520process-based%2520grounding%2520reward.%2520This%2520reward%2520not%2520only%2520eliminates%2520the%2520need%2520for%2520dense%2520labels%2520but%2520also%2520circumvents%2520the%2520instability%2520of%2520external%2520tool%2520calls%2520by%2520enforcing%2520real-time%2520cross-modal%2520consistency%2520between%2520the%2520visual%2520focus%2520and%2520the%2520textual%2520reasoning.%2520Extensive%2520experiments%2520on%2520DriveLMM-o1%2520demonstrate%2520our%2520model%2527s%2520significant%2520improvements.%2520Compared%2520to%2520the%2520baseline%2520Qwen2.5VL-7B%252C%2520OmniDrive-R1%2520improves%2520the%2520overall%2520reasoning%2520score%2520from%252051.77%2525%2520to%252080.35%2525%252C%2520and%2520the%2520final%2520answer%2520accuracy%2520from%252037.81%2525%2520to%252073.62%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDrive-R1%3A%20Reinforcement-driven%20Interleaved%20Multi-modal%20Chain-of-Thought%20for%20Trustworthy%20Vision-Language%20Autonomous%20Driving&entry.906535625=Zhenguo%20Zhang%20and%20Haohan%20Zheng%20and%20Yishen%20Wang%20and%20Le%20Xu%20and%20Tianchen%20Deng%20and%20Xuefeng%20Chen%20and%20Qu%20Chen%20and%20Bo%20Zhang%20and%20Wuxiong%20Huang&entry.1292438233=The%20deployment%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20safety-critical%20domains%20like%20autonomous%20driving%20%28AD%29%20is%20critically%20hindered%20by%20reliability%20failures%2C%20most%20notably%20object%20hallucination.%20This%20failure%20stems%20from%20their%20reliance%20on%20ungrounded%2C%20text-based%20Chain-of-Thought%20%28CoT%29%20reasoning.%20While%20existing%20multi-modal%20CoT%20approaches%20attempt%20mitigation%2C%20they%20suffer%20from%20two%20fundamental%20flaws%3A%20%281%29%20decoupled%20perception%20and%20reasoning%20stages%20that%20prevent%20end-to-end%20joint%20optimization%2C%20and%20%282%29%20reliance%20on%20expensive%2C%20dense%20localization%20labels.%20Thus%20we%20introduce%20OmniDrive-R1%2C%20an%20end-to-end%20VLM%20framework%20designed%20for%20autonomous%20driving%2C%20which%20unifies%20perception%20and%20reasoning%20through%20an%20interleaved%20Multi-modal%20Chain-of-Thought%20%28iMCoT%29%20mechanism.%20Our%20core%20innovation%20is%20an%20Reinforcement-driven%20visual%20grounding%20capability%2C%20enabling%20the%20model%20to%20autonomously%20direct%20its%20attention%20and%20%22zoom%20in%22%20on%20critical%20regions%20for%20fine-grained%20analysis.%20This%20capability%20is%20enabled%20by%20our%20pure%20two-stage%20reinforcement%20learning%20training%20pipeline%20and%20Clip-GRPO%20algorithm.%20Crucially%2C%20Clip-GRPO%20introduces%20an%20annotation-free%2C%20process-based%20grounding%20reward.%20This%20reward%20not%20only%20eliminates%20the%20need%20for%20dense%20labels%20but%20also%20circumvents%20the%20instability%20of%20external%20tool%20calls%20by%20enforcing%20real-time%20cross-modal%20consistency%20between%20the%20visual%20focus%20and%20the%20textual%20reasoning.%20Extensive%20experiments%20on%20DriveLMM-o1%20demonstrate%20our%20model%27s%20significant%20improvements.%20Compared%20to%20the%20baseline%20Qwen2.5VL-7B%2C%20OmniDrive-R1%20improves%20the%20overall%20reasoning%20score%20from%2051.77%25%20to%2080.35%25%2C%20and%20the%20final%20answer%20accuracy%20from%2037.81%25%20to%2073.62%25.&entry.1838667208=http%3A//arxiv.org/abs/2512.14044v2&entry.124074799=Read"},
{"title": "Towards Integrating Uncertainty for Domain-Agnostic Segmentation", "author": "Jesse Brouwers and Xiaoyan Xing and Alexander Timans", "abstract": "Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.", "link": "http://arxiv.org/abs/2512.23427v1", "date": "2025-12-29", "relevancy": 2.3228, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6008}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Integrating%20Uncertainty%20for%20Domain-Agnostic%20Segmentation&body=Title%3A%20Towards%20Integrating%20Uncertainty%20for%20Domain-Agnostic%20Segmentation%0AAuthor%3A%20Jesse%20Brouwers%20and%20Xiaoyan%20Xing%20and%20Alexander%20Timans%0AAbstract%3A%20Foundation%20models%20for%20segmentation%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20family%20exhibit%20strong%20zero-shot%20performance%2C%20but%20remain%20vulnerable%20in%20shifted%20or%20limited-knowledge%20domains.%20This%20work%20investigates%20whether%20uncertainty%20quantification%20can%20mitigate%20such%20challenges%20and%20enhance%20model%20generalisability%20in%20a%20domain-agnostic%20manner.%20To%20this%20end%2C%20we%20%281%29%20curate%20UncertSAM%2C%20a%20benchmark%20comprising%20eight%20datasets%20designed%20to%20stress-test%20SAM%20under%20challenging%20segmentation%20conditions%20including%20shadows%2C%20transparency%2C%20and%20camouflage%3B%20%282%29%20evaluate%20a%20suite%20of%20lightweight%2C%20post-hoc%20uncertainty%20estimation%20methods%3B%20and%20%283%29%20assess%20a%20preliminary%20uncertainty-guided%20prediction%20refinement%20step.%20Among%20evaluated%20approaches%2C%20a%20last-layer%20Laplace%20approximation%20yields%20uncertainty%20estimates%20that%20correlate%20well%20with%20segmentation%20errors%2C%20indicating%20a%20meaningful%20signal.%20While%20refinement%20benefits%20are%20preliminary%2C%20our%20findings%20underscore%20the%20potential%20of%20incorporating%20uncertainty%20into%20segmentation%20models%20to%20support%20robust%2C%20domain-agnostic%20performance.%20Our%20benchmark%20and%20code%20are%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Integrating%2520Uncertainty%2520for%2520Domain-Agnostic%2520Segmentation%26entry.906535625%3DJesse%2520Brouwers%2520and%2520Xiaoyan%2520Xing%2520and%2520Alexander%2520Timans%26entry.1292438233%3DFoundation%2520models%2520for%2520segmentation%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520family%2520exhibit%2520strong%2520zero-shot%2520performance%252C%2520but%2520remain%2520vulnerable%2520in%2520shifted%2520or%2520limited-knowledge%2520domains.%2520This%2520work%2520investigates%2520whether%2520uncertainty%2520quantification%2520can%2520mitigate%2520such%2520challenges%2520and%2520enhance%2520model%2520generalisability%2520in%2520a%2520domain-agnostic%2520manner.%2520To%2520this%2520end%252C%2520we%2520%25281%2529%2520curate%2520UncertSAM%252C%2520a%2520benchmark%2520comprising%2520eight%2520datasets%2520designed%2520to%2520stress-test%2520SAM%2520under%2520challenging%2520segmentation%2520conditions%2520including%2520shadows%252C%2520transparency%252C%2520and%2520camouflage%253B%2520%25282%2529%2520evaluate%2520a%2520suite%2520of%2520lightweight%252C%2520post-hoc%2520uncertainty%2520estimation%2520methods%253B%2520and%2520%25283%2529%2520assess%2520a%2520preliminary%2520uncertainty-guided%2520prediction%2520refinement%2520step.%2520Among%2520evaluated%2520approaches%252C%2520a%2520last-layer%2520Laplace%2520approximation%2520yields%2520uncertainty%2520estimates%2520that%2520correlate%2520well%2520with%2520segmentation%2520errors%252C%2520indicating%2520a%2520meaningful%2520signal.%2520While%2520refinement%2520benefits%2520are%2520preliminary%252C%2520our%2520findings%2520underscore%2520the%2520potential%2520of%2520incorporating%2520uncertainty%2520into%2520segmentation%2520models%2520to%2520support%2520robust%252C%2520domain-agnostic%2520performance.%2520Our%2520benchmark%2520and%2520code%2520are%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Integrating%20Uncertainty%20for%20Domain-Agnostic%20Segmentation&entry.906535625=Jesse%20Brouwers%20and%20Xiaoyan%20Xing%20and%20Alexander%20Timans&entry.1292438233=Foundation%20models%20for%20segmentation%20such%20as%20the%20Segment%20Anything%20Model%20%28SAM%29%20family%20exhibit%20strong%20zero-shot%20performance%2C%20but%20remain%20vulnerable%20in%20shifted%20or%20limited-knowledge%20domains.%20This%20work%20investigates%20whether%20uncertainty%20quantification%20can%20mitigate%20such%20challenges%20and%20enhance%20model%20generalisability%20in%20a%20domain-agnostic%20manner.%20To%20this%20end%2C%20we%20%281%29%20curate%20UncertSAM%2C%20a%20benchmark%20comprising%20eight%20datasets%20designed%20to%20stress-test%20SAM%20under%20challenging%20segmentation%20conditions%20including%20shadows%2C%20transparency%2C%20and%20camouflage%3B%20%282%29%20evaluate%20a%20suite%20of%20lightweight%2C%20post-hoc%20uncertainty%20estimation%20methods%3B%20and%20%283%29%20assess%20a%20preliminary%20uncertainty-guided%20prediction%20refinement%20step.%20Among%20evaluated%20approaches%2C%20a%20last-layer%20Laplace%20approximation%20yields%20uncertainty%20estimates%20that%20correlate%20well%20with%20segmentation%20errors%2C%20indicating%20a%20meaningful%20signal.%20While%20refinement%20benefits%20are%20preliminary%2C%20our%20findings%20underscore%20the%20potential%20of%20incorporating%20uncertainty%20into%20segmentation%20models%20to%20support%20robust%2C%20domain-agnostic%20performance.%20Our%20benchmark%20and%20code%20are%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.23427v1&entry.124074799=Read"},
{"title": "EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition", "author": "Maryam Mirzaei and Farzaneh Shayegh and Hamed Narimani", "abstract": "Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.", "link": "http://arxiv.org/abs/2512.23526v1", "date": "2025-12-29", "relevancy": 2.3144, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition&body=Title%3A%20EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition%0AAuthor%3A%20Maryam%20Mirzaei%20and%20Farzaneh%20Shayegh%20and%20Hamed%20Narimani%0AAbstract%3A%20Accurate%20recognition%20of%20human%20emotional%20states%20is%20critical%20for%20effective%20human-machine%20interaction.%20Electroencephalography%20%28EEG%29%20offers%20a%20reliable%20source%20for%20emotion%20recognition%20due%20to%20its%20high%20temporal%20resolution%20and%20its%20direct%20reflection%20of%20neural%20activity.%20Nevertheless%2C%20variations%20across%20recording%20sessions%20present%20a%20major%20challenge%20for%20model%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20EGDA%2C%20a%20framework%20that%20reduces%20cross-session%20discrepancies%20by%20jointly%20aligning%20the%20global%20%28marginal%29%20and%20class-specific%20%28conditional%29%20distributions%2C%20while%20preserving%20the%20intrinsic%20structure%20of%20EEG%20data%20through%20graph%20regularization.%20Experimental%20results%20on%20the%20SEED-IV%20dataset%20demonstrate%20that%20EGDA%20achieves%20robust%20cross-session%20performance%2C%20obtaining%20accuracies%20of%2081.22%25%2C%2080.15%25%2C%20and%2083.27%25%20across%20three%20transfer%20tasks%2C%20and%20surpassing%20several%20baseline%20methods.%20Furthermore%2C%20the%20analysis%20highlights%20the%20Gamma%20frequency%20band%20as%20the%20most%20discriminative%20and%20identifies%20the%20central-parietal%20and%20prefrontal%20brain%20regions%20as%20critical%20for%20reliable%20emotion%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-based%2520Graph-guided%2520Domain%2520Adaptation%2520for%2520Robust%2520Cross-Session%2520Emotion%2520Recognition%26entry.906535625%3DMaryam%2520Mirzaei%2520and%2520Farzaneh%2520Shayegh%2520and%2520Hamed%2520Narimani%26entry.1292438233%3DAccurate%2520recognition%2520of%2520human%2520emotional%2520states%2520is%2520critical%2520for%2520effective%2520human-machine%2520interaction.%2520Electroencephalography%2520%2528EEG%2529%2520offers%2520a%2520reliable%2520source%2520for%2520emotion%2520recognition%2520due%2520to%2520its%2520high%2520temporal%2520resolution%2520and%2520its%2520direct%2520reflection%2520of%2520neural%2520activity.%2520Nevertheless%252C%2520variations%2520across%2520recording%2520sessions%2520present%2520a%2520major%2520challenge%2520for%2520model%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520EGDA%252C%2520a%2520framework%2520that%2520reduces%2520cross-session%2520discrepancies%2520by%2520jointly%2520aligning%2520the%2520global%2520%2528marginal%2529%2520and%2520class-specific%2520%2528conditional%2529%2520distributions%252C%2520while%2520preserving%2520the%2520intrinsic%2520structure%2520of%2520EEG%2520data%2520through%2520graph%2520regularization.%2520Experimental%2520results%2520on%2520the%2520SEED-IV%2520dataset%2520demonstrate%2520that%2520EGDA%2520achieves%2520robust%2520cross-session%2520performance%252C%2520obtaining%2520accuracies%2520of%252081.22%2525%252C%252080.15%2525%252C%2520and%252083.27%2525%2520across%2520three%2520transfer%2520tasks%252C%2520and%2520surpassing%2520several%2520baseline%2520methods.%2520Furthermore%252C%2520the%2520analysis%2520highlights%2520the%2520Gamma%2520frequency%2520band%2520as%2520the%2520most%2520discriminative%2520and%2520identifies%2520the%2520central-parietal%2520and%2520prefrontal%2520brain%2520regions%2520as%2520critical%2520for%2520reliable%2520emotion%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition&entry.906535625=Maryam%20Mirzaei%20and%20Farzaneh%20Shayegh%20and%20Hamed%20Narimani&entry.1292438233=Accurate%20recognition%20of%20human%20emotional%20states%20is%20critical%20for%20effective%20human-machine%20interaction.%20Electroencephalography%20%28EEG%29%20offers%20a%20reliable%20source%20for%20emotion%20recognition%20due%20to%20its%20high%20temporal%20resolution%20and%20its%20direct%20reflection%20of%20neural%20activity.%20Nevertheless%2C%20variations%20across%20recording%20sessions%20present%20a%20major%20challenge%20for%20model%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20EGDA%2C%20a%20framework%20that%20reduces%20cross-session%20discrepancies%20by%20jointly%20aligning%20the%20global%20%28marginal%29%20and%20class-specific%20%28conditional%29%20distributions%2C%20while%20preserving%20the%20intrinsic%20structure%20of%20EEG%20data%20through%20graph%20regularization.%20Experimental%20results%20on%20the%20SEED-IV%20dataset%20demonstrate%20that%20EGDA%20achieves%20robust%20cross-session%20performance%2C%20obtaining%20accuracies%20of%2081.22%25%2C%2080.15%25%2C%20and%2083.27%25%20across%20three%20transfer%20tasks%2C%20and%20surpassing%20several%20baseline%20methods.%20Furthermore%2C%20the%20analysis%20highlights%20the%20Gamma%20frequency%20band%20as%20the%20most%20discriminative%20and%20identifies%20the%20central-parietal%20and%20prefrontal%20brain%20regions%20as%20critical%20for%20reliable%20emotion%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2512.23526v1&entry.124074799=Read"},
{"title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "author": "Zhangcheng Qiang and Kerry Taylor and Weiqing Wang", "abstract": "Due to the dynamic nature of the Semantic Web, version control is necessary to manage changes in widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse similarities and differences between OM and OV and formalise the OM4OV pipeline to offer more advanced OV support. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability of false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, which builds on existing OM alignments to reduce the number of matching candidates and to improve overall OV performance.", "link": "http://arxiv.org/abs/2409.20302v7", "date": "2025-12-29", "relevancy": 2.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&body=Title%3A%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang%0AAbstract%3A%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20manage%20changes%20in%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline%20to%20offer%20more%20advanced%20OV%20support.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explainability%20of%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20which%20builds%20on%20existing%20OM%20alignments%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20to%20improve%20overall%20OV%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2409.20302v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOM4OV%253A%2520Leveraging%2520Ontology%2520Matching%2520for%2520Ontology%2520Versioning%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Kerry%2520Taylor%2520and%2520Weiqing%2520Wang%26entry.1292438233%3DDue%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520Semantic%2520Web%252C%2520version%2520control%2520is%2520necessary%2520to%2520manage%2520changes%2520in%2520widely%2520used%2520ontologies.%2520Despite%2520the%2520long-standing%2520recognition%2520of%2520ontology%2520versioning%2520%2528OV%2529%2520as%2520a%2520crucial%2520component%2520of%2520efficient%2520ontology%2520management%252C%2520many%2520approaches%2520treat%2520OV%2520as%2520similar%2520to%2520ontology%2520matching%2520%2528OM%2529%2520and%2520directly%2520reuse%2520OM%2520systems%2520for%2520OV%2520tasks.%2520In%2520this%2520study%252C%2520we%2520systematically%2520analyse%2520similarities%2520and%2520differences%2520between%2520OM%2520and%2520OV%2520and%2520formalise%2520the%2520OM4OV%2520pipeline%2520to%2520offer%2520more%2520advanced%2520OV%2520support.%2520The%2520pipeline%2520is%2520implemented%2520and%2520evaluated%2520in%2520the%2520state-of-the-art%2520OM%2520system%2520Agent-OM.%2520The%2520experimental%2520results%2520indicate%2520that%2520OM%2520systems%2520can%2520be%2520reused%2520for%2520OV%2520tasks%252C%2520but%2520without%2520necessary%2520extensions%252C%2520the%2520current%2520OM4OV%2520pipeline%2520can%2520produce%2520skewed%2520measurements%252C%2520poor%2520performance%2520in%2520detecting%2520update%2520entities%252C%2520and%2520limited%2520explainability%2520of%2520false%2520mappings.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520an%2520optimisation%2520method%2520called%2520the%2520cross-reference%2520%2528CR%2529%2520mechanism%252C%2520which%2520builds%2520on%2520existing%2520OM%2520alignments%2520to%2520reduce%2520the%2520number%2520of%2520matching%2520candidates%2520and%2520to%2520improve%2520overall%2520OV%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20302v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&entry.906535625=Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang&entry.1292438233=Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20manage%20changes%20in%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline%20to%20offer%20more%20advanced%20OV%20support.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explainability%20of%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20which%20builds%20on%20existing%20OM%20alignments%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20to%20improve%20overall%20OV%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2409.20302v7&entry.124074799=Read"},
{"title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives", "author": "Aheli Poddar and Saptarshi Sahoo and Sujata Ghosh", "abstract": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.", "link": "http://arxiv.org/abs/2512.12620v3", "date": "2025-12-29", "relevancy": 2.2913, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Syllogistic%20Reasoning%20in%20LLMs%20from%20Formal%20and%20Natural%20Language%20Perspectives&body=Title%3A%20Understanding%20Syllogistic%20Reasoning%20in%20LLMs%20from%20Formal%20and%20Natural%20Language%20Perspectives%0AAuthor%3A%20Aheli%20Poddar%20and%20Saptarshi%20Sahoo%20and%20Sujata%20Ghosh%0AAbstract%3A%20We%20study%20syllogistic%20reasoning%20in%20LLMs%20from%20the%20logical%20and%20natural%20language%20perspectives.%20In%20process%2C%20we%20explore%20fundamental%20reasoning%20capabilities%20of%20the%20LLMs%20and%20the%20direction%20this%20research%20is%20moving%20forward.%20To%20aid%20in%20our%20studies%2C%20we%20use%2014%20large%20language%20models%20and%20investigate%20their%20syllogistic%20reasoning%20capabilities%20in%20terms%20of%20symbolic%20inferences%20as%20well%20as%20natural%20language%20understanding.%20Even%20though%20this%20reasoning%20mechanism%20is%20not%20a%20uniform%20emergent%20property%20across%20LLMs%2C%20the%20perfect%20symbolic%20performances%20in%20certain%20models%20make%20us%20wonder%20whether%20LLMs%20are%20becoming%20more%20and%20more%20formal%20reasoning%20mechanisms%2C%20rather%20than%20making%20explicit%20the%20nuances%20of%20human%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12620v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Syllogistic%2520Reasoning%2520in%2520LLMs%2520from%2520Formal%2520and%2520Natural%2520Language%2520Perspectives%26entry.906535625%3DAheli%2520Poddar%2520and%2520Saptarshi%2520Sahoo%2520and%2520Sujata%2520Ghosh%26entry.1292438233%3DWe%2520study%2520syllogistic%2520reasoning%2520in%2520LLMs%2520from%2520the%2520logical%2520and%2520natural%2520language%2520perspectives.%2520In%2520process%252C%2520we%2520explore%2520fundamental%2520reasoning%2520capabilities%2520of%2520the%2520LLMs%2520and%2520the%2520direction%2520this%2520research%2520is%2520moving%2520forward.%2520To%2520aid%2520in%2520our%2520studies%252C%2520we%2520use%252014%2520large%2520language%2520models%2520and%2520investigate%2520their%2520syllogistic%2520reasoning%2520capabilities%2520in%2520terms%2520of%2520symbolic%2520inferences%2520as%2520well%2520as%2520natural%2520language%2520understanding.%2520Even%2520though%2520this%2520reasoning%2520mechanism%2520is%2520not%2520a%2520uniform%2520emergent%2520property%2520across%2520LLMs%252C%2520the%2520perfect%2520symbolic%2520performances%2520in%2520certain%2520models%2520make%2520us%2520wonder%2520whether%2520LLMs%2520are%2520becoming%2520more%2520and%2520more%2520formal%2520reasoning%2520mechanisms%252C%2520rather%2520than%2520making%2520explicit%2520the%2520nuances%2520of%2520human%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12620v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Syllogistic%20Reasoning%20in%20LLMs%20from%20Formal%20and%20Natural%20Language%20Perspectives&entry.906535625=Aheli%20Poddar%20and%20Saptarshi%20Sahoo%20and%20Sujata%20Ghosh&entry.1292438233=We%20study%20syllogistic%20reasoning%20in%20LLMs%20from%20the%20logical%20and%20natural%20language%20perspectives.%20In%20process%2C%20we%20explore%20fundamental%20reasoning%20capabilities%20of%20the%20LLMs%20and%20the%20direction%20this%20research%20is%20moving%20forward.%20To%20aid%20in%20our%20studies%2C%20we%20use%2014%20large%20language%20models%20and%20investigate%20their%20syllogistic%20reasoning%20capabilities%20in%20terms%20of%20symbolic%20inferences%20as%20well%20as%20natural%20language%20understanding.%20Even%20though%20this%20reasoning%20mechanism%20is%20not%20a%20uniform%20emergent%20property%20across%20LLMs%2C%20the%20perfect%20symbolic%20performances%20in%20certain%20models%20make%20us%20wonder%20whether%20LLMs%20are%20becoming%20more%20and%20more%20formal%20reasoning%20mechanisms%2C%20rather%20than%20making%20explicit%20the%20nuances%20of%20human%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.12620v3&entry.124074799=Read"},
{"title": "GrOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation", "author": "William van den Bogert and Gregory Linkowski and Nima Fazeli", "abstract": "Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GrOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GrOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GrOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GrOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GrOMPsite.", "link": "http://arxiv.org/abs/2512.03347v2", "date": "2025-12-29", "relevancy": 2.283, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.546}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrOMP%3A%20Grasped%20Object%20Manifold%20Projection%20for%20Multimodal%20Imitation%20Learning%20of%20Manipulation&body=Title%3A%20GrOMP%3A%20Grasped%20Object%20Manifold%20Projection%20for%20Multimodal%20Imitation%20Learning%20of%20Manipulation%0AAuthor%3A%20William%20van%20den%20Bogert%20and%20Gregory%20Linkowski%20and%20Nima%20Fazeli%0AAbstract%3A%20Imitation%20Learning%20%28IL%29%20holds%20great%20potential%20for%20learning%20repetitive%20manipulation%20tasks%2C%20such%20as%20those%20in%20industrial%20assembly.%20However%2C%20its%20effectiveness%20is%20often%20limited%20by%20insufficient%20trajectory%20precision%20due%20to%20compounding%20errors.%20In%20this%20paper%2C%20we%20introduce%20Grasped%20Object%20Manifold%20Projection%20%28GrOMP%29%2C%20an%20interactive%20method%20that%20mitigates%20these%20errors%20by%20constraining%20a%20non-rigidly%20grasped%20object%20to%20a%20lower-dimensional%20manifold.%20GrOMP%20assumes%20a%20precise%20task%20in%20which%20a%20manipulator%20holds%20an%20object%20that%20may%20shift%20within%20the%20grasp%20in%20an%20observable%20manner%20and%20must%20be%20mated%20with%20a%20grounded%20part.%20Crucially%2C%20all%20GrOMP%20enhancements%20are%20learned%20from%20the%20same%20expert%20dataset%20used%20to%20train%20the%20base%20IL%20policy%2C%20and%20are%20adjusted%20with%20an%20n-arm%20bandit-based%20interactive%20component.%20We%20propose%20a%20theoretical%20basis%20for%20GrOMP%27s%20improvement%20upon%20the%20well-known%20compounding%20error%20bound%20in%20IL%20literature.%20We%20demonstrate%20the%20framework%20on%20four%20precise%20assembly%20tasks%20using%20tactile%20feedback%2C%20and%20note%20that%20the%20approach%20remains%20modality-agnostic.%20Data%20and%20videos%20are%20available%20at%20williamvdb.github.io/GrOMPsite.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrOMP%253A%2520Grasped%2520Object%2520Manifold%2520Projection%2520for%2520Multimodal%2520Imitation%2520Learning%2520of%2520Manipulation%26entry.906535625%3DWilliam%2520van%2520den%2520Bogert%2520and%2520Gregory%2520Linkowski%2520and%2520Nima%2520Fazeli%26entry.1292438233%3DImitation%2520Learning%2520%2528IL%2529%2520holds%2520great%2520potential%2520for%2520learning%2520repetitive%2520manipulation%2520tasks%252C%2520such%2520as%2520those%2520in%2520industrial%2520assembly.%2520However%252C%2520its%2520effectiveness%2520is%2520often%2520limited%2520by%2520insufficient%2520trajectory%2520precision%2520due%2520to%2520compounding%2520errors.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Grasped%2520Object%2520Manifold%2520Projection%2520%2528GrOMP%2529%252C%2520an%2520interactive%2520method%2520that%2520mitigates%2520these%2520errors%2520by%2520constraining%2520a%2520non-rigidly%2520grasped%2520object%2520to%2520a%2520lower-dimensional%2520manifold.%2520GrOMP%2520assumes%2520a%2520precise%2520task%2520in%2520which%2520a%2520manipulator%2520holds%2520an%2520object%2520that%2520may%2520shift%2520within%2520the%2520grasp%2520in%2520an%2520observable%2520manner%2520and%2520must%2520be%2520mated%2520with%2520a%2520grounded%2520part.%2520Crucially%252C%2520all%2520GrOMP%2520enhancements%2520are%2520learned%2520from%2520the%2520same%2520expert%2520dataset%2520used%2520to%2520train%2520the%2520base%2520IL%2520policy%252C%2520and%2520are%2520adjusted%2520with%2520an%2520n-arm%2520bandit-based%2520interactive%2520component.%2520We%2520propose%2520a%2520theoretical%2520basis%2520for%2520GrOMP%2527s%2520improvement%2520upon%2520the%2520well-known%2520compounding%2520error%2520bound%2520in%2520IL%2520literature.%2520We%2520demonstrate%2520the%2520framework%2520on%2520four%2520precise%2520assembly%2520tasks%2520using%2520tactile%2520feedback%252C%2520and%2520note%2520that%2520the%2520approach%2520remains%2520modality-agnostic.%2520Data%2520and%2520videos%2520are%2520available%2520at%2520williamvdb.github.io/GrOMPsite.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrOMP%3A%20Grasped%20Object%20Manifold%20Projection%20for%20Multimodal%20Imitation%20Learning%20of%20Manipulation&entry.906535625=William%20van%20den%20Bogert%20and%20Gregory%20Linkowski%20and%20Nima%20Fazeli&entry.1292438233=Imitation%20Learning%20%28IL%29%20holds%20great%20potential%20for%20learning%20repetitive%20manipulation%20tasks%2C%20such%20as%20those%20in%20industrial%20assembly.%20However%2C%20its%20effectiveness%20is%20often%20limited%20by%20insufficient%20trajectory%20precision%20due%20to%20compounding%20errors.%20In%20this%20paper%2C%20we%20introduce%20Grasped%20Object%20Manifold%20Projection%20%28GrOMP%29%2C%20an%20interactive%20method%20that%20mitigates%20these%20errors%20by%20constraining%20a%20non-rigidly%20grasped%20object%20to%20a%20lower-dimensional%20manifold.%20GrOMP%20assumes%20a%20precise%20task%20in%20which%20a%20manipulator%20holds%20an%20object%20that%20may%20shift%20within%20the%20grasp%20in%20an%20observable%20manner%20and%20must%20be%20mated%20with%20a%20grounded%20part.%20Crucially%2C%20all%20GrOMP%20enhancements%20are%20learned%20from%20the%20same%20expert%20dataset%20used%20to%20train%20the%20base%20IL%20policy%2C%20and%20are%20adjusted%20with%20an%20n-arm%20bandit-based%20interactive%20component.%20We%20propose%20a%20theoretical%20basis%20for%20GrOMP%27s%20improvement%20upon%20the%20well-known%20compounding%20error%20bound%20in%20IL%20literature.%20We%20demonstrate%20the%20framework%20on%20four%20precise%20assembly%20tasks%20using%20tactile%20feedback%2C%20and%20note%20that%20the%20approach%20remains%20modality-agnostic.%20Data%20and%20videos%20are%20available%20at%20williamvdb.github.io/GrOMPsite.&entry.1838667208=http%3A//arxiv.org/abs/2512.03347v2&entry.124074799=Read"},
{"title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms", "author": " LearnLM Team and  Eedi and  : and Albert Wang and Aliya Rysbek and Andrea Huber and Anjali Nambiar and Anna Kenolty and Ben Caulfield and Beth Lilley-Draper and Bibi Groot and Brian Veprek and Chelsea Burdett and Claire Willis and Craig Barton and Digory Smith and George Mu and Harriet Walters and Irina Jurenka and Iris Hulls and James Stalley-Moores and Jonathan Caton and Julia Wilkowski and Kaiz Alarakyia and Kevin R. McKee and Liam McCafferty and Lucy Dalton and Markus Kunesch and Pauline Malubay and Rachel Kidson and Rich Wells and Sam Wheeler and Sara Wiltberger and Shakir Mohamed and Simon Woodhead and Vasco Braz\u00e3o", "abstract": "One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.", "link": "http://arxiv.org/abs/2512.23633v1", "date": "2025-12-29", "relevancy": 2.2749, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4657}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4567}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20tutoring%20can%20safely%20and%20effectively%20support%20students%3A%20An%20exploratory%20RCT%20in%20UK%20classrooms&body=Title%3A%20AI%20tutoring%20can%20safely%20and%20effectively%20support%20students%3A%20An%20exploratory%20RCT%20in%20UK%20classrooms%0AAuthor%3A%20%20LearnLM%20Team%20and%20%20Eedi%20and%20%20%3A%20and%20Albert%20Wang%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Anjali%20Nambiar%20and%20Anna%20Kenolty%20and%20Ben%20Caulfield%20and%20Beth%20Lilley-Draper%20and%20Bibi%20Groot%20and%20Brian%20Veprek%20and%20Chelsea%20Burdett%20and%20Claire%20Willis%20and%20Craig%20Barton%20and%20Digory%20Smith%20and%20George%20Mu%20and%20Harriet%20Walters%20and%20Irina%20Jurenka%20and%20Iris%20Hulls%20and%20James%20Stalley-Moores%20and%20Jonathan%20Caton%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Liam%20McCafferty%20and%20Lucy%20Dalton%20and%20Markus%20Kunesch%20and%20Pauline%20Malubay%20and%20Rachel%20Kidson%20and%20Rich%20Wells%20and%20Sam%20Wheeler%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Simon%20Woodhead%20and%20Vasco%20Braz%C3%A3o%0AAbstract%3A%20One-to-one%20tutoring%20is%20widely%20considered%20the%20gold%20standard%20for%20personalized%20education%2C%20yet%20it%20remains%20prohibitively%20expensive%20to%20scale.%20To%20evaluate%20whether%20generative%20AI%20might%20help%20expand%20access%20to%20this%20resource%2C%20we%20conducted%20an%20exploratory%20randomized%20controlled%20trial%20%28RCT%29%20with%20%24N%20%3D%20165%24%20students%20across%20five%20UK%20secondary%20schools.%20We%20integrated%20LearnLM%20--%20a%20generative%20AI%20model%20fine-tuned%20for%20pedagogy%20--%20into%20chat-based%20tutoring%20sessions%20on%20the%20Eedi%20mathematics%20platform.%20In%20the%20RCT%2C%20expert%20tutors%20directly%20supervised%20LearnLM%2C%20with%20the%20remit%20to%20revise%20each%20message%20it%20drafted%20until%20they%20would%20be%20satisfied%20sending%20it%20themselves.%20LearnLM%20proved%20to%20be%20a%20reliable%20source%20of%20pedagogical%20instruction%2C%20with%20supervising%20tutors%20approving%2076.4%25%20of%20its%20drafted%20messages%20making%20zero%20or%20minimal%20edits%20%28i.e.%2C%20changing%20only%20one%20or%20two%20characters%29.%20This%20translated%20into%20effective%20tutoring%20support%3A%20students%20guided%20by%20LearnLM%20performed%20at%20least%20as%20well%20as%20students%20chatting%20with%20human%20tutors%20on%20each%20learning%20outcome%20we%20measured.%20In%20fact%2C%20students%20who%20received%20support%20from%20LearnLM%20were%205.5%20percentage%20points%20more%20likely%20to%20solve%20novel%20problems%20on%20subsequent%20topics%20%28with%20a%20success%20rate%20of%2066.2%25%29%20than%20those%20who%20received%20tutoring%20from%20human%20tutors%20alone%20%28rate%20of%2060.7%25%29.%20In%20interviews%2C%20tutors%20highlighted%20LearnLM%27s%20strength%20at%20drafting%20Socratic%20questions%20that%20encouraged%20deeper%20reflection%20from%20students%2C%20with%20multiple%20tutors%20even%20reporting%20that%20they%20learned%20new%20pedagogical%20practices%20from%20the%20model.%20Overall%2C%20our%20results%20suggest%20that%20pedagogically%20fine-tuned%20AI%20tutoring%20systems%20may%20play%20a%20promising%20role%20in%20delivering%20effective%2C%20individualized%20learning%20support%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520tutoring%2520can%2520safely%2520and%2520effectively%2520support%2520students%253A%2520An%2520exploratory%2520RCT%2520in%2520UK%2520classrooms%26entry.906535625%3D%2520LearnLM%2520Team%2520and%2520%2520Eedi%2520and%2520%2520%253A%2520and%2520Albert%2520Wang%2520and%2520Aliya%2520Rysbek%2520and%2520Andrea%2520Huber%2520and%2520Anjali%2520Nambiar%2520and%2520Anna%2520Kenolty%2520and%2520Ben%2520Caulfield%2520and%2520Beth%2520Lilley-Draper%2520and%2520Bibi%2520Groot%2520and%2520Brian%2520Veprek%2520and%2520Chelsea%2520Burdett%2520and%2520Claire%2520Willis%2520and%2520Craig%2520Barton%2520and%2520Digory%2520Smith%2520and%2520George%2520Mu%2520and%2520Harriet%2520Walters%2520and%2520Irina%2520Jurenka%2520and%2520Iris%2520Hulls%2520and%2520James%2520Stalley-Moores%2520and%2520Jonathan%2520Caton%2520and%2520Julia%2520Wilkowski%2520and%2520Kaiz%2520Alarakyia%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Liam%2520McCafferty%2520and%2520Lucy%2520Dalton%2520and%2520Markus%2520Kunesch%2520and%2520Pauline%2520Malubay%2520and%2520Rachel%2520Kidson%2520and%2520Rich%2520Wells%2520and%2520Sam%2520Wheeler%2520and%2520Sara%2520Wiltberger%2520and%2520Shakir%2520Mohamed%2520and%2520Simon%2520Woodhead%2520and%2520Vasco%2520Braz%25C3%25A3o%26entry.1292438233%3DOne-to-one%2520tutoring%2520is%2520widely%2520considered%2520the%2520gold%2520standard%2520for%2520personalized%2520education%252C%2520yet%2520it%2520remains%2520prohibitively%2520expensive%2520to%2520scale.%2520To%2520evaluate%2520whether%2520generative%2520AI%2520might%2520help%2520expand%2520access%2520to%2520this%2520resource%252C%2520we%2520conducted%2520an%2520exploratory%2520randomized%2520controlled%2520trial%2520%2528RCT%2529%2520with%2520%2524N%2520%253D%2520165%2524%2520students%2520across%2520five%2520UK%2520secondary%2520schools.%2520We%2520integrated%2520LearnLM%2520--%2520a%2520generative%2520AI%2520model%2520fine-tuned%2520for%2520pedagogy%2520--%2520into%2520chat-based%2520tutoring%2520sessions%2520on%2520the%2520Eedi%2520mathematics%2520platform.%2520In%2520the%2520RCT%252C%2520expert%2520tutors%2520directly%2520supervised%2520LearnLM%252C%2520with%2520the%2520remit%2520to%2520revise%2520each%2520message%2520it%2520drafted%2520until%2520they%2520would%2520be%2520satisfied%2520sending%2520it%2520themselves.%2520LearnLM%2520proved%2520to%2520be%2520a%2520reliable%2520source%2520of%2520pedagogical%2520instruction%252C%2520with%2520supervising%2520tutors%2520approving%252076.4%2525%2520of%2520its%2520drafted%2520messages%2520making%2520zero%2520or%2520minimal%2520edits%2520%2528i.e.%252C%2520changing%2520only%2520one%2520or%2520two%2520characters%2529.%2520This%2520translated%2520into%2520effective%2520tutoring%2520support%253A%2520students%2520guided%2520by%2520LearnLM%2520performed%2520at%2520least%2520as%2520well%2520as%2520students%2520chatting%2520with%2520human%2520tutors%2520on%2520each%2520learning%2520outcome%2520we%2520measured.%2520In%2520fact%252C%2520students%2520who%2520received%2520support%2520from%2520LearnLM%2520were%25205.5%2520percentage%2520points%2520more%2520likely%2520to%2520solve%2520novel%2520problems%2520on%2520subsequent%2520topics%2520%2528with%2520a%2520success%2520rate%2520of%252066.2%2525%2529%2520than%2520those%2520who%2520received%2520tutoring%2520from%2520human%2520tutors%2520alone%2520%2528rate%2520of%252060.7%2525%2529.%2520In%2520interviews%252C%2520tutors%2520highlighted%2520LearnLM%2527s%2520strength%2520at%2520drafting%2520Socratic%2520questions%2520that%2520encouraged%2520deeper%2520reflection%2520from%2520students%252C%2520with%2520multiple%2520tutors%2520even%2520reporting%2520that%2520they%2520learned%2520new%2520pedagogical%2520practices%2520from%2520the%2520model.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520pedagogically%2520fine-tuned%2520AI%2520tutoring%2520systems%2520may%2520play%2520a%2520promising%2520role%2520in%2520delivering%2520effective%252C%2520individualized%2520learning%2520support%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20tutoring%20can%20safely%20and%20effectively%20support%20students%3A%20An%20exploratory%20RCT%20in%20UK%20classrooms&entry.906535625=%20LearnLM%20Team%20and%20%20Eedi%20and%20%20%3A%20and%20Albert%20Wang%20and%20Aliya%20Rysbek%20and%20Andrea%20Huber%20and%20Anjali%20Nambiar%20and%20Anna%20Kenolty%20and%20Ben%20Caulfield%20and%20Beth%20Lilley-Draper%20and%20Bibi%20Groot%20and%20Brian%20Veprek%20and%20Chelsea%20Burdett%20and%20Claire%20Willis%20and%20Craig%20Barton%20and%20Digory%20Smith%20and%20George%20Mu%20and%20Harriet%20Walters%20and%20Irina%20Jurenka%20and%20Iris%20Hulls%20and%20James%20Stalley-Moores%20and%20Jonathan%20Caton%20and%20Julia%20Wilkowski%20and%20Kaiz%20Alarakyia%20and%20Kevin%20R.%20McKee%20and%20Liam%20McCafferty%20and%20Lucy%20Dalton%20and%20Markus%20Kunesch%20and%20Pauline%20Malubay%20and%20Rachel%20Kidson%20and%20Rich%20Wells%20and%20Sam%20Wheeler%20and%20Sara%20Wiltberger%20and%20Shakir%20Mohamed%20and%20Simon%20Woodhead%20and%20Vasco%20Braz%C3%A3o&entry.1292438233=One-to-one%20tutoring%20is%20widely%20considered%20the%20gold%20standard%20for%20personalized%20education%2C%20yet%20it%20remains%20prohibitively%20expensive%20to%20scale.%20To%20evaluate%20whether%20generative%20AI%20might%20help%20expand%20access%20to%20this%20resource%2C%20we%20conducted%20an%20exploratory%20randomized%20controlled%20trial%20%28RCT%29%20with%20%24N%20%3D%20165%24%20students%20across%20five%20UK%20secondary%20schools.%20We%20integrated%20LearnLM%20--%20a%20generative%20AI%20model%20fine-tuned%20for%20pedagogy%20--%20into%20chat-based%20tutoring%20sessions%20on%20the%20Eedi%20mathematics%20platform.%20In%20the%20RCT%2C%20expert%20tutors%20directly%20supervised%20LearnLM%2C%20with%20the%20remit%20to%20revise%20each%20message%20it%20drafted%20until%20they%20would%20be%20satisfied%20sending%20it%20themselves.%20LearnLM%20proved%20to%20be%20a%20reliable%20source%20of%20pedagogical%20instruction%2C%20with%20supervising%20tutors%20approving%2076.4%25%20of%20its%20drafted%20messages%20making%20zero%20or%20minimal%20edits%20%28i.e.%2C%20changing%20only%20one%20or%20two%20characters%29.%20This%20translated%20into%20effective%20tutoring%20support%3A%20students%20guided%20by%20LearnLM%20performed%20at%20least%20as%20well%20as%20students%20chatting%20with%20human%20tutors%20on%20each%20learning%20outcome%20we%20measured.%20In%20fact%2C%20students%20who%20received%20support%20from%20LearnLM%20were%205.5%20percentage%20points%20more%20likely%20to%20solve%20novel%20problems%20on%20subsequent%20topics%20%28with%20a%20success%20rate%20of%2066.2%25%29%20than%20those%20who%20received%20tutoring%20from%20human%20tutors%20alone%20%28rate%20of%2060.7%25%29.%20In%20interviews%2C%20tutors%20highlighted%20LearnLM%27s%20strength%20at%20drafting%20Socratic%20questions%20that%20encouraged%20deeper%20reflection%20from%20students%2C%20with%20multiple%20tutors%20even%20reporting%20that%20they%20learned%20new%20pedagogical%20practices%20from%20the%20model.%20Overall%2C%20our%20results%20suggest%20that%20pedagogically%20fine-tuned%20AI%20tutoring%20systems%20may%20play%20a%20promising%20role%20in%20delivering%20effective%2C%20individualized%20learning%20support%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2512.23633v1&entry.124074799=Read"},
{"title": "MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning", "author": "Shuyuan Lin and Mengtin Lo and Haosheng Chen and Yanjie Liang and Qiangqiang Wu", "abstract": "Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.", "link": "http://arxiv.org/abs/2512.23369v1", "date": "2025-12-29", "relevancy": 2.2592, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGCA-Net%3A%20Multi-Graph%20Contextual%20Attention%20Network%20for%20Two-View%20Correspondence%20Learning&body=Title%3A%20MGCA-Net%3A%20Multi-Graph%20Contextual%20Attention%20Network%20for%20Two-View%20Correspondence%20Learning%0AAuthor%3A%20Shuyuan%20Lin%20and%20Mengtin%20Lo%20and%20Haosheng%20Chen%20and%20Yanjie%20Liang%20and%20Qiangqiang%20Wu%0AAbstract%3A%20Two-view%20correspondence%20learning%20is%20a%20key%20task%20in%20computer%20vision%2C%20which%20aims%20to%20establish%20reliable%20matching%20relationships%20for%20applications%20such%20as%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20existing%20methods%20have%20limitations%20in%20local%20geometric%20modeling%20and%20cross-stage%20information%20optimization%2C%20which%20make%20it%20difficult%20to%20accurately%20capture%20the%20geometric%20constraints%20of%20matched%20pairs%20and%20thus%20reduce%20the%20robustness%20of%20the%20model.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Multi-Graph%20Contextual%20Attention%20Network%20%28MGCA-Net%29%2C%20which%20consists%20of%20a%20Contextual%20Geometric%20Attention%20%28CGA%29%20module%20and%20a%20Cross-Stage%20Multi-Graph%20Consensus%20%28CSMGC%29%20module.%20Specifically%2C%20CGA%20dynamically%20integrates%20spatial%20position%20and%20feature%20information%20via%20an%20adaptive%20attention%20mechanism%20and%20enhances%20the%20capability%20to%20capture%20both%20local%20and%20global%20geometric%20relationships.%20Meanwhile%2C%20CSMGC%20establishes%20geometric%20consensus%20via%20a%20cross-stage%20sparse%20graph%20network%2C%20ensuring%20the%20consistency%20of%20geometric%20information%20across%20different%20stages.%20Experimental%20results%20on%20two%20representative%20YFCC100M%20and%20SUN3D%20datasets%20show%20that%20MGCA-Net%20significantly%20outperforms%20existing%20SOTA%20methods%20in%20the%20outlier%20rejection%20and%20camera%20pose%20estimation%20tasks.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGCA-Net%253A%2520Multi-Graph%2520Contextual%2520Attention%2520Network%2520for%2520Two-View%2520Correspondence%2520Learning%26entry.906535625%3DShuyuan%2520Lin%2520and%2520Mengtin%2520Lo%2520and%2520Haosheng%2520Chen%2520and%2520Yanjie%2520Liang%2520and%2520Qiangqiang%2520Wu%26entry.1292438233%3DTwo-view%2520correspondence%2520learning%2520is%2520a%2520key%2520task%2520in%2520computer%2520vision%252C%2520which%2520aims%2520to%2520establish%2520reliable%2520matching%2520relationships%2520for%2520applications%2520such%2520as%2520camera%2520pose%2520estimation%2520and%25203D%2520reconstruction.%2520However%252C%2520existing%2520methods%2520have%2520limitations%2520in%2520local%2520geometric%2520modeling%2520and%2520cross-stage%2520information%2520optimization%252C%2520which%2520make%2520it%2520difficult%2520to%2520accurately%2520capture%2520the%2520geometric%2520constraints%2520of%2520matched%2520pairs%2520and%2520thus%2520reduce%2520the%2520robustness%2520of%2520the%2520model.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Multi-Graph%2520Contextual%2520Attention%2520Network%2520%2528MGCA-Net%2529%252C%2520which%2520consists%2520of%2520a%2520Contextual%2520Geometric%2520Attention%2520%2528CGA%2529%2520module%2520and%2520a%2520Cross-Stage%2520Multi-Graph%2520Consensus%2520%2528CSMGC%2529%2520module.%2520Specifically%252C%2520CGA%2520dynamically%2520integrates%2520spatial%2520position%2520and%2520feature%2520information%2520via%2520an%2520adaptive%2520attention%2520mechanism%2520and%2520enhances%2520the%2520capability%2520to%2520capture%2520both%2520local%2520and%2520global%2520geometric%2520relationships.%2520Meanwhile%252C%2520CSMGC%2520establishes%2520geometric%2520consensus%2520via%2520a%2520cross-stage%2520sparse%2520graph%2520network%252C%2520ensuring%2520the%2520consistency%2520of%2520geometric%2520information%2520across%2520different%2520stages.%2520Experimental%2520results%2520on%2520two%2520representative%2520YFCC100M%2520and%2520SUN3D%2520datasets%2520show%2520that%2520MGCA-Net%2520significantly%2520outperforms%2520existing%2520SOTA%2520methods%2520in%2520the%2520outlier%2520rejection%2520and%2520camera%2520pose%2520estimation%2520tasks.%2520Source%2520code%2520is%2520available%2520at%2520http%253A//www.linshuyuan.com.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGCA-Net%3A%20Multi-Graph%20Contextual%20Attention%20Network%20for%20Two-View%20Correspondence%20Learning&entry.906535625=Shuyuan%20Lin%20and%20Mengtin%20Lo%20and%20Haosheng%20Chen%20and%20Yanjie%20Liang%20and%20Qiangqiang%20Wu&entry.1292438233=Two-view%20correspondence%20learning%20is%20a%20key%20task%20in%20computer%20vision%2C%20which%20aims%20to%20establish%20reliable%20matching%20relationships%20for%20applications%20such%20as%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20existing%20methods%20have%20limitations%20in%20local%20geometric%20modeling%20and%20cross-stage%20information%20optimization%2C%20which%20make%20it%20difficult%20to%20accurately%20capture%20the%20geometric%20constraints%20of%20matched%20pairs%20and%20thus%20reduce%20the%20robustness%20of%20the%20model.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Multi-Graph%20Contextual%20Attention%20Network%20%28MGCA-Net%29%2C%20which%20consists%20of%20a%20Contextual%20Geometric%20Attention%20%28CGA%29%20module%20and%20a%20Cross-Stage%20Multi-Graph%20Consensus%20%28CSMGC%29%20module.%20Specifically%2C%20CGA%20dynamically%20integrates%20spatial%20position%20and%20feature%20information%20via%20an%20adaptive%20attention%20mechanism%20and%20enhances%20the%20capability%20to%20capture%20both%20local%20and%20global%20geometric%20relationships.%20Meanwhile%2C%20CSMGC%20establishes%20geometric%20consensus%20via%20a%20cross-stage%20sparse%20graph%20network%2C%20ensuring%20the%20consistency%20of%20geometric%20information%20across%20different%20stages.%20Experimental%20results%20on%20two%20representative%20YFCC100M%20and%20SUN3D%20datasets%20show%20that%20MGCA-Net%20significantly%20outperforms%20existing%20SOTA%20methods%20in%20the%20outlier%20rejection%20and%20camera%20pose%20estimation%20tasks.%20Source%20code%20is%20available%20at%20http%3A//www.linshuyuan.com.&entry.1838667208=http%3A//arxiv.org/abs/2512.23369v1&entry.124074799=Read"},
{"title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge", "author": "Kabir Khan and Manju Sarkar and Anita Kar and Suresh Ghosh", "abstract": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.", "link": "http://arxiv.org/abs/2511.11585v3", "date": "2025-12-29", "relevancy": 2.2566, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5682}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5646}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20and%20Personalized%20Federated%20Training%20of%20Generative%20Models%20at%20the%20Edge&body=Title%3A%20Parameter-Efficient%20and%20Personalized%20Federated%20Training%20of%20Generative%20Models%20at%20the%20Edge%0AAuthor%3A%20Kabir%20Khan%20and%20Manju%20Sarkar%20and%20Anita%20Kar%20and%20Suresh%20Ghosh%0AAbstract%3A%20Large%20generative%20models%20%28for%20example%2C%20language%20and%20diffusion%20models%29%20enable%20high-quality%20text%20and%20image%20synthesis%20but%20are%20hard%20to%20train%20or%20adapt%20in%20cross-device%20federated%20settings%20due%20to%20heavy%20computation%20and%20communication%20and%20statistical/system%20heterogeneity.%20We%20propose%20FedGen-Edge%2C%20a%20framework%20that%20decouples%20a%20frozen%2C%20pre-trained%20global%20backbone%20from%20lightweight%20client-side%20adapters%20and%20federates%20only%20the%20adapters.%20Using%20Low-Rank%20Adaptation%20%28LoRA%29%20constrains%20client%20updates%20to%20a%20compact%20subspace%2C%20which%20reduces%20uplink%20traffic%20by%20more%20than%2099%20percent%20versus%20full-model%20FedAvg%2C%20stabilizes%20aggregation%20under%20non-IID%20data%2C%20and%20naturally%20supports%20personalization%20because%20each%20client%20can%20keep%20a%20locally%20tuned%20adapter.%20On%20language%20modeling%20%28PTB%29%20and%20image%20generation%20%28CIFAR-10%29%2C%20FedGen-Edge%20achieves%20lower%20perplexity/FID%20and%20faster%20convergence%20than%20strong%20baselines%20while%20retaining%20a%20simple%20FedAvg-style%20server.%20A%20brief%20ablation%20shows%20diminishing%20returns%20beyond%20moderate%20LoRA%20rank%20and%20a%20trade-off%20between%20local%20epochs%20and%20client%20drift.%20FedGen-Edge%20offers%20a%20practical%20path%20toward%20privacy-preserving%2C%20resource-aware%2C%20and%20personalized%20generative%20AI%20on%20heterogeneous%20edge%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11585v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520and%2520Personalized%2520Federated%2520Training%2520of%2520Generative%2520Models%2520at%2520the%2520Edge%26entry.906535625%3DKabir%2520Khan%2520and%2520Manju%2520Sarkar%2520and%2520Anita%2520Kar%2520and%2520Suresh%2520Ghosh%26entry.1292438233%3DLarge%2520generative%2520models%2520%2528for%2520example%252C%2520language%2520and%2520diffusion%2520models%2529%2520enable%2520high-quality%2520text%2520and%2520image%2520synthesis%2520but%2520are%2520hard%2520to%2520train%2520or%2520adapt%2520in%2520cross-device%2520federated%2520settings%2520due%2520to%2520heavy%2520computation%2520and%2520communication%2520and%2520statistical/system%2520heterogeneity.%2520We%2520propose%2520FedGen-Edge%252C%2520a%2520framework%2520that%2520decouples%2520a%2520frozen%252C%2520pre-trained%2520global%2520backbone%2520from%2520lightweight%2520client-side%2520adapters%2520and%2520federates%2520only%2520the%2520adapters.%2520Using%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520constrains%2520client%2520updates%2520to%2520a%2520compact%2520subspace%252C%2520which%2520reduces%2520uplink%2520traffic%2520by%2520more%2520than%252099%2520percent%2520versus%2520full-model%2520FedAvg%252C%2520stabilizes%2520aggregation%2520under%2520non-IID%2520data%252C%2520and%2520naturally%2520supports%2520personalization%2520because%2520each%2520client%2520can%2520keep%2520a%2520locally%2520tuned%2520adapter.%2520On%2520language%2520modeling%2520%2528PTB%2529%2520and%2520image%2520generation%2520%2528CIFAR-10%2529%252C%2520FedGen-Edge%2520achieves%2520lower%2520perplexity/FID%2520and%2520faster%2520convergence%2520than%2520strong%2520baselines%2520while%2520retaining%2520a%2520simple%2520FedAvg-style%2520server.%2520A%2520brief%2520ablation%2520shows%2520diminishing%2520returns%2520beyond%2520moderate%2520LoRA%2520rank%2520and%2520a%2520trade-off%2520between%2520local%2520epochs%2520and%2520client%2520drift.%2520FedGen-Edge%2520offers%2520a%2520practical%2520path%2520toward%2520privacy-preserving%252C%2520resource-aware%252C%2520and%2520personalized%2520generative%2520AI%2520on%2520heterogeneous%2520edge%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11585v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20and%20Personalized%20Federated%20Training%20of%20Generative%20Models%20at%20the%20Edge&entry.906535625=Kabir%20Khan%20and%20Manju%20Sarkar%20and%20Anita%20Kar%20and%20Suresh%20Ghosh&entry.1292438233=Large%20generative%20models%20%28for%20example%2C%20language%20and%20diffusion%20models%29%20enable%20high-quality%20text%20and%20image%20synthesis%20but%20are%20hard%20to%20train%20or%20adapt%20in%20cross-device%20federated%20settings%20due%20to%20heavy%20computation%20and%20communication%20and%20statistical/system%20heterogeneity.%20We%20propose%20FedGen-Edge%2C%20a%20framework%20that%20decouples%20a%20frozen%2C%20pre-trained%20global%20backbone%20from%20lightweight%20client-side%20adapters%20and%20federates%20only%20the%20adapters.%20Using%20Low-Rank%20Adaptation%20%28LoRA%29%20constrains%20client%20updates%20to%20a%20compact%20subspace%2C%20which%20reduces%20uplink%20traffic%20by%20more%20than%2099%20percent%20versus%20full-model%20FedAvg%2C%20stabilizes%20aggregation%20under%20non-IID%20data%2C%20and%20naturally%20supports%20personalization%20because%20each%20client%20can%20keep%20a%20locally%20tuned%20adapter.%20On%20language%20modeling%20%28PTB%29%20and%20image%20generation%20%28CIFAR-10%29%2C%20FedGen-Edge%20achieves%20lower%20perplexity/FID%20and%20faster%20convergence%20than%20strong%20baselines%20while%20retaining%20a%20simple%20FedAvg-style%20server.%20A%20brief%20ablation%20shows%20diminishing%20returns%20beyond%20moderate%20LoRA%20rank%20and%20a%20trade-off%20between%20local%20epochs%20and%20client%20drift.%20FedGen-Edge%20offers%20a%20practical%20path%20toward%20privacy-preserving%2C%20resource-aware%2C%20and%20personalized%20generative%20AI%20on%20heterogeneous%20edge%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2511.11585v3&entry.124074799=Read"},
{"title": "Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment", "author": "Henglin Liu and Nisha Huang and Chang Liu and Jiangpeng Yan and Huijuan Huang and Jixuan Ying and Tong-Yee Lee and Pengfei Wan and Xiangyang Ji", "abstract": "The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.", "link": "http://arxiv.org/abs/2512.23413v1", "date": "2025-12-29", "relevancy": 2.2539, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5752}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Cognitive%20Gap%3A%20Hierarchical%20Description%20Learning%20for%20Artistic%20Image%20Aesthetics%20Assessment&body=Title%3A%20Bridging%20Cognitive%20Gap%3A%20Hierarchical%20Description%20Learning%20for%20Artistic%20Image%20Aesthetics%20Assessment%0AAuthor%3A%20Henglin%20Liu%20and%20Nisha%20Huang%20and%20Chang%20Liu%20and%20Jiangpeng%20Yan%20and%20Huijuan%20Huang%20and%20Jixuan%20Ying%20and%20Tong-Yee%20Lee%20and%20Pengfei%20Wan%20and%20Xiangyang%20Ji%0AAbstract%3A%20The%20aesthetic%20quality%20assessment%20task%20is%20crucial%20for%20developing%20a%20human-aligned%20quantitative%20evaluation%20system%20for%20AIGC.%20However%2C%20its%20inherently%20complex%20nature%2C%20spanning%20visual%20perception%2C%20cognition%2C%20and%20emotion%2C%20poses%20fundamental%20challenges.%20Although%20aesthetic%20descriptions%20offer%20a%20viable%20representation%20of%20this%20complexity%2C%20two%20critical%20challenges%20persist%3A%20%281%29%20data%20scarcity%20and%20imbalance%3A%20existing%20dataset%20overly%20focuses%20on%20visual%20perception%20and%20neglects%20deeper%20dimensions%20due%20to%20the%20expensive%20manual%20annotation%3B%20and%20%282%29%20model%20fragmentation%3A%20current%20visual%20networks%20isolate%20aesthetic%20attributes%20with%20multi-branch%20encoder%2C%20while%20multimodal%20methods%20represented%20by%20contrastive%20learning%20struggle%20to%20effectively%20process%20long-form%20textual%20descriptions.%20To%20resolve%20challenge%20%281%29%2C%20we%20first%20present%20the%20Refined%20Aesthetic%20Description%20%28RAD%29%20dataset%2C%20a%20large-scale%20%2870k%29%2C%20multi-dimensional%20structured%20dataset%2C%20generated%20via%20an%20iterative%20pipeline%20without%20heavy%20annotation%20costs%20and%20easy%20to%20scale.%20To%20address%20challenge%20%282%29%2C%20we%20propose%20ArtQuant%2C%20an%20aesthetics%20assessment%20framework%20for%20artistic%20images%20which%20not%20only%20couples%20isolated%20aesthetic%20dimensions%20through%20joint%20description%20generation%2C%20but%20also%20better%20models%20long-text%20semantics%20with%20the%20help%20of%20LLM%20decoders.%20Besides%2C%20theoretical%20analysis%20confirms%20this%20symbiosis%3A%20RAD%27s%20semantic%20adequacy%20%28data%29%20and%20generation%20paradigm%20%28model%29%20collectively%20minimize%20prediction%20entropy%2C%20providing%20mathematical%20grounding%20for%20the%20framework.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20several%20datasets%20while%20requiring%20only%2033%25%20of%20conventional%20training%20epochs%2C%20narrowing%20the%20cognitive%20gap%20between%20artistic%20images%20and%20aesthetic%20judgment.%20We%20will%20release%20both%20code%20and%20dataset%20to%20support%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Cognitive%2520Gap%253A%2520Hierarchical%2520Description%2520Learning%2520for%2520Artistic%2520Image%2520Aesthetics%2520Assessment%26entry.906535625%3DHenglin%2520Liu%2520and%2520Nisha%2520Huang%2520and%2520Chang%2520Liu%2520and%2520Jiangpeng%2520Yan%2520and%2520Huijuan%2520Huang%2520and%2520Jixuan%2520Ying%2520and%2520Tong-Yee%2520Lee%2520and%2520Pengfei%2520Wan%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3DThe%2520aesthetic%2520quality%2520assessment%2520task%2520is%2520crucial%2520for%2520developing%2520a%2520human-aligned%2520quantitative%2520evaluation%2520system%2520for%2520AIGC.%2520However%252C%2520its%2520inherently%2520complex%2520nature%252C%2520spanning%2520visual%2520perception%252C%2520cognition%252C%2520and%2520emotion%252C%2520poses%2520fundamental%2520challenges.%2520Although%2520aesthetic%2520descriptions%2520offer%2520a%2520viable%2520representation%2520of%2520this%2520complexity%252C%2520two%2520critical%2520challenges%2520persist%253A%2520%25281%2529%2520data%2520scarcity%2520and%2520imbalance%253A%2520existing%2520dataset%2520overly%2520focuses%2520on%2520visual%2520perception%2520and%2520neglects%2520deeper%2520dimensions%2520due%2520to%2520the%2520expensive%2520manual%2520annotation%253B%2520and%2520%25282%2529%2520model%2520fragmentation%253A%2520current%2520visual%2520networks%2520isolate%2520aesthetic%2520attributes%2520with%2520multi-branch%2520encoder%252C%2520while%2520multimodal%2520methods%2520represented%2520by%2520contrastive%2520learning%2520struggle%2520to%2520effectively%2520process%2520long-form%2520textual%2520descriptions.%2520To%2520resolve%2520challenge%2520%25281%2529%252C%2520we%2520first%2520present%2520the%2520Refined%2520Aesthetic%2520Description%2520%2528RAD%2529%2520dataset%252C%2520a%2520large-scale%2520%252870k%2529%252C%2520multi-dimensional%2520structured%2520dataset%252C%2520generated%2520via%2520an%2520iterative%2520pipeline%2520without%2520heavy%2520annotation%2520costs%2520and%2520easy%2520to%2520scale.%2520To%2520address%2520challenge%2520%25282%2529%252C%2520we%2520propose%2520ArtQuant%252C%2520an%2520aesthetics%2520assessment%2520framework%2520for%2520artistic%2520images%2520which%2520not%2520only%2520couples%2520isolated%2520aesthetic%2520dimensions%2520through%2520joint%2520description%2520generation%252C%2520but%2520also%2520better%2520models%2520long-text%2520semantics%2520with%2520the%2520help%2520of%2520LLM%2520decoders.%2520Besides%252C%2520theoretical%2520analysis%2520confirms%2520this%2520symbiosis%253A%2520RAD%2527s%2520semantic%2520adequacy%2520%2528data%2529%2520and%2520generation%2520paradigm%2520%2528model%2529%2520collectively%2520minimize%2520prediction%2520entropy%252C%2520providing%2520mathematical%2520grounding%2520for%2520the%2520framework.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520several%2520datasets%2520while%2520requiring%2520only%252033%2525%2520of%2520conventional%2520training%2520epochs%252C%2520narrowing%2520the%2520cognitive%2520gap%2520between%2520artistic%2520images%2520and%2520aesthetic%2520judgment.%2520We%2520will%2520release%2520both%2520code%2520and%2520dataset%2520to%2520support%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Cognitive%20Gap%3A%20Hierarchical%20Description%20Learning%20for%20Artistic%20Image%20Aesthetics%20Assessment&entry.906535625=Henglin%20Liu%20and%20Nisha%20Huang%20and%20Chang%20Liu%20and%20Jiangpeng%20Yan%20and%20Huijuan%20Huang%20and%20Jixuan%20Ying%20and%20Tong-Yee%20Lee%20and%20Pengfei%20Wan%20and%20Xiangyang%20Ji&entry.1292438233=The%20aesthetic%20quality%20assessment%20task%20is%20crucial%20for%20developing%20a%20human-aligned%20quantitative%20evaluation%20system%20for%20AIGC.%20However%2C%20its%20inherently%20complex%20nature%2C%20spanning%20visual%20perception%2C%20cognition%2C%20and%20emotion%2C%20poses%20fundamental%20challenges.%20Although%20aesthetic%20descriptions%20offer%20a%20viable%20representation%20of%20this%20complexity%2C%20two%20critical%20challenges%20persist%3A%20%281%29%20data%20scarcity%20and%20imbalance%3A%20existing%20dataset%20overly%20focuses%20on%20visual%20perception%20and%20neglects%20deeper%20dimensions%20due%20to%20the%20expensive%20manual%20annotation%3B%20and%20%282%29%20model%20fragmentation%3A%20current%20visual%20networks%20isolate%20aesthetic%20attributes%20with%20multi-branch%20encoder%2C%20while%20multimodal%20methods%20represented%20by%20contrastive%20learning%20struggle%20to%20effectively%20process%20long-form%20textual%20descriptions.%20To%20resolve%20challenge%20%281%29%2C%20we%20first%20present%20the%20Refined%20Aesthetic%20Description%20%28RAD%29%20dataset%2C%20a%20large-scale%20%2870k%29%2C%20multi-dimensional%20structured%20dataset%2C%20generated%20via%20an%20iterative%20pipeline%20without%20heavy%20annotation%20costs%20and%20easy%20to%20scale.%20To%20address%20challenge%20%282%29%2C%20we%20propose%20ArtQuant%2C%20an%20aesthetics%20assessment%20framework%20for%20artistic%20images%20which%20not%20only%20couples%20isolated%20aesthetic%20dimensions%20through%20joint%20description%20generation%2C%20but%20also%20better%20models%20long-text%20semantics%20with%20the%20help%20of%20LLM%20decoders.%20Besides%2C%20theoretical%20analysis%20confirms%20this%20symbiosis%3A%20RAD%27s%20semantic%20adequacy%20%28data%29%20and%20generation%20paradigm%20%28model%29%20collectively%20minimize%20prediction%20entropy%2C%20providing%20mathematical%20grounding%20for%20the%20framework.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20several%20datasets%20while%20requiring%20only%2033%25%20of%20conventional%20training%20epochs%2C%20narrowing%20the%20cognitive%20gap%20between%20artistic%20images%20and%20aesthetic%20judgment.%20We%20will%20release%20both%20code%20and%20dataset%20to%20support%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.23413v1&entry.124074799=Read"},
{"title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis", "author": "Shengyi Hua and Jianfeng Wu and Tianle Shen and Kangzhe Hu and Zhongzhen Huang and Shujuan Ni and Zhihong Zhang and Yuan Li and Zhe Wang and Xiaofan Zhang", "abstract": "Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.", "link": "http://arxiv.org/abs/2512.23545v1", "date": "2025-12-29", "relevancy": 2.2415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathFound%3A%20An%20Agentic%20Multimodal%20Model%20Activating%20Evidence-seeking%20Pathological%20Diagnosis&body=Title%3A%20PathFound%3A%20An%20Agentic%20Multimodal%20Model%20Activating%20Evidence-seeking%20Pathological%20Diagnosis%0AAuthor%3A%20Shengyi%20Hua%20and%20Jianfeng%20Wu%20and%20Tianle%20Shen%20and%20Kangzhe%20Hu%20and%20Zhongzhen%20Huang%20and%20Shujuan%20Ni%20and%20Zhihong%20Zhang%20and%20Yuan%20Li%20and%20Zhe%20Wang%20and%20Xiaofan%20Zhang%0AAbstract%3A%20Recent%20pathological%20foundation%20models%20have%20substantially%20advanced%20visual%20representation%20learning%20and%20multimodal%20interaction.%20However%2C%20most%20models%20still%20rely%20on%20a%20static%20inference%20paradigm%20in%20which%20whole-slide%20images%20are%20processed%20once%20to%20produce%20predictions%2C%20without%20reassessment%20or%20targeted%20evidence%20acquisition%20under%20ambiguous%20diagnoses.%20This%20contrasts%20with%20clinical%20diagnostic%20workflows%20that%20refine%20hypotheses%20through%20repeated%20slide%20observations%20and%20further%20examination%20requests.%20We%20propose%20PathFound%2C%20an%20agentic%20multimodal%20model%20designed%20to%20support%20evidence-seeking%20inference%20in%20pathological%20diagnosis.%20PathFound%20integrates%20the%20power%20of%20pathological%20visual%20foundation%20models%2C%20vision-language%20models%2C%20and%20reasoning%20models%20trained%20with%20reinforcement%20learning%20to%20perform%20proactive%20information%20acquisition%20and%20diagnosis%20refinement%20by%20progressing%20through%20the%20initial%20diagnosis%2C%20evidence-seeking%2C%20and%20final%20decision%20stages.%20Across%20several%20large%20multimodal%20models%2C%20adopting%20this%20strategy%20consistently%20improves%20diagnostic%20accuracy%2C%20indicating%20the%20effectiveness%20of%20evidence-seeking%20workflows%20in%20computational%20pathology.%20Among%20these%20models%2C%20PathFound%20achieves%20state-of-the-art%20diagnostic%20performance%20across%20diverse%20clinical%20scenarios%20and%20demonstrates%20strong%20potential%20to%20discover%20subtle%20details%2C%20such%20as%20nuclear%20features%20and%20local%20invasions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathFound%253A%2520An%2520Agentic%2520Multimodal%2520Model%2520Activating%2520Evidence-seeking%2520Pathological%2520Diagnosis%26entry.906535625%3DShengyi%2520Hua%2520and%2520Jianfeng%2520Wu%2520and%2520Tianle%2520Shen%2520and%2520Kangzhe%2520Hu%2520and%2520Zhongzhen%2520Huang%2520and%2520Shujuan%2520Ni%2520and%2520Zhihong%2520Zhang%2520and%2520Yuan%2520Li%2520and%2520Zhe%2520Wang%2520and%2520Xiaofan%2520Zhang%26entry.1292438233%3DRecent%2520pathological%2520foundation%2520models%2520have%2520substantially%2520advanced%2520visual%2520representation%2520learning%2520and%2520multimodal%2520interaction.%2520However%252C%2520most%2520models%2520still%2520rely%2520on%2520a%2520static%2520inference%2520paradigm%2520in%2520which%2520whole-slide%2520images%2520are%2520processed%2520once%2520to%2520produce%2520predictions%252C%2520without%2520reassessment%2520or%2520targeted%2520evidence%2520acquisition%2520under%2520ambiguous%2520diagnoses.%2520This%2520contrasts%2520with%2520clinical%2520diagnostic%2520workflows%2520that%2520refine%2520hypotheses%2520through%2520repeated%2520slide%2520observations%2520and%2520further%2520examination%2520requests.%2520We%2520propose%2520PathFound%252C%2520an%2520agentic%2520multimodal%2520model%2520designed%2520to%2520support%2520evidence-seeking%2520inference%2520in%2520pathological%2520diagnosis.%2520PathFound%2520integrates%2520the%2520power%2520of%2520pathological%2520visual%2520foundation%2520models%252C%2520vision-language%2520models%252C%2520and%2520reasoning%2520models%2520trained%2520with%2520reinforcement%2520learning%2520to%2520perform%2520proactive%2520information%2520acquisition%2520and%2520diagnosis%2520refinement%2520by%2520progressing%2520through%2520the%2520initial%2520diagnosis%252C%2520evidence-seeking%252C%2520and%2520final%2520decision%2520stages.%2520Across%2520several%2520large%2520multimodal%2520models%252C%2520adopting%2520this%2520strategy%2520consistently%2520improves%2520diagnostic%2520accuracy%252C%2520indicating%2520the%2520effectiveness%2520of%2520evidence-seeking%2520workflows%2520in%2520computational%2520pathology.%2520Among%2520these%2520models%252C%2520PathFound%2520achieves%2520state-of-the-art%2520diagnostic%2520performance%2520across%2520diverse%2520clinical%2520scenarios%2520and%2520demonstrates%2520strong%2520potential%2520to%2520discover%2520subtle%2520details%252C%2520such%2520as%2520nuclear%2520features%2520and%2520local%2520invasions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathFound%3A%20An%20Agentic%20Multimodal%20Model%20Activating%20Evidence-seeking%20Pathological%20Diagnosis&entry.906535625=Shengyi%20Hua%20and%20Jianfeng%20Wu%20and%20Tianle%20Shen%20and%20Kangzhe%20Hu%20and%20Zhongzhen%20Huang%20and%20Shujuan%20Ni%20and%20Zhihong%20Zhang%20and%20Yuan%20Li%20and%20Zhe%20Wang%20and%20Xiaofan%20Zhang&entry.1292438233=Recent%20pathological%20foundation%20models%20have%20substantially%20advanced%20visual%20representation%20learning%20and%20multimodal%20interaction.%20However%2C%20most%20models%20still%20rely%20on%20a%20static%20inference%20paradigm%20in%20which%20whole-slide%20images%20are%20processed%20once%20to%20produce%20predictions%2C%20without%20reassessment%20or%20targeted%20evidence%20acquisition%20under%20ambiguous%20diagnoses.%20This%20contrasts%20with%20clinical%20diagnostic%20workflows%20that%20refine%20hypotheses%20through%20repeated%20slide%20observations%20and%20further%20examination%20requests.%20We%20propose%20PathFound%2C%20an%20agentic%20multimodal%20model%20designed%20to%20support%20evidence-seeking%20inference%20in%20pathological%20diagnosis.%20PathFound%20integrates%20the%20power%20of%20pathological%20visual%20foundation%20models%2C%20vision-language%20models%2C%20and%20reasoning%20models%20trained%20with%20reinforcement%20learning%20to%20perform%20proactive%20information%20acquisition%20and%20diagnosis%20refinement%20by%20progressing%20through%20the%20initial%20diagnosis%2C%20evidence-seeking%2C%20and%20final%20decision%20stages.%20Across%20several%20large%20multimodal%20models%2C%20adopting%20this%20strategy%20consistently%20improves%20diagnostic%20accuracy%2C%20indicating%20the%20effectiveness%20of%20evidence-seeking%20workflows%20in%20computational%20pathology.%20Among%20these%20models%2C%20PathFound%20achieves%20state-of-the-art%20diagnostic%20performance%20across%20diverse%20clinical%20scenarios%20and%20demonstrates%20strong%20potential%20to%20discover%20subtle%20details%2C%20such%20as%20nuclear%20features%20and%20local%20invasions.&entry.1838667208=http%3A//arxiv.org/abs/2512.23545v1&entry.124074799=Read"},
{"title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation", "author": "Mouhyemen Khan and Tatsuya Ibuki and Abhijit Chatterjee", "abstract": "Level set methods underpin modern safety techniques such as control barrier functions (CBFs), while also serving as implicit surface representations for geometric shapes via distance fields. Inspired by these two paradigms, we propose a unified framework where the implicit surface itself acts as a CBF. We leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety boundaries, using safety samples which are derived from sensor measurements to condition the GP. The GP posterior mean defines the implicit safety surface (safety belief), while the posterior variance provides a robust safety margin. Although GPs have favorable properties such as uncertainty estimation and analytical tractability, they scale cubically with data. To alleviate this issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of our knowledge, GPIS have not been explicitly used to synthesize CBFs. We validate the approach on collision avoidance tasks in two settings: a simulated 7-DOF manipulator operating around the Stanford bunny, and a quadrotor navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with and without sparsity) enable safe interaction and collision-free execution of trajectories that would otherwise intersect the objects.", "link": "http://arxiv.org/abs/2510.12919v2", "date": "2025-12-29", "relevancy": 2.2413, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5759}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5694}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Implicit%20Surfaces%20as%20Control%20Barrier%20Functions%20for%20Safe%20Robot%20Navigation&body=Title%3A%20Gaussian%20Process%20Implicit%20Surfaces%20as%20Control%20Barrier%20Functions%20for%20Safe%20Robot%20Navigation%0AAuthor%3A%20Mouhyemen%20Khan%20and%20Tatsuya%20Ibuki%20and%20Abhijit%20Chatterjee%0AAbstract%3A%20Level%20set%20methods%20underpin%20modern%20safety%20techniques%20such%20as%20control%20barrier%20functions%20%28CBFs%29%2C%20while%20also%20serving%20as%20implicit%20surface%20representations%20for%20geometric%20shapes%20via%20distance%20fields.%20Inspired%20by%20these%20two%20paradigms%2C%20we%20propose%20a%20unified%20framework%20where%20the%20implicit%20surface%20itself%20acts%20as%20a%20CBF.%20We%20leverage%20Gaussian%20process%20%28GP%29%20implicit%20surface%20%28GPIS%29%20to%20represent%20the%20safety%20boundaries%2C%20using%20safety%20samples%20which%20are%20derived%20from%20sensor%20measurements%20to%20condition%20the%20GP.%20The%20GP%20posterior%20mean%20defines%20the%20implicit%20safety%20surface%20%28safety%20belief%29%2C%20while%20the%20posterior%20variance%20provides%20a%20robust%20safety%20margin.%20Although%20GPs%20have%20favorable%20properties%20such%20as%20uncertainty%20estimation%20and%20analytical%20tractability%2C%20they%20scale%20cubically%20with%20data.%20To%20alleviate%20this%20issue%2C%20we%20develop%20a%20sparse%20solution%20called%20sparse%20Gaussian%20CBFs.%20To%20the%20best%20of%20our%20knowledge%2C%20GPIS%20have%20not%20been%20explicitly%20used%20to%20synthesize%20CBFs.%20We%20validate%20the%20approach%20on%20collision%20avoidance%20tasks%20in%20two%20settings%3A%20a%20simulated%207-DOF%20manipulator%20operating%20around%20the%20Stanford%20bunny%2C%20and%20a%20quadrotor%20navigating%20in%203D%20around%20a%20physical%20chair.%20In%20both%20cases%2C%20Gaussian%20CBFs%20%28with%20and%20without%20sparsity%29%20enable%20safe%20interaction%20and%20collision-free%20execution%20of%20trajectories%20that%20would%20otherwise%20intersect%20the%20objects.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Implicit%2520Surfaces%2520as%2520Control%2520Barrier%2520Functions%2520for%2520Safe%2520Robot%2520Navigation%26entry.906535625%3DMouhyemen%2520Khan%2520and%2520Tatsuya%2520Ibuki%2520and%2520Abhijit%2520Chatterjee%26entry.1292438233%3DLevel%2520set%2520methods%2520underpin%2520modern%2520safety%2520techniques%2520such%2520as%2520control%2520barrier%2520functions%2520%2528CBFs%2529%252C%2520while%2520also%2520serving%2520as%2520implicit%2520surface%2520representations%2520for%2520geometric%2520shapes%2520via%2520distance%2520fields.%2520Inspired%2520by%2520these%2520two%2520paradigms%252C%2520we%2520propose%2520a%2520unified%2520framework%2520where%2520the%2520implicit%2520surface%2520itself%2520acts%2520as%2520a%2520CBF.%2520We%2520leverage%2520Gaussian%2520process%2520%2528GP%2529%2520implicit%2520surface%2520%2528GPIS%2529%2520to%2520represent%2520the%2520safety%2520boundaries%252C%2520using%2520safety%2520samples%2520which%2520are%2520derived%2520from%2520sensor%2520measurements%2520to%2520condition%2520the%2520GP.%2520The%2520GP%2520posterior%2520mean%2520defines%2520the%2520implicit%2520safety%2520surface%2520%2528safety%2520belief%2529%252C%2520while%2520the%2520posterior%2520variance%2520provides%2520a%2520robust%2520safety%2520margin.%2520Although%2520GPs%2520have%2520favorable%2520properties%2520such%2520as%2520uncertainty%2520estimation%2520and%2520analytical%2520tractability%252C%2520they%2520scale%2520cubically%2520with%2520data.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520develop%2520a%2520sparse%2520solution%2520called%2520sparse%2520Gaussian%2520CBFs.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520GPIS%2520have%2520not%2520been%2520explicitly%2520used%2520to%2520synthesize%2520CBFs.%2520We%2520validate%2520the%2520approach%2520on%2520collision%2520avoidance%2520tasks%2520in%2520two%2520settings%253A%2520a%2520simulated%25207-DOF%2520manipulator%2520operating%2520around%2520the%2520Stanford%2520bunny%252C%2520and%2520a%2520quadrotor%2520navigating%2520in%25203D%2520around%2520a%2520physical%2520chair.%2520In%2520both%2520cases%252C%2520Gaussian%2520CBFs%2520%2528with%2520and%2520without%2520sparsity%2529%2520enable%2520safe%2520interaction%2520and%2520collision-free%2520execution%2520of%2520trajectories%2520that%2520would%2520otherwise%2520intersect%2520the%2520objects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Implicit%20Surfaces%20as%20Control%20Barrier%20Functions%20for%20Safe%20Robot%20Navigation&entry.906535625=Mouhyemen%20Khan%20and%20Tatsuya%20Ibuki%20and%20Abhijit%20Chatterjee&entry.1292438233=Level%20set%20methods%20underpin%20modern%20safety%20techniques%20such%20as%20control%20barrier%20functions%20%28CBFs%29%2C%20while%20also%20serving%20as%20implicit%20surface%20representations%20for%20geometric%20shapes%20via%20distance%20fields.%20Inspired%20by%20these%20two%20paradigms%2C%20we%20propose%20a%20unified%20framework%20where%20the%20implicit%20surface%20itself%20acts%20as%20a%20CBF.%20We%20leverage%20Gaussian%20process%20%28GP%29%20implicit%20surface%20%28GPIS%29%20to%20represent%20the%20safety%20boundaries%2C%20using%20safety%20samples%20which%20are%20derived%20from%20sensor%20measurements%20to%20condition%20the%20GP.%20The%20GP%20posterior%20mean%20defines%20the%20implicit%20safety%20surface%20%28safety%20belief%29%2C%20while%20the%20posterior%20variance%20provides%20a%20robust%20safety%20margin.%20Although%20GPs%20have%20favorable%20properties%20such%20as%20uncertainty%20estimation%20and%20analytical%20tractability%2C%20they%20scale%20cubically%20with%20data.%20To%20alleviate%20this%20issue%2C%20we%20develop%20a%20sparse%20solution%20called%20sparse%20Gaussian%20CBFs.%20To%20the%20best%20of%20our%20knowledge%2C%20GPIS%20have%20not%20been%20explicitly%20used%20to%20synthesize%20CBFs.%20We%20validate%20the%20approach%20on%20collision%20avoidance%20tasks%20in%20two%20settings%3A%20a%20simulated%207-DOF%20manipulator%20operating%20around%20the%20Stanford%20bunny%2C%20and%20a%20quadrotor%20navigating%20in%203D%20around%20a%20physical%20chair.%20In%20both%20cases%2C%20Gaussian%20CBFs%20%28with%20and%20without%20sparsity%29%20enable%20safe%20interaction%20and%20collision-free%20execution%20of%20trajectories%20that%20would%20otherwise%20intersect%20the%20objects.&entry.1838667208=http%3A//arxiv.org/abs/2510.12919v2&entry.124074799=Read"},
{"title": "AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization", "author": "Binhe Yu and Zhen Wang and Kexin Li and Yuqian Yuan and Wenqiao Zhang and Long Chen and Juncheng Li and Jun Xiao and Yueting Zhuang", "abstract": "Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.", "link": "http://arxiv.org/abs/2512.23537v1", "date": "2025-12-29", "relevancy": 2.2404, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5763}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyMS%3A%20Bottom-up%20Attention%20Decoupling%20for%20Layout-guided%20and%20Training-free%20Multi-subject%20Customization&body=Title%3A%20AnyMS%3A%20Bottom-up%20Attention%20Decoupling%20for%20Layout-guided%20and%20Training-free%20Multi-subject%20Customization%0AAuthor%3A%20Binhe%20Yu%20and%20Zhen%20Wang%20and%20Kexin%20Li%20and%20Yuqian%20Yuan%20and%20Wenqiao%20Zhang%20and%20Long%20Chen%20and%20Juncheng%20Li%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20Multi-subject%20customization%20aims%20to%20synthesize%20multiple%20user-specified%20subjects%20into%20a%20coherent%20image.%20To%20address%20issues%20such%20as%20subjects%20missing%20or%20conflicts%2C%20recent%20works%20incorporate%20layout%20guidance%20to%20provide%20explicit%20spatial%20constraints.%20However%2C%20existing%20methods%20still%20struggle%20to%20balance%20three%20critical%20objectives%3A%20text%20alignment%2C%20subject%20identity%20preservation%2C%20and%20layout%20control%2C%20while%20the%20reliance%20on%20additional%20training%20further%20limits%20their%20scalability%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20AnyMS%2C%20a%20novel%20training-free%20framework%20for%20layout-guided%20multi-subject%20customization.%20AnyMS%20leverages%20three%20input%20conditions%3A%20text%20prompt%2C%20subject%20images%2C%20and%20layout%20constraints%2C%20and%20introduces%20a%20bottom-up%20dual-level%20attention%20decoupling%20mechanism%20to%20harmonize%20their%20integration%20during%20generation.%20Specifically%2C%20global%20decoupling%20separates%20cross-attention%20between%20textual%20and%20visual%20conditions%20to%20ensure%20text%20alignment.%20Local%20decoupling%20confines%20each%20subject%27s%20attention%20to%20its%20designated%20area%2C%20which%20prevents%20subject%20conflicts%20and%20thus%20guarantees%20identity%20preservation%20and%20layout%20control.%20Moreover%2C%20AnyMS%20employs%20pre-trained%20image%20adapters%20to%20extract%20subject-specific%20features%20aligned%20with%20the%20diffusion%20model%2C%20removing%20the%20need%20for%20subject%20learning%20or%20adapter%20tuning.%20Extensive%20experiments%20demonstrate%20that%20AnyMS%20achieves%20state-of-the-art%20performance%2C%20supporting%20complex%20compositions%20and%20scaling%20to%20a%20larger%20number%20of%20subjects.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyMS%253A%2520Bottom-up%2520Attention%2520Decoupling%2520for%2520Layout-guided%2520and%2520Training-free%2520Multi-subject%2520Customization%26entry.906535625%3DBinhe%2520Yu%2520and%2520Zhen%2520Wang%2520and%2520Kexin%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Wenqiao%2520Zhang%2520and%2520Long%2520Chen%2520and%2520Juncheng%2520Li%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3DMulti-subject%2520customization%2520aims%2520to%2520synthesize%2520multiple%2520user-specified%2520subjects%2520into%2520a%2520coherent%2520image.%2520To%2520address%2520issues%2520such%2520as%2520subjects%2520missing%2520or%2520conflicts%252C%2520recent%2520works%2520incorporate%2520layout%2520guidance%2520to%2520provide%2520explicit%2520spatial%2520constraints.%2520However%252C%2520existing%2520methods%2520still%2520struggle%2520to%2520balance%2520three%2520critical%2520objectives%253A%2520text%2520alignment%252C%2520subject%2520identity%2520preservation%252C%2520and%2520layout%2520control%252C%2520while%2520the%2520reliance%2520on%2520additional%2520training%2520further%2520limits%2520their%2520scalability%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520present%2520AnyMS%252C%2520a%2520novel%2520training-free%2520framework%2520for%2520layout-guided%2520multi-subject%2520customization.%2520AnyMS%2520leverages%2520three%2520input%2520conditions%253A%2520text%2520prompt%252C%2520subject%2520images%252C%2520and%2520layout%2520constraints%252C%2520and%2520introduces%2520a%2520bottom-up%2520dual-level%2520attention%2520decoupling%2520mechanism%2520to%2520harmonize%2520their%2520integration%2520during%2520generation.%2520Specifically%252C%2520global%2520decoupling%2520separates%2520cross-attention%2520between%2520textual%2520and%2520visual%2520conditions%2520to%2520ensure%2520text%2520alignment.%2520Local%2520decoupling%2520confines%2520each%2520subject%2527s%2520attention%2520to%2520its%2520designated%2520area%252C%2520which%2520prevents%2520subject%2520conflicts%2520and%2520thus%2520guarantees%2520identity%2520preservation%2520and%2520layout%2520control.%2520Moreover%252C%2520AnyMS%2520employs%2520pre-trained%2520image%2520adapters%2520to%2520extract%2520subject-specific%2520features%2520aligned%2520with%2520the%2520diffusion%2520model%252C%2520removing%2520the%2520need%2520for%2520subject%2520learning%2520or%2520adapter%2520tuning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520AnyMS%2520achieves%2520state-of-the-art%2520performance%252C%2520supporting%2520complex%2520compositions%2520and%2520scaling%2520to%2520a%2520larger%2520number%2520of%2520subjects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyMS%3A%20Bottom-up%20Attention%20Decoupling%20for%20Layout-guided%20and%20Training-free%20Multi-subject%20Customization&entry.906535625=Binhe%20Yu%20and%20Zhen%20Wang%20and%20Kexin%20Li%20and%20Yuqian%20Yuan%20and%20Wenqiao%20Zhang%20and%20Long%20Chen%20and%20Juncheng%20Li%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=Multi-subject%20customization%20aims%20to%20synthesize%20multiple%20user-specified%20subjects%20into%20a%20coherent%20image.%20To%20address%20issues%20such%20as%20subjects%20missing%20or%20conflicts%2C%20recent%20works%20incorporate%20layout%20guidance%20to%20provide%20explicit%20spatial%20constraints.%20However%2C%20existing%20methods%20still%20struggle%20to%20balance%20three%20critical%20objectives%3A%20text%20alignment%2C%20subject%20identity%20preservation%2C%20and%20layout%20control%2C%20while%20the%20reliance%20on%20additional%20training%20further%20limits%20their%20scalability%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20AnyMS%2C%20a%20novel%20training-free%20framework%20for%20layout-guided%20multi-subject%20customization.%20AnyMS%20leverages%20three%20input%20conditions%3A%20text%20prompt%2C%20subject%20images%2C%20and%20layout%20constraints%2C%20and%20introduces%20a%20bottom-up%20dual-level%20attention%20decoupling%20mechanism%20to%20harmonize%20their%20integration%20during%20generation.%20Specifically%2C%20global%20decoupling%20separates%20cross-attention%20between%20textual%20and%20visual%20conditions%20to%20ensure%20text%20alignment.%20Local%20decoupling%20confines%20each%20subject%27s%20attention%20to%20its%20designated%20area%2C%20which%20prevents%20subject%20conflicts%20and%20thus%20guarantees%20identity%20preservation%20and%20layout%20control.%20Moreover%2C%20AnyMS%20employs%20pre-trained%20image%20adapters%20to%20extract%20subject-specific%20features%20aligned%20with%20the%20diffusion%20model%2C%20removing%20the%20need%20for%20subject%20learning%20or%20adapter%20tuning.%20Extensive%20experiments%20demonstrate%20that%20AnyMS%20achieves%20state-of-the-art%20performance%2C%20supporting%20complex%20compositions%20and%20scaling%20to%20a%20larger%20number%20of%20subjects.&entry.1838667208=http%3A//arxiv.org/abs/2512.23537v1&entry.124074799=Read"},
{"title": "PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation", "author": "Zongsheng Cao and Yangfan He and Anran Liu and Jun Xie and Feng Chen and Zepeng Wang", "abstract": "Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.", "link": "http://arxiv.org/abs/2512.23546v1", "date": "2025-12-29", "relevancy": 2.2386, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.57}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5605}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PurifyGen%3A%20A%20Risk-Discrimination%20and%20Semantic-Purification%20Model%20for%20Safe%20Text-to-Image%20Generation&body=Title%3A%20PurifyGen%3A%20A%20Risk-Discrimination%20and%20Semantic-Purification%20Model%20for%20Safe%20Text-to-Image%20Generation%0AAuthor%3A%20Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Jun%20Xie%20and%20Feng%20Chen%20and%20Zepeng%20Wang%0AAbstract%3A%20Recent%20advances%20in%20diffusion%20models%20have%20notably%20enhanced%20text-to-image%20%28T2I%29%20generation%20quality%2C%20but%20they%20also%20raise%20the%20risk%20of%20generating%20unsafe%20content.%20Traditional%20safety%20methods%20like%20text%20blacklisting%20or%20harmful%20content%20classification%20have%20significant%20drawbacks%3A%20they%20can%20be%20easily%20circumvented%20or%20require%20extensive%20datasets%20and%20extra%20training.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20PurifyGen%2C%20a%20novel%2C%20training-free%20approach%20for%20safe%20T2I%20generation%20that%20retains%20the%20model%27s%20original%20weights.%20PurifyGen%20introduces%20a%20dual-stage%20strategy%20for%20prompt%20purification.%20First%2C%20we%20evaluate%20the%20safety%20of%20each%20token%20in%20a%20prompt%20by%20computing%20its%20complementary%20semantic%20distance%2C%20which%20measures%20the%20semantic%20proximity%20between%20the%20prompt%20tokens%20and%20concept%20embeddings%20from%20predefined%20toxic%20and%20clean%20lists.%20This%20enables%20fine-grained%20prompt%20classification%20without%20explicit%20keyword%20matching%20or%20retraining.%20Tokens%20closer%20to%20toxic%20concepts%20are%20flagged%20as%20risky.%20Second%2C%20for%20risky%20prompts%2C%20we%20apply%20a%20dual-space%20transformation%3A%20we%20project%20toxic-aligned%20embeddings%20into%20the%20null%20space%20of%20the%20toxic%20concept%20matrix%2C%20effectively%20removing%20harmful%20semantic%20components%2C%20and%20simultaneously%20align%20them%20into%20the%20range%20space%20of%20clean%20concepts.%20This%20dual%20alignment%20purifies%20risky%20prompts%20by%20both%20subtracting%20unsafe%20semantics%20and%20reinforcing%20safe%20ones%2C%20while%20retaining%20the%20original%20intent%20and%20coherence.%20We%20further%20define%20a%20token-wise%20strategy%20to%20selectively%20replace%20only%20risky%20token%20embeddings%2C%20ensuring%20minimal%20disruption%20to%20safe%20content.%20PurifyGen%20offers%20a%20plug-and-play%20solution%20with%20theoretical%20grounding%20and%20strong%20generalization%20to%20unseen%20prompts%20and%20models.%20Extensive%20testing%20shows%20that%20PurifyGen%20surpasses%20current%20methods%20in%20reducing%20unsafe%20content%20across%20five%20datasets%20and%20competes%20well%20with%20training-dependent%20approaches.%20The%20code%20can%20refer%20to%20https%3A//github.com/AI-Researcher-Team/PurifyGen.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPurifyGen%253A%2520A%2520Risk-Discrimination%2520and%2520Semantic-Purification%2520Model%2520for%2520Safe%2520Text-to-Image%2520Generation%26entry.906535625%3DZongsheng%2520Cao%2520and%2520Yangfan%2520He%2520and%2520Anran%2520Liu%2520and%2520Jun%2520Xie%2520and%2520Feng%2520Chen%2520and%2520Zepeng%2520Wang%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion%2520models%2520have%2520notably%2520enhanced%2520text-to-image%2520%2528T2I%2529%2520generation%2520quality%252C%2520but%2520they%2520also%2520raise%2520the%2520risk%2520of%2520generating%2520unsafe%2520content.%2520Traditional%2520safety%2520methods%2520like%2520text%2520blacklisting%2520or%2520harmful%2520content%2520classification%2520have%2520significant%2520drawbacks%253A%2520they%2520can%2520be%2520easily%2520circumvented%2520or%2520require%2520extensive%2520datasets%2520and%2520extra%2520training.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520PurifyGen%252C%2520a%2520novel%252C%2520training-free%2520approach%2520for%2520safe%2520T2I%2520generation%2520that%2520retains%2520the%2520model%2527s%2520original%2520weights.%2520PurifyGen%2520introduces%2520a%2520dual-stage%2520strategy%2520for%2520prompt%2520purification.%2520First%252C%2520we%2520evaluate%2520the%2520safety%2520of%2520each%2520token%2520in%2520a%2520prompt%2520by%2520computing%2520its%2520complementary%2520semantic%2520distance%252C%2520which%2520measures%2520the%2520semantic%2520proximity%2520between%2520the%2520prompt%2520tokens%2520and%2520concept%2520embeddings%2520from%2520predefined%2520toxic%2520and%2520clean%2520lists.%2520This%2520enables%2520fine-grained%2520prompt%2520classification%2520without%2520explicit%2520keyword%2520matching%2520or%2520retraining.%2520Tokens%2520closer%2520to%2520toxic%2520concepts%2520are%2520flagged%2520as%2520risky.%2520Second%252C%2520for%2520risky%2520prompts%252C%2520we%2520apply%2520a%2520dual-space%2520transformation%253A%2520we%2520project%2520toxic-aligned%2520embeddings%2520into%2520the%2520null%2520space%2520of%2520the%2520toxic%2520concept%2520matrix%252C%2520effectively%2520removing%2520harmful%2520semantic%2520components%252C%2520and%2520simultaneously%2520align%2520them%2520into%2520the%2520range%2520space%2520of%2520clean%2520concepts.%2520This%2520dual%2520alignment%2520purifies%2520risky%2520prompts%2520by%2520both%2520subtracting%2520unsafe%2520semantics%2520and%2520reinforcing%2520safe%2520ones%252C%2520while%2520retaining%2520the%2520original%2520intent%2520and%2520coherence.%2520We%2520further%2520define%2520a%2520token-wise%2520strategy%2520to%2520selectively%2520replace%2520only%2520risky%2520token%2520embeddings%252C%2520ensuring%2520minimal%2520disruption%2520to%2520safe%2520content.%2520PurifyGen%2520offers%2520a%2520plug-and-play%2520solution%2520with%2520theoretical%2520grounding%2520and%2520strong%2520generalization%2520to%2520unseen%2520prompts%2520and%2520models.%2520Extensive%2520testing%2520shows%2520that%2520PurifyGen%2520surpasses%2520current%2520methods%2520in%2520reducing%2520unsafe%2520content%2520across%2520five%2520datasets%2520and%2520competes%2520well%2520with%2520training-dependent%2520approaches.%2520The%2520code%2520can%2520refer%2520to%2520https%253A//github.com/AI-Researcher-Team/PurifyGen.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PurifyGen%3A%20A%20Risk-Discrimination%20and%20Semantic-Purification%20Model%20for%20Safe%20Text-to-Image%20Generation&entry.906535625=Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Jun%20Xie%20and%20Feng%20Chen%20and%20Zepeng%20Wang&entry.1292438233=Recent%20advances%20in%20diffusion%20models%20have%20notably%20enhanced%20text-to-image%20%28T2I%29%20generation%20quality%2C%20but%20they%20also%20raise%20the%20risk%20of%20generating%20unsafe%20content.%20Traditional%20safety%20methods%20like%20text%20blacklisting%20or%20harmful%20content%20classification%20have%20significant%20drawbacks%3A%20they%20can%20be%20easily%20circumvented%20or%20require%20extensive%20datasets%20and%20extra%20training.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20PurifyGen%2C%20a%20novel%2C%20training-free%20approach%20for%20safe%20T2I%20generation%20that%20retains%20the%20model%27s%20original%20weights.%20PurifyGen%20introduces%20a%20dual-stage%20strategy%20for%20prompt%20purification.%20First%2C%20we%20evaluate%20the%20safety%20of%20each%20token%20in%20a%20prompt%20by%20computing%20its%20complementary%20semantic%20distance%2C%20which%20measures%20the%20semantic%20proximity%20between%20the%20prompt%20tokens%20and%20concept%20embeddings%20from%20predefined%20toxic%20and%20clean%20lists.%20This%20enables%20fine-grained%20prompt%20classification%20without%20explicit%20keyword%20matching%20or%20retraining.%20Tokens%20closer%20to%20toxic%20concepts%20are%20flagged%20as%20risky.%20Second%2C%20for%20risky%20prompts%2C%20we%20apply%20a%20dual-space%20transformation%3A%20we%20project%20toxic-aligned%20embeddings%20into%20the%20null%20space%20of%20the%20toxic%20concept%20matrix%2C%20effectively%20removing%20harmful%20semantic%20components%2C%20and%20simultaneously%20align%20them%20into%20the%20range%20space%20of%20clean%20concepts.%20This%20dual%20alignment%20purifies%20risky%20prompts%20by%20both%20subtracting%20unsafe%20semantics%20and%20reinforcing%20safe%20ones%2C%20while%20retaining%20the%20original%20intent%20and%20coherence.%20We%20further%20define%20a%20token-wise%20strategy%20to%20selectively%20replace%20only%20risky%20token%20embeddings%2C%20ensuring%20minimal%20disruption%20to%20safe%20content.%20PurifyGen%20offers%20a%20plug-and-play%20solution%20with%20theoretical%20grounding%20and%20strong%20generalization%20to%20unseen%20prompts%20and%20models.%20Extensive%20testing%20shows%20that%20PurifyGen%20surpasses%20current%20methods%20in%20reducing%20unsafe%20content%20across%20five%20datasets%20and%20competes%20well%20with%20training-dependent%20approaches.%20The%20code%20can%20refer%20to%20https%3A//github.com/AI-Researcher-Team/PurifyGen.&entry.1838667208=http%3A//arxiv.org/abs/2512.23546v1&entry.124074799=Read"},
{"title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models", "author": "Jonathan Katzy and Razvan Mihai Popescu and Arie van Deursen and Maliheh Izadi", "abstract": "The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.", "link": "http://arxiv.org/abs/2501.09653v2", "date": "2025-12-29", "relevancy": 2.2063, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%20Large%20Language%20Models&body=Title%3A%20The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%20Large%20Language%20Models%0AAuthor%3A%20Jonathan%20Katzy%20and%20Razvan%20Mihai%20Popescu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi%0AAbstract%3A%20The%20recent%20rise%20in%20the%20popularity%20of%20large%20language%20models%20has%20spurred%20the%20development%20of%20extensive%20code%20datasets%20needed%20to%20train%20them.%20This%20has%20left%20limited%20code%20available%20for%20collection%20and%20use%20in%20the%20downstream%20investigation%20of%20specific%20behaviors%2C%20or%20evaluation%20of%20large%20language%20models%20without%20suffering%20from%20data%20contamination.%20To%20address%20this%20problem%2C%20we%20release%20The%20Heap%2C%20a%20large%20multilingual%20dataset%20covering%2057%20programming%20languages%20that%20has%20been%20deduplicated%20with%20respect%20to%20other%20open%20datasets%20of%20code%2C%20enabling%20researchers%20to%20conduct%20fair%20evaluations%20of%20large%20language%20models%20without%20significant%20data%20cleaning%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2501.09653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Heap%253A%2520A%2520Contamination-Free%2520Multilingual%2520Code%2520Dataset%2520for%2520Evaluating%2520Large%2520Language%2520Models%26entry.906535625%3DJonathan%2520Katzy%2520and%2520Razvan%2520Mihai%2520Popescu%2520and%2520Arie%2520van%2520Deursen%2520and%2520Maliheh%2520Izadi%26entry.1292438233%3DThe%2520recent%2520rise%2520in%2520the%2520popularity%2520of%2520large%2520language%2520models%2520has%2520spurred%2520the%2520development%2520of%2520extensive%2520code%2520datasets%2520needed%2520to%2520train%2520them.%2520This%2520has%2520left%2520limited%2520code%2520available%2520for%2520collection%2520and%2520use%2520in%2520the%2520downstream%2520investigation%2520of%2520specific%2520behaviors%252C%2520or%2520evaluation%2520of%2520large%2520language%2520models%2520without%2520suffering%2520from%2520data%2520contamination.%2520To%2520address%2520this%2520problem%252C%2520we%2520release%2520The%2520Heap%252C%2520a%2520large%2520multilingual%2520dataset%2520covering%252057%2520programming%2520languages%2520that%2520has%2520been%2520deduplicated%2520with%2520respect%2520to%2520other%2520open%2520datasets%2520of%2520code%252C%2520enabling%2520researchers%2520to%2520conduct%2520fair%2520evaluations%2520of%2520large%2520language%2520models%2520without%2520significant%2520data%2520cleaning%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Heap%3A%20A%20Contamination-Free%20Multilingual%20Code%20Dataset%20for%20Evaluating%20Large%20Language%20Models&entry.906535625=Jonathan%20Katzy%20and%20Razvan%20Mihai%20Popescu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi&entry.1292438233=The%20recent%20rise%20in%20the%20popularity%20of%20large%20language%20models%20has%20spurred%20the%20development%20of%20extensive%20code%20datasets%20needed%20to%20train%20them.%20This%20has%20left%20limited%20code%20available%20for%20collection%20and%20use%20in%20the%20downstream%20investigation%20of%20specific%20behaviors%2C%20or%20evaluation%20of%20large%20language%20models%20without%20suffering%20from%20data%20contamination.%20To%20address%20this%20problem%2C%20we%20release%20The%20Heap%2C%20a%20large%20multilingual%20dataset%20covering%2057%20programming%20languages%20that%20has%20been%20deduplicated%20with%20respect%20to%20other%20open%20datasets%20of%20code%2C%20enabling%20researchers%20to%20conduct%20fair%20evaluations%20of%20large%20language%20models%20without%20significant%20data%20cleaning%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2501.09653v2&entry.124074799=Read"},
{"title": "TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding", "author": "Zongsheng Cao and Yangfan He and Anran Liu and Feng Chen and Zepeng Wang and Jun Xie", "abstract": "Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \\emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \\emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.", "link": "http://arxiv.org/abs/2512.23483v1", "date": "2025-12-29", "relevancy": 2.2056, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5705}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5563}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV-RAG%3A%20A%20Temporal-aware%20and%20Semantic%20Entropy-Weighted%20Framework%20for%20Long%20Video%20Retrieval%20and%20Understanding&body=Title%3A%20TV-RAG%3A%20A%20Temporal-aware%20and%20Semantic%20Entropy-Weighted%20Framework%20for%20Long%20Video%20Retrieval%20and%20Understanding%0AAuthor%3A%20Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Feng%20Chen%20and%20Zepeng%20Wang%20and%20Jun%20Xie%0AAbstract%3A%20Large%20Video%20Language%20Models%20%28LVLMs%29%20have%20rapidly%20emerged%20as%20the%20focus%20of%20multimedia%20AI%20research.%20Nonetheless%2C%20when%20confronted%20with%20lengthy%20videos%2C%20these%20models%20struggle%3A%20their%20temporal%20windows%20are%20narrow%2C%20and%20they%20fail%20to%20notice%20fine-grained%20semantic%20shifts%20that%20unfold%20over%20extended%20durations.%20Moreover%2C%20mainstream%20text-based%20retrieval%20pipelines%2C%20which%20rely%20chiefly%20on%20surface-level%20lexical%20overlap%2C%20ignore%20the%20rich%20temporal%20interdependence%20among%20visual%2C%20audio%2C%20and%20subtitle%20channels.%20To%20mitigate%20these%20limitations%2C%20we%20propose%20TV-RAG%2C%20a%20training-free%20architecture%20that%20couples%20temporal%20alignment%20with%20entropy-guided%20semantics%20to%20improve%20long-video%20reasoning.%20The%20framework%20contributes%20two%20main%20mechanisms%3A%20%5Cemph%7B%28i%29%7D%20a%20time-decay%20retrieval%20module%20that%20injects%20explicit%20temporal%20offsets%20into%20the%20similarity%20computation%2C%20thereby%20ranking%20text%20queries%20according%20to%20their%20true%20multimedia%20context%3B%20and%20%5Cemph%7B%28ii%29%7D%20an%20entropy-weighted%20key-frame%20sampler%20that%20selects%20evenly%20spaced%2C%20information-dense%20frames%2C%20reducing%20redundancy%20while%20preserving%20representativeness.%20By%20weaving%20these%20temporal%20and%20semantic%20signals%20together%2C%20TV-RAG%20realises%20a%20dual-level%20reasoning%20routine%20that%20can%20be%20grafted%20onto%20any%20LVLM%20without%20re-training%20or%20fine-tuning.%20The%20resulting%20system%20offers%20a%20lightweight%2C%20budget-friendly%20upgrade%20path%20and%20consistently%20surpasses%20most%20leading%20baselines%20across%20established%20long-video%20benchmarks%20such%20as%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench%2C%20confirming%20the%20effectiveness%20of%20our%20model.%20The%20code%20can%20be%20found%20at%20https%3A//github.com/AI-Researcher-Team/TV-RAG.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV-RAG%253A%2520A%2520Temporal-aware%2520and%2520Semantic%2520Entropy-Weighted%2520Framework%2520for%2520Long%2520Video%2520Retrieval%2520and%2520Understanding%26entry.906535625%3DZongsheng%2520Cao%2520and%2520Yangfan%2520He%2520and%2520Anran%2520Liu%2520and%2520Feng%2520Chen%2520and%2520Zepeng%2520Wang%2520and%2520Jun%2520Xie%26entry.1292438233%3DLarge%2520Video%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520rapidly%2520emerged%2520as%2520the%2520focus%2520of%2520multimedia%2520AI%2520research.%2520Nonetheless%252C%2520when%2520confronted%2520with%2520lengthy%2520videos%252C%2520these%2520models%2520struggle%253A%2520their%2520temporal%2520windows%2520are%2520narrow%252C%2520and%2520they%2520fail%2520to%2520notice%2520fine-grained%2520semantic%2520shifts%2520that%2520unfold%2520over%2520extended%2520durations.%2520Moreover%252C%2520mainstream%2520text-based%2520retrieval%2520pipelines%252C%2520which%2520rely%2520chiefly%2520on%2520surface-level%2520lexical%2520overlap%252C%2520ignore%2520the%2520rich%2520temporal%2520interdependence%2520among%2520visual%252C%2520audio%252C%2520and%2520subtitle%2520channels.%2520To%2520mitigate%2520these%2520limitations%252C%2520we%2520propose%2520TV-RAG%252C%2520a%2520training-free%2520architecture%2520that%2520couples%2520temporal%2520alignment%2520with%2520entropy-guided%2520semantics%2520to%2520improve%2520long-video%2520reasoning.%2520The%2520framework%2520contributes%2520two%2520main%2520mechanisms%253A%2520%255Cemph%257B%2528i%2529%257D%2520a%2520time-decay%2520retrieval%2520module%2520that%2520injects%2520explicit%2520temporal%2520offsets%2520into%2520the%2520similarity%2520computation%252C%2520thereby%2520ranking%2520text%2520queries%2520according%2520to%2520their%2520true%2520multimedia%2520context%253B%2520and%2520%255Cemph%257B%2528ii%2529%257D%2520an%2520entropy-weighted%2520key-frame%2520sampler%2520that%2520selects%2520evenly%2520spaced%252C%2520information-dense%2520frames%252C%2520reducing%2520redundancy%2520while%2520preserving%2520representativeness.%2520By%2520weaving%2520these%2520temporal%2520and%2520semantic%2520signals%2520together%252C%2520TV-RAG%2520realises%2520a%2520dual-level%2520reasoning%2520routine%2520that%2520can%2520be%2520grafted%2520onto%2520any%2520LVLM%2520without%2520re-training%2520or%2520fine-tuning.%2520The%2520resulting%2520system%2520offers%2520a%2520lightweight%252C%2520budget-friendly%2520upgrade%2520path%2520and%2520consistently%2520surpasses%2520most%2520leading%2520baselines%2520across%2520established%2520long-video%2520benchmarks%2520such%2520as%2520Video-MME%252C%2520MLVU%252C%2520and%2520LongVideoBench%252C%2520confirming%2520the%2520effectiveness%2520of%2520our%2520model.%2520The%2520code%2520can%2520be%2520found%2520at%2520https%253A//github.com/AI-Researcher-Team/TV-RAG.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV-RAG%3A%20A%20Temporal-aware%20and%20Semantic%20Entropy-Weighted%20Framework%20for%20Long%20Video%20Retrieval%20and%20Understanding&entry.906535625=Zongsheng%20Cao%20and%20Yangfan%20He%20and%20Anran%20Liu%20and%20Feng%20Chen%20and%20Zepeng%20Wang%20and%20Jun%20Xie&entry.1292438233=Large%20Video%20Language%20Models%20%28LVLMs%29%20have%20rapidly%20emerged%20as%20the%20focus%20of%20multimedia%20AI%20research.%20Nonetheless%2C%20when%20confronted%20with%20lengthy%20videos%2C%20these%20models%20struggle%3A%20their%20temporal%20windows%20are%20narrow%2C%20and%20they%20fail%20to%20notice%20fine-grained%20semantic%20shifts%20that%20unfold%20over%20extended%20durations.%20Moreover%2C%20mainstream%20text-based%20retrieval%20pipelines%2C%20which%20rely%20chiefly%20on%20surface-level%20lexical%20overlap%2C%20ignore%20the%20rich%20temporal%20interdependence%20among%20visual%2C%20audio%2C%20and%20subtitle%20channels.%20To%20mitigate%20these%20limitations%2C%20we%20propose%20TV-RAG%2C%20a%20training-free%20architecture%20that%20couples%20temporal%20alignment%20with%20entropy-guided%20semantics%20to%20improve%20long-video%20reasoning.%20The%20framework%20contributes%20two%20main%20mechanisms%3A%20%5Cemph%7B%28i%29%7D%20a%20time-decay%20retrieval%20module%20that%20injects%20explicit%20temporal%20offsets%20into%20the%20similarity%20computation%2C%20thereby%20ranking%20text%20queries%20according%20to%20their%20true%20multimedia%20context%3B%20and%20%5Cemph%7B%28ii%29%7D%20an%20entropy-weighted%20key-frame%20sampler%20that%20selects%20evenly%20spaced%2C%20information-dense%20frames%2C%20reducing%20redundancy%20while%20preserving%20representativeness.%20By%20weaving%20these%20temporal%20and%20semantic%20signals%20together%2C%20TV-RAG%20realises%20a%20dual-level%20reasoning%20routine%20that%20can%20be%20grafted%20onto%20any%20LVLM%20without%20re-training%20or%20fine-tuning.%20The%20resulting%20system%20offers%20a%20lightweight%2C%20budget-friendly%20upgrade%20path%20and%20consistently%20surpasses%20most%20leading%20baselines%20across%20established%20long-video%20benchmarks%20such%20as%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench%2C%20confirming%20the%20effectiveness%20of%20our%20model.%20The%20code%20can%20be%20found%20at%20https%3A//github.com/AI-Researcher-Team/TV-RAG.&entry.1838667208=http%3A//arxiv.org/abs/2512.23483v1&entry.124074799=Read"},
{"title": "Stochastic Siamese MAE Pretraining for Longitudinal Medical Images", "author": "Taha Emre and Arunava Chakravarty and Thomas Pinetz and Dmitrii Lachinov and Martin J. Menten and Hendrik Scholl and Sobha Sivaprasad and Daniel Rueckert and Andrew Lotery and Stefan Sacu and Ursula Schmidt-Erfurth and Hrvoje Bogunovi\u0107", "abstract": "Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.", "link": "http://arxiv.org/abs/2512.23441v1", "date": "2025-12-29", "relevancy": 2.2051, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5675}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5576}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Siamese%20MAE%20Pretraining%20for%20Longitudinal%20Medical%20Images&body=Title%3A%20Stochastic%20Siamese%20MAE%20Pretraining%20for%20Longitudinal%20Medical%20Images%0AAuthor%3A%20Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Thomas%20Pinetz%20and%20Dmitrii%20Lachinov%20and%20Martin%20J.%20Menten%20and%20Hendrik%20Scholl%20and%20Sobha%20Sivaprasad%20and%20Daniel%20Rueckert%20and%20Andrew%20Lotery%20and%20Stefan%20Sacu%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20Temporally%20aware%20image%20representations%20are%20crucial%20for%20capturing%20disease%20progression%20in%203D%20volumes%20of%20longitudinal%20medical%20datasets.%20However%2C%20recent%20state-of-the-art%20self-supervised%20learning%20approaches%20like%20Masked%20Autoencoding%20%28MAE%29%2C%20despite%20their%20strong%20representation%20learning%20capabilities%2C%20lack%20temporal%20awareness.%20In%20this%20paper%2C%20we%20propose%20STAMP%20%28Stochastic%20Temporal%20Autoencoder%20with%20Masked%20Pretraining%29%2C%20a%20Siamese%20MAE%20framework%20that%20encodes%20temporal%20information%20through%20a%20stochastic%20process%20by%20conditioning%20on%20the%20time%20difference%20between%20the%202%20input%20volumes.%20Unlike%20deterministic%20Siamese%20approaches%2C%20which%20compare%20scans%20from%20different%20time%20points%20but%20fail%20to%20account%20for%20the%20inherent%20uncertainty%20in%20disease%20evolution%2C%20STAMP%20learns%20temporal%20dynamics%20stochastically%20by%20reframing%20the%20MAE%20reconstruction%20loss%20as%20a%20conditional%20variational%20inference%20objective.%20We%20evaluated%20STAMP%20on%20two%20OCT%20and%20one%20MRI%20datasets%20with%20multiple%20visits%20per%20patient.%20STAMP%20pretrained%20ViT%20models%20outperformed%20both%20existing%20temporal%20MAE%20methods%20and%20foundation%20models%20on%20different%20late%20stage%20Age-Related%20Macular%20Degeneration%20and%20Alzheimer%27s%20Disease%20progression%20prediction%20which%20require%20models%20to%20learn%20the%20underlying%20non-deterministic%20temporal%20dynamics%20of%20the%20diseases.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Siamese%2520MAE%2520Pretraining%2520for%2520Longitudinal%2520Medical%2520Images%26entry.906535625%3DTaha%2520Emre%2520and%2520Arunava%2520Chakravarty%2520and%2520Thomas%2520Pinetz%2520and%2520Dmitrii%2520Lachinov%2520and%2520Martin%2520J.%2520Menten%2520and%2520Hendrik%2520Scholl%2520and%2520Sobha%2520Sivaprasad%2520and%2520Daniel%2520Rueckert%2520and%2520Andrew%2520Lotery%2520and%2520Stefan%2520Sacu%2520and%2520Ursula%2520Schmidt-Erfurth%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3DTemporally%2520aware%2520image%2520representations%2520are%2520crucial%2520for%2520capturing%2520disease%2520progression%2520in%25203D%2520volumes%2520of%2520longitudinal%2520medical%2520datasets.%2520However%252C%2520recent%2520state-of-the-art%2520self-supervised%2520learning%2520approaches%2520like%2520Masked%2520Autoencoding%2520%2528MAE%2529%252C%2520despite%2520their%2520strong%2520representation%2520learning%2520capabilities%252C%2520lack%2520temporal%2520awareness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520STAMP%2520%2528Stochastic%2520Temporal%2520Autoencoder%2520with%2520Masked%2520Pretraining%2529%252C%2520a%2520Siamese%2520MAE%2520framework%2520that%2520encodes%2520temporal%2520information%2520through%2520a%2520stochastic%2520process%2520by%2520conditioning%2520on%2520the%2520time%2520difference%2520between%2520the%25202%2520input%2520volumes.%2520Unlike%2520deterministic%2520Siamese%2520approaches%252C%2520which%2520compare%2520scans%2520from%2520different%2520time%2520points%2520but%2520fail%2520to%2520account%2520for%2520the%2520inherent%2520uncertainty%2520in%2520disease%2520evolution%252C%2520STAMP%2520learns%2520temporal%2520dynamics%2520stochastically%2520by%2520reframing%2520the%2520MAE%2520reconstruction%2520loss%2520as%2520a%2520conditional%2520variational%2520inference%2520objective.%2520We%2520evaluated%2520STAMP%2520on%2520two%2520OCT%2520and%2520one%2520MRI%2520datasets%2520with%2520multiple%2520visits%2520per%2520patient.%2520STAMP%2520pretrained%2520ViT%2520models%2520outperformed%2520both%2520existing%2520temporal%2520MAE%2520methods%2520and%2520foundation%2520models%2520on%2520different%2520late%2520stage%2520Age-Related%2520Macular%2520Degeneration%2520and%2520Alzheimer%2527s%2520Disease%2520progression%2520prediction%2520which%2520require%2520models%2520to%2520learn%2520the%2520underlying%2520non-deterministic%2520temporal%2520dynamics%2520of%2520the%2520diseases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Siamese%20MAE%20Pretraining%20for%20Longitudinal%20Medical%20Images&entry.906535625=Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Thomas%20Pinetz%20and%20Dmitrii%20Lachinov%20and%20Martin%20J.%20Menten%20and%20Hendrik%20Scholl%20and%20Sobha%20Sivaprasad%20and%20Daniel%20Rueckert%20and%20Andrew%20Lotery%20and%20Stefan%20Sacu%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=Temporally%20aware%20image%20representations%20are%20crucial%20for%20capturing%20disease%20progression%20in%203D%20volumes%20of%20longitudinal%20medical%20datasets.%20However%2C%20recent%20state-of-the-art%20self-supervised%20learning%20approaches%20like%20Masked%20Autoencoding%20%28MAE%29%2C%20despite%20their%20strong%20representation%20learning%20capabilities%2C%20lack%20temporal%20awareness.%20In%20this%20paper%2C%20we%20propose%20STAMP%20%28Stochastic%20Temporal%20Autoencoder%20with%20Masked%20Pretraining%29%2C%20a%20Siamese%20MAE%20framework%20that%20encodes%20temporal%20information%20through%20a%20stochastic%20process%20by%20conditioning%20on%20the%20time%20difference%20between%20the%202%20input%20volumes.%20Unlike%20deterministic%20Siamese%20approaches%2C%20which%20compare%20scans%20from%20different%20time%20points%20but%20fail%20to%20account%20for%20the%20inherent%20uncertainty%20in%20disease%20evolution%2C%20STAMP%20learns%20temporal%20dynamics%20stochastically%20by%20reframing%20the%20MAE%20reconstruction%20loss%20as%20a%20conditional%20variational%20inference%20objective.%20We%20evaluated%20STAMP%20on%20two%20OCT%20and%20one%20MRI%20datasets%20with%20multiple%20visits%20per%20patient.%20STAMP%20pretrained%20ViT%20models%20outperformed%20both%20existing%20temporal%20MAE%20methods%20and%20foundation%20models%20on%20different%20late%20stage%20Age-Related%20Macular%20Degeneration%20and%20Alzheimer%27s%20Disease%20progression%20prediction%20which%20require%20models%20to%20learn%20the%20underlying%20non-deterministic%20temporal%20dynamics%20of%20the%20diseases.&entry.1838667208=http%3A//arxiv.org/abs/2512.23441v1&entry.124074799=Read"},
{"title": "Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery", "author": "Mehdi Heydari Shahna", "abstract": "Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.\n  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.\n  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.\n  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.", "link": "http://arxiv.org/abs/2512.23505v1", "date": "2025-12-29", "relevancy": 2.1965, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.56}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5453}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Deep%20Learning%20Control%20with%20Guaranteed%20Performance%20for%20Safe%20and%20Reliable%20Robotization%20in%20Heavy-Duty%20Machinery&body=Title%3A%20Robust%20Deep%20Learning%20Control%20with%20Guaranteed%20Performance%20for%20Safe%20and%20Reliable%20Robotization%20in%20Heavy-Duty%20Machinery%0AAuthor%3A%20Mehdi%20Heydari%20Shahna%0AAbstract%3A%20Today%27s%20heavy-duty%20mobile%20machines%20%28HDMMs%29%20face%20two%20transitions%3A%20from%20diesel-hydraulic%20actuation%20to%20clean%20electric%20systems%20driven%20by%20climate%20goals%2C%20and%20from%20human%20supervision%20toward%20greater%20autonomy.%20Diesel-hydraulic%20systems%20have%20long%20dominated%2C%20so%20full%20electrification%2C%20via%20direct%20replacement%20or%20redesign%2C%20raises%20major%20technical%20and%20economic%20challenges.%20Although%20advanced%20artificial%20intelligence%20%28AI%29%20could%20enable%20higher%20autonomy%2C%20adoption%20in%20HDMMs%20is%20limited%20by%20strict%20safety%20requirements%2C%20and%20these%20machines%20still%20rely%20heavily%20on%20human%20supervision.%0A%20%20This%20dissertation%20develops%20a%20control%20framework%20that%20%281%29%20simplifies%20control%20design%20for%20electrified%20HDMMs%20through%20a%20generic%20modular%20approach%20that%20is%20energy-source%20independent%20and%20supports%20future%20modifications%2C%20and%20%282%29%20defines%20hierarchical%20control%20policies%20that%20partially%20integrate%20AI%20while%20guaranteeing%20safety-defined%20performance%20and%20stability.%0A%20%20Five%20research%20questions%20align%20with%20three%20lines%20of%20investigation%3A%20a%20generic%20robust%20control%20strategy%20for%20multi-body%20HDMMs%20with%20strong%20stability%20across%20actuation%20types%20and%20energy%20sources%3B%20control%20solutions%20that%20keep%20strict%20performance%20under%20uncertainty%20and%20faults%20while%20balancing%20robustness%20and%20responsiveness%3B%20and%20methods%20to%20interpret%20and%20trust%20black-box%20learning%20strategies%20so%20they%20can%20be%20integrated%20stably%20and%20verified%20against%20international%20safety%20standards.%0A%20%20The%20framework%20is%20validated%20in%20three%20case%20studies%20spanning%20different%20actuators%20and%20conditions%2C%20covering%20heavy-duty%20mobile%20robots%20and%20robotic%20manipulators.%20Results%20appear%20in%20five%20peer-reviewed%20publications%20and%20one%20unpublished%20manuscript%2C%20advancing%20nonlinear%20control%20and%20robotics%20and%20supporting%20both%20transitions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Deep%2520Learning%2520Control%2520with%2520Guaranteed%2520Performance%2520for%2520Safe%2520and%2520Reliable%2520Robotization%2520in%2520Heavy-Duty%2520Machinery%26entry.906535625%3DMehdi%2520Heydari%2520Shahna%26entry.1292438233%3DToday%2527s%2520heavy-duty%2520mobile%2520machines%2520%2528HDMMs%2529%2520face%2520two%2520transitions%253A%2520from%2520diesel-hydraulic%2520actuation%2520to%2520clean%2520electric%2520systems%2520driven%2520by%2520climate%2520goals%252C%2520and%2520from%2520human%2520supervision%2520toward%2520greater%2520autonomy.%2520Diesel-hydraulic%2520systems%2520have%2520long%2520dominated%252C%2520so%2520full%2520electrification%252C%2520via%2520direct%2520replacement%2520or%2520redesign%252C%2520raises%2520major%2520technical%2520and%2520economic%2520challenges.%2520Although%2520advanced%2520artificial%2520intelligence%2520%2528AI%2529%2520could%2520enable%2520higher%2520autonomy%252C%2520adoption%2520in%2520HDMMs%2520is%2520limited%2520by%2520strict%2520safety%2520requirements%252C%2520and%2520these%2520machines%2520still%2520rely%2520heavily%2520on%2520human%2520supervision.%250A%2520%2520This%2520dissertation%2520develops%2520a%2520control%2520framework%2520that%2520%25281%2529%2520simplifies%2520control%2520design%2520for%2520electrified%2520HDMMs%2520through%2520a%2520generic%2520modular%2520approach%2520that%2520is%2520energy-source%2520independent%2520and%2520supports%2520future%2520modifications%252C%2520and%2520%25282%2529%2520defines%2520hierarchical%2520control%2520policies%2520that%2520partially%2520integrate%2520AI%2520while%2520guaranteeing%2520safety-defined%2520performance%2520and%2520stability.%250A%2520%2520Five%2520research%2520questions%2520align%2520with%2520three%2520lines%2520of%2520investigation%253A%2520a%2520generic%2520robust%2520control%2520strategy%2520for%2520multi-body%2520HDMMs%2520with%2520strong%2520stability%2520across%2520actuation%2520types%2520and%2520energy%2520sources%253B%2520control%2520solutions%2520that%2520keep%2520strict%2520performance%2520under%2520uncertainty%2520and%2520faults%2520while%2520balancing%2520robustness%2520and%2520responsiveness%253B%2520and%2520methods%2520to%2520interpret%2520and%2520trust%2520black-box%2520learning%2520strategies%2520so%2520they%2520can%2520be%2520integrated%2520stably%2520and%2520verified%2520against%2520international%2520safety%2520standards.%250A%2520%2520The%2520framework%2520is%2520validated%2520in%2520three%2520case%2520studies%2520spanning%2520different%2520actuators%2520and%2520conditions%252C%2520covering%2520heavy-duty%2520mobile%2520robots%2520and%2520robotic%2520manipulators.%2520Results%2520appear%2520in%2520five%2520peer-reviewed%2520publications%2520and%2520one%2520unpublished%2520manuscript%252C%2520advancing%2520nonlinear%2520control%2520and%2520robotics%2520and%2520supporting%2520both%2520transitions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Deep%20Learning%20Control%20with%20Guaranteed%20Performance%20for%20Safe%20and%20Reliable%20Robotization%20in%20Heavy-Duty%20Machinery&entry.906535625=Mehdi%20Heydari%20Shahna&entry.1292438233=Today%27s%20heavy-duty%20mobile%20machines%20%28HDMMs%29%20face%20two%20transitions%3A%20from%20diesel-hydraulic%20actuation%20to%20clean%20electric%20systems%20driven%20by%20climate%20goals%2C%20and%20from%20human%20supervision%20toward%20greater%20autonomy.%20Diesel-hydraulic%20systems%20have%20long%20dominated%2C%20so%20full%20electrification%2C%20via%20direct%20replacement%20or%20redesign%2C%20raises%20major%20technical%20and%20economic%20challenges.%20Although%20advanced%20artificial%20intelligence%20%28AI%29%20could%20enable%20higher%20autonomy%2C%20adoption%20in%20HDMMs%20is%20limited%20by%20strict%20safety%20requirements%2C%20and%20these%20machines%20still%20rely%20heavily%20on%20human%20supervision.%0A%20%20This%20dissertation%20develops%20a%20control%20framework%20that%20%281%29%20simplifies%20control%20design%20for%20electrified%20HDMMs%20through%20a%20generic%20modular%20approach%20that%20is%20energy-source%20independent%20and%20supports%20future%20modifications%2C%20and%20%282%29%20defines%20hierarchical%20control%20policies%20that%20partially%20integrate%20AI%20while%20guaranteeing%20safety-defined%20performance%20and%20stability.%0A%20%20Five%20research%20questions%20align%20with%20three%20lines%20of%20investigation%3A%20a%20generic%20robust%20control%20strategy%20for%20multi-body%20HDMMs%20with%20strong%20stability%20across%20actuation%20types%20and%20energy%20sources%3B%20control%20solutions%20that%20keep%20strict%20performance%20under%20uncertainty%20and%20faults%20while%20balancing%20robustness%20and%20responsiveness%3B%20and%20methods%20to%20interpret%20and%20trust%20black-box%20learning%20strategies%20so%20they%20can%20be%20integrated%20stably%20and%20verified%20against%20international%20safety%20standards.%0A%20%20The%20framework%20is%20validated%20in%20three%20case%20studies%20spanning%20different%20actuators%20and%20conditions%2C%20covering%20heavy-duty%20mobile%20robots%20and%20robotic%20manipulators.%20Results%20appear%20in%20five%20peer-reviewed%20publications%20and%20one%20unpublished%20manuscript%2C%20advancing%20nonlinear%20control%20and%20robotics%20and%20supporting%20both%20transitions.&entry.1838667208=http%3A//arxiv.org/abs/2512.23505v1&entry.124074799=Read"},
{"title": "Memorization in 3D Shape Generation: An Empirical Study", "author": "Shu Pu and Boya Zeng and Kaichen Zhou and Mengyu Wang and Zhuang Liu", "abstract": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.", "link": "http://arxiv.org/abs/2512.23628v1", "date": "2025-12-29", "relevancy": 2.193, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%20in%203D%20Shape%20Generation%3A%20An%20Empirical%20Study&body=Title%3A%20Memorization%20in%203D%20Shape%20Generation%3A%20An%20Empirical%20Study%0AAuthor%3A%20Shu%20Pu%20and%20Boya%20Zeng%20and%20Kaichen%20Zhou%20and%20Mengyu%20Wang%20and%20Zhuang%20Liu%0AAbstract%3A%20Generative%20models%20are%20increasingly%20used%20in%203D%20vision%20to%20synthesize%20novel%20shapes%2C%20yet%20it%20remains%20unclear%20whether%20their%20generation%20relies%20on%20memorizing%20training%20shapes.%20Understanding%20their%20memorization%20could%20help%20prevent%20training%20data%20leakage%20and%20improve%20the%20diversity%20of%20generated%20results.%20In%20this%20paper%2C%20we%20design%20an%20evaluation%20framework%20to%20quantify%20memorization%20in%203D%20generative%20models%20and%20study%20the%20influence%20of%20different%20data%20and%20modeling%20designs%20on%20memorization.%20We%20first%20apply%20our%20framework%20to%20quantify%20memorization%20in%20existing%20methods.%20Next%2C%20through%20controlled%20experiments%20with%20a%20latent%20vector-set%20%28Vecset%29%20diffusion%20model%2C%20we%20find%20that%2C%20on%20the%20data%20side%2C%20memorization%20depends%20on%20data%20modality%2C%20and%20increases%20with%20data%20diversity%20and%20finer-grained%20conditioning%3B%20on%20the%20modeling%20side%2C%20it%20peaks%20at%20a%20moderate%20guidance%20scale%20and%20can%20be%20mitigated%20by%20longer%20Vecsets%20and%20simple%20rotation%20augmentation.%20Together%2C%20our%20framework%20and%20analysis%20provide%20an%20empirical%20understanding%20of%20memorization%20in%203D%20generative%20models%20and%20suggest%20simple%20yet%20effective%20strategies%20to%20reduce%20it%20without%20degrading%20generation%20quality.%20Our%20code%20is%20available%20at%20https%3A//github.com/zlab-princeton/3d_mem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%2520in%25203D%2520Shape%2520Generation%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DShu%2520Pu%2520and%2520Boya%2520Zeng%2520and%2520Kaichen%2520Zhou%2520and%2520Mengyu%2520Wang%2520and%2520Zhuang%2520Liu%26entry.1292438233%3DGenerative%2520models%2520are%2520increasingly%2520used%2520in%25203D%2520vision%2520to%2520synthesize%2520novel%2520shapes%252C%2520yet%2520it%2520remains%2520unclear%2520whether%2520their%2520generation%2520relies%2520on%2520memorizing%2520training%2520shapes.%2520Understanding%2520their%2520memorization%2520could%2520help%2520prevent%2520training%2520data%2520leakage%2520and%2520improve%2520the%2520diversity%2520of%2520generated%2520results.%2520In%2520this%2520paper%252C%2520we%2520design%2520an%2520evaluation%2520framework%2520to%2520quantify%2520memorization%2520in%25203D%2520generative%2520models%2520and%2520study%2520the%2520influence%2520of%2520different%2520data%2520and%2520modeling%2520designs%2520on%2520memorization.%2520We%2520first%2520apply%2520our%2520framework%2520to%2520quantify%2520memorization%2520in%2520existing%2520methods.%2520Next%252C%2520through%2520controlled%2520experiments%2520with%2520a%2520latent%2520vector-set%2520%2528Vecset%2529%2520diffusion%2520model%252C%2520we%2520find%2520that%252C%2520on%2520the%2520data%2520side%252C%2520memorization%2520depends%2520on%2520data%2520modality%252C%2520and%2520increases%2520with%2520data%2520diversity%2520and%2520finer-grained%2520conditioning%253B%2520on%2520the%2520modeling%2520side%252C%2520it%2520peaks%2520at%2520a%2520moderate%2520guidance%2520scale%2520and%2520can%2520be%2520mitigated%2520by%2520longer%2520Vecsets%2520and%2520simple%2520rotation%2520augmentation.%2520Together%252C%2520our%2520framework%2520and%2520analysis%2520provide%2520an%2520empirical%2520understanding%2520of%2520memorization%2520in%25203D%2520generative%2520models%2520and%2520suggest%2520simple%2520yet%2520effective%2520strategies%2520to%2520reduce%2520it%2520without%2520degrading%2520generation%2520quality.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/zlab-princeton/3d_mem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20in%203D%20Shape%20Generation%3A%20An%20Empirical%20Study&entry.906535625=Shu%20Pu%20and%20Boya%20Zeng%20and%20Kaichen%20Zhou%20and%20Mengyu%20Wang%20and%20Zhuang%20Liu&entry.1292438233=Generative%20models%20are%20increasingly%20used%20in%203D%20vision%20to%20synthesize%20novel%20shapes%2C%20yet%20it%20remains%20unclear%20whether%20their%20generation%20relies%20on%20memorizing%20training%20shapes.%20Understanding%20their%20memorization%20could%20help%20prevent%20training%20data%20leakage%20and%20improve%20the%20diversity%20of%20generated%20results.%20In%20this%20paper%2C%20we%20design%20an%20evaluation%20framework%20to%20quantify%20memorization%20in%203D%20generative%20models%20and%20study%20the%20influence%20of%20different%20data%20and%20modeling%20designs%20on%20memorization.%20We%20first%20apply%20our%20framework%20to%20quantify%20memorization%20in%20existing%20methods.%20Next%2C%20through%20controlled%20experiments%20with%20a%20latent%20vector-set%20%28Vecset%29%20diffusion%20model%2C%20we%20find%20that%2C%20on%20the%20data%20side%2C%20memorization%20depends%20on%20data%20modality%2C%20and%20increases%20with%20data%20diversity%20and%20finer-grained%20conditioning%3B%20on%20the%20modeling%20side%2C%20it%20peaks%20at%20a%20moderate%20guidance%20scale%20and%20can%20be%20mitigated%20by%20longer%20Vecsets%20and%20simple%20rotation%20augmentation.%20Together%2C%20our%20framework%20and%20analysis%20provide%20an%20empirical%20understanding%20of%20memorization%20in%203D%20generative%20models%20and%20suggest%20simple%20yet%20effective%20strategies%20to%20reduce%20it%20without%20degrading%20generation%20quality.%20Our%20code%20is%20available%20at%20https%3A//github.com/zlab-princeton/3d_mem.&entry.1838667208=http%3A//arxiv.org/abs/2512.23628v1&entry.124074799=Read"},
{"title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "author": "Yiqing Zhou and Yu Lei and Shuzheng Si and Qingyan Sun and Wei Wang and Yifei Wu and Hao Wen and Gang Chen and Fanchao Qi and Maosong Sun", "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "link": "http://arxiv.org/abs/2512.14244v3", "date": "2025-12-29", "relevancy": 2.1829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&body=Title%3A%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition%0AAuthor%3A%20Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun%0AAbstract%3A%20Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14244v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Context%2520to%2520EDUs%253A%2520Faithful%2520and%2520Structured%2520Context%2520Compression%2520via%2520Elementary%2520Discourse%2520Unit%2520Decomposition%26entry.906535625%3DYiqing%2520Zhou%2520and%2520Yu%2520Lei%2520and%2520Shuzheng%2520Si%2520and%2520Qingyan%2520Sun%2520and%2520Wei%2520Wang%2520and%2520Yifei%2520Wu%2520and%2520Hao%2520Wen%2520and%2520Gang%2520Chen%2520and%2520Fanchao%2520Qi%2520and%2520Maosong%2520Sun%26entry.1292438233%3DManaging%2520extensive%2520context%2520remains%2520a%2520critical%2520bottleneck%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520in%2520applications%2520like%2520long-document%2520question%2520answering%2520and%2520autonomous%2520agents%2520where%2520lengthy%2520inputs%2520incur%2520high%2520computational%2520costs%2520and%2520introduce%2520noise.%2520Existing%2520compression%2520techniques%2520often%2520disrupt%2520local%2520coherence%2520through%2520discrete%2520token%2520removal%2520or%2520rely%2520on%2520implicit%2520latent%2520encoding%2520that%2520suffers%2520from%2520positional%2520bias%2520and%2520incompatibility%2520with%2520closed-source%2520APIs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520EDU-based%2520Context%2520Compressor%252C%2520a%2520novel%2520explicit%2520compression%2520framework%2520designed%2520to%2520preserve%2520both%2520global%2520structure%2520and%2520fine-grained%2520details.%2520Our%2520approach%2520reformulates%2520context%2520compression%2520as%2520a%2520structure-then-select%2520process.%2520First%252C%2520our%2520LingoEDU%2520transforms%2520linear%2520text%2520into%2520a%2520structural%2520relation%2520tree%2520of%2520Elementary%2520Discourse%2520Units%2520%2528EDUs%2529%2520which%2520are%2520anchored%2520strictly%2520to%2520source%2520indices%2520to%2520eliminate%2520hallucination.%2520Second%252C%2520a%2520lightweight%2520ranking%2520module%2520selects%2520query-relevant%2520sub-trees%2520for%2520linearization.%2520To%2520rigorously%2520evaluate%2520structural%2520understanding%252C%2520we%2520release%2520StructBench%252C%2520a%2520manually%2520annotated%2520dataset%2520of%2520248%2520diverse%2520documents.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520structural%2520prediction%2520accuracy%2520and%2520significantly%2520outperforms%2520frontier%2520LLMs%2520while%2520reducing%2520costs.%2520Furthermore%252C%2520our%2520structure-aware%2520compression%2520substantially%2520enhances%2520performance%2520across%2520downstream%2520tasks%2520ranging%2520from%2520long-context%2520tasks%2520to%2520complex%2520Deep%2520Search%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14244v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&entry.906535625=Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun&entry.1292438233=Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.14244v3&entry.124074799=Read"},
{"title": "Machine Unlearning using Forgetting Neural Networks", "author": "Amartya Hatua and Trung T. Nguyen and Filip Cano and Andrew H. Sung", "abstract": "Modern computer systems store vast amounts of personal data, enabling advances in AI and ML but risking user privacy and trust. For privacy reasons, it is sometimes desired for an ML model to forget part of the data it was trained on. In this paper, we introduce a novel unlearning approach based on Forgetting Neural Networks (FNNs), a neuroscience-inspired architecture that explicitly encodes forgetting through multiplicative decay factors. While FNNs had previously been studied as a theoretical construct, we provide the first concrete implementation and demonstrate their effectiveness for targeted unlearning. We propose several variants with per-neuron forgetting factors, including rank-based assignments guided by activation levels, and evaluate them on MNIST and Fashion-MNIST benchmarks. Our method systematically removes information associated with forget sets while preserving performance on retained data. Membership inference attacks confirm the effectiveness of FNN-based unlearning in erasing information about the training data from the neural network. These results establish FNNs as a promising foundation for efficient and interpretable unlearning.", "link": "http://arxiv.org/abs/2410.22374v2", "date": "2025-12-29", "relevancy": 2.1809, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4504}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4319}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Unlearning%20using%20Forgetting%20Neural%20Networks&body=Title%3A%20Machine%20Unlearning%20using%20Forgetting%20Neural%20Networks%0AAuthor%3A%20Amartya%20Hatua%20and%20Trung%20T.%20Nguyen%20and%20Filip%20Cano%20and%20Andrew%20H.%20Sung%0AAbstract%3A%20Modern%20computer%20systems%20store%20vast%20amounts%20of%20personal%20data%2C%20enabling%20advances%20in%20AI%20and%20ML%20but%20risking%20user%20privacy%20and%20trust.%20For%20privacy%20reasons%2C%20it%20is%20sometimes%20desired%20for%20an%20ML%20model%20to%20forget%20part%20of%20the%20data%20it%20was%20trained%20on.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20unlearning%20approach%20based%20on%20Forgetting%20Neural%20Networks%20%28FNNs%29%2C%20a%20neuroscience-inspired%20architecture%20that%20explicitly%20encodes%20forgetting%20through%20multiplicative%20decay%20factors.%20While%20FNNs%20had%20previously%20been%20studied%20as%20a%20theoretical%20construct%2C%20we%20provide%20the%20first%20concrete%20implementation%20and%20demonstrate%20their%20effectiveness%20for%20targeted%20unlearning.%20We%20propose%20several%20variants%20with%20per-neuron%20forgetting%20factors%2C%20including%20rank-based%20assignments%20guided%20by%20activation%20levels%2C%20and%20evaluate%20them%20on%20MNIST%20and%20Fashion-MNIST%20benchmarks.%20Our%20method%20systematically%20removes%20information%20associated%20with%20forget%20sets%20while%20preserving%20performance%20on%20retained%20data.%20Membership%20inference%20attacks%20confirm%20the%20effectiveness%20of%20FNN-based%20unlearning%20in%20erasing%20information%20about%20the%20training%20data%20from%20the%20neural%20network.%20These%20results%20establish%20FNNs%20as%20a%20promising%20foundation%20for%20efficient%20and%20interpretable%20unlearning.%0ALink%3A%20http%3A//arxiv.org/abs/2410.22374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Unlearning%2520using%2520Forgetting%2520Neural%2520Networks%26entry.906535625%3DAmartya%2520Hatua%2520and%2520Trung%2520T.%2520Nguyen%2520and%2520Filip%2520Cano%2520and%2520Andrew%2520H.%2520Sung%26entry.1292438233%3DModern%2520computer%2520systems%2520store%2520vast%2520amounts%2520of%2520personal%2520data%252C%2520enabling%2520advances%2520in%2520AI%2520and%2520ML%2520but%2520risking%2520user%2520privacy%2520and%2520trust.%2520For%2520privacy%2520reasons%252C%2520it%2520is%2520sometimes%2520desired%2520for%2520an%2520ML%2520model%2520to%2520forget%2520part%2520of%2520the%2520data%2520it%2520was%2520trained%2520on.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520unlearning%2520approach%2520based%2520on%2520Forgetting%2520Neural%2520Networks%2520%2528FNNs%2529%252C%2520a%2520neuroscience-inspired%2520architecture%2520that%2520explicitly%2520encodes%2520forgetting%2520through%2520multiplicative%2520decay%2520factors.%2520While%2520FNNs%2520had%2520previously%2520been%2520studied%2520as%2520a%2520theoretical%2520construct%252C%2520we%2520provide%2520the%2520first%2520concrete%2520implementation%2520and%2520demonstrate%2520their%2520effectiveness%2520for%2520targeted%2520unlearning.%2520We%2520propose%2520several%2520variants%2520with%2520per-neuron%2520forgetting%2520factors%252C%2520including%2520rank-based%2520assignments%2520guided%2520by%2520activation%2520levels%252C%2520and%2520evaluate%2520them%2520on%2520MNIST%2520and%2520Fashion-MNIST%2520benchmarks.%2520Our%2520method%2520systematically%2520removes%2520information%2520associated%2520with%2520forget%2520sets%2520while%2520preserving%2520performance%2520on%2520retained%2520data.%2520Membership%2520inference%2520attacks%2520confirm%2520the%2520effectiveness%2520of%2520FNN-based%2520unlearning%2520in%2520erasing%2520information%2520about%2520the%2520training%2520data%2520from%2520the%2520neural%2520network.%2520These%2520results%2520establish%2520FNNs%2520as%2520a%2520promising%2520foundation%2520for%2520efficient%2520and%2520interpretable%2520unlearning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Unlearning%20using%20Forgetting%20Neural%20Networks&entry.906535625=Amartya%20Hatua%20and%20Trung%20T.%20Nguyen%20and%20Filip%20Cano%20and%20Andrew%20H.%20Sung&entry.1292438233=Modern%20computer%20systems%20store%20vast%20amounts%20of%20personal%20data%2C%20enabling%20advances%20in%20AI%20and%20ML%20but%20risking%20user%20privacy%20and%20trust.%20For%20privacy%20reasons%2C%20it%20is%20sometimes%20desired%20for%20an%20ML%20model%20to%20forget%20part%20of%20the%20data%20it%20was%20trained%20on.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20unlearning%20approach%20based%20on%20Forgetting%20Neural%20Networks%20%28FNNs%29%2C%20a%20neuroscience-inspired%20architecture%20that%20explicitly%20encodes%20forgetting%20through%20multiplicative%20decay%20factors.%20While%20FNNs%20had%20previously%20been%20studied%20as%20a%20theoretical%20construct%2C%20we%20provide%20the%20first%20concrete%20implementation%20and%20demonstrate%20their%20effectiveness%20for%20targeted%20unlearning.%20We%20propose%20several%20variants%20with%20per-neuron%20forgetting%20factors%2C%20including%20rank-based%20assignments%20guided%20by%20activation%20levels%2C%20and%20evaluate%20them%20on%20MNIST%20and%20Fashion-MNIST%20benchmarks.%20Our%20method%20systematically%20removes%20information%20associated%20with%20forget%20sets%20while%20preserving%20performance%20on%20retained%20data.%20Membership%20inference%20attacks%20confirm%20the%20effectiveness%20of%20FNN-based%20unlearning%20in%20erasing%20information%20about%20the%20training%20data%20from%20the%20neural%20network.%20These%20results%20establish%20FNNs%20as%20a%20promising%20foundation%20for%20efficient%20and%20interpretable%20unlearning.&entry.1838667208=http%3A//arxiv.org/abs/2410.22374v2&entry.124074799=Read"},
{"title": "PINNs for Electromagnetic Wave Propagation", "author": "Nilufer K. Bulut", "abstract": "Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.\n  This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.\n  In the developed PINN model, high field accuracy is achieved with an average 0.09\\% $NRMSE$ and 1.01\\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.", "link": "http://arxiv.org/abs/2512.23396v1", "date": "2025-12-29", "relevancy": 2.1755, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4671}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4311}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PINNs%20for%20Electromagnetic%20Wave%20Propagation&body=Title%3A%20PINNs%20for%20Electromagnetic%20Wave%20Propagation%0AAuthor%3A%20Nilufer%20K.%20Bulut%0AAbstract%3A%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20a%20methodology%20that%20aims%20to%20solve%20physical%20systems%20by%20directly%20embedding%20PDE%20constraints%20into%20the%20neural%20network%20training%20process.%20In%20electromagnetism%2C%20where%20well-established%20methodologies%20such%20as%20FDTD%20and%20FEM%20already%20exist%2C%20new%20methodologies%20are%20expected%20to%20provide%20clear%20advantages%20to%20be%20accepted.%20Despite%20their%20mesh-free%20nature%20and%20applicability%20to%20inverse%20problems%2C%20PINNs%20can%20exhibit%20deficiencies%20in%20terms%20of%20accuracy%20and%20energy%20metrics%20when%20compared%20to%20FDTD%20solutions.%20This%20study%20demonstrates%20hybrid%20training%20strategies%20can%20bring%20PINNs%20closer%20to%20FDTD-level%20accuracy%20and%20energy%20consistency.%0A%20%20This%20study%20presents%20a%20hybrid%20methodology%20addressing%20common%20challenges%20in%20wave%20propagation%20scenarios.%20The%20causality%20collapse%20problem%20in%20time-dependent%20PINN%20training%20is%20addressed%20via%20time%20marching%20and%20causality-aware%20weighting.%20In%20order%20to%20mitigate%20the%20discontinuities%20that%20are%20introduced%20by%20time%20marching%2C%20a%20two-stage%20interface%20continuity%20loss%20is%20applied.%20In%20order%20to%20suppress%20loss%20accumulation%2C%20which%20is%20manifested%20as%20cumulative%20energy%20drift%20in%20electromagnetic%20waves%2C%20a%20local%20Poynting-based%20regularizer%20has%20been%20developed.%0A%20%20In%20the%20developed%20PINN%20model%2C%20high%20field%20accuracy%20is%20achieved%20with%20an%20average%200.09%5C%25%20%24NRMSE%24%20and%201.01%5C%25%20%24L%5E2%24%20error%20over%20time.%20Energy%20conservation%20is%20achieved%20on%20the%20PINN%20side%20with%20only%20a%200.024%5C%25%20relative%20energy%20mismatch%20in%20the%202D%20PEC%20cavity%20scenario.%20Training%20is%20performed%20without%20labeled%20field%20data%2C%20using%20only%20physics-based%20residual%20losses%3B%20FDTD%20is%20used%20solely%20for%20post-training%20evaluation.%20The%20results%20demonstrate%20that%20PINNs%20can%20achieve%20competitive%20results%20with%20FDTD%20in%20canonical%20electromagnetic%20examples%20and%20are%20a%20viable%20alternative.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPINNs%2520for%2520Electromagnetic%2520Wave%2520Propagation%26entry.906535625%3DNilufer%2520K.%2520Bulut%26entry.1292438233%3DPhysics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520are%2520a%2520methodology%2520that%2520aims%2520to%2520solve%2520physical%2520systems%2520by%2520directly%2520embedding%2520PDE%2520constraints%2520into%2520the%2520neural%2520network%2520training%2520process.%2520In%2520electromagnetism%252C%2520where%2520well-established%2520methodologies%2520such%2520as%2520FDTD%2520and%2520FEM%2520already%2520exist%252C%2520new%2520methodologies%2520are%2520expected%2520to%2520provide%2520clear%2520advantages%2520to%2520be%2520accepted.%2520Despite%2520their%2520mesh-free%2520nature%2520and%2520applicability%2520to%2520inverse%2520problems%252C%2520PINNs%2520can%2520exhibit%2520deficiencies%2520in%2520terms%2520of%2520accuracy%2520and%2520energy%2520metrics%2520when%2520compared%2520to%2520FDTD%2520solutions.%2520This%2520study%2520demonstrates%2520hybrid%2520training%2520strategies%2520can%2520bring%2520PINNs%2520closer%2520to%2520FDTD-level%2520accuracy%2520and%2520energy%2520consistency.%250A%2520%2520This%2520study%2520presents%2520a%2520hybrid%2520methodology%2520addressing%2520common%2520challenges%2520in%2520wave%2520propagation%2520scenarios.%2520The%2520causality%2520collapse%2520problem%2520in%2520time-dependent%2520PINN%2520training%2520is%2520addressed%2520via%2520time%2520marching%2520and%2520causality-aware%2520weighting.%2520In%2520order%2520to%2520mitigate%2520the%2520discontinuities%2520that%2520are%2520introduced%2520by%2520time%2520marching%252C%2520a%2520two-stage%2520interface%2520continuity%2520loss%2520is%2520applied.%2520In%2520order%2520to%2520suppress%2520loss%2520accumulation%252C%2520which%2520is%2520manifested%2520as%2520cumulative%2520energy%2520drift%2520in%2520electromagnetic%2520waves%252C%2520a%2520local%2520Poynting-based%2520regularizer%2520has%2520been%2520developed.%250A%2520%2520In%2520the%2520developed%2520PINN%2520model%252C%2520high%2520field%2520accuracy%2520is%2520achieved%2520with%2520an%2520average%25200.09%255C%2525%2520%2524NRMSE%2524%2520and%25201.01%255C%2525%2520%2524L%255E2%2524%2520error%2520over%2520time.%2520Energy%2520conservation%2520is%2520achieved%2520on%2520the%2520PINN%2520side%2520with%2520only%2520a%25200.024%255C%2525%2520relative%2520energy%2520mismatch%2520in%2520the%25202D%2520PEC%2520cavity%2520scenario.%2520Training%2520is%2520performed%2520without%2520labeled%2520field%2520data%252C%2520using%2520only%2520physics-based%2520residual%2520losses%253B%2520FDTD%2520is%2520used%2520solely%2520for%2520post-training%2520evaluation.%2520The%2520results%2520demonstrate%2520that%2520PINNs%2520can%2520achieve%2520competitive%2520results%2520with%2520FDTD%2520in%2520canonical%2520electromagnetic%2520examples%2520and%2520are%2520a%2520viable%2520alternative.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PINNs%20for%20Electromagnetic%20Wave%20Propagation&entry.906535625=Nilufer%20K.%20Bulut&entry.1292438233=Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20a%20methodology%20that%20aims%20to%20solve%20physical%20systems%20by%20directly%20embedding%20PDE%20constraints%20into%20the%20neural%20network%20training%20process.%20In%20electromagnetism%2C%20where%20well-established%20methodologies%20such%20as%20FDTD%20and%20FEM%20already%20exist%2C%20new%20methodologies%20are%20expected%20to%20provide%20clear%20advantages%20to%20be%20accepted.%20Despite%20their%20mesh-free%20nature%20and%20applicability%20to%20inverse%20problems%2C%20PINNs%20can%20exhibit%20deficiencies%20in%20terms%20of%20accuracy%20and%20energy%20metrics%20when%20compared%20to%20FDTD%20solutions.%20This%20study%20demonstrates%20hybrid%20training%20strategies%20can%20bring%20PINNs%20closer%20to%20FDTD-level%20accuracy%20and%20energy%20consistency.%0A%20%20This%20study%20presents%20a%20hybrid%20methodology%20addressing%20common%20challenges%20in%20wave%20propagation%20scenarios.%20The%20causality%20collapse%20problem%20in%20time-dependent%20PINN%20training%20is%20addressed%20via%20time%20marching%20and%20causality-aware%20weighting.%20In%20order%20to%20mitigate%20the%20discontinuities%20that%20are%20introduced%20by%20time%20marching%2C%20a%20two-stage%20interface%20continuity%20loss%20is%20applied.%20In%20order%20to%20suppress%20loss%20accumulation%2C%20which%20is%20manifested%20as%20cumulative%20energy%20drift%20in%20electromagnetic%20waves%2C%20a%20local%20Poynting-based%20regularizer%20has%20been%20developed.%0A%20%20In%20the%20developed%20PINN%20model%2C%20high%20field%20accuracy%20is%20achieved%20with%20an%20average%200.09%5C%25%20%24NRMSE%24%20and%201.01%5C%25%20%24L%5E2%24%20error%20over%20time.%20Energy%20conservation%20is%20achieved%20on%20the%20PINN%20side%20with%20only%20a%200.024%5C%25%20relative%20energy%20mismatch%20in%20the%202D%20PEC%20cavity%20scenario.%20Training%20is%20performed%20without%20labeled%20field%20data%2C%20using%20only%20physics-based%20residual%20losses%3B%20FDTD%20is%20used%20solely%20for%20post-training%20evaluation.%20The%20results%20demonstrate%20that%20PINNs%20can%20achieve%20competitive%20results%20with%20FDTD%20in%20canonical%20electromagnetic%20examples%20and%20are%20a%20viable%20alternative.&entry.1838667208=http%3A//arxiv.org/abs/2512.23396v1&entry.124074799=Read"},
{"title": "From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints", "author": "Dimitra Maoutsa", "abstract": "How can we learn the laws underlying the dynamics of stochastic systems when their trajectories are sampled sparsely in time? Existing methods either require temporally resolved high-frequency observations, or rely on geometric arguments that apply only to conservative systems, limiting the range of dynamics they can recover. Here, we present a new framework that reconciles these two perspectives by reformulating inference as a stochastic control problem. Our method uses geometry-driven path augmentation, guided by the geometry in the system's invariant density to reconstruct likely trajectories and infer the underlying dynamics without assuming specific parametric models. Applied to overdamped Langevin systems, our approach accurately recovers stochastic dynamics even from extremely undersampled data, outperforming existing methods in synthetic benchmarks. This work demonstrates the effectiveness of incorporating geometric inductive biases into stochastic system identification methods.", "link": "http://arxiv.org/abs/2512.23566v1", "date": "2025-12-29", "relevancy": 2.173, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5683}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20geometry%20to%20dynamics%3A%20Learning%20overdamped%20Langevin%20dynamics%20from%20sparse%20observations%20with%20geometric%20constraints&body=Title%3A%20From%20geometry%20to%20dynamics%3A%20Learning%20overdamped%20Langevin%20dynamics%20from%20sparse%20observations%20with%20geometric%20constraints%0AAuthor%3A%20Dimitra%20Maoutsa%0AAbstract%3A%20How%20can%20we%20learn%20the%20laws%20underlying%20the%20dynamics%20of%20stochastic%20systems%20when%20their%20trajectories%20are%20sampled%20sparsely%20in%20time%3F%20Existing%20methods%20either%20require%20temporally%20resolved%20high-frequency%20observations%2C%20or%20rely%20on%20geometric%20arguments%20that%20apply%20only%20to%20conservative%20systems%2C%20limiting%20the%20range%20of%20dynamics%20they%20can%20recover.%20Here%2C%20we%20present%20a%20new%20framework%20that%20reconciles%20these%20two%20perspectives%20by%20reformulating%20inference%20as%20a%20stochastic%20control%20problem.%20Our%20method%20uses%20geometry-driven%20path%20augmentation%2C%20guided%20by%20the%20geometry%20in%20the%20system%27s%20invariant%20density%20to%20reconstruct%20likely%20trajectories%20and%20infer%20the%20underlying%20dynamics%20without%20assuming%20specific%20parametric%20models.%20Applied%20to%20overdamped%20Langevin%20systems%2C%20our%20approach%20accurately%20recovers%20stochastic%20dynamics%20even%20from%20extremely%20undersampled%20data%2C%20outperforming%20existing%20methods%20in%20synthetic%20benchmarks.%20This%20work%20demonstrates%20the%20effectiveness%20of%20incorporating%20geometric%20inductive%20biases%20into%20stochastic%20system%20identification%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520geometry%2520to%2520dynamics%253A%2520Learning%2520overdamped%2520Langevin%2520dynamics%2520from%2520sparse%2520observations%2520with%2520geometric%2520constraints%26entry.906535625%3DDimitra%2520Maoutsa%26entry.1292438233%3DHow%2520can%2520we%2520learn%2520the%2520laws%2520underlying%2520the%2520dynamics%2520of%2520stochastic%2520systems%2520when%2520their%2520trajectories%2520are%2520sampled%2520sparsely%2520in%2520time%253F%2520Existing%2520methods%2520either%2520require%2520temporally%2520resolved%2520high-frequency%2520observations%252C%2520or%2520rely%2520on%2520geometric%2520arguments%2520that%2520apply%2520only%2520to%2520conservative%2520systems%252C%2520limiting%2520the%2520range%2520of%2520dynamics%2520they%2520can%2520recover.%2520Here%252C%2520we%2520present%2520a%2520new%2520framework%2520that%2520reconciles%2520these%2520two%2520perspectives%2520by%2520reformulating%2520inference%2520as%2520a%2520stochastic%2520control%2520problem.%2520Our%2520method%2520uses%2520geometry-driven%2520path%2520augmentation%252C%2520guided%2520by%2520the%2520geometry%2520in%2520the%2520system%2527s%2520invariant%2520density%2520to%2520reconstruct%2520likely%2520trajectories%2520and%2520infer%2520the%2520underlying%2520dynamics%2520without%2520assuming%2520specific%2520parametric%2520models.%2520Applied%2520to%2520overdamped%2520Langevin%2520systems%252C%2520our%2520approach%2520accurately%2520recovers%2520stochastic%2520dynamics%2520even%2520from%2520extremely%2520undersampled%2520data%252C%2520outperforming%2520existing%2520methods%2520in%2520synthetic%2520benchmarks.%2520This%2520work%2520demonstrates%2520the%2520effectiveness%2520of%2520incorporating%2520geometric%2520inductive%2520biases%2520into%2520stochastic%2520system%2520identification%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20geometry%20to%20dynamics%3A%20Learning%20overdamped%20Langevin%20dynamics%20from%20sparse%20observations%20with%20geometric%20constraints&entry.906535625=Dimitra%20Maoutsa&entry.1292438233=How%20can%20we%20learn%20the%20laws%20underlying%20the%20dynamics%20of%20stochastic%20systems%20when%20their%20trajectories%20are%20sampled%20sparsely%20in%20time%3F%20Existing%20methods%20either%20require%20temporally%20resolved%20high-frequency%20observations%2C%20or%20rely%20on%20geometric%20arguments%20that%20apply%20only%20to%20conservative%20systems%2C%20limiting%20the%20range%20of%20dynamics%20they%20can%20recover.%20Here%2C%20we%20present%20a%20new%20framework%20that%20reconciles%20these%20two%20perspectives%20by%20reformulating%20inference%20as%20a%20stochastic%20control%20problem.%20Our%20method%20uses%20geometry-driven%20path%20augmentation%2C%20guided%20by%20the%20geometry%20in%20the%20system%27s%20invariant%20density%20to%20reconstruct%20likely%20trajectories%20and%20infer%20the%20underlying%20dynamics%20without%20assuming%20specific%20parametric%20models.%20Applied%20to%20overdamped%20Langevin%20systems%2C%20our%20approach%20accurately%20recovers%20stochastic%20dynamics%20even%20from%20extremely%20undersampled%20data%2C%20outperforming%20existing%20methods%20in%20synthetic%20benchmarks.%20This%20work%20demonstrates%20the%20effectiveness%20of%20incorporating%20geometric%20inductive%20biases%20into%20stochastic%20system%20identification%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.23566v1&entry.124074799=Read"},
{"title": "Anti-Slip AI-Driven Model-Free Control with Global Exponential Stability in Skid-Steering Robots", "author": "Mehdi Heydari Shahna and Pauli Mustalahti and Jouni Mattila", "abstract": "Undesired lateral and longitudinal wheel slippage can disrupt a mobile robot's heading angle, traction, and, eventually, desired motion. This issue makes the robotization and accurate modeling of heavy-duty machinery very challenging because the application primarily involves off-road terrains, which are susceptible to uneven motion and severe slippage. As a step toward robotization in skid-steering heavy-duty robot (SSHDR), this paper aims to design an innovative robust model-free control system developed by neural networks to strongly stabilize the robot dynamics in the presence of a broad range of potential wheel slippages. Before the control design, the dynamics of the SSHDR are first investigated by mathematically incorporating slippage effects, assuming that all functional modeling terms of the system are unknown to the control system. Then, a novel tracking control framework to guarantee global exponential stability of the SSHDR is designed as follows: 1) the unknown modeling of wheel dynamics is approximated using radial basis function neural networks (RBFNNs); and 2) a new adaptive law is proposed to compensate for slippage effects and tune the weights of the RBFNNs online during execution. Simulation and experimental results verify the proposed tracking control performance of a 4,836 kg SSHDR operating on slippery terrain.", "link": "http://arxiv.org/abs/2504.08831v2", "date": "2025-12-29", "relevancy": 2.1663, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anti-Slip%20AI-Driven%20Model-Free%20Control%20with%20Global%20Exponential%20Stability%20in%20Skid-Steering%20Robots&body=Title%3A%20Anti-Slip%20AI-Driven%20Model-Free%20Control%20with%20Global%20Exponential%20Stability%20in%20Skid-Steering%20Robots%0AAuthor%3A%20Mehdi%20Heydari%20Shahna%20and%20Pauli%20Mustalahti%20and%20Jouni%20Mattila%0AAbstract%3A%20Undesired%20lateral%20and%20longitudinal%20wheel%20slippage%20can%20disrupt%20a%20mobile%20robot%27s%20heading%20angle%2C%20traction%2C%20and%2C%20eventually%2C%20desired%20motion.%20This%20issue%20makes%20the%20robotization%20and%20accurate%20modeling%20of%20heavy-duty%20machinery%20very%20challenging%20because%20the%20application%20primarily%20involves%20off-road%20terrains%2C%20which%20are%20susceptible%20to%20uneven%20motion%20and%20severe%20slippage.%20As%20a%20step%20toward%20robotization%20in%20skid-steering%20heavy-duty%20robot%20%28SSHDR%29%2C%20this%20paper%20aims%20to%20design%20an%20innovative%20robust%20model-free%20control%20system%20developed%20by%20neural%20networks%20to%20strongly%20stabilize%20the%20robot%20dynamics%20in%20the%20presence%20of%20a%20broad%20range%20of%20potential%20wheel%20slippages.%20Before%20the%20control%20design%2C%20the%20dynamics%20of%20the%20SSHDR%20are%20first%20investigated%20by%20mathematically%20incorporating%20slippage%20effects%2C%20assuming%20that%20all%20functional%20modeling%20terms%20of%20the%20system%20are%20unknown%20to%20the%20control%20system.%20Then%2C%20a%20novel%20tracking%20control%20framework%20to%20guarantee%20global%20exponential%20stability%20of%20the%20SSHDR%20is%20designed%20as%20follows%3A%201%29%20the%20unknown%20modeling%20of%20wheel%20dynamics%20is%20approximated%20using%20radial%20basis%20function%20neural%20networks%20%28RBFNNs%29%3B%20and%202%29%20a%20new%20adaptive%20law%20is%20proposed%20to%20compensate%20for%20slippage%20effects%20and%20tune%20the%20weights%20of%20the%20RBFNNs%20online%20during%20execution.%20Simulation%20and%20experimental%20results%20verify%20the%20proposed%20tracking%20control%20performance%20of%20a%204%2C836%20kg%20SSHDR%20operating%20on%20slippery%20terrain.%0ALink%3A%20http%3A//arxiv.org/abs/2504.08831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnti-Slip%2520AI-Driven%2520Model-Free%2520Control%2520with%2520Global%2520Exponential%2520Stability%2520in%2520Skid-Steering%2520Robots%26entry.906535625%3DMehdi%2520Heydari%2520Shahna%2520and%2520Pauli%2520Mustalahti%2520and%2520Jouni%2520Mattila%26entry.1292438233%3DUndesired%2520lateral%2520and%2520longitudinal%2520wheel%2520slippage%2520can%2520disrupt%2520a%2520mobile%2520robot%2527s%2520heading%2520angle%252C%2520traction%252C%2520and%252C%2520eventually%252C%2520desired%2520motion.%2520This%2520issue%2520makes%2520the%2520robotization%2520and%2520accurate%2520modeling%2520of%2520heavy-duty%2520machinery%2520very%2520challenging%2520because%2520the%2520application%2520primarily%2520involves%2520off-road%2520terrains%252C%2520which%2520are%2520susceptible%2520to%2520uneven%2520motion%2520and%2520severe%2520slippage.%2520As%2520a%2520step%2520toward%2520robotization%2520in%2520skid-steering%2520heavy-duty%2520robot%2520%2528SSHDR%2529%252C%2520this%2520paper%2520aims%2520to%2520design%2520an%2520innovative%2520robust%2520model-free%2520control%2520system%2520developed%2520by%2520neural%2520networks%2520to%2520strongly%2520stabilize%2520the%2520robot%2520dynamics%2520in%2520the%2520presence%2520of%2520a%2520broad%2520range%2520of%2520potential%2520wheel%2520slippages.%2520Before%2520the%2520control%2520design%252C%2520the%2520dynamics%2520of%2520the%2520SSHDR%2520are%2520first%2520investigated%2520by%2520mathematically%2520incorporating%2520slippage%2520effects%252C%2520assuming%2520that%2520all%2520functional%2520modeling%2520terms%2520of%2520the%2520system%2520are%2520unknown%2520to%2520the%2520control%2520system.%2520Then%252C%2520a%2520novel%2520tracking%2520control%2520framework%2520to%2520guarantee%2520global%2520exponential%2520stability%2520of%2520the%2520SSHDR%2520is%2520designed%2520as%2520follows%253A%25201%2529%2520the%2520unknown%2520modeling%2520of%2520wheel%2520dynamics%2520is%2520approximated%2520using%2520radial%2520basis%2520function%2520neural%2520networks%2520%2528RBFNNs%2529%253B%2520and%25202%2529%2520a%2520new%2520adaptive%2520law%2520is%2520proposed%2520to%2520compensate%2520for%2520slippage%2520effects%2520and%2520tune%2520the%2520weights%2520of%2520the%2520RBFNNs%2520online%2520during%2520execution.%2520Simulation%2520and%2520experimental%2520results%2520verify%2520the%2520proposed%2520tracking%2520control%2520performance%2520of%2520a%25204%252C836%2520kg%2520SSHDR%2520operating%2520on%2520slippery%2520terrain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anti-Slip%20AI-Driven%20Model-Free%20Control%20with%20Global%20Exponential%20Stability%20in%20Skid-Steering%20Robots&entry.906535625=Mehdi%20Heydari%20Shahna%20and%20Pauli%20Mustalahti%20and%20Jouni%20Mattila&entry.1292438233=Undesired%20lateral%20and%20longitudinal%20wheel%20slippage%20can%20disrupt%20a%20mobile%20robot%27s%20heading%20angle%2C%20traction%2C%20and%2C%20eventually%2C%20desired%20motion.%20This%20issue%20makes%20the%20robotization%20and%20accurate%20modeling%20of%20heavy-duty%20machinery%20very%20challenging%20because%20the%20application%20primarily%20involves%20off-road%20terrains%2C%20which%20are%20susceptible%20to%20uneven%20motion%20and%20severe%20slippage.%20As%20a%20step%20toward%20robotization%20in%20skid-steering%20heavy-duty%20robot%20%28SSHDR%29%2C%20this%20paper%20aims%20to%20design%20an%20innovative%20robust%20model-free%20control%20system%20developed%20by%20neural%20networks%20to%20strongly%20stabilize%20the%20robot%20dynamics%20in%20the%20presence%20of%20a%20broad%20range%20of%20potential%20wheel%20slippages.%20Before%20the%20control%20design%2C%20the%20dynamics%20of%20the%20SSHDR%20are%20first%20investigated%20by%20mathematically%20incorporating%20slippage%20effects%2C%20assuming%20that%20all%20functional%20modeling%20terms%20of%20the%20system%20are%20unknown%20to%20the%20control%20system.%20Then%2C%20a%20novel%20tracking%20control%20framework%20to%20guarantee%20global%20exponential%20stability%20of%20the%20SSHDR%20is%20designed%20as%20follows%3A%201%29%20the%20unknown%20modeling%20of%20wheel%20dynamics%20is%20approximated%20using%20radial%20basis%20function%20neural%20networks%20%28RBFNNs%29%3B%20and%202%29%20a%20new%20adaptive%20law%20is%20proposed%20to%20compensate%20for%20slippage%20effects%20and%20tune%20the%20weights%20of%20the%20RBFNNs%20online%20during%20execution.%20Simulation%20and%20experimental%20results%20verify%20the%20proposed%20tracking%20control%20performance%20of%20a%204%2C836%20kg%20SSHDR%20operating%20on%20slippery%20terrain.&entry.1838667208=http%3A//arxiv.org/abs/2504.08831v2&entry.124074799=Read"},
{"title": "AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis", "author": "Cehua Yang and Dongyu Xiao and Junming Lin and Yuyang Song and Hanxu Yan and Shawn Guo and Wei Zhang and Jian Yang and Mingjie Tang and Bryan Dai", "abstract": "The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent's reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.", "link": "http://arxiv.org/abs/2512.23366v1", "date": "2025-12-29", "relevancy": 2.1594, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5575}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5478}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGRO-SQL%3A%20Agentic%20Group-Relative%20Optimization%20with%20High-Fidelity%20Data%20Synthesis&body=Title%3A%20AGRO-SQL%3A%20Agentic%20Group-Relative%20Optimization%20with%20High-Fidelity%20Data%20Synthesis%0AAuthor%3A%20Cehua%20Yang%20and%20Dongyu%20Xiao%20and%20Junming%20Lin%20and%20Yuyang%20Song%20and%20Hanxu%20Yan%20and%20Shawn%20Guo%20and%20Wei%20Zhang%20and%20Jian%20Yang%20and%20Mingjie%20Tang%20and%20Bryan%20Dai%0AAbstract%3A%20The%20advancement%20of%20Text-to-SQL%20systems%20is%20currently%20hindered%20by%20the%20scarcity%20of%20high-quality%20training%20data%20and%20the%20limited%20reasoning%20capabilities%20of%20models%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20holistic%20framework%20that%20addresses%20these%20issues%20through%20a%20dual-centric%20approach.%20From%20a%20Data-Centric%20perspective%2C%20we%20construct%20an%20iterative%20data%20factory%20that%20synthesizes%20RL-ready%20data%20characterized%20by%20high%20correctness%20and%20precise%20semantic-logic%20alignment%2C%20ensured%20by%20strict%20verification.%20From%20a%20Model-Centric%20perspective%2C%20we%20introduce%20a%20novel%20Agentic%20Reinforcement%20Learning%20framework.%20This%20framework%20employs%20a%20Diversity-Aware%20Cold%20Start%20stage%20to%20initialize%20a%20robust%20policy%2C%20followed%20by%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20refine%20the%20agent%27s%20reasoning%20via%20environmental%20feedback.%20Extensive%20experiments%20on%20BIRD%20and%20Spider%20benchmarks%20demonstrate%20that%20our%20synergistic%20approach%20achieves%20state-of-the-art%20performance%20among%20single-model%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGRO-SQL%253A%2520Agentic%2520Group-Relative%2520Optimization%2520with%2520High-Fidelity%2520Data%2520Synthesis%26entry.906535625%3DCehua%2520Yang%2520and%2520Dongyu%2520Xiao%2520and%2520Junming%2520Lin%2520and%2520Yuyang%2520Song%2520and%2520Hanxu%2520Yan%2520and%2520Shawn%2520Guo%2520and%2520Wei%2520Zhang%2520and%2520Jian%2520Yang%2520and%2520Mingjie%2520Tang%2520and%2520Bryan%2520Dai%26entry.1292438233%3DThe%2520advancement%2520of%2520Text-to-SQL%2520systems%2520is%2520currently%2520hindered%2520by%2520the%2520scarcity%2520of%2520high-quality%2520training%2520data%2520and%2520the%2520limited%2520reasoning%2520capabilities%2520of%2520models%2520in%2520complex%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520holistic%2520framework%2520that%2520addresses%2520these%2520issues%2520through%2520a%2520dual-centric%2520approach.%2520From%2520a%2520Data-Centric%2520perspective%252C%2520we%2520construct%2520an%2520iterative%2520data%2520factory%2520that%2520synthesizes%2520RL-ready%2520data%2520characterized%2520by%2520high%2520correctness%2520and%2520precise%2520semantic-logic%2520alignment%252C%2520ensured%2520by%2520strict%2520verification.%2520From%2520a%2520Model-Centric%2520perspective%252C%2520we%2520introduce%2520a%2520novel%2520Agentic%2520Reinforcement%2520Learning%2520framework.%2520This%2520framework%2520employs%2520a%2520Diversity-Aware%2520Cold%2520Start%2520stage%2520to%2520initialize%2520a%2520robust%2520policy%252C%2520followed%2520by%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520refine%2520the%2520agent%2527s%2520reasoning%2520via%2520environmental%2520feedback.%2520Extensive%2520experiments%2520on%2520BIRD%2520and%2520Spider%2520benchmarks%2520demonstrate%2520that%2520our%2520synergistic%2520approach%2520achieves%2520state-of-the-art%2520performance%2520among%2520single-model%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGRO-SQL%3A%20Agentic%20Group-Relative%20Optimization%20with%20High-Fidelity%20Data%20Synthesis&entry.906535625=Cehua%20Yang%20and%20Dongyu%20Xiao%20and%20Junming%20Lin%20and%20Yuyang%20Song%20and%20Hanxu%20Yan%20and%20Shawn%20Guo%20and%20Wei%20Zhang%20and%20Jian%20Yang%20and%20Mingjie%20Tang%20and%20Bryan%20Dai&entry.1292438233=The%20advancement%20of%20Text-to-SQL%20systems%20is%20currently%20hindered%20by%20the%20scarcity%20of%20high-quality%20training%20data%20and%20the%20limited%20reasoning%20capabilities%20of%20models%20in%20complex%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20holistic%20framework%20that%20addresses%20these%20issues%20through%20a%20dual-centric%20approach.%20From%20a%20Data-Centric%20perspective%2C%20we%20construct%20an%20iterative%20data%20factory%20that%20synthesizes%20RL-ready%20data%20characterized%20by%20high%20correctness%20and%20precise%20semantic-logic%20alignment%2C%20ensured%20by%20strict%20verification.%20From%20a%20Model-Centric%20perspective%2C%20we%20introduce%20a%20novel%20Agentic%20Reinforcement%20Learning%20framework.%20This%20framework%20employs%20a%20Diversity-Aware%20Cold%20Start%20stage%20to%20initialize%20a%20robust%20policy%2C%20followed%20by%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20refine%20the%20agent%27s%20reasoning%20via%20environmental%20feedback.%20Extensive%20experiments%20on%20BIRD%20and%20Spider%20benchmarks%20demonstrate%20that%20our%20synergistic%20approach%20achieves%20state-of-the-art%20performance%20among%20single-model%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.23366v1&entry.124074799=Read"},
{"title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning", "author": "Yuan Yuan and Yukun Liu and Chonghua Han and Jie Feng and Yong Li", "abstract": "Human mobility is a fundamental pillar of urban science and sustainability, providing critical insights into energy consumption, carbon emissions, and public health. However, the discovery of universal mobility laws is currently hindered by the ``data silo'' problem, where institutional boundaries and privacy regulations fragment the necessary large-scale datasets. In this paper, we propose MoveGCL, a transformative framework that facilitates collaborative and decentralized mobility science via generative continual learning. MoveGCL enables a distributed ecosystem of data holders to jointly evolve a foundation model without compromising individual privacy. The core of MoveGCL lies in its ability to replay synthetic trajectories derived from a generative teacher and utilize a mobility-pattern-aware Mixture-of-Experts (MoE) architecture. This allows the model to encapsulate the unique characteristics of diverse urban structures while mitigating the risk of knowledge erosion (catastrophic forgetting). With a specialized layer-wise progressive adaptation strategy, MoveGCL ensures stable convergence during the continuous integration of new urban domains. Our experiments on six global urban datasets demonstrate that MoveGCL achieves performance parity with joint training, a previously unattainable feat under siloed conditions. This work provides a scalable, privacy-preserving pathway toward Open Mobility Science, empowering researchers to address global sustainability challenges through cross-institutional AI collaboration. To facilitate reproducibility and future research, we have released the code and models at \\color{blue}{https://github.com/tsinghua-fib-lab/MoveGCL}.", "link": "http://arxiv.org/abs/2506.06694v6", "date": "2025-12-29", "relevancy": 2.1573, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5426}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5384}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Data%20Silos%3A%20Towards%20Open%20and%20Scalable%20Mobility%20Foundation%20Models%20via%20Generative%20Continual%20Learning&body=Title%3A%20Breaking%20Data%20Silos%3A%20Towards%20Open%20and%20Scalable%20Mobility%20Foundation%20Models%20via%20Generative%20Continual%20Learning%0AAuthor%3A%20Yuan%20Yuan%20and%20Yukun%20Liu%20and%20Chonghua%20Han%20and%20Jie%20Feng%20and%20Yong%20Li%0AAbstract%3A%20Human%20mobility%20is%20a%20fundamental%20pillar%20of%20urban%20science%20and%20sustainability%2C%20providing%20critical%20insights%20into%20energy%20consumption%2C%20carbon%20emissions%2C%20and%20public%20health.%20However%2C%20the%20discovery%20of%20universal%20mobility%20laws%20is%20currently%20hindered%20by%20the%20%60%60data%20silo%27%27%20problem%2C%20where%20institutional%20boundaries%20and%20privacy%20regulations%20fragment%20the%20necessary%20large-scale%20datasets.%20In%20this%20paper%2C%20we%20propose%20MoveGCL%2C%20a%20transformative%20framework%20that%20facilitates%20collaborative%20and%20decentralized%20mobility%20science%20via%20generative%20continual%20learning.%20MoveGCL%20enables%20a%20distributed%20ecosystem%20of%20data%20holders%20to%20jointly%20evolve%20a%20foundation%20model%20without%20compromising%20individual%20privacy.%20The%20core%20of%20MoveGCL%20lies%20in%20its%20ability%20to%20replay%20synthetic%20trajectories%20derived%20from%20a%20generative%20teacher%20and%20utilize%20a%20mobility-pattern-aware%20Mixture-of-Experts%20%28MoE%29%20architecture.%20This%20allows%20the%20model%20to%20encapsulate%20the%20unique%20characteristics%20of%20diverse%20urban%20structures%20while%20mitigating%20the%20risk%20of%20knowledge%20erosion%20%28catastrophic%20forgetting%29.%20With%20a%20specialized%20layer-wise%20progressive%20adaptation%20strategy%2C%20MoveGCL%20ensures%20stable%20convergence%20during%20the%20continuous%20integration%20of%20new%20urban%20domains.%20Our%20experiments%20on%20six%20global%20urban%20datasets%20demonstrate%20that%20MoveGCL%20achieves%20performance%20parity%20with%20joint%20training%2C%20a%20previously%20unattainable%20feat%20under%20siloed%20conditions.%20This%20work%20provides%20a%20scalable%2C%20privacy-preserving%20pathway%20toward%20Open%20Mobility%20Science%2C%20empowering%20researchers%20to%20address%20global%20sustainability%20challenges%20through%20cross-institutional%20AI%20collaboration.%20To%20facilitate%20reproducibility%20and%20future%20research%2C%20we%20have%20released%20the%20code%20and%20models%20at%20%5Ccolor%7Bblue%7D%7Bhttps%3A//github.com/tsinghua-fib-lab/MoveGCL%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06694v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Data%2520Silos%253A%2520Towards%2520Open%2520and%2520Scalable%2520Mobility%2520Foundation%2520Models%2520via%2520Generative%2520Continual%2520Learning%26entry.906535625%3DYuan%2520Yuan%2520and%2520Yukun%2520Liu%2520and%2520Chonghua%2520Han%2520and%2520Jie%2520Feng%2520and%2520Yong%2520Li%26entry.1292438233%3DHuman%2520mobility%2520is%2520a%2520fundamental%2520pillar%2520of%2520urban%2520science%2520and%2520sustainability%252C%2520providing%2520critical%2520insights%2520into%2520energy%2520consumption%252C%2520carbon%2520emissions%252C%2520and%2520public%2520health.%2520However%252C%2520the%2520discovery%2520of%2520universal%2520mobility%2520laws%2520is%2520currently%2520hindered%2520by%2520the%2520%2560%2560data%2520silo%2527%2527%2520problem%252C%2520where%2520institutional%2520boundaries%2520and%2520privacy%2520regulations%2520fragment%2520the%2520necessary%2520large-scale%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MoveGCL%252C%2520a%2520transformative%2520framework%2520that%2520facilitates%2520collaborative%2520and%2520decentralized%2520mobility%2520science%2520via%2520generative%2520continual%2520learning.%2520MoveGCL%2520enables%2520a%2520distributed%2520ecosystem%2520of%2520data%2520holders%2520to%2520jointly%2520evolve%2520a%2520foundation%2520model%2520without%2520compromising%2520individual%2520privacy.%2520The%2520core%2520of%2520MoveGCL%2520lies%2520in%2520its%2520ability%2520to%2520replay%2520synthetic%2520trajectories%2520derived%2520from%2520a%2520generative%2520teacher%2520and%2520utilize%2520a%2520mobility-pattern-aware%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture.%2520This%2520allows%2520the%2520model%2520to%2520encapsulate%2520the%2520unique%2520characteristics%2520of%2520diverse%2520urban%2520structures%2520while%2520mitigating%2520the%2520risk%2520of%2520knowledge%2520erosion%2520%2528catastrophic%2520forgetting%2529.%2520With%2520a%2520specialized%2520layer-wise%2520progressive%2520adaptation%2520strategy%252C%2520MoveGCL%2520ensures%2520stable%2520convergence%2520during%2520the%2520continuous%2520integration%2520of%2520new%2520urban%2520domains.%2520Our%2520experiments%2520on%2520six%2520global%2520urban%2520datasets%2520demonstrate%2520that%2520MoveGCL%2520achieves%2520performance%2520parity%2520with%2520joint%2520training%252C%2520a%2520previously%2520unattainable%2520feat%2520under%2520siloed%2520conditions.%2520This%2520work%2520provides%2520a%2520scalable%252C%2520privacy-preserving%2520pathway%2520toward%2520Open%2520Mobility%2520Science%252C%2520empowering%2520researchers%2520to%2520address%2520global%2520sustainability%2520challenges%2520through%2520cross-institutional%2520AI%2520collaboration.%2520To%2520facilitate%2520reproducibility%2520and%2520future%2520research%252C%2520we%2520have%2520released%2520the%2520code%2520and%2520models%2520at%2520%255Ccolor%257Bblue%257D%257Bhttps%253A//github.com/tsinghua-fib-lab/MoveGCL%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06694v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Data%20Silos%3A%20Towards%20Open%20and%20Scalable%20Mobility%20Foundation%20Models%20via%20Generative%20Continual%20Learning&entry.906535625=Yuan%20Yuan%20and%20Yukun%20Liu%20and%20Chonghua%20Han%20and%20Jie%20Feng%20and%20Yong%20Li&entry.1292438233=Human%20mobility%20is%20a%20fundamental%20pillar%20of%20urban%20science%20and%20sustainability%2C%20providing%20critical%20insights%20into%20energy%20consumption%2C%20carbon%20emissions%2C%20and%20public%20health.%20However%2C%20the%20discovery%20of%20universal%20mobility%20laws%20is%20currently%20hindered%20by%20the%20%60%60data%20silo%27%27%20problem%2C%20where%20institutional%20boundaries%20and%20privacy%20regulations%20fragment%20the%20necessary%20large-scale%20datasets.%20In%20this%20paper%2C%20we%20propose%20MoveGCL%2C%20a%20transformative%20framework%20that%20facilitates%20collaborative%20and%20decentralized%20mobility%20science%20via%20generative%20continual%20learning.%20MoveGCL%20enables%20a%20distributed%20ecosystem%20of%20data%20holders%20to%20jointly%20evolve%20a%20foundation%20model%20without%20compromising%20individual%20privacy.%20The%20core%20of%20MoveGCL%20lies%20in%20its%20ability%20to%20replay%20synthetic%20trajectories%20derived%20from%20a%20generative%20teacher%20and%20utilize%20a%20mobility-pattern-aware%20Mixture-of-Experts%20%28MoE%29%20architecture.%20This%20allows%20the%20model%20to%20encapsulate%20the%20unique%20characteristics%20of%20diverse%20urban%20structures%20while%20mitigating%20the%20risk%20of%20knowledge%20erosion%20%28catastrophic%20forgetting%29.%20With%20a%20specialized%20layer-wise%20progressive%20adaptation%20strategy%2C%20MoveGCL%20ensures%20stable%20convergence%20during%20the%20continuous%20integration%20of%20new%20urban%20domains.%20Our%20experiments%20on%20six%20global%20urban%20datasets%20demonstrate%20that%20MoveGCL%20achieves%20performance%20parity%20with%20joint%20training%2C%20a%20previously%20unattainable%20feat%20under%20siloed%20conditions.%20This%20work%20provides%20a%20scalable%2C%20privacy-preserving%20pathway%20toward%20Open%20Mobility%20Science%2C%20empowering%20researchers%20to%20address%20global%20sustainability%20challenges%20through%20cross-institutional%20AI%20collaboration.%20To%20facilitate%20reproducibility%20and%20future%20research%2C%20we%20have%20released%20the%20code%20and%20models%20at%20%5Ccolor%7Bblue%7D%7Bhttps%3A//github.com/tsinghua-fib-lab/MoveGCL%7D.&entry.1838667208=http%3A//arxiv.org/abs/2506.06694v6&entry.124074799=Read"},
{"title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning", "author": "Jiawei Chen and Xintian Shen and Lihao Zheng and Zhenwei Shao and Hongyuan Zhang and Pengfei Yu and Xudong Rao and Ning Mao and Xiaobo Liu and Lian Wen and Chaoqun Du and Feng Gu and Wei He and Qizhen Li and Shanshan Li and Zide Liu and Jing Luo and Lifu Mu and Xuhao Pan and Chang Ren and Haoyi Sun and Qian Wang and Wei Wang and Hongfu Yang and Jiqing Zhan and Chunpeng Zhou and Zheng Zhou and Hao Ma and Tao Wei and Pan Zhou and Wei Chen", "abstract": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.", "link": "http://arxiv.org/abs/2512.23412v1", "date": "2025-12-29", "relevancy": 2.1563, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning&body=Title%3A%20MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning%0AAuthor%3A%20Jiawei%20Chen%20and%20Xintian%20Shen%20and%20Lihao%20Zheng%20and%20Zhenwei%20Shao%20and%20Hongyuan%20Zhang%20and%20Pengfei%20Yu%20and%20Xudong%20Rao%20and%20Ning%20Mao%20and%20Xiaobo%20Liu%20and%20Lian%20Wen%20and%20Chaoqun%20Du%20and%20Feng%20Gu%20and%20Wei%20He%20and%20Qizhen%20Li%20and%20Shanshan%20Li%20and%20Zide%20Liu%20and%20Jing%20Luo%20and%20Lifu%20Mu%20and%20Xuhao%20Pan%20and%20Chang%20Ren%20and%20Haoyi%20Sun%20and%20Qian%20Wang%20and%20Wei%20Wang%20and%20Hongfu%20Yang%20and%20Jiqing%20Zhan%20and%20Chunpeng%20Zhou%20and%20Zheng%20Zhou%20and%20Hao%20Ma%20and%20Tao%20Wei%20and%20Pan%20Zhou%20and%20Wei%20Chen%0AAbstract%3A%20Traditional%20workflow-based%20agents%20exhibit%20limited%20intelligence%20when%20addressing%20real-world%20problems%20requiring%20tool%20invocation.%20Tool-integrated%20reasoning%20%28TIR%29%20agents%20capable%20of%20autonomous%20reasoning%20and%20tool%20invocation%20are%20rapidly%20emerging%20as%20a%20powerful%20approach%20for%20complex%20decision-making%20tasks%20involving%20multi-step%20interactions%20with%20external%20environments.%20In%20this%20work%2C%20we%20introduce%20MindWatcher%2C%20a%20TIR%20agent%20integrating%20interleaved%20thinking%20and%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning.%20MindWatcher%20can%20autonomously%20decide%20whether%20and%20how%20to%20invoke%20diverse%20tools%20and%20coordinate%20their%20use%2C%20without%20relying%20on%20human%20prompts%20or%20workflows.%20The%20interleaved%20thinking%20paradigm%20enables%20the%20model%20to%20switch%20between%20thinking%20and%20tool%20calling%20at%20any%20intermediate%20stage%2C%20while%20its%20multimodal%20CoT%20capability%20allows%20manipulation%20of%20images%20during%20reasoning%20to%20yield%20more%20precise%20search%20results.%20We%20implement%20automated%20data%20auditing%20and%20evaluation%20pipelines%2C%20complemented%20by%20manually%20curated%20high-quality%20datasets%20for%20training%2C%20and%20we%20construct%20a%20benchmark%2C%20called%20MindWatcher-Evaluate%20Bench%20%28MWE-Bench%29%2C%20to%20evaluate%20its%20performance.%20MindWatcher%20is%20equipped%20with%20a%20comprehensive%20suite%20of%20auxiliary%20reasoning%20tools%2C%20enabling%20it%20to%20address%20broad-domain%20multimodal%20problems.%20A%20large-scale%2C%20high-quality%20local%20image%20retrieval%20database%2C%20covering%20eight%20categories%20including%20cars%2C%20animals%2C%20and%20plants%2C%20endows%20model%20with%20robust%20object%20recognition%20despite%20its%20small%20size.%20Finally%2C%20we%20design%20a%20more%20efficient%20training%20infrastructure%20for%20MindWatcher%2C%20enhancing%20training%20speed%20and%20hardware%20utilization.%20Experiments%20not%20only%20demonstrate%20that%20MindWatcher%20matches%20or%20exceeds%20the%20performance%20of%20larger%20or%20more%20recent%20models%20through%20superior%20tool%20invocation%2C%20but%20also%20uncover%20critical%20insights%20for%20agent%20training%2C%20such%20as%20the%20genetic%20inheritance%20phenomenon%20in%20agentic%20RL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindWatcher%253A%2520Toward%2520Smarter%2520Multimodal%2520Tool-Integrated%2520Reasoning%26entry.906535625%3DJiawei%2520Chen%2520and%2520Xintian%2520Shen%2520and%2520Lihao%2520Zheng%2520and%2520Zhenwei%2520Shao%2520and%2520Hongyuan%2520Zhang%2520and%2520Pengfei%2520Yu%2520and%2520Xudong%2520Rao%2520and%2520Ning%2520Mao%2520and%2520Xiaobo%2520Liu%2520and%2520Lian%2520Wen%2520and%2520Chaoqun%2520Du%2520and%2520Feng%2520Gu%2520and%2520Wei%2520He%2520and%2520Qizhen%2520Li%2520and%2520Shanshan%2520Li%2520and%2520Zide%2520Liu%2520and%2520Jing%2520Luo%2520and%2520Lifu%2520Mu%2520and%2520Xuhao%2520Pan%2520and%2520Chang%2520Ren%2520and%2520Haoyi%2520Sun%2520and%2520Qian%2520Wang%2520and%2520Wei%2520Wang%2520and%2520Hongfu%2520Yang%2520and%2520Jiqing%2520Zhan%2520and%2520Chunpeng%2520Zhou%2520and%2520Zheng%2520Zhou%2520and%2520Hao%2520Ma%2520and%2520Tao%2520Wei%2520and%2520Pan%2520Zhou%2520and%2520Wei%2520Chen%26entry.1292438233%3DTraditional%2520workflow-based%2520agents%2520exhibit%2520limited%2520intelligence%2520when%2520addressing%2520real-world%2520problems%2520requiring%2520tool%2520invocation.%2520Tool-integrated%2520reasoning%2520%2528TIR%2529%2520agents%2520capable%2520of%2520autonomous%2520reasoning%2520and%2520tool%2520invocation%2520are%2520rapidly%2520emerging%2520as%2520a%2520powerful%2520approach%2520for%2520complex%2520decision-making%2520tasks%2520involving%2520multi-step%2520interactions%2520with%2520external%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MindWatcher%252C%2520a%2520TIR%2520agent%2520integrating%2520interleaved%2520thinking%2520and%2520multimodal%2520chain-of-thought%2520%2528CoT%2529%2520reasoning.%2520MindWatcher%2520can%2520autonomously%2520decide%2520whether%2520and%2520how%2520to%2520invoke%2520diverse%2520tools%2520and%2520coordinate%2520their%2520use%252C%2520without%2520relying%2520on%2520human%2520prompts%2520or%2520workflows.%2520The%2520interleaved%2520thinking%2520paradigm%2520enables%2520the%2520model%2520to%2520switch%2520between%2520thinking%2520and%2520tool%2520calling%2520at%2520any%2520intermediate%2520stage%252C%2520while%2520its%2520multimodal%2520CoT%2520capability%2520allows%2520manipulation%2520of%2520images%2520during%2520reasoning%2520to%2520yield%2520more%2520precise%2520search%2520results.%2520We%2520implement%2520automated%2520data%2520auditing%2520and%2520evaluation%2520pipelines%252C%2520complemented%2520by%2520manually%2520curated%2520high-quality%2520datasets%2520for%2520training%252C%2520and%2520we%2520construct%2520a%2520benchmark%252C%2520called%2520MindWatcher-Evaluate%2520Bench%2520%2528MWE-Bench%2529%252C%2520to%2520evaluate%2520its%2520performance.%2520MindWatcher%2520is%2520equipped%2520with%2520a%2520comprehensive%2520suite%2520of%2520auxiliary%2520reasoning%2520tools%252C%2520enabling%2520it%2520to%2520address%2520broad-domain%2520multimodal%2520problems.%2520A%2520large-scale%252C%2520high-quality%2520local%2520image%2520retrieval%2520database%252C%2520covering%2520eight%2520categories%2520including%2520cars%252C%2520animals%252C%2520and%2520plants%252C%2520endows%2520model%2520with%2520robust%2520object%2520recognition%2520despite%2520its%2520small%2520size.%2520Finally%252C%2520we%2520design%2520a%2520more%2520efficient%2520training%2520infrastructure%2520for%2520MindWatcher%252C%2520enhancing%2520training%2520speed%2520and%2520hardware%2520utilization.%2520Experiments%2520not%2520only%2520demonstrate%2520that%2520MindWatcher%2520matches%2520or%2520exceeds%2520the%2520performance%2520of%2520larger%2520or%2520more%2520recent%2520models%2520through%2520superior%2520tool%2520invocation%252C%2520but%2520also%2520uncover%2520critical%2520insights%2520for%2520agent%2520training%252C%2520such%2520as%2520the%2520genetic%2520inheritance%2520phenomenon%2520in%2520agentic%2520RL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning&entry.906535625=Jiawei%20Chen%20and%20Xintian%20Shen%20and%20Lihao%20Zheng%20and%20Zhenwei%20Shao%20and%20Hongyuan%20Zhang%20and%20Pengfei%20Yu%20and%20Xudong%20Rao%20and%20Ning%20Mao%20and%20Xiaobo%20Liu%20and%20Lian%20Wen%20and%20Chaoqun%20Du%20and%20Feng%20Gu%20and%20Wei%20He%20and%20Qizhen%20Li%20and%20Shanshan%20Li%20and%20Zide%20Liu%20and%20Jing%20Luo%20and%20Lifu%20Mu%20and%20Xuhao%20Pan%20and%20Chang%20Ren%20and%20Haoyi%20Sun%20and%20Qian%20Wang%20and%20Wei%20Wang%20and%20Hongfu%20Yang%20and%20Jiqing%20Zhan%20and%20Chunpeng%20Zhou%20and%20Zheng%20Zhou%20and%20Hao%20Ma%20and%20Tao%20Wei%20and%20Pan%20Zhou%20and%20Wei%20Chen&entry.1292438233=Traditional%20workflow-based%20agents%20exhibit%20limited%20intelligence%20when%20addressing%20real-world%20problems%20requiring%20tool%20invocation.%20Tool-integrated%20reasoning%20%28TIR%29%20agents%20capable%20of%20autonomous%20reasoning%20and%20tool%20invocation%20are%20rapidly%20emerging%20as%20a%20powerful%20approach%20for%20complex%20decision-making%20tasks%20involving%20multi-step%20interactions%20with%20external%20environments.%20In%20this%20work%2C%20we%20introduce%20MindWatcher%2C%20a%20TIR%20agent%20integrating%20interleaved%20thinking%20and%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning.%20MindWatcher%20can%20autonomously%20decide%20whether%20and%20how%20to%20invoke%20diverse%20tools%20and%20coordinate%20their%20use%2C%20without%20relying%20on%20human%20prompts%20or%20workflows.%20The%20interleaved%20thinking%20paradigm%20enables%20the%20model%20to%20switch%20between%20thinking%20and%20tool%20calling%20at%20any%20intermediate%20stage%2C%20while%20its%20multimodal%20CoT%20capability%20allows%20manipulation%20of%20images%20during%20reasoning%20to%20yield%20more%20precise%20search%20results.%20We%20implement%20automated%20data%20auditing%20and%20evaluation%20pipelines%2C%20complemented%20by%20manually%20curated%20high-quality%20datasets%20for%20training%2C%20and%20we%20construct%20a%20benchmark%2C%20called%20MindWatcher-Evaluate%20Bench%20%28MWE-Bench%29%2C%20to%20evaluate%20its%20performance.%20MindWatcher%20is%20equipped%20with%20a%20comprehensive%20suite%20of%20auxiliary%20reasoning%20tools%2C%20enabling%20it%20to%20address%20broad-domain%20multimodal%20problems.%20A%20large-scale%2C%20high-quality%20local%20image%20retrieval%20database%2C%20covering%20eight%20categories%20including%20cars%2C%20animals%2C%20and%20plants%2C%20endows%20model%20with%20robust%20object%20recognition%20despite%20its%20small%20size.%20Finally%2C%20we%20design%20a%20more%20efficient%20training%20infrastructure%20for%20MindWatcher%2C%20enhancing%20training%20speed%20and%20hardware%20utilization.%20Experiments%20not%20only%20demonstrate%20that%20MindWatcher%20matches%20or%20exceeds%20the%20performance%20of%20larger%20or%20more%20recent%20models%20through%20superior%20tool%20invocation%2C%20but%20also%20uncover%20critical%20insights%20for%20agent%20training%2C%20such%20as%20the%20genetic%20inheritance%20phenomenon%20in%20agentic%20RL.&entry.1838667208=http%3A//arxiv.org/abs/2512.23412v1&entry.124074799=Read"},
{"title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation", "author": "Manh Hung Nguyen and Adish Singla", "abstract": "Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.", "link": "http://arxiv.org/abs/2512.23601v1", "date": "2025-12-29", "relevancy": 2.1526, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divergent-Convergent%20Thinking%20in%20Large%20Language%20Models%20for%20Creative%20Problem%20Generation&body=Title%3A%20Divergent-Convergent%20Thinking%20in%20Large%20Language%20Models%20for%20Creative%20Problem%20Generation%0AAuthor%3A%20Manh%20Hung%20Nguyen%20and%20Adish%20Singla%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20significant%20potential%20for%20generating%20educational%20questions%20and%20problems%2C%20enabling%20educators%20to%20create%20large-scale%20learning%20materials.%20However%2C%20LLMs%20are%20fundamentally%20limited%20by%20the%20%60%60Artificial%20Hivemind%27%27%20effect%2C%20where%20they%20generate%20similar%20responses%20within%20the%20same%20model%20and%20produce%20homogeneous%20outputs%20across%20different%20models.%20As%20a%20consequence%2C%20students%20may%20be%20exposed%20to%20overly%20similar%20and%20repetitive%20LLM-generated%20problems%2C%20which%20harms%20diversity%20of%20thought.%20Drawing%20inspiration%20from%20Wallas%27s%20theory%20of%20creativity%20and%20Guilford%27s%20framework%20of%20divergent-convergent%20thinking%2C%20we%20propose%20CreativeDC%2C%20a%20two-phase%20prompting%20method%20that%20explicitly%20scaffolds%20the%20LLM%27s%20reasoning%20into%20distinct%20phases.%20By%20decoupling%20creative%20exploration%20from%20constraint%20satisfaction%2C%20our%20method%20enables%20LLMs%20to%20explore%20a%20broader%20space%20of%20ideas%20before%20committing%20to%20a%20final%20problem.%20We%20evaluate%20CreativeDC%20for%20creative%20problem%20generation%20using%20a%20comprehensive%20set%20of%20metrics%20that%20capture%20diversity%2C%20novelty%2C%20and%20utility.%20The%20results%20show%20that%20CreativeDC%20achieves%20significantly%20higher%20diversity%20and%20novelty%20compared%20to%20baselines%20while%20maintaining%20high%20utility.%20Moreover%2C%20scaling%20analysis%20shows%20that%20CreativeDC%20generates%20a%20larger%20effective%20number%20of%20distinct%20problems%20as%20more%20are%20sampled%2C%20increasing%20at%20a%20faster%20rate%20than%20baseline%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivergent-Convergent%2520Thinking%2520in%2520Large%2520Language%2520Models%2520for%2520Creative%2520Problem%2520Generation%26entry.906535625%3DManh%2520Hung%2520Nguyen%2520and%2520Adish%2520Singla%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520significant%2520potential%2520for%2520generating%2520educational%2520questions%2520and%2520problems%252C%2520enabling%2520educators%2520to%2520create%2520large-scale%2520learning%2520materials.%2520However%252C%2520LLMs%2520are%2520fundamentally%2520limited%2520by%2520the%2520%2560%2560Artificial%2520Hivemind%2527%2527%2520effect%252C%2520where%2520they%2520generate%2520similar%2520responses%2520within%2520the%2520same%2520model%2520and%2520produce%2520homogeneous%2520outputs%2520across%2520different%2520models.%2520As%2520a%2520consequence%252C%2520students%2520may%2520be%2520exposed%2520to%2520overly%2520similar%2520and%2520repetitive%2520LLM-generated%2520problems%252C%2520which%2520harms%2520diversity%2520of%2520thought.%2520Drawing%2520inspiration%2520from%2520Wallas%2527s%2520theory%2520of%2520creativity%2520and%2520Guilford%2527s%2520framework%2520of%2520divergent-convergent%2520thinking%252C%2520we%2520propose%2520CreativeDC%252C%2520a%2520two-phase%2520prompting%2520method%2520that%2520explicitly%2520scaffolds%2520the%2520LLM%2527s%2520reasoning%2520into%2520distinct%2520phases.%2520By%2520decoupling%2520creative%2520exploration%2520from%2520constraint%2520satisfaction%252C%2520our%2520method%2520enables%2520LLMs%2520to%2520explore%2520a%2520broader%2520space%2520of%2520ideas%2520before%2520committing%2520to%2520a%2520final%2520problem.%2520We%2520evaluate%2520CreativeDC%2520for%2520creative%2520problem%2520generation%2520using%2520a%2520comprehensive%2520set%2520of%2520metrics%2520that%2520capture%2520diversity%252C%2520novelty%252C%2520and%2520utility.%2520The%2520results%2520show%2520that%2520CreativeDC%2520achieves%2520significantly%2520higher%2520diversity%2520and%2520novelty%2520compared%2520to%2520baselines%2520while%2520maintaining%2520high%2520utility.%2520Moreover%252C%2520scaling%2520analysis%2520shows%2520that%2520CreativeDC%2520generates%2520a%2520larger%2520effective%2520number%2520of%2520distinct%2520problems%2520as%2520more%2520are%2520sampled%252C%2520increasing%2520at%2520a%2520faster%2520rate%2520than%2520baseline%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divergent-Convergent%20Thinking%20in%20Large%20Language%20Models%20for%20Creative%20Problem%20Generation&entry.906535625=Manh%20Hung%20Nguyen%20and%20Adish%20Singla&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20significant%20potential%20for%20generating%20educational%20questions%20and%20problems%2C%20enabling%20educators%20to%20create%20large-scale%20learning%20materials.%20However%2C%20LLMs%20are%20fundamentally%20limited%20by%20the%20%60%60Artificial%20Hivemind%27%27%20effect%2C%20where%20they%20generate%20similar%20responses%20within%20the%20same%20model%20and%20produce%20homogeneous%20outputs%20across%20different%20models.%20As%20a%20consequence%2C%20students%20may%20be%20exposed%20to%20overly%20similar%20and%20repetitive%20LLM-generated%20problems%2C%20which%20harms%20diversity%20of%20thought.%20Drawing%20inspiration%20from%20Wallas%27s%20theory%20of%20creativity%20and%20Guilford%27s%20framework%20of%20divergent-convergent%20thinking%2C%20we%20propose%20CreativeDC%2C%20a%20two-phase%20prompting%20method%20that%20explicitly%20scaffolds%20the%20LLM%27s%20reasoning%20into%20distinct%20phases.%20By%20decoupling%20creative%20exploration%20from%20constraint%20satisfaction%2C%20our%20method%20enables%20LLMs%20to%20explore%20a%20broader%20space%20of%20ideas%20before%20committing%20to%20a%20final%20problem.%20We%20evaluate%20CreativeDC%20for%20creative%20problem%20generation%20using%20a%20comprehensive%20set%20of%20metrics%20that%20capture%20diversity%2C%20novelty%2C%20and%20utility.%20The%20results%20show%20that%20CreativeDC%20achieves%20significantly%20higher%20diversity%20and%20novelty%20compared%20to%20baselines%20while%20maintaining%20high%20utility.%20Moreover%2C%20scaling%20analysis%20shows%20that%20CreativeDC%20generates%20a%20larger%20effective%20number%20of%20distinct%20problems%20as%20more%20are%20sampled%2C%20increasing%20at%20a%20faster%20rate%20than%20baseline%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.23601v1&entry.124074799=Read"},
{"title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature", "author": "Hanzheng Li and Xi Fang and Yixuan Li and Chaozheng Huang and Junjie Wang and Xi Wang and Hongzhe Bai and Bojun Hao and Shenyu Lin and Huiqi Liang and Linfeng Zhang and Guolin Ke", "abstract": "The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.", "link": "http://arxiv.org/abs/2512.23565v1", "date": "2025-12-29", "relevancy": 2.1265, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RxnBench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20on%20Chemical%20Reaction%20Understanding%20from%20Scientific%20Literature&body=Title%3A%20RxnBench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20on%20Chemical%20Reaction%20Understanding%20from%20Scientific%20Literature%0AAuthor%3A%20Hanzheng%20Li%20and%20Xi%20Fang%20and%20Yixuan%20Li%20and%20Chaozheng%20Huang%20and%20Junjie%20Wang%20and%20Xi%20Wang%20and%20Hongzhe%20Bai%20and%20Bojun%20Hao%20and%20Shenyu%20Lin%20and%20Huiqi%20Liang%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke%0AAbstract%3A%20The%20integration%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20into%20chemistry%20promises%20to%20revolutionize%20scientific%20discovery%2C%20yet%20their%20ability%20to%20comprehend%20the%20dense%2C%20graphical%20language%20of%20reactions%20within%20authentic%20literature%20remains%20underexplored.%20Here%2C%20we%20introduce%20RxnBench%2C%20a%20multi-tiered%20benchmark%20designed%20to%20rigorously%20evaluate%20MLLMs%20on%20chemical%20reaction%20understanding%20from%20scientific%20PDFs.%20RxnBench%20comprises%20two%20tasks%3A%20Single-Figure%20QA%20%28SF-QA%29%2C%20which%20tests%20fine-grained%20visual%20perception%20and%20mechanistic%20reasoning%20using%201%2C525%20questions%20derived%20from%20305%20curated%20reaction%20schemes%2C%20and%20Full-Document%20QA%20%28FD-QA%29%2C%20which%20challenges%20models%20to%20synthesize%20information%20from%20108%20articles%2C%20requiring%20cross-modal%20integration%20of%20text%2C%20schemes%2C%20and%20tables.%20Our%20evaluation%20of%20MLLMs%20reveals%20a%20critical%20capability%20gap%3A%20while%20models%20excel%20at%20extracting%20explicit%20text%2C%20they%20struggle%20with%20deep%20chemical%20logic%20and%20precise%20structural%20recognition.%20Notably%2C%20models%20with%20inference-time%20reasoning%20significantly%20outperform%20standard%20architectures%2C%20yet%20none%20achieve%2050%5C%25%20accuracy%20on%20FD-QA.%20These%20findings%20underscore%20the%20urgent%20need%20for%20domain-specific%20visual%20encoders%20and%20stronger%20reasoning%20engines%20to%20advance%20autonomous%20AI%20chemists.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRxnBench%253A%2520A%2520Multimodal%2520Benchmark%2520for%2520Evaluating%2520Large%2520Language%2520Models%2520on%2520Chemical%2520Reaction%2520Understanding%2520from%2520Scientific%2520Literature%26entry.906535625%3DHanzheng%2520Li%2520and%2520Xi%2520Fang%2520and%2520Yixuan%2520Li%2520and%2520Chaozheng%2520Huang%2520and%2520Junjie%2520Wang%2520and%2520Xi%2520Wang%2520and%2520Hongzhe%2520Bai%2520and%2520Bojun%2520Hao%2520and%2520Shenyu%2520Lin%2520and%2520Huiqi%2520Liang%2520and%2520Linfeng%2520Zhang%2520and%2520Guolin%2520Ke%26entry.1292438233%3DThe%2520integration%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520into%2520chemistry%2520promises%2520to%2520revolutionize%2520scientific%2520discovery%252C%2520yet%2520their%2520ability%2520to%2520comprehend%2520the%2520dense%252C%2520graphical%2520language%2520of%2520reactions%2520within%2520authentic%2520literature%2520remains%2520underexplored.%2520Here%252C%2520we%2520introduce%2520RxnBench%252C%2520a%2520multi-tiered%2520benchmark%2520designed%2520to%2520rigorously%2520evaluate%2520MLLMs%2520on%2520chemical%2520reaction%2520understanding%2520from%2520scientific%2520PDFs.%2520RxnBench%2520comprises%2520two%2520tasks%253A%2520Single-Figure%2520QA%2520%2528SF-QA%2529%252C%2520which%2520tests%2520fine-grained%2520visual%2520perception%2520and%2520mechanistic%2520reasoning%2520using%25201%252C525%2520questions%2520derived%2520from%2520305%2520curated%2520reaction%2520schemes%252C%2520and%2520Full-Document%2520QA%2520%2528FD-QA%2529%252C%2520which%2520challenges%2520models%2520to%2520synthesize%2520information%2520from%2520108%2520articles%252C%2520requiring%2520cross-modal%2520integration%2520of%2520text%252C%2520schemes%252C%2520and%2520tables.%2520Our%2520evaluation%2520of%2520MLLMs%2520reveals%2520a%2520critical%2520capability%2520gap%253A%2520while%2520models%2520excel%2520at%2520extracting%2520explicit%2520text%252C%2520they%2520struggle%2520with%2520deep%2520chemical%2520logic%2520and%2520precise%2520structural%2520recognition.%2520Notably%252C%2520models%2520with%2520inference-time%2520reasoning%2520significantly%2520outperform%2520standard%2520architectures%252C%2520yet%2520none%2520achieve%252050%255C%2525%2520accuracy%2520on%2520FD-QA.%2520These%2520findings%2520underscore%2520the%2520urgent%2520need%2520for%2520domain-specific%2520visual%2520encoders%2520and%2520stronger%2520reasoning%2520engines%2520to%2520advance%2520autonomous%2520AI%2520chemists.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RxnBench%3A%20A%20Multimodal%20Benchmark%20for%20Evaluating%20Large%20Language%20Models%20on%20Chemical%20Reaction%20Understanding%20from%20Scientific%20Literature&entry.906535625=Hanzheng%20Li%20and%20Xi%20Fang%20and%20Yixuan%20Li%20and%20Chaozheng%20Huang%20and%20Junjie%20Wang%20and%20Xi%20Wang%20and%20Hongzhe%20Bai%20and%20Bojun%20Hao%20and%20Shenyu%20Lin%20and%20Huiqi%20Liang%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke&entry.1292438233=The%20integration%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20into%20chemistry%20promises%20to%20revolutionize%20scientific%20discovery%2C%20yet%20their%20ability%20to%20comprehend%20the%20dense%2C%20graphical%20language%20of%20reactions%20within%20authentic%20literature%20remains%20underexplored.%20Here%2C%20we%20introduce%20RxnBench%2C%20a%20multi-tiered%20benchmark%20designed%20to%20rigorously%20evaluate%20MLLMs%20on%20chemical%20reaction%20understanding%20from%20scientific%20PDFs.%20RxnBench%20comprises%20two%20tasks%3A%20Single-Figure%20QA%20%28SF-QA%29%2C%20which%20tests%20fine-grained%20visual%20perception%20and%20mechanistic%20reasoning%20using%201%2C525%20questions%20derived%20from%20305%20curated%20reaction%20schemes%2C%20and%20Full-Document%20QA%20%28FD-QA%29%2C%20which%20challenges%20models%20to%20synthesize%20information%20from%20108%20articles%2C%20requiring%20cross-modal%20integration%20of%20text%2C%20schemes%2C%20and%20tables.%20Our%20evaluation%20of%20MLLMs%20reveals%20a%20critical%20capability%20gap%3A%20while%20models%20excel%20at%20extracting%20explicit%20text%2C%20they%20struggle%20with%20deep%20chemical%20logic%20and%20precise%20structural%20recognition.%20Notably%2C%20models%20with%20inference-time%20reasoning%20significantly%20outperform%20standard%20architectures%2C%20yet%20none%20achieve%2050%5C%25%20accuracy%20on%20FD-QA.%20These%20findings%20underscore%20the%20urgent%20need%20for%20domain-specific%20visual%20encoders%20and%20stronger%20reasoning%20engines%20to%20advance%20autonomous%20AI%20chemists.&entry.1838667208=http%3A//arxiv.org/abs/2512.23565v1&entry.124074799=Read"},
{"title": "Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing", "author": "Manuel Franco-Vivo", "abstract": "As autonomous vehicle technology advances, ensuring the safety and reliability of these systems becomes paramount. Consequently, comprehensive testing methodologies are essential to evaluate the performance of autonomous vehicles in diverse and complex real-world scenarios. This study focuses on the behaviour coverage analysis of a multi-agent system simulation designed for autonomous vehicle testing, and provides a systematic approach to measure and assess behaviour coverage within the simulation environment. By defining a set of driving scenarios, and agent interactions, we evaluate the extent to which the simulation encompasses a broad range of behaviours relevant to autonomous driving.\n  Our findings highlight the importance of behaviour coverage in validating the effectiveness and robustness of autonomous vehicle systems. Through the analysis of behaviour coverage metrics and coverage-based testing, we identify key areas for improvement and optimization in the simulation framework. Thus, a Model Predictive Control (MPC) pedestrian agent is proposed, where its objective function is formulated to encourage \\textit{interesting} tests while promoting a more realistic behaviour than other previously studied pedestrian agents. This research contributes to advancing the field of autonomous vehicle testing by providing insights into the comprehensive evaluation of system behaviour in simulated environments. The results offer valuable implications for enhancing the safety, reliability, and performance of autonomous vehicles through rigorous testing methodologies.", "link": "http://arxiv.org/abs/2512.23445v1", "date": "2025-12-29", "relevancy": 2.1264, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5777}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5418}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20behaviour%20coverage%20in%20a%20multi-agent%20system%20simulation%20for%20autonomous%20vehicle%20testing&body=Title%3A%20Assessing%20behaviour%20coverage%20in%20a%20multi-agent%20system%20simulation%20for%20autonomous%20vehicle%20testing%0AAuthor%3A%20Manuel%20Franco-Vivo%0AAbstract%3A%20As%20autonomous%20vehicle%20technology%20advances%2C%20ensuring%20the%20safety%20and%20reliability%20of%20these%20systems%20becomes%20paramount.%20Consequently%2C%20comprehensive%20testing%20methodologies%20are%20essential%20to%20evaluate%20the%20performance%20of%20autonomous%20vehicles%20in%20diverse%20and%20complex%20real-world%20scenarios.%20This%20study%20focuses%20on%20the%20behaviour%20coverage%20analysis%20of%20a%20multi-agent%20system%20simulation%20designed%20for%20autonomous%20vehicle%20testing%2C%20and%20provides%20a%20systematic%20approach%20to%20measure%20and%20assess%20behaviour%20coverage%20within%20the%20simulation%20environment.%20By%20defining%20a%20set%20of%20driving%20scenarios%2C%20and%20agent%20interactions%2C%20we%20evaluate%20the%20extent%20to%20which%20the%20simulation%20encompasses%20a%20broad%20range%20of%20behaviours%20relevant%20to%20autonomous%20driving.%0A%20%20Our%20findings%20highlight%20the%20importance%20of%20behaviour%20coverage%20in%20validating%20the%20effectiveness%20and%20robustness%20of%20autonomous%20vehicle%20systems.%20Through%20the%20analysis%20of%20behaviour%20coverage%20metrics%20and%20coverage-based%20testing%2C%20we%20identify%20key%20areas%20for%20improvement%20and%20optimization%20in%20the%20simulation%20framework.%20Thus%2C%20a%20Model%20Predictive%20Control%20%28MPC%29%20pedestrian%20agent%20is%20proposed%2C%20where%20its%20objective%20function%20is%20formulated%20to%20encourage%20%5Ctextit%7Binteresting%7D%20tests%20while%20promoting%20a%20more%20realistic%20behaviour%20than%20other%20previously%20studied%20pedestrian%20agents.%20This%20research%20contributes%20to%20advancing%20the%20field%20of%20autonomous%20vehicle%20testing%20by%20providing%20insights%20into%20the%20comprehensive%20evaluation%20of%20system%20behaviour%20in%20simulated%20environments.%20The%20results%20offer%20valuable%20implications%20for%20enhancing%20the%20safety%2C%20reliability%2C%20and%20performance%20of%20autonomous%20vehicles%20through%20rigorous%20testing%20methodologies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520behaviour%2520coverage%2520in%2520a%2520multi-agent%2520system%2520simulation%2520for%2520autonomous%2520vehicle%2520testing%26entry.906535625%3DManuel%2520Franco-Vivo%26entry.1292438233%3DAs%2520autonomous%2520vehicle%2520technology%2520advances%252C%2520ensuring%2520the%2520safety%2520and%2520reliability%2520of%2520these%2520systems%2520becomes%2520paramount.%2520Consequently%252C%2520comprehensive%2520testing%2520methodologies%2520are%2520essential%2520to%2520evaluate%2520the%2520performance%2520of%2520autonomous%2520vehicles%2520in%2520diverse%2520and%2520complex%2520real-world%2520scenarios.%2520This%2520study%2520focuses%2520on%2520the%2520behaviour%2520coverage%2520analysis%2520of%2520a%2520multi-agent%2520system%2520simulation%2520designed%2520for%2520autonomous%2520vehicle%2520testing%252C%2520and%2520provides%2520a%2520systematic%2520approach%2520to%2520measure%2520and%2520assess%2520behaviour%2520coverage%2520within%2520the%2520simulation%2520environment.%2520By%2520defining%2520a%2520set%2520of%2520driving%2520scenarios%252C%2520and%2520agent%2520interactions%252C%2520we%2520evaluate%2520the%2520extent%2520to%2520which%2520the%2520simulation%2520encompasses%2520a%2520broad%2520range%2520of%2520behaviours%2520relevant%2520to%2520autonomous%2520driving.%250A%2520%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520behaviour%2520coverage%2520in%2520validating%2520the%2520effectiveness%2520and%2520robustness%2520of%2520autonomous%2520vehicle%2520systems.%2520Through%2520the%2520analysis%2520of%2520behaviour%2520coverage%2520metrics%2520and%2520coverage-based%2520testing%252C%2520we%2520identify%2520key%2520areas%2520for%2520improvement%2520and%2520optimization%2520in%2520the%2520simulation%2520framework.%2520Thus%252C%2520a%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520pedestrian%2520agent%2520is%2520proposed%252C%2520where%2520its%2520objective%2520function%2520is%2520formulated%2520to%2520encourage%2520%255Ctextit%257Binteresting%257D%2520tests%2520while%2520promoting%2520a%2520more%2520realistic%2520behaviour%2520than%2520other%2520previously%2520studied%2520pedestrian%2520agents.%2520This%2520research%2520contributes%2520to%2520advancing%2520the%2520field%2520of%2520autonomous%2520vehicle%2520testing%2520by%2520providing%2520insights%2520into%2520the%2520comprehensive%2520evaluation%2520of%2520system%2520behaviour%2520in%2520simulated%2520environments.%2520The%2520results%2520offer%2520valuable%2520implications%2520for%2520enhancing%2520the%2520safety%252C%2520reliability%252C%2520and%2520performance%2520of%2520autonomous%2520vehicles%2520through%2520rigorous%2520testing%2520methodologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20behaviour%20coverage%20in%20a%20multi-agent%20system%20simulation%20for%20autonomous%20vehicle%20testing&entry.906535625=Manuel%20Franco-Vivo&entry.1292438233=As%20autonomous%20vehicle%20technology%20advances%2C%20ensuring%20the%20safety%20and%20reliability%20of%20these%20systems%20becomes%20paramount.%20Consequently%2C%20comprehensive%20testing%20methodologies%20are%20essential%20to%20evaluate%20the%20performance%20of%20autonomous%20vehicles%20in%20diverse%20and%20complex%20real-world%20scenarios.%20This%20study%20focuses%20on%20the%20behaviour%20coverage%20analysis%20of%20a%20multi-agent%20system%20simulation%20designed%20for%20autonomous%20vehicle%20testing%2C%20and%20provides%20a%20systematic%20approach%20to%20measure%20and%20assess%20behaviour%20coverage%20within%20the%20simulation%20environment.%20By%20defining%20a%20set%20of%20driving%20scenarios%2C%20and%20agent%20interactions%2C%20we%20evaluate%20the%20extent%20to%20which%20the%20simulation%20encompasses%20a%20broad%20range%20of%20behaviours%20relevant%20to%20autonomous%20driving.%0A%20%20Our%20findings%20highlight%20the%20importance%20of%20behaviour%20coverage%20in%20validating%20the%20effectiveness%20and%20robustness%20of%20autonomous%20vehicle%20systems.%20Through%20the%20analysis%20of%20behaviour%20coverage%20metrics%20and%20coverage-based%20testing%2C%20we%20identify%20key%20areas%20for%20improvement%20and%20optimization%20in%20the%20simulation%20framework.%20Thus%2C%20a%20Model%20Predictive%20Control%20%28MPC%29%20pedestrian%20agent%20is%20proposed%2C%20where%20its%20objective%20function%20is%20formulated%20to%20encourage%20%5Ctextit%7Binteresting%7D%20tests%20while%20promoting%20a%20more%20realistic%20behaviour%20than%20other%20previously%20studied%20pedestrian%20agents.%20This%20research%20contributes%20to%20advancing%20the%20field%20of%20autonomous%20vehicle%20testing%20by%20providing%20insights%20into%20the%20comprehensive%20evaluation%20of%20system%20behaviour%20in%20simulated%20environments.%20The%20results%20offer%20valuable%20implications%20for%20enhancing%20the%20safety%2C%20reliability%2C%20and%20performance%20of%20autonomous%20vehicles%20through%20rigorous%20testing%20methodologies.&entry.1838667208=http%3A//arxiv.org/abs/2512.23445v1&entry.124074799=Read"},
{"title": "Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems", "author": "Quercus Hernandez and Max Win and Thomas C. O'Connor and Paulo E. Arratia and Nathaniel Trask", "abstract": "Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.", "link": "http://arxiv.org/abs/2508.12569v3", "date": "2025-12-29", "relevancy": 2.1225, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5406}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5332}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20particle%20dynamics%3A%20Structure-preserving%20coarse-graining%20for%20emergent%20behavior%20in%20non-equilibrium%20systems&body=Title%3A%20Data-driven%20particle%20dynamics%3A%20Structure-preserving%20coarse-graining%20for%20emergent%20behavior%20in%20non-equilibrium%20systems%0AAuthor%3A%20Quercus%20Hernandez%20and%20Max%20Win%20and%20Thomas%20C.%20O%27Connor%20and%20Paulo%20E.%20Arratia%20and%20Nathaniel%20Trask%0AAbstract%3A%20Multiscale%20systems%20are%20ubiquitous%20in%20science%20and%20technology%2C%20but%20are%20notoriously%20challenging%20to%20simulate%20as%20short%20spatiotemporal%20scales%20must%20be%20appropriately%20linked%20to%20emergent%20bulk%20physics.%20When%20expensive%20high-dimensional%20dynamical%20systems%20are%20coarse-grained%20into%20low-dimensional%20models%2C%20the%20entropic%20loss%20of%20information%20leads%20to%20emergent%20physics%20which%20are%20dissipative%2C%20history-dependent%2C%20and%20stochastic.%20To%20machine%20learn%20coarse-grained%20dynamics%20from%20time-series%20observations%20of%20particle%20trajectories%2C%20we%20propose%20a%20framework%20using%20the%20metriplectic%20bracket%20formalism%20that%20preserves%20these%20properties%20by%20construction%3B%20most%20notably%2C%20the%20framework%20guarantees%20discrete%20notions%20of%20the%20first%20and%20second%20laws%20of%20thermodynamics%2C%20conservation%20of%20momentum%2C%20and%20a%20discrete%20fluctuation-dissipation%20balance%20crucial%20for%20capturing%20non-equilibrium%20statistics.%20We%20introduce%20the%20mathematical%20framework%20abstractly%20before%20specializing%20to%20a%20particle%20discretization.%20As%20labels%20are%20generally%20unavailable%20for%20entropic%20state%20variables%2C%20we%20introduce%20a%20novel%20self-supervised%20learning%20strategy%20to%20identify%20emergent%20structural%20variables.%20We%20validate%20the%20method%20on%20benchmark%20systems%20and%20demonstrate%20its%20utility%20on%20two%20challenging%20examples%3A%20%281%29%20coarse-graining%20star%20polymers%20at%20challenging%20levels%20of%20coarse-graining%20while%20preserving%20non-equilibrium%20statistics%2C%20and%20%282%29%20learning%20models%20from%20high-speed%20video%20of%20colloidal%20suspensions%20that%20capture%20coupling%20between%20local%20rearrangement%20events%20and%20emergent%20stochastic%20dynamics.%20We%20provide%20open-source%20implementations%20in%20both%20PyTorch%20and%20LAMMPS%2C%20enabling%20large-scale%20inference%20and%20extensibility%20to%20diverse%20particle-based%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2508.12569v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520particle%2520dynamics%253A%2520Structure-preserving%2520coarse-graining%2520for%2520emergent%2520behavior%2520in%2520non-equilibrium%2520systems%26entry.906535625%3DQuercus%2520Hernandez%2520and%2520Max%2520Win%2520and%2520Thomas%2520C.%2520O%2527Connor%2520and%2520Paulo%2520E.%2520Arratia%2520and%2520Nathaniel%2520Trask%26entry.1292438233%3DMultiscale%2520systems%2520are%2520ubiquitous%2520in%2520science%2520and%2520technology%252C%2520but%2520are%2520notoriously%2520challenging%2520to%2520simulate%2520as%2520short%2520spatiotemporal%2520scales%2520must%2520be%2520appropriately%2520linked%2520to%2520emergent%2520bulk%2520physics.%2520When%2520expensive%2520high-dimensional%2520dynamical%2520systems%2520are%2520coarse-grained%2520into%2520low-dimensional%2520models%252C%2520the%2520entropic%2520loss%2520of%2520information%2520leads%2520to%2520emergent%2520physics%2520which%2520are%2520dissipative%252C%2520history-dependent%252C%2520and%2520stochastic.%2520To%2520machine%2520learn%2520coarse-grained%2520dynamics%2520from%2520time-series%2520observations%2520of%2520particle%2520trajectories%252C%2520we%2520propose%2520a%2520framework%2520using%2520the%2520metriplectic%2520bracket%2520formalism%2520that%2520preserves%2520these%2520properties%2520by%2520construction%253B%2520most%2520notably%252C%2520the%2520framework%2520guarantees%2520discrete%2520notions%2520of%2520the%2520first%2520and%2520second%2520laws%2520of%2520thermodynamics%252C%2520conservation%2520of%2520momentum%252C%2520and%2520a%2520discrete%2520fluctuation-dissipation%2520balance%2520crucial%2520for%2520capturing%2520non-equilibrium%2520statistics.%2520We%2520introduce%2520the%2520mathematical%2520framework%2520abstractly%2520before%2520specializing%2520to%2520a%2520particle%2520discretization.%2520As%2520labels%2520are%2520generally%2520unavailable%2520for%2520entropic%2520state%2520variables%252C%2520we%2520introduce%2520a%2520novel%2520self-supervised%2520learning%2520strategy%2520to%2520identify%2520emergent%2520structural%2520variables.%2520We%2520validate%2520the%2520method%2520on%2520benchmark%2520systems%2520and%2520demonstrate%2520its%2520utility%2520on%2520two%2520challenging%2520examples%253A%2520%25281%2529%2520coarse-graining%2520star%2520polymers%2520at%2520challenging%2520levels%2520of%2520coarse-graining%2520while%2520preserving%2520non-equilibrium%2520statistics%252C%2520and%2520%25282%2529%2520learning%2520models%2520from%2520high-speed%2520video%2520of%2520colloidal%2520suspensions%2520that%2520capture%2520coupling%2520between%2520local%2520rearrangement%2520events%2520and%2520emergent%2520stochastic%2520dynamics.%2520We%2520provide%2520open-source%2520implementations%2520in%2520both%2520PyTorch%2520and%2520LAMMPS%252C%2520enabling%2520large-scale%2520inference%2520and%2520extensibility%2520to%2520diverse%2520particle-based%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12569v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20particle%20dynamics%3A%20Structure-preserving%20coarse-graining%20for%20emergent%20behavior%20in%20non-equilibrium%20systems&entry.906535625=Quercus%20Hernandez%20and%20Max%20Win%20and%20Thomas%20C.%20O%27Connor%20and%20Paulo%20E.%20Arratia%20and%20Nathaniel%20Trask&entry.1292438233=Multiscale%20systems%20are%20ubiquitous%20in%20science%20and%20technology%2C%20but%20are%20notoriously%20challenging%20to%20simulate%20as%20short%20spatiotemporal%20scales%20must%20be%20appropriately%20linked%20to%20emergent%20bulk%20physics.%20When%20expensive%20high-dimensional%20dynamical%20systems%20are%20coarse-grained%20into%20low-dimensional%20models%2C%20the%20entropic%20loss%20of%20information%20leads%20to%20emergent%20physics%20which%20are%20dissipative%2C%20history-dependent%2C%20and%20stochastic.%20To%20machine%20learn%20coarse-grained%20dynamics%20from%20time-series%20observations%20of%20particle%20trajectories%2C%20we%20propose%20a%20framework%20using%20the%20metriplectic%20bracket%20formalism%20that%20preserves%20these%20properties%20by%20construction%3B%20most%20notably%2C%20the%20framework%20guarantees%20discrete%20notions%20of%20the%20first%20and%20second%20laws%20of%20thermodynamics%2C%20conservation%20of%20momentum%2C%20and%20a%20discrete%20fluctuation-dissipation%20balance%20crucial%20for%20capturing%20non-equilibrium%20statistics.%20We%20introduce%20the%20mathematical%20framework%20abstractly%20before%20specializing%20to%20a%20particle%20discretization.%20As%20labels%20are%20generally%20unavailable%20for%20entropic%20state%20variables%2C%20we%20introduce%20a%20novel%20self-supervised%20learning%20strategy%20to%20identify%20emergent%20structural%20variables.%20We%20validate%20the%20method%20on%20benchmark%20systems%20and%20demonstrate%20its%20utility%20on%20two%20challenging%20examples%3A%20%281%29%20coarse-graining%20star%20polymers%20at%20challenging%20levels%20of%20coarse-graining%20while%20preserving%20non-equilibrium%20statistics%2C%20and%20%282%29%20learning%20models%20from%20high-speed%20video%20of%20colloidal%20suspensions%20that%20capture%20coupling%20between%20local%20rearrangement%20events%20and%20emergent%20stochastic%20dynamics.%20We%20provide%20open-source%20implementations%20in%20both%20PyTorch%20and%20LAMMPS%2C%20enabling%20large-scale%20inference%20and%20extensibility%20to%20diverse%20particle-based%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2508.12569v3&entry.124074799=Read"},
{"title": "Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs", "author": "Daniel Furelos-Blanco and Charles Pert and Frederik Kelbel and Alex F. Spies and Alessandra Russo and Michael Dennis", "abstract": "Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.", "link": "http://arxiv.org/abs/2511.12706v2", "date": "2025-12-29", "relevancy": 2.1221, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5593}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.532}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Fixed%20Tasks%3A%20Unsupervised%20Environment%20Design%20for%20Task-Level%20Pairs&body=Title%3A%20Beyond%20Fixed%20Tasks%3A%20Unsupervised%20Environment%20Design%20for%20Task-Level%20Pairs%0AAuthor%3A%20Daniel%20Furelos-Blanco%20and%20Charles%20Pert%20and%20Frederik%20Kelbel%20and%20Alex%20F.%20Spies%20and%20Alessandra%20Russo%20and%20Michael%20Dennis%0AAbstract%3A%20Training%20general%20agents%20to%20follow%20complex%20instructions%20%28tasks%29%20in%20intricate%20environments%20%28levels%29%20remains%20a%20core%20challenge%20in%20reinforcement%20learning.%20Random%20sampling%20of%20task-level%20pairs%20often%20produces%20unsolvable%20combinations%2C%20highlighting%20the%20need%20to%20co-design%20tasks%20and%20levels.%20While%20unsupervised%20environment%20design%20%28UED%29%20has%20proven%20effective%20at%20automatically%20designing%20level%20curricula%2C%20prior%20work%20has%20only%20considered%20a%20fixed%20task.%20We%20present%20ATLAS%20%28Aligning%20Tasks%20and%20Levels%20for%20Autocurricula%20of%20Specifications%29%2C%20a%20novel%20method%20that%20generates%20joint%20autocurricula%20over%20tasks%20and%20levels.%20Our%20approach%20builds%20upon%20UED%20to%20automatically%20produce%20solvable%20yet%20challenging%20task-level%20pairs%20for%20policy%20training.%20To%20evaluate%20ATLAS%20and%20drive%20progress%20in%20the%20field%2C%20we%20introduce%20an%20evaluation%20suite%20that%20models%20tasks%20as%20reward%20machines%20in%20Minigrid%20levels.%20Experiments%20demonstrate%20that%20ATLAS%20vastly%20outperforms%20random%20sampling%20approaches%2C%20particularly%20when%20sampling%20solvable%20pairs%20is%20unlikely.%20We%20further%20show%20that%20mutations%20leveraging%20the%20structure%20of%20both%20tasks%20and%20levels%20accelerate%20convergence%20to%20performant%20policies.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Fixed%2520Tasks%253A%2520Unsupervised%2520Environment%2520Design%2520for%2520Task-Level%2520Pairs%26entry.906535625%3DDaniel%2520Furelos-Blanco%2520and%2520Charles%2520Pert%2520and%2520Frederik%2520Kelbel%2520and%2520Alex%2520F.%2520Spies%2520and%2520Alessandra%2520Russo%2520and%2520Michael%2520Dennis%26entry.1292438233%3DTraining%2520general%2520agents%2520to%2520follow%2520complex%2520instructions%2520%2528tasks%2529%2520in%2520intricate%2520environments%2520%2528levels%2529%2520remains%2520a%2520core%2520challenge%2520in%2520reinforcement%2520learning.%2520Random%2520sampling%2520of%2520task-level%2520pairs%2520often%2520produces%2520unsolvable%2520combinations%252C%2520highlighting%2520the%2520need%2520to%2520co-design%2520tasks%2520and%2520levels.%2520While%2520unsupervised%2520environment%2520design%2520%2528UED%2529%2520has%2520proven%2520effective%2520at%2520automatically%2520designing%2520level%2520curricula%252C%2520prior%2520work%2520has%2520only%2520considered%2520a%2520fixed%2520task.%2520We%2520present%2520ATLAS%2520%2528Aligning%2520Tasks%2520and%2520Levels%2520for%2520Autocurricula%2520of%2520Specifications%2529%252C%2520a%2520novel%2520method%2520that%2520generates%2520joint%2520autocurricula%2520over%2520tasks%2520and%2520levels.%2520Our%2520approach%2520builds%2520upon%2520UED%2520to%2520automatically%2520produce%2520solvable%2520yet%2520challenging%2520task-level%2520pairs%2520for%2520policy%2520training.%2520To%2520evaluate%2520ATLAS%2520and%2520drive%2520progress%2520in%2520the%2520field%252C%2520we%2520introduce%2520an%2520evaluation%2520suite%2520that%2520models%2520tasks%2520as%2520reward%2520machines%2520in%2520Minigrid%2520levels.%2520Experiments%2520demonstrate%2520that%2520ATLAS%2520vastly%2520outperforms%2520random%2520sampling%2520approaches%252C%2520particularly%2520when%2520sampling%2520solvable%2520pairs%2520is%2520unlikely.%2520We%2520further%2520show%2520that%2520mutations%2520leveraging%2520the%2520structure%2520of%2520both%2520tasks%2520and%2520levels%2520accelerate%2520convergence%2520to%2520performant%2520policies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Fixed%20Tasks%3A%20Unsupervised%20Environment%20Design%20for%20Task-Level%20Pairs&entry.906535625=Daniel%20Furelos-Blanco%20and%20Charles%20Pert%20and%20Frederik%20Kelbel%20and%20Alex%20F.%20Spies%20and%20Alessandra%20Russo%20and%20Michael%20Dennis&entry.1292438233=Training%20general%20agents%20to%20follow%20complex%20instructions%20%28tasks%29%20in%20intricate%20environments%20%28levels%29%20remains%20a%20core%20challenge%20in%20reinforcement%20learning.%20Random%20sampling%20of%20task-level%20pairs%20often%20produces%20unsolvable%20combinations%2C%20highlighting%20the%20need%20to%20co-design%20tasks%20and%20levels.%20While%20unsupervised%20environment%20design%20%28UED%29%20has%20proven%20effective%20at%20automatically%20designing%20level%20curricula%2C%20prior%20work%20has%20only%20considered%20a%20fixed%20task.%20We%20present%20ATLAS%20%28Aligning%20Tasks%20and%20Levels%20for%20Autocurricula%20of%20Specifications%29%2C%20a%20novel%20method%20that%20generates%20joint%20autocurricula%20over%20tasks%20and%20levels.%20Our%20approach%20builds%20upon%20UED%20to%20automatically%20produce%20solvable%20yet%20challenging%20task-level%20pairs%20for%20policy%20training.%20To%20evaluate%20ATLAS%20and%20drive%20progress%20in%20the%20field%2C%20we%20introduce%20an%20evaluation%20suite%20that%20models%20tasks%20as%20reward%20machines%20in%20Minigrid%20levels.%20Experiments%20demonstrate%20that%20ATLAS%20vastly%20outperforms%20random%20sampling%20approaches%2C%20particularly%20when%20sampling%20solvable%20pairs%20is%20unlikely.%20We%20further%20show%20that%20mutations%20leveraging%20the%20structure%20of%20both%20tasks%20and%20levels%20accelerate%20convergence%20to%20performant%20policies.&entry.1838667208=http%3A//arxiv.org/abs/2511.12706v2&entry.124074799=Read"},
{"title": "Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection", "author": "Nico Baumgart and Markus Lange-Hegermann and Mike M\u00fccke", "abstract": "In industrial manufacturing, deploying deep learning models for visual inspection is mostly hindered by the high and often intractable cost of collecting and annotating large-scale training datasets. While image synthesis from 3D CAD models is a common solution, the individual techniques of domain and rendering randomization to create rich synthetic training datasets have been well studied mainly in simple domains. Hence, their effectiveness on complex industrial tasks with densely arranged and similar objects remains unclear. In this paper, we investigate the sim-to-real generalization performance of standard object detectors on the complex industrial application of terminal strip object detection, carefully combining randomization and domain knowledge. We describe step-by-step the creation of our image synthesis pipeline that achieves high realism with minimal implementation effort and explain how this approach could be transferred to other industrial settings. Moreover, we created a dataset comprising 30.000 synthetic images and 300 manually annotated real images of terminal strips, which is publicly available for reference and future research. To provide a baseline as a lower bound of the expectable performance in these challenging industrial parts detection tasks, we show the sim-to-real generalization performance of standard object detectors on our dataset based on a fully synthetic training. While all considered models behave similarly, the transformer-based DINO model achieves the best score with 98.40 % mean average precision on the real test set, demonstrating that our pipeline enables high quality detections in complex industrial environments from existing CAD data and with a manageable image synthesis effort.", "link": "http://arxiv.org/abs/2403.04809v2", "date": "2025-12-29", "relevancy": 2.1192, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5448}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5288}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigation%20of%20the%20Impact%20of%20Synthetic%20Training%20Data%20in%20the%20Industrial%20Application%20of%20Terminal%20Strip%20Object%20Detection&body=Title%3A%20Investigation%20of%20the%20Impact%20of%20Synthetic%20Training%20Data%20in%20the%20Industrial%20Application%20of%20Terminal%20Strip%20Object%20Detection%0AAuthor%3A%20Nico%20Baumgart%20and%20Markus%20Lange-Hegermann%20and%20Mike%20M%C3%BCcke%0AAbstract%3A%20In%20industrial%20manufacturing%2C%20deploying%20deep%20learning%20models%20for%20visual%20inspection%20is%20mostly%20hindered%20by%20the%20high%20and%20often%20intractable%20cost%20of%20collecting%20and%20annotating%20large-scale%20training%20datasets.%20While%20image%20synthesis%20from%203D%20CAD%20models%20is%20a%20common%20solution%2C%20the%20individual%20techniques%20of%20domain%20and%20rendering%20randomization%20to%20create%20rich%20synthetic%20training%20datasets%20have%20been%20well%20studied%20mainly%20in%20simple%20domains.%20Hence%2C%20their%20effectiveness%20on%20complex%20industrial%20tasks%20with%20densely%20arranged%20and%20similar%20objects%20remains%20unclear.%20In%20this%20paper%2C%20we%20investigate%20the%20sim-to-real%20generalization%20performance%20of%20standard%20object%20detectors%20on%20the%20complex%20industrial%20application%20of%20terminal%20strip%20object%20detection%2C%20carefully%20combining%20randomization%20and%20domain%20knowledge.%20We%20describe%20step-by-step%20the%20creation%20of%20our%20image%20synthesis%20pipeline%20that%20achieves%20high%20realism%20with%20minimal%20implementation%20effort%20and%20explain%20how%20this%20approach%20could%20be%20transferred%20to%20other%20industrial%20settings.%20Moreover%2C%20we%20created%20a%20dataset%20comprising%2030.000%20synthetic%20images%20and%20300%20manually%20annotated%20real%20images%20of%20terminal%20strips%2C%20which%20is%20publicly%20available%20for%20reference%20and%20future%20research.%20To%20provide%20a%20baseline%20as%20a%20lower%20bound%20of%20the%20expectable%20performance%20in%20these%20challenging%20industrial%20parts%20detection%20tasks%2C%20we%20show%20the%20sim-to-real%20generalization%20performance%20of%20standard%20object%20detectors%20on%20our%20dataset%20based%20on%20a%20fully%20synthetic%20training.%20While%20all%20considered%20models%20behave%20similarly%2C%20the%20transformer-based%20DINO%20model%20achieves%20the%20best%20score%20with%2098.40%20%25%20mean%20average%20precision%20on%20the%20real%20test%20set%2C%20demonstrating%20that%20our%20pipeline%20enables%20high%20quality%20detections%20in%20complex%20industrial%20environments%20from%20existing%20CAD%20data%20and%20with%20a%20manageable%20image%20synthesis%20effort.%0ALink%3A%20http%3A//arxiv.org/abs/2403.04809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigation%2520of%2520the%2520Impact%2520of%2520Synthetic%2520Training%2520Data%2520in%2520the%2520Industrial%2520Application%2520of%2520Terminal%2520Strip%2520Object%2520Detection%26entry.906535625%3DNico%2520Baumgart%2520and%2520Markus%2520Lange-Hegermann%2520and%2520Mike%2520M%25C3%25BCcke%26entry.1292438233%3DIn%2520industrial%2520manufacturing%252C%2520deploying%2520deep%2520learning%2520models%2520for%2520visual%2520inspection%2520is%2520mostly%2520hindered%2520by%2520the%2520high%2520and%2520often%2520intractable%2520cost%2520of%2520collecting%2520and%2520annotating%2520large-scale%2520training%2520datasets.%2520While%2520image%2520synthesis%2520from%25203D%2520CAD%2520models%2520is%2520a%2520common%2520solution%252C%2520the%2520individual%2520techniques%2520of%2520domain%2520and%2520rendering%2520randomization%2520to%2520create%2520rich%2520synthetic%2520training%2520datasets%2520have%2520been%2520well%2520studied%2520mainly%2520in%2520simple%2520domains.%2520Hence%252C%2520their%2520effectiveness%2520on%2520complex%2520industrial%2520tasks%2520with%2520densely%2520arranged%2520and%2520similar%2520objects%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520sim-to-real%2520generalization%2520performance%2520of%2520standard%2520object%2520detectors%2520on%2520the%2520complex%2520industrial%2520application%2520of%2520terminal%2520strip%2520object%2520detection%252C%2520carefully%2520combining%2520randomization%2520and%2520domain%2520knowledge.%2520We%2520describe%2520step-by-step%2520the%2520creation%2520of%2520our%2520image%2520synthesis%2520pipeline%2520that%2520achieves%2520high%2520realism%2520with%2520minimal%2520implementation%2520effort%2520and%2520explain%2520how%2520this%2520approach%2520could%2520be%2520transferred%2520to%2520other%2520industrial%2520settings.%2520Moreover%252C%2520we%2520created%2520a%2520dataset%2520comprising%252030.000%2520synthetic%2520images%2520and%2520300%2520manually%2520annotated%2520real%2520images%2520of%2520terminal%2520strips%252C%2520which%2520is%2520publicly%2520available%2520for%2520reference%2520and%2520future%2520research.%2520To%2520provide%2520a%2520baseline%2520as%2520a%2520lower%2520bound%2520of%2520the%2520expectable%2520performance%2520in%2520these%2520challenging%2520industrial%2520parts%2520detection%2520tasks%252C%2520we%2520show%2520the%2520sim-to-real%2520generalization%2520performance%2520of%2520standard%2520object%2520detectors%2520on%2520our%2520dataset%2520based%2520on%2520a%2520fully%2520synthetic%2520training.%2520While%2520all%2520considered%2520models%2520behave%2520similarly%252C%2520the%2520transformer-based%2520DINO%2520model%2520achieves%2520the%2520best%2520score%2520with%252098.40%2520%2525%2520mean%2520average%2520precision%2520on%2520the%2520real%2520test%2520set%252C%2520demonstrating%2520that%2520our%2520pipeline%2520enables%2520high%2520quality%2520detections%2520in%2520complex%2520industrial%2520environments%2520from%2520existing%2520CAD%2520data%2520and%2520with%2520a%2520manageable%2520image%2520synthesis%2520effort.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20of%20the%20Impact%20of%20Synthetic%20Training%20Data%20in%20the%20Industrial%20Application%20of%20Terminal%20Strip%20Object%20Detection&entry.906535625=Nico%20Baumgart%20and%20Markus%20Lange-Hegermann%20and%20Mike%20M%C3%BCcke&entry.1292438233=In%20industrial%20manufacturing%2C%20deploying%20deep%20learning%20models%20for%20visual%20inspection%20is%20mostly%20hindered%20by%20the%20high%20and%20often%20intractable%20cost%20of%20collecting%20and%20annotating%20large-scale%20training%20datasets.%20While%20image%20synthesis%20from%203D%20CAD%20models%20is%20a%20common%20solution%2C%20the%20individual%20techniques%20of%20domain%20and%20rendering%20randomization%20to%20create%20rich%20synthetic%20training%20datasets%20have%20been%20well%20studied%20mainly%20in%20simple%20domains.%20Hence%2C%20their%20effectiveness%20on%20complex%20industrial%20tasks%20with%20densely%20arranged%20and%20similar%20objects%20remains%20unclear.%20In%20this%20paper%2C%20we%20investigate%20the%20sim-to-real%20generalization%20performance%20of%20standard%20object%20detectors%20on%20the%20complex%20industrial%20application%20of%20terminal%20strip%20object%20detection%2C%20carefully%20combining%20randomization%20and%20domain%20knowledge.%20We%20describe%20step-by-step%20the%20creation%20of%20our%20image%20synthesis%20pipeline%20that%20achieves%20high%20realism%20with%20minimal%20implementation%20effort%20and%20explain%20how%20this%20approach%20could%20be%20transferred%20to%20other%20industrial%20settings.%20Moreover%2C%20we%20created%20a%20dataset%20comprising%2030.000%20synthetic%20images%20and%20300%20manually%20annotated%20real%20images%20of%20terminal%20strips%2C%20which%20is%20publicly%20available%20for%20reference%20and%20future%20research.%20To%20provide%20a%20baseline%20as%20a%20lower%20bound%20of%20the%20expectable%20performance%20in%20these%20challenging%20industrial%20parts%20detection%20tasks%2C%20we%20show%20the%20sim-to-real%20generalization%20performance%20of%20standard%20object%20detectors%20on%20our%20dataset%20based%20on%20a%20fully%20synthetic%20training.%20While%20all%20considered%20models%20behave%20similarly%2C%20the%20transformer-based%20DINO%20model%20achieves%20the%20best%20score%20with%2098.40%20%25%20mean%20average%20precision%20on%20the%20real%20test%20set%2C%20demonstrating%20that%20our%20pipeline%20enables%20high%20quality%20detections%20in%20complex%20industrial%20environments%20from%20existing%20CAD%20data%20and%20with%20a%20manageable%20image%20synthesis%20effort.&entry.1838667208=http%3A//arxiv.org/abs/2403.04809v2&entry.124074799=Read"},
{"title": "Image Denoising Using Global and Local Circulant Representation", "author": "Zhaoming Kong and Xiaowei Yang and Jiahuan Zhang", "abstract": "The proliferation of imaging devices and countless image data generated every day impose an increasingly high demand on efficient and effective image denoising. In this paper, we establish a theoretical connection between principal component analysis (PCA) and the Haar transform under circulant representation, and present a computationally simple denoising algorithm. The proposed method, termed Haar-tSVD, exploits a unified tensor singular value decomposition (t-SVD) projection combined with Haar transform to efficiently capture global and local patch correlations. Haar-tSVD operates as a one-step, parallelizable plug-and-play denoiser that eliminates the need for learning local bases, thereby striking a balance between denoising speed and performance. Besides, an adaptive noise estimation scheme is introduced to improve robustness according to eigenvalue analysis of the circulant structure. To further enhance the performance under severe noise conditions, we integrate deep neural networks with Haar-tSVD based on the established Haar-PCA relationship. Experimental results on various denoising datasets demonstrate the efficiency and effectiveness of proposed method for noise removal. Our code is publicly available at https://github.com/ZhaomingKong/Haar-tSVD.", "link": "http://arxiv.org/abs/2512.23569v1", "date": "2025-12-29", "relevancy": 2.119, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5713}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation&body=Title%3A%20Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation%0AAuthor%3A%20Zhaoming%20Kong%20and%20Xiaowei%20Yang%20and%20Jiahuan%20Zhang%0AAbstract%3A%20The%20proliferation%20of%20imaging%20devices%20and%20countless%20image%20data%20generated%20every%20day%20impose%20an%20increasingly%20high%20demand%20on%20efficient%20and%20effective%20image%20denoising.%20In%20this%20paper%2C%20we%20establish%20a%20theoretical%20connection%20between%20principal%20component%20analysis%20%28PCA%29%20and%20the%20Haar%20transform%20under%20circulant%20representation%2C%20and%20present%20a%20computationally%20simple%20denoising%20algorithm.%20The%20proposed%20method%2C%20termed%20Haar-tSVD%2C%20exploits%20a%20unified%20tensor%20singular%20value%20decomposition%20%28t-SVD%29%20projection%20combined%20with%20Haar%20transform%20to%20efficiently%20capture%20global%20and%20local%20patch%20correlations.%20Haar-tSVD%20operates%20as%20a%20one-step%2C%20parallelizable%20plug-and-play%20denoiser%20that%20eliminates%20the%20need%20for%20learning%20local%20bases%2C%20thereby%20striking%20a%20balance%20between%20denoising%20speed%20and%20performance.%20Besides%2C%20an%20adaptive%20noise%20estimation%20scheme%20is%20introduced%20to%20improve%20robustness%20according%20to%20eigenvalue%20analysis%20of%20the%20circulant%20structure.%20To%20further%20enhance%20the%20performance%20under%20severe%20noise%20conditions%2C%20we%20integrate%20deep%20neural%20networks%20with%20Haar-tSVD%20based%20on%20the%20established%20Haar-PCA%20relationship.%20Experimental%20results%20on%20various%20denoising%20datasets%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%20proposed%20method%20for%20noise%20removal.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/ZhaomingKong/Haar-tSVD.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Denoising%2520Using%2520Global%2520and%2520Local%2520Circulant%2520Representation%26entry.906535625%3DZhaoming%2520Kong%2520and%2520Xiaowei%2520Yang%2520and%2520Jiahuan%2520Zhang%26entry.1292438233%3DThe%2520proliferation%2520of%2520imaging%2520devices%2520and%2520countless%2520image%2520data%2520generated%2520every%2520day%2520impose%2520an%2520increasingly%2520high%2520demand%2520on%2520efficient%2520and%2520effective%2520image%2520denoising.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520theoretical%2520connection%2520between%2520principal%2520component%2520analysis%2520%2528PCA%2529%2520and%2520the%2520Haar%2520transform%2520under%2520circulant%2520representation%252C%2520and%2520present%2520a%2520computationally%2520simple%2520denoising%2520algorithm.%2520The%2520proposed%2520method%252C%2520termed%2520Haar-tSVD%252C%2520exploits%2520a%2520unified%2520tensor%2520singular%2520value%2520decomposition%2520%2528t-SVD%2529%2520projection%2520combined%2520with%2520Haar%2520transform%2520to%2520efficiently%2520capture%2520global%2520and%2520local%2520patch%2520correlations.%2520Haar-tSVD%2520operates%2520as%2520a%2520one-step%252C%2520parallelizable%2520plug-and-play%2520denoiser%2520that%2520eliminates%2520the%2520need%2520for%2520learning%2520local%2520bases%252C%2520thereby%2520striking%2520a%2520balance%2520between%2520denoising%2520speed%2520and%2520performance.%2520Besides%252C%2520an%2520adaptive%2520noise%2520estimation%2520scheme%2520is%2520introduced%2520to%2520improve%2520robustness%2520according%2520to%2520eigenvalue%2520analysis%2520of%2520the%2520circulant%2520structure.%2520To%2520further%2520enhance%2520the%2520performance%2520under%2520severe%2520noise%2520conditions%252C%2520we%2520integrate%2520deep%2520neural%2520networks%2520with%2520Haar-tSVD%2520based%2520on%2520the%2520established%2520Haar-PCA%2520relationship.%2520Experimental%2520results%2520on%2520various%2520denoising%2520datasets%2520demonstrate%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520proposed%2520method%2520for%2520noise%2520removal.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/ZhaomingKong/Haar-tSVD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation&entry.906535625=Zhaoming%20Kong%20and%20Xiaowei%20Yang%20and%20Jiahuan%20Zhang&entry.1292438233=The%20proliferation%20of%20imaging%20devices%20and%20countless%20image%20data%20generated%20every%20day%20impose%20an%20increasingly%20high%20demand%20on%20efficient%20and%20effective%20image%20denoising.%20In%20this%20paper%2C%20we%20establish%20a%20theoretical%20connection%20between%20principal%20component%20analysis%20%28PCA%29%20and%20the%20Haar%20transform%20under%20circulant%20representation%2C%20and%20present%20a%20computationally%20simple%20denoising%20algorithm.%20The%20proposed%20method%2C%20termed%20Haar-tSVD%2C%20exploits%20a%20unified%20tensor%20singular%20value%20decomposition%20%28t-SVD%29%20projection%20combined%20with%20Haar%20transform%20to%20efficiently%20capture%20global%20and%20local%20patch%20correlations.%20Haar-tSVD%20operates%20as%20a%20one-step%2C%20parallelizable%20plug-and-play%20denoiser%20that%20eliminates%20the%20need%20for%20learning%20local%20bases%2C%20thereby%20striking%20a%20balance%20between%20denoising%20speed%20and%20performance.%20Besides%2C%20an%20adaptive%20noise%20estimation%20scheme%20is%20introduced%20to%20improve%20robustness%20according%20to%20eigenvalue%20analysis%20of%20the%20circulant%20structure.%20To%20further%20enhance%20the%20performance%20under%20severe%20noise%20conditions%2C%20we%20integrate%20deep%20neural%20networks%20with%20Haar-tSVD%20based%20on%20the%20established%20Haar-PCA%20relationship.%20Experimental%20results%20on%20various%20denoising%20datasets%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%20proposed%20method%20for%20noise%20removal.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/ZhaomingKong/Haar-tSVD.&entry.1838667208=http%3A//arxiv.org/abs/2512.23569v1&entry.124074799=Read"},
{"title": "Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification", "author": "Mustafa Demetgul and Sanja Lazarova Molnar", "abstract": "Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.", "link": "http://arxiv.org/abs/2512.23436v1", "date": "2025-12-29", "relevancy": 2.1124, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5302}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification&body=Title%3A%20Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification%0AAuthor%3A%20Mustafa%20Demetgul%20and%20Sanja%20Lazarova%20Molnar%0AAbstract%3A%20Monitoring%20states%20of%20road%20surfaces%20provides%20valuable%20information%20for%20the%20planning%20and%20controlling%20vehicles%20and%20active%20vehicle%20control%20systems.%20Classical%20road%20monitoring%20methods%20are%20expensive%20and%20unsystematic%20because%20they%20require%20time%20for%20measurements.%20This%20article%20proposes%20an%20real%20time%20system%20based%20on%20weather%20conditional%20data%20and%20road%20surface%20condition%20data.%20For%20this%20purpose%2C%20we%20collected%20data%20with%20a%20mobile%20phone%20camera%20on%20the%20roads%20around%20the%20campus%20of%20the%20Karlsruhe%20Institute%20of%20Technology.%20We%20tested%20a%20large%20number%20of%20different%20image-based%20deep%20learning%20algorithms%20for%20road%20classification.%20In%20addition%2C%20we%20used%20road%20acceleration%20data%20along%20with%20road%20image%20data%20for%20training%20by%20using%20them%20as%20images.%20We%20compared%20the%20performances%20of%20acceleration-based%20and%20camera%20image-based%20approaches.%20The%20performances%20of%20the%20simple%20Alexnet%2C%20LeNet%2C%20VGG%2C%20and%20Resnet%20algorithms%20were%20compared%20as%20deep%20learning%20algorithms.%20For%20road%20condition%20classification%2C%205%20classes%20were%20considered%3A%20asphalt%2C%20damaged%20asphalt%2C%20gravel%20road%2C%20damaged%20gravel%20road%2C%20pavement%20road%20and%20over%2095%25%20accuracy%20performance%20was%20achieved.%20It%20is%20also%20proposed%20to%20use%20the%20acceleration%20or%20the%20camera%20image%20to%20classify%20the%20road%20surface%20according%20to%20the%20weather%20and%20the%20time%20of%20day%20using%20fuzzy%20logic.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuzzy-Logic%2520and%2520Deep%2520Learning%2520for%2520Environmental%2520Condition-Aware%2520Road%2520Surface%2520Classification%26entry.906535625%3DMustafa%2520Demetgul%2520and%2520Sanja%2520Lazarova%2520Molnar%26entry.1292438233%3DMonitoring%2520states%2520of%2520road%2520surfaces%2520provides%2520valuable%2520information%2520for%2520the%2520planning%2520and%2520controlling%2520vehicles%2520and%2520active%2520vehicle%2520control%2520systems.%2520Classical%2520road%2520monitoring%2520methods%2520are%2520expensive%2520and%2520unsystematic%2520because%2520they%2520require%2520time%2520for%2520measurements.%2520This%2520article%2520proposes%2520an%2520real%2520time%2520system%2520based%2520on%2520weather%2520conditional%2520data%2520and%2520road%2520surface%2520condition%2520data.%2520For%2520this%2520purpose%252C%2520we%2520collected%2520data%2520with%2520a%2520mobile%2520phone%2520camera%2520on%2520the%2520roads%2520around%2520the%2520campus%2520of%2520the%2520Karlsruhe%2520Institute%2520of%2520Technology.%2520We%2520tested%2520a%2520large%2520number%2520of%2520different%2520image-based%2520deep%2520learning%2520algorithms%2520for%2520road%2520classification.%2520In%2520addition%252C%2520we%2520used%2520road%2520acceleration%2520data%2520along%2520with%2520road%2520image%2520data%2520for%2520training%2520by%2520using%2520them%2520as%2520images.%2520We%2520compared%2520the%2520performances%2520of%2520acceleration-based%2520and%2520camera%2520image-based%2520approaches.%2520The%2520performances%2520of%2520the%2520simple%2520Alexnet%252C%2520LeNet%252C%2520VGG%252C%2520and%2520Resnet%2520algorithms%2520were%2520compared%2520as%2520deep%2520learning%2520algorithms.%2520For%2520road%2520condition%2520classification%252C%25205%2520classes%2520were%2520considered%253A%2520asphalt%252C%2520damaged%2520asphalt%252C%2520gravel%2520road%252C%2520damaged%2520gravel%2520road%252C%2520pavement%2520road%2520and%2520over%252095%2525%2520accuracy%2520performance%2520was%2520achieved.%2520It%2520is%2520also%2520proposed%2520to%2520use%2520the%2520acceleration%2520or%2520the%2520camera%2520image%2520to%2520classify%2520the%2520road%2520surface%2520according%2520to%2520the%2520weather%2520and%2520the%2520time%2520of%2520day%2520using%2520fuzzy%2520logic.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification&entry.906535625=Mustafa%20Demetgul%20and%20Sanja%20Lazarova%20Molnar&entry.1292438233=Monitoring%20states%20of%20road%20surfaces%20provides%20valuable%20information%20for%20the%20planning%20and%20controlling%20vehicles%20and%20active%20vehicle%20control%20systems.%20Classical%20road%20monitoring%20methods%20are%20expensive%20and%20unsystematic%20because%20they%20require%20time%20for%20measurements.%20This%20article%20proposes%20an%20real%20time%20system%20based%20on%20weather%20conditional%20data%20and%20road%20surface%20condition%20data.%20For%20this%20purpose%2C%20we%20collected%20data%20with%20a%20mobile%20phone%20camera%20on%20the%20roads%20around%20the%20campus%20of%20the%20Karlsruhe%20Institute%20of%20Technology.%20We%20tested%20a%20large%20number%20of%20different%20image-based%20deep%20learning%20algorithms%20for%20road%20classification.%20In%20addition%2C%20we%20used%20road%20acceleration%20data%20along%20with%20road%20image%20data%20for%20training%20by%20using%20them%20as%20images.%20We%20compared%20the%20performances%20of%20acceleration-based%20and%20camera%20image-based%20approaches.%20The%20performances%20of%20the%20simple%20Alexnet%2C%20LeNet%2C%20VGG%2C%20and%20Resnet%20algorithms%20were%20compared%20as%20deep%20learning%20algorithms.%20For%20road%20condition%20classification%2C%205%20classes%20were%20considered%3A%20asphalt%2C%20damaged%20asphalt%2C%20gravel%20road%2C%20damaged%20gravel%20road%2C%20pavement%20road%20and%20over%2095%25%20accuracy%20performance%20was%20achieved.%20It%20is%20also%20proposed%20to%20use%20the%20acceleration%20or%20the%20camera%20image%20to%20classify%20the%20road%20surface%20according%20to%20the%20weather%20and%20the%20time%20of%20day%20using%20fuzzy%20logic.&entry.1838667208=http%3A//arxiv.org/abs/2512.23436v1&entry.124074799=Read"},
{"title": "Predicting large scale cosmological structure evolution with generative adversarial network-based autoencoders", "author": "Marion Ullmo and Nabila Aghanim and Aur\u00e9lien Decelle and Miguel Aragon-Calvo", "abstract": "Predicting the nonlinear evolution of cosmic structure from initial conditions is typically approached using Lagrangian, particle-based methods. These techniques excel in terms of tracking individual trajectories, but they might not be suitable for applications where point-based information is unavailable or impractical. In this work, we explore an alternative, field-based approach using Eulerian inputs. Specifically, we developed an autoencoder architecture based on a generative adversarial network (GAN) and trained it to evolve density fields drawn from dark matter N-body simulations. We tested this method on both 2D and 3D data. We find that while predictions on 2D density maps perform well based on density alone, accurate 3D predictions require the inclusion of associated velocity fields. Our results demonstrate the potential of field-based representations to model cosmic structure evolution, offering a complementary path to Lagrangian methods in contexts where field-level data is more accessible.", "link": "http://arxiv.org/abs/2403.02171v2", "date": "2025-12-29", "relevancy": 2.107, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5511}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5116}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20large%20scale%20cosmological%20structure%20evolution%20with%20generative%20adversarial%20network-based%20autoencoders&body=Title%3A%20Predicting%20large%20scale%20cosmological%20structure%20evolution%20with%20generative%20adversarial%20network-based%20autoencoders%0AAuthor%3A%20Marion%20Ullmo%20and%20Nabila%20Aghanim%20and%20Aur%C3%A9lien%20Decelle%20and%20Miguel%20Aragon-Calvo%0AAbstract%3A%20Predicting%20the%20nonlinear%20evolution%20of%20cosmic%20structure%20from%20initial%20conditions%20is%20typically%20approached%20using%20Lagrangian%2C%20particle-based%20methods.%20These%20techniques%20excel%20in%20terms%20of%20tracking%20individual%20trajectories%2C%20but%20they%20might%20not%20be%20suitable%20for%20applications%20where%20point-based%20information%20is%20unavailable%20or%20impractical.%20In%20this%20work%2C%20we%20explore%20an%20alternative%2C%20field-based%20approach%20using%20Eulerian%20inputs.%20Specifically%2C%20we%20developed%20an%20autoencoder%20architecture%20based%20on%20a%20generative%20adversarial%20network%20%28GAN%29%20and%20trained%20it%20to%20evolve%20density%20fields%20drawn%20from%20dark%20matter%20N-body%20simulations.%20We%20tested%20this%20method%20on%20both%202D%20and%203D%20data.%20We%20find%20that%20while%20predictions%20on%202D%20density%20maps%20perform%20well%20based%20on%20density%20alone%2C%20accurate%203D%20predictions%20require%20the%20inclusion%20of%20associated%20velocity%20fields.%20Our%20results%20demonstrate%20the%20potential%20of%20field-based%20representations%20to%20model%20cosmic%20structure%20evolution%2C%20offering%20a%20complementary%20path%20to%20Lagrangian%20methods%20in%20contexts%20where%20field-level%20data%20is%20more%20accessible.%0ALink%3A%20http%3A//arxiv.org/abs/2403.02171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520large%2520scale%2520cosmological%2520structure%2520evolution%2520with%2520generative%2520adversarial%2520network-based%2520autoencoders%26entry.906535625%3DMarion%2520Ullmo%2520and%2520Nabila%2520Aghanim%2520and%2520Aur%25C3%25A9lien%2520Decelle%2520and%2520Miguel%2520Aragon-Calvo%26entry.1292438233%3DPredicting%2520the%2520nonlinear%2520evolution%2520of%2520cosmic%2520structure%2520from%2520initial%2520conditions%2520is%2520typically%2520approached%2520using%2520Lagrangian%252C%2520particle-based%2520methods.%2520These%2520techniques%2520excel%2520in%2520terms%2520of%2520tracking%2520individual%2520trajectories%252C%2520but%2520they%2520might%2520not%2520be%2520suitable%2520for%2520applications%2520where%2520point-based%2520information%2520is%2520unavailable%2520or%2520impractical.%2520In%2520this%2520work%252C%2520we%2520explore%2520an%2520alternative%252C%2520field-based%2520approach%2520using%2520Eulerian%2520inputs.%2520Specifically%252C%2520we%2520developed%2520an%2520autoencoder%2520architecture%2520based%2520on%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520and%2520trained%2520it%2520to%2520evolve%2520density%2520fields%2520drawn%2520from%2520dark%2520matter%2520N-body%2520simulations.%2520We%2520tested%2520this%2520method%2520on%2520both%25202D%2520and%25203D%2520data.%2520We%2520find%2520that%2520while%2520predictions%2520on%25202D%2520density%2520maps%2520perform%2520well%2520based%2520on%2520density%2520alone%252C%2520accurate%25203D%2520predictions%2520require%2520the%2520inclusion%2520of%2520associated%2520velocity%2520fields.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%2520field-based%2520representations%2520to%2520model%2520cosmic%2520structure%2520evolution%252C%2520offering%2520a%2520complementary%2520path%2520to%2520Lagrangian%2520methods%2520in%2520contexts%2520where%2520field-level%2520data%2520is%2520more%2520accessible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20large%20scale%20cosmological%20structure%20evolution%20with%20generative%20adversarial%20network-based%20autoencoders&entry.906535625=Marion%20Ullmo%20and%20Nabila%20Aghanim%20and%20Aur%C3%A9lien%20Decelle%20and%20Miguel%20Aragon-Calvo&entry.1292438233=Predicting%20the%20nonlinear%20evolution%20of%20cosmic%20structure%20from%20initial%20conditions%20is%20typically%20approached%20using%20Lagrangian%2C%20particle-based%20methods.%20These%20techniques%20excel%20in%20terms%20of%20tracking%20individual%20trajectories%2C%20but%20they%20might%20not%20be%20suitable%20for%20applications%20where%20point-based%20information%20is%20unavailable%20or%20impractical.%20In%20this%20work%2C%20we%20explore%20an%20alternative%2C%20field-based%20approach%20using%20Eulerian%20inputs.%20Specifically%2C%20we%20developed%20an%20autoencoder%20architecture%20based%20on%20a%20generative%20adversarial%20network%20%28GAN%29%20and%20trained%20it%20to%20evolve%20density%20fields%20drawn%20from%20dark%20matter%20N-body%20simulations.%20We%20tested%20this%20method%20on%20both%202D%20and%203D%20data.%20We%20find%20that%20while%20predictions%20on%202D%20density%20maps%20perform%20well%20based%20on%20density%20alone%2C%20accurate%203D%20predictions%20require%20the%20inclusion%20of%20associated%20velocity%20fields.%20Our%20results%20demonstrate%20the%20potential%20of%20field-based%20representations%20to%20model%20cosmic%20structure%20evolution%2C%20offering%20a%20complementary%20path%20to%20Lagrangian%20methods%20in%20contexts%20where%20field-level%20data%20is%20more%20accessible.&entry.1838667208=http%3A//arxiv.org/abs/2403.02171v2&entry.124074799=Read"},
{"title": "Soft Robotic Technological Probe for Speculative Fashion Futures", "author": "Amy Ingold and Loong Yi Lee and Richard Suphapol Diteesawat and Ajmal Roshan and Yael Zekaria and Edith-Clare Hall and Enrico Werner and Nahian Rahman and Elaine Czech and Jonathan Rossiter", "abstract": "Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.", "link": "http://arxiv.org/abs/2512.23570v1", "date": "2025-12-29", "relevancy": 2.0959, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5706}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4928}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Robotic%20Technological%20Probe%20for%20Speculative%20Fashion%20Futures&body=Title%3A%20Soft%20Robotic%20Technological%20Probe%20for%20Speculative%20Fashion%20Futures%0AAuthor%3A%20Amy%20Ingold%20and%20Loong%20Yi%20Lee%20and%20Richard%20Suphapol%20Diteesawat%20and%20Ajmal%20Roshan%20and%20Yael%20Zekaria%20and%20Edith-Clare%20Hall%20and%20Enrico%20Werner%20and%20Nahian%20Rahman%20and%20Elaine%20Czech%20and%20Jonathan%20Rossiter%0AAbstract%3A%20Emerging%20wearable%20robotics%20demand%20design%20approaches%20that%20address%20not%20only%20function%2C%20but%20also%20social%20meaning.%20In%20response%2C%20we%20present%20Sumbrella%2C%20a%20soft%20robotic%20garment%20developed%20as%20a%20speculative%20fashion%20probe.%20We%20first%20detail%20the%20design%20and%20fabrication%20of%20the%20Sumbrella%2C%20including%20sequenced%20origami-inspired%20bistable%20units%2C%20fabric%20pneumatic%20actuation%20chambers%2C%20cable%20driven%20shape%20morphing%20mechanisms%2C%20computer%20vision%20components%2C%20and%20an%20integrated%20wearable%20system%20comprising%20a%20hat%20and%20bolero%20jacket%20housing%20power%20and%20control%20electronics.%20Through%20a%20focus%20group%20with%20twelve%20creative%20technologists%2C%20we%20then%20used%20Sumbrella%20as%20a%20technological%20probe%20to%20explore%20how%20people%20interpreted%2C%20interacted%2C%20and%20imagined%20future%20relationships%20with%20soft%20robotic%20wearables.%20While%20Sumbrella%20allowed%20our%20participants%20to%20engage%20in%20rich%20discussion%20around%20speculative%20futures%20and%20expressive%20potential%2C%20it%20also%20surfaced%20concerns%20about%20exploitation%2C%20surveillance%2C%20and%20the%20personal%20risks%20and%20societal%20ethics%20of%20embedding%20biosensing%20technologies%20in%20public%20life.%20We%20contribute%20to%20the%20Human-Robot%20Interaction%20%28HRI%29%20field%20key%20considerations%20and%20recommendations%20for%20designing%20soft%20robotic%20garments%2C%20including%20the%20potential%20for%20kinesic%20communication%2C%20the%20impact%20of%20such%20technologies%20on%20social%20dynamics%2C%20and%20the%20importance%20of%20ethical%20guidelines.%20Finally%2C%20we%20provide%20a%20reflection%20on%20our%20application%20of%20speculative%20design%3B%20proposing%20that%20it%20allows%20HRI%20researchers%20to%20not%20only%20consider%20functionality%2C%20but%20also%20how%20wearable%20robots%20influence%20definitions%20of%20what%20is%20considered%20acceptable%20or%20desirable%20in%20public%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Robotic%2520Technological%2520Probe%2520for%2520Speculative%2520Fashion%2520Futures%26entry.906535625%3DAmy%2520Ingold%2520and%2520Loong%2520Yi%2520Lee%2520and%2520Richard%2520Suphapol%2520Diteesawat%2520and%2520Ajmal%2520Roshan%2520and%2520Yael%2520Zekaria%2520and%2520Edith-Clare%2520Hall%2520and%2520Enrico%2520Werner%2520and%2520Nahian%2520Rahman%2520and%2520Elaine%2520Czech%2520and%2520Jonathan%2520Rossiter%26entry.1292438233%3DEmerging%2520wearable%2520robotics%2520demand%2520design%2520approaches%2520that%2520address%2520not%2520only%2520function%252C%2520but%2520also%2520social%2520meaning.%2520In%2520response%252C%2520we%2520present%2520Sumbrella%252C%2520a%2520soft%2520robotic%2520garment%2520developed%2520as%2520a%2520speculative%2520fashion%2520probe.%2520We%2520first%2520detail%2520the%2520design%2520and%2520fabrication%2520of%2520the%2520Sumbrella%252C%2520including%2520sequenced%2520origami-inspired%2520bistable%2520units%252C%2520fabric%2520pneumatic%2520actuation%2520chambers%252C%2520cable%2520driven%2520shape%2520morphing%2520mechanisms%252C%2520computer%2520vision%2520components%252C%2520and%2520an%2520integrated%2520wearable%2520system%2520comprising%2520a%2520hat%2520and%2520bolero%2520jacket%2520housing%2520power%2520and%2520control%2520electronics.%2520Through%2520a%2520focus%2520group%2520with%2520twelve%2520creative%2520technologists%252C%2520we%2520then%2520used%2520Sumbrella%2520as%2520a%2520technological%2520probe%2520to%2520explore%2520how%2520people%2520interpreted%252C%2520interacted%252C%2520and%2520imagined%2520future%2520relationships%2520with%2520soft%2520robotic%2520wearables.%2520While%2520Sumbrella%2520allowed%2520our%2520participants%2520to%2520engage%2520in%2520rich%2520discussion%2520around%2520speculative%2520futures%2520and%2520expressive%2520potential%252C%2520it%2520also%2520surfaced%2520concerns%2520about%2520exploitation%252C%2520surveillance%252C%2520and%2520the%2520personal%2520risks%2520and%2520societal%2520ethics%2520of%2520embedding%2520biosensing%2520technologies%2520in%2520public%2520life.%2520We%2520contribute%2520to%2520the%2520Human-Robot%2520Interaction%2520%2528HRI%2529%2520field%2520key%2520considerations%2520and%2520recommendations%2520for%2520designing%2520soft%2520robotic%2520garments%252C%2520including%2520the%2520potential%2520for%2520kinesic%2520communication%252C%2520the%2520impact%2520of%2520such%2520technologies%2520on%2520social%2520dynamics%252C%2520and%2520the%2520importance%2520of%2520ethical%2520guidelines.%2520Finally%252C%2520we%2520provide%2520a%2520reflection%2520on%2520our%2520application%2520of%2520speculative%2520design%253B%2520proposing%2520that%2520it%2520allows%2520HRI%2520researchers%2520to%2520not%2520only%2520consider%2520functionality%252C%2520but%2520also%2520how%2520wearable%2520robots%2520influence%2520definitions%2520of%2520what%2520is%2520considered%2520acceptable%2520or%2520desirable%2520in%2520public%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Robotic%20Technological%20Probe%20for%20Speculative%20Fashion%20Futures&entry.906535625=Amy%20Ingold%20and%20Loong%20Yi%20Lee%20and%20Richard%20Suphapol%20Diteesawat%20and%20Ajmal%20Roshan%20and%20Yael%20Zekaria%20and%20Edith-Clare%20Hall%20and%20Enrico%20Werner%20and%20Nahian%20Rahman%20and%20Elaine%20Czech%20and%20Jonathan%20Rossiter&entry.1292438233=Emerging%20wearable%20robotics%20demand%20design%20approaches%20that%20address%20not%20only%20function%2C%20but%20also%20social%20meaning.%20In%20response%2C%20we%20present%20Sumbrella%2C%20a%20soft%20robotic%20garment%20developed%20as%20a%20speculative%20fashion%20probe.%20We%20first%20detail%20the%20design%20and%20fabrication%20of%20the%20Sumbrella%2C%20including%20sequenced%20origami-inspired%20bistable%20units%2C%20fabric%20pneumatic%20actuation%20chambers%2C%20cable%20driven%20shape%20morphing%20mechanisms%2C%20computer%20vision%20components%2C%20and%20an%20integrated%20wearable%20system%20comprising%20a%20hat%20and%20bolero%20jacket%20housing%20power%20and%20control%20electronics.%20Through%20a%20focus%20group%20with%20twelve%20creative%20technologists%2C%20we%20then%20used%20Sumbrella%20as%20a%20technological%20probe%20to%20explore%20how%20people%20interpreted%2C%20interacted%2C%20and%20imagined%20future%20relationships%20with%20soft%20robotic%20wearables.%20While%20Sumbrella%20allowed%20our%20participants%20to%20engage%20in%20rich%20discussion%20around%20speculative%20futures%20and%20expressive%20potential%2C%20it%20also%20surfaced%20concerns%20about%20exploitation%2C%20surveillance%2C%20and%20the%20personal%20risks%20and%20societal%20ethics%20of%20embedding%20biosensing%20technologies%20in%20public%20life.%20We%20contribute%20to%20the%20Human-Robot%20Interaction%20%28HRI%29%20field%20key%20considerations%20and%20recommendations%20for%20designing%20soft%20robotic%20garments%2C%20including%20the%20potential%20for%20kinesic%20communication%2C%20the%20impact%20of%20such%20technologies%20on%20social%20dynamics%2C%20and%20the%20importance%20of%20ethical%20guidelines.%20Finally%2C%20we%20provide%20a%20reflection%20on%20our%20application%20of%20speculative%20design%3B%20proposing%20that%20it%20allows%20HRI%20researchers%20to%20not%20only%20consider%20functionality%2C%20but%20also%20how%20wearable%20robots%20influence%20definitions%20of%20what%20is%20considered%20acceptable%20or%20desirable%20in%20public%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.23570v1&entry.124074799=Read"},
{"title": "CountGD++: Generalized Prompting for Open-World Counting", "author": "Niki Amini-Naieni and Andrew Zisserman", "abstract": "The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.", "link": "http://arxiv.org/abs/2512.23351v1", "date": "2025-12-29", "relevancy": 2.0925, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5547}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5281}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountGD%2B%2B%3A%20Generalized%20Prompting%20for%20Open-World%20Counting&body=Title%3A%20CountGD%2B%2B%3A%20Generalized%20Prompting%20for%20Open-World%20Counting%0AAuthor%3A%20Niki%20Amini-Naieni%20and%20Andrew%20Zisserman%0AAbstract%3A%20The%20flexibility%20and%20accuracy%20of%20methods%20for%20automatically%20counting%20objects%20in%20images%20and%20videos%20are%20limited%20by%20the%20way%20the%20object%20can%20be%20specified.%20While%20existing%20methods%20allow%20users%20to%20describe%20the%20target%20object%20with%20text%20and%20visual%20examples%2C%20the%20visual%20examples%20must%20be%20manually%20annotated%20inside%20the%20image%2C%20and%20there%20is%20no%20way%20to%20specify%20what%20not%20to%20count.%20To%20address%20these%20gaps%2C%20we%20introduce%20novel%20capabilities%20that%20expand%20how%20the%20target%20object%20can%20be%20specified.%20Specifically%2C%20we%20extend%20the%20prompt%20to%20enable%20what%20not%20to%20count%20to%20be%20described%20with%20text%20and/or%20visual%20examples%2C%20introduce%20the%20concept%20of%20%60pseudo-exemplars%27%20that%20automate%20the%20annotation%20of%20visual%20examples%20at%20inference%2C%20and%20extend%20counting%20models%20to%20accept%20visual%20examples%20from%20both%20natural%20and%20synthetic%20external%20images.%20We%20also%20use%20our%20new%20counting%20model%2C%20CountGD%2B%2B%2C%20as%20a%20vision%20expert%20agent%20for%20an%20LLM.%20Together%2C%20these%20contributions%20expand%20the%20prompt%20flexibility%20of%20multi-modal%20open-world%20counting%20and%20lead%20to%20significant%20improvements%20in%20accuracy%2C%20efficiency%2C%20and%20generalization%20across%20multiple%20datasets.%20Code%20is%20available%20at%20https%3A//github.com/niki-amini-naieni/CountGDPlusPlus.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountGD%252B%252B%253A%2520Generalized%2520Prompting%2520for%2520Open-World%2520Counting%26entry.906535625%3DNiki%2520Amini-Naieni%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3DThe%2520flexibility%2520and%2520accuracy%2520of%2520methods%2520for%2520automatically%2520counting%2520objects%2520in%2520images%2520and%2520videos%2520are%2520limited%2520by%2520the%2520way%2520the%2520object%2520can%2520be%2520specified.%2520While%2520existing%2520methods%2520allow%2520users%2520to%2520describe%2520the%2520target%2520object%2520with%2520text%2520and%2520visual%2520examples%252C%2520the%2520visual%2520examples%2520must%2520be%2520manually%2520annotated%2520inside%2520the%2520image%252C%2520and%2520there%2520is%2520no%2520way%2520to%2520specify%2520what%2520not%2520to%2520count.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520novel%2520capabilities%2520that%2520expand%2520how%2520the%2520target%2520object%2520can%2520be%2520specified.%2520Specifically%252C%2520we%2520extend%2520the%2520prompt%2520to%2520enable%2520what%2520not%2520to%2520count%2520to%2520be%2520described%2520with%2520text%2520and/or%2520visual%2520examples%252C%2520introduce%2520the%2520concept%2520of%2520%2560pseudo-exemplars%2527%2520that%2520automate%2520the%2520annotation%2520of%2520visual%2520examples%2520at%2520inference%252C%2520and%2520extend%2520counting%2520models%2520to%2520accept%2520visual%2520examples%2520from%2520both%2520natural%2520and%2520synthetic%2520external%2520images.%2520We%2520also%2520use%2520our%2520new%2520counting%2520model%252C%2520CountGD%252B%252B%252C%2520as%2520a%2520vision%2520expert%2520agent%2520for%2520an%2520LLM.%2520Together%252C%2520these%2520contributions%2520expand%2520the%2520prompt%2520flexibility%2520of%2520multi-modal%2520open-world%2520counting%2520and%2520lead%2520to%2520significant%2520improvements%2520in%2520accuracy%252C%2520efficiency%252C%2520and%2520generalization%2520across%2520multiple%2520datasets.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/niki-amini-naieni/CountGDPlusPlus.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountGD%2B%2B%3A%20Generalized%20Prompting%20for%20Open-World%20Counting&entry.906535625=Niki%20Amini-Naieni%20and%20Andrew%20Zisserman&entry.1292438233=The%20flexibility%20and%20accuracy%20of%20methods%20for%20automatically%20counting%20objects%20in%20images%20and%20videos%20are%20limited%20by%20the%20way%20the%20object%20can%20be%20specified.%20While%20existing%20methods%20allow%20users%20to%20describe%20the%20target%20object%20with%20text%20and%20visual%20examples%2C%20the%20visual%20examples%20must%20be%20manually%20annotated%20inside%20the%20image%2C%20and%20there%20is%20no%20way%20to%20specify%20what%20not%20to%20count.%20To%20address%20these%20gaps%2C%20we%20introduce%20novel%20capabilities%20that%20expand%20how%20the%20target%20object%20can%20be%20specified.%20Specifically%2C%20we%20extend%20the%20prompt%20to%20enable%20what%20not%20to%20count%20to%20be%20described%20with%20text%20and/or%20visual%20examples%2C%20introduce%20the%20concept%20of%20%60pseudo-exemplars%27%20that%20automate%20the%20annotation%20of%20visual%20examples%20at%20inference%2C%20and%20extend%20counting%20models%20to%20accept%20visual%20examples%20from%20both%20natural%20and%20synthetic%20external%20images.%20We%20also%20use%20our%20new%20counting%20model%2C%20CountGD%2B%2B%2C%20as%20a%20vision%20expert%20agent%20for%20an%20LLM.%20Together%2C%20these%20contributions%20expand%20the%20prompt%20flexibility%20of%20multi-modal%20open-world%20counting%20and%20lead%20to%20significant%20improvements%20in%20accuracy%2C%20efficiency%2C%20and%20generalization%20across%20multiple%20datasets.%20Code%20is%20available%20at%20https%3A//github.com/niki-amini-naieni/CountGDPlusPlus.&entry.1838667208=http%3A//arxiv.org/abs/2512.23351v1&entry.124074799=Read"},
{"title": "Unsupervised Learning for Detection of Rare Driving Scenarios", "author": "Dat Le and Thomas Manhardt and Moritz Venator and Johannes Betz", "abstract": "The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.", "link": "http://arxiv.org/abs/2512.23585v1", "date": "2025-12-29", "relevancy": 2.0766, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20for%20Detection%20of%20Rare%20Driving%20Scenarios&body=Title%3A%20Unsupervised%20Learning%20for%20Detection%20of%20Rare%20Driving%20Scenarios%0AAuthor%3A%20Dat%20Le%20and%20Thomas%20Manhardt%20and%20Moritz%20Venator%20and%20Johannes%20Betz%0AAbstract%3A%20The%20detection%20of%20rare%20and%20hazardous%20driving%20scenarios%20is%20a%20critical%20challenge%20for%20ensuring%20the%20safety%20and%20reliability%20of%20autonomous%20systems.%20This%20research%20explores%20an%20unsupervised%20learning%20framework%20for%20detecting%20rare%20and%20extreme%20driving%20scenarios%20using%20naturalistic%20driving%20data%20%28NDD%29.%20We%20leverage%20the%20recently%20proposed%20Deep%20Isolation%20Forest%20%28DIF%29%2C%20an%20anomaly%20detection%20algorithm%20that%20combines%20neural%20network-based%20feature%20representations%20with%20Isolation%20Forests%20%28IFs%29%2C%20to%20identify%20non-linear%20and%20complex%20anomalies.%20Data%20from%20perception%20modules%2C%20capturing%20vehicle%20dynamics%20and%20environmental%20conditions%2C%20is%20preprocessed%20into%20structured%20statistical%20features%20extracted%20from%20sliding%20windows.%20The%20framework%20incorporates%20t-distributed%20stochastic%20neighbor%20embedding%20%28t-SNE%29%20for%20dimensionality%20reduction%20and%20visualization%2C%20enabling%20better%20interpretability%20of%20detected%20anomalies.%20Evaluation%20is%20conducted%20using%20a%20proxy%20ground%20truth%2C%20combining%20quantitative%20metrics%20with%20qualitative%20video%20frame%20inspection.%20Our%20results%20demonstrate%20that%20the%20proposed%20approach%20effectively%20identifies%20rare%20and%20hazardous%20driving%20scenarios%2C%20providing%20a%20scalable%20solution%20for%20anomaly%20detection%20in%20autonomous%20driving%20systems.%20Given%20the%20study%27s%20methodology%2C%20it%20was%20unavoidable%20to%20depend%20on%20proxy%20ground%20truth%20and%20manually%20defined%20feature%20combinations%2C%20which%20do%20not%20encompass%20the%20full%20range%20of%20real-world%20driving%20anomalies%20or%20their%20nuanced%20contextual%20dependencies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Learning%2520for%2520Detection%2520of%2520Rare%2520Driving%2520Scenarios%26entry.906535625%3DDat%2520Le%2520and%2520Thomas%2520Manhardt%2520and%2520Moritz%2520Venator%2520and%2520Johannes%2520Betz%26entry.1292438233%3DThe%2520detection%2520of%2520rare%2520and%2520hazardous%2520driving%2520scenarios%2520is%2520a%2520critical%2520challenge%2520for%2520ensuring%2520the%2520safety%2520and%2520reliability%2520of%2520autonomous%2520systems.%2520This%2520research%2520explores%2520an%2520unsupervised%2520learning%2520framework%2520for%2520detecting%2520rare%2520and%2520extreme%2520driving%2520scenarios%2520using%2520naturalistic%2520driving%2520data%2520%2528NDD%2529.%2520We%2520leverage%2520the%2520recently%2520proposed%2520Deep%2520Isolation%2520Forest%2520%2528DIF%2529%252C%2520an%2520anomaly%2520detection%2520algorithm%2520that%2520combines%2520neural%2520network-based%2520feature%2520representations%2520with%2520Isolation%2520Forests%2520%2528IFs%2529%252C%2520to%2520identify%2520non-linear%2520and%2520complex%2520anomalies.%2520Data%2520from%2520perception%2520modules%252C%2520capturing%2520vehicle%2520dynamics%2520and%2520environmental%2520conditions%252C%2520is%2520preprocessed%2520into%2520structured%2520statistical%2520features%2520extracted%2520from%2520sliding%2520windows.%2520The%2520framework%2520incorporates%2520t-distributed%2520stochastic%2520neighbor%2520embedding%2520%2528t-SNE%2529%2520for%2520dimensionality%2520reduction%2520and%2520visualization%252C%2520enabling%2520better%2520interpretability%2520of%2520detected%2520anomalies.%2520Evaluation%2520is%2520conducted%2520using%2520a%2520proxy%2520ground%2520truth%252C%2520combining%2520quantitative%2520metrics%2520with%2520qualitative%2520video%2520frame%2520inspection.%2520Our%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520effectively%2520identifies%2520rare%2520and%2520hazardous%2520driving%2520scenarios%252C%2520providing%2520a%2520scalable%2520solution%2520for%2520anomaly%2520detection%2520in%2520autonomous%2520driving%2520systems.%2520Given%2520the%2520study%2527s%2520methodology%252C%2520it%2520was%2520unavoidable%2520to%2520depend%2520on%2520proxy%2520ground%2520truth%2520and%2520manually%2520defined%2520feature%2520combinations%252C%2520which%2520do%2520not%2520encompass%2520the%2520full%2520range%2520of%2520real-world%2520driving%2520anomalies%2520or%2520their%2520nuanced%2520contextual%2520dependencies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20for%20Detection%20of%20Rare%20Driving%20Scenarios&entry.906535625=Dat%20Le%20and%20Thomas%20Manhardt%20and%20Moritz%20Venator%20and%20Johannes%20Betz&entry.1292438233=The%20detection%20of%20rare%20and%20hazardous%20driving%20scenarios%20is%20a%20critical%20challenge%20for%20ensuring%20the%20safety%20and%20reliability%20of%20autonomous%20systems.%20This%20research%20explores%20an%20unsupervised%20learning%20framework%20for%20detecting%20rare%20and%20extreme%20driving%20scenarios%20using%20naturalistic%20driving%20data%20%28NDD%29.%20We%20leverage%20the%20recently%20proposed%20Deep%20Isolation%20Forest%20%28DIF%29%2C%20an%20anomaly%20detection%20algorithm%20that%20combines%20neural%20network-based%20feature%20representations%20with%20Isolation%20Forests%20%28IFs%29%2C%20to%20identify%20non-linear%20and%20complex%20anomalies.%20Data%20from%20perception%20modules%2C%20capturing%20vehicle%20dynamics%20and%20environmental%20conditions%2C%20is%20preprocessed%20into%20structured%20statistical%20features%20extracted%20from%20sliding%20windows.%20The%20framework%20incorporates%20t-distributed%20stochastic%20neighbor%20embedding%20%28t-SNE%29%20for%20dimensionality%20reduction%20and%20visualization%2C%20enabling%20better%20interpretability%20of%20detected%20anomalies.%20Evaluation%20is%20conducted%20using%20a%20proxy%20ground%20truth%2C%20combining%20quantitative%20metrics%20with%20qualitative%20video%20frame%20inspection.%20Our%20results%20demonstrate%20that%20the%20proposed%20approach%20effectively%20identifies%20rare%20and%20hazardous%20driving%20scenarios%2C%20providing%20a%20scalable%20solution%20for%20anomaly%20detection%20in%20autonomous%20driving%20systems.%20Given%20the%20study%27s%20methodology%2C%20it%20was%20unavoidable%20to%20depend%20on%20proxy%20ground%20truth%20and%20manually%20defined%20feature%20combinations%2C%20which%20do%20not%20encompass%20the%20full%20range%20of%20real-world%20driving%20anomalies%20or%20their%20nuanced%20contextual%20dependencies.&entry.1838667208=http%3A//arxiv.org/abs/2512.23585v1&entry.124074799=Read"},
{"title": "Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative System for Underwater Tasks in Extreme Sea Conditions", "author": "Jingzehua Xu and Guanwen Xie and Jiwei Tang and Yimian Ding and Weiyi Liu and Junhao Huang and Shuai Zhang and Yi Li", "abstract": "This paper develops a novel unmanned surface vehicle (USV)-autonomous underwater vehicle (AUV) collaborative system designed to enhance underwater task performance in extreme sea conditions. The system integrates a dual strategy: (1) high-precision multi-AUV localization enabled by Fisher information matrix-optimized USV path planning, and (2) reinforcement learning-based cooperative planning and control method for multi-AUV task execution. Extensive experimental evaluations in the underwater data collection task demonstrate the system's operational feasibility, with quantitative results showing significant performance improvements over baseline methods. The proposed system exhibits robust coordination capabilities between USV and AUVs while maintaining stability in extreme sea conditions. To facilitate reproducibility and community advancement, we provide an open-source simulation toolkit available at: https://github.com/360ZMEM/USV-AUV-colab .", "link": "http://arxiv.org/abs/2504.14894v3", "date": "2025-12-29", "relevancy": 2.0673, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5373}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5111}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Never%20too%20Cocky%20to%20Cooperate%3A%20An%20FIM%20and%20RL-based%20USV-AUV%20Collaborative%20System%20for%20Underwater%20Tasks%20in%20Extreme%20Sea%20Conditions&body=Title%3A%20Never%20too%20Cocky%20to%20Cooperate%3A%20An%20FIM%20and%20RL-based%20USV-AUV%20Collaborative%20System%20for%20Underwater%20Tasks%20in%20Extreme%20Sea%20Conditions%0AAuthor%3A%20Jingzehua%20Xu%20and%20Guanwen%20Xie%20and%20Jiwei%20Tang%20and%20Yimian%20Ding%20and%20Weiyi%20Liu%20and%20Junhao%20Huang%20and%20Shuai%20Zhang%20and%20Yi%20Li%0AAbstract%3A%20This%20paper%20develops%20a%20novel%20unmanned%20surface%20vehicle%20%28USV%29-autonomous%20underwater%20vehicle%20%28AUV%29%20collaborative%20system%20designed%20to%20enhance%20underwater%20task%20performance%20in%20extreme%20sea%20conditions.%20The%20system%20integrates%20a%20dual%20strategy%3A%20%281%29%20high-precision%20multi-AUV%20localization%20enabled%20by%20Fisher%20information%20matrix-optimized%20USV%20path%20planning%2C%20and%20%282%29%20reinforcement%20learning-based%20cooperative%20planning%20and%20control%20method%20for%20multi-AUV%20task%20execution.%20Extensive%20experimental%20evaluations%20in%20the%20underwater%20data%20collection%20task%20demonstrate%20the%20system%27s%20operational%20feasibility%2C%20with%20quantitative%20results%20showing%20significant%20performance%20improvements%20over%20baseline%20methods.%20The%20proposed%20system%20exhibits%20robust%20coordination%20capabilities%20between%20USV%20and%20AUVs%20while%20maintaining%20stability%20in%20extreme%20sea%20conditions.%20To%20facilitate%20reproducibility%20and%20community%20advancement%2C%20we%20provide%20an%20open-source%20simulation%20toolkit%20available%20at%3A%20https%3A//github.com/360ZMEM/USV-AUV-colab%20.%0ALink%3A%20http%3A//arxiv.org/abs/2504.14894v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNever%2520too%2520Cocky%2520to%2520Cooperate%253A%2520An%2520FIM%2520and%2520RL-based%2520USV-AUV%2520Collaborative%2520System%2520for%2520Underwater%2520Tasks%2520in%2520Extreme%2520Sea%2520Conditions%26entry.906535625%3DJingzehua%2520Xu%2520and%2520Guanwen%2520Xie%2520and%2520Jiwei%2520Tang%2520and%2520Yimian%2520Ding%2520and%2520Weiyi%2520Liu%2520and%2520Junhao%2520Huang%2520and%2520Shuai%2520Zhang%2520and%2520Yi%2520Li%26entry.1292438233%3DThis%2520paper%2520develops%2520a%2520novel%2520unmanned%2520surface%2520vehicle%2520%2528USV%2529-autonomous%2520underwater%2520vehicle%2520%2528AUV%2529%2520collaborative%2520system%2520designed%2520to%2520enhance%2520underwater%2520task%2520performance%2520in%2520extreme%2520sea%2520conditions.%2520The%2520system%2520integrates%2520a%2520dual%2520strategy%253A%2520%25281%2529%2520high-precision%2520multi-AUV%2520localization%2520enabled%2520by%2520Fisher%2520information%2520matrix-optimized%2520USV%2520path%2520planning%252C%2520and%2520%25282%2529%2520reinforcement%2520learning-based%2520cooperative%2520planning%2520and%2520control%2520method%2520for%2520multi-AUV%2520task%2520execution.%2520Extensive%2520experimental%2520evaluations%2520in%2520the%2520underwater%2520data%2520collection%2520task%2520demonstrate%2520the%2520system%2527s%2520operational%2520feasibility%252C%2520with%2520quantitative%2520results%2520showing%2520significant%2520performance%2520improvements%2520over%2520baseline%2520methods.%2520The%2520proposed%2520system%2520exhibits%2520robust%2520coordination%2520capabilities%2520between%2520USV%2520and%2520AUVs%2520while%2520maintaining%2520stability%2520in%2520extreme%2520sea%2520conditions.%2520To%2520facilitate%2520reproducibility%2520and%2520community%2520advancement%252C%2520we%2520provide%2520an%2520open-source%2520simulation%2520toolkit%2520available%2520at%253A%2520https%253A//github.com/360ZMEM/USV-AUV-colab%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14894v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Never%20too%20Cocky%20to%20Cooperate%3A%20An%20FIM%20and%20RL-based%20USV-AUV%20Collaborative%20System%20for%20Underwater%20Tasks%20in%20Extreme%20Sea%20Conditions&entry.906535625=Jingzehua%20Xu%20and%20Guanwen%20Xie%20and%20Jiwei%20Tang%20and%20Yimian%20Ding%20and%20Weiyi%20Liu%20and%20Junhao%20Huang%20and%20Shuai%20Zhang%20and%20Yi%20Li&entry.1292438233=This%20paper%20develops%20a%20novel%20unmanned%20surface%20vehicle%20%28USV%29-autonomous%20underwater%20vehicle%20%28AUV%29%20collaborative%20system%20designed%20to%20enhance%20underwater%20task%20performance%20in%20extreme%20sea%20conditions.%20The%20system%20integrates%20a%20dual%20strategy%3A%20%281%29%20high-precision%20multi-AUV%20localization%20enabled%20by%20Fisher%20information%20matrix-optimized%20USV%20path%20planning%2C%20and%20%282%29%20reinforcement%20learning-based%20cooperative%20planning%20and%20control%20method%20for%20multi-AUV%20task%20execution.%20Extensive%20experimental%20evaluations%20in%20the%20underwater%20data%20collection%20task%20demonstrate%20the%20system%27s%20operational%20feasibility%2C%20with%20quantitative%20results%20showing%20significant%20performance%20improvements%20over%20baseline%20methods.%20The%20proposed%20system%20exhibits%20robust%20coordination%20capabilities%20between%20USV%20and%20AUVs%20while%20maintaining%20stability%20in%20extreme%20sea%20conditions.%20To%20facilitate%20reproducibility%20and%20community%20advancement%2C%20we%20provide%20an%20open-source%20simulation%20toolkit%20available%20at%3A%20https%3A//github.com/360ZMEM/USV-AUV-colab%20.&entry.1838667208=http%3A//arxiv.org/abs/2504.14894v3&entry.124074799=Read"},
{"title": "The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation", "author": "Mohammed Baziyad and Manal Al Shohna and Tamer Rabie", "abstract": "Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.", "link": "http://arxiv.org/abs/2512.23672v1", "date": "2025-12-29", "relevancy": 2.0625, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.518}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5155}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Bulldozer%20Technique%3A%20Efficient%20Elimination%20of%20Local%20Minima%20Traps%20for%20APF-Based%20Robot%20Navigation&body=Title%3A%20The%20Bulldozer%20Technique%3A%20Efficient%20Elimination%20of%20Local%20Minima%20Traps%20for%20APF-Based%20Robot%20Navigation%0AAuthor%3A%20Mohammed%20Baziyad%20and%20Manal%20Al%20Shohna%20and%20Tamer%20Rabie%0AAbstract%3A%20Path%20planning%20is%20a%20fundamental%20component%20in%20autonomous%20mobile%20robotics%2C%20enabling%20a%20robot%20to%20navigate%20from%20its%20current%20location%20to%20a%20desired%20goal%20while%20avoiding%20obstacles.%20Among%20the%20various%20techniques%2C%20Artificial%20Potential%20Field%20%28APF%29%20methods%20have%20gained%20popularity%20due%20to%20their%20simplicity%2C%20real-time%20responsiveness%2C%20and%20low%20computational%20requirements.%20However%2C%20a%20major%20limitation%20of%20conventional%20APF%20approaches%20is%20the%20local%20minima%20trap%20problem%2C%20where%20the%20robot%20becomes%20stuck%20in%20a%20position%20with%20no%20clear%20direction%20toward%20the%20goal.%20This%20paper%20proposes%20a%20novel%20path%20planning%20technique%2C%20termed%20the%20Bulldozer%2C%20which%20addresses%20the%20local%20minima%20issue%20while%20preserving%20the%20inherent%20advantages%20of%20APF.%20The%20Bulldozer%20technique%20introduces%20a%20backfilling%20mechanism%20that%20systematically%20identifies%20and%20eliminates%20local%20minima%20regions%20by%20increasing%20their%20potential%20values%2C%20analogous%20to%20a%20bulldozer%20filling%20potholes%20in%20a%20road.%20Additionally%2C%20a%20ramp-based%20enhancement%20is%20incorporated%20to%20assist%20the%20robot%20in%20escaping%20trap%20areas%20when%20starting%20within%20a%20local%20minimum.%20The%20proposed%20technique%20is%20experimentally%20validated%20using%20a%20physical%20mobile%20robot%20across%20various%20maps%20with%20increasing%20complexity.%20Comparative%20analyses%20are%20conducted%20against%20standard%20APF%2C%20adaptive%20APF%2C%20and%20well-established%20planning%20algorithms%20such%20as%20A%2A%2C%20PRM%2C%20and%20RRT.%20Results%20demonstrate%20that%20the%20Bulldozer%20technique%20effectively%20resolves%20the%20local%20minima%20problem%20while%20achieving%20superior%20execution%20speed%20and%20competitive%20path%20quality.%20Furthermore%2C%20a%20kinematic%20tracking%20controller%20is%20employed%20to%20assess%20the%20smoothness%20and%20traceability%20of%20the%20planned%20paths%2C%20confirming%20their%20suitability%20for%20real-world%20execution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Bulldozer%2520Technique%253A%2520Efficient%2520Elimination%2520of%2520Local%2520Minima%2520Traps%2520for%2520APF-Based%2520Robot%2520Navigation%26entry.906535625%3DMohammed%2520Baziyad%2520and%2520Manal%2520Al%2520Shohna%2520and%2520Tamer%2520Rabie%26entry.1292438233%3DPath%2520planning%2520is%2520a%2520fundamental%2520component%2520in%2520autonomous%2520mobile%2520robotics%252C%2520enabling%2520a%2520robot%2520to%2520navigate%2520from%2520its%2520current%2520location%2520to%2520a%2520desired%2520goal%2520while%2520avoiding%2520obstacles.%2520Among%2520the%2520various%2520techniques%252C%2520Artificial%2520Potential%2520Field%2520%2528APF%2529%2520methods%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520simplicity%252C%2520real-time%2520responsiveness%252C%2520and%2520low%2520computational%2520requirements.%2520However%252C%2520a%2520major%2520limitation%2520of%2520conventional%2520APF%2520approaches%2520is%2520the%2520local%2520minima%2520trap%2520problem%252C%2520where%2520the%2520robot%2520becomes%2520stuck%2520in%2520a%2520position%2520with%2520no%2520clear%2520direction%2520toward%2520the%2520goal.%2520This%2520paper%2520proposes%2520a%2520novel%2520path%2520planning%2520technique%252C%2520termed%2520the%2520Bulldozer%252C%2520which%2520addresses%2520the%2520local%2520minima%2520issue%2520while%2520preserving%2520the%2520inherent%2520advantages%2520of%2520APF.%2520The%2520Bulldozer%2520technique%2520introduces%2520a%2520backfilling%2520mechanism%2520that%2520systematically%2520identifies%2520and%2520eliminates%2520local%2520minima%2520regions%2520by%2520increasing%2520their%2520potential%2520values%252C%2520analogous%2520to%2520a%2520bulldozer%2520filling%2520potholes%2520in%2520a%2520road.%2520Additionally%252C%2520a%2520ramp-based%2520enhancement%2520is%2520incorporated%2520to%2520assist%2520the%2520robot%2520in%2520escaping%2520trap%2520areas%2520when%2520starting%2520within%2520a%2520local%2520minimum.%2520The%2520proposed%2520technique%2520is%2520experimentally%2520validated%2520using%2520a%2520physical%2520mobile%2520robot%2520across%2520various%2520maps%2520with%2520increasing%2520complexity.%2520Comparative%2520analyses%2520are%2520conducted%2520against%2520standard%2520APF%252C%2520adaptive%2520APF%252C%2520and%2520well-established%2520planning%2520algorithms%2520such%2520as%2520A%252A%252C%2520PRM%252C%2520and%2520RRT.%2520Results%2520demonstrate%2520that%2520the%2520Bulldozer%2520technique%2520effectively%2520resolves%2520the%2520local%2520minima%2520problem%2520while%2520achieving%2520superior%2520execution%2520speed%2520and%2520competitive%2520path%2520quality.%2520Furthermore%252C%2520a%2520kinematic%2520tracking%2520controller%2520is%2520employed%2520to%2520assess%2520the%2520smoothness%2520and%2520traceability%2520of%2520the%2520planned%2520paths%252C%2520confirming%2520their%2520suitability%2520for%2520real-world%2520execution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Bulldozer%20Technique%3A%20Efficient%20Elimination%20of%20Local%20Minima%20Traps%20for%20APF-Based%20Robot%20Navigation&entry.906535625=Mohammed%20Baziyad%20and%20Manal%20Al%20Shohna%20and%20Tamer%20Rabie&entry.1292438233=Path%20planning%20is%20a%20fundamental%20component%20in%20autonomous%20mobile%20robotics%2C%20enabling%20a%20robot%20to%20navigate%20from%20its%20current%20location%20to%20a%20desired%20goal%20while%20avoiding%20obstacles.%20Among%20the%20various%20techniques%2C%20Artificial%20Potential%20Field%20%28APF%29%20methods%20have%20gained%20popularity%20due%20to%20their%20simplicity%2C%20real-time%20responsiveness%2C%20and%20low%20computational%20requirements.%20However%2C%20a%20major%20limitation%20of%20conventional%20APF%20approaches%20is%20the%20local%20minima%20trap%20problem%2C%20where%20the%20robot%20becomes%20stuck%20in%20a%20position%20with%20no%20clear%20direction%20toward%20the%20goal.%20This%20paper%20proposes%20a%20novel%20path%20planning%20technique%2C%20termed%20the%20Bulldozer%2C%20which%20addresses%20the%20local%20minima%20issue%20while%20preserving%20the%20inherent%20advantages%20of%20APF.%20The%20Bulldozer%20technique%20introduces%20a%20backfilling%20mechanism%20that%20systematically%20identifies%20and%20eliminates%20local%20minima%20regions%20by%20increasing%20their%20potential%20values%2C%20analogous%20to%20a%20bulldozer%20filling%20potholes%20in%20a%20road.%20Additionally%2C%20a%20ramp-based%20enhancement%20is%20incorporated%20to%20assist%20the%20robot%20in%20escaping%20trap%20areas%20when%20starting%20within%20a%20local%20minimum.%20The%20proposed%20technique%20is%20experimentally%20validated%20using%20a%20physical%20mobile%20robot%20across%20various%20maps%20with%20increasing%20complexity.%20Comparative%20analyses%20are%20conducted%20against%20standard%20APF%2C%20adaptive%20APF%2C%20and%20well-established%20planning%20algorithms%20such%20as%20A%2A%2C%20PRM%2C%20and%20RRT.%20Results%20demonstrate%20that%20the%20Bulldozer%20technique%20effectively%20resolves%20the%20local%20minima%20problem%20while%20achieving%20superior%20execution%20speed%20and%20competitive%20path%20quality.%20Furthermore%2C%20a%20kinematic%20tracking%20controller%20is%20employed%20to%20assess%20the%20smoothness%20and%20traceability%20of%20the%20planned%20paths%2C%20confirming%20their%20suitability%20for%20real-world%20execution.&entry.1838667208=http%3A//arxiv.org/abs/2512.23672v1&entry.124074799=Read"},
{"title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery", "author": "Qinfeng Zhu and Yunxi Jiang and Lei Fan", "abstract": "We propose a result-level category-specific fusion architecture called ClassWise-CRF. This architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture treats the segmentation predictions from multiple networks as confidence vector fields. It leverages segmentation metrics (such as Intersection over Union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. This fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. Building on this, the architecture further optimizes the fused results using unary and pairwise potentials in CRF to ensure spatial consistency and boundary accuracy. To validate the effectiveness of ClassWise-CRF, we conducted experiments on two remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced semantic segmentation networks. The results show that the ClassWise-CRF architecture significantly improves segmentation performance: on the LoveDA dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the Vaihingen dataset, the mIoU improved by 0.87% on the validation set and by 0.91% on the test set. These results fully demonstrate the effectiveness and generality of the ClassWise-CRF architecture in semantic segmentation of remote sensing images. The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.", "link": "http://arxiv.org/abs/2504.21491v2", "date": "2025-12-29", "relevancy": 2.05, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5197}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%20Segmentation%20of%20Remote%20Sensing%20Imagery&body=Title%3A%20ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%20Segmentation%20of%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Qinfeng%20Zhu%20and%20Yunxi%20Jiang%20and%20Lei%20Fan%0AAbstract%3A%20We%20propose%20a%20result-level%20category-specific%20fusion%20architecture%20called%20ClassWise-CRF.%20This%20architecture%20employs%20a%20two-stage%20process%3A%20first%2C%20it%20selects%20expert%20networks%20that%20perform%20well%20in%20specific%20categories%20from%20a%20pool%20of%20candidate%20networks%20using%20a%20greedy%20algorithm%3B%20second%2C%20it%20integrates%20the%20segmentation%20predictions%20of%20these%20selected%20networks%20by%20adaptively%20weighting%20their%20contributions%20based%20on%20their%20segmentation%20performance%20in%20each%20category.%20Inspired%20by%20Conditional%20Random%20Field%20%28CRF%29%2C%20the%20ClassWise-CRF%20architecture%20treats%20the%20segmentation%20predictions%20from%20multiple%20networks%20as%20confidence%20vector%20fields.%20It%20leverages%20segmentation%20metrics%20%28such%20as%20Intersection%20over%20Union%29%20from%20the%20validation%20set%20as%20priors%20and%20employs%20an%20exponential%20weighting%20strategy%20to%20fuse%20the%20category-specific%20confidence%20scores%20predicted%20by%20each%20network.%20This%20fusion%20method%20dynamically%20adjusts%20the%20weights%20of%20each%20network%20for%20different%20categories%2C%20achieving%20category-specific%20optimization.%20Building%20on%20this%2C%20the%20architecture%20further%20optimizes%20the%20fused%20results%20using%20unary%20and%20pairwise%20potentials%20in%20CRF%20to%20ensure%20spatial%20consistency%20and%20boundary%20accuracy.%20To%20validate%20the%20effectiveness%20of%20ClassWise-CRF%2C%20we%20conducted%20experiments%20on%20two%20remote%20sensing%20datasets%2C%20LoveDA%20and%20Vaihingen%2C%20using%20eight%20classic%20and%20advanced%20semantic%20segmentation%20networks.%20The%20results%20show%20that%20the%20ClassWise-CRF%20architecture%20significantly%20improves%20segmentation%20performance%3A%20on%20the%20LoveDA%20dataset%2C%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20metric%20increased%20by%201.00%25%20on%20the%20validation%20set%20and%20by%200.68%25%20on%20the%20test%20set%3B%20on%20the%20Vaihingen%20dataset%2C%20the%20mIoU%20improved%20by%200.87%25%20on%20the%20validation%20set%20and%20by%200.91%25%20on%20the%20test%20set.%20These%20results%20fully%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%20ClassWise-CRF%20architecture%20in%20semantic%20segmentation%20of%20remote%20sensing%20images.%20The%20full%20code%20is%20available%20at%20https%3A//github.com/zhuqinfeng1999/ClassWise-CRF.%0ALink%3A%20http%3A//arxiv.org/abs/2504.21491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassWise-CRF%253A%2520Category-Specific%2520Fusion%2520for%2520Enhanced%2520Semantic%2520Segmentation%2520of%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DQinfeng%2520Zhu%2520and%2520Yunxi%2520Jiang%2520and%2520Lei%2520Fan%26entry.1292438233%3DWe%2520propose%2520a%2520result-level%2520category-specific%2520fusion%2520architecture%2520called%2520ClassWise-CRF.%2520This%2520architecture%2520employs%2520a%2520two-stage%2520process%253A%2520first%252C%2520it%2520selects%2520expert%2520networks%2520that%2520perform%2520well%2520in%2520specific%2520categories%2520from%2520a%2520pool%2520of%2520candidate%2520networks%2520using%2520a%2520greedy%2520algorithm%253B%2520second%252C%2520it%2520integrates%2520the%2520segmentation%2520predictions%2520of%2520these%2520selected%2520networks%2520by%2520adaptively%2520weighting%2520their%2520contributions%2520based%2520on%2520their%2520segmentation%2520performance%2520in%2520each%2520category.%2520Inspired%2520by%2520Conditional%2520Random%2520Field%2520%2528CRF%2529%252C%2520the%2520ClassWise-CRF%2520architecture%2520treats%2520the%2520segmentation%2520predictions%2520from%2520multiple%2520networks%2520as%2520confidence%2520vector%2520fields.%2520It%2520leverages%2520segmentation%2520metrics%2520%2528such%2520as%2520Intersection%2520over%2520Union%2529%2520from%2520the%2520validation%2520set%2520as%2520priors%2520and%2520employs%2520an%2520exponential%2520weighting%2520strategy%2520to%2520fuse%2520the%2520category-specific%2520confidence%2520scores%2520predicted%2520by%2520each%2520network.%2520This%2520fusion%2520method%2520dynamically%2520adjusts%2520the%2520weights%2520of%2520each%2520network%2520for%2520different%2520categories%252C%2520achieving%2520category-specific%2520optimization.%2520Building%2520on%2520this%252C%2520the%2520architecture%2520further%2520optimizes%2520the%2520fused%2520results%2520using%2520unary%2520and%2520pairwise%2520potentials%2520in%2520CRF%2520to%2520ensure%2520spatial%2520consistency%2520and%2520boundary%2520accuracy.%2520To%2520validate%2520the%2520effectiveness%2520of%2520ClassWise-CRF%252C%2520we%2520conducted%2520experiments%2520on%2520two%2520remote%2520sensing%2520datasets%252C%2520LoveDA%2520and%2520Vaihingen%252C%2520using%2520eight%2520classic%2520and%2520advanced%2520semantic%2520segmentation%2520networks.%2520The%2520results%2520show%2520that%2520the%2520ClassWise-CRF%2520architecture%2520significantly%2520improves%2520segmentation%2520performance%253A%2520on%2520the%2520LoveDA%2520dataset%252C%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520metric%2520increased%2520by%25201.00%2525%2520on%2520the%2520validation%2520set%2520and%2520by%25200.68%2525%2520on%2520the%2520test%2520set%253B%2520on%2520the%2520Vaihingen%2520dataset%252C%2520the%2520mIoU%2520improved%2520by%25200.87%2525%2520on%2520the%2520validation%2520set%2520and%2520by%25200.91%2525%2520on%2520the%2520test%2520set.%2520These%2520results%2520fully%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%2520the%2520ClassWise-CRF%2520architecture%2520in%2520semantic%2520segmentation%2520of%2520remote%2520sensing%2520images.%2520The%2520full%2520code%2520is%2520available%2520at%2520https%253A//github.com/zhuqinfeng1999/ClassWise-CRF.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClassWise-CRF%3A%20Category-Specific%20Fusion%20for%20Enhanced%20Semantic%20Segmentation%20of%20Remote%20Sensing%20Imagery&entry.906535625=Qinfeng%20Zhu%20and%20Yunxi%20Jiang%20and%20Lei%20Fan&entry.1292438233=We%20propose%20a%20result-level%20category-specific%20fusion%20architecture%20called%20ClassWise-CRF.%20This%20architecture%20employs%20a%20two-stage%20process%3A%20first%2C%20it%20selects%20expert%20networks%20that%20perform%20well%20in%20specific%20categories%20from%20a%20pool%20of%20candidate%20networks%20using%20a%20greedy%20algorithm%3B%20second%2C%20it%20integrates%20the%20segmentation%20predictions%20of%20these%20selected%20networks%20by%20adaptively%20weighting%20their%20contributions%20based%20on%20their%20segmentation%20performance%20in%20each%20category.%20Inspired%20by%20Conditional%20Random%20Field%20%28CRF%29%2C%20the%20ClassWise-CRF%20architecture%20treats%20the%20segmentation%20predictions%20from%20multiple%20networks%20as%20confidence%20vector%20fields.%20It%20leverages%20segmentation%20metrics%20%28such%20as%20Intersection%20over%20Union%29%20from%20the%20validation%20set%20as%20priors%20and%20employs%20an%20exponential%20weighting%20strategy%20to%20fuse%20the%20category-specific%20confidence%20scores%20predicted%20by%20each%20network.%20This%20fusion%20method%20dynamically%20adjusts%20the%20weights%20of%20each%20network%20for%20different%20categories%2C%20achieving%20category-specific%20optimization.%20Building%20on%20this%2C%20the%20architecture%20further%20optimizes%20the%20fused%20results%20using%20unary%20and%20pairwise%20potentials%20in%20CRF%20to%20ensure%20spatial%20consistency%20and%20boundary%20accuracy.%20To%20validate%20the%20effectiveness%20of%20ClassWise-CRF%2C%20we%20conducted%20experiments%20on%20two%20remote%20sensing%20datasets%2C%20LoveDA%20and%20Vaihingen%2C%20using%20eight%20classic%20and%20advanced%20semantic%20segmentation%20networks.%20The%20results%20show%20that%20the%20ClassWise-CRF%20architecture%20significantly%20improves%20segmentation%20performance%3A%20on%20the%20LoveDA%20dataset%2C%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20metric%20increased%20by%201.00%25%20on%20the%20validation%20set%20and%20by%200.68%25%20on%20the%20test%20set%3B%20on%20the%20Vaihingen%20dataset%2C%20the%20mIoU%20improved%20by%200.87%25%20on%20the%20validation%20set%20and%20by%200.91%25%20on%20the%20test%20set.%20These%20results%20fully%20demonstrate%20the%20effectiveness%20and%20generality%20of%20the%20ClassWise-CRF%20architecture%20in%20semantic%20segmentation%20of%20remote%20sensing%20images.%20The%20full%20code%20is%20available%20at%20https%3A//github.com/zhuqinfeng1999/ClassWise-CRF.&entry.1838667208=http%3A//arxiv.org/abs/2504.21491v2&entry.124074799=Read"},
{"title": "Beyond Context: Large Language Models Failure to Grasp Users Intent", "author": "Ahmed M. Hussain and Salahuddin Salahuddin and Panos Papadimitratos", "abstract": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.", "link": "http://arxiv.org/abs/2512.21110v2", "date": "2025-12-29", "relevancy": 2.0364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Context%3A%20Large%20Language%20Models%20Failure%20to%20Grasp%20Users%20Intent&body=Title%3A%20Beyond%20Context%3A%20Large%20Language%20Models%20Failure%20to%20Grasp%20Users%20Intent%0AAuthor%3A%20Ahmed%20M.%20Hussain%20and%20Salahuddin%20Salahuddin%20and%20Panos%20Papadimitratos%0AAbstract%3A%20Current%20Large%20Language%20Models%20%28LLMs%29%20safety%20approaches%20focus%20on%20explicitly%20harmful%20content%20while%20overlooking%20a%20critical%20vulnerability%3A%20the%20inability%20to%20understand%20context%20and%20recognize%20user%20intent.%20This%20creates%20exploitable%20vulnerabilities%20that%20malicious%20users%20can%20systematically%20leverage%20to%20circumvent%20safety%20mechanisms.%20We%20empirically%20evaluate%20multiple%20state-of-the-art%20LLMs%2C%20including%20ChatGPT%2C%20Claude%2C%20Gemini%2C%20and%20DeepSeek.%20Our%20analysis%20demonstrates%20the%20circumvention%20of%20reliable%20safety%20mechanisms%20through%20emotional%20framing%2C%20progressive%20revelation%2C%20and%20academic%20justification%20techniques.%20Notably%2C%20reasoning-enabled%20configurations%20amplified%20rather%20than%20mitigated%20the%20effectiveness%20of%20exploitation%2C%20increasing%20factual%20precision%20while%20failing%20to%20interrogate%20the%20underlying%20intent.%20The%20exception%20was%20Claude%20Opus%204.1%2C%20which%20prioritized%20intent%20detection%20over%20information%20provision%20in%20some%20use%20cases.%20This%20pattern%20reveals%20that%20current%20architectural%20designs%20create%20systematic%20vulnerabilities.%20These%20limitations%20require%20paradigmatic%20shifts%20toward%20contextual%20understanding%20and%20intent%20recognition%20as%20core%20safety%20capabilities%20rather%20than%20post-hoc%20protective%20mechanisms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Context%253A%2520Large%2520Language%2520Models%2520Failure%2520to%2520Grasp%2520Users%2520Intent%26entry.906535625%3DAhmed%2520M.%2520Hussain%2520and%2520Salahuddin%2520Salahuddin%2520and%2520Panos%2520Papadimitratos%26entry.1292438233%3DCurrent%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520safety%2520approaches%2520focus%2520on%2520explicitly%2520harmful%2520content%2520while%2520overlooking%2520a%2520critical%2520vulnerability%253A%2520the%2520inability%2520to%2520understand%2520context%2520and%2520recognize%2520user%2520intent.%2520This%2520creates%2520exploitable%2520vulnerabilities%2520that%2520malicious%2520users%2520can%2520systematically%2520leverage%2520to%2520circumvent%2520safety%2520mechanisms.%2520We%2520empirically%2520evaluate%2520multiple%2520state-of-the-art%2520LLMs%252C%2520including%2520ChatGPT%252C%2520Claude%252C%2520Gemini%252C%2520and%2520DeepSeek.%2520Our%2520analysis%2520demonstrates%2520the%2520circumvention%2520of%2520reliable%2520safety%2520mechanisms%2520through%2520emotional%2520framing%252C%2520progressive%2520revelation%252C%2520and%2520academic%2520justification%2520techniques.%2520Notably%252C%2520reasoning-enabled%2520configurations%2520amplified%2520rather%2520than%2520mitigated%2520the%2520effectiveness%2520of%2520exploitation%252C%2520increasing%2520factual%2520precision%2520while%2520failing%2520to%2520interrogate%2520the%2520underlying%2520intent.%2520The%2520exception%2520was%2520Claude%2520Opus%25204.1%252C%2520which%2520prioritized%2520intent%2520detection%2520over%2520information%2520provision%2520in%2520some%2520use%2520cases.%2520This%2520pattern%2520reveals%2520that%2520current%2520architectural%2520designs%2520create%2520systematic%2520vulnerabilities.%2520These%2520limitations%2520require%2520paradigmatic%2520shifts%2520toward%2520contextual%2520understanding%2520and%2520intent%2520recognition%2520as%2520core%2520safety%2520capabilities%2520rather%2520than%2520post-hoc%2520protective%2520mechanisms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Context%3A%20Large%20Language%20Models%20Failure%20to%20Grasp%20Users%20Intent&entry.906535625=Ahmed%20M.%20Hussain%20and%20Salahuddin%20Salahuddin%20and%20Panos%20Papadimitratos&entry.1292438233=Current%20Large%20Language%20Models%20%28LLMs%29%20safety%20approaches%20focus%20on%20explicitly%20harmful%20content%20while%20overlooking%20a%20critical%20vulnerability%3A%20the%20inability%20to%20understand%20context%20and%20recognize%20user%20intent.%20This%20creates%20exploitable%20vulnerabilities%20that%20malicious%20users%20can%20systematically%20leverage%20to%20circumvent%20safety%20mechanisms.%20We%20empirically%20evaluate%20multiple%20state-of-the-art%20LLMs%2C%20including%20ChatGPT%2C%20Claude%2C%20Gemini%2C%20and%20DeepSeek.%20Our%20analysis%20demonstrates%20the%20circumvention%20of%20reliable%20safety%20mechanisms%20through%20emotional%20framing%2C%20progressive%20revelation%2C%20and%20academic%20justification%20techniques.%20Notably%2C%20reasoning-enabled%20configurations%20amplified%20rather%20than%20mitigated%20the%20effectiveness%20of%20exploitation%2C%20increasing%20factual%20precision%20while%20failing%20to%20interrogate%20the%20underlying%20intent.%20The%20exception%20was%20Claude%20Opus%204.1%2C%20which%20prioritized%20intent%20detection%20over%20information%20provision%20in%20some%20use%20cases.%20This%20pattern%20reveals%20that%20current%20architectural%20designs%20create%20systematic%20vulnerabilities.%20These%20limitations%20require%20paradigmatic%20shifts%20toward%20contextual%20understanding%20and%20intent%20recognition%20as%20core%20safety%20capabilities%20rather%20than%20post-hoc%20protective%20mechanisms.&entry.1838667208=http%3A//arxiv.org/abs/2512.21110v2&entry.124074799=Read"},
{"title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs", "author": "Sahil Kale and Antonio Luca Alfeo", "abstract": "Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.", "link": "http://arxiv.org/abs/2512.23547v1", "date": "2025-12-29", "relevancy": 2.0286, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5117}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5064}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lie%20to%20Me%3A%20Knowledge%20Graphs%20for%20Robust%20Hallucination%20Self-Detection%20in%20LLMs&body=Title%3A%20Lie%20to%20Me%3A%20Knowledge%20Graphs%20for%20Robust%20Hallucination%20Self-Detection%20in%20LLMs%0AAuthor%3A%20Sahil%20Kale%20and%20Antonio%20Luca%20Alfeo%0AAbstract%3A%20Hallucinations%2C%20the%20generation%20of%20apparently%20convincing%20yet%20false%20statements%2C%20remain%20a%20major%20barrier%20to%20the%20safe%20deployment%20of%20LLMs.%20Building%20on%20the%20strong%20performance%20of%20self-detection%20methods%2C%20we%20examine%20the%20use%20of%20structured%20knowledge%20representations%2C%20namely%20knowledge%20graphs%2C%20to%20improve%20hallucination%20self-detection.%20Specifically%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20that%20enriches%20hallucination%20self-detection%20by%20%28i%29%20converting%20LLM%20responses%20into%20knowledge%20graphs%20of%20entities%20and%20relations%2C%20and%20%28ii%29%20using%20these%20graphs%20to%20estimate%20the%20likelihood%20that%20a%20response%20contains%20hallucinations.%20We%20evaluate%20the%20proposed%20approach%20using%20two%20widely%20used%20LLMs%2C%20GPT-4o%20and%20Gemini-2.5-Flash%2C%20across%20two%20hallucination%20detection%20datasets.%20To%20support%20more%20reliable%20future%20benchmarking%2C%20one%20of%20these%20datasets%20has%20been%20manually%20curated%20and%20enhanced%20and%20is%20released%20as%20a%20secondary%20outcome%20of%20this%20work.%20Compared%20to%20standard%20self-detection%20methods%20and%20SelfCheckGPT%2C%20a%20state-of-the-art%20approach%2C%20our%20method%20achieves%20up%20to%2016%25%20relative%20improvement%20in%20accuracy%20and%2020%25%20in%20F1-score.%20Our%20results%20show%20that%20LLMs%20can%20better%20analyse%20atomic%20facts%20when%20they%20are%20structured%20as%20knowledge%20graphs%2C%20even%20when%20initial%20outputs%20contain%20inaccuracies.%20This%20low-cost%2C%20model-agnostic%20approach%20paves%20the%20way%20toward%20safer%20and%20more%20trustworthy%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLie%2520to%2520Me%253A%2520Knowledge%2520Graphs%2520for%2520Robust%2520Hallucination%2520Self-Detection%2520in%2520LLMs%26entry.906535625%3DSahil%2520Kale%2520and%2520Antonio%2520Luca%2520Alfeo%26entry.1292438233%3DHallucinations%252C%2520the%2520generation%2520of%2520apparently%2520convincing%2520yet%2520false%2520statements%252C%2520remain%2520a%2520major%2520barrier%2520to%2520the%2520safe%2520deployment%2520of%2520LLMs.%2520Building%2520on%2520the%2520strong%2520performance%2520of%2520self-detection%2520methods%252C%2520we%2520examine%2520the%2520use%2520of%2520structured%2520knowledge%2520representations%252C%2520namely%2520knowledge%2520graphs%252C%2520to%2520improve%2520hallucination%2520self-detection.%2520Specifically%252C%2520we%2520propose%2520a%2520simple%2520yet%2520powerful%2520approach%2520that%2520enriches%2520hallucination%2520self-detection%2520by%2520%2528i%2529%2520converting%2520LLM%2520responses%2520into%2520knowledge%2520graphs%2520of%2520entities%2520and%2520relations%252C%2520and%2520%2528ii%2529%2520using%2520these%2520graphs%2520to%2520estimate%2520the%2520likelihood%2520that%2520a%2520response%2520contains%2520hallucinations.%2520We%2520evaluate%2520the%2520proposed%2520approach%2520using%2520two%2520widely%2520used%2520LLMs%252C%2520GPT-4o%2520and%2520Gemini-2.5-Flash%252C%2520across%2520two%2520hallucination%2520detection%2520datasets.%2520To%2520support%2520more%2520reliable%2520future%2520benchmarking%252C%2520one%2520of%2520these%2520datasets%2520has%2520been%2520manually%2520curated%2520and%2520enhanced%2520and%2520is%2520released%2520as%2520a%2520secondary%2520outcome%2520of%2520this%2520work.%2520Compared%2520to%2520standard%2520self-detection%2520methods%2520and%2520SelfCheckGPT%252C%2520a%2520state-of-the-art%2520approach%252C%2520our%2520method%2520achieves%2520up%2520to%252016%2525%2520relative%2520improvement%2520in%2520accuracy%2520and%252020%2525%2520in%2520F1-score.%2520Our%2520results%2520show%2520that%2520LLMs%2520can%2520better%2520analyse%2520atomic%2520facts%2520when%2520they%2520are%2520structured%2520as%2520knowledge%2520graphs%252C%2520even%2520when%2520initial%2520outputs%2520contain%2520inaccuracies.%2520This%2520low-cost%252C%2520model-agnostic%2520approach%2520paves%2520the%2520way%2520toward%2520safer%2520and%2520more%2520trustworthy%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lie%20to%20Me%3A%20Knowledge%20Graphs%20for%20Robust%20Hallucination%20Self-Detection%20in%20LLMs&entry.906535625=Sahil%20Kale%20and%20Antonio%20Luca%20Alfeo&entry.1292438233=Hallucinations%2C%20the%20generation%20of%20apparently%20convincing%20yet%20false%20statements%2C%20remain%20a%20major%20barrier%20to%20the%20safe%20deployment%20of%20LLMs.%20Building%20on%20the%20strong%20performance%20of%20self-detection%20methods%2C%20we%20examine%20the%20use%20of%20structured%20knowledge%20representations%2C%20namely%20knowledge%20graphs%2C%20to%20improve%20hallucination%20self-detection.%20Specifically%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20that%20enriches%20hallucination%20self-detection%20by%20%28i%29%20converting%20LLM%20responses%20into%20knowledge%20graphs%20of%20entities%20and%20relations%2C%20and%20%28ii%29%20using%20these%20graphs%20to%20estimate%20the%20likelihood%20that%20a%20response%20contains%20hallucinations.%20We%20evaluate%20the%20proposed%20approach%20using%20two%20widely%20used%20LLMs%2C%20GPT-4o%20and%20Gemini-2.5-Flash%2C%20across%20two%20hallucination%20detection%20datasets.%20To%20support%20more%20reliable%20future%20benchmarking%2C%20one%20of%20these%20datasets%20has%20been%20manually%20curated%20and%20enhanced%20and%20is%20released%20as%20a%20secondary%20outcome%20of%20this%20work.%20Compared%20to%20standard%20self-detection%20methods%20and%20SelfCheckGPT%2C%20a%20state-of-the-art%20approach%2C%20our%20method%20achieves%20up%20to%2016%25%20relative%20improvement%20in%20accuracy%20and%2020%25%20in%20F1-score.%20Our%20results%20show%20that%20LLMs%20can%20better%20analyse%20atomic%20facts%20when%20they%20are%20structured%20as%20knowledge%20graphs%2C%20even%20when%20initial%20outputs%20contain%20inaccuracies.%20This%20low-cost%2C%20model-agnostic%20approach%20paves%20the%20way%20toward%20safer%20and%20more%20trustworthy%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.23547v1&entry.124074799=Read"},
{"title": "Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels", "author": "Alireza Sedighi Moghaddam and Mohammad Reza Mohammadi", "abstract": "Labeled data is a fundamental component in training supervised deep learning models for computer vision tasks. However, the labeling process, especially for ordinal image classification where class boundaries are often ambiguous, is prone to error and noise. Such label noise can significantly degrade the performance and reliability of machine learning models. This paper addresses the problem of detecting and correcting label noise in ordinal image classification tasks. To this end, a novel data-centric method called ORDinal Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy labels. The proposed approach leverages the capabilities of Label Distribution Learning (LDL) to model the inherent ambiguity and uncertainty present in ordinal labels. During training, ORDAC dynamically adjusts the mean and standard deviation of the label distribution for each sample. Rather than discarding potentially noisy samples, this approach aims to correct them and make optimal use of the entire training dataset. The effectiveness of the proposed method is evaluated on benchmark datasets for age estimation (Adience) and disease severity detection (Diabetic Retinopathy) under various asymmetric Gaussian noise scenarios. Results show that ORDAC and its extended versions (ORDAC_C and ORDAC_R) lead to significant improvements in model performance. For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to 0.49. The method also demonstrated its effectiveness in correcting intrinsic noise present in the original datasets. This research indicates that adaptive label correction using label distributions is an effective strategy to enhance the robustness and accuracy of ordinal classification models in the presence of noisy data.", "link": "http://arxiv.org/abs/2509.02351v2", "date": "2025-12-29", "relevancy": 2.0268, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5039}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ordinal%20Adaptive%20Correction%3A%20A%20Data-Centric%20Approach%20to%20Ordinal%20Image%20Classification%20with%20Noisy%20Labels&body=Title%3A%20Ordinal%20Adaptive%20Correction%3A%20A%20Data-Centric%20Approach%20to%20Ordinal%20Image%20Classification%20with%20Noisy%20Labels%0AAuthor%3A%20Alireza%20Sedighi%20Moghaddam%20and%20Mohammad%20Reza%20Mohammadi%0AAbstract%3A%20Labeled%20data%20is%20a%20fundamental%20component%20in%20training%20supervised%20deep%20learning%20models%20for%20computer%20vision%20tasks.%20However%2C%20the%20labeling%20process%2C%20especially%20for%20ordinal%20image%20classification%20where%20class%20boundaries%20are%20often%20ambiguous%2C%20is%20prone%20to%20error%20and%20noise.%20Such%20label%20noise%20can%20significantly%20degrade%20the%20performance%20and%20reliability%20of%20machine%20learning%20models.%20This%20paper%20addresses%20the%20problem%20of%20detecting%20and%20correcting%20label%20noise%20in%20ordinal%20image%20classification%20tasks.%20To%20this%20end%2C%20a%20novel%20data-centric%20method%20called%20ORDinal%20Adaptive%20Correction%20%28ORDAC%29%20is%20proposed%20for%20adaptive%20correction%20of%20noisy%20labels.%20The%20proposed%20approach%20leverages%20the%20capabilities%20of%20Label%20Distribution%20Learning%20%28LDL%29%20to%20model%20the%20inherent%20ambiguity%20and%20uncertainty%20present%20in%20ordinal%20labels.%20During%20training%2C%20ORDAC%20dynamically%20adjusts%20the%20mean%20and%20standard%20deviation%20of%20the%20label%20distribution%20for%20each%20sample.%20Rather%20than%20discarding%20potentially%20noisy%20samples%2C%20this%20approach%20aims%20to%20correct%20them%20and%20make%20optimal%20use%20of%20the%20entire%20training%20dataset.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20evaluated%20on%20benchmark%20datasets%20for%20age%20estimation%20%28Adience%29%20and%20disease%20severity%20detection%20%28Diabetic%20Retinopathy%29%20under%20various%20asymmetric%20Gaussian%20noise%20scenarios.%20Results%20show%20that%20ORDAC%20and%20its%20extended%20versions%20%28ORDAC_C%20and%20ORDAC_R%29%20lead%20to%20significant%20improvements%20in%20model%20performance.%20For%20instance%2C%20on%20the%20Adience%20dataset%20with%2040%25%20noise%2C%20ORDAC_R%20reduced%20the%20mean%20absolute%20error%20from%200.86%20to%200.62%20and%20increased%20the%20recall%20metric%20from%200.37%20to%200.49.%20The%20method%20also%20demonstrated%20its%20effectiveness%20in%20correcting%20intrinsic%20noise%20present%20in%20the%20original%20datasets.%20This%20research%20indicates%20that%20adaptive%20label%20correction%20using%20label%20distributions%20is%20an%20effective%20strategy%20to%20enhance%20the%20robustness%20and%20accuracy%20of%20ordinal%20classification%20models%20in%20the%20presence%20of%20noisy%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2509.02351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrdinal%2520Adaptive%2520Correction%253A%2520A%2520Data-Centric%2520Approach%2520to%2520Ordinal%2520Image%2520Classification%2520with%2520Noisy%2520Labels%26entry.906535625%3DAlireza%2520Sedighi%2520Moghaddam%2520and%2520Mohammad%2520Reza%2520Mohammadi%26entry.1292438233%3DLabeled%2520data%2520is%2520a%2520fundamental%2520component%2520in%2520training%2520supervised%2520deep%2520learning%2520models%2520for%2520computer%2520vision%2520tasks.%2520However%252C%2520the%2520labeling%2520process%252C%2520especially%2520for%2520ordinal%2520image%2520classification%2520where%2520class%2520boundaries%2520are%2520often%2520ambiguous%252C%2520is%2520prone%2520to%2520error%2520and%2520noise.%2520Such%2520label%2520noise%2520can%2520significantly%2520degrade%2520the%2520performance%2520and%2520reliability%2520of%2520machine%2520learning%2520models.%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520detecting%2520and%2520correcting%2520label%2520noise%2520in%2520ordinal%2520image%2520classification%2520tasks.%2520To%2520this%2520end%252C%2520a%2520novel%2520data-centric%2520method%2520called%2520ORDinal%2520Adaptive%2520Correction%2520%2528ORDAC%2529%2520is%2520proposed%2520for%2520adaptive%2520correction%2520of%2520noisy%2520labels.%2520The%2520proposed%2520approach%2520leverages%2520the%2520capabilities%2520of%2520Label%2520Distribution%2520Learning%2520%2528LDL%2529%2520to%2520model%2520the%2520inherent%2520ambiguity%2520and%2520uncertainty%2520present%2520in%2520ordinal%2520labels.%2520During%2520training%252C%2520ORDAC%2520dynamically%2520adjusts%2520the%2520mean%2520and%2520standard%2520deviation%2520of%2520the%2520label%2520distribution%2520for%2520each%2520sample.%2520Rather%2520than%2520discarding%2520potentially%2520noisy%2520samples%252C%2520this%2520approach%2520aims%2520to%2520correct%2520them%2520and%2520make%2520optimal%2520use%2520of%2520the%2520entire%2520training%2520dataset.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%2520is%2520evaluated%2520on%2520benchmark%2520datasets%2520for%2520age%2520estimation%2520%2528Adience%2529%2520and%2520disease%2520severity%2520detection%2520%2528Diabetic%2520Retinopathy%2529%2520under%2520various%2520asymmetric%2520Gaussian%2520noise%2520scenarios.%2520Results%2520show%2520that%2520ORDAC%2520and%2520its%2520extended%2520versions%2520%2528ORDAC_C%2520and%2520ORDAC_R%2529%2520lead%2520to%2520significant%2520improvements%2520in%2520model%2520performance.%2520For%2520instance%252C%2520on%2520the%2520Adience%2520dataset%2520with%252040%2525%2520noise%252C%2520ORDAC_R%2520reduced%2520the%2520mean%2520absolute%2520error%2520from%25200.86%2520to%25200.62%2520and%2520increased%2520the%2520recall%2520metric%2520from%25200.37%2520to%25200.49.%2520The%2520method%2520also%2520demonstrated%2520its%2520effectiveness%2520in%2520correcting%2520intrinsic%2520noise%2520present%2520in%2520the%2520original%2520datasets.%2520This%2520research%2520indicates%2520that%2520adaptive%2520label%2520correction%2520using%2520label%2520distributions%2520is%2520an%2520effective%2520strategy%2520to%2520enhance%2520the%2520robustness%2520and%2520accuracy%2520of%2520ordinal%2520classification%2520models%2520in%2520the%2520presence%2520of%2520noisy%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ordinal%20Adaptive%20Correction%3A%20A%20Data-Centric%20Approach%20to%20Ordinal%20Image%20Classification%20with%20Noisy%20Labels&entry.906535625=Alireza%20Sedighi%20Moghaddam%20and%20Mohammad%20Reza%20Mohammadi&entry.1292438233=Labeled%20data%20is%20a%20fundamental%20component%20in%20training%20supervised%20deep%20learning%20models%20for%20computer%20vision%20tasks.%20However%2C%20the%20labeling%20process%2C%20especially%20for%20ordinal%20image%20classification%20where%20class%20boundaries%20are%20often%20ambiguous%2C%20is%20prone%20to%20error%20and%20noise.%20Such%20label%20noise%20can%20significantly%20degrade%20the%20performance%20and%20reliability%20of%20machine%20learning%20models.%20This%20paper%20addresses%20the%20problem%20of%20detecting%20and%20correcting%20label%20noise%20in%20ordinal%20image%20classification%20tasks.%20To%20this%20end%2C%20a%20novel%20data-centric%20method%20called%20ORDinal%20Adaptive%20Correction%20%28ORDAC%29%20is%20proposed%20for%20adaptive%20correction%20of%20noisy%20labels.%20The%20proposed%20approach%20leverages%20the%20capabilities%20of%20Label%20Distribution%20Learning%20%28LDL%29%20to%20model%20the%20inherent%20ambiguity%20and%20uncertainty%20present%20in%20ordinal%20labels.%20During%20training%2C%20ORDAC%20dynamically%20adjusts%20the%20mean%20and%20standard%20deviation%20of%20the%20label%20distribution%20for%20each%20sample.%20Rather%20than%20discarding%20potentially%20noisy%20samples%2C%20this%20approach%20aims%20to%20correct%20them%20and%20make%20optimal%20use%20of%20the%20entire%20training%20dataset.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20evaluated%20on%20benchmark%20datasets%20for%20age%20estimation%20%28Adience%29%20and%20disease%20severity%20detection%20%28Diabetic%20Retinopathy%29%20under%20various%20asymmetric%20Gaussian%20noise%20scenarios.%20Results%20show%20that%20ORDAC%20and%20its%20extended%20versions%20%28ORDAC_C%20and%20ORDAC_R%29%20lead%20to%20significant%20improvements%20in%20model%20performance.%20For%20instance%2C%20on%20the%20Adience%20dataset%20with%2040%25%20noise%2C%20ORDAC_R%20reduced%20the%20mean%20absolute%20error%20from%200.86%20to%200.62%20and%20increased%20the%20recall%20metric%20from%200.37%20to%200.49.%20The%20method%20also%20demonstrated%20its%20effectiveness%20in%20correcting%20intrinsic%20noise%20present%20in%20the%20original%20datasets.%20This%20research%20indicates%20that%20adaptive%20label%20correction%20using%20label%20distributions%20is%20an%20effective%20strategy%20to%20enhance%20the%20robustness%20and%20accuracy%20of%20ordinal%20classification%20models%20in%20the%20presence%20of%20noisy%20data.&entry.1838667208=http%3A//arxiv.org/abs/2509.02351v2&entry.124074799=Read"},
{"title": "Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks", "author": "Yusuf Kalyoncuoglu", "abstract": "While it is well-established that the weight matrices and feature manifolds of deep neural networks exhibit a low Intrinsic Dimension (ID), current state-of-the-art models still rely on massive high-dimensional widths. This redundancy is not required for representation, but is strictly necessary to solve the non-convex optimization search problem-finding a global minimum, which remains intractable for compact networks. In this work, we propose a constructive approach to bypass this optimization bottleneck. By decoupling the solution geometry from the ambient search space, we empirically demonstrate across ResNet-50, ViT, and BERT that the classification head can be compressed by even huge factors of 16 with negligible performance degradation. This motivates Subspace-Native Distillation as a novel paradigm: by defining the target directly in this constructed subspace, we provide a stable geometric coordinate system for student models, potentially allowing them to circumvent the high-dimensional search problem entirely and realize the vision of Train Big, Deploy Small.", "link": "http://arxiv.org/abs/2512.23410v1", "date": "2025-12-29", "relevancy": 2.0228, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5183}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4979}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directly%20Constructing%20Low-Dimensional%20Solution%20Subspaces%20in%20Deep%20Neural%20Networks&body=Title%3A%20Directly%20Constructing%20Low-Dimensional%20Solution%20Subspaces%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Yusuf%20Kalyoncuoglu%0AAbstract%3A%20While%20it%20is%20well-established%20that%20the%20weight%20matrices%20and%20feature%20manifolds%20of%20deep%20neural%20networks%20exhibit%20a%20low%20Intrinsic%20Dimension%20%28ID%29%2C%20current%20state-of-the-art%20models%20still%20rely%20on%20massive%20high-dimensional%20widths.%20This%20redundancy%20is%20not%20required%20for%20representation%2C%20but%20is%20strictly%20necessary%20to%20solve%20the%20non-convex%20optimization%20search%20problem-finding%20a%20global%20minimum%2C%20which%20remains%20intractable%20for%20compact%20networks.%20In%20this%20work%2C%20we%20propose%20a%20constructive%20approach%20to%20bypass%20this%20optimization%20bottleneck.%20By%20decoupling%20the%20solution%20geometry%20from%20the%20ambient%20search%20space%2C%20we%20empirically%20demonstrate%20across%20ResNet-50%2C%20ViT%2C%20and%20BERT%20that%20the%20classification%20head%20can%20be%20compressed%20by%20even%20huge%20factors%20of%2016%20with%20negligible%20performance%20degradation.%20This%20motivates%20Subspace-Native%20Distillation%20as%20a%20novel%20paradigm%3A%20by%20defining%20the%20target%20directly%20in%20this%20constructed%20subspace%2C%20we%20provide%20a%20stable%20geometric%20coordinate%20system%20for%20student%20models%2C%20potentially%20allowing%20them%20to%20circumvent%20the%20high-dimensional%20search%20problem%20entirely%20and%20realize%20the%20vision%20of%20Train%20Big%2C%20Deploy%20Small.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirectly%2520Constructing%2520Low-Dimensional%2520Solution%2520Subspaces%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DYusuf%2520Kalyoncuoglu%26entry.1292438233%3DWhile%2520it%2520is%2520well-established%2520that%2520the%2520weight%2520matrices%2520and%2520feature%2520manifolds%2520of%2520deep%2520neural%2520networks%2520exhibit%2520a%2520low%2520Intrinsic%2520Dimension%2520%2528ID%2529%252C%2520current%2520state-of-the-art%2520models%2520still%2520rely%2520on%2520massive%2520high-dimensional%2520widths.%2520This%2520redundancy%2520is%2520not%2520required%2520for%2520representation%252C%2520but%2520is%2520strictly%2520necessary%2520to%2520solve%2520the%2520non-convex%2520optimization%2520search%2520problem-finding%2520a%2520global%2520minimum%252C%2520which%2520remains%2520intractable%2520for%2520compact%2520networks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520constructive%2520approach%2520to%2520bypass%2520this%2520optimization%2520bottleneck.%2520By%2520decoupling%2520the%2520solution%2520geometry%2520from%2520the%2520ambient%2520search%2520space%252C%2520we%2520empirically%2520demonstrate%2520across%2520ResNet-50%252C%2520ViT%252C%2520and%2520BERT%2520that%2520the%2520classification%2520head%2520can%2520be%2520compressed%2520by%2520even%2520huge%2520factors%2520of%252016%2520with%2520negligible%2520performance%2520degradation.%2520This%2520motivates%2520Subspace-Native%2520Distillation%2520as%2520a%2520novel%2520paradigm%253A%2520by%2520defining%2520the%2520target%2520directly%2520in%2520this%2520constructed%2520subspace%252C%2520we%2520provide%2520a%2520stable%2520geometric%2520coordinate%2520system%2520for%2520student%2520models%252C%2520potentially%2520allowing%2520them%2520to%2520circumvent%2520the%2520high-dimensional%2520search%2520problem%2520entirely%2520and%2520realize%2520the%2520vision%2520of%2520Train%2520Big%252C%2520Deploy%2520Small.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directly%20Constructing%20Low-Dimensional%20Solution%20Subspaces%20in%20Deep%20Neural%20Networks&entry.906535625=Yusuf%20Kalyoncuoglu&entry.1292438233=While%20it%20is%20well-established%20that%20the%20weight%20matrices%20and%20feature%20manifolds%20of%20deep%20neural%20networks%20exhibit%20a%20low%20Intrinsic%20Dimension%20%28ID%29%2C%20current%20state-of-the-art%20models%20still%20rely%20on%20massive%20high-dimensional%20widths.%20This%20redundancy%20is%20not%20required%20for%20representation%2C%20but%20is%20strictly%20necessary%20to%20solve%20the%20non-convex%20optimization%20search%20problem-finding%20a%20global%20minimum%2C%20which%20remains%20intractable%20for%20compact%20networks.%20In%20this%20work%2C%20we%20propose%20a%20constructive%20approach%20to%20bypass%20this%20optimization%20bottleneck.%20By%20decoupling%20the%20solution%20geometry%20from%20the%20ambient%20search%20space%2C%20we%20empirically%20demonstrate%20across%20ResNet-50%2C%20ViT%2C%20and%20BERT%20that%20the%20classification%20head%20can%20be%20compressed%20by%20even%20huge%20factors%20of%2016%20with%20negligible%20performance%20degradation.%20This%20motivates%20Subspace-Native%20Distillation%20as%20a%20novel%20paradigm%3A%20by%20defining%20the%20target%20directly%20in%20this%20constructed%20subspace%2C%20we%20provide%20a%20stable%20geometric%20coordinate%20system%20for%20student%20models%2C%20potentially%20allowing%20them%20to%20circumvent%20the%20high-dimensional%20search%20problem%20entirely%20and%20realize%20the%20vision%20of%20Train%20Big%2C%20Deploy%20Small.&entry.1838667208=http%3A//arxiv.org/abs/2512.23410v1&entry.124074799=Read"},
{"title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing", "author": "Zhehao Huang and Baijiong Lin and Jingyuan Zhang and Jingying Wang and Yuhang Liu and Ning Lu and Tao Li and Xiaolin Huang", "abstract": "Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.", "link": "http://arxiv.org/abs/2512.23562v1", "date": "2025-12-29", "relevancy": 2.0187, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-RouterBench%3A%20A%20Benchmark%20for%20Vision-Language%20Model%20Routing&body=Title%3A%20VL-RouterBench%3A%20A%20Benchmark%20for%20Vision-Language%20Model%20Routing%0AAuthor%3A%20Zhehao%20Huang%20and%20Baijiong%20Lin%20and%20Jingyuan%20Zhang%20and%20Jingying%20Wang%20and%20Yuhang%20Liu%20and%20Ning%20Lu%20and%20Tao%20Li%20and%20Xiaolin%20Huang%0AAbstract%3A%20Multi-model%20routing%20has%20evolved%20from%20an%20engineering%20technique%20into%20essential%20infrastructure%2C%20yet%20existing%20work%20lacks%20a%20systematic%2C%20reproducible%20benchmark%20for%20evaluating%20vision-language%20models%20%28VLMs%29.%20We%20present%20VL-RouterBench%20to%20assess%20the%20overall%20capability%20of%20VLM%20routing%20systems%20systematically.%20The%20benchmark%20is%20grounded%20in%20raw%20inference%20and%20scoring%20logs%20from%20VLMs%20and%20constructs%20quality%20and%20cost%20matrices%20over%20sample-model%20pairs.%20In%20scale%2C%20VL-RouterBench%20covers%2014%20datasets%20across%203%20task%20groups%2C%20totaling%2030%2C540%20samples%2C%20and%20includes%2015%20open-source%20models%20and%202%20API%20models%2C%20yielding%20519%2C180%20sample-model%20pairs%20and%20a%20total%20input-output%20token%20volume%20of%2034%2C494%2C977.%20The%20evaluation%20protocol%20jointly%20measures%20average%20accuracy%2C%20average%20cost%2C%20and%20throughput%2C%20and%20builds%20a%20ranking%20score%20from%20the%20harmonic%20mean%20of%20normalized%20cost%20and%20accuracy%20to%20enable%20comparison%20across%20router%20configurations%20and%20cost%20budgets.%20On%20this%20benchmark%2C%20we%20evaluate%2010%20routing%20methods%20and%20baselines%20and%20observe%20a%20significant%20routability%20gain%2C%20while%20the%20best%20current%20routers%20still%20show%20a%20clear%20gap%20to%20the%20ideal%20Oracle%2C%20indicating%20considerable%20room%20for%20improvement%20in%20router%20architecture%20through%20finer%20visual%20cues%20and%20modeling%20of%20textual%20structure.%20We%20will%20open-source%20the%20complete%20data%20construction%20and%20evaluation%20toolchain%20to%20promote%20comparability%2C%20reproducibility%2C%20and%20practical%20deployment%20in%20multimodal%20routing%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-RouterBench%253A%2520A%2520Benchmark%2520for%2520Vision-Language%2520Model%2520Routing%26entry.906535625%3DZhehao%2520Huang%2520and%2520Baijiong%2520Lin%2520and%2520Jingyuan%2520Zhang%2520and%2520Jingying%2520Wang%2520and%2520Yuhang%2520Liu%2520and%2520Ning%2520Lu%2520and%2520Tao%2520Li%2520and%2520Xiaolin%2520Huang%26entry.1292438233%3DMulti-model%2520routing%2520has%2520evolved%2520from%2520an%2520engineering%2520technique%2520into%2520essential%2520infrastructure%252C%2520yet%2520existing%2520work%2520lacks%2520a%2520systematic%252C%2520reproducible%2520benchmark%2520for%2520evaluating%2520vision-language%2520models%2520%2528VLMs%2529.%2520We%2520present%2520VL-RouterBench%2520to%2520assess%2520the%2520overall%2520capability%2520of%2520VLM%2520routing%2520systems%2520systematically.%2520The%2520benchmark%2520is%2520grounded%2520in%2520raw%2520inference%2520and%2520scoring%2520logs%2520from%2520VLMs%2520and%2520constructs%2520quality%2520and%2520cost%2520matrices%2520over%2520sample-model%2520pairs.%2520In%2520scale%252C%2520VL-RouterBench%2520covers%252014%2520datasets%2520across%25203%2520task%2520groups%252C%2520totaling%252030%252C540%2520samples%252C%2520and%2520includes%252015%2520open-source%2520models%2520and%25202%2520API%2520models%252C%2520yielding%2520519%252C180%2520sample-model%2520pairs%2520and%2520a%2520total%2520input-output%2520token%2520volume%2520of%252034%252C494%252C977.%2520The%2520evaluation%2520protocol%2520jointly%2520measures%2520average%2520accuracy%252C%2520average%2520cost%252C%2520and%2520throughput%252C%2520and%2520builds%2520a%2520ranking%2520score%2520from%2520the%2520harmonic%2520mean%2520of%2520normalized%2520cost%2520and%2520accuracy%2520to%2520enable%2520comparison%2520across%2520router%2520configurations%2520and%2520cost%2520budgets.%2520On%2520this%2520benchmark%252C%2520we%2520evaluate%252010%2520routing%2520methods%2520and%2520baselines%2520and%2520observe%2520a%2520significant%2520routability%2520gain%252C%2520while%2520the%2520best%2520current%2520routers%2520still%2520show%2520a%2520clear%2520gap%2520to%2520the%2520ideal%2520Oracle%252C%2520indicating%2520considerable%2520room%2520for%2520improvement%2520in%2520router%2520architecture%2520through%2520finer%2520visual%2520cues%2520and%2520modeling%2520of%2520textual%2520structure.%2520We%2520will%2520open-source%2520the%2520complete%2520data%2520construction%2520and%2520evaluation%2520toolchain%2520to%2520promote%2520comparability%252C%2520reproducibility%252C%2520and%2520practical%2520deployment%2520in%2520multimodal%2520routing%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-RouterBench%3A%20A%20Benchmark%20for%20Vision-Language%20Model%20Routing&entry.906535625=Zhehao%20Huang%20and%20Baijiong%20Lin%20and%20Jingyuan%20Zhang%20and%20Jingying%20Wang%20and%20Yuhang%20Liu%20and%20Ning%20Lu%20and%20Tao%20Li%20and%20Xiaolin%20Huang&entry.1292438233=Multi-model%20routing%20has%20evolved%20from%20an%20engineering%20technique%20into%20essential%20infrastructure%2C%20yet%20existing%20work%20lacks%20a%20systematic%2C%20reproducible%20benchmark%20for%20evaluating%20vision-language%20models%20%28VLMs%29.%20We%20present%20VL-RouterBench%20to%20assess%20the%20overall%20capability%20of%20VLM%20routing%20systems%20systematically.%20The%20benchmark%20is%20grounded%20in%20raw%20inference%20and%20scoring%20logs%20from%20VLMs%20and%20constructs%20quality%20and%20cost%20matrices%20over%20sample-model%20pairs.%20In%20scale%2C%20VL-RouterBench%20covers%2014%20datasets%20across%203%20task%20groups%2C%20totaling%2030%2C540%20samples%2C%20and%20includes%2015%20open-source%20models%20and%202%20API%20models%2C%20yielding%20519%2C180%20sample-model%20pairs%20and%20a%20total%20input-output%20token%20volume%20of%2034%2C494%2C977.%20The%20evaluation%20protocol%20jointly%20measures%20average%20accuracy%2C%20average%20cost%2C%20and%20throughput%2C%20and%20builds%20a%20ranking%20score%20from%20the%20harmonic%20mean%20of%20normalized%20cost%20and%20accuracy%20to%20enable%20comparison%20across%20router%20configurations%20and%20cost%20budgets.%20On%20this%20benchmark%2C%20we%20evaluate%2010%20routing%20methods%20and%20baselines%20and%20observe%20a%20significant%20routability%20gain%2C%20while%20the%20best%20current%20routers%20still%20show%20a%20clear%20gap%20to%20the%20ideal%20Oracle%2C%20indicating%20considerable%20room%20for%20improvement%20in%20router%20architecture%20through%20finer%20visual%20cues%20and%20modeling%20of%20textual%20structure.%20We%20will%20open-source%20the%20complete%20data%20construction%20and%20evaluation%20toolchain%20to%20promote%20comparability%2C%20reproducibility%2C%20and%20practical%20deployment%20in%20multimodal%20routing%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.23562v1&entry.124074799=Read"},
{"title": "Training AI Co-Scientists Using Rubric Rewards", "author": "Shashwat Goel and Rishi Hazra and Dulhan Jayalath and Timon Willi and Parag Jain and William F. Shen and Ilias Leontiadis and Francesco Barbieri and Yoram Bachrach and Jonas Geiping and Chenxi Whitehouse", "abstract": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.", "link": "http://arxiv.org/abs/2512.23707v1", "date": "2025-12-29", "relevancy": 2.013, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5096}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5043}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20AI%20Co-Scientists%20Using%20Rubric%20Rewards&body=Title%3A%20Training%20AI%20Co-Scientists%20Using%20Rubric%20Rewards%0AAuthor%3A%20Shashwat%20Goel%20and%20Rishi%20Hazra%20and%20Dulhan%20Jayalath%20and%20Timon%20Willi%20and%20Parag%20Jain%20and%20William%20F.%20Shen%20and%20Ilias%20Leontiadis%20and%20Francesco%20Barbieri%20and%20Yoram%20Bachrach%20and%20Jonas%20Geiping%20and%20Chenxi%20Whitehouse%0AAbstract%3A%20AI%20co-scientists%20are%20emerging%20as%20a%20tool%20to%20assist%20human%20researchers%20in%20achieving%20their%20research%20goals.%20A%20crucial%20feature%20of%20these%20AI%20co-scientists%20is%20the%20ability%20to%20generate%20a%20research%20plan%20given%20a%20set%20of%20aims%20and%20constraints.%20The%20plan%20may%20be%20used%20by%20researchers%20for%20brainstorming%2C%20or%20may%20even%20be%20implemented%20after%20further%20refinement.%20However%2C%20language%20models%20currently%20struggle%20to%20generate%20research%20plans%20that%20follow%20all%20constraints%20and%20implicit%20requirements.%20In%20this%20work%2C%20we%20study%20how%20to%20leverage%20the%20vast%20corpus%20of%20existing%20research%20papers%20to%20train%20language%20models%20that%20generate%20better%20research%20plans.%20We%20build%20a%20scalable%2C%20diverse%20training%20corpus%20by%20automatically%20extracting%20research%20goals%20and%20goal-specific%20grading%20rubrics%20from%20papers%20across%20several%20domains.%20We%20then%20train%20models%20for%20research%20plan%20generation%20via%20reinforcement%20learning%20with%20self-grading.%20A%20frozen%20copy%20of%20the%20initial%20policy%20acts%20as%20the%20grader%20during%20training%2C%20with%20the%20rubrics%20creating%20a%20generator-verifier%20gap%20that%20enables%20improvements%20without%20external%20human%20supervision.%20To%20validate%20this%20approach%2C%20we%20conduct%20a%20study%20with%20human%20experts%20for%20machine%20learning%20research%20goals%2C%20spanning%20225%20hours.%20The%20experts%20prefer%20plans%20generated%20by%20our%20finetuned%20Qwen3-30B-A3B%20model%20over%20the%20initial%20model%20for%2070%25%20of%20research%20goals%2C%20and%20approve%2084%25%20of%20the%20automatically%20extracted%20goal-specific%20grading%20rubrics.%20To%20assess%20generality%2C%20we%20also%20extend%20our%20approach%20to%20research%20goals%20from%20medical%20papers%2C%20and%20new%20arXiv%20preprints%2C%20evaluating%20with%20a%20jury%20of%20frontier%20models.%20Our%20finetuning%20yields%2012-22%25%20relative%20improvements%20and%20significant%20cross-domain%20generalization%2C%20proving%20effective%20even%20in%20problem%20settings%20like%20medical%20research%20where%20execution%20feedback%20is%20infeasible.%20Together%2C%20these%20findings%20demonstrate%20the%20potential%20of%20a%20scalable%2C%20automated%20training%20recipe%20as%20a%20step%20towards%20improving%20general%20AI%20co-scientists.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520AI%2520Co-Scientists%2520Using%2520Rubric%2520Rewards%26entry.906535625%3DShashwat%2520Goel%2520and%2520Rishi%2520Hazra%2520and%2520Dulhan%2520Jayalath%2520and%2520Timon%2520Willi%2520and%2520Parag%2520Jain%2520and%2520William%2520F.%2520Shen%2520and%2520Ilias%2520Leontiadis%2520and%2520Francesco%2520Barbieri%2520and%2520Yoram%2520Bachrach%2520and%2520Jonas%2520Geiping%2520and%2520Chenxi%2520Whitehouse%26entry.1292438233%3DAI%2520co-scientists%2520are%2520emerging%2520as%2520a%2520tool%2520to%2520assist%2520human%2520researchers%2520in%2520achieving%2520their%2520research%2520goals.%2520A%2520crucial%2520feature%2520of%2520these%2520AI%2520co-scientists%2520is%2520the%2520ability%2520to%2520generate%2520a%2520research%2520plan%2520given%2520a%2520set%2520of%2520aims%2520and%2520constraints.%2520The%2520plan%2520may%2520be%2520used%2520by%2520researchers%2520for%2520brainstorming%252C%2520or%2520may%2520even%2520be%2520implemented%2520after%2520further%2520refinement.%2520However%252C%2520language%2520models%2520currently%2520struggle%2520to%2520generate%2520research%2520plans%2520that%2520follow%2520all%2520constraints%2520and%2520implicit%2520requirements.%2520In%2520this%2520work%252C%2520we%2520study%2520how%2520to%2520leverage%2520the%2520vast%2520corpus%2520of%2520existing%2520research%2520papers%2520to%2520train%2520language%2520models%2520that%2520generate%2520better%2520research%2520plans.%2520We%2520build%2520a%2520scalable%252C%2520diverse%2520training%2520corpus%2520by%2520automatically%2520extracting%2520research%2520goals%2520and%2520goal-specific%2520grading%2520rubrics%2520from%2520papers%2520across%2520several%2520domains.%2520We%2520then%2520train%2520models%2520for%2520research%2520plan%2520generation%2520via%2520reinforcement%2520learning%2520with%2520self-grading.%2520A%2520frozen%2520copy%2520of%2520the%2520initial%2520policy%2520acts%2520as%2520the%2520grader%2520during%2520training%252C%2520with%2520the%2520rubrics%2520creating%2520a%2520generator-verifier%2520gap%2520that%2520enables%2520improvements%2520without%2520external%2520human%2520supervision.%2520To%2520validate%2520this%2520approach%252C%2520we%2520conduct%2520a%2520study%2520with%2520human%2520experts%2520for%2520machine%2520learning%2520research%2520goals%252C%2520spanning%2520225%2520hours.%2520The%2520experts%2520prefer%2520plans%2520generated%2520by%2520our%2520finetuned%2520Qwen3-30B-A3B%2520model%2520over%2520the%2520initial%2520model%2520for%252070%2525%2520of%2520research%2520goals%252C%2520and%2520approve%252084%2525%2520of%2520the%2520automatically%2520extracted%2520goal-specific%2520grading%2520rubrics.%2520To%2520assess%2520generality%252C%2520we%2520also%2520extend%2520our%2520approach%2520to%2520research%2520goals%2520from%2520medical%2520papers%252C%2520and%2520new%2520arXiv%2520preprints%252C%2520evaluating%2520with%2520a%2520jury%2520of%2520frontier%2520models.%2520Our%2520finetuning%2520yields%252012-22%2525%2520relative%2520improvements%2520and%2520significant%2520cross-domain%2520generalization%252C%2520proving%2520effective%2520even%2520in%2520problem%2520settings%2520like%2520medical%2520research%2520where%2520execution%2520feedback%2520is%2520infeasible.%2520Together%252C%2520these%2520findings%2520demonstrate%2520the%2520potential%2520of%2520a%2520scalable%252C%2520automated%2520training%2520recipe%2520as%2520a%2520step%2520towards%2520improving%2520general%2520AI%2520co-scientists.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20AI%20Co-Scientists%20Using%20Rubric%20Rewards&entry.906535625=Shashwat%20Goel%20and%20Rishi%20Hazra%20and%20Dulhan%20Jayalath%20and%20Timon%20Willi%20and%20Parag%20Jain%20and%20William%20F.%20Shen%20and%20Ilias%20Leontiadis%20and%20Francesco%20Barbieri%20and%20Yoram%20Bachrach%20and%20Jonas%20Geiping%20and%20Chenxi%20Whitehouse&entry.1292438233=AI%20co-scientists%20are%20emerging%20as%20a%20tool%20to%20assist%20human%20researchers%20in%20achieving%20their%20research%20goals.%20A%20crucial%20feature%20of%20these%20AI%20co-scientists%20is%20the%20ability%20to%20generate%20a%20research%20plan%20given%20a%20set%20of%20aims%20and%20constraints.%20The%20plan%20may%20be%20used%20by%20researchers%20for%20brainstorming%2C%20or%20may%20even%20be%20implemented%20after%20further%20refinement.%20However%2C%20language%20models%20currently%20struggle%20to%20generate%20research%20plans%20that%20follow%20all%20constraints%20and%20implicit%20requirements.%20In%20this%20work%2C%20we%20study%20how%20to%20leverage%20the%20vast%20corpus%20of%20existing%20research%20papers%20to%20train%20language%20models%20that%20generate%20better%20research%20plans.%20We%20build%20a%20scalable%2C%20diverse%20training%20corpus%20by%20automatically%20extracting%20research%20goals%20and%20goal-specific%20grading%20rubrics%20from%20papers%20across%20several%20domains.%20We%20then%20train%20models%20for%20research%20plan%20generation%20via%20reinforcement%20learning%20with%20self-grading.%20A%20frozen%20copy%20of%20the%20initial%20policy%20acts%20as%20the%20grader%20during%20training%2C%20with%20the%20rubrics%20creating%20a%20generator-verifier%20gap%20that%20enables%20improvements%20without%20external%20human%20supervision.%20To%20validate%20this%20approach%2C%20we%20conduct%20a%20study%20with%20human%20experts%20for%20machine%20learning%20research%20goals%2C%20spanning%20225%20hours.%20The%20experts%20prefer%20plans%20generated%20by%20our%20finetuned%20Qwen3-30B-A3B%20model%20over%20the%20initial%20model%20for%2070%25%20of%20research%20goals%2C%20and%20approve%2084%25%20of%20the%20automatically%20extracted%20goal-specific%20grading%20rubrics.%20To%20assess%20generality%2C%20we%20also%20extend%20our%20approach%20to%20research%20goals%20from%20medical%20papers%2C%20and%20new%20arXiv%20preprints%2C%20evaluating%20with%20a%20jury%20of%20frontier%20models.%20Our%20finetuning%20yields%2012-22%25%20relative%20improvements%20and%20significant%20cross-domain%20generalization%2C%20proving%20effective%20even%20in%20problem%20settings%20like%20medical%20research%20where%20execution%20feedback%20is%20infeasible.%20Together%2C%20these%20findings%20demonstrate%20the%20potential%20of%20a%20scalable%2C%20automated%20training%20recipe%20as%20a%20step%20towards%20improving%20general%20AI%20co-scientists.&entry.1838667208=http%3A//arxiv.org/abs/2512.23707v1&entry.124074799=Read"},
{"title": "A general framework for deep learning", "author": "William Kengne and Modou Wade", "abstract": "This paper develops a general approach for deep learning for a setting that includes nonparametric regression and classification. We perform a framework from data that fulfills a generalized Bernstein-type inequality, including independent, $\u03c6$-mixing, strongly mixing and $\\mathcal{C}$-mixing observations. Two estimators are proposed: a non-penalized deep neural network estimator (NPDNN) and a sparse-penalized deep neural network estimator (SPDNN). For each of these estimators, bounds of the expected excess risk on the class of H\u00f6lder smooth functions and composition H\u00f6lder functions are established. Applications to independent data, as well as to $\u03c6$-mixing, strongly mixing, $\\mathcal{C}$-mixing processes are considered. For each of these examples, the upper bounds of the expected excess risk of the proposed NPDNN and SPDNN predictors are derived. It is shown that both the NPDNN and SPDNN estimators are minimax optimal (up to a logarithmic factor) in many classical settings.", "link": "http://arxiv.org/abs/2512.23425v1", "date": "2025-12-29", "relevancy": 1.9915, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20general%20framework%20for%20deep%20learning&body=Title%3A%20A%20general%20framework%20for%20deep%20learning%0AAuthor%3A%20William%20Kengne%20and%20Modou%20Wade%0AAbstract%3A%20This%20paper%20develops%20a%20general%20approach%20for%20deep%20learning%20for%20a%20setting%20that%20includes%20nonparametric%20regression%20and%20classification.%20We%20perform%20a%20framework%20from%20data%20that%20fulfills%20a%20generalized%20Bernstein-type%20inequality%2C%20including%20independent%2C%20%24%CF%86%24-mixing%2C%20strongly%20mixing%20and%20%24%5Cmathcal%7BC%7D%24-mixing%20observations.%20Two%20estimators%20are%20proposed%3A%20a%20non-penalized%20deep%20neural%20network%20estimator%20%28NPDNN%29%20and%20a%20sparse-penalized%20deep%20neural%20network%20estimator%20%28SPDNN%29.%20For%20each%20of%20these%20estimators%2C%20bounds%20of%20the%20expected%20excess%20risk%20on%20the%20class%20of%20H%C3%B6lder%20smooth%20functions%20and%20composition%20H%C3%B6lder%20functions%20are%20established.%20Applications%20to%20independent%20data%2C%20as%20well%20as%20to%20%24%CF%86%24-mixing%2C%20strongly%20mixing%2C%20%24%5Cmathcal%7BC%7D%24-mixing%20processes%20are%20considered.%20For%20each%20of%20these%20examples%2C%20the%20upper%20bounds%20of%20the%20expected%20excess%20risk%20of%20the%20proposed%20NPDNN%20and%20SPDNN%20predictors%20are%20derived.%20It%20is%20shown%20that%20both%20the%20NPDNN%20and%20SPDNN%20estimators%20are%20minimax%20optimal%20%28up%20to%20a%20logarithmic%20factor%29%20in%20many%20classical%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520general%2520framework%2520for%2520deep%2520learning%26entry.906535625%3DWilliam%2520Kengne%2520and%2520Modou%2520Wade%26entry.1292438233%3DThis%2520paper%2520develops%2520a%2520general%2520approach%2520for%2520deep%2520learning%2520for%2520a%2520setting%2520that%2520includes%2520nonparametric%2520regression%2520and%2520classification.%2520We%2520perform%2520a%2520framework%2520from%2520data%2520that%2520fulfills%2520a%2520generalized%2520Bernstein-type%2520inequality%252C%2520including%2520independent%252C%2520%2524%25CF%2586%2524-mixing%252C%2520strongly%2520mixing%2520and%2520%2524%255Cmathcal%257BC%257D%2524-mixing%2520observations.%2520Two%2520estimators%2520are%2520proposed%253A%2520a%2520non-penalized%2520deep%2520neural%2520network%2520estimator%2520%2528NPDNN%2529%2520and%2520a%2520sparse-penalized%2520deep%2520neural%2520network%2520estimator%2520%2528SPDNN%2529.%2520For%2520each%2520of%2520these%2520estimators%252C%2520bounds%2520of%2520the%2520expected%2520excess%2520risk%2520on%2520the%2520class%2520of%2520H%25C3%25B6lder%2520smooth%2520functions%2520and%2520composition%2520H%25C3%25B6lder%2520functions%2520are%2520established.%2520Applications%2520to%2520independent%2520data%252C%2520as%2520well%2520as%2520to%2520%2524%25CF%2586%2524-mixing%252C%2520strongly%2520mixing%252C%2520%2524%255Cmathcal%257BC%257D%2524-mixing%2520processes%2520are%2520considered.%2520For%2520each%2520of%2520these%2520examples%252C%2520the%2520upper%2520bounds%2520of%2520the%2520expected%2520excess%2520risk%2520of%2520the%2520proposed%2520NPDNN%2520and%2520SPDNN%2520predictors%2520are%2520derived.%2520It%2520is%2520shown%2520that%2520both%2520the%2520NPDNN%2520and%2520SPDNN%2520estimators%2520are%2520minimax%2520optimal%2520%2528up%2520to%2520a%2520logarithmic%2520factor%2529%2520in%2520many%2520classical%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20general%20framework%20for%20deep%20learning&entry.906535625=William%20Kengne%20and%20Modou%20Wade&entry.1292438233=This%20paper%20develops%20a%20general%20approach%20for%20deep%20learning%20for%20a%20setting%20that%20includes%20nonparametric%20regression%20and%20classification.%20We%20perform%20a%20framework%20from%20data%20that%20fulfills%20a%20generalized%20Bernstein-type%20inequality%2C%20including%20independent%2C%20%24%CF%86%24-mixing%2C%20strongly%20mixing%20and%20%24%5Cmathcal%7BC%7D%24-mixing%20observations.%20Two%20estimators%20are%20proposed%3A%20a%20non-penalized%20deep%20neural%20network%20estimator%20%28NPDNN%29%20and%20a%20sparse-penalized%20deep%20neural%20network%20estimator%20%28SPDNN%29.%20For%20each%20of%20these%20estimators%2C%20bounds%20of%20the%20expected%20excess%20risk%20on%20the%20class%20of%20H%C3%B6lder%20smooth%20functions%20and%20composition%20H%C3%B6lder%20functions%20are%20established.%20Applications%20to%20independent%20data%2C%20as%20well%20as%20to%20%24%CF%86%24-mixing%2C%20strongly%20mixing%2C%20%24%5Cmathcal%7BC%7D%24-mixing%20processes%20are%20considered.%20For%20each%20of%20these%20examples%2C%20the%20upper%20bounds%20of%20the%20expected%20excess%20risk%20of%20the%20proposed%20NPDNN%20and%20SPDNN%20predictors%20are%20derived.%20It%20is%20shown%20that%20both%20the%20NPDNN%20and%20SPDNN%20estimators%20are%20minimax%20optimal%20%28up%20to%20a%20logarithmic%20factor%29%20in%20many%20classical%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.23425v1&entry.124074799=Read"},
{"title": "Communication-Efficient Federated Learning under Dynamic Device Arrival and Departure: Convergence Analysis and Algorithm Design", "author": "Zhan-Lun Chang and Dong-Jun Han and Seyyedali Hosseinalipour and Mung Chiang and Christopher G. Brinton", "abstract": "Most federated learning (FL) approaches assume a fixed device set. However, real-world scenarios often involve devices dynamically joining or leaving the system, driven by, e.g., user mobility patterns or handovers across cell boundaries. This dynamic setting introduces unique challenges: (1) the optimization objective evolves with the active device set, unlike traditional FL's static objective; and (2) the current global model may no longer serve as an effective initialization for subsequent rounds, potentially hindering adaptation, delaying convergence, and reducing resource efficiency. To address these challenges, we first provide a convergence analysis for FL under a dynamic device set, accounting for factors such as gradient noise, local training iterations, and data heterogeneity. Building on this analysis, we propose a model initialization algorithm that enables rapid adaptation whenever devices join or leave the network. Our key idea is to compute a weighted average of previous global models, guided by gradient similarity, to prioritize models trained on data distributions that closely align with the current device set, thereby accelerating recovery from distribution shifts in fewer training rounds. This plug-and-play algorithm is designed to integrate seamlessly with existing FL methods, offering broad applicability. Experiments demonstrate that our approach achieves convergence speedups typically an order of magnitude or more compared to baselines, which we show drastically reduces energy consumption to reach a target accuracy.", "link": "http://arxiv.org/abs/2410.05662v3", "date": "2025-12-29", "relevancy": 1.9847, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Federated%20Learning%20under%20Dynamic%20Device%20Arrival%20and%20Departure%3A%20Convergence%20Analysis%20and%20Algorithm%20Design&body=Title%3A%20Communication-Efficient%20Federated%20Learning%20under%20Dynamic%20Device%20Arrival%20and%20Departure%3A%20Convergence%20Analysis%20and%20Algorithm%20Design%0AAuthor%3A%20Zhan-Lun%20Chang%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Mung%20Chiang%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20Most%20federated%20learning%20%28FL%29%20approaches%20assume%20a%20fixed%20device%20set.%20However%2C%20real-world%20scenarios%20often%20involve%20devices%20dynamically%20joining%20or%20leaving%20the%20system%2C%20driven%20by%2C%20e.g.%2C%20user%20mobility%20patterns%20or%20handovers%20across%20cell%20boundaries.%20This%20dynamic%20setting%20introduces%20unique%20challenges%3A%20%281%29%20the%20optimization%20objective%20evolves%20with%20the%20active%20device%20set%2C%20unlike%20traditional%20FL%27s%20static%20objective%3B%20and%20%282%29%20the%20current%20global%20model%20may%20no%20longer%20serve%20as%20an%20effective%20initialization%20for%20subsequent%20rounds%2C%20potentially%20hindering%20adaptation%2C%20delaying%20convergence%2C%20and%20reducing%20resource%20efficiency.%20To%20address%20these%20challenges%2C%20we%20first%20provide%20a%20convergence%20analysis%20for%20FL%20under%20a%20dynamic%20device%20set%2C%20accounting%20for%20factors%20such%20as%20gradient%20noise%2C%20local%20training%20iterations%2C%20and%20data%20heterogeneity.%20Building%20on%20this%20analysis%2C%20we%20propose%20a%20model%20initialization%20algorithm%20that%20enables%20rapid%20adaptation%20whenever%20devices%20join%20or%20leave%20the%20network.%20Our%20key%20idea%20is%20to%20compute%20a%20weighted%20average%20of%20previous%20global%20models%2C%20guided%20by%20gradient%20similarity%2C%20to%20prioritize%20models%20trained%20on%20data%20distributions%20that%20closely%20align%20with%20the%20current%20device%20set%2C%20thereby%20accelerating%20recovery%20from%20distribution%20shifts%20in%20fewer%20training%20rounds.%20This%20plug-and-play%20algorithm%20is%20designed%20to%20integrate%20seamlessly%20with%20existing%20FL%20methods%2C%20offering%20broad%20applicability.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20convergence%20speedups%20typically%20an%20order%20of%20magnitude%20or%20more%20compared%20to%20baselines%2C%20which%20we%20show%20drastically%20reduces%20energy%20consumption%20to%20reach%20a%20target%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2410.05662v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Federated%2520Learning%2520under%2520Dynamic%2520Device%2520Arrival%2520and%2520Departure%253A%2520Convergence%2520Analysis%2520and%2520Algorithm%2520Design%26entry.906535625%3DZhan-Lun%2520Chang%2520and%2520Dong-Jun%2520Han%2520and%2520Seyyedali%2520Hosseinalipour%2520and%2520Mung%2520Chiang%2520and%2520Christopher%2520G.%2520Brinton%26entry.1292438233%3DMost%2520federated%2520learning%2520%2528FL%2529%2520approaches%2520assume%2520a%2520fixed%2520device%2520set.%2520However%252C%2520real-world%2520scenarios%2520often%2520involve%2520devices%2520dynamically%2520joining%2520or%2520leaving%2520the%2520system%252C%2520driven%2520by%252C%2520e.g.%252C%2520user%2520mobility%2520patterns%2520or%2520handovers%2520across%2520cell%2520boundaries.%2520This%2520dynamic%2520setting%2520introduces%2520unique%2520challenges%253A%2520%25281%2529%2520the%2520optimization%2520objective%2520evolves%2520with%2520the%2520active%2520device%2520set%252C%2520unlike%2520traditional%2520FL%2527s%2520static%2520objective%253B%2520and%2520%25282%2529%2520the%2520current%2520global%2520model%2520may%2520no%2520longer%2520serve%2520as%2520an%2520effective%2520initialization%2520for%2520subsequent%2520rounds%252C%2520potentially%2520hindering%2520adaptation%252C%2520delaying%2520convergence%252C%2520and%2520reducing%2520resource%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520first%2520provide%2520a%2520convergence%2520analysis%2520for%2520FL%2520under%2520a%2520dynamic%2520device%2520set%252C%2520accounting%2520for%2520factors%2520such%2520as%2520gradient%2520noise%252C%2520local%2520training%2520iterations%252C%2520and%2520data%2520heterogeneity.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520propose%2520a%2520model%2520initialization%2520algorithm%2520that%2520enables%2520rapid%2520adaptation%2520whenever%2520devices%2520join%2520or%2520leave%2520the%2520network.%2520Our%2520key%2520idea%2520is%2520to%2520compute%2520a%2520weighted%2520average%2520of%2520previous%2520global%2520models%252C%2520guided%2520by%2520gradient%2520similarity%252C%2520to%2520prioritize%2520models%2520trained%2520on%2520data%2520distributions%2520that%2520closely%2520align%2520with%2520the%2520current%2520device%2520set%252C%2520thereby%2520accelerating%2520recovery%2520from%2520distribution%2520shifts%2520in%2520fewer%2520training%2520rounds.%2520This%2520plug-and-play%2520algorithm%2520is%2520designed%2520to%2520integrate%2520seamlessly%2520with%2520existing%2520FL%2520methods%252C%2520offering%2520broad%2520applicability.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520convergence%2520speedups%2520typically%2520an%2520order%2520of%2520magnitude%2520or%2520more%2520compared%2520to%2520baselines%252C%2520which%2520we%2520show%2520drastically%2520reduces%2520energy%2520consumption%2520to%2520reach%2520a%2520target%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05662v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Federated%20Learning%20under%20Dynamic%20Device%20Arrival%20and%20Departure%3A%20Convergence%20Analysis%20and%20Algorithm%20Design&entry.906535625=Zhan-Lun%20Chang%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Mung%20Chiang%20and%20Christopher%20G.%20Brinton&entry.1292438233=Most%20federated%20learning%20%28FL%29%20approaches%20assume%20a%20fixed%20device%20set.%20However%2C%20real-world%20scenarios%20often%20involve%20devices%20dynamically%20joining%20or%20leaving%20the%20system%2C%20driven%20by%2C%20e.g.%2C%20user%20mobility%20patterns%20or%20handovers%20across%20cell%20boundaries.%20This%20dynamic%20setting%20introduces%20unique%20challenges%3A%20%281%29%20the%20optimization%20objective%20evolves%20with%20the%20active%20device%20set%2C%20unlike%20traditional%20FL%27s%20static%20objective%3B%20and%20%282%29%20the%20current%20global%20model%20may%20no%20longer%20serve%20as%20an%20effective%20initialization%20for%20subsequent%20rounds%2C%20potentially%20hindering%20adaptation%2C%20delaying%20convergence%2C%20and%20reducing%20resource%20efficiency.%20To%20address%20these%20challenges%2C%20we%20first%20provide%20a%20convergence%20analysis%20for%20FL%20under%20a%20dynamic%20device%20set%2C%20accounting%20for%20factors%20such%20as%20gradient%20noise%2C%20local%20training%20iterations%2C%20and%20data%20heterogeneity.%20Building%20on%20this%20analysis%2C%20we%20propose%20a%20model%20initialization%20algorithm%20that%20enables%20rapid%20adaptation%20whenever%20devices%20join%20or%20leave%20the%20network.%20Our%20key%20idea%20is%20to%20compute%20a%20weighted%20average%20of%20previous%20global%20models%2C%20guided%20by%20gradient%20similarity%2C%20to%20prioritize%20models%20trained%20on%20data%20distributions%20that%20closely%20align%20with%20the%20current%20device%20set%2C%20thereby%20accelerating%20recovery%20from%20distribution%20shifts%20in%20fewer%20training%20rounds.%20This%20plug-and-play%20algorithm%20is%20designed%20to%20integrate%20seamlessly%20with%20existing%20FL%20methods%2C%20offering%20broad%20applicability.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20convergence%20speedups%20typically%20an%20order%20of%20magnitude%20or%20more%20compared%20to%20baselines%2C%20which%20we%20show%20drastically%20reduces%20energy%20consumption%20to%20reach%20a%20target%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2410.05662v3&entry.124074799=Read"},
{"title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings", "author": "Thomas Haschka and Joseph Bakarji", "abstract": "Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.", "link": "http://arxiv.org/abs/2512.23471v1", "date": "2025-12-29", "relevancy": 1.9846, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Tree%20Inference%20on%20Text%20Corpa%20using%20a%20Nested%20Density%20Approach%20together%20with%20Large%20Language%20Model%20Embeddings&body=Title%3A%20Semantic%20Tree%20Inference%20on%20Text%20Corpa%20using%20a%20Nested%20Density%20Approach%20together%20with%20Large%20Language%20Model%20Embeddings%0AAuthor%3A%20Thomas%20Haschka%20and%20Joseph%20Bakarji%0AAbstract%3A%20Semantic%20text%20classification%20has%20undergone%20significant%20advances%20in%20recent%20years%20due%20to%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20and%20their%20high%20dimensional%20embeddings.%20While%20LLM-embeddings%20are%20frequently%20used%20to%20store%20and%20retrieve%20text%20by%20semantic%20similarity%20in%20vector%20databases%2C%20the%20global%20structure%20semantic%20relationships%20in%20text%20corpora%20often%20remains%20opaque.%20Herein%20we%20propose%20a%20nested%20density%20clustering%20approach%2C%20to%20infer%20hierarchical%20trees%20of%20semantically%20related%20texts.%20The%20method%20starts%20by%20identifying%20texts%20of%20strong%20semantic%20similarity%20as%20it%20searches%20for%20dense%20clusters%20in%20LLM%20embedding%20space.%20As%20the%20density%20criterion%20is%20gradually%20relaxed%2C%20these%20dense%20clusters%20merge%20into%20more%20diffuse%20clusters%2C%20until%20the%20whole%20dataset%20is%20represented%20by%20a%20single%20cluster%20--%20the%20root%20of%20the%20tree.%20By%20embedding%20dense%20clusters%20into%20increasingly%20diffuse%20ones%2C%20we%20construct%20a%20tree%20structure%20that%20captures%20hierarchical%20semantic%20relationships%20among%20texts.%20We%20outline%20how%20this%20approach%20can%20be%20used%20to%20classify%20textual%20data%20for%20abstracts%20of%20scientific%20abstracts%20as%20a%20case%20study.%20This%20enables%20the%20data-driven%20discovery%20research%20areas%20and%20their%20subfields%20without%20predefined%20categories.%20To%20evaluate%20the%20general%20applicability%20of%20the%20method%2C%20we%20further%20apply%20it%20to%20established%20benchmark%20datasets%20such%20as%20the%2020%20Newsgroups%20and%20IMDB%2050k%20Movie%20Reviews%2C%20demonstrating%20its%20robustness%20across%20domains.%20Finally%20we%20discuss%20possible%20applications%20on%20scientometrics%2C%20topic%20evolution%2C%20highlighting%20how%20nested%20density%20trees%20can%20reveal%20semantic%20structure%20and%20evolution%20in%20textual%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Tree%2520Inference%2520on%2520Text%2520Corpa%2520using%2520a%2520Nested%2520Density%2520Approach%2520together%2520with%2520Large%2520Language%2520Model%2520Embeddings%26entry.906535625%3DThomas%2520Haschka%2520and%2520Joseph%2520Bakarji%26entry.1292438233%3DSemantic%2520text%2520classification%2520has%2520undergone%2520significant%2520advances%2520in%2520recent%2520years%2520due%2520to%2520the%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520their%2520high%2520dimensional%2520embeddings.%2520While%2520LLM-embeddings%2520are%2520frequently%2520used%2520to%2520store%2520and%2520retrieve%2520text%2520by%2520semantic%2520similarity%2520in%2520vector%2520databases%252C%2520the%2520global%2520structure%2520semantic%2520relationships%2520in%2520text%2520corpora%2520often%2520remains%2520opaque.%2520Herein%2520we%2520propose%2520a%2520nested%2520density%2520clustering%2520approach%252C%2520to%2520infer%2520hierarchical%2520trees%2520of%2520semantically%2520related%2520texts.%2520The%2520method%2520starts%2520by%2520identifying%2520texts%2520of%2520strong%2520semantic%2520similarity%2520as%2520it%2520searches%2520for%2520dense%2520clusters%2520in%2520LLM%2520embedding%2520space.%2520As%2520the%2520density%2520criterion%2520is%2520gradually%2520relaxed%252C%2520these%2520dense%2520clusters%2520merge%2520into%2520more%2520diffuse%2520clusters%252C%2520until%2520the%2520whole%2520dataset%2520is%2520represented%2520by%2520a%2520single%2520cluster%2520--%2520the%2520root%2520of%2520the%2520tree.%2520By%2520embedding%2520dense%2520clusters%2520into%2520increasingly%2520diffuse%2520ones%252C%2520we%2520construct%2520a%2520tree%2520structure%2520that%2520captures%2520hierarchical%2520semantic%2520relationships%2520among%2520texts.%2520We%2520outline%2520how%2520this%2520approach%2520can%2520be%2520used%2520to%2520classify%2520textual%2520data%2520for%2520abstracts%2520of%2520scientific%2520abstracts%2520as%2520a%2520case%2520study.%2520This%2520enables%2520the%2520data-driven%2520discovery%2520research%2520areas%2520and%2520their%2520subfields%2520without%2520predefined%2520categories.%2520To%2520evaluate%2520the%2520general%2520applicability%2520of%2520the%2520method%252C%2520we%2520further%2520apply%2520it%2520to%2520established%2520benchmark%2520datasets%2520such%2520as%2520the%252020%2520Newsgroups%2520and%2520IMDB%252050k%2520Movie%2520Reviews%252C%2520demonstrating%2520its%2520robustness%2520across%2520domains.%2520Finally%2520we%2520discuss%2520possible%2520applications%2520on%2520scientometrics%252C%2520topic%2520evolution%252C%2520highlighting%2520how%2520nested%2520density%2520trees%2520can%2520reveal%2520semantic%2520structure%2520and%2520evolution%2520in%2520textual%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Tree%20Inference%20on%20Text%20Corpa%20using%20a%20Nested%20Density%20Approach%20together%20with%20Large%20Language%20Model%20Embeddings&entry.906535625=Thomas%20Haschka%20and%20Joseph%20Bakarji&entry.1292438233=Semantic%20text%20classification%20has%20undergone%20significant%20advances%20in%20recent%20years%20due%20to%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%20and%20their%20high%20dimensional%20embeddings.%20While%20LLM-embeddings%20are%20frequently%20used%20to%20store%20and%20retrieve%20text%20by%20semantic%20similarity%20in%20vector%20databases%2C%20the%20global%20structure%20semantic%20relationships%20in%20text%20corpora%20often%20remains%20opaque.%20Herein%20we%20propose%20a%20nested%20density%20clustering%20approach%2C%20to%20infer%20hierarchical%20trees%20of%20semantically%20related%20texts.%20The%20method%20starts%20by%20identifying%20texts%20of%20strong%20semantic%20similarity%20as%20it%20searches%20for%20dense%20clusters%20in%20LLM%20embedding%20space.%20As%20the%20density%20criterion%20is%20gradually%20relaxed%2C%20these%20dense%20clusters%20merge%20into%20more%20diffuse%20clusters%2C%20until%20the%20whole%20dataset%20is%20represented%20by%20a%20single%20cluster%20--%20the%20root%20of%20the%20tree.%20By%20embedding%20dense%20clusters%20into%20increasingly%20diffuse%20ones%2C%20we%20construct%20a%20tree%20structure%20that%20captures%20hierarchical%20semantic%20relationships%20among%20texts.%20We%20outline%20how%20this%20approach%20can%20be%20used%20to%20classify%20textual%20data%20for%20abstracts%20of%20scientific%20abstracts%20as%20a%20case%20study.%20This%20enables%20the%20data-driven%20discovery%20research%20areas%20and%20their%20subfields%20without%20predefined%20categories.%20To%20evaluate%20the%20general%20applicability%20of%20the%20method%2C%20we%20further%20apply%20it%20to%20established%20benchmark%20datasets%20such%20as%20the%2020%20Newsgroups%20and%20IMDB%2050k%20Movie%20Reviews%2C%20demonstrating%20its%20robustness%20across%20domains.%20Finally%20we%20discuss%20possible%20applications%20on%20scientometrics%2C%20topic%20evolution%2C%20highlighting%20how%20nested%20density%20trees%20can%20reveal%20semantic%20structure%20and%20evolution%20in%20textual%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.23471v1&entry.124074799=Read"},
{"title": "FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence", "author": "Guoan Wan and Tianyu Chen and Fangzheng Feng and Haoyi Zhou and Runhua Xu", "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.", "link": "http://arxiv.org/abs/2512.23485v1", "date": "2025-12-29", "relevancy": 1.979, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5341}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4891}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRoD%3A%20Full-Rank%20Efficient%20Fine-Tuning%20with%20Rotational%20Degrees%20for%20Fast%20Convergence&body=Title%3A%20FRoD%3A%20Full-Rank%20Efficient%20Fine-Tuning%20with%20Rotational%20Degrees%20for%20Fast%20Convergence%0AAuthor%3A%20Guoan%20Wan%20and%20Tianyu%20Chen%20and%20Fangzheng%20Feng%20and%20Haoyi%20Zhou%20and%20Runhua%20Xu%0AAbstract%3A%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20emerged%20as%20a%20practical%20solution%20for%20adapting%20large%20foundation%20models%20to%20downstream%20tasks%2C%20reducing%20computational%20and%20memory%20costs%20by%20updating%20only%20a%20small%20subset%20of%20parameters.%20Among%20them%2C%20approaches%20like%20LoRA%20aim%20to%20strike%20a%20balance%20between%20efficiency%20and%20expressiveness%2C%20but%20often%20suffer%20from%20slow%20convergence%20and%20limited%20adaptation%20capacity%20due%20to%20their%20inherent%20low-rank%20constraints.%20This%20trade-off%20hampers%20the%20ability%20of%20PEFT%20methods%20to%20capture%20complex%20patterns%20needed%20for%20diverse%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20FRoD%2C%20a%20novel%20fine-tuning%20method%20that%20combines%20hierarchical%20joint%20decomposition%20with%20rotational%20degrees%20of%20freedom.%20By%20extracting%20a%20globally%20shared%20basis%20across%20layers%20and%20injecting%20sparse%2C%20learnable%20perturbations%20into%20scaling%20factors%20for%20flexible%20full-rank%20updates%2C%20FRoD%20enhances%20expressiveness%20and%20efficiency%2C%20leading%20to%20faster%20and%20more%20robust%20convergence.%20On%2020%20benchmarks%20spanning%20vision%2C%20reasoning%2C%20and%20language%20understanding%2C%20FRoD%20matches%20full%20model%20fine-tuning%20in%20accuracy%2C%20while%20using%20only%201.72%25%20of%20trainable%20parameters%20under%20identical%20training%20budgets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRoD%253A%2520Full-Rank%2520Efficient%2520Fine-Tuning%2520with%2520Rotational%2520Degrees%2520for%2520Fast%2520Convergence%26entry.906535625%3DGuoan%2520Wan%2520and%2520Tianyu%2520Chen%2520and%2520Fangzheng%2520Feng%2520and%2520Haoyi%2520Zhou%2520and%2520Runhua%2520Xu%26entry.1292438233%3DParameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520have%2520emerged%2520as%2520a%2520practical%2520solution%2520for%2520adapting%2520large%2520foundation%2520models%2520to%2520downstream%2520tasks%252C%2520reducing%2520computational%2520and%2520memory%2520costs%2520by%2520updating%2520only%2520a%2520small%2520subset%2520of%2520parameters.%2520Among%2520them%252C%2520approaches%2520like%2520LoRA%2520aim%2520to%2520strike%2520a%2520balance%2520between%2520efficiency%2520and%2520expressiveness%252C%2520but%2520often%2520suffer%2520from%2520slow%2520convergence%2520and%2520limited%2520adaptation%2520capacity%2520due%2520to%2520their%2520inherent%2520low-rank%2520constraints.%2520This%2520trade-off%2520hampers%2520the%2520ability%2520of%2520PEFT%2520methods%2520to%2520capture%2520complex%2520patterns%2520needed%2520for%2520diverse%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FRoD%252C%2520a%2520novel%2520fine-tuning%2520method%2520that%2520combines%2520hierarchical%2520joint%2520decomposition%2520with%2520rotational%2520degrees%2520of%2520freedom.%2520By%2520extracting%2520a%2520globally%2520shared%2520basis%2520across%2520layers%2520and%2520injecting%2520sparse%252C%2520learnable%2520perturbations%2520into%2520scaling%2520factors%2520for%2520flexible%2520full-rank%2520updates%252C%2520FRoD%2520enhances%2520expressiveness%2520and%2520efficiency%252C%2520leading%2520to%2520faster%2520and%2520more%2520robust%2520convergence.%2520On%252020%2520benchmarks%2520spanning%2520vision%252C%2520reasoning%252C%2520and%2520language%2520understanding%252C%2520FRoD%2520matches%2520full%2520model%2520fine-tuning%2520in%2520accuracy%252C%2520while%2520using%2520only%25201.72%2525%2520of%2520trainable%2520parameters%2520under%2520identical%2520training%2520budgets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRoD%3A%20Full-Rank%20Efficient%20Fine-Tuning%20with%20Rotational%20Degrees%20for%20Fast%20Convergence&entry.906535625=Guoan%20Wan%20and%20Tianyu%20Chen%20and%20Fangzheng%20Feng%20and%20Haoyi%20Zhou%20and%20Runhua%20Xu&entry.1292438233=Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20emerged%20as%20a%20practical%20solution%20for%20adapting%20large%20foundation%20models%20to%20downstream%20tasks%2C%20reducing%20computational%20and%20memory%20costs%20by%20updating%20only%20a%20small%20subset%20of%20parameters.%20Among%20them%2C%20approaches%20like%20LoRA%20aim%20to%20strike%20a%20balance%20between%20efficiency%20and%20expressiveness%2C%20but%20often%20suffer%20from%20slow%20convergence%20and%20limited%20adaptation%20capacity%20due%20to%20their%20inherent%20low-rank%20constraints.%20This%20trade-off%20hampers%20the%20ability%20of%20PEFT%20methods%20to%20capture%20complex%20patterns%20needed%20for%20diverse%20tasks.%20To%20address%20these%20challenges%2C%20we%20propose%20FRoD%2C%20a%20novel%20fine-tuning%20method%20that%20combines%20hierarchical%20joint%20decomposition%20with%20rotational%20degrees%20of%20freedom.%20By%20extracting%20a%20globally%20shared%20basis%20across%20layers%20and%20injecting%20sparse%2C%20learnable%20perturbations%20into%20scaling%20factors%20for%20flexible%20full-rank%20updates%2C%20FRoD%20enhances%20expressiveness%20and%20efficiency%2C%20leading%20to%20faster%20and%20more%20robust%20convergence.%20On%2020%20benchmarks%20spanning%20vision%2C%20reasoning%2C%20and%20language%20understanding%2C%20FRoD%20matches%20full%20model%20fine-tuning%20in%20accuracy%2C%20while%20using%20only%201.72%25%20of%20trainable%20parameters%20under%20identical%20training%20budgets.&entry.1838667208=http%3A//arxiv.org/abs/2512.23485v1&entry.124074799=Read"},
{"title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion", "author": "Weijie Wang and Jiagang Zhu and Zeyu Zhang and Xiaofeng Wang and Zheng Zhu and Guosheng Zhao and Chaojun Ni and Haoxiao Wang and Guan Huang and Xinze Chen and Yukun Zhou and Wenkang Qin and Duochao Shi and Haoyun Li and Yicheng Xiao and Donny Y. Chen and Jiwen Lu", "abstract": "We present DriveGen3D, a novel framework for generating high-quality and highly controllable dynamic 3D driving scenes that addresses critical limitations in existing methodologies. Current approaches to driving scene synthesis either suffer from prohibitive computational demands for extended temporal generation, focus exclusively on prolonged video synthesis without 3D representation, or restrict themselves to static single-scene reconstruction. Our work bridges this methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control. DriveGen3D introduces a unified pipeline consisting of two specialized components: FastDrive-DiT, an efficient video diffusion transformer for high-resolution, temporally coherent video synthesis under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a feed-forward module that rapidly builds 3D Gaussian representations across time, ensuring spatial-temporal consistency. DriveGen3D enable the generation of long driving videos (up to $800\\times424$ at $12$ FPS) and corresponding 3D scenes, achieving state-of-the-art results while maintaining efficiency.", "link": "http://arxiv.org/abs/2510.15264v2", "date": "2025-12-29", "relevancy": 1.9735, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6992}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6571}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveGen3D%3A%20Boosting%20Feed-Forward%20Driving%20Scene%20Generation%20with%20Efficient%20Video%20Diffusion&body=Title%3A%20DriveGen3D%3A%20Boosting%20Feed-Forward%20Driving%20Scene%20Generation%20with%20Efficient%20Video%20Diffusion%0AAuthor%3A%20Weijie%20Wang%20and%20Jiagang%20Zhu%20and%20Zeyu%20Zhang%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Guosheng%20Zhao%20and%20Chaojun%20Ni%20and%20Haoxiao%20Wang%20and%20Guan%20Huang%20and%20Xinze%20Chen%20and%20Yukun%20Zhou%20and%20Wenkang%20Qin%20and%20Duochao%20Shi%20and%20Haoyun%20Li%20and%20Yicheng%20Xiao%20and%20Donny%20Y.%20Chen%20and%20Jiwen%20Lu%0AAbstract%3A%20We%20present%20DriveGen3D%2C%20a%20novel%20framework%20for%20generating%20high-quality%20and%20highly%20controllable%20dynamic%203D%20driving%20scenes%20that%20addresses%20critical%20limitations%20in%20existing%20methodologies.%20Current%20approaches%20to%20driving%20scene%20synthesis%20either%20suffer%20from%20prohibitive%20computational%20demands%20for%20extended%20temporal%20generation%2C%20focus%20exclusively%20on%20prolonged%20video%20synthesis%20without%203D%20representation%2C%20or%20restrict%20themselves%20to%20static%20single-scene%20reconstruction.%20Our%20work%20bridges%20this%20methodological%20gap%20by%20integrating%20accelerated%20long-term%20video%20generation%20with%20large-scale%20dynamic%20scene%20reconstruction%20through%20multimodal%20conditional%20control.%20DriveGen3D%20introduces%20a%20unified%20pipeline%20consisting%20of%20two%20specialized%20components%3A%20FastDrive-DiT%2C%20an%20efficient%20video%20diffusion%20transformer%20for%20high-resolution%2C%20temporally%20coherent%20video%20synthesis%20under%20text%20and%20Bird%27s-Eye-View%20%28BEV%29%20layout%20guidance%3B%20and%20FastRecon3D%2C%20a%20feed-forward%20module%20that%20rapidly%20builds%203D%20Gaussian%20representations%20across%20time%2C%20ensuring%20spatial-temporal%20consistency.%20DriveGen3D%20enable%20the%20generation%20of%20long%20driving%20videos%20%28up%20to%20%24800%5Ctimes424%24%20at%20%2412%24%20FPS%29%20and%20corresponding%203D%20scenes%2C%20achieving%20state-of-the-art%20results%20while%20maintaining%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveGen3D%253A%2520Boosting%2520Feed-Forward%2520Driving%2520Scene%2520Generation%2520with%2520Efficient%2520Video%2520Diffusion%26entry.906535625%3DWeijie%2520Wang%2520and%2520Jiagang%2520Zhu%2520and%2520Zeyu%2520Zhang%2520and%2520Xiaofeng%2520Wang%2520and%2520Zheng%2520Zhu%2520and%2520Guosheng%2520Zhao%2520and%2520Chaojun%2520Ni%2520and%2520Haoxiao%2520Wang%2520and%2520Guan%2520Huang%2520and%2520Xinze%2520Chen%2520and%2520Yukun%2520Zhou%2520and%2520Wenkang%2520Qin%2520and%2520Duochao%2520Shi%2520and%2520Haoyun%2520Li%2520and%2520Yicheng%2520Xiao%2520and%2520Donny%2520Y.%2520Chen%2520and%2520Jiwen%2520Lu%26entry.1292438233%3DWe%2520present%2520DriveGen3D%252C%2520a%2520novel%2520framework%2520for%2520generating%2520high-quality%2520and%2520highly%2520controllable%2520dynamic%25203D%2520driving%2520scenes%2520that%2520addresses%2520critical%2520limitations%2520in%2520existing%2520methodologies.%2520Current%2520approaches%2520to%2520driving%2520scene%2520synthesis%2520either%2520suffer%2520from%2520prohibitive%2520computational%2520demands%2520for%2520extended%2520temporal%2520generation%252C%2520focus%2520exclusively%2520on%2520prolonged%2520video%2520synthesis%2520without%25203D%2520representation%252C%2520or%2520restrict%2520themselves%2520to%2520static%2520single-scene%2520reconstruction.%2520Our%2520work%2520bridges%2520this%2520methodological%2520gap%2520by%2520integrating%2520accelerated%2520long-term%2520video%2520generation%2520with%2520large-scale%2520dynamic%2520scene%2520reconstruction%2520through%2520multimodal%2520conditional%2520control.%2520DriveGen3D%2520introduces%2520a%2520unified%2520pipeline%2520consisting%2520of%2520two%2520specialized%2520components%253A%2520FastDrive-DiT%252C%2520an%2520efficient%2520video%2520diffusion%2520transformer%2520for%2520high-resolution%252C%2520temporally%2520coherent%2520video%2520synthesis%2520under%2520text%2520and%2520Bird%2527s-Eye-View%2520%2528BEV%2529%2520layout%2520guidance%253B%2520and%2520FastRecon3D%252C%2520a%2520feed-forward%2520module%2520that%2520rapidly%2520builds%25203D%2520Gaussian%2520representations%2520across%2520time%252C%2520ensuring%2520spatial-temporal%2520consistency.%2520DriveGen3D%2520enable%2520the%2520generation%2520of%2520long%2520driving%2520videos%2520%2528up%2520to%2520%2524800%255Ctimes424%2524%2520at%2520%252412%2524%2520FPS%2529%2520and%2520corresponding%25203D%2520scenes%252C%2520achieving%2520state-of-the-art%2520results%2520while%2520maintaining%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveGen3D%3A%20Boosting%20Feed-Forward%20Driving%20Scene%20Generation%20with%20Efficient%20Video%20Diffusion&entry.906535625=Weijie%20Wang%20and%20Jiagang%20Zhu%20and%20Zeyu%20Zhang%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Guosheng%20Zhao%20and%20Chaojun%20Ni%20and%20Haoxiao%20Wang%20and%20Guan%20Huang%20and%20Xinze%20Chen%20and%20Yukun%20Zhou%20and%20Wenkang%20Qin%20and%20Duochao%20Shi%20and%20Haoyun%20Li%20and%20Yicheng%20Xiao%20and%20Donny%20Y.%20Chen%20and%20Jiwen%20Lu&entry.1292438233=We%20present%20DriveGen3D%2C%20a%20novel%20framework%20for%20generating%20high-quality%20and%20highly%20controllable%20dynamic%203D%20driving%20scenes%20that%20addresses%20critical%20limitations%20in%20existing%20methodologies.%20Current%20approaches%20to%20driving%20scene%20synthesis%20either%20suffer%20from%20prohibitive%20computational%20demands%20for%20extended%20temporal%20generation%2C%20focus%20exclusively%20on%20prolonged%20video%20synthesis%20without%203D%20representation%2C%20or%20restrict%20themselves%20to%20static%20single-scene%20reconstruction.%20Our%20work%20bridges%20this%20methodological%20gap%20by%20integrating%20accelerated%20long-term%20video%20generation%20with%20large-scale%20dynamic%20scene%20reconstruction%20through%20multimodal%20conditional%20control.%20DriveGen3D%20introduces%20a%20unified%20pipeline%20consisting%20of%20two%20specialized%20components%3A%20FastDrive-DiT%2C%20an%20efficient%20video%20diffusion%20transformer%20for%20high-resolution%2C%20temporally%20coherent%20video%20synthesis%20under%20text%20and%20Bird%27s-Eye-View%20%28BEV%29%20layout%20guidance%3B%20and%20FastRecon3D%2C%20a%20feed-forward%20module%20that%20rapidly%20builds%203D%20Gaussian%20representations%20across%20time%2C%20ensuring%20spatial-temporal%20consistency.%20DriveGen3D%20enable%20the%20generation%20of%20long%20driving%20videos%20%28up%20to%20%24800%5Ctimes424%24%20at%20%2412%24%20FPS%29%20and%20corresponding%203D%20scenes%2C%20achieving%20state-of-the-art%20results%20while%20maintaining%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2510.15264v2&entry.124074799=Read"},
{"title": "HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation", "author": "Yuxin Wen and Qing Shuai and Di Kang and Jing Li and Cheng Wen and Yue Qian and Ningxin Jiao and Changhai Chen and Weijie Chen and Yiran Wang and Jinkun Guo and Dongyue An and Han Liu and Yanyu Tong and Chao Zhang and Qing Guo and Juan Chen and Qiao Zhang and Youyi Zhang and Zihao Yao and Cheng Zhang and Hong Duan and Xiaoping Wu and Qi Chen and Fei Cheng and Liang Dong and Peng He and Hao Zhang and Jiaxin Lin and Chao Zhang and Zhongyi Fan and Yifan Li and Zhichao Hu and Yuhong Liu and  Linus and Jie Jiang and Xiaolong Li and Linchao Bao", "abstract": "We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.", "link": "http://arxiv.org/abs/2512.23464v1", "date": "2025-12-29", "relevancy": 1.9709, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6779}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6472}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HY-Motion%201.0%3A%20Scaling%20Flow%20Matching%20Models%20for%20Text-To-Motion%20Generation&body=Title%3A%20HY-Motion%201.0%3A%20Scaling%20Flow%20Matching%20Models%20for%20Text-To-Motion%20Generation%0AAuthor%3A%20Yuxin%20Wen%20and%20Qing%20Shuai%20and%20Di%20Kang%20and%20Jing%20Li%20and%20Cheng%20Wen%20and%20Yue%20Qian%20and%20Ningxin%20Jiao%20and%20Changhai%20Chen%20and%20Weijie%20Chen%20and%20Yiran%20Wang%20and%20Jinkun%20Guo%20and%20Dongyue%20An%20and%20Han%20Liu%20and%20Yanyu%20Tong%20and%20Chao%20Zhang%20and%20Qing%20Guo%20and%20Juan%20Chen%20and%20Qiao%20Zhang%20and%20Youyi%20Zhang%20and%20Zihao%20Yao%20and%20Cheng%20Zhang%20and%20Hong%20Duan%20and%20Xiaoping%20Wu%20and%20Qi%20Chen%20and%20Fei%20Cheng%20and%20Liang%20Dong%20and%20Peng%20He%20and%20Hao%20Zhang%20and%20Jiaxin%20Lin%20and%20Chao%20Zhang%20and%20Zhongyi%20Fan%20and%20Yifan%20Li%20and%20Zhichao%20Hu%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Xiaolong%20Li%20and%20Linchao%20Bao%0AAbstract%3A%20We%20present%20HY-Motion%201.0%2C%20a%20series%20of%20state-of-the-art%2C%20large-scale%2C%20motion%20generation%20models%20capable%20of%20generating%203D%20human%20motions%20from%20textual%20descriptions.%20HY-Motion%201.0%20represents%20the%20first%20successful%20attempt%20to%20scale%20up%20Diffusion%20Transformer%20%28DiT%29-based%20flow%20matching%20models%20to%20the%20billion-parameter%20scale%20within%20the%20motion%20generation%20domain%2C%20delivering%20instruction-following%20capabilities%20that%20significantly%20outperform%20current%20open-source%20benchmarks.%20Uniquely%2C%20we%20introduce%20a%20comprehensive%2C%20full-stage%20training%20paradigm%20--%20including%20large-scale%20pretraining%20on%20over%203%2C000%20hours%20of%20motion%20data%2C%20high-quality%20fine-tuning%20on%20400%20hours%20of%20curated%20data%2C%20and%20reinforcement%20learning%20from%20both%20human%20feedback%20and%20reward%20models%20--%20to%20ensure%20precise%20alignment%20with%20the%20text%20instruction%20and%20high%20motion%20quality.%20This%20framework%20is%20supported%20by%20our%20meticulous%20data%20processing%20pipeline%2C%20which%20performs%20rigorous%20motion%20cleaning%20and%20captioning.%20Consequently%2C%20our%20model%20achieves%20the%20most%20extensive%20coverage%2C%20spanning%20over%20200%20motion%20categories%20across%206%20major%20classes.%20We%20release%20HY-Motion%201.0%20to%20the%20open-source%20community%20to%20foster%20future%20research%20and%20accelerate%20the%20transition%20of%203D%20human%20motion%20generation%20models%20towards%20commercial%20maturity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHY-Motion%25201.0%253A%2520Scaling%2520Flow%2520Matching%2520Models%2520for%2520Text-To-Motion%2520Generation%26entry.906535625%3DYuxin%2520Wen%2520and%2520Qing%2520Shuai%2520and%2520Di%2520Kang%2520and%2520Jing%2520Li%2520and%2520Cheng%2520Wen%2520and%2520Yue%2520Qian%2520and%2520Ningxin%2520Jiao%2520and%2520Changhai%2520Chen%2520and%2520Weijie%2520Chen%2520and%2520Yiran%2520Wang%2520and%2520Jinkun%2520Guo%2520and%2520Dongyue%2520An%2520and%2520Han%2520Liu%2520and%2520Yanyu%2520Tong%2520and%2520Chao%2520Zhang%2520and%2520Qing%2520Guo%2520and%2520Juan%2520Chen%2520and%2520Qiao%2520Zhang%2520and%2520Youyi%2520Zhang%2520and%2520Zihao%2520Yao%2520and%2520Cheng%2520Zhang%2520and%2520Hong%2520Duan%2520and%2520Xiaoping%2520Wu%2520and%2520Qi%2520Chen%2520and%2520Fei%2520Cheng%2520and%2520Liang%2520Dong%2520and%2520Peng%2520He%2520and%2520Hao%2520Zhang%2520and%2520Jiaxin%2520Lin%2520and%2520Chao%2520Zhang%2520and%2520Zhongyi%2520Fan%2520and%2520Yifan%2520Li%2520and%2520Zhichao%2520Hu%2520and%2520Yuhong%2520Liu%2520and%2520%2520Linus%2520and%2520Jie%2520Jiang%2520and%2520Xiaolong%2520Li%2520and%2520Linchao%2520Bao%26entry.1292438233%3DWe%2520present%2520HY-Motion%25201.0%252C%2520a%2520series%2520of%2520state-of-the-art%252C%2520large-scale%252C%2520motion%2520generation%2520models%2520capable%2520of%2520generating%25203D%2520human%2520motions%2520from%2520textual%2520descriptions.%2520HY-Motion%25201.0%2520represents%2520the%2520first%2520successful%2520attempt%2520to%2520scale%2520up%2520Diffusion%2520Transformer%2520%2528DiT%2529-based%2520flow%2520matching%2520models%2520to%2520the%2520billion-parameter%2520scale%2520within%2520the%2520motion%2520generation%2520domain%252C%2520delivering%2520instruction-following%2520capabilities%2520that%2520significantly%2520outperform%2520current%2520open-source%2520benchmarks.%2520Uniquely%252C%2520we%2520introduce%2520a%2520comprehensive%252C%2520full-stage%2520training%2520paradigm%2520--%2520including%2520large-scale%2520pretraining%2520on%2520over%25203%252C000%2520hours%2520of%2520motion%2520data%252C%2520high-quality%2520fine-tuning%2520on%2520400%2520hours%2520of%2520curated%2520data%252C%2520and%2520reinforcement%2520learning%2520from%2520both%2520human%2520feedback%2520and%2520reward%2520models%2520--%2520to%2520ensure%2520precise%2520alignment%2520with%2520the%2520text%2520instruction%2520and%2520high%2520motion%2520quality.%2520This%2520framework%2520is%2520supported%2520by%2520our%2520meticulous%2520data%2520processing%2520pipeline%252C%2520which%2520performs%2520rigorous%2520motion%2520cleaning%2520and%2520captioning.%2520Consequently%252C%2520our%2520model%2520achieves%2520the%2520most%2520extensive%2520coverage%252C%2520spanning%2520over%2520200%2520motion%2520categories%2520across%25206%2520major%2520classes.%2520We%2520release%2520HY-Motion%25201.0%2520to%2520the%2520open-source%2520community%2520to%2520foster%2520future%2520research%2520and%2520accelerate%2520the%2520transition%2520of%25203D%2520human%2520motion%2520generation%2520models%2520towards%2520commercial%2520maturity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HY-Motion%201.0%3A%20Scaling%20Flow%20Matching%20Models%20for%20Text-To-Motion%20Generation&entry.906535625=Yuxin%20Wen%20and%20Qing%20Shuai%20and%20Di%20Kang%20and%20Jing%20Li%20and%20Cheng%20Wen%20and%20Yue%20Qian%20and%20Ningxin%20Jiao%20and%20Changhai%20Chen%20and%20Weijie%20Chen%20and%20Yiran%20Wang%20and%20Jinkun%20Guo%20and%20Dongyue%20An%20and%20Han%20Liu%20and%20Yanyu%20Tong%20and%20Chao%20Zhang%20and%20Qing%20Guo%20and%20Juan%20Chen%20and%20Qiao%20Zhang%20and%20Youyi%20Zhang%20and%20Zihao%20Yao%20and%20Cheng%20Zhang%20and%20Hong%20Duan%20and%20Xiaoping%20Wu%20and%20Qi%20Chen%20and%20Fei%20Cheng%20and%20Liang%20Dong%20and%20Peng%20He%20and%20Hao%20Zhang%20and%20Jiaxin%20Lin%20and%20Chao%20Zhang%20and%20Zhongyi%20Fan%20and%20Yifan%20Li%20and%20Zhichao%20Hu%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Xiaolong%20Li%20and%20Linchao%20Bao&entry.1292438233=We%20present%20HY-Motion%201.0%2C%20a%20series%20of%20state-of-the-art%2C%20large-scale%2C%20motion%20generation%20models%20capable%20of%20generating%203D%20human%20motions%20from%20textual%20descriptions.%20HY-Motion%201.0%20represents%20the%20first%20successful%20attempt%20to%20scale%20up%20Diffusion%20Transformer%20%28DiT%29-based%20flow%20matching%20models%20to%20the%20billion-parameter%20scale%20within%20the%20motion%20generation%20domain%2C%20delivering%20instruction-following%20capabilities%20that%20significantly%20outperform%20current%20open-source%20benchmarks.%20Uniquely%2C%20we%20introduce%20a%20comprehensive%2C%20full-stage%20training%20paradigm%20--%20including%20large-scale%20pretraining%20on%20over%203%2C000%20hours%20of%20motion%20data%2C%20high-quality%20fine-tuning%20on%20400%20hours%20of%20curated%20data%2C%20and%20reinforcement%20learning%20from%20both%20human%20feedback%20and%20reward%20models%20--%20to%20ensure%20precise%20alignment%20with%20the%20text%20instruction%20and%20high%20motion%20quality.%20This%20framework%20is%20supported%20by%20our%20meticulous%20data%20processing%20pipeline%2C%20which%20performs%20rigorous%20motion%20cleaning%20and%20captioning.%20Consequently%2C%20our%20model%20achieves%20the%20most%20extensive%20coverage%2C%20spanning%20over%20200%20motion%20categories%20across%206%20major%20classes.%20We%20release%20HY-Motion%201.0%20to%20the%20open-source%20community%20to%20foster%20future%20research%20and%20accelerate%20the%20transition%20of%203D%20human%20motion%20generation%20models%20towards%20commercial%20maturity.&entry.1838667208=http%3A//arxiv.org/abs/2512.23464v1&entry.124074799=Read"},
{"title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "author": "Kabir Khan and Priya Sharma and Arjun Mehta and Neha Gupta and Ravi Narayanan", "abstract": "Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.", "link": "http://arxiv.org/abs/2508.07185v3", "date": "2025-12-29", "relevancy": 1.9671, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DySK-Attn%3A%20A%20Framework%20for%20Efficient%2C%20Real-Time%20Knowledge%20Updating%20in%20Large%20Language%20Models%20via%20Dynamic%20Sparse%20Knowledge%20Attention&body=Title%3A%20DySK-Attn%3A%20A%20Framework%20for%20Efficient%2C%20Real-Time%20Knowledge%20Updating%20in%20Large%20Language%20Models%20via%20Dynamic%20Sparse%20Knowledge%20Attention%0AAuthor%3A%20Kabir%20Khan%20and%20Priya%20Sharma%20and%20Arjun%20Mehta%20and%20Neha%20Gupta%20and%20Ravi%20Narayanan%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20a%20critical%20limitation%3A%20their%20knowledge%20is%20static%20and%20quickly%20becomes%20outdated.%20Retraining%20these%20massive%20models%20is%20computationally%20prohibitive%2C%20while%20existing%20knowledge%20editing%20techniques%20can%20be%20slow%20and%20may%20introduce%20unforeseen%20side%20effects.%20To%20address%20this%2C%20we%20propose%20DySK-Attn%2C%20a%20novel%20framework%20that%20enables%20LLMs%20to%20efficiently%20integrate%20real-time%20knowledge%20from%20a%20dynamic%20external%20source.%20Our%20approach%20synergizes%20an%20LLM%20with%20a%20dynamic%20Knowledge%20Graph%20%28KG%29%20that%20can%20be%20updated%20instantaneously.%20The%20core%20of%20our%20framework%20is%20a%20sparse%20knowledge%20attention%20mechanism%2C%20which%20allows%20the%20LLM%20to%20perform%20a%20coarse-to-fine%20grained%20search%2C%20efficiently%20identifying%20and%20focusing%20on%20a%20small%2C%20highly%20relevant%20subset%20of%20facts%20from%20the%20vast%20KG.%20This%20mechanism%20avoids%20the%20high%20computational%20cost%20of%20dense%20attention%20over%20the%20entire%20knowledge%20base%20and%20mitigates%20noise%20from%20irrelevant%20information.%20We%20demonstrate%20through%20extensive%20experiments%20on%20time-sensitive%20question-answering%20tasks%20that%20DySK-Attn%20significantly%20outperforms%20strong%20baselines%2C%20including%20standard%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20model%20editing%20techniques%2C%20in%20both%20factual%20accuracy%20for%20updated%20knowledge%20and%20computational%20efficiency.%20Our%20framework%20offers%20a%20scalable%20and%20effective%20solution%20for%20building%20LLMs%20that%20can%20stay%20current%20with%20the%20ever-changing%20world.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07185v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDySK-Attn%253A%2520A%2520Framework%2520for%2520Efficient%252C%2520Real-Time%2520Knowledge%2520Updating%2520in%2520Large%2520Language%2520Models%2520via%2520Dynamic%2520Sparse%2520Knowledge%2520Attention%26entry.906535625%3DKabir%2520Khan%2520and%2520Priya%2520Sharma%2520and%2520Arjun%2520Mehta%2520and%2520Neha%2520Gupta%2520and%2520Ravi%2520Narayanan%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520suffer%2520from%2520a%2520critical%2520limitation%253A%2520their%2520knowledge%2520is%2520static%2520and%2520quickly%2520becomes%2520outdated.%2520Retraining%2520these%2520massive%2520models%2520is%2520computationally%2520prohibitive%252C%2520while%2520existing%2520knowledge%2520editing%2520techniques%2520can%2520be%2520slow%2520and%2520may%2520introduce%2520unforeseen%2520side%2520effects.%2520To%2520address%2520this%252C%2520we%2520propose%2520DySK-Attn%252C%2520a%2520novel%2520framework%2520that%2520enables%2520LLMs%2520to%2520efficiently%2520integrate%2520real-time%2520knowledge%2520from%2520a%2520dynamic%2520external%2520source.%2520Our%2520approach%2520synergizes%2520an%2520LLM%2520with%2520a%2520dynamic%2520Knowledge%2520Graph%2520%2528KG%2529%2520that%2520can%2520be%2520updated%2520instantaneously.%2520The%2520core%2520of%2520our%2520framework%2520is%2520a%2520sparse%2520knowledge%2520attention%2520mechanism%252C%2520which%2520allows%2520the%2520LLM%2520to%2520perform%2520a%2520coarse-to-fine%2520grained%2520search%252C%2520efficiently%2520identifying%2520and%2520focusing%2520on%2520a%2520small%252C%2520highly%2520relevant%2520subset%2520of%2520facts%2520from%2520the%2520vast%2520KG.%2520This%2520mechanism%2520avoids%2520the%2520high%2520computational%2520cost%2520of%2520dense%2520attention%2520over%2520the%2520entire%2520knowledge%2520base%2520and%2520mitigates%2520noise%2520from%2520irrelevant%2520information.%2520We%2520demonstrate%2520through%2520extensive%2520experiments%2520on%2520time-sensitive%2520question-answering%2520tasks%2520that%2520DySK-Attn%2520significantly%2520outperforms%2520strong%2520baselines%252C%2520including%2520standard%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520and%2520model%2520editing%2520techniques%252C%2520in%2520both%2520factual%2520accuracy%2520for%2520updated%2520knowledge%2520and%2520computational%2520efficiency.%2520Our%2520framework%2520offers%2520a%2520scalable%2520and%2520effective%2520solution%2520for%2520building%2520LLMs%2520that%2520can%2520stay%2520current%2520with%2520the%2520ever-changing%2520world.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07185v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DySK-Attn%3A%20A%20Framework%20for%20Efficient%2C%20Real-Time%20Knowledge%20Updating%20in%20Large%20Language%20Models%20via%20Dynamic%20Sparse%20Knowledge%20Attention&entry.906535625=Kabir%20Khan%20and%20Priya%20Sharma%20and%20Arjun%20Mehta%20and%20Neha%20Gupta%20and%20Ravi%20Narayanan&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20a%20critical%20limitation%3A%20their%20knowledge%20is%20static%20and%20quickly%20becomes%20outdated.%20Retraining%20these%20massive%20models%20is%20computationally%20prohibitive%2C%20while%20existing%20knowledge%20editing%20techniques%20can%20be%20slow%20and%20may%20introduce%20unforeseen%20side%20effects.%20To%20address%20this%2C%20we%20propose%20DySK-Attn%2C%20a%20novel%20framework%20that%20enables%20LLMs%20to%20efficiently%20integrate%20real-time%20knowledge%20from%20a%20dynamic%20external%20source.%20Our%20approach%20synergizes%20an%20LLM%20with%20a%20dynamic%20Knowledge%20Graph%20%28KG%29%20that%20can%20be%20updated%20instantaneously.%20The%20core%20of%20our%20framework%20is%20a%20sparse%20knowledge%20attention%20mechanism%2C%20which%20allows%20the%20LLM%20to%20perform%20a%20coarse-to-fine%20grained%20search%2C%20efficiently%20identifying%20and%20focusing%20on%20a%20small%2C%20highly%20relevant%20subset%20of%20facts%20from%20the%20vast%20KG.%20This%20mechanism%20avoids%20the%20high%20computational%20cost%20of%20dense%20attention%20over%20the%20entire%20knowledge%20base%20and%20mitigates%20noise%20from%20irrelevant%20information.%20We%20demonstrate%20through%20extensive%20experiments%20on%20time-sensitive%20question-answering%20tasks%20that%20DySK-Attn%20significantly%20outperforms%20strong%20baselines%2C%20including%20standard%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20model%20editing%20techniques%2C%20in%20both%20factual%20accuracy%20for%20updated%20knowledge%20and%20computational%20efficiency.%20Our%20framework%20offers%20a%20scalable%20and%20effective%20solution%20for%20building%20LLMs%20that%20can%20stay%20current%20with%20the%20ever-changing%20world.&entry.1838667208=http%3A//arxiv.org/abs/2508.07185v3&entry.124074799=Read"},
{"title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "author": "Haoyuan Li and Yuanbo Tong and Yuchen Li and Zirui Wang and Chunhou Liu and Jiamou Liu", "abstract": "Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.", "link": "http://arxiv.org/abs/2511.00115v2", "date": "2025-12-29", "relevancy": 1.9623, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Alignment%20in%20Personality%20Reasoning%3A%20Leveraging%20Prototype%20Theory%20for%20MBTI%20Inference&body=Title%3A%20Cognitive%20Alignment%20in%20Personality%20Reasoning%3A%20Leveraging%20Prototype%20Theory%20for%20MBTI%20Inference%0AAuthor%3A%20Haoyuan%20Li%20and%20Yuanbo%20Tong%20and%20Yuchen%20Li%20and%20Zirui%20Wang%20and%20Chunhou%20Liu%20and%20Jiamou%20Liu%0AAbstract%3A%20Personality%20recognition%20from%20text%20is%20typically%20cast%20as%20hard-label%20classification%2C%20which%20obscures%20the%20graded%2C%20prototype-like%20nature%20of%20human%20personality%20judgments.%20We%20present%20ProtoMBTI%2C%20a%20cognitively%20aligned%20framework%20for%20MBTI%20inference%20that%20operationalizes%20prototype%20theory%20within%20an%20LLM-based%20pipeline.%20First%2C%20we%20construct%20a%20balanced%2C%20quality-controlled%20corpus%20via%20LLM-guided%20multi-dimensional%20augmentation%20%28semantic%2C%20linguistic%2C%20sentiment%29.%20Next%2C%20we%20LoRA-fine-tune%20a%20lightweight%20%28%3C%3D2B%29%20encoder%20to%20learn%20discriminative%20embeddings%20and%20to%20standardize%20a%20bank%20of%20personality%20prototypes.%20At%20inference%2C%20we%20retrieve%20top-k%20prototypes%20for%20a%20query%20post%20and%20perform%20a%20retrieve--reuse--revise--retain%20cycle%3A%20the%20model%20aggregates%20prototype%20evidence%20via%20prompt-based%20voting%2C%20revises%20when%20inconsistencies%20arise%2C%20and%2C%20upon%20correct%20prediction%2C%20retains%20the%20sample%20to%20continually%20enrich%20the%20prototype%20library.%20Across%20Kaggle%20and%20Pandora%20benchmarks%2C%20ProtoMBTI%20improves%20over%20baselines%20on%20both%20the%20four%20MBTI%20dichotomies%20and%20the%20full%2016-type%20task%2C%20and%20exhibits%20robust%20cross-dataset%20generalization.%20Our%20results%20indicate%20that%20aligning%20the%20inference%20process%20with%20psychological%20prototype%20reasoning%20yields%20gains%20in%20accuracy%2C%20interpretability%2C%20and%20transfer%20for%20text-based%20personality%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Alignment%2520in%2520Personality%2520Reasoning%253A%2520Leveraging%2520Prototype%2520Theory%2520for%2520MBTI%2520Inference%26entry.906535625%3DHaoyuan%2520Li%2520and%2520Yuanbo%2520Tong%2520and%2520Yuchen%2520Li%2520and%2520Zirui%2520Wang%2520and%2520Chunhou%2520Liu%2520and%2520Jiamou%2520Liu%26entry.1292438233%3DPersonality%2520recognition%2520from%2520text%2520is%2520typically%2520cast%2520as%2520hard-label%2520classification%252C%2520which%2520obscures%2520the%2520graded%252C%2520prototype-like%2520nature%2520of%2520human%2520personality%2520judgments.%2520We%2520present%2520ProtoMBTI%252C%2520a%2520cognitively%2520aligned%2520framework%2520for%2520MBTI%2520inference%2520that%2520operationalizes%2520prototype%2520theory%2520within%2520an%2520LLM-based%2520pipeline.%2520First%252C%2520we%2520construct%2520a%2520balanced%252C%2520quality-controlled%2520corpus%2520via%2520LLM-guided%2520multi-dimensional%2520augmentation%2520%2528semantic%252C%2520linguistic%252C%2520sentiment%2529.%2520Next%252C%2520we%2520LoRA-fine-tune%2520a%2520lightweight%2520%2528%253C%253D2B%2529%2520encoder%2520to%2520learn%2520discriminative%2520embeddings%2520and%2520to%2520standardize%2520a%2520bank%2520of%2520personality%2520prototypes.%2520At%2520inference%252C%2520we%2520retrieve%2520top-k%2520prototypes%2520for%2520a%2520query%2520post%2520and%2520perform%2520a%2520retrieve--reuse--revise--retain%2520cycle%253A%2520the%2520model%2520aggregates%2520prototype%2520evidence%2520via%2520prompt-based%2520voting%252C%2520revises%2520when%2520inconsistencies%2520arise%252C%2520and%252C%2520upon%2520correct%2520prediction%252C%2520retains%2520the%2520sample%2520to%2520continually%2520enrich%2520the%2520prototype%2520library.%2520Across%2520Kaggle%2520and%2520Pandora%2520benchmarks%252C%2520ProtoMBTI%2520improves%2520over%2520baselines%2520on%2520both%2520the%2520four%2520MBTI%2520dichotomies%2520and%2520the%2520full%252016-type%2520task%252C%2520and%2520exhibits%2520robust%2520cross-dataset%2520generalization.%2520Our%2520results%2520indicate%2520that%2520aligning%2520the%2520inference%2520process%2520with%2520psychological%2520prototype%2520reasoning%2520yields%2520gains%2520in%2520accuracy%252C%2520interpretability%252C%2520and%2520transfer%2520for%2520text-based%2520personality%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Alignment%20in%20Personality%20Reasoning%3A%20Leveraging%20Prototype%20Theory%20for%20MBTI%20Inference&entry.906535625=Haoyuan%20Li%20and%20Yuanbo%20Tong%20and%20Yuchen%20Li%20and%20Zirui%20Wang%20and%20Chunhou%20Liu%20and%20Jiamou%20Liu&entry.1292438233=Personality%20recognition%20from%20text%20is%20typically%20cast%20as%20hard-label%20classification%2C%20which%20obscures%20the%20graded%2C%20prototype-like%20nature%20of%20human%20personality%20judgments.%20We%20present%20ProtoMBTI%2C%20a%20cognitively%20aligned%20framework%20for%20MBTI%20inference%20that%20operationalizes%20prototype%20theory%20within%20an%20LLM-based%20pipeline.%20First%2C%20we%20construct%20a%20balanced%2C%20quality-controlled%20corpus%20via%20LLM-guided%20multi-dimensional%20augmentation%20%28semantic%2C%20linguistic%2C%20sentiment%29.%20Next%2C%20we%20LoRA-fine-tune%20a%20lightweight%20%28%3C%3D2B%29%20encoder%20to%20learn%20discriminative%20embeddings%20and%20to%20standardize%20a%20bank%20of%20personality%20prototypes.%20At%20inference%2C%20we%20retrieve%20top-k%20prototypes%20for%20a%20query%20post%20and%20perform%20a%20retrieve--reuse--revise--retain%20cycle%3A%20the%20model%20aggregates%20prototype%20evidence%20via%20prompt-based%20voting%2C%20revises%20when%20inconsistencies%20arise%2C%20and%2C%20upon%20correct%20prediction%2C%20retains%20the%20sample%20to%20continually%20enrich%20the%20prototype%20library.%20Across%20Kaggle%20and%20Pandora%20benchmarks%2C%20ProtoMBTI%20improves%20over%20baselines%20on%20both%20the%20four%20MBTI%20dichotomies%20and%20the%20full%2016-type%20task%2C%20and%20exhibits%20robust%20cross-dataset%20generalization.%20Our%20results%20indicate%20that%20aligning%20the%20inference%20process%20with%20psychological%20prototype%20reasoning%20yields%20gains%20in%20accuracy%2C%20interpretability%2C%20and%20transfer%20for%20text-based%20personality%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.00115v2&entry.124074799=Read"},
{"title": "The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction", "author": "Haoyu Pei and Zhongyang Liu and Xiangyi Xiao and Xiaocong Du and Haipeng Zhang and Kunpeng Zhang and Suting Hong", "abstract": "Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.", "link": "http://arxiv.org/abs/2512.23489v1", "date": "2025-12-29", "relevancy": 1.959, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5046}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Gaining%20Paths%20to%20Investment%20Success%3A%20Information-Driven%20LLM%20Graph%20Reasoning%20for%20Venture%20Capital%20Prediction&body=Title%3A%20The%20Gaining%20Paths%20to%20Investment%20Success%3A%20Information-Driven%20LLM%20Graph%20Reasoning%20for%20Venture%20Capital%20Prediction%0AAuthor%3A%20Haoyu%20Pei%20and%20Zhongyang%20Liu%20and%20Xiangyi%20Xiao%20and%20Xiaocong%20Du%20and%20Haipeng%20Zhang%20and%20Kunpeng%20Zhang%20and%20Suting%20Hong%0AAbstract%3A%20Most%20venture%20capital%20%28VC%29%20investments%20fail%2C%20while%20a%20few%20deliver%20outsized%20returns.%20Accurately%20predicting%20startup%20success%20requires%20synthesizing%20complex%20relational%20evidence%2C%20including%20company%20disclosures%2C%20investor%20track%20records%2C%20and%20investment%20network%20structures%2C%20through%20explicit%20reasoning%20to%20form%20coherent%2C%20interpretable%20investment%20theses.%20Traditional%20machine%20learning%20and%20graph%20neural%20networks%20both%20lack%20this%20reasoning%20capability.%20Large%20language%20models%20%28LLMs%29%20offer%20strong%20reasoning%20but%20face%20a%20modality%20mismatch%20with%20graphs.%20Recent%20graph-LLM%20methods%20target%20in-graph%20tasks%20where%20answers%20lie%20within%20the%20graph%2C%20whereas%20VC%20prediction%20is%20off-graph%3A%20the%20target%20exists%20outside%20the%20network.%20The%20core%20challenge%20is%20selecting%20graph%20paths%20that%20maximize%20predictor%20performance%20on%20an%20external%20objective%20while%20enabling%20step-by-step%20reasoning.%20We%20present%20MIRAGE-VC%2C%20a%20multi-perspective%20retrieval-augmented%20generation%20framework%20that%20addresses%20two%20obstacles%3A%20path%20explosion%20%28thousands%20of%20candidate%20paths%20overwhelm%20LLM%20context%29%20and%20heterogeneous%20evidence%20fusion%20%28different%20startups%20need%20different%20analytical%20emphasis%29.%20Our%20information-gain-driven%20path%20retriever%20iteratively%20selects%20high-value%20neighbors%2C%20distilling%20investment%20networks%20into%20compact%20chains%20for%20explicit%20reasoning.%20A%20multi-agent%20architecture%20integrates%20three%20evidence%20streams%20via%20a%20learnable%20gating%20mechanism%20based%20on%20company%20attributes.%20Under%20strict%20anti-leakage%20controls%2C%20MIRAGE-VC%20achieves%20%2B5.0%25%20F1%20and%20%2B16.6%25%20PrecisionAt5%2C%20and%20sheds%20light%20on%20other%20off-graph%20prediction%20tasks%20such%20as%20recommendation%20and%20risk%20assessment.%20Code%3A%20https%3A//anonymous.4open.science/r/MIRAGE-VC-323F.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Gaining%2520Paths%2520to%2520Investment%2520Success%253A%2520Information-Driven%2520LLM%2520Graph%2520Reasoning%2520for%2520Venture%2520Capital%2520Prediction%26entry.906535625%3DHaoyu%2520Pei%2520and%2520Zhongyang%2520Liu%2520and%2520Xiangyi%2520Xiao%2520and%2520Xiaocong%2520Du%2520and%2520Haipeng%2520Zhang%2520and%2520Kunpeng%2520Zhang%2520and%2520Suting%2520Hong%26entry.1292438233%3DMost%2520venture%2520capital%2520%2528VC%2529%2520investments%2520fail%252C%2520while%2520a%2520few%2520deliver%2520outsized%2520returns.%2520Accurately%2520predicting%2520startup%2520success%2520requires%2520synthesizing%2520complex%2520relational%2520evidence%252C%2520including%2520company%2520disclosures%252C%2520investor%2520track%2520records%252C%2520and%2520investment%2520network%2520structures%252C%2520through%2520explicit%2520reasoning%2520to%2520form%2520coherent%252C%2520interpretable%2520investment%2520theses.%2520Traditional%2520machine%2520learning%2520and%2520graph%2520neural%2520networks%2520both%2520lack%2520this%2520reasoning%2520capability.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520strong%2520reasoning%2520but%2520face%2520a%2520modality%2520mismatch%2520with%2520graphs.%2520Recent%2520graph-LLM%2520methods%2520target%2520in-graph%2520tasks%2520where%2520answers%2520lie%2520within%2520the%2520graph%252C%2520whereas%2520VC%2520prediction%2520is%2520off-graph%253A%2520the%2520target%2520exists%2520outside%2520the%2520network.%2520The%2520core%2520challenge%2520is%2520selecting%2520graph%2520paths%2520that%2520maximize%2520predictor%2520performance%2520on%2520an%2520external%2520objective%2520while%2520enabling%2520step-by-step%2520reasoning.%2520We%2520present%2520MIRAGE-VC%252C%2520a%2520multi-perspective%2520retrieval-augmented%2520generation%2520framework%2520that%2520addresses%2520two%2520obstacles%253A%2520path%2520explosion%2520%2528thousands%2520of%2520candidate%2520paths%2520overwhelm%2520LLM%2520context%2529%2520and%2520heterogeneous%2520evidence%2520fusion%2520%2528different%2520startups%2520need%2520different%2520analytical%2520emphasis%2529.%2520Our%2520information-gain-driven%2520path%2520retriever%2520iteratively%2520selects%2520high-value%2520neighbors%252C%2520distilling%2520investment%2520networks%2520into%2520compact%2520chains%2520for%2520explicit%2520reasoning.%2520A%2520multi-agent%2520architecture%2520integrates%2520three%2520evidence%2520streams%2520via%2520a%2520learnable%2520gating%2520mechanism%2520based%2520on%2520company%2520attributes.%2520Under%2520strict%2520anti-leakage%2520controls%252C%2520MIRAGE-VC%2520achieves%2520%252B5.0%2525%2520F1%2520and%2520%252B16.6%2525%2520PrecisionAt5%252C%2520and%2520sheds%2520light%2520on%2520other%2520off-graph%2520prediction%2520tasks%2520such%2520as%2520recommendation%2520and%2520risk%2520assessment.%2520Code%253A%2520https%253A//anonymous.4open.science/r/MIRAGE-VC-323F.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Gaining%20Paths%20to%20Investment%20Success%3A%20Information-Driven%20LLM%20Graph%20Reasoning%20for%20Venture%20Capital%20Prediction&entry.906535625=Haoyu%20Pei%20and%20Zhongyang%20Liu%20and%20Xiangyi%20Xiao%20and%20Xiaocong%20Du%20and%20Haipeng%20Zhang%20and%20Kunpeng%20Zhang%20and%20Suting%20Hong&entry.1292438233=Most%20venture%20capital%20%28VC%29%20investments%20fail%2C%20while%20a%20few%20deliver%20outsized%20returns.%20Accurately%20predicting%20startup%20success%20requires%20synthesizing%20complex%20relational%20evidence%2C%20including%20company%20disclosures%2C%20investor%20track%20records%2C%20and%20investment%20network%20structures%2C%20through%20explicit%20reasoning%20to%20form%20coherent%2C%20interpretable%20investment%20theses.%20Traditional%20machine%20learning%20and%20graph%20neural%20networks%20both%20lack%20this%20reasoning%20capability.%20Large%20language%20models%20%28LLMs%29%20offer%20strong%20reasoning%20but%20face%20a%20modality%20mismatch%20with%20graphs.%20Recent%20graph-LLM%20methods%20target%20in-graph%20tasks%20where%20answers%20lie%20within%20the%20graph%2C%20whereas%20VC%20prediction%20is%20off-graph%3A%20the%20target%20exists%20outside%20the%20network.%20The%20core%20challenge%20is%20selecting%20graph%20paths%20that%20maximize%20predictor%20performance%20on%20an%20external%20objective%20while%20enabling%20step-by-step%20reasoning.%20We%20present%20MIRAGE-VC%2C%20a%20multi-perspective%20retrieval-augmented%20generation%20framework%20that%20addresses%20two%20obstacles%3A%20path%20explosion%20%28thousands%20of%20candidate%20paths%20overwhelm%20LLM%20context%29%20and%20heterogeneous%20evidence%20fusion%20%28different%20startups%20need%20different%20analytical%20emphasis%29.%20Our%20information-gain-driven%20path%20retriever%20iteratively%20selects%20high-value%20neighbors%2C%20distilling%20investment%20networks%20into%20compact%20chains%20for%20explicit%20reasoning.%20A%20multi-agent%20architecture%20integrates%20three%20evidence%20streams%20via%20a%20learnable%20gating%20mechanism%20based%20on%20company%20attributes.%20Under%20strict%20anti-leakage%20controls%2C%20MIRAGE-VC%20achieves%20%2B5.0%25%20F1%20and%20%2B16.6%25%20PrecisionAt5%2C%20and%20sheds%20light%20on%20other%20off-graph%20prediction%20tasks%20such%20as%20recommendation%20and%20risk%20assessment.%20Code%3A%20https%3A//anonymous.4open.science/r/MIRAGE-VC-323F.&entry.1838667208=http%3A//arxiv.org/abs/2512.23489v1&entry.124074799=Read"},
{"title": "Timepoint-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI", "author": "Wenhao Guo and Golrokh Mirzaei", "abstract": "Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.", "link": "http://arxiv.org/abs/2511.18595v2", "date": "2025-12-29", "relevancy": 1.9526, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4785}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timepoint-Specific%20Benchmarking%20of%20Deep%20Learning%20Models%20for%20Glioblastoma%20Follow-Up%20MRI&body=Title%3A%20Timepoint-Specific%20Benchmarking%20of%20Deep%20Learning%20Models%20for%20Glioblastoma%20Follow-Up%20MRI%0AAuthor%3A%20Wenhao%20Guo%20and%20Golrokh%20Mirzaei%0AAbstract%3A%20Differentiating%20true%20tumor%20progression%20%28TP%29%20from%20treatment-related%20pseudoprogression%20%28PsP%29%20in%20glioblastoma%20remains%20challenging%2C%20especially%20at%20early%20follow-up.%20We%20present%20the%20first%20stage-specific%2C%20cross-sectional%20benchmarking%20of%20deep%20learning%20models%20for%20follow-up%20MRI%20using%20the%20Burdenko%20GBM%20Progression%20cohort%20%28n%20%3D%20180%29.%20We%20analyze%20different%20post-RT%20scans%20independently%20to%20test%20whether%20architecture%20performance%20depends%20on%20time-point.%20Eleven%20representative%20DL%20families%20%28CNNs%2C%20LSTMs%2C%20hybrids%2C%20transformers%2C%20and%20selective%20state-space%20models%29%20were%20trained%20under%20a%20unified%2C%20QC-driven%20pipeline%20with%20patient-level%20cross-validation.%20Across%20both%20stages%2C%20accuracies%20were%20comparable%20%28~0.70-0.74%29%2C%20but%20discrimination%20improved%20at%20the%20second%20follow-up%2C%20with%20F1%20and%20AUC%20increasing%20for%20several%20models%2C%20indicating%20richer%20separability%20later%20in%20the%20care%20pathway.%20A%20Mamba%2BCNN%20hybrid%20consistently%20offered%20the%20best%20accuracy-efficiency%20trade-off%2C%20while%20transformer%20variants%20delivered%20competitive%20AUCs%20at%20substantially%20higher%20computational%20cost%20and%20lightweight%20CNNs%20were%20efficient%20but%20less%20reliable.%20Performance%20also%20showed%20sensitivity%20to%20batch%20size%2C%20underscoring%20the%20need%20for%20standardized%20training%20protocols.%20Notably%2C%20absolute%20discrimination%20remained%20modest%20overall%2C%20reflecting%20the%20intrinsic%20difficulty%20of%20TP%20vs.%20PsP%20and%20the%20dataset%27s%20size%20imbalance.%20These%20results%20establish%20a%20stage-aware%20benchmark%20and%20motivate%20future%20work%20incorporating%20longitudinal%20modeling%2C%20multi-sequence%20MRI%2C%20and%20larger%20multi-center%20cohorts.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimepoint-Specific%2520Benchmarking%2520of%2520Deep%2520Learning%2520Models%2520for%2520Glioblastoma%2520Follow-Up%2520MRI%26entry.906535625%3DWenhao%2520Guo%2520and%2520Golrokh%2520Mirzaei%26entry.1292438233%3DDifferentiating%2520true%2520tumor%2520progression%2520%2528TP%2529%2520from%2520treatment-related%2520pseudoprogression%2520%2528PsP%2529%2520in%2520glioblastoma%2520remains%2520challenging%252C%2520especially%2520at%2520early%2520follow-up.%2520We%2520present%2520the%2520first%2520stage-specific%252C%2520cross-sectional%2520benchmarking%2520of%2520deep%2520learning%2520models%2520for%2520follow-up%2520MRI%2520using%2520the%2520Burdenko%2520GBM%2520Progression%2520cohort%2520%2528n%2520%253D%2520180%2529.%2520We%2520analyze%2520different%2520post-RT%2520scans%2520independently%2520to%2520test%2520whether%2520architecture%2520performance%2520depends%2520on%2520time-point.%2520Eleven%2520representative%2520DL%2520families%2520%2528CNNs%252C%2520LSTMs%252C%2520hybrids%252C%2520transformers%252C%2520and%2520selective%2520state-space%2520models%2529%2520were%2520trained%2520under%2520a%2520unified%252C%2520QC-driven%2520pipeline%2520with%2520patient-level%2520cross-validation.%2520Across%2520both%2520stages%252C%2520accuracies%2520were%2520comparable%2520%2528~0.70-0.74%2529%252C%2520but%2520discrimination%2520improved%2520at%2520the%2520second%2520follow-up%252C%2520with%2520F1%2520and%2520AUC%2520increasing%2520for%2520several%2520models%252C%2520indicating%2520richer%2520separability%2520later%2520in%2520the%2520care%2520pathway.%2520A%2520Mamba%252BCNN%2520hybrid%2520consistently%2520offered%2520the%2520best%2520accuracy-efficiency%2520trade-off%252C%2520while%2520transformer%2520variants%2520delivered%2520competitive%2520AUCs%2520at%2520substantially%2520higher%2520computational%2520cost%2520and%2520lightweight%2520CNNs%2520were%2520efficient%2520but%2520less%2520reliable.%2520Performance%2520also%2520showed%2520sensitivity%2520to%2520batch%2520size%252C%2520underscoring%2520the%2520need%2520for%2520standardized%2520training%2520protocols.%2520Notably%252C%2520absolute%2520discrimination%2520remained%2520modest%2520overall%252C%2520reflecting%2520the%2520intrinsic%2520difficulty%2520of%2520TP%2520vs.%2520PsP%2520and%2520the%2520dataset%2527s%2520size%2520imbalance.%2520These%2520results%2520establish%2520a%2520stage-aware%2520benchmark%2520and%2520motivate%2520future%2520work%2520incorporating%2520longitudinal%2520modeling%252C%2520multi-sequence%2520MRI%252C%2520and%2520larger%2520multi-center%2520cohorts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timepoint-Specific%20Benchmarking%20of%20Deep%20Learning%20Models%20for%20Glioblastoma%20Follow-Up%20MRI&entry.906535625=Wenhao%20Guo%20and%20Golrokh%20Mirzaei&entry.1292438233=Differentiating%20true%20tumor%20progression%20%28TP%29%20from%20treatment-related%20pseudoprogression%20%28PsP%29%20in%20glioblastoma%20remains%20challenging%2C%20especially%20at%20early%20follow-up.%20We%20present%20the%20first%20stage-specific%2C%20cross-sectional%20benchmarking%20of%20deep%20learning%20models%20for%20follow-up%20MRI%20using%20the%20Burdenko%20GBM%20Progression%20cohort%20%28n%20%3D%20180%29.%20We%20analyze%20different%20post-RT%20scans%20independently%20to%20test%20whether%20architecture%20performance%20depends%20on%20time-point.%20Eleven%20representative%20DL%20families%20%28CNNs%2C%20LSTMs%2C%20hybrids%2C%20transformers%2C%20and%20selective%20state-space%20models%29%20were%20trained%20under%20a%20unified%2C%20QC-driven%20pipeline%20with%20patient-level%20cross-validation.%20Across%20both%20stages%2C%20accuracies%20were%20comparable%20%28~0.70-0.74%29%2C%20but%20discrimination%20improved%20at%20the%20second%20follow-up%2C%20with%20F1%20and%20AUC%20increasing%20for%20several%20models%2C%20indicating%20richer%20separability%20later%20in%20the%20care%20pathway.%20A%20Mamba%2BCNN%20hybrid%20consistently%20offered%20the%20best%20accuracy-efficiency%20trade-off%2C%20while%20transformer%20variants%20delivered%20competitive%20AUCs%20at%20substantially%20higher%20computational%20cost%20and%20lightweight%20CNNs%20were%20efficient%20but%20less%20reliable.%20Performance%20also%20showed%20sensitivity%20to%20batch%20size%2C%20underscoring%20the%20need%20for%20standardized%20training%20protocols.%20Notably%2C%20absolute%20discrimination%20remained%20modest%20overall%2C%20reflecting%20the%20intrinsic%20difficulty%20of%20TP%20vs.%20PsP%20and%20the%20dataset%27s%20size%20imbalance.%20These%20results%20establish%20a%20stage-aware%20benchmark%20and%20motivate%20future%20work%20incorporating%20longitudinal%20modeling%2C%20multi-sequence%20MRI%2C%20and%20larger%20multi-center%20cohorts.&entry.1838667208=http%3A//arxiv.org/abs/2511.18595v2&entry.124074799=Read"},
{"title": "Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling", "author": "Christophe D. Hounwanou and Yae Ulrich Gaba", "abstract": "Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance. This paper investigates the use of deep generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) to generate realistic synthetic financial return series for portfolio construction and risk modeling applications. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean--variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.", "link": "http://arxiv.org/abs/2512.21798v2", "date": "2025-12-29", "relevancy": 1.5992, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5007}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Models%20for%20Synthetic%20Financial%20Data%3A%20Applications%20to%20Portfolio%20and%20Risk%20Modeling&body=Title%3A%20Deep%20Generative%20Models%20for%20Synthetic%20Financial%20Data%3A%20Applications%20to%20Portfolio%20and%20Risk%20Modeling%0AAuthor%3A%20Christophe%20D.%20Hounwanou%20and%20Yae%20Ulrich%20Gaba%0AAbstract%3A%20Synthetic%20financial%20data%20provides%20a%20practical%20solution%20to%20the%20privacy%2C%20accessibility%2C%20and%20reproducibility%20challenges%20that%20often%20constrain%20empirical%20research%20in%20quantitative%20finance.%20This%20paper%20investigates%20the%20use%20of%20deep%20generative%20models%2C%20specifically%20Time-series%20Generative%20Adversarial%20Networks%20%28TimeGAN%29%20and%20Variational%20Autoencoders%20%28VAEs%29%20to%20generate%20realistic%20synthetic%20financial%20return%20series%20for%20portfolio%20construction%20and%20risk%20modeling%20applications.%20Using%20historical%20daily%20returns%20from%20the%20S%20and%20P%20500%20as%20a%20benchmark%2C%20we%20generate%20synthetic%20datasets%20under%20comparable%20market%20conditions%20and%20evaluate%20them%20using%20statistical%20similarity%20metrics%2C%20temporal%20structure%20tests%2C%20and%20downstream%20financial%20tasks.%20The%20study%20shows%20that%20TimeGAN%20produces%20synthetic%20data%20with%20distributional%20shapes%2C%20volatility%20patterns%2C%20and%20autocorrelation%20behaviour%20that%20are%20close%20to%20those%20observed%20in%20real%20returns.%20When%20applied%20to%20mean--variance%20portfolio%20optimization%2C%20the%20resulting%20synthetic%20datasets%20lead%20to%20portfolio%20weights%2C%20Sharpe%20ratios%2C%20and%20risk%20levels%20that%20remain%20close%20to%20those%20obtained%20from%20real%20data.%20The%20VAE%20provides%20more%20stable%20training%20but%20tends%20to%20smooth%20extreme%20market%20movements%2C%20which%20affects%20risk%20estimation.%20Finally%2C%20the%20analysis%20supports%20the%20use%20of%20synthetic%20datasets%20as%20substitutes%20for%20real%20financial%20data%20in%20portfolio%20analysis%20and%20risk%20simulation%2C%20particularly%20when%20models%20are%20able%20to%20capture%20temporal%20dynamics.%20Synthetic%20data%20therefore%20provides%20a%20privacy-preserving%2C%20cost-effective%2C%20and%20reproducible%20tool%20for%20financial%20experimentation%20and%20model%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21798v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Models%2520for%2520Synthetic%2520Financial%2520Data%253A%2520Applications%2520to%2520Portfolio%2520and%2520Risk%2520Modeling%26entry.906535625%3DChristophe%2520D.%2520Hounwanou%2520and%2520Yae%2520Ulrich%2520Gaba%26entry.1292438233%3DSynthetic%2520financial%2520data%2520provides%2520a%2520practical%2520solution%2520to%2520the%2520privacy%252C%2520accessibility%252C%2520and%2520reproducibility%2520challenges%2520that%2520often%2520constrain%2520empirical%2520research%2520in%2520quantitative%2520finance.%2520This%2520paper%2520investigates%2520the%2520use%2520of%2520deep%2520generative%2520models%252C%2520specifically%2520Time-series%2520Generative%2520Adversarial%2520Networks%2520%2528TimeGAN%2529%2520and%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520to%2520generate%2520realistic%2520synthetic%2520financial%2520return%2520series%2520for%2520portfolio%2520construction%2520and%2520risk%2520modeling%2520applications.%2520Using%2520historical%2520daily%2520returns%2520from%2520the%2520S%2520and%2520P%2520500%2520as%2520a%2520benchmark%252C%2520we%2520generate%2520synthetic%2520datasets%2520under%2520comparable%2520market%2520conditions%2520and%2520evaluate%2520them%2520using%2520statistical%2520similarity%2520metrics%252C%2520temporal%2520structure%2520tests%252C%2520and%2520downstream%2520financial%2520tasks.%2520The%2520study%2520shows%2520that%2520TimeGAN%2520produces%2520synthetic%2520data%2520with%2520distributional%2520shapes%252C%2520volatility%2520patterns%252C%2520and%2520autocorrelation%2520behaviour%2520that%2520are%2520close%2520to%2520those%2520observed%2520in%2520real%2520returns.%2520When%2520applied%2520to%2520mean--variance%2520portfolio%2520optimization%252C%2520the%2520resulting%2520synthetic%2520datasets%2520lead%2520to%2520portfolio%2520weights%252C%2520Sharpe%2520ratios%252C%2520and%2520risk%2520levels%2520that%2520remain%2520close%2520to%2520those%2520obtained%2520from%2520real%2520data.%2520The%2520VAE%2520provides%2520more%2520stable%2520training%2520but%2520tends%2520to%2520smooth%2520extreme%2520market%2520movements%252C%2520which%2520affects%2520risk%2520estimation.%2520Finally%252C%2520the%2520analysis%2520supports%2520the%2520use%2520of%2520synthetic%2520datasets%2520as%2520substitutes%2520for%2520real%2520financial%2520data%2520in%2520portfolio%2520analysis%2520and%2520risk%2520simulation%252C%2520particularly%2520when%2520models%2520are%2520able%2520to%2520capture%2520temporal%2520dynamics.%2520Synthetic%2520data%2520therefore%2520provides%2520a%2520privacy-preserving%252C%2520cost-effective%252C%2520and%2520reproducible%2520tool%2520for%2520financial%2520experimentation%2520and%2520model%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21798v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Models%20for%20Synthetic%20Financial%20Data%3A%20Applications%20to%20Portfolio%20and%20Risk%20Modeling&entry.906535625=Christophe%20D.%20Hounwanou%20and%20Yae%20Ulrich%20Gaba&entry.1292438233=Synthetic%20financial%20data%20provides%20a%20practical%20solution%20to%20the%20privacy%2C%20accessibility%2C%20and%20reproducibility%20challenges%20that%20often%20constrain%20empirical%20research%20in%20quantitative%20finance.%20This%20paper%20investigates%20the%20use%20of%20deep%20generative%20models%2C%20specifically%20Time-series%20Generative%20Adversarial%20Networks%20%28TimeGAN%29%20and%20Variational%20Autoencoders%20%28VAEs%29%20to%20generate%20realistic%20synthetic%20financial%20return%20series%20for%20portfolio%20construction%20and%20risk%20modeling%20applications.%20Using%20historical%20daily%20returns%20from%20the%20S%20and%20P%20500%20as%20a%20benchmark%2C%20we%20generate%20synthetic%20datasets%20under%20comparable%20market%20conditions%20and%20evaluate%20them%20using%20statistical%20similarity%20metrics%2C%20temporal%20structure%20tests%2C%20and%20downstream%20financial%20tasks.%20The%20study%20shows%20that%20TimeGAN%20produces%20synthetic%20data%20with%20distributional%20shapes%2C%20volatility%20patterns%2C%20and%20autocorrelation%20behaviour%20that%20are%20close%20to%20those%20observed%20in%20real%20returns.%20When%20applied%20to%20mean--variance%20portfolio%20optimization%2C%20the%20resulting%20synthetic%20datasets%20lead%20to%20portfolio%20weights%2C%20Sharpe%20ratios%2C%20and%20risk%20levels%20that%20remain%20close%20to%20those%20obtained%20from%20real%20data.%20The%20VAE%20provides%20more%20stable%20training%20but%20tends%20to%20smooth%20extreme%20market%20movements%2C%20which%20affects%20risk%20estimation.%20Finally%2C%20the%20analysis%20supports%20the%20use%20of%20synthetic%20datasets%20as%20substitutes%20for%20real%20financial%20data%20in%20portfolio%20analysis%20and%20risk%20simulation%2C%20particularly%20when%20models%20are%20able%20to%20capture%20temporal%20dynamics.%20Synthetic%20data%20therefore%20provides%20a%20privacy-preserving%2C%20cost-effective%2C%20and%20reproducible%20tool%20for%20financial%20experimentation%20and%20model%20development.&entry.1838667208=http%3A//arxiv.org/abs/2512.21798v2&entry.124074799=Read"},
{"title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks", "author": "Toqeer Ali Syed and Mishal Ateeq Almutairi and Mahmoud Abdel Moaty", "abstract": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.", "link": "http://arxiv.org/abs/2512.23557v1", "date": "2025-12-29", "relevancy": 1.9465, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Trustworthy%20Agentic%20AI%3A%20A%20Multimodal%20Framework%20for%20Preventing%20Prompt%20Injection%20Attacks&body=Title%3A%20Toward%20Trustworthy%20Agentic%20AI%3A%20A%20Multimodal%20Framework%20for%20Preventing%20Prompt%20Injection%20Attacks%0AAuthor%3A%20Toqeer%20Ali%20Syed%20and%20Mishal%20Ateeq%20Almutairi%20and%20Mahmoud%20Abdel%20Moaty%0AAbstract%3A%20Powerful%20autonomous%20systems%2C%20which%20reason%2C%20plan%2C%20and%20converse%20using%20and%20between%20numerous%20tools%20and%20agents%2C%20are%20made%20possible%20by%20Large%20Language%20Models%20%28LLMs%29%2C%20Vision-Language%20Models%20%28VLMs%29%2C%20and%20new%20agentic%20AI%20systems%2C%20like%20LangChain%20and%20GraphChain.%20Nevertheless%2C%20this%20agentic%20environment%20increases%20the%20probability%20of%20the%20occurrence%20of%20multimodal%20prompt%20injection%20%28PI%29%20attacks%2C%20in%20which%20concealed%20or%20malicious%20instructions%20carried%20in%20text%2C%20pictures%2C%20metadata%2C%20or%20agent-to-agent%20messages%20may%20spread%20throughout%20the%20graph%20and%20lead%20to%20unintended%20behavior%2C%20a%20breach%20of%20policy%2C%20or%20corruption%20of%20state.%20In%20order%20to%20mitigate%20these%20risks%2C%20this%20paper%20suggests%20a%20Cross-Agent%20Multimodal%20Provenanc-%20Aware%20Defense%20Framework%20whereby%20all%20the%20prompts%2C%20either%20user-generated%20or%20produced%20by%20upstream%20agents%2C%20are%20sanitized%20and%20all%20the%20outputs%20generated%20by%20an%20LLM%20are%20verified%20independently%20before%20being%20sent%20to%20downstream%20nodes.%20This%20framework%20contains%20a%20Text%20sanitizer%20agent%2C%20visual%20sanitizer%20agent%2C%20and%20output%20validator%20agent%20all%20coordinated%20by%20a%20provenance%20ledger%2C%20which%20keeps%20metadata%20of%20modality%2C%20source%2C%20and%20trust%20level%20throughout%20the%20entire%20agent%20network.%20This%20architecture%20makes%20sure%20that%20agent-to-agent%20communication%20abides%20by%20clear%20trust%20frames%20such%20such%20that%20injected%20instructions%20are%20not%20propagated%20down%20LangChain%20or%20GraphChain-style-workflows.%20The%20experimental%20assessments%20show%20that%20multimodal%20injection%20detection%20accuracy%20is%20significantly%20enhanced%2C%20and%20the%20cross-agent%20trust%20leakage%20is%20minimized%2C%20as%20well%20as%2C%20agentic%20execution%20pathways%20become%20stable.%20The%20framework%2C%20which%20expands%20the%20concept%20of%20provenance%20tracking%20and%20validation%20to%20the%20multi-agent%20orchestration%2C%20enhances%20the%20establishment%20of%20secure%2C%20understandable%20and%20reliable%20agentic%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Trustworthy%2520Agentic%2520AI%253A%2520A%2520Multimodal%2520Framework%2520for%2520Preventing%2520Prompt%2520Injection%2520Attacks%26entry.906535625%3DToqeer%2520Ali%2520Syed%2520and%2520Mishal%2520Ateeq%2520Almutairi%2520and%2520Mahmoud%2520Abdel%2520Moaty%26entry.1292438233%3DPowerful%2520autonomous%2520systems%252C%2520which%2520reason%252C%2520plan%252C%2520and%2520converse%2520using%2520and%2520between%2520numerous%2520tools%2520and%2520agents%252C%2520are%2520made%2520possible%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520and%2520new%2520agentic%2520AI%2520systems%252C%2520like%2520LangChain%2520and%2520GraphChain.%2520Nevertheless%252C%2520this%2520agentic%2520environment%2520increases%2520the%2520probability%2520of%2520the%2520occurrence%2520of%2520multimodal%2520prompt%2520injection%2520%2528PI%2529%2520attacks%252C%2520in%2520which%2520concealed%2520or%2520malicious%2520instructions%2520carried%2520in%2520text%252C%2520pictures%252C%2520metadata%252C%2520or%2520agent-to-agent%2520messages%2520may%2520spread%2520throughout%2520the%2520graph%2520and%2520lead%2520to%2520unintended%2520behavior%252C%2520a%2520breach%2520of%2520policy%252C%2520or%2520corruption%2520of%2520state.%2520In%2520order%2520to%2520mitigate%2520these%2520risks%252C%2520this%2520paper%2520suggests%2520a%2520Cross-Agent%2520Multimodal%2520Provenanc-%2520Aware%2520Defense%2520Framework%2520whereby%2520all%2520the%2520prompts%252C%2520either%2520user-generated%2520or%2520produced%2520by%2520upstream%2520agents%252C%2520are%2520sanitized%2520and%2520all%2520the%2520outputs%2520generated%2520by%2520an%2520LLM%2520are%2520verified%2520independently%2520before%2520being%2520sent%2520to%2520downstream%2520nodes.%2520This%2520framework%2520contains%2520a%2520Text%2520sanitizer%2520agent%252C%2520visual%2520sanitizer%2520agent%252C%2520and%2520output%2520validator%2520agent%2520all%2520coordinated%2520by%2520a%2520provenance%2520ledger%252C%2520which%2520keeps%2520metadata%2520of%2520modality%252C%2520source%252C%2520and%2520trust%2520level%2520throughout%2520the%2520entire%2520agent%2520network.%2520This%2520architecture%2520makes%2520sure%2520that%2520agent-to-agent%2520communication%2520abides%2520by%2520clear%2520trust%2520frames%2520such%2520such%2520that%2520injected%2520instructions%2520are%2520not%2520propagated%2520down%2520LangChain%2520or%2520GraphChain-style-workflows.%2520The%2520experimental%2520assessments%2520show%2520that%2520multimodal%2520injection%2520detection%2520accuracy%2520is%2520significantly%2520enhanced%252C%2520and%2520the%2520cross-agent%2520trust%2520leakage%2520is%2520minimized%252C%2520as%2520well%2520as%252C%2520agentic%2520execution%2520pathways%2520become%2520stable.%2520The%2520framework%252C%2520which%2520expands%2520the%2520concept%2520of%2520provenance%2520tracking%2520and%2520validation%2520to%2520the%2520multi-agent%2520orchestration%252C%2520enhances%2520the%2520establishment%2520of%2520secure%252C%2520understandable%2520and%2520reliable%2520agentic%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Trustworthy%20Agentic%20AI%3A%20A%20Multimodal%20Framework%20for%20Preventing%20Prompt%20Injection%20Attacks&entry.906535625=Toqeer%20Ali%20Syed%20and%20Mishal%20Ateeq%20Almutairi%20and%20Mahmoud%20Abdel%20Moaty&entry.1292438233=Powerful%20autonomous%20systems%2C%20which%20reason%2C%20plan%2C%20and%20converse%20using%20and%20between%20numerous%20tools%20and%20agents%2C%20are%20made%20possible%20by%20Large%20Language%20Models%20%28LLMs%29%2C%20Vision-Language%20Models%20%28VLMs%29%2C%20and%20new%20agentic%20AI%20systems%2C%20like%20LangChain%20and%20GraphChain.%20Nevertheless%2C%20this%20agentic%20environment%20increases%20the%20probability%20of%20the%20occurrence%20of%20multimodal%20prompt%20injection%20%28PI%29%20attacks%2C%20in%20which%20concealed%20or%20malicious%20instructions%20carried%20in%20text%2C%20pictures%2C%20metadata%2C%20or%20agent-to-agent%20messages%20may%20spread%20throughout%20the%20graph%20and%20lead%20to%20unintended%20behavior%2C%20a%20breach%20of%20policy%2C%20or%20corruption%20of%20state.%20In%20order%20to%20mitigate%20these%20risks%2C%20this%20paper%20suggests%20a%20Cross-Agent%20Multimodal%20Provenanc-%20Aware%20Defense%20Framework%20whereby%20all%20the%20prompts%2C%20either%20user-generated%20or%20produced%20by%20upstream%20agents%2C%20are%20sanitized%20and%20all%20the%20outputs%20generated%20by%20an%20LLM%20are%20verified%20independently%20before%20being%20sent%20to%20downstream%20nodes.%20This%20framework%20contains%20a%20Text%20sanitizer%20agent%2C%20visual%20sanitizer%20agent%2C%20and%20output%20validator%20agent%20all%20coordinated%20by%20a%20provenance%20ledger%2C%20which%20keeps%20metadata%20of%20modality%2C%20source%2C%20and%20trust%20level%20throughout%20the%20entire%20agent%20network.%20This%20architecture%20makes%20sure%20that%20agent-to-agent%20communication%20abides%20by%20clear%20trust%20frames%20such%20such%20that%20injected%20instructions%20are%20not%20propagated%20down%20LangChain%20or%20GraphChain-style-workflows.%20The%20experimental%20assessments%20show%20that%20multimodal%20injection%20detection%20accuracy%20is%20significantly%20enhanced%2C%20and%20the%20cross-agent%20trust%20leakage%20is%20minimized%2C%20as%20well%20as%2C%20agentic%20execution%20pathways%20become%20stable.%20The%20framework%2C%20which%20expands%20the%20concept%20of%20provenance%20tracking%20and%20validation%20to%20the%20multi-agent%20orchestration%2C%20enhances%20the%20establishment%20of%20secure%2C%20understandable%20and%20reliable%20agentic%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.23557v1&entry.124074799=Read"},
{"title": "Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study", "author": "Saifelden M. Ismail", "abstract": "Speech Emotion Recognition (SER) has significant potential for mobile applications, yet deployment remains constrained by the computational demands of state-of-the-art transformer architectures. This paper presents a mobile-efficient SER system based on DistilHuBERT, a distilled and 8-bit quantized transformer that achieves 92% parameter reduction compared to full-scale Wav2Vec 2.0 models while maintaining competitive accuracy. We conduct a rigorous 5-fold Leave-One-Session-Out (LOSO) cross-validation on the IEMOCAP dataset to ensure speaker independence, augmented with cross-corpus training on CREMA-D to enhance generalization. Cross-corpus training with CREMA-D yields a 1.2% improvement in Weighted Accuracy, a 1.4% gain in Macro F1-score, and a 32% reduction in cross-fold variance, with the Neutral class showing the most substantial benefit at 5.4% F1-score improvement. Our approach achieves an Unweighted Accuracy of 61.4% with a quantized model footprint of only 23 MB, representing approximately 91% of full-scale baseline performance. Cross-corpus evaluation on RAVDESS reveals that the theatrical nature of acted emotions causes predictions to cluster by arousal level rather than valence: happiness is systematically confused with anger due to acoustic saturation in high-energy expressions. Despite this theatricality effect reducing overall RAVDESS accuracy to 43.29%, the model maintains robust arousal detection with 97% recall for anger and 64% for sadness. These findings establish a Pareto-optimal tradeoff between model size and accuracy, enabling practical affect recognition on resource-constrained mobile devices.", "link": "http://arxiv.org/abs/2512.23435v1", "date": "2025-12-29", "relevancy": 0.9352, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4802}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mobile-Efficient%20Speech%20Emotion%20Recognition%20Using%20DistilHuBERT%3A%20A%20Cross-Corpus%20Validation%20Study&body=Title%3A%20Mobile-Efficient%20Speech%20Emotion%20Recognition%20Using%20DistilHuBERT%3A%20A%20Cross-Corpus%20Validation%20Study%0AAuthor%3A%20Saifelden%20M.%20Ismail%0AAbstract%3A%20Speech%20Emotion%20Recognition%20%28SER%29%20has%20significant%20potential%20for%20mobile%20applications%2C%20yet%20deployment%20remains%20constrained%20by%20the%20computational%20demands%20of%20state-of-the-art%20transformer%20architectures.%20This%20paper%20presents%20a%20mobile-efficient%20SER%20system%20based%20on%20DistilHuBERT%2C%20a%20distilled%20and%208-bit%20quantized%20transformer%20that%20achieves%2092%25%20parameter%20reduction%20compared%20to%20full-scale%20Wav2Vec%202.0%20models%20while%20maintaining%20competitive%20accuracy.%20We%20conduct%20a%20rigorous%205-fold%20Leave-One-Session-Out%20%28LOSO%29%20cross-validation%20on%20the%20IEMOCAP%20dataset%20to%20ensure%20speaker%20independence%2C%20augmented%20with%20cross-corpus%20training%20on%20CREMA-D%20to%20enhance%20generalization.%20Cross-corpus%20training%20with%20CREMA-D%20yields%20a%201.2%25%20improvement%20in%20Weighted%20Accuracy%2C%20a%201.4%25%20gain%20in%20Macro%20F1-score%2C%20and%20a%2032%25%20reduction%20in%20cross-fold%20variance%2C%20with%20the%20Neutral%20class%20showing%20the%20most%20substantial%20benefit%20at%205.4%25%20F1-score%20improvement.%20Our%20approach%20achieves%20an%20Unweighted%20Accuracy%20of%2061.4%25%20with%20a%20quantized%20model%20footprint%20of%20only%2023%20MB%2C%20representing%20approximately%2091%25%20of%20full-scale%20baseline%20performance.%20Cross-corpus%20evaluation%20on%20RAVDESS%20reveals%20that%20the%20theatrical%20nature%20of%20acted%20emotions%20causes%20predictions%20to%20cluster%20by%20arousal%20level%20rather%20than%20valence%3A%20happiness%20is%20systematically%20confused%20with%20anger%20due%20to%20acoustic%20saturation%20in%20high-energy%20expressions.%20Despite%20this%20theatricality%20effect%20reducing%20overall%20RAVDESS%20accuracy%20to%2043.29%25%2C%20the%20model%20maintains%20robust%20arousal%20detection%20with%2097%25%20recall%20for%20anger%20and%2064%25%20for%20sadness.%20These%20findings%20establish%20a%20Pareto-optimal%20tradeoff%20between%20model%20size%20and%20accuracy%2C%20enabling%20practical%20affect%20recognition%20on%20resource-constrained%20mobile%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobile-Efficient%2520Speech%2520Emotion%2520Recognition%2520Using%2520DistilHuBERT%253A%2520A%2520Cross-Corpus%2520Validation%2520Study%26entry.906535625%3DSaifelden%2520M.%2520Ismail%26entry.1292438233%3DSpeech%2520Emotion%2520Recognition%2520%2528SER%2529%2520has%2520significant%2520potential%2520for%2520mobile%2520applications%252C%2520yet%2520deployment%2520remains%2520constrained%2520by%2520the%2520computational%2520demands%2520of%2520state-of-the-art%2520transformer%2520architectures.%2520This%2520paper%2520presents%2520a%2520mobile-efficient%2520SER%2520system%2520based%2520on%2520DistilHuBERT%252C%2520a%2520distilled%2520and%25208-bit%2520quantized%2520transformer%2520that%2520achieves%252092%2525%2520parameter%2520reduction%2520compared%2520to%2520full-scale%2520Wav2Vec%25202.0%2520models%2520while%2520maintaining%2520competitive%2520accuracy.%2520We%2520conduct%2520a%2520rigorous%25205-fold%2520Leave-One-Session-Out%2520%2528LOSO%2529%2520cross-validation%2520on%2520the%2520IEMOCAP%2520dataset%2520to%2520ensure%2520speaker%2520independence%252C%2520augmented%2520with%2520cross-corpus%2520training%2520on%2520CREMA-D%2520to%2520enhance%2520generalization.%2520Cross-corpus%2520training%2520with%2520CREMA-D%2520yields%2520a%25201.2%2525%2520improvement%2520in%2520Weighted%2520Accuracy%252C%2520a%25201.4%2525%2520gain%2520in%2520Macro%2520F1-score%252C%2520and%2520a%252032%2525%2520reduction%2520in%2520cross-fold%2520variance%252C%2520with%2520the%2520Neutral%2520class%2520showing%2520the%2520most%2520substantial%2520benefit%2520at%25205.4%2525%2520F1-score%2520improvement.%2520Our%2520approach%2520achieves%2520an%2520Unweighted%2520Accuracy%2520of%252061.4%2525%2520with%2520a%2520quantized%2520model%2520footprint%2520of%2520only%252023%2520MB%252C%2520representing%2520approximately%252091%2525%2520of%2520full-scale%2520baseline%2520performance.%2520Cross-corpus%2520evaluation%2520on%2520RAVDESS%2520reveals%2520that%2520the%2520theatrical%2520nature%2520of%2520acted%2520emotions%2520causes%2520predictions%2520to%2520cluster%2520by%2520arousal%2520level%2520rather%2520than%2520valence%253A%2520happiness%2520is%2520systematically%2520confused%2520with%2520anger%2520due%2520to%2520acoustic%2520saturation%2520in%2520high-energy%2520expressions.%2520Despite%2520this%2520theatricality%2520effect%2520reducing%2520overall%2520RAVDESS%2520accuracy%2520to%252043.29%2525%252C%2520the%2520model%2520maintains%2520robust%2520arousal%2520detection%2520with%252097%2525%2520recall%2520for%2520anger%2520and%252064%2525%2520for%2520sadness.%2520These%2520findings%2520establish%2520a%2520Pareto-optimal%2520tradeoff%2520between%2520model%2520size%2520and%2520accuracy%252C%2520enabling%2520practical%2520affect%2520recognition%2520on%2520resource-constrained%2520mobile%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobile-Efficient%20Speech%20Emotion%20Recognition%20Using%20DistilHuBERT%3A%20A%20Cross-Corpus%20Validation%20Study&entry.906535625=Saifelden%20M.%20Ismail&entry.1292438233=Speech%20Emotion%20Recognition%20%28SER%29%20has%20significant%20potential%20for%20mobile%20applications%2C%20yet%20deployment%20remains%20constrained%20by%20the%20computational%20demands%20of%20state-of-the-art%20transformer%20architectures.%20This%20paper%20presents%20a%20mobile-efficient%20SER%20system%20based%20on%20DistilHuBERT%2C%20a%20distilled%20and%208-bit%20quantized%20transformer%20that%20achieves%2092%25%20parameter%20reduction%20compared%20to%20full-scale%20Wav2Vec%202.0%20models%20while%20maintaining%20competitive%20accuracy.%20We%20conduct%20a%20rigorous%205-fold%20Leave-One-Session-Out%20%28LOSO%29%20cross-validation%20on%20the%20IEMOCAP%20dataset%20to%20ensure%20speaker%20independence%2C%20augmented%20with%20cross-corpus%20training%20on%20CREMA-D%20to%20enhance%20generalization.%20Cross-corpus%20training%20with%20CREMA-D%20yields%20a%201.2%25%20improvement%20in%20Weighted%20Accuracy%2C%20a%201.4%25%20gain%20in%20Macro%20F1-score%2C%20and%20a%2032%25%20reduction%20in%20cross-fold%20variance%2C%20with%20the%20Neutral%20class%20showing%20the%20most%20substantial%20benefit%20at%205.4%25%20F1-score%20improvement.%20Our%20approach%20achieves%20an%20Unweighted%20Accuracy%20of%2061.4%25%20with%20a%20quantized%20model%20footprint%20of%20only%2023%20MB%2C%20representing%20approximately%2091%25%20of%20full-scale%20baseline%20performance.%20Cross-corpus%20evaluation%20on%20RAVDESS%20reveals%20that%20the%20theatrical%20nature%20of%20acted%20emotions%20causes%20predictions%20to%20cluster%20by%20arousal%20level%20rather%20than%20valence%3A%20happiness%20is%20systematically%20confused%20with%20anger%20due%20to%20acoustic%20saturation%20in%20high-energy%20expressions.%20Despite%20this%20theatricality%20effect%20reducing%20overall%20RAVDESS%20accuracy%20to%2043.29%25%2C%20the%20model%20maintains%20robust%20arousal%20detection%20with%2097%25%20recall%20for%20anger%20and%2064%25%20for%20sadness.%20These%20findings%20establish%20a%20Pareto-optimal%20tradeoff%20between%20model%20size%20and%20accuracy%2C%20enabling%20practical%20affect%20recognition%20on%20resource-constrained%20mobile%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2512.23435v1&entry.124074799=Read"},
{"title": "PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing", "author": "Michael Bezick and Blake A. Wilson and Vaishnavi Iyer and Yuheng Chen and Vladimir M. Shalaev and Sabre Kais and Alexander V. Kildishev and Alexandra Boltasseva and Brad Lackey", "abstract": "PearSAN is a machine learning-assisted optimization algorithm applicable to inverse design problems with large design spaces, where traditional optimizers struggle. The algorithm leverages the latent space of a generative model for rapid sampling and employs a Pearson correlated surrogate model to predict the figure of merit of the true design metric. As a showcase example, PearSAN is applied to thermophotovoltaic (TPV) metasurface design by matching the working bands between a thermal radiator and a photovoltaic cell. PearSAN can work with any pretrained generative model with a discretized latent space, making it easy to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson correlational loss can be used as both a latent regularization method, similar to batch and layer normalization, and as a surrogate training loss. We compare both to previous energy matching losses, which are shown to enforce poor regularization and performance, even with upgraded affine parameters. PearSAN achieves a state-of-the-art maximum design efficiency of 97%, and is at least an order of magnitude faster than previous methods, with an improved maximum figure-of-merit gain.", "link": "http://arxiv.org/abs/2412.19284v2", "date": "2025-12-29", "relevancy": 1.8126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PearSAN%3A%20A%20Machine%20Learning%20Method%20for%20Inverse%20Design%20using%20Pearson%20Correlated%20Surrogate%20Annealing&body=Title%3A%20PearSAN%3A%20A%20Machine%20Learning%20Method%20for%20Inverse%20Design%20using%20Pearson%20Correlated%20Surrogate%20Annealing%0AAuthor%3A%20Michael%20Bezick%20and%20Blake%20A.%20Wilson%20and%20Vaishnavi%20Iyer%20and%20Yuheng%20Chen%20and%20Vladimir%20M.%20Shalaev%20and%20Sabre%20Kais%20and%20Alexander%20V.%20Kildishev%20and%20Alexandra%20Boltasseva%20and%20Brad%20Lackey%0AAbstract%3A%20PearSAN%20is%20a%20machine%20learning-assisted%20optimization%20algorithm%20applicable%20to%20inverse%20design%20problems%20with%20large%20design%20spaces%2C%20where%20traditional%20optimizers%20struggle.%20The%20algorithm%20leverages%20the%20latent%20space%20of%20a%20generative%20model%20for%20rapid%20sampling%20and%20employs%20a%20Pearson%20correlated%20surrogate%20model%20to%20predict%20the%20figure%20of%20merit%20of%20the%20true%20design%20metric.%20As%20a%20showcase%20example%2C%20PearSAN%20is%20applied%20to%20thermophotovoltaic%20%28TPV%29%20metasurface%20design%20by%20matching%20the%20working%20bands%20between%20a%20thermal%20radiator%20and%20a%20photovoltaic%20cell.%20PearSAN%20can%20work%20with%20any%20pretrained%20generative%20model%20with%20a%20discretized%20latent%20space%2C%20making%20it%20easy%20to%20integrate%20with%20VQ-VAEs%20and%20binary%20autoencoders.%20Its%20novel%20Pearson%20correlational%20loss%20can%20be%20used%20as%20both%20a%20latent%20regularization%20method%2C%20similar%20to%20batch%20and%20layer%20normalization%2C%20and%20as%20a%20surrogate%20training%20loss.%20We%20compare%20both%20to%20previous%20energy%20matching%20losses%2C%20which%20are%20shown%20to%20enforce%20poor%20regularization%20and%20performance%2C%20even%20with%20upgraded%20affine%20parameters.%20PearSAN%20achieves%20a%20state-of-the-art%20maximum%20design%20efficiency%20of%2097%25%2C%20and%20is%20at%20least%20an%20order%20of%20magnitude%20faster%20than%20previous%20methods%2C%20with%20an%20improved%20maximum%20figure-of-merit%20gain.%0ALink%3A%20http%3A//arxiv.org/abs/2412.19284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPearSAN%253A%2520A%2520Machine%2520Learning%2520Method%2520for%2520Inverse%2520Design%2520using%2520Pearson%2520Correlated%2520Surrogate%2520Annealing%26entry.906535625%3DMichael%2520Bezick%2520and%2520Blake%2520A.%2520Wilson%2520and%2520Vaishnavi%2520Iyer%2520and%2520Yuheng%2520Chen%2520and%2520Vladimir%2520M.%2520Shalaev%2520and%2520Sabre%2520Kais%2520and%2520Alexander%2520V.%2520Kildishev%2520and%2520Alexandra%2520Boltasseva%2520and%2520Brad%2520Lackey%26entry.1292438233%3DPearSAN%2520is%2520a%2520machine%2520learning-assisted%2520optimization%2520algorithm%2520applicable%2520to%2520inverse%2520design%2520problems%2520with%2520large%2520design%2520spaces%252C%2520where%2520traditional%2520optimizers%2520struggle.%2520The%2520algorithm%2520leverages%2520the%2520latent%2520space%2520of%2520a%2520generative%2520model%2520for%2520rapid%2520sampling%2520and%2520employs%2520a%2520Pearson%2520correlated%2520surrogate%2520model%2520to%2520predict%2520the%2520figure%2520of%2520merit%2520of%2520the%2520true%2520design%2520metric.%2520As%2520a%2520showcase%2520example%252C%2520PearSAN%2520is%2520applied%2520to%2520thermophotovoltaic%2520%2528TPV%2529%2520metasurface%2520design%2520by%2520matching%2520the%2520working%2520bands%2520between%2520a%2520thermal%2520radiator%2520and%2520a%2520photovoltaic%2520cell.%2520PearSAN%2520can%2520work%2520with%2520any%2520pretrained%2520generative%2520model%2520with%2520a%2520discretized%2520latent%2520space%252C%2520making%2520it%2520easy%2520to%2520integrate%2520with%2520VQ-VAEs%2520and%2520binary%2520autoencoders.%2520Its%2520novel%2520Pearson%2520correlational%2520loss%2520can%2520be%2520used%2520as%2520both%2520a%2520latent%2520regularization%2520method%252C%2520similar%2520to%2520batch%2520and%2520layer%2520normalization%252C%2520and%2520as%2520a%2520surrogate%2520training%2520loss.%2520We%2520compare%2520both%2520to%2520previous%2520energy%2520matching%2520losses%252C%2520which%2520are%2520shown%2520to%2520enforce%2520poor%2520regularization%2520and%2520performance%252C%2520even%2520with%2520upgraded%2520affine%2520parameters.%2520PearSAN%2520achieves%2520a%2520state-of-the-art%2520maximum%2520design%2520efficiency%2520of%252097%2525%252C%2520and%2520is%2520at%2520least%2520an%2520order%2520of%2520magnitude%2520faster%2520than%2520previous%2520methods%252C%2520with%2520an%2520improved%2520maximum%2520figure-of-merit%2520gain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PearSAN%3A%20A%20Machine%20Learning%20Method%20for%20Inverse%20Design%20using%20Pearson%20Correlated%20Surrogate%20Annealing&entry.906535625=Michael%20Bezick%20and%20Blake%20A.%20Wilson%20and%20Vaishnavi%20Iyer%20and%20Yuheng%20Chen%20and%20Vladimir%20M.%20Shalaev%20and%20Sabre%20Kais%20and%20Alexander%20V.%20Kildishev%20and%20Alexandra%20Boltasseva%20and%20Brad%20Lackey&entry.1292438233=PearSAN%20is%20a%20machine%20learning-assisted%20optimization%20algorithm%20applicable%20to%20inverse%20design%20problems%20with%20large%20design%20spaces%2C%20where%20traditional%20optimizers%20struggle.%20The%20algorithm%20leverages%20the%20latent%20space%20of%20a%20generative%20model%20for%20rapid%20sampling%20and%20employs%20a%20Pearson%20correlated%20surrogate%20model%20to%20predict%20the%20figure%20of%20merit%20of%20the%20true%20design%20metric.%20As%20a%20showcase%20example%2C%20PearSAN%20is%20applied%20to%20thermophotovoltaic%20%28TPV%29%20metasurface%20design%20by%20matching%20the%20working%20bands%20between%20a%20thermal%20radiator%20and%20a%20photovoltaic%20cell.%20PearSAN%20can%20work%20with%20any%20pretrained%20generative%20model%20with%20a%20discretized%20latent%20space%2C%20making%20it%20easy%20to%20integrate%20with%20VQ-VAEs%20and%20binary%20autoencoders.%20Its%20novel%20Pearson%20correlational%20loss%20can%20be%20used%20as%20both%20a%20latent%20regularization%20method%2C%20similar%20to%20batch%20and%20layer%20normalization%2C%20and%20as%20a%20surrogate%20training%20loss.%20We%20compare%20both%20to%20previous%20energy%20matching%20losses%2C%20which%20are%20shown%20to%20enforce%20poor%20regularization%20and%20performance%2C%20even%20with%20upgraded%20affine%20parameters.%20PearSAN%20achieves%20a%20state-of-the-art%20maximum%20design%20efficiency%20of%2097%25%2C%20and%20is%20at%20least%20an%20order%20of%20magnitude%20faster%20than%20previous%20methods%2C%20with%20an%20improved%20maximum%20figure-of-merit%20gain.&entry.1838667208=http%3A//arxiv.org/abs/2412.19284v2&entry.124074799=Read"},
{"title": "Scaling Laws for Energy Efficiency of Local LLMs", "author": "Ander Alvarez and Alessandro Genuardi and Nilotpal Sinha and Antonio Tiene and Mikail Okyay and Bakbergen Ryskulov and David Montero and Samuel Mugel and Rom\u00e1n Or\u00fas", "abstract": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.", "link": "http://arxiv.org/abs/2512.16531v4", "date": "2025-12-29", "relevancy": 1.5961, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5469}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Laws%20for%20Energy%20Efficiency%20of%20Local%20LLMs&body=Title%3A%20Scaling%20Laws%20for%20Energy%20Efficiency%20of%20Local%20LLMs%0AAuthor%3A%20Ander%20Alvarez%20and%20Alessandro%20Genuardi%20and%20Nilotpal%20Sinha%20and%20Antonio%20Tiene%20and%20Mikail%20Okyay%20and%20Bakbergen%20Ryskulov%20and%20David%20Montero%20and%20Samuel%20Mugel%20and%20Rom%C3%A1n%20Or%C3%BAs%0AAbstract%3A%20Deploying%20local%20large%20language%20models%20and%20vision-language%20models%20on%20edge%20devices%20requires%20balancing%20accuracy%20with%20constrained%20computational%20and%20energy%20budgets.%20Although%20graphics%20processors%20dominate%20modern%20artificial-intelligence%20deployment%2C%20most%20consumer%20hardware--including%20laptops%2C%20desktops%2C%20industrial%20controllers%2C%20and%20embedded%20systems--relies%20on%20central%20processing%20units.%20Despite%20this%2C%20the%20computational%20laws%20governing%20central-processing-unit-only%20inference%20for%20local%20language%20and%20vision-language%20workloads%20remain%20largely%20unexplored.%20We%20systematically%20benchmark%20large%20language%20and%20vision-language%20models%20on%20two%20representative%20central-processing-unit%20tiers%20widely%20used%20for%20local%20inference%3A%20a%20MacBook%20Pro%20M2%2C%20reflecting%20mainstream%20laptop-class%20deployment%2C%20and%20a%20Raspberry%20Pi%205%2C%20representing%20constrained%2C%20low-power%20embedded%20settings.%20Using%20a%20unified%20methodology%20based%20on%20continuous%20sampling%20of%20processor%20and%20memory%20usage%20together%20with%20area-under-curve%20integration%2C%20we%20characterize%20how%20computational%20load%20scales%20with%20input%20text%20length%20for%20language%20models%20and%20with%20image%20resolution%20for%20vision-language%20models.%20We%20uncover%20two%20empirical%20scaling%20laws%3A%20%281%29%20computational%20cost%20for%20language-model%20inference%20scales%20approximately%20linearly%20with%20token%20length%3B%20and%20%282%29%20vision-language%20models%20exhibit%20a%20preprocessing-driven%20%22resolution%20knee%22%2C%20where%20compute%20remains%20constant%20above%20an%20internal%20resolution%20clamp%20and%20decreases%20sharply%20below%20it.%20Beyond%20these%20laws%2C%20we%20show%20that%20quantum-inspired%20compression%20reduces%20processor%20and%20memory%20usage%20by%20up%20to%2071.9%25%20and%20energy%20consumption%20by%20up%20to%2062%25%2C%20while%20preserving%20or%20improving%20semantic%20accuracy.%20These%20results%20provide%20a%20systematic%20quantification%20of%20multimodal%20central-processing-unit-only%20scaling%20for%20local%20language%20and%20vision-language%20workloads%2C%20and%20they%20identify%20model%20compression%20and%20input-resolution%20preprocessing%20as%20effective%2C%20low-cost%20levers%20for%20sustainable%20edge%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16531v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Laws%2520for%2520Energy%2520Efficiency%2520of%2520Local%2520LLMs%26entry.906535625%3DAnder%2520Alvarez%2520and%2520Alessandro%2520Genuardi%2520and%2520Nilotpal%2520Sinha%2520and%2520Antonio%2520Tiene%2520and%2520Mikail%2520Okyay%2520and%2520Bakbergen%2520Ryskulov%2520and%2520David%2520Montero%2520and%2520Samuel%2520Mugel%2520and%2520Rom%25C3%25A1n%2520Or%25C3%25BAs%26entry.1292438233%3DDeploying%2520local%2520large%2520language%2520models%2520and%2520vision-language%2520models%2520on%2520edge%2520devices%2520requires%2520balancing%2520accuracy%2520with%2520constrained%2520computational%2520and%2520energy%2520budgets.%2520Although%2520graphics%2520processors%2520dominate%2520modern%2520artificial-intelligence%2520deployment%252C%2520most%2520consumer%2520hardware--including%2520laptops%252C%2520desktops%252C%2520industrial%2520controllers%252C%2520and%2520embedded%2520systems--relies%2520on%2520central%2520processing%2520units.%2520Despite%2520this%252C%2520the%2520computational%2520laws%2520governing%2520central-processing-unit-only%2520inference%2520for%2520local%2520language%2520and%2520vision-language%2520workloads%2520remain%2520largely%2520unexplored.%2520We%2520systematically%2520benchmark%2520large%2520language%2520and%2520vision-language%2520models%2520on%2520two%2520representative%2520central-processing-unit%2520tiers%2520widely%2520used%2520for%2520local%2520inference%253A%2520a%2520MacBook%2520Pro%2520M2%252C%2520reflecting%2520mainstream%2520laptop-class%2520deployment%252C%2520and%2520a%2520Raspberry%2520Pi%25205%252C%2520representing%2520constrained%252C%2520low-power%2520embedded%2520settings.%2520Using%2520a%2520unified%2520methodology%2520based%2520on%2520continuous%2520sampling%2520of%2520processor%2520and%2520memory%2520usage%2520together%2520with%2520area-under-curve%2520integration%252C%2520we%2520characterize%2520how%2520computational%2520load%2520scales%2520with%2520input%2520text%2520length%2520for%2520language%2520models%2520and%2520with%2520image%2520resolution%2520for%2520vision-language%2520models.%2520We%2520uncover%2520two%2520empirical%2520scaling%2520laws%253A%2520%25281%2529%2520computational%2520cost%2520for%2520language-model%2520inference%2520scales%2520approximately%2520linearly%2520with%2520token%2520length%253B%2520and%2520%25282%2529%2520vision-language%2520models%2520exhibit%2520a%2520preprocessing-driven%2520%2522resolution%2520knee%2522%252C%2520where%2520compute%2520remains%2520constant%2520above%2520an%2520internal%2520resolution%2520clamp%2520and%2520decreases%2520sharply%2520below%2520it.%2520Beyond%2520these%2520laws%252C%2520we%2520show%2520that%2520quantum-inspired%2520compression%2520reduces%2520processor%2520and%2520memory%2520usage%2520by%2520up%2520to%252071.9%2525%2520and%2520energy%2520consumption%2520by%2520up%2520to%252062%2525%252C%2520while%2520preserving%2520or%2520improving%2520semantic%2520accuracy.%2520These%2520results%2520provide%2520a%2520systematic%2520quantification%2520of%2520multimodal%2520central-processing-unit-only%2520scaling%2520for%2520local%2520language%2520and%2520vision-language%2520workloads%252C%2520and%2520they%2520identify%2520model%2520compression%2520and%2520input-resolution%2520preprocessing%2520as%2520effective%252C%2520low-cost%2520levers%2520for%2520sustainable%2520edge%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16531v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Laws%20for%20Energy%20Efficiency%20of%20Local%20LLMs&entry.906535625=Ander%20Alvarez%20and%20Alessandro%20Genuardi%20and%20Nilotpal%20Sinha%20and%20Antonio%20Tiene%20and%20Mikail%20Okyay%20and%20Bakbergen%20Ryskulov%20and%20David%20Montero%20and%20Samuel%20Mugel%20and%20Rom%C3%A1n%20Or%C3%BAs&entry.1292438233=Deploying%20local%20large%20language%20models%20and%20vision-language%20models%20on%20edge%20devices%20requires%20balancing%20accuracy%20with%20constrained%20computational%20and%20energy%20budgets.%20Although%20graphics%20processors%20dominate%20modern%20artificial-intelligence%20deployment%2C%20most%20consumer%20hardware--including%20laptops%2C%20desktops%2C%20industrial%20controllers%2C%20and%20embedded%20systems--relies%20on%20central%20processing%20units.%20Despite%20this%2C%20the%20computational%20laws%20governing%20central-processing-unit-only%20inference%20for%20local%20language%20and%20vision-language%20workloads%20remain%20largely%20unexplored.%20We%20systematically%20benchmark%20large%20language%20and%20vision-language%20models%20on%20two%20representative%20central-processing-unit%20tiers%20widely%20used%20for%20local%20inference%3A%20a%20MacBook%20Pro%20M2%2C%20reflecting%20mainstream%20laptop-class%20deployment%2C%20and%20a%20Raspberry%20Pi%205%2C%20representing%20constrained%2C%20low-power%20embedded%20settings.%20Using%20a%20unified%20methodology%20based%20on%20continuous%20sampling%20of%20processor%20and%20memory%20usage%20together%20with%20area-under-curve%20integration%2C%20we%20characterize%20how%20computational%20load%20scales%20with%20input%20text%20length%20for%20language%20models%20and%20with%20image%20resolution%20for%20vision-language%20models.%20We%20uncover%20two%20empirical%20scaling%20laws%3A%20%281%29%20computational%20cost%20for%20language-model%20inference%20scales%20approximately%20linearly%20with%20token%20length%3B%20and%20%282%29%20vision-language%20models%20exhibit%20a%20preprocessing-driven%20%22resolution%20knee%22%2C%20where%20compute%20remains%20constant%20above%20an%20internal%20resolution%20clamp%20and%20decreases%20sharply%20below%20it.%20Beyond%20these%20laws%2C%20we%20show%20that%20quantum-inspired%20compression%20reduces%20processor%20and%20memory%20usage%20by%20up%20to%2071.9%25%20and%20energy%20consumption%20by%20up%20to%2062%25%2C%20while%20preserving%20or%20improving%20semantic%20accuracy.%20These%20results%20provide%20a%20systematic%20quantification%20of%20multimodal%20central-processing-unit-only%20scaling%20for%20local%20language%20and%20vision-language%20workloads%2C%20and%20they%20identify%20model%20compression%20and%20input-resolution%20preprocessing%20as%20effective%2C%20low-cost%20levers%20for%20sustainable%20edge%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2512.16531v4&entry.124074799=Read"},
{"title": "Forecasting Clinical Risk from Textual Time Series: Structuring Narratives for Temporal AI in Healthcare", "author": "Shahriar Noroozizadeh and Sayantan Kumar and Jeremy C. Weiss", "abstract": "Clinical case reports encode temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings -- extracted via an LLM-assisted annotation pipeline -- serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.", "link": "http://arxiv.org/abs/2504.10340v5", "date": "2025-12-29", "relevancy": 1.4256, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.489}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Clinical%20Risk%20from%20Textual%20Time%20Series%3A%20Structuring%20Narratives%20for%20Temporal%20AI%20in%20Healthcare&body=Title%3A%20Forecasting%20Clinical%20Risk%20from%20Textual%20Time%20Series%3A%20Structuring%20Narratives%20for%20Temporal%20AI%20in%20Healthcare%0AAuthor%3A%20Shahriar%20Noroozizadeh%20and%20Sayantan%20Kumar%20and%20Jeremy%20C.%20Weiss%0AAbstract%3A%20Clinical%20case%20reports%20encode%20temporal%20patient%20trajectories%20that%20are%20often%20underexploited%20by%20traditional%20machine%20learning%20methods%20relying%20on%20structured%20data.%20In%20this%20work%2C%20we%20introduce%20the%20forecasting%20problem%20from%20textual%20time%20series%2C%20where%20timestamped%20clinical%20findings%20--%20extracted%20via%20an%20LLM-assisted%20annotation%20pipeline%20--%20serve%20as%20the%20primary%20input%20for%20prediction.%20We%20systematically%20evaluate%20a%20diverse%20suite%20of%20models%2C%20including%20fine-tuned%20decoder-based%20large%20language%20models%20and%20encoder-based%20transformers%2C%20on%20tasks%20of%20event%20occurrence%20prediction%2C%20temporal%20ordering%2C%20and%20survival%20analysis.%20Our%20experiments%20reveal%20that%20encoder-based%20models%20consistently%20achieve%20higher%20F1%20scores%20and%20superior%20temporal%20concordance%20for%20short-%20and%20long-horizon%20event%20forecasting%2C%20while%20fine-tuned%20masking%20approaches%20enhance%20ranking%20performance.%20In%20contrast%2C%20instruction-tuned%20decoder%20models%20demonstrate%20a%20relative%20advantage%20in%20survival%20analysis%2C%20especially%20in%20early%20prognosis%20settings.%20Our%20sensitivity%20analyses%20further%20demonstrate%20the%20importance%20of%20time%20ordering%2C%20which%20requires%20clinical%20time%20series%20construction%2C%20as%20compared%20to%20text%20ordering%2C%20the%20format%20of%20the%20text%20inputs%20that%20LLMs%20are%20classically%20trained%20on.%20This%20highlights%20the%20additional%20benefit%20that%20can%20be%20ascertained%20from%20time-ordered%20corpora%2C%20with%20implications%20for%20temporal%20tasks%20in%20the%20era%20of%20widespread%20LLM%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2504.10340v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Clinical%2520Risk%2520from%2520Textual%2520Time%2520Series%253A%2520Structuring%2520Narratives%2520for%2520Temporal%2520AI%2520in%2520Healthcare%26entry.906535625%3DShahriar%2520Noroozizadeh%2520and%2520Sayantan%2520Kumar%2520and%2520Jeremy%2520C.%2520Weiss%26entry.1292438233%3DClinical%2520case%2520reports%2520encode%2520temporal%2520patient%2520trajectories%2520that%2520are%2520often%2520underexploited%2520by%2520traditional%2520machine%2520learning%2520methods%2520relying%2520on%2520structured%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520forecasting%2520problem%2520from%2520textual%2520time%2520series%252C%2520where%2520timestamped%2520clinical%2520findings%2520--%2520extracted%2520via%2520an%2520LLM-assisted%2520annotation%2520pipeline%2520--%2520serve%2520as%2520the%2520primary%2520input%2520for%2520prediction.%2520We%2520systematically%2520evaluate%2520a%2520diverse%2520suite%2520of%2520models%252C%2520including%2520fine-tuned%2520decoder-based%2520large%2520language%2520models%2520and%2520encoder-based%2520transformers%252C%2520on%2520tasks%2520of%2520event%2520occurrence%2520prediction%252C%2520temporal%2520ordering%252C%2520and%2520survival%2520analysis.%2520Our%2520experiments%2520reveal%2520that%2520encoder-based%2520models%2520consistently%2520achieve%2520higher%2520F1%2520scores%2520and%2520superior%2520temporal%2520concordance%2520for%2520short-%2520and%2520long-horizon%2520event%2520forecasting%252C%2520while%2520fine-tuned%2520masking%2520approaches%2520enhance%2520ranking%2520performance.%2520In%2520contrast%252C%2520instruction-tuned%2520decoder%2520models%2520demonstrate%2520a%2520relative%2520advantage%2520in%2520survival%2520analysis%252C%2520especially%2520in%2520early%2520prognosis%2520settings.%2520Our%2520sensitivity%2520analyses%2520further%2520demonstrate%2520the%2520importance%2520of%2520time%2520ordering%252C%2520which%2520requires%2520clinical%2520time%2520series%2520construction%252C%2520as%2520compared%2520to%2520text%2520ordering%252C%2520the%2520format%2520of%2520the%2520text%2520inputs%2520that%2520LLMs%2520are%2520classically%2520trained%2520on.%2520This%2520highlights%2520the%2520additional%2520benefit%2520that%2520can%2520be%2520ascertained%2520from%2520time-ordered%2520corpora%252C%2520with%2520implications%2520for%2520temporal%2520tasks%2520in%2520the%2520era%2520of%2520widespread%2520LLM%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10340v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Clinical%20Risk%20from%20Textual%20Time%20Series%3A%20Structuring%20Narratives%20for%20Temporal%20AI%20in%20Healthcare&entry.906535625=Shahriar%20Noroozizadeh%20and%20Sayantan%20Kumar%20and%20Jeremy%20C.%20Weiss&entry.1292438233=Clinical%20case%20reports%20encode%20temporal%20patient%20trajectories%20that%20are%20often%20underexploited%20by%20traditional%20machine%20learning%20methods%20relying%20on%20structured%20data.%20In%20this%20work%2C%20we%20introduce%20the%20forecasting%20problem%20from%20textual%20time%20series%2C%20where%20timestamped%20clinical%20findings%20--%20extracted%20via%20an%20LLM-assisted%20annotation%20pipeline%20--%20serve%20as%20the%20primary%20input%20for%20prediction.%20We%20systematically%20evaluate%20a%20diverse%20suite%20of%20models%2C%20including%20fine-tuned%20decoder-based%20large%20language%20models%20and%20encoder-based%20transformers%2C%20on%20tasks%20of%20event%20occurrence%20prediction%2C%20temporal%20ordering%2C%20and%20survival%20analysis.%20Our%20experiments%20reveal%20that%20encoder-based%20models%20consistently%20achieve%20higher%20F1%20scores%20and%20superior%20temporal%20concordance%20for%20short-%20and%20long-horizon%20event%20forecasting%2C%20while%20fine-tuned%20masking%20approaches%20enhance%20ranking%20performance.%20In%20contrast%2C%20instruction-tuned%20decoder%20models%20demonstrate%20a%20relative%20advantage%20in%20survival%20analysis%2C%20especially%20in%20early%20prognosis%20settings.%20Our%20sensitivity%20analyses%20further%20demonstrate%20the%20importance%20of%20time%20ordering%2C%20which%20requires%20clinical%20time%20series%20construction%2C%20as%20compared%20to%20text%20ordering%2C%20the%20format%20of%20the%20text%20inputs%20that%20LLMs%20are%20classically%20trained%20on.%20This%20highlights%20the%20additional%20benefit%20that%20can%20be%20ascertained%20from%20time-ordered%20corpora%2C%20with%20implications%20for%20temporal%20tasks%20in%20the%20era%20of%20widespread%20LLM%20use.&entry.1838667208=http%3A//arxiv.org/abs/2504.10340v5&entry.124074799=Read"},
{"title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents", "author": "Jiafeng Liang and Hao Li and Chang Li and Jiaqi Zhou and Shixin Jiang and Zekun Wang and Changkai Ji and Zhihao Zhu and Runxuan Liu and Tao Ren and Jinlan Fu and See-Kiong Ng and Xia Liang and Ming Liu and Bing Qin", "abstract": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.", "link": "http://arxiv.org/abs/2512.23343v1", "date": "2025-12-29", "relevancy": 1.4541, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.491}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Meets%20Brain%3A%20Memory%20Systems%20from%20Cognitive%20Neuroscience%20to%20Autonomous%20Agents&body=Title%3A%20AI%20Meets%20Brain%3A%20Memory%20Systems%20from%20Cognitive%20Neuroscience%20to%20Autonomous%20Agents%0AAuthor%3A%20Jiafeng%20Liang%20and%20Hao%20Li%20and%20Chang%20Li%20and%20Jiaqi%20Zhou%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Changkai%20Ji%20and%20Zhihao%20Zhu%20and%20Runxuan%20Liu%20and%20Tao%20Ren%20and%20Jinlan%20Fu%20and%20See-Kiong%20Ng%20and%20Xia%20Liang%20and%20Ming%20Liu%20and%20Bing%20Qin%0AAbstract%3A%20Memory%20serves%20as%20the%20pivotal%20nexus%20bridging%20past%20and%20future%2C%20providing%20both%20humans%20and%20AI%20systems%20with%20invaluable%20concepts%20and%20experience%20to%20navigate%20complex%20tasks.%20Recent%20research%20on%20autonomous%20agents%20has%20increasingly%20focused%20on%20designing%20efficient%20memory%20workflows%20by%20drawing%20on%20cognitive%20neuroscience.%20However%2C%20constrained%20by%20interdisciplinary%20barriers%2C%20existing%20works%20struggle%20to%20assimilate%20the%20essence%20of%20human%20memory%20mechanisms.%20To%20bridge%20this%20gap%2C%20we%20systematically%20synthesizes%20interdisciplinary%20knowledge%20of%20memory%2C%20connecting%20insights%20from%20cognitive%20neuroscience%20with%20LLM-driven%20agents.%20Specifically%2C%20we%20first%20elucidate%20the%20definition%20and%20function%20of%20memory%20along%20a%20progressive%20trajectory%20from%20cognitive%20neuroscience%20through%20LLMs%20to%20agents.%20We%20then%20provide%20a%20comparative%20analysis%20of%20memory%20taxonomy%2C%20storage%20mechanisms%2C%20and%20the%20complete%20management%20lifecycle%20from%20both%20biological%20and%20artificial%20perspectives.%20Subsequently%2C%20we%20review%20the%20mainstream%20benchmarks%20for%20evaluating%20agent%20memory.%20Additionally%2C%20we%20explore%20memory%20security%20from%20dual%20perspectives%20of%20attack%20and%20defense.%20Finally%2C%20we%20envision%20future%20research%20directions%2C%20with%20a%20focus%20on%20multimodal%20memory%20systems%20and%20skill%20acquisition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Meets%2520Brain%253A%2520Memory%2520Systems%2520from%2520Cognitive%2520Neuroscience%2520to%2520Autonomous%2520Agents%26entry.906535625%3DJiafeng%2520Liang%2520and%2520Hao%2520Li%2520and%2520Chang%2520Li%2520and%2520Jiaqi%2520Zhou%2520and%2520Shixin%2520Jiang%2520and%2520Zekun%2520Wang%2520and%2520Changkai%2520Ji%2520and%2520Zhihao%2520Zhu%2520and%2520Runxuan%2520Liu%2520and%2520Tao%2520Ren%2520and%2520Jinlan%2520Fu%2520and%2520See-Kiong%2520Ng%2520and%2520Xia%2520Liang%2520and%2520Ming%2520Liu%2520and%2520Bing%2520Qin%26entry.1292438233%3DMemory%2520serves%2520as%2520the%2520pivotal%2520nexus%2520bridging%2520past%2520and%2520future%252C%2520providing%2520both%2520humans%2520and%2520AI%2520systems%2520with%2520invaluable%2520concepts%2520and%2520experience%2520to%2520navigate%2520complex%2520tasks.%2520Recent%2520research%2520on%2520autonomous%2520agents%2520has%2520increasingly%2520focused%2520on%2520designing%2520efficient%2520memory%2520workflows%2520by%2520drawing%2520on%2520cognitive%2520neuroscience.%2520However%252C%2520constrained%2520by%2520interdisciplinary%2520barriers%252C%2520existing%2520works%2520struggle%2520to%2520assimilate%2520the%2520essence%2520of%2520human%2520memory%2520mechanisms.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520systematically%2520synthesizes%2520interdisciplinary%2520knowledge%2520of%2520memory%252C%2520connecting%2520insights%2520from%2520cognitive%2520neuroscience%2520with%2520LLM-driven%2520agents.%2520Specifically%252C%2520we%2520first%2520elucidate%2520the%2520definition%2520and%2520function%2520of%2520memory%2520along%2520a%2520progressive%2520trajectory%2520from%2520cognitive%2520neuroscience%2520through%2520LLMs%2520to%2520agents.%2520We%2520then%2520provide%2520a%2520comparative%2520analysis%2520of%2520memory%2520taxonomy%252C%2520storage%2520mechanisms%252C%2520and%2520the%2520complete%2520management%2520lifecycle%2520from%2520both%2520biological%2520and%2520artificial%2520perspectives.%2520Subsequently%252C%2520we%2520review%2520the%2520mainstream%2520benchmarks%2520for%2520evaluating%2520agent%2520memory.%2520Additionally%252C%2520we%2520explore%2520memory%2520security%2520from%2520dual%2520perspectives%2520of%2520attack%2520and%2520defense.%2520Finally%252C%2520we%2520envision%2520future%2520research%2520directions%252C%2520with%2520a%2520focus%2520on%2520multimodal%2520memory%2520systems%2520and%2520skill%2520acquisition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Meets%20Brain%3A%20Memory%20Systems%20from%20Cognitive%20Neuroscience%20to%20Autonomous%20Agents&entry.906535625=Jiafeng%20Liang%20and%20Hao%20Li%20and%20Chang%20Li%20and%20Jiaqi%20Zhou%20and%20Shixin%20Jiang%20and%20Zekun%20Wang%20and%20Changkai%20Ji%20and%20Zhihao%20Zhu%20and%20Runxuan%20Liu%20and%20Tao%20Ren%20and%20Jinlan%20Fu%20and%20See-Kiong%20Ng%20and%20Xia%20Liang%20and%20Ming%20Liu%20and%20Bing%20Qin&entry.1292438233=Memory%20serves%20as%20the%20pivotal%20nexus%20bridging%20past%20and%20future%2C%20providing%20both%20humans%20and%20AI%20systems%20with%20invaluable%20concepts%20and%20experience%20to%20navigate%20complex%20tasks.%20Recent%20research%20on%20autonomous%20agents%20has%20increasingly%20focused%20on%20designing%20efficient%20memory%20workflows%20by%20drawing%20on%20cognitive%20neuroscience.%20However%2C%20constrained%20by%20interdisciplinary%20barriers%2C%20existing%20works%20struggle%20to%20assimilate%20the%20essence%20of%20human%20memory%20mechanisms.%20To%20bridge%20this%20gap%2C%20we%20systematically%20synthesizes%20interdisciplinary%20knowledge%20of%20memory%2C%20connecting%20insights%20from%20cognitive%20neuroscience%20with%20LLM-driven%20agents.%20Specifically%2C%20we%20first%20elucidate%20the%20definition%20and%20function%20of%20memory%20along%20a%20progressive%20trajectory%20from%20cognitive%20neuroscience%20through%20LLMs%20to%20agents.%20We%20then%20provide%20a%20comparative%20analysis%20of%20memory%20taxonomy%2C%20storage%20mechanisms%2C%20and%20the%20complete%20management%20lifecycle%20from%20both%20biological%20and%20artificial%20perspectives.%20Subsequently%2C%20we%20review%20the%20mainstream%20benchmarks%20for%20evaluating%20agent%20memory.%20Additionally%2C%20we%20explore%20memory%20security%20from%20dual%20perspectives%20of%20attack%20and%20defense.%20Finally%2C%20we%20envision%20future%20research%20directions%2C%20with%20a%20focus%20on%20multimodal%20memory%20systems%20and%20skill%20acquisition.&entry.1838667208=http%3A//arxiv.org/abs/2512.23343v1&entry.124074799=Read"},
{"title": "Diffusion MRI with Machine Learning", "author": "Davood Karimi and Simon K. Warfield", "abstract": "\\hspace{2mm} Diffusion-weighted magnetic resonance imaging (dMRI) of the brain offers unique capabilities including noninvasive probing of tissue microstructure and structural connectivity. It is widely used for clinical assessment of disease and injury, and for neuroscience research. Analyzing the dMRI data to extract useful information for medical and scientific purposes can be challenging. The dMRI measurements may suffer from strong noise and artifacts, and may exhibit high inter-session and inter-scanner variability in the data, as well as inter-subject heterogeneity in brain structure. Moreover, the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed data preprocessing and harmonization, microstructure mapping, tractography, and white matter tract analysis. We study the main findings, strengths, and weaknesses of the existing methods and suggest topics for future research. We find that machine learning may be exceptionally suited to tackle some of the difficult tasks in dMRI analysis. However, for this to happen, several shortcomings of existing methods and critical unresolved issues need to be addressed. There is a pressing need to improve evaluation practices, to increase the availability of rich training datasets and validation benchmarks, as well as model generalizability, reliability, and explainability concerns.", "link": "http://arxiv.org/abs/2402.00019v6", "date": "2025-12-29", "relevancy": 1.9445, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20MRI%20with%20Machine%20Learning&body=Title%3A%20Diffusion%20MRI%20with%20Machine%20Learning%0AAuthor%3A%20Davood%20Karimi%20and%20Simon%20K.%20Warfield%0AAbstract%3A%20%5Chspace%7B2mm%7D%20Diffusion-weighted%20magnetic%20resonance%20imaging%20%28dMRI%29%20of%20the%20brain%20offers%20unique%20capabilities%20including%20noninvasive%20probing%20of%20tissue%20microstructure%20and%20structural%20connectivity.%20It%20is%20widely%20used%20for%20clinical%20assessment%20of%20disease%20and%20injury%2C%20and%20for%20neuroscience%20research.%20Analyzing%20the%20dMRI%20data%20to%20extract%20useful%20information%20for%20medical%20and%20scientific%20purposes%20can%20be%20challenging.%20The%20dMRI%20measurements%20may%20suffer%20from%20strong%20noise%20and%20artifacts%2C%20and%20may%20exhibit%20high%20inter-session%20and%20inter-scanner%20variability%20in%20the%20data%2C%20as%20well%20as%20inter-subject%20heterogeneity%20in%20brain%20structure.%20Moreover%2C%20the%20relationship%20between%20measurements%20and%20the%20phenomena%20of%20interest%20can%20be%20highly%20complex.%20Recent%20years%20have%20witnessed%20increasing%20use%20of%20machine%20learning%20methods%20for%20dMRI%20analysis.%20This%20manuscript%20aims%20to%20assess%20these%20efforts%2C%20with%20a%20focus%20on%20methods%20that%20have%20addressed%20data%20preprocessing%20and%20harmonization%2C%20microstructure%20mapping%2C%20tractography%2C%20and%20white%20matter%20tract%20analysis.%20We%20study%20the%20main%20findings%2C%20strengths%2C%20and%20weaknesses%20of%20the%20existing%20methods%20and%20suggest%20topics%20for%20future%20research.%20We%20find%20that%20machine%20learning%20may%20be%20exceptionally%20suited%20to%20tackle%20some%20of%20the%20difficult%20tasks%20in%20dMRI%20analysis.%20However%2C%20for%20this%20to%20happen%2C%20several%20shortcomings%20of%20existing%20methods%20and%20critical%20unresolved%20issues%20need%20to%20be%20addressed.%20There%20is%20a%20pressing%20need%20to%20improve%20evaluation%20practices%2C%20to%20increase%20the%20availability%20of%20rich%20training%20datasets%20and%20validation%20benchmarks%2C%20as%20well%20as%20model%20generalizability%2C%20reliability%2C%20and%20explainability%20concerns.%0ALink%3A%20http%3A//arxiv.org/abs/2402.00019v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520MRI%2520with%2520Machine%2520Learning%26entry.906535625%3DDavood%2520Karimi%2520and%2520Simon%2520K.%2520Warfield%26entry.1292438233%3D%255Chspace%257B2mm%257D%2520Diffusion-weighted%2520magnetic%2520resonance%2520imaging%2520%2528dMRI%2529%2520of%2520the%2520brain%2520offers%2520unique%2520capabilities%2520including%2520noninvasive%2520probing%2520of%2520tissue%2520microstructure%2520and%2520structural%2520connectivity.%2520It%2520is%2520widely%2520used%2520for%2520clinical%2520assessment%2520of%2520disease%2520and%2520injury%252C%2520and%2520for%2520neuroscience%2520research.%2520Analyzing%2520the%2520dMRI%2520data%2520to%2520extract%2520useful%2520information%2520for%2520medical%2520and%2520scientific%2520purposes%2520can%2520be%2520challenging.%2520The%2520dMRI%2520measurements%2520may%2520suffer%2520from%2520strong%2520noise%2520and%2520artifacts%252C%2520and%2520may%2520exhibit%2520high%2520inter-session%2520and%2520inter-scanner%2520variability%2520in%2520the%2520data%252C%2520as%2520well%2520as%2520inter-subject%2520heterogeneity%2520in%2520brain%2520structure.%2520Moreover%252C%2520the%2520relationship%2520between%2520measurements%2520and%2520the%2520phenomena%2520of%2520interest%2520can%2520be%2520highly%2520complex.%2520Recent%2520years%2520have%2520witnessed%2520increasing%2520use%2520of%2520machine%2520learning%2520methods%2520for%2520dMRI%2520analysis.%2520This%2520manuscript%2520aims%2520to%2520assess%2520these%2520efforts%252C%2520with%2520a%2520focus%2520on%2520methods%2520that%2520have%2520addressed%2520data%2520preprocessing%2520and%2520harmonization%252C%2520microstructure%2520mapping%252C%2520tractography%252C%2520and%2520white%2520matter%2520tract%2520analysis.%2520We%2520study%2520the%2520main%2520findings%252C%2520strengths%252C%2520and%2520weaknesses%2520of%2520the%2520existing%2520methods%2520and%2520suggest%2520topics%2520for%2520future%2520research.%2520We%2520find%2520that%2520machine%2520learning%2520may%2520be%2520exceptionally%2520suited%2520to%2520tackle%2520some%2520of%2520the%2520difficult%2520tasks%2520in%2520dMRI%2520analysis.%2520However%252C%2520for%2520this%2520to%2520happen%252C%2520several%2520shortcomings%2520of%2520existing%2520methods%2520and%2520critical%2520unresolved%2520issues%2520need%2520to%2520be%2520addressed.%2520There%2520is%2520a%2520pressing%2520need%2520to%2520improve%2520evaluation%2520practices%252C%2520to%2520increase%2520the%2520availability%2520of%2520rich%2520training%2520datasets%2520and%2520validation%2520benchmarks%252C%2520as%2520well%2520as%2520model%2520generalizability%252C%2520reliability%252C%2520and%2520explainability%2520concerns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00019v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20MRI%20with%20Machine%20Learning&entry.906535625=Davood%20Karimi%20and%20Simon%20K.%20Warfield&entry.1292438233=%5Chspace%7B2mm%7D%20Diffusion-weighted%20magnetic%20resonance%20imaging%20%28dMRI%29%20of%20the%20brain%20offers%20unique%20capabilities%20including%20noninvasive%20probing%20of%20tissue%20microstructure%20and%20structural%20connectivity.%20It%20is%20widely%20used%20for%20clinical%20assessment%20of%20disease%20and%20injury%2C%20and%20for%20neuroscience%20research.%20Analyzing%20the%20dMRI%20data%20to%20extract%20useful%20information%20for%20medical%20and%20scientific%20purposes%20can%20be%20challenging.%20The%20dMRI%20measurements%20may%20suffer%20from%20strong%20noise%20and%20artifacts%2C%20and%20may%20exhibit%20high%20inter-session%20and%20inter-scanner%20variability%20in%20the%20data%2C%20as%20well%20as%20inter-subject%20heterogeneity%20in%20brain%20structure.%20Moreover%2C%20the%20relationship%20between%20measurements%20and%20the%20phenomena%20of%20interest%20can%20be%20highly%20complex.%20Recent%20years%20have%20witnessed%20increasing%20use%20of%20machine%20learning%20methods%20for%20dMRI%20analysis.%20This%20manuscript%20aims%20to%20assess%20these%20efforts%2C%20with%20a%20focus%20on%20methods%20that%20have%20addressed%20data%20preprocessing%20and%20harmonization%2C%20microstructure%20mapping%2C%20tractography%2C%20and%20white%20matter%20tract%20analysis.%20We%20study%20the%20main%20findings%2C%20strengths%2C%20and%20weaknesses%20of%20the%20existing%20methods%20and%20suggest%20topics%20for%20future%20research.%20We%20find%20that%20machine%20learning%20may%20be%20exceptionally%20suited%20to%20tackle%20some%20of%20the%20difficult%20tasks%20in%20dMRI%20analysis.%20However%2C%20for%20this%20to%20happen%2C%20several%20shortcomings%20of%20existing%20methods%20and%20critical%20unresolved%20issues%20need%20to%20be%20addressed.%20There%20is%20a%20pressing%20need%20to%20improve%20evaluation%20practices%2C%20to%20increase%20the%20availability%20of%20rich%20training%20datasets%20and%20validation%20benchmarks%2C%20as%20well%20as%20model%20generalizability%2C%20reliability%2C%20and%20explainability%20concerns.&entry.1838667208=http%3A//arxiv.org/abs/2402.00019v6&entry.124074799=Read"},
{"title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "author": "Hau-Shiang Shiu and Chin-Yang Lin and Zhixiang Wang and Chi-Wei Hsiao and Po-Fan Yu and Yu-Chih Chen and Yu-Lun Liu", "abstract": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "link": "http://arxiv.org/abs/2512.23709v1", "date": "2025-12-29", "relevancy": 1.7707, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.64}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5764}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stream-DiffVSR%3A%20Low-Latency%20Streamable%20Video%20Super-Resolution%20via%20Auto-Regressive%20Diffusion&body=Title%3A%20Stream-DiffVSR%3A%20Low-Latency%20Streamable%20Video%20Super-Resolution%20via%20Auto-Regressive%20Diffusion%0AAuthor%3A%20Hau-Shiang%20Shiu%20and%20Chin-Yang%20Lin%20and%20Zhixiang%20Wang%20and%20Chi-Wei%20Hsiao%20and%20Po-Fan%20Yu%20and%20Yu-Chih%20Chen%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Diffusion-based%20video%20super-resolution%20%28VSR%29%20methods%20achieve%20strong%20perceptual%20quality%20but%20remain%20impractical%20for%20latency-sensitive%20settings%20due%20to%20reliance%20on%20future%20frames%20and%20expensive%20multi-step%20denoising.%20We%20propose%20Stream-DiffVSR%2C%20a%20causally%20conditioned%20diffusion%20framework%20for%20efficient%20online%20VSR.%20Operating%20strictly%20on%20past%20frames%2C%20it%20combines%20a%20four-step%20distilled%20denoiser%20for%20fast%20inference%2C%20an%20Auto-regressive%20Temporal%20Guidance%20%28ARTG%29%20module%20that%20injects%20motion-aligned%20cues%20during%20latent%20denoising%2C%20and%20a%20lightweight%20temporal-aware%20decoder%20with%20a%20Temporal%20Processor%20Module%20%28TPM%29%20that%20enhances%20detail%20and%20temporal%20coherence.%20Stream-DiffVSR%20processes%20720p%20frames%20in%200.328%20seconds%20on%20an%20RTX4090%20GPU%20and%20significantly%20outperforms%20prior%20diffusion-based%20methods.%20Compared%20with%20the%20online%20SOTA%20TMP%2C%20it%20boosts%20perceptual%20quality%20%28LPIPS%20%2B0.095%29%20while%20reducing%20latency%20by%20over%20130x.%20Stream-DiffVSR%20achieves%20the%20lowest%20latency%20reported%20for%20diffusion-based%20VSR%2C%20reducing%20initial%20delay%20from%20over%204600%20seconds%20to%200.328%20seconds%2C%20thereby%20making%20it%20the%20first%20diffusion%20VSR%20method%20suitable%20for%20low-latency%20online%20deployment.%20Project%20page%3A%20https%3A//jamichss.github.io/stream-diffvsr-project-page/%0ALink%3A%20http%3A//arxiv.org/abs/2512.23709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStream-DiffVSR%253A%2520Low-Latency%2520Streamable%2520Video%2520Super-Resolution%2520via%2520Auto-Regressive%2520Diffusion%26entry.906535625%3DHau-Shiang%2520Shiu%2520and%2520Chin-Yang%2520Lin%2520and%2520Zhixiang%2520Wang%2520and%2520Chi-Wei%2520Hsiao%2520and%2520Po-Fan%2520Yu%2520and%2520Yu-Chih%2520Chen%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DDiffusion-based%2520video%2520super-resolution%2520%2528VSR%2529%2520methods%2520achieve%2520strong%2520perceptual%2520quality%2520but%2520remain%2520impractical%2520for%2520latency-sensitive%2520settings%2520due%2520to%2520reliance%2520on%2520future%2520frames%2520and%2520expensive%2520multi-step%2520denoising.%2520We%2520propose%2520Stream-DiffVSR%252C%2520a%2520causally%2520conditioned%2520diffusion%2520framework%2520for%2520efficient%2520online%2520VSR.%2520Operating%2520strictly%2520on%2520past%2520frames%252C%2520it%2520combines%2520a%2520four-step%2520distilled%2520denoiser%2520for%2520fast%2520inference%252C%2520an%2520Auto-regressive%2520Temporal%2520Guidance%2520%2528ARTG%2529%2520module%2520that%2520injects%2520motion-aligned%2520cues%2520during%2520latent%2520denoising%252C%2520and%2520a%2520lightweight%2520temporal-aware%2520decoder%2520with%2520a%2520Temporal%2520Processor%2520Module%2520%2528TPM%2529%2520that%2520enhances%2520detail%2520and%2520temporal%2520coherence.%2520Stream-DiffVSR%2520processes%2520720p%2520frames%2520in%25200.328%2520seconds%2520on%2520an%2520RTX4090%2520GPU%2520and%2520significantly%2520outperforms%2520prior%2520diffusion-based%2520methods.%2520Compared%2520with%2520the%2520online%2520SOTA%2520TMP%252C%2520it%2520boosts%2520perceptual%2520quality%2520%2528LPIPS%2520%252B0.095%2529%2520while%2520reducing%2520latency%2520by%2520over%2520130x.%2520Stream-DiffVSR%2520achieves%2520the%2520lowest%2520latency%2520reported%2520for%2520diffusion-based%2520VSR%252C%2520reducing%2520initial%2520delay%2520from%2520over%25204600%2520seconds%2520to%25200.328%2520seconds%252C%2520thereby%2520making%2520it%2520the%2520first%2520diffusion%2520VSR%2520method%2520suitable%2520for%2520low-latency%2520online%2520deployment.%2520Project%2520page%253A%2520https%253A//jamichss.github.io/stream-diffvsr-project-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stream-DiffVSR%3A%20Low-Latency%20Streamable%20Video%20Super-Resolution%20via%20Auto-Regressive%20Diffusion&entry.906535625=Hau-Shiang%20Shiu%20and%20Chin-Yang%20Lin%20and%20Zhixiang%20Wang%20and%20Chi-Wei%20Hsiao%20and%20Po-Fan%20Yu%20and%20Yu-Chih%20Chen%20and%20Yu-Lun%20Liu&entry.1292438233=Diffusion-based%20video%20super-resolution%20%28VSR%29%20methods%20achieve%20strong%20perceptual%20quality%20but%20remain%20impractical%20for%20latency-sensitive%20settings%20due%20to%20reliance%20on%20future%20frames%20and%20expensive%20multi-step%20denoising.%20We%20propose%20Stream-DiffVSR%2C%20a%20causally%20conditioned%20diffusion%20framework%20for%20efficient%20online%20VSR.%20Operating%20strictly%20on%20past%20frames%2C%20it%20combines%20a%20four-step%20distilled%20denoiser%20for%20fast%20inference%2C%20an%20Auto-regressive%20Temporal%20Guidance%20%28ARTG%29%20module%20that%20injects%20motion-aligned%20cues%20during%20latent%20denoising%2C%20and%20a%20lightweight%20temporal-aware%20decoder%20with%20a%20Temporal%20Processor%20Module%20%28TPM%29%20that%20enhances%20detail%20and%20temporal%20coherence.%20Stream-DiffVSR%20processes%20720p%20frames%20in%200.328%20seconds%20on%20an%20RTX4090%20GPU%20and%20significantly%20outperforms%20prior%20diffusion-based%20methods.%20Compared%20with%20the%20online%20SOTA%20TMP%2C%20it%20boosts%20perceptual%20quality%20%28LPIPS%20%2B0.095%29%20while%20reducing%20latency%20by%20over%20130x.%20Stream-DiffVSR%20achieves%20the%20lowest%20latency%20reported%20for%20diffusion-based%20VSR%2C%20reducing%20initial%20delay%20from%20over%204600%20seconds%20to%200.328%20seconds%2C%20thereby%20making%20it%20the%20first%20diffusion%20VSR%20method%20suitable%20for%20low-latency%20online%20deployment.%20Project%20page%3A%20https%3A//jamichss.github.io/stream-diffvsr-project-page/&entry.1838667208=http%3A//arxiv.org/abs/2512.23709v1&entry.124074799=Read"},
{"title": "Probabilistic Modelling is Sufficient for Causal Inference", "author": "Bruno Mlodozeniec and David Krueger and Richard E. Turner", "abstract": "Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \\emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.", "link": "http://arxiv.org/abs/2512.23408v1", "date": "2025-12-29", "relevancy": 1.3073, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4427}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Modelling%20is%20Sufficient%20for%20Causal%20Inference&body=Title%3A%20Probabilistic%20Modelling%20is%20Sufficient%20for%20Causal%20Inference%0AAuthor%3A%20Bruno%20Mlodozeniec%20and%20David%20Krueger%20and%20Richard%20E.%20Turner%0AAbstract%3A%20Causal%20inference%20is%20a%20key%20research%20area%20in%20machine%20learning%2C%20yet%20confusion%20reigns%20over%20the%20tools%20needed%20to%20tackle%20it.%20There%20are%20prevalent%20claims%20in%20the%20machine%20learning%20literature%20that%20you%20need%20a%20bespoke%20causal%20framework%20or%20notation%20to%20answer%20causal%20questions.%20In%20this%20paper%2C%20we%20want%20to%20make%20it%20clear%20that%20you%20%5Cemph%7Bcan%7D%20answer%20any%20causal%20inference%20question%20within%20the%20realm%20of%20probabilistic%20modelling%20and%20inference%2C%20without%20causal-specific%20tools%20or%20notation.%20Through%20concrete%20examples%2C%20we%20demonstrate%20how%20causal%20questions%20can%20be%20tackled%20by%20writing%20down%20the%20probability%20of%20everything.%20Lastly%2C%20we%20reinterpret%20causal%20tools%20as%20emerging%20from%20standard%20probabilistic%20modelling%20and%20inference%2C%20elucidating%20their%20necessity%20and%20utility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Modelling%2520is%2520Sufficient%2520for%2520Causal%2520Inference%26entry.906535625%3DBruno%2520Mlodozeniec%2520and%2520David%2520Krueger%2520and%2520Richard%2520E.%2520Turner%26entry.1292438233%3DCausal%2520inference%2520is%2520a%2520key%2520research%2520area%2520in%2520machine%2520learning%252C%2520yet%2520confusion%2520reigns%2520over%2520the%2520tools%2520needed%2520to%2520tackle%2520it.%2520There%2520are%2520prevalent%2520claims%2520in%2520the%2520machine%2520learning%2520literature%2520that%2520you%2520need%2520a%2520bespoke%2520causal%2520framework%2520or%2520notation%2520to%2520answer%2520causal%2520questions.%2520In%2520this%2520paper%252C%2520we%2520want%2520to%2520make%2520it%2520clear%2520that%2520you%2520%255Cemph%257Bcan%257D%2520answer%2520any%2520causal%2520inference%2520question%2520within%2520the%2520realm%2520of%2520probabilistic%2520modelling%2520and%2520inference%252C%2520without%2520causal-specific%2520tools%2520or%2520notation.%2520Through%2520concrete%2520examples%252C%2520we%2520demonstrate%2520how%2520causal%2520questions%2520can%2520be%2520tackled%2520by%2520writing%2520down%2520the%2520probability%2520of%2520everything.%2520Lastly%252C%2520we%2520reinterpret%2520causal%2520tools%2520as%2520emerging%2520from%2520standard%2520probabilistic%2520modelling%2520and%2520inference%252C%2520elucidating%2520their%2520necessity%2520and%2520utility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Modelling%20is%20Sufficient%20for%20Causal%20Inference&entry.906535625=Bruno%20Mlodozeniec%20and%20David%20Krueger%20and%20Richard%20E.%20Turner&entry.1292438233=Causal%20inference%20is%20a%20key%20research%20area%20in%20machine%20learning%2C%20yet%20confusion%20reigns%20over%20the%20tools%20needed%20to%20tackle%20it.%20There%20are%20prevalent%20claims%20in%20the%20machine%20learning%20literature%20that%20you%20need%20a%20bespoke%20causal%20framework%20or%20notation%20to%20answer%20causal%20questions.%20In%20this%20paper%2C%20we%20want%20to%20make%20it%20clear%20that%20you%20%5Cemph%7Bcan%7D%20answer%20any%20causal%20inference%20question%20within%20the%20realm%20of%20probabilistic%20modelling%20and%20inference%2C%20without%20causal-specific%20tools%20or%20notation.%20Through%20concrete%20examples%2C%20we%20demonstrate%20how%20causal%20questions%20can%20be%20tackled%20by%20writing%20down%20the%20probability%20of%20everything.%20Lastly%2C%20we%20reinterpret%20causal%20tools%20as%20emerging%20from%20standard%20probabilistic%20modelling%20and%20inference%2C%20elucidating%20their%20necessity%20and%20utility.&entry.1838667208=http%3A//arxiv.org/abs/2512.23408v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


