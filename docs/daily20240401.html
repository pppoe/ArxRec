<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Long-Tailed Anomaly Detection with Learnable Class Names", "author": "Chih-Hui Ho and Kuan-Chuan Peng and Nuno Vasconcelos", "abstract": "  Anomaly detection (AD) aims to identify defective images and localize their\ndefects (if any). Ideally, AD models should be able to detect defects over many\nimage classes; without relying on hard-coded class names that can be\nuninformative or inconsistent across datasets; learn without anomaly\nsupervision; and be robust to the long-tailed distributions of real-world\napplications. To address these challenges, we formulate the problem of\nlong-tailed AD by introducing several datasets with different levels of class\nimbalance and metrics for performance evaluation. We then propose a novel\nmethod, LTAD, to detect defects from multiple and long-tailed classes, without\nrelying on dataset class names. LTAD combines AD by reconstruction and semantic\nAD modules. AD by reconstruction is implemented with a transformer-based\nreconstruction module. Semantic AD is implemented with a binary classifier,\nwhich relies on learned pseudo class names and a pretrained foundation model.\nThese modules are learned over two phases. Phase 1 learns the pseudo-class\nnames and a variational autoencoder (VAE) for feature synthesis that augments\nthe training data to combat long-tails. Phase 2 then learns the parameters of\nthe reconstruction and classification modules of LTAD. Extensive experiments\nusing the proposed long-tailed datasets show that LTAD substantially\noutperforms the state-of-the-art methods for most forms of dataset imbalance.\nThe long-tailed dataset split is available at\nhttps://zenodo.org/records/10854201 .\n", "link": "http://arxiv.org/abs/2403.20236v1", "date": "2024-03-29", "relevancy": 2.9185, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%20Anomaly%20Detection%20with%20Learnable%20Class%20Names&body=Title%3A%20Long-Tailed%20Anomaly%20Detection%20with%20Learnable%20Class%20Names%0AAuthor%3A%20Chih-Hui%20Ho%20and%20Kuan-Chuan%20Peng%20and%20Nuno%20Vasconcelos%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20aims%20to%20identify%20defective%20images%20and%20localize%20their%0Adefects%20%28if%20any%29.%20Ideally%2C%20AD%20models%20should%20be%20able%20to%20detect%20defects%20over%20many%0Aimage%20classes%3B%20without%20relying%20on%20hard-coded%20class%20names%20that%20can%20be%0Auninformative%20or%20inconsistent%20across%20datasets%3B%20learn%20without%20anomaly%0Asupervision%3B%20and%20be%20robust%20to%20the%20long-tailed%20distributions%20of%20real-world%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20formulate%20the%20problem%20of%0Along-tailed%20AD%20by%20introducing%20several%20datasets%20with%20different%20levels%20of%20class%0Aimbalance%20and%20metrics%20for%20performance%20evaluation.%20We%20then%20propose%20a%20novel%0Amethod%2C%20LTAD%2C%20to%20detect%20defects%20from%20multiple%20and%20long-tailed%20classes%2C%20without%0Arelying%20on%20dataset%20class%20names.%20LTAD%20combines%20AD%20by%20reconstruction%20and%20semantic%0AAD%20modules.%20AD%20by%20reconstruction%20is%20implemented%20with%20a%20transformer-based%0Areconstruction%20module.%20Semantic%20AD%20is%20implemented%20with%20a%20binary%20classifier%2C%0Awhich%20relies%20on%20learned%20pseudo%20class%20names%20and%20a%20pretrained%20foundation%20model.%0AThese%20modules%20are%20learned%20over%20two%20phases.%20Phase%201%20learns%20the%20pseudo-class%0Anames%20and%20a%20variational%20autoencoder%20%28VAE%29%20for%20feature%20synthesis%20that%20augments%0Athe%20training%20data%20to%20combat%20long-tails.%20Phase%202%20then%20learns%20the%20parameters%20of%0Athe%20reconstruction%20and%20classification%20modules%20of%20LTAD.%20Extensive%20experiments%0Ausing%20the%20proposed%20long-tailed%20datasets%20show%20that%20LTAD%20substantially%0Aoutperforms%20the%20state-of-the-art%20methods%20for%20most%20forms%20of%20dataset%20imbalance.%0AThe%20long-tailed%20dataset%20split%20is%20available%20at%0Ahttps%3A//zenodo.org/records/10854201%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20236v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%20Anomaly%20Detection%20with%20Learnable%20Class%20Names&entry.906535625=Chih-Hui%20Ho%20and%20Kuan-Chuan%20Peng%20and%20Nuno%20Vasconcelos&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20aims%20to%20identify%20defective%20images%20and%20localize%20their%0Adefects%20%28if%20any%29.%20Ideally%2C%20AD%20models%20should%20be%20able%20to%20detect%20defects%20over%20many%0Aimage%20classes%3B%20without%20relying%20on%20hard-coded%20class%20names%20that%20can%20be%0Auninformative%20or%20inconsistent%20across%20datasets%3B%20learn%20without%20anomaly%0Asupervision%3B%20and%20be%20robust%20to%20the%20long-tailed%20distributions%20of%20real-world%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20formulate%20the%20problem%20of%0Along-tailed%20AD%20by%20introducing%20several%20datasets%20with%20different%20levels%20of%20class%0Aimbalance%20and%20metrics%20for%20performance%20evaluation.%20We%20then%20propose%20a%20novel%0Amethod%2C%20LTAD%2C%20to%20detect%20defects%20from%20multiple%20and%20long-tailed%20classes%2C%20without%0Arelying%20on%20dataset%20class%20names.%20LTAD%20combines%20AD%20by%20reconstruction%20and%20semantic%0AAD%20modules.%20AD%20by%20reconstruction%20is%20implemented%20with%20a%20transformer-based%0Areconstruction%20module.%20Semantic%20AD%20is%20implemented%20with%20a%20binary%20classifier%2C%0Awhich%20relies%20on%20learned%20pseudo%20class%20names%20and%20a%20pretrained%20foundation%20model.%0AThese%20modules%20are%20learned%20over%20two%20phases.%20Phase%201%20learns%20the%20pseudo-class%0Anames%20and%20a%20variational%20autoencoder%20%28VAE%29%20for%20feature%20synthesis%20that%20augments%0Athe%20training%20data%20to%20combat%20long-tails.%20Phase%202%20then%20learns%20the%20parameters%20of%0Athe%20reconstruction%20and%20classification%20modules%20of%20LTAD.%20Extensive%20experiments%0Ausing%20the%20proposed%20long-tailed%20datasets%20show%20that%20LTAD%20substantially%0Aoutperforms%20the%20state-of-the-art%20methods%20for%20most%20forms%20of%20dataset%20imbalance.%0AThe%20long-tailed%20dataset%20split%20is%20available%20at%0Ahttps%3A//zenodo.org/records/10854201%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20236v1&entry.124074799=Read"},
{"title": "SPOT: Self-Training with Patch-Order Permutation for Object-Centric\n  Learning with Autoregressive Transformers", "author": "Ioannis Kakogeorgiou and Spyros Gidaris and Konstantinos Karantzalos and Nikos Komodakis", "abstract": "  Unsupervised object-centric learning aims to decompose scenes into\ninterpretable object entities, termed slots. Slot-based auto-encoders stand out\nas a prominent method for this task. Within them, crucial aspects include\nguiding the encoder to generate object-specific slots and ensuring the decoder\nutilizes them during reconstruction. This work introduces two novel techniques,\n(i) an attention-based self-training approach, which distills superior\nslot-based attention masks from the decoder to the encoder, enhancing object\nsegmentation, and (ii) an innovative patch-order permutation strategy for\nautoregressive transformers that strengthens the role of slot vectors in\nreconstruction. The effectiveness of these strategies is showcased\nexperimentally. The combined approach significantly surpasses prior slot-based\nautoencoder methods in unsupervised object segmentation, especially with\ncomplex real-world images. We provide the implementation code at\nhttps://github.com/gkakogeorgiou/spot .\n", "link": "http://arxiv.org/abs/2312.00648v2", "date": "2024-03-29", "relevancy": 2.7992, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5425}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers&body=Title%3A%20SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers%0AAuthor%3A%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Konstantinos%20Karantzalos%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Unsupervised%20object-centric%20learning%20aims%20to%20decompose%20scenes%20into%0Ainterpretable%20object%20entities%2C%20termed%20slots.%20Slot-based%20auto-encoders%20stand%20out%0Aas%20a%20prominent%20method%20for%20this%20task.%20Within%20them%2C%20crucial%20aspects%20include%0Aguiding%20the%20encoder%20to%20generate%20object-specific%20slots%20and%20ensuring%20the%20decoder%0Autilizes%20them%20during%20reconstruction.%20This%20work%20introduces%20two%20novel%20techniques%2C%0A%28i%29%20an%20attention-based%20self-training%20approach%2C%20which%20distills%20superior%0Aslot-based%20attention%20masks%20from%20the%20decoder%20to%20the%20encoder%2C%20enhancing%20object%0Asegmentation%2C%20and%20%28ii%29%20an%20innovative%20patch-order%20permutation%20strategy%20for%0Aautoregressive%20transformers%20that%20strengthens%20the%20role%20of%20slot%20vectors%20in%0Areconstruction.%20The%20effectiveness%20of%20these%20strategies%20is%20showcased%0Aexperimentally.%20The%20combined%20approach%20significantly%20surpasses%20prior%20slot-based%0Aautoencoder%20methods%20in%20unsupervised%20object%20segmentation%2C%20especially%20with%0Acomplex%20real-world%20images.%20We%20provide%20the%20implementation%20code%20at%0Ahttps%3A//github.com/gkakogeorgiou/spot%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00648v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%3A%20Self-Training%20with%20Patch-Order%20Permutation%20for%20Object-Centric%0A%20%20Learning%20with%20Autoregressive%20Transformers&entry.906535625=Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Konstantinos%20Karantzalos%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Unsupervised%20object-centric%20learning%20aims%20to%20decompose%20scenes%20into%0Ainterpretable%20object%20entities%2C%20termed%20slots.%20Slot-based%20auto-encoders%20stand%20out%0Aas%20a%20prominent%20method%20for%20this%20task.%20Within%20them%2C%20crucial%20aspects%20include%0Aguiding%20the%20encoder%20to%20generate%20object-specific%20slots%20and%20ensuring%20the%20decoder%0Autilizes%20them%20during%20reconstruction.%20This%20work%20introduces%20two%20novel%20techniques%2C%0A%28i%29%20an%20attention-based%20self-training%20approach%2C%20which%20distills%20superior%0Aslot-based%20attention%20masks%20from%20the%20decoder%20to%20the%20encoder%2C%20enhancing%20object%0Asegmentation%2C%20and%20%28ii%29%20an%20innovative%20patch-order%20permutation%20strategy%20for%0Aautoregressive%20transformers%20that%20strengthens%20the%20role%20of%20slot%20vectors%20in%0Areconstruction.%20The%20effectiveness%20of%20these%20strategies%20is%20showcased%0Aexperimentally.%20The%20combined%20approach%20significantly%20surpasses%20prior%20slot-based%0Aautoencoder%20methods%20in%20unsupervised%20object%20segmentation%2C%20especially%20with%0Acomplex%20real-world%20images.%20We%20provide%20the%20implementation%20code%20at%0Ahttps%3A//github.com/gkakogeorgiou/spot%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00648v2&entry.124074799=Read"},
{"title": "3DInAction: Understanding Human Actions in 3D Point Clouds", "author": "Yizhak Ben-Shabat and Oren Shrout and Stephen Gould", "abstract": "  We propose a novel method for 3D point cloud action recognition.\nUnderstanding human actions in RGB videos has been widely studied in recent\nyears, however, its 3D point cloud counterpart remains under-explored. This is\nmostly due to the inherent limitation of the point cloud data modality -- lack\nof structure, permutation invariance, and varying number of points -- which\nmakes it difficult to learn a spatio-temporal representation. To address this\nlimitation, we propose the 3DinAction pipeline that first estimates patches\nmoving in time (t-patches) as a key building block, alongside a hierarchical\narchitecture that learns an informative spatio-temporal representation. We show\nthat our method achieves improved performance on existing datasets, including\nDFAUST and IKEA ASM. Code is publicly available at\nhttps://github.com/sitzikbs/3dincaction.\n", "link": "http://arxiv.org/abs/2303.06346v2", "date": "2024-03-29", "relevancy": 2.7634, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5581}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203DInAction%3A%20Understanding%20Human%20Actions%20in%203D%20Point%20Clouds&body=Title%3A%203DInAction%3A%20Understanding%20Human%20Actions%20in%203D%20Point%20Clouds%0AAuthor%3A%20Yizhak%20Ben-Shabat%20and%20Oren%20Shrout%20and%20Stephen%20Gould%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20for%203D%20point%20cloud%20action%20recognition.%0AUnderstanding%20human%20actions%20in%20RGB%20videos%20has%20been%20widely%20studied%20in%20recent%0Ayears%2C%20however%2C%20its%203D%20point%20cloud%20counterpart%20remains%20under-explored.%20This%20is%0Amostly%20due%20to%20the%20inherent%20limitation%20of%20the%20point%20cloud%20data%20modality%20--%20lack%0Aof%20structure%2C%20permutation%20invariance%2C%20and%20varying%20number%20of%20points%20--%20which%0Amakes%20it%20difficult%20to%20learn%20a%20spatio-temporal%20representation.%20To%20address%20this%0Alimitation%2C%20we%20propose%20the%203DinAction%20pipeline%20that%20first%20estimates%20patches%0Amoving%20in%20time%20%28t-patches%29%20as%20a%20key%20building%20block%2C%20alongside%20a%20hierarchical%0Aarchitecture%20that%20learns%20an%20informative%20spatio-temporal%20representation.%20We%20show%0Athat%20our%20method%20achieves%20improved%20performance%20on%20existing%20datasets%2C%20including%0ADFAUST%20and%20IKEA%20ASM.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/sitzikbs/3dincaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06346v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DInAction%3A%20Understanding%20Human%20Actions%20in%203D%20Point%20Clouds&entry.906535625=Yizhak%20Ben-Shabat%20and%20Oren%20Shrout%20and%20Stephen%20Gould&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20for%203D%20point%20cloud%20action%20recognition.%0AUnderstanding%20human%20actions%20in%20RGB%20videos%20has%20been%20widely%20studied%20in%20recent%0Ayears%2C%20however%2C%20its%203D%20point%20cloud%20counterpart%20remains%20under-explored.%20This%20is%0Amostly%20due%20to%20the%20inherent%20limitation%20of%20the%20point%20cloud%20data%20modality%20--%20lack%0Aof%20structure%2C%20permutation%20invariance%2C%20and%20varying%20number%20of%20points%20--%20which%0Amakes%20it%20difficult%20to%20learn%20a%20spatio-temporal%20representation.%20To%20address%20this%0Alimitation%2C%20we%20propose%20the%203DinAction%20pipeline%20that%20first%20estimates%20patches%0Amoving%20in%20time%20%28t-patches%29%20as%20a%20key%20building%20block%2C%20alongside%20a%20hierarchical%0Aarchitecture%20that%20learns%20an%20informative%20spatio-temporal%20representation.%20We%20show%0Athat%20our%20method%20achieves%20improved%20performance%20on%20existing%20datasets%2C%20including%0ADFAUST%20and%20IKEA%20ASM.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/sitzikbs/3dincaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06346v2&entry.124074799=Read"},
{"title": "A Strong Baseline for Point Cloud Registration via Direct Superpoints\n  Matching", "author": "Aniket Gupta and Yiming Xie and Hanumant Singh and Huaizu Jiang", "abstract": "  Deep neural networks endow the downsampled superpoints with highly\ndiscriminative feature representations. Previous dominant point cloud\nregistration approaches match these feature representations as the first step,\ne.g., using the Sinkhorn algorithm. A RANSAC-like method is then usually\nadopted as a post-processing refinement to filter the outliers. Other dominant\nmethod is to directly predict the superpoint matchings using learned MLP\nlayers. Both of them have drawbacks: RANSAC-based methods are computationally\nintensive and prediction-based methods suffer from outputing non-existing\npoints in the point cloud. In this paper, we propose a straightforward and\neffective baseline to find correspondences of superpoints in a global matching\nmanner. We employ the normalized matching scores as weights for each\ncorrespondence, allowing us to reject the outliers and further weigh the rest\ninliers when fitting the transformation matrix without relying on the\ncumbersome RANSAC. Moreover, the entire model can be trained in an end-to-end\nfashion, leading to better accuracy. Our simple yet effective baseline shows\ncomparable or even better results than state-of-the-art methods on three\ndatasets including ModelNet, 3DMatch, and KITTI. We do not advocate our\napproach to be \\emph{the} solution for point cloud registration but use the\nresults to emphasize the role of matching strategy for point cloud\nregistration. The code and models are available at\nhttps://github.com/neu-vi/Superpoints_Registration.\n", "link": "http://arxiv.org/abs/2307.01362v4", "date": "2024-03-29", "relevancy": 2.7449, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Strong%20Baseline%20for%20Point%20Cloud%20Registration%20via%20Direct%20Superpoints%0A%20%20Matching&body=Title%3A%20A%20Strong%20Baseline%20for%20Point%20Cloud%20Registration%20via%20Direct%20Superpoints%0A%20%20Matching%0AAuthor%3A%20Aniket%20Gupta%20and%20Yiming%20Xie%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20endow%20the%20downsampled%20superpoints%20with%20highly%0Adiscriminative%20feature%20representations.%20Previous%20dominant%20point%20cloud%0Aregistration%20approaches%20match%20these%20feature%20representations%20as%20the%20first%20step%2C%0Ae.g.%2C%20using%20the%20Sinkhorn%20algorithm.%20A%20RANSAC-like%20method%20is%20then%20usually%0Aadopted%20as%20a%20post-processing%20refinement%20to%20filter%20the%20outliers.%20Other%20dominant%0Amethod%20is%20to%20directly%20predict%20the%20superpoint%20matchings%20using%20learned%20MLP%0Alayers.%20Both%20of%20them%20have%20drawbacks%3A%20RANSAC-based%20methods%20are%20computationally%0Aintensive%20and%20prediction-based%20methods%20suffer%20from%20outputing%20non-existing%0Apoints%20in%20the%20point%20cloud.%20In%20this%20paper%2C%20we%20propose%20a%20straightforward%20and%0Aeffective%20baseline%20to%20find%20correspondences%20of%20superpoints%20in%20a%20global%20matching%0Amanner.%20We%20employ%20the%20normalized%20matching%20scores%20as%20weights%20for%20each%0Acorrespondence%2C%20allowing%20us%20to%20reject%20the%20outliers%20and%20further%20weigh%20the%20rest%0Ainliers%20when%20fitting%20the%20transformation%20matrix%20without%20relying%20on%20the%0Acumbersome%20RANSAC.%20Moreover%2C%20the%20entire%20model%20can%20be%20trained%20in%20an%20end-to-end%0Afashion%2C%20leading%20to%20better%20accuracy.%20Our%20simple%20yet%20effective%20baseline%20shows%0Acomparable%20or%20even%20better%20results%20than%20state-of-the-art%20methods%20on%20three%0Adatasets%20including%20ModelNet%2C%203DMatch%2C%20and%20KITTI.%20We%20do%20not%20advocate%20our%0Aapproach%20to%20be%20%5Cemph%7Bthe%7D%20solution%20for%20point%20cloud%20registration%20but%20use%20the%0Aresults%20to%20emphasize%20the%20role%20of%20matching%20strategy%20for%20point%20cloud%0Aregistration.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/neu-vi/Superpoints_Registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01362v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Strong%20Baseline%20for%20Point%20Cloud%20Registration%20via%20Direct%20Superpoints%0A%20%20Matching&entry.906535625=Aniket%20Gupta%20and%20Yiming%20Xie%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Deep%20neural%20networks%20endow%20the%20downsampled%20superpoints%20with%20highly%0Adiscriminative%20feature%20representations.%20Previous%20dominant%20point%20cloud%0Aregistration%20approaches%20match%20these%20feature%20representations%20as%20the%20first%20step%2C%0Ae.g.%2C%20using%20the%20Sinkhorn%20algorithm.%20A%20RANSAC-like%20method%20is%20then%20usually%0Aadopted%20as%20a%20post-processing%20refinement%20to%20filter%20the%20outliers.%20Other%20dominant%0Amethod%20is%20to%20directly%20predict%20the%20superpoint%20matchings%20using%20learned%20MLP%0Alayers.%20Both%20of%20them%20have%20drawbacks%3A%20RANSAC-based%20methods%20are%20computationally%0Aintensive%20and%20prediction-based%20methods%20suffer%20from%20outputing%20non-existing%0Apoints%20in%20the%20point%20cloud.%20In%20this%20paper%2C%20we%20propose%20a%20straightforward%20and%0Aeffective%20baseline%20to%20find%20correspondences%20of%20superpoints%20in%20a%20global%20matching%0Amanner.%20We%20employ%20the%20normalized%20matching%20scores%20as%20weights%20for%20each%0Acorrespondence%2C%20allowing%20us%20to%20reject%20the%20outliers%20and%20further%20weigh%20the%20rest%0Ainliers%20when%20fitting%20the%20transformation%20matrix%20without%20relying%20on%20the%0Acumbersome%20RANSAC.%20Moreover%2C%20the%20entire%20model%20can%20be%20trained%20in%20an%20end-to-end%0Afashion%2C%20leading%20to%20better%20accuracy.%20Our%20simple%20yet%20effective%20baseline%20shows%0Acomparable%20or%20even%20better%20results%20than%20state-of-the-art%20methods%20on%20three%0Adatasets%20including%20ModelNet%2C%203DMatch%2C%20and%20KITTI.%20We%20do%20not%20advocate%20our%0Aapproach%20to%20be%20%5Cemph%7Bthe%7D%20solution%20for%20point%20cloud%20registration%20but%20use%20the%0Aresults%20to%20emphasize%20the%20role%20of%20matching%20strategy%20for%20point%20cloud%0Aregistration.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/neu-vi/Superpoints_Registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01362v4&entry.124074799=Read"},
{"title": "RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction", "author": "Baptiste Brument and Robin Bruneau and Yvain Qu\u00e9au and Jean M\u00e9lou and Fran\u00e7ois Bernard Lauze and  Jean-Denis and Jean-Denis Durou and Lilian Calvet", "abstract": "  This paper introduces a versatile paradigm for integrating multi-view\nreflectance (optional) and normal maps acquired through photometric stereo. Our\napproach employs a pixel-wise joint re-parameterization of reflectance and\nnormal, considering them as a vector of radiances rendered under simulated,\nvarying illumination. This re-parameterization enables the seamless integration\nof reflectance and normal maps as input data in neural volume rendering-based\n3D reconstruction while preserving a single optimization objective. In\ncontrast, recent multi-view photometric stereo (MVPS) methods depend on\nmultiple, potentially conflicting objectives. Despite its apparent simplicity,\nour proposed approach outperforms state-of-the-art approaches in MVPS\nbenchmarks across F-score, Chamfer distance, and mean angular error metrics.\nNotably, it significantly improves the detailed 3D reconstruction of areas with\nhigh curvature or low visibility.\n", "link": "http://arxiv.org/abs/2312.01215v2", "date": "2024-03-29", "relevancy": 2.662, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5396}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RNb-NeuS%3A%20Reflectance%20and%20Normal-based%20Multi-View%203D%20Reconstruction&body=Title%3A%20RNb-NeuS%3A%20Reflectance%20and%20Normal-based%20Multi-View%203D%20Reconstruction%0AAuthor%3A%20Baptiste%20Brument%20and%20Robin%20Bruneau%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20%20Jean-Denis%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20versatile%20paradigm%20for%20integrating%20multi-view%0Areflectance%20%28optional%29%20and%20normal%20maps%20acquired%20through%20photometric%20stereo.%20Our%0Aapproach%20employs%20a%20pixel-wise%20joint%20re-parameterization%20of%20reflectance%20and%0Anormal%2C%20considering%20them%20as%20a%20vector%20of%20radiances%20rendered%20under%20simulated%2C%0Avarying%20illumination.%20This%20re-parameterization%20enables%20the%20seamless%20integration%0Aof%20reflectance%20and%20normal%20maps%20as%20input%20data%20in%20neural%20volume%20rendering-based%0A3D%20reconstruction%20while%20preserving%20a%20single%20optimization%20objective.%20In%0Acontrast%2C%20recent%20multi-view%20photometric%20stereo%20%28MVPS%29%20methods%20depend%20on%0Amultiple%2C%20potentially%20conflicting%20objectives.%20Despite%20its%20apparent%20simplicity%2C%0Aour%20proposed%20approach%20outperforms%20state-of-the-art%20approaches%20in%20MVPS%0Abenchmarks%20across%20F-score%2C%20Chamfer%20distance%2C%20and%20mean%20angular%20error%20metrics.%0ANotably%2C%20it%20significantly%20improves%20the%20detailed%203D%20reconstruction%20of%20areas%20with%0Ahigh%20curvature%20or%20low%20visibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01215v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RNb-NeuS%3A%20Reflectance%20and%20Normal-based%20Multi-View%203D%20Reconstruction&entry.906535625=Baptiste%20Brument%20and%20Robin%20Bruneau%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20%20Jean-Denis%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet&entry.1292438233=%20%20This%20paper%20introduces%20a%20versatile%20paradigm%20for%20integrating%20multi-view%0Areflectance%20%28optional%29%20and%20normal%20maps%20acquired%20through%20photometric%20stereo.%20Our%0Aapproach%20employs%20a%20pixel-wise%20joint%20re-parameterization%20of%20reflectance%20and%0Anormal%2C%20considering%20them%20as%20a%20vector%20of%20radiances%20rendered%20under%20simulated%2C%0Avarying%20illumination.%20This%20re-parameterization%20enables%20the%20seamless%20integration%0Aof%20reflectance%20and%20normal%20maps%20as%20input%20data%20in%20neural%20volume%20rendering-based%0A3D%20reconstruction%20while%20preserving%20a%20single%20optimization%20objective.%20In%0Acontrast%2C%20recent%20multi-view%20photometric%20stereo%20%28MVPS%29%20methods%20depend%20on%0Amultiple%2C%20potentially%20conflicting%20objectives.%20Despite%20its%20apparent%20simplicity%2C%0Aour%20proposed%20approach%20outperforms%20state-of-the-art%20approaches%20in%20MVPS%0Abenchmarks%20across%20F-score%2C%20Chamfer%20distance%2C%20and%20mean%20angular%20error%20metrics.%0ANotably%2C%20it%20significantly%20improves%20the%20detailed%203D%20reconstruction%20of%20areas%20with%0Ahigh%20curvature%20or%20low%20visibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01215v2&entry.124074799=Read"},
{"title": "Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy\n  Tradeoff for Out-of-Distribution Few-shot Learning", "author": "Christopher Liao and Theodoros Tsiligkaridis and Brian Kulis", "abstract": "  Over the past year, a large body of multimodal research has emerged around\nzero-shot evaluation using GPT descriptors. These studies boost the zero-shot\naccuracy of pretrained VL models with an ensemble of label-specific text\ngenerated by GPT. A recent study, WaffleCLIP, demonstrated that similar\nzero-shot accuracy can be achieved with an ensemble of random descriptors.\nHowever, both zero-shot methods are un-trainable and consequently sub-optimal\nwhen some few-shot out-of-distribution (OOD) training data is available.\nInspired by these prior works, we present two more flexible methods called\ndescriptor and word soups, which do not require an LLM at test time and can\nleverage training data to increase OOD target accuracy. Descriptor soup\ngreedily selects a small set of textual descriptors using generic few-shot\ntraining data, then calculates robust class embeddings using the selected\ndescriptors. Word soup greedily assembles a chain of words in a similar manner.\nCompared to existing few-shot soft prompt tuning methods, word soup requires\nfewer parameters by construction and less GPU memory, since it does not require\nbackpropagation. Both soups outperform current published few-shot methods, even\nwhen combined with SoTA zero-shot methods, on cross-dataset and domain\ngeneralization benchmarks. Compared with SoTA prompt and descriptor ensembling\nmethods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy\nwith fewer ensemble members. Please checkout our code:\ngithub.com/Chris210634/word_soups\n", "link": "http://arxiv.org/abs/2311.13612v2", "date": "2024-03-29", "relevancy": 2.6379, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5318}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Descriptor%20and%20Word%20Soups%3A%20Overcoming%20the%20Parameter%20Efficiency%20Accuracy%0A%20%20Tradeoff%20for%20Out-of-Distribution%20Few-shot%20Learning&body=Title%3A%20Descriptor%20and%20Word%20Soups%3A%20Overcoming%20the%20Parameter%20Efficiency%20Accuracy%0A%20%20Tradeoff%20for%20Out-of-Distribution%20Few-shot%20Learning%0AAuthor%3A%20Christopher%20Liao%20and%20Theodoros%20Tsiligkaridis%20and%20Brian%20Kulis%0AAbstract%3A%20%20%20Over%20the%20past%20year%2C%20a%20large%20body%20of%20multimodal%20research%20has%20emerged%20around%0Azero-shot%20evaluation%20using%20GPT%20descriptors.%20These%20studies%20boost%20the%20zero-shot%0Aaccuracy%20of%20pretrained%20VL%20models%20with%20an%20ensemble%20of%20label-specific%20text%0Agenerated%20by%20GPT.%20A%20recent%20study%2C%20WaffleCLIP%2C%20demonstrated%20that%20similar%0Azero-shot%20accuracy%20can%20be%20achieved%20with%20an%20ensemble%20of%20random%20descriptors.%0AHowever%2C%20both%20zero-shot%20methods%20are%20un-trainable%20and%20consequently%20sub-optimal%0Awhen%20some%20few-shot%20out-of-distribution%20%28OOD%29%20training%20data%20is%20available.%0AInspired%20by%20these%20prior%20works%2C%20we%20present%20two%20more%20flexible%20methods%20called%0Adescriptor%20and%20word%20soups%2C%20which%20do%20not%20require%20an%20LLM%20at%20test%20time%20and%20can%0Aleverage%20training%20data%20to%20increase%20OOD%20target%20accuracy.%20Descriptor%20soup%0Agreedily%20selects%20a%20small%20set%20of%20textual%20descriptors%20using%20generic%20few-shot%0Atraining%20data%2C%20then%20calculates%20robust%20class%20embeddings%20using%20the%20selected%0Adescriptors.%20Word%20soup%20greedily%20assembles%20a%20chain%20of%20words%20in%20a%20similar%20manner.%0ACompared%20to%20existing%20few-shot%20soft%20prompt%20tuning%20methods%2C%20word%20soup%20requires%0Afewer%20parameters%20by%20construction%20and%20less%20GPU%20memory%2C%20since%20it%20does%20not%20require%0Abackpropagation.%20Both%20soups%20outperform%20current%20published%20few-shot%20methods%2C%20even%0Awhen%20combined%20with%20SoTA%20zero-shot%20methods%2C%20on%20cross-dataset%20and%20domain%0Ageneralization%20benchmarks.%20Compared%20with%20SoTA%20prompt%20and%20descriptor%20ensembling%0Amethods%2C%20such%20as%20ProDA%20and%20WaffleCLIP%2C%20word%20soup%20achieves%20higher%20OOD%20accuracy%0Awith%20fewer%20ensemble%20members.%20Please%20checkout%20our%20code%3A%0Agithub.com/Chris210634/word_soups%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Descriptor%20and%20Word%20Soups%3A%20Overcoming%20the%20Parameter%20Efficiency%20Accuracy%0A%20%20Tradeoff%20for%20Out-of-Distribution%20Few-shot%20Learning&entry.906535625=Christopher%20Liao%20and%20Theodoros%20Tsiligkaridis%20and%20Brian%20Kulis&entry.1292438233=%20%20Over%20the%20past%20year%2C%20a%20large%20body%20of%20multimodal%20research%20has%20emerged%20around%0Azero-shot%20evaluation%20using%20GPT%20descriptors.%20These%20studies%20boost%20the%20zero-shot%0Aaccuracy%20of%20pretrained%20VL%20models%20with%20an%20ensemble%20of%20label-specific%20text%0Agenerated%20by%20GPT.%20A%20recent%20study%2C%20WaffleCLIP%2C%20demonstrated%20that%20similar%0Azero-shot%20accuracy%20can%20be%20achieved%20with%20an%20ensemble%20of%20random%20descriptors.%0AHowever%2C%20both%20zero-shot%20methods%20are%20un-trainable%20and%20consequently%20sub-optimal%0Awhen%20some%20few-shot%20out-of-distribution%20%28OOD%29%20training%20data%20is%20available.%0AInspired%20by%20these%20prior%20works%2C%20we%20present%20two%20more%20flexible%20methods%20called%0Adescriptor%20and%20word%20soups%2C%20which%20do%20not%20require%20an%20LLM%20at%20test%20time%20and%20can%0Aleverage%20training%20data%20to%20increase%20OOD%20target%20accuracy.%20Descriptor%20soup%0Agreedily%20selects%20a%20small%20set%20of%20textual%20descriptors%20using%20generic%20few-shot%0Atraining%20data%2C%20then%20calculates%20robust%20class%20embeddings%20using%20the%20selected%0Adescriptors.%20Word%20soup%20greedily%20assembles%20a%20chain%20of%20words%20in%20a%20similar%20manner.%0ACompared%20to%20existing%20few-shot%20soft%20prompt%20tuning%20methods%2C%20word%20soup%20requires%0Afewer%20parameters%20by%20construction%20and%20less%20GPU%20memory%2C%20since%20it%20does%20not%20require%0Abackpropagation.%20Both%20soups%20outperform%20current%20published%20few-shot%20methods%2C%20even%0Awhen%20combined%20with%20SoTA%20zero-shot%20methods%2C%20on%20cross-dataset%20and%20domain%0Ageneralization%20benchmarks.%20Compared%20with%20SoTA%20prompt%20and%20descriptor%20ensembling%0Amethods%2C%20such%20as%20ProDA%20and%20WaffleCLIP%2C%20word%20soup%20achieves%20higher%20OOD%20accuracy%0Awith%20fewer%20ensemble%20members.%20Please%20checkout%20our%20code%3A%0Agithub.com/Chris210634/word_soups%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13612v2&entry.124074799=Read"},
{"title": "Cross-modal tumor segmentation using generative blending augmentation\n  and self training", "author": "Guillaume Sall\u00e9 and Pierre-Henri Conze and Julien Bert and Nicolas Boussion and Dimitris Visvikis and Vincent Jaouen", "abstract": "  \\textit{Objectives}: Data scarcity and domain shifts lead to biased training\nsets that do not accurately represent deployment conditions. A related\npractical problem is cross-modal image segmentation, where the objective is to\nsegment unlabelled images using previously labelled datasets from other imaging\nmodalities. \\textit{Methods}: We propose a cross-modal segmentation method\nbased on conventional image synthesis boosted by a new data augmentation\ntechnique called Generative Blending Augmentation (GBA). GBA leverages a SinGAN\nmodel to learn representative generative features from a single training image\nto diversify realistically tumor appearances. This way, we compensate for image\nsynthesis errors, subsequently improving the generalization power of a\ndownstream segmentation model. The proposed augmentation is further combined to\nan iterative self-training procedure leveraging pseudo labels at each pass.\n\\textit{Results}: The proposed solution ranked first for vestibular schwannoma\n(VS) segmentation during the validation and test phases of the MICCAI CrossMoDA\n2022 challenge, with best mean Dice similarity and average symmetric surface\ndistance measures. \\textit{Conclusion and significance}: Local contrast\nalteration of tumor appearances and iterative self-training with pseudo labels\nare likely to lead to performance improvements in a variety of segmentation\ncontexts.\n", "link": "http://arxiv.org/abs/2304.01705v2", "date": "2024-03-29", "relevancy": 2.6076, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4978}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20tumor%20segmentation%20using%20generative%20blending%20augmentation%0A%20%20and%20self%20training&body=Title%3A%20Cross-modal%20tumor%20segmentation%20using%20generative%20blending%20augmentation%0A%20%20and%20self%20training%0AAuthor%3A%20Guillaume%20Sall%C3%A9%20and%20Pierre-Henri%20Conze%20and%20Julien%20Bert%20and%20Nicolas%20Boussion%20and%20Dimitris%20Visvikis%20and%20Vincent%20Jaouen%0AAbstract%3A%20%20%20%5Ctextit%7BObjectives%7D%3A%20Data%20scarcity%20and%20domain%20shifts%20lead%20to%20biased%20training%0Asets%20that%20do%20not%20accurately%20represent%20deployment%20conditions.%20A%20related%0Apractical%20problem%20is%20cross-modal%20image%20segmentation%2C%20where%20the%20objective%20is%20to%0Asegment%20unlabelled%20images%20using%20previously%20labelled%20datasets%20from%20other%20imaging%0Amodalities.%20%5Ctextit%7BMethods%7D%3A%20We%20propose%20a%20cross-modal%20segmentation%20method%0Abased%20on%20conventional%20image%20synthesis%20boosted%20by%20a%20new%20data%20augmentation%0Atechnique%20called%20Generative%20Blending%20Augmentation%20%28GBA%29.%20GBA%20leverages%20a%20SinGAN%0Amodel%20to%20learn%20representative%20generative%20features%20from%20a%20single%20training%20image%0Ato%20diversify%20realistically%20tumor%20appearances.%20This%20way%2C%20we%20compensate%20for%20image%0Asynthesis%20errors%2C%20subsequently%20improving%20the%20generalization%20power%20of%20a%0Adownstream%20segmentation%20model.%20The%20proposed%20augmentation%20is%20further%20combined%20to%0Aan%20iterative%20self-training%20procedure%20leveraging%20pseudo%20labels%20at%20each%20pass.%0A%5Ctextit%7BResults%7D%3A%20The%20proposed%20solution%20ranked%20first%20for%20vestibular%20schwannoma%0A%28VS%29%20segmentation%20during%20the%20validation%20and%20test%20phases%20of%20the%20MICCAI%20CrossMoDA%0A2022%20challenge%2C%20with%20best%20mean%20Dice%20similarity%20and%20average%20symmetric%20surface%0Adistance%20measures.%20%5Ctextit%7BConclusion%20and%20significance%7D%3A%20Local%20contrast%0Aalteration%20of%20tumor%20appearances%20and%20iterative%20self-training%20with%20pseudo%20labels%0Aare%20likely%20to%20lead%20to%20performance%20improvements%20in%20a%20variety%20of%20segmentation%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.01705v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20tumor%20segmentation%20using%20generative%20blending%20augmentation%0A%20%20and%20self%20training&entry.906535625=Guillaume%20Sall%C3%A9%20and%20Pierre-Henri%20Conze%20and%20Julien%20Bert%20and%20Nicolas%20Boussion%20and%20Dimitris%20Visvikis%20and%20Vincent%20Jaouen&entry.1292438233=%20%20%5Ctextit%7BObjectives%7D%3A%20Data%20scarcity%20and%20domain%20shifts%20lead%20to%20biased%20training%0Asets%20that%20do%20not%20accurately%20represent%20deployment%20conditions.%20A%20related%0Apractical%20problem%20is%20cross-modal%20image%20segmentation%2C%20where%20the%20objective%20is%20to%0Asegment%20unlabelled%20images%20using%20previously%20labelled%20datasets%20from%20other%20imaging%0Amodalities.%20%5Ctextit%7BMethods%7D%3A%20We%20propose%20a%20cross-modal%20segmentation%20method%0Abased%20on%20conventional%20image%20synthesis%20boosted%20by%20a%20new%20data%20augmentation%0Atechnique%20called%20Generative%20Blending%20Augmentation%20%28GBA%29.%20GBA%20leverages%20a%20SinGAN%0Amodel%20to%20learn%20representative%20generative%20features%20from%20a%20single%20training%20image%0Ato%20diversify%20realistically%20tumor%20appearances.%20This%20way%2C%20we%20compensate%20for%20image%0Asynthesis%20errors%2C%20subsequently%20improving%20the%20generalization%20power%20of%20a%0Adownstream%20segmentation%20model.%20The%20proposed%20augmentation%20is%20further%20combined%20to%0Aan%20iterative%20self-training%20procedure%20leveraging%20pseudo%20labels%20at%20each%20pass.%0A%5Ctextit%7BResults%7D%3A%20The%20proposed%20solution%20ranked%20first%20for%20vestibular%20schwannoma%0A%28VS%29%20segmentation%20during%20the%20validation%20and%20test%20phases%20of%20the%20MICCAI%20CrossMoDA%0A2022%20challenge%2C%20with%20best%20mean%20Dice%20similarity%20and%20average%20symmetric%20surface%0Adistance%20measures.%20%5Ctextit%7BConclusion%20and%20significance%7D%3A%20Local%20contrast%0Aalteration%20of%20tumor%20appearances%20and%20iterative%20self-training%20with%20pseudo%20labels%0Aare%20likely%20to%20lead%20to%20performance%20improvements%20in%20a%20variety%20of%20segmentation%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.01705v2&entry.124074799=Read"},
{"title": "VGTS: Visually Guided Text Spotting for Novel Categories in Historical\n  Manuscripts", "author": "Wenbo Hu and Hongjian Zhan and Xinchen Ma and Cong Liu and Bing Yin and Yue Lu", "abstract": "  In the field of historical manuscript research, scholars frequently encounter\nnovel symbols in ancient texts, investing considerable effort in their\nidentification and documentation. Although existing object detection methods\nachieve impressive performance on known categories, they struggle to recognize\nnovel symbols without retraining. To address this limitation, we propose a\nVisually Guided Text Spotting (VGTS) approach that accurately spots novel\ncharacters using just one annotated support sample. The core of VGTS is a\nspatial alignment module consisting of a Dual Spatial Attention (DSA) block and\na Geometric Matching (GM) block. The DSA block aims to identify, focus on, and\nlearn discriminative spatial regions in the support and query images, mimicking\nthe human visual spotting process. It first refines the support image by\nanalyzing inter-channel relationships to identify critical areas, and then\nrefines the query image by focusing on informative key points. The GM block, on\nthe other hand, establishes the spatial correspondence between the two images,\nenabling accurate localization of the target character in the query image. To\ntackle the example imbalance problem in low-resource spotting tasks, we develop\na novel torus loss function that enhances the discriminative power of the\nembedding space for distance metric learning. To further validate our approach,\nwe introduce a new dataset featuring ancient Dongba hieroglyphics (DBH)\nassociated with the Naxi minority of China. Extensive experiments on the DBH\ndataset and other public datasets, including EGY, VML-HD, TKH, and NC, show\nthat VGTS consistently surpasses state-of-the-art methods. The proposed\nframework exhibits great potential for application in historical manuscript\ntext spotting, enabling scholars to efficiently identify and document novel\nsymbols with minimal annotation effort.\n", "link": "http://arxiv.org/abs/2304.00746v4", "date": "2024-03-29", "relevancy": 2.604, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5153}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4982}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VGTS%3A%20Visually%20Guided%20Text%20Spotting%20for%20Novel%20Categories%20in%20Historical%0A%20%20Manuscripts&body=Title%3A%20VGTS%3A%20Visually%20Guided%20Text%20Spotting%20for%20Novel%20Categories%20in%20Historical%0A%20%20Manuscripts%0AAuthor%3A%20Wenbo%20Hu%20and%20Hongjian%20Zhan%20and%20Xinchen%20Ma%20and%20Cong%20Liu%20and%20Bing%20Yin%20and%20Yue%20Lu%0AAbstract%3A%20%20%20In%20the%20field%20of%20historical%20manuscript%20research%2C%20scholars%20frequently%20encounter%0Anovel%20symbols%20in%20ancient%20texts%2C%20investing%20considerable%20effort%20in%20their%0Aidentification%20and%20documentation.%20Although%20existing%20object%20detection%20methods%0Aachieve%20impressive%20performance%20on%20known%20categories%2C%20they%20struggle%20to%20recognize%0Anovel%20symbols%20without%20retraining.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0AVisually%20Guided%20Text%20Spotting%20%28VGTS%29%20approach%20that%20accurately%20spots%20novel%0Acharacters%20using%20just%20one%20annotated%20support%20sample.%20The%20core%20of%20VGTS%20is%20a%0Aspatial%20alignment%20module%20consisting%20of%20a%20Dual%20Spatial%20Attention%20%28DSA%29%20block%20and%0Aa%20Geometric%20Matching%20%28GM%29%20block.%20The%20DSA%20block%20aims%20to%20identify%2C%20focus%20on%2C%20and%0Alearn%20discriminative%20spatial%20regions%20in%20the%20support%20and%20query%20images%2C%20mimicking%0Athe%20human%20visual%20spotting%20process.%20It%20first%20refines%20the%20support%20image%20by%0Aanalyzing%20inter-channel%20relationships%20to%20identify%20critical%20areas%2C%20and%20then%0Arefines%20the%20query%20image%20by%20focusing%20on%20informative%20key%20points.%20The%20GM%20block%2C%20on%0Athe%20other%20hand%2C%20establishes%20the%20spatial%20correspondence%20between%20the%20two%20images%2C%0Aenabling%20accurate%20localization%20of%20the%20target%20character%20in%20the%20query%20image.%20To%0Atackle%20the%20example%20imbalance%20problem%20in%20low-resource%20spotting%20tasks%2C%20we%20develop%0Aa%20novel%20torus%20loss%20function%20that%20enhances%20the%20discriminative%20power%20of%20the%0Aembedding%20space%20for%20distance%20metric%20learning.%20To%20further%20validate%20our%20approach%2C%0Awe%20introduce%20a%20new%20dataset%20featuring%20ancient%20Dongba%20hieroglyphics%20%28DBH%29%0Aassociated%20with%20the%20Naxi%20minority%20of%20China.%20Extensive%20experiments%20on%20the%20DBH%0Adataset%20and%20other%20public%20datasets%2C%20including%20EGY%2C%20VML-HD%2C%20TKH%2C%20and%20NC%2C%20show%0Athat%20VGTS%20consistently%20surpasses%20state-of-the-art%20methods.%20The%20proposed%0Aframework%20exhibits%20great%20potential%20for%20application%20in%20historical%20manuscript%0Atext%20spotting%2C%20enabling%20scholars%20to%20efficiently%20identify%20and%20document%20novel%0Asymbols%20with%20minimal%20annotation%20effort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00746v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGTS%3A%20Visually%20Guided%20Text%20Spotting%20for%20Novel%20Categories%20in%20Historical%0A%20%20Manuscripts&entry.906535625=Wenbo%20Hu%20and%20Hongjian%20Zhan%20and%20Xinchen%20Ma%20and%20Cong%20Liu%20and%20Bing%20Yin%20and%20Yue%20Lu&entry.1292438233=%20%20In%20the%20field%20of%20historical%20manuscript%20research%2C%20scholars%20frequently%20encounter%0Anovel%20symbols%20in%20ancient%20texts%2C%20investing%20considerable%20effort%20in%20their%0Aidentification%20and%20documentation.%20Although%20existing%20object%20detection%20methods%0Aachieve%20impressive%20performance%20on%20known%20categories%2C%20they%20struggle%20to%20recognize%0Anovel%20symbols%20without%20retraining.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0AVisually%20Guided%20Text%20Spotting%20%28VGTS%29%20approach%20that%20accurately%20spots%20novel%0Acharacters%20using%20just%20one%20annotated%20support%20sample.%20The%20core%20of%20VGTS%20is%20a%0Aspatial%20alignment%20module%20consisting%20of%20a%20Dual%20Spatial%20Attention%20%28DSA%29%20block%20and%0Aa%20Geometric%20Matching%20%28GM%29%20block.%20The%20DSA%20block%20aims%20to%20identify%2C%20focus%20on%2C%20and%0Alearn%20discriminative%20spatial%20regions%20in%20the%20support%20and%20query%20images%2C%20mimicking%0Athe%20human%20visual%20spotting%20process.%20It%20first%20refines%20the%20support%20image%20by%0Aanalyzing%20inter-channel%20relationships%20to%20identify%20critical%20areas%2C%20and%20then%0Arefines%20the%20query%20image%20by%20focusing%20on%20informative%20key%20points.%20The%20GM%20block%2C%20on%0Athe%20other%20hand%2C%20establishes%20the%20spatial%20correspondence%20between%20the%20two%20images%2C%0Aenabling%20accurate%20localization%20of%20the%20target%20character%20in%20the%20query%20image.%20To%0Atackle%20the%20example%20imbalance%20problem%20in%20low-resource%20spotting%20tasks%2C%20we%20develop%0Aa%20novel%20torus%20loss%20function%20that%20enhances%20the%20discriminative%20power%20of%20the%0Aembedding%20space%20for%20distance%20metric%20learning.%20To%20further%20validate%20our%20approach%2C%0Awe%20introduce%20a%20new%20dataset%20featuring%20ancient%20Dongba%20hieroglyphics%20%28DBH%29%0Aassociated%20with%20the%20Naxi%20minority%20of%20China.%20Extensive%20experiments%20on%20the%20DBH%0Adataset%20and%20other%20public%20datasets%2C%20including%20EGY%2C%20VML-HD%2C%20TKH%2C%20and%20NC%2C%20show%0Athat%20VGTS%20consistently%20surpasses%20state-of-the-art%20methods.%20The%20proposed%0Aframework%20exhibits%20great%20potential%20for%20application%20in%20historical%20manuscript%0Atext%20spotting%2C%20enabling%20scholars%20to%20efficiently%20identify%20and%20document%20novel%0Asymbols%20with%20minimal%20annotation%20effort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00746v4&entry.124074799=Read"},
{"title": "Motion Inversion for Video Customization", "author": "Luozhou Wang and Guibao Shen and Yixun Liang and Xin Tao and Pengfei Wan and Di Zhang and Yijun Li and Yingcong Chen", "abstract": "  In this research, we present a novel approach to motion customization in\nvideo generation, addressing the widespread gap in the thorough exploration of\nmotion representation within video generative models. Recognizing the unique\nchallenges posed by video's spatiotemporal nature, our method introduces Motion\nEmbeddings, a set of explicit, temporally coherent one-dimensional embeddings\nderived from a given video. These embeddings are designed to integrate\nseamlessly with the temporal transformer modules of video diffusion models,\nmodulating self-attention computations across frames without compromising\nspatial integrity. Our approach offers a compact and efficient solution to\nmotion representation and enables complex manipulations of motion\ncharacteristics through vector arithmetic in the embedding space. Furthermore,\nwe identify the Temporal Discrepancy in video generative models, which refers\nto variations in how different motion modules process temporal relationships\nbetween frames. We leverage this understanding to optimize the integration of\nour motion embeddings. Our contributions include the introduction of a tailored\nmotion embedding for customization tasks, insights into the temporal processing\ndifferences in video models, and a demonstration of the practical advantages\nand effectiveness of our method through extensive experiments.\n", "link": "http://arxiv.org/abs/2403.20193v1", "date": "2024-03-29", "relevancy": 2.4994, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.774}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Motion%20Inversion%20for%20Video%20Customization&body=Title%3A%20Motion%20Inversion%20for%20Video%20Customization%0AAuthor%3A%20Luozhou%20Wang%20and%20Guibao%20Shen%20and%20Yixun%20Liang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Yijun%20Li%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20In%20this%20research%2C%20we%20present%20a%20novel%20approach%20to%20motion%20customization%20in%0Avideo%20generation%2C%20addressing%20the%20widespread%20gap%20in%20the%20thorough%20exploration%20of%0Amotion%20representation%20within%20video%20generative%20models.%20Recognizing%20the%20unique%0Achallenges%20posed%20by%20video%27s%20spatiotemporal%20nature%2C%20our%20method%20introduces%20Motion%0AEmbeddings%2C%20a%20set%20of%20explicit%2C%20temporally%20coherent%20one-dimensional%20embeddings%0Aderived%20from%20a%20given%20video.%20These%20embeddings%20are%20designed%20to%20integrate%0Aseamlessly%20with%20the%20temporal%20transformer%20modules%20of%20video%20diffusion%20models%2C%0Amodulating%20self-attention%20computations%20across%20frames%20without%20compromising%0Aspatial%20integrity.%20Our%20approach%20offers%20a%20compact%20and%20efficient%20solution%20to%0Amotion%20representation%20and%20enables%20complex%20manipulations%20of%20motion%0Acharacteristics%20through%20vector%20arithmetic%20in%20the%20embedding%20space.%20Furthermore%2C%0Awe%20identify%20the%20Temporal%20Discrepancy%20in%20video%20generative%20models%2C%20which%20refers%0Ato%20variations%20in%20how%20different%20motion%20modules%20process%20temporal%20relationships%0Abetween%20frames.%20We%20leverage%20this%20understanding%20to%20optimize%20the%20integration%20of%0Aour%20motion%20embeddings.%20Our%20contributions%20include%20the%20introduction%20of%20a%20tailored%0Amotion%20embedding%20for%20customization%20tasks%2C%20insights%20into%20the%20temporal%20processing%0Adifferences%20in%20video%20models%2C%20and%20a%20demonstration%20of%20the%20practical%20advantages%0Aand%20effectiveness%20of%20our%20method%20through%20extensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20193v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Inversion%20for%20Video%20Customization&entry.906535625=Luozhou%20Wang%20and%20Guibao%20Shen%20and%20Yixun%20Liang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Yijun%20Li%20and%20Yingcong%20Chen&entry.1292438233=%20%20In%20this%20research%2C%20we%20present%20a%20novel%20approach%20to%20motion%20customization%20in%0Avideo%20generation%2C%20addressing%20the%20widespread%20gap%20in%20the%20thorough%20exploration%20of%0Amotion%20representation%20within%20video%20generative%20models.%20Recognizing%20the%20unique%0Achallenges%20posed%20by%20video%27s%20spatiotemporal%20nature%2C%20our%20method%20introduces%20Motion%0AEmbeddings%2C%20a%20set%20of%20explicit%2C%20temporally%20coherent%20one-dimensional%20embeddings%0Aderived%20from%20a%20given%20video.%20These%20embeddings%20are%20designed%20to%20integrate%0Aseamlessly%20with%20the%20temporal%20transformer%20modules%20of%20video%20diffusion%20models%2C%0Amodulating%20self-attention%20computations%20across%20frames%20without%20compromising%0Aspatial%20integrity.%20Our%20approach%20offers%20a%20compact%20and%20efficient%20solution%20to%0Amotion%20representation%20and%20enables%20complex%20manipulations%20of%20motion%0Acharacteristics%20through%20vector%20arithmetic%20in%20the%20embedding%20space.%20Furthermore%2C%0Awe%20identify%20the%20Temporal%20Discrepancy%20in%20video%20generative%20models%2C%20which%20refers%0Ato%20variations%20in%20how%20different%20motion%20modules%20process%20temporal%20relationships%0Abetween%20frames.%20We%20leverage%20this%20understanding%20to%20optimize%20the%20integration%20of%0Aour%20motion%20embeddings.%20Our%20contributions%20include%20the%20introduction%20of%20a%20tailored%0Amotion%20embedding%20for%20customization%20tasks%2C%20insights%20into%20the%20temporal%20processing%0Adifferences%20in%20video%20models%2C%20and%20a%20demonstration%20of%20the%20practical%20advantages%0Aand%20effectiveness%20of%20our%20method%20through%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20193v1&entry.124074799=Read"},
{"title": "StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image\n  Translation", "author": "Sidi Wu and Yizi Chen and Samuel Mermet and Lorenz Hurni and Konrad Schindler and Nicolas Gonthier and Loic Landrieu", "abstract": "  Most image-to-image translation models postulate that a unique correspondence\nexists between the semantic classes of the source and target domains. However,\nthis assumption does not always hold in real-world scenarios due to divergent\ndistributions, different class sets, and asymmetrical information\nrepresentation. As conventional GANs attempt to generate images that match the\ndistribution of the target domain, they may hallucinate spurious instances of\nclasses absent from the source domain, thereby diminishing the usefulness and\nreliability of translated images. CycleGAN-based methods are also known to hide\nthe mismatched information in the generated images to bypass cycle consistency\nobjectives, a process known as steganography. In response to the challenge of\nnon-bijective image translation, we introduce StegoGAN, a novel model that\nleverages steganography to prevent spurious features in generated images. Our\napproach enhances the semantic consistency of the translated images without\nrequiring additional postprocessing or supervision. Our experimental\nevaluations demonstrate that StegoGAN outperforms existing GAN-based models\nacross various non-bijective image-to-image translation tasks, both\nqualitatively and quantitatively. Our code and pretrained models are accessible\nat https://github.com/sian-wusidi/StegoGAN.\n", "link": "http://arxiv.org/abs/2403.20142v1", "date": "2024-03-29", "relevancy": 2.4846, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5138}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4895}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4875}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20StegoGAN%3A%20Leveraging%20Steganography%20for%20Non-Bijective%20Image-to-Image%0A%20%20Translation&body=Title%3A%20StegoGAN%3A%20Leveraging%20Steganography%20for%20Non-Bijective%20Image-to-Image%0A%20%20Translation%0AAuthor%3A%20Sidi%20Wu%20and%20Yizi%20Chen%20and%20Samuel%20Mermet%20and%20Lorenz%20Hurni%20and%20Konrad%20Schindler%20and%20Nicolas%20Gonthier%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20Most%20image-to-image%20translation%20models%20postulate%20that%20a%20unique%20correspondence%0Aexists%20between%20the%20semantic%20classes%20of%20the%20source%20and%20target%20domains.%20However%2C%0Athis%20assumption%20does%20not%20always%20hold%20in%20real-world%20scenarios%20due%20to%20divergent%0Adistributions%2C%20different%20class%20sets%2C%20and%20asymmetrical%20information%0Arepresentation.%20As%20conventional%20GANs%20attempt%20to%20generate%20images%20that%20match%20the%0Adistribution%20of%20the%20target%20domain%2C%20they%20may%20hallucinate%20spurious%20instances%20of%0Aclasses%20absent%20from%20the%20source%20domain%2C%20thereby%20diminishing%20the%20usefulness%20and%0Areliability%20of%20translated%20images.%20CycleGAN-based%20methods%20are%20also%20known%20to%20hide%0Athe%20mismatched%20information%20in%20the%20generated%20images%20to%20bypass%20cycle%20consistency%0Aobjectives%2C%20a%20process%20known%20as%20steganography.%20In%20response%20to%20the%20challenge%20of%0Anon-bijective%20image%20translation%2C%20we%20introduce%20StegoGAN%2C%20a%20novel%20model%20that%0Aleverages%20steganography%20to%20prevent%20spurious%20features%20in%20generated%20images.%20Our%0Aapproach%20enhances%20the%20semantic%20consistency%20of%20the%20translated%20images%20without%0Arequiring%20additional%20postprocessing%20or%20supervision.%20Our%20experimental%0Aevaluations%20demonstrate%20that%20StegoGAN%20outperforms%20existing%20GAN-based%20models%0Aacross%20various%20non-bijective%20image-to-image%20translation%20tasks%2C%20both%0Aqualitatively%20and%20quantitatively.%20Our%20code%20and%20pretrained%20models%20are%20accessible%0Aat%20https%3A//github.com/sian-wusidi/StegoGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20142v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StegoGAN%3A%20Leveraging%20Steganography%20for%20Non-Bijective%20Image-to-Image%0A%20%20Translation&entry.906535625=Sidi%20Wu%20and%20Yizi%20Chen%20and%20Samuel%20Mermet%20and%20Lorenz%20Hurni%20and%20Konrad%20Schindler%20and%20Nicolas%20Gonthier%20and%20Loic%20Landrieu&entry.1292438233=%20%20Most%20image-to-image%20translation%20models%20postulate%20that%20a%20unique%20correspondence%0Aexists%20between%20the%20semantic%20classes%20of%20the%20source%20and%20target%20domains.%20However%2C%0Athis%20assumption%20does%20not%20always%20hold%20in%20real-world%20scenarios%20due%20to%20divergent%0Adistributions%2C%20different%20class%20sets%2C%20and%20asymmetrical%20information%0Arepresentation.%20As%20conventional%20GANs%20attempt%20to%20generate%20images%20that%20match%20the%0Adistribution%20of%20the%20target%20domain%2C%20they%20may%20hallucinate%20spurious%20instances%20of%0Aclasses%20absent%20from%20the%20source%20domain%2C%20thereby%20diminishing%20the%20usefulness%20and%0Areliability%20of%20translated%20images.%20CycleGAN-based%20methods%20are%20also%20known%20to%20hide%0Athe%20mismatched%20information%20in%20the%20generated%20images%20to%20bypass%20cycle%20consistency%0Aobjectives%2C%20a%20process%20known%20as%20steganography.%20In%20response%20to%20the%20challenge%20of%0Anon-bijective%20image%20translation%2C%20we%20introduce%20StegoGAN%2C%20a%20novel%20model%20that%0Aleverages%20steganography%20to%20prevent%20spurious%20features%20in%20generated%20images.%20Our%0Aapproach%20enhances%20the%20semantic%20consistency%20of%20the%20translated%20images%20without%0Arequiring%20additional%20postprocessing%20or%20supervision.%20Our%20experimental%0Aevaluations%20demonstrate%20that%20StegoGAN%20outperforms%20existing%20GAN-based%20models%0Aacross%20various%20non-bijective%20image-to-image%20translation%20tasks%2C%20both%0Aqualitatively%20and%20quantitatively.%20Our%20code%20and%20pretrained%20models%20are%20accessible%0Aat%20https%3A//github.com/sian-wusidi/StegoGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20142v1&entry.124074799=Read"},
{"title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation", "author": "Lijun Yu and Jos\u00e9 Lezama and Nitesh B. Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Vighnesh Birodkar and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang", "abstract": "  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n", "link": "http://arxiv.org/abs/2310.05737v3", "date": "2024-03-29", "relevancy": 2.4608, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6272}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6133}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5898}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Beats%20Diffusion%20--%20Tokenizer%20is%20Key%20to%20Visual%20Generation&body=Title%3A%20Language%20Model%20Beats%20Diffusion%20--%20Tokenizer%20is%20Key%20to%20Visual%20Generation%0AAuthor%3A%20Lijun%20Yu%20and%20Jos%C3%A9%20Lezama%20and%20Nitesh%20B.%20Gundavarapu%20and%20Luca%20Versari%20and%20Kihyuk%20Sohn%20and%20David%20Minnen%20and%20Yong%20Cheng%20and%20Vighnesh%20Birodkar%20and%20Agrim%20Gupta%20and%20Xiuye%20Gu%20and%20Alexander%20G.%20Hauptmann%20and%20Boqing%20Gong%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20David%20A.%20Ross%20and%20Lu%20Jiang%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20are%20the%20dominant%20models%20for%20generative%0Atasks%20in%20language%2C%20they%20do%20not%20perform%20as%20well%20as%20diffusion%20models%20on%20image%20and%0Avideo%20generation.%20To%20effectively%20use%20LLMs%20for%20visual%20generation%2C%20one%20crucial%0Acomponent%20is%20the%20visual%20tokenizer%20that%20maps%20pixel-space%20inputs%20to%20discrete%0Atokens%20appropriate%20for%20LLM%20learning.%20In%20this%20paper%2C%20we%20introduce%20MAGVIT-v2%2C%20a%0Avideo%20tokenizer%20designed%20to%20generate%20concise%20and%20expressive%20tokens%20for%20both%0Avideos%20and%20images%20using%20a%20common%20token%20vocabulary.%20Equipped%20with%20this%20new%0Atokenizer%2C%20we%20show%20that%20LLMs%20outperform%20diffusion%20models%20on%20standard%20image%20and%0Avideo%20generation%20benchmarks%20including%20ImageNet%20and%20Kinetics.%20In%20addition%2C%20we%0Ademonstrate%20that%20our%20tokenizer%20surpasses%20the%20previously%20top-performing%20video%0Atokenizer%20on%20two%20more%20tasks%3A%20%281%29%20video%20compression%20comparable%20to%20the%0Anext-generation%20video%20codec%20%28VCC%29%20according%20to%20human%20evaluations%2C%20and%20%282%29%0Alearning%20effective%20representations%20for%20action%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05737v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Beats%20Diffusion%20--%20Tokenizer%20is%20Key%20to%20Visual%20Generation&entry.906535625=Lijun%20Yu%20and%20Jos%C3%A9%20Lezama%20and%20Nitesh%20B.%20Gundavarapu%20and%20Luca%20Versari%20and%20Kihyuk%20Sohn%20and%20David%20Minnen%20and%20Yong%20Cheng%20and%20Vighnesh%20Birodkar%20and%20Agrim%20Gupta%20and%20Xiuye%20Gu%20and%20Alexander%20G.%20Hauptmann%20and%20Boqing%20Gong%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20David%20A.%20Ross%20and%20Lu%20Jiang&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20are%20the%20dominant%20models%20for%20generative%0Atasks%20in%20language%2C%20they%20do%20not%20perform%20as%20well%20as%20diffusion%20models%20on%20image%20and%0Avideo%20generation.%20To%20effectively%20use%20LLMs%20for%20visual%20generation%2C%20one%20crucial%0Acomponent%20is%20the%20visual%20tokenizer%20that%20maps%20pixel-space%20inputs%20to%20discrete%0Atokens%20appropriate%20for%20LLM%20learning.%20In%20this%20paper%2C%20we%20introduce%20MAGVIT-v2%2C%20a%0Avideo%20tokenizer%20designed%20to%20generate%20concise%20and%20expressive%20tokens%20for%20both%0Avideos%20and%20images%20using%20a%20common%20token%20vocabulary.%20Equipped%20with%20this%20new%0Atokenizer%2C%20we%20show%20that%20LLMs%20outperform%20diffusion%20models%20on%20standard%20image%20and%0Avideo%20generation%20benchmarks%20including%20ImageNet%20and%20Kinetics.%20In%20addition%2C%20we%0Ademonstrate%20that%20our%20tokenizer%20surpasses%20the%20previously%20top-performing%20video%0Atokenizer%20on%20two%20more%20tasks%3A%20%281%29%20video%20compression%20comparable%20to%20the%0Anext-generation%20video%20codec%20%28VCC%29%20according%20to%20human%20evaluations%2C%20and%20%282%29%0Alearning%20effective%20representations%20for%20action%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05737v3&entry.124074799=Read"},
{"title": "Training neural networks with structured noise improves classification\n  and generalization", "author": "Marco Benedetti and Enrico Ventura", "abstract": "  The beneficial role of noise-injection in learning is a consolidated concept\nin the field of artificial neural networks, suggesting that even biological\nsystems might take advantage of similar mechanisms to optimize their\nperformance. The training-with-noise algorithm proposed by Gardner and\ncollaborators is an emblematic example of a noise-injection procedure in\nrecurrent networks, which can be used to model biological neural systems. We\nshow how adding structure to noisy training data can substantially improve the\nalgorithm performance, allowing the network to approach perfect retrieval of\nthe memories and wide basins of attraction, even in the scenario of maximal\ninjected noise. We also prove that the so-called Hebbian Unlearning rule\ncoincides with the training-with-noise algorithm when noise is maximal and data\nare stable fixed points of the network dynamics.\n", "link": "http://arxiv.org/abs/2302.13417v5", "date": "2024-03-29", "relevancy": 2.4486, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4865}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4583}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Training%20neural%20networks%20with%20structured%20noise%20improves%20classification%0A%20%20and%20generalization&body=Title%3A%20Training%20neural%20networks%20with%20structured%20noise%20improves%20classification%0A%20%20and%20generalization%0AAuthor%3A%20Marco%20Benedetti%20and%20Enrico%20Ventura%0AAbstract%3A%20%20%20The%20beneficial%20role%20of%20noise-injection%20in%20learning%20is%20a%20consolidated%20concept%0Ain%20the%20field%20of%20artificial%20neural%20networks%2C%20suggesting%20that%20even%20biological%0Asystems%20might%20take%20advantage%20of%20similar%20mechanisms%20to%20optimize%20their%0Aperformance.%20The%20training-with-noise%20algorithm%20proposed%20by%20Gardner%20and%0Acollaborators%20is%20an%20emblematic%20example%20of%20a%20noise-injection%20procedure%20in%0Arecurrent%20networks%2C%20which%20can%20be%20used%20to%20model%20biological%20neural%20systems.%20We%0Ashow%20how%20adding%20structure%20to%20noisy%20training%20data%20can%20substantially%20improve%20the%0Aalgorithm%20performance%2C%20allowing%20the%20network%20to%20approach%20perfect%20retrieval%20of%0Athe%20memories%20and%20wide%20basins%20of%20attraction%2C%20even%20in%20the%20scenario%20of%20maximal%0Ainjected%20noise.%20We%20also%20prove%20that%20the%20so-called%20Hebbian%20Unlearning%20rule%0Acoincides%20with%20the%20training-with-noise%20algorithm%20when%20noise%20is%20maximal%20and%20data%0Aare%20stable%20fixed%20points%20of%20the%20network%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13417v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20neural%20networks%20with%20structured%20noise%20improves%20classification%0A%20%20and%20generalization&entry.906535625=Marco%20Benedetti%20and%20Enrico%20Ventura&entry.1292438233=%20%20The%20beneficial%20role%20of%20noise-injection%20in%20learning%20is%20a%20consolidated%20concept%0Ain%20the%20field%20of%20artificial%20neural%20networks%2C%20suggesting%20that%20even%20biological%0Asystems%20might%20take%20advantage%20of%20similar%20mechanisms%20to%20optimize%20their%0Aperformance.%20The%20training-with-noise%20algorithm%20proposed%20by%20Gardner%20and%0Acollaborators%20is%20an%20emblematic%20example%20of%20a%20noise-injection%20procedure%20in%0Arecurrent%20networks%2C%20which%20can%20be%20used%20to%20model%20biological%20neural%20systems.%20We%0Ashow%20how%20adding%20structure%20to%20noisy%20training%20data%20can%20substantially%20improve%20the%0Aalgorithm%20performance%2C%20allowing%20the%20network%20to%20approach%20perfect%20retrieval%20of%0Athe%20memories%20and%20wide%20basins%20of%20attraction%2C%20even%20in%20the%20scenario%20of%20maximal%0Ainjected%20noise.%20We%20also%20prove%20that%20the%20so-called%20Hebbian%20Unlearning%20rule%0Acoincides%20with%20the%20training-with-noise%20algorithm%20when%20noise%20is%20maximal%20and%20data%0Aare%20stable%20fixed%20points%20of%20the%20network%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13417v5&entry.124074799=Read"},
{"title": "DragVideo: Interactive Drag-style Video Editing", "author": "Yufan Deng and Ruida Wang and Yuhao Zhang and Yu-Wing Tai and Chi-Keung Tang", "abstract": "  Video generation models have shown their superior ability to generate\nphoto-realistic video. However, how to accurately control (or edit) the video\nremains a formidable challenge. The main issues are: 1) how to perform direct\nand accurate user control in editing; 2) how to execute editings like changing\nshape, expression, and layout without unsightly distortion and artifacts to the\nedited content; and 3) how to maintain spatio-temporal consistency of video\nafter editing. To address the above issues, we propose DragVideo, a general\ndrag-style video editing framework. Inspired by DragGAN, DragVideo addresses\nissues 1) and 2) by proposing the drag-style video latent optimization method\nwhich gives desired control by updating noisy video latent according to drag\ninstructions through video-level drag objective function. We amend issue 3) by\nintegrating the video diffusion model with sample-specific LoRA and Mutual\nSelf-Attention in DragVideo to ensure the edited result is spatio-temporally\nconsistent. We also present a series of testing examples for drag-style video\nediting and conduct extensive experiments across a wide array of challenging\nediting tasks, such as motion, skeleton editing, etc, underscoring DragVideo\ncan edit video in an intuitive, faithful to the user's intention manner, with\nnearly unnoticeable distortion and artifacts, while maintaining spatio-temporal\nconsistency. While traditional prompt-based video editing fails to do the\nformer two and directly applying image drag editing fails in the last,\nDragVideo's versatility and generality are emphasized. Github link:\nhttps://github.com/RickySkywalker/DragVideo-Official.\n", "link": "http://arxiv.org/abs/2312.02216v2", "date": "2024-03-29", "relevancy": 2.4218, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6633}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6201}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5676}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DragVideo%3A%20Interactive%20Drag-style%20Video%20Editing&body=Title%3A%20DragVideo%3A%20Interactive%20Drag-style%20Video%20Editing%0AAuthor%3A%20Yufan%20Deng%20and%20Ruida%20Wang%20and%20Yuhao%20Zhang%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20%20%20Video%20generation%20models%20have%20shown%20their%20superior%20ability%20to%20generate%0Aphoto-realistic%20video.%20However%2C%20how%20to%20accurately%20control%20%28or%20edit%29%20the%20video%0Aremains%20a%20formidable%20challenge.%20The%20main%20issues%20are%3A%201%29%20how%20to%20perform%20direct%0Aand%20accurate%20user%20control%20in%20editing%3B%202%29%20how%20to%20execute%20editings%20like%20changing%0Ashape%2C%20expression%2C%20and%20layout%20without%20unsightly%20distortion%20and%20artifacts%20to%20the%0Aedited%20content%3B%20and%203%29%20how%20to%20maintain%20spatio-temporal%20consistency%20of%20video%0Aafter%20editing.%20To%20address%20the%20above%20issues%2C%20we%20propose%20DragVideo%2C%20a%20general%0Adrag-style%20video%20editing%20framework.%20Inspired%20by%20DragGAN%2C%20DragVideo%20addresses%0Aissues%201%29%20and%202%29%20by%20proposing%20the%20drag-style%20video%20latent%20optimization%20method%0Awhich%20gives%20desired%20control%20by%20updating%20noisy%20video%20latent%20according%20to%20drag%0Ainstructions%20through%20video-level%20drag%20objective%20function.%20We%20amend%20issue%203%29%20by%0Aintegrating%20the%20video%20diffusion%20model%20with%20sample-specific%20LoRA%20and%20Mutual%0ASelf-Attention%20in%20DragVideo%20to%20ensure%20the%20edited%20result%20is%20spatio-temporally%0Aconsistent.%20We%20also%20present%20a%20series%20of%20testing%20examples%20for%20drag-style%20video%0Aediting%20and%20conduct%20extensive%20experiments%20across%20a%20wide%20array%20of%20challenging%0Aediting%20tasks%2C%20such%20as%20motion%2C%20skeleton%20editing%2C%20etc%2C%20underscoring%20DragVideo%0Acan%20edit%20video%20in%20an%20intuitive%2C%20faithful%20to%20the%20user%27s%20intention%20manner%2C%20with%0Anearly%20unnoticeable%20distortion%20and%20artifacts%2C%20while%20maintaining%20spatio-temporal%0Aconsistency.%20While%20traditional%20prompt-based%20video%20editing%20fails%20to%20do%20the%0Aformer%20two%20and%20directly%20applying%20image%20drag%20editing%20fails%20in%20the%20last%2C%0ADragVideo%27s%20versatility%20and%20generality%20are%20emphasized.%20Github%20link%3A%0Ahttps%3A//github.com/RickySkywalker/DragVideo-Official.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02216v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragVideo%3A%20Interactive%20Drag-style%20Video%20Editing&entry.906535625=Yufan%20Deng%20and%20Ruida%20Wang%20and%20Yuhao%20Zhang%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=%20%20Video%20generation%20models%20have%20shown%20their%20superior%20ability%20to%20generate%0Aphoto-realistic%20video.%20However%2C%20how%20to%20accurately%20control%20%28or%20edit%29%20the%20video%0Aremains%20a%20formidable%20challenge.%20The%20main%20issues%20are%3A%201%29%20how%20to%20perform%20direct%0Aand%20accurate%20user%20control%20in%20editing%3B%202%29%20how%20to%20execute%20editings%20like%20changing%0Ashape%2C%20expression%2C%20and%20layout%20without%20unsightly%20distortion%20and%20artifacts%20to%20the%0Aedited%20content%3B%20and%203%29%20how%20to%20maintain%20spatio-temporal%20consistency%20of%20video%0Aafter%20editing.%20To%20address%20the%20above%20issues%2C%20we%20propose%20DragVideo%2C%20a%20general%0Adrag-style%20video%20editing%20framework.%20Inspired%20by%20DragGAN%2C%20DragVideo%20addresses%0Aissues%201%29%20and%202%29%20by%20proposing%20the%20drag-style%20video%20latent%20optimization%20method%0Awhich%20gives%20desired%20control%20by%20updating%20noisy%20video%20latent%20according%20to%20drag%0Ainstructions%20through%20video-level%20drag%20objective%20function.%20We%20amend%20issue%203%29%20by%0Aintegrating%20the%20video%20diffusion%20model%20with%20sample-specific%20LoRA%20and%20Mutual%0ASelf-Attention%20in%20DragVideo%20to%20ensure%20the%20edited%20result%20is%20spatio-temporally%0Aconsistent.%20We%20also%20present%20a%20series%20of%20testing%20examples%20for%20drag-style%20video%0Aediting%20and%20conduct%20extensive%20experiments%20across%20a%20wide%20array%20of%20challenging%0Aediting%20tasks%2C%20such%20as%20motion%2C%20skeleton%20editing%2C%20etc%2C%20underscoring%20DragVideo%0Acan%20edit%20video%20in%20an%20intuitive%2C%20faithful%20to%20the%20user%27s%20intention%20manner%2C%20with%0Anearly%20unnoticeable%20distortion%20and%20artifacts%2C%20while%20maintaining%20spatio-temporal%0Aconsistency.%20While%20traditional%20prompt-based%20video%20editing%20fails%20to%20do%20the%0Aformer%20two%20and%20directly%20applying%20image%20drag%20editing%20fails%20in%20the%20last%2C%0ADragVideo%27s%20versatility%20and%20generality%20are%20emphasized.%20Github%20link%3A%0Ahttps%3A//github.com/RickySkywalker/DragVideo-Official.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02216v2&entry.124074799=Read"},
{"title": "GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for\n  One-shot Generalizable Neural Radiance Fields", "author": "Xiao Pan and Zongxin Yang and Shuai Bai and Yi Yang", "abstract": "  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\nwhich targets synthesizing photo-realistic novel views given only one reference\nimage per scene. Previous One-shot Generalizable Neural Radiance Fields\n(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\nyet suffer the blurry issue due to the encoder-only architecture that highly\nrelies on the limited reference image. On the other hand, recent\ndiffusion-based image-to-3d methods show vivid plausible results via distilling\npre-trained 2D diffusion models into a 3D representation, yet require tedious\nper-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a\nGenerative Detail compensation framework via GAN and Diffusion that is both\ninference-time finetuning-free and with vivid plausible details. In detail,\nfollowing a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a\nOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\n(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\ninto the existing OG-NeRF pipeline for primarily relieving the blurry issue\nwith in-distribution priors captured from the training dataset, achieving a\ngood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\nthe fine stage, Diff3DE further leverages the pre-trained image diffusion\nmodels to complement rich out-distribution details while maintaining decent 3D\nconsistency. Extensive experiments on both the synthetic and real-world\ndatasets show that GD$^2$-NeRF noticeably improves the details while without\nper-scene finetuning.\n", "link": "http://arxiv.org/abs/2401.00616v3", "date": "2024-03-29", "relevancy": 2.4043, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6198}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GD%5E2-NeRF%3A%20Generative%20Detail%20Compensation%20via%20GAN%20and%20Diffusion%20for%0A%20%20One-shot%20Generalizable%20Neural%20Radiance%20Fields&body=Title%3A%20GD%5E2-NeRF%3A%20Generative%20Detail%20Compensation%20via%20GAN%20and%20Diffusion%20for%0A%20%20One-shot%20Generalizable%20Neural%20Radiance%20Fields%0AAuthor%3A%20Xiao%20Pan%20and%20Zongxin%20Yang%20and%20Shuai%20Bai%20and%20Yi%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20One-shot%20Novel%20View%20Synthesis%20%28O-NVS%29%20task%0Awhich%20targets%20synthesizing%20photo-realistic%20novel%20views%20given%20only%20one%20reference%0Aimage%20per%20scene.%20Previous%20One-shot%20Generalizable%20Neural%20Radiance%20Fields%0A%28OG-NeRF%29%20methods%20solve%20this%20task%20in%20an%20inference-time%20finetuning-free%20manner%2C%0Ayet%20suffer%20the%20blurry%20issue%20due%20to%20the%20encoder-only%20architecture%20that%20highly%0Arelies%20on%20the%20limited%20reference%20image.%20On%20the%20other%20hand%2C%20recent%0Adiffusion-based%20image-to-3d%20methods%20show%20vivid%20plausible%20results%20via%20distilling%0Apre-trained%202D%20diffusion%20models%20into%20a%203D%20representation%2C%20yet%20require%20tedious%0Aper-scene%20optimization.%20Targeting%20these%20issues%2C%20we%20propose%20the%20GD%24%5E2%24-NeRF%2C%20a%0AGenerative%20Detail%20compensation%20framework%20via%20GAN%20and%20Diffusion%20that%20is%20both%0Ainference-time%20finetuning-free%20and%20with%20vivid%20plausible%20details.%20In%20detail%2C%0Afollowing%20a%20coarse-to-fine%20strategy%2C%20GD%24%5E2%24-NeRF%20is%20mainly%20composed%20of%20a%0AOne-stage%20Parallel%20Pipeline%20%28OPP%29%20and%20a%203D-consistent%20Detail%20Enhancer%0A%28Diff3DE%29.%20At%20the%20coarse%20stage%2C%20OPP%20first%20efficiently%20inserts%20the%20GAN%20model%0Ainto%20the%20existing%20OG-NeRF%20pipeline%20for%20primarily%20relieving%20the%20blurry%20issue%0Awith%20in-distribution%20priors%20captured%20from%20the%20training%20dataset%2C%20achieving%20a%0Agood%20balance%20between%20sharpness%20%28LPIPS%2C%20FID%29%20and%20fidelity%20%28PSNR%2C%20SSIM%29.%20Then%2C%20at%0Athe%20fine%20stage%2C%20Diff3DE%20further%20leverages%20the%20pre-trained%20image%20diffusion%0Amodels%20to%20complement%20rich%20out-distribution%20details%20while%20maintaining%20decent%203D%0Aconsistency.%20Extensive%20experiments%20on%20both%20the%20synthetic%20and%20real-world%0Adatasets%20show%20that%20GD%24%5E2%24-NeRF%20noticeably%20improves%20the%20details%20while%20without%0Aper-scene%20finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00616v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GD%5E2-NeRF%3A%20Generative%20Detail%20Compensation%20via%20GAN%20and%20Diffusion%20for%0A%20%20One-shot%20Generalizable%20Neural%20Radiance%20Fields&entry.906535625=Xiao%20Pan%20and%20Zongxin%20Yang%20and%20Shuai%20Bai%20and%20Yi%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20One-shot%20Novel%20View%20Synthesis%20%28O-NVS%29%20task%0Awhich%20targets%20synthesizing%20photo-realistic%20novel%20views%20given%20only%20one%20reference%0Aimage%20per%20scene.%20Previous%20One-shot%20Generalizable%20Neural%20Radiance%20Fields%0A%28OG-NeRF%29%20methods%20solve%20this%20task%20in%20an%20inference-time%20finetuning-free%20manner%2C%0Ayet%20suffer%20the%20blurry%20issue%20due%20to%20the%20encoder-only%20architecture%20that%20highly%0Arelies%20on%20the%20limited%20reference%20image.%20On%20the%20other%20hand%2C%20recent%0Adiffusion-based%20image-to-3d%20methods%20show%20vivid%20plausible%20results%20via%20distilling%0Apre-trained%202D%20diffusion%20models%20into%20a%203D%20representation%2C%20yet%20require%20tedious%0Aper-scene%20optimization.%20Targeting%20these%20issues%2C%20we%20propose%20the%20GD%24%5E2%24-NeRF%2C%20a%0AGenerative%20Detail%20compensation%20framework%20via%20GAN%20and%20Diffusion%20that%20is%20both%0Ainference-time%20finetuning-free%20and%20with%20vivid%20plausible%20details.%20In%20detail%2C%0Afollowing%20a%20coarse-to-fine%20strategy%2C%20GD%24%5E2%24-NeRF%20is%20mainly%20composed%20of%20a%0AOne-stage%20Parallel%20Pipeline%20%28OPP%29%20and%20a%203D-consistent%20Detail%20Enhancer%0A%28Diff3DE%29.%20At%20the%20coarse%20stage%2C%20OPP%20first%20efficiently%20inserts%20the%20GAN%20model%0Ainto%20the%20existing%20OG-NeRF%20pipeline%20for%20primarily%20relieving%20the%20blurry%20issue%0Awith%20in-distribution%20priors%20captured%20from%20the%20training%20dataset%2C%20achieving%20a%0Agood%20balance%20between%20sharpness%20%28LPIPS%2C%20FID%29%20and%20fidelity%20%28PSNR%2C%20SSIM%29.%20Then%2C%20at%0Athe%20fine%20stage%2C%20Diff3DE%20further%20leverages%20the%20pre-trained%20image%20diffusion%0Amodels%20to%20complement%20rich%20out-distribution%20details%20while%20maintaining%20decent%203D%0Aconsistency.%20Extensive%20experiments%20on%20both%20the%20synthetic%20and%20real-world%0Adatasets%20show%20that%20GD%24%5E2%24-NeRF%20noticeably%20improves%20the%20details%20while%20without%0Aper-scene%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00616v3&entry.124074799=Read"},
{"title": "Self-learning Canonical Space for Multi-view 3D Human Pose Estimation", "author": "Xiaoben Li and Mancheng Meng and Ziyan Wu and Terrence Chen and Fan Yang and Dinggang Shen", "abstract": "  Multi-view 3D human pose estimation is naturally superior to single view one,\nbenefiting from more comprehensive information provided by images of multiple\nviews. The information includes camera poses, 2D/3D human poses, and 3D\ngeometry. However, the accurate annotation of these information is hard to\nobtain, making it challenging to predict accurate 3D human pose from multi-view\nimages. To deal with this issue, we propose a fully self-supervised framework,\nnamed cascaded multi-view aggregating network (CMANet), to construct a\ncanonical parameter space to holistically integrate and exploit multi-view\ninformation. In our framework, the multi-view information is grouped into two\ncategories: 1) intra-view information , 2) inter-view information. Accordingly,\nCMANet consists of two components: intra-view module (IRV) and inter-view\nmodule (IEV). IRV is used for extracting initial camera pose and 3D human pose\nof each view; IEV is to fuse complementary pose information and cross-view 3D\ngeometry for a final 3D human pose. To facilitate the aggregation of the intra-\nand inter-view, we define a canonical parameter space, depicted by per-view\ncamera pose and human pose and shape parameters ($\\theta$ and $\\beta$) of SMPL\nmodel, and propose a two-stage learning procedure. At first stage, IRV learns\nto estimate camera pose and view-dependent 3D human pose supervised by\nconfident output of an off-the-shelf 2D keypoint detector. At second stage, IRV\nis frozen and IEV further refines the camera pose and optimizes the 3D human\npose by implicitly encoding the cross-view complement and 3D geometry\nconstraint, achieved by jointly fitting predicted multi-view 2D keypoints. The\nproposed framework, modules, and learning strategy are demonstrated to be\neffective by comprehensive experiments and CMANet is superior to\nstate-of-the-art methods in extensive quantitative and qualitative analysis.\n", "link": "http://arxiv.org/abs/2403.12440v2", "date": "2024-03-29", "relevancy": 2.4008, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-learning%20Canonical%20Space%20for%20Multi-view%203D%20Human%20Pose%20Estimation&body=Title%3A%20Self-learning%20Canonical%20Space%20for%20Multi-view%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Xiaoben%20Li%20and%20Mancheng%20Meng%20and%20Ziyan%20Wu%20and%20Terrence%20Chen%20and%20Fan%20Yang%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20Multi-view%203D%20human%20pose%20estimation%20is%20naturally%20superior%20to%20single%20view%20one%2C%0Abenefiting%20from%20more%20comprehensive%20information%20provided%20by%20images%20of%20multiple%0Aviews.%20The%20information%20includes%20camera%20poses%2C%202D/3D%20human%20poses%2C%20and%203D%0Ageometry.%20However%2C%20the%20accurate%20annotation%20of%20these%20information%20is%20hard%20to%0Aobtain%2C%20making%20it%20challenging%20to%20predict%20accurate%203D%20human%20pose%20from%20multi-view%0Aimages.%20To%20deal%20with%20this%20issue%2C%20we%20propose%20a%20fully%20self-supervised%20framework%2C%0Anamed%20cascaded%20multi-view%20aggregating%20network%20%28CMANet%29%2C%20to%20construct%20a%0Acanonical%20parameter%20space%20to%20holistically%20integrate%20and%20exploit%20multi-view%0Ainformation.%20In%20our%20framework%2C%20the%20multi-view%20information%20is%20grouped%20into%20two%0Acategories%3A%201%29%20intra-view%20information%20%2C%202%29%20inter-view%20information.%20Accordingly%2C%0ACMANet%20consists%20of%20two%20components%3A%20intra-view%20module%20%28IRV%29%20and%20inter-view%0Amodule%20%28IEV%29.%20IRV%20is%20used%20for%20extracting%20initial%20camera%20pose%20and%203D%20human%20pose%0Aof%20each%20view%3B%20IEV%20is%20to%20fuse%20complementary%20pose%20information%20and%20cross-view%203D%0Ageometry%20for%20a%20final%203D%20human%20pose.%20To%20facilitate%20the%20aggregation%20of%20the%20intra-%0Aand%20inter-view%2C%20we%20define%20a%20canonical%20parameter%20space%2C%20depicted%20by%20per-view%0Acamera%20pose%20and%20human%20pose%20and%20shape%20parameters%20%28%24%5Ctheta%24%20and%20%24%5Cbeta%24%29%20of%20SMPL%0Amodel%2C%20and%20propose%20a%20two-stage%20learning%20procedure.%20At%20first%20stage%2C%20IRV%20learns%0Ato%20estimate%20camera%20pose%20and%20view-dependent%203D%20human%20pose%20supervised%20by%0Aconfident%20output%20of%20an%20off-the-shelf%202D%20keypoint%20detector.%20At%20second%20stage%2C%20IRV%0Ais%20frozen%20and%20IEV%20further%20refines%20the%20camera%20pose%20and%20optimizes%20the%203D%20human%0Apose%20by%20implicitly%20encoding%20the%20cross-view%20complement%20and%203D%20geometry%0Aconstraint%2C%20achieved%20by%20jointly%20fitting%20predicted%20multi-view%202D%20keypoints.%20The%0Aproposed%20framework%2C%20modules%2C%20and%20learning%20strategy%20are%20demonstrated%20to%20be%0Aeffective%20by%20comprehensive%20experiments%20and%20CMANet%20is%20superior%20to%0Astate-of-the-art%20methods%20in%20extensive%20quantitative%20and%20qualitative%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12440v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-learning%20Canonical%20Space%20for%20Multi-view%203D%20Human%20Pose%20Estimation&entry.906535625=Xiaoben%20Li%20and%20Mancheng%20Meng%20and%20Ziyan%20Wu%20and%20Terrence%20Chen%20and%20Fan%20Yang%20and%20Dinggang%20Shen&entry.1292438233=%20%20Multi-view%203D%20human%20pose%20estimation%20is%20naturally%20superior%20to%20single%20view%20one%2C%0Abenefiting%20from%20more%20comprehensive%20information%20provided%20by%20images%20of%20multiple%0Aviews.%20The%20information%20includes%20camera%20poses%2C%202D/3D%20human%20poses%2C%20and%203D%0Ageometry.%20However%2C%20the%20accurate%20annotation%20of%20these%20information%20is%20hard%20to%0Aobtain%2C%20making%20it%20challenging%20to%20predict%20accurate%203D%20human%20pose%20from%20multi-view%0Aimages.%20To%20deal%20with%20this%20issue%2C%20we%20propose%20a%20fully%20self-supervised%20framework%2C%0Anamed%20cascaded%20multi-view%20aggregating%20network%20%28CMANet%29%2C%20to%20construct%20a%0Acanonical%20parameter%20space%20to%20holistically%20integrate%20and%20exploit%20multi-view%0Ainformation.%20In%20our%20framework%2C%20the%20multi-view%20information%20is%20grouped%20into%20two%0Acategories%3A%201%29%20intra-view%20information%20%2C%202%29%20inter-view%20information.%20Accordingly%2C%0ACMANet%20consists%20of%20two%20components%3A%20intra-view%20module%20%28IRV%29%20and%20inter-view%0Amodule%20%28IEV%29.%20IRV%20is%20used%20for%20extracting%20initial%20camera%20pose%20and%203D%20human%20pose%0Aof%20each%20view%3B%20IEV%20is%20to%20fuse%20complementary%20pose%20information%20and%20cross-view%203D%0Ageometry%20for%20a%20final%203D%20human%20pose.%20To%20facilitate%20the%20aggregation%20of%20the%20intra-%0Aand%20inter-view%2C%20we%20define%20a%20canonical%20parameter%20space%2C%20depicted%20by%20per-view%0Acamera%20pose%20and%20human%20pose%20and%20shape%20parameters%20%28%24%5Ctheta%24%20and%20%24%5Cbeta%24%29%20of%20SMPL%0Amodel%2C%20and%20propose%20a%20two-stage%20learning%20procedure.%20At%20first%20stage%2C%20IRV%20learns%0Ato%20estimate%20camera%20pose%20and%20view-dependent%203D%20human%20pose%20supervised%20by%0Aconfident%20output%20of%20an%20off-the-shelf%202D%20keypoint%20detector.%20At%20second%20stage%2C%20IRV%0Ais%20frozen%20and%20IEV%20further%20refines%20the%20camera%20pose%20and%20optimizes%20the%203D%20human%0Apose%20by%20implicitly%20encoding%20the%20cross-view%20complement%20and%203D%20geometry%0Aconstraint%2C%20achieved%20by%20jointly%20fitting%20predicted%20multi-view%202D%20keypoints.%20The%0Aproposed%20framework%2C%20modules%2C%20and%20learning%20strategy%20are%20demonstrated%20to%20be%0Aeffective%20by%20comprehensive%20experiments%20and%20CMANet%20is%20superior%20to%0Astate-of-the-art%20methods%20in%20extensive%20quantitative%20and%20qualitative%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12440v2&entry.124074799=Read"},
{"title": "Benchmarking the Robustness of Temporal Action Detection Models Against\n  Temporal Corruptions", "author": "Runhao Zeng and Xiaoyong Chen and Jiaming Liang and Huisi Wu and Guangzhong Cao and Yong Guo", "abstract": "  Temporal action detection (TAD) aims to locate action positions and recognize\naction categories in long-term untrimmed videos. Although many methods have\nachieved promising results, their robustness has not been thoroughly studied.\nIn practice, we observe that temporal information in videos can be occasionally\ncorrupted, such as missing or blurred frames. Interestingly, existing methods\noften incur a significant performance drop even if only one frame is affected.\nTo formally evaluate the robustness, we establish two temporal corruption\nrobustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper,\nwe extensively analyze the robustness of seven leading TAD methods and obtain\nsome interesting findings: 1) Existing methods are particularly vulnerable to\ntemporal corruptions, and end-to-end methods are often more susceptible than\nthose with a pre-trained feature extractor; 2) Vulnerability mainly comes from\nlocalization error rather than classification error; 3) When corruptions occur\nin the middle of an action instance, TAD models tend to yield the largest\nperformance drop. Besides building a benchmark, we further develop a simple but\neffective robust training method to defend against temporal corruptions,\nthrough the FrameDrop augmentation and Temporal-Robust Consistency loss.\nRemarkably, our approach not only improves robustness but also yields promising\nimprovements on clean data. We believe that this study will serve as a\nbenchmark for future research in robust video analysis. Source code and models\nare available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.\n", "link": "http://arxiv.org/abs/2403.20254v1", "date": "2024-03-29", "relevancy": 2.3541, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6482}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Robustness%20of%20Temporal%20Action%20Detection%20Models%20Against%0A%20%20Temporal%20Corruptions&body=Title%3A%20Benchmarking%20the%20Robustness%20of%20Temporal%20Action%20Detection%20Models%20Against%0A%20%20Temporal%20Corruptions%0AAuthor%3A%20Runhao%20Zeng%20and%20Xiaoyong%20Chen%20and%20Jiaming%20Liang%20and%20Huisi%20Wu%20and%20Guangzhong%20Cao%20and%20Yong%20Guo%0AAbstract%3A%20%20%20Temporal%20action%20detection%20%28TAD%29%20aims%20to%20locate%20action%20positions%20and%20recognize%0Aaction%20categories%20in%20long-term%20untrimmed%20videos.%20Although%20many%20methods%20have%0Aachieved%20promising%20results%2C%20their%20robustness%20has%20not%20been%20thoroughly%20studied.%0AIn%20practice%2C%20we%20observe%20that%20temporal%20information%20in%20videos%20can%20be%20occasionally%0Acorrupted%2C%20such%20as%20missing%20or%20blurred%20frames.%20Interestingly%2C%20existing%20methods%0Aoften%20incur%20a%20significant%20performance%20drop%20even%20if%20only%20one%20frame%20is%20affected.%0ATo%20formally%20evaluate%20the%20robustness%2C%20we%20establish%20two%20temporal%20corruption%0Arobustness%20benchmarks%2C%20namely%20THUMOS14-C%20and%20ActivityNet-v1.3-C.%20In%20this%20paper%2C%0Awe%20extensively%20analyze%20the%20robustness%20of%20seven%20leading%20TAD%20methods%20and%20obtain%0Asome%20interesting%20findings%3A%201%29%20Existing%20methods%20are%20particularly%20vulnerable%20to%0Atemporal%20corruptions%2C%20and%20end-to-end%20methods%20are%20often%20more%20susceptible%20than%0Athose%20with%20a%20pre-trained%20feature%20extractor%3B%202%29%20Vulnerability%20mainly%20comes%20from%0Alocalization%20error%20rather%20than%20classification%20error%3B%203%29%20When%20corruptions%20occur%0Ain%20the%20middle%20of%20an%20action%20instance%2C%20TAD%20models%20tend%20to%20yield%20the%20largest%0Aperformance%20drop.%20Besides%20building%20a%20benchmark%2C%20we%20further%20develop%20a%20simple%20but%0Aeffective%20robust%20training%20method%20to%20defend%20against%20temporal%20corruptions%2C%0Athrough%20the%20FrameDrop%20augmentation%20and%20Temporal-Robust%20Consistency%20loss.%0ARemarkably%2C%20our%20approach%20not%20only%20improves%20robustness%20but%20also%20yields%20promising%0Aimprovements%20on%20clean%20data.%20We%20believe%20that%20this%20study%20will%20serve%20as%20a%0Abenchmark%20for%20future%20research%20in%20robust%20video%20analysis.%20Source%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/Alvin-Zeng/temporal-robustness-benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20254v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Robustness%20of%20Temporal%20Action%20Detection%20Models%20Against%0A%20%20Temporal%20Corruptions&entry.906535625=Runhao%20Zeng%20and%20Xiaoyong%20Chen%20and%20Jiaming%20Liang%20and%20Huisi%20Wu%20and%20Guangzhong%20Cao%20and%20Yong%20Guo&entry.1292438233=%20%20Temporal%20action%20detection%20%28TAD%29%20aims%20to%20locate%20action%20positions%20and%20recognize%0Aaction%20categories%20in%20long-term%20untrimmed%20videos.%20Although%20many%20methods%20have%0Aachieved%20promising%20results%2C%20their%20robustness%20has%20not%20been%20thoroughly%20studied.%0AIn%20practice%2C%20we%20observe%20that%20temporal%20information%20in%20videos%20can%20be%20occasionally%0Acorrupted%2C%20such%20as%20missing%20or%20blurred%20frames.%20Interestingly%2C%20existing%20methods%0Aoften%20incur%20a%20significant%20performance%20drop%20even%20if%20only%20one%20frame%20is%20affected.%0ATo%20formally%20evaluate%20the%20robustness%2C%20we%20establish%20two%20temporal%20corruption%0Arobustness%20benchmarks%2C%20namely%20THUMOS14-C%20and%20ActivityNet-v1.3-C.%20In%20this%20paper%2C%0Awe%20extensively%20analyze%20the%20robustness%20of%20seven%20leading%20TAD%20methods%20and%20obtain%0Asome%20interesting%20findings%3A%201%29%20Existing%20methods%20are%20particularly%20vulnerable%20to%0Atemporal%20corruptions%2C%20and%20end-to-end%20methods%20are%20often%20more%20susceptible%20than%0Athose%20with%20a%20pre-trained%20feature%20extractor%3B%202%29%20Vulnerability%20mainly%20comes%20from%0Alocalization%20error%20rather%20than%20classification%20error%3B%203%29%20When%20corruptions%20occur%0Ain%20the%20middle%20of%20an%20action%20instance%2C%20TAD%20models%20tend%20to%20yield%20the%20largest%0Aperformance%20drop.%20Besides%20building%20a%20benchmark%2C%20we%20further%20develop%20a%20simple%20but%0Aeffective%20robust%20training%20method%20to%20defend%20against%20temporal%20corruptions%2C%0Athrough%20the%20FrameDrop%20augmentation%20and%20Temporal-Robust%20Consistency%20loss.%0ARemarkably%2C%20our%20approach%20not%20only%20improves%20robustness%20but%20also%20yields%20promising%0Aimprovements%20on%20clean%20data.%20We%20believe%20that%20this%20study%20will%20serve%20as%20a%0Abenchmark%20for%20future%20research%20in%20robust%20video%20analysis.%20Source%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/Alvin-Zeng/temporal-robustness-benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20254v1&entry.124074799=Read"},
{"title": "A Learning-based Incentive Mechanism for Mobile AIGC Service in\n  Decentralized Internet of Vehicles", "author": "Jiani Fan and Minrui Xu and Ziyao Liu and Huanyi Ye and Chaojie Gu and Dusit Niyato and Kwok-Yan Lam", "abstract": "  Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of\nautomated content generation utilizing AI models. Mobile AIGC services in the\nInternet of Vehicles (IoV) network have numerous advantages over traditional\ncloud-based AIGC services, including enhanced network efficiency, better\nreconfigurability, and stronger data security and privacy. Nonetheless, AIGC\nservice provisioning frequently demands significant resources. Consequently,\nresource-constrained roadside units (RSUs) face challenges in maintaining a\nheterogeneous pool of AIGC services and addressing all user service requests\nwithout degrading overall performance. Therefore, in this paper, we propose a\ndecentralized incentive mechanism for mobile AIGC service allocation, employing\nmulti-agent deep reinforcement learning to find the balance between the supply\nof AIGC services on RSUs and user demand for services within the IoV context,\noptimizing user experience and minimizing transmission latency. Experimental\nresults demonstrate that our approach achieves superior performance compared to\nother baseline models.\n", "link": "http://arxiv.org/abs/2403.20151v1", "date": "2024-03-29", "relevancy": 2.3452, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4781}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4704}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4586}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Learning-based%20Incentive%20Mechanism%20for%20Mobile%20AIGC%20Service%20in%0A%20%20Decentralized%20Internet%20of%20Vehicles&body=Title%3A%20A%20Learning-based%20Incentive%20Mechanism%20for%20Mobile%20AIGC%20Service%20in%0A%20%20Decentralized%20Internet%20of%20Vehicles%0AAuthor%3A%20Jiani%20Fan%20and%20Minrui%20Xu%20and%20Ziyao%20Liu%20and%20Huanyi%20Ye%20and%20Chaojie%20Gu%20and%20Dusit%20Niyato%20and%20Kwok-Yan%20Lam%0AAbstract%3A%20%20%20Artificial%20Intelligence-Generated%20Content%20%28AIGC%29%20refers%20to%20the%20paradigm%20of%0Aautomated%20content%20generation%20utilizing%20AI%20models.%20Mobile%20AIGC%20services%20in%20the%0AInternet%20of%20Vehicles%20%28IoV%29%20network%20have%20numerous%20advantages%20over%20traditional%0Acloud-based%20AIGC%20services%2C%20including%20enhanced%20network%20efficiency%2C%20better%0Areconfigurability%2C%20and%20stronger%20data%20security%20and%20privacy.%20Nonetheless%2C%20AIGC%0Aservice%20provisioning%20frequently%20demands%20significant%20resources.%20Consequently%2C%0Aresource-constrained%20roadside%20units%20%28RSUs%29%20face%20challenges%20in%20maintaining%20a%0Aheterogeneous%20pool%20of%20AIGC%20services%20and%20addressing%20all%20user%20service%20requests%0Awithout%20degrading%20overall%20performance.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%0Adecentralized%20incentive%20mechanism%20for%20mobile%20AIGC%20service%20allocation%2C%20employing%0Amulti-agent%20deep%20reinforcement%20learning%20to%20find%20the%20balance%20between%20the%20supply%0Aof%20AIGC%20services%20on%20RSUs%20and%20user%20demand%20for%20services%20within%20the%20IoV%20context%2C%0Aoptimizing%20user%20experience%20and%20minimizing%20transmission%20latency.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20achieves%20superior%20performance%20compared%20to%0Aother%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20151v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Learning-based%20Incentive%20Mechanism%20for%20Mobile%20AIGC%20Service%20in%0A%20%20Decentralized%20Internet%20of%20Vehicles&entry.906535625=Jiani%20Fan%20and%20Minrui%20Xu%20and%20Ziyao%20Liu%20and%20Huanyi%20Ye%20and%20Chaojie%20Gu%20and%20Dusit%20Niyato%20and%20Kwok-Yan%20Lam&entry.1292438233=%20%20Artificial%20Intelligence-Generated%20Content%20%28AIGC%29%20refers%20to%20the%20paradigm%20of%0Aautomated%20content%20generation%20utilizing%20AI%20models.%20Mobile%20AIGC%20services%20in%20the%0AInternet%20of%20Vehicles%20%28IoV%29%20network%20have%20numerous%20advantages%20over%20traditional%0Acloud-based%20AIGC%20services%2C%20including%20enhanced%20network%20efficiency%2C%20better%0Areconfigurability%2C%20and%20stronger%20data%20security%20and%20privacy.%20Nonetheless%2C%20AIGC%0Aservice%20provisioning%20frequently%20demands%20significant%20resources.%20Consequently%2C%0Aresource-constrained%20roadside%20units%20%28RSUs%29%20face%20challenges%20in%20maintaining%20a%0Aheterogeneous%20pool%20of%20AIGC%20services%20and%20addressing%20all%20user%20service%20requests%0Awithout%20degrading%20overall%20performance.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20a%0Adecentralized%20incentive%20mechanism%20for%20mobile%20AIGC%20service%20allocation%2C%20employing%0Amulti-agent%20deep%20reinforcement%20learning%20to%20find%20the%20balance%20between%20the%20supply%0Aof%20AIGC%20services%20on%20RSUs%20and%20user%20demand%20for%20services%20within%20the%20IoV%20context%2C%0Aoptimizing%20user%20experience%20and%20minimizing%20transmission%20latency.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20achieves%20superior%20performance%20compared%20to%0Aother%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20151v1&entry.124074799=Read"},
{"title": "Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and\n  Hierarchy-Aware Domain Disentanglement", "author": "Yoonhyuk Choi", "abstract": "  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2403.20298v1", "date": "2024-03-29", "relevancy": 2.3286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4758}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Review-Based%20Cross-Domain%20Recommendation%20via%20Hyperbolic%20Embedding%20and%0A%20%20Hierarchy-Aware%20Domain%20Disentanglement&body=Title%3A%20Review-Based%20Cross-Domain%20Recommendation%20via%20Hyperbolic%20Embedding%20and%0A%20%20Hierarchy-Aware%20Domain%20Disentanglement%0AAuthor%3A%20Yoonhyuk%20Choi%0AAbstract%3A%20%20%20The%20issue%20of%20data%20sparsity%20poses%20a%20significant%20challenge%20to%20recommender%0Asystems.%20In%20response%20to%20this%2C%20algorithms%20that%20leverage%20side%20information%20such%20as%0Areview%20texts%20have%20been%20proposed.%20Furthermore%2C%20Cross-Domain%20Recommendation%0A%28CDR%29%2C%20which%20captures%20domain-shareable%20knowledge%20and%20transfers%20it%20from%20a%20richer%0Adomain%20%28source%29%20to%20a%20sparser%20one%20%28target%29%2C%20has%20received%20notable%20attention.%0ANevertheless%2C%20the%20majority%20of%20existing%20methodologies%20assume%20a%20Euclidean%0Aembedding%20space%2C%20encountering%20difficulties%20in%20accurately%20representing%20richer%0Atext%20information%20and%20managing%20complex%20interactions%20between%20users%20and%20items.%0AThis%20paper%20advocates%20a%20hyperbolic%20CDR%20approach%20based%20on%20review%20texts%20for%0Amodeling%20user-item%20relationships.%20We%20first%20emphasize%20that%20conventional%0Adistance-based%20domain%20alignment%20techniques%20may%20cause%20problems%20because%20small%0Amodifications%20in%20hyperbolic%20geometry%20result%20in%20magnified%20perturbations%2C%0Aultimately%20leading%20to%20the%20collapse%20of%20hierarchical%20structures.%20To%20address%20this%0Achallenge%2C%20we%20propose%20hierarchy-aware%20embedding%20and%20domain%20alignment%20schemes%0Athat%20adjust%20the%20scale%20to%20extract%20domain-shareable%20information%20without%0Adisrupting%20structural%20forms.%20The%20process%20involves%20the%20initial%20embedding%20of%0Areview%20texts%20in%20hyperbolic%20space%2C%20followed%20by%20feature%20extraction%20incorporating%0Adegree-based%20normalization%20and%20structure%20alignment.%20We%20conducted%20extensive%0Aexperiments%20to%20substantiate%20the%20efficiency%2C%20robustness%2C%20and%20scalability%20of%20our%0Aproposed%20model%20in%20comparison%20to%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20298v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Review-Based%20Cross-Domain%20Recommendation%20via%20Hyperbolic%20Embedding%20and%0A%20%20Hierarchy-Aware%20Domain%20Disentanglement&entry.906535625=Yoonhyuk%20Choi&entry.1292438233=%20%20The%20issue%20of%20data%20sparsity%20poses%20a%20significant%20challenge%20to%20recommender%0Asystems.%20In%20response%20to%20this%2C%20algorithms%20that%20leverage%20side%20information%20such%20as%0Areview%20texts%20have%20been%20proposed.%20Furthermore%2C%20Cross-Domain%20Recommendation%0A%28CDR%29%2C%20which%20captures%20domain-shareable%20knowledge%20and%20transfers%20it%20from%20a%20richer%0Adomain%20%28source%29%20to%20a%20sparser%20one%20%28target%29%2C%20has%20received%20notable%20attention.%0ANevertheless%2C%20the%20majority%20of%20existing%20methodologies%20assume%20a%20Euclidean%0Aembedding%20space%2C%20encountering%20difficulties%20in%20accurately%20representing%20richer%0Atext%20information%20and%20managing%20complex%20interactions%20between%20users%20and%20items.%0AThis%20paper%20advocates%20a%20hyperbolic%20CDR%20approach%20based%20on%20review%20texts%20for%0Amodeling%20user-item%20relationships.%20We%20first%20emphasize%20that%20conventional%0Adistance-based%20domain%20alignment%20techniques%20may%20cause%20problems%20because%20small%0Amodifications%20in%20hyperbolic%20geometry%20result%20in%20magnified%20perturbations%2C%0Aultimately%20leading%20to%20the%20collapse%20of%20hierarchical%20structures.%20To%20address%20this%0Achallenge%2C%20we%20propose%20hierarchy-aware%20embedding%20and%20domain%20alignment%20schemes%0Athat%20adjust%20the%20scale%20to%20extract%20domain-shareable%20information%20without%0Adisrupting%20structural%20forms.%20The%20process%20involves%20the%20initial%20embedding%20of%0Areview%20texts%20in%20hyperbolic%20space%2C%20followed%20by%20feature%20extraction%20incorporating%0Adegree-based%20normalization%20and%20structure%20alignment.%20We%20conducted%20extensive%0Aexperiments%20to%20substantiate%20the%20efficiency%2C%20robustness%2C%20and%20scalability%20of%20our%0Aproposed%20model%20in%20comparison%20to%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20298v1&entry.124074799=Read"},
{"title": "Video Super-Resolution Transformer with Masked Inter&Intra-Frame\n  Attention", "author": "Xingyu Zhou and Leheng Zhang and Xiaorui Zhao and Keze Wang and Leida Li and Shuhang Gu", "abstract": "  Recently, Vision Transformer has achieved great success in recovering missing\ndetails in low-resolution sequences, i.e., the video super-resolution (VSR)\ntask. Despite its superiority in VSR accuracy, the heavy computational burden\nas well as the large memory footprint hinder the deployment of\nTransformer-based VSR models on constrained devices. In this paper, we address\nthe above issue by proposing a novel feature-level masked processing framework:\nVSR with Masked Intra and inter frame Attention (MIA-VSR). The core of MIA-VSR\nis leveraging feature-level temporal continuity between adjacent frames to\nreduce redundant computations and make more rational use of previously enhanced\nSR features. Concretely, we propose an intra-frame and inter-frame attention\nblock which takes the respective roles of past features and input features into\nconsideration and only exploits previously enhanced features to provide\nsupplementary information. In addition, an adaptive block-wise mask prediction\nmodule is developed to skip unimportant computations according to feature\nsimilarity between adjacent frames. We conduct detailed ablation studies to\nvalidate our contributions and compare the proposed method with recent\nstate-of-the-art VSR approaches. The experimental results demonstrate that\nMIA-VSR improves the memory and computation efficiency over state-of-the-art\nmethods, without trading off PSNR accuracy. The code is available at\nhttps://github.com/LabShuHangGU/MIA-VSR.\n", "link": "http://arxiv.org/abs/2401.06312v4", "date": "2024-03-29", "relevancy": 2.3104, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5871}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5751}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5602}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Video%20Super-Resolution%20Transformer%20with%20Masked%20Inter%26Intra-Frame%0A%20%20Attention&body=Title%3A%20Video%20Super-Resolution%20Transformer%20with%20Masked%20Inter%26Intra-Frame%0A%20%20Attention%0AAuthor%3A%20Xingyu%20Zhou%20and%20Leheng%20Zhang%20and%20Xiaorui%20Zhao%20and%20Keze%20Wang%20and%20Leida%20Li%20and%20Shuhang%20Gu%0AAbstract%3A%20%20%20Recently%2C%20Vision%20Transformer%20has%20achieved%20great%20success%20in%20recovering%20missing%0Adetails%20in%20low-resolution%20sequences%2C%20i.e.%2C%20the%20video%20super-resolution%20%28VSR%29%0Atask.%20Despite%20its%20superiority%20in%20VSR%20accuracy%2C%20the%20heavy%20computational%20burden%0Aas%20well%20as%20the%20large%20memory%20footprint%20hinder%20the%20deployment%20of%0ATransformer-based%20VSR%20models%20on%20constrained%20devices.%20In%20this%20paper%2C%20we%20address%0Athe%20above%20issue%20by%20proposing%20a%20novel%20feature-level%20masked%20processing%20framework%3A%0AVSR%20with%20Masked%20Intra%20and%20inter%20frame%20Attention%20%28MIA-VSR%29.%20The%20core%20of%20MIA-VSR%0Ais%20leveraging%20feature-level%20temporal%20continuity%20between%20adjacent%20frames%20to%0Areduce%20redundant%20computations%20and%20make%20more%20rational%20use%20of%20previously%20enhanced%0ASR%20features.%20Concretely%2C%20we%20propose%20an%20intra-frame%20and%20inter-frame%20attention%0Ablock%20which%20takes%20the%20respective%20roles%20of%20past%20features%20and%20input%20features%20into%0Aconsideration%20and%20only%20exploits%20previously%20enhanced%20features%20to%20provide%0Asupplementary%20information.%20In%20addition%2C%20an%20adaptive%20block-wise%20mask%20prediction%0Amodule%20is%20developed%20to%20skip%20unimportant%20computations%20according%20to%20feature%0Asimilarity%20between%20adjacent%20frames.%20We%20conduct%20detailed%20ablation%20studies%20to%0Avalidate%20our%20contributions%20and%20compare%20the%20proposed%20method%20with%20recent%0Astate-of-the-art%20VSR%20approaches.%20The%20experimental%20results%20demonstrate%20that%0AMIA-VSR%20improves%20the%20memory%20and%20computation%20efficiency%20over%20state-of-the-art%0Amethods%2C%20without%20trading%20off%20PSNR%20accuracy.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LabShuHangGU/MIA-VSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06312v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Super-Resolution%20Transformer%20with%20Masked%20Inter%26Intra-Frame%0A%20%20Attention&entry.906535625=Xingyu%20Zhou%20and%20Leheng%20Zhang%20and%20Xiaorui%20Zhao%20and%20Keze%20Wang%20and%20Leida%20Li%20and%20Shuhang%20Gu&entry.1292438233=%20%20Recently%2C%20Vision%20Transformer%20has%20achieved%20great%20success%20in%20recovering%20missing%0Adetails%20in%20low-resolution%20sequences%2C%20i.e.%2C%20the%20video%20super-resolution%20%28VSR%29%0Atask.%20Despite%20its%20superiority%20in%20VSR%20accuracy%2C%20the%20heavy%20computational%20burden%0Aas%20well%20as%20the%20large%20memory%20footprint%20hinder%20the%20deployment%20of%0ATransformer-based%20VSR%20models%20on%20constrained%20devices.%20In%20this%20paper%2C%20we%20address%0Athe%20above%20issue%20by%20proposing%20a%20novel%20feature-level%20masked%20processing%20framework%3A%0AVSR%20with%20Masked%20Intra%20and%20inter%20frame%20Attention%20%28MIA-VSR%29.%20The%20core%20of%20MIA-VSR%0Ais%20leveraging%20feature-level%20temporal%20continuity%20between%20adjacent%20frames%20to%0Areduce%20redundant%20computations%20and%20make%20more%20rational%20use%20of%20previously%20enhanced%0ASR%20features.%20Concretely%2C%20we%20propose%20an%20intra-frame%20and%20inter-frame%20attention%0Ablock%20which%20takes%20the%20respective%20roles%20of%20past%20features%20and%20input%20features%20into%0Aconsideration%20and%20only%20exploits%20previously%20enhanced%20features%20to%20provide%0Asupplementary%20information.%20In%20addition%2C%20an%20adaptive%20block-wise%20mask%20prediction%0Amodule%20is%20developed%20to%20skip%20unimportant%20computations%20according%20to%20feature%0Asimilarity%20between%20adjacent%20frames.%20We%20conduct%20detailed%20ablation%20studies%20to%0Avalidate%20our%20contributions%20and%20compare%20the%20proposed%20method%20with%20recent%0Astate-of-the-art%20VSR%20approaches.%20The%20experimental%20results%20demonstrate%20that%0AMIA-VSR%20improves%20the%20memory%20and%20computation%20efficiency%20over%20state-of-the-art%0Amethods%2C%20without%20trading%20off%20PSNR%20accuracy.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LabShuHangGU/MIA-VSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06312v4&entry.124074799=Read"},
{"title": "Single-Model and Any-Modality for Video Object Tracking", "author": "Zongwei Wu and Jilai Zheng and Xiangxuan Ren and Florin-Alexandru Vasluianu and Chao Ma and Danda Pani Paudel and Luc Van Gool and Radu Timofte", "abstract": "  In the realm of video object tracking, auxiliary modalities such as depth,\nthermal, or event data have emerged as valuable assets to complement the RGB\ntrackers. In practice, most existing RGB trackers learn a single set of\nparameters to use them across datasets and applications. However, a similar\nsingle-model unification for multi-modality tracking presents several\nchallenges. These challenges stem from the inherent heterogeneity of inputs --\neach with modality-specific representations, the scarcity of multi-modal\ndatasets, and the absence of all the modalities at all times. In this work, we\nintroduce Un-Track, a Unified Tracker of a single set of parameters for any\nmodality. To handle any modality, our method learns their common latent space\nthrough low-rank factorization and reconstruction techniques. More importantly,\nwe use only the RGB-X pairs to learn the common latent space. This unique\nshared representation seamlessly binds all modalities together, enabling\neffective unification and accommodating any missing modality, all within a\nsingle transformer-based architecture. Our Un-Track achieves +8.1 absolute\nF-score gain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50)\nGFLOPs with +6.6M (over 93M) parameters, through a simple yet efficient\nprompting strategy. Extensive comparisons on five benchmark datasets with\ndifferent modalities show that Un-Track surpasses both SOTA unified trackers\nand modality-specific counterparts, validating our effectiveness and\npracticality. The source code is publicly available at\nhttps://github.com/Zongwei97/UnTrack.\n", "link": "http://arxiv.org/abs/2311.15851v3", "date": "2024-03-29", "relevancy": 2.2835, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Single-Model%20and%20Any-Modality%20for%20Video%20Object%20Tracking&body=Title%3A%20Single-Model%20and%20Any-Modality%20for%20Video%20Object%20Tracking%0AAuthor%3A%20Zongwei%20Wu%20and%20Jilai%20Zheng%20and%20Xiangxuan%20Ren%20and%20Florin-Alexandru%20Vasluianu%20and%20Chao%20Ma%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20In%20the%20realm%20of%20video%20object%20tracking%2C%20auxiliary%20modalities%20such%20as%20depth%2C%0Athermal%2C%20or%20event%20data%20have%20emerged%20as%20valuable%20assets%20to%20complement%20the%20RGB%0Atrackers.%20In%20practice%2C%20most%20existing%20RGB%20trackers%20learn%20a%20single%20set%20of%0Aparameters%20to%20use%20them%20across%20datasets%20and%20applications.%20However%2C%20a%20similar%0Asingle-model%20unification%20for%20multi-modality%20tracking%20presents%20several%0Achallenges.%20These%20challenges%20stem%20from%20the%20inherent%20heterogeneity%20of%20inputs%20--%0Aeach%20with%20modality-specific%20representations%2C%20the%20scarcity%20of%20multi-modal%0Adatasets%2C%20and%20the%20absence%20of%20all%20the%20modalities%20at%20all%20times.%20In%20this%20work%2C%20we%0Aintroduce%20Un-Track%2C%20a%20Unified%20Tracker%20of%20a%20single%20set%20of%20parameters%20for%20any%0Amodality.%20To%20handle%20any%20modality%2C%20our%20method%20learns%20their%20common%20latent%20space%0Athrough%20low-rank%20factorization%20and%20reconstruction%20techniques.%20More%20importantly%2C%0Awe%20use%20only%20the%20RGB-X%20pairs%20to%20learn%20the%20common%20latent%20space.%20This%20unique%0Ashared%20representation%20seamlessly%20binds%20all%20modalities%20together%2C%20enabling%0Aeffective%20unification%20and%20accommodating%20any%20missing%20modality%2C%20all%20within%20a%0Asingle%20transformer-based%20architecture.%20Our%20Un-Track%20achieves%20%2B8.1%20absolute%0AF-score%20gain%2C%20on%20the%20DepthTrack%20dataset%2C%20by%20introducing%20only%20%2B2.14%20%28over%2021.50%29%0AGFLOPs%20with%20%2B6.6M%20%28over%2093M%29%20parameters%2C%20through%20a%20simple%20yet%20efficient%0Aprompting%20strategy.%20Extensive%20comparisons%20on%20five%20benchmark%20datasets%20with%0Adifferent%20modalities%20show%20that%20Un-Track%20surpasses%20both%20SOTA%20unified%20trackers%0Aand%20modality-specific%20counterparts%2C%20validating%20our%20effectiveness%20and%0Apracticality.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Zongwei97/UnTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15851v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Model%20and%20Any-Modality%20for%20Video%20Object%20Tracking&entry.906535625=Zongwei%20Wu%20and%20Jilai%20Zheng%20and%20Xiangxuan%20Ren%20and%20Florin-Alexandru%20Vasluianu%20and%20Chao%20Ma%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Radu%20Timofte&entry.1292438233=%20%20In%20the%20realm%20of%20video%20object%20tracking%2C%20auxiliary%20modalities%20such%20as%20depth%2C%0Athermal%2C%20or%20event%20data%20have%20emerged%20as%20valuable%20assets%20to%20complement%20the%20RGB%0Atrackers.%20In%20practice%2C%20most%20existing%20RGB%20trackers%20learn%20a%20single%20set%20of%0Aparameters%20to%20use%20them%20across%20datasets%20and%20applications.%20However%2C%20a%20similar%0Asingle-model%20unification%20for%20multi-modality%20tracking%20presents%20several%0Achallenges.%20These%20challenges%20stem%20from%20the%20inherent%20heterogeneity%20of%20inputs%20--%0Aeach%20with%20modality-specific%20representations%2C%20the%20scarcity%20of%20multi-modal%0Adatasets%2C%20and%20the%20absence%20of%20all%20the%20modalities%20at%20all%20times.%20In%20this%20work%2C%20we%0Aintroduce%20Un-Track%2C%20a%20Unified%20Tracker%20of%20a%20single%20set%20of%20parameters%20for%20any%0Amodality.%20To%20handle%20any%20modality%2C%20our%20method%20learns%20their%20common%20latent%20space%0Athrough%20low-rank%20factorization%20and%20reconstruction%20techniques.%20More%20importantly%2C%0Awe%20use%20only%20the%20RGB-X%20pairs%20to%20learn%20the%20common%20latent%20space.%20This%20unique%0Ashared%20representation%20seamlessly%20binds%20all%20modalities%20together%2C%20enabling%0Aeffective%20unification%20and%20accommodating%20any%20missing%20modality%2C%20all%20within%20a%0Asingle%20transformer-based%20architecture.%20Our%20Un-Track%20achieves%20%2B8.1%20absolute%0AF-score%20gain%2C%20on%20the%20DepthTrack%20dataset%2C%20by%20introducing%20only%20%2B2.14%20%28over%2021.50%29%0AGFLOPs%20with%20%2B6.6M%20%28over%2093M%29%20parameters%2C%20through%20a%20simple%20yet%20efficient%0Aprompting%20strategy.%20Extensive%20comparisons%20on%20five%20benchmark%20datasets%20with%0Adifferent%20modalities%20show%20that%20Un-Track%20surpasses%20both%20SOTA%20unified%20trackers%0Aand%20modality-specific%20counterparts%2C%20validating%20our%20effectiveness%20and%0Apracticality.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Zongwei97/UnTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15851v3&entry.124074799=Read"},
{"title": "Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image\n  Translation", "author": "Chuan Huang and Jia Wei and Rui Li", "abstract": "  Multi-modal brain images from MRI scans are widely used in clinical diagnosis\nto provide complementary information from different modalities. However,\nobtaining fully paired multi-modal images in practice is challenging due to\nvarious factors, such as time, cost, and artifacts, resulting in\nmodality-missing brain images. To address this problem, unsupervised\nmulti-modal brain image translation has been extensively studied. Existing\nmethods suffer from the problem of brain tumor deformation during translation,\nas they fail to focus on the tumor areas when translating the whole images. In\nthis paper, we propose an unsupervised tumor-aware distillation teacher-student\nnetwork called UTAD-Net, which is capable of perceiving and translating tumor\nareas precisely. Specifically, our model consists of two parts: a teacher\nnetwork and a student network. The teacher network learns an end-to-end mapping\nfrom source to target modality using unpaired images and corresponding tumor\nmasks first. Then, the translation knowledge is distilled into the student\nnetwork, enabling it to generate more realistic tumor areas and whole images\nwithout masks. Experiments show that our model achieves competitive performance\non both quantitative and qualitative evaluations of image quality compared with\nstate-of-the-art methods. Furthermore, we demonstrate the effectiveness of the\ngenerated images on downstream segmentation tasks. Our code is available at\nhttps://github.com/scut-HC/UTAD-Net.\n", "link": "http://arxiv.org/abs/2403.20168v1", "date": "2024-03-29", "relevancy": 2.2787, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5482}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5389}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Tumor-Aware%20Distillation%20for%20Multi-Modal%20Brain%20Image%0A%20%20Translation&body=Title%3A%20Unsupervised%20Tumor-Aware%20Distillation%20for%20Multi-Modal%20Brain%20Image%0A%20%20Translation%0AAuthor%3A%20Chuan%20Huang%20and%20Jia%20Wei%20and%20Rui%20Li%0AAbstract%3A%20%20%20Multi-modal%20brain%20images%20from%20MRI%20scans%20are%20widely%20used%20in%20clinical%20diagnosis%0Ato%20provide%20complementary%20information%20from%20different%20modalities.%20However%2C%0Aobtaining%20fully%20paired%20multi-modal%20images%20in%20practice%20is%20challenging%20due%20to%0Avarious%20factors%2C%20such%20as%20time%2C%20cost%2C%20and%20artifacts%2C%20resulting%20in%0Amodality-missing%20brain%20images.%20To%20address%20this%20problem%2C%20unsupervised%0Amulti-modal%20brain%20image%20translation%20has%20been%20extensively%20studied.%20Existing%0Amethods%20suffer%20from%20the%20problem%20of%20brain%20tumor%20deformation%20during%20translation%2C%0Aas%20they%20fail%20to%20focus%20on%20the%20tumor%20areas%20when%20translating%20the%20whole%20images.%20In%0Athis%20paper%2C%20we%20propose%20an%20unsupervised%20tumor-aware%20distillation%20teacher-student%0Anetwork%20called%20UTAD-Net%2C%20which%20is%20capable%20of%20perceiving%20and%20translating%20tumor%0Aareas%20precisely.%20Specifically%2C%20our%20model%20consists%20of%20two%20parts%3A%20a%20teacher%0Anetwork%20and%20a%20student%20network.%20The%20teacher%20network%20learns%20an%20end-to-end%20mapping%0Afrom%20source%20to%20target%20modality%20using%20unpaired%20images%20and%20corresponding%20tumor%0Amasks%20first.%20Then%2C%20the%20translation%20knowledge%20is%20distilled%20into%20the%20student%0Anetwork%2C%20enabling%20it%20to%20generate%20more%20realistic%20tumor%20areas%20and%20whole%20images%0Awithout%20masks.%20Experiments%20show%20that%20our%20model%20achieves%20competitive%20performance%0Aon%20both%20quantitative%20and%20qualitative%20evaluations%20of%20image%20quality%20compared%20with%0Astate-of-the-art%20methods.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Agenerated%20images%20on%20downstream%20segmentation%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/scut-HC/UTAD-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20168v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Tumor-Aware%20Distillation%20for%20Multi-Modal%20Brain%20Image%0A%20%20Translation&entry.906535625=Chuan%20Huang%20and%20Jia%20Wei%20and%20Rui%20Li&entry.1292438233=%20%20Multi-modal%20brain%20images%20from%20MRI%20scans%20are%20widely%20used%20in%20clinical%20diagnosis%0Ato%20provide%20complementary%20information%20from%20different%20modalities.%20However%2C%0Aobtaining%20fully%20paired%20multi-modal%20images%20in%20practice%20is%20challenging%20due%20to%0Avarious%20factors%2C%20such%20as%20time%2C%20cost%2C%20and%20artifacts%2C%20resulting%20in%0Amodality-missing%20brain%20images.%20To%20address%20this%20problem%2C%20unsupervised%0Amulti-modal%20brain%20image%20translation%20has%20been%20extensively%20studied.%20Existing%0Amethods%20suffer%20from%20the%20problem%20of%20brain%20tumor%20deformation%20during%20translation%2C%0Aas%20they%20fail%20to%20focus%20on%20the%20tumor%20areas%20when%20translating%20the%20whole%20images.%20In%0Athis%20paper%2C%20we%20propose%20an%20unsupervised%20tumor-aware%20distillation%20teacher-student%0Anetwork%20called%20UTAD-Net%2C%20which%20is%20capable%20of%20perceiving%20and%20translating%20tumor%0Aareas%20precisely.%20Specifically%2C%20our%20model%20consists%20of%20two%20parts%3A%20a%20teacher%0Anetwork%20and%20a%20student%20network.%20The%20teacher%20network%20learns%20an%20end-to-end%20mapping%0Afrom%20source%20to%20target%20modality%20using%20unpaired%20images%20and%20corresponding%20tumor%0Amasks%20first.%20Then%2C%20the%20translation%20knowledge%20is%20distilled%20into%20the%20student%0Anetwork%2C%20enabling%20it%20to%20generate%20more%20realistic%20tumor%20areas%20and%20whole%20images%0Awithout%20masks.%20Experiments%20show%20that%20our%20model%20achieves%20competitive%20performance%0Aon%20both%20quantitative%20and%20qualitative%20evaluations%20of%20image%20quality%20compared%20with%0Astate-of-the-art%20methods.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Agenerated%20images%20on%20downstream%20segmentation%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/scut-HC/UTAD-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20168v1&entry.124074799=Read"},
{"title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models", "author": "Jinhyuk Lee and Zhuyun Dai and Xiaoqi Ren and Blair Chen and Daniel Cer and Jeremy R. Cole and Kai Hui and Michael Boratko and Rajvi Kapadia and Wen Ding and Yi Luan and Sai Meher Karthik Duddu and Gustavo Hernandez Abrego and Weiqiang Shi and Nithi Gupta and Aditya Kusupati and Prateek Jain and Siddhartha Reddy Jonnalagadda and Ming-Wei Chang and Iftekhar Naim", "abstract": "  We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.\n", "link": "http://arxiv.org/abs/2403.20327v1", "date": "2024-03-29", "relevancy": 2.2746, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4649}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gecko%3A%20Versatile%20Text%20Embeddings%20Distilled%20from%20Large%20Language%20Models&body=Title%3A%20Gecko%3A%20Versatile%20Text%20Embeddings%20Distilled%20from%20Large%20Language%20Models%0AAuthor%3A%20Jinhyuk%20Lee%20and%20Zhuyun%20Dai%20and%20Xiaoqi%20Ren%20and%20Blair%20Chen%20and%20Daniel%20Cer%20and%20Jeremy%20R.%20Cole%20and%20Kai%20Hui%20and%20Michael%20Boratko%20and%20Rajvi%20Kapadia%20and%20Wen%20Ding%20and%20Yi%20Luan%20and%20Sai%20Meher%20Karthik%20Duddu%20and%20Gustavo%20Hernandez%20Abrego%20and%20Weiqiang%20Shi%20and%20Nithi%20Gupta%20and%20Aditya%20Kusupati%20and%20Prateek%20Jain%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Ming-Wei%20Chang%20and%20Iftekhar%20Naim%0AAbstract%3A%20%20%20We%20present%20Gecko%2C%20a%20compact%20and%20versatile%20text%20embedding%20model.%20Gecko%0Aachieves%20strong%20retrieval%20performance%20by%20leveraging%20a%20key%20idea%3A%20distilling%0Aknowledge%20from%20large%20language%20models%20%28LLMs%29%20into%20a%20retriever.%20Our%20two-step%0Adistillation%20process%20begins%20with%20generating%20diverse%2C%20synthetic%20paired%20data%0Ausing%20an%20LLM.%20Next%2C%20we%20further%20refine%20the%20data%20quality%20by%20retrieving%20a%20set%20of%0Acandidate%20passages%20for%20each%20query%2C%20and%20relabeling%20the%20positive%20and%20hard%0Anegative%20passages%20using%20the%20same%20LLM.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20compactness%20of%20the%20Gecko.%20On%20the%20Massive%20Text%20Embedding%0ABenchmark%20%28MTEB%29%2C%20Gecko%20with%20256%20embedding%20dimensions%20outperforms%20all%20existing%0Aentries%20with%20768%20embedding%20size.%20Gecko%20with%20768%20embedding%20dimensions%20achieves%0Aan%20average%20score%20of%2066.31%2C%20competing%20with%207x%20larger%20models%20and%205x%20higher%0Adimensional%20embeddings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20327v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gecko%3A%20Versatile%20Text%20Embeddings%20Distilled%20from%20Large%20Language%20Models&entry.906535625=Jinhyuk%20Lee%20and%20Zhuyun%20Dai%20and%20Xiaoqi%20Ren%20and%20Blair%20Chen%20and%20Daniel%20Cer%20and%20Jeremy%20R.%20Cole%20and%20Kai%20Hui%20and%20Michael%20Boratko%20and%20Rajvi%20Kapadia%20and%20Wen%20Ding%20and%20Yi%20Luan%20and%20Sai%20Meher%20Karthik%20Duddu%20and%20Gustavo%20Hernandez%20Abrego%20and%20Weiqiang%20Shi%20and%20Nithi%20Gupta%20and%20Aditya%20Kusupati%20and%20Prateek%20Jain%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Ming-Wei%20Chang%20and%20Iftekhar%20Naim&entry.1292438233=%20%20We%20present%20Gecko%2C%20a%20compact%20and%20versatile%20text%20embedding%20model.%20Gecko%0Aachieves%20strong%20retrieval%20performance%20by%20leveraging%20a%20key%20idea%3A%20distilling%0Aknowledge%20from%20large%20language%20models%20%28LLMs%29%20into%20a%20retriever.%20Our%20two-step%0Adistillation%20process%20begins%20with%20generating%20diverse%2C%20synthetic%20paired%20data%0Ausing%20an%20LLM.%20Next%2C%20we%20further%20refine%20the%20data%20quality%20by%20retrieving%20a%20set%20of%0Acandidate%20passages%20for%20each%20query%2C%20and%20relabeling%20the%20positive%20and%20hard%0Anegative%20passages%20using%20the%20same%20LLM.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20compactness%20of%20the%20Gecko.%20On%20the%20Massive%20Text%20Embedding%0ABenchmark%20%28MTEB%29%2C%20Gecko%20with%20256%20embedding%20dimensions%20outperforms%20all%20existing%0Aentries%20with%20768%20embedding%20size.%20Gecko%20with%20768%20embedding%20dimensions%20achieves%0Aan%20average%20score%20of%2066.31%2C%20competing%20with%207x%20larger%20models%20and%205x%20higher%0Adimensional%20embeddings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20327v1&entry.124074799=Read"},
{"title": "On Size and Hardness Generalization in Unsupervised Learning for the\n  Travelling Salesman Problem", "author": "Yimeng Min and Carla P. Gomes", "abstract": "  We study the generalization capability of Unsupervised Learning in solving\nthe Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN)\ntrained with a surrogate loss function to generate an embedding for each node.\nWe use these embeddings to construct a heat map that indicates the likelihood\nof each edge being part of the optimal route. We then apply local search to\ngenerate our final predictions. Our investigation explores how different\ntraining instance sizes, embedding dimensions, and distributions influence the\noutcomes of Unsupervised Learning methods. Our results show that training with\nlarger instance sizes and increasing embedding dimensions can build a more\neffective representation, enhancing the model's ability to solve TSP.\nFurthermore, in evaluating generalization across different distributions, we\nfirst determine the hardness of various distributions and explore how different\nhardnesses affect the final results. Our findings suggest that models trained\non harder instances exhibit better generalization capabilities, highlighting\nthe importance of selecting appropriate training instances in solving TSP using\nUnsupervised Learning.\n", "link": "http://arxiv.org/abs/2403.20212v1", "date": "2024-03-29", "relevancy": 2.273, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4925}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4375}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Size%20and%20Hardness%20Generalization%20in%20Unsupervised%20Learning%20for%20the%0A%20%20Travelling%20Salesman%20Problem&body=Title%3A%20On%20Size%20and%20Hardness%20Generalization%20in%20Unsupervised%20Learning%20for%20the%0A%20%20Travelling%20Salesman%20Problem%0AAuthor%3A%20Yimeng%20Min%20and%20Carla%20P.%20Gomes%0AAbstract%3A%20%20%20We%20study%20the%20generalization%20capability%20of%20Unsupervised%20Learning%20in%20solving%0Athe%20Travelling%20Salesman%20Problem%20%28TSP%29.%20We%20use%20a%20Graph%20Neural%20Network%20%28GNN%29%0Atrained%20with%20a%20surrogate%20loss%20function%20to%20generate%20an%20embedding%20for%20each%20node.%0AWe%20use%20these%20embeddings%20to%20construct%20a%20heat%20map%20that%20indicates%20the%20likelihood%0Aof%20each%20edge%20being%20part%20of%20the%20optimal%20route.%20We%20then%20apply%20local%20search%20to%0Agenerate%20our%20final%20predictions.%20Our%20investigation%20explores%20how%20different%0Atraining%20instance%20sizes%2C%20embedding%20dimensions%2C%20and%20distributions%20influence%20the%0Aoutcomes%20of%20Unsupervised%20Learning%20methods.%20Our%20results%20show%20that%20training%20with%0Alarger%20instance%20sizes%20and%20increasing%20embedding%20dimensions%20can%20build%20a%20more%0Aeffective%20representation%2C%20enhancing%20the%20model%27s%20ability%20to%20solve%20TSP.%0AFurthermore%2C%20in%20evaluating%20generalization%20across%20different%20distributions%2C%20we%0Afirst%20determine%20the%20hardness%20of%20various%20distributions%20and%20explore%20how%20different%0Ahardnesses%20affect%20the%20final%20results.%20Our%20findings%20suggest%20that%20models%20trained%0Aon%20harder%20instances%20exhibit%20better%20generalization%20capabilities%2C%20highlighting%0Athe%20importance%20of%20selecting%20appropriate%20training%20instances%20in%20solving%20TSP%20using%0AUnsupervised%20Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20212v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Size%20and%20Hardness%20Generalization%20in%20Unsupervised%20Learning%20for%20the%0A%20%20Travelling%20Salesman%20Problem&entry.906535625=Yimeng%20Min%20and%20Carla%20P.%20Gomes&entry.1292438233=%20%20We%20study%20the%20generalization%20capability%20of%20Unsupervised%20Learning%20in%20solving%0Athe%20Travelling%20Salesman%20Problem%20%28TSP%29.%20We%20use%20a%20Graph%20Neural%20Network%20%28GNN%29%0Atrained%20with%20a%20surrogate%20loss%20function%20to%20generate%20an%20embedding%20for%20each%20node.%0AWe%20use%20these%20embeddings%20to%20construct%20a%20heat%20map%20that%20indicates%20the%20likelihood%0Aof%20each%20edge%20being%20part%20of%20the%20optimal%20route.%20We%20then%20apply%20local%20search%20to%0Agenerate%20our%20final%20predictions.%20Our%20investigation%20explores%20how%20different%0Atraining%20instance%20sizes%2C%20embedding%20dimensions%2C%20and%20distributions%20influence%20the%0Aoutcomes%20of%20Unsupervised%20Learning%20methods.%20Our%20results%20show%20that%20training%20with%0Alarger%20instance%20sizes%20and%20increasing%20embedding%20dimensions%20can%20build%20a%20more%0Aeffective%20representation%2C%20enhancing%20the%20model%27s%20ability%20to%20solve%20TSP.%0AFurthermore%2C%20in%20evaluating%20generalization%20across%20different%20distributions%2C%20we%0Afirst%20determine%20the%20hardness%20of%20various%20distributions%20and%20explore%20how%20different%0Ahardnesses%20affect%20the%20final%20results.%20Our%20findings%20suggest%20that%20models%20trained%0Aon%20harder%20instances%20exhibit%20better%20generalization%20capabilities%2C%20highlighting%0Athe%20importance%20of%20selecting%20appropriate%20training%20instances%20in%20solving%20TSP%20using%0AUnsupervised%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20212v1&entry.124074799=Read"},
{"title": "VicTR: Video-conditioned Text Representations for Activity Recognition", "author": "Kumara Kahatapitiya and Anurag Arnab and Arsha Nagrani and Michael S. Ryoo", "abstract": "  Vision-Language models (VLMs) have excelled in the image-domain -- especially\nin zero-shot settings -- thanks to the availability of vast pretraining data\n(i.e., paired image-text samples). However for videos, such paired data is not\nas abundant. Therefore, video-VLMs are usually designed by adapting pretrained\nimage-VLMs to the video-domain, instead of training from scratch. All such\nrecipes rely on augmenting visual embeddings with temporal information (i.e.,\nimage $\\rightarrow$ video), often keeping text embeddings unchanged or even\nbeing discarded. In this paper, we argue the contrary, that better video-VLMs\ncan be designed by focusing more on augmenting text, rather than visual\ninformation. More specifically, we introduce Video-conditioned Text\nRepresentations (VicTR): a form of text embeddings optimized w.r.t. visual\nembeddings, creating a more-flexible contrastive latent space. Our model can\nfurther make use of freely-available semantic information, in the form of\nvisually-grounded auxiliary text (e.g. object or scene information). We\nevaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form\n(Kinetics-400) and long-form (Charades) activity recognition benchmarks,\nshowing strong performance among video-VLMs.\n", "link": "http://arxiv.org/abs/2304.02560v2", "date": "2024-03-29", "relevancy": 2.2634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5705}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5552}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VicTR%3A%20Video-conditioned%20Text%20Representations%20for%20Activity%20Recognition&body=Title%3A%20VicTR%3A%20Video-conditioned%20Text%20Representations%20for%20Activity%20Recognition%0AAuthor%3A%20Kumara%20Kahatapitiya%20and%20Anurag%20Arnab%20and%20Arsha%20Nagrani%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Vision-Language%20models%20%28VLMs%29%20have%20excelled%20in%20the%20image-domain%20--%20especially%0Ain%20zero-shot%20settings%20--%20thanks%20to%20the%20availability%20of%20vast%20pretraining%20data%0A%28i.e.%2C%20paired%20image-text%20samples%29.%20However%20for%20videos%2C%20such%20paired%20data%20is%20not%0Aas%20abundant.%20Therefore%2C%20video-VLMs%20are%20usually%20designed%20by%20adapting%20pretrained%0Aimage-VLMs%20to%20the%20video-domain%2C%20instead%20of%20training%20from%20scratch.%20All%20such%0Arecipes%20rely%20on%20augmenting%20visual%20embeddings%20with%20temporal%20information%20%28i.e.%2C%0Aimage%20%24%5Crightarrow%24%20video%29%2C%20often%20keeping%20text%20embeddings%20unchanged%20or%20even%0Abeing%20discarded.%20In%20this%20paper%2C%20we%20argue%20the%20contrary%2C%20that%20better%20video-VLMs%0Acan%20be%20designed%20by%20focusing%20more%20on%20augmenting%20text%2C%20rather%20than%20visual%0Ainformation.%20More%20specifically%2C%20we%20introduce%20Video-conditioned%20Text%0ARepresentations%20%28VicTR%29%3A%20a%20form%20of%20text%20embeddings%20optimized%20w.r.t.%20visual%0Aembeddings%2C%20creating%20a%20more-flexible%20contrastive%20latent%20space.%20Our%20model%20can%0Afurther%20make%20use%20of%20freely-available%20semantic%20information%2C%20in%20the%20form%20of%0Avisually-grounded%20auxiliary%20text%20%28e.g.%20object%20or%20scene%20information%29.%20We%0Aevaluate%20our%20model%20on%20few-shot%2C%20zero-shot%20%28HMDB-51%2C%20UCF-101%29%2C%20short-form%0A%28Kinetics-400%29%20and%20long-form%20%28Charades%29%20activity%20recognition%20benchmarks%2C%0Ashowing%20strong%20performance%20among%20video-VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02560v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VicTR%3A%20Video-conditioned%20Text%20Representations%20for%20Activity%20Recognition&entry.906535625=Kumara%20Kahatapitiya%20and%20Anurag%20Arnab%20and%20Arsha%20Nagrani%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Vision-Language%20models%20%28VLMs%29%20have%20excelled%20in%20the%20image-domain%20--%20especially%0Ain%20zero-shot%20settings%20--%20thanks%20to%20the%20availability%20of%20vast%20pretraining%20data%0A%28i.e.%2C%20paired%20image-text%20samples%29.%20However%20for%20videos%2C%20such%20paired%20data%20is%20not%0Aas%20abundant.%20Therefore%2C%20video-VLMs%20are%20usually%20designed%20by%20adapting%20pretrained%0Aimage-VLMs%20to%20the%20video-domain%2C%20instead%20of%20training%20from%20scratch.%20All%20such%0Arecipes%20rely%20on%20augmenting%20visual%20embeddings%20with%20temporal%20information%20%28i.e.%2C%0Aimage%20%24%5Crightarrow%24%20video%29%2C%20often%20keeping%20text%20embeddings%20unchanged%20or%20even%0Abeing%20discarded.%20In%20this%20paper%2C%20we%20argue%20the%20contrary%2C%20that%20better%20video-VLMs%0Acan%20be%20designed%20by%20focusing%20more%20on%20augmenting%20text%2C%20rather%20than%20visual%0Ainformation.%20More%20specifically%2C%20we%20introduce%20Video-conditioned%20Text%0ARepresentations%20%28VicTR%29%3A%20a%20form%20of%20text%20embeddings%20optimized%20w.r.t.%20visual%0Aembeddings%2C%20creating%20a%20more-flexible%20contrastive%20latent%20space.%20Our%20model%20can%0Afurther%20make%20use%20of%20freely-available%20semantic%20information%2C%20in%20the%20form%20of%0Avisually-grounded%20auxiliary%20text%20%28e.g.%20object%20or%20scene%20information%29.%20We%0Aevaluate%20our%20model%20on%20few-shot%2C%20zero-shot%20%28HMDB-51%2C%20UCF-101%29%2C%20short-form%0A%28Kinetics-400%29%20and%20long-form%20%28Charades%29%20activity%20recognition%20benchmarks%2C%0Ashowing%20strong%20performance%20among%20video-VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02560v2&entry.124074799=Read"},
{"title": "Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D\n  Generative Prior", "author": "Jaehoon Ko and Kyusun Cho and Joungbin Lee and Heeji Yoon and Sangmin Lee and Sangjun Ahn and Seungryong Kim", "abstract": "  Recent methods for audio-driven talking head synthesis often optimize neural\nradiance fields (NeRF) on a monocular talking portrait video, leveraging its\ncapability to render high-fidelity and 3D-consistent novel-view frames.\nHowever, they often struggle to reconstruct complete face geometry due to the\nabsence of comprehensive 3D information in the input monocular videos. In this\npaper, we introduce a novel audio-driven talking head synthesis framework,\ncalled Talk3D, that can faithfully reconstruct its plausible facial geometries\nby effectively adopting the pre-trained 3D-aware generative prior. Given the\npersonalized 3D generative model, we present a novel audio-guided attention\nU-Net architecture that predicts the dynamic face variations in the NeRF space\ndriven by audio. Furthermore, our model is further modulated by audio-unrelated\nconditioning tokens which effectively disentangle variations unrelated to audio\nfeatures. Compared to existing methods, our method excels in generating\nrealistic facial geometries even under extreme head poses. We also conduct\nextensive experiments showing our approach surpasses state-of-the-art\nbenchmarks in terms of both quantitative and qualitative evaluations.\n", "link": "http://arxiv.org/abs/2403.20153v1", "date": "2024-03-29", "relevancy": 2.2446, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5666}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.564}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5403}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Talk3D%3A%20High-Fidelity%20Talking%20Portrait%20Synthesis%20via%20Personalized%203D%0A%20%20Generative%20Prior&body=Title%3A%20Talk3D%3A%20High-Fidelity%20Talking%20Portrait%20Synthesis%20via%20Personalized%203D%0A%20%20Generative%20Prior%0AAuthor%3A%20Jaehoon%20Ko%20and%20Kyusun%20Cho%20and%20Joungbin%20Lee%20and%20Heeji%20Yoon%20and%20Sangmin%20Lee%20and%20Sangjun%20Ahn%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Recent%20methods%20for%20audio-driven%20talking%20head%20synthesis%20often%20optimize%20neural%0Aradiance%20fields%20%28NeRF%29%20on%20a%20monocular%20talking%20portrait%20video%2C%20leveraging%20its%0Acapability%20to%20render%20high-fidelity%20and%203D-consistent%20novel-view%20frames.%0AHowever%2C%20they%20often%20struggle%20to%20reconstruct%20complete%20face%20geometry%20due%20to%20the%0Aabsence%20of%20comprehensive%203D%20information%20in%20the%20input%20monocular%20videos.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20audio-driven%20talking%20head%20synthesis%20framework%2C%0Acalled%20Talk3D%2C%20that%20can%20faithfully%20reconstruct%20its%20plausible%20facial%20geometries%0Aby%20effectively%20adopting%20the%20pre-trained%203D-aware%20generative%20prior.%20Given%20the%0Apersonalized%203D%20generative%20model%2C%20we%20present%20a%20novel%20audio-guided%20attention%0AU-Net%20architecture%20that%20predicts%20the%20dynamic%20face%20variations%20in%20the%20NeRF%20space%0Adriven%20by%20audio.%20Furthermore%2C%20our%20model%20is%20further%20modulated%20by%20audio-unrelated%0Aconditioning%20tokens%20which%20effectively%20disentangle%20variations%20unrelated%20to%20audio%0Afeatures.%20Compared%20to%20existing%20methods%2C%20our%20method%20excels%20in%20generating%0Arealistic%20facial%20geometries%20even%20under%20extreme%20head%20poses.%20We%20also%20conduct%0Aextensive%20experiments%20showing%20our%20approach%20surpasses%20state-of-the-art%0Abenchmarks%20in%20terms%20of%20both%20quantitative%20and%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20153v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk3D%3A%20High-Fidelity%20Talking%20Portrait%20Synthesis%20via%20Personalized%203D%0A%20%20Generative%20Prior&entry.906535625=Jaehoon%20Ko%20and%20Kyusun%20Cho%20and%20Joungbin%20Lee%20and%20Heeji%20Yoon%20and%20Sangmin%20Lee%20and%20Sangjun%20Ahn%20and%20Seungryong%20Kim&entry.1292438233=%20%20Recent%20methods%20for%20audio-driven%20talking%20head%20synthesis%20often%20optimize%20neural%0Aradiance%20fields%20%28NeRF%29%20on%20a%20monocular%20talking%20portrait%20video%2C%20leveraging%20its%0Acapability%20to%20render%20high-fidelity%20and%203D-consistent%20novel-view%20frames.%0AHowever%2C%20they%20often%20struggle%20to%20reconstruct%20complete%20face%20geometry%20due%20to%20the%0Aabsence%20of%20comprehensive%203D%20information%20in%20the%20input%20monocular%20videos.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20audio-driven%20talking%20head%20synthesis%20framework%2C%0Acalled%20Talk3D%2C%20that%20can%20faithfully%20reconstruct%20its%20plausible%20facial%20geometries%0Aby%20effectively%20adopting%20the%20pre-trained%203D-aware%20generative%20prior.%20Given%20the%0Apersonalized%203D%20generative%20model%2C%20we%20present%20a%20novel%20audio-guided%20attention%0AU-Net%20architecture%20that%20predicts%20the%20dynamic%20face%20variations%20in%20the%20NeRF%20space%0Adriven%20by%20audio.%20Furthermore%2C%20our%20model%20is%20further%20modulated%20by%20audio-unrelated%0Aconditioning%20tokens%20which%20effectively%20disentangle%20variations%20unrelated%20to%20audio%0Afeatures.%20Compared%20to%20existing%20methods%2C%20our%20method%20excels%20in%20generating%0Arealistic%20facial%20geometries%20even%20under%20extreme%20head%20poses.%20We%20also%20conduct%0Aextensive%20experiments%20showing%20our%20approach%20surpasses%20state-of-the-art%0Abenchmarks%20in%20terms%20of%20both%20quantitative%20and%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20153v1&entry.124074799=Read"},
{"title": "Enhancing Lithological Mapping with Spatially Constrained Bayesian\n  Network (SCB-Net): An Approach for Field Data-Constrained Predictions with\n  Uncertainty Evaluation", "author": "Victor Silva dos Santos and Erwan Gloaguen and Shiva Tirdad", "abstract": "  Geological maps are an extremely valuable source of information for the Earth\nsciences. They provide insights into mineral exploration, vulnerability to\nnatural hazards, and many other applications. These maps are created using\nnumerical or conceptual models that use geological observations to extrapolate\ndata. Geostatistical techniques have traditionally been used to generate\nreliable predictions that take into account the spatial patterns inherent in\nthe data. However, as the number of auxiliary variables increases, these\nmethods become more labor-intensive. Additionally, traditional machine learning\nmethods often struggle with spatially correlated data and extracting valuable\nnon-linear information from geoscientific datasets. To address these\nlimitations, a new architecture called the Spatially Constrained Bayesian\nNetwork (SCB-Net) has been developed. The SCB-Net aims to effectively exploit\nthe information from auxiliary variables while producing spatially constrained\npredictions. It is made up of two parts, the first part focuses on learning\nunderlying patterns in the auxiliary variables while the second part integrates\nground-truth data and the learned embeddings from the first part. Moreover, to\nassess model uncertainty, a technique called Monte Carlo dropout is used as a\nBayesian approximation. The SCB-Net has been applied to two selected areas in\nnorthern Quebec, Canada, and has demonstrated its potential in generating\nfield-data-constrained lithological maps while allowing assessment of\nprediction uncertainty for decision-making. This study highlights the promising\nadvancements of deep neural networks in geostatistics, particularly in handling\ncomplex spatial feature learning tasks, leading to improved spatial information\ntechniques.\n", "link": "http://arxiv.org/abs/2403.20195v1", "date": "2024-03-29", "relevancy": 2.2351, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5043}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Lithological%20Mapping%20with%20Spatially%20Constrained%20Bayesian%0A%20%20Network%20%28SCB-Net%29%3A%20An%20Approach%20for%20Field%20Data-Constrained%20Predictions%20with%0A%20%20Uncertainty%20Evaluation&body=Title%3A%20Enhancing%20Lithological%20Mapping%20with%20Spatially%20Constrained%20Bayesian%0A%20%20Network%20%28SCB-Net%29%3A%20An%20Approach%20for%20Field%20Data-Constrained%20Predictions%20with%0A%20%20Uncertainty%20Evaluation%0AAuthor%3A%20Victor%20Silva%20dos%20Santos%20and%20Erwan%20Gloaguen%20and%20Shiva%20Tirdad%0AAbstract%3A%20%20%20Geological%20maps%20are%20an%20extremely%20valuable%20source%20of%20information%20for%20the%20Earth%0Asciences.%20They%20provide%20insights%20into%20mineral%20exploration%2C%20vulnerability%20to%0Anatural%20hazards%2C%20and%20many%20other%20applications.%20These%20maps%20are%20created%20using%0Anumerical%20or%20conceptual%20models%20that%20use%20geological%20observations%20to%20extrapolate%0Adata.%20Geostatistical%20techniques%20have%20traditionally%20been%20used%20to%20generate%0Areliable%20predictions%20that%20take%20into%20account%20the%20spatial%20patterns%20inherent%20in%0Athe%20data.%20However%2C%20as%20the%20number%20of%20auxiliary%20variables%20increases%2C%20these%0Amethods%20become%20more%20labor-intensive.%20Additionally%2C%20traditional%20machine%20learning%0Amethods%20often%20struggle%20with%20spatially%20correlated%20data%20and%20extracting%20valuable%0Anon-linear%20information%20from%20geoscientific%20datasets.%20To%20address%20these%0Alimitations%2C%20a%20new%20architecture%20called%20the%20Spatially%20Constrained%20Bayesian%0ANetwork%20%28SCB-Net%29%20has%20been%20developed.%20The%20SCB-Net%20aims%20to%20effectively%20exploit%0Athe%20information%20from%20auxiliary%20variables%20while%20producing%20spatially%20constrained%0Apredictions.%20It%20is%20made%20up%20of%20two%20parts%2C%20the%20first%20part%20focuses%20on%20learning%0Aunderlying%20patterns%20in%20the%20auxiliary%20variables%20while%20the%20second%20part%20integrates%0Aground-truth%20data%20and%20the%20learned%20embeddings%20from%20the%20first%20part.%20Moreover%2C%20to%0Aassess%20model%20uncertainty%2C%20a%20technique%20called%20Monte%20Carlo%20dropout%20is%20used%20as%20a%0ABayesian%20approximation.%20The%20SCB-Net%20has%20been%20applied%20to%20two%20selected%20areas%20in%0Anorthern%20Quebec%2C%20Canada%2C%20and%20has%20demonstrated%20its%20potential%20in%20generating%0Afield-data-constrained%20lithological%20maps%20while%20allowing%20assessment%20of%0Aprediction%20uncertainty%20for%20decision-making.%20This%20study%20highlights%20the%20promising%0Aadvancements%20of%20deep%20neural%20networks%20in%20geostatistics%2C%20particularly%20in%20handling%0Acomplex%20spatial%20feature%20learning%20tasks%2C%20leading%20to%20improved%20spatial%20information%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20195v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Lithological%20Mapping%20with%20Spatially%20Constrained%20Bayesian%0A%20%20Network%20%28SCB-Net%29%3A%20An%20Approach%20for%20Field%20Data-Constrained%20Predictions%20with%0A%20%20Uncertainty%20Evaluation&entry.906535625=Victor%20Silva%20dos%20Santos%20and%20Erwan%20Gloaguen%20and%20Shiva%20Tirdad&entry.1292438233=%20%20Geological%20maps%20are%20an%20extremely%20valuable%20source%20of%20information%20for%20the%20Earth%0Asciences.%20They%20provide%20insights%20into%20mineral%20exploration%2C%20vulnerability%20to%0Anatural%20hazards%2C%20and%20many%20other%20applications.%20These%20maps%20are%20created%20using%0Anumerical%20or%20conceptual%20models%20that%20use%20geological%20observations%20to%20extrapolate%0Adata.%20Geostatistical%20techniques%20have%20traditionally%20been%20used%20to%20generate%0Areliable%20predictions%20that%20take%20into%20account%20the%20spatial%20patterns%20inherent%20in%0Athe%20data.%20However%2C%20as%20the%20number%20of%20auxiliary%20variables%20increases%2C%20these%0Amethods%20become%20more%20labor-intensive.%20Additionally%2C%20traditional%20machine%20learning%0Amethods%20often%20struggle%20with%20spatially%20correlated%20data%20and%20extracting%20valuable%0Anon-linear%20information%20from%20geoscientific%20datasets.%20To%20address%20these%0Alimitations%2C%20a%20new%20architecture%20called%20the%20Spatially%20Constrained%20Bayesian%0ANetwork%20%28SCB-Net%29%20has%20been%20developed.%20The%20SCB-Net%20aims%20to%20effectively%20exploit%0Athe%20information%20from%20auxiliary%20variables%20while%20producing%20spatially%20constrained%0Apredictions.%20It%20is%20made%20up%20of%20two%20parts%2C%20the%20first%20part%20focuses%20on%20learning%0Aunderlying%20patterns%20in%20the%20auxiliary%20variables%20while%20the%20second%20part%20integrates%0Aground-truth%20data%20and%20the%20learned%20embeddings%20from%20the%20first%20part.%20Moreover%2C%20to%0Aassess%20model%20uncertainty%2C%20a%20technique%20called%20Monte%20Carlo%20dropout%20is%20used%20as%20a%0ABayesian%20approximation.%20The%20SCB-Net%20has%20been%20applied%20to%20two%20selected%20areas%20in%0Anorthern%20Quebec%2C%20Canada%2C%20and%20has%20demonstrated%20its%20potential%20in%20generating%0Afield-data-constrained%20lithological%20maps%20while%20allowing%20assessment%20of%0Aprediction%20uncertainty%20for%20decision-making.%20This%20study%20highlights%20the%20promising%0Aadvancements%20of%20deep%20neural%20networks%20in%20geostatistics%2C%20particularly%20in%20handling%0Acomplex%20spatial%20feature%20learning%20tasks%2C%20leading%20to%20improved%20spatial%20information%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20195v1&entry.124074799=Read"},
{"title": "HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation\n  in Urban Scenes", "author": "Ke Wu and Kaizhao Zhang and Zhiwei Zhang and Shanshuai Yuan and Muer Tie and Julong Wei and Zijun Xu and Jieru Zhao and Zhongxue Gan and Wenchao Ding", "abstract": "  Online dense mapping of urban scenes forms a fundamental cornerstone for\nscene understanding and navigation of autonomous vehicles. Recent advancements\nin mapping methods are mainly based on NeRF, whose rendering speed is too slow\nto meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering\nspeed hundreds of times faster than NeRF, holds greater potential in online\ndense mapping. However, integrating 3DGS into a street-view dense mapping\nframework still faces two challenges, including incomplete reconstruction due\nto the absence of geometric information beyond the LiDAR coverage area and\nextensive computation for reconstruction in large urban scenes. To this end, we\npropose HGS-Mapping, an online dense mapping framework in unbounded large-scale\nscenes. To attain complete construction, our framework introduces Hybrid\nGaussian Representation, which models different parts of the entire scene using\nGaussians with distinct properties. Furthermore, we employ a hybrid Gaussian\ninitialization mechanism and an adaptive update method to achieve high-fidelity\nand rapid reconstruction. To the best of our knowledge, we are the first to\nintegrate Gaussian representation into online dense mapping of urban scenes.\nOur approach achieves SOTA reconstruction accuracy while only employing 66%\nnumber of Gaussians, leading to 20% faster reconstruction speed.\n", "link": "http://arxiv.org/abs/2403.20159v1", "date": "2024-03-29", "relevancy": 2.233, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5932}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5257}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HGS-Mapping%3A%20Online%20Dense%20Mapping%20Using%20Hybrid%20Gaussian%20Representation%0A%20%20in%20Urban%20Scenes&body=Title%3A%20HGS-Mapping%3A%20Online%20Dense%20Mapping%20Using%20Hybrid%20Gaussian%20Representation%0A%20%20in%20Urban%20Scenes%0AAuthor%3A%20Ke%20Wu%20and%20Kaizhao%20Zhang%20and%20Zhiwei%20Zhang%20and%20Shanshuai%20Yuan%20and%20Muer%20Tie%20and%20Julong%20Wei%20and%20Zijun%20Xu%20and%20Jieru%20Zhao%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding%0AAbstract%3A%20%20%20Online%20dense%20mapping%20of%20urban%20scenes%20forms%20a%20fundamental%20cornerstone%20for%0Ascene%20understanding%20and%20navigation%20of%20autonomous%20vehicles.%20Recent%20advancements%0Ain%20mapping%20methods%20are%20mainly%20based%20on%20NeRF%2C%20whose%20rendering%20speed%20is%20too%20slow%0Ato%20meet%20online%20requirements.%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20with%20its%20rendering%0Aspeed%20hundreds%20of%20times%20faster%20than%20NeRF%2C%20holds%20greater%20potential%20in%20online%0Adense%20mapping.%20However%2C%20integrating%203DGS%20into%20a%20street-view%20dense%20mapping%0Aframework%20still%20faces%20two%20challenges%2C%20including%20incomplete%20reconstruction%20due%0Ato%20the%20absence%20of%20geometric%20information%20beyond%20the%20LiDAR%20coverage%20area%20and%0Aextensive%20computation%20for%20reconstruction%20in%20large%20urban%20scenes.%20To%20this%20end%2C%20we%0Apropose%20HGS-Mapping%2C%20an%20online%20dense%20mapping%20framework%20in%20unbounded%20large-scale%0Ascenes.%20To%20attain%20complete%20construction%2C%20our%20framework%20introduces%20Hybrid%0AGaussian%20Representation%2C%20which%20models%20different%20parts%20of%20the%20entire%20scene%20using%0AGaussians%20with%20distinct%20properties.%20Furthermore%2C%20we%20employ%20a%20hybrid%20Gaussian%0Ainitialization%20mechanism%20and%20an%20adaptive%20update%20method%20to%20achieve%20high-fidelity%0Aand%20rapid%20reconstruction.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%0Aintegrate%20Gaussian%20representation%20into%20online%20dense%20mapping%20of%20urban%20scenes.%0AOur%20approach%20achieves%20SOTA%20reconstruction%20accuracy%20while%20only%20employing%2066%25%0Anumber%20of%20Gaussians%2C%20leading%20to%2020%25%20faster%20reconstruction%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20159v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGS-Mapping%3A%20Online%20Dense%20Mapping%20Using%20Hybrid%20Gaussian%20Representation%0A%20%20in%20Urban%20Scenes&entry.906535625=Ke%20Wu%20and%20Kaizhao%20Zhang%20and%20Zhiwei%20Zhang%20and%20Shanshuai%20Yuan%20and%20Muer%20Tie%20and%20Julong%20Wei%20and%20Zijun%20Xu%20and%20Jieru%20Zhao%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding&entry.1292438233=%20%20Online%20dense%20mapping%20of%20urban%20scenes%20forms%20a%20fundamental%20cornerstone%20for%0Ascene%20understanding%20and%20navigation%20of%20autonomous%20vehicles.%20Recent%20advancements%0Ain%20mapping%20methods%20are%20mainly%20based%20on%20NeRF%2C%20whose%20rendering%20speed%20is%20too%20slow%0Ato%20meet%20online%20requirements.%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20with%20its%20rendering%0Aspeed%20hundreds%20of%20times%20faster%20than%20NeRF%2C%20holds%20greater%20potential%20in%20online%0Adense%20mapping.%20However%2C%20integrating%203DGS%20into%20a%20street-view%20dense%20mapping%0Aframework%20still%20faces%20two%20challenges%2C%20including%20incomplete%20reconstruction%20due%0Ato%20the%20absence%20of%20geometric%20information%20beyond%20the%20LiDAR%20coverage%20area%20and%0Aextensive%20computation%20for%20reconstruction%20in%20large%20urban%20scenes.%20To%20this%20end%2C%20we%0Apropose%20HGS-Mapping%2C%20an%20online%20dense%20mapping%20framework%20in%20unbounded%20large-scale%0Ascenes.%20To%20attain%20complete%20construction%2C%20our%20framework%20introduces%20Hybrid%0AGaussian%20Representation%2C%20which%20models%20different%20parts%20of%20the%20entire%20scene%20using%0AGaussians%20with%20distinct%20properties.%20Furthermore%2C%20we%20employ%20a%20hybrid%20Gaussian%0Ainitialization%20mechanism%20and%20an%20adaptive%20update%20method%20to%20achieve%20high-fidelity%0Aand%20rapid%20reconstruction.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%0Aintegrate%20Gaussian%20representation%20into%20online%20dense%20mapping%20of%20urban%20scenes.%0AOur%20approach%20achieves%20SOTA%20reconstruction%20accuracy%20while%20only%20employing%2066%25%0Anumber%20of%20Gaussians%2C%20leading%20to%2020%25%20faster%20reconstruction%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20159v1&entry.124074799=Read"},
{"title": "BAMM: Bidirectional Autoregressive Motion Model", "author": "Ekkasit Pinyoanuntapong and Muhammad Usama Saleem and Pu Wang and Minwoo Lee and Srijan Das and Chen Chen", "abstract": "  Generating human motion from text has been dominated by denoising motion\nmodels either through diffusion or generative masking process. However, these\nmodels face great limitations in usability by requiring prior knowledge of the\nmotion length. Conversely, autoregressive motion models address this limitation\nby adaptively predicting motion endpoints, at the cost of degraded generation\nquality and editing capabilities. To address these challenges, we propose\nBidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion\ngeneration framework. BAMM consists of two key components: (1) a motion\ntokenizer that transforms 3D human motion into discrete tokens in latent space,\nand (2) a masked self-attention transformer that autoregressively predicts\nrandomly masked tokens via a hybrid attention masking strategy. By unifying\ngenerative masked modeling and autoregressive modeling, BAMM captures rich and\nbidirectional dependencies among motion tokens, while learning the\nprobabilistic mapping from textual inputs to motion outputs with\ndynamically-adjusted motion sequence length. This feature enables BAMM to\nsimultaneously achieving high-quality motion generation with enhanced usability\nand built-in motion editability. Extensive experiments on HumanML3D and KIT-ML\ndatasets demonstrate that BAMM surpasses current state-of-the-art methods in\nboth qualitative and quantitative measures. Our project page is available at\nhttps://github.com/exitudio/BAMM-page.\n", "link": "http://arxiv.org/abs/2403.19435v2", "date": "2024-03-29", "relevancy": 2.2305, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5594}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5379}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model&body=Title%3A%20BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model%0AAuthor%3A%20Ekkasit%20Pinyoanuntapong%20and%20Muhammad%20Usama%20Saleem%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Srijan%20Das%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Generating%20human%20motion%20from%20text%20has%20been%20dominated%20by%20denoising%20motion%0Amodels%20either%20through%20diffusion%20or%20generative%20masking%20process.%20However%2C%20these%0Amodels%20face%20great%20limitations%20in%20usability%20by%20requiring%20prior%20knowledge%20of%20the%0Amotion%20length.%20Conversely%2C%20autoregressive%20motion%20models%20address%20this%20limitation%0Aby%20adaptively%20predicting%20motion%20endpoints%2C%20at%20the%20cost%20of%20degraded%20generation%0Aquality%20and%20editing%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%0ABidirectional%20Autoregressive%20Motion%20Model%20%28BAMM%29%2C%20a%20novel%20text-to-motion%0Ageneration%20framework.%20BAMM%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20motion%0Atokenizer%20that%20transforms%203D%20human%20motion%20into%20discrete%20tokens%20in%20latent%20space%2C%0Aand%20%282%29%20a%20masked%20self-attention%20transformer%20that%20autoregressively%20predicts%0Arandomly%20masked%20tokens%20via%20a%20hybrid%20attention%20masking%20strategy.%20By%20unifying%0Agenerative%20masked%20modeling%20and%20autoregressive%20modeling%2C%20BAMM%20captures%20rich%20and%0Abidirectional%20dependencies%20among%20motion%20tokens%2C%20while%20learning%20the%0Aprobabilistic%20mapping%20from%20textual%20inputs%20to%20motion%20outputs%20with%0Adynamically-adjusted%20motion%20sequence%20length.%20This%20feature%20enables%20BAMM%20to%0Asimultaneously%20achieving%20high-quality%20motion%20generation%20with%20enhanced%20usability%0Aand%20built-in%20motion%20editability.%20Extensive%20experiments%20on%20HumanML3D%20and%20KIT-ML%0Adatasets%20demonstrate%20that%20BAMM%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20qualitative%20and%20quantitative%20measures.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//github.com/exitudio/BAMM-page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19435v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model&entry.906535625=Ekkasit%20Pinyoanuntapong%20and%20Muhammad%20Usama%20Saleem%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Srijan%20Das%20and%20Chen%20Chen&entry.1292438233=%20%20Generating%20human%20motion%20from%20text%20has%20been%20dominated%20by%20denoising%20motion%0Amodels%20either%20through%20diffusion%20or%20generative%20masking%20process.%20However%2C%20these%0Amodels%20face%20great%20limitations%20in%20usability%20by%20requiring%20prior%20knowledge%20of%20the%0Amotion%20length.%20Conversely%2C%20autoregressive%20motion%20models%20address%20this%20limitation%0Aby%20adaptively%20predicting%20motion%20endpoints%2C%20at%20the%20cost%20of%20degraded%20generation%0Aquality%20and%20editing%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%0ABidirectional%20Autoregressive%20Motion%20Model%20%28BAMM%29%2C%20a%20novel%20text-to-motion%0Ageneration%20framework.%20BAMM%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20motion%0Atokenizer%20that%20transforms%203D%20human%20motion%20into%20discrete%20tokens%20in%20latent%20space%2C%0Aand%20%282%29%20a%20masked%20self-attention%20transformer%20that%20autoregressively%20predicts%0Arandomly%20masked%20tokens%20via%20a%20hybrid%20attention%20masking%20strategy.%20By%20unifying%0Agenerative%20masked%20modeling%20and%20autoregressive%20modeling%2C%20BAMM%20captures%20rich%20and%0Abidirectional%20dependencies%20among%20motion%20tokens%2C%20while%20learning%20the%0Aprobabilistic%20mapping%20from%20textual%20inputs%20to%20motion%20outputs%20with%0Adynamically-adjusted%20motion%20sequence%20length.%20This%20feature%20enables%20BAMM%20to%0Asimultaneously%20achieving%20high-quality%20motion%20generation%20with%20enhanced%20usability%0Aand%20built-in%20motion%20editability.%20Extensive%20experiments%20on%20HumanML3D%20and%20KIT-ML%0Adatasets%20demonstrate%20that%20BAMM%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20qualitative%20and%20quantitative%20measures.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//github.com/exitudio/BAMM-page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19435v2&entry.124074799=Read"},
{"title": "Learning using granularity statistical invariants for classification", "author": "Ting-Ting Zhu and Yuan-Hai Shao and Chun-Na Li and Tian Liu", "abstract": "  Learning using statistical invariants (LUSI) is a new learning paradigm,\nwhich adopts weak convergence mechanism, and can be applied to a wider range of\nclassification problems. However, the computation cost of invariant matrices in\nLUSI is high for large-scale datasets during training. To settle this issue,\nthis paper introduces a granularity statistical invariant for LUSI, and\ndevelops a new learning paradigm called learning using granularity statistical\ninvariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms,\ntaking a perspective of minimizing expected risk. As far as we know, it is the\nfirst time to construct granularity statistical invariants. Compared to LUSI,\nthe introduction of this new statistical invariant brings two advantages.\nFirstly, it enhances the structural information of the data. Secondly, LUGSI\ntransforms a large invariant matrix into a smaller one by maximizing the\ndistance between classes, achieving feasibility for large-scale datasets\nclassification problems and significantly enhancing the training speed of model\noperations. Experimental results indicate that LUGSI not only exhibits improved\ngeneralization capabilities but also demonstrates faster training speed,\nparticularly for large-scale datasets.\n", "link": "http://arxiv.org/abs/2403.20122v1", "date": "2024-03-29", "relevancy": 2.2136, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4597}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4352}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4332}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20using%20granularity%20statistical%20invariants%20for%20classification&body=Title%3A%20Learning%20using%20granularity%20statistical%20invariants%20for%20classification%0AAuthor%3A%20Ting-Ting%20Zhu%20and%20Yuan-Hai%20Shao%20and%20Chun-Na%20Li%20and%20Tian%20Liu%0AAbstract%3A%20%20%20Learning%20using%20statistical%20invariants%20%28LUSI%29%20is%20a%20new%20learning%20paradigm%2C%0Awhich%20adopts%20weak%20convergence%20mechanism%2C%20and%20can%20be%20applied%20to%20a%20wider%20range%20of%0Aclassification%20problems.%20However%2C%20the%20computation%20cost%20of%20invariant%20matrices%20in%0ALUSI%20is%20high%20for%20large-scale%20datasets%20during%20training.%20To%20settle%20this%20issue%2C%0Athis%20paper%20introduces%20a%20granularity%20statistical%20invariant%20for%20LUSI%2C%20and%0Adevelops%20a%20new%20learning%20paradigm%20called%20learning%20using%20granularity%20statistical%0Ainvariants%20%28LUGSI%29.%20LUGSI%20employs%20both%20strong%20and%20weak%20convergence%20mechanisms%2C%0Ataking%20a%20perspective%20of%20minimizing%20expected%20risk.%20As%20far%20as%20we%20know%2C%20it%20is%20the%0Afirst%20time%20to%20construct%20granularity%20statistical%20invariants.%20Compared%20to%20LUSI%2C%0Athe%20introduction%20of%20this%20new%20statistical%20invariant%20brings%20two%20advantages.%0AFirstly%2C%20it%20enhances%20the%20structural%20information%20of%20the%20data.%20Secondly%2C%20LUGSI%0Atransforms%20a%20large%20invariant%20matrix%20into%20a%20smaller%20one%20by%20maximizing%20the%0Adistance%20between%20classes%2C%20achieving%20feasibility%20for%20large-scale%20datasets%0Aclassification%20problems%20and%20significantly%20enhancing%20the%20training%20speed%20of%20model%0Aoperations.%20Experimental%20results%20indicate%20that%20LUGSI%20not%20only%20exhibits%20improved%0Ageneralization%20capabilities%20but%20also%20demonstrates%20faster%20training%20speed%2C%0Aparticularly%20for%20large-scale%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20122v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20using%20granularity%20statistical%20invariants%20for%20classification&entry.906535625=Ting-Ting%20Zhu%20and%20Yuan-Hai%20Shao%20and%20Chun-Na%20Li%20and%20Tian%20Liu&entry.1292438233=%20%20Learning%20using%20statistical%20invariants%20%28LUSI%29%20is%20a%20new%20learning%20paradigm%2C%0Awhich%20adopts%20weak%20convergence%20mechanism%2C%20and%20can%20be%20applied%20to%20a%20wider%20range%20of%0Aclassification%20problems.%20However%2C%20the%20computation%20cost%20of%20invariant%20matrices%20in%0ALUSI%20is%20high%20for%20large-scale%20datasets%20during%20training.%20To%20settle%20this%20issue%2C%0Athis%20paper%20introduces%20a%20granularity%20statistical%20invariant%20for%20LUSI%2C%20and%0Adevelops%20a%20new%20learning%20paradigm%20called%20learning%20using%20granularity%20statistical%0Ainvariants%20%28LUGSI%29.%20LUGSI%20employs%20both%20strong%20and%20weak%20convergence%20mechanisms%2C%0Ataking%20a%20perspective%20of%20minimizing%20expected%20risk.%20As%20far%20as%20we%20know%2C%20it%20is%20the%0Afirst%20time%20to%20construct%20granularity%20statistical%20invariants.%20Compared%20to%20LUSI%2C%0Athe%20introduction%20of%20this%20new%20statistical%20invariant%20brings%20two%20advantages.%0AFirstly%2C%20it%20enhances%20the%20structural%20information%20of%20the%20data.%20Secondly%2C%20LUGSI%0Atransforms%20a%20large%20invariant%20matrix%20into%20a%20smaller%20one%20by%20maximizing%20the%0Adistance%20between%20classes%2C%20achieving%20feasibility%20for%20large-scale%20datasets%0Aclassification%20problems%20and%20significantly%20enhancing%20the%20training%20speed%20of%20model%0Aoperations.%20Experimental%20results%20indicate%20that%20LUGSI%20not%20only%20exhibits%20improved%0Ageneralization%20capabilities%20but%20also%20demonstrates%20faster%20training%20speed%2C%0Aparticularly%20for%20large-scale%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20122v1&entry.124074799=Read"},
{"title": "EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised\n  Semantic Segmentation", "author": "Chanyoung Kim and Woojung Han and Dayun Ju and Seong Jae Hwang", "abstract": "  Semantic segmentation has innately relied on extensive pixel-level annotated\ndata, leading to the emergence of unsupervised methodologies. Among them,\nleveraging self-supervised Vision Transformers for unsupervised semantic\nsegmentation (USS) has been making steady progress with expressive deep\nfeatures. Yet, for semantically segmenting images with complex objects, a\npredominant challenge remains: the lack of explicit object-level semantic\nencoding in patch-level features. This technical limitation often leads to\ninadequate segmentation of complex objects with diverse structures. To address\nthis gap, we present a novel approach, EAGLE, which emphasizes object-centric\nrepresentation learning for unsupervised semantic segmentation. Specifically,\nwe introduce EiCue, a spectral technique providing semantic and structural cues\nthrough an eigenbasis derived from the semantic similarity matrix of deep image\nfeatures and color affinity from an image. Further, by incorporating our\nobject-centric contrastive loss with EiCue, we guide our model to learn\nobject-level representations with intra- and inter-image object-feature\nconsistency, thereby enhancing semantic accuracy. Extensive experiments on\nCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art\nUSS results of EAGLE with accurate and consistent semantic segmentation across\ncomplex scenes.\n", "link": "http://arxiv.org/abs/2403.01482v3", "date": "2024-03-29", "relevancy": 2.2107, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5569}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5534}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation&body=Title%3A%20EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Chanyoung%20Kim%20and%20Woojung%20Han%20and%20Dayun%20Ju%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20Semantic%20segmentation%20has%20innately%20relied%20on%20extensive%20pixel-level%20annotated%0Adata%2C%20leading%20to%20the%20emergence%20of%20unsupervised%20methodologies.%20Among%20them%2C%0Aleveraging%20self-supervised%20Vision%20Transformers%20for%20unsupervised%20semantic%0Asegmentation%20%28USS%29%20has%20been%20making%20steady%20progress%20with%20expressive%20deep%0Afeatures.%20Yet%2C%20for%20semantically%20segmenting%20images%20with%20complex%20objects%2C%20a%0Apredominant%20challenge%20remains%3A%20the%20lack%20of%20explicit%20object-level%20semantic%0Aencoding%20in%20patch-level%20features.%20This%20technical%20limitation%20often%20leads%20to%0Ainadequate%20segmentation%20of%20complex%20objects%20with%20diverse%20structures.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20novel%20approach%2C%20EAGLE%2C%20which%20emphasizes%20object-centric%0Arepresentation%20learning%20for%20unsupervised%20semantic%20segmentation.%20Specifically%2C%0Awe%20introduce%20EiCue%2C%20a%20spectral%20technique%20providing%20semantic%20and%20structural%20cues%0Athrough%20an%20eigenbasis%20derived%20from%20the%20semantic%20similarity%20matrix%20of%20deep%20image%0Afeatures%20and%20color%20affinity%20from%20an%20image.%20Further%2C%20by%20incorporating%20our%0Aobject-centric%20contrastive%20loss%20with%20EiCue%2C%20we%20guide%20our%20model%20to%20learn%0Aobject-level%20representations%20with%20intra-%20and%20inter-image%20object-feature%0Aconsistency%2C%20thereby%20enhancing%20semantic%20accuracy.%20Extensive%20experiments%20on%0ACOCO-Stuff%2C%20Cityscapes%2C%20and%20Potsdam-3%20datasets%20demonstrate%20the%20state-of-the-art%0AUSS%20results%20of%20EAGLE%20with%20accurate%20and%20consistent%20semantic%20segmentation%20across%0Acomplex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01482v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAGLE%3A%20Eigen%20Aggregation%20Learning%20for%20Object-Centric%20Unsupervised%0A%20%20Semantic%20Segmentation&entry.906535625=Chanyoung%20Kim%20and%20Woojung%20Han%20and%20Dayun%20Ju%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20Semantic%20segmentation%20has%20innately%20relied%20on%20extensive%20pixel-level%20annotated%0Adata%2C%20leading%20to%20the%20emergence%20of%20unsupervised%20methodologies.%20Among%20them%2C%0Aleveraging%20self-supervised%20Vision%20Transformers%20for%20unsupervised%20semantic%0Asegmentation%20%28USS%29%20has%20been%20making%20steady%20progress%20with%20expressive%20deep%0Afeatures.%20Yet%2C%20for%20semantically%20segmenting%20images%20with%20complex%20objects%2C%20a%0Apredominant%20challenge%20remains%3A%20the%20lack%20of%20explicit%20object-level%20semantic%0Aencoding%20in%20patch-level%20features.%20This%20technical%20limitation%20often%20leads%20to%0Ainadequate%20segmentation%20of%20complex%20objects%20with%20diverse%20structures.%20To%20address%0Athis%20gap%2C%20we%20present%20a%20novel%20approach%2C%20EAGLE%2C%20which%20emphasizes%20object-centric%0Arepresentation%20learning%20for%20unsupervised%20semantic%20segmentation.%20Specifically%2C%0Awe%20introduce%20EiCue%2C%20a%20spectral%20technique%20providing%20semantic%20and%20structural%20cues%0Athrough%20an%20eigenbasis%20derived%20from%20the%20semantic%20similarity%20matrix%20of%20deep%20image%0Afeatures%20and%20color%20affinity%20from%20an%20image.%20Further%2C%20by%20incorporating%20our%0Aobject-centric%20contrastive%20loss%20with%20EiCue%2C%20we%20guide%20our%20model%20to%20learn%0Aobject-level%20representations%20with%20intra-%20and%20inter-image%20object-feature%0Aconsistency%2C%20thereby%20enhancing%20semantic%20accuracy.%20Extensive%20experiments%20on%0ACOCO-Stuff%2C%20Cityscapes%2C%20and%20Potsdam-3%20datasets%20demonstrate%20the%20state-of-the-art%0AUSS%20results%20of%20EAGLE%20with%20accurate%20and%20consistent%20semantic%20segmentation%20across%0Acomplex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01482v3&entry.124074799=Read"},
{"title": "V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse\n  Weather Conditions", "author": "Baolu Li and Jinlong Li and Xinyu Liu and Runsheng Xu and Zhengzhong Tu and Jiacheng Guo and Xiaopeng Li and Hongkai Yu", "abstract": "  Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception\nsystems have shown the significant success on 3D object detection. While these\nmodels perform well in the trained clean weather, they struggle in unseen\nadverse weather conditions with the real-world domain gap. In this paper, we\npropose a domain generalization approach, named V2X-DGW, for LiDAR-based 3D\nobject detection on multi-agent perception system under adverse weather\nconditions. Not only in the clean weather does our research aim to ensure\nfavorable multi-agent performance, but also in the unseen adverse weather\nconditions by learning only on the clean weather data. To advance research in\nthis area, we have simulated the impact of three prevalent adverse weather\nconditions on two widely-used multi-agent datasets, resulting in the creation\nof two novel benchmark datasets: OPV2V-w and V2XSet-w.\n  To this end, we first introduce the Adaptive Weather Augmentation (AWA) to\nmimic the unseen adverse weather conditions, and then propose two alignments\nfor generalizable representation learning: Trust-region Weather-invariant\nAlignment (TWA) and Agent-aware Contrastive Alignment (ACA). Extensive\nexperimental results demonstrate that our V2X-DGW achieved improvements in the\nunseen adverse weather conditions.\n", "link": "http://arxiv.org/abs/2403.11371v4", "date": "2024-03-29", "relevancy": 2.21, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20V2X-DGW%3A%20Domain%20Generalization%20for%20Multi-agent%20Perception%20under%20Adverse%0A%20%20Weather%20Conditions&body=Title%3A%20V2X-DGW%3A%20Domain%20Generalization%20for%20Multi-agent%20Perception%20under%20Adverse%0A%20%20Weather%20Conditions%0AAuthor%3A%20Baolu%20Li%20and%20Jinlong%20Li%20and%20Xinyu%20Liu%20and%20Runsheng%20Xu%20and%20Zhengzhong%20Tu%20and%20Jiacheng%20Guo%20and%20Xiaopeng%20Li%20and%20Hongkai%20Yu%0AAbstract%3A%20%20%20Current%20LiDAR-based%20Vehicle-to-Everything%20%28V2X%29%20multi-agent%20perception%0Asystems%20have%20shown%20the%20significant%20success%20on%203D%20object%20detection.%20While%20these%0Amodels%20perform%20well%20in%20the%20trained%20clean%20weather%2C%20they%20struggle%20in%20unseen%0Aadverse%20weather%20conditions%20with%20the%20real-world%20domain%20gap.%20In%20this%20paper%2C%20we%0Apropose%20a%20domain%20generalization%20approach%2C%20named%20V2X-DGW%2C%20for%20LiDAR-based%203D%0Aobject%20detection%20on%20multi-agent%20perception%20system%20under%20adverse%20weather%0Aconditions.%20Not%20only%20in%20the%20clean%20weather%20does%20our%20research%20aim%20to%20ensure%0Afavorable%20multi-agent%20performance%2C%20but%20also%20in%20the%20unseen%20adverse%20weather%0Aconditions%20by%20learning%20only%20on%20the%20clean%20weather%20data.%20To%20advance%20research%20in%0Athis%20area%2C%20we%20have%20simulated%20the%20impact%20of%20three%20prevalent%20adverse%20weather%0Aconditions%20on%20two%20widely-used%20multi-agent%20datasets%2C%20resulting%20in%20the%20creation%0Aof%20two%20novel%20benchmark%20datasets%3A%20OPV2V-w%20and%20V2XSet-w.%0A%20%20To%20this%20end%2C%20we%20first%20introduce%20the%20Adaptive%20Weather%20Augmentation%20%28AWA%29%20to%0Amimic%20the%20unseen%20adverse%20weather%20conditions%2C%20and%20then%20propose%20two%20alignments%0Afor%20generalizable%20representation%20learning%3A%20Trust-region%20Weather-invariant%0AAlignment%20%28TWA%29%20and%20Agent-aware%20Contrastive%20Alignment%20%28ACA%29.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20V2X-DGW%20achieved%20improvements%20in%20the%0Aunseen%20adverse%20weather%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11371v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X-DGW%3A%20Domain%20Generalization%20for%20Multi-agent%20Perception%20under%20Adverse%0A%20%20Weather%20Conditions&entry.906535625=Baolu%20Li%20and%20Jinlong%20Li%20and%20Xinyu%20Liu%20and%20Runsheng%20Xu%20and%20Zhengzhong%20Tu%20and%20Jiacheng%20Guo%20and%20Xiaopeng%20Li%20and%20Hongkai%20Yu&entry.1292438233=%20%20Current%20LiDAR-based%20Vehicle-to-Everything%20%28V2X%29%20multi-agent%20perception%0Asystems%20have%20shown%20the%20significant%20success%20on%203D%20object%20detection.%20While%20these%0Amodels%20perform%20well%20in%20the%20trained%20clean%20weather%2C%20they%20struggle%20in%20unseen%0Aadverse%20weather%20conditions%20with%20the%20real-world%20domain%20gap.%20In%20this%20paper%2C%20we%0Apropose%20a%20domain%20generalization%20approach%2C%20named%20V2X-DGW%2C%20for%20LiDAR-based%203D%0Aobject%20detection%20on%20multi-agent%20perception%20system%20under%20adverse%20weather%0Aconditions.%20Not%20only%20in%20the%20clean%20weather%20does%20our%20research%20aim%20to%20ensure%0Afavorable%20multi-agent%20performance%2C%20but%20also%20in%20the%20unseen%20adverse%20weather%0Aconditions%20by%20learning%20only%20on%20the%20clean%20weather%20data.%20To%20advance%20research%20in%0Athis%20area%2C%20we%20have%20simulated%20the%20impact%20of%20three%20prevalent%20adverse%20weather%0Aconditions%20on%20two%20widely-used%20multi-agent%20datasets%2C%20resulting%20in%20the%20creation%0Aof%20two%20novel%20benchmark%20datasets%3A%20OPV2V-w%20and%20V2XSet-w.%0A%20%20To%20this%20end%2C%20we%20first%20introduce%20the%20Adaptive%20Weather%20Augmentation%20%28AWA%29%20to%0Amimic%20the%20unseen%20adverse%20weather%20conditions%2C%20and%20then%20propose%20two%20alignments%0Afor%20generalizable%20representation%20learning%3A%20Trust-region%20Weather-invariant%0AAlignment%20%28TWA%29%20and%20Agent-aware%20Contrastive%20Alignment%20%28ACA%29.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20V2X-DGW%20achieved%20improvements%20in%20the%0Aunseen%20adverse%20weather%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11371v4&entry.124074799=Read"},
{"title": "ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with\n  Visual Prompt Tuning", "author": "Beomyoung Kim and Joonsang Yu and Sung Ju Hwang", "abstract": "  Panoptic segmentation, combining semantic and instance segmentation, stands\nas a cutting-edge computer vision task. Despite recent progress with deep\nlearning models, the dynamic nature of real-world applications necessitates\ncontinual learning, where models adapt to new classes (plasticity) over time\nwithout forgetting old ones (catastrophic forgetting). Current continual\nsegmentation methods often rely on distillation strategies like knowledge\ndistillation and pseudo-labeling, which are effective but result in increased\ntraining complexity and computational overhead. In this paper, we introduce a\nnovel and efficient method for continual panoptic segmentation based on Visual\nPrompt Tuning, dubbed ECLIPSE. Our approach involves freezing the base model\nparameters and fine-tuning only a small set of prompt embeddings, addressing\nboth catastrophic forgetting and plasticity and significantly reducing the\ntrainable parameters. To mitigate inherent challenges such as error propagation\nand semantic drift in continual segmentation, we propose logit manipulation to\neffectively leverage common knowledge across the classes. Experiments on ADE20K\ncontinual panoptic segmentation benchmark demonstrate the superiority of\nECLIPSE, notably its robustness against catastrophic forgetting and its\nreasonable plasticity, achieving a new state-of-the-art. The code is available\nat https://github.com/clovaai/ECLIPSE.\n", "link": "http://arxiv.org/abs/2403.20126v1", "date": "2024-03-29", "relevancy": 2.2062, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECLIPSE%3A%20Efficient%20Continual%20Learning%20in%20Panoptic%20Segmentation%20with%0A%20%20Visual%20Prompt%20Tuning&body=Title%3A%20ECLIPSE%3A%20Efficient%20Continual%20Learning%20in%20Panoptic%20Segmentation%20with%0A%20%20Visual%20Prompt%20Tuning%0AAuthor%3A%20Beomyoung%20Kim%20and%20Joonsang%20Yu%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Panoptic%20segmentation%2C%20combining%20semantic%20and%20instance%20segmentation%2C%20stands%0Aas%20a%20cutting-edge%20computer%20vision%20task.%20Despite%20recent%20progress%20with%20deep%0Alearning%20models%2C%20the%20dynamic%20nature%20of%20real-world%20applications%20necessitates%0Acontinual%20learning%2C%20where%20models%20adapt%20to%20new%20classes%20%28plasticity%29%20over%20time%0Awithout%20forgetting%20old%20ones%20%28catastrophic%20forgetting%29.%20Current%20continual%0Asegmentation%20methods%20often%20rely%20on%20distillation%20strategies%20like%20knowledge%0Adistillation%20and%20pseudo-labeling%2C%20which%20are%20effective%20but%20result%20in%20increased%0Atraining%20complexity%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20and%20efficient%20method%20for%20continual%20panoptic%20segmentation%20based%20on%20Visual%0APrompt%20Tuning%2C%20dubbed%20ECLIPSE.%20Our%20approach%20involves%20freezing%20the%20base%20model%0Aparameters%20and%20fine-tuning%20only%20a%20small%20set%20of%20prompt%20embeddings%2C%20addressing%0Aboth%20catastrophic%20forgetting%20and%20plasticity%20and%20significantly%20reducing%20the%0Atrainable%20parameters.%20To%20mitigate%20inherent%20challenges%20such%20as%20error%20propagation%0Aand%20semantic%20drift%20in%20continual%20segmentation%2C%20we%20propose%20logit%20manipulation%20to%0Aeffectively%20leverage%20common%20knowledge%20across%20the%20classes.%20Experiments%20on%20ADE20K%0Acontinual%20panoptic%20segmentation%20benchmark%20demonstrate%20the%20superiority%20of%0AECLIPSE%2C%20notably%20its%20robustness%20against%20catastrophic%20forgetting%20and%20its%0Areasonable%20plasticity%2C%20achieving%20a%20new%20state-of-the-art.%20The%20code%20is%20available%0Aat%20https%3A//github.com/clovaai/ECLIPSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20126v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECLIPSE%3A%20Efficient%20Continual%20Learning%20in%20Panoptic%20Segmentation%20with%0A%20%20Visual%20Prompt%20Tuning&entry.906535625=Beomyoung%20Kim%20and%20Joonsang%20Yu%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Panoptic%20segmentation%2C%20combining%20semantic%20and%20instance%20segmentation%2C%20stands%0Aas%20a%20cutting-edge%20computer%20vision%20task.%20Despite%20recent%20progress%20with%20deep%0Alearning%20models%2C%20the%20dynamic%20nature%20of%20real-world%20applications%20necessitates%0Acontinual%20learning%2C%20where%20models%20adapt%20to%20new%20classes%20%28plasticity%29%20over%20time%0Awithout%20forgetting%20old%20ones%20%28catastrophic%20forgetting%29.%20Current%20continual%0Asegmentation%20methods%20often%20rely%20on%20distillation%20strategies%20like%20knowledge%0Adistillation%20and%20pseudo-labeling%2C%20which%20are%20effective%20but%20result%20in%20increased%0Atraining%20complexity%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20and%20efficient%20method%20for%20continual%20panoptic%20segmentation%20based%20on%20Visual%0APrompt%20Tuning%2C%20dubbed%20ECLIPSE.%20Our%20approach%20involves%20freezing%20the%20base%20model%0Aparameters%20and%20fine-tuning%20only%20a%20small%20set%20of%20prompt%20embeddings%2C%20addressing%0Aboth%20catastrophic%20forgetting%20and%20plasticity%20and%20significantly%20reducing%20the%0Atrainable%20parameters.%20To%20mitigate%20inherent%20challenges%20such%20as%20error%20propagation%0Aand%20semantic%20drift%20in%20continual%20segmentation%2C%20we%20propose%20logit%20manipulation%20to%0Aeffectively%20leverage%20common%20knowledge%20across%20the%20classes.%20Experiments%20on%20ADE20K%0Acontinual%20panoptic%20segmentation%20benchmark%20demonstrate%20the%20superiority%20of%0AECLIPSE%2C%20notably%20its%20robustness%20against%20catastrophic%20forgetting%20and%20its%0Areasonable%20plasticity%2C%20achieving%20a%20new%20state-of-the-art.%20The%20code%20is%20available%0Aat%20https%3A//github.com/clovaai/ECLIPSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20126v1&entry.124074799=Read"},
{"title": "LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End\n  Autonomous Driving", "author": "Pranjal Paul and Anant Garg and Tushar Choudhary and Arun Kumar Singh and K. Madhava Krishna", "abstract": "  Existing Vision-Language models (VLMs) estimate either long-term trajectory\nwaypoints or a set of control actions as a reactive solution for closed-loop\nplanning based on their rich scene comprehension. However, these estimations\nare coarse and are subjective to their \"world understanding\" which may generate\nsub-optimal decisions due to perception errors. In this paper, we introduce\nLeGo-Drive, which aims to address this issue by estimating a goal location\nbased on the given language command as an intermediate representation in an\nend-to-end setting. The estimated goal might fall in a non-desirable region,\nlike on top of a car for a parking-like command, leading to inadequate\nplanning. Hence, we propose to train the architecture in an end-to-end manner,\nresulting in iterative refinement of both the goal and the trajectory\ncollectively. We validate the effectiveness of our method through comprehensive\nexperiments conducted in diverse simulated environments. We report significant\nimprovements in standard autonomous driving metrics, with a goal reaching\nSuccess Rate of 81%. We further showcase the versatility of LeGo-Drive across\ndifferent driving scenarios and linguistic inputs, underscoring its potential\nfor practical deployment in autonomous vehicles and intelligent transportation\nsystems.\n", "link": "http://arxiv.org/abs/2403.20116v1", "date": "2024-03-29", "relevancy": 2.2033, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5625}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LeGo-Drive%3A%20Language-enhanced%20Goal-oriented%20Closed-Loop%20End-to-End%0A%20%20Autonomous%20Driving&body=Title%3A%20LeGo-Drive%3A%20Language-enhanced%20Goal-oriented%20Closed-Loop%20End-to-End%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Pranjal%20Paul%20and%20Anant%20Garg%20and%20Tushar%20Choudhary%20and%20Arun%20Kumar%20Singh%20and%20K.%20Madhava%20Krishna%0AAbstract%3A%20%20%20Existing%20Vision-Language%20models%20%28VLMs%29%20estimate%20either%20long-term%20trajectory%0Awaypoints%20or%20a%20set%20of%20control%20actions%20as%20a%20reactive%20solution%20for%20closed-loop%0Aplanning%20based%20on%20their%20rich%20scene%20comprehension.%20However%2C%20these%20estimations%0Aare%20coarse%20and%20are%20subjective%20to%20their%20%22world%20understanding%22%20which%20may%20generate%0Asub-optimal%20decisions%20due%20to%20perception%20errors.%20In%20this%20paper%2C%20we%20introduce%0ALeGo-Drive%2C%20which%20aims%20to%20address%20this%20issue%20by%20estimating%20a%20goal%20location%0Abased%20on%20the%20given%20language%20command%20as%20an%20intermediate%20representation%20in%20an%0Aend-to-end%20setting.%20The%20estimated%20goal%20might%20fall%20in%20a%20non-desirable%20region%2C%0Alike%20on%20top%20of%20a%20car%20for%20a%20parking-like%20command%2C%20leading%20to%20inadequate%0Aplanning.%20Hence%2C%20we%20propose%20to%20train%20the%20architecture%20in%20an%20end-to-end%20manner%2C%0Aresulting%20in%20iterative%20refinement%20of%20both%20the%20goal%20and%20the%20trajectory%0Acollectively.%20We%20validate%20the%20effectiveness%20of%20our%20method%20through%20comprehensive%0Aexperiments%20conducted%20in%20diverse%20simulated%20environments.%20We%20report%20significant%0Aimprovements%20in%20standard%20autonomous%20driving%20metrics%2C%20with%20a%20goal%20reaching%0ASuccess%20Rate%20of%2081%25.%20We%20further%20showcase%20the%20versatility%20of%20LeGo-Drive%20across%0Adifferent%20driving%20scenarios%20and%20linguistic%20inputs%2C%20underscoring%20its%20potential%0Afor%20practical%20deployment%20in%20autonomous%20vehicles%20and%20intelligent%20transportation%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20116v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeGo-Drive%3A%20Language-enhanced%20Goal-oriented%20Closed-Loop%20End-to-End%0A%20%20Autonomous%20Driving&entry.906535625=Pranjal%20Paul%20and%20Anant%20Garg%20and%20Tushar%20Choudhary%20and%20Arun%20Kumar%20Singh%20and%20K.%20Madhava%20Krishna&entry.1292438233=%20%20Existing%20Vision-Language%20models%20%28VLMs%29%20estimate%20either%20long-term%20trajectory%0Awaypoints%20or%20a%20set%20of%20control%20actions%20as%20a%20reactive%20solution%20for%20closed-loop%0Aplanning%20based%20on%20their%20rich%20scene%20comprehension.%20However%2C%20these%20estimations%0Aare%20coarse%20and%20are%20subjective%20to%20their%20%22world%20understanding%22%20which%20may%20generate%0Asub-optimal%20decisions%20due%20to%20perception%20errors.%20In%20this%20paper%2C%20we%20introduce%0ALeGo-Drive%2C%20which%20aims%20to%20address%20this%20issue%20by%20estimating%20a%20goal%20location%0Abased%20on%20the%20given%20language%20command%20as%20an%20intermediate%20representation%20in%20an%0Aend-to-end%20setting.%20The%20estimated%20goal%20might%20fall%20in%20a%20non-desirable%20region%2C%0Alike%20on%20top%20of%20a%20car%20for%20a%20parking-like%20command%2C%20leading%20to%20inadequate%0Aplanning.%20Hence%2C%20we%20propose%20to%20train%20the%20architecture%20in%20an%20end-to-end%20manner%2C%0Aresulting%20in%20iterative%20refinement%20of%20both%20the%20goal%20and%20the%20trajectory%0Acollectively.%20We%20validate%20the%20effectiveness%20of%20our%20method%20through%20comprehensive%0Aexperiments%20conducted%20in%20diverse%20simulated%20environments.%20We%20report%20significant%0Aimprovements%20in%20standard%20autonomous%20driving%20metrics%2C%20with%20a%20goal%20reaching%0ASuccess%20Rate%20of%2081%25.%20We%20further%20showcase%20the%20versatility%20of%20LeGo-Drive%20across%0Adifferent%20driving%20scenarios%20and%20linguistic%20inputs%2C%20underscoring%20its%20potential%0Afor%20practical%20deployment%20in%20autonomous%20vehicles%20and%20intelligent%20transportation%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20116v1&entry.124074799=Read"},
{"title": "CustomListener: Text-guided Responsive Interaction for User-friendly\n  Listening Head Generation", "author": "Xi Liu and Ying Guo and Cheng Zhen and Tong Li and Yingying Ao and Pengfei Yan", "abstract": "  Listening head generation aims to synthesize a non-verbal responsive listener\nhead by modeling the correlation between the speaker and the listener in\ndynamic conversion.The applications of listener agent generation in virtual\ninteraction have promoted many works achieving the diverse and fine-grained\nmotion generation. However, they can only manipulate motions through simple\nemotional labels, but cannot freely control the listener's motions. Since\nlistener agents should have human-like attributes (e.g. identity, personality)\nwhich can be freely customized by users, this limits their realism. In this\npaper, we propose a user-friendly framework called CustomListener to realize\nthe free-form text prior guided listener generation. To achieve\nspeaker-listener coordination, we design a Static to Dynamic Portrait module\n(SDP), which interacts with speaker information to transform static text into\ndynamic portrait token with completion rhythm and amplitude information. To\nachieve coherence between segments, we design a Past Guided Generation Module\n(PGG) to maintain the consistency of customized listener attributes through the\nmotion prior, and utilize a diffusion-based structure conditioned on the\nportrait token and the motion prior to realize the controllable generation. To\ntrain and evaluate our model, we have constructed two text-annotated listening\nhead datasets based on ViCo and RealTalk, which provide text-video paired\nlabels. Extensive experiments have verified the effectiveness of our model.\n", "link": "http://arxiv.org/abs/2403.00274v2", "date": "2024-03-29", "relevancy": 2.2024, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5909}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.544}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.541}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CustomListener%3A%20Text-guided%20Responsive%20Interaction%20for%20User-friendly%0A%20%20Listening%20Head%20Generation&body=Title%3A%20CustomListener%3A%20Text-guided%20Responsive%20Interaction%20for%20User-friendly%0A%20%20Listening%20Head%20Generation%0AAuthor%3A%20Xi%20Liu%20and%20Ying%20Guo%20and%20Cheng%20Zhen%20and%20Tong%20Li%20and%20Yingying%20Ao%20and%20Pengfei%20Yan%0AAbstract%3A%20%20%20Listening%20head%20generation%20aims%20to%20synthesize%20a%20non-verbal%20responsive%20listener%0Ahead%20by%20modeling%20the%20correlation%20between%20the%20speaker%20and%20the%20listener%20in%0Adynamic%20conversion.The%20applications%20of%20listener%20agent%20generation%20in%20virtual%0Ainteraction%20have%20promoted%20many%20works%20achieving%20the%20diverse%20and%20fine-grained%0Amotion%20generation.%20However%2C%20they%20can%20only%20manipulate%20motions%20through%20simple%0Aemotional%20labels%2C%20but%20cannot%20freely%20control%20the%20listener%27s%20motions.%20Since%0Alistener%20agents%20should%20have%20human-like%20attributes%20%28e.g.%20identity%2C%20personality%29%0Awhich%20can%20be%20freely%20customized%20by%20users%2C%20this%20limits%20their%20realism.%20In%20this%0Apaper%2C%20we%20propose%20a%20user-friendly%20framework%20called%20CustomListener%20to%20realize%0Athe%20free-form%20text%20prior%20guided%20listener%20generation.%20To%20achieve%0Aspeaker-listener%20coordination%2C%20we%20design%20a%20Static%20to%20Dynamic%20Portrait%20module%0A%28SDP%29%2C%20which%20interacts%20with%20speaker%20information%20to%20transform%20static%20text%20into%0Adynamic%20portrait%20token%20with%20completion%20rhythm%20and%20amplitude%20information.%20To%0Aachieve%20coherence%20between%20segments%2C%20we%20design%20a%20Past%20Guided%20Generation%20Module%0A%28PGG%29%20to%20maintain%20the%20consistency%20of%20customized%20listener%20attributes%20through%20the%0Amotion%20prior%2C%20and%20utilize%20a%20diffusion-based%20structure%20conditioned%20on%20the%0Aportrait%20token%20and%20the%20motion%20prior%20to%20realize%20the%20controllable%20generation.%20To%0Atrain%20and%20evaluate%20our%20model%2C%20we%20have%20constructed%20two%20text-annotated%20listening%0Ahead%20datasets%20based%20on%20ViCo%20and%20RealTalk%2C%20which%20provide%20text-video%20paired%0Alabels.%20Extensive%20experiments%20have%20verified%20the%20effectiveness%20of%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00274v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CustomListener%3A%20Text-guided%20Responsive%20Interaction%20for%20User-friendly%0A%20%20Listening%20Head%20Generation&entry.906535625=Xi%20Liu%20and%20Ying%20Guo%20and%20Cheng%20Zhen%20and%20Tong%20Li%20and%20Yingying%20Ao%20and%20Pengfei%20Yan&entry.1292438233=%20%20Listening%20head%20generation%20aims%20to%20synthesize%20a%20non-verbal%20responsive%20listener%0Ahead%20by%20modeling%20the%20correlation%20between%20the%20speaker%20and%20the%20listener%20in%0Adynamic%20conversion.The%20applications%20of%20listener%20agent%20generation%20in%20virtual%0Ainteraction%20have%20promoted%20many%20works%20achieving%20the%20diverse%20and%20fine-grained%0Amotion%20generation.%20However%2C%20they%20can%20only%20manipulate%20motions%20through%20simple%0Aemotional%20labels%2C%20but%20cannot%20freely%20control%20the%20listener%27s%20motions.%20Since%0Alistener%20agents%20should%20have%20human-like%20attributes%20%28e.g.%20identity%2C%20personality%29%0Awhich%20can%20be%20freely%20customized%20by%20users%2C%20this%20limits%20their%20realism.%20In%20this%0Apaper%2C%20we%20propose%20a%20user-friendly%20framework%20called%20CustomListener%20to%20realize%0Athe%20free-form%20text%20prior%20guided%20listener%20generation.%20To%20achieve%0Aspeaker-listener%20coordination%2C%20we%20design%20a%20Static%20to%20Dynamic%20Portrait%20module%0A%28SDP%29%2C%20which%20interacts%20with%20speaker%20information%20to%20transform%20static%20text%20into%0Adynamic%20portrait%20token%20with%20completion%20rhythm%20and%20amplitude%20information.%20To%0Aachieve%20coherence%20between%20segments%2C%20we%20design%20a%20Past%20Guided%20Generation%20Module%0A%28PGG%29%20to%20maintain%20the%20consistency%20of%20customized%20listener%20attributes%20through%20the%0Amotion%20prior%2C%20and%20utilize%20a%20diffusion-based%20structure%20conditioned%20on%20the%0Aportrait%20token%20and%20the%20motion%20prior%20to%20realize%20the%20controllable%20generation.%20To%0Atrain%20and%20evaluate%20our%20model%2C%20we%20have%20constructed%20two%20text-annotated%20listening%0Ahead%20datasets%20based%20on%20ViCo%20and%20RealTalk%2C%20which%20provide%20text-video%20paired%0Alabels.%20Extensive%20experiments%20have%20verified%20the%20effectiveness%20of%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00274v2&entry.124074799=Read"},
{"title": "Distribution-Aware Continual Test-Time Adaptation for Semantic\n  Segmentation", "author": "Jiayi Ni and Senqiao Yang and Ran Xu and Jiaming Liu and Xiaoqi Li and Wenyu Jiao and Zehui Chen and Yi Liu and Shanghang Zhang", "abstract": "  Since autonomous driving systems usually face dynamic and ever-changing\nenvironments, continual test-time adaptation (CTTA) has been proposed as a\nstrategy for transferring deployed models to continually changing target\ndomains. However, the pursuit of long-term adaptation often introduces\ncatastrophic forgetting and error accumulation problems, which impede the\npractical implementation of CTTA in the real world. Recently, existing CTTA\nmethods mainly focus on utilizing a majority of parameters to fit target domain\nknowledge through self-training. Unfortunately, these approaches often amplify\nthe challenge of error accumulation due to noisy pseudo-labels, and pose\npractical limitations stemming from the heavy computational costs associated\nwith entire model updates. In this paper, we propose a distribution-aware\ntuning (DAT) method to make the semantic segmentation CTTA efficient and\npractical in real-world applications. DAT adaptively selects and updates two\nsmall groups of trainable parameters based on data distribution during the\ncontinual adaptation process, including domain-specific parameters (DSP) and\ntask-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to\noutputs with substantial distribution shifts, effectively mitigating the\nproblem of error accumulation. In contrast, TRP are allocated to positions that\nare responsive to outputs with minor distribution shifts, which are fine-tuned\nto avoid the catastrophic forgetting problem. In addition, since CTTA is a\ntemporal task, we introduce the Parameter Accumulation Update (PAU) strategy to\ncollect the updated DSP and TRP in target domain sequences. We conduct\nextensive experiments on two widely-used semantic segmentation CTTA benchmarks,\nachieving promising performance compared to previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2309.13604v2", "date": "2024-03-29", "relevancy": 2.1659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5235}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distribution-Aware%20Continual%20Test-Time%20Adaptation%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Distribution-Aware%20Continual%20Test-Time%20Adaptation%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jiayi%20Ni%20and%20Senqiao%20Yang%20and%20Ran%20Xu%20and%20Jiaming%20Liu%20and%20Xiaoqi%20Li%20and%20Wenyu%20Jiao%20and%20Zehui%20Chen%20and%20Yi%20Liu%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Since%20autonomous%20driving%20systems%20usually%20face%20dynamic%20and%20ever-changing%0Aenvironments%2C%20continual%20test-time%20adaptation%20%28CTTA%29%20has%20been%20proposed%20as%20a%0Astrategy%20for%20transferring%20deployed%20models%20to%20continually%20changing%20target%0Adomains.%20However%2C%20the%20pursuit%20of%20long-term%20adaptation%20often%20introduces%0Acatastrophic%20forgetting%20and%20error%20accumulation%20problems%2C%20which%20impede%20the%0Apractical%20implementation%20of%20CTTA%20in%20the%20real%20world.%20Recently%2C%20existing%20CTTA%0Amethods%20mainly%20focus%20on%20utilizing%20a%20majority%20of%20parameters%20to%20fit%20target%20domain%0Aknowledge%20through%20self-training.%20Unfortunately%2C%20these%20approaches%20often%20amplify%0Athe%20challenge%20of%20error%20accumulation%20due%20to%20noisy%20pseudo-labels%2C%20and%20pose%0Apractical%20limitations%20stemming%20from%20the%20heavy%20computational%20costs%20associated%0Awith%20entire%20model%20updates.%20In%20this%20paper%2C%20we%20propose%20a%20distribution-aware%0Atuning%20%28DAT%29%20method%20to%20make%20the%20semantic%20segmentation%20CTTA%20efficient%20and%0Apractical%20in%20real-world%20applications.%20DAT%20adaptively%20selects%20and%20updates%20two%0Asmall%20groups%20of%20trainable%20parameters%20based%20on%20data%20distribution%20during%20the%0Acontinual%20adaptation%20process%2C%20including%20domain-specific%20parameters%20%28DSP%29%20and%0Atask-relevant%20parameters%20%28TRP%29.%20Specifically%2C%20DSP%20exhibits%20sensitivity%20to%0Aoutputs%20with%20substantial%20distribution%20shifts%2C%20effectively%20mitigating%20the%0Aproblem%20of%20error%20accumulation.%20In%20contrast%2C%20TRP%20are%20allocated%20to%20positions%20that%0Aare%20responsive%20to%20outputs%20with%20minor%20distribution%20shifts%2C%20which%20are%20fine-tuned%0Ato%20avoid%20the%20catastrophic%20forgetting%20problem.%20In%20addition%2C%20since%20CTTA%20is%20a%0Atemporal%20task%2C%20we%20introduce%20the%20Parameter%20Accumulation%20Update%20%28PAU%29%20strategy%20to%0Acollect%20the%20updated%20DSP%20and%20TRP%20in%20target%20domain%20sequences.%20We%20conduct%0Aextensive%20experiments%20on%20two%20widely-used%20semantic%20segmentation%20CTTA%20benchmarks%2C%0Aachieving%20promising%20performance%20compared%20to%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13604v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-Aware%20Continual%20Test-Time%20Adaptation%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Jiayi%20Ni%20and%20Senqiao%20Yang%20and%20Ran%20Xu%20and%20Jiaming%20Liu%20and%20Xiaoqi%20Li%20and%20Wenyu%20Jiao%20and%20Zehui%20Chen%20and%20Yi%20Liu%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Since%20autonomous%20driving%20systems%20usually%20face%20dynamic%20and%20ever-changing%0Aenvironments%2C%20continual%20test-time%20adaptation%20%28CTTA%29%20has%20been%20proposed%20as%20a%0Astrategy%20for%20transferring%20deployed%20models%20to%20continually%20changing%20target%0Adomains.%20However%2C%20the%20pursuit%20of%20long-term%20adaptation%20often%20introduces%0Acatastrophic%20forgetting%20and%20error%20accumulation%20problems%2C%20which%20impede%20the%0Apractical%20implementation%20of%20CTTA%20in%20the%20real%20world.%20Recently%2C%20existing%20CTTA%0Amethods%20mainly%20focus%20on%20utilizing%20a%20majority%20of%20parameters%20to%20fit%20target%20domain%0Aknowledge%20through%20self-training.%20Unfortunately%2C%20these%20approaches%20often%20amplify%0Athe%20challenge%20of%20error%20accumulation%20due%20to%20noisy%20pseudo-labels%2C%20and%20pose%0Apractical%20limitations%20stemming%20from%20the%20heavy%20computational%20costs%20associated%0Awith%20entire%20model%20updates.%20In%20this%20paper%2C%20we%20propose%20a%20distribution-aware%0Atuning%20%28DAT%29%20method%20to%20make%20the%20semantic%20segmentation%20CTTA%20efficient%20and%0Apractical%20in%20real-world%20applications.%20DAT%20adaptively%20selects%20and%20updates%20two%0Asmall%20groups%20of%20trainable%20parameters%20based%20on%20data%20distribution%20during%20the%0Acontinual%20adaptation%20process%2C%20including%20domain-specific%20parameters%20%28DSP%29%20and%0Atask-relevant%20parameters%20%28TRP%29.%20Specifically%2C%20DSP%20exhibits%20sensitivity%20to%0Aoutputs%20with%20substantial%20distribution%20shifts%2C%20effectively%20mitigating%20the%0Aproblem%20of%20error%20accumulation.%20In%20contrast%2C%20TRP%20are%20allocated%20to%20positions%20that%0Aare%20responsive%20to%20outputs%20with%20minor%20distribution%20shifts%2C%20which%20are%20fine-tuned%0Ato%20avoid%20the%20catastrophic%20forgetting%20problem.%20In%20addition%2C%20since%20CTTA%20is%20a%0Atemporal%20task%2C%20we%20introduce%20the%20Parameter%20Accumulation%20Update%20%28PAU%29%20strategy%20to%0Acollect%20the%20updated%20DSP%20and%20TRP%20in%20target%20domain%20sequences.%20We%20conduct%0Aextensive%20experiments%20on%20two%20widely-used%20semantic%20segmentation%20CTTA%20benchmarks%2C%0Aachieving%20promising%20performance%20compared%20to%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13604v2&entry.124074799=Read"},
{"title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal\n  Dialogue Dataset", "author": "Young-Jun Lee and Byungsoo Ko and Han-Gyu Kim and Jonghwan Hyeon and Ho-Jin Choi", "abstract": "  As sharing images in an instant message is a crucial factor, there has been\nactive research on learning an image-text multi-modal dialogue models. However,\ntraining a well-generalized multi-modal dialogue model remains challenging due\nto the low quality and limited diversity of images per dialogue in existing\nmulti-modal dialogue datasets. In this paper, we propose an automated pipeline\nto construct a multi-modal dialogue dataset, ensuring both dialogue quality and\nimage diversity without requiring minimum human effort. In our pipeline, to\nguarantee the coherence between images and dialogue, we prompt GPT-4 to infer\npotential image-sharing moments - specifically, the utterance, speaker,\nrationale, and image description. Furthermore, we leverage CLIP similarity to\nmaintain consistency between aligned multiple images to the utterance. Through\nthis pipeline, we introduce DialogCC, a high-quality and diverse multi-modal\ndialogue dataset that surpasses existing datasets in terms of quality and\ndiversity in human evaluation. Our comprehensive experiments highlight that\nwhen multi-modal dialogue models are trained using our dataset, their\ngeneralization performance on unseen dialogue datasets is significantly\nenhanced. We make our source code and dataset publicly available.\n", "link": "http://arxiv.org/abs/2212.04119v2", "date": "2024-03-29", "relevancy": 2.1647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5845}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5188}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DialogCC%3A%20An%20Automated%20Pipeline%20for%20Creating%20High-Quality%20Multi-Modal%0A%20%20Dialogue%20Dataset&body=Title%3A%20DialogCC%3A%20An%20Automated%20Pipeline%20for%20Creating%20High-Quality%20Multi-Modal%0A%20%20Dialogue%20Dataset%0AAuthor%3A%20Young-Jun%20Lee%20and%20Byungsoo%20Ko%20and%20Han-Gyu%20Kim%20and%20Jonghwan%20Hyeon%20and%20Ho-Jin%20Choi%0AAbstract%3A%20%20%20As%20sharing%20images%20in%20an%20instant%20message%20is%20a%20crucial%20factor%2C%20there%20has%20been%0Aactive%20research%20on%20learning%20an%20image-text%20multi-modal%20dialogue%20models.%20However%2C%0Atraining%20a%20well-generalized%20multi-modal%20dialogue%20model%20remains%20challenging%20due%0Ato%20the%20low%20quality%20and%20limited%20diversity%20of%20images%20per%20dialogue%20in%20existing%0Amulti-modal%20dialogue%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20automated%20pipeline%0Ato%20construct%20a%20multi-modal%20dialogue%20dataset%2C%20ensuring%20both%20dialogue%20quality%20and%0Aimage%20diversity%20without%20requiring%20minimum%20human%20effort.%20In%20our%20pipeline%2C%20to%0Aguarantee%20the%20coherence%20between%20images%20and%20dialogue%2C%20we%20prompt%20GPT-4%20to%20infer%0Apotential%20image-sharing%20moments%20-%20specifically%2C%20the%20utterance%2C%20speaker%2C%0Arationale%2C%20and%20image%20description.%20Furthermore%2C%20we%20leverage%20CLIP%20similarity%20to%0Amaintain%20consistency%20between%20aligned%20multiple%20images%20to%20the%20utterance.%20Through%0Athis%20pipeline%2C%20we%20introduce%20DialogCC%2C%20a%20high-quality%20and%20diverse%20multi-modal%0Adialogue%20dataset%20that%20surpasses%20existing%20datasets%20in%20terms%20of%20quality%20and%0Adiversity%20in%20human%20evaluation.%20Our%20comprehensive%20experiments%20highlight%20that%0Awhen%20multi-modal%20dialogue%20models%20are%20trained%20using%20our%20dataset%2C%20their%0Ageneralization%20performance%20on%20unseen%20dialogue%20datasets%20is%20significantly%0Aenhanced.%20We%20make%20our%20source%20code%20and%20dataset%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.04119v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialogCC%3A%20An%20Automated%20Pipeline%20for%20Creating%20High-Quality%20Multi-Modal%0A%20%20Dialogue%20Dataset&entry.906535625=Young-Jun%20Lee%20and%20Byungsoo%20Ko%20and%20Han-Gyu%20Kim%20and%20Jonghwan%20Hyeon%20and%20Ho-Jin%20Choi&entry.1292438233=%20%20As%20sharing%20images%20in%20an%20instant%20message%20is%20a%20crucial%20factor%2C%20there%20has%20been%0Aactive%20research%20on%20learning%20an%20image-text%20multi-modal%20dialogue%20models.%20However%2C%0Atraining%20a%20well-generalized%20multi-modal%20dialogue%20model%20remains%20challenging%20due%0Ato%20the%20low%20quality%20and%20limited%20diversity%20of%20images%20per%20dialogue%20in%20existing%0Amulti-modal%20dialogue%20datasets.%20In%20this%20paper%2C%20we%20propose%20an%20automated%20pipeline%0Ato%20construct%20a%20multi-modal%20dialogue%20dataset%2C%20ensuring%20both%20dialogue%20quality%20and%0Aimage%20diversity%20without%20requiring%20minimum%20human%20effort.%20In%20our%20pipeline%2C%20to%0Aguarantee%20the%20coherence%20between%20images%20and%20dialogue%2C%20we%20prompt%20GPT-4%20to%20infer%0Apotential%20image-sharing%20moments%20-%20specifically%2C%20the%20utterance%2C%20speaker%2C%0Arationale%2C%20and%20image%20description.%20Furthermore%2C%20we%20leverage%20CLIP%20similarity%20to%0Amaintain%20consistency%20between%20aligned%20multiple%20images%20to%20the%20utterance.%20Through%0Athis%20pipeline%2C%20we%20introduce%20DialogCC%2C%20a%20high-quality%20and%20diverse%20multi-modal%0Adialogue%20dataset%20that%20surpasses%20existing%20datasets%20in%20terms%20of%20quality%20and%0Adiversity%20in%20human%20evaluation.%20Our%20comprehensive%20experiments%20highlight%20that%0Awhen%20multi-modal%20dialogue%20models%20are%20trained%20using%20our%20dataset%2C%20their%0Ageneralization%20performance%20on%20unseen%20dialogue%20datasets%20is%20significantly%0Aenhanced.%20We%20make%20our%20source%20code%20and%20dataset%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.04119v2&entry.124074799=Read"},
{"title": "Formal Verification of Robustness and Resilience of Learning-Enabled\n  State Estimation Systems", "author": "Wei Huang and Yifan Zhou and Gaojie Jin and Youcheng Sun and Jie Meng and Fan Zhang and Xiaowei Huang", "abstract": "  This paper presents a formal verification guided approach for a principled\ndesign and implementation of robust and resilient learning-enabled systems. We\nfocus on learning-enabled state estimation systems (LE-SESs), which have been\nwidely used in robotics applications to determine the current state (e.g.,\nlocation, speed, direction, etc.) of a complex system. The LE-SESs are\nnetworked systems, composed of a set of connected components including: Bayes\nfilters for state estimation, and neural networks for processing sensory input.\nWe study LE-SESs from the perspective of formal verification, which determines\nthe satisfiabilty of a system model against the specified properties. Over\nLE-SESs, we investigate two key properties -- robustness and resilience -- and\nprovide their formal definitions. To enable formal verification, we reduce the\nLE-SESs to a novel class of labelled transition systems, named {PO}^2-LTS in\nthe paper, and formally express the properties as constrained optimisation\nobjectives. We prove that the verification problems are NP-complete. Based on\n{PO}^2-LTS and the optimisation objectives, practical verification algorithms\nare developed to check the satisfiability of the properties on the LE-SESs. As\na major case study, we interrogate a real-world dynamic tracking system which\nuses a single Kalman Filter (KF) -- a special case of Bayes filter -- to\nlocalise and track a ground vehicle. Its perception system, based on\nconvolutional neural networks, processes a high-resolution Wide Area Motion\nImagery (WAMI) data stream. Experimental results show that our algorithms can\nnot only verify the properties of the WAMI tracking system but also provide\nrepresentative examples, the latter of which inspired us to take an enhanced\nLE-SESs design where runtime monitors or joint-KFs are required. Experimental\nresults confirm the improvement in the robustness of the enhanced design.\n", "link": "http://arxiv.org/abs/2010.08311v4", "date": "2024-03-29", "relevancy": 2.1598, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5801}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5531}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5108}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Formal%20Verification%20of%20Robustness%20and%20Resilience%20of%20Learning-Enabled%0A%20%20State%20Estimation%20Systems&body=Title%3A%20Formal%20Verification%20of%20Robustness%20and%20Resilience%20of%20Learning-Enabled%0A%20%20State%20Estimation%20Systems%0AAuthor%3A%20Wei%20Huang%20and%20Yifan%20Zhou%20and%20Gaojie%20Jin%20and%20Youcheng%20Sun%20and%20Jie%20Meng%20and%20Fan%20Zhang%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20formal%20verification%20guided%20approach%20for%20a%20principled%0Adesign%20and%20implementation%20of%20robust%20and%20resilient%20learning-enabled%20systems.%20We%0Afocus%20on%20learning-enabled%20state%20estimation%20systems%20%28LE-SESs%29%2C%20which%20have%20been%0Awidely%20used%20in%20robotics%20applications%20to%20determine%20the%20current%20state%20%28e.g.%2C%0Alocation%2C%20speed%2C%20direction%2C%20etc.%29%20of%20a%20complex%20system.%20The%20LE-SESs%20are%0Anetworked%20systems%2C%20composed%20of%20a%20set%20of%20connected%20components%20including%3A%20Bayes%0Afilters%20for%20state%20estimation%2C%20and%20neural%20networks%20for%20processing%20sensory%20input.%0AWe%20study%20LE-SESs%20from%20the%20perspective%20of%20formal%20verification%2C%20which%20determines%0Athe%20satisfiabilty%20of%20a%20system%20model%20against%20the%20specified%20properties.%20Over%0ALE-SESs%2C%20we%20investigate%20two%20key%20properties%20--%20robustness%20and%20resilience%20--%20and%0Aprovide%20their%20formal%20definitions.%20To%20enable%20formal%20verification%2C%20we%20reduce%20the%0ALE-SESs%20to%20a%20novel%20class%20of%20labelled%20transition%20systems%2C%20named%20%7BPO%7D%5E2-LTS%20in%0Athe%20paper%2C%20and%20formally%20express%20the%20properties%20as%20constrained%20optimisation%0Aobjectives.%20We%20prove%20that%20the%20verification%20problems%20are%20NP-complete.%20Based%20on%0A%7BPO%7D%5E2-LTS%20and%20the%20optimisation%20objectives%2C%20practical%20verification%20algorithms%0Aare%20developed%20to%20check%20the%20satisfiability%20of%20the%20properties%20on%20the%20LE-SESs.%20As%0Aa%20major%20case%20study%2C%20we%20interrogate%20a%20real-world%20dynamic%20tracking%20system%20which%0Auses%20a%20single%20Kalman%20Filter%20%28KF%29%20--%20a%20special%20case%20of%20Bayes%20filter%20--%20to%0Alocalise%20and%20track%20a%20ground%20vehicle.%20Its%20perception%20system%2C%20based%20on%0Aconvolutional%20neural%20networks%2C%20processes%20a%20high-resolution%20Wide%20Area%20Motion%0AImagery%20%28WAMI%29%20data%20stream.%20Experimental%20results%20show%20that%20our%20algorithms%20can%0Anot%20only%20verify%20the%20properties%20of%20the%20WAMI%20tracking%20system%20but%20also%20provide%0Arepresentative%20examples%2C%20the%20latter%20of%20which%20inspired%20us%20to%20take%20an%20enhanced%0ALE-SESs%20design%20where%20runtime%20monitors%20or%20joint-KFs%20are%20required.%20Experimental%0Aresults%20confirm%20the%20improvement%20in%20the%20robustness%20of%20the%20enhanced%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2010.08311v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Formal%20Verification%20of%20Robustness%20and%20Resilience%20of%20Learning-Enabled%0A%20%20State%20Estimation%20Systems&entry.906535625=Wei%20Huang%20and%20Yifan%20Zhou%20and%20Gaojie%20Jin%20and%20Youcheng%20Sun%20and%20Jie%20Meng%20and%20Fan%20Zhang%20and%20Xiaowei%20Huang&entry.1292438233=%20%20This%20paper%20presents%20a%20formal%20verification%20guided%20approach%20for%20a%20principled%0Adesign%20and%20implementation%20of%20robust%20and%20resilient%20learning-enabled%20systems.%20We%0Afocus%20on%20learning-enabled%20state%20estimation%20systems%20%28LE-SESs%29%2C%20which%20have%20been%0Awidely%20used%20in%20robotics%20applications%20to%20determine%20the%20current%20state%20%28e.g.%2C%0Alocation%2C%20speed%2C%20direction%2C%20etc.%29%20of%20a%20complex%20system.%20The%20LE-SESs%20are%0Anetworked%20systems%2C%20composed%20of%20a%20set%20of%20connected%20components%20including%3A%20Bayes%0Afilters%20for%20state%20estimation%2C%20and%20neural%20networks%20for%20processing%20sensory%20input.%0AWe%20study%20LE-SESs%20from%20the%20perspective%20of%20formal%20verification%2C%20which%20determines%0Athe%20satisfiabilty%20of%20a%20system%20model%20against%20the%20specified%20properties.%20Over%0ALE-SESs%2C%20we%20investigate%20two%20key%20properties%20--%20robustness%20and%20resilience%20--%20and%0Aprovide%20their%20formal%20definitions.%20To%20enable%20formal%20verification%2C%20we%20reduce%20the%0ALE-SESs%20to%20a%20novel%20class%20of%20labelled%20transition%20systems%2C%20named%20%7BPO%7D%5E2-LTS%20in%0Athe%20paper%2C%20and%20formally%20express%20the%20properties%20as%20constrained%20optimisation%0Aobjectives.%20We%20prove%20that%20the%20verification%20problems%20are%20NP-complete.%20Based%20on%0A%7BPO%7D%5E2-LTS%20and%20the%20optimisation%20objectives%2C%20practical%20verification%20algorithms%0Aare%20developed%20to%20check%20the%20satisfiability%20of%20the%20properties%20on%20the%20LE-SESs.%20As%0Aa%20major%20case%20study%2C%20we%20interrogate%20a%20real-world%20dynamic%20tracking%20system%20which%0Auses%20a%20single%20Kalman%20Filter%20%28KF%29%20--%20a%20special%20case%20of%20Bayes%20filter%20--%20to%0Alocalise%20and%20track%20a%20ground%20vehicle.%20Its%20perception%20system%2C%20based%20on%0Aconvolutional%20neural%20networks%2C%20processes%20a%20high-resolution%20Wide%20Area%20Motion%0AImagery%20%28WAMI%29%20data%20stream.%20Experimental%20results%20show%20that%20our%20algorithms%20can%0Anot%20only%20verify%20the%20properties%20of%20the%20WAMI%20tracking%20system%20but%20also%20provide%0Arepresentative%20examples%2C%20the%20latter%20of%20which%20inspired%20us%20to%20take%20an%20enhanced%0ALE-SESs%20design%20where%20runtime%20monitors%20or%20joint-KFs%20are%20required.%20Experimental%0Aresults%20confirm%20the%20improvement%20in%20the%20robustness%20of%20the%20enhanced%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2010.08311v4&entry.124074799=Read"},
{"title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models", "author": "Atsuyuki Miyai and Jingkang Yang and Jingyang Zhang and Yifei Ming and Qing Yu and Go Irie and Yixuan Li and Hai Li and Ziwei Liu and Kiyoharu Aizawa", "abstract": "  This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.\n", "link": "http://arxiv.org/abs/2403.20331v1", "date": "2024-03-29", "relevancy": 2.1595, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5669}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5468}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5221}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsolvable%20Problem%20Detection%3A%20Evaluating%20Trustworthiness%20of%20Vision%0A%20%20Language%20Models&body=Title%3A%20Unsolvable%20Problem%20Detection%3A%20Evaluating%20Trustworthiness%20of%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20and%20significant%20challenge%20for%20Vision%20Language%0AModels%20%28VLMs%29%2C%20termed%20Unsolvable%20Problem%20Detection%20%28UPD%29.%20UPD%20examines%20the%0AVLM%27s%20ability%20to%20withhold%20answers%20when%20faced%20with%20unsolvable%20problems%20in%20the%0Acontext%20of%20Visual%20Question%20Answering%20%28VQA%29%20tasks.%20UPD%20encompasses%20three%0Adistinct%20settings%3A%20Absent%20Answer%20Detection%20%28AAD%29%2C%20Incompatible%20Answer%20Set%0ADetection%20%28IASD%29%2C%20and%20Incompatible%20Visual%20Question%20Detection%20%28IVQD%29.%20To%20deeply%0Ainvestigate%20the%20UPD%20problem%2C%20extensive%20experiments%20indicate%20that%20most%20VLMs%2C%0Aincluding%20GPT-4V%20and%20LLaVA-Next-34B%2C%20struggle%20with%20our%20benchmarks%20to%20varying%0Aextents%2C%20highlighting%20significant%20room%20for%20the%20improvements.%20To%20address%20UPD%2C%20we%0Aexplore%20both%20training-free%20and%20training-based%20solutions%2C%20offering%20new%20insights%0Ainto%20their%20effectiveness%20and%20limitations.%20We%20hope%20our%20insights%2C%20together%20with%0Afuture%20efforts%20within%20the%20proposed%20UPD%20settings%2C%20will%20enhance%20the%20broader%0Aunderstanding%20and%20development%20of%20more%20practical%20and%20reliable%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20331v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsolvable%20Problem%20Detection%3A%20Evaluating%20Trustworthiness%20of%20Vision%0A%20%20Language%20Models&entry.906535625=Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20and%20significant%20challenge%20for%20Vision%20Language%0AModels%20%28VLMs%29%2C%20termed%20Unsolvable%20Problem%20Detection%20%28UPD%29.%20UPD%20examines%20the%0AVLM%27s%20ability%20to%20withhold%20answers%20when%20faced%20with%20unsolvable%20problems%20in%20the%0Acontext%20of%20Visual%20Question%20Answering%20%28VQA%29%20tasks.%20UPD%20encompasses%20three%0Adistinct%20settings%3A%20Absent%20Answer%20Detection%20%28AAD%29%2C%20Incompatible%20Answer%20Set%0ADetection%20%28IASD%29%2C%20and%20Incompatible%20Visual%20Question%20Detection%20%28IVQD%29.%20To%20deeply%0Ainvestigate%20the%20UPD%20problem%2C%20extensive%20experiments%20indicate%20that%20most%20VLMs%2C%0Aincluding%20GPT-4V%20and%20LLaVA-Next-34B%2C%20struggle%20with%20our%20benchmarks%20to%20varying%0Aextents%2C%20highlighting%20significant%20room%20for%20the%20improvements.%20To%20address%20UPD%2C%20we%0Aexplore%20both%20training-free%20and%20training-based%20solutions%2C%20offering%20new%20insights%0Ainto%20their%20effectiveness%20and%20limitations.%20We%20hope%20our%20insights%2C%20together%20with%0Afuture%20efforts%20within%20the%20proposed%20UPD%20settings%2C%20will%20enhance%20the%20broader%0Aunderstanding%20and%20development%20of%20more%20practical%20and%20reliable%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20331v1&entry.124074799=Read"},
{"title": "MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark", "author": "Sanghyun Woo and Kwanyong Park and Inkyu Shin and Myungchul Kim and In So Kweon", "abstract": "  Multi-target multi-camera tracking is a crucial task that involves\nidentifying and tracking individuals over time using video streams from\nmultiple cameras. This task has practical applications in various fields, such\nas visual surveillance, crowd behavior analysis, and anomaly detection.\nHowever, due to the difficulty and cost of collecting and labeling data,\nexisting datasets for this task are either synthetically generated or\nartificially constructed within a controlled camera network setting, which\nlimits their ability to model real-world dynamics and generalize to diverse\ncamera configurations. To address this issue, we present MTMMC, a real-world,\nlarge-scale dataset that includes long video sequences captured by 16\nmulti-modal cameras in two different environments - campus and factory - across\nvarious time, weather, and season conditions. This dataset provides a\nchallenging test-bed for studying multi-camera tracking under diverse\nreal-world complexities and includes an additional input modality of spatially\naligned and temporally synchronized RGB and thermal cameras, which enhances the\naccuracy of multi-camera tracking. MTMMC is a super-set of existing datasets,\nbenefiting independent fields such as person detection, re-identification, and\nmultiple object tracking. We provide baselines and new learning setups on this\ndataset and set the reference scores for future studies. The datasets, models,\nand test server will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.20225v1", "date": "2024-03-29", "relevancy": 2.158, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MTMMC%3A%20A%20Large-Scale%20Real-World%20Multi-Modal%20Camera%20Tracking%20Benchmark&body=Title%3A%20MTMMC%3A%20A%20Large-Scale%20Real-World%20Multi-Modal%20Camera%20Tracking%20Benchmark%0AAuthor%3A%20Sanghyun%20Woo%20and%20Kwanyong%20Park%20and%20Inkyu%20Shin%20and%20Myungchul%20Kim%20and%20In%20So%20Kweon%0AAbstract%3A%20%20%20Multi-target%20multi-camera%20tracking%20is%20a%20crucial%20task%20that%20involves%0Aidentifying%20and%20tracking%20individuals%20over%20time%20using%20video%20streams%20from%0Amultiple%20cameras.%20This%20task%20has%20practical%20applications%20in%20various%20fields%2C%20such%0Aas%20visual%20surveillance%2C%20crowd%20behavior%20analysis%2C%20and%20anomaly%20detection.%0AHowever%2C%20due%20to%20the%20difficulty%20and%20cost%20of%20collecting%20and%20labeling%20data%2C%0Aexisting%20datasets%20for%20this%20task%20are%20either%20synthetically%20generated%20or%0Aartificially%20constructed%20within%20a%20controlled%20camera%20network%20setting%2C%20which%0Alimits%20their%20ability%20to%20model%20real-world%20dynamics%20and%20generalize%20to%20diverse%0Acamera%20configurations.%20To%20address%20this%20issue%2C%20we%20present%20MTMMC%2C%20a%20real-world%2C%0Alarge-scale%20dataset%20that%20includes%20long%20video%20sequences%20captured%20by%2016%0Amulti-modal%20cameras%20in%20two%20different%20environments%20-%20campus%20and%20factory%20-%20across%0Avarious%20time%2C%20weather%2C%20and%20season%20conditions.%20This%20dataset%20provides%20a%0Achallenging%20test-bed%20for%20studying%20multi-camera%20tracking%20under%20diverse%0Areal-world%20complexities%20and%20includes%20an%20additional%20input%20modality%20of%20spatially%0Aaligned%20and%20temporally%20synchronized%20RGB%20and%20thermal%20cameras%2C%20which%20enhances%20the%0Aaccuracy%20of%20multi-camera%20tracking.%20MTMMC%20is%20a%20super-set%20of%20existing%20datasets%2C%0Abenefiting%20independent%20fields%20such%20as%20person%20detection%2C%20re-identification%2C%20and%0Amultiple%20object%20tracking.%20We%20provide%20baselines%20and%20new%20learning%20setups%20on%20this%0Adataset%20and%20set%20the%20reference%20scores%20for%20future%20studies.%20The%20datasets%2C%20models%2C%0Aand%20test%20server%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20225v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTMMC%3A%20A%20Large-Scale%20Real-World%20Multi-Modal%20Camera%20Tracking%20Benchmark&entry.906535625=Sanghyun%20Woo%20and%20Kwanyong%20Park%20and%20Inkyu%20Shin%20and%20Myungchul%20Kim%20and%20In%20So%20Kweon&entry.1292438233=%20%20Multi-target%20multi-camera%20tracking%20is%20a%20crucial%20task%20that%20involves%0Aidentifying%20and%20tracking%20individuals%20over%20time%20using%20video%20streams%20from%0Amultiple%20cameras.%20This%20task%20has%20practical%20applications%20in%20various%20fields%2C%20such%0Aas%20visual%20surveillance%2C%20crowd%20behavior%20analysis%2C%20and%20anomaly%20detection.%0AHowever%2C%20due%20to%20the%20difficulty%20and%20cost%20of%20collecting%20and%20labeling%20data%2C%0Aexisting%20datasets%20for%20this%20task%20are%20either%20synthetically%20generated%20or%0Aartificially%20constructed%20within%20a%20controlled%20camera%20network%20setting%2C%20which%0Alimits%20their%20ability%20to%20model%20real-world%20dynamics%20and%20generalize%20to%20diverse%0Acamera%20configurations.%20To%20address%20this%20issue%2C%20we%20present%20MTMMC%2C%20a%20real-world%2C%0Alarge-scale%20dataset%20that%20includes%20long%20video%20sequences%20captured%20by%2016%0Amulti-modal%20cameras%20in%20two%20different%20environments%20-%20campus%20and%20factory%20-%20across%0Avarious%20time%2C%20weather%2C%20and%20season%20conditions.%20This%20dataset%20provides%20a%0Achallenging%20test-bed%20for%20studying%20multi-camera%20tracking%20under%20diverse%0Areal-world%20complexities%20and%20includes%20an%20additional%20input%20modality%20of%20spatially%0Aaligned%20and%20temporally%20synchronized%20RGB%20and%20thermal%20cameras%2C%20which%20enhances%20the%0Aaccuracy%20of%20multi-camera%20tracking.%20MTMMC%20is%20a%20super-set%20of%20existing%20datasets%2C%0Abenefiting%20independent%20fields%20such%20as%20person%20detection%2C%20re-identification%2C%20and%0Amultiple%20object%20tracking.%20We%20provide%20baselines%20and%20new%20learning%20setups%20on%20this%0Adataset%20and%20set%20the%20reference%20scores%20for%20future%20studies.%20The%20datasets%2C%20models%2C%0Aand%20test%20server%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20225v1&entry.124074799=Read"},
{"title": "HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on\n  Bidirectional Selective SSM", "author": "Shuangjian Li and Tao Zhu and Furong Duan and Liming Chen and Huansheng Ning and Yaping Wan", "abstract": "  Wearable sensor human activity recognition (HAR) is a crucial area of\nresearch in activity sensing. While transformer-based temporal deep learning\nmodels have been extensively studied and implemented, their large number of\nparameters present significant challenges in terms of system computing load and\nmemory usage, rendering them unsuitable for real-time mobile activity\nrecognition applications. Recently, an efficient hardware-aware state space\nmodel (SSM) called Mamba has emerged as a promising alternative. Mamba\ndemonstrates strong potential in long sequence modeling, boasts a simpler\nnetwork architecture, and offers an efficient hardware-aware design. Leveraging\nSSM for activity recognition represents an appealing avenue for exploration. In\nthis study, we introduce HARMamba, which employs a more lightweight selective\nSSM as the foundational model architecture for activity recognition. The goal\nis to address the computational resource constraints encountered in real-time\nactivity recognition scenarios. Our approach involves processing sensor data\nflow by independently learning each channel and segmenting the data into\n\"patches\". The marked sensor sequence's position embedding serves as the input\ntoken for the bidirectional state space model, ultimately leading to activity\ncategorization through the classification head. Compared to established\nactivity recognition frameworks like Transformer-based models, HARMamba\nachieves superior performance while also reducing computational and memory\noverhead. Furthermore, our proposed method has been extensively tested on four\npublic activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating\nimpressive performance in activity recognition tasks.\n", "link": "http://arxiv.org/abs/2403.20183v1", "date": "2024-03-29", "relevancy": 2.1526, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5275}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5125}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HARMamba%3A%20Efficient%20Wearable%20Sensor%20Human%20Activity%20Recognition%20Based%20on%0A%20%20Bidirectional%20Selective%20SSM&body=Title%3A%20HARMamba%3A%20Efficient%20Wearable%20Sensor%20Human%20Activity%20Recognition%20Based%20on%0A%20%20Bidirectional%20Selective%20SSM%0AAuthor%3A%20Shuangjian%20Li%20and%20Tao%20Zhu%20and%20Furong%20Duan%20and%20Liming%20Chen%20and%20Huansheng%20Ning%20and%20Yaping%20Wan%0AAbstract%3A%20%20%20Wearable%20sensor%20human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%0Aresearch%20in%20activity%20sensing.%20While%20transformer-based%20temporal%20deep%20learning%0Amodels%20have%20been%20extensively%20studied%20and%20implemented%2C%20their%20large%20number%20of%0Aparameters%20present%20significant%20challenges%20in%20terms%20of%20system%20computing%20load%20and%0Amemory%20usage%2C%20rendering%20them%20unsuitable%20for%20real-time%20mobile%20activity%0Arecognition%20applications.%20Recently%2C%20an%20efficient%20hardware-aware%20state%20space%0Amodel%20%28SSM%29%20called%20Mamba%20has%20emerged%20as%20a%20promising%20alternative.%20Mamba%0Ademonstrates%20strong%20potential%20in%20long%20sequence%20modeling%2C%20boasts%20a%20simpler%0Anetwork%20architecture%2C%20and%20offers%20an%20efficient%20hardware-aware%20design.%20Leveraging%0ASSM%20for%20activity%20recognition%20represents%20an%20appealing%20avenue%20for%20exploration.%20In%0Athis%20study%2C%20we%20introduce%20HARMamba%2C%20which%20employs%20a%20more%20lightweight%20selective%0ASSM%20as%20the%20foundational%20model%20architecture%20for%20activity%20recognition.%20The%20goal%0Ais%20to%20address%20the%20computational%20resource%20constraints%20encountered%20in%20real-time%0Aactivity%20recognition%20scenarios.%20Our%20approach%20involves%20processing%20sensor%20data%0Aflow%20by%20independently%20learning%20each%20channel%20and%20segmenting%20the%20data%20into%0A%22patches%22.%20The%20marked%20sensor%20sequence%27s%20position%20embedding%20serves%20as%20the%20input%0Atoken%20for%20the%20bidirectional%20state%20space%20model%2C%20ultimately%20leading%20to%20activity%0Acategorization%20through%20the%20classification%20head.%20Compared%20to%20established%0Aactivity%20recognition%20frameworks%20like%20Transformer-based%20models%2C%20HARMamba%0Aachieves%20superior%20performance%20while%20also%20reducing%20computational%20and%20memory%0Aoverhead.%20Furthermore%2C%20our%20proposed%20method%20has%20been%20extensively%20tested%20on%20four%0Apublic%20activity%20datasets%3A%20PAMAP2%2C%20WISDM%2C%20UNIMIB%2C%20and%20UCI%2C%20demonstrating%0Aimpressive%20performance%20in%20activity%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20183v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARMamba%3A%20Efficient%20Wearable%20Sensor%20Human%20Activity%20Recognition%20Based%20on%0A%20%20Bidirectional%20Selective%20SSM&entry.906535625=Shuangjian%20Li%20and%20Tao%20Zhu%20and%20Furong%20Duan%20and%20Liming%20Chen%20and%20Huansheng%20Ning%20and%20Yaping%20Wan&entry.1292438233=%20%20Wearable%20sensor%20human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%0Aresearch%20in%20activity%20sensing.%20While%20transformer-based%20temporal%20deep%20learning%0Amodels%20have%20been%20extensively%20studied%20and%20implemented%2C%20their%20large%20number%20of%0Aparameters%20present%20significant%20challenges%20in%20terms%20of%20system%20computing%20load%20and%0Amemory%20usage%2C%20rendering%20them%20unsuitable%20for%20real-time%20mobile%20activity%0Arecognition%20applications.%20Recently%2C%20an%20efficient%20hardware-aware%20state%20space%0Amodel%20%28SSM%29%20called%20Mamba%20has%20emerged%20as%20a%20promising%20alternative.%20Mamba%0Ademonstrates%20strong%20potential%20in%20long%20sequence%20modeling%2C%20boasts%20a%20simpler%0Anetwork%20architecture%2C%20and%20offers%20an%20efficient%20hardware-aware%20design.%20Leveraging%0ASSM%20for%20activity%20recognition%20represents%20an%20appealing%20avenue%20for%20exploration.%20In%0Athis%20study%2C%20we%20introduce%20HARMamba%2C%20which%20employs%20a%20more%20lightweight%20selective%0ASSM%20as%20the%20foundational%20model%20architecture%20for%20activity%20recognition.%20The%20goal%0Ais%20to%20address%20the%20computational%20resource%20constraints%20encountered%20in%20real-time%0Aactivity%20recognition%20scenarios.%20Our%20approach%20involves%20processing%20sensor%20data%0Aflow%20by%20independently%20learning%20each%20channel%20and%20segmenting%20the%20data%20into%0A%22patches%22.%20The%20marked%20sensor%20sequence%27s%20position%20embedding%20serves%20as%20the%20input%0Atoken%20for%20the%20bidirectional%20state%20space%20model%2C%20ultimately%20leading%20to%20activity%0Acategorization%20through%20the%20classification%20head.%20Compared%20to%20established%0Aactivity%20recognition%20frameworks%20like%20Transformer-based%20models%2C%20HARMamba%0Aachieves%20superior%20performance%20while%20also%20reducing%20computational%20and%20memory%0Aoverhead.%20Furthermore%2C%20our%20proposed%20method%20has%20been%20extensively%20tested%20on%20four%0Apublic%20activity%20datasets%3A%20PAMAP2%2C%20WISDM%2C%20UNIMIB%2C%20and%20UCI%2C%20demonstrating%0Aimpressive%20performance%20in%20activity%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20183v1&entry.124074799=Read"},
{"title": "Enhancing Dimension-Reduced Scatter Plots with Class and Feature\n  Centroids", "author": "Daniel B. Hier and Tayo Obafemi-Ajayi and Gayla R. Olbricht and Devin M. Burns and Sasha Petrenko and Donald C. Wunsch II", "abstract": "  Dimension reduction is increasingly applied to high-dimensional biomedical\ndata to improve its interpretability. When datasets are reduced to two\ndimensions, each observation is assigned an x and y coordinates and is\nrepresented as a point on a scatter plot. A significant challenge lies in\ninterpreting the meaning of the x and y axes due to the complexities inherent\nin dimension reduction. This study addresses this challenge by using the x and\ny coordinates derived from dimension reduction to calculate class and feature\ncentroids, which can be overlaid onto the scatter plots. This method connects\nthe low-dimension space to the original high-dimensional space. We illustrate\nthe utility of this approach with data derived from the phenotypes of three\nneurogenetic diseases and demonstrate how the addition of class and feature\ncentroids increases the interpretability of scatter plots.\n", "link": "http://arxiv.org/abs/2403.20246v1", "date": "2024-03-29", "relevancy": 2.1449, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4483}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4188}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Dimension-Reduced%20Scatter%20Plots%20with%20Class%20and%20Feature%0A%20%20Centroids&body=Title%3A%20Enhancing%20Dimension-Reduced%20Scatter%20Plots%20with%20Class%20and%20Feature%0A%20%20Centroids%0AAuthor%3A%20Daniel%20B.%20Hier%20and%20Tayo%20Obafemi-Ajayi%20and%20Gayla%20R.%20Olbricht%20and%20Devin%20M.%20Burns%20and%20Sasha%20Petrenko%20and%20Donald%20C.%20Wunsch%20II%0AAbstract%3A%20%20%20Dimension%20reduction%20is%20increasingly%20applied%20to%20high-dimensional%20biomedical%0Adata%20to%20improve%20its%20interpretability.%20When%20datasets%20are%20reduced%20to%20two%0Adimensions%2C%20each%20observation%20is%20assigned%20an%20x%20and%20y%20coordinates%20and%20is%0Arepresented%20as%20a%20point%20on%20a%20scatter%20plot.%20A%20significant%20challenge%20lies%20in%0Ainterpreting%20the%20meaning%20of%20the%20x%20and%20y%20axes%20due%20to%20the%20complexities%20inherent%0Ain%20dimension%20reduction.%20This%20study%20addresses%20this%20challenge%20by%20using%20the%20x%20and%0Ay%20coordinates%20derived%20from%20dimension%20reduction%20to%20calculate%20class%20and%20feature%0Acentroids%2C%20which%20can%20be%20overlaid%20onto%20the%20scatter%20plots.%20This%20method%20connects%0Athe%20low-dimension%20space%20to%20the%20original%20high-dimensional%20space.%20We%20illustrate%0Athe%20utility%20of%20this%20approach%20with%20data%20derived%20from%20the%20phenotypes%20of%20three%0Aneurogenetic%20diseases%20and%20demonstrate%20how%20the%20addition%20of%20class%20and%20feature%0Acentroids%20increases%20the%20interpretability%20of%20scatter%20plots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20246v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Dimension-Reduced%20Scatter%20Plots%20with%20Class%20and%20Feature%0A%20%20Centroids&entry.906535625=Daniel%20B.%20Hier%20and%20Tayo%20Obafemi-Ajayi%20and%20Gayla%20R.%20Olbricht%20and%20Devin%20M.%20Burns%20and%20Sasha%20Petrenko%20and%20Donald%20C.%20Wunsch%20II&entry.1292438233=%20%20Dimension%20reduction%20is%20increasingly%20applied%20to%20high-dimensional%20biomedical%0Adata%20to%20improve%20its%20interpretability.%20When%20datasets%20are%20reduced%20to%20two%0Adimensions%2C%20each%20observation%20is%20assigned%20an%20x%20and%20y%20coordinates%20and%20is%0Arepresented%20as%20a%20point%20on%20a%20scatter%20plot.%20A%20significant%20challenge%20lies%20in%0Ainterpreting%20the%20meaning%20of%20the%20x%20and%20y%20axes%20due%20to%20the%20complexities%20inherent%0Ain%20dimension%20reduction.%20This%20study%20addresses%20this%20challenge%20by%20using%20the%20x%20and%0Ay%20coordinates%20derived%20from%20dimension%20reduction%20to%20calculate%20class%20and%20feature%0Acentroids%2C%20which%20can%20be%20overlaid%20onto%20the%20scatter%20plots.%20This%20method%20connects%0Athe%20low-dimension%20space%20to%20the%20original%20high-dimensional%20space.%20We%20illustrate%0Athe%20utility%20of%20this%20approach%20with%20data%20derived%20from%20the%20phenotypes%20of%20three%0Aneurogenetic%20diseases%20and%20demonstrate%20how%20the%20addition%20of%20class%20and%20feature%0Acentroids%20increases%20the%20interpretability%20of%20scatter%20plots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20246v1&entry.124074799=Read"},
{"title": "Are We on the Right Way for Evaluating Large Vision-Language Models?", "author": "Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Jiaqi Wang and Yu Qiao and Dahua Lin and Feng Zhao", "abstract": "  Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 20% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.\n", "link": "http://arxiv.org/abs/2403.20330v1", "date": "2024-03-29", "relevancy": 2.1442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5276}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F&body=Title%3A%20Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F%0AAuthor%3A%20Lin%20Chen%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Jiaqi%20Wang%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20recently%20achieved%20rapid%20progress%2C%0Asparking%20numerous%20studies%20to%20evaluate%20their%20multi-modal%20capabilities.%20However%2C%0Awe%20dig%20into%20current%20evaluation%20works%20and%20identify%20two%20primary%20issues%3A%201%29%20Visual%0Acontent%20is%20unnecessary%20for%20many%20samples.%20The%20answers%20can%20be%20directly%20inferred%0Afrom%20the%20questions%20and%20options%2C%20or%20the%20world%20knowledge%20embedded%20in%20LLMs.%20This%0Aphenomenon%20is%20prevalent%20across%20current%20benchmarks.%20For%20instance%2C%20GeminiPro%0Aachieves%2042.9%25%20on%20the%20MMMU%20benchmark%20without%20any%20visual%20input%2C%20and%20outperforms%0Athe%20random%20choice%20baseline%20across%20six%20benchmarks%20over%2020%25%20on%20average.%202%29%0AUnintentional%20data%20leakage%20exists%20in%20LLM%20and%20LVLM%20training.%20LLM%20and%20LVLM%20could%0Astill%20answer%20some%20visual-necessary%20questions%20without%20visual%20content%2C%20indicating%0Athe%20memorizing%20of%20these%20samples%20within%20large-scale%20training%20data.%20For%20example%2C%0ASphinx-X-MoE%20gets%2043.6%25%20on%20MMMU%20without%20accessing%20images%2C%20surpassing%20its%20LLM%0Abackbone%20with%2017.9%25.%20Both%20problems%20lead%20to%20misjudgments%20of%20actual%20multi-modal%0Agains%20and%20potentially%20misguide%20the%20study%20of%20LVLM.%20To%20this%20end%2C%20we%20present%0AMMStar%2C%20an%20elite%20vision-indispensable%20multi-modal%20benchmark%20comprising%201%2C500%0Asamples%20meticulously%20selected%20by%20humans.%20MMStar%20benchmarks%206%20core%20capabilities%0Aand%2018%20detailed%20axes%2C%20aiming%20to%20evaluate%20LVLMs%27%20multi-modal%20capacities%20with%0Acarefully%20balanced%20and%20purified%20samples.%20These%20samples%20are%20first%20roughly%0Aselected%20from%20current%20benchmarks%20with%20an%20automated%20pipeline%2C%20human%20review%20is%0Athen%20involved%20to%20ensure%20each%20curated%20sample%20exhibits%20visual%20dependency%2C%20minimal%0Adata%20leakage%2C%20and%20requires%20advanced%20multi-modal%20capabilities.%20Moreover%2C%20two%0Ametrics%20are%20developed%20to%20measure%20data%20leakage%20and%20actual%20performance%20gain%20in%0Amulti-modal%20training.%20We%20evaluate%2016%20leading%20LVLMs%20on%20MMStar%20to%20assess%20their%0Amulti-modal%20capabilities%2C%20and%20on%207%20benchmarks%20with%20the%20proposed%20metrics%20to%0Ainvestigate%20their%20data%20leakage%20and%20actual%20multi-modal%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20330v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20on%20the%20Right%20Way%20for%20Evaluating%20Large%20Vision-Language%20Models%3F&entry.906535625=Lin%20Chen%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Jiaqi%20Wang%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20recently%20achieved%20rapid%20progress%2C%0Asparking%20numerous%20studies%20to%20evaluate%20their%20multi-modal%20capabilities.%20However%2C%0Awe%20dig%20into%20current%20evaluation%20works%20and%20identify%20two%20primary%20issues%3A%201%29%20Visual%0Acontent%20is%20unnecessary%20for%20many%20samples.%20The%20answers%20can%20be%20directly%20inferred%0Afrom%20the%20questions%20and%20options%2C%20or%20the%20world%20knowledge%20embedded%20in%20LLMs.%20This%0Aphenomenon%20is%20prevalent%20across%20current%20benchmarks.%20For%20instance%2C%20GeminiPro%0Aachieves%2042.9%25%20on%20the%20MMMU%20benchmark%20without%20any%20visual%20input%2C%20and%20outperforms%0Athe%20random%20choice%20baseline%20across%20six%20benchmarks%20over%2020%25%20on%20average.%202%29%0AUnintentional%20data%20leakage%20exists%20in%20LLM%20and%20LVLM%20training.%20LLM%20and%20LVLM%20could%0Astill%20answer%20some%20visual-necessary%20questions%20without%20visual%20content%2C%20indicating%0Athe%20memorizing%20of%20these%20samples%20within%20large-scale%20training%20data.%20For%20example%2C%0ASphinx-X-MoE%20gets%2043.6%25%20on%20MMMU%20without%20accessing%20images%2C%20surpassing%20its%20LLM%0Abackbone%20with%2017.9%25.%20Both%20problems%20lead%20to%20misjudgments%20of%20actual%20multi-modal%0Agains%20and%20potentially%20misguide%20the%20study%20of%20LVLM.%20To%20this%20end%2C%20we%20present%0AMMStar%2C%20an%20elite%20vision-indispensable%20multi-modal%20benchmark%20comprising%201%2C500%0Asamples%20meticulously%20selected%20by%20humans.%20MMStar%20benchmarks%206%20core%20capabilities%0Aand%2018%20detailed%20axes%2C%20aiming%20to%20evaluate%20LVLMs%27%20multi-modal%20capacities%20with%0Acarefully%20balanced%20and%20purified%20samples.%20These%20samples%20are%20first%20roughly%0Aselected%20from%20current%20benchmarks%20with%20an%20automated%20pipeline%2C%20human%20review%20is%0Athen%20involved%20to%20ensure%20each%20curated%20sample%20exhibits%20visual%20dependency%2C%20minimal%0Adata%20leakage%2C%20and%20requires%20advanced%20multi-modal%20capabilities.%20Moreover%2C%20two%0Ametrics%20are%20developed%20to%20measure%20data%20leakage%20and%20actual%20performance%20gain%20in%0Amulti-modal%20training.%20We%20evaluate%2016%20leading%20LVLMs%20on%20MMStar%20to%20assess%20their%0Amulti-modal%20capabilities%2C%20and%20on%207%20benchmarks%20with%20the%20proposed%20metrics%20to%0Ainvestigate%20their%20data%20leakage%20and%20actual%20multi-modal%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20330v1&entry.124074799=Read"},
{"title": "Rethinking Multi-view Representation Learning via Distilled\n  Disentangling", "author": "Guanzhou Ke and Bo Wang and Xiaoli Wang and Shengfeng He", "abstract": "  Multi-view representation learning aims to derive robust representations that\nare both view-consistent and view-specific from diverse data sources. This\npaper presents an in-depth analysis of existing approaches in this domain,\nhighlighting a commonly overlooked aspect: the redundancy between\nview-consistent and view-specific representations. To this end, we propose an\ninnovative framework for multi-view representation learning, which incorporates\na technique we term 'distilled disentangling'. Our method introduces the\nconcept of masked cross-view prediction, enabling the extraction of compact,\nhigh-quality view-consistent representations from various sources without\nincurring extra computational overhead. Additionally, we develop a distilled\ndisentangling module that efficiently filters out consistency-related\ninformation from multi-view representations, resulting in purer view-specific\nrepresentations. This approach significantly reduces redundancy between\nview-consistent and view-specific representations, enhancing the overall\nefficiency of the learning process. Our empirical evaluations reveal that\nhigher mask ratios substantially improve the quality of view-consistent\nrepresentations. Moreover, we find that reducing the dimensionality of\nview-consistent representations relative to that of view-specific\nrepresentations further refines the quality of the combined representations.\nOur code is accessible at: https://github.com/Guanzhou-Ke/MRDD.\n", "link": "http://arxiv.org/abs/2403.10897v2", "date": "2024-03-29", "relevancy": 2.1355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5165}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Multi-view%20Representation%20Learning%20via%20Distilled%0A%20%20Disentangling&body=Title%3A%20Rethinking%20Multi-view%20Representation%20Learning%20via%20Distilled%0A%20%20Disentangling%0AAuthor%3A%20Guanzhou%20Ke%20and%20Bo%20Wang%20and%20Xiaoli%20Wang%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Multi-view%20representation%20learning%20aims%20to%20derive%20robust%20representations%20that%0Aare%20both%20view-consistent%20and%20view-specific%20from%20diverse%20data%20sources.%20This%0Apaper%20presents%20an%20in-depth%20analysis%20of%20existing%20approaches%20in%20this%20domain%2C%0Ahighlighting%20a%20commonly%20overlooked%20aspect%3A%20the%20redundancy%20between%0Aview-consistent%20and%20view-specific%20representations.%20To%20this%20end%2C%20we%20propose%20an%0Ainnovative%20framework%20for%20multi-view%20representation%20learning%2C%20which%20incorporates%0Aa%20technique%20we%20term%20%27distilled%20disentangling%27.%20Our%20method%20introduces%20the%0Aconcept%20of%20masked%20cross-view%20prediction%2C%20enabling%20the%20extraction%20of%20compact%2C%0Ahigh-quality%20view-consistent%20representations%20from%20various%20sources%20without%0Aincurring%20extra%20computational%20overhead.%20Additionally%2C%20we%20develop%20a%20distilled%0Adisentangling%20module%20that%20efficiently%20filters%20out%20consistency-related%0Ainformation%20from%20multi-view%20representations%2C%20resulting%20in%20purer%20view-specific%0Arepresentations.%20This%20approach%20significantly%20reduces%20redundancy%20between%0Aview-consistent%20and%20view-specific%20representations%2C%20enhancing%20the%20overall%0Aefficiency%20of%20the%20learning%20process.%20Our%20empirical%20evaluations%20reveal%20that%0Ahigher%20mask%20ratios%20substantially%20improve%20the%20quality%20of%20view-consistent%0Arepresentations.%20Moreover%2C%20we%20find%20that%20reducing%20the%20dimensionality%20of%0Aview-consistent%20representations%20relative%20to%20that%20of%20view-specific%0Arepresentations%20further%20refines%20the%20quality%20of%20the%20combined%20representations.%0AOur%20code%20is%20accessible%20at%3A%20https%3A//github.com/Guanzhou-Ke/MRDD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10897v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Multi-view%20Representation%20Learning%20via%20Distilled%0A%20%20Disentangling&entry.906535625=Guanzhou%20Ke%20and%20Bo%20Wang%20and%20Xiaoli%20Wang%20and%20Shengfeng%20He&entry.1292438233=%20%20Multi-view%20representation%20learning%20aims%20to%20derive%20robust%20representations%20that%0Aare%20both%20view-consistent%20and%20view-specific%20from%20diverse%20data%20sources.%20This%0Apaper%20presents%20an%20in-depth%20analysis%20of%20existing%20approaches%20in%20this%20domain%2C%0Ahighlighting%20a%20commonly%20overlooked%20aspect%3A%20the%20redundancy%20between%0Aview-consistent%20and%20view-specific%20representations.%20To%20this%20end%2C%20we%20propose%20an%0Ainnovative%20framework%20for%20multi-view%20representation%20learning%2C%20which%20incorporates%0Aa%20technique%20we%20term%20%27distilled%20disentangling%27.%20Our%20method%20introduces%20the%0Aconcept%20of%20masked%20cross-view%20prediction%2C%20enabling%20the%20extraction%20of%20compact%2C%0Ahigh-quality%20view-consistent%20representations%20from%20various%20sources%20without%0Aincurring%20extra%20computational%20overhead.%20Additionally%2C%20we%20develop%20a%20distilled%0Adisentangling%20module%20that%20efficiently%20filters%20out%20consistency-related%0Ainformation%20from%20multi-view%20representations%2C%20resulting%20in%20purer%20view-specific%0Arepresentations.%20This%20approach%20significantly%20reduces%20redundancy%20between%0Aview-consistent%20and%20view-specific%20representations%2C%20enhancing%20the%20overall%0Aefficiency%20of%20the%20learning%20process.%20Our%20empirical%20evaluations%20reveal%20that%0Ahigher%20mask%20ratios%20substantially%20improve%20the%20quality%20of%20view-consistent%0Arepresentations.%20Moreover%2C%20we%20find%20that%20reducing%20the%20dimensionality%20of%0Aview-consistent%20representations%20relative%20to%20that%20of%20view-specific%0Arepresentations%20further%20refines%20the%20quality%20of%20the%20combined%20representations.%0AOur%20code%20is%20accessible%20at%3A%20https%3A//github.com/Guanzhou-Ke/MRDD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10897v2&entry.124074799=Read"},
{"title": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation", "author": "Abdul Rehman Khan and Asifullah Khan", "abstract": "  Since their emergence, Convolutional Neural Networks (CNNs) have made\nsignificant strides in medical image analysis. However, the local nature of the\nconvolution operator may pose a limitation for capturing global and long-range\ninteractions in CNNs. Recently, Transformers have gained popularity in the\ncomputer vision community and also in medical image segmentation due to their\nability to process global features effectively. The scalability issues of the\nself-attention mechanism and lack of the CNN-like inductive bias may have\nlimited their adoption. Therefore, hybrid Vision transformers\n(CNN-Transformer), exploiting the advantages of both Convolution and\nSelf-attention Mechanisms, have gained importance. In this work, we present\nMaxViT-UNet, a new Encoder-Decoder based UNet type hybrid vision transformer\n(CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder\nis designed to harness the power of both the convolution and self-attention\nmechanisms at each decoding stage with a nominal memory and computational\nburden. The inclusion of multi-axis self-attention, within each decoder stage,\nsignificantly enhances the discriminating capacity between the object and\nbackground regions, thereby helping in improving the segmentation efficiency.\nIn the Hybrid Decoder, a new block is also proposed. The fusion process\ncommences by integrating the upsampled lower-level decoder features, obtained\nthrough transpose convolution, with the skip-connection features derived from\nthe hybrid encoder. Subsequently, the fused features undergo refinement through\nthe utilization of a multi-axis attention mechanism. The proposed decoder block\nis repeated multiple times to segment the nuclei regions progressively.\nExperimental results on MoNuSeg18 and MoNuSAC20 datasets demonstrate the\neffectiveness of the proposed technique.\n", "link": "http://arxiv.org/abs/2305.08396v5", "date": "2024-03-29", "relevancy": 2.1239, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5256}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MaxViT-UNet%3A%20Multi-Axis%20Attention%20for%20Medical%20Image%20Segmentation&body=Title%3A%20MaxViT-UNet%3A%20Multi-Axis%20Attention%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Abdul%20Rehman%20Khan%20and%20Asifullah%20Khan%0AAbstract%3A%20%20%20Since%20their%20emergence%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20made%0Asignificant%20strides%20in%20medical%20image%20analysis.%20However%2C%20the%20local%20nature%20of%20the%0Aconvolution%20operator%20may%20pose%20a%20limitation%20for%20capturing%20global%20and%20long-range%0Ainteractions%20in%20CNNs.%20Recently%2C%20Transformers%20have%20gained%20popularity%20in%20the%0Acomputer%20vision%20community%20and%20also%20in%20medical%20image%20segmentation%20due%20to%20their%0Aability%20to%20process%20global%20features%20effectively.%20The%20scalability%20issues%20of%20the%0Aself-attention%20mechanism%20and%20lack%20of%20the%20CNN-like%20inductive%20bias%20may%20have%0Alimited%20their%20adoption.%20Therefore%2C%20hybrid%20Vision%20transformers%0A%28CNN-Transformer%29%2C%20exploiting%20the%20advantages%20of%20both%20Convolution%20and%0ASelf-attention%20Mechanisms%2C%20have%20gained%20importance.%20In%20this%20work%2C%20we%20present%0AMaxViT-UNet%2C%20a%20new%20Encoder-Decoder%20based%20UNet%20type%20hybrid%20vision%20transformer%0A%28CNN-Transformer%29%20for%20medical%20image%20segmentation.%20The%20proposed%20Hybrid%20Decoder%0Ais%20designed%20to%20harness%20the%20power%20of%20both%20the%20convolution%20and%20self-attention%0Amechanisms%20at%20each%20decoding%20stage%20with%20a%20nominal%20memory%20and%20computational%0Aburden.%20The%20inclusion%20of%20multi-axis%20self-attention%2C%20within%20each%20decoder%20stage%2C%0Asignificantly%20enhances%20the%20discriminating%20capacity%20between%20the%20object%20and%0Abackground%20regions%2C%20thereby%20helping%20in%20improving%20the%20segmentation%20efficiency.%0AIn%20the%20Hybrid%20Decoder%2C%20a%20new%20block%20is%20also%20proposed.%20The%20fusion%20process%0Acommences%20by%20integrating%20the%20upsampled%20lower-level%20decoder%20features%2C%20obtained%0Athrough%20transpose%20convolution%2C%20with%20the%20skip-connection%20features%20derived%20from%0Athe%20hybrid%20encoder.%20Subsequently%2C%20the%20fused%20features%20undergo%20refinement%20through%0Athe%20utilization%20of%20a%20multi-axis%20attention%20mechanism.%20The%20proposed%20decoder%20block%0Ais%20repeated%20multiple%20times%20to%20segment%20the%20nuclei%20regions%20progressively.%0AExperimental%20results%20on%20MoNuSeg18%20and%20MoNuSAC20%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08396v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaxViT-UNet%3A%20Multi-Axis%20Attention%20for%20Medical%20Image%20Segmentation&entry.906535625=Abdul%20Rehman%20Khan%20and%20Asifullah%20Khan&entry.1292438233=%20%20Since%20their%20emergence%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20made%0Asignificant%20strides%20in%20medical%20image%20analysis.%20However%2C%20the%20local%20nature%20of%20the%0Aconvolution%20operator%20may%20pose%20a%20limitation%20for%20capturing%20global%20and%20long-range%0Ainteractions%20in%20CNNs.%20Recently%2C%20Transformers%20have%20gained%20popularity%20in%20the%0Acomputer%20vision%20community%20and%20also%20in%20medical%20image%20segmentation%20due%20to%20their%0Aability%20to%20process%20global%20features%20effectively.%20The%20scalability%20issues%20of%20the%0Aself-attention%20mechanism%20and%20lack%20of%20the%20CNN-like%20inductive%20bias%20may%20have%0Alimited%20their%20adoption.%20Therefore%2C%20hybrid%20Vision%20transformers%0A%28CNN-Transformer%29%2C%20exploiting%20the%20advantages%20of%20both%20Convolution%20and%0ASelf-attention%20Mechanisms%2C%20have%20gained%20importance.%20In%20this%20work%2C%20we%20present%0AMaxViT-UNet%2C%20a%20new%20Encoder-Decoder%20based%20UNet%20type%20hybrid%20vision%20transformer%0A%28CNN-Transformer%29%20for%20medical%20image%20segmentation.%20The%20proposed%20Hybrid%20Decoder%0Ais%20designed%20to%20harness%20the%20power%20of%20both%20the%20convolution%20and%20self-attention%0Amechanisms%20at%20each%20decoding%20stage%20with%20a%20nominal%20memory%20and%20computational%0Aburden.%20The%20inclusion%20of%20multi-axis%20self-attention%2C%20within%20each%20decoder%20stage%2C%0Asignificantly%20enhances%20the%20discriminating%20capacity%20between%20the%20object%20and%0Abackground%20regions%2C%20thereby%20helping%20in%20improving%20the%20segmentation%20efficiency.%0AIn%20the%20Hybrid%20Decoder%2C%20a%20new%20block%20is%20also%20proposed.%20The%20fusion%20process%0Acommences%20by%20integrating%20the%20upsampled%20lower-level%20decoder%20features%2C%20obtained%0Athrough%20transpose%20convolution%2C%20with%20the%20skip-connection%20features%20derived%20from%0Athe%20hybrid%20encoder.%20Subsequently%2C%20the%20fused%20features%20undergo%20refinement%20through%0Athe%20utilization%20of%20a%20multi-axis%20attention%20mechanism.%20The%20proposed%20decoder%20block%0Ais%20repeated%20multiple%20times%20to%20segment%20the%20nuclei%20regions%20progressively.%0AExperimental%20results%20on%20MoNuSeg18%20and%20MoNuSAC20%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08396v5&entry.124074799=Read"},
{"title": "Learning to Count without Annotations", "author": "Lukas Knobel and Tengda Han and Yuki M. Asano", "abstract": "  While recent supervised methods for reference-based object counting continue\nto improve the performance on benchmark datasets, they have to rely on small\ndatasets due to the cost associated with manually annotating dozens of objects\nin images. We propose UnCounTR, a model that can learn this task without\nrequiring any manual annotations. To this end, we construct \"Self-Collages\",\nimages with various pasted objects as training samples, that provide a rich\nlearning signal covering arbitrary object types and counts. Our method builds\non existing unsupervised representations and segmentation techniques to\nsuccessfully demonstrate for the first time the ability of reference-based\ncounting without manual supervision. Our experiments show that our method not\nonly outperforms simple baselines and generic models such as FasterRCNN and\nDETR, but also matches the performance of supervised counting models in some\ndomains.\n", "link": "http://arxiv.org/abs/2307.08727v2", "date": "2024-03-29", "relevancy": 2.1235, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5657}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5143}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Count%20without%20Annotations&body=Title%3A%20Learning%20to%20Count%20without%20Annotations%0AAuthor%3A%20Lukas%20Knobel%20and%20Tengda%20Han%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20While%20recent%20supervised%20methods%20for%20reference-based%20object%20counting%20continue%0Ato%20improve%20the%20performance%20on%20benchmark%20datasets%2C%20they%20have%20to%20rely%20on%20small%0Adatasets%20due%20to%20the%20cost%20associated%20with%20manually%20annotating%20dozens%20of%20objects%0Ain%20images.%20We%20propose%20UnCounTR%2C%20a%20model%20that%20can%20learn%20this%20task%20without%0Arequiring%20any%20manual%20annotations.%20To%20this%20end%2C%20we%20construct%20%22Self-Collages%22%2C%0Aimages%20with%20various%20pasted%20objects%20as%20training%20samples%2C%20that%20provide%20a%20rich%0Alearning%20signal%20covering%20arbitrary%20object%20types%20and%20counts.%20Our%20method%20builds%0Aon%20existing%20unsupervised%20representations%20and%20segmentation%20techniques%20to%0Asuccessfully%20demonstrate%20for%20the%20first%20time%20the%20ability%20of%20reference-based%0Acounting%20without%20manual%20supervision.%20Our%20experiments%20show%20that%20our%20method%20not%0Aonly%20outperforms%20simple%20baselines%20and%20generic%20models%20such%20as%20FasterRCNN%20and%0ADETR%2C%20but%20also%20matches%20the%20performance%20of%20supervised%20counting%20models%20in%20some%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08727v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Count%20without%20Annotations&entry.906535625=Lukas%20Knobel%20and%20Tengda%20Han%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20While%20recent%20supervised%20methods%20for%20reference-based%20object%20counting%20continue%0Ato%20improve%20the%20performance%20on%20benchmark%20datasets%2C%20they%20have%20to%20rely%20on%20small%0Adatasets%20due%20to%20the%20cost%20associated%20with%20manually%20annotating%20dozens%20of%20objects%0Ain%20images.%20We%20propose%20UnCounTR%2C%20a%20model%20that%20can%20learn%20this%20task%20without%0Arequiring%20any%20manual%20annotations.%20To%20this%20end%2C%20we%20construct%20%22Self-Collages%22%2C%0Aimages%20with%20various%20pasted%20objects%20as%20training%20samples%2C%20that%20provide%20a%20rich%0Alearning%20signal%20covering%20arbitrary%20object%20types%20and%20counts.%20Our%20method%20builds%0Aon%20existing%20unsupervised%20representations%20and%20segmentation%20techniques%20to%0Asuccessfully%20demonstrate%20for%20the%20first%20time%20the%20ability%20of%20reference-based%0Acounting%20without%20manual%20supervision.%20Our%20experiments%20show%20that%20our%20method%20not%0Aonly%20outperforms%20simple%20baselines%20and%20generic%20models%20such%20as%20FasterRCNN%20and%0ADETR%2C%20but%20also%20matches%20the%20performance%20of%20supervised%20counting%20models%20in%20some%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08727v2&entry.124074799=Read"},
{"title": "Improving Learnt Local MAPF Policies with Heuristic Search", "author": "Rishi Veerapaneni and Qian Wang and Kevin Ren and Arthur Jakobsson and Jiaoyang Li and Maxim Likhachev", "abstract": "  Multi-agent path finding (MAPF) is the problem of finding collision-free\npaths for a team of agents to reach their goal locations. State-of-the-art\nclassical MAPF solvers typically employ heuristic search to find solutions for\nhundreds of agents but are typically centralized and can struggle to scale when\nrun with short timeouts. Machine learning (ML) approaches that learn policies\nfor each agent are appealing as these could enable decentralized systems and\nscale well while maintaining good solution quality. Current ML approaches to\nMAPF have proposed methods that have started to scratch the surface of this\npotential. However, state-of-the-art ML approaches produce \"local\" policies\nthat only plan for a single timestep and have poor success rates and\nscalability. Our main idea is that we can improve a ML local policy by using\nheuristic search methods on the output probability distribution to resolve\ndeadlocks and enable full horizon planning. We show several model-agnostic ways\nto use heuristic search with learnt policies that significantly improve the\npolicies' success rates and scalability. To our best knowledge, we demonstrate\nthe first time ML-based MAPF approaches have scaled to high congestion\nscenarios (e.g. 20% agent density).\n", "link": "http://arxiv.org/abs/2403.20300v1", "date": "2024-03-29", "relevancy": 2.1064, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Learnt%20Local%20MAPF%20Policies%20with%20Heuristic%20Search&body=Title%3A%20Improving%20Learnt%20Local%20MAPF%20Policies%20with%20Heuristic%20Search%0AAuthor%3A%20Rishi%20Veerapaneni%20and%20Qian%20Wang%20and%20Kevin%20Ren%20and%20Arthur%20Jakobsson%20and%20Jiaoyang%20Li%20and%20Maxim%20Likhachev%0AAbstract%3A%20%20%20Multi-agent%20path%20finding%20%28MAPF%29%20is%20the%20problem%20of%20finding%20collision-free%0Apaths%20for%20a%20team%20of%20agents%20to%20reach%20their%20goal%20locations.%20State-of-the-art%0Aclassical%20MAPF%20solvers%20typically%20employ%20heuristic%20search%20to%20find%20solutions%20for%0Ahundreds%20of%20agents%20but%20are%20typically%20centralized%20and%20can%20struggle%20to%20scale%20when%0Arun%20with%20short%20timeouts.%20Machine%20learning%20%28ML%29%20approaches%20that%20learn%20policies%0Afor%20each%20agent%20are%20appealing%20as%20these%20could%20enable%20decentralized%20systems%20and%0Ascale%20well%20while%20maintaining%20good%20solution%20quality.%20Current%20ML%20approaches%20to%0AMAPF%20have%20proposed%20methods%20that%20have%20started%20to%20scratch%20the%20surface%20of%20this%0Apotential.%20However%2C%20state-of-the-art%20ML%20approaches%20produce%20%22local%22%20policies%0Athat%20only%20plan%20for%20a%20single%20timestep%20and%20have%20poor%20success%20rates%20and%0Ascalability.%20Our%20main%20idea%20is%20that%20we%20can%20improve%20a%20ML%20local%20policy%20by%20using%0Aheuristic%20search%20methods%20on%20the%20output%20probability%20distribution%20to%20resolve%0Adeadlocks%20and%20enable%20full%20horizon%20planning.%20We%20show%20several%20model-agnostic%20ways%0Ato%20use%20heuristic%20search%20with%20learnt%20policies%20that%20significantly%20improve%20the%0Apolicies%27%20success%20rates%20and%20scalability.%20To%20our%20best%20knowledge%2C%20we%20demonstrate%0Athe%20first%20time%20ML-based%20MAPF%20approaches%20have%20scaled%20to%20high%20congestion%0Ascenarios%20%28e.g.%2020%25%20agent%20density%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20300v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Learnt%20Local%20MAPF%20Policies%20with%20Heuristic%20Search&entry.906535625=Rishi%20Veerapaneni%20and%20Qian%20Wang%20and%20Kevin%20Ren%20and%20Arthur%20Jakobsson%20and%20Jiaoyang%20Li%20and%20Maxim%20Likhachev&entry.1292438233=%20%20Multi-agent%20path%20finding%20%28MAPF%29%20is%20the%20problem%20of%20finding%20collision-free%0Apaths%20for%20a%20team%20of%20agents%20to%20reach%20their%20goal%20locations.%20State-of-the-art%0Aclassical%20MAPF%20solvers%20typically%20employ%20heuristic%20search%20to%20find%20solutions%20for%0Ahundreds%20of%20agents%20but%20are%20typically%20centralized%20and%20can%20struggle%20to%20scale%20when%0Arun%20with%20short%20timeouts.%20Machine%20learning%20%28ML%29%20approaches%20that%20learn%20policies%0Afor%20each%20agent%20are%20appealing%20as%20these%20could%20enable%20decentralized%20systems%20and%0Ascale%20well%20while%20maintaining%20good%20solution%20quality.%20Current%20ML%20approaches%20to%0AMAPF%20have%20proposed%20methods%20that%20have%20started%20to%20scratch%20the%20surface%20of%20this%0Apotential.%20However%2C%20state-of-the-art%20ML%20approaches%20produce%20%22local%22%20policies%0Athat%20only%20plan%20for%20a%20single%20timestep%20and%20have%20poor%20success%20rates%20and%0Ascalability.%20Our%20main%20idea%20is%20that%20we%20can%20improve%20a%20ML%20local%20policy%20by%20using%0Aheuristic%20search%20methods%20on%20the%20output%20probability%20distribution%20to%20resolve%0Adeadlocks%20and%20enable%20full%20horizon%20planning.%20We%20show%20several%20model-agnostic%20ways%0Ato%20use%20heuristic%20search%20with%20learnt%20policies%20that%20significantly%20improve%20the%0Apolicies%27%20success%20rates%20and%20scalability.%20To%20our%20best%20knowledge%2C%20we%20demonstrate%0Athe%20first%20time%20ML-based%20MAPF%20approaches%20have%20scaled%20to%20high%20congestion%0Ascenarios%20%28e.g.%2020%25%20agent%20density%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20300v1&entry.124074799=Read"},
{"title": "FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding", "author": "Jun Xiang and Xuan Gao and Yudong Guo and Juyong Zhang", "abstract": "  We propose FlashAvatar, a novel and lightweight 3D animatable avatar\nrepresentation that could reconstruct a digital avatar from a short monocular\nvideo sequence in minutes and render high-fidelity photo-realistic images at\n300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D\nGaussian field embedded in the surface of a parametric face model and learn\nextra spatial offset to model non-surface regions and subtle facial details.\nWhile full use of geometric priors can capture high-frequency facial details\nand preserve exaggerated expressions, proper initialization can help reduce the\nnumber of Gaussians, thus enabling super-fast rendering speed. Extensive\nexperimental results demonstrate that FlashAvatar outperforms existing works\nregarding visual quality and personalized details and is almost an order of\nmagnitude faster in rendering speed. Project page:\nhttps://ustc3dv.github.io/FlashAvatar/\n", "link": "http://arxiv.org/abs/2312.02214v2", "date": "2024-03-29", "relevancy": 2.0968, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5418}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlashAvatar%3A%20High-fidelity%20Head%20Avatar%20with%20Efficient%20Gaussian%20Embedding&body=Title%3A%20FlashAvatar%3A%20High-fidelity%20Head%20Avatar%20with%20Efficient%20Gaussian%20Embedding%0AAuthor%3A%20Jun%20Xiang%20and%20Xuan%20Gao%20and%20Yudong%20Guo%20and%20Juyong%20Zhang%0AAbstract%3A%20%20%20We%20propose%20FlashAvatar%2C%20a%20novel%20and%20lightweight%203D%20animatable%20avatar%0Arepresentation%20that%20could%20reconstruct%20a%20digital%20avatar%20from%20a%20short%20monocular%0Avideo%20sequence%20in%20minutes%20and%20render%20high-fidelity%20photo-realistic%20images%20at%0A300FPS%20on%20a%20consumer-grade%20GPU.%20To%20achieve%20this%2C%20we%20maintain%20a%20uniform%203D%0AGaussian%20field%20embedded%20in%20the%20surface%20of%20a%20parametric%20face%20model%20and%20learn%0Aextra%20spatial%20offset%20to%20model%20non-surface%20regions%20and%20subtle%20facial%20details.%0AWhile%20full%20use%20of%20geometric%20priors%20can%20capture%20high-frequency%20facial%20details%0Aand%20preserve%20exaggerated%20expressions%2C%20proper%20initialization%20can%20help%20reduce%20the%0Anumber%20of%20Gaussians%2C%20thus%20enabling%20super-fast%20rendering%20speed.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20FlashAvatar%20outperforms%20existing%20works%0Aregarding%20visual%20quality%20and%20personalized%20details%20and%20is%20almost%20an%20order%20of%0Amagnitude%20faster%20in%20rendering%20speed.%20Project%20page%3A%0Ahttps%3A//ustc3dv.github.io/FlashAvatar/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02214v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashAvatar%3A%20High-fidelity%20Head%20Avatar%20with%20Efficient%20Gaussian%20Embedding&entry.906535625=Jun%20Xiang%20and%20Xuan%20Gao%20and%20Yudong%20Guo%20and%20Juyong%20Zhang&entry.1292438233=%20%20We%20propose%20FlashAvatar%2C%20a%20novel%20and%20lightweight%203D%20animatable%20avatar%0Arepresentation%20that%20could%20reconstruct%20a%20digital%20avatar%20from%20a%20short%20monocular%0Avideo%20sequence%20in%20minutes%20and%20render%20high-fidelity%20photo-realistic%20images%20at%0A300FPS%20on%20a%20consumer-grade%20GPU.%20To%20achieve%20this%2C%20we%20maintain%20a%20uniform%203D%0AGaussian%20field%20embedded%20in%20the%20surface%20of%20a%20parametric%20face%20model%20and%20learn%0Aextra%20spatial%20offset%20to%20model%20non-surface%20regions%20and%20subtle%20facial%20details.%0AWhile%20full%20use%20of%20geometric%20priors%20can%20capture%20high-frequency%20facial%20details%0Aand%20preserve%20exaggerated%20expressions%2C%20proper%20initialization%20can%20help%20reduce%20the%0Anumber%20of%20Gaussians%2C%20thus%20enabling%20super-fast%20rendering%20speed.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20FlashAvatar%20outperforms%20existing%20works%0Aregarding%20visual%20quality%20and%20personalized%20details%20and%20is%20almost%20an%20order%20of%0Amagnitude%20faster%20in%20rendering%20speed.%20Project%20page%3A%0Ahttps%3A//ustc3dv.github.io/FlashAvatar/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02214v2&entry.124074799=Read"},
{"title": "Convolutional Prompting meets Language Models for Continual Learning", "author": "Anurag Roy and Riddhiman Moulick and Vinay K. Verma and Saptarshi Ghosh and Abir Das", "abstract": "  Continual Learning (CL) enables machine learning models to learn from\ncontinuously shifting new training data in absence of data from old tasks.\nRecently, pretrained vision transformers combined with prompt tuning have shown\npromise for overcoming catastrophic forgetting in CL. These approaches rely on\na pool of learnable prompts which can be inefficient in sharing knowledge\nacross tasks leading to inferior performance. In addition, the lack of\nfine-grained layer specific prompts does not allow these to fully express the\nstrength of the prompts for CL. We address these limitations by proposing\nConvPrompt, a novel convolutional prompt creation mechanism that maintains\nlayer-wise shared embeddings, enabling both layer-specific learning and better\nconcept transfer across tasks. The intelligent use of convolution enables us to\nmaintain a low parameter overhead without compromising performance. We further\nleverage Large Language Models to generate fine-grained text descriptions of\neach category which are used to get task similarity and dynamically decide the\nnumber of prompts to be learned. Extensive experiments demonstrate the\nsuperiority of ConvPrompt and improves SOTA by ~3% with significantly less\nparameter overhead. We also perform strong ablation over various modules to\ndisentangle the importance of different components.\n", "link": "http://arxiv.org/abs/2403.20317v1", "date": "2024-03-29", "relevancy": 2.077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5472}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Prompting%20meets%20Language%20Models%20for%20Continual%20Learning&body=Title%3A%20Convolutional%20Prompting%20meets%20Language%20Models%20for%20Continual%20Learning%0AAuthor%3A%20Anurag%20Roy%20and%20Riddhiman%20Moulick%20and%20Vinay%20K.%20Verma%20and%20Saptarshi%20Ghosh%20and%20Abir%20Das%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20enables%20machine%20learning%20models%20to%20learn%20from%0Acontinuously%20shifting%20new%20training%20data%20in%20absence%20of%20data%20from%20old%20tasks.%0ARecently%2C%20pretrained%20vision%20transformers%20combined%20with%20prompt%20tuning%20have%20shown%0Apromise%20for%20overcoming%20catastrophic%20forgetting%20in%20CL.%20These%20approaches%20rely%20on%0Aa%20pool%20of%20learnable%20prompts%20which%20can%20be%20inefficient%20in%20sharing%20knowledge%0Aacross%20tasks%20leading%20to%20inferior%20performance.%20In%20addition%2C%20the%20lack%20of%0Afine-grained%20layer%20specific%20prompts%20does%20not%20allow%20these%20to%20fully%20express%20the%0Astrength%20of%20the%20prompts%20for%20CL.%20We%20address%20these%20limitations%20by%20proposing%0AConvPrompt%2C%20a%20novel%20convolutional%20prompt%20creation%20mechanism%20that%20maintains%0Alayer-wise%20shared%20embeddings%2C%20enabling%20both%20layer-specific%20learning%20and%20better%0Aconcept%20transfer%20across%20tasks.%20The%20intelligent%20use%20of%20convolution%20enables%20us%20to%0Amaintain%20a%20low%20parameter%20overhead%20without%20compromising%20performance.%20We%20further%0Aleverage%20Large%20Language%20Models%20to%20generate%20fine-grained%20text%20descriptions%20of%0Aeach%20category%20which%20are%20used%20to%20get%20task%20similarity%20and%20dynamically%20decide%20the%0Anumber%20of%20prompts%20to%20be%20learned.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20ConvPrompt%20and%20improves%20SOTA%20by%20~3%25%20with%20significantly%20less%0Aparameter%20overhead.%20We%20also%20perform%20strong%20ablation%20over%20various%20modules%20to%0Adisentangle%20the%20importance%20of%20different%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20317v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Prompting%20meets%20Language%20Models%20for%20Continual%20Learning&entry.906535625=Anurag%20Roy%20and%20Riddhiman%20Moulick%20and%20Vinay%20K.%20Verma%20and%20Saptarshi%20Ghosh%20and%20Abir%20Das&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20enables%20machine%20learning%20models%20to%20learn%20from%0Acontinuously%20shifting%20new%20training%20data%20in%20absence%20of%20data%20from%20old%20tasks.%0ARecently%2C%20pretrained%20vision%20transformers%20combined%20with%20prompt%20tuning%20have%20shown%0Apromise%20for%20overcoming%20catastrophic%20forgetting%20in%20CL.%20These%20approaches%20rely%20on%0Aa%20pool%20of%20learnable%20prompts%20which%20can%20be%20inefficient%20in%20sharing%20knowledge%0Aacross%20tasks%20leading%20to%20inferior%20performance.%20In%20addition%2C%20the%20lack%20of%0Afine-grained%20layer%20specific%20prompts%20does%20not%20allow%20these%20to%20fully%20express%20the%0Astrength%20of%20the%20prompts%20for%20CL.%20We%20address%20these%20limitations%20by%20proposing%0AConvPrompt%2C%20a%20novel%20convolutional%20prompt%20creation%20mechanism%20that%20maintains%0Alayer-wise%20shared%20embeddings%2C%20enabling%20both%20layer-specific%20learning%20and%20better%0Aconcept%20transfer%20across%20tasks.%20The%20intelligent%20use%20of%20convolution%20enables%20us%20to%0Amaintain%20a%20low%20parameter%20overhead%20without%20compromising%20performance.%20We%20further%0Aleverage%20Large%20Language%20Models%20to%20generate%20fine-grained%20text%20descriptions%20of%0Aeach%20category%20which%20are%20used%20to%20get%20task%20similarity%20and%20dynamically%20decide%20the%0Anumber%20of%20prompts%20to%20be%20learned.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20ConvPrompt%20and%20improves%20SOTA%20by%20~3%25%20with%20significantly%20less%0Aparameter%20overhead.%20We%20also%20perform%20strong%20ablation%20over%20various%20modules%20to%0Adisentangle%20the%20importance%20of%20different%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20317v1&entry.124074799=Read"},
{"title": "Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for\n  Reconstructing Challenging Surfaces", "author": "Mauro Comi and Alessio Tonioni and Max Yang and Jonathan Tremblay and Valts Blukis and Yijiong Lin and Nathan F. Lepora and Laurence Aitchison", "abstract": "  Touch and vision go hand in hand, mutually enhancing our ability to\nunderstand the world. From a research perspective, the problem of mixing touch\nand vision is underexplored and presents interesting challenges. To this end,\nwe propose Tactile-Informed 3DGS, a novel approach that incorporates touch data\n(local depth maps) with multi-view vision data to achieve surface\nreconstruction and novel view synthesis. Our method optimises 3D Gaussian\nprimitives to accurately model the object's geometry at points of contact. By\ncreating a framework that decreases the transmittance at touch locations, we\nachieve a refined surface reconstruction, ensuring a uniformly smooth depth\nmap. Touch is particularly useful when considering non-Lambertian objects (e.g.\nshiny or reflective surfaces) since contemporary methods tend to fail to\nreconstruct with fidelity specular highlights. By combining vision and tactile\nsensing, we achieve more accurate geometry reconstructions with fewer images\nthan prior methods. We conduct evaluation on objects with glossy and reflective\nsurfaces and demonstrate the effectiveness of our approach, offering\nsignificant improvements in reconstruction quality.\n", "link": "http://arxiv.org/abs/2403.20275v1", "date": "2024-03-29", "relevancy": 2.0667, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Snap-it%2C%20Tap-it%2C%20Splat-it%3A%20Tactile-Informed%203D%20Gaussian%20Splatting%20for%0A%20%20Reconstructing%20Challenging%20Surfaces&body=Title%3A%20Snap-it%2C%20Tap-it%2C%20Splat-it%3A%20Tactile-Informed%203D%20Gaussian%20Splatting%20for%0A%20%20Reconstructing%20Challenging%20Surfaces%0AAuthor%3A%20Mauro%20Comi%20and%20Alessio%20Tonioni%20and%20Max%20Yang%20and%20Jonathan%20Tremblay%20and%20Valts%20Blukis%20and%20Yijiong%20Lin%20and%20Nathan%20F.%20Lepora%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20Touch%20and%20vision%20go%20hand%20in%20hand%2C%20mutually%20enhancing%20our%20ability%20to%0Aunderstand%20the%20world.%20From%20a%20research%20perspective%2C%20the%20problem%20of%20mixing%20touch%0Aand%20vision%20is%20underexplored%20and%20presents%20interesting%20challenges.%20To%20this%20end%2C%0Awe%20propose%20Tactile-Informed%203DGS%2C%20a%20novel%20approach%20that%20incorporates%20touch%20data%0A%28local%20depth%20maps%29%20with%20multi-view%20vision%20data%20to%20achieve%20surface%0Areconstruction%20and%20novel%20view%20synthesis.%20Our%20method%20optimises%203D%20Gaussian%0Aprimitives%20to%20accurately%20model%20the%20object%27s%20geometry%20at%20points%20of%20contact.%20By%0Acreating%20a%20framework%20that%20decreases%20the%20transmittance%20at%20touch%20locations%2C%20we%0Aachieve%20a%20refined%20surface%20reconstruction%2C%20ensuring%20a%20uniformly%20smooth%20depth%0Amap.%20Touch%20is%20particularly%20useful%20when%20considering%20non-Lambertian%20objects%20%28e.g.%0Ashiny%20or%20reflective%20surfaces%29%20since%20contemporary%20methods%20tend%20to%20fail%20to%0Areconstruct%20with%20fidelity%20specular%20highlights.%20By%20combining%20vision%20and%20tactile%0Asensing%2C%20we%20achieve%20more%20accurate%20geometry%20reconstructions%20with%20fewer%20images%0Athan%20prior%20methods.%20We%20conduct%20evaluation%20on%20objects%20with%20glossy%20and%20reflective%0Asurfaces%20and%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20offering%0Asignificant%20improvements%20in%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20275v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snap-it%2C%20Tap-it%2C%20Splat-it%3A%20Tactile-Informed%203D%20Gaussian%20Splatting%20for%0A%20%20Reconstructing%20Challenging%20Surfaces&entry.906535625=Mauro%20Comi%20and%20Alessio%20Tonioni%20and%20Max%20Yang%20and%20Jonathan%20Tremblay%20and%20Valts%20Blukis%20and%20Yijiong%20Lin%20and%20Nathan%20F.%20Lepora%20and%20Laurence%20Aitchison&entry.1292438233=%20%20Touch%20and%20vision%20go%20hand%20in%20hand%2C%20mutually%20enhancing%20our%20ability%20to%0Aunderstand%20the%20world.%20From%20a%20research%20perspective%2C%20the%20problem%20of%20mixing%20touch%0Aand%20vision%20is%20underexplored%20and%20presents%20interesting%20challenges.%20To%20this%20end%2C%0Awe%20propose%20Tactile-Informed%203DGS%2C%20a%20novel%20approach%20that%20incorporates%20touch%20data%0A%28local%20depth%20maps%29%20with%20multi-view%20vision%20data%20to%20achieve%20surface%0Areconstruction%20and%20novel%20view%20synthesis.%20Our%20method%20optimises%203D%20Gaussian%0Aprimitives%20to%20accurately%20model%20the%20object%27s%20geometry%20at%20points%20of%20contact.%20By%0Acreating%20a%20framework%20that%20decreases%20the%20transmittance%20at%20touch%20locations%2C%20we%0Aachieve%20a%20refined%20surface%20reconstruction%2C%20ensuring%20a%20uniformly%20smooth%20depth%0Amap.%20Touch%20is%20particularly%20useful%20when%20considering%20non-Lambertian%20objects%20%28e.g.%0Ashiny%20or%20reflective%20surfaces%29%20since%20contemporary%20methods%20tend%20to%20fail%20to%0Areconstruct%20with%20fidelity%20specular%20highlights.%20By%20combining%20vision%20and%20tactile%0Asensing%2C%20we%20achieve%20more%20accurate%20geometry%20reconstructions%20with%20fewer%20images%0Athan%20prior%20methods.%20We%20conduct%20evaluation%20on%20objects%20with%20glossy%20and%20reflective%0Asurfaces%20and%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20offering%0Asignificant%20improvements%20in%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20275v1&entry.124074799=Read"},
{"title": "H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language\n  Model", "author": "Chao Pang and Jiang Wu and Jiayu Li and Yi Liu and Jiaxing Sun and Weijia Li and Xingxing Weng and Shuai Wang and Litong Feng and Gui-Song Xia and Conghui He", "abstract": "  The generic large Vision-Language Models (VLMs) is rapidly developing, but\nstill perform poorly in Remote Sensing (RS) domain, which is due to the unique\nand specialized nature of RS imagery and the comparatively limited spatial\nperception of current VLMs. Existing Remote Sensing specific Vision Language\nModels (RSVLMs) still have considerable potential for improvement, primarily\nowing to the lack of large-scale, high-quality RS vision-language datasets. We\nconstructed HqDC-1.4M, the large scale High quality and Detailed Captions for\nRS images, containing 1.4 million image-caption pairs, which not only enhance\nthe RSVLM's understanding of RS images but also significantly improve the\nmodel's spatial perception abilities, such as localization and counting,\nthereby increasing the helpfulness of the RSVLM. Moreover, to address the\ninevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first\ndataset aimed at enhancing the Self-Awareness capability of RSVLMs. By\nincorporating a variety of unanswerable questions into typical RS visual\nquestion-answering tasks, RSSA effectively improves the truthfulness and\nreduces the hallucinations of the model's outputs, thereby enhancing the\nhonesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the\nHelpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved\noutstanding performance on multiple RS public datasets and is capable of\nrecognizing and refusing to answer the unanswerable questions, effectively\nmitigating the incorrect generations. We will release the code, data and model\nweights at https://github.com/opendatalab/H2RSVLM .\n", "link": "http://arxiv.org/abs/2403.20213v1", "date": "2024-03-29", "relevancy": 2.0652, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.521}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5134}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5117}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20H2RSVLM%3A%20Towards%20Helpful%20and%20Honest%20Remote%20Sensing%20Large%20Vision%20Language%0A%20%20Model&body=Title%3A%20H2RSVLM%3A%20Towards%20Helpful%20and%20Honest%20Remote%20Sensing%20Large%20Vision%20Language%0A%20%20Model%0AAuthor%3A%20Chao%20Pang%20and%20Jiang%20Wu%20and%20Jiayu%20Li%20and%20Yi%20Liu%20and%20Jiaxing%20Sun%20and%20Weijia%20Li%20and%20Xingxing%20Weng%20and%20Shuai%20Wang%20and%20Litong%20Feng%20and%20Gui-Song%20Xia%20and%20Conghui%20He%0AAbstract%3A%20%20%20The%20generic%20large%20Vision-Language%20Models%20%28VLMs%29%20is%20rapidly%20developing%2C%20but%0Astill%20perform%20poorly%20in%20Remote%20Sensing%20%28RS%29%20domain%2C%20which%20is%20due%20to%20the%20unique%0Aand%20specialized%20nature%20of%20RS%20imagery%20and%20the%20comparatively%20limited%20spatial%0Aperception%20of%20current%20VLMs.%20Existing%20Remote%20Sensing%20specific%20Vision%20Language%0AModels%20%28RSVLMs%29%20still%20have%20considerable%20potential%20for%20improvement%2C%20primarily%0Aowing%20to%20the%20lack%20of%20large-scale%2C%20high-quality%20RS%20vision-language%20datasets.%20We%0Aconstructed%20HqDC-1.4M%2C%20the%20large%20scale%20High%20quality%20and%20Detailed%20Captions%20for%0ARS%20images%2C%20containing%201.4%20million%20image-caption%20pairs%2C%20which%20not%20only%20enhance%0Athe%20RSVLM%27s%20understanding%20of%20RS%20images%20but%20also%20significantly%20improve%20the%0Amodel%27s%20spatial%20perception%20abilities%2C%20such%20as%20localization%20and%20counting%2C%0Athereby%20increasing%20the%20helpfulness%20of%20the%20RSVLM.%20Moreover%2C%20to%20address%20the%0Ainevitable%20%22hallucination%22%20problem%20in%20RSVLM%2C%20we%20developed%20RSSA%2C%20the%20first%0Adataset%20aimed%20at%20enhancing%20the%20Self-Awareness%20capability%20of%20RSVLMs.%20By%0Aincorporating%20a%20variety%20of%20unanswerable%20questions%20into%20typical%20RS%20visual%0Aquestion-answering%20tasks%2C%20RSSA%20effectively%20improves%20the%20truthfulness%20and%0Areduces%20the%20hallucinations%20of%20the%20model%27s%20outputs%2C%20thereby%20enhancing%20the%0Ahonesty%20of%20the%20RSVLM.%20Based%20on%20these%20datasets%2C%20we%20proposed%20the%20H2RSVLM%2C%20the%0AHelpful%20and%20Honest%20Remote%20Sensing%20Vision%20Language%20Model.%20H2RSVLM%20has%20achieved%0Aoutstanding%20performance%20on%20multiple%20RS%20public%20datasets%20and%20is%20capable%20of%0Arecognizing%20and%20refusing%20to%20answer%20the%20unanswerable%20questions%2C%20effectively%0Amitigating%20the%20incorrect%20generations.%20We%20will%20release%20the%20code%2C%20data%20and%20model%0Aweights%20at%20https%3A//github.com/opendatalab/H2RSVLM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20213v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H2RSVLM%3A%20Towards%20Helpful%20and%20Honest%20Remote%20Sensing%20Large%20Vision%20Language%0A%20%20Model&entry.906535625=Chao%20Pang%20and%20Jiang%20Wu%20and%20Jiayu%20Li%20and%20Yi%20Liu%20and%20Jiaxing%20Sun%20and%20Weijia%20Li%20and%20Xingxing%20Weng%20and%20Shuai%20Wang%20and%20Litong%20Feng%20and%20Gui-Song%20Xia%20and%20Conghui%20He&entry.1292438233=%20%20The%20generic%20large%20Vision-Language%20Models%20%28VLMs%29%20is%20rapidly%20developing%2C%20but%0Astill%20perform%20poorly%20in%20Remote%20Sensing%20%28RS%29%20domain%2C%20which%20is%20due%20to%20the%20unique%0Aand%20specialized%20nature%20of%20RS%20imagery%20and%20the%20comparatively%20limited%20spatial%0Aperception%20of%20current%20VLMs.%20Existing%20Remote%20Sensing%20specific%20Vision%20Language%0AModels%20%28RSVLMs%29%20still%20have%20considerable%20potential%20for%20improvement%2C%20primarily%0Aowing%20to%20the%20lack%20of%20large-scale%2C%20high-quality%20RS%20vision-language%20datasets.%20We%0Aconstructed%20HqDC-1.4M%2C%20the%20large%20scale%20High%20quality%20and%20Detailed%20Captions%20for%0ARS%20images%2C%20containing%201.4%20million%20image-caption%20pairs%2C%20which%20not%20only%20enhance%0Athe%20RSVLM%27s%20understanding%20of%20RS%20images%20but%20also%20significantly%20improve%20the%0Amodel%27s%20spatial%20perception%20abilities%2C%20such%20as%20localization%20and%20counting%2C%0Athereby%20increasing%20the%20helpfulness%20of%20the%20RSVLM.%20Moreover%2C%20to%20address%20the%0Ainevitable%20%22hallucination%22%20problem%20in%20RSVLM%2C%20we%20developed%20RSSA%2C%20the%20first%0Adataset%20aimed%20at%20enhancing%20the%20Self-Awareness%20capability%20of%20RSVLMs.%20By%0Aincorporating%20a%20variety%20of%20unanswerable%20questions%20into%20typical%20RS%20visual%0Aquestion-answering%20tasks%2C%20RSSA%20effectively%20improves%20the%20truthfulness%20and%0Areduces%20the%20hallucinations%20of%20the%20model%27s%20outputs%2C%20thereby%20enhancing%20the%0Ahonesty%20of%20the%20RSVLM.%20Based%20on%20these%20datasets%2C%20we%20proposed%20the%20H2RSVLM%2C%20the%0AHelpful%20and%20Honest%20Remote%20Sensing%20Vision%20Language%20Model.%20H2RSVLM%20has%20achieved%0Aoutstanding%20performance%20on%20multiple%20RS%20public%20datasets%20and%20is%20capable%20of%0Arecognizing%20and%20refusing%20to%20answer%20the%20unanswerable%20questions%2C%20effectively%0Amitigating%20the%20incorrect%20generations.%20We%20will%20release%20the%20code%2C%20data%20and%20model%0Aweights%20at%20https%3A//github.com/opendatalab/H2RSVLM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20213v1&entry.124074799=Read"},
{"title": "MCNet: A crowd denstity estimation network based on integrating\n  multiscale attention module", "author": "Qiang Guo and Rubo Zhang and Di Zhao", "abstract": "  Aiming at the metro video surveillance system has not been able to\neffectively solve the metro crowd density estimation problem, a Metro Crowd\ndensity estimation Network (called MCNet) is proposed to automatically classify\ncrowd density level of passengers. Firstly, an Integrating Multi-scale\nAttention (IMA) module is proposed to enhance the ability of the plain\nclassifiers to extract semantic crowd texture features to accommodate to the\ncharacteristics of the crowd texture feature. The innovation of the IMA module\nis to fuse the dilation convolution, multiscale feature extraction and\nattention mechanism to obtain multi-scale crowd feature activation from a\nlarger receptive field with lower computational cost, and to strengthen the\ncrowds activation state of convolutional features in top layers. Secondly, a\nnovel lightweight crowd texture feature extraction network is proposed, which\ncan directly process video frames and automatically extract texture features\nfor crowd density estimation, while its faster image processing speed and fewer\nnetwork parameters make it flexible to be deployed on embedded platforms with\nlimited hardware resources. Finally, this paper integrates IMA module and the\nlightweight crowd texture feature extraction network to construct the MCNet,\nand validate the feasibility of this network on image classification dataset:\nCifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to\nvalidate the MCNet whether can be a suitable solution for crowd density\nestimation in metro video surveillance where there are image processing\nchallenges such as high density, high occlusion, perspective distortion and\nlimited hardware resources.\n", "link": "http://arxiv.org/abs/2403.20173v1", "date": "2024-03-29", "relevancy": 2.0327, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5222}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4991}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MCNet%3A%20A%20crowd%20denstity%20estimation%20network%20based%20on%20integrating%0A%20%20multiscale%20attention%20module&body=Title%3A%20MCNet%3A%20A%20crowd%20denstity%20estimation%20network%20based%20on%20integrating%0A%20%20multiscale%20attention%20module%0AAuthor%3A%20Qiang%20Guo%20and%20Rubo%20Zhang%20and%20Di%20Zhao%0AAbstract%3A%20%20%20Aiming%20at%20the%20metro%20video%20surveillance%20system%20has%20not%20been%20able%20to%0Aeffectively%20solve%20the%20metro%20crowd%20density%20estimation%20problem%2C%20a%20Metro%20Crowd%0Adensity%20estimation%20Network%20%28called%20MCNet%29%20is%20proposed%20to%20automatically%20classify%0Acrowd%20density%20level%20of%20passengers.%20Firstly%2C%20an%20Integrating%20Multi-scale%0AAttention%20%28IMA%29%20module%20is%20proposed%20to%20enhance%20the%20ability%20of%20the%20plain%0Aclassifiers%20to%20extract%20semantic%20crowd%20texture%20features%20to%20accommodate%20to%20the%0Acharacteristics%20of%20the%20crowd%20texture%20feature.%20The%20innovation%20of%20the%20IMA%20module%0Ais%20to%20fuse%20the%20dilation%20convolution%2C%20multiscale%20feature%20extraction%20and%0Aattention%20mechanism%20to%20obtain%20multi-scale%20crowd%20feature%20activation%20from%20a%0Alarger%20receptive%20field%20with%20lower%20computational%20cost%2C%20and%20to%20strengthen%20the%0Acrowds%20activation%20state%20of%20convolutional%20features%20in%20top%20layers.%20Secondly%2C%20a%0Anovel%20lightweight%20crowd%20texture%20feature%20extraction%20network%20is%20proposed%2C%20which%0Acan%20directly%20process%20video%20frames%20and%20automatically%20extract%20texture%20features%0Afor%20crowd%20density%20estimation%2C%20while%20its%20faster%20image%20processing%20speed%20and%20fewer%0Anetwork%20parameters%20make%20it%20flexible%20to%20be%20deployed%20on%20embedded%20platforms%20with%0Alimited%20hardware%20resources.%20Finally%2C%20this%20paper%20integrates%20IMA%20module%20and%20the%0Alightweight%20crowd%20texture%20feature%20extraction%20network%20to%20construct%20the%20MCNet%2C%0Aand%20validate%20the%20feasibility%20of%20this%20network%20on%20image%20classification%20dataset%3A%0ACifar10%20and%20four%20crowd%20density%20datasets%3A%20PETS2009%2C%20Mall%2C%20QUT%20and%20SH_METRO%20to%0Avalidate%20the%20MCNet%20whether%20can%20be%20a%20suitable%20solution%20for%20crowd%20density%0Aestimation%20in%20metro%20video%20surveillance%20where%20there%20are%20image%20processing%0Achallenges%20such%20as%20high%20density%2C%20high%20occlusion%2C%20perspective%20distortion%20and%0Alimited%20hardware%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20173v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCNet%3A%20A%20crowd%20denstity%20estimation%20network%20based%20on%20integrating%0A%20%20multiscale%20attention%20module&entry.906535625=Qiang%20Guo%20and%20Rubo%20Zhang%20and%20Di%20Zhao&entry.1292438233=%20%20Aiming%20at%20the%20metro%20video%20surveillance%20system%20has%20not%20been%20able%20to%0Aeffectively%20solve%20the%20metro%20crowd%20density%20estimation%20problem%2C%20a%20Metro%20Crowd%0Adensity%20estimation%20Network%20%28called%20MCNet%29%20is%20proposed%20to%20automatically%20classify%0Acrowd%20density%20level%20of%20passengers.%20Firstly%2C%20an%20Integrating%20Multi-scale%0AAttention%20%28IMA%29%20module%20is%20proposed%20to%20enhance%20the%20ability%20of%20the%20plain%0Aclassifiers%20to%20extract%20semantic%20crowd%20texture%20features%20to%20accommodate%20to%20the%0Acharacteristics%20of%20the%20crowd%20texture%20feature.%20The%20innovation%20of%20the%20IMA%20module%0Ais%20to%20fuse%20the%20dilation%20convolution%2C%20multiscale%20feature%20extraction%20and%0Aattention%20mechanism%20to%20obtain%20multi-scale%20crowd%20feature%20activation%20from%20a%0Alarger%20receptive%20field%20with%20lower%20computational%20cost%2C%20and%20to%20strengthen%20the%0Acrowds%20activation%20state%20of%20convolutional%20features%20in%20top%20layers.%20Secondly%2C%20a%0Anovel%20lightweight%20crowd%20texture%20feature%20extraction%20network%20is%20proposed%2C%20which%0Acan%20directly%20process%20video%20frames%20and%20automatically%20extract%20texture%20features%0Afor%20crowd%20density%20estimation%2C%20while%20its%20faster%20image%20processing%20speed%20and%20fewer%0Anetwork%20parameters%20make%20it%20flexible%20to%20be%20deployed%20on%20embedded%20platforms%20with%0Alimited%20hardware%20resources.%20Finally%2C%20this%20paper%20integrates%20IMA%20module%20and%20the%0Alightweight%20crowd%20texture%20feature%20extraction%20network%20to%20construct%20the%20MCNet%2C%0Aand%20validate%20the%20feasibility%20of%20this%20network%20on%20image%20classification%20dataset%3A%0ACifar10%20and%20four%20crowd%20density%20datasets%3A%20PETS2009%2C%20Mall%2C%20QUT%20and%20SH_METRO%20to%0Avalidate%20the%20MCNet%20whether%20can%20be%20a%20suitable%20solution%20for%20crowd%20density%0Aestimation%20in%20metro%20video%20surveillance%20where%20there%20are%20image%20processing%0Achallenges%20such%20as%20high%20density%2C%20high%20occlusion%2C%20perspective%20distortion%20and%0Alimited%20hardware%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20173v1&entry.124074799=Read"},
{"title": "CATSNet: a context-aware network for Height Estimation in a Forested\n  Area based on Pol-TomoSAR data", "author": "Wenyu Yang and Sergio Vitale and Hossein Aghababaei and Giampaolo Ferraioli and Vito Pascazio and Gilda Schirinzi", "abstract": "  Tropical forests are a key component of the global carbon cycle. With plans\nfor upcoming space-borne missions like BIOMASS to monitor forestry, several\nairborne missions, including TropiSAR and AfriSAR campaigns, have been\nsuccessfully launched and experimented. Typical Synthetic Aperture Radar\nTomography (TomoSAR) methods involve complex models with low accuracy and high\ncomputation costs. In recent years, deep learning methods have also gained\nattention in the TomoSAR framework, showing interesting performance. Recently,\na solution based on a fully connected Tomographic Neural Network (TSNN) has\ndemonstrated its effectiveness in accurately estimating forest and ground\nheights by exploiting the pixel-wise elements of the covariance matrix derived\nfrom TomoSAR data. This work instead goes beyond the pixel-wise approach to\ndefine a context-aware deep learning-based solution named CATSNet. A\nconvolutional neural network is considered to leverage patch-based information\nand extract features from a neighborhood rather than focus on a single pixel.\nThe training is conducted by considering TomoSAR data as the input and Light\nDetection and Ranging (LiDAR) values as the ground truth. The experimental\nresults show striking advantages in both performance and generalization ability\nby leveraging context information within Multiple Baselines (MB) TomoSAR data\nacross different polarimetric modalities, surpassing existing techniques.\n", "link": "http://arxiv.org/abs/2403.20273v1", "date": "2024-03-29", "relevancy": 2.0242, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CATSNet%3A%20a%20context-aware%20network%20for%20Height%20Estimation%20in%20a%20Forested%0A%20%20Area%20based%20on%20Pol-TomoSAR%20data&body=Title%3A%20CATSNet%3A%20a%20context-aware%20network%20for%20Height%20Estimation%20in%20a%20Forested%0A%20%20Area%20based%20on%20Pol-TomoSAR%20data%0AAuthor%3A%20Wenyu%20Yang%20and%20Sergio%20Vitale%20and%20Hossein%20Aghababaei%20and%20Giampaolo%20Ferraioli%20and%20Vito%20Pascazio%20and%20Gilda%20Schirinzi%0AAbstract%3A%20%20%20Tropical%20forests%20are%20a%20key%20component%20of%20the%20global%20carbon%20cycle.%20With%20plans%0Afor%20upcoming%20space-borne%20missions%20like%20BIOMASS%20to%20monitor%20forestry%2C%20several%0Aairborne%20missions%2C%20including%20TropiSAR%20and%20AfriSAR%20campaigns%2C%20have%20been%0Asuccessfully%20launched%20and%20experimented.%20Typical%20Synthetic%20Aperture%20Radar%0ATomography%20%28TomoSAR%29%20methods%20involve%20complex%20models%20with%20low%20accuracy%20and%20high%0Acomputation%20costs.%20In%20recent%20years%2C%20deep%20learning%20methods%20have%20also%20gained%0Aattention%20in%20the%20TomoSAR%20framework%2C%20showing%20interesting%20performance.%20Recently%2C%0Aa%20solution%20based%20on%20a%20fully%20connected%20Tomographic%20Neural%20Network%20%28TSNN%29%20has%0Ademonstrated%20its%20effectiveness%20in%20accurately%20estimating%20forest%20and%20ground%0Aheights%20by%20exploiting%20the%20pixel-wise%20elements%20of%20the%20covariance%20matrix%20derived%0Afrom%20TomoSAR%20data.%20This%20work%20instead%20goes%20beyond%20the%20pixel-wise%20approach%20to%0Adefine%20a%20context-aware%20deep%20learning-based%20solution%20named%20CATSNet.%20A%0Aconvolutional%20neural%20network%20is%20considered%20to%20leverage%20patch-based%20information%0Aand%20extract%20features%20from%20a%20neighborhood%20rather%20than%20focus%20on%20a%20single%20pixel.%0AThe%20training%20is%20conducted%20by%20considering%20TomoSAR%20data%20as%20the%20input%20and%20Light%0ADetection%20and%20Ranging%20%28LiDAR%29%20values%20as%20the%20ground%20truth.%20The%20experimental%0Aresults%20show%20striking%20advantages%20in%20both%20performance%20and%20generalization%20ability%0Aby%20leveraging%20context%20information%20within%20Multiple%20Baselines%20%28MB%29%20TomoSAR%20data%0Aacross%20different%20polarimetric%20modalities%2C%20surpassing%20existing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20273v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CATSNet%3A%20a%20context-aware%20network%20for%20Height%20Estimation%20in%20a%20Forested%0A%20%20Area%20based%20on%20Pol-TomoSAR%20data&entry.906535625=Wenyu%20Yang%20and%20Sergio%20Vitale%20and%20Hossein%20Aghababaei%20and%20Giampaolo%20Ferraioli%20and%20Vito%20Pascazio%20and%20Gilda%20Schirinzi&entry.1292438233=%20%20Tropical%20forests%20are%20a%20key%20component%20of%20the%20global%20carbon%20cycle.%20With%20plans%0Afor%20upcoming%20space-borne%20missions%20like%20BIOMASS%20to%20monitor%20forestry%2C%20several%0Aairborne%20missions%2C%20including%20TropiSAR%20and%20AfriSAR%20campaigns%2C%20have%20been%0Asuccessfully%20launched%20and%20experimented.%20Typical%20Synthetic%20Aperture%20Radar%0ATomography%20%28TomoSAR%29%20methods%20involve%20complex%20models%20with%20low%20accuracy%20and%20high%0Acomputation%20costs.%20In%20recent%20years%2C%20deep%20learning%20methods%20have%20also%20gained%0Aattention%20in%20the%20TomoSAR%20framework%2C%20showing%20interesting%20performance.%20Recently%2C%0Aa%20solution%20based%20on%20a%20fully%20connected%20Tomographic%20Neural%20Network%20%28TSNN%29%20has%0Ademonstrated%20its%20effectiveness%20in%20accurately%20estimating%20forest%20and%20ground%0Aheights%20by%20exploiting%20the%20pixel-wise%20elements%20of%20the%20covariance%20matrix%20derived%0Afrom%20TomoSAR%20data.%20This%20work%20instead%20goes%20beyond%20the%20pixel-wise%20approach%20to%0Adefine%20a%20context-aware%20deep%20learning-based%20solution%20named%20CATSNet.%20A%0Aconvolutional%20neural%20network%20is%20considered%20to%20leverage%20patch-based%20information%0Aand%20extract%20features%20from%20a%20neighborhood%20rather%20than%20focus%20on%20a%20single%20pixel.%0AThe%20training%20is%20conducted%20by%20considering%20TomoSAR%20data%20as%20the%20input%20and%20Light%0ADetection%20and%20Ranging%20%28LiDAR%29%20values%20as%20the%20ground%20truth.%20The%20experimental%0Aresults%20show%20striking%20advantages%20in%20both%20performance%20and%20generalization%20ability%0Aby%20leveraging%20context%20information%20within%20Multiple%20Baselines%20%28MB%29%20TomoSAR%20data%0Aacross%20different%20polarimetric%20modalities%2C%20surpassing%20existing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20273v1&entry.124074799=Read"},
{"title": "Genetic Quantization-Aware Approximation for Non-Linear Operations in\n  Transformers", "author": "Pingcheng Dong and Yonghao Tan and Dong Zhang and Tianwei Ni and Xuejiao Liu and Yu Liu and Peng Luo and Luhong Liang and Shih-Yang Liu and Xijie Huang and Huaiyu Zhu and Yun Pan and Fengwei An and Kwang-Ting Cheng", "abstract": "  Non-linear functions are prevalent in Transformers and their lightweight\nvariants, incurring substantial and frequently underestimated hardware costs.\nPrevious state-of-the-art works optimize these operations by piece-wise linear\napproximation and store the parameters in look-up tables (LUT), but most of\nthem require unfriendly high-precision arithmetics such as FP/INT 32 and lack\nconsideration of integer-only INT quantization. This paper proposed a genetic\nLUT-Approximation algorithm namely GQA-LUT that can automatically determine the\nparameters with quantization awareness. The results demonstrate that GQA-LUT\nachieves negligible degradation on the challenging semantic segmentation task\nfor both vanilla and linear Transformer models. Besides, proposed GQA-LUT\nenables the employment of INT8-based LUT-Approximation that achieves an area\nsavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the\nhigh-precision FP/INT 32 alternatives. Code is available at https://\ngithub.com/PingchengDong/GQA-LUT.\n", "link": "http://arxiv.org/abs/2403.19591v2", "date": "2024-03-29", "relevancy": 2.0207, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5061}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4913}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers&body=Title%3A%20Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers%0AAuthor%3A%20Pingcheng%20Dong%20and%20Yonghao%20Tan%20and%20Dong%20Zhang%20and%20Tianwei%20Ni%20and%20Xuejiao%20Liu%20and%20Yu%20Liu%20and%20Peng%20Luo%20and%20Luhong%20Liang%20and%20Shih-Yang%20Liu%20and%20Xijie%20Huang%20and%20Huaiyu%20Zhu%20and%20Yun%20Pan%20and%20Fengwei%20An%20and%20Kwang-Ting%20Cheng%0AAbstract%3A%20%20%20Non-linear%20functions%20are%20prevalent%20in%20Transformers%20and%20their%20lightweight%0Avariants%2C%20incurring%20substantial%20and%20frequently%20underestimated%20hardware%20costs.%0APrevious%20state-of-the-art%20works%20optimize%20these%20operations%20by%20piece-wise%20linear%0Aapproximation%20and%20store%20the%20parameters%20in%20look-up%20tables%20%28LUT%29%2C%20but%20most%20of%0Athem%20require%20unfriendly%20high-precision%20arithmetics%20such%20as%20FP/INT%2032%20and%20lack%0Aconsideration%20of%20integer-only%20INT%20quantization.%20This%20paper%20proposed%20a%20genetic%0ALUT-Approximation%20algorithm%20namely%20GQA-LUT%20that%20can%20automatically%20determine%20the%0Aparameters%20with%20quantization%20awareness.%20The%20results%20demonstrate%20that%20GQA-LUT%0Aachieves%20negligible%20degradation%20on%20the%20challenging%20semantic%20segmentation%20task%0Afor%20both%20vanilla%20and%20linear%20Transformer%20models.%20Besides%2C%20proposed%20GQA-LUT%0Aenables%20the%20employment%20of%20INT8-based%20LUT-Approximation%20that%20achieves%20an%20area%0Asavings%20of%2081.3~81.7%25%20and%20a%20power%20reduction%20of%2079.3~80.2%25%20compared%20to%20the%0Ahigh-precision%20FP/INT%2032%20alternatives.%20Code%20is%20available%20at%20https%3A//%0Agithub.com/PingchengDong/GQA-LUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19591v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers&entry.906535625=Pingcheng%20Dong%20and%20Yonghao%20Tan%20and%20Dong%20Zhang%20and%20Tianwei%20Ni%20and%20Xuejiao%20Liu%20and%20Yu%20Liu%20and%20Peng%20Luo%20and%20Luhong%20Liang%20and%20Shih-Yang%20Liu%20and%20Xijie%20Huang%20and%20Huaiyu%20Zhu%20and%20Yun%20Pan%20and%20Fengwei%20An%20and%20Kwang-Ting%20Cheng&entry.1292438233=%20%20Non-linear%20functions%20are%20prevalent%20in%20Transformers%20and%20their%20lightweight%0Avariants%2C%20incurring%20substantial%20and%20frequently%20underestimated%20hardware%20costs.%0APrevious%20state-of-the-art%20works%20optimize%20these%20operations%20by%20piece-wise%20linear%0Aapproximation%20and%20store%20the%20parameters%20in%20look-up%20tables%20%28LUT%29%2C%20but%20most%20of%0Athem%20require%20unfriendly%20high-precision%20arithmetics%20such%20as%20FP/INT%2032%20and%20lack%0Aconsideration%20of%20integer-only%20INT%20quantization.%20This%20paper%20proposed%20a%20genetic%0ALUT-Approximation%20algorithm%20namely%20GQA-LUT%20that%20can%20automatically%20determine%20the%0Aparameters%20with%20quantization%20awareness.%20The%20results%20demonstrate%20that%20GQA-LUT%0Aachieves%20negligible%20degradation%20on%20the%20challenging%20semantic%20segmentation%20task%0Afor%20both%20vanilla%20and%20linear%20Transformer%20models.%20Besides%2C%20proposed%20GQA-LUT%0Aenables%20the%20employment%20of%20INT8-based%20LUT-Approximation%20that%20achieves%20an%20area%0Asavings%20of%2081.3~81.7%25%20and%20a%20power%20reduction%20of%2079.3~80.2%25%20compared%20to%20the%0Ahigh-precision%20FP/INT%2032%20alternatives.%20Code%20is%20available%20at%20https%3A//%0Agithub.com/PingchengDong/GQA-LUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19591v2&entry.124074799=Read"},
{"title": "Dual-Channel Multiplex Graph Neural Networks for Recommendation", "author": "Xiang Li and Chaofan Fu and Zhongying Zhao and Guanjie Zheng and Chao Huang and Junyu Dong and Yanwei Yu", "abstract": "  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n", "link": "http://arxiv.org/abs/2403.11624v3", "date": "2024-03-29", "relevancy": 2.0197, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5363}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4813}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation&body=Title%3A%20Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation%0AAuthor%3A%20Xiang%20Li%20and%20Chaofan%20Fu%20and%20Zhongying%20Zhao%20and%20Guanjie%20Zheng%20and%20Chao%20Huang%20and%20Junyu%20Dong%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Efficient%20recommender%20systems%20play%20a%20crucial%20role%20in%20accurately%20capturing%0Auser%20and%20item%20attributes%20that%20mirror%20individual%20preferences.%20Some%20existing%0Arecommendation%20techniques%20have%20started%20to%20shift%20their%20focus%20towards%20modeling%0Avarious%20types%20of%20interaction%20relations%20between%20users%20and%20items%20in%20real-world%0Arecommendation%20scenarios%2C%20such%20as%20clicks%2C%20marking%20favorites%2C%20and%20purchases%20on%0Aonline%20shopping%20platforms.%20Nevertheless%2C%20these%20approaches%20still%20grapple%20with%0Atwo%20significant%20shortcomings%3A%20%281%29%20Insufficient%20modeling%20and%20exploitation%20of%20the%0Aimpact%20of%20various%20behavior%20patterns%20formed%20by%20multiplex%20relations%20between%20users%0Aand%20items%20on%20representation%20learning%2C%20and%20%282%29%20ignoring%20the%20effect%20of%20different%0Arelations%20in%20the%20behavior%20patterns%20on%20the%20target%20relation%20in%20recommender%20system%0Ascenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20recommendation%20framework%2C%0ADual-Channel%20Multiplex%20Graph%20Neural%20Network%20%28DCMGNN%29%2C%20which%20addresses%20the%0Aaforementioned%20challenges.%20It%20incorporates%20an%20explicit%20behavior%20pattern%0Arepresentation%20learner%20to%20capture%20the%20behavior%20patterns%20composed%20of%20multiplex%0Auser-item%20interaction%20relations%2C%20and%20includes%20a%20relation%20chain%20representation%0Alearning%20and%20a%20relation%20chain-aware%20encoder%20to%20discover%20the%20impact%20of%20various%0Aauxiliary%20relations%20on%20the%20target%20relation%2C%20the%20dependencies%20between%20different%0Arelations%2C%20and%20mine%20the%20appropriate%20order%20of%20relations%20in%20a%20behavior%20pattern.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20our%20%5Cmodel%0Asurpasses%20various%20state-of-the-art%20recommendation%20methods.%20It%20outperforms%20the%0Abest%20baselines%20by%2010.06%5C%25%20and%2012.15%5C%25%20on%20average%20across%20all%20datasets%20in%20terms%0Aof%20R%4010%20and%20N%4010%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11624v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation&entry.906535625=Xiang%20Li%20and%20Chaofan%20Fu%20and%20Zhongying%20Zhao%20and%20Guanjie%20Zheng%20and%20Chao%20Huang%20and%20Junyu%20Dong%20and%20Yanwei%20Yu&entry.1292438233=%20%20Efficient%20recommender%20systems%20play%20a%20crucial%20role%20in%20accurately%20capturing%0Auser%20and%20item%20attributes%20that%20mirror%20individual%20preferences.%20Some%20existing%0Arecommendation%20techniques%20have%20started%20to%20shift%20their%20focus%20towards%20modeling%0Avarious%20types%20of%20interaction%20relations%20between%20users%20and%20items%20in%20real-world%0Arecommendation%20scenarios%2C%20such%20as%20clicks%2C%20marking%20favorites%2C%20and%20purchases%20on%0Aonline%20shopping%20platforms.%20Nevertheless%2C%20these%20approaches%20still%20grapple%20with%0Atwo%20significant%20shortcomings%3A%20%281%29%20Insufficient%20modeling%20and%20exploitation%20of%20the%0Aimpact%20of%20various%20behavior%20patterns%20formed%20by%20multiplex%20relations%20between%20users%0Aand%20items%20on%20representation%20learning%2C%20and%20%282%29%20ignoring%20the%20effect%20of%20different%0Arelations%20in%20the%20behavior%20patterns%20on%20the%20target%20relation%20in%20recommender%20system%0Ascenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20recommendation%20framework%2C%0ADual-Channel%20Multiplex%20Graph%20Neural%20Network%20%28DCMGNN%29%2C%20which%20addresses%20the%0Aaforementioned%20challenges.%20It%20incorporates%20an%20explicit%20behavior%20pattern%0Arepresentation%20learner%20to%20capture%20the%20behavior%20patterns%20composed%20of%20multiplex%0Auser-item%20interaction%20relations%2C%20and%20includes%20a%20relation%20chain%20representation%0Alearning%20and%20a%20relation%20chain-aware%20encoder%20to%20discover%20the%20impact%20of%20various%0Aauxiliary%20relations%20on%20the%20target%20relation%2C%20the%20dependencies%20between%20different%0Arelations%2C%20and%20mine%20the%20appropriate%20order%20of%20relations%20in%20a%20behavior%20pattern.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20our%20%5Cmodel%0Asurpasses%20various%20state-of-the-art%20recommendation%20methods.%20It%20outperforms%20the%0Abest%20baselines%20by%2010.06%5C%25%20and%2012.15%5C%25%20on%20average%20across%20all%20datasets%20in%20terms%0Aof%20R%4010%20and%20N%4010%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11624v3&entry.124074799=Read"},
{"title": "Distributed agency in second language learning and teaching through\n  generative AI", "author": "Robert Godwin-Jones", "abstract": "  Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.\n", "link": "http://arxiv.org/abs/2403.20216v1", "date": "2024-03-29", "relevancy": 2.0151, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5571}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4589}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&body=Title%3A%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI%0AAuthor%3A%20Robert%20Godwin-Jones%0AAbstract%3A%20%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20216v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&entry.906535625=Robert%20Godwin-Jones&entry.1292438233=%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20216v1&entry.124074799=Read"},
{"title": "InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40\n  Seconds", "author": "Zhiwen Fan and Wenyan Cong and Kairun Wen and Kevin Wang and Jian Zhang and Xinghao Ding and Danfei Xu and Boris Ivanovic and Marco Pavone and Georgios Pavlakos and Zhangyang Wang and Yue Wang", "abstract": "  While novel view synthesis (NVS) has made substantial progress in 3D computer\nvision, it typically requires an initial estimation of camera intrinsics and\nextrinsics from dense viewpoints. This pre-processing is usually conducted via\na Structure-from-Motion (SfM) pipeline, a procedure that can be slow and\nunreliable, particularly in sparse-view scenarios with insufficient matched\nfeatures for accurate reconstruction. In this work, we integrate the strengths\nof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with\nend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved\nissues in NVS under unconstrained settings, which encompasses pose-free and\nsparse view challenges. Our framework, InstantSplat, unifies dense stereo\npriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &\npose-free images in less than 1 minute. Specifically, InstantSplat comprises a\nCoarse Geometric Initialization (CGI) module that swiftly establishes a\npreliminary scene structure and camera parameters across all training views,\nutilizing globally-aligned 3D point maps derived from a pre-trained dense\nstereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)\nmodule, which jointly optimizes the 3D Gaussian attributes and the initialized\nposes with pose regularization. Experiments conducted on the large-scale\noutdoor Tanks & Temples datasets demonstrate that InstantSplat significantly\nimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error\n(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios\ninvolving posefree and sparse-view conditions. Project page:\ninstantsplat.github.io.\n", "link": "http://arxiv.org/abs/2403.20309v1", "date": "2024-03-29", "relevancy": 2.0119, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5376}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4987}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4934}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstantSplat%3A%20Unbounded%20Sparse-view%20Pose-free%20Gaussian%20Splatting%20in%2040%0A%20%20Seconds&body=Title%3A%20InstantSplat%3A%20Unbounded%20Sparse-view%20Pose-free%20Gaussian%20Splatting%20in%2040%0A%20%20Seconds%0AAuthor%3A%20Zhiwen%20Fan%20and%20Wenyan%20Cong%20and%20Kairun%20Wen%20and%20Kevin%20Wang%20and%20Jian%20Zhang%20and%20Xinghao%20Ding%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang%20and%20Yue%20Wang%0AAbstract%3A%20%20%20While%20novel%20view%20synthesis%20%28NVS%29%20has%20made%20substantial%20progress%20in%203D%20computer%0Avision%2C%20it%20typically%20requires%20an%20initial%20estimation%20of%20camera%20intrinsics%20and%0Aextrinsics%20from%20dense%20viewpoints.%20This%20pre-processing%20is%20usually%20conducted%20via%0Aa%20Structure-from-Motion%20%28SfM%29%20pipeline%2C%20a%20procedure%20that%20can%20be%20slow%20and%0Aunreliable%2C%20particularly%20in%20sparse-view%20scenarios%20with%20insufficient%20matched%0Afeatures%20for%20accurate%20reconstruction.%20In%20this%20work%2C%20we%20integrate%20the%20strengths%0Aof%20point-based%20representations%20%28e.g.%2C%203D%20Gaussian%20Splatting%2C%203D-GS%29%20with%0Aend-to-end%20dense%20stereo%20models%20%28DUSt3R%29%20to%20tackle%20the%20complex%20yet%20unresolved%0Aissues%20in%20NVS%20under%20unconstrained%20settings%2C%20which%20encompasses%20pose-free%20and%0Asparse%20view%20challenges.%20Our%20framework%2C%20InstantSplat%2C%20unifies%20dense%20stereo%0Apriors%20with%203D-GS%20to%20build%203D%20Gaussians%20of%20large-scale%20scenes%20from%20sparseview%20%26%0Apose-free%20images%20in%20less%20than%201%20minute.%20Specifically%2C%20InstantSplat%20comprises%20a%0ACoarse%20Geometric%20Initialization%20%28CGI%29%20module%20that%20swiftly%20establishes%20a%0Apreliminary%20scene%20structure%20and%20camera%20parameters%20across%20all%20training%20views%2C%0Autilizing%20globally-aligned%203D%20point%20maps%20derived%20from%20a%20pre-trained%20dense%0Astereo%20pipeline.%20This%20is%20followed%20by%20the%20Fast%203D-Gaussian%20Optimization%20%28F-3DGO%29%0Amodule%2C%20which%20jointly%20optimizes%20the%203D%20Gaussian%20attributes%20and%20the%20initialized%0Aposes%20with%20pose%20regularization.%20Experiments%20conducted%20on%20the%20large-scale%0Aoutdoor%20Tanks%20%26%20Temples%20datasets%20demonstrate%20that%20InstantSplat%20significantly%0Aimproves%20SSIM%20%28by%2032%25%29%20while%20concurrently%20reducing%20Absolute%20Trajectory%20Error%0A%28ATE%29%20by%2080%25.%20These%20establish%20InstantSplat%20as%20a%20viable%20solution%20for%20scenarios%0Ainvolving%20posefree%20and%20sparse-view%20conditions.%20Project%20page%3A%0Ainstantsplat.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantSplat%3A%20Unbounded%20Sparse-view%20Pose-free%20Gaussian%20Splatting%20in%2040%0A%20%20Seconds&entry.906535625=Zhiwen%20Fan%20and%20Wenyan%20Cong%20and%20Kairun%20Wen%20and%20Kevin%20Wang%20and%20Jian%20Zhang%20and%20Xinghao%20Ding%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang%20and%20Yue%20Wang&entry.1292438233=%20%20While%20novel%20view%20synthesis%20%28NVS%29%20has%20made%20substantial%20progress%20in%203D%20computer%0Avision%2C%20it%20typically%20requires%20an%20initial%20estimation%20of%20camera%20intrinsics%20and%0Aextrinsics%20from%20dense%20viewpoints.%20This%20pre-processing%20is%20usually%20conducted%20via%0Aa%20Structure-from-Motion%20%28SfM%29%20pipeline%2C%20a%20procedure%20that%20can%20be%20slow%20and%0Aunreliable%2C%20particularly%20in%20sparse-view%20scenarios%20with%20insufficient%20matched%0Afeatures%20for%20accurate%20reconstruction.%20In%20this%20work%2C%20we%20integrate%20the%20strengths%0Aof%20point-based%20representations%20%28e.g.%2C%203D%20Gaussian%20Splatting%2C%203D-GS%29%20with%0Aend-to-end%20dense%20stereo%20models%20%28DUSt3R%29%20to%20tackle%20the%20complex%20yet%20unresolved%0Aissues%20in%20NVS%20under%20unconstrained%20settings%2C%20which%20encompasses%20pose-free%20and%0Asparse%20view%20challenges.%20Our%20framework%2C%20InstantSplat%2C%20unifies%20dense%20stereo%0Apriors%20with%203D-GS%20to%20build%203D%20Gaussians%20of%20large-scale%20scenes%20from%20sparseview%20%26%0Apose-free%20images%20in%20less%20than%201%20minute.%20Specifically%2C%20InstantSplat%20comprises%20a%0ACoarse%20Geometric%20Initialization%20%28CGI%29%20module%20that%20swiftly%20establishes%20a%0Apreliminary%20scene%20structure%20and%20camera%20parameters%20across%20all%20training%20views%2C%0Autilizing%20globally-aligned%203D%20point%20maps%20derived%20from%20a%20pre-trained%20dense%0Astereo%20pipeline.%20This%20is%20followed%20by%20the%20Fast%203D-Gaussian%20Optimization%20%28F-3DGO%29%0Amodule%2C%20which%20jointly%20optimizes%20the%203D%20Gaussian%20attributes%20and%20the%20initialized%0Aposes%20with%20pose%20regularization.%20Experiments%20conducted%20on%20the%20large-scale%0Aoutdoor%20Tanks%20%26%20Temples%20datasets%20demonstrate%20that%20InstantSplat%20significantly%0Aimproves%20SSIM%20%28by%2032%25%29%20while%20concurrently%20reducing%20Absolute%20Trajectory%20Error%0A%28ATE%29%20by%2080%25.%20These%20establish%20InstantSplat%20as%20a%20viable%20solution%20for%20scenarios%0Ainvolving%20posefree%20and%20sparse-view%20conditions.%20Project%20page%3A%0Ainstantsplat.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20309v1&entry.124074799=Read"},
{"title": "SERNet-Former: Semantic Segmentation by Efficient Residual Network with\n  Attention-Boosting Gates and Attention-Fusion Networks", "author": "Serdar Erisen", "abstract": "  Improving the efficiency of state-of-the-art methods in semantic segmentation\nrequires overcoming the increasing computational cost as well as issues such as\nfusing semantic information from global and local contexts. Based on the recent\nsuccess and problems that convolutional neural networks (CNNs) encounter in\nsemantic segmentation, this research proposes an encoder-decoder architecture\nwith a unique efficient residual network, Efficient-ResNet. Attention-boosting\ngates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to\nfuse the equivariant and feature-based semantic information with the equivalent\nsizes of the output of global context of the efficient residual network in the\nencoder. Respectively, the decoder network is developed with the additional\nattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve\nthe efficiency in the one-to-one conversion of the semantic information by\ndeploying additional convolution layers in the decoder part. Our network is\ntested on the challenging CamVid and Cityscapes datasets, and the proposed\nmethods reveal significant improvements on the residual networks. To the best\nof our knowledge, the developed network, SERNet-Former, achieves\nstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challenging\nresults (87.35 % mean IoU) on Cityscapes validation dataset.\n", "link": "http://arxiv.org/abs/2401.15741v4", "date": "2024-03-29", "relevancy": 2.0021, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5082}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks&body=Title%3A%20SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks%0AAuthor%3A%20Serdar%20Erisen%0AAbstract%3A%20%20%20Improving%20the%20efficiency%20of%20state-of-the-art%20methods%20in%20semantic%20segmentation%0Arequires%20overcoming%20the%20increasing%20computational%20cost%20as%20well%20as%20issues%20such%20as%0Afusing%20semantic%20information%20from%20global%20and%20local%20contexts.%20Based%20on%20the%20recent%0Asuccess%20and%20problems%20that%20convolutional%20neural%20networks%20%28CNNs%29%20encounter%20in%0Asemantic%20segmentation%2C%20this%20research%20proposes%20an%20encoder-decoder%20architecture%0Awith%20a%20unique%20efficient%20residual%20network%2C%20Efficient-ResNet.%20Attention-boosting%0Agates%20%28AbGs%29%20and%20attention-boosting%20modules%20%28AbMs%29%20are%20deployed%20by%20aiming%20to%0Afuse%20the%20equivariant%20and%20feature-based%20semantic%20information%20with%20the%20equivalent%0Asizes%20of%20the%20output%20of%20global%20context%20of%20the%20efficient%20residual%20network%20in%20the%0Aencoder.%20Respectively%2C%20the%20decoder%20network%20is%20developed%20with%20the%20additional%0Aattention-fusion%20networks%20%28AfNs%29%20inspired%20by%20AbM.%20AfNs%20are%20designed%20to%20improve%0Athe%20efficiency%20in%20the%20one-to-one%20conversion%20of%20the%20semantic%20information%20by%0Adeploying%20additional%20convolution%20layers%20in%20the%20decoder%20part.%20Our%20network%20is%0Atested%20on%20the%20challenging%20CamVid%20and%20Cityscapes%20datasets%2C%20and%20the%20proposed%0Amethods%20reveal%20significant%20improvements%20on%20the%20residual%20networks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20the%20developed%20network%2C%20SERNet-Former%2C%20achieves%0Astate-of-the-art%20results%20%2884.62%20%25%20mean%20IoU%29%20on%20CamVid%20dataset%20and%20challenging%0Aresults%20%2887.35%20%25%20mean%20IoU%29%20on%20Cityscapes%20validation%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15741v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERNet-Former%3A%20Semantic%20Segmentation%20by%20Efficient%20Residual%20Network%20with%0A%20%20Attention-Boosting%20Gates%20and%20Attention-Fusion%20Networks&entry.906535625=Serdar%20Erisen&entry.1292438233=%20%20Improving%20the%20efficiency%20of%20state-of-the-art%20methods%20in%20semantic%20segmentation%0Arequires%20overcoming%20the%20increasing%20computational%20cost%20as%20well%20as%20issues%20such%20as%0Afusing%20semantic%20information%20from%20global%20and%20local%20contexts.%20Based%20on%20the%20recent%0Asuccess%20and%20problems%20that%20convolutional%20neural%20networks%20%28CNNs%29%20encounter%20in%0Asemantic%20segmentation%2C%20this%20research%20proposes%20an%20encoder-decoder%20architecture%0Awith%20a%20unique%20efficient%20residual%20network%2C%20Efficient-ResNet.%20Attention-boosting%0Agates%20%28AbGs%29%20and%20attention-boosting%20modules%20%28AbMs%29%20are%20deployed%20by%20aiming%20to%0Afuse%20the%20equivariant%20and%20feature-based%20semantic%20information%20with%20the%20equivalent%0Asizes%20of%20the%20output%20of%20global%20context%20of%20the%20efficient%20residual%20network%20in%20the%0Aencoder.%20Respectively%2C%20the%20decoder%20network%20is%20developed%20with%20the%20additional%0Aattention-fusion%20networks%20%28AfNs%29%20inspired%20by%20AbM.%20AfNs%20are%20designed%20to%20improve%0Athe%20efficiency%20in%20the%20one-to-one%20conversion%20of%20the%20semantic%20information%20by%0Adeploying%20additional%20convolution%20layers%20in%20the%20decoder%20part.%20Our%20network%20is%0Atested%20on%20the%20challenging%20CamVid%20and%20Cityscapes%20datasets%2C%20and%20the%20proposed%0Amethods%20reveal%20significant%20improvements%20on%20the%20residual%20networks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20the%20developed%20network%2C%20SERNet-Former%2C%20achieves%0Astate-of-the-art%20results%20%2884.62%20%25%20mean%20IoU%29%20on%20CamVid%20dataset%20and%20challenging%0Aresults%20%2887.35%20%25%20mean%20IoU%29%20on%20Cityscapes%20validation%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15741v4&entry.124074799=Read"},
{"title": "LipSim: A Provably Robust Perceptual Similarity Metric", "author": "Sara Ghazanfari and Alexandre Araujo and Prashanth Krishnamurthy and Farshad Khorrami and Siddharth Garg", "abstract": "  Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.\n", "link": "http://arxiv.org/abs/2310.18274v2", "date": "2024-03-29", "relevancy": 1.9893, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LipSim%3A%20A%20Provably%20Robust%20Perceptual%20Similarity%20Metric&body=Title%3A%20LipSim%3A%20A%20Provably%20Robust%20Perceptual%20Similarity%20Metric%0AAuthor%3A%20Sara%20Ghazanfari%20and%20Alexandre%20Araujo%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%20and%20Siddharth%20Garg%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20growing%20interest%20in%20developing%20and%20applying%20perceptual%0Asimilarity%20metrics.%20Research%20has%20shown%20the%20superiority%20of%20perceptual%20metrics%0Aover%20pixel-wise%20metrics%20in%20aligning%20with%20human%20perception%20and%20serving%20as%20a%0Aproxy%20for%20the%20human%20visual%20system.%20On%20the%20other%20hand%2C%20as%20perceptual%20metrics%0Arely%20on%20neural%20networks%2C%20there%20is%20a%20growing%20concern%20regarding%20their%20resilience%2C%0Agiven%20the%20established%20vulnerability%20of%20neural%20networks%20to%20adversarial%20attacks.%0AIt%20is%20indeed%20logical%20to%20infer%20that%20perceptual%20metrics%20may%20inherit%20both%20the%0Astrengths%20and%20shortcomings%20of%20neural%20networks.%20In%20this%20work%2C%20we%20demonstrate%20the%0Avulnerability%20of%20state-of-the-art%20perceptual%20similarity%20metrics%20based%20on%20an%0Aensemble%20of%20ViT-based%20feature%20extractors%20to%20adversarial%20attacks.%20We%20then%0Apropose%20a%20framework%20to%20train%20a%20robust%20perceptual%20similarity%20metric%20called%0ALipSim%20%28Lipschitz%20Similarity%20Metric%29%20with%20provable%20guarantees.%20By%20leveraging%0A1-Lipschitz%20neural%20networks%20as%20the%20backbone%2C%20LipSim%20provides%20guarded%20areas%0Aaround%20each%20data%20point%20and%20certificates%20for%20all%20perturbations%20within%20an%0A%24%5Cell_2%24%20ball.%20Finally%2C%20a%20comprehensive%20set%20of%20experiments%20shows%20the%0Aperformance%20of%20LipSim%20in%20terms%20of%20natural%20and%20certified%20scores%20and%20on%20the%20image%0Aretrieval%20application.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/SaraGhazanfari/LipSim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18274v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LipSim%3A%20A%20Provably%20Robust%20Perceptual%20Similarity%20Metric&entry.906535625=Sara%20Ghazanfari%20and%20Alexandre%20Araujo%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%20and%20Siddharth%20Garg&entry.1292438233=%20%20Recent%20years%20have%20seen%20growing%20interest%20in%20developing%20and%20applying%20perceptual%0Asimilarity%20metrics.%20Research%20has%20shown%20the%20superiority%20of%20perceptual%20metrics%0Aover%20pixel-wise%20metrics%20in%20aligning%20with%20human%20perception%20and%20serving%20as%20a%0Aproxy%20for%20the%20human%20visual%20system.%20On%20the%20other%20hand%2C%20as%20perceptual%20metrics%0Arely%20on%20neural%20networks%2C%20there%20is%20a%20growing%20concern%20regarding%20their%20resilience%2C%0Agiven%20the%20established%20vulnerability%20of%20neural%20networks%20to%20adversarial%20attacks.%0AIt%20is%20indeed%20logical%20to%20infer%20that%20perceptual%20metrics%20may%20inherit%20both%20the%0Astrengths%20and%20shortcomings%20of%20neural%20networks.%20In%20this%20work%2C%20we%20demonstrate%20the%0Avulnerability%20of%20state-of-the-art%20perceptual%20similarity%20metrics%20based%20on%20an%0Aensemble%20of%20ViT-based%20feature%20extractors%20to%20adversarial%20attacks.%20We%20then%0Apropose%20a%20framework%20to%20train%20a%20robust%20perceptual%20similarity%20metric%20called%0ALipSim%20%28Lipschitz%20Similarity%20Metric%29%20with%20provable%20guarantees.%20By%20leveraging%0A1-Lipschitz%20neural%20networks%20as%20the%20backbone%2C%20LipSim%20provides%20guarded%20areas%0Aaround%20each%20data%20point%20and%20certificates%20for%20all%20perturbations%20within%20an%0A%24%5Cell_2%24%20ball.%20Finally%2C%20a%20comprehensive%20set%20of%20experiments%20shows%20the%0Aperformance%20of%20LipSim%20in%20terms%20of%20natural%20and%20certified%20scores%20and%20on%20the%20image%0Aretrieval%20application.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/SaraGhazanfari/LipSim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18274v2&entry.124074799=Read"},
{"title": "Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via\n  Negations", "author": "Jaisidh Singh and Ishaan Shrivastava and Mayank Vatsa and Richa Singh and Aparna Bharati", "abstract": "  Existing vision-language models (VLMs) treat text descriptions as a unit,\nconfusing individual concepts in a prompt and impairing visual semantic\nmatching and reasoning. An important aspect of reasoning in logic and language\nis negations. This paper highlights the limitations of popular VLMs such as\nCLIP, at understanding the implications of negations, i.e., the effect of the\nword \"not\" in a given prompt. To enable evaluation of VLMs on fluent prompts\nwith negations, we present CC-Neg, a dataset containing 228,246 images, true\ncaptions and their corresponding negated captions. Using CC-Neg along with\nmodifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework,\nhas an improved understanding of negations. This training paradigm improves\nCoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average\ngain in top-1 accuracy for zero-shot image classification across 8 datasets.\nFurther, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks\nsuch as SugarCREPE by 4.4%, showcasing emergent compositional understanding of\nobjects, relations, and attributes in text. Overall, our work addresses a\ncrucial limitation of VLMs by introducing a dataset and framework that\nstrengthens semantic associations between images and text, demonstrating\nimproved large-scale foundation models with significantly reduced computational\ncost, promoting efficiency and accessibility.\n", "link": "http://arxiv.org/abs/2403.20312v1", "date": "2024-03-29", "relevancy": 1.9795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.494}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4886}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learn%20%22No%22%20to%20Say%20%22Yes%22%20Better%3A%20Improving%20Vision-Language%20Models%20via%0A%20%20Negations&body=Title%3A%20Learn%20%22No%22%20to%20Say%20%22Yes%22%20Better%3A%20Improving%20Vision-Language%20Models%20via%0A%20%20Negations%0AAuthor%3A%20Jaisidh%20Singh%20and%20Ishaan%20Shrivastava%20and%20Mayank%20Vatsa%20and%20Richa%20Singh%20and%20Aparna%20Bharati%0AAbstract%3A%20%20%20Existing%20vision-language%20models%20%28VLMs%29%20treat%20text%20descriptions%20as%20a%20unit%2C%0Aconfusing%20individual%20concepts%20in%20a%20prompt%20and%20impairing%20visual%20semantic%0Amatching%20and%20reasoning.%20An%20important%20aspect%20of%20reasoning%20in%20logic%20and%20language%0Ais%20negations.%20This%20paper%20highlights%20the%20limitations%20of%20popular%20VLMs%20such%20as%0ACLIP%2C%20at%20understanding%20the%20implications%20of%20negations%2C%20i.e.%2C%20the%20effect%20of%20the%0Aword%20%22not%22%20in%20a%20given%20prompt.%20To%20enable%20evaluation%20of%20VLMs%20on%20fluent%20prompts%0Awith%20negations%2C%20we%20present%20CC-Neg%2C%20a%20dataset%20containing%20228%2C246%20images%2C%20true%0Acaptions%20and%20their%20corresponding%20negated%20captions.%20Using%20CC-Neg%20along%20with%0Amodifications%20to%20the%20contrastive%20loss%20of%20CLIP%2C%20our%20proposed%20CoN-CLIP%20framework%2C%0Ahas%20an%20improved%20understanding%20of%20negations.%20This%20training%20paradigm%20improves%0ACoN-CLIP%27s%20ability%20to%20encode%20semantics%20reliably%2C%20resulting%20in%203.85%25%20average%0Again%20in%20top-1%20accuracy%20for%20zero-shot%20image%20classification%20across%208%20datasets.%0AFurther%2C%20CoN-CLIP%20outperforms%20CLIP%20on%20challenging%20compositionality%20benchmarks%0Asuch%20as%20SugarCREPE%20by%204.4%25%2C%20showcasing%20emergent%20compositional%20understanding%20of%0Aobjects%2C%20relations%2C%20and%20attributes%20in%20text.%20Overall%2C%20our%20work%20addresses%20a%0Acrucial%20limitation%20of%20VLMs%20by%20introducing%20a%20dataset%20and%20framework%20that%0Astrengthens%20semantic%20associations%20between%20images%20and%20text%2C%20demonstrating%0Aimproved%20large-scale%20foundation%20models%20with%20significantly%20reduced%20computational%0Acost%2C%20promoting%20efficiency%20and%20accessibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20312v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20%22No%22%20to%20Say%20%22Yes%22%20Better%3A%20Improving%20Vision-Language%20Models%20via%0A%20%20Negations&entry.906535625=Jaisidh%20Singh%20and%20Ishaan%20Shrivastava%20and%20Mayank%20Vatsa%20and%20Richa%20Singh%20and%20Aparna%20Bharati&entry.1292438233=%20%20Existing%20vision-language%20models%20%28VLMs%29%20treat%20text%20descriptions%20as%20a%20unit%2C%0Aconfusing%20individual%20concepts%20in%20a%20prompt%20and%20impairing%20visual%20semantic%0Amatching%20and%20reasoning.%20An%20important%20aspect%20of%20reasoning%20in%20logic%20and%20language%0Ais%20negations.%20This%20paper%20highlights%20the%20limitations%20of%20popular%20VLMs%20such%20as%0ACLIP%2C%20at%20understanding%20the%20implications%20of%20negations%2C%20i.e.%2C%20the%20effect%20of%20the%0Aword%20%22not%22%20in%20a%20given%20prompt.%20To%20enable%20evaluation%20of%20VLMs%20on%20fluent%20prompts%0Awith%20negations%2C%20we%20present%20CC-Neg%2C%20a%20dataset%20containing%20228%2C246%20images%2C%20true%0Acaptions%20and%20their%20corresponding%20negated%20captions.%20Using%20CC-Neg%20along%20with%0Amodifications%20to%20the%20contrastive%20loss%20of%20CLIP%2C%20our%20proposed%20CoN-CLIP%20framework%2C%0Ahas%20an%20improved%20understanding%20of%20negations.%20This%20training%20paradigm%20improves%0ACoN-CLIP%27s%20ability%20to%20encode%20semantics%20reliably%2C%20resulting%20in%203.85%25%20average%0Again%20in%20top-1%20accuracy%20for%20zero-shot%20image%20classification%20across%208%20datasets.%0AFurther%2C%20CoN-CLIP%20outperforms%20CLIP%20on%20challenging%20compositionality%20benchmarks%0Asuch%20as%20SugarCREPE%20by%204.4%25%2C%20showcasing%20emergent%20compositional%20understanding%20of%0Aobjects%2C%20relations%2C%20and%20attributes%20in%20text.%20Overall%2C%20our%20work%20addresses%20a%0Acrucial%20limitation%20of%20VLMs%20by%20introducing%20a%20dataset%20and%20framework%20that%0Astrengthens%20semantic%20associations%20between%20images%20and%20text%2C%20demonstrating%0Aimproved%20large-scale%20foundation%20models%20with%20significantly%20reduced%20computational%0Acost%2C%20promoting%20efficiency%20and%20accessibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20312v1&entry.124074799=Read"},
{"title": "Sketch-to-Architecture: Generative AI-aided Architectural Design", "author": "Pengzhi Li and Baijuan Li and Zhiheng Li", "abstract": "  Recently, the development of large-scale models has paved the way for various\ninterdisciplinary research, including architecture. By using generative AI, we\npresent a novel workflow that utilizes AI models to generate conceptual\nfloorplans and 3D models from simple sketches, enabling rapid ideation and\ncontrolled generation of architectural renderings based on textual\ndescriptions. Our work demonstrates the potential of generative AI in the\narchitectural design process, pointing towards a new direction of\ncomputer-aided architectural design. Our project website is available at:\nhttps://zrealli.github.io/sketch2arc\n", "link": "http://arxiv.org/abs/2403.20186v1", "date": "2024-03-29", "relevancy": 1.9773, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5017}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5008}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4849}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sketch-to-Architecture%3A%20Generative%20AI-aided%20Architectural%20Design&body=Title%3A%20Sketch-to-Architecture%3A%20Generative%20AI-aided%20Architectural%20Design%0AAuthor%3A%20Pengzhi%20Li%20and%20Baijuan%20Li%20and%20Zhiheng%20Li%0AAbstract%3A%20%20%20Recently%2C%20the%20development%20of%20large-scale%20models%20has%20paved%20the%20way%20for%20various%0Ainterdisciplinary%20research%2C%20including%20architecture.%20By%20using%20generative%20AI%2C%20we%0Apresent%20a%20novel%20workflow%20that%20utilizes%20AI%20models%20to%20generate%20conceptual%0Afloorplans%20and%203D%20models%20from%20simple%20sketches%2C%20enabling%20rapid%20ideation%20and%0Acontrolled%20generation%20of%20architectural%20renderings%20based%20on%20textual%0Adescriptions.%20Our%20work%20demonstrates%20the%20potential%20of%20generative%20AI%20in%20the%0Aarchitectural%20design%20process%2C%20pointing%20towards%20a%20new%20direction%20of%0Acomputer-aided%20architectural%20design.%20Our%20project%20website%20is%20available%20at%3A%0Ahttps%3A//zrealli.github.io/sketch2arc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20186v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-to-Architecture%3A%20Generative%20AI-aided%20Architectural%20Design&entry.906535625=Pengzhi%20Li%20and%20Baijuan%20Li%20and%20Zhiheng%20Li&entry.1292438233=%20%20Recently%2C%20the%20development%20of%20large-scale%20models%20has%20paved%20the%20way%20for%20various%0Ainterdisciplinary%20research%2C%20including%20architecture.%20By%20using%20generative%20AI%2C%20we%0Apresent%20a%20novel%20workflow%20that%20utilizes%20AI%20models%20to%20generate%20conceptual%0Afloorplans%20and%203D%20models%20from%20simple%20sketches%2C%20enabling%20rapid%20ideation%20and%0Acontrolled%20generation%20of%20architectural%20renderings%20based%20on%20textual%0Adescriptions.%20Our%20work%20demonstrates%20the%20potential%20of%20generative%20AI%20in%20the%0Aarchitectural%20design%20process%2C%20pointing%20towards%20a%20new%20direction%20of%0Acomputer-aided%20architectural%20design.%20Our%20project%20website%20is%20available%20at%3A%0Ahttps%3A//zrealli.github.io/sketch2arc%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20186v1&entry.124074799=Read"},
{"title": "Sound event localization and classification using WASN in Outdoor\n  Environment", "author": "Dongzhe Zhang and Jianfeng Chen and Jisheng Bai and Mou Wang", "abstract": "  Deep learning-based sound event localization and classification is an\nemerging research area within wireless acoustic sensor networks. However,\ncurrent methods for sound event localization and classification typically rely\non a single microphone array, making them susceptible to signal attenuation and\nenvironmental noise, which limits their monitoring range. Moreover, methods\nusing multiple microphone arrays often focus solely on source localization,\nneglecting the aspect of sound event classification. In this paper, we propose\na deep learning-based method that employs multiple features and attention\nmechanisms to estimate the location and class of sound source. We introduce a\nSoundmap feature to capture spatial information across multiple frequency\nbands. We also use the Gammatone filter to generate acoustic features more\nsuitable for outdoor environments. Furthermore, we integrate attention\nmechanisms to learn channel-wise relationships and temporal dependencies within\nthe acoustic features. To evaluate our proposed method, we conduct experiments\nusing simulated datasets with different levels of noise and size of monitoring\nareas, as well as different arrays and source positions. The experimental\nresults demonstrate the superiority of our proposed method over\nstate-of-the-art methods in both sound event classification and sound source\nlocalization tasks. And we provide further analysis to explain the reasons for\nthe observed errors.\n", "link": "http://arxiv.org/abs/2403.20130v1", "date": "2024-03-29", "relevancy": 1.9684, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5169}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4709}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sound%20event%20localization%20and%20classification%20using%20WASN%20in%20Outdoor%0A%20%20Environment&body=Title%3A%20Sound%20event%20localization%20and%20classification%20using%20WASN%20in%20Outdoor%0A%20%20Environment%0AAuthor%3A%20Dongzhe%20Zhang%20and%20Jianfeng%20Chen%20and%20Jisheng%20Bai%20and%20Mou%20Wang%0AAbstract%3A%20%20%20Deep%20learning-based%20sound%20event%20localization%20and%20classification%20is%20an%0Aemerging%20research%20area%20within%20wireless%20acoustic%20sensor%20networks.%20However%2C%0Acurrent%20methods%20for%20sound%20event%20localization%20and%20classification%20typically%20rely%0Aon%20a%20single%20microphone%20array%2C%20making%20them%20susceptible%20to%20signal%20attenuation%20and%0Aenvironmental%20noise%2C%20which%20limits%20their%20monitoring%20range.%20Moreover%2C%20methods%0Ausing%20multiple%20microphone%20arrays%20often%20focus%20solely%20on%20source%20localization%2C%0Aneglecting%20the%20aspect%20of%20sound%20event%20classification.%20In%20this%20paper%2C%20we%20propose%0Aa%20deep%20learning-based%20method%20that%20employs%20multiple%20features%20and%20attention%0Amechanisms%20to%20estimate%20the%20location%20and%20class%20of%20sound%20source.%20We%20introduce%20a%0ASoundmap%20feature%20to%20capture%20spatial%20information%20across%20multiple%20frequency%0Abands.%20We%20also%20use%20the%20Gammatone%20filter%20to%20generate%20acoustic%20features%20more%0Asuitable%20for%20outdoor%20environments.%20Furthermore%2C%20we%20integrate%20attention%0Amechanisms%20to%20learn%20channel-wise%20relationships%20and%20temporal%20dependencies%20within%0Athe%20acoustic%20features.%20To%20evaluate%20our%20proposed%20method%2C%20we%20conduct%20experiments%0Ausing%20simulated%20datasets%20with%20different%20levels%20of%20noise%20and%20size%20of%20monitoring%0Aareas%2C%20as%20well%20as%20different%20arrays%20and%20source%20positions.%20The%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20over%0Astate-of-the-art%20methods%20in%20both%20sound%20event%20classification%20and%20sound%20source%0Alocalization%20tasks.%20And%20we%20provide%20further%20analysis%20to%20explain%20the%20reasons%20for%0Athe%20observed%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20130v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sound%20event%20localization%20and%20classification%20using%20WASN%20in%20Outdoor%0A%20%20Environment&entry.906535625=Dongzhe%20Zhang%20and%20Jianfeng%20Chen%20and%20Jisheng%20Bai%20and%20Mou%20Wang&entry.1292438233=%20%20Deep%20learning-based%20sound%20event%20localization%20and%20classification%20is%20an%0Aemerging%20research%20area%20within%20wireless%20acoustic%20sensor%20networks.%20However%2C%0Acurrent%20methods%20for%20sound%20event%20localization%20and%20classification%20typically%20rely%0Aon%20a%20single%20microphone%20array%2C%20making%20them%20susceptible%20to%20signal%20attenuation%20and%0Aenvironmental%20noise%2C%20which%20limits%20their%20monitoring%20range.%20Moreover%2C%20methods%0Ausing%20multiple%20microphone%20arrays%20often%20focus%20solely%20on%20source%20localization%2C%0Aneglecting%20the%20aspect%20of%20sound%20event%20classification.%20In%20this%20paper%2C%20we%20propose%0Aa%20deep%20learning-based%20method%20that%20employs%20multiple%20features%20and%20attention%0Amechanisms%20to%20estimate%20the%20location%20and%20class%20of%20sound%20source.%20We%20introduce%20a%0ASoundmap%20feature%20to%20capture%20spatial%20information%20across%20multiple%20frequency%0Abands.%20We%20also%20use%20the%20Gammatone%20filter%20to%20generate%20acoustic%20features%20more%0Asuitable%20for%20outdoor%20environments.%20Furthermore%2C%20we%20integrate%20attention%0Amechanisms%20to%20learn%20channel-wise%20relationships%20and%20temporal%20dependencies%20within%0Athe%20acoustic%20features.%20To%20evaluate%20our%20proposed%20method%2C%20we%20conduct%20experiments%0Ausing%20simulated%20datasets%20with%20different%20levels%20of%20noise%20and%20size%20of%20monitoring%0Aareas%2C%20as%20well%20as%20different%20arrays%20and%20source%20positions.%20The%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20over%0Astate-of-the-art%20methods%20in%20both%20sound%20event%20classification%20and%20sound%20source%0Alocalization%20tasks.%20And%20we%20provide%20further%20analysis%20to%20explain%20the%20reasons%20for%0Athe%20observed%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20130v1&entry.124074799=Read"},
{"title": "TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed\n  Machine Learning", "author": "William Won and Midhilesh Elavazhagan and Sudarshan Srinivasan and Ajaya Durg and Samvit Kaul and Swati Gupta and Tushar Krishna", "abstract": "  The surge of artificial intelligence, specifically large language models, has\nled to a rapid advent towards the development of large-scale machine learning\ntraining clusters. Collective communications within these clusters tend to be\nheavily bandwidth-bound, necessitating techniques to optimally utilize the\navailable network bandwidth. This puts the routing algorithm for the collective\nat the forefront of determining the performance. Unfortunately, communication\nlibraries used in distributed machine learning today are limited by a fixed set\nof routing algorithms. This constraints collective performance within the\ndomain of next-generation training clusters that employ intricate,\nheterogeneous, and asymmetric, large-scale topologies. Further, the emergence\nof irregular topologies attributed to runtime phenomena such as device failures\nserves to compound the complexity of the challenge. To this end, this paper\nintroduces TACOS, an automated synthesizer that generates topology-aware\ncollective algorithms for common distributed machine learning collectives\nacross arbitrary input network topologies. TACOS was able to synthesize\nAll-Reduce algorithm for a heterogeneous 512-NPU system in just 6.09 minutes\nwhile achieving performance improvement up to 4.27x over state-of-the-art prior\nwork. TACOS exhibits high scalability, with synthesis time scaling\nquadratically with the number of NPUs. In contrast to prior works' NP-hard\napproaches, TACOS with 40K NPUs completes in 2.52 hours.\n", "link": "http://arxiv.org/abs/2304.05301v2", "date": "2024-03-29", "relevancy": 1.9682, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TACOS%3A%20Topology-Aware%20Collective%20Algorithm%20Synthesizer%20for%20Distributed%0A%20%20Machine%20Learning&body=Title%3A%20TACOS%3A%20Topology-Aware%20Collective%20Algorithm%20Synthesizer%20for%20Distributed%0A%20%20Machine%20Learning%0AAuthor%3A%20William%20Won%20and%20Midhilesh%20Elavazhagan%20and%20Sudarshan%20Srinivasan%20and%20Ajaya%20Durg%20and%20Samvit%20Kaul%20and%20Swati%20Gupta%20and%20Tushar%20Krishna%0AAbstract%3A%20%20%20The%20surge%20of%20artificial%20intelligence%2C%20specifically%20large%20language%20models%2C%20has%0Aled%20to%20a%20rapid%20advent%20towards%20the%20development%20of%20large-scale%20machine%20learning%0Atraining%20clusters.%20Collective%20communications%20within%20these%20clusters%20tend%20to%20be%0Aheavily%20bandwidth-bound%2C%20necessitating%20techniques%20to%20optimally%20utilize%20the%0Aavailable%20network%20bandwidth.%20This%20puts%20the%20routing%20algorithm%20for%20the%20collective%0Aat%20the%20forefront%20of%20determining%20the%20performance.%20Unfortunately%2C%20communication%0Alibraries%20used%20in%20distributed%20machine%20learning%20today%20are%20limited%20by%20a%20fixed%20set%0Aof%20routing%20algorithms.%20This%20constraints%20collective%20performance%20within%20the%0Adomain%20of%20next-generation%20training%20clusters%20that%20employ%20intricate%2C%0Aheterogeneous%2C%20and%20asymmetric%2C%20large-scale%20topologies.%20Further%2C%20the%20emergence%0Aof%20irregular%20topologies%20attributed%20to%20runtime%20phenomena%20such%20as%20device%20failures%0Aserves%20to%20compound%20the%20complexity%20of%20the%20challenge.%20To%20this%20end%2C%20this%20paper%0Aintroduces%20TACOS%2C%20an%20automated%20synthesizer%20that%20generates%20topology-aware%0Acollective%20algorithms%20for%20common%20distributed%20machine%20learning%20collectives%0Aacross%20arbitrary%20input%20network%20topologies.%20TACOS%20was%20able%20to%20synthesize%0AAll-Reduce%20algorithm%20for%20a%20heterogeneous%20512-NPU%20system%20in%20just%206.09%20minutes%0Awhile%20achieving%20performance%20improvement%20up%20to%204.27x%20over%20state-of-the-art%20prior%0Awork.%20TACOS%20exhibits%20high%20scalability%2C%20with%20synthesis%20time%20scaling%0Aquadratically%20with%20the%20number%20of%20NPUs.%20In%20contrast%20to%20prior%20works%27%20NP-hard%0Aapproaches%2C%20TACOS%20with%2040K%20NPUs%20completes%20in%202.52%20hours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.05301v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACOS%3A%20Topology-Aware%20Collective%20Algorithm%20Synthesizer%20for%20Distributed%0A%20%20Machine%20Learning&entry.906535625=William%20Won%20and%20Midhilesh%20Elavazhagan%20and%20Sudarshan%20Srinivasan%20and%20Ajaya%20Durg%20and%20Samvit%20Kaul%20and%20Swati%20Gupta%20and%20Tushar%20Krishna&entry.1292438233=%20%20The%20surge%20of%20artificial%20intelligence%2C%20specifically%20large%20language%20models%2C%20has%0Aled%20to%20a%20rapid%20advent%20towards%20the%20development%20of%20large-scale%20machine%20learning%0Atraining%20clusters.%20Collective%20communications%20within%20these%20clusters%20tend%20to%20be%0Aheavily%20bandwidth-bound%2C%20necessitating%20techniques%20to%20optimally%20utilize%20the%0Aavailable%20network%20bandwidth.%20This%20puts%20the%20routing%20algorithm%20for%20the%20collective%0Aat%20the%20forefront%20of%20determining%20the%20performance.%20Unfortunately%2C%20communication%0Alibraries%20used%20in%20distributed%20machine%20learning%20today%20are%20limited%20by%20a%20fixed%20set%0Aof%20routing%20algorithms.%20This%20constraints%20collective%20performance%20within%20the%0Adomain%20of%20next-generation%20training%20clusters%20that%20employ%20intricate%2C%0Aheterogeneous%2C%20and%20asymmetric%2C%20large-scale%20topologies.%20Further%2C%20the%20emergence%0Aof%20irregular%20topologies%20attributed%20to%20runtime%20phenomena%20such%20as%20device%20failures%0Aserves%20to%20compound%20the%20complexity%20of%20the%20challenge.%20To%20this%20end%2C%20this%20paper%0Aintroduces%20TACOS%2C%20an%20automated%20synthesizer%20that%20generates%20topology-aware%0Acollective%20algorithms%20for%20common%20distributed%20machine%20learning%20collectives%0Aacross%20arbitrary%20input%20network%20topologies.%20TACOS%20was%20able%20to%20synthesize%0AAll-Reduce%20algorithm%20for%20a%20heterogeneous%20512-NPU%20system%20in%20just%206.09%20minutes%0Awhile%20achieving%20performance%20improvement%20up%20to%204.27x%20over%20state-of-the-art%20prior%0Awork.%20TACOS%20exhibits%20high%20scalability%2C%20with%20synthesis%20time%20scaling%0Aquadratically%20with%20the%20number%20of%20NPUs.%20In%20contrast%20to%20prior%20works%27%20NP-hard%0Aapproaches%2C%20TACOS%20with%2040K%20NPUs%20completes%20in%202.52%20hours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.05301v2&entry.124074799=Read"},
{"title": "Unleashing the Potential of Large Language Models for Predictive Tabular\n  Tasks in Data Science", "author": "Yazheng Yang and Yuqi Wang and Sankalok Sen and Lei Li and Qi Liu", "abstract": "  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n", "link": "http://arxiv.org/abs/2403.20208v1", "date": "2024-03-29", "relevancy": 1.9442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.48}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science&body=Title%3A%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science%0AAuthor%3A%20Yazheng%20Yang%20and%20Yuqi%20Wang%20and%20Sankalok%20Sen%20and%20Lei%20Li%20and%20Qi%20Liu%0AAbstract%3A%20%20%20In%20the%20domain%20of%20data%20science%2C%20the%20predictive%20tasks%20of%20classification%2C%0Aregression%2C%20and%20imputation%20of%20missing%20values%20are%20commonly%20encountered%0Achallenges%20associated%20with%20tabular%20data.%20This%20research%20endeavors%20to%20apply%20Large%0ALanguage%20Models%20%28LLMs%29%20towards%20addressing%20these%20predictive%20tasks.%20Despite%20their%0Aproficiency%20in%20comprehending%20natural%20language%2C%20LLMs%20fall%20short%20in%20dealing%20with%0Astructured%20tabular%20data.%20This%20limitation%20stems%20from%20their%20lacking%20exposure%20to%0Athe%20intricacies%20of%20tabular%20data%20during%20their%20foundational%20training.%20Our%0Aresearch%20aims%20to%20mitigate%20this%20gap%20by%20compiling%20a%20comprehensive%20corpus%20of%0Atables%20annotated%20with%20instructions%20and%20executing%20large-scale%20training%20of%0ALlama-2%20on%20this%20enriched%20dataset.%20Furthermore%2C%20we%20investigate%20the%20practical%0Aapplication%20of%20applying%20the%20trained%20model%20to%20zero-shot%20prediction%2C%20few-shot%0Aprediction%2C%20and%20in-context%20learning%20scenarios.%20Through%20extensive%20experiments%2C%0Aour%20methodology%20has%20shown%20significant%20improvements%20over%20existing%20benchmarks.%0AThese%20advancements%20highlight%20the%20efficacy%20of%20tailoring%20LLM%20training%20to%20solve%0Atable-related%20problems%20in%20data%20science%2C%20thereby%20establishing%20a%20new%20benchmark%20in%0Athe%20utilization%20of%20LLMs%20for%20enhancing%20tabular%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20208v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Predictive%20Tabular%0A%20%20Tasks%20in%20Data%20Science&entry.906535625=Yazheng%20Yang%20and%20Yuqi%20Wang%20and%20Sankalok%20Sen%20and%20Lei%20Li%20and%20Qi%20Liu&entry.1292438233=%20%20In%20the%20domain%20of%20data%20science%2C%20the%20predictive%20tasks%20of%20classification%2C%0Aregression%2C%20and%20imputation%20of%20missing%20values%20are%20commonly%20encountered%0Achallenges%20associated%20with%20tabular%20data.%20This%20research%20endeavors%20to%20apply%20Large%0ALanguage%20Models%20%28LLMs%29%20towards%20addressing%20these%20predictive%20tasks.%20Despite%20their%0Aproficiency%20in%20comprehending%20natural%20language%2C%20LLMs%20fall%20short%20in%20dealing%20with%0Astructured%20tabular%20data.%20This%20limitation%20stems%20from%20their%20lacking%20exposure%20to%0Athe%20intricacies%20of%20tabular%20data%20during%20their%20foundational%20training.%20Our%0Aresearch%20aims%20to%20mitigate%20this%20gap%20by%20compiling%20a%20comprehensive%20corpus%20of%0Atables%20annotated%20with%20instructions%20and%20executing%20large-scale%20training%20of%0ALlama-2%20on%20this%20enriched%20dataset.%20Furthermore%2C%20we%20investigate%20the%20practical%0Aapplication%20of%20applying%20the%20trained%20model%20to%20zero-shot%20prediction%2C%20few-shot%0Aprediction%2C%20and%20in-context%20learning%20scenarios.%20Through%20extensive%20experiments%2C%0Aour%20methodology%20has%20shown%20significant%20improvements%20over%20existing%20benchmarks.%0AThese%20advancements%20highlight%20the%20efficacy%20of%20tailoring%20LLM%20training%20to%20solve%0Atable-related%20problems%20in%20data%20science%2C%20thereby%20establishing%20a%20new%20benchmark%20in%0Athe%20utilization%20of%20LLMs%20for%20enhancing%20tabular%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20208v1&entry.124074799=Read"},
{"title": "MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning", "author": "Ahmed Agiza and Marina Neseem and Sherief Reda", "abstract": "  Adapting models pre-trained on large-scale datasets to a variety of\ndownstream tasks is a common strategy in deep learning. Consequently,\nparameter-efficient fine-tuning methods have emerged as a promising way to\nadapt pre-trained models to different tasks while training only a minimal\nnumber of parameters. While most of these methods are designed for single-task\nadaptation, parameter-efficient training in Multi-Task Learning (MTL)\narchitectures is still unexplored. In this paper, we introduce MTLoRA, a novel\nframework for parameter-efficient training of MTL models. MTLoRA employs\nTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively\ndisentangle the parameter space in MTL fine-tuning, thereby enabling the model\nto adeptly handle both task specialization and interaction within MTL contexts.\nWe applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting\nthem to multiple downstream dense prediction tasks. Our extensive experiments\non the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream\ntasks compared to fully fine-tuning the MTL model while reducing the number of\ntrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal\ntrade-off between the number of trainable parameters and the accuracy of the\ndownstream tasks, outperforming current state-of-the-art parameter-efficient\ntraining methods in both accuracy and efficiency. Our code is publicly\navailable.\n", "link": "http://arxiv.org/abs/2403.20320v1", "date": "2024-03-29", "relevancy": 1.9367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MTLoRA%3A%20A%20Low-Rank%20Adaptation%20Approach%20for%20Efficient%20Multi-Task%20Learning&body=Title%3A%20MTLoRA%3A%20A%20Low-Rank%20Adaptation%20Approach%20for%20Efficient%20Multi-Task%20Learning%0AAuthor%3A%20Ahmed%20Agiza%20and%20Marina%20Neseem%20and%20Sherief%20Reda%0AAbstract%3A%20%20%20Adapting%20models%20pre-trained%20on%20large-scale%20datasets%20to%20a%20variety%20of%0Adownstream%20tasks%20is%20a%20common%20strategy%20in%20deep%20learning.%20Consequently%2C%0Aparameter-efficient%20fine-tuning%20methods%20have%20emerged%20as%20a%20promising%20way%20to%0Aadapt%20pre-trained%20models%20to%20different%20tasks%20while%20training%20only%20a%20minimal%0Anumber%20of%20parameters.%20While%20most%20of%20these%20methods%20are%20designed%20for%20single-task%0Aadaptation%2C%20parameter-efficient%20training%20in%20Multi-Task%20Learning%20%28MTL%29%0Aarchitectures%20is%20still%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20MTLoRA%2C%20a%20novel%0Aframework%20for%20parameter-efficient%20training%20of%20MTL%20models.%20MTLoRA%20employs%0ATask-Agnostic%20and%20Task-Specific%20Low-Rank%20Adaptation%20modules%2C%20which%20effectively%0Adisentangle%20the%20parameter%20space%20in%20MTL%20fine-tuning%2C%20thereby%20enabling%20the%20model%0Ato%20adeptly%20handle%20both%20task%20specialization%20and%20interaction%20within%20MTL%20contexts.%0AWe%20applied%20MTLoRA%20to%20hierarchical-transformer-based%20MTL%20architectures%2C%20adapting%0Athem%20to%20multiple%20downstream%20dense%20prediction%20tasks.%20Our%20extensive%20experiments%0Aon%20the%20PASCAL%20dataset%20show%20that%20MTLoRA%20achieves%20higher%20accuracy%20on%20downstream%0Atasks%20compared%20to%20fully%20fine-tuning%20the%20MTL%20model%20while%20reducing%20the%20number%20of%0Atrainable%20parameters%20by%203.6x.%20Furthermore%2C%20MTLoRA%20establishes%20a%20Pareto-optimal%0Atrade-off%20between%20the%20number%20of%20trainable%20parameters%20and%20the%20accuracy%20of%20the%0Adownstream%20tasks%2C%20outperforming%20current%20state-of-the-art%20parameter-efficient%0Atraining%20methods%20in%20both%20accuracy%20and%20efficiency.%20Our%20code%20is%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20320v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTLoRA%3A%20A%20Low-Rank%20Adaptation%20Approach%20for%20Efficient%20Multi-Task%20Learning&entry.906535625=Ahmed%20Agiza%20and%20Marina%20Neseem%20and%20Sherief%20Reda&entry.1292438233=%20%20Adapting%20models%20pre-trained%20on%20large-scale%20datasets%20to%20a%20variety%20of%0Adownstream%20tasks%20is%20a%20common%20strategy%20in%20deep%20learning.%20Consequently%2C%0Aparameter-efficient%20fine-tuning%20methods%20have%20emerged%20as%20a%20promising%20way%20to%0Aadapt%20pre-trained%20models%20to%20different%20tasks%20while%20training%20only%20a%20minimal%0Anumber%20of%20parameters.%20While%20most%20of%20these%20methods%20are%20designed%20for%20single-task%0Aadaptation%2C%20parameter-efficient%20training%20in%20Multi-Task%20Learning%20%28MTL%29%0Aarchitectures%20is%20still%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20MTLoRA%2C%20a%20novel%0Aframework%20for%20parameter-efficient%20training%20of%20MTL%20models.%20MTLoRA%20employs%0ATask-Agnostic%20and%20Task-Specific%20Low-Rank%20Adaptation%20modules%2C%20which%20effectively%0Adisentangle%20the%20parameter%20space%20in%20MTL%20fine-tuning%2C%20thereby%20enabling%20the%20model%0Ato%20adeptly%20handle%20both%20task%20specialization%20and%20interaction%20within%20MTL%20contexts.%0AWe%20applied%20MTLoRA%20to%20hierarchical-transformer-based%20MTL%20architectures%2C%20adapting%0Athem%20to%20multiple%20downstream%20dense%20prediction%20tasks.%20Our%20extensive%20experiments%0Aon%20the%20PASCAL%20dataset%20show%20that%20MTLoRA%20achieves%20higher%20accuracy%20on%20downstream%0Atasks%20compared%20to%20fully%20fine-tuning%20the%20MTL%20model%20while%20reducing%20the%20number%20of%0Atrainable%20parameters%20by%203.6x.%20Furthermore%2C%20MTLoRA%20establishes%20a%20Pareto-optimal%0Atrade-off%20between%20the%20number%20of%20trainable%20parameters%20and%20the%20accuracy%20of%20the%0Adownstream%20tasks%2C%20outperforming%20current%20state-of-the-art%20parameter-efficient%0Atraining%20methods%20in%20both%20accuracy%20and%20efficiency.%20Our%20code%20is%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20320v1&entry.124074799=Read"},
{"title": "Audio-Visual Compound Expression Recognition Method based on Late\n  Modality Fusion and Rule-based Decision", "author": "Elena Ryumina and Maxim Markitantov and Dmitry Ryumin and Heysem Kaya and Alexey Karpov", "abstract": "  This paper presents the results of the SUN team for the Compound Expressions\nRecognition Challenge of the 6th ABAW Competition. We propose a novel\naudio-visual method for compound expression recognition. Our method relies on\nemotion recognition models that fuse modalities at the emotion probability\nlevel, while decisions regarding the prediction of compound expressions are\nbased on predefined rules. Notably, our method does not use any training data\nspecific to the target task. Thus, the problem is a zero-shot classification\ntask. The method is evaluated in multi-corpus training and cross-corpus\nvalidation setups. Using our proposed method is achieved an F1-score value\nequals to 22.01% on the C-EXPR-DB test subset. Our findings from the challenge\ndemonstrate that the proposed method can potentially form a basis for\ndeveloping intelligent tools for annotating audio-visual data in the context of\nhuman's basic and compound emotions.\n", "link": "http://arxiv.org/abs/2403.12687v2", "date": "2024-03-29", "relevancy": 1.9364, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5189}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4761}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4525}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Compound%20Expression%20Recognition%20Method%20based%20on%20Late%0A%20%20Modality%20Fusion%20and%20Rule-based%20Decision&body=Title%3A%20Audio-Visual%20Compound%20Expression%20Recognition%20Method%20based%20on%20Late%0A%20%20Modality%20Fusion%20and%20Rule-based%20Decision%0AAuthor%3A%20Elena%20Ryumina%20and%20Maxim%20Markitantov%20and%20Dmitry%20Ryumin%20and%20Heysem%20Kaya%20and%20Alexey%20Karpov%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20results%20of%20the%20SUN%20team%20for%20the%20Compound%20Expressions%0ARecognition%20Challenge%20of%20the%206th%20ABAW%20Competition.%20We%20propose%20a%20novel%0Aaudio-visual%20method%20for%20compound%20expression%20recognition.%20Our%20method%20relies%20on%0Aemotion%20recognition%20models%20that%20fuse%20modalities%20at%20the%20emotion%20probability%0Alevel%2C%20while%20decisions%20regarding%20the%20prediction%20of%20compound%20expressions%20are%0Abased%20on%20predefined%20rules.%20Notably%2C%20our%20method%20does%20not%20use%20any%20training%20data%0Aspecific%20to%20the%20target%20task.%20Thus%2C%20the%20problem%20is%20a%20zero-shot%20classification%0Atask.%20The%20method%20is%20evaluated%20in%20multi-corpus%20training%20and%20cross-corpus%0Avalidation%20setups.%20Using%20our%20proposed%20method%20is%20achieved%20an%20F1-score%20value%0Aequals%20to%2022.01%25%20on%20the%20C-EXPR-DB%20test%20subset.%20Our%20findings%20from%20the%20challenge%0Ademonstrate%20that%20the%20proposed%20method%20can%20potentially%20form%20a%20basis%20for%0Adeveloping%20intelligent%20tools%20for%20annotating%20audio-visual%20data%20in%20the%20context%20of%0Ahuman%27s%20basic%20and%20compound%20emotions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12687v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Compound%20Expression%20Recognition%20Method%20based%20on%20Late%0A%20%20Modality%20Fusion%20and%20Rule-based%20Decision&entry.906535625=Elena%20Ryumina%20and%20Maxim%20Markitantov%20and%20Dmitry%20Ryumin%20and%20Heysem%20Kaya%20and%20Alexey%20Karpov&entry.1292438233=%20%20This%20paper%20presents%20the%20results%20of%20the%20SUN%20team%20for%20the%20Compound%20Expressions%0ARecognition%20Challenge%20of%20the%206th%20ABAW%20Competition.%20We%20propose%20a%20novel%0Aaudio-visual%20method%20for%20compound%20expression%20recognition.%20Our%20method%20relies%20on%0Aemotion%20recognition%20models%20that%20fuse%20modalities%20at%20the%20emotion%20probability%0Alevel%2C%20while%20decisions%20regarding%20the%20prediction%20of%20compound%20expressions%20are%0Abased%20on%20predefined%20rules.%20Notably%2C%20our%20method%20does%20not%20use%20any%20training%20data%0Aspecific%20to%20the%20target%20task.%20Thus%2C%20the%20problem%20is%20a%20zero-shot%20classification%0Atask.%20The%20method%20is%20evaluated%20in%20multi-corpus%20training%20and%20cross-corpus%0Avalidation%20setups.%20Using%20our%20proposed%20method%20is%20achieved%20an%20F1-score%20value%0Aequals%20to%2022.01%25%20on%20the%20C-EXPR-DB%20test%20subset.%20Our%20findings%20from%20the%20challenge%0Ademonstrate%20that%20the%20proposed%20method%20can%20potentially%20form%20a%20basis%20for%0Adeveloping%20intelligent%20tools%20for%20annotating%20audio-visual%20data%20in%20the%20context%20of%0Ahuman%27s%20basic%20and%20compound%20emotions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12687v2&entry.124074799=Read"},
{"title": "DXAI: Explaining Classification by Image Decomposition", "author": "Elnatan Kadar and Guy Gilboa", "abstract": "  We propose a new way to explain and to visualize neural network\nclassification through a decomposition-based explainable AI (DXAI). Instead of\nproviding an explanation heatmap, our method yields a decomposition of the\nimage into class-agnostic and class-distinct parts, with respect to the data\nand chosen classifier. Following a fundamental signal processing paradigm of\nanalysis and synthesis, the original image is the sum of the decomposed parts.\nWe thus obtain a radically different way of explaining classification. The\nclass-agnostic part ideally is composed of all image features which do not\nposses class information, where the class-distinct part is its complementary.\nThis new visualization can be more helpful and informative in certain\nscenarios, especially when the attributes are dense, global and additive in\nnature, for instance, when colors or textures are essential for class\ndistinction. Code is available at https://github.com/dxai2024/dxai.\n", "link": "http://arxiv.org/abs/2401.00320v2", "date": "2024-03-29", "relevancy": 1.9271, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5071}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4765}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DXAI%3A%20Explaining%20Classification%20by%20Image%20Decomposition&body=Title%3A%20DXAI%3A%20Explaining%20Classification%20by%20Image%20Decomposition%0AAuthor%3A%20Elnatan%20Kadar%20and%20Guy%20Gilboa%0AAbstract%3A%20%20%20We%20propose%20a%20new%20way%20to%20explain%20and%20to%20visualize%20neural%20network%0Aclassification%20through%20a%20decomposition-based%20explainable%20AI%20%28DXAI%29.%20Instead%20of%0Aproviding%20an%20explanation%20heatmap%2C%20our%20method%20yields%20a%20decomposition%20of%20the%0Aimage%20into%20class-agnostic%20and%20class-distinct%20parts%2C%20with%20respect%20to%20the%20data%0Aand%20chosen%20classifier.%20Following%20a%20fundamental%20signal%20processing%20paradigm%20of%0Aanalysis%20and%20synthesis%2C%20the%20original%20image%20is%20the%20sum%20of%20the%20decomposed%20parts.%0AWe%20thus%20obtain%20a%20radically%20different%20way%20of%20explaining%20classification.%20The%0Aclass-agnostic%20part%20ideally%20is%20composed%20of%20all%20image%20features%20which%20do%20not%0Aposses%20class%20information%2C%20where%20the%20class-distinct%20part%20is%20its%20complementary.%0AThis%20new%20visualization%20can%20be%20more%20helpful%20and%20informative%20in%20certain%0Ascenarios%2C%20especially%20when%20the%20attributes%20are%20dense%2C%20global%20and%20additive%20in%0Anature%2C%20for%20instance%2C%20when%20colors%20or%20textures%20are%20essential%20for%20class%0Adistinction.%20Code%20is%20available%20at%20https%3A//github.com/dxai2024/dxai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00320v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DXAI%3A%20Explaining%20Classification%20by%20Image%20Decomposition&entry.906535625=Elnatan%20Kadar%20and%20Guy%20Gilboa&entry.1292438233=%20%20We%20propose%20a%20new%20way%20to%20explain%20and%20to%20visualize%20neural%20network%0Aclassification%20through%20a%20decomposition-based%20explainable%20AI%20%28DXAI%29.%20Instead%20of%0Aproviding%20an%20explanation%20heatmap%2C%20our%20method%20yields%20a%20decomposition%20of%20the%0Aimage%20into%20class-agnostic%20and%20class-distinct%20parts%2C%20with%20respect%20to%20the%20data%0Aand%20chosen%20classifier.%20Following%20a%20fundamental%20signal%20processing%20paradigm%20of%0Aanalysis%20and%20synthesis%2C%20the%20original%20image%20is%20the%20sum%20of%20the%20decomposed%20parts.%0AWe%20thus%20obtain%20a%20radically%20different%20way%20of%20explaining%20classification.%20The%0Aclass-agnostic%20part%20ideally%20is%20composed%20of%20all%20image%20features%20which%20do%20not%0Aposses%20class%20information%2C%20where%20the%20class-distinct%20part%20is%20its%20complementary.%0AThis%20new%20visualization%20can%20be%20more%20helpful%20and%20informative%20in%20certain%0Ascenarios%2C%20especially%20when%20the%20attributes%20are%20dense%2C%20global%20and%20additive%20in%0Anature%2C%20for%20instance%2C%20when%20colors%20or%20textures%20are%20essential%20for%20class%0Adistinction.%20Code%20is%20available%20at%20https%3A//github.com/dxai2024/dxai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00320v2&entry.124074799=Read"},
{"title": "Homomorphic WiSARDs: Efficient Weightless Neural Network training over\n  encrypted data", "author": "Leonardo Neumann and Antonio Guimar\u00e3es and Diego F. Aranha and Edson Borin", "abstract": "  The widespread application of machine learning algorithms is a matter of\nincreasing concern for the data privacy research community, and many have\nsought to develop privacy-preserving techniques for it. Among existing\napproaches, the homomorphic evaluation of ML algorithms stands out by\nperforming operations directly over encrypted data, enabling strong guarantees\nof confidentiality. The homomorphic evaluation of inference algorithms is\npractical even for relatively deep Convolution Neural Networks (CNNs). However,\ntraining is still a major challenge, with current solutions often resorting to\nlightweight algorithms that can be unfit for solving more complex problems,\nsuch as image recognition. This work introduces the homomorphic evaluation of\nWilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent\nWeightless Neural Networks (WNNs) for training and inference on encrypted data.\nCompared to CNNs, WNNs offer better performance with a relatively small\naccuracy drop. We develop a complete framework for it, including several\nbuilding blocks that can be of independent interest. Our framework achieves\n91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted\ntraining (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000\ndataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after\n1 hour. Compared to the state of the art on the HE evaluation of CNN training,\nGlyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to\n1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even\nachieved a 0.65% accuracy improvement while being 60 times faster than Glyph.\nWe also provide solutions for small-scale encrypted training. In a single\nthread on a desktop machine using less than 200MB of memory, we train over 1000\nMNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset\nin just 11 seconds.\n", "link": "http://arxiv.org/abs/2403.20190v1", "date": "2024-03-29", "relevancy": 1.9178, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5125}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4841}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Homomorphic%20WiSARDs%3A%20Efficient%20Weightless%20Neural%20Network%20training%20over%0A%20%20encrypted%20data&body=Title%3A%20Homomorphic%20WiSARDs%3A%20Efficient%20Weightless%20Neural%20Network%20training%20over%0A%20%20encrypted%20data%0AAuthor%3A%20Leonardo%20Neumann%20and%20Antonio%20Guimar%C3%A3es%20and%20Diego%20F.%20Aranha%20and%20Edson%20Borin%0AAbstract%3A%20%20%20The%20widespread%20application%20of%20machine%20learning%20algorithms%20is%20a%20matter%20of%0Aincreasing%20concern%20for%20the%20data%20privacy%20research%20community%2C%20and%20many%20have%0Asought%20to%20develop%20privacy-preserving%20techniques%20for%20it.%20Among%20existing%0Aapproaches%2C%20the%20homomorphic%20evaluation%20of%20ML%20algorithms%20stands%20out%20by%0Aperforming%20operations%20directly%20over%20encrypted%20data%2C%20enabling%20strong%20guarantees%0Aof%20confidentiality.%20The%20homomorphic%20evaluation%20of%20inference%20algorithms%20is%0Apractical%20even%20for%20relatively%20deep%20Convolution%20Neural%20Networks%20%28CNNs%29.%20However%2C%0Atraining%20is%20still%20a%20major%20challenge%2C%20with%20current%20solutions%20often%20resorting%20to%0Alightweight%20algorithms%20that%20can%20be%20unfit%20for%20solving%20more%20complex%20problems%2C%0Asuch%20as%20image%20recognition.%20This%20work%20introduces%20the%20homomorphic%20evaluation%20of%0AWilkie%2C%20Stonham%2C%20and%20Aleksander%27s%20Recognition%20Device%20%28WiSARD%29%20and%20subsequent%0AWeightless%20Neural%20Networks%20%28WNNs%29%20for%20training%20and%20inference%20on%20encrypted%20data.%0ACompared%20to%20CNNs%2C%20WNNs%20offer%20better%20performance%20with%20a%20relatively%20small%0Aaccuracy%20drop.%20We%20develop%20a%20complete%20framework%20for%20it%2C%20including%20several%0Abuilding%20blocks%20that%20can%20be%20of%20independent%20interest.%20Our%20framework%20achieves%0A91.7%25%20accuracy%20on%20the%20MNIST%20dataset%20after%20only%203.5%20minutes%20of%20encrypted%0Atraining%20%28multi-threaded%29%2C%20going%20up%20to%2093.8%25%20in%203.5%20hours.%20For%20the%20HAM10000%0Adataset%2C%20we%20achieve%2067.9%25%20accuracy%20in%20just%201.5%20minutes%2C%20going%20up%20to%2069.9%25%20after%0A1%20hour.%20Compared%20to%20the%20state%20of%20the%20art%20on%20the%20HE%20evaluation%20of%20CNN%20training%2C%0AGlyph%20%28Lou%20et%20al.%2C%20NeurIPS%202020%29%2C%20these%20results%20represent%20a%20speedup%20of%20up%20to%0A1200%20times%20with%20an%20accuracy%20loss%20of%20at%20most%205.4%25.%20For%20HAM10000%2C%20we%20even%0Aachieved%20a%200.65%25%20accuracy%20improvement%20while%20being%2060%20times%20faster%20than%20Glyph.%0AWe%20also%20provide%20solutions%20for%20small-scale%20encrypted%20training.%20In%20a%20single%0Athread%20on%20a%20desktop%20machine%20using%20less%20than%20200MB%20of%20memory%2C%20we%20train%20over%201000%0AMNIST%20images%20in%2012%20minutes%20or%20over%20the%20entire%20Wisconsin%20Breast%20Cancer%20dataset%0Ain%20just%2011%20seconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20190v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homomorphic%20WiSARDs%3A%20Efficient%20Weightless%20Neural%20Network%20training%20over%0A%20%20encrypted%20data&entry.906535625=Leonardo%20Neumann%20and%20Antonio%20Guimar%C3%A3es%20and%20Diego%20F.%20Aranha%20and%20Edson%20Borin&entry.1292438233=%20%20The%20widespread%20application%20of%20machine%20learning%20algorithms%20is%20a%20matter%20of%0Aincreasing%20concern%20for%20the%20data%20privacy%20research%20community%2C%20and%20many%20have%0Asought%20to%20develop%20privacy-preserving%20techniques%20for%20it.%20Among%20existing%0Aapproaches%2C%20the%20homomorphic%20evaluation%20of%20ML%20algorithms%20stands%20out%20by%0Aperforming%20operations%20directly%20over%20encrypted%20data%2C%20enabling%20strong%20guarantees%0Aof%20confidentiality.%20The%20homomorphic%20evaluation%20of%20inference%20algorithms%20is%0Apractical%20even%20for%20relatively%20deep%20Convolution%20Neural%20Networks%20%28CNNs%29.%20However%2C%0Atraining%20is%20still%20a%20major%20challenge%2C%20with%20current%20solutions%20often%20resorting%20to%0Alightweight%20algorithms%20that%20can%20be%20unfit%20for%20solving%20more%20complex%20problems%2C%0Asuch%20as%20image%20recognition.%20This%20work%20introduces%20the%20homomorphic%20evaluation%20of%0AWilkie%2C%20Stonham%2C%20and%20Aleksander%27s%20Recognition%20Device%20%28WiSARD%29%20and%20subsequent%0AWeightless%20Neural%20Networks%20%28WNNs%29%20for%20training%20and%20inference%20on%20encrypted%20data.%0ACompared%20to%20CNNs%2C%20WNNs%20offer%20better%20performance%20with%20a%20relatively%20small%0Aaccuracy%20drop.%20We%20develop%20a%20complete%20framework%20for%20it%2C%20including%20several%0Abuilding%20blocks%20that%20can%20be%20of%20independent%20interest.%20Our%20framework%20achieves%0A91.7%25%20accuracy%20on%20the%20MNIST%20dataset%20after%20only%203.5%20minutes%20of%20encrypted%0Atraining%20%28multi-threaded%29%2C%20going%20up%20to%2093.8%25%20in%203.5%20hours.%20For%20the%20HAM10000%0Adataset%2C%20we%20achieve%2067.9%25%20accuracy%20in%20just%201.5%20minutes%2C%20going%20up%20to%2069.9%25%20after%0A1%20hour.%20Compared%20to%20the%20state%20of%20the%20art%20on%20the%20HE%20evaluation%20of%20CNN%20training%2C%0AGlyph%20%28Lou%20et%20al.%2C%20NeurIPS%202020%29%2C%20these%20results%20represent%20a%20speedup%20of%20up%20to%0A1200%20times%20with%20an%20accuracy%20loss%20of%20at%20most%205.4%25.%20For%20HAM10000%2C%20we%20even%0Aachieved%20a%200.65%25%20accuracy%20improvement%20while%20being%2060%20times%20faster%20than%20Glyph.%0AWe%20also%20provide%20solutions%20for%20small-scale%20encrypted%20training.%20In%20a%20single%0Athread%20on%20a%20desktop%20machine%20using%20less%20than%20200MB%20of%20memory%2C%20we%20train%20over%201000%0AMNIST%20images%20in%2012%20minutes%20or%20over%20the%20entire%20Wisconsin%20Breast%20Cancer%20dataset%0Ain%20just%2011%20seconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20190v1&entry.124074799=Read"},
{"title": "Gradient strikes back: How filtering out high frequencies improves\n  explanations", "author": "Sabine Muzellec and Thomas Fel and Victor Boutin and L\u00e9o and\u00e9ol and Rufin VanRullen and Thomas Serre", "abstract": "  Attribution methods correspond to a class of explainability methods (XAI)\nthat aim to assess how individual inputs contribute to a model's\ndecision-making process. We have identified a significant limitation in one\ntype of attribution methods, known as \"white-box\" methods. Although highly\nefficient, these methods rely on a gradient signal that is often contaminated\nby high-frequency noise. To overcome this limitation, we introduce a new\napproach called \"FORGrad\". This simple method effectively filters out noise\nartifacts by using optimal cut-off frequencies tailored to the unique\ncharacteristics of each model architecture. Our findings show that FORGrad\nconsistently enhances the performance of already existing white-box methods,\nenabling them to compete effectively with more accurate yet computationally\ndemanding \"black-box\" methods. We anticipate that our research will foster\nbroader adoption of simpler and more efficient white-box methods for\nexplainability, offering a better balance between faithfulness and\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2307.09591v2", "date": "2024-03-29", "relevancy": 1.9093, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5048}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4746}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4691}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gradient%20strikes%20back%3A%20How%20filtering%20out%20high%20frequencies%20improves%0A%20%20explanations&body=Title%3A%20Gradient%20strikes%20back%3A%20How%20filtering%20out%20high%20frequencies%20improves%0A%20%20explanations%0AAuthor%3A%20Sabine%20Muzellec%20and%20Thomas%20Fel%20and%20Victor%20Boutin%20and%20L%C3%A9o%20and%C3%A9ol%20and%20Rufin%20VanRullen%20and%20Thomas%20Serre%0AAbstract%3A%20%20%20Attribution%20methods%20correspond%20to%20a%20class%20of%20explainability%20methods%20%28XAI%29%0Athat%20aim%20to%20assess%20how%20individual%20inputs%20contribute%20to%20a%20model%27s%0Adecision-making%20process.%20We%20have%20identified%20a%20significant%20limitation%20in%20one%0Atype%20of%20attribution%20methods%2C%20known%20as%20%22white-box%22%20methods.%20Although%20highly%0Aefficient%2C%20these%20methods%20rely%20on%20a%20gradient%20signal%20that%20is%20often%20contaminated%0Aby%20high-frequency%20noise.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20new%0Aapproach%20called%20%22FORGrad%22.%20This%20simple%20method%20effectively%20filters%20out%20noise%0Aartifacts%20by%20using%20optimal%20cut-off%20frequencies%20tailored%20to%20the%20unique%0Acharacteristics%20of%20each%20model%20architecture.%20Our%20findings%20show%20that%20FORGrad%0Aconsistently%20enhances%20the%20performance%20of%20already%20existing%20white-box%20methods%2C%0Aenabling%20them%20to%20compete%20effectively%20with%20more%20accurate%20yet%20computationally%0Ademanding%20%22black-box%22%20methods.%20We%20anticipate%20that%20our%20research%20will%20foster%0Abroader%20adoption%20of%20simpler%20and%20more%20efficient%20white-box%20methods%20for%0Aexplainability%2C%20offering%20a%20better%20balance%20between%20faithfulness%20and%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09591v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20strikes%20back%3A%20How%20filtering%20out%20high%20frequencies%20improves%0A%20%20explanations&entry.906535625=Sabine%20Muzellec%20and%20Thomas%20Fel%20and%20Victor%20Boutin%20and%20L%C3%A9o%20and%C3%A9ol%20and%20Rufin%20VanRullen%20and%20Thomas%20Serre&entry.1292438233=%20%20Attribution%20methods%20correspond%20to%20a%20class%20of%20explainability%20methods%20%28XAI%29%0Athat%20aim%20to%20assess%20how%20individual%20inputs%20contribute%20to%20a%20model%27s%0Adecision-making%20process.%20We%20have%20identified%20a%20significant%20limitation%20in%20one%0Atype%20of%20attribution%20methods%2C%20known%20as%20%22white-box%22%20methods.%20Although%20highly%0Aefficient%2C%20these%20methods%20rely%20on%20a%20gradient%20signal%20that%20is%20often%20contaminated%0Aby%20high-frequency%20noise.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20new%0Aapproach%20called%20%22FORGrad%22.%20This%20simple%20method%20effectively%20filters%20out%20noise%0Aartifacts%20by%20using%20optimal%20cut-off%20frequencies%20tailored%20to%20the%20unique%0Acharacteristics%20of%20each%20model%20architecture.%20Our%20findings%20show%20that%20FORGrad%0Aconsistently%20enhances%20the%20performance%20of%20already%20existing%20white-box%20methods%2C%0Aenabling%20them%20to%20compete%20effectively%20with%20more%20accurate%20yet%20computationally%0Ademanding%20%22black-box%22%20methods.%20We%20anticipate%20that%20our%20research%20will%20foster%0Abroader%20adoption%20of%20simpler%20and%20more%20efficient%20white-box%20methods%20for%0Aexplainability%2C%20offering%20a%20better%20balance%20between%20faithfulness%20and%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09591v2&entry.124074799=Read"},
{"title": "Artificial Neural Networks-based Real-time Classification of ENG Signals\n  for Implanted Nerve Interfaces", "author": "ntonio Coviello and Francesco Linsalata and Umberto Spagnolini and Maurizio Magarini", "abstract": "  Neuropathies are gaining higher relevance in clinical settings, as they risk\npermanently jeopardizing a person's life. To support the recovery of patients,\nthe use of fully implanted devices is emerging as one of the most promising\nsolutions. However, these devices, even if becoming an integral part of a fully\ncomplex neural nanonetwork system, pose numerous challenges. In this article,\nwe address one of them, which consists of the classification of motor/sensory\nstimuli. The task is performed by exploring four different types of artificial\nneural networks (ANNs) to extract various sensory stimuli from the\nelectroneurographic (ENG) signal measured in the sciatic nerve of rats.\nDifferent sizes of the data sets are considered to analyze the feasibility of\nthe investigated ANNs for real-time classification through a comparison of\ntheir performance in terms of accuracy, F1-score, and prediction time. The\ndesign of the ANNs takes advantage of the modelling of the ENG signal as a\nmultiple-input multiple-output (MIMO) system to describe the measures taken by\nstate-of-the-art implanted nerve interfaces. These are based on the use of\nmulti-contact cuff electrodes to achieve nanoscale spatial discrimination of\nthe nerve activity. The MIMO ENG signal model is another contribution of this\npaper. Our results show that some ANNs are more suitable for real-time\napplications, being capable of achieving accuracies over $90\\%$ for signal\nwindows of $100$ and $200\\,$ms with a low enough processing time to be\neffective for pathology recovery.\n", "link": "http://arxiv.org/abs/2403.20234v1", "date": "2024-03-29", "relevancy": 1.9055, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4292}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Artificial%20Neural%20Networks-based%20Real-time%20Classification%20of%20ENG%20Signals%0A%20%20for%20Implanted%20Nerve%20Interfaces&body=Title%3A%20Artificial%20Neural%20Networks-based%20Real-time%20Classification%20of%20ENG%20Signals%0A%20%20for%20Implanted%20Nerve%20Interfaces%0AAuthor%3A%20ntonio%20Coviello%20and%20Francesco%20Linsalata%20and%20Umberto%20Spagnolini%20and%20Maurizio%20Magarini%0AAbstract%3A%20%20%20Neuropathies%20are%20gaining%20higher%20relevance%20in%20clinical%20settings%2C%20as%20they%20risk%0Apermanently%20jeopardizing%20a%20person%27s%20life.%20To%20support%20the%20recovery%20of%20patients%2C%0Athe%20use%20of%20fully%20implanted%20devices%20is%20emerging%20as%20one%20of%20the%20most%20promising%0Asolutions.%20However%2C%20these%20devices%2C%20even%20if%20becoming%20an%20integral%20part%20of%20a%20fully%0Acomplex%20neural%20nanonetwork%20system%2C%20pose%20numerous%20challenges.%20In%20this%20article%2C%0Awe%20address%20one%20of%20them%2C%20which%20consists%20of%20the%20classification%20of%20motor/sensory%0Astimuli.%20The%20task%20is%20performed%20by%20exploring%20four%20different%20types%20of%20artificial%0Aneural%20networks%20%28ANNs%29%20to%20extract%20various%20sensory%20stimuli%20from%20the%0Aelectroneurographic%20%28ENG%29%20signal%20measured%20in%20the%20sciatic%20nerve%20of%20rats.%0ADifferent%20sizes%20of%20the%20data%20sets%20are%20considered%20to%20analyze%20the%20feasibility%20of%0Athe%20investigated%20ANNs%20for%20real-time%20classification%20through%20a%20comparison%20of%0Atheir%20performance%20in%20terms%20of%20accuracy%2C%20F1-score%2C%20and%20prediction%20time.%20The%0Adesign%20of%20the%20ANNs%20takes%20advantage%20of%20the%20modelling%20of%20the%20ENG%20signal%20as%20a%0Amultiple-input%20multiple-output%20%28MIMO%29%20system%20to%20describe%20the%20measures%20taken%20by%0Astate-of-the-art%20implanted%20nerve%20interfaces.%20These%20are%20based%20on%20the%20use%20of%0Amulti-contact%20cuff%20electrodes%20to%20achieve%20nanoscale%20spatial%20discrimination%20of%0Athe%20nerve%20activity.%20The%20MIMO%20ENG%20signal%20model%20is%20another%20contribution%20of%20this%0Apaper.%20Our%20results%20show%20that%20some%20ANNs%20are%20more%20suitable%20for%20real-time%0Aapplications%2C%20being%20capable%20of%20achieving%20accuracies%20over%20%2490%5C%25%24%20for%20signal%0Awindows%20of%20%24100%24%20and%20%24200%5C%2C%24ms%20with%20a%20low%20enough%20processing%20time%20to%20be%0Aeffective%20for%20pathology%20recovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20234v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Neural%20Networks-based%20Real-time%20Classification%20of%20ENG%20Signals%0A%20%20for%20Implanted%20Nerve%20Interfaces&entry.906535625=ntonio%20Coviello%20and%20Francesco%20Linsalata%20and%20Umberto%20Spagnolini%20and%20Maurizio%20Magarini&entry.1292438233=%20%20Neuropathies%20are%20gaining%20higher%20relevance%20in%20clinical%20settings%2C%20as%20they%20risk%0Apermanently%20jeopardizing%20a%20person%27s%20life.%20To%20support%20the%20recovery%20of%20patients%2C%0Athe%20use%20of%20fully%20implanted%20devices%20is%20emerging%20as%20one%20of%20the%20most%20promising%0Asolutions.%20However%2C%20these%20devices%2C%20even%20if%20becoming%20an%20integral%20part%20of%20a%20fully%0Acomplex%20neural%20nanonetwork%20system%2C%20pose%20numerous%20challenges.%20In%20this%20article%2C%0Awe%20address%20one%20of%20them%2C%20which%20consists%20of%20the%20classification%20of%20motor/sensory%0Astimuli.%20The%20task%20is%20performed%20by%20exploring%20four%20different%20types%20of%20artificial%0Aneural%20networks%20%28ANNs%29%20to%20extract%20various%20sensory%20stimuli%20from%20the%0Aelectroneurographic%20%28ENG%29%20signal%20measured%20in%20the%20sciatic%20nerve%20of%20rats.%0ADifferent%20sizes%20of%20the%20data%20sets%20are%20considered%20to%20analyze%20the%20feasibility%20of%0Athe%20investigated%20ANNs%20for%20real-time%20classification%20through%20a%20comparison%20of%0Atheir%20performance%20in%20terms%20of%20accuracy%2C%20F1-score%2C%20and%20prediction%20time.%20The%0Adesign%20of%20the%20ANNs%20takes%20advantage%20of%20the%20modelling%20of%20the%20ENG%20signal%20as%20a%0Amultiple-input%20multiple-output%20%28MIMO%29%20system%20to%20describe%20the%20measures%20taken%20by%0Astate-of-the-art%20implanted%20nerve%20interfaces.%20These%20are%20based%20on%20the%20use%20of%0Amulti-contact%20cuff%20electrodes%20to%20achieve%20nanoscale%20spatial%20discrimination%20of%0Athe%20nerve%20activity.%20The%20MIMO%20ENG%20signal%20model%20is%20another%20contribution%20of%20this%0Apaper.%20Our%20results%20show%20that%20some%20ANNs%20are%20more%20suitable%20for%20real-time%0Aapplications%2C%20being%20capable%20of%20achieving%20accuracies%20over%20%2490%5C%25%24%20for%20signal%0Awindows%20of%20%24100%24%20and%20%24200%5C%2C%24ms%20with%20a%20low%20enough%20processing%20time%20to%20be%0Aeffective%20for%20pathology%20recovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20234v1&entry.124074799=Read"},
{"title": "A multiobjective continuation method to compute the regularization path\n  of deep neural networks", "author": "Augustina C. Amakor and Konstantin Sonntag and Sebastian Peitz", "abstract": "  Sparsity is a highly desired feature in deep neural networks (DNNs) since it\nensures numerical efficiency, improves the interpretability of models (due to\nthe smaller number of relevant features), and robustness. For linear models, it\nis well known that there exists a \\emph{regularization path} connecting the\nsparsest solution in terms of the $\\ell^1$ norm, i.e., zero weights and the\nnon-regularized solution. Very recently, there was a first attempt to extend\nthe concept of regularization paths to DNNs by means of treating the empirical\nloss and sparsity ($\\ell^1$ norm) as two conflicting criteria and solving the\nresulting multiobjective optimization problem for low-dimensional DNN. However,\ndue to the non-smoothness of the $\\ell^1$ norm and the high number of\nparameters, this approach is not very efficient from a computational\nperspective for high-dimensional DNNs. To overcome this limitation, we present\nan algorithm that allows for the approximation of the entire Pareto front for\nthe above-mentioned objectives in a very efficient manner for high-dimensional\nDNNs with millions of parameters. We present numerical examples using both\ndeterministic and stochastic gradients. We furthermore demonstrate that\nknowledge of the regularization path allows for a well-generalizing network\nparametrization. To the best of our knowledge, this is the first algorithm to\ncompute the regularization path for non-convex multiobjective optimization\nproblems (MOPs) with millions of degrees of freedom.\n", "link": "http://arxiv.org/abs/2308.12044v5", "date": "2024-03-29", "relevancy": 1.9019, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4863}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4847}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4609}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20multiobjective%20continuation%20method%20to%20compute%20the%20regularization%20path%0A%20%20of%20deep%20neural%20networks&body=Title%3A%20A%20multiobjective%20continuation%20method%20to%20compute%20the%20regularization%20path%0A%20%20of%20deep%20neural%20networks%0AAuthor%3A%20Augustina%20C.%20Amakor%20and%20Konstantin%20Sonntag%20and%20Sebastian%20Peitz%0AAbstract%3A%20%20%20Sparsity%20is%20a%20highly%20desired%20feature%20in%20deep%20neural%20networks%20%28DNNs%29%20since%20it%0Aensures%20numerical%20efficiency%2C%20improves%20the%20interpretability%20of%20models%20%28due%20to%0Athe%20smaller%20number%20of%20relevant%20features%29%2C%20and%20robustness.%20For%20linear%20models%2C%20it%0Ais%20well%20known%20that%20there%20exists%20a%20%5Cemph%7Bregularization%20path%7D%20connecting%20the%0Asparsest%20solution%20in%20terms%20of%20the%20%24%5Cell%5E1%24%20norm%2C%20i.e.%2C%20zero%20weights%20and%20the%0Anon-regularized%20solution.%20Very%20recently%2C%20there%20was%20a%20first%20attempt%20to%20extend%0Athe%20concept%20of%20regularization%20paths%20to%20DNNs%20by%20means%20of%20treating%20the%20empirical%0Aloss%20and%20sparsity%20%28%24%5Cell%5E1%24%20norm%29%20as%20two%20conflicting%20criteria%20and%20solving%20the%0Aresulting%20multiobjective%20optimization%20problem%20for%20low-dimensional%20DNN.%20However%2C%0Adue%20to%20the%20non-smoothness%20of%20the%20%24%5Cell%5E1%24%20norm%20and%20the%20high%20number%20of%0Aparameters%2C%20this%20approach%20is%20not%20very%20efficient%20from%20a%20computational%0Aperspective%20for%20high-dimensional%20DNNs.%20To%20overcome%20this%20limitation%2C%20we%20present%0Aan%20algorithm%20that%20allows%20for%20the%20approximation%20of%20the%20entire%20Pareto%20front%20for%0Athe%20above-mentioned%20objectives%20in%20a%20very%20efficient%20manner%20for%20high-dimensional%0ADNNs%20with%20millions%20of%20parameters.%20We%20present%20numerical%20examples%20using%20both%0Adeterministic%20and%20stochastic%20gradients.%20We%20furthermore%20demonstrate%20that%0Aknowledge%20of%20the%20regularization%20path%20allows%20for%20a%20well-generalizing%20network%0Aparametrization.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20algorithm%20to%0Acompute%20the%20regularization%20path%20for%20non-convex%20multiobjective%20optimization%0Aproblems%20%28MOPs%29%20with%20millions%20of%20degrees%20of%20freedom.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12044v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20multiobjective%20continuation%20method%20to%20compute%20the%20regularization%20path%0A%20%20of%20deep%20neural%20networks&entry.906535625=Augustina%20C.%20Amakor%20and%20Konstantin%20Sonntag%20and%20Sebastian%20Peitz&entry.1292438233=%20%20Sparsity%20is%20a%20highly%20desired%20feature%20in%20deep%20neural%20networks%20%28DNNs%29%20since%20it%0Aensures%20numerical%20efficiency%2C%20improves%20the%20interpretability%20of%20models%20%28due%20to%0Athe%20smaller%20number%20of%20relevant%20features%29%2C%20and%20robustness.%20For%20linear%20models%2C%20it%0Ais%20well%20known%20that%20there%20exists%20a%20%5Cemph%7Bregularization%20path%7D%20connecting%20the%0Asparsest%20solution%20in%20terms%20of%20the%20%24%5Cell%5E1%24%20norm%2C%20i.e.%2C%20zero%20weights%20and%20the%0Anon-regularized%20solution.%20Very%20recently%2C%20there%20was%20a%20first%20attempt%20to%20extend%0Athe%20concept%20of%20regularization%20paths%20to%20DNNs%20by%20means%20of%20treating%20the%20empirical%0Aloss%20and%20sparsity%20%28%24%5Cell%5E1%24%20norm%29%20as%20two%20conflicting%20criteria%20and%20solving%20the%0Aresulting%20multiobjective%20optimization%20problem%20for%20low-dimensional%20DNN.%20However%2C%0Adue%20to%20the%20non-smoothness%20of%20the%20%24%5Cell%5E1%24%20norm%20and%20the%20high%20number%20of%0Aparameters%2C%20this%20approach%20is%20not%20very%20efficient%20from%20a%20computational%0Aperspective%20for%20high-dimensional%20DNNs.%20To%20overcome%20this%20limitation%2C%20we%20present%0Aan%20algorithm%20that%20allows%20for%20the%20approximation%20of%20the%20entire%20Pareto%20front%20for%0Athe%20above-mentioned%20objectives%20in%20a%20very%20efficient%20manner%20for%20high-dimensional%0ADNNs%20with%20millions%20of%20parameters.%20We%20present%20numerical%20examples%20using%20both%0Adeterministic%20and%20stochastic%20gradients.%20We%20furthermore%20demonstrate%20that%0Aknowledge%20of%20the%20regularization%20path%20allows%20for%20a%20well-generalizing%20network%0Aparametrization.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20algorithm%20to%0Acompute%20the%20regularization%20path%20for%20non-convex%20multiobjective%20optimization%0Aproblems%20%28MOPs%29%20with%20millions%20of%20degrees%20of%20freedom.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12044v5&entry.124074799=Read"},
{"title": "Accurate Block Quantization in LLMs with Outliers", "author": "Nikita Trukhanov and Ilya Soloveychik", "abstract": "  The demand for inference on extremely large scale LLMs has seen enormous\ngrowth in the recent months. It made evident the colossal shortage of dedicated\nhardware capable of efficient and fast processing of the involved compute and\nmemory movement. The problem is aggravated by the exploding raise in the\nlengths of the sequences being processed, since those require efficient on-chip\nstorage of the KV-cache of size proportional to the sequence length. To make\nthe required compute feasible and fit the involved data into available memory,\nnumerous quantization techniques have been proposed that allow accurate\nquantization for both weights and activations. One of the main recent\nbreakthroughs in this direction was introduction of the family of Block\nFloating Point (BFP) formats characterized by a block of mantissas with a\nshared scale factor. These enable memory- power-, and compute- efficient\nhardware support of the tensor operations and provide extremely good\nquantization accuracy. The main issues preventing widespread application of\nblock formats is caused by the presence of outliers in weights and activations\nsince those affect the accuracy of the other values in the same block. In this\npaper, we focus on the most critical problem of limited KV-cache storage. We\npropose a novel approach enabling usage of low precision BFP formats without\ncompromising the resulting model accuracy. We exploit the common channel-wise\npatterns exhibited by the outliers to rearrange them in such a way, that their\nquantization quality is significantly improved. The methodology yields 2x\nsavings in the memory footprint without significant degradation of the model's\naccuracy. Importantly, the rearrangement of channels happens at the compile\ntime and thus has no impact on the inference latency.\n", "link": "http://arxiv.org/abs/2403.20137v1", "date": "2024-03-29", "relevancy": 1.8902, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4769}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4708}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.469}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Accurate%20Block%20Quantization%20in%20LLMs%20with%20Outliers&body=Title%3A%20Accurate%20Block%20Quantization%20in%20LLMs%20with%20Outliers%0AAuthor%3A%20Nikita%20Trukhanov%20and%20Ilya%20Soloveychik%0AAbstract%3A%20%20%20The%20demand%20for%20inference%20on%20extremely%20large%20scale%20LLMs%20has%20seen%20enormous%0Agrowth%20in%20the%20recent%20months.%20It%20made%20evident%20the%20colossal%20shortage%20of%20dedicated%0Ahardware%20capable%20of%20efficient%20and%20fast%20processing%20of%20the%20involved%20compute%20and%0Amemory%20movement.%20The%20problem%20is%20aggravated%20by%20the%20exploding%20raise%20in%20the%0Alengths%20of%20the%20sequences%20being%20processed%2C%20since%20those%20require%20efficient%20on-chip%0Astorage%20of%20the%20KV-cache%20of%20size%20proportional%20to%20the%20sequence%20length.%20To%20make%0Athe%20required%20compute%20feasible%20and%20fit%20the%20involved%20data%20into%20available%20memory%2C%0Anumerous%20quantization%20techniques%20have%20been%20proposed%20that%20allow%20accurate%0Aquantization%20for%20both%20weights%20and%20activations.%20One%20of%20the%20main%20recent%0Abreakthroughs%20in%20this%20direction%20was%20introduction%20of%20the%20family%20of%20Block%0AFloating%20Point%20%28BFP%29%20formats%20characterized%20by%20a%20block%20of%20mantissas%20with%20a%0Ashared%20scale%20factor.%20These%20enable%20memory-%20power-%2C%20and%20compute-%20efficient%0Ahardware%20support%20of%20the%20tensor%20operations%20and%20provide%20extremely%20good%0Aquantization%20accuracy.%20The%20main%20issues%20preventing%20widespread%20application%20of%0Ablock%20formats%20is%20caused%20by%20the%20presence%20of%20outliers%20in%20weights%20and%20activations%0Asince%20those%20affect%20the%20accuracy%20of%20the%20other%20values%20in%20the%20same%20block.%20In%20this%0Apaper%2C%20we%20focus%20on%20the%20most%20critical%20problem%20of%20limited%20KV-cache%20storage.%20We%0Apropose%20a%20novel%20approach%20enabling%20usage%20of%20low%20precision%20BFP%20formats%20without%0Acompromising%20the%20resulting%20model%20accuracy.%20We%20exploit%20the%20common%20channel-wise%0Apatterns%20exhibited%20by%20the%20outliers%20to%20rearrange%20them%20in%20such%20a%20way%2C%20that%20their%0Aquantization%20quality%20is%20significantly%20improved.%20The%20methodology%20yields%202x%0Asavings%20in%20the%20memory%20footprint%20without%20significant%20degradation%20of%20the%20model%27s%0Aaccuracy.%20Importantly%2C%20the%20rearrangement%20of%20channels%20happens%20at%20the%20compile%0Atime%20and%20thus%20has%20no%20impact%20on%20the%20inference%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20137v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20Block%20Quantization%20in%20LLMs%20with%20Outliers&entry.906535625=Nikita%20Trukhanov%20and%20Ilya%20Soloveychik&entry.1292438233=%20%20The%20demand%20for%20inference%20on%20extremely%20large%20scale%20LLMs%20has%20seen%20enormous%0Agrowth%20in%20the%20recent%20months.%20It%20made%20evident%20the%20colossal%20shortage%20of%20dedicated%0Ahardware%20capable%20of%20efficient%20and%20fast%20processing%20of%20the%20involved%20compute%20and%0Amemory%20movement.%20The%20problem%20is%20aggravated%20by%20the%20exploding%20raise%20in%20the%0Alengths%20of%20the%20sequences%20being%20processed%2C%20since%20those%20require%20efficient%20on-chip%0Astorage%20of%20the%20KV-cache%20of%20size%20proportional%20to%20the%20sequence%20length.%20To%20make%0Athe%20required%20compute%20feasible%20and%20fit%20the%20involved%20data%20into%20available%20memory%2C%0Anumerous%20quantization%20techniques%20have%20been%20proposed%20that%20allow%20accurate%0Aquantization%20for%20both%20weights%20and%20activations.%20One%20of%20the%20main%20recent%0Abreakthroughs%20in%20this%20direction%20was%20introduction%20of%20the%20family%20of%20Block%0AFloating%20Point%20%28BFP%29%20formats%20characterized%20by%20a%20block%20of%20mantissas%20with%20a%0Ashared%20scale%20factor.%20These%20enable%20memory-%20power-%2C%20and%20compute-%20efficient%0Ahardware%20support%20of%20the%20tensor%20operations%20and%20provide%20extremely%20good%0Aquantization%20accuracy.%20The%20main%20issues%20preventing%20widespread%20application%20of%0Ablock%20formats%20is%20caused%20by%20the%20presence%20of%20outliers%20in%20weights%20and%20activations%0Asince%20those%20affect%20the%20accuracy%20of%20the%20other%20values%20in%20the%20same%20block.%20In%20this%0Apaper%2C%20we%20focus%20on%20the%20most%20critical%20problem%20of%20limited%20KV-cache%20storage.%20We%0Apropose%20a%20novel%20approach%20enabling%20usage%20of%20low%20precision%20BFP%20formats%20without%0Acompromising%20the%20resulting%20model%20accuracy.%20We%20exploit%20the%20common%20channel-wise%0Apatterns%20exhibited%20by%20the%20outliers%20to%20rearrange%20them%20in%20such%20a%20way%2C%20that%20their%0Aquantization%20quality%20is%20significantly%20improved.%20The%20methodology%20yields%202x%0Asavings%20in%20the%20memory%20footprint%20without%20significant%20degradation%20of%20the%20model%27s%0Aaccuracy.%20Importantly%2C%20the%20rearrangement%20of%20channels%20happens%20at%20the%20compile%0Atime%20and%20thus%20has%20no%20impact%20on%20the%20inference%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20137v1&entry.124074799=Read"},
{"title": "What Generative Artificial Intelligence Means for Terminological\n  Definitions", "author": "Antonio San Mart\u00edn", "abstract": "  This paper examines the impact of Generative Artificial Intelligence (GenAI)\ntools like ChatGPT on the creation and consumption of terminological\ndefinitions. From the terminologist's point of view, the strategic use of GenAI\ntools can streamline the process of crafting definitions, reducing both time\nand effort, while potentially enhancing quality. GenAI tools enable AI-assisted\nterminography, notably post-editing terminography, where the machine produces a\ndefinition that the terminologist then corrects or refines. However, the\npotential of GenAI tools to fulfill all the terminological needs of a user,\nincluding term definitions, challenges the very existence of terminological\ndefinitions and resources as we know them. Unlike terminological definitions,\nGenAI tools can describe the knowledge activated by a term in a specific\ncontext. However, a main drawback of these tools is that their output can\ncontain errors. For this reason, users requiring reliability will likely still\nresort to terminological resources for definitions. Nevertheless, with the\ninevitable integration of AI into terminology work, the distinction between\nhuman-created and AI-created content will become increasingly blurred.\n", "link": "http://arxiv.org/abs/2402.16139v2", "date": "2024-03-29", "relevancy": 1.8659, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5347}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.401}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions&body=Title%3A%20What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions%0AAuthor%3A%20Antonio%20San%20Mart%C3%ADn%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Atools%20like%20ChatGPT%20on%20the%20creation%20and%20consumption%20of%20terminological%0Adefinitions.%20From%20the%20terminologist%27s%20point%20of%20view%2C%20the%20strategic%20use%20of%20GenAI%0Atools%20can%20streamline%20the%20process%20of%20crafting%20definitions%2C%20reducing%20both%20time%0Aand%20effort%2C%20while%20potentially%20enhancing%20quality.%20GenAI%20tools%20enable%20AI-assisted%0Aterminography%2C%20notably%20post-editing%20terminography%2C%20where%20the%20machine%20produces%20a%0Adefinition%20that%20the%20terminologist%20then%20corrects%20or%20refines.%20However%2C%20the%0Apotential%20of%20GenAI%20tools%20to%20fulfill%20all%20the%20terminological%20needs%20of%20a%20user%2C%0Aincluding%20term%20definitions%2C%20challenges%20the%20very%20existence%20of%20terminological%0Adefinitions%20and%20resources%20as%20we%20know%20them.%20Unlike%20terminological%20definitions%2C%0AGenAI%20tools%20can%20describe%20the%20knowledge%20activated%20by%20a%20term%20in%20a%20specific%0Acontext.%20However%2C%20a%20main%20drawback%20of%20these%20tools%20is%20that%20their%20output%20can%0Acontain%20errors.%20For%20this%20reason%2C%20users%20requiring%20reliability%20will%20likely%20still%0Aresort%20to%20terminological%20resources%20for%20definitions.%20Nevertheless%2C%20with%20the%0Ainevitable%20integration%20of%20AI%20into%20terminology%20work%2C%20the%20distinction%20between%0Ahuman-created%20and%20AI-created%20content%20will%20become%20increasingly%20blurred.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16139v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Generative%20Artificial%20Intelligence%20Means%20for%20Terminological%0A%20%20Definitions&entry.906535625=Antonio%20San%20Mart%C3%ADn&entry.1292438233=%20%20This%20paper%20examines%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%0Atools%20like%20ChatGPT%20on%20the%20creation%20and%20consumption%20of%20terminological%0Adefinitions.%20From%20the%20terminologist%27s%20point%20of%20view%2C%20the%20strategic%20use%20of%20GenAI%0Atools%20can%20streamline%20the%20process%20of%20crafting%20definitions%2C%20reducing%20both%20time%0Aand%20effort%2C%20while%20potentially%20enhancing%20quality.%20GenAI%20tools%20enable%20AI-assisted%0Aterminography%2C%20notably%20post-editing%20terminography%2C%20where%20the%20machine%20produces%20a%0Adefinition%20that%20the%20terminologist%20then%20corrects%20or%20refines.%20However%2C%20the%0Apotential%20of%20GenAI%20tools%20to%20fulfill%20all%20the%20terminological%20needs%20of%20a%20user%2C%0Aincluding%20term%20definitions%2C%20challenges%20the%20very%20existence%20of%20terminological%0Adefinitions%20and%20resources%20as%20we%20know%20them.%20Unlike%20terminological%20definitions%2C%0AGenAI%20tools%20can%20describe%20the%20knowledge%20activated%20by%20a%20term%20in%20a%20specific%0Acontext.%20However%2C%20a%20main%20drawback%20of%20these%20tools%20is%20that%20their%20output%20can%0Acontain%20errors.%20For%20this%20reason%2C%20users%20requiring%20reliability%20will%20likely%20still%0Aresort%20to%20terminological%20resources%20for%20definitions.%20Nevertheless%2C%20with%20the%0Ainevitable%20integration%20of%20AI%20into%20terminology%20work%2C%20the%20distinction%20between%0Ahuman-created%20and%20AI-created%20content%20will%20become%20increasingly%20blurred.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16139v2&entry.124074799=Read"},
{"title": "Learning Visual Quadrupedal Loco-Manipulation from Demonstrations", "author": "Zhengmao He and Kun Lei and Yanjie Ze and Koushil Sreenath and Zhongyu Li and Huazhe Xu", "abstract": "  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n", "link": "http://arxiv.org/abs/2403.20328v1", "date": "2024-03-29", "relevancy": 1.849, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6107}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations&body=Title%3A%20Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations%0AAuthor%3A%20Zhengmao%20He%20and%20Kun%20Lei%20and%20Yanjie%20Ze%20and%20Koushil%20Sreenath%20and%20Zhongyu%20Li%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Quadruped%20robots%20are%20progressively%20being%20integrated%20into%20human%20environments.%0ADespite%20the%20growing%20locomotion%20capabilities%20of%20quadrupedal%20robots%2C%20their%0Ainteraction%20with%20objects%20in%20realistic%20scenes%20is%20still%20limited.%20While%20additional%0Arobotic%20arms%20on%20quadrupedal%20robots%20enable%20manipulating%20objects%2C%20they%20are%0Asometimes%20redundant%20given%20that%20a%20quadruped%20robot%20is%20essentially%20a%20mobile%20unit%0Aequipped%20with%20four%20limbs%2C%20each%20possessing%203%20degrees%20of%20freedom%20%28DoFs%29.%20Hence%2C%0Awe%20aim%20to%20empower%20a%20quadruped%20robot%20to%20execute%20real-world%20manipulation%20tasks%0Ausing%20only%20its%20legs.%20We%20decompose%20the%20loco-manipulation%20process%20into%20a%0Alow-level%20reinforcement%20learning%20%28RL%29-based%20controller%20and%20a%20high-level%0ABehavior%20Cloning%20%28BC%29-based%20planner.%20By%20parameterizing%20the%20manipulation%0Atrajectory%2C%20we%20synchronize%20the%20efforts%20of%20the%20upper%20and%20lower%20layers%2C%20thereby%0Aleveraging%20the%20advantages%20of%20both%20RL%20and%20BC.%20Our%20approach%20is%20validated%20through%0Asimulations%20and%20real-world%20experiments%2C%20demonstrating%20the%20robot%27s%20ability%20to%0Aperform%20tasks%20that%20demand%20mobility%20and%20high%20precision%2C%20such%20as%20lifting%20a%20basket%0Afrom%20the%20ground%20while%20moving%2C%20closing%20a%20dishwasher%2C%20pressing%20a%20button%2C%20and%0Apushing%20a%20door.%20Project%20website%3A%20https%3A//zhengmaohe.github.io/leg-manip%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20328v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations&entry.906535625=Zhengmao%20He%20and%20Kun%20Lei%20and%20Yanjie%20Ze%20and%20Koushil%20Sreenath%20and%20Zhongyu%20Li%20and%20Huazhe%20Xu&entry.1292438233=%20%20Quadruped%20robots%20are%20progressively%20being%20integrated%20into%20human%20environments.%0ADespite%20the%20growing%20locomotion%20capabilities%20of%20quadrupedal%20robots%2C%20their%0Ainteraction%20with%20objects%20in%20realistic%20scenes%20is%20still%20limited.%20While%20additional%0Arobotic%20arms%20on%20quadrupedal%20robots%20enable%20manipulating%20objects%2C%20they%20are%0Asometimes%20redundant%20given%20that%20a%20quadruped%20robot%20is%20essentially%20a%20mobile%20unit%0Aequipped%20with%20four%20limbs%2C%20each%20possessing%203%20degrees%20of%20freedom%20%28DoFs%29.%20Hence%2C%0Awe%20aim%20to%20empower%20a%20quadruped%20robot%20to%20execute%20real-world%20manipulation%20tasks%0Ausing%20only%20its%20legs.%20We%20decompose%20the%20loco-manipulation%20process%20into%20a%0Alow-level%20reinforcement%20learning%20%28RL%29-based%20controller%20and%20a%20high-level%0ABehavior%20Cloning%20%28BC%29-based%20planner.%20By%20parameterizing%20the%20manipulation%0Atrajectory%2C%20we%20synchronize%20the%20efforts%20of%20the%20upper%20and%20lower%20layers%2C%20thereby%0Aleveraging%20the%20advantages%20of%20both%20RL%20and%20BC.%20Our%20approach%20is%20validated%20through%0Asimulations%20and%20real-world%20experiments%2C%20demonstrating%20the%20robot%27s%20ability%20to%0Aperform%20tasks%20that%20demand%20mobility%20and%20high%20precision%2C%20such%20as%20lifting%20a%20basket%0Afrom%20the%20ground%20while%20moving%2C%20closing%20a%20dishwasher%2C%20pressing%20a%20button%2C%20and%0Apushing%20a%20door.%20Project%20website%3A%20https%3A//zhengmaohe.github.io/leg-manip%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20328v1&entry.124074799=Read"},
{"title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text", "author": "Kaito Taguchi and Yujie Gu and Kouichi Sakurai", "abstract": "  In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.\n", "link": "http://arxiv.org/abs/2403.20127v1", "date": "2024-03-29", "relevancy": 1.847, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4807}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4705}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4454}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Prompts%20on%20Zero-Shot%20Detection%20of%20AI-Generated%20Text&body=Title%3A%20The%20Impact%20of%20Prompts%20on%20Zero-Shot%20Detection%20of%20AI-Generated%20Text%0AAuthor%3A%20Kaito%20Taguchi%20and%20Yujie%20Gu%20and%20Kouichi%20Sakurai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20have%20been%20significant%20advancements%20in%20the%20development%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20While%20their%20practical%20applications%20are%20now%0Awidespread%2C%20their%20potential%20for%20misuse%2C%20such%20as%20generating%20fake%20news%20and%0Acommitting%20plagiarism%2C%20has%20posed%20significant%20concerns.%20To%20address%20this%20issue%2C%0Adetectors%20have%20been%20developed%20to%20evaluate%20whether%20a%20given%20text%20is%0Ahuman-generated%20or%20AI-generated.%20Among%20others%2C%20zero-shot%20detectors%20stand%20out%20as%0Aeffective%20approaches%20that%20do%20not%20require%20additional%20training%20data%20and%20are%20often%0Alikelihood-based.%20In%20chat-based%20applications%2C%20users%20commonly%20input%20prompts%20and%0Autilize%20the%20AI-generated%20texts.%20However%2C%20zero-shot%20detectors%20typically%20analyze%0Athese%20texts%20in%20isolation%2C%20neglecting%20the%20impact%20of%20the%20original%20prompts.%20It%20is%0Aconceivable%20that%20this%20approach%20may%20lead%20to%20a%20discrepancy%20in%20likelihood%0Aassessments%20between%20the%20text%20generation%20phase%20and%20the%20detection%20phase.%20So%20far%2C%0Athere%20remains%20an%20unverified%20gap%20concerning%20how%20the%20presence%20or%20absence%20of%0Aprompts%20impacts%20detection%20accuracy%20for%20zero-shot%20detectors.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20evaluative%20framework%20to%20empirically%20analyze%20the%20impact%20of%20prompts%0Aon%20the%20detection%20accuracy%20of%20AI-generated%20text.%20We%20assess%20various%20zero-shot%0Adetectors%20using%20both%20white-box%20detection%2C%20which%20leverages%20the%20prompt%2C%20and%0Ablack-box%20detection%2C%20which%20operates%20without%20prompt%20information.%20Our%20experiments%0Areveal%20the%20significant%20influence%20of%20prompts%20on%20detection%20accuracy.%20Remarkably%2C%0Acompared%20with%20black-box%20detection%20without%20prompts%2C%20the%20white-box%20methods%20using%0Aprompts%20demonstrate%20an%20increase%20in%20AUC%20of%20at%20least%20%240.1%24%20across%20all%20zero-shot%0Adetectors%20tested.%20Code%20is%20available%3A%0A%5Curl%7Bhttps%3A//github.com/kaito25atugich/Detector%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20127v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Prompts%20on%20Zero-Shot%20Detection%20of%20AI-Generated%20Text&entry.906535625=Kaito%20Taguchi%20and%20Yujie%20Gu%20and%20Kouichi%20Sakurai&entry.1292438233=%20%20In%20recent%20years%2C%20there%20have%20been%20significant%20advancements%20in%20the%20development%0Aof%20Large%20Language%20Models%20%28LLMs%29.%20While%20their%20practical%20applications%20are%20now%0Awidespread%2C%20their%20potential%20for%20misuse%2C%20such%20as%20generating%20fake%20news%20and%0Acommitting%20plagiarism%2C%20has%20posed%20significant%20concerns.%20To%20address%20this%20issue%2C%0Adetectors%20have%20been%20developed%20to%20evaluate%20whether%20a%20given%20text%20is%0Ahuman-generated%20or%20AI-generated.%20Among%20others%2C%20zero-shot%20detectors%20stand%20out%20as%0Aeffective%20approaches%20that%20do%20not%20require%20additional%20training%20data%20and%20are%20often%0Alikelihood-based.%20In%20chat-based%20applications%2C%20users%20commonly%20input%20prompts%20and%0Autilize%20the%20AI-generated%20texts.%20However%2C%20zero-shot%20detectors%20typically%20analyze%0Athese%20texts%20in%20isolation%2C%20neglecting%20the%20impact%20of%20the%20original%20prompts.%20It%20is%0Aconceivable%20that%20this%20approach%20may%20lead%20to%20a%20discrepancy%20in%20likelihood%0Aassessments%20between%20the%20text%20generation%20phase%20and%20the%20detection%20phase.%20So%20far%2C%0Athere%20remains%20an%20unverified%20gap%20concerning%20how%20the%20presence%20or%20absence%20of%0Aprompts%20impacts%20detection%20accuracy%20for%20zero-shot%20detectors.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20evaluative%20framework%20to%20empirically%20analyze%20the%20impact%20of%20prompts%0Aon%20the%20detection%20accuracy%20of%20AI-generated%20text.%20We%20assess%20various%20zero-shot%0Adetectors%20using%20both%20white-box%20detection%2C%20which%20leverages%20the%20prompt%2C%20and%0Ablack-box%20detection%2C%20which%20operates%20without%20prompt%20information.%20Our%20experiments%0Areveal%20the%20significant%20influence%20of%20prompts%20on%20detection%20accuracy.%20Remarkably%2C%0Acompared%20with%20black-box%20detection%20without%20prompts%2C%20the%20white-box%20methods%20using%0Aprompts%20demonstrate%20an%20increase%20in%20AUC%20of%20at%20least%20%240.1%24%20across%20all%20zero-shot%0Adetectors%20tested.%20Code%20is%20available%3A%0A%5Curl%7Bhttps%3A//github.com/kaito25atugich/Detector%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20127v1&entry.124074799=Read"},
{"title": "Defending Against Weight-Poisoning Backdoor Attacks for\n  Parameter-Efficient Fine-Tuning", "author": "Shuai Zhao and Leilei Gan and Luu Anh Tuan and Jie Fu and Lingjuan Lyu and Meihuizi Jia and Jinming Wen", "abstract": "  Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.\n", "link": "http://arxiv.org/abs/2402.12168v3", "date": "2024-03-29", "relevancy": 1.8258, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4733}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4446}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4444}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Defending%20Against%20Weight-Poisoning%20Backdoor%20Attacks%20for%0A%20%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20Defending%20Against%20Weight-Poisoning%20Backdoor%20Attacks%20for%0A%20%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Shuai%20Zhao%20and%20Leilei%20Gan%20and%20Luu%20Anh%20Tuan%20and%20Jie%20Fu%20and%20Lingjuan%20Lyu%20and%20Meihuizi%20Jia%20and%20Jinming%20Wen%0AAbstract%3A%20%20%20Recently%2C%20various%20parameter-efficient%20fine-tuning%20%28PEFT%29%20strategies%20for%0Aapplication%20to%20language%20models%20have%20been%20proposed%20and%20successfully%20implemented.%0AHowever%2C%20this%20raises%20the%20question%20of%20whether%20PEFT%2C%20which%20only%20updates%20a%20limited%0Aset%20of%20model%20parameters%2C%20constitutes%20security%20vulnerabilities%20when%20confronted%0Awith%20weight-poisoning%20backdoor%20attacks.%20In%20this%20study%2C%20we%20show%20that%20PEFT%20is%0Amore%20susceptible%20to%20weight-poisoning%20backdoor%20attacks%20compared%20to%20the%0Afull-parameter%20fine-tuning%20method%2C%20with%20pre-defined%20triggers%20remaining%0Aexploitable%20and%20pre-defined%20targets%20maintaining%20high%20confidence%2C%20even%20after%0Afine-tuning.%20Motivated%20by%20this%20insight%2C%20we%20developed%20a%20Poisoned%20Sample%0AIdentification%20Module%20%28PSIM%29%20leveraging%20PEFT%2C%20which%20identifies%20poisoned%20samples%0Athrough%20confidence%2C%20providing%20robust%20defense%20against%20weight-poisoning%20backdoor%0Aattacks.%20Specifically%2C%20we%20leverage%20PEFT%20to%20train%20the%20PSIM%20with%20randomly%20reset%0Asample%20labels.%20During%20the%20inference%20process%2C%20extreme%20confidence%20serves%20as%20an%0Aindicator%20for%20poisoned%20samples%2C%20while%20others%20are%20clean.%20We%20conduct%20experiments%0Aon%20text%20classification%20tasks%2C%20five%20fine-tuning%20strategies%2C%20and%20three%0Aweight-poisoning%20backdoor%20attack%20methods.%20Experiments%20show%20near%20100%25%20success%0Arates%20for%20weight-poisoning%20backdoor%20attacks%20when%20utilizing%20PEFT.%20Furthermore%2C%0Aour%20defensive%20approach%20exhibits%20overall%20competitive%20performance%20in%20mitigating%0Aweight-poisoning%20backdoor%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12168v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Against%20Weight-Poisoning%20Backdoor%20Attacks%20for%0A%20%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Shuai%20Zhao%20and%20Leilei%20Gan%20and%20Luu%20Anh%20Tuan%20and%20Jie%20Fu%20and%20Lingjuan%20Lyu%20and%20Meihuizi%20Jia%20and%20Jinming%20Wen&entry.1292438233=%20%20Recently%2C%20various%20parameter-efficient%20fine-tuning%20%28PEFT%29%20strategies%20for%0Aapplication%20to%20language%20models%20have%20been%20proposed%20and%20successfully%20implemented.%0AHowever%2C%20this%20raises%20the%20question%20of%20whether%20PEFT%2C%20which%20only%20updates%20a%20limited%0Aset%20of%20model%20parameters%2C%20constitutes%20security%20vulnerabilities%20when%20confronted%0Awith%20weight-poisoning%20backdoor%20attacks.%20In%20this%20study%2C%20we%20show%20that%20PEFT%20is%0Amore%20susceptible%20to%20weight-poisoning%20backdoor%20attacks%20compared%20to%20the%0Afull-parameter%20fine-tuning%20method%2C%20with%20pre-defined%20triggers%20remaining%0Aexploitable%20and%20pre-defined%20targets%20maintaining%20high%20confidence%2C%20even%20after%0Afine-tuning.%20Motivated%20by%20this%20insight%2C%20we%20developed%20a%20Poisoned%20Sample%0AIdentification%20Module%20%28PSIM%29%20leveraging%20PEFT%2C%20which%20identifies%20poisoned%20samples%0Athrough%20confidence%2C%20providing%20robust%20defense%20against%20weight-poisoning%20backdoor%0Aattacks.%20Specifically%2C%20we%20leverage%20PEFT%20to%20train%20the%20PSIM%20with%20randomly%20reset%0Asample%20labels.%20During%20the%20inference%20process%2C%20extreme%20confidence%20serves%20as%20an%0Aindicator%20for%20poisoned%20samples%2C%20while%20others%20are%20clean.%20We%20conduct%20experiments%0Aon%20text%20classification%20tasks%2C%20five%20fine-tuning%20strategies%2C%20and%20three%0Aweight-poisoning%20backdoor%20attack%20methods.%20Experiments%20show%20near%20100%25%20success%0Arates%20for%20weight-poisoning%20backdoor%20attacks%20when%20utilizing%20PEFT.%20Furthermore%2C%0Aour%20defensive%20approach%20exhibits%20overall%20competitive%20performance%20in%20mitigating%0Aweight-poisoning%20backdoor%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12168v3&entry.124074799=Read"},
{"title": "FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion\n  Models", "author": "Barbara Toniella Corradini and Mustafa Shukor and Paul Couairon and Guillaume Couairon and Franco Scarselli and Matthieu Cord", "abstract": "  Foundation models have exhibited unprecedented capabilities in tackling many\ndomains and tasks. Models such as CLIP are currently widely used to bridge\ncross-modal representations, and text-to-image diffusion models are arguably\nthe leading models in terms of realistic image generation. Image generative\nmodels are trained on massive datasets that provide them with powerful internal\nspatial representations. In this work, we explore the potential benefits of\nsuch representations, beyond image generation, in particular, for dense visual\nprediction tasks. We focus on the task of image segmentation, which is\ntraditionally solved by training models on closed-vocabulary datasets, with\npixel-level annotations. To avoid the annotation cost or training large\ndiffusion models, we constraint our setup to be zero-shot and training-free. In\na nutshell, our pipeline leverages different and relatively small-sized,\nopen-source foundation models for zero-shot open-vocabulary segmentation. The\npipeline is as follows: the image is passed to both a captioner model (i.e.\nBLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text\ndescription and visual representation, respectively. The features are clustered\nand binarized to obtain class agnostic masks for each object. These masks are\nthen mapped to a textual class, using the CLIP model to support\nopen-vocabulary. Finally, we add a refinement step that allows to obtain a more\nprecise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not\nrely on any training, outperforms many training-based approaches on both Pascal\nVOC and COCO datasets. In addition, we show very competitive results compared\nto the recent weakly-supervised segmentation approaches. We provide\ncomprehensive experiments showing the superiority of diffusion model features\ncompared to other pretrained models. Project page:\nhttps://bcorrad.github.io/freesegdiff/\n", "link": "http://arxiv.org/abs/2403.20105v1", "date": "2024-03-29", "relevancy": 1.8204, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6557}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6019}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5892}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models&body=Title%3A%20FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models%0AAuthor%3A%20Barbara%20Toniella%20Corradini%20and%20Mustafa%20Shukor%20and%20Paul%20Couairon%20and%20Guillaume%20Couairon%20and%20Franco%20Scarselli%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Foundation%20models%20have%20exhibited%20unprecedented%20capabilities%20in%20tackling%20many%0Adomains%20and%20tasks.%20Models%20such%20as%20CLIP%20are%20currently%20widely%20used%20to%20bridge%0Across-modal%20representations%2C%20and%20text-to-image%20diffusion%20models%20are%20arguably%0Athe%20leading%20models%20in%20terms%20of%20realistic%20image%20generation.%20Image%20generative%0Amodels%20are%20trained%20on%20massive%20datasets%20that%20provide%20them%20with%20powerful%20internal%0Aspatial%20representations.%20In%20this%20work%2C%20we%20explore%20the%20potential%20benefits%20of%0Asuch%20representations%2C%20beyond%20image%20generation%2C%20in%20particular%2C%20for%20dense%20visual%0Aprediction%20tasks.%20We%20focus%20on%20the%20task%20of%20image%20segmentation%2C%20which%20is%0Atraditionally%20solved%20by%20training%20models%20on%20closed-vocabulary%20datasets%2C%20with%0Apixel-level%20annotations.%20To%20avoid%20the%20annotation%20cost%20or%20training%20large%0Adiffusion%20models%2C%20we%20constraint%20our%20setup%20to%20be%20zero-shot%20and%20training-free.%20In%0Aa%20nutshell%2C%20our%20pipeline%20leverages%20different%20and%20relatively%20small-sized%2C%0Aopen-source%20foundation%20models%20for%20zero-shot%20open-vocabulary%20segmentation.%20The%0Apipeline%20is%20as%20follows%3A%20the%20image%20is%20passed%20to%20both%20a%20captioner%20model%20%28i.e.%0ABLIP%29%20and%20a%20diffusion%20model%20%28i.e.%2C%20Stable%20Diffusion%20Model%29%20to%20generate%20a%20text%0Adescription%20and%20visual%20representation%2C%20respectively.%20The%20features%20are%20clustered%0Aand%20binarized%20to%20obtain%20class%20agnostic%20masks%20for%20each%20object.%20These%20masks%20are%0Athen%20mapped%20to%20a%20textual%20class%2C%20using%20the%20CLIP%20model%20to%20support%0Aopen-vocabulary.%20Finally%2C%20we%20add%20a%20refinement%20step%20that%20allows%20to%20obtain%20a%20more%0Aprecise%20segmentation%20mask.%20Our%20approach%20%28dubbed%20FreeSeg-Diff%29%2C%20which%20does%20not%0Arely%20on%20any%20training%2C%20outperforms%20many%20training-based%20approaches%20on%20both%20Pascal%0AVOC%20and%20COCO%20datasets.%20In%20addition%2C%20we%20show%20very%20competitive%20results%20compared%0Ato%20the%20recent%20weakly-supervised%20segmentation%20approaches.%20We%20provide%0Acomprehensive%20experiments%20showing%20the%20superiority%20of%20diffusion%20model%20features%0Acompared%20to%20other%20pretrained%20models.%20Project%20page%3A%0Ahttps%3A//bcorrad.github.io/freesegdiff/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20105v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models&entry.906535625=Barbara%20Toniella%20Corradini%20and%20Mustafa%20Shukor%20and%20Paul%20Couairon%20and%20Guillaume%20Couairon%20and%20Franco%20Scarselli%20and%20Matthieu%20Cord&entry.1292438233=%20%20Foundation%20models%20have%20exhibited%20unprecedented%20capabilities%20in%20tackling%20many%0Adomains%20and%20tasks.%20Models%20such%20as%20CLIP%20are%20currently%20widely%20used%20to%20bridge%0Across-modal%20representations%2C%20and%20text-to-image%20diffusion%20models%20are%20arguably%0Athe%20leading%20models%20in%20terms%20of%20realistic%20image%20generation.%20Image%20generative%0Amodels%20are%20trained%20on%20massive%20datasets%20that%20provide%20them%20with%20powerful%20internal%0Aspatial%20representations.%20In%20this%20work%2C%20we%20explore%20the%20potential%20benefits%20of%0Asuch%20representations%2C%20beyond%20image%20generation%2C%20in%20particular%2C%20for%20dense%20visual%0Aprediction%20tasks.%20We%20focus%20on%20the%20task%20of%20image%20segmentation%2C%20which%20is%0Atraditionally%20solved%20by%20training%20models%20on%20closed-vocabulary%20datasets%2C%20with%0Apixel-level%20annotations.%20To%20avoid%20the%20annotation%20cost%20or%20training%20large%0Adiffusion%20models%2C%20we%20constraint%20our%20setup%20to%20be%20zero-shot%20and%20training-free.%20In%0Aa%20nutshell%2C%20our%20pipeline%20leverages%20different%20and%20relatively%20small-sized%2C%0Aopen-source%20foundation%20models%20for%20zero-shot%20open-vocabulary%20segmentation.%20The%0Apipeline%20is%20as%20follows%3A%20the%20image%20is%20passed%20to%20both%20a%20captioner%20model%20%28i.e.%0ABLIP%29%20and%20a%20diffusion%20model%20%28i.e.%2C%20Stable%20Diffusion%20Model%29%20to%20generate%20a%20text%0Adescription%20and%20visual%20representation%2C%20respectively.%20The%20features%20are%20clustered%0Aand%20binarized%20to%20obtain%20class%20agnostic%20masks%20for%20each%20object.%20These%20masks%20are%0Athen%20mapped%20to%20a%20textual%20class%2C%20using%20the%20CLIP%20model%20to%20support%0Aopen-vocabulary.%20Finally%2C%20we%20add%20a%20refinement%20step%20that%20allows%20to%20obtain%20a%20more%0Aprecise%20segmentation%20mask.%20Our%20approach%20%28dubbed%20FreeSeg-Diff%29%2C%20which%20does%20not%0Arely%20on%20any%20training%2C%20outperforms%20many%20training-based%20approaches%20on%20both%20Pascal%0AVOC%20and%20COCO%20datasets.%20In%20addition%2C%20we%20show%20very%20competitive%20results%20compared%0Ato%20the%20recent%20weakly-supervised%20segmentation%20approaches.%20We%20provide%0Acomprehensive%20experiments%20showing%20the%20superiority%20of%20diffusion%20model%20features%0Acompared%20to%20other%20pretrained%20models.%20Project%20page%3A%0Ahttps%3A//bcorrad.github.io/freesegdiff/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20105v1&entry.124074799=Read"},
{"title": "U-VAP: User-specified Visual Appearance Personalization via Decoupled\n  Self Augmentation", "author": "You Wu and Kean Liu and Xiaoyue Mi and Fan Tang and Juan Cao and Jintao Li", "abstract": "  Concept personalization methods enable large text-to-image models to learn\nspecific subjects (e.g., objects/poses/3D models) and synthesize renditions in\nnew contexts. Given that the image references are highly biased towards visual\nattributes, state-of-the-art personalization models tend to overfit the whole\nsubject and cannot disentangle visual characteristics in pixel space. In this\nstudy, we proposed a more challenging setting, namely fine-grained visual\nappearance personalization. Different from existing methods, we allow users to\nprovide a sentence describing the desired attributes. A novel decoupled\nself-augmentation strategy is proposed to generate target-related and\nnon-target samples to learn user-specified visual attributes. These augmented\ndata allow for refining the model's understanding of the target attribute while\nmitigating the impact of unrelated attributes. At the inference stage,\nadjustments are conducted on semantic space through the learned target and\nnon-target embeddings to further enhance the disentanglement of target\nattributes. Extensive experiments on various kinds of visual attributes with\nSOTA personalization methods show the ability of the proposed method to mimic\ntarget visual appearance in novel contexts, thus improving the controllability\nand flexibility of personalization.\n", "link": "http://arxiv.org/abs/2403.20231v1", "date": "2024-03-29", "relevancy": 1.8137, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6421}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5813}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20U-VAP%3A%20User-specified%20Visual%20Appearance%20Personalization%20via%20Decoupled%0A%20%20Self%20Augmentation&body=Title%3A%20U-VAP%3A%20User-specified%20Visual%20Appearance%20Personalization%20via%20Decoupled%0A%20%20Self%20Augmentation%0AAuthor%3A%20You%20Wu%20and%20Kean%20Liu%20and%20Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Jintao%20Li%0AAbstract%3A%20%20%20Concept%20personalization%20methods%20enable%20large%20text-to-image%20models%20to%20learn%0Aspecific%20subjects%20%28e.g.%2C%20objects/poses/3D%20models%29%20and%20synthesize%20renditions%20in%0Anew%20contexts.%20Given%20that%20the%20image%20references%20are%20highly%20biased%20towards%20visual%0Aattributes%2C%20state-of-the-art%20personalization%20models%20tend%20to%20overfit%20the%20whole%0Asubject%20and%20cannot%20disentangle%20visual%20characteristics%20in%20pixel%20space.%20In%20this%0Astudy%2C%20we%20proposed%20a%20more%20challenging%20setting%2C%20namely%20fine-grained%20visual%0Aappearance%20personalization.%20Different%20from%20existing%20methods%2C%20we%20allow%20users%20to%0Aprovide%20a%20sentence%20describing%20the%20desired%20attributes.%20A%20novel%20decoupled%0Aself-augmentation%20strategy%20is%20proposed%20to%20generate%20target-related%20and%0Anon-target%20samples%20to%20learn%20user-specified%20visual%20attributes.%20These%20augmented%0Adata%20allow%20for%20refining%20the%20model%27s%20understanding%20of%20the%20target%20attribute%20while%0Amitigating%20the%20impact%20of%20unrelated%20attributes.%20At%20the%20inference%20stage%2C%0Aadjustments%20are%20conducted%20on%20semantic%20space%20through%20the%20learned%20target%20and%0Anon-target%20embeddings%20to%20further%20enhance%20the%20disentanglement%20of%20target%0Aattributes.%20Extensive%20experiments%20on%20various%20kinds%20of%20visual%20attributes%20with%0ASOTA%20personalization%20methods%20show%20the%20ability%20of%20the%20proposed%20method%20to%20mimic%0Atarget%20visual%20appearance%20in%20novel%20contexts%2C%20thus%20improving%20the%20controllability%0Aand%20flexibility%20of%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20231v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-VAP%3A%20User-specified%20Visual%20Appearance%20Personalization%20via%20Decoupled%0A%20%20Self%20Augmentation&entry.906535625=You%20Wu%20and%20Kean%20Liu%20and%20Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Jintao%20Li&entry.1292438233=%20%20Concept%20personalization%20methods%20enable%20large%20text-to-image%20models%20to%20learn%0Aspecific%20subjects%20%28e.g.%2C%20objects/poses/3D%20models%29%20and%20synthesize%20renditions%20in%0Anew%20contexts.%20Given%20that%20the%20image%20references%20are%20highly%20biased%20towards%20visual%0Aattributes%2C%20state-of-the-art%20personalization%20models%20tend%20to%20overfit%20the%20whole%0Asubject%20and%20cannot%20disentangle%20visual%20characteristics%20in%20pixel%20space.%20In%20this%0Astudy%2C%20we%20proposed%20a%20more%20challenging%20setting%2C%20namely%20fine-grained%20visual%0Aappearance%20personalization.%20Different%20from%20existing%20methods%2C%20we%20allow%20users%20to%0Aprovide%20a%20sentence%20describing%20the%20desired%20attributes.%20A%20novel%20decoupled%0Aself-augmentation%20strategy%20is%20proposed%20to%20generate%20target-related%20and%0Anon-target%20samples%20to%20learn%20user-specified%20visual%20attributes.%20These%20augmented%0Adata%20allow%20for%20refining%20the%20model%27s%20understanding%20of%20the%20target%20attribute%20while%0Amitigating%20the%20impact%20of%20unrelated%20attributes.%20At%20the%20inference%20stage%2C%0Aadjustments%20are%20conducted%20on%20semantic%20space%20through%20the%20learned%20target%20and%0Anon-target%20embeddings%20to%20further%20enhance%20the%20disentanglement%20of%20target%0Aattributes.%20Extensive%20experiments%20on%20various%20kinds%20of%20visual%20attributes%20with%0ASOTA%20personalization%20methods%20show%20the%20ability%20of%20the%20proposed%20method%20to%20mimic%0Atarget%20visual%20appearance%20in%20novel%20contexts%2C%20thus%20improving%20the%20controllability%0Aand%20flexibility%20of%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20231v1&entry.124074799=Read"},
{"title": "You Only Sample Once: Taming One-Step Text-To-Image Synthesis by\n  Self-Cooperative Diffusion GANs", "author": "Yihong Luo and Xiaolong Chen and Jing Tang", "abstract": "  We introduce YOSO, a novel generative model designed for rapid, scalable, and\nhigh-fidelity one-step image synthesis. This is achieved by integrating the\ndiffusion process with GANs. Specifically, we smooth the distribution by the\ndenoising generator itself, performing self-cooperative learning. We show that\nour method can serve as a one-step generation model training from scratch with\ncompetitive performance. Moreover, we show that our method can be extended to\nfinetune pre-trained text-to-image diffusion for high-quality one-step\ntext-to-image synthesis even with LoRA fine-tuning. In particular, we provide\nthe first diffusion transformer that can generate images in one step trained on\n512 resolution, with the capability of adapting to 1024 resolution without\nexplicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.\n", "link": "http://arxiv.org/abs/2403.12931v2", "date": "2024-03-29", "relevancy": 1.8126, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6095}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6034}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5918}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Sample%20Once%3A%20Taming%20One-Step%20Text-To-Image%20Synthesis%20by%0A%20%20Self-Cooperative%20Diffusion%20GANs&body=Title%3A%20You%20Only%20Sample%20Once%3A%20Taming%20One-Step%20Text-To-Image%20Synthesis%20by%0A%20%20Self-Cooperative%20Diffusion%20GANs%0AAuthor%3A%20Yihong%20Luo%20and%20Xiaolong%20Chen%20and%20Jing%20Tang%0AAbstract%3A%20%20%20We%20introduce%20YOSO%2C%20a%20novel%20generative%20model%20designed%20for%20rapid%2C%20scalable%2C%20and%0Ahigh-fidelity%20one-step%20image%20synthesis.%20This%20is%20achieved%20by%20integrating%20the%0Adiffusion%20process%20with%20GANs.%20Specifically%2C%20we%20smooth%20the%20distribution%20by%20the%0Adenoising%20generator%20itself%2C%20performing%20self-cooperative%20learning.%20We%20show%20that%0Aour%20method%20can%20serve%20as%20a%20one-step%20generation%20model%20training%20from%20scratch%20with%0Acompetitive%20performance.%20Moreover%2C%20we%20show%20that%20our%20method%20can%20be%20extended%20to%0Afinetune%20pre-trained%20text-to-image%20diffusion%20for%20high-quality%20one-step%0Atext-to-image%20synthesis%20even%20with%20LoRA%20fine-tuning.%20In%20particular%2C%20we%20provide%0Athe%20first%20diffusion%20transformer%20that%20can%20generate%20images%20in%20one%20step%20trained%20on%0A512%20resolution%2C%20with%20the%20capability%20of%20adapting%20to%201024%20resolution%20without%0Aexplicit%20training.%20Our%20code%20is%20provided%20at%20https%3A//github.com/Luo-Yihong/YOSO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12931v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Sample%20Once%3A%20Taming%20One-Step%20Text-To-Image%20Synthesis%20by%0A%20%20Self-Cooperative%20Diffusion%20GANs&entry.906535625=Yihong%20Luo%20and%20Xiaolong%20Chen%20and%20Jing%20Tang&entry.1292438233=%20%20We%20introduce%20YOSO%2C%20a%20novel%20generative%20model%20designed%20for%20rapid%2C%20scalable%2C%20and%0Ahigh-fidelity%20one-step%20image%20synthesis.%20This%20is%20achieved%20by%20integrating%20the%0Adiffusion%20process%20with%20GANs.%20Specifically%2C%20we%20smooth%20the%20distribution%20by%20the%0Adenoising%20generator%20itself%2C%20performing%20self-cooperative%20learning.%20We%20show%20that%0Aour%20method%20can%20serve%20as%20a%20one-step%20generation%20model%20training%20from%20scratch%20with%0Acompetitive%20performance.%20Moreover%2C%20we%20show%20that%20our%20method%20can%20be%20extended%20to%0Afinetune%20pre-trained%20text-to-image%20diffusion%20for%20high-quality%20one-step%0Atext-to-image%20synthesis%20even%20with%20LoRA%20fine-tuning.%20In%20particular%2C%20we%20provide%0Athe%20first%20diffusion%20transformer%20that%20can%20generate%20images%20in%20one%20step%20trained%20on%0A512%20resolution%2C%20with%20the%20capability%20of%20adapting%20to%201024%20resolution%20without%0Aexplicit%20training.%20Our%20code%20is%20provided%20at%20https%3A//github.com/Luo-Yihong/YOSO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12931v2&entry.124074799=Read"},
{"title": "Autonomous Field-of-View Adjustment Using Adaptive Kinematic Constrained\n  Control with Robot-Held Microscopic Camera Feedback", "author": "Hung-Ching Lin and Murilo Marques Marinho and Kanako Harada", "abstract": "  Robotic systems for manipulation in millimeter scale often use a camera with\nhigh magnification for visual feedback of the target region. However, the\nlimited field-of-view (FoV) of the microscopic camera necessitates camera\nmotion to capture a broader workspace environment. In this work, we propose an\nautonomous robotic control method to constrain a robot-held camera within a\ndesignated FoV. Furthermore, we model the camera extrinsics as part of the\nkinematic model and use camera measurements coupled with a U-Net based tool\ntracking to adapt the complete robotic model during task execution. As a\nproof-of-concept demonstration, the proposed framework was evaluated in a\nbi-manual setup, where the microscopic camera was controlled to view a tool\nmoving in a pre-defined trajectory. The proposed method allowed the camera to\nstay 94.1% of the time within the real FoV, compared to 54.4% without the\nproposed adaptive control.\n", "link": "http://arxiv.org/abs/2309.10287v2", "date": "2024-03-29", "relevancy": 1.8054, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5658}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Field-of-View%20Adjustment%20Using%20Adaptive%20Kinematic%20Constrained%0A%20%20Control%20with%20Robot-Held%20Microscopic%20Camera%20Feedback&body=Title%3A%20Autonomous%20Field-of-View%20Adjustment%20Using%20Adaptive%20Kinematic%20Constrained%0A%20%20Control%20with%20Robot-Held%20Microscopic%20Camera%20Feedback%0AAuthor%3A%20Hung-Ching%20Lin%20and%20Murilo%20Marques%20Marinho%20and%20Kanako%20Harada%0AAbstract%3A%20%20%20Robotic%20systems%20for%20manipulation%20in%20millimeter%20scale%20often%20use%20a%20camera%20with%0Ahigh%20magnification%20for%20visual%20feedback%20of%20the%20target%20region.%20However%2C%20the%0Alimited%20field-of-view%20%28FoV%29%20of%20the%20microscopic%20camera%20necessitates%20camera%0Amotion%20to%20capture%20a%20broader%20workspace%20environment.%20In%20this%20work%2C%20we%20propose%20an%0Aautonomous%20robotic%20control%20method%20to%20constrain%20a%20robot-held%20camera%20within%20a%0Adesignated%20FoV.%20Furthermore%2C%20we%20model%20the%20camera%20extrinsics%20as%20part%20of%20the%0Akinematic%20model%20and%20use%20camera%20measurements%20coupled%20with%20a%20U-Net%20based%20tool%0Atracking%20to%20adapt%20the%20complete%20robotic%20model%20during%20task%20execution.%20As%20a%0Aproof-of-concept%20demonstration%2C%20the%20proposed%20framework%20was%20evaluated%20in%20a%0Abi-manual%20setup%2C%20where%20the%20microscopic%20camera%20was%20controlled%20to%20view%20a%20tool%0Amoving%20in%20a%20pre-defined%20trajectory.%20The%20proposed%20method%20allowed%20the%20camera%20to%0Astay%2094.1%25%20of%20the%20time%20within%20the%20real%20FoV%2C%20compared%20to%2054.4%25%20without%20the%0Aproposed%20adaptive%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10287v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Field-of-View%20Adjustment%20Using%20Adaptive%20Kinematic%20Constrained%0A%20%20Control%20with%20Robot-Held%20Microscopic%20Camera%20Feedback&entry.906535625=Hung-Ching%20Lin%20and%20Murilo%20Marques%20Marinho%20and%20Kanako%20Harada&entry.1292438233=%20%20Robotic%20systems%20for%20manipulation%20in%20millimeter%20scale%20often%20use%20a%20camera%20with%0Ahigh%20magnification%20for%20visual%20feedback%20of%20the%20target%20region.%20However%2C%20the%0Alimited%20field-of-view%20%28FoV%29%20of%20the%20microscopic%20camera%20necessitates%20camera%0Amotion%20to%20capture%20a%20broader%20workspace%20environment.%20In%20this%20work%2C%20we%20propose%20an%0Aautonomous%20robotic%20control%20method%20to%20constrain%20a%20robot-held%20camera%20within%20a%0Adesignated%20FoV.%20Furthermore%2C%20we%20model%20the%20camera%20extrinsics%20as%20part%20of%20the%0Akinematic%20model%20and%20use%20camera%20measurements%20coupled%20with%20a%20U-Net%20based%20tool%0Atracking%20to%20adapt%20the%20complete%20robotic%20model%20during%20task%20execution.%20As%20a%0Aproof-of-concept%20demonstration%2C%20the%20proposed%20framework%20was%20evaluated%20in%20a%0Abi-manual%20setup%2C%20where%20the%20microscopic%20camera%20was%20controlled%20to%20view%20a%20tool%0Amoving%20in%20a%20pre-defined%20trajectory.%20The%20proposed%20method%20allowed%20the%20camera%20to%0Astay%2094.1%25%20of%20the%20time%20within%20the%20real%20FoV%2C%20compared%20to%2054.4%25%20without%20the%0Aproposed%20adaptive%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10287v2&entry.124074799=Read"},
{"title": "Algorithms for Non-Negative Matrix Factorization on Noisy Data With\n  Negative Values", "author": "Dylan Green and Stephen Bailey", "abstract": "  Non-negative matrix factorization (NMF) is a dimensionality reduction\ntechnique that has shown promise for analyzing noisy data, especially\nastronomical data. For these datasets, the observed data may contain negative\nvalues due to noise even when the true underlying physical signal is strictly\npositive. Prior NMF work has not treated negative data in a statistically\nconsistent manner, which becomes problematic for low signal-to-noise data with\nmany negative values. In this paper we present two algorithms, Shift-NMF and\nNearly-NMF, that can handle both the noisiness of the input data and also any\nintroduced negativity. Both of these algorithms use the negative data space\nwithout clipping, and correctly recover non-negative signals without any\nintroduced positive offset that occurs when clipping negative data. We\ndemonstrate this numerically on both simple and more realistic examples, and\nprove that both algorithms have monotonically decreasing update rules.\n", "link": "http://arxiv.org/abs/2311.04855v2", "date": "2024-03-29", "relevancy": 1.8, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5012}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4395}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.403}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Algorithms%20for%20Non-Negative%20Matrix%20Factorization%20on%20Noisy%20Data%20With%0A%20%20Negative%20Values&body=Title%3A%20Algorithms%20for%20Non-Negative%20Matrix%20Factorization%20on%20Noisy%20Data%20With%0A%20%20Negative%20Values%0AAuthor%3A%20Dylan%20Green%20and%20Stephen%20Bailey%0AAbstract%3A%20%20%20Non-negative%20matrix%20factorization%20%28NMF%29%20is%20a%20dimensionality%20reduction%0Atechnique%20that%20has%20shown%20promise%20for%20analyzing%20noisy%20data%2C%20especially%0Aastronomical%20data.%20For%20these%20datasets%2C%20the%20observed%20data%20may%20contain%20negative%0Avalues%20due%20to%20noise%20even%20when%20the%20true%20underlying%20physical%20signal%20is%20strictly%0Apositive.%20Prior%20NMF%20work%20has%20not%20treated%20negative%20data%20in%20a%20statistically%0Aconsistent%20manner%2C%20which%20becomes%20problematic%20for%20low%20signal-to-noise%20data%20with%0Amany%20negative%20values.%20In%20this%20paper%20we%20present%20two%20algorithms%2C%20Shift-NMF%20and%0ANearly-NMF%2C%20that%20can%20handle%20both%20the%20noisiness%20of%20the%20input%20data%20and%20also%20any%0Aintroduced%20negativity.%20Both%20of%20these%20algorithms%20use%20the%20negative%20data%20space%0Awithout%20clipping%2C%20and%20correctly%20recover%20non-negative%20signals%20without%20any%0Aintroduced%20positive%20offset%20that%20occurs%20when%20clipping%20negative%20data.%20We%0Ademonstrate%20this%20numerically%20on%20both%20simple%20and%20more%20realistic%20examples%2C%20and%0Aprove%20that%20both%20algorithms%20have%20monotonically%20decreasing%20update%20rules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04855v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithms%20for%20Non-Negative%20Matrix%20Factorization%20on%20Noisy%20Data%20With%0A%20%20Negative%20Values&entry.906535625=Dylan%20Green%20and%20Stephen%20Bailey&entry.1292438233=%20%20Non-negative%20matrix%20factorization%20%28NMF%29%20is%20a%20dimensionality%20reduction%0Atechnique%20that%20has%20shown%20promise%20for%20analyzing%20noisy%20data%2C%20especially%0Aastronomical%20data.%20For%20these%20datasets%2C%20the%20observed%20data%20may%20contain%20negative%0Avalues%20due%20to%20noise%20even%20when%20the%20true%20underlying%20physical%20signal%20is%20strictly%0Apositive.%20Prior%20NMF%20work%20has%20not%20treated%20negative%20data%20in%20a%20statistically%0Aconsistent%20manner%2C%20which%20becomes%20problematic%20for%20low%20signal-to-noise%20data%20with%0Amany%20negative%20values.%20In%20this%20paper%20we%20present%20two%20algorithms%2C%20Shift-NMF%20and%0ANearly-NMF%2C%20that%20can%20handle%20both%20the%20noisiness%20of%20the%20input%20data%20and%20also%20any%0Aintroduced%20negativity.%20Both%20of%20these%20algorithms%20use%20the%20negative%20data%20space%0Awithout%20clipping%2C%20and%20correctly%20recover%20non-negative%20signals%20without%20any%0Aintroduced%20positive%20offset%20that%20occurs%20when%20clipping%20negative%20data.%20We%0Ademonstrate%20this%20numerically%20on%20both%20simple%20and%20more%20realistic%20examples%2C%20and%0Aprove%20that%20both%20algorithms%20have%20monotonically%20decreasing%20update%20rules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04855v2&entry.124074799=Read"},
{"title": "Functional Bilevel Optimization for Machine Learning", "author": "Ieva Petrulionyte and Julien Mairal and Michael Arbel", "abstract": "  In this paper, we introduce a new functional point of view on bilevel\noptimization problems for machine learning, where the inner objective is\nminimized over a function space. These types of problems are most often solved\nby using methods developed in the parametric setting, where the inner objective\nis strongly convex with respect to the parameters of the prediction function.\nThe functional point of view does not rely on this assumption and notably\nallows using over-parameterized neural networks as the inner prediction\nfunction. We propose scalable and efficient algorithms for the functional\nbilevel optimization problem and illustrate the benefits of our approach on\ninstrumental regression and reinforcement learning tasks, which admit natural\nfunctional bilevel structures.\n", "link": "http://arxiv.org/abs/2403.20233v1", "date": "2024-03-29", "relevancy": 1.7994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4688}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4168}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Functional%20Bilevel%20Optimization%20for%20Machine%20Learning&body=Title%3A%20Functional%20Bilevel%20Optimization%20for%20Machine%20Learning%0AAuthor%3A%20Ieva%20Petrulionyte%20and%20Julien%20Mairal%20and%20Michael%20Arbel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20functional%20point%20of%20view%20on%20bilevel%0Aoptimization%20problems%20for%20machine%20learning%2C%20where%20the%20inner%20objective%20is%0Aminimized%20over%20a%20function%20space.%20These%20types%20of%20problems%20are%20most%20often%20solved%0Aby%20using%20methods%20developed%20in%20the%20parametric%20setting%2C%20where%20the%20inner%20objective%0Ais%20strongly%20convex%20with%20respect%20to%20the%20parameters%20of%20the%20prediction%20function.%0AThe%20functional%20point%20of%20view%20does%20not%20rely%20on%20this%20assumption%20and%20notably%0Aallows%20using%20over-parameterized%20neural%20networks%20as%20the%20inner%20prediction%0Afunction.%20We%20propose%20scalable%20and%20efficient%20algorithms%20for%20the%20functional%0Abilevel%20optimization%20problem%20and%20illustrate%20the%20benefits%20of%20our%20approach%20on%0Ainstrumental%20regression%20and%20reinforcement%20learning%20tasks%2C%20which%20admit%20natural%0Afunctional%20bilevel%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20233v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Bilevel%20Optimization%20for%20Machine%20Learning&entry.906535625=Ieva%20Petrulionyte%20and%20Julien%20Mairal%20and%20Michael%20Arbel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20functional%20point%20of%20view%20on%20bilevel%0Aoptimization%20problems%20for%20machine%20learning%2C%20where%20the%20inner%20objective%20is%0Aminimized%20over%20a%20function%20space.%20These%20types%20of%20problems%20are%20most%20often%20solved%0Aby%20using%20methods%20developed%20in%20the%20parametric%20setting%2C%20where%20the%20inner%20objective%0Ais%20strongly%20convex%20with%20respect%20to%20the%20parameters%20of%20the%20prediction%20function.%0AThe%20functional%20point%20of%20view%20does%20not%20rely%20on%20this%20assumption%20and%20notably%0Aallows%20using%20over-parameterized%20neural%20networks%20as%20the%20inner%20prediction%0Afunction.%20We%20propose%20scalable%20and%20efficient%20algorithms%20for%20the%20functional%0Abilevel%20optimization%20problem%20and%20illustrate%20the%20benefits%20of%20our%20approach%20on%0Ainstrumental%20regression%20and%20reinforcement%20learning%20tasks%2C%20which%20admit%20natural%0Afunctional%20bilevel%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20233v1&entry.124074799=Read"},
{"title": "LayerNorm: A key component in parameter-efficient fine-tuning", "author": "Taha ValizadehAslani and Hualou Liang", "abstract": "  Fine-tuning a pre-trained model, such as Bidirectional Encoder\nRepresentations from Transformers (BERT), has been proven to be an effective\nmethod for solving many natural language processing (NLP) tasks. However, due\nto the large number of parameters in many state-of-the-art NLP models,\nincluding BERT, the process of fine-tuning is computationally expensive. One\nattractive solution to this issue is parameter-efficient fine-tuning, which\ninvolves modifying only a minimal segment of the model while keeping the\nremainder unchanged. Yet, it remains unclear which segment of the BERT model is\ncrucial for fine-tuning. In this paper, we first analyze different components\nin the BERT model to pinpoint which one undergoes the most significant changes\nafter fine-tuning. We find that output LayerNorm changes more than any other\ncomponents when fine-tuned for different General Language Understanding\nEvaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can\nreach comparable, or in some cases better, performance to full fine-tuning and\nother parameter-efficient fine-tuning methods. Moreover, we use Fisher\ninformation to determine the most critical subset of LayerNorm and demonstrate\nthat many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a\nsmall portion of LayerNorm with negligible performance degradation.\n", "link": "http://arxiv.org/abs/2403.20284v1", "date": "2024-03-29", "relevancy": 1.7992, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4718}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4498}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.441}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LayerNorm%3A%20A%20key%20component%20in%20parameter-efficient%20fine-tuning&body=Title%3A%20LayerNorm%3A%20A%20key%20component%20in%20parameter-efficient%20fine-tuning%0AAuthor%3A%20Taha%20ValizadehAslani%20and%20Hualou%20Liang%0AAbstract%3A%20%20%20Fine-tuning%20a%20pre-trained%20model%2C%20such%20as%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%2C%20has%20been%20proven%20to%20be%20an%20effective%0Amethod%20for%20solving%20many%20natural%20language%20processing%20%28NLP%29%20tasks.%20However%2C%20due%0Ato%20the%20large%20number%20of%20parameters%20in%20many%20state-of-the-art%20NLP%20models%2C%0Aincluding%20BERT%2C%20the%20process%20of%20fine-tuning%20is%20computationally%20expensive.%20One%0Aattractive%20solution%20to%20this%20issue%20is%20parameter-efficient%20fine-tuning%2C%20which%0Ainvolves%20modifying%20only%20a%20minimal%20segment%20of%20the%20model%20while%20keeping%20the%0Aremainder%20unchanged.%20Yet%2C%20it%20remains%20unclear%20which%20segment%20of%20the%20BERT%20model%20is%0Acrucial%20for%20fine-tuning.%20In%20this%20paper%2C%20we%20first%20analyze%20different%20components%0Ain%20the%20BERT%20model%20to%20pinpoint%20which%20one%20undergoes%20the%20most%20significant%20changes%0Aafter%20fine-tuning.%20We%20find%20that%20output%20LayerNorm%20changes%20more%20than%20any%20other%0Acomponents%20when%20fine-tuned%20for%20different%20General%20Language%20Understanding%0AEvaluation%20%28GLUE%29%20tasks.%20Then%20we%20show%20that%20only%20fine-tuning%20the%20LayerNorm%20can%0Areach%20comparable%2C%20or%20in%20some%20cases%20better%2C%20performance%20to%20full%20fine-tuning%20and%0Aother%20parameter-efficient%20fine-tuning%20methods.%20Moreover%2C%20we%20use%20Fisher%0Ainformation%20to%20determine%20the%20most%20critical%20subset%20of%20LayerNorm%20and%20demonstrate%0Athat%20many%20NLP%20tasks%20in%20the%20GLUE%20benchmark%20can%20be%20solved%20by%20fine-tuning%20only%20a%0Asmall%20portion%20of%20LayerNorm%20with%20negligible%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20284v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerNorm%3A%20A%20key%20component%20in%20parameter-efficient%20fine-tuning&entry.906535625=Taha%20ValizadehAslani%20and%20Hualou%20Liang&entry.1292438233=%20%20Fine-tuning%20a%20pre-trained%20model%2C%20such%20as%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%2C%20has%20been%20proven%20to%20be%20an%20effective%0Amethod%20for%20solving%20many%20natural%20language%20processing%20%28NLP%29%20tasks.%20However%2C%20due%0Ato%20the%20large%20number%20of%20parameters%20in%20many%20state-of-the-art%20NLP%20models%2C%0Aincluding%20BERT%2C%20the%20process%20of%20fine-tuning%20is%20computationally%20expensive.%20One%0Aattractive%20solution%20to%20this%20issue%20is%20parameter-efficient%20fine-tuning%2C%20which%0Ainvolves%20modifying%20only%20a%20minimal%20segment%20of%20the%20model%20while%20keeping%20the%0Aremainder%20unchanged.%20Yet%2C%20it%20remains%20unclear%20which%20segment%20of%20the%20BERT%20model%20is%0Acrucial%20for%20fine-tuning.%20In%20this%20paper%2C%20we%20first%20analyze%20different%20components%0Ain%20the%20BERT%20model%20to%20pinpoint%20which%20one%20undergoes%20the%20most%20significant%20changes%0Aafter%20fine-tuning.%20We%20find%20that%20output%20LayerNorm%20changes%20more%20than%20any%20other%0Acomponents%20when%20fine-tuned%20for%20different%20General%20Language%20Understanding%0AEvaluation%20%28GLUE%29%20tasks.%20Then%20we%20show%20that%20only%20fine-tuning%20the%20LayerNorm%20can%0Areach%20comparable%2C%20or%20in%20some%20cases%20better%2C%20performance%20to%20full%20fine-tuning%20and%0Aother%20parameter-efficient%20fine-tuning%20methods.%20Moreover%2C%20we%20use%20Fisher%0Ainformation%20to%20determine%20the%20most%20critical%20subset%20of%20LayerNorm%20and%20demonstrate%0Athat%20many%20NLP%20tasks%20in%20the%20GLUE%20benchmark%20can%20be%20solved%20by%20fine-tuning%20only%20a%0Asmall%20portion%20of%20LayerNorm%20with%20negligible%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20284v1&entry.124074799=Read"},
{"title": "Rapid Motor Adaptation for Robotic Manipulator Arms", "author": "Yichao Liang and Kevin Ellis and Jo\u00e3o Henriques", "abstract": "  Developing generalizable manipulation skills is a core challenge in embodied\nAI. This includes generalization across diverse task configurations,\nencompassing variations in object shape, density, friction coefficient, and\nexternal disturbances such as forces applied to the robot. Rapid Motor\nAdaptation (RMA) offers a promising solution to this challenge. It posits that\nessential hidden variables influencing an agent's task performance, such as\nobject mass and shape, can be effectively inferred from the agent's action and\nproprioceptive history. Drawing inspiration from RMA in locomotion and in-hand\nrotation, we use depth perception to develop agents tailored for rapid motor\nadaptation in a variety of manipulation tasks. We evaluated our agents on four\nchallenging tasks from the Maniskill2 benchmark, namely pick-and-place\noperations with hundreds of objects from the YCB and EGAD datasets, peg\ninsertion with precise position and orientation, and operating a variety of\nfaucets and handles, with customized environment variations. Empirical results\ndemonstrate that our agents surpass state-of-the-art methods like automatic\ndomain randomization and vision-based policies, obtaining better generalization\nperformance and sample efficiency.\n", "link": "http://arxiv.org/abs/2312.04670v2", "date": "2024-03-29", "relevancy": 1.7959, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6104}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5656}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rapid%20Motor%20Adaptation%20for%20Robotic%20Manipulator%20Arms&body=Title%3A%20Rapid%20Motor%20Adaptation%20for%20Robotic%20Manipulator%20Arms%0AAuthor%3A%20Yichao%20Liang%20and%20Kevin%20Ellis%20and%20Jo%C3%A3o%20Henriques%0AAbstract%3A%20%20%20Developing%20generalizable%20manipulation%20skills%20is%20a%20core%20challenge%20in%20embodied%0AAI.%20This%20includes%20generalization%20across%20diverse%20task%20configurations%2C%0Aencompassing%20variations%20in%20object%20shape%2C%20density%2C%20friction%20coefficient%2C%20and%0Aexternal%20disturbances%20such%20as%20forces%20applied%20to%20the%20robot.%20Rapid%20Motor%0AAdaptation%20%28RMA%29%20offers%20a%20promising%20solution%20to%20this%20challenge.%20It%20posits%20that%0Aessential%20hidden%20variables%20influencing%20an%20agent%27s%20task%20performance%2C%20such%20as%0Aobject%20mass%20and%20shape%2C%20can%20be%20effectively%20inferred%20from%20the%20agent%27s%20action%20and%0Aproprioceptive%20history.%20Drawing%20inspiration%20from%20RMA%20in%20locomotion%20and%20in-hand%0Arotation%2C%20we%20use%20depth%20perception%20to%20develop%20agents%20tailored%20for%20rapid%20motor%0Aadaptation%20in%20a%20variety%20of%20manipulation%20tasks.%20We%20evaluated%20our%20agents%20on%20four%0Achallenging%20tasks%20from%20the%20Maniskill2%20benchmark%2C%20namely%20pick-and-place%0Aoperations%20with%20hundreds%20of%20objects%20from%20the%20YCB%20and%20EGAD%20datasets%2C%20peg%0Ainsertion%20with%20precise%20position%20and%20orientation%2C%20and%20operating%20a%20variety%20of%0Afaucets%20and%20handles%2C%20with%20customized%20environment%20variations.%20Empirical%20results%0Ademonstrate%20that%20our%20agents%20surpass%20state-of-the-art%20methods%20like%20automatic%0Adomain%20randomization%20and%20vision-based%20policies%2C%20obtaining%20better%20generalization%0Aperformance%20and%20sample%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04670v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Motor%20Adaptation%20for%20Robotic%20Manipulator%20Arms&entry.906535625=Yichao%20Liang%20and%20Kevin%20Ellis%20and%20Jo%C3%A3o%20Henriques&entry.1292438233=%20%20Developing%20generalizable%20manipulation%20skills%20is%20a%20core%20challenge%20in%20embodied%0AAI.%20This%20includes%20generalization%20across%20diverse%20task%20configurations%2C%0Aencompassing%20variations%20in%20object%20shape%2C%20density%2C%20friction%20coefficient%2C%20and%0Aexternal%20disturbances%20such%20as%20forces%20applied%20to%20the%20robot.%20Rapid%20Motor%0AAdaptation%20%28RMA%29%20offers%20a%20promising%20solution%20to%20this%20challenge.%20It%20posits%20that%0Aessential%20hidden%20variables%20influencing%20an%20agent%27s%20task%20performance%2C%20such%20as%0Aobject%20mass%20and%20shape%2C%20can%20be%20effectively%20inferred%20from%20the%20agent%27s%20action%20and%0Aproprioceptive%20history.%20Drawing%20inspiration%20from%20RMA%20in%20locomotion%20and%20in-hand%0Arotation%2C%20we%20use%20depth%20perception%20to%20develop%20agents%20tailored%20for%20rapid%20motor%0Aadaptation%20in%20a%20variety%20of%20manipulation%20tasks.%20We%20evaluated%20our%20agents%20on%20four%0Achallenging%20tasks%20from%20the%20Maniskill2%20benchmark%2C%20namely%20pick-and-place%0Aoperations%20with%20hundreds%20of%20objects%20from%20the%20YCB%20and%20EGAD%20datasets%2C%20peg%0Ainsertion%20with%20precise%20position%20and%20orientation%2C%20and%20operating%20a%20variety%20of%0Afaucets%20and%20handles%2C%20with%20customized%20environment%20variations.%20Empirical%20results%0Ademonstrate%20that%20our%20agents%20surpass%20state-of-the-art%20methods%20like%20automatic%0Adomain%20randomization%20and%20vision-based%20policies%2C%20obtaining%20better%20generalization%0Aperformance%20and%20sample%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04670v2&entry.124074799=Read"},
{"title": "The Future of Combating Rumors? Retrieval, Discrimination, and\n  Generation", "author": "Junhao Xu and Longdi Xian and Zening Liu and Mingliang Chen and Qiuyang Yin and Fenghua Song", "abstract": "  Artificial Intelligence Generated Content (AIGC) technology development has\nfacilitated the creation of rumors with misinformation, impacting societal,\neconomic, and political ecosystems, challenging democracy. Current rumor\ndetection efforts fall short by merely labeling potentially misinformation\n(classification task), inadequately addressing the issue, and it is unrealistic\nto have authoritative institutions debunk every piece of information on social\nmedia. Our proposed comprehensive debunking process not only detects rumors but\nalso provides explanatory generated content to refute the authenticity of the\ninformation. The Expert-Citizen Collective Wisdom (ECCW) module we designed\naensures high-precision assessment of the credibility of information and the\nretrieval module is responsible for retrieving relevant knowledge from a\nReal-time updated debunking database based on information keywords. By using\nprompt engineering techniques, we feed results and knowledge into a LLM (Large\nLanguage Model), achieving satisfactory discrimination and explanatory effects\nwhile eliminating the need for fine-tuning, saving computational costs, and\ncontributing to debunking efforts.\n", "link": "http://arxiv.org/abs/2403.20204v1", "date": "2024-03-29", "relevancy": 1.7826, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.45}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4233}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Future%20of%20Combating%20Rumors%3F%20Retrieval%2C%20Discrimination%2C%20and%0A%20%20Generation&body=Title%3A%20The%20Future%20of%20Combating%20Rumors%3F%20Retrieval%2C%20Discrimination%2C%20and%0A%20%20Generation%0AAuthor%3A%20Junhao%20Xu%20and%20Longdi%20Xian%20and%20Zening%20Liu%20and%20Mingliang%20Chen%20and%20Qiuyang%20Yin%20and%20Fenghua%20Song%0AAbstract%3A%20%20%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20technology%20development%20has%0Afacilitated%20the%20creation%20of%20rumors%20with%20misinformation%2C%20impacting%20societal%2C%0Aeconomic%2C%20and%20political%20ecosystems%2C%20challenging%20democracy.%20Current%20rumor%0Adetection%20efforts%20fall%20short%20by%20merely%20labeling%20potentially%20misinformation%0A%28classification%20task%29%2C%20inadequately%20addressing%20the%20issue%2C%20and%20it%20is%20unrealistic%0Ato%20have%20authoritative%20institutions%20debunk%20every%20piece%20of%20information%20on%20social%0Amedia.%20Our%20proposed%20comprehensive%20debunking%20process%20not%20only%20detects%20rumors%20but%0Aalso%20provides%20explanatory%20generated%20content%20to%20refute%20the%20authenticity%20of%20the%0Ainformation.%20The%20Expert-Citizen%20Collective%20Wisdom%20%28ECCW%29%20module%20we%20designed%0Aaensures%20high-precision%20assessment%20of%20the%20credibility%20of%20information%20and%20the%0Aretrieval%20module%20is%20responsible%20for%20retrieving%20relevant%20knowledge%20from%20a%0AReal-time%20updated%20debunking%20database%20based%20on%20information%20keywords.%20By%20using%0Aprompt%20engineering%20techniques%2C%20we%20feed%20results%20and%20knowledge%20into%20a%20LLM%20%28Large%0ALanguage%20Model%29%2C%20achieving%20satisfactory%20discrimination%20and%20explanatory%20effects%0Awhile%20eliminating%20the%20need%20for%20fine-tuning%2C%20saving%20computational%20costs%2C%20and%0Acontributing%20to%20debunking%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20204v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Future%20of%20Combating%20Rumors%3F%20Retrieval%2C%20Discrimination%2C%20and%0A%20%20Generation&entry.906535625=Junhao%20Xu%20and%20Longdi%20Xian%20and%20Zening%20Liu%20and%20Mingliang%20Chen%20and%20Qiuyang%20Yin%20and%20Fenghua%20Song&entry.1292438233=%20%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20technology%20development%20has%0Afacilitated%20the%20creation%20of%20rumors%20with%20misinformation%2C%20impacting%20societal%2C%0Aeconomic%2C%20and%20political%20ecosystems%2C%20challenging%20democracy.%20Current%20rumor%0Adetection%20efforts%20fall%20short%20by%20merely%20labeling%20potentially%20misinformation%0A%28classification%20task%29%2C%20inadequately%20addressing%20the%20issue%2C%20and%20it%20is%20unrealistic%0Ato%20have%20authoritative%20institutions%20debunk%20every%20piece%20of%20information%20on%20social%0Amedia.%20Our%20proposed%20comprehensive%20debunking%20process%20not%20only%20detects%20rumors%20but%0Aalso%20provides%20explanatory%20generated%20content%20to%20refute%20the%20authenticity%20of%20the%0Ainformation.%20The%20Expert-Citizen%20Collective%20Wisdom%20%28ECCW%29%20module%20we%20designed%0Aaensures%20high-precision%20assessment%20of%20the%20credibility%20of%20information%20and%20the%0Aretrieval%20module%20is%20responsible%20for%20retrieving%20relevant%20knowledge%20from%20a%0AReal-time%20updated%20debunking%20database%20based%20on%20information%20keywords.%20By%20using%0Aprompt%20engineering%20techniques%2C%20we%20feed%20results%20and%20knowledge%20into%20a%20LLM%20%28Large%0ALanguage%20Model%29%2C%20achieving%20satisfactory%20discrimination%20and%20explanatory%20effects%0Awhile%20eliminating%20the%20need%20for%20fine-tuning%2C%20saving%20computational%20costs%2C%20and%0Acontributing%20to%20debunking%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20204v1&entry.124074799=Read"},
{"title": "Latxa: An Open Language Model and Evaluation Suite for Basque", "author": "Julen Etxaniz and Oscar Sainz and Naiara Perez and Itziar Aldabe and German Rigau and Eneko Agirre and Aitor Ormazabal and Mikel Artetxe and Aitor Soroa", "abstract": "  We introduce Latxa, a family of large language models for Basque ranging from\n7 to 70 billion parameters. Latxa is based on Llama 2, which we continue\npretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.\nAddressing the scarcity of high-quality benchmarks for Basque, we further\nintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising\n5,169 questions from official language proficiency exams; EusReading,\ncomprising 352 reading comprehension questions; EusTrivia, comprising 1,715\ntrivia questions from 5 knowledge areas; and EusExams, comprising 16,774\nquestions from public examinations. In our extensive evaluation, Latxa\noutperforms all previous open models we compare to by a large margin. In\naddition, it is competitive with GPT-4 Turbo in language proficiency and\nunderstanding, despite lagging behind in reading comprehension and\nknowledge-intensive tasks. Both the Latxa family of models, as well as our new\npretraining corpora and evaluation datasets, are publicly available under open\nlicenses at https://github.com/hitz-zentroa/latxa. Our suite enables\nreproducible research on methods to build LLMs for low-resource languages.\n", "link": "http://arxiv.org/abs/2403.20266v1", "date": "2024-03-29", "relevancy": 1.7757, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4403}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Latxa%3A%20An%20Open%20Language%20Model%20and%20Evaluation%20Suite%20for%20Basque&body=Title%3A%20Latxa%3A%20An%20Open%20Language%20Model%20and%20Evaluation%20Suite%20for%20Basque%0AAuthor%3A%20Julen%20Etxaniz%20and%20Oscar%20Sainz%20and%20Naiara%20Perez%20and%20Itziar%20Aldabe%20and%20German%20Rigau%20and%20Eneko%20Agirre%20and%20Aitor%20Ormazabal%20and%20Mikel%20Artetxe%20and%20Aitor%20Soroa%0AAbstract%3A%20%20%20We%20introduce%20Latxa%2C%20a%20family%20of%20large%20language%20models%20for%20Basque%20ranging%20from%0A7%20to%2070%20billion%20parameters.%20Latxa%20is%20based%20on%20Llama%202%2C%20which%20we%20continue%0Apretraining%20on%20a%20new%20Basque%20corpus%20comprising%204.3M%20documents%20and%204.2B%20tokens.%0AAddressing%20the%20scarcity%20of%20high-quality%20benchmarks%20for%20Basque%2C%20we%20further%0Aintroduce%204%20multiple%20choice%20evaluation%20datasets%3A%20EusProficiency%2C%20comprising%0A5%2C169%20questions%20from%20official%20language%20proficiency%20exams%3B%20EusReading%2C%0Acomprising%20352%20reading%20comprehension%20questions%3B%20EusTrivia%2C%20comprising%201%2C715%0Atrivia%20questions%20from%205%20knowledge%20areas%3B%20and%20EusExams%2C%20comprising%2016%2C774%0Aquestions%20from%20public%20examinations.%20In%20our%20extensive%20evaluation%2C%20Latxa%0Aoutperforms%20all%20previous%20open%20models%20we%20compare%20to%20by%20a%20large%20margin.%20In%0Aaddition%2C%20it%20is%20competitive%20with%20GPT-4%20Turbo%20in%20language%20proficiency%20and%0Aunderstanding%2C%20despite%20lagging%20behind%20in%20reading%20comprehension%20and%0Aknowledge-intensive%20tasks.%20Both%20the%20Latxa%20family%20of%20models%2C%20as%20well%20as%20our%20new%0Apretraining%20corpora%20and%20evaluation%20datasets%2C%20are%20publicly%20available%20under%20open%0Alicenses%20at%20https%3A//github.com/hitz-zentroa/latxa.%20Our%20suite%20enables%0Areproducible%20research%20on%20methods%20to%20build%20LLMs%20for%20low-resource%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20266v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latxa%3A%20An%20Open%20Language%20Model%20and%20Evaluation%20Suite%20for%20Basque&entry.906535625=Julen%20Etxaniz%20and%20Oscar%20Sainz%20and%20Naiara%20Perez%20and%20Itziar%20Aldabe%20and%20German%20Rigau%20and%20Eneko%20Agirre%20and%20Aitor%20Ormazabal%20and%20Mikel%20Artetxe%20and%20Aitor%20Soroa&entry.1292438233=%20%20We%20introduce%20Latxa%2C%20a%20family%20of%20large%20language%20models%20for%20Basque%20ranging%20from%0A7%20to%2070%20billion%20parameters.%20Latxa%20is%20based%20on%20Llama%202%2C%20which%20we%20continue%0Apretraining%20on%20a%20new%20Basque%20corpus%20comprising%204.3M%20documents%20and%204.2B%20tokens.%0AAddressing%20the%20scarcity%20of%20high-quality%20benchmarks%20for%20Basque%2C%20we%20further%0Aintroduce%204%20multiple%20choice%20evaluation%20datasets%3A%20EusProficiency%2C%20comprising%0A5%2C169%20questions%20from%20official%20language%20proficiency%20exams%3B%20EusReading%2C%0Acomprising%20352%20reading%20comprehension%20questions%3B%20EusTrivia%2C%20comprising%201%2C715%0Atrivia%20questions%20from%205%20knowledge%20areas%3B%20and%20EusExams%2C%20comprising%2016%2C774%0Aquestions%20from%20public%20examinations.%20In%20our%20extensive%20evaluation%2C%20Latxa%0Aoutperforms%20all%20previous%20open%20models%20we%20compare%20to%20by%20a%20large%20margin.%20In%0Aaddition%2C%20it%20is%20competitive%20with%20GPT-4%20Turbo%20in%20language%20proficiency%20and%0Aunderstanding%2C%20despite%20lagging%20behind%20in%20reading%20comprehension%20and%0Aknowledge-intensive%20tasks.%20Both%20the%20Latxa%20family%20of%20models%2C%20as%20well%20as%20our%20new%0Apretraining%20corpora%20and%20evaluation%20datasets%2C%20are%20publicly%20available%20under%20open%0Alicenses%20at%20https%3A//github.com/hitz-zentroa/latxa.%20Our%20suite%20enables%0Areproducible%20research%20on%20methods%20to%20build%20LLMs%20for%20low-resource%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20266v1&entry.124074799=Read"},
{"title": "Relation Rectification in Diffusion Model", "author": "Yinwei Wu and Xingyi Yang and Xinchao Wang", "abstract": "  Despite their exceptional generative abilities, large text-to-image diffusion\nmodels, much like skilled but careless artists, often struggle with accurately\ndepicting visual relationships between objects. This issue, as we uncover\nthrough careful analysis, arises from a misaligned text encoder that struggles\nto interpret specific relationships and differentiate the logical order of\nassociated objects. To resolve this, we introduce a novel task termed Relation\nRectification, aiming to refine the model to accurately represent a given\nrelationship it initially fails to generate. To address this, we propose an\ninnovative solution utilizing a Heterogeneous Graph Convolutional Network\n(HGCN). It models the directional relationships between relation terms and\ncorresponding objects within the input prompts. Specifically, we optimize the\nHGCN on a pair of prompts with identical relational words but reversed object\norders, supplemented by a few reference images. The lightweight HGCN adjusts\nthe text embeddings generated by the text encoder, ensuring the accurate\nreflection of the textual relation in the embedding space. Crucially, our\nmethod retains the parameters of the text encoder and diffusion model,\npreserving the model's robust performance on unrelated descriptions. We\nvalidated our approach on a newly curated dataset of diverse relational data,\ndemonstrating both quantitative and qualitative enhancements in generating\nimages with precise visual relations. Project page:\nhttps://wuyinwei-hah.github.io/rrnet.github.io/.\n", "link": "http://arxiv.org/abs/2403.20249v1", "date": "2024-03-29", "relevancy": 1.7652, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6592}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5738}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5541}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Relation%20Rectification%20in%20Diffusion%20Model&body=Title%3A%20Relation%20Rectification%20in%20Diffusion%20Model%0AAuthor%3A%20Yinwei%20Wu%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Despite%20their%20exceptional%20generative%20abilities%2C%20large%20text-to-image%20diffusion%0Amodels%2C%20much%20like%20skilled%20but%20careless%20artists%2C%20often%20struggle%20with%20accurately%0Adepicting%20visual%20relationships%20between%20objects.%20This%20issue%2C%20as%20we%20uncover%0Athrough%20careful%20analysis%2C%20arises%20from%20a%20misaligned%20text%20encoder%20that%20struggles%0Ato%20interpret%20specific%20relationships%20and%20differentiate%20the%20logical%20order%20of%0Aassociated%20objects.%20To%20resolve%20this%2C%20we%20introduce%20a%20novel%20task%20termed%20Relation%0ARectification%2C%20aiming%20to%20refine%20the%20model%20to%20accurately%20represent%20a%20given%0Arelationship%20it%20initially%20fails%20to%20generate.%20To%20address%20this%2C%20we%20propose%20an%0Ainnovative%20solution%20utilizing%20a%20Heterogeneous%20Graph%20Convolutional%20Network%0A%28HGCN%29.%20It%20models%20the%20directional%20relationships%20between%20relation%20terms%20and%0Acorresponding%20objects%20within%20the%20input%20prompts.%20Specifically%2C%20we%20optimize%20the%0AHGCN%20on%20a%20pair%20of%20prompts%20with%20identical%20relational%20words%20but%20reversed%20object%0Aorders%2C%20supplemented%20by%20a%20few%20reference%20images.%20The%20lightweight%20HGCN%20adjusts%0Athe%20text%20embeddings%20generated%20by%20the%20text%20encoder%2C%20ensuring%20the%20accurate%0Areflection%20of%20the%20textual%20relation%20in%20the%20embedding%20space.%20Crucially%2C%20our%0Amethod%20retains%20the%20parameters%20of%20the%20text%20encoder%20and%20diffusion%20model%2C%0Apreserving%20the%20model%27s%20robust%20performance%20on%20unrelated%20descriptions.%20We%0Avalidated%20our%20approach%20on%20a%20newly%20curated%20dataset%20of%20diverse%20relational%20data%2C%0Ademonstrating%20both%20quantitative%20and%20qualitative%20enhancements%20in%20generating%0Aimages%20with%20precise%20visual%20relations.%20Project%20page%3A%0Ahttps%3A//wuyinwei-hah.github.io/rrnet.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20249v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relation%20Rectification%20in%20Diffusion%20Model&entry.906535625=Yinwei%20Wu%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Despite%20their%20exceptional%20generative%20abilities%2C%20large%20text-to-image%20diffusion%0Amodels%2C%20much%20like%20skilled%20but%20careless%20artists%2C%20often%20struggle%20with%20accurately%0Adepicting%20visual%20relationships%20between%20objects.%20This%20issue%2C%20as%20we%20uncover%0Athrough%20careful%20analysis%2C%20arises%20from%20a%20misaligned%20text%20encoder%20that%20struggles%0Ato%20interpret%20specific%20relationships%20and%20differentiate%20the%20logical%20order%20of%0Aassociated%20objects.%20To%20resolve%20this%2C%20we%20introduce%20a%20novel%20task%20termed%20Relation%0ARectification%2C%20aiming%20to%20refine%20the%20model%20to%20accurately%20represent%20a%20given%0Arelationship%20it%20initially%20fails%20to%20generate.%20To%20address%20this%2C%20we%20propose%20an%0Ainnovative%20solution%20utilizing%20a%20Heterogeneous%20Graph%20Convolutional%20Network%0A%28HGCN%29.%20It%20models%20the%20directional%20relationships%20between%20relation%20terms%20and%0Acorresponding%20objects%20within%20the%20input%20prompts.%20Specifically%2C%20we%20optimize%20the%0AHGCN%20on%20a%20pair%20of%20prompts%20with%20identical%20relational%20words%20but%20reversed%20object%0Aorders%2C%20supplemented%20by%20a%20few%20reference%20images.%20The%20lightweight%20HGCN%20adjusts%0Athe%20text%20embeddings%20generated%20by%20the%20text%20encoder%2C%20ensuring%20the%20accurate%0Areflection%20of%20the%20textual%20relation%20in%20the%20embedding%20space.%20Crucially%2C%20our%0Amethod%20retains%20the%20parameters%20of%20the%20text%20encoder%20and%20diffusion%20model%2C%0Apreserving%20the%20model%27s%20robust%20performance%20on%20unrelated%20descriptions.%20We%0Avalidated%20our%20approach%20on%20a%20newly%20curated%20dataset%20of%20diverse%20relational%20data%2C%0Ademonstrating%20both%20quantitative%20and%20qualitative%20enhancements%20in%20generating%0Aimages%20with%20precise%20visual%20relations.%20Project%20page%3A%0Ahttps%3A//wuyinwei-hah.github.io/rrnet.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20249v1&entry.124074799=Read"},
{"title": "RLSynC: Offline-Online Reinforcement Learning for Synthon Completion", "author": "Frazier N. Baker and Ziqi Chen and Daniel Adu-Ampratwum and Xia Ning", "abstract": "  Retrosynthesis is the process of determining the set of reactant molecules\nthat can react to form a desired product. Semi-template-based retrosynthesis\nmethods, which imitate the reverse logic of synthesis reactions, first predict\nthe reaction centers in the products, and then complete the resulting synthons\nback into reactants. We develop a new offline-online reinforcement learning\nmethod RLSynC for synthon completion in semi-template-based methods. RLSynC\nassigns one agent to each synthon, all of which complete the synthons by\nconducting actions step by step in a synchronized fashion. RLSynC learns the\npolicy from both offline training episodes and online interactions, which\nallows RLSynC to explore new reaction spaces. RLSynC uses a standalone forward\nsynthesis model to evaluate the likelihood of the predicted reactants in\nsynthesizing a product, and thus guides the action search. Our results\ndemonstrate that RLSynC can outperform state-of-the-art synthon completion\nmethods with improvements as high as 14.9%, highlighting its potential in\nsynthesis planning.\n", "link": "http://arxiv.org/abs/2309.02671v3", "date": "2024-03-29", "relevancy": 1.7638, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4598}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4404}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.434}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RLSynC%3A%20Offline-Online%20Reinforcement%20Learning%20for%20Synthon%20Completion&body=Title%3A%20RLSynC%3A%20Offline-Online%20Reinforcement%20Learning%20for%20Synthon%20Completion%0AAuthor%3A%20Frazier%20N.%20Baker%20and%20Ziqi%20Chen%20and%20Daniel%20Adu-Ampratwum%20and%20Xia%20Ning%0AAbstract%3A%20%20%20Retrosynthesis%20is%20the%20process%20of%20determining%20the%20set%20of%20reactant%20molecules%0Athat%20can%20react%20to%20form%20a%20desired%20product.%20Semi-template-based%20retrosynthesis%0Amethods%2C%20which%20imitate%20the%20reverse%20logic%20of%20synthesis%20reactions%2C%20first%20predict%0Athe%20reaction%20centers%20in%20the%20products%2C%20and%20then%20complete%20the%20resulting%20synthons%0Aback%20into%20reactants.%20We%20develop%20a%20new%20offline-online%20reinforcement%20learning%0Amethod%20RLSynC%20for%20synthon%20completion%20in%20semi-template-based%20methods.%20RLSynC%0Aassigns%20one%20agent%20to%20each%20synthon%2C%20all%20of%20which%20complete%20the%20synthons%20by%0Aconducting%20actions%20step%20by%20step%20in%20a%20synchronized%20fashion.%20RLSynC%20learns%20the%0Apolicy%20from%20both%20offline%20training%20episodes%20and%20online%20interactions%2C%20which%0Aallows%20RLSynC%20to%20explore%20new%20reaction%20spaces.%20RLSynC%20uses%20a%20standalone%20forward%0Asynthesis%20model%20to%20evaluate%20the%20likelihood%20of%20the%20predicted%20reactants%20in%0Asynthesizing%20a%20product%2C%20and%20thus%20guides%20the%20action%20search.%20Our%20results%0Ademonstrate%20that%20RLSynC%20can%20outperform%20state-of-the-art%20synthon%20completion%0Amethods%20with%20improvements%20as%20high%20as%2014.9%25%2C%20highlighting%20its%20potential%20in%0Asynthesis%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.02671v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLSynC%3A%20Offline-Online%20Reinforcement%20Learning%20for%20Synthon%20Completion&entry.906535625=Frazier%20N.%20Baker%20and%20Ziqi%20Chen%20and%20Daniel%20Adu-Ampratwum%20and%20Xia%20Ning&entry.1292438233=%20%20Retrosynthesis%20is%20the%20process%20of%20determining%20the%20set%20of%20reactant%20molecules%0Athat%20can%20react%20to%20form%20a%20desired%20product.%20Semi-template-based%20retrosynthesis%0Amethods%2C%20which%20imitate%20the%20reverse%20logic%20of%20synthesis%20reactions%2C%20first%20predict%0Athe%20reaction%20centers%20in%20the%20products%2C%20and%20then%20complete%20the%20resulting%20synthons%0Aback%20into%20reactants.%20We%20develop%20a%20new%20offline-online%20reinforcement%20learning%0Amethod%20RLSynC%20for%20synthon%20completion%20in%20semi-template-based%20methods.%20RLSynC%0Aassigns%20one%20agent%20to%20each%20synthon%2C%20all%20of%20which%20complete%20the%20synthons%20by%0Aconducting%20actions%20step%20by%20step%20in%20a%20synchronized%20fashion.%20RLSynC%20learns%20the%0Apolicy%20from%20both%20offline%20training%20episodes%20and%20online%20interactions%2C%20which%0Aallows%20RLSynC%20to%20explore%20new%20reaction%20spaces.%20RLSynC%20uses%20a%20standalone%20forward%0Asynthesis%20model%20to%20evaluate%20the%20likelihood%20of%20the%20predicted%20reactants%20in%0Asynthesizing%20a%20product%2C%20and%20thus%20guides%20the%20action%20search.%20Our%20results%0Ademonstrate%20that%20RLSynC%20can%20outperform%20state-of-the-art%20synthon%20completion%0Amethods%20with%20improvements%20as%20high%20as%2014.9%25%2C%20highlighting%20its%20potential%20in%0Asynthesis%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.02671v3&entry.124074799=Read"},
{"title": "Conformal Prediction for Stochastic Decision-Making of PV Power in\n  Electricity Markets", "author": "Yvet Renkema and Nico Brinkel and Tarek Alskaif", "abstract": "  This paper studies the use of conformal prediction (CP), an emerging\nprobabilistic forecasting method, for day-ahead photovoltaic power predictions\nto enhance participation in electricity markets. First, machine learning models\nare used to construct point predictions. Thereafter, several variants of CP are\nimplemented to quantify the uncertainty of those predictions by creating CP\nintervals and cumulative distribution functions. Optimal quantity bids for the\nelectricity market are estimated using several bidding strategies under\nuncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected\nutility maximization (EUM). Results show that CP in combination with k-nearest\nneighbors and/or Mondrian binning outperforms its corresponding linear quantile\nregressors. Using CP in combination with certain bidding strategies can yield\nhigh profit with minimal energy imbalance. In concrete, using conformal\npredictive systems with k-nearest neighbors and Mondrian binning after random\nforest regression yields the best profit and imbalance regardless of the\ndecision-making strategy. Combining this uncertainty quantification method with\nthe EUM strategy with conditional value at risk (CVaR) can yield up to 93\\% of\nthe potential profit with minimal energy imbalance.\n", "link": "http://arxiv.org/abs/2403.20149v1", "date": "2024-03-29", "relevancy": 1.7557, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.443}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20for%20Stochastic%20Decision-Making%20of%20PV%20Power%20in%0A%20%20Electricity%20Markets&body=Title%3A%20Conformal%20Prediction%20for%20Stochastic%20Decision-Making%20of%20PV%20Power%20in%0A%20%20Electricity%20Markets%0AAuthor%3A%20Yvet%20Renkema%20and%20Nico%20Brinkel%20and%20Tarek%20Alskaif%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20use%20of%20conformal%20prediction%20%28CP%29%2C%20an%20emerging%0Aprobabilistic%20forecasting%20method%2C%20for%20day-ahead%20photovoltaic%20power%20predictions%0Ato%20enhance%20participation%20in%20electricity%20markets.%20First%2C%20machine%20learning%20models%0Aare%20used%20to%20construct%20point%20predictions.%20Thereafter%2C%20several%20variants%20of%20CP%20are%0Aimplemented%20to%20quantify%20the%20uncertainty%20of%20those%20predictions%20by%20creating%20CP%0Aintervals%20and%20cumulative%20distribution%20functions.%20Optimal%20quantity%20bids%20for%20the%0Aelectricity%20market%20are%20estimated%20using%20several%20bidding%20strategies%20under%0Auncertainty%2C%20namely%3A%20trust-the-forecast%2C%20worst-case%2C%20Newsvendor%20and%20expected%0Autility%20maximization%20%28EUM%29.%20Results%20show%20that%20CP%20in%20combination%20with%20k-nearest%0Aneighbors%20and/or%20Mondrian%20binning%20outperforms%20its%20corresponding%20linear%20quantile%0Aregressors.%20Using%20CP%20in%20combination%20with%20certain%20bidding%20strategies%20can%20yield%0Ahigh%20profit%20with%20minimal%20energy%20imbalance.%20In%20concrete%2C%20using%20conformal%0Apredictive%20systems%20with%20k-nearest%20neighbors%20and%20Mondrian%20binning%20after%20random%0Aforest%20regression%20yields%20the%20best%20profit%20and%20imbalance%20regardless%20of%20the%0Adecision-making%20strategy.%20Combining%20this%20uncertainty%20quantification%20method%20with%0Athe%20EUM%20strategy%20with%20conditional%20value%20at%20risk%20%28CVaR%29%20can%20yield%20up%20to%2093%5C%25%20of%0Athe%20potential%20profit%20with%20minimal%20energy%20imbalance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20149v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20for%20Stochastic%20Decision-Making%20of%20PV%20Power%20in%0A%20%20Electricity%20Markets&entry.906535625=Yvet%20Renkema%20and%20Nico%20Brinkel%20and%20Tarek%20Alskaif&entry.1292438233=%20%20This%20paper%20studies%20the%20use%20of%20conformal%20prediction%20%28CP%29%2C%20an%20emerging%0Aprobabilistic%20forecasting%20method%2C%20for%20day-ahead%20photovoltaic%20power%20predictions%0Ato%20enhance%20participation%20in%20electricity%20markets.%20First%2C%20machine%20learning%20models%0Aare%20used%20to%20construct%20point%20predictions.%20Thereafter%2C%20several%20variants%20of%20CP%20are%0Aimplemented%20to%20quantify%20the%20uncertainty%20of%20those%20predictions%20by%20creating%20CP%0Aintervals%20and%20cumulative%20distribution%20functions.%20Optimal%20quantity%20bids%20for%20the%0Aelectricity%20market%20are%20estimated%20using%20several%20bidding%20strategies%20under%0Auncertainty%2C%20namely%3A%20trust-the-forecast%2C%20worst-case%2C%20Newsvendor%20and%20expected%0Autility%20maximization%20%28EUM%29.%20Results%20show%20that%20CP%20in%20combination%20with%20k-nearest%0Aneighbors%20and/or%20Mondrian%20binning%20outperforms%20its%20corresponding%20linear%20quantile%0Aregressors.%20Using%20CP%20in%20combination%20with%20certain%20bidding%20strategies%20can%20yield%0Ahigh%20profit%20with%20minimal%20energy%20imbalance.%20In%20concrete%2C%20using%20conformal%0Apredictive%20systems%20with%20k-nearest%20neighbors%20and%20Mondrian%20binning%20after%20random%0Aforest%20regression%20yields%20the%20best%20profit%20and%20imbalance%20regardless%20of%20the%0Adecision-making%20strategy.%20Combining%20this%20uncertainty%20quantification%20method%20with%0Athe%20EUM%20strategy%20with%20conditional%20value%20at%20risk%20%28CVaR%29%20can%20yield%20up%20to%2093%5C%25%20of%0Athe%20potential%20profit%20with%20minimal%20energy%20imbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20149v1&entry.124074799=Read"},
{"title": "MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image\n  Segmentation", "author": "Taha Koleilat and Hojat Asgariandehkordi and Hassan Rivaz and Yiming Xiao", "abstract": "  Medical image segmentation of anatomical structures and pathology is crucial\nin modern clinical diagnosis, disease study, and treatment planning. To date,\ngreat progress has been made in deep learning-based segmentation techniques,\nbut most methods still lack data efficiency, generalizability, and\ninteractability. Consequently, the development of new, precise segmentation\nmethods that demand fewer labeled datasets is of utmost importance in medical\nimage analysis. Recently, the emergence of foundation models, such as CLIP and\nSegment-Anything-Model (SAM), with comprehensive cross-domain representation\nopened the door for interactive and universal image segmentation. However,\nexploration of these models for data-efficient medical image segmentation is\nstill limited, but is highly necessary. In this paper, we propose a novel\nframework, called MedCLIP-SAM that combines CLIP and SAM models to generate\nsegmentation of clinical scans using text prompts in both zero-shot and weakly\nsupervised settings. To achieve this, we employed a new Decoupled Hard Negative\nNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model\nand the recent gScoreCAM to generate prompts to obtain segmentation masks from\nSAM in a zero-shot setting. Additionally, we explored the use of zero-shot\nsegmentation labels in a weakly supervised paradigm to improve the segmentation\nquality further. By extensively testing three diverse segmentation tasks and\nmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lung\nX-ray), our proposed framework has demonstrated excellent accuracy.\n", "link": "http://arxiv.org/abs/2403.20253v1", "date": "2024-03-29", "relevancy": 1.7493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6239}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5516}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MedCLIP-SAM%3A%20Bridging%20Text%20and%20Image%20Towards%20Universal%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20MedCLIP-SAM%3A%20Bridging%20Text%20and%20Image%20Towards%20Universal%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Taha%20Koleilat%20and%20Hojat%20Asgariandehkordi%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20of%20anatomical%20structures%20and%20pathology%20is%20crucial%0Ain%20modern%20clinical%20diagnosis%2C%20disease%20study%2C%20and%20treatment%20planning.%20To%20date%2C%0Agreat%20progress%20has%20been%20made%20in%20deep%20learning-based%20segmentation%20techniques%2C%0Abut%20most%20methods%20still%20lack%20data%20efficiency%2C%20generalizability%2C%20and%0Ainteractability.%20Consequently%2C%20the%20development%20of%20new%2C%20precise%20segmentation%0Amethods%20that%20demand%20fewer%20labeled%20datasets%20is%20of%20utmost%20importance%20in%20medical%0Aimage%20analysis.%20Recently%2C%20the%20emergence%20of%20foundation%20models%2C%20such%20as%20CLIP%20and%0ASegment-Anything-Model%20%28SAM%29%2C%20with%20comprehensive%20cross-domain%20representation%0Aopened%20the%20door%20for%20interactive%20and%20universal%20image%20segmentation.%20However%2C%0Aexploration%20of%20these%20models%20for%20data-efficient%20medical%20image%20segmentation%20is%0Astill%20limited%2C%20but%20is%20highly%20necessary.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20called%20MedCLIP-SAM%20that%20combines%20CLIP%20and%20SAM%20models%20to%20generate%0Asegmentation%20of%20clinical%20scans%20using%20text%20prompts%20in%20both%20zero-shot%20and%20weakly%0Asupervised%20settings.%20To%20achieve%20this%2C%20we%20employed%20a%20new%20Decoupled%20Hard%20Negative%0ANoise%20Contrastive%20Estimation%20%28DHN-NCE%29%20loss%20to%20fine-tune%20the%20BiomedCLIP%20model%0Aand%20the%20recent%20gScoreCAM%20to%20generate%20prompts%20to%20obtain%20segmentation%20masks%20from%0ASAM%20in%20a%20zero-shot%20setting.%20Additionally%2C%20we%20explored%20the%20use%20of%20zero-shot%0Asegmentation%20labels%20in%20a%20weakly%20supervised%20paradigm%20to%20improve%20the%20segmentation%0Aquality%20further.%20By%20extensively%20testing%20three%20diverse%20segmentation%20tasks%20and%0Amedical%20image%20modalities%20%28breast%20tumor%20ultrasound%2C%20brain%20tumor%20MRI%2C%20and%20lung%0AX-ray%29%2C%20our%20proposed%20framework%20has%20demonstrated%20excellent%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20253v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedCLIP-SAM%3A%20Bridging%20Text%20and%20Image%20Towards%20Universal%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Taha%20Koleilat%20and%20Hojat%20Asgariandehkordi%20and%20Hassan%20Rivaz%20and%20Yiming%20Xiao&entry.1292438233=%20%20Medical%20image%20segmentation%20of%20anatomical%20structures%20and%20pathology%20is%20crucial%0Ain%20modern%20clinical%20diagnosis%2C%20disease%20study%2C%20and%20treatment%20planning.%20To%20date%2C%0Agreat%20progress%20has%20been%20made%20in%20deep%20learning-based%20segmentation%20techniques%2C%0Abut%20most%20methods%20still%20lack%20data%20efficiency%2C%20generalizability%2C%20and%0Ainteractability.%20Consequently%2C%20the%20development%20of%20new%2C%20precise%20segmentation%0Amethods%20that%20demand%20fewer%20labeled%20datasets%20is%20of%20utmost%20importance%20in%20medical%0Aimage%20analysis.%20Recently%2C%20the%20emergence%20of%20foundation%20models%2C%20such%20as%20CLIP%20and%0ASegment-Anything-Model%20%28SAM%29%2C%20with%20comprehensive%20cross-domain%20representation%0Aopened%20the%20door%20for%20interactive%20and%20universal%20image%20segmentation.%20However%2C%0Aexploration%20of%20these%20models%20for%20data-efficient%20medical%20image%20segmentation%20is%0Astill%20limited%2C%20but%20is%20highly%20necessary.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20called%20MedCLIP-SAM%20that%20combines%20CLIP%20and%20SAM%20models%20to%20generate%0Asegmentation%20of%20clinical%20scans%20using%20text%20prompts%20in%20both%20zero-shot%20and%20weakly%0Asupervised%20settings.%20To%20achieve%20this%2C%20we%20employed%20a%20new%20Decoupled%20Hard%20Negative%0ANoise%20Contrastive%20Estimation%20%28DHN-NCE%29%20loss%20to%20fine-tune%20the%20BiomedCLIP%20model%0Aand%20the%20recent%20gScoreCAM%20to%20generate%20prompts%20to%20obtain%20segmentation%20masks%20from%0ASAM%20in%20a%20zero-shot%20setting.%20Additionally%2C%20we%20explored%20the%20use%20of%20zero-shot%0Asegmentation%20labels%20in%20a%20weakly%20supervised%20paradigm%20to%20improve%20the%20segmentation%0Aquality%20further.%20By%20extensively%20testing%20three%20diverse%20segmentation%20tasks%20and%0Amedical%20image%20modalities%20%28breast%20tumor%20ultrasound%2C%20brain%20tumor%20MRI%2C%20and%20lung%0AX-ray%29%2C%20our%20proposed%20framework%20has%20demonstrated%20excellent%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20253v1&entry.124074799=Read"},
{"title": "Two-sample Test using Projected Wasserstein Distance", "author": "Jie Wang and Rui Gao and Yao Xie", "abstract": "  We develop a projected Wasserstein distance for the two-sample test, a\nfundamental problem in statistics and machine learning: given two sets of\nsamples, to determine whether they are from the same distribution. In\nparticular, we aim to circumvent the curse of dimensionality in Wasserstein\ndistance: when the dimension is high, it has diminishing testing power, which\nis inherently due to the slow concentration property of Wasserstein metrics in\nthe high dimension space. A key contribution is to couple optimal projection to\nfind the low dimensional linear mapping to maximize the Wasserstein distance\nbetween projected probability distributions. We characterize the theoretical\nproperty of the finite-sample convergence rate on IPMs and present practical\nalgorithms for computing this metric. Numerical examples validate our\ntheoretical results.\n", "link": "http://arxiv.org/abs/2010.11970v4", "date": "2024-03-29", "relevancy": 1.7452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4425}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4422}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4279}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two-sample%20Test%20using%20Projected%20Wasserstein%20Distance&body=Title%3A%20Two-sample%20Test%20using%20Projected%20Wasserstein%20Distance%0AAuthor%3A%20Jie%20Wang%20and%20Rui%20Gao%20and%20Yao%20Xie%0AAbstract%3A%20%20%20We%20develop%20a%20projected%20Wasserstein%20distance%20for%20the%20two-sample%20test%2C%20a%0Afundamental%20problem%20in%20statistics%20and%20machine%20learning%3A%20given%20two%20sets%20of%0Asamples%2C%20to%20determine%20whether%20they%20are%20from%20the%20same%20distribution.%20In%0Aparticular%2C%20we%20aim%20to%20circumvent%20the%20curse%20of%20dimensionality%20in%20Wasserstein%0Adistance%3A%20when%20the%20dimension%20is%20high%2C%20it%20has%20diminishing%20testing%20power%2C%20which%0Ais%20inherently%20due%20to%20the%20slow%20concentration%20property%20of%20Wasserstein%20metrics%20in%0Athe%20high%20dimension%20space.%20A%20key%20contribution%20is%20to%20couple%20optimal%20projection%20to%0Afind%20the%20low%20dimensional%20linear%20mapping%20to%20maximize%20the%20Wasserstein%20distance%0Abetween%20projected%20probability%20distributions.%20We%20characterize%20the%20theoretical%0Aproperty%20of%20the%20finite-sample%20convergence%20rate%20on%20IPMs%20and%20present%20practical%0Aalgorithms%20for%20computing%20this%20metric.%20Numerical%20examples%20validate%20our%0Atheoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2010.11970v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-sample%20Test%20using%20Projected%20Wasserstein%20Distance&entry.906535625=Jie%20Wang%20and%20Rui%20Gao%20and%20Yao%20Xie&entry.1292438233=%20%20We%20develop%20a%20projected%20Wasserstein%20distance%20for%20the%20two-sample%20test%2C%20a%0Afundamental%20problem%20in%20statistics%20and%20machine%20learning%3A%20given%20two%20sets%20of%0Asamples%2C%20to%20determine%20whether%20they%20are%20from%20the%20same%20distribution.%20In%0Aparticular%2C%20we%20aim%20to%20circumvent%20the%20curse%20of%20dimensionality%20in%20Wasserstein%0Adistance%3A%20when%20the%20dimension%20is%20high%2C%20it%20has%20diminishing%20testing%20power%2C%20which%0Ais%20inherently%20due%20to%20the%20slow%20concentration%20property%20of%20Wasserstein%20metrics%20in%0Athe%20high%20dimension%20space.%20A%20key%20contribution%20is%20to%20couple%20optimal%20projection%20to%0Afind%20the%20low%20dimensional%20linear%20mapping%20to%20maximize%20the%20Wasserstein%20distance%0Abetween%20projected%20probability%20distributions.%20We%20characterize%20the%20theoretical%0Aproperty%20of%20the%20finite-sample%20convergence%20rate%20on%20IPMs%20and%20present%20practical%0Aalgorithms%20for%20computing%20this%20metric.%20Numerical%20examples%20validate%20our%0Atheoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2010.11970v4&entry.124074799=Read"},
{"title": "A Multi-Aspect Framework for Counter Narrative Evaluation using Large\n  Language Models", "author": "Jaylen Jones and Lingbo Mo and Eric Fosler-Lussier and Huan Sun", "abstract": "  Counter narratives - informed responses to hate speech contexts designed to\nrefute hateful claims and de-escalate encounters - have emerged as an effective\nhate speech intervention strategy. While previous work has proposed automatic\ncounter narrative generation methods to aid manual interventions, the\nevaluation of these approaches remains underdeveloped. Previous automatic\nmetrics for counter narrative evaluation lack alignment with human judgment as\nthey rely on superficial reference comparisons instead of incorporating key\naspects of counter narrative quality as evaluation criteria. To address prior\nevaluation limitations, we propose a novel evaluation framework prompting LLMs\nto provide scores and feedback for generated counter narrative candidates using\n5 defined aspects derived from guidelines from counter narrative specialized\nNGOs. We found that LLM evaluators achieve strong alignment to human-annotated\nscores and feedback and outperform alternative metrics, indicating their\npotential as multi-aspect, reference-free and interpretable evaluators for\ncounter narrative evaluation.\n", "link": "http://arxiv.org/abs/2402.11676v2", "date": "2024-03-29", "relevancy": 1.433, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4906}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Aspect%20Framework%20for%20Counter%20Narrative%20Evaluation%20using%20Large%0A%20%20Language%20Models&body=Title%3A%20A%20Multi-Aspect%20Framework%20for%20Counter%20Narrative%20Evaluation%20using%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Jaylen%20Jones%20and%20Lingbo%20Mo%20and%20Eric%20Fosler-Lussier%20and%20Huan%20Sun%0AAbstract%3A%20%20%20Counter%20narratives%20-%20informed%20responses%20to%20hate%20speech%20contexts%20designed%20to%0Arefute%20hateful%20claims%20and%20de-escalate%20encounters%20-%20have%20emerged%20as%20an%20effective%0Ahate%20speech%20intervention%20strategy.%20While%20previous%20work%20has%20proposed%20automatic%0Acounter%20narrative%20generation%20methods%20to%20aid%20manual%20interventions%2C%20the%0Aevaluation%20of%20these%20approaches%20remains%20underdeveloped.%20Previous%20automatic%0Ametrics%20for%20counter%20narrative%20evaluation%20lack%20alignment%20with%20human%20judgment%20as%0Athey%20rely%20on%20superficial%20reference%20comparisons%20instead%20of%20incorporating%20key%0Aaspects%20of%20counter%20narrative%20quality%20as%20evaluation%20criteria.%20To%20address%20prior%0Aevaluation%20limitations%2C%20we%20propose%20a%20novel%20evaluation%20framework%20prompting%20LLMs%0Ato%20provide%20scores%20and%20feedback%20for%20generated%20counter%20narrative%20candidates%20using%0A5%20defined%20aspects%20derived%20from%20guidelines%20from%20counter%20narrative%20specialized%0ANGOs.%20We%20found%20that%20LLM%20evaluators%20achieve%20strong%20alignment%20to%20human-annotated%0Ascores%20and%20feedback%20and%20outperform%20alternative%20metrics%2C%20indicating%20their%0Apotential%20as%20multi-aspect%2C%20reference-free%20and%20interpretable%20evaluators%20for%0Acounter%20narrative%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11676v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Aspect%20Framework%20for%20Counter%20Narrative%20Evaluation%20using%20Large%0A%20%20Language%20Models&entry.906535625=Jaylen%20Jones%20and%20Lingbo%20Mo%20and%20Eric%20Fosler-Lussier%20and%20Huan%20Sun&entry.1292438233=%20%20Counter%20narratives%20-%20informed%20responses%20to%20hate%20speech%20contexts%20designed%20to%0Arefute%20hateful%20claims%20and%20de-escalate%20encounters%20-%20have%20emerged%20as%20an%20effective%0Ahate%20speech%20intervention%20strategy.%20While%20previous%20work%20has%20proposed%20automatic%0Acounter%20narrative%20generation%20methods%20to%20aid%20manual%20interventions%2C%20the%0Aevaluation%20of%20these%20approaches%20remains%20underdeveloped.%20Previous%20automatic%0Ametrics%20for%20counter%20narrative%20evaluation%20lack%20alignment%20with%20human%20judgment%20as%0Athey%20rely%20on%20superficial%20reference%20comparisons%20instead%20of%20incorporating%20key%0Aaspects%20of%20counter%20narrative%20quality%20as%20evaluation%20criteria.%20To%20address%20prior%0Aevaluation%20limitations%2C%20we%20propose%20a%20novel%20evaluation%20framework%20prompting%20LLMs%0Ato%20provide%20scores%20and%20feedback%20for%20generated%20counter%20narrative%20candidates%20using%0A5%20defined%20aspects%20derived%20from%20guidelines%20from%20counter%20narrative%20specialized%0ANGOs.%20We%20found%20that%20LLM%20evaluators%20achieve%20strong%20alignment%20to%20human-annotated%0Ascores%20and%20feedback%20and%20outperform%20alternative%20metrics%2C%20indicating%20their%0Apotential%20as%20multi-aspect%2C%20reference-free%20and%20interpretable%20evaluators%20for%0Acounter%20narrative%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11676v2&entry.124074799=Read"},
{"title": "Simple inverse kinematics computation considering joint motion\n  efficiency", "author": "Ansei Yonezawa and Heisei Yonezawa and Itsuro Kajiwara", "abstract": "  Inverse kinematics is an important and challenging problem in the operation\nof industrial manipulators. This study proposes a simple inverse kinematics\ncalculation scheme for an industrial serial manipulator. The proposed technique\ncan calculate appropriate values of the joint variables to realize the desired\nend-effector position and orientation while considering the motion costs of\neach joint. Two scalar functions are defined for the joint variables: one is to\nevaluate the end-effector position and orientation, whereas the other is to\nevaluate the motion efficiency of the joints. By combining the two scalar\nfunctions, the inverse kinematics calculation of the manipulator is formulated\nas a numerical optimization problem. Furthermore, a simple algorithm for\nsolving the inverse kinematics via the aforementioned optimization is\nconstructed on the basis of the simultaneous perturbation stochastic\napproximation with a norm-limited update vector (NLSPSA). The proposed scheme\nconsiders not only the accuracy of the position and orientation of the\nend-effector but also the efficiency of the robot movement. Therefore, it\nyields a practical result of the inverse problem. Moreover, the proposed\nalgorithm is simple and easy to implement owing to the high calculation\nefficiency of NLSPSA. Finally, the effectiveness of the proposed method is\nverified through numerical examples using a redundant manipulator.\n", "link": "http://arxiv.org/abs/2403.20128v1", "date": "2024-03-29", "relevancy": 1.3848, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4593}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4428}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20inverse%20kinematics%20computation%20considering%20joint%20motion%0A%20%20efficiency&body=Title%3A%20Simple%20inverse%20kinematics%20computation%20considering%20joint%20motion%0A%20%20efficiency%0AAuthor%3A%20Ansei%20Yonezawa%20and%20Heisei%20Yonezawa%20and%20Itsuro%20Kajiwara%0AAbstract%3A%20%20%20Inverse%20kinematics%20is%20an%20important%20and%20challenging%20problem%20in%20the%20operation%0Aof%20industrial%20manipulators.%20This%20study%20proposes%20a%20simple%20inverse%20kinematics%0Acalculation%20scheme%20for%20an%20industrial%20serial%20manipulator.%20The%20proposed%20technique%0Acan%20calculate%20appropriate%20values%20of%20the%20joint%20variables%20to%20realize%20the%20desired%0Aend-effector%20position%20and%20orientation%20while%20considering%20the%20motion%20costs%20of%0Aeach%20joint.%20Two%20scalar%20functions%20are%20defined%20for%20the%20joint%20variables%3A%20one%20is%20to%0Aevaluate%20the%20end-effector%20position%20and%20orientation%2C%20whereas%20the%20other%20is%20to%0Aevaluate%20the%20motion%20efficiency%20of%20the%20joints.%20By%20combining%20the%20two%20scalar%0Afunctions%2C%20the%20inverse%20kinematics%20calculation%20of%20the%20manipulator%20is%20formulated%0Aas%20a%20numerical%20optimization%20problem.%20Furthermore%2C%20a%20simple%20algorithm%20for%0Asolving%20the%20inverse%20kinematics%20via%20the%20aforementioned%20optimization%20is%0Aconstructed%20on%20the%20basis%20of%20the%20simultaneous%20perturbation%20stochastic%0Aapproximation%20with%20a%20norm-limited%20update%20vector%20%28NLSPSA%29.%20The%20proposed%20scheme%0Aconsiders%20not%20only%20the%20accuracy%20of%20the%20position%20and%20orientation%20of%20the%0Aend-effector%20but%20also%20the%20efficiency%20of%20the%20robot%20movement.%20Therefore%2C%20it%0Ayields%20a%20practical%20result%20of%20the%20inverse%20problem.%20Moreover%2C%20the%20proposed%0Aalgorithm%20is%20simple%20and%20easy%20to%20implement%20owing%20to%20the%20high%20calculation%0Aefficiency%20of%20NLSPSA.%20Finally%2C%20the%20effectiveness%20of%20the%20proposed%20method%20is%0Averified%20through%20numerical%20examples%20using%20a%20redundant%20manipulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20128v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20inverse%20kinematics%20computation%20considering%20joint%20motion%0A%20%20efficiency&entry.906535625=Ansei%20Yonezawa%20and%20Heisei%20Yonezawa%20and%20Itsuro%20Kajiwara&entry.1292438233=%20%20Inverse%20kinematics%20is%20an%20important%20and%20challenging%20problem%20in%20the%20operation%0Aof%20industrial%20manipulators.%20This%20study%20proposes%20a%20simple%20inverse%20kinematics%0Acalculation%20scheme%20for%20an%20industrial%20serial%20manipulator.%20The%20proposed%20technique%0Acan%20calculate%20appropriate%20values%20of%20the%20joint%20variables%20to%20realize%20the%20desired%0Aend-effector%20position%20and%20orientation%20while%20considering%20the%20motion%20costs%20of%0Aeach%20joint.%20Two%20scalar%20functions%20are%20defined%20for%20the%20joint%20variables%3A%20one%20is%20to%0Aevaluate%20the%20end-effector%20position%20and%20orientation%2C%20whereas%20the%20other%20is%20to%0Aevaluate%20the%20motion%20efficiency%20of%20the%20joints.%20By%20combining%20the%20two%20scalar%0Afunctions%2C%20the%20inverse%20kinematics%20calculation%20of%20the%20manipulator%20is%20formulated%0Aas%20a%20numerical%20optimization%20problem.%20Furthermore%2C%20a%20simple%20algorithm%20for%0Asolving%20the%20inverse%20kinematics%20via%20the%20aforementioned%20optimization%20is%0Aconstructed%20on%20the%20basis%20of%20the%20simultaneous%20perturbation%20stochastic%0Aapproximation%20with%20a%20norm-limited%20update%20vector%20%28NLSPSA%29.%20The%20proposed%20scheme%0Aconsiders%20not%20only%20the%20accuracy%20of%20the%20position%20and%20orientation%20of%20the%0Aend-effector%20but%20also%20the%20efficiency%20of%20the%20robot%20movement.%20Therefore%2C%20it%0Ayields%20a%20practical%20result%20of%20the%20inverse%20problem.%20Moreover%2C%20the%20proposed%0Aalgorithm%20is%20simple%20and%20easy%20to%20implement%20owing%20to%20the%20high%20calculation%0Aefficiency%20of%20NLSPSA.%20Finally%2C%20the%20effectiveness%20of%20the%20proposed%20method%20is%0Averified%20through%20numerical%20examples%20using%20a%20redundant%20manipulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20128v1&entry.124074799=Read"},
{"title": "Design, Fabrication and Evaluation of a Stretchable High-Density\n  Electromyography Array", "author": "Rejin John Varghese and Matteo Pizzi and Aritra Kundu and Agnese Grison and Etienne Burdet and Dario Farina", "abstract": "  The adoption of high-density electrode systems for human-machine interfaces\nin real-life applications has been impeded by practical and technical\nchallenges, including noise interference, motion artifacts and the lack of\ncompact electrode interfaces. To overcome some of these challenges, we\nintroduce a wearable and stretchable electromyography (EMG) array, and present\nits design, fabrication methodology, characterisation, and comprehensive\nevaluation. Our proposed solution comprises dry-electrodes on flexible printed\ncircuit board (PCB) substrates, eliminating the need for time-consuming skin\npreparation. The proposed fabrication method allows the manufacturing of\nstretchable sleeves, with consistent and standardised coverage across subjects.\nWe thoroughly tested our developed prototype, evaluating its potential for\napplication in both research and real-world environments. The results of our\nstudy showed that the developed stretchable array matches or outperforms\ntraditional EMG grids and holds promise in furthering the real-world\ntranslation of high-density EMG for human-machine interfaces.\n", "link": "http://arxiv.org/abs/2403.20117v1", "date": "2024-03-29", "relevancy": 1.2306, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4374}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4035}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3997}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Design%2C%20Fabrication%20and%20Evaluation%20of%20a%20Stretchable%20High-Density%0A%20%20Electromyography%20Array&body=Title%3A%20Design%2C%20Fabrication%20and%20Evaluation%20of%20a%20Stretchable%20High-Density%0A%20%20Electromyography%20Array%0AAuthor%3A%20Rejin%20John%20Varghese%20and%20Matteo%20Pizzi%20and%20Aritra%20Kundu%20and%20Agnese%20Grison%20and%20Etienne%20Burdet%20and%20Dario%20Farina%0AAbstract%3A%20%20%20The%20adoption%20of%20high-density%20electrode%20systems%20for%20human-machine%20interfaces%0Ain%20real-life%20applications%20has%20been%20impeded%20by%20practical%20and%20technical%0Achallenges%2C%20including%20noise%20interference%2C%20motion%20artifacts%20and%20the%20lack%20of%0Acompact%20electrode%20interfaces.%20To%20overcome%20some%20of%20these%20challenges%2C%20we%0Aintroduce%20a%20wearable%20and%20stretchable%20electromyography%20%28EMG%29%20array%2C%20and%20present%0Aits%20design%2C%20fabrication%20methodology%2C%20characterisation%2C%20and%20comprehensive%0Aevaluation.%20Our%20proposed%20solution%20comprises%20dry-electrodes%20on%20flexible%20printed%0Acircuit%20board%20%28PCB%29%20substrates%2C%20eliminating%20the%20need%20for%20time-consuming%20skin%0Apreparation.%20The%20proposed%20fabrication%20method%20allows%20the%20manufacturing%20of%0Astretchable%20sleeves%2C%20with%20consistent%20and%20standardised%20coverage%20across%20subjects.%0AWe%20thoroughly%20tested%20our%20developed%20prototype%2C%20evaluating%20its%20potential%20for%0Aapplication%20in%20both%20research%20and%20real-world%20environments.%20The%20results%20of%20our%0Astudy%20showed%20that%20the%20developed%20stretchable%20array%20matches%20or%20outperforms%0Atraditional%20EMG%20grids%20and%20holds%20promise%20in%20furthering%20the%20real-world%0Atranslation%20of%20high-density%20EMG%20for%20human-machine%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20117v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%2C%20Fabrication%20and%20Evaluation%20of%20a%20Stretchable%20High-Density%0A%20%20Electromyography%20Array&entry.906535625=Rejin%20John%20Varghese%20and%20Matteo%20Pizzi%20and%20Aritra%20Kundu%20and%20Agnese%20Grison%20and%20Etienne%20Burdet%20and%20Dario%20Farina&entry.1292438233=%20%20The%20adoption%20of%20high-density%20electrode%20systems%20for%20human-machine%20interfaces%0Ain%20real-life%20applications%20has%20been%20impeded%20by%20practical%20and%20technical%0Achallenges%2C%20including%20noise%20interference%2C%20motion%20artifacts%20and%20the%20lack%20of%0Acompact%20electrode%20interfaces.%20To%20overcome%20some%20of%20these%20challenges%2C%20we%0Aintroduce%20a%20wearable%20and%20stretchable%20electromyography%20%28EMG%29%20array%2C%20and%20present%0Aits%20design%2C%20fabrication%20methodology%2C%20characterisation%2C%20and%20comprehensive%0Aevaluation.%20Our%20proposed%20solution%20comprises%20dry-electrodes%20on%20flexible%20printed%0Acircuit%20board%20%28PCB%29%20substrates%2C%20eliminating%20the%20need%20for%20time-consuming%20skin%0Apreparation.%20The%20proposed%20fabrication%20method%20allows%20the%20manufacturing%20of%0Astretchable%20sleeves%2C%20with%20consistent%20and%20standardised%20coverage%20across%20subjects.%0AWe%20thoroughly%20tested%20our%20developed%20prototype%2C%20evaluating%20its%20potential%20for%0Aapplication%20in%20both%20research%20and%20real-world%20environments.%20The%20results%20of%20our%0Astudy%20showed%20that%20the%20developed%20stretchable%20array%20matches%20or%20outperforms%0Atraditional%20EMG%20grids%20and%20holds%20promise%20in%20furthering%20the%20real-world%0Atranslation%20of%20high-density%20EMG%20for%20human-machine%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20117v1&entry.124074799=Read"},
{"title": "Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM\n  Inference", "author": "Jovan Stojkovic and Esha Choukse and Chaojie Zhang and Inigo Goiri and Josep Torrellas", "abstract": "  With the ubiquitous use of modern large language models (LLMs) across\nindustries, the inference serving for these models is ever expanding. Given the\nhigh compute and memory requirements of modern LLMs, more and more\ntop-of-the-line GPUs are being deployed to serve these models. Energy\navailability has come to the forefront as the biggest challenge for data center\nexpansion to serve these models. In this paper, we present the trade-offs\nbrought up by making energy efficiency the primary goal of LLM serving under\nperformance SLOs. We show that depending on the inputs, the model, and the\nservice-level agreements, there are several knobs available to the LLM\ninference provider to use for being energy efficient. We characterize the\nimpact of these knobs on the latency, throughput, as well as the energy. By\nexploring these trade-offs, we offer valuable insights into optimizing energy\nusage without compromising on performance, thereby paving the way for\nsustainable and cost-effective LLM deployment in data center environments.\n", "link": "http://arxiv.org/abs/2403.20306v1", "date": "2024-03-29", "relevancy": 0.8839, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4465}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4403}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Greener%20LLMs%3A%20Bringing%20Energy-Efficiency%20to%20the%20Forefront%20of%20LLM%0A%20%20Inference&body=Title%3A%20Towards%20Greener%20LLMs%3A%20Bringing%20Energy-Efficiency%20to%20the%20Forefront%20of%20LLM%0A%20%20Inference%0AAuthor%3A%20Jovan%20Stojkovic%20and%20Esha%20Choukse%20and%20Chaojie%20Zhang%20and%20Inigo%20Goiri%20and%20Josep%20Torrellas%0AAbstract%3A%20%20%20With%20the%20ubiquitous%20use%20of%20modern%20large%20language%20models%20%28LLMs%29%20across%0Aindustries%2C%20the%20inference%20serving%20for%20these%20models%20is%20ever%20expanding.%20Given%20the%0Ahigh%20compute%20and%20memory%20requirements%20of%20modern%20LLMs%2C%20more%20and%20more%0Atop-of-the-line%20GPUs%20are%20being%20deployed%20to%20serve%20these%20models.%20Energy%0Aavailability%20has%20come%20to%20the%20forefront%20as%20the%20biggest%20challenge%20for%20data%20center%0Aexpansion%20to%20serve%20these%20models.%20In%20this%20paper%2C%20we%20present%20the%20trade-offs%0Abrought%20up%20by%20making%20energy%20efficiency%20the%20primary%20goal%20of%20LLM%20serving%20under%0Aperformance%20SLOs.%20We%20show%20that%20depending%20on%20the%20inputs%2C%20the%20model%2C%20and%20the%0Aservice-level%20agreements%2C%20there%20are%20several%20knobs%20available%20to%20the%20LLM%0Ainference%20provider%20to%20use%20for%20being%20energy%20efficient.%20We%20characterize%20the%0Aimpact%20of%20these%20knobs%20on%20the%20latency%2C%20throughput%2C%20as%20well%20as%20the%20energy.%20By%0Aexploring%20these%20trade-offs%2C%20we%20offer%20valuable%20insights%20into%20optimizing%20energy%0Ausage%20without%20compromising%20on%20performance%2C%20thereby%20paving%20the%20way%20for%0Asustainable%20and%20cost-effective%20LLM%20deployment%20in%20data%20center%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20306v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Greener%20LLMs%3A%20Bringing%20Energy-Efficiency%20to%20the%20Forefront%20of%20LLM%0A%20%20Inference&entry.906535625=Jovan%20Stojkovic%20and%20Esha%20Choukse%20and%20Chaojie%20Zhang%20and%20Inigo%20Goiri%20and%20Josep%20Torrellas&entry.1292438233=%20%20With%20the%20ubiquitous%20use%20of%20modern%20large%20language%20models%20%28LLMs%29%20across%0Aindustries%2C%20the%20inference%20serving%20for%20these%20models%20is%20ever%20expanding.%20Given%20the%0Ahigh%20compute%20and%20memory%20requirements%20of%20modern%20LLMs%2C%20more%20and%20more%0Atop-of-the-line%20GPUs%20are%20being%20deployed%20to%20serve%20these%20models.%20Energy%0Aavailability%20has%20come%20to%20the%20forefront%20as%20the%20biggest%20challenge%20for%20data%20center%0Aexpansion%20to%20serve%20these%20models.%20In%20this%20paper%2C%20we%20present%20the%20trade-offs%0Abrought%20up%20by%20making%20energy%20efficiency%20the%20primary%20goal%20of%20LLM%20serving%20under%0Aperformance%20SLOs.%20We%20show%20that%20depending%20on%20the%20inputs%2C%20the%20model%2C%20and%20the%0Aservice-level%20agreements%2C%20there%20are%20several%20knobs%20available%20to%20the%20LLM%0Ainference%20provider%20to%20use%20for%20being%20energy%20efficient.%20We%20characterize%20the%0Aimpact%20of%20these%20knobs%20on%20the%20latency%2C%20throughput%2C%20as%20well%20as%20the%20energy.%20By%0Aexploring%20these%20trade-offs%2C%20we%20offer%20valuable%20insights%20into%20optimizing%20energy%0Ausage%20without%20compromising%20on%20performance%2C%20thereby%20paving%20the%20way%20for%0Asustainable%20and%20cost-effective%20LLM%20deployment%20in%20data%20center%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20306v1&entry.124074799=Read"},
{"title": "Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to\n  Comprehend What You Want", "author": "Weifeng Lin and Xinyu Wei and Ruichuan An and Peng Gao and Bocheng Zou and Yulin Luo and Siyuan Huang and Shanghang Zhang and Hongsheng Li", "abstract": "  The interaction between humans and artificial intelligence (AI) is a crucial\nfactor that reflects the effectiveness of multimodal large language models\n(MLLMs). However, current MLLMs primarily focus on image-level comprehension\nand limit interaction to textual instructions, thereby constraining their\nflexibility in usage and depth of response. In this paper, we introduce the\nDraw-and-Understand project: a new model, a multi-domain dataset, and a\nchallenging benchmark for visual prompting. Specifically, we propose SPHINX-V,\na new end-to-end trained Multimodal Large Language Model (MLLM) that connects a\nvision encoder, a visual prompt encoder and an LLM for various visual prompts\n(points, bounding boxes, and free-form shape) and language understanding. To\nadvance visual prompting research for MLLMs, we introduce MDVP-Data and\nMDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M unique\nimage-visual prompt-text instruction-following samples, including natural\nimages, document images, OCR images, mobile screenshots, web screenshots, and\nmulti-panel images. Furthermore, we present MDVP-Bench, a comprehensive and\nchallenging benchmark to assess a model's capability in understanding visual\nprompting instructions. Our experiments demonstrate SPHINX-V's impressive\nmultimodal interaction capabilities through visual prompting, revealing\nsignificant improvements in detailed pixel-level description and\nquestion-answering abilities.\n", "link": "http://arxiv.org/abs/2403.20271v1", "date": "2024-03-29", "relevancy": 1.6616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5745}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5449}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5111}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Draw-and-Understand%3A%20Leveraging%20Visual%20Prompts%20to%20Enable%20MLLMs%20to%0A%20%20Comprehend%20What%20You%20Want&body=Title%3A%20Draw-and-Understand%3A%20Leveraging%20Visual%20Prompts%20to%20Enable%20MLLMs%20to%0A%20%20Comprehend%20What%20You%20Want%0AAuthor%3A%20Weifeng%20Lin%20and%20Xinyu%20Wei%20and%20Ruichuan%20An%20and%20Peng%20Gao%20and%20Bocheng%20Zou%20and%20Yulin%20Luo%20and%20Siyuan%20Huang%20and%20Shanghang%20Zhang%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20The%20interaction%20between%20humans%20and%20artificial%20intelligence%20%28AI%29%20is%20a%20crucial%0Afactor%20that%20reflects%20the%20effectiveness%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29.%20However%2C%20current%20MLLMs%20primarily%20focus%20on%20image-level%20comprehension%0Aand%20limit%20interaction%20to%20textual%20instructions%2C%20thereby%20constraining%20their%0Aflexibility%20in%20usage%20and%20depth%20of%20response.%20In%20this%20paper%2C%20we%20introduce%20the%0ADraw-and-Understand%20project%3A%20a%20new%20model%2C%20a%20multi-domain%20dataset%2C%20and%20a%0Achallenging%20benchmark%20for%20visual%20prompting.%20Specifically%2C%20we%20propose%20SPHINX-V%2C%0Aa%20new%20end-to-end%20trained%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20that%20connects%20a%0Avision%20encoder%2C%20a%20visual%20prompt%20encoder%20and%20an%20LLM%20for%20various%20visual%20prompts%0A%28points%2C%20bounding%20boxes%2C%20and%20free-form%20shape%29%20and%20language%20understanding.%20To%0Aadvance%20visual%20prompting%20research%20for%20MLLMs%2C%20we%20introduce%20MDVP-Data%20and%0AMDVP-Bench.%20MDVP-Data%20features%20a%20multi-domain%20dataset%20containing%201.6M%20unique%0Aimage-visual%20prompt-text%20instruction-following%20samples%2C%20including%20natural%0Aimages%2C%20document%20images%2C%20OCR%20images%2C%20mobile%20screenshots%2C%20web%20screenshots%2C%20and%0Amulti-panel%20images.%20Furthermore%2C%20we%20present%20MDVP-Bench%2C%20a%20comprehensive%20and%0Achallenging%20benchmark%20to%20assess%20a%20model%27s%20capability%20in%20understanding%20visual%0Aprompting%20instructions.%20Our%20experiments%20demonstrate%20SPHINX-V%27s%20impressive%0Amultimodal%20interaction%20capabilities%20through%20visual%20prompting%2C%20revealing%0Asignificant%20improvements%20in%20detailed%20pixel-level%20description%20and%0Aquestion-answering%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20271v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Draw-and-Understand%3A%20Leveraging%20Visual%20Prompts%20to%20Enable%20MLLMs%20to%0A%20%20Comprehend%20What%20You%20Want&entry.906535625=Weifeng%20Lin%20and%20Xinyu%20Wei%20and%20Ruichuan%20An%20and%20Peng%20Gao%20and%20Bocheng%20Zou%20and%20Yulin%20Luo%20and%20Siyuan%20Huang%20and%20Shanghang%20Zhang%20and%20Hongsheng%20Li&entry.1292438233=%20%20The%20interaction%20between%20humans%20and%20artificial%20intelligence%20%28AI%29%20is%20a%20crucial%0Afactor%20that%20reflects%20the%20effectiveness%20of%20multimodal%20large%20language%20models%0A%28MLLMs%29.%20However%2C%20current%20MLLMs%20primarily%20focus%20on%20image-level%20comprehension%0Aand%20limit%20interaction%20to%20textual%20instructions%2C%20thereby%20constraining%20their%0Aflexibility%20in%20usage%20and%20depth%20of%20response.%20In%20this%20paper%2C%20we%20introduce%20the%0ADraw-and-Understand%20project%3A%20a%20new%20model%2C%20a%20multi-domain%20dataset%2C%20and%20a%0Achallenging%20benchmark%20for%20visual%20prompting.%20Specifically%2C%20we%20propose%20SPHINX-V%2C%0Aa%20new%20end-to-end%20trained%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20that%20connects%20a%0Avision%20encoder%2C%20a%20visual%20prompt%20encoder%20and%20an%20LLM%20for%20various%20visual%20prompts%0A%28points%2C%20bounding%20boxes%2C%20and%20free-form%20shape%29%20and%20language%20understanding.%20To%0Aadvance%20visual%20prompting%20research%20for%20MLLMs%2C%20we%20introduce%20MDVP-Data%20and%0AMDVP-Bench.%20MDVP-Data%20features%20a%20multi-domain%20dataset%20containing%201.6M%20unique%0Aimage-visual%20prompt-text%20instruction-following%20samples%2C%20including%20natural%0Aimages%2C%20document%20images%2C%20OCR%20images%2C%20mobile%20screenshots%2C%20web%20screenshots%2C%20and%0Amulti-panel%20images.%20Furthermore%2C%20we%20present%20MDVP-Bench%2C%20a%20comprehensive%20and%0Achallenging%20benchmark%20to%20assess%20a%20model%27s%20capability%20in%20understanding%20visual%0Aprompting%20instructions.%20Our%20experiments%20demonstrate%20SPHINX-V%27s%20impressive%0Amultimodal%20interaction%20capabilities%20through%20visual%20prompting%2C%20revealing%0Asignificant%20improvements%20in%20detailed%20pixel-level%20description%20and%0Aquestion-answering%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20271v1&entry.124074799=Read"},
{"title": "Latent Embedding Clustering for Occlusion Robust Head Pose Estimation", "author": "Jos\u00e9 Celestino and Manuel Marques and Jacinto C. Nascimento", "abstract": "  Head pose estimation has become a crucial area of research in computer vision\ngiven its usefulness in a wide range of applications, including robotics,\nsurveillance, or driver attention monitoring. One of the most difficult\nchallenges in this field is managing head occlusions that frequently take place\nin real-world scenarios. In this paper, we propose a novel and efficient\nframework that is robust in real world head occlusion scenarios. In particular,\nwe propose an unsupervised latent embedding clustering with regression and\nclassification components for each pose angle. The model optimizes latent\nfeature representations for occluded and non-occluded images through a\nclustering term while improving fine-grained angle predictions. Experimental\nevaluation on in-the-wild head pose benchmark datasets reveal competitive\nperformance in comparison to state-of-the-art methodologies with the advantage\nof having a significant data reduction. We observe a substantial improvement in\noccluded head pose estimation. Also, an ablation study is conducted to\nascertain the impact of the clustering term within our proposed framework.\n", "link": "http://arxiv.org/abs/2403.20251v1", "date": "2024-03-29", "relevancy": 1.5967, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5227}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Latent%20Embedding%20Clustering%20for%20Occlusion%20Robust%20Head%20Pose%20Estimation&body=Title%3A%20Latent%20Embedding%20Clustering%20for%20Occlusion%20Robust%20Head%20Pose%20Estimation%0AAuthor%3A%20Jos%C3%A9%20Celestino%20and%20Manuel%20Marques%20and%20Jacinto%20C.%20Nascimento%0AAbstract%3A%20%20%20Head%20pose%20estimation%20has%20become%20a%20crucial%20area%20of%20research%20in%20computer%20vision%0Agiven%20its%20usefulness%20in%20a%20wide%20range%20of%20applications%2C%20including%20robotics%2C%0Asurveillance%2C%20or%20driver%20attention%20monitoring.%20One%20of%20the%20most%20difficult%0Achallenges%20in%20this%20field%20is%20managing%20head%20occlusions%20that%20frequently%20take%20place%0Ain%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20efficient%0Aframework%20that%20is%20robust%20in%20real%20world%20head%20occlusion%20scenarios.%20In%20particular%2C%0Awe%20propose%20an%20unsupervised%20latent%20embedding%20clustering%20with%20regression%20and%0Aclassification%20components%20for%20each%20pose%20angle.%20The%20model%20optimizes%20latent%0Afeature%20representations%20for%20occluded%20and%20non-occluded%20images%20through%20a%0Aclustering%20term%20while%20improving%20fine-grained%20angle%20predictions.%20Experimental%0Aevaluation%20on%20in-the-wild%20head%20pose%20benchmark%20datasets%20reveal%20competitive%0Aperformance%20in%20comparison%20to%20state-of-the-art%20methodologies%20with%20the%20advantage%0Aof%20having%20a%20significant%20data%20reduction.%20We%20observe%20a%20substantial%20improvement%20in%0Aoccluded%20head%20pose%20estimation.%20Also%2C%20an%20ablation%20study%20is%20conducted%20to%0Aascertain%20the%20impact%20of%20the%20clustering%20term%20within%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20251v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Embedding%20Clustering%20for%20Occlusion%20Robust%20Head%20Pose%20Estimation&entry.906535625=Jos%C3%A9%20Celestino%20and%20Manuel%20Marques%20and%20Jacinto%20C.%20Nascimento&entry.1292438233=%20%20Head%20pose%20estimation%20has%20become%20a%20crucial%20area%20of%20research%20in%20computer%20vision%0Agiven%20its%20usefulness%20in%20a%20wide%20range%20of%20applications%2C%20including%20robotics%2C%0Asurveillance%2C%20or%20driver%20attention%20monitoring.%20One%20of%20the%20most%20difficult%0Achallenges%20in%20this%20field%20is%20managing%20head%20occlusions%20that%20frequently%20take%20place%0Ain%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20efficient%0Aframework%20that%20is%20robust%20in%20real%20world%20head%20occlusion%20scenarios.%20In%20particular%2C%0Awe%20propose%20an%20unsupervised%20latent%20embedding%20clustering%20with%20regression%20and%0Aclassification%20components%20for%20each%20pose%20angle.%20The%20model%20optimizes%20latent%0Afeature%20representations%20for%20occluded%20and%20non-occluded%20images%20through%20a%0Aclustering%20term%20while%20improving%20fine-grained%20angle%20predictions.%20Experimental%0Aevaluation%20on%20in-the-wild%20head%20pose%20benchmark%20datasets%20reveal%20competitive%0Aperformance%20in%20comparison%20to%20state-of-the-art%20methodologies%20with%20the%20advantage%0Aof%20having%20a%20significant%20data%20reduction.%20We%20observe%20a%20substantial%20improvement%20in%0Aoccluded%20head%20pose%20estimation.%20Also%2C%20an%20ablation%20study%20is%20conducted%20to%0Aascertain%20the%20impact%20of%20the%20clustering%20term%20within%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20251v1&entry.124074799=Read"},
{"title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems", "author": "Jiao Ou and Junda Lu and Che Liu and Yihong Tang and Fuzheng Zhang and Di Zhang and Kun Gai", "abstract": "  Large language models (LLMs) have achieved remarkable breakthroughs in new\ndialogue capabilities by leveraging instruction tuning, which refreshes human\nimpressions of dialogue systems. The long-standing goal of dialogue systems is\nto be human-like enough to establish long-term connections with users.\nTherefore, there has been an urgent need to evaluate LLMs as human-like\ndialogue systems. In this paper, we propose DialogBench, a dialogue evaluation\nbenchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as\nhuman-like dialogue systems should have. Specifically, we prompt GPT-4 to\ngenerate evaluation instances for each task. We first design the basic prompt\nbased on widely used design principles and further mitigate the existing biases\nto generate higher-quality evaluation instances. Our extensive tests on English\nand Chinese DialogBench of 26 LLMs show that instruction tuning improves the\nhuman likeness of LLMs to a certain extent, but most LLMs still have much room\nfor improvement as human-like dialogue systems. Interestingly, results also\nshow that the positioning of assistant AI can make instruction tuning weaken\nthe human emotional perception of LLMs and their mastery of information about\nhuman daily life.\n", "link": "http://arxiv.org/abs/2311.01677v2", "date": "2024-03-29", "relevancy": 1.3322, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4551}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DialogBench%3A%20Evaluating%20LLMs%20as%20Human-like%20Dialogue%20Systems&body=Title%3A%20DialogBench%3A%20Evaluating%20LLMs%20as%20Human-like%20Dialogue%20Systems%0AAuthor%3A%20Jiao%20Ou%20and%20Junda%20Lu%20and%20Che%20Liu%20and%20Yihong%20Tang%20and%20Fuzheng%20Zhang%20and%20Di%20Zhang%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20breakthroughs%20in%20new%0Adialogue%20capabilities%20by%20leveraging%20instruction%20tuning%2C%20which%20refreshes%20human%0Aimpressions%20of%20dialogue%20systems.%20The%20long-standing%20goal%20of%20dialogue%20systems%20is%0Ato%20be%20human-like%20enough%20to%20establish%20long-term%20connections%20with%20users.%0ATherefore%2C%20there%20has%20been%20an%20urgent%20need%20to%20evaluate%20LLMs%20as%20human-like%0Adialogue%20systems.%20In%20this%20paper%2C%20we%20propose%20DialogBench%2C%20a%20dialogue%20evaluation%0Abenchmark%20that%20contains%2012%20dialogue%20tasks%20to%20probe%20the%20capabilities%20of%20LLMs%20as%0Ahuman-like%20dialogue%20systems%20should%20have.%20Specifically%2C%20we%20prompt%20GPT-4%20to%0Agenerate%20evaluation%20instances%20for%20each%20task.%20We%20first%20design%20the%20basic%20prompt%0Abased%20on%20widely%20used%20design%20principles%20and%20further%20mitigate%20the%20existing%20biases%0Ato%20generate%20higher-quality%20evaluation%20instances.%20Our%20extensive%20tests%20on%20English%0Aand%20Chinese%20DialogBench%20of%2026%20LLMs%20show%20that%20instruction%20tuning%20improves%20the%0Ahuman%20likeness%20of%20LLMs%20to%20a%20certain%20extent%2C%20but%20most%20LLMs%20still%20have%20much%20room%0Afor%20improvement%20as%20human-like%20dialogue%20systems.%20Interestingly%2C%20results%20also%0Ashow%20that%20the%20positioning%20of%20assistant%20AI%20can%20make%20instruction%20tuning%20weaken%0Athe%20human%20emotional%20perception%20of%20LLMs%20and%20their%20mastery%20of%20information%20about%0Ahuman%20daily%20life.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01677v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialogBench%3A%20Evaluating%20LLMs%20as%20Human-like%20Dialogue%20Systems&entry.906535625=Jiao%20Ou%20and%20Junda%20Lu%20and%20Che%20Liu%20and%20Yihong%20Tang%20and%20Fuzheng%20Zhang%20and%20Di%20Zhang%20and%20Kun%20Gai&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20breakthroughs%20in%20new%0Adialogue%20capabilities%20by%20leveraging%20instruction%20tuning%2C%20which%20refreshes%20human%0Aimpressions%20of%20dialogue%20systems.%20The%20long-standing%20goal%20of%20dialogue%20systems%20is%0Ato%20be%20human-like%20enough%20to%20establish%20long-term%20connections%20with%20users.%0ATherefore%2C%20there%20has%20been%20an%20urgent%20need%20to%20evaluate%20LLMs%20as%20human-like%0Adialogue%20systems.%20In%20this%20paper%2C%20we%20propose%20DialogBench%2C%20a%20dialogue%20evaluation%0Abenchmark%20that%20contains%2012%20dialogue%20tasks%20to%20probe%20the%20capabilities%20of%20LLMs%20as%0Ahuman-like%20dialogue%20systems%20should%20have.%20Specifically%2C%20we%20prompt%20GPT-4%20to%0Agenerate%20evaluation%20instances%20for%20each%20task.%20We%20first%20design%20the%20basic%20prompt%0Abased%20on%20widely%20used%20design%20principles%20and%20further%20mitigate%20the%20existing%20biases%0Ato%20generate%20higher-quality%20evaluation%20instances.%20Our%20extensive%20tests%20on%20English%0Aand%20Chinese%20DialogBench%20of%2026%20LLMs%20show%20that%20instruction%20tuning%20improves%20the%0Ahuman%20likeness%20of%20LLMs%20to%20a%20certain%20extent%2C%20but%20most%20LLMs%20still%20have%20much%20room%0Afor%20improvement%20as%20human-like%20dialogue%20systems.%20Interestingly%2C%20results%20also%0Ashow%20that%20the%20positioning%20of%20assistant%20AI%20can%20make%20instruction%20tuning%20weaken%0Athe%20human%20emotional%20perception%20of%20LLMs%20and%20their%20mastery%20of%20information%20about%0Ahuman%20daily%20life.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01677v2&entry.124074799=Read"},
{"title": "Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation", "author": "Qi Bi and Shaodi You and Theo Gevers", "abstract": "  Images from outdoor scenes may be taken under various weather conditions. It\nis well studied that weather impacts the performance of computer vision\nalgorithms and needs to be handled properly. However, existing algorithms model\nweather condition as a discrete status and estimate it using multi-label\nclassification. The fact is that, physically, specifically in meteorology,\nweather are modeled as a continuous and transitional status. Instead of\ndirectly implementing hard classification as existing multi-weather\nclassification methods do, we consider the physical formulation of\nmulti-weather conditions and model the impact of physical-related parameter on\nlearning from the image appearance. In this paper, we start with solid revisit\nof the physics definition of weather and how it can be described as a\ncontinuous machine learning and computer vision task. Namely, we propose to\nmodel the weather uncertainty, where the level of probability and co-existence\nof multiple weather conditions are both considered. A Gaussian mixture model is\nused to encapsulate the weather uncertainty and a uncertainty-aware\nmulti-weather learning scheme is proposed based on prior-posterior learning. A\nnovel multi-weather co-presence estimation transformer (MeFormer) is proposed.\nIn addition, a new multi-weather co-presence estimation (MePe) dataset, along\nwith 14 fine-grained weather categories and 16,078 samples, is proposed to\nbenchmark both conventional multi-label weather classification task and\nmulti-weather co-presence estimation task. Large scale experiments show that\nthe proposed method achieves state-of-the-art performance and substantial\ngeneralization capabilities on both the conventional multi-label weather\nclassification task and the proposed multi-weather co-presence estimation task.\nBesides, modeling weather uncertainty also benefits adverse-weather semantic\nsegmentation.\n", "link": "http://arxiv.org/abs/2403.20092v1", "date": "2024-03-29", "relevancy": 1.1122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5758}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5465}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modeling%20Weather%20Uncertainty%20for%20Multi-weather%20Co-Presence%20Estimation&body=Title%3A%20Modeling%20Weather%20Uncertainty%20for%20Multi-weather%20Co-Presence%20Estimation%0AAuthor%3A%20Qi%20Bi%20and%20Shaodi%20You%20and%20Theo%20Gevers%0AAbstract%3A%20%20%20Images%20from%20outdoor%20scenes%20may%20be%20taken%20under%20various%20weather%20conditions.%20It%0Ais%20well%20studied%20that%20weather%20impacts%20the%20performance%20of%20computer%20vision%0Aalgorithms%20and%20needs%20to%20be%20handled%20properly.%20However%2C%20existing%20algorithms%20model%0Aweather%20condition%20as%20a%20discrete%20status%20and%20estimate%20it%20using%20multi-label%0Aclassification.%20The%20fact%20is%20that%2C%20physically%2C%20specifically%20in%20meteorology%2C%0Aweather%20are%20modeled%20as%20a%20continuous%20and%20transitional%20status.%20Instead%20of%0Adirectly%20implementing%20hard%20classification%20as%20existing%20multi-weather%0Aclassification%20methods%20do%2C%20we%20consider%20the%20physical%20formulation%20of%0Amulti-weather%20conditions%20and%20model%20the%20impact%20of%20physical-related%20parameter%20on%0Alearning%20from%20the%20image%20appearance.%20In%20this%20paper%2C%20we%20start%20with%20solid%20revisit%0Aof%20the%20physics%20definition%20of%20weather%20and%20how%20it%20can%20be%20described%20as%20a%0Acontinuous%20machine%20learning%20and%20computer%20vision%20task.%20Namely%2C%20we%20propose%20to%0Amodel%20the%20weather%20uncertainty%2C%20where%20the%20level%20of%20probability%20and%20co-existence%0Aof%20multiple%20weather%20conditions%20are%20both%20considered.%20A%20Gaussian%20mixture%20model%20is%0Aused%20to%20encapsulate%20the%20weather%20uncertainty%20and%20a%20uncertainty-aware%0Amulti-weather%20learning%20scheme%20is%20proposed%20based%20on%20prior-posterior%20learning.%20A%0Anovel%20multi-weather%20co-presence%20estimation%20transformer%20%28MeFormer%29%20is%20proposed.%0AIn%20addition%2C%20a%20new%20multi-weather%20co-presence%20estimation%20%28MePe%29%20dataset%2C%20along%0Awith%2014%20fine-grained%20weather%20categories%20and%2016%2C078%20samples%2C%20is%20proposed%20to%0Abenchmark%20both%20conventional%20multi-label%20weather%20classification%20task%20and%0Amulti-weather%20co-presence%20estimation%20task.%20Large%20scale%20experiments%20show%20that%0Athe%20proposed%20method%20achieves%20state-of-the-art%20performance%20and%20substantial%0Ageneralization%20capabilities%20on%20both%20the%20conventional%20multi-label%20weather%0Aclassification%20task%20and%20the%20proposed%20multi-weather%20co-presence%20estimation%20task.%0ABesides%2C%20modeling%20weather%20uncertainty%20also%20benefits%20adverse-weather%20semantic%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20092v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Weather%20Uncertainty%20for%20Multi-weather%20Co-Presence%20Estimation&entry.906535625=Qi%20Bi%20and%20Shaodi%20You%20and%20Theo%20Gevers&entry.1292438233=%20%20Images%20from%20outdoor%20scenes%20may%20be%20taken%20under%20various%20weather%20conditions.%20It%0Ais%20well%20studied%20that%20weather%20impacts%20the%20performance%20of%20computer%20vision%0Aalgorithms%20and%20needs%20to%20be%20handled%20properly.%20However%2C%20existing%20algorithms%20model%0Aweather%20condition%20as%20a%20discrete%20status%20and%20estimate%20it%20using%20multi-label%0Aclassification.%20The%20fact%20is%20that%2C%20physically%2C%20specifically%20in%20meteorology%2C%0Aweather%20are%20modeled%20as%20a%20continuous%20and%20transitional%20status.%20Instead%20of%0Adirectly%20implementing%20hard%20classification%20as%20existing%20multi-weather%0Aclassification%20methods%20do%2C%20we%20consider%20the%20physical%20formulation%20of%0Amulti-weather%20conditions%20and%20model%20the%20impact%20of%20physical-related%20parameter%20on%0Alearning%20from%20the%20image%20appearance.%20In%20this%20paper%2C%20we%20start%20with%20solid%20revisit%0Aof%20the%20physics%20definition%20of%20weather%20and%20how%20it%20can%20be%20described%20as%20a%0Acontinuous%20machine%20learning%20and%20computer%20vision%20task.%20Namely%2C%20we%20propose%20to%0Amodel%20the%20weather%20uncertainty%2C%20where%20the%20level%20of%20probability%20and%20co-existence%0Aof%20multiple%20weather%20conditions%20are%20both%20considered.%20A%20Gaussian%20mixture%20model%20is%0Aused%20to%20encapsulate%20the%20weather%20uncertainty%20and%20a%20uncertainty-aware%0Amulti-weather%20learning%20scheme%20is%20proposed%20based%20on%20prior-posterior%20learning.%20A%0Anovel%20multi-weather%20co-presence%20estimation%20transformer%20%28MeFormer%29%20is%20proposed.%0AIn%20addition%2C%20a%20new%20multi-weather%20co-presence%20estimation%20%28MePe%29%20dataset%2C%20along%0Awith%2014%20fine-grained%20weather%20categories%20and%2016%2C078%20samples%2C%20is%20proposed%20to%0Abenchmark%20both%20conventional%20multi-label%20weather%20classification%20task%20and%0Amulti-weather%20co-presence%20estimation%20task.%20Large%20scale%20experiments%20show%20that%0Athe%20proposed%20method%20achieves%20state-of-the-art%20performance%20and%20substantial%0Ageneralization%20capabilities%20on%20both%20the%20conventional%20multi-label%20weather%0Aclassification%20task%20and%20the%20proposed%20multi-weather%20co-presence%20estimation%20task.%0ABesides%2C%20modeling%20weather%20uncertainty%20also%20benefits%20adverse-weather%20semantic%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20092v1&entry.124074799=Read"},
{"title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and\n  200+ FPS", "author": "Zhiwen Fan and Kevin Wang and Kairun Wen and Zehao Zhu and Dejia Xu and Zhangyang Wang", "abstract": "  Recent advancements in real-time neural rendering using point-based\ntechniques have paved the way for the widespread adoption of 3D\nrepresentations. However, foundational approaches like 3D Gaussian Splatting\ncome with a substantial storage overhead caused by growing the SfM points to\nmillions, often demanding gigabyte-level disk space for a single unbounded\nscene, posing significant scalability challenges and hindering the splatting\nefficiency.\n  To address this challenge, we introduce LightGaussian, a novel method\ndesigned to transform 3D Gaussians into a more efficient and compact format.\nDrawing inspiration from the concept of Network Pruning, LightGaussian\nidentifies Gaussians that are insignificant in contributing to the scene\nreconstruction and adopts a pruning and recovery process, effectively reducing\nredundancy in Gaussian counts while preserving visual effects. Additionally,\nLightGaussian employs distillation and pseudo-view augmentation to distill\nspherical harmonics to a lower degree, allowing knowledge transfer to more\ncompact representations while maintaining reflectance. Furthermore, we propose\na hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in\nlower bitwidth representations with minimal accuracy losses.\n  In summary, LightGaussian achieves an averaged compression rate over 15x\nwhile boosting the FPS from 139 to 215, enabling an efficient representation of\ncomplex scenes on Mip-NeRF 360, Tank and Temple datasets.\n  Project website: https://lightgaussian.github.io/\n", "link": "http://arxiv.org/abs/2311.17245v5", "date": "2024-03-29", "relevancy": 1.5777, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.534}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS&body=Title%3A%20LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS%0AAuthor%3A%20Zhiwen%20Fan%20and%20Kevin%20Wang%20and%20Kairun%20Wen%20and%20Zehao%20Zhu%20and%20Dejia%20Xu%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20real-time%20neural%20rendering%20using%20point-based%0Atechniques%20have%20paved%20the%20way%20for%20the%20widespread%20adoption%20of%203D%0Arepresentations.%20However%2C%20foundational%20approaches%20like%203D%20Gaussian%20Splatting%0Acome%20with%20a%20substantial%20storage%20overhead%20caused%20by%20growing%20the%20SfM%20points%20to%0Amillions%2C%20often%20demanding%20gigabyte-level%20disk%20space%20for%20a%20single%20unbounded%0Ascene%2C%20posing%20significant%20scalability%20challenges%20and%20hindering%20the%20splatting%0Aefficiency.%0A%20%20To%20address%20this%20challenge%2C%20we%20introduce%20LightGaussian%2C%20a%20novel%20method%0Adesigned%20to%20transform%203D%20Gaussians%20into%20a%20more%20efficient%20and%20compact%20format.%0ADrawing%20inspiration%20from%20the%20concept%20of%20Network%20Pruning%2C%20LightGaussian%0Aidentifies%20Gaussians%20that%20are%20insignificant%20in%20contributing%20to%20the%20scene%0Areconstruction%20and%20adopts%20a%20pruning%20and%20recovery%20process%2C%20effectively%20reducing%0Aredundancy%20in%20Gaussian%20counts%20while%20preserving%20visual%20effects.%20Additionally%2C%0ALightGaussian%20employs%20distillation%20and%20pseudo-view%20augmentation%20to%20distill%0Aspherical%20harmonics%20to%20a%20lower%20degree%2C%20allowing%20knowledge%20transfer%20to%20more%0Acompact%20representations%20while%20maintaining%20reflectance.%20Furthermore%2C%20we%20propose%0Aa%20hybrid%20scheme%2C%20VecTree%20Quantization%2C%20to%20quantize%20all%20attributes%2C%20resulting%20in%0Alower%20bitwidth%20representations%20with%20minimal%20accuracy%20losses.%0A%20%20In%20summary%2C%20LightGaussian%20achieves%20an%20averaged%20compression%20rate%20over%2015x%0Awhile%20boosting%20the%20FPS%20from%20139%20to%20215%2C%20enabling%20an%20efficient%20representation%20of%0Acomplex%20scenes%20on%20Mip-NeRF%20360%2C%20Tank%20and%20Temple%20datasets.%0A%20%20Project%20website%3A%20https%3A//lightgaussian.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17245v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS&entry.906535625=Zhiwen%20Fan%20and%20Kevin%20Wang%20and%20Kairun%20Wen%20and%20Zehao%20Zhu%20and%20Dejia%20Xu%20and%20Zhangyang%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20real-time%20neural%20rendering%20using%20point-based%0Atechniques%20have%20paved%20the%20way%20for%20the%20widespread%20adoption%20of%203D%0Arepresentations.%20However%2C%20foundational%20approaches%20like%203D%20Gaussian%20Splatting%0Acome%20with%20a%20substantial%20storage%20overhead%20caused%20by%20growing%20the%20SfM%20points%20to%0Amillions%2C%20often%20demanding%20gigabyte-level%20disk%20space%20for%20a%20single%20unbounded%0Ascene%2C%20posing%20significant%20scalability%20challenges%20and%20hindering%20the%20splatting%0Aefficiency.%0A%20%20To%20address%20this%20challenge%2C%20we%20introduce%20LightGaussian%2C%20a%20novel%20method%0Adesigned%20to%20transform%203D%20Gaussians%20into%20a%20more%20efficient%20and%20compact%20format.%0ADrawing%20inspiration%20from%20the%20concept%20of%20Network%20Pruning%2C%20LightGaussian%0Aidentifies%20Gaussians%20that%20are%20insignificant%20in%20contributing%20to%20the%20scene%0Areconstruction%20and%20adopts%20a%20pruning%20and%20recovery%20process%2C%20effectively%20reducing%0Aredundancy%20in%20Gaussian%20counts%20while%20preserving%20visual%20effects.%20Additionally%2C%0ALightGaussian%20employs%20distillation%20and%20pseudo-view%20augmentation%20to%20distill%0Aspherical%20harmonics%20to%20a%20lower%20degree%2C%20allowing%20knowledge%20transfer%20to%20more%0Acompact%20representations%20while%20maintaining%20reflectance.%20Furthermore%2C%20we%20propose%0Aa%20hybrid%20scheme%2C%20VecTree%20Quantization%2C%20to%20quantize%20all%20attributes%2C%20resulting%20in%0Alower%20bitwidth%20representations%20with%20minimal%20accuracy%20losses.%0A%20%20In%20summary%2C%20LightGaussian%20achieves%20an%20averaged%20compression%20rate%20over%2015x%0Awhile%20boosting%20the%20FPS%20from%20139%20to%20215%2C%20enabling%20an%20efficient%20representation%20of%0Acomplex%20scenes%20on%20Mip-NeRF%20360%2C%20Tank%20and%20Temple%20datasets.%0A%20%20Project%20website%3A%20https%3A//lightgaussian.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17245v5&entry.124074799=Read"},
{"title": "Prototype-based Interpretable Breast Cancer Prediction Models: Analysis\n  and Challenges", "author": "Shreyasi Pathak and J\u00f6rg Schl\u00f6tterer and Jeroen Veltman and Jeroen Geerdink and Maurice van Keulen and Christin Seifert", "abstract": "  Deep learning models have achieved high performance in medical applications,\nhowever, their adoption in clinical practice is hindered due to their black-box\nnature. Self-explainable models, like prototype-based models, can be especially\nbeneficial as they are interpretable by design. However, if the learnt\nprototypes are of low quality then the prototype-based models are as good as\nblack-box. Having high quality prototypes is a pre-requisite for a truly\ninterpretable model. In this work, we propose a prototype evaluation framework\nfor coherence (PEF-C) for quantitatively evaluating the quality of the\nprototypes based on domain knowledge. We show the use of PEF-C in the context\nof breast cancer prediction using mammography. Existing works on\nprototype-based models on breast cancer prediction using mammography have\nfocused on improving the classification performance of prototype-based models\ncompared to black-box models and have evaluated prototype quality through\nanecdotal evidence. We are the first to go beyond anecdotal evidence and\nevaluate the quality of the mammography prototypes systematically using our\nPEF-C. Specifically, we apply three state-of-the-art prototype-based models,\nProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer\nprediction and evaluate these models w.r.t. i) classification performance, and\nii) quality of the prototypes, on three public datasets. Our results show that\nprototype-based models are competitive with black-box models in terms of\nclassification performance, and achieve a higher score in detecting ROIs.\nHowever, the quality of the prototypes are not yet sufficient and can be\nimproved in aspects of relevance, purity and learning a variety of prototypes.\nWe call the XAI community to systematically evaluate the quality of the\nprototypes to check their true usability in high stake decisions and improve\nsuch models further.\n", "link": "http://arxiv.org/abs/2403.20260v1", "date": "2024-03-29", "relevancy": 0.9776, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4872}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4782}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prototype-based%20Interpretable%20Breast%20Cancer%20Prediction%20Models%3A%20Analysis%0A%20%20and%20Challenges&body=Title%3A%20Prototype-based%20Interpretable%20Breast%20Cancer%20Prediction%20Models%3A%20Analysis%0A%20%20and%20Challenges%0AAuthor%3A%20Shreyasi%20Pathak%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Jeroen%20Veltman%20and%20Jeroen%20Geerdink%20and%20Maurice%20van%20Keulen%20and%20Christin%20Seifert%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20achieved%20high%20performance%20in%20medical%20applications%2C%0Ahowever%2C%20their%20adoption%20in%20clinical%20practice%20is%20hindered%20due%20to%20their%20black-box%0Anature.%20Self-explainable%20models%2C%20like%20prototype-based%20models%2C%20can%20be%20especially%0Abeneficial%20as%20they%20are%20interpretable%20by%20design.%20However%2C%20if%20the%20learnt%0Aprototypes%20are%20of%20low%20quality%20then%20the%20prototype-based%20models%20are%20as%20good%20as%0Ablack-box.%20Having%20high%20quality%20prototypes%20is%20a%20pre-requisite%20for%20a%20truly%0Ainterpretable%20model.%20In%20this%20work%2C%20we%20propose%20a%20prototype%20evaluation%20framework%0Afor%20coherence%20%28PEF-C%29%20for%20quantitatively%20evaluating%20the%20quality%20of%20the%0Aprototypes%20based%20on%20domain%20knowledge.%20We%20show%20the%20use%20of%20PEF-C%20in%20the%20context%0Aof%20breast%20cancer%20prediction%20using%20mammography.%20Existing%20works%20on%0Aprototype-based%20models%20on%20breast%20cancer%20prediction%20using%20mammography%20have%0Afocused%20on%20improving%20the%20classification%20performance%20of%20prototype-based%20models%0Acompared%20to%20black-box%20models%20and%20have%20evaluated%20prototype%20quality%20through%0Aanecdotal%20evidence.%20We%20are%20the%20first%20to%20go%20beyond%20anecdotal%20evidence%20and%0Aevaluate%20the%20quality%20of%20the%20mammography%20prototypes%20systematically%20using%20our%0APEF-C.%20Specifically%2C%20we%20apply%20three%20state-of-the-art%20prototype-based%20models%2C%0AProtoPNet%2C%20BRAIxProtoPNet%2B%2B%20and%20PIP-Net%20on%20mammography%20images%20for%20breast%20cancer%0Aprediction%20and%20evaluate%20these%20models%20w.r.t.%20i%29%20classification%20performance%2C%20and%0Aii%29%20quality%20of%20the%20prototypes%2C%20on%20three%20public%20datasets.%20Our%20results%20show%20that%0Aprototype-based%20models%20are%20competitive%20with%20black-box%20models%20in%20terms%20of%0Aclassification%20performance%2C%20and%20achieve%20a%20higher%20score%20in%20detecting%20ROIs.%0AHowever%2C%20the%20quality%20of%20the%20prototypes%20are%20not%20yet%20sufficient%20and%20can%20be%0Aimproved%20in%20aspects%20of%20relevance%2C%20purity%20and%20learning%20a%20variety%20of%20prototypes.%0AWe%20call%20the%20XAI%20community%20to%20systematically%20evaluate%20the%20quality%20of%20the%0Aprototypes%20to%20check%20their%20true%20usability%20in%20high%20stake%20decisions%20and%20improve%0Asuch%20models%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20260v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype-based%20Interpretable%20Breast%20Cancer%20Prediction%20Models%3A%20Analysis%0A%20%20and%20Challenges&entry.906535625=Shreyasi%20Pathak%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Jeroen%20Veltman%20and%20Jeroen%20Geerdink%20and%20Maurice%20van%20Keulen%20and%20Christin%20Seifert&entry.1292438233=%20%20Deep%20learning%20models%20have%20achieved%20high%20performance%20in%20medical%20applications%2C%0Ahowever%2C%20their%20adoption%20in%20clinical%20practice%20is%20hindered%20due%20to%20their%20black-box%0Anature.%20Self-explainable%20models%2C%20like%20prototype-based%20models%2C%20can%20be%20especially%0Abeneficial%20as%20they%20are%20interpretable%20by%20design.%20However%2C%20if%20the%20learnt%0Aprototypes%20are%20of%20low%20quality%20then%20the%20prototype-based%20models%20are%20as%20good%20as%0Ablack-box.%20Having%20high%20quality%20prototypes%20is%20a%20pre-requisite%20for%20a%20truly%0Ainterpretable%20model.%20In%20this%20work%2C%20we%20propose%20a%20prototype%20evaluation%20framework%0Afor%20coherence%20%28PEF-C%29%20for%20quantitatively%20evaluating%20the%20quality%20of%20the%0Aprototypes%20based%20on%20domain%20knowledge.%20We%20show%20the%20use%20of%20PEF-C%20in%20the%20context%0Aof%20breast%20cancer%20prediction%20using%20mammography.%20Existing%20works%20on%0Aprototype-based%20models%20on%20breast%20cancer%20prediction%20using%20mammography%20have%0Afocused%20on%20improving%20the%20classification%20performance%20of%20prototype-based%20models%0Acompared%20to%20black-box%20models%20and%20have%20evaluated%20prototype%20quality%20through%0Aanecdotal%20evidence.%20We%20are%20the%20first%20to%20go%20beyond%20anecdotal%20evidence%20and%0Aevaluate%20the%20quality%20of%20the%20mammography%20prototypes%20systematically%20using%20our%0APEF-C.%20Specifically%2C%20we%20apply%20three%20state-of-the-art%20prototype-based%20models%2C%0AProtoPNet%2C%20BRAIxProtoPNet%2B%2B%20and%20PIP-Net%20on%20mammography%20images%20for%20breast%20cancer%0Aprediction%20and%20evaluate%20these%20models%20w.r.t.%20i%29%20classification%20performance%2C%20and%0Aii%29%20quality%20of%20the%20prototypes%2C%20on%20three%20public%20datasets.%20Our%20results%20show%20that%0Aprototype-based%20models%20are%20competitive%20with%20black-box%20models%20in%20terms%20of%0Aclassification%20performance%2C%20and%20achieve%20a%20higher%20score%20in%20detecting%20ROIs.%0AHowever%2C%20the%20quality%20of%20the%20prototypes%20are%20not%20yet%20sufficient%20and%20can%20be%0Aimproved%20in%20aspects%20of%20relevance%2C%20purity%20and%20learning%20a%20variety%20of%20prototypes.%0AWe%20call%20the%20XAI%20community%20to%20systematically%20evaluate%20the%20quality%20of%20the%0Aprototypes%20to%20check%20their%20true%20usability%20in%20high%20stake%20decisions%20and%20improve%0Asuch%20models%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20260v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


