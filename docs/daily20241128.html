<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241127.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models", "author": "Rundi Wu and Ruiqi Gao and Ben Poole and Alex Trevithick and Changxi Zheng and Jonathan T. Barron and Aleksander Holynski", "abstract": "  We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular\nvideo. CAT4D leverages a multi-view video diffusion model trained on a diverse\ncombination of datasets to enable novel view synthesis at any specified camera\nposes and timestamps. Combined with a novel sampling approach, this model can\ntransform a single monocular video into a multi-view video, enabling robust 4D\nreconstruction via optimization of a deformable 3D Gaussian representation. We\ndemonstrate competitive performance on novel view synthesis and dynamic scene\nreconstruction benchmarks, and highlight the creative capabilities for 4D scene\ngeneration from real or generated videos. See our project page for results and\ninteractive demos: \\url{cat-4d.github.io}.\n", "link": "http://arxiv.org/abs/2411.18613v1", "date": "2024-11-27", "relevancy": 3.8776, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.8585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.8585}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT4D%3A%20Create%20Anything%20in%204D%20with%20Multi-View%20Video%20Diffusion%20Models&body=Title%3A%20CAT4D%3A%20Create%20Anything%20in%204D%20with%20Multi-View%20Video%20Diffusion%20Models%0AAuthor%3A%20Rundi%20Wu%20and%20Ruiqi%20Gao%20and%20Ben%20Poole%20and%20Alex%20Trevithick%20and%20Changxi%20Zheng%20and%20Jonathan%20T.%20Barron%20and%20Aleksander%20Holynski%0AAbstract%3A%20%20%20We%20present%20CAT4D%2C%20a%20method%20for%20creating%204D%20%28dynamic%203D%29%20scenes%20from%20monocular%0Avideo.%20CAT4D%20leverages%20a%20multi-view%20video%20diffusion%20model%20trained%20on%20a%20diverse%0Acombination%20of%20datasets%20to%20enable%20novel%20view%20synthesis%20at%20any%20specified%20camera%0Aposes%20and%20timestamps.%20Combined%20with%20a%20novel%20sampling%20approach%2C%20this%20model%20can%0Atransform%20a%20single%20monocular%20video%20into%20a%20multi-view%20video%2C%20enabling%20robust%204D%0Areconstruction%20via%20optimization%20of%20a%20deformable%203D%20Gaussian%20representation.%20We%0Ademonstrate%20competitive%20performance%20on%20novel%20view%20synthesis%20and%20dynamic%20scene%0Areconstruction%20benchmarks%2C%20and%20highlight%20the%20creative%20capabilities%20for%204D%20scene%0Ageneration%20from%20real%20or%20generated%20videos.%20See%20our%20project%20page%20for%20results%20and%0Ainteractive%20demos%3A%20%5Curl%7Bcat-4d.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT4D%253A%2520Create%2520Anything%2520in%25204D%2520with%2520Multi-View%2520Video%2520Diffusion%2520Models%26entry.906535625%3DRundi%2520Wu%2520and%2520Ruiqi%2520Gao%2520and%2520Ben%2520Poole%2520and%2520Alex%2520Trevithick%2520and%2520Changxi%2520Zheng%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Aleksander%2520Holynski%26entry.1292438233%3D%2520%2520We%2520present%2520CAT4D%252C%2520a%2520method%2520for%2520creating%25204D%2520%2528dynamic%25203D%2529%2520scenes%2520from%2520monocular%250Avideo.%2520CAT4D%2520leverages%2520a%2520multi-view%2520video%2520diffusion%2520model%2520trained%2520on%2520a%2520diverse%250Acombination%2520of%2520datasets%2520to%2520enable%2520novel%2520view%2520synthesis%2520at%2520any%2520specified%2520camera%250Aposes%2520and%2520timestamps.%2520Combined%2520with%2520a%2520novel%2520sampling%2520approach%252C%2520this%2520model%2520can%250Atransform%2520a%2520single%2520monocular%2520video%2520into%2520a%2520multi-view%2520video%252C%2520enabling%2520robust%25204D%250Areconstruction%2520via%2520optimization%2520of%2520a%2520deformable%25203D%2520Gaussian%2520representation.%2520We%250Ademonstrate%2520competitive%2520performance%2520on%2520novel%2520view%2520synthesis%2520and%2520dynamic%2520scene%250Areconstruction%2520benchmarks%252C%2520and%2520highlight%2520the%2520creative%2520capabilities%2520for%25204D%2520scene%250Ageneration%2520from%2520real%2520or%2520generated%2520videos.%2520See%2520our%2520project%2520page%2520for%2520results%2520and%250Ainteractive%2520demos%253A%2520%255Curl%257Bcat-4d.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT4D%3A%20Create%20Anything%20in%204D%20with%20Multi-View%20Video%20Diffusion%20Models&entry.906535625=Rundi%20Wu%20and%20Ruiqi%20Gao%20and%20Ben%20Poole%20and%20Alex%20Trevithick%20and%20Changxi%20Zheng%20and%20Jonathan%20T.%20Barron%20and%20Aleksander%20Holynski&entry.1292438233=%20%20We%20present%20CAT4D%2C%20a%20method%20for%20creating%204D%20%28dynamic%203D%29%20scenes%20from%20monocular%0Avideo.%20CAT4D%20leverages%20a%20multi-view%20video%20diffusion%20model%20trained%20on%20a%20diverse%0Acombination%20of%20datasets%20to%20enable%20novel%20view%20synthesis%20at%20any%20specified%20camera%0Aposes%20and%20timestamps.%20Combined%20with%20a%20novel%20sampling%20approach%2C%20this%20model%20can%0Atransform%20a%20single%20monocular%20video%20into%20a%20multi-view%20video%2C%20enabling%20robust%204D%0Areconstruction%20via%20optimization%20of%20a%20deformable%203D%20Gaussian%20representation.%20We%0Ademonstrate%20competitive%20performance%20on%20novel%20view%20synthesis%20and%20dynamic%20scene%0Areconstruction%20benchmarks%2C%20and%20highlight%20the%20creative%20capabilities%20for%204D%20scene%0Ageneration%20from%20real%20or%20generated%20videos.%20See%20our%20project%20page%20for%20results%20and%0Ainteractive%20demos%3A%20%5Curl%7Bcat-4d.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18613v1&entry.124074799=Read"},
{"title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling", "author": "Brian Chao and Hung-Yu Tseng and Lorenzo Porzi and Chen Gao and Tuotuo Li and Qinbo Li and Ayush Saraf and Jia-Bin Huang and Johannes Kopf and Gordon Wetzstein and Changil Kim", "abstract": "  3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D\nreconstruction and rendering technique due to its high-quality results and fast\ntraining and rendering time. However, pixels covered by the same Gaussian are\nalways shaded in the same color up to a Gaussian falloff scaling factor.\nFurthermore, the finest geometric detail any individual Gaussian can represent\nis a simple ellipsoid. These properties of 3DGS greatly limit the expressivity\nof individual Gaussian primitives. To address these issues, we draw inspiration\nfrom texture and alpha mapping in traditional graphics and integrate it with\n3DGS. Specifically, we propose a new generalized Gaussian appearance\nrepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texture\nmaps to model spatially varying color and opacity across the extent of each\nGaussian. As such, each Gaussian can represent a richer set of texture patterns\nand geometric structures, instead of just a single color and ellipsoid as in\nnaive Gaussian Splatting. Surprisingly, we found that the expressivity of\nGaussians can be greatly improved by using alpha-only texture maps, and further\naugmenting Gaussians with RGB texture maps achieves the highest expressivity.\nWe validate our method on a wide variety of standard benchmark datasets and our\nown custom captures at both the object and scene levels. We demonstrate image\nquality improvements over existing methods while using a similar or lower\nnumber of Gaussians.\n", "link": "http://arxiv.org/abs/2411.18625v1", "date": "2024-11-27", "relevancy": 3.4596, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.704}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6936}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textured%20Gaussians%20for%20Enhanced%203D%20Scene%20Appearance%20Modeling&body=Title%3A%20Textured%20Gaussians%20for%20Enhanced%203D%20Scene%20Appearance%20Modeling%0AAuthor%3A%20Brian%20Chao%20and%20Hung-Yu%20Tseng%20and%20Lorenzo%20Porzi%20and%20Chen%20Gao%20and%20Tuotuo%20Li%20and%20Qinbo%20Li%20and%20Ayush%20Saraf%20and%20Jia-Bin%20Huang%20and%20Johannes%20Kopf%20and%20Gordon%20Wetzstein%20and%20Changil%20Kim%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20state-of-the-art%203D%0Areconstruction%20and%20rendering%20technique%20due%20to%20its%20high-quality%20results%20and%20fast%0Atraining%20and%20rendering%20time.%20However%2C%20pixels%20covered%20by%20the%20same%20Gaussian%20are%0Aalways%20shaded%20in%20the%20same%20color%20up%20to%20a%20Gaussian%20falloff%20scaling%20factor.%0AFurthermore%2C%20the%20finest%20geometric%20detail%20any%20individual%20Gaussian%20can%20represent%0Ais%20a%20simple%20ellipsoid.%20These%20properties%20of%203DGS%20greatly%20limit%20the%20expressivity%0Aof%20individual%20Gaussian%20primitives.%20To%20address%20these%20issues%2C%20we%20draw%20inspiration%0Afrom%20texture%20and%20alpha%20mapping%20in%20traditional%20graphics%20and%20integrate%20it%20with%0A3DGS.%20Specifically%2C%20we%20propose%20a%20new%20generalized%20Gaussian%20appearance%0Arepresentation%20that%20augments%20each%20Gaussian%20with%20alpha~%28A%29%2C%20RGB%2C%20or%20RGBA%20texture%0Amaps%20to%20model%20spatially%20varying%20color%20and%20opacity%20across%20the%20extent%20of%20each%0AGaussian.%20As%20such%2C%20each%20Gaussian%20can%20represent%20a%20richer%20set%20of%20texture%20patterns%0Aand%20geometric%20structures%2C%20instead%20of%20just%20a%20single%20color%20and%20ellipsoid%20as%20in%0Anaive%20Gaussian%20Splatting.%20Surprisingly%2C%20we%20found%20that%20the%20expressivity%20of%0AGaussians%20can%20be%20greatly%20improved%20by%20using%20alpha-only%20texture%20maps%2C%20and%20further%0Aaugmenting%20Gaussians%20with%20RGB%20texture%20maps%20achieves%20the%20highest%20expressivity.%0AWe%20validate%20our%20method%20on%20a%20wide%20variety%20of%20standard%20benchmark%20datasets%20and%20our%0Aown%20custom%20captures%20at%20both%20the%20object%20and%20scene%20levels.%20We%20demonstrate%20image%0Aquality%20improvements%20over%20existing%20methods%20while%20using%20a%20similar%20or%20lower%0Anumber%20of%20Gaussians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextured%2520Gaussians%2520for%2520Enhanced%25203D%2520Scene%2520Appearance%2520Modeling%26entry.906535625%3DBrian%2520Chao%2520and%2520Hung-Yu%2520Tseng%2520and%2520Lorenzo%2520Porzi%2520and%2520Chen%2520Gao%2520and%2520Tuotuo%2520Li%2520and%2520Qinbo%2520Li%2520and%2520Ayush%2520Saraf%2520and%2520Jia-Bin%2520Huang%2520and%2520Johannes%2520Kopf%2520and%2520Gordon%2520Wetzstein%2520and%2520Changil%2520Kim%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520state-of-the-art%25203D%250Areconstruction%2520and%2520rendering%2520technique%2520due%2520to%2520its%2520high-quality%2520results%2520and%2520fast%250Atraining%2520and%2520rendering%2520time.%2520However%252C%2520pixels%2520covered%2520by%2520the%2520same%2520Gaussian%2520are%250Aalways%2520shaded%2520in%2520the%2520same%2520color%2520up%2520to%2520a%2520Gaussian%2520falloff%2520scaling%2520factor.%250AFurthermore%252C%2520the%2520finest%2520geometric%2520detail%2520any%2520individual%2520Gaussian%2520can%2520represent%250Ais%2520a%2520simple%2520ellipsoid.%2520These%2520properties%2520of%25203DGS%2520greatly%2520limit%2520the%2520expressivity%250Aof%2520individual%2520Gaussian%2520primitives.%2520To%2520address%2520these%2520issues%252C%2520we%2520draw%2520inspiration%250Afrom%2520texture%2520and%2520alpha%2520mapping%2520in%2520traditional%2520graphics%2520and%2520integrate%2520it%2520with%250A3DGS.%2520Specifically%252C%2520we%2520propose%2520a%2520new%2520generalized%2520Gaussian%2520appearance%250Arepresentation%2520that%2520augments%2520each%2520Gaussian%2520with%2520alpha~%2528A%2529%252C%2520RGB%252C%2520or%2520RGBA%2520texture%250Amaps%2520to%2520model%2520spatially%2520varying%2520color%2520and%2520opacity%2520across%2520the%2520extent%2520of%2520each%250AGaussian.%2520As%2520such%252C%2520each%2520Gaussian%2520can%2520represent%2520a%2520richer%2520set%2520of%2520texture%2520patterns%250Aand%2520geometric%2520structures%252C%2520instead%2520of%2520just%2520a%2520single%2520color%2520and%2520ellipsoid%2520as%2520in%250Anaive%2520Gaussian%2520Splatting.%2520Surprisingly%252C%2520we%2520found%2520that%2520the%2520expressivity%2520of%250AGaussians%2520can%2520be%2520greatly%2520improved%2520by%2520using%2520alpha-only%2520texture%2520maps%252C%2520and%2520further%250Aaugmenting%2520Gaussians%2520with%2520RGB%2520texture%2520maps%2520achieves%2520the%2520highest%2520expressivity.%250AWe%2520validate%2520our%2520method%2520on%2520a%2520wide%2520variety%2520of%2520standard%2520benchmark%2520datasets%2520and%2520our%250Aown%2520custom%2520captures%2520at%2520both%2520the%2520object%2520and%2520scene%2520levels.%2520We%2520demonstrate%2520image%250Aquality%2520improvements%2520over%2520existing%2520methods%2520while%2520using%2520a%2520similar%2520or%2520lower%250Anumber%2520of%2520Gaussians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textured%20Gaussians%20for%20Enhanced%203D%20Scene%20Appearance%20Modeling&entry.906535625=Brian%20Chao%20and%20Hung-Yu%20Tseng%20and%20Lorenzo%20Porzi%20and%20Chen%20Gao%20and%20Tuotuo%20Li%20and%20Qinbo%20Li%20and%20Ayush%20Saraf%20and%20Jia-Bin%20Huang%20and%20Johannes%20Kopf%20and%20Gordon%20Wetzstein%20and%20Changil%20Kim&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20state-of-the-art%203D%0Areconstruction%20and%20rendering%20technique%20due%20to%20its%20high-quality%20results%20and%20fast%0Atraining%20and%20rendering%20time.%20However%2C%20pixels%20covered%20by%20the%20same%20Gaussian%20are%0Aalways%20shaded%20in%20the%20same%20color%20up%20to%20a%20Gaussian%20falloff%20scaling%20factor.%0AFurthermore%2C%20the%20finest%20geometric%20detail%20any%20individual%20Gaussian%20can%20represent%0Ais%20a%20simple%20ellipsoid.%20These%20properties%20of%203DGS%20greatly%20limit%20the%20expressivity%0Aof%20individual%20Gaussian%20primitives.%20To%20address%20these%20issues%2C%20we%20draw%20inspiration%0Afrom%20texture%20and%20alpha%20mapping%20in%20traditional%20graphics%20and%20integrate%20it%20with%0A3DGS.%20Specifically%2C%20we%20propose%20a%20new%20generalized%20Gaussian%20appearance%0Arepresentation%20that%20augments%20each%20Gaussian%20with%20alpha~%28A%29%2C%20RGB%2C%20or%20RGBA%20texture%0Amaps%20to%20model%20spatially%20varying%20color%20and%20opacity%20across%20the%20extent%20of%20each%0AGaussian.%20As%20such%2C%20each%20Gaussian%20can%20represent%20a%20richer%20set%20of%20texture%20patterns%0Aand%20geometric%20structures%2C%20instead%20of%20just%20a%20single%20color%20and%20ellipsoid%20as%20in%0Anaive%20Gaussian%20Splatting.%20Surprisingly%2C%20we%20found%20that%20the%20expressivity%20of%0AGaussians%20can%20be%20greatly%20improved%20by%20using%20alpha-only%20texture%20maps%2C%20and%20further%0Aaugmenting%20Gaussians%20with%20RGB%20texture%20maps%20achieves%20the%20highest%20expressivity.%0AWe%20validate%20our%20method%20on%20a%20wide%20variety%20of%20standard%20benchmark%20datasets%20and%20our%0Aown%20custom%20captures%20at%20both%20the%20object%20and%20scene%20levels.%20We%20demonstrate%20image%0Aquality%20improvements%20over%20existing%20methods%20while%20using%20a%20similar%20or%20lower%0Anumber%20of%20Gaussians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18625v1&entry.124074799=Read"},
{"title": "Neural Surface Priors for Editable Gaussian Splatting", "author": "Jakub Szymkowiak and Weronika Jakubowska and Dawid Malarz and Weronika Smolak-Dy\u017cewska and Maciej Zi\u0119ba and Przemys\u0142aw Musialski and Wojtek Pa\u0142ubicki and Przemys\u0142aw Spurek", "abstract": "  In computer graphics, there is a need to recover easily modifiable\nrepresentations of 3D geometry and appearance from image data. We introduce a\nnovel method for this task using 3D Gaussian Splatting, which enables intuitive\nscene editing through mesh adjustments. Starting with input images and camera\nposes, we reconstruct the underlying geometry using a neural Signed Distance\nField and extract a high-quality mesh. Our model then estimates a set of\nGaussians, where each component is flat, and the opacity is conditioned on the\nrecovered neural surface. To facilitate editing, we produce a proxy\nrepresentation that encodes information about the Gaussians' shape and\nposition. Unlike other methods, our pipeline allows modifications applied to\nthe extracted mesh to be propagated to the proxy representation, from which we\nrecover the updated parameters of the Gaussians. This effectively transfers the\nmesh edits back to the recovered appearance representation. By leveraging\nmesh-guided transformations, our approach simplifies 3D scene editing and\noffers improvements over existing methods in terms of usability and visual\nfidelity of edits. The complete source code for this project can be accessed at\n\\url{https://github.com/WJakubowska/NeuralSurfacePriors}\n", "link": "http://arxiv.org/abs/2411.18311v1", "date": "2024-11-27", "relevancy": 3.4154, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.7186}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6829}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Surface%20Priors%20for%20Editable%20Gaussian%20Splatting&body=Title%3A%20Neural%20Surface%20Priors%20for%20Editable%20Gaussian%20Splatting%0AAuthor%3A%20Jakub%20Szymkowiak%20and%20Weronika%20Jakubowska%20and%20Dawid%20Malarz%20and%20Weronika%20Smolak-Dy%C5%BCewska%20and%20Maciej%20Zi%C4%99ba%20and%20Przemys%C5%82aw%20Musialski%20and%20Wojtek%20Pa%C5%82ubicki%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20In%20computer%20graphics%2C%20there%20is%20a%20need%20to%20recover%20easily%20modifiable%0Arepresentations%20of%203D%20geometry%20and%20appearance%20from%20image%20data.%20We%20introduce%20a%0Anovel%20method%20for%20this%20task%20using%203D%20Gaussian%20Splatting%2C%20which%20enables%20intuitive%0Ascene%20editing%20through%20mesh%20adjustments.%20Starting%20with%20input%20images%20and%20camera%0Aposes%2C%20we%20reconstruct%20the%20underlying%20geometry%20using%20a%20neural%20Signed%20Distance%0AField%20and%20extract%20a%20high-quality%20mesh.%20Our%20model%20then%20estimates%20a%20set%20of%0AGaussians%2C%20where%20each%20component%20is%20flat%2C%20and%20the%20opacity%20is%20conditioned%20on%20the%0Arecovered%20neural%20surface.%20To%20facilitate%20editing%2C%20we%20produce%20a%20proxy%0Arepresentation%20that%20encodes%20information%20about%20the%20Gaussians%27%20shape%20and%0Aposition.%20Unlike%20other%20methods%2C%20our%20pipeline%20allows%20modifications%20applied%20to%0Athe%20extracted%20mesh%20to%20be%20propagated%20to%20the%20proxy%20representation%2C%20from%20which%20we%0Arecover%20the%20updated%20parameters%20of%20the%20Gaussians.%20This%20effectively%20transfers%20the%0Amesh%20edits%20back%20to%20the%20recovered%20appearance%20representation.%20By%20leveraging%0Amesh-guided%20transformations%2C%20our%20approach%20simplifies%203D%20scene%20editing%20and%0Aoffers%20improvements%20over%20existing%20methods%20in%20terms%20of%20usability%20and%20visual%0Afidelity%20of%20edits.%20The%20complete%20source%20code%20for%20this%20project%20can%20be%20accessed%20at%0A%5Curl%7Bhttps%3A//github.com/WJakubowska/NeuralSurfacePriors%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Surface%2520Priors%2520for%2520Editable%2520Gaussian%2520Splatting%26entry.906535625%3DJakub%2520Szymkowiak%2520and%2520Weronika%2520Jakubowska%2520and%2520Dawid%2520Malarz%2520and%2520Weronika%2520Smolak-Dy%25C5%25BCewska%2520and%2520Maciej%2520Zi%25C4%2599ba%2520and%2520Przemys%25C5%2582aw%2520Musialski%2520and%2520Wojtek%2520Pa%25C5%2582ubicki%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520In%2520computer%2520graphics%252C%2520there%2520is%2520a%2520need%2520to%2520recover%2520easily%2520modifiable%250Arepresentations%2520of%25203D%2520geometry%2520and%2520appearance%2520from%2520image%2520data.%2520We%2520introduce%2520a%250Anovel%2520method%2520for%2520this%2520task%2520using%25203D%2520Gaussian%2520Splatting%252C%2520which%2520enables%2520intuitive%250Ascene%2520editing%2520through%2520mesh%2520adjustments.%2520Starting%2520with%2520input%2520images%2520and%2520camera%250Aposes%252C%2520we%2520reconstruct%2520the%2520underlying%2520geometry%2520using%2520a%2520neural%2520Signed%2520Distance%250AField%2520and%2520extract%2520a%2520high-quality%2520mesh.%2520Our%2520model%2520then%2520estimates%2520a%2520set%2520of%250AGaussians%252C%2520where%2520each%2520component%2520is%2520flat%252C%2520and%2520the%2520opacity%2520is%2520conditioned%2520on%2520the%250Arecovered%2520neural%2520surface.%2520To%2520facilitate%2520editing%252C%2520we%2520produce%2520a%2520proxy%250Arepresentation%2520that%2520encodes%2520information%2520about%2520the%2520Gaussians%2527%2520shape%2520and%250Aposition.%2520Unlike%2520other%2520methods%252C%2520our%2520pipeline%2520allows%2520modifications%2520applied%2520to%250Athe%2520extracted%2520mesh%2520to%2520be%2520propagated%2520to%2520the%2520proxy%2520representation%252C%2520from%2520which%2520we%250Arecover%2520the%2520updated%2520parameters%2520of%2520the%2520Gaussians.%2520This%2520effectively%2520transfers%2520the%250Amesh%2520edits%2520back%2520to%2520the%2520recovered%2520appearance%2520representation.%2520By%2520leveraging%250Amesh-guided%2520transformations%252C%2520our%2520approach%2520simplifies%25203D%2520scene%2520editing%2520and%250Aoffers%2520improvements%2520over%2520existing%2520methods%2520in%2520terms%2520of%2520usability%2520and%2520visual%250Afidelity%2520of%2520edits.%2520The%2520complete%2520source%2520code%2520for%2520this%2520project%2520can%2520be%2520accessed%2520at%250A%255Curl%257Bhttps%253A//github.com/WJakubowska/NeuralSurfacePriors%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Surface%20Priors%20for%20Editable%20Gaussian%20Splatting&entry.906535625=Jakub%20Szymkowiak%20and%20Weronika%20Jakubowska%20and%20Dawid%20Malarz%20and%20Weronika%20Smolak-Dy%C5%BCewska%20and%20Maciej%20Zi%C4%99ba%20and%20Przemys%C5%82aw%20Musialski%20and%20Wojtek%20Pa%C5%82ubicki%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20In%20computer%20graphics%2C%20there%20is%20a%20need%20to%20recover%20easily%20modifiable%0Arepresentations%20of%203D%20geometry%20and%20appearance%20from%20image%20data.%20We%20introduce%20a%0Anovel%20method%20for%20this%20task%20using%203D%20Gaussian%20Splatting%2C%20which%20enables%20intuitive%0Ascene%20editing%20through%20mesh%20adjustments.%20Starting%20with%20input%20images%20and%20camera%0Aposes%2C%20we%20reconstruct%20the%20underlying%20geometry%20using%20a%20neural%20Signed%20Distance%0AField%20and%20extract%20a%20high-quality%20mesh.%20Our%20model%20then%20estimates%20a%20set%20of%0AGaussians%2C%20where%20each%20component%20is%20flat%2C%20and%20the%20opacity%20is%20conditioned%20on%20the%0Arecovered%20neural%20surface.%20To%20facilitate%20editing%2C%20we%20produce%20a%20proxy%0Arepresentation%20that%20encodes%20information%20about%20the%20Gaussians%27%20shape%20and%0Aposition.%20Unlike%20other%20methods%2C%20our%20pipeline%20allows%20modifications%20applied%20to%0Athe%20extracted%20mesh%20to%20be%20propagated%20to%20the%20proxy%20representation%2C%20from%20which%20we%0Arecover%20the%20updated%20parameters%20of%20the%20Gaussians.%20This%20effectively%20transfers%20the%0Amesh%20edits%20back%20to%20the%20recovered%20appearance%20representation.%20By%20leveraging%0Amesh-guided%20transformations%2C%20our%20approach%20simplifies%203D%20scene%20editing%20and%0Aoffers%20improvements%20over%20existing%20methods%20in%20terms%20of%20usability%20and%20visual%0Afidelity%20of%20edits.%20The%20complete%20source%20code%20for%20this%20project%20can%20be%20accessed%20at%0A%5Curl%7Bhttps%3A//github.com/WJakubowska/NeuralSurfacePriors%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18311v1&entry.124074799=Read"},
{"title": "Phys4DGen: A Physics-Driven Framework for Controllable and Efficient 4D\n  Content Generation from a Single Image", "author": "Jiajing Lin and Zhenzhong Wang and Shu Jiang and Yongjie Hou and Min Jiang", "abstract": "  The task of 4D content generation involves creating dynamic 3D models that\nevolve over time in response to specific input conditions, such as images.\nExisting methods rely heavily on pre-trained video diffusion models to guide 4D\ncontent dynamics, but these approaches often fail to capture essential physical\nprinciples, as video diffusion models lack a robust understanding of real-world\nphysics. Moreover, these models face challenges in providing fine-grained\ncontrol over dynamics and exhibit high computational costs. In this work, we\npropose Phys4DGen, a novel, high-efficiency framework that generates\nphysics-compliant 4D content from a single image with enhanced control\ncapabilities. Our approach uniquely integrates physical simulations into the 4D\ngeneration pipeline, ensuring adherence to fundamental physical laws. Inspired\nby the human ability to infer physical properties visually, we introduce a\nPhysical Perception Module (PPM) that discerns the material properties and\nstructural components of the 3D object from the input image, facilitating\naccurate downstream simulations. Phys4DGen significantly accelerates the 4D\ngeneration process by eliminating iterative optimization steps in the dynamics\nmodeling phase. It allows users to intuitively control the movement speed and\ndirection of generated 4D content by adjusting external forces, achieving\nfinely tunable, physically plausible animations. Extensive evaluations show\nthat Phys4DGen outperforms existing methods in both inference speed and\nphysical realism, producing high-quality, controllable 4D content. Our project\npage is available at the link: \\url{https://jiajinglin.github.io/Phys4DGen/}.\n", "link": "http://arxiv.org/abs/2411.16800v2", "date": "2024-11-27", "relevancy": 3.4014, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6223}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phys4DGen%3A%20A%20Physics-Driven%20Framework%20for%20Controllable%20and%20Efficient%204D%0A%20%20Content%20Generation%20from%20a%20Single%20Image&body=Title%3A%20Phys4DGen%3A%20A%20Physics-Driven%20Framework%20for%20Controllable%20and%20Efficient%204D%0A%20%20Content%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Jiajing%20Lin%20and%20Zhenzhong%20Wang%20and%20Shu%20Jiang%20and%20Yongjie%20Hou%20and%20Min%20Jiang%0AAbstract%3A%20%20%20The%20task%20of%204D%20content%20generation%20involves%20creating%20dynamic%203D%20models%20that%0Aevolve%20over%20time%20in%20response%20to%20specific%20input%20conditions%2C%20such%20as%20images.%0AExisting%20methods%20rely%20heavily%20on%20pre-trained%20video%20diffusion%20models%20to%20guide%204D%0Acontent%20dynamics%2C%20but%20these%20approaches%20often%20fail%20to%20capture%20essential%20physical%0Aprinciples%2C%20as%20video%20diffusion%20models%20lack%20a%20robust%20understanding%20of%20real-world%0Aphysics.%20Moreover%2C%20these%20models%20face%20challenges%20in%20providing%20fine-grained%0Acontrol%20over%20dynamics%20and%20exhibit%20high%20computational%20costs.%20In%20this%20work%2C%20we%0Apropose%20Phys4DGen%2C%20a%20novel%2C%20high-efficiency%20framework%20that%20generates%0Aphysics-compliant%204D%20content%20from%20a%20single%20image%20with%20enhanced%20control%0Acapabilities.%20Our%20approach%20uniquely%20integrates%20physical%20simulations%20into%20the%204D%0Ageneration%20pipeline%2C%20ensuring%20adherence%20to%20fundamental%20physical%20laws.%20Inspired%0Aby%20the%20human%20ability%20to%20infer%20physical%20properties%20visually%2C%20we%20introduce%20a%0APhysical%20Perception%20Module%20%28PPM%29%20that%20discerns%20the%20material%20properties%20and%0Astructural%20components%20of%20the%203D%20object%20from%20the%20input%20image%2C%20facilitating%0Aaccurate%20downstream%20simulations.%20Phys4DGen%20significantly%20accelerates%20the%204D%0Ageneration%20process%20by%20eliminating%20iterative%20optimization%20steps%20in%20the%20dynamics%0Amodeling%20phase.%20It%20allows%20users%20to%20intuitively%20control%20the%20movement%20speed%20and%0Adirection%20of%20generated%204D%20content%20by%20adjusting%20external%20forces%2C%20achieving%0Afinely%20tunable%2C%20physically%20plausible%20animations.%20Extensive%20evaluations%20show%0Athat%20Phys4DGen%20outperforms%20existing%20methods%20in%20both%20inference%20speed%20and%0Aphysical%20realism%2C%20producing%20high-quality%2C%20controllable%204D%20content.%20Our%20project%0Apage%20is%20available%20at%20the%20link%3A%20%5Curl%7Bhttps%3A//jiajinglin.github.io/Phys4DGen/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhys4DGen%253A%2520A%2520Physics-Driven%2520Framework%2520for%2520Controllable%2520and%2520Efficient%25204D%250A%2520%2520Content%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DJiajing%2520Lin%2520and%2520Zhenzhong%2520Wang%2520and%2520Shu%2520Jiang%2520and%2520Yongjie%2520Hou%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520The%2520task%2520of%25204D%2520content%2520generation%2520involves%2520creating%2520dynamic%25203D%2520models%2520that%250Aevolve%2520over%2520time%2520in%2520response%2520to%2520specific%2520input%2520conditions%252C%2520such%2520as%2520images.%250AExisting%2520methods%2520rely%2520heavily%2520on%2520pre-trained%2520video%2520diffusion%2520models%2520to%2520guide%25204D%250Acontent%2520dynamics%252C%2520but%2520these%2520approaches%2520often%2520fail%2520to%2520capture%2520essential%2520physical%250Aprinciples%252C%2520as%2520video%2520diffusion%2520models%2520lack%2520a%2520robust%2520understanding%2520of%2520real-world%250Aphysics.%2520Moreover%252C%2520these%2520models%2520face%2520challenges%2520in%2520providing%2520fine-grained%250Acontrol%2520over%2520dynamics%2520and%2520exhibit%2520high%2520computational%2520costs.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Phys4DGen%252C%2520a%2520novel%252C%2520high-efficiency%2520framework%2520that%2520generates%250Aphysics-compliant%25204D%2520content%2520from%2520a%2520single%2520image%2520with%2520enhanced%2520control%250Acapabilities.%2520Our%2520approach%2520uniquely%2520integrates%2520physical%2520simulations%2520into%2520the%25204D%250Ageneration%2520pipeline%252C%2520ensuring%2520adherence%2520to%2520fundamental%2520physical%2520laws.%2520Inspired%250Aby%2520the%2520human%2520ability%2520to%2520infer%2520physical%2520properties%2520visually%252C%2520we%2520introduce%2520a%250APhysical%2520Perception%2520Module%2520%2528PPM%2529%2520that%2520discerns%2520the%2520material%2520properties%2520and%250Astructural%2520components%2520of%2520the%25203D%2520object%2520from%2520the%2520input%2520image%252C%2520facilitating%250Aaccurate%2520downstream%2520simulations.%2520Phys4DGen%2520significantly%2520accelerates%2520the%25204D%250Ageneration%2520process%2520by%2520eliminating%2520iterative%2520optimization%2520steps%2520in%2520the%2520dynamics%250Amodeling%2520phase.%2520It%2520allows%2520users%2520to%2520intuitively%2520control%2520the%2520movement%2520speed%2520and%250Adirection%2520of%2520generated%25204D%2520content%2520by%2520adjusting%2520external%2520forces%252C%2520achieving%250Afinely%2520tunable%252C%2520physically%2520plausible%2520animations.%2520Extensive%2520evaluations%2520show%250Athat%2520Phys4DGen%2520outperforms%2520existing%2520methods%2520in%2520both%2520inference%2520speed%2520and%250Aphysical%2520realism%252C%2520producing%2520high-quality%252C%2520controllable%25204D%2520content.%2520Our%2520project%250Apage%2520is%2520available%2520at%2520the%2520link%253A%2520%255Curl%257Bhttps%253A//jiajinglin.github.io/Phys4DGen/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phys4DGen%3A%20A%20Physics-Driven%20Framework%20for%20Controllable%20and%20Efficient%204D%0A%20%20Content%20Generation%20from%20a%20Single%20Image&entry.906535625=Jiajing%20Lin%20and%20Zhenzhong%20Wang%20and%20Shu%20Jiang%20and%20Yongjie%20Hou%20and%20Min%20Jiang&entry.1292438233=%20%20The%20task%20of%204D%20content%20generation%20involves%20creating%20dynamic%203D%20models%20that%0Aevolve%20over%20time%20in%20response%20to%20specific%20input%20conditions%2C%20such%20as%20images.%0AExisting%20methods%20rely%20heavily%20on%20pre-trained%20video%20diffusion%20models%20to%20guide%204D%0Acontent%20dynamics%2C%20but%20these%20approaches%20often%20fail%20to%20capture%20essential%20physical%0Aprinciples%2C%20as%20video%20diffusion%20models%20lack%20a%20robust%20understanding%20of%20real-world%0Aphysics.%20Moreover%2C%20these%20models%20face%20challenges%20in%20providing%20fine-grained%0Acontrol%20over%20dynamics%20and%20exhibit%20high%20computational%20costs.%20In%20this%20work%2C%20we%0Apropose%20Phys4DGen%2C%20a%20novel%2C%20high-efficiency%20framework%20that%20generates%0Aphysics-compliant%204D%20content%20from%20a%20single%20image%20with%20enhanced%20control%0Acapabilities.%20Our%20approach%20uniquely%20integrates%20physical%20simulations%20into%20the%204D%0Ageneration%20pipeline%2C%20ensuring%20adherence%20to%20fundamental%20physical%20laws.%20Inspired%0Aby%20the%20human%20ability%20to%20infer%20physical%20properties%20visually%2C%20we%20introduce%20a%0APhysical%20Perception%20Module%20%28PPM%29%20that%20discerns%20the%20material%20properties%20and%0Astructural%20components%20of%20the%203D%20object%20from%20the%20input%20image%2C%20facilitating%0Aaccurate%20downstream%20simulations.%20Phys4DGen%20significantly%20accelerates%20the%204D%0Ageneration%20process%20by%20eliminating%20iterative%20optimization%20steps%20in%20the%20dynamics%0Amodeling%20phase.%20It%20allows%20users%20to%20intuitively%20control%20the%20movement%20speed%20and%0Adirection%20of%20generated%204D%20content%20by%20adjusting%20external%20forces%2C%20achieving%0Afinely%20tunable%2C%20physically%20plausible%20animations.%20Extensive%20evaluations%20show%0Athat%20Phys4DGen%20outperforms%20existing%20methods%20in%20both%20inference%20speed%20and%0Aphysical%20realism%2C%20producing%20high-quality%2C%20controllable%204D%20content.%20Our%20project%0Apage%20is%20available%20at%20the%20link%3A%20%5Curl%7Bhttps%3A//jiajinglin.github.io/Phys4DGen/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16800v2&entry.124074799=Read"},
{"title": "A Unified Framework for 3D Scene Understanding", "author": "Wei Xu and Chunsheng Shi and Sifan Tu and Xin Zhou and Dingkang Liang and Xiang Bai", "abstract": "  We propose UniSeg3D, a unified 3D scene understanding framework that achieves\npanoptic, semantic, instance, interactive, referring, and open-vocabulary\nsegmentation tasks within a single model. Most previous 3D segmentation\napproaches are typically tailored to a specific task, limiting their\nunderstanding of 3D scenes to a task-specific perspective. In contrast, the\nproposed method unifies six tasks into unified representations processed by the\nsame Transformer. It facilitates inter-task knowledge sharing, thereby\npromoting comprehensive 3D scene understanding. To take advantage of multi-task\nunification, we enhance performance by establishing explicit inter-task\nassociations. Specifically, we design knowledge distillation and contrastive\nlearning methods to transfer task-specific knowledge across different tasks.\nExperiments on three benchmarks, including ScanNet20, ScanRefer, and\nScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA\nmethods, even those specialized for individual tasks. We hope UniSeg3D can\nserve as a solid unified baseline and inspire future work. Code and models are\navailable at https://github.com/dk-liang/UniSeg3D.\n", "link": "http://arxiv.org/abs/2407.03263v2", "date": "2024-11-27", "relevancy": 3.3857, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%203D%20Scene%20Understanding&body=Title%3A%20A%20Unified%20Framework%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Wei%20Xu%20and%20Chunsheng%20Shi%20and%20Sifan%20Tu%20and%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20We%20propose%20UniSeg3D%2C%20a%20unified%203D%20scene%20understanding%20framework%20that%20achieves%0Apanoptic%2C%20semantic%2C%20instance%2C%20interactive%2C%20referring%2C%20and%20open-vocabulary%0Asegmentation%20tasks%20within%20a%20single%20model.%20Most%20previous%203D%20segmentation%0Aapproaches%20are%20typically%20tailored%20to%20a%20specific%20task%2C%20limiting%20their%0Aunderstanding%20of%203D%20scenes%20to%20a%20task-specific%20perspective.%20In%20contrast%2C%20the%0Aproposed%20method%20unifies%20six%20tasks%20into%20unified%20representations%20processed%20by%20the%0Asame%20Transformer.%20It%20facilitates%20inter-task%20knowledge%20sharing%2C%20thereby%0Apromoting%20comprehensive%203D%20scene%20understanding.%20To%20take%20advantage%20of%20multi-task%0Aunification%2C%20we%20enhance%20performance%20by%20establishing%20explicit%20inter-task%0Aassociations.%20Specifically%2C%20we%20design%20knowledge%20distillation%20and%20contrastive%0Alearning%20methods%20to%20transfer%20task-specific%20knowledge%20across%20different%20tasks.%0AExperiments%20on%20three%20benchmarks%2C%20including%20ScanNet20%2C%20ScanRefer%2C%20and%0AScanNet200%2C%20demonstrate%20that%20the%20UniSeg3D%20consistently%20outperforms%20current%20SOTA%0Amethods%2C%20even%20those%20specialized%20for%20individual%20tasks.%20We%20hope%20UniSeg3D%20can%0Aserve%20as%20a%20solid%20unified%20baseline%20and%20inspire%20future%20work.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/dk-liang/UniSeg3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DWei%2520Xu%2520and%2520Chunsheng%2520Shi%2520and%2520Sifan%2520Tu%2520and%2520Xin%2520Zhou%2520and%2520Dingkang%2520Liang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520We%2520propose%2520UniSeg3D%252C%2520a%2520unified%25203D%2520scene%2520understanding%2520framework%2520that%2520achieves%250Apanoptic%252C%2520semantic%252C%2520instance%252C%2520interactive%252C%2520referring%252C%2520and%2520open-vocabulary%250Asegmentation%2520tasks%2520within%2520a%2520single%2520model.%2520Most%2520previous%25203D%2520segmentation%250Aapproaches%2520are%2520typically%2520tailored%2520to%2520a%2520specific%2520task%252C%2520limiting%2520their%250Aunderstanding%2520of%25203D%2520scenes%2520to%2520a%2520task-specific%2520perspective.%2520In%2520contrast%252C%2520the%250Aproposed%2520method%2520unifies%2520six%2520tasks%2520into%2520unified%2520representations%2520processed%2520by%2520the%250Asame%2520Transformer.%2520It%2520facilitates%2520inter-task%2520knowledge%2520sharing%252C%2520thereby%250Apromoting%2520comprehensive%25203D%2520scene%2520understanding.%2520To%2520take%2520advantage%2520of%2520multi-task%250Aunification%252C%2520we%2520enhance%2520performance%2520by%2520establishing%2520explicit%2520inter-task%250Aassociations.%2520Specifically%252C%2520we%2520design%2520knowledge%2520distillation%2520and%2520contrastive%250Alearning%2520methods%2520to%2520transfer%2520task-specific%2520knowledge%2520across%2520different%2520tasks.%250AExperiments%2520on%2520three%2520benchmarks%252C%2520including%2520ScanNet20%252C%2520ScanRefer%252C%2520and%250AScanNet200%252C%2520demonstrate%2520that%2520the%2520UniSeg3D%2520consistently%2520outperforms%2520current%2520SOTA%250Amethods%252C%2520even%2520those%2520specialized%2520for%2520individual%2520tasks.%2520We%2520hope%2520UniSeg3D%2520can%250Aserve%2520as%2520a%2520solid%2520unified%2520baseline%2520and%2520inspire%2520future%2520work.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/dk-liang/UniSeg3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%203D%20Scene%20Understanding&entry.906535625=Wei%20Xu%20and%20Chunsheng%20Shi%20and%20Sifan%20Tu%20and%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Xiang%20Bai&entry.1292438233=%20%20We%20propose%20UniSeg3D%2C%20a%20unified%203D%20scene%20understanding%20framework%20that%20achieves%0Apanoptic%2C%20semantic%2C%20instance%2C%20interactive%2C%20referring%2C%20and%20open-vocabulary%0Asegmentation%20tasks%20within%20a%20single%20model.%20Most%20previous%203D%20segmentation%0Aapproaches%20are%20typically%20tailored%20to%20a%20specific%20task%2C%20limiting%20their%0Aunderstanding%20of%203D%20scenes%20to%20a%20task-specific%20perspective.%20In%20contrast%2C%20the%0Aproposed%20method%20unifies%20six%20tasks%20into%20unified%20representations%20processed%20by%20the%0Asame%20Transformer.%20It%20facilitates%20inter-task%20knowledge%20sharing%2C%20thereby%0Apromoting%20comprehensive%203D%20scene%20understanding.%20To%20take%20advantage%20of%20multi-task%0Aunification%2C%20we%20enhance%20performance%20by%20establishing%20explicit%20inter-task%0Aassociations.%20Specifically%2C%20we%20design%20knowledge%20distillation%20and%20contrastive%0Alearning%20methods%20to%20transfer%20task-specific%20knowledge%20across%20different%20tasks.%0AExperiments%20on%20three%20benchmarks%2C%20including%20ScanNet20%2C%20ScanRefer%2C%20and%0AScanNet200%2C%20demonstrate%20that%20the%20UniSeg3D%20consistently%20outperforms%20current%20SOTA%0Amethods%2C%20even%20those%20specialized%20for%20individual%20tasks.%20We%20hope%20UniSeg3D%20can%0Aserve%20as%20a%20solid%20unified%20baseline%20and%20inspire%20future%20work.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/dk-liang/UniSeg3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03263v2&entry.124074799=Read"},
{"title": "HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression", "author": "Lei Liu and Zhenghao Chen and Dong Xu", "abstract": "  Fast progress in 3D Gaussian Splatting (3DGS) has made 3D Gaussians popular\nfor 3D modeling and image rendering, but this creates big challenges in data\nstorage and transmission. To obtain a highly compact 3DGS representation, we\npropose a hybrid entropy model for Gaussian Splatting (HEMGS) data compression,\nwhich comprises two primary components, a hyperprior network and an\nautoregressive network. To effectively reduce structural redundancy across\nattributes, we apply a progressive coding algorithm to generate hyperprior\nfeatures, in which we use previously compressed attributes and location as\nprior information. In particular, to better extract the location features from\nthese compressed attributes, we adopt a domain-aware and instance-aware\narchitecture to respectively capture domain-aware structural relations without\nadditional storage costs and reveal scene-specific features through MLPs.\nAdditionally, to reduce redundancy within each attribute, we leverage\nrelationships between neighboring compressed elements within the attributes\nthrough an autoregressive network. Given its unique structure, we propose an\nadaptive context coding algorithm with flexible receptive fields to effectively\ncapture adjacent compressed elements. Overall, we integrate our HEMGS into an\nend-to-end optimized 3DGS compression framework and the extensive experimental\nresults on four benchmarks indicate that our method achieves about 40\\% average\nreduction in size while maintaining the rendering quality over our baseline\nmethod and achieving state-of-the-art compression results.\n", "link": "http://arxiv.org/abs/2411.18473v1", "date": "2024-11-27", "relevancy": 3.2325, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6709}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6367}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression&body=Title%3A%20HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression%0AAuthor%3A%20Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Fast%20progress%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20made%203D%20Gaussians%20popular%0Afor%203D%20modeling%20and%20image%20rendering%2C%20but%20this%20creates%20big%20challenges%20in%20data%0Astorage%20and%20transmission.%20To%20obtain%20a%20highly%20compact%203DGS%20representation%2C%20we%0Apropose%20a%20hybrid%20entropy%20model%20for%20Gaussian%20Splatting%20%28HEMGS%29%20data%20compression%2C%0Awhich%20comprises%20two%20primary%20components%2C%20a%20hyperprior%20network%20and%20an%0Aautoregressive%20network.%20To%20effectively%20reduce%20structural%20redundancy%20across%0Aattributes%2C%20we%20apply%20a%20progressive%20coding%20algorithm%20to%20generate%20hyperprior%0Afeatures%2C%20in%20which%20we%20use%20previously%20compressed%20attributes%20and%20location%20as%0Aprior%20information.%20In%20particular%2C%20to%20better%20extract%20the%20location%20features%20from%0Athese%20compressed%20attributes%2C%20we%20adopt%20a%20domain-aware%20and%20instance-aware%0Aarchitecture%20to%20respectively%20capture%20domain-aware%20structural%20relations%20without%0Aadditional%20storage%20costs%20and%20reveal%20scene-specific%20features%20through%20MLPs.%0AAdditionally%2C%20to%20reduce%20redundancy%20within%20each%20attribute%2C%20we%20leverage%0Arelationships%20between%20neighboring%20compressed%20elements%20within%20the%20attributes%0Athrough%20an%20autoregressive%20network.%20Given%20its%20unique%20structure%2C%20we%20propose%20an%0Aadaptive%20context%20coding%20algorithm%20with%20flexible%20receptive%20fields%20to%20effectively%0Acapture%20adjacent%20compressed%20elements.%20Overall%2C%20we%20integrate%20our%20HEMGS%20into%20an%0Aend-to-end%20optimized%203DGS%20compression%20framework%20and%20the%20extensive%20experimental%0Aresults%20on%20four%20benchmarks%20indicate%20that%20our%20method%20achieves%20about%2040%5C%25%20average%0Areduction%20in%20size%20while%20maintaining%20the%20rendering%20quality%20over%20our%20baseline%0Amethod%20and%20achieving%20state-of-the-art%20compression%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEMGS%253A%2520A%2520Hybrid%2520Entropy%2520Model%2520for%25203D%2520Gaussian%2520Splatting%2520Data%2520Compression%26entry.906535625%3DLei%2520Liu%2520and%2520Zhenghao%2520Chen%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Fast%2520progress%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520made%25203D%2520Gaussians%2520popular%250Afor%25203D%2520modeling%2520and%2520image%2520rendering%252C%2520but%2520this%2520creates%2520big%2520challenges%2520in%2520data%250Astorage%2520and%2520transmission.%2520To%2520obtain%2520a%2520highly%2520compact%25203DGS%2520representation%252C%2520we%250Apropose%2520a%2520hybrid%2520entropy%2520model%2520for%2520Gaussian%2520Splatting%2520%2528HEMGS%2529%2520data%2520compression%252C%250Awhich%2520comprises%2520two%2520primary%2520components%252C%2520a%2520hyperprior%2520network%2520and%2520an%250Aautoregressive%2520network.%2520To%2520effectively%2520reduce%2520structural%2520redundancy%2520across%250Aattributes%252C%2520we%2520apply%2520a%2520progressive%2520coding%2520algorithm%2520to%2520generate%2520hyperprior%250Afeatures%252C%2520in%2520which%2520we%2520use%2520previously%2520compressed%2520attributes%2520and%2520location%2520as%250Aprior%2520information.%2520In%2520particular%252C%2520to%2520better%2520extract%2520the%2520location%2520features%2520from%250Athese%2520compressed%2520attributes%252C%2520we%2520adopt%2520a%2520domain-aware%2520and%2520instance-aware%250Aarchitecture%2520to%2520respectively%2520capture%2520domain-aware%2520structural%2520relations%2520without%250Aadditional%2520storage%2520costs%2520and%2520reveal%2520scene-specific%2520features%2520through%2520MLPs.%250AAdditionally%252C%2520to%2520reduce%2520redundancy%2520within%2520each%2520attribute%252C%2520we%2520leverage%250Arelationships%2520between%2520neighboring%2520compressed%2520elements%2520within%2520the%2520attributes%250Athrough%2520an%2520autoregressive%2520network.%2520Given%2520its%2520unique%2520structure%252C%2520we%2520propose%2520an%250Aadaptive%2520context%2520coding%2520algorithm%2520with%2520flexible%2520receptive%2520fields%2520to%2520effectively%250Acapture%2520adjacent%2520compressed%2520elements.%2520Overall%252C%2520we%2520integrate%2520our%2520HEMGS%2520into%2520an%250Aend-to-end%2520optimized%25203DGS%2520compression%2520framework%2520and%2520the%2520extensive%2520experimental%250Aresults%2520on%2520four%2520benchmarks%2520indicate%2520that%2520our%2520method%2520achieves%2520about%252040%255C%2525%2520average%250Areduction%2520in%2520size%2520while%2520maintaining%2520the%2520rendering%2520quality%2520over%2520our%2520baseline%250Amethod%2520and%2520achieving%2520state-of-the-art%2520compression%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression&entry.906535625=Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Dong%20Xu&entry.1292438233=%20%20Fast%20progress%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20made%203D%20Gaussians%20popular%0Afor%203D%20modeling%20and%20image%20rendering%2C%20but%20this%20creates%20big%20challenges%20in%20data%0Astorage%20and%20transmission.%20To%20obtain%20a%20highly%20compact%203DGS%20representation%2C%20we%0Apropose%20a%20hybrid%20entropy%20model%20for%20Gaussian%20Splatting%20%28HEMGS%29%20data%20compression%2C%0Awhich%20comprises%20two%20primary%20components%2C%20a%20hyperprior%20network%20and%20an%0Aautoregressive%20network.%20To%20effectively%20reduce%20structural%20redundancy%20across%0Aattributes%2C%20we%20apply%20a%20progressive%20coding%20algorithm%20to%20generate%20hyperprior%0Afeatures%2C%20in%20which%20we%20use%20previously%20compressed%20attributes%20and%20location%20as%0Aprior%20information.%20In%20particular%2C%20to%20better%20extract%20the%20location%20features%20from%0Athese%20compressed%20attributes%2C%20we%20adopt%20a%20domain-aware%20and%20instance-aware%0Aarchitecture%20to%20respectively%20capture%20domain-aware%20structural%20relations%20without%0Aadditional%20storage%20costs%20and%20reveal%20scene-specific%20features%20through%20MLPs.%0AAdditionally%2C%20to%20reduce%20redundancy%20within%20each%20attribute%2C%20we%20leverage%0Arelationships%20between%20neighboring%20compressed%20elements%20within%20the%20attributes%0Athrough%20an%20autoregressive%20network.%20Given%20its%20unique%20structure%2C%20we%20propose%20an%0Aadaptive%20context%20coding%20algorithm%20with%20flexible%20receptive%20fields%20to%20effectively%0Acapture%20adjacent%20compressed%20elements.%20Overall%2C%20we%20integrate%20our%20HEMGS%20into%20an%0Aend-to-end%20optimized%203DGS%20compression%20framework%20and%20the%20extensive%20experimental%0Aresults%20on%20four%20benchmarks%20indicate%20that%20our%20method%20achieves%20about%2040%5C%25%20average%0Areduction%20in%20size%20while%20maintaining%20the%20rendering%20quality%20over%20our%20baseline%0Amethod%20and%20achieving%20state-of-the-art%20compression%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18473v1&entry.124074799=Read"},
{"title": "GeneMAN: Generalizable Single-Image 3D Human Reconstruction from\n  Multi-Source Human Data", "author": "Wentao Wang and Hang Ye and Fangzhou Hong and Xue Yang and Jianfu Zhang and Yizhou Wang and Ziwei Liu and Liang Pan", "abstract": "  Given a single in-the-wild human photo, it remains a challenging task to\nreconstruct a high-fidelity 3D human model. Existing methods face difficulties\nincluding a) the varying body proportions captured by in-the-wild human images;\nb) diverse personal belongings within the shot; and c) ambiguities in human\npostures and inconsistency in human textures. In addition, the scarcity of\nhigh-quality human data intensifies the challenge. To address these problems,\nwe propose a Generalizable image-to-3D huMAN reconstruction framework, dubbed\nGeneMAN, building upon a comprehensive multi-source collection of high-quality\nhuman data, including 3D scans, multi-view videos, single photos, and our\ngenerated synthetic human data. GeneMAN encompasses three key modules. 1)\nWithout relying on parametric human models (e.g., SMPL), GeneMAN first trains a\nhuman-specific text-to-image diffusion model and a view-conditioned diffusion\nmodel, serving as GeneMAN 2D human prior and 3D human prior for reconstruction,\nrespectively. 2) With the help of the pretrained human prior models, the\nGeometry Initialization-&-Sculpting pipeline is leveraged to recover\nhigh-quality 3D human geometry given a single image. 3) To achieve\nhigh-fidelity 3D human textures, GeneMAN employs the Multi-Space Texture\nRefinement pipeline, consecutively refining textures in the latent and the\npixel spaces. Extensive experimental results demonstrate that GeneMAN could\ngenerate high-quality 3D human models from a single image input, outperforming\nprior state-of-the-art methods. Notably, GeneMAN could reveal much better\ngeneralizability in dealing with in-the-wild images, often yielding\nhigh-quality 3D human models in natural poses with common items, regardless of\nthe body proportions in the input images.\n", "link": "http://arxiv.org/abs/2411.18624v1", "date": "2024-11-27", "relevancy": 3.2144, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6839}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6224}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeneMAN%3A%20Generalizable%20Single-Image%203D%20Human%20Reconstruction%20from%0A%20%20Multi-Source%20Human%20Data&body=Title%3A%20GeneMAN%3A%20Generalizable%20Single-Image%203D%20Human%20Reconstruction%20from%0A%20%20Multi-Source%20Human%20Data%0AAuthor%3A%20Wentao%20Wang%20and%20Hang%20Ye%20and%20Fangzhou%20Hong%20and%20Xue%20Yang%20and%20Jianfu%20Zhang%20and%20Yizhou%20Wang%20and%20Ziwei%20Liu%20and%20Liang%20Pan%0AAbstract%3A%20%20%20Given%20a%20single%20in-the-wild%20human%20photo%2C%20it%20remains%20a%20challenging%20task%20to%0Areconstruct%20a%20high-fidelity%203D%20human%20model.%20Existing%20methods%20face%20difficulties%0Aincluding%20a%29%20the%20varying%20body%20proportions%20captured%20by%20in-the-wild%20human%20images%3B%0Ab%29%20diverse%20personal%20belongings%20within%20the%20shot%3B%20and%20c%29%20ambiguities%20in%20human%0Apostures%20and%20inconsistency%20in%20human%20textures.%20In%20addition%2C%20the%20scarcity%20of%0Ahigh-quality%20human%20data%20intensifies%20the%20challenge.%20To%20address%20these%20problems%2C%0Awe%20propose%20a%20Generalizable%20image-to-3D%20huMAN%20reconstruction%20framework%2C%20dubbed%0AGeneMAN%2C%20building%20upon%20a%20comprehensive%20multi-source%20collection%20of%20high-quality%0Ahuman%20data%2C%20including%203D%20scans%2C%20multi-view%20videos%2C%20single%20photos%2C%20and%20our%0Agenerated%20synthetic%20human%20data.%20GeneMAN%20encompasses%20three%20key%20modules.%201%29%0AWithout%20relying%20on%20parametric%20human%20models%20%28e.g.%2C%20SMPL%29%2C%20GeneMAN%20first%20trains%20a%0Ahuman-specific%20text-to-image%20diffusion%20model%20and%20a%20view-conditioned%20diffusion%0Amodel%2C%20serving%20as%20GeneMAN%202D%20human%20prior%20and%203D%20human%20prior%20for%20reconstruction%2C%0Arespectively.%202%29%20With%20the%20help%20of%20the%20pretrained%20human%20prior%20models%2C%20the%0AGeometry%20Initialization-%26-Sculpting%20pipeline%20is%20leveraged%20to%20recover%0Ahigh-quality%203D%20human%20geometry%20given%20a%20single%20image.%203%29%20To%20achieve%0Ahigh-fidelity%203D%20human%20textures%2C%20GeneMAN%20employs%20the%20Multi-Space%20Texture%0ARefinement%20pipeline%2C%20consecutively%20refining%20textures%20in%20the%20latent%20and%20the%0Apixel%20spaces.%20Extensive%20experimental%20results%20demonstrate%20that%20GeneMAN%20could%0Agenerate%20high-quality%203D%20human%20models%20from%20a%20single%20image%20input%2C%20outperforming%0Aprior%20state-of-the-art%20methods.%20Notably%2C%20GeneMAN%20could%20reveal%20much%20better%0Ageneralizability%20in%20dealing%20with%20in-the-wild%20images%2C%20often%20yielding%0Ahigh-quality%203D%20human%20models%20in%20natural%20poses%20with%20common%20items%2C%20regardless%20of%0Athe%20body%20proportions%20in%20the%20input%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneMAN%253A%2520Generalizable%2520Single-Image%25203D%2520Human%2520Reconstruction%2520from%250A%2520%2520Multi-Source%2520Human%2520Data%26entry.906535625%3DWentao%2520Wang%2520and%2520Hang%2520Ye%2520and%2520Fangzhou%2520Hong%2520and%2520Xue%2520Yang%2520and%2520Jianfu%2520Zhang%2520and%2520Yizhou%2520Wang%2520and%2520Ziwei%2520Liu%2520and%2520Liang%2520Pan%26entry.1292438233%3D%2520%2520Given%2520a%2520single%2520in-the-wild%2520human%2520photo%252C%2520it%2520remains%2520a%2520challenging%2520task%2520to%250Areconstruct%2520a%2520high-fidelity%25203D%2520human%2520model.%2520Existing%2520methods%2520face%2520difficulties%250Aincluding%2520a%2529%2520the%2520varying%2520body%2520proportions%2520captured%2520by%2520in-the-wild%2520human%2520images%253B%250Ab%2529%2520diverse%2520personal%2520belongings%2520within%2520the%2520shot%253B%2520and%2520c%2529%2520ambiguities%2520in%2520human%250Apostures%2520and%2520inconsistency%2520in%2520human%2520textures.%2520In%2520addition%252C%2520the%2520scarcity%2520of%250Ahigh-quality%2520human%2520data%2520intensifies%2520the%2520challenge.%2520To%2520address%2520these%2520problems%252C%250Awe%2520propose%2520a%2520Generalizable%2520image-to-3D%2520huMAN%2520reconstruction%2520framework%252C%2520dubbed%250AGeneMAN%252C%2520building%2520upon%2520a%2520comprehensive%2520multi-source%2520collection%2520of%2520high-quality%250Ahuman%2520data%252C%2520including%25203D%2520scans%252C%2520multi-view%2520videos%252C%2520single%2520photos%252C%2520and%2520our%250Agenerated%2520synthetic%2520human%2520data.%2520GeneMAN%2520encompasses%2520three%2520key%2520modules.%25201%2529%250AWithout%2520relying%2520on%2520parametric%2520human%2520models%2520%2528e.g.%252C%2520SMPL%2529%252C%2520GeneMAN%2520first%2520trains%2520a%250Ahuman-specific%2520text-to-image%2520diffusion%2520model%2520and%2520a%2520view-conditioned%2520diffusion%250Amodel%252C%2520serving%2520as%2520GeneMAN%25202D%2520human%2520prior%2520and%25203D%2520human%2520prior%2520for%2520reconstruction%252C%250Arespectively.%25202%2529%2520With%2520the%2520help%2520of%2520the%2520pretrained%2520human%2520prior%2520models%252C%2520the%250AGeometry%2520Initialization-%2526-Sculpting%2520pipeline%2520is%2520leveraged%2520to%2520recover%250Ahigh-quality%25203D%2520human%2520geometry%2520given%2520a%2520single%2520image.%25203%2529%2520To%2520achieve%250Ahigh-fidelity%25203D%2520human%2520textures%252C%2520GeneMAN%2520employs%2520the%2520Multi-Space%2520Texture%250ARefinement%2520pipeline%252C%2520consecutively%2520refining%2520textures%2520in%2520the%2520latent%2520and%2520the%250Apixel%2520spaces.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520GeneMAN%2520could%250Agenerate%2520high-quality%25203D%2520human%2520models%2520from%2520a%2520single%2520image%2520input%252C%2520outperforming%250Aprior%2520state-of-the-art%2520methods.%2520Notably%252C%2520GeneMAN%2520could%2520reveal%2520much%2520better%250Ageneralizability%2520in%2520dealing%2520with%2520in-the-wild%2520images%252C%2520often%2520yielding%250Ahigh-quality%25203D%2520human%2520models%2520in%2520natural%2520poses%2520with%2520common%2520items%252C%2520regardless%2520of%250Athe%2520body%2520proportions%2520in%2520the%2520input%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeneMAN%3A%20Generalizable%20Single-Image%203D%20Human%20Reconstruction%20from%0A%20%20Multi-Source%20Human%20Data&entry.906535625=Wentao%20Wang%20and%20Hang%20Ye%20and%20Fangzhou%20Hong%20and%20Xue%20Yang%20and%20Jianfu%20Zhang%20and%20Yizhou%20Wang%20and%20Ziwei%20Liu%20and%20Liang%20Pan&entry.1292438233=%20%20Given%20a%20single%20in-the-wild%20human%20photo%2C%20it%20remains%20a%20challenging%20task%20to%0Areconstruct%20a%20high-fidelity%203D%20human%20model.%20Existing%20methods%20face%20difficulties%0Aincluding%20a%29%20the%20varying%20body%20proportions%20captured%20by%20in-the-wild%20human%20images%3B%0Ab%29%20diverse%20personal%20belongings%20within%20the%20shot%3B%20and%20c%29%20ambiguities%20in%20human%0Apostures%20and%20inconsistency%20in%20human%20textures.%20In%20addition%2C%20the%20scarcity%20of%0Ahigh-quality%20human%20data%20intensifies%20the%20challenge.%20To%20address%20these%20problems%2C%0Awe%20propose%20a%20Generalizable%20image-to-3D%20huMAN%20reconstruction%20framework%2C%20dubbed%0AGeneMAN%2C%20building%20upon%20a%20comprehensive%20multi-source%20collection%20of%20high-quality%0Ahuman%20data%2C%20including%203D%20scans%2C%20multi-view%20videos%2C%20single%20photos%2C%20and%20our%0Agenerated%20synthetic%20human%20data.%20GeneMAN%20encompasses%20three%20key%20modules.%201%29%0AWithout%20relying%20on%20parametric%20human%20models%20%28e.g.%2C%20SMPL%29%2C%20GeneMAN%20first%20trains%20a%0Ahuman-specific%20text-to-image%20diffusion%20model%20and%20a%20view-conditioned%20diffusion%0Amodel%2C%20serving%20as%20GeneMAN%202D%20human%20prior%20and%203D%20human%20prior%20for%20reconstruction%2C%0Arespectively.%202%29%20With%20the%20help%20of%20the%20pretrained%20human%20prior%20models%2C%20the%0AGeometry%20Initialization-%26-Sculpting%20pipeline%20is%20leveraged%20to%20recover%0Ahigh-quality%203D%20human%20geometry%20given%20a%20single%20image.%203%29%20To%20achieve%0Ahigh-fidelity%203D%20human%20textures%2C%20GeneMAN%20employs%20the%20Multi-Space%20Texture%0ARefinement%20pipeline%2C%20consecutively%20refining%20textures%20in%20the%20latent%20and%20the%0Apixel%20spaces.%20Extensive%20experimental%20results%20demonstrate%20that%20GeneMAN%20could%0Agenerate%20high-quality%203D%20human%20models%20from%20a%20single%20image%20input%2C%20outperforming%0Aprior%20state-of-the-art%20methods.%20Notably%2C%20GeneMAN%20could%20reveal%20much%20better%0Ageneralizability%20in%20dealing%20with%20in-the-wild%20images%2C%20often%20yielding%0Ahigh-quality%203D%20human%20models%20in%20natural%20poses%20with%20common%20items%2C%20regardless%20of%0Athe%20body%20proportions%20in%20the%20input%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18624v1&entry.124074799=Read"},
{"title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready\n  3D Characters", "author": "Zhiyang Guo and Jinxu Xiang and Kai Ma and Wengang Zhou and Houqiang Li and Ran Zhang", "abstract": "  3D characters are essential to modern creative industries, but making them\nanimatable often demands extensive manual work in tasks like rigging and\nskinning. Existing automatic rigging tools face several limitations, including\nthe necessity for manual annotations, rigid skeleton topologies, and limited\ngeneralization across diverse shapes and poses. An alternative approach is to\ngenerate animatable avatars pre-bound to a rigged template mesh. However, this\nmethod often lacks flexibility and is typically limited to realistic human\nshapes. To address these issues, we present Make-It-Animatable, a novel\ndata-driven method to make any 3D humanoid model ready for character animation\nin less than one second, regardless of its shapes and poses. Our unified\nframework generates high-quality blend weights, bones, and pose\ntransformations. By incorporating a particle-based shape autoencoder, our\napproach supports various 3D representations, including meshes and 3D Gaussian\nsplats. Additionally, we employ a coarse-to-fine representation and a\nstructure-aware modeling strategy to ensure both accuracy and robustness, even\nfor characters with non-standard skeleton structures. We conducted extensive\nexperiments to validate our framework's effectiveness. Compared to existing\nmethods, our approach demonstrates significant improvements in both quality and\nspeed.\n", "link": "http://arxiv.org/abs/2411.18197v1", "date": "2024-11-27", "relevancy": 3.0956, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6697}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5938}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make-It-Animatable%3A%20An%20Efficient%20Framework%20for%20Authoring%20Animation-Ready%0A%20%203D%20Characters&body=Title%3A%20Make-It-Animatable%3A%20An%20Efficient%20Framework%20for%20Authoring%20Animation-Ready%0A%20%203D%20Characters%0AAuthor%3A%20Zhiyang%20Guo%20and%20Jinxu%20Xiang%20and%20Kai%20Ma%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Ran%20Zhang%0AAbstract%3A%20%20%203D%20characters%20are%20essential%20to%20modern%20creative%20industries%2C%20but%20making%20them%0Aanimatable%20often%20demands%20extensive%20manual%20work%20in%20tasks%20like%20rigging%20and%0Askinning.%20Existing%20automatic%20rigging%20tools%20face%20several%20limitations%2C%20including%0Athe%20necessity%20for%20manual%20annotations%2C%20rigid%20skeleton%20topologies%2C%20and%20limited%0Ageneralization%20across%20diverse%20shapes%20and%20poses.%20An%20alternative%20approach%20is%20to%0Agenerate%20animatable%20avatars%20pre-bound%20to%20a%20rigged%20template%20mesh.%20However%2C%20this%0Amethod%20often%20lacks%20flexibility%20and%20is%20typically%20limited%20to%20realistic%20human%0Ashapes.%20To%20address%20these%20issues%2C%20we%20present%20Make-It-Animatable%2C%20a%20novel%0Adata-driven%20method%20to%20make%20any%203D%20humanoid%20model%20ready%20for%20character%20animation%0Ain%20less%20than%20one%20second%2C%20regardless%20of%20its%20shapes%20and%20poses.%20Our%20unified%0Aframework%20generates%20high-quality%20blend%20weights%2C%20bones%2C%20and%20pose%0Atransformations.%20By%20incorporating%20a%20particle-based%20shape%20autoencoder%2C%20our%0Aapproach%20supports%20various%203D%20representations%2C%20including%20meshes%20and%203D%20Gaussian%0Asplats.%20Additionally%2C%20we%20employ%20a%20coarse-to-fine%20representation%20and%20a%0Astructure-aware%20modeling%20strategy%20to%20ensure%20both%20accuracy%20and%20robustness%2C%20even%0Afor%20characters%20with%20non-standard%20skeleton%20structures.%20We%20conducted%20extensive%0Aexperiments%20to%20validate%20our%20framework%27s%20effectiveness.%20Compared%20to%20existing%0Amethods%2C%20our%20approach%20demonstrates%20significant%20improvements%20in%20both%20quality%20and%0Aspeed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake-It-Animatable%253A%2520An%2520Efficient%2520Framework%2520for%2520Authoring%2520Animation-Ready%250A%2520%25203D%2520Characters%26entry.906535625%3DZhiyang%2520Guo%2520and%2520Jinxu%2520Xiang%2520and%2520Kai%2520Ma%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%2520and%2520Ran%2520Zhang%26entry.1292438233%3D%2520%25203D%2520characters%2520are%2520essential%2520to%2520modern%2520creative%2520industries%252C%2520but%2520making%2520them%250Aanimatable%2520often%2520demands%2520extensive%2520manual%2520work%2520in%2520tasks%2520like%2520rigging%2520and%250Askinning.%2520Existing%2520automatic%2520rigging%2520tools%2520face%2520several%2520limitations%252C%2520including%250Athe%2520necessity%2520for%2520manual%2520annotations%252C%2520rigid%2520skeleton%2520topologies%252C%2520and%2520limited%250Ageneralization%2520across%2520diverse%2520shapes%2520and%2520poses.%2520An%2520alternative%2520approach%2520is%2520to%250Agenerate%2520animatable%2520avatars%2520pre-bound%2520to%2520a%2520rigged%2520template%2520mesh.%2520However%252C%2520this%250Amethod%2520often%2520lacks%2520flexibility%2520and%2520is%2520typically%2520limited%2520to%2520realistic%2520human%250Ashapes.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520Make-It-Animatable%252C%2520a%2520novel%250Adata-driven%2520method%2520to%2520make%2520any%25203D%2520humanoid%2520model%2520ready%2520for%2520character%2520animation%250Ain%2520less%2520than%2520one%2520second%252C%2520regardless%2520of%2520its%2520shapes%2520and%2520poses.%2520Our%2520unified%250Aframework%2520generates%2520high-quality%2520blend%2520weights%252C%2520bones%252C%2520and%2520pose%250Atransformations.%2520By%2520incorporating%2520a%2520particle-based%2520shape%2520autoencoder%252C%2520our%250Aapproach%2520supports%2520various%25203D%2520representations%252C%2520including%2520meshes%2520and%25203D%2520Gaussian%250Asplats.%2520Additionally%252C%2520we%2520employ%2520a%2520coarse-to-fine%2520representation%2520and%2520a%250Astructure-aware%2520modeling%2520strategy%2520to%2520ensure%2520both%2520accuracy%2520and%2520robustness%252C%2520even%250Afor%2520characters%2520with%2520non-standard%2520skeleton%2520structures.%2520We%2520conducted%2520extensive%250Aexperiments%2520to%2520validate%2520our%2520framework%2527s%2520effectiveness.%2520Compared%2520to%2520existing%250Amethods%252C%2520our%2520approach%2520demonstrates%2520significant%2520improvements%2520in%2520both%2520quality%2520and%250Aspeed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make-It-Animatable%3A%20An%20Efficient%20Framework%20for%20Authoring%20Animation-Ready%0A%20%203D%20Characters&entry.906535625=Zhiyang%20Guo%20and%20Jinxu%20Xiang%20and%20Kai%20Ma%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Ran%20Zhang&entry.1292438233=%20%203D%20characters%20are%20essential%20to%20modern%20creative%20industries%2C%20but%20making%20them%0Aanimatable%20often%20demands%20extensive%20manual%20work%20in%20tasks%20like%20rigging%20and%0Askinning.%20Existing%20automatic%20rigging%20tools%20face%20several%20limitations%2C%20including%0Athe%20necessity%20for%20manual%20annotations%2C%20rigid%20skeleton%20topologies%2C%20and%20limited%0Ageneralization%20across%20diverse%20shapes%20and%20poses.%20An%20alternative%20approach%20is%20to%0Agenerate%20animatable%20avatars%20pre-bound%20to%20a%20rigged%20template%20mesh.%20However%2C%20this%0Amethod%20often%20lacks%20flexibility%20and%20is%20typically%20limited%20to%20realistic%20human%0Ashapes.%20To%20address%20these%20issues%2C%20we%20present%20Make-It-Animatable%2C%20a%20novel%0Adata-driven%20method%20to%20make%20any%203D%20humanoid%20model%20ready%20for%20character%20animation%0Ain%20less%20than%20one%20second%2C%20regardless%20of%20its%20shapes%20and%20poses.%20Our%20unified%0Aframework%20generates%20high-quality%20blend%20weights%2C%20bones%2C%20and%20pose%0Atransformations.%20By%20incorporating%20a%20particle-based%20shape%20autoencoder%2C%20our%0Aapproach%20supports%20various%203D%20representations%2C%20including%20meshes%20and%203D%20Gaussian%0Asplats.%20Additionally%2C%20we%20employ%20a%20coarse-to-fine%20representation%20and%20a%0Astructure-aware%20modeling%20strategy%20to%20ensure%20both%20accuracy%20and%20robustness%2C%20even%0Afor%20characters%20with%20non-standard%20skeleton%20structures.%20We%20conducted%20extensive%0Aexperiments%20to%20validate%20our%20framework%27s%20effectiveness.%20Compared%20to%20existing%0Amethods%2C%20our%20approach%20demonstrates%20significant%20improvements%20in%20both%20quality%20and%0Aspeed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18197v1&entry.124074799=Read"},
{"title": "From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects", "author": "Zizhao Li and Zhengkang Xiang and Joseph West and Kourosh Khoshelham", "abstract": "  Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks.\n", "link": "http://arxiv.org/abs/2411.18207v1", "date": "2024-11-27", "relevancy": 3.0212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6176}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Open%20Vocabulary%20to%20Open%20World%3A%20Teaching%20Vision%20Language%20Models%20to%0A%20%20Detect%20Novel%20Objects&body=Title%3A%20From%20Open%20Vocabulary%20to%20Open%20World%3A%20Teaching%20Vision%20Language%20Models%20to%0A%20%20Detect%20Novel%20Objects%0AAuthor%3A%20Zizhao%20Li%20and%20Zhengkang%20Xiang%20and%20Joseph%20West%20and%20Kourosh%20Khoshelham%0AAbstract%3A%20%20%20Traditional%20object%20detection%20methods%20operate%20under%20the%20closed-set%20assumption%2C%0Awhere%20models%20can%20only%20detect%20a%20fixed%20number%20of%20objects%20predefined%20in%20the%0Atraining%20set.%20Recent%20works%20on%20open%20vocabulary%20object%20detection%20%28OVD%29%20enable%20the%0Adetection%20of%20objects%20defined%20by%20an%20unbounded%20vocabulary%2C%20which%20reduces%20the%20cost%0Aof%20training%20models%20for%20specific%20tasks.%20However%2C%20OVD%20heavily%20relies%20on%20accurate%0Aprompts%20provided%20by%20an%20%27%27oracle%27%27%2C%20which%20limits%20their%20use%20in%20critical%0Aapplications%20such%20as%20driving%20scene%20perception.%20OVD%20models%20tend%20to%20misclassify%0Anear-out-of-distribution%20%28NOOD%29%20objects%20that%20have%20similar%20semantics%20to%20known%0Aclasses%2C%20and%20ignore%20far-out-of-distribution%20%28FOOD%29%20objects.%20To%20address%20theses%0Alimitations%2C%20we%20propose%20a%20framework%20that%20enables%20OVD%20models%20to%20operate%20in%20open%0Aworld%20settings%2C%20by%20identifying%20and%20incrementally%20learning%20novel%20objects.%20To%0Adetect%20FOOD%20objects%2C%20we%20propose%20Open%20World%20Embedding%20Learning%20%28OWEL%29%20and%0Aintroduce%20the%20concept%20of%20Pseudo%20Unknown%20Embedding%20which%20infers%20the%20location%20of%0Aunknown%20classes%20in%20a%20continuous%20semantic%20space%20based%20on%20the%20information%20of%0Aknown%20classes.%20We%20also%20propose%20Multi-Scale%20Contrastive%20Anchor%20Learning%20%28MSCAL%29%2C%0Awhich%20enables%20the%20identification%20of%20misclassified%20unknown%20objects%20by%20promoting%0Athe%20intra-class%20consistency%20of%20object%20embeddings%20at%20different%20scales.%20The%0Aproposed%20method%20achieves%20state-of-the-art%20performance%20in%20common%20open%20world%0Aobject%20detection%20and%20autonomous%20driving%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Open%2520Vocabulary%2520to%2520Open%2520World%253A%2520Teaching%2520Vision%2520Language%2520Models%2520to%250A%2520%2520Detect%2520Novel%2520Objects%26entry.906535625%3DZizhao%2520Li%2520and%2520Zhengkang%2520Xiang%2520and%2520Joseph%2520West%2520and%2520Kourosh%2520Khoshelham%26entry.1292438233%3D%2520%2520Traditional%2520object%2520detection%2520methods%2520operate%2520under%2520the%2520closed-set%2520assumption%252C%250Awhere%2520models%2520can%2520only%2520detect%2520a%2520fixed%2520number%2520of%2520objects%2520predefined%2520in%2520the%250Atraining%2520set.%2520Recent%2520works%2520on%2520open%2520vocabulary%2520object%2520detection%2520%2528OVD%2529%2520enable%2520the%250Adetection%2520of%2520objects%2520defined%2520by%2520an%2520unbounded%2520vocabulary%252C%2520which%2520reduces%2520the%2520cost%250Aof%2520training%2520models%2520for%2520specific%2520tasks.%2520However%252C%2520OVD%2520heavily%2520relies%2520on%2520accurate%250Aprompts%2520provided%2520by%2520an%2520%2527%2527oracle%2527%2527%252C%2520which%2520limits%2520their%2520use%2520in%2520critical%250Aapplications%2520such%2520as%2520driving%2520scene%2520perception.%2520OVD%2520models%2520tend%2520to%2520misclassify%250Anear-out-of-distribution%2520%2528NOOD%2529%2520objects%2520that%2520have%2520similar%2520semantics%2520to%2520known%250Aclasses%252C%2520and%2520ignore%2520far-out-of-distribution%2520%2528FOOD%2529%2520objects.%2520To%2520address%2520theses%250Alimitations%252C%2520we%2520propose%2520a%2520framework%2520that%2520enables%2520OVD%2520models%2520to%2520operate%2520in%2520open%250Aworld%2520settings%252C%2520by%2520identifying%2520and%2520incrementally%2520learning%2520novel%2520objects.%2520To%250Adetect%2520FOOD%2520objects%252C%2520we%2520propose%2520Open%2520World%2520Embedding%2520Learning%2520%2528OWEL%2529%2520and%250Aintroduce%2520the%2520concept%2520of%2520Pseudo%2520Unknown%2520Embedding%2520which%2520infers%2520the%2520location%2520of%250Aunknown%2520classes%2520in%2520a%2520continuous%2520semantic%2520space%2520based%2520on%2520the%2520information%2520of%250Aknown%2520classes.%2520We%2520also%2520propose%2520Multi-Scale%2520Contrastive%2520Anchor%2520Learning%2520%2528MSCAL%2529%252C%250Awhich%2520enables%2520the%2520identification%2520of%2520misclassified%2520unknown%2520objects%2520by%2520promoting%250Athe%2520intra-class%2520consistency%2520of%2520object%2520embeddings%2520at%2520different%2520scales.%2520The%250Aproposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520common%2520open%2520world%250Aobject%2520detection%2520and%2520autonomous%2520driving%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Open%20Vocabulary%20to%20Open%20World%3A%20Teaching%20Vision%20Language%20Models%20to%0A%20%20Detect%20Novel%20Objects&entry.906535625=Zizhao%20Li%20and%20Zhengkang%20Xiang%20and%20Joseph%20West%20and%20Kourosh%20Khoshelham&entry.1292438233=%20%20Traditional%20object%20detection%20methods%20operate%20under%20the%20closed-set%20assumption%2C%0Awhere%20models%20can%20only%20detect%20a%20fixed%20number%20of%20objects%20predefined%20in%20the%0Atraining%20set.%20Recent%20works%20on%20open%20vocabulary%20object%20detection%20%28OVD%29%20enable%20the%0Adetection%20of%20objects%20defined%20by%20an%20unbounded%20vocabulary%2C%20which%20reduces%20the%20cost%0Aof%20training%20models%20for%20specific%20tasks.%20However%2C%20OVD%20heavily%20relies%20on%20accurate%0Aprompts%20provided%20by%20an%20%27%27oracle%27%27%2C%20which%20limits%20their%20use%20in%20critical%0Aapplications%20such%20as%20driving%20scene%20perception.%20OVD%20models%20tend%20to%20misclassify%0Anear-out-of-distribution%20%28NOOD%29%20objects%20that%20have%20similar%20semantics%20to%20known%0Aclasses%2C%20and%20ignore%20far-out-of-distribution%20%28FOOD%29%20objects.%20To%20address%20theses%0Alimitations%2C%20we%20propose%20a%20framework%20that%20enables%20OVD%20models%20to%20operate%20in%20open%0Aworld%20settings%2C%20by%20identifying%20and%20incrementally%20learning%20novel%20objects.%20To%0Adetect%20FOOD%20objects%2C%20we%20propose%20Open%20World%20Embedding%20Learning%20%28OWEL%29%20and%0Aintroduce%20the%20concept%20of%20Pseudo%20Unknown%20Embedding%20which%20infers%20the%20location%20of%0Aunknown%20classes%20in%20a%20continuous%20semantic%20space%20based%20on%20the%20information%20of%0Aknown%20classes.%20We%20also%20propose%20Multi-Scale%20Contrastive%20Anchor%20Learning%20%28MSCAL%29%2C%0Awhich%20enables%20the%20identification%20of%20misclassified%20unknown%20objects%20by%20promoting%0Athe%20intra-class%20consistency%20of%20object%20embeddings%20at%20different%20scales.%20The%0Aproposed%20method%20achieves%20state-of-the-art%20performance%20in%20common%20open%20world%0Aobject%20detection%20and%20autonomous%20driving%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18207v1&entry.124074799=Read"},
{"title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding", "author": "Qing Jiang and Gen luo and Yuqin Yang and Yuda Xiong and Yihao Chen and Zhaoyang Zeng and Tianhe Ren and Lei Zhang", "abstract": "  Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.\n", "link": "http://arxiv.org/abs/2411.18363v1", "date": "2024-11-27", "relevancy": 3.0029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatRex%3A%20Taming%20Multimodal%20LLM%20for%20Joint%20Perception%20and%20Understanding&body=Title%3A%20ChatRex%3A%20Taming%20Multimodal%20LLM%20for%20Joint%20Perception%20and%20Understanding%0AAuthor%3A%20Qing%20Jiang%20and%20Gen%20luo%20and%20Yuqin%20Yang%20and%20Yuda%20Xiong%20and%20Yihao%20Chen%20and%20Zhaoyang%20Zeng%20and%20Tianhe%20Ren%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Perception%20and%20understanding%20are%20two%20pillars%20of%20computer%20vision.%20While%0Amultimodal%20large%20language%20models%20%28MLLM%29%20have%20demonstrated%20remarkable%20visual%0Aunderstanding%20capabilities%2C%20they%20arguably%20lack%20accurate%20perception%20abilities%2C%0Ae.g.%20the%20stage-of-the-art%20model%20Qwen2-VL%20only%20achieves%20a%2043.9%20recall%20rate%20on%0Athe%20COCO%20dataset%2C%20limiting%20many%20tasks%20requiring%20the%20combination%20of%20perception%0Aand%20understanding.%20In%20this%20work%2C%20we%20aim%20to%20bridge%20this%20perception%20gap%20from%20both%0Amodel%20designing%20and%20data%20development%20perspectives.%20We%20first%20introduce%20ChatRex%2C%0Aan%20MLLM%20with%20a%20decoupled%20perception%20design.%20Instead%20of%20having%20the%20LLM%20directly%0Apredict%20box%20coordinates%2C%20we%20feed%20the%20output%20boxes%20from%20a%20universal%20proposal%0Anetwork%20into%20the%20LLM%2C%20allowing%20it%20to%20output%20the%20corresponding%20box%20indices%20to%0Arepresent%20its%20detection%20results%2C%20turning%20the%20regression%20task%20into%20a%0Aretrieval-based%20task%20that%20LLM%20handles%20more%20proficiently.%20From%20the%20data%0Aperspective%2C%20we%20build%20a%20fully%20automated%20data%20engine%20and%20construct%20the%0ARexverse-2M%20dataset%20which%20possesses%20multiple%20granularities%20to%20support%20the%20joint%0Atraining%20of%20perception%20and%20understanding.%20After%20standard%20two-stage%20training%2C%0AChatRex%20demonstrates%20strong%20perception%20capabilities%20while%20preserving%20multimodal%0Aunderstanding%20performance.%20The%20combination%20of%20these%20two%20capabilities%0Asimultaneously%20unlocks%20many%20attractive%20applications%2C%20demonstrating%20the%0Acomplementary%20roles%20of%20both%20perception%20and%20understanding%20in%20MLLM.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/IDEA-Research/ChatRex%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatRex%253A%2520Taming%2520Multimodal%2520LLM%2520for%2520Joint%2520Perception%2520and%2520Understanding%26entry.906535625%3DQing%2520Jiang%2520and%2520Gen%2520luo%2520and%2520Yuqin%2520Yang%2520and%2520Yuda%2520Xiong%2520and%2520Yihao%2520Chen%2520and%2520Zhaoyang%2520Zeng%2520and%2520Tianhe%2520Ren%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Perception%2520and%2520understanding%2520are%2520two%2520pillars%2520of%2520computer%2520vision.%2520While%250Amultimodal%2520large%2520language%2520models%2520%2528MLLM%2529%2520have%2520demonstrated%2520remarkable%2520visual%250Aunderstanding%2520capabilities%252C%2520they%2520arguably%2520lack%2520accurate%2520perception%2520abilities%252C%250Ae.g.%2520the%2520stage-of-the-art%2520model%2520Qwen2-VL%2520only%2520achieves%2520a%252043.9%2520recall%2520rate%2520on%250Athe%2520COCO%2520dataset%252C%2520limiting%2520many%2520tasks%2520requiring%2520the%2520combination%2520of%2520perception%250Aand%2520understanding.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520bridge%2520this%2520perception%2520gap%2520from%2520both%250Amodel%2520designing%2520and%2520data%2520development%2520perspectives.%2520We%2520first%2520introduce%2520ChatRex%252C%250Aan%2520MLLM%2520with%2520a%2520decoupled%2520perception%2520design.%2520Instead%2520of%2520having%2520the%2520LLM%2520directly%250Apredict%2520box%2520coordinates%252C%2520we%2520feed%2520the%2520output%2520boxes%2520from%2520a%2520universal%2520proposal%250Anetwork%2520into%2520the%2520LLM%252C%2520allowing%2520it%2520to%2520output%2520the%2520corresponding%2520box%2520indices%2520to%250Arepresent%2520its%2520detection%2520results%252C%2520turning%2520the%2520regression%2520task%2520into%2520a%250Aretrieval-based%2520task%2520that%2520LLM%2520handles%2520more%2520proficiently.%2520From%2520the%2520data%250Aperspective%252C%2520we%2520build%2520a%2520fully%2520automated%2520data%2520engine%2520and%2520construct%2520the%250ARexverse-2M%2520dataset%2520which%2520possesses%2520multiple%2520granularities%2520to%2520support%2520the%2520joint%250Atraining%2520of%2520perception%2520and%2520understanding.%2520After%2520standard%2520two-stage%2520training%252C%250AChatRex%2520demonstrates%2520strong%2520perception%2520capabilities%2520while%2520preserving%2520multimodal%250Aunderstanding%2520performance.%2520The%2520combination%2520of%2520these%2520two%2520capabilities%250Asimultaneously%2520unlocks%2520many%2520attractive%2520applications%252C%2520demonstrating%2520the%250Acomplementary%2520roles%2520of%2520both%2520perception%2520and%2520understanding%2520in%2520MLLM.%2520Code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/IDEA-Research/ChatRex%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatRex%3A%20Taming%20Multimodal%20LLM%20for%20Joint%20Perception%20and%20Understanding&entry.906535625=Qing%20Jiang%20and%20Gen%20luo%20and%20Yuqin%20Yang%20and%20Yuda%20Xiong%20and%20Yihao%20Chen%20and%20Zhaoyang%20Zeng%20and%20Tianhe%20Ren%20and%20Lei%20Zhang&entry.1292438233=%20%20Perception%20and%20understanding%20are%20two%20pillars%20of%20computer%20vision.%20While%0Amultimodal%20large%20language%20models%20%28MLLM%29%20have%20demonstrated%20remarkable%20visual%0Aunderstanding%20capabilities%2C%20they%20arguably%20lack%20accurate%20perception%20abilities%2C%0Ae.g.%20the%20stage-of-the-art%20model%20Qwen2-VL%20only%20achieves%20a%2043.9%20recall%20rate%20on%0Athe%20COCO%20dataset%2C%20limiting%20many%20tasks%20requiring%20the%20combination%20of%20perception%0Aand%20understanding.%20In%20this%20work%2C%20we%20aim%20to%20bridge%20this%20perception%20gap%20from%20both%0Amodel%20designing%20and%20data%20development%20perspectives.%20We%20first%20introduce%20ChatRex%2C%0Aan%20MLLM%20with%20a%20decoupled%20perception%20design.%20Instead%20of%20having%20the%20LLM%20directly%0Apredict%20box%20coordinates%2C%20we%20feed%20the%20output%20boxes%20from%20a%20universal%20proposal%0Anetwork%20into%20the%20LLM%2C%20allowing%20it%20to%20output%20the%20corresponding%20box%20indices%20to%0Arepresent%20its%20detection%20results%2C%20turning%20the%20regression%20task%20into%20a%0Aretrieval-based%20task%20that%20LLM%20handles%20more%20proficiently.%20From%20the%20data%0Aperspective%2C%20we%20build%20a%20fully%20automated%20data%20engine%20and%20construct%20the%0ARexverse-2M%20dataset%20which%20possesses%20multiple%20granularities%20to%20support%20the%20joint%0Atraining%20of%20perception%20and%20understanding.%20After%20standard%20two-stage%20training%2C%0AChatRex%20demonstrates%20strong%20perception%20capabilities%20while%20preserving%20multimodal%0Aunderstanding%20performance.%20The%20combination%20of%20these%20two%20capabilities%0Asimultaneously%20unlocks%20many%20attractive%20applications%2C%20demonstrating%20the%0Acomplementary%20roles%20of%20both%20perception%20and%20understanding%20in%20MLLM.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/IDEA-Research/ChatRex%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18363v1&entry.124074799=Read"},
{"title": "Grid-augumented vision: A simple yet effective approach for enhanced\n  spatial understanding in multi-modal agents", "author": "Joongwon Chae and Zhenyu Wang and Peiwu Qin", "abstract": "  Recent advances in multimodal models have demonstrated impressive\ncapabilities in object recognition and scene understanding. However, these\nmodels often struggle with precise spatial localization - a critical capability\nfor real-world applications. Inspired by how humans use grid-based references\nlike chess boards and maps, we propose introducing explicit visual position\nencoding through a simple grid overlay approach. By adding a 9x9 black grid\npattern onto input images, our method provides visual spatial guidance\nanalogous to how positional encoding works in transformers, but in an explicit,\nvisual form.\n  Experiments on the COCO 2017 dataset demonstrate that our grid-based approach\nachieves significant improvements in localization accuracy, with a 107.4%\nincrease in IoU (from 0.27 to 0.56) and a 194.4% improvement in GIoU (from 0.18\nto 0.53) compared to baseline performance. Through attention visualization\nanalysis, we show how this visual position encoding helps models better ground\nspatial relationships. Our method's simplicity and effectiveness make it\nparticularly valuable for applications requiring accurate spatial reasoning,\nsuch as robotic manipulation, medical imaging, and autonomous navigation.\n", "link": "http://arxiv.org/abs/2411.18270v1", "date": "2024-11-27", "relevancy": 2.9591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5928}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grid-augumented%20vision%3A%20A%20simple%20yet%20effective%20approach%20for%20enhanced%0A%20%20spatial%20understanding%20in%20multi-modal%20agents&body=Title%3A%20Grid-augumented%20vision%3A%20A%20simple%20yet%20effective%20approach%20for%20enhanced%0A%20%20spatial%20understanding%20in%20multi-modal%20agents%0AAuthor%3A%20Joongwon%20Chae%20and%20Zhenyu%20Wang%20and%20Peiwu%20Qin%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20object%20recognition%20and%20scene%20understanding.%20However%2C%20these%0Amodels%20often%20struggle%20with%20precise%20spatial%20localization%20-%20a%20critical%20capability%0Afor%20real-world%20applications.%20Inspired%20by%20how%20humans%20use%20grid-based%20references%0Alike%20chess%20boards%20and%20maps%2C%20we%20propose%20introducing%20explicit%20visual%20position%0Aencoding%20through%20a%20simple%20grid%20overlay%20approach.%20By%20adding%20a%209x9%20black%20grid%0Apattern%20onto%20input%20images%2C%20our%20method%20provides%20visual%20spatial%20guidance%0Aanalogous%20to%20how%20positional%20encoding%20works%20in%20transformers%2C%20but%20in%20an%20explicit%2C%0Avisual%20form.%0A%20%20Experiments%20on%20the%20COCO%202017%20dataset%20demonstrate%20that%20our%20grid-based%20approach%0Aachieves%20significant%20improvements%20in%20localization%20accuracy%2C%20with%20a%20107.4%25%0Aincrease%20in%20IoU%20%28from%200.27%20to%200.56%29%20and%20a%20194.4%25%20improvement%20in%20GIoU%20%28from%200.18%0Ato%200.53%29%20compared%20to%20baseline%20performance.%20Through%20attention%20visualization%0Aanalysis%2C%20we%20show%20how%20this%20visual%20position%20encoding%20helps%20models%20better%20ground%0Aspatial%20relationships.%20Our%20method%27s%20simplicity%20and%20effectiveness%20make%20it%0Aparticularly%20valuable%20for%20applications%20requiring%20accurate%20spatial%20reasoning%2C%0Asuch%20as%20robotic%20manipulation%2C%20medical%20imaging%2C%20and%20autonomous%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrid-augumented%2520vision%253A%2520A%2520simple%2520yet%2520effective%2520approach%2520for%2520enhanced%250A%2520%2520spatial%2520understanding%2520in%2520multi-modal%2520agents%26entry.906535625%3DJoongwon%2520Chae%2520and%2520Zhenyu%2520Wang%2520and%2520Peiwu%2520Qin%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520models%2520have%2520demonstrated%2520impressive%250Acapabilities%2520in%2520object%2520recognition%2520and%2520scene%2520understanding.%2520However%252C%2520these%250Amodels%2520often%2520struggle%2520with%2520precise%2520spatial%2520localization%2520-%2520a%2520critical%2520capability%250Afor%2520real-world%2520applications.%2520Inspired%2520by%2520how%2520humans%2520use%2520grid-based%2520references%250Alike%2520chess%2520boards%2520and%2520maps%252C%2520we%2520propose%2520introducing%2520explicit%2520visual%2520position%250Aencoding%2520through%2520a%2520simple%2520grid%2520overlay%2520approach.%2520By%2520adding%2520a%25209x9%2520black%2520grid%250Apattern%2520onto%2520input%2520images%252C%2520our%2520method%2520provides%2520visual%2520spatial%2520guidance%250Aanalogous%2520to%2520how%2520positional%2520encoding%2520works%2520in%2520transformers%252C%2520but%2520in%2520an%2520explicit%252C%250Avisual%2520form.%250A%2520%2520Experiments%2520on%2520the%2520COCO%25202017%2520dataset%2520demonstrate%2520that%2520our%2520grid-based%2520approach%250Aachieves%2520significant%2520improvements%2520in%2520localization%2520accuracy%252C%2520with%2520a%2520107.4%2525%250Aincrease%2520in%2520IoU%2520%2528from%25200.27%2520to%25200.56%2529%2520and%2520a%2520194.4%2525%2520improvement%2520in%2520GIoU%2520%2528from%25200.18%250Ato%25200.53%2529%2520compared%2520to%2520baseline%2520performance.%2520Through%2520attention%2520visualization%250Aanalysis%252C%2520we%2520show%2520how%2520this%2520visual%2520position%2520encoding%2520helps%2520models%2520better%2520ground%250Aspatial%2520relationships.%2520Our%2520method%2527s%2520simplicity%2520and%2520effectiveness%2520make%2520it%250Aparticularly%2520valuable%2520for%2520applications%2520requiring%2520accurate%2520spatial%2520reasoning%252C%250Asuch%2520as%2520robotic%2520manipulation%252C%2520medical%2520imaging%252C%2520and%2520autonomous%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid-augumented%20vision%3A%20A%20simple%20yet%20effective%20approach%20for%20enhanced%0A%20%20spatial%20understanding%20in%20multi-modal%20agents&entry.906535625=Joongwon%20Chae%20and%20Zhenyu%20Wang%20and%20Peiwu%20Qin&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20object%20recognition%20and%20scene%20understanding.%20However%2C%20these%0Amodels%20often%20struggle%20with%20precise%20spatial%20localization%20-%20a%20critical%20capability%0Afor%20real-world%20applications.%20Inspired%20by%20how%20humans%20use%20grid-based%20references%0Alike%20chess%20boards%20and%20maps%2C%20we%20propose%20introducing%20explicit%20visual%20position%0Aencoding%20through%20a%20simple%20grid%20overlay%20approach.%20By%20adding%20a%209x9%20black%20grid%0Apattern%20onto%20input%20images%2C%20our%20method%20provides%20visual%20spatial%20guidance%0Aanalogous%20to%20how%20positional%20encoding%20works%20in%20transformers%2C%20but%20in%20an%20explicit%2C%0Avisual%20form.%0A%20%20Experiments%20on%20the%20COCO%202017%20dataset%20demonstrate%20that%20our%20grid-based%20approach%0Aachieves%20significant%20improvements%20in%20localization%20accuracy%2C%20with%20a%20107.4%25%0Aincrease%20in%20IoU%20%28from%200.27%20to%200.56%29%20and%20a%20194.4%25%20improvement%20in%20GIoU%20%28from%200.18%0Ato%200.53%29%20compared%20to%20baseline%20performance.%20Through%20attention%20visualization%0Aanalysis%2C%20we%20show%20how%20this%20visual%20position%20encoding%20helps%20models%20better%20ground%0Aspatial%20relationships.%20Our%20method%27s%20simplicity%20and%20effectiveness%20make%20it%0Aparticularly%20valuable%20for%20applications%20requiring%20accurate%20spatial%20reasoning%2C%0Asuch%20as%20robotic%20manipulation%2C%20medical%20imaging%2C%20and%20autonomous%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18270v1&entry.124074799=Read"},
{"title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in\n  Open-Vocabulary Image Segmentation", "author": "Yuanbing Zhu and Bingke Zhu and Yingying Chen and Yunfang Niu and Ming Tang and Jinqiao Wang", "abstract": "  Pretrained vision-language models (VLMs), \\eg CLIP, are increasingly used to\nbridge the gap between open- and close-vocabulary recognition in\nopen-vocabulary image segmentation. As VLMs are generally pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. A typical solution is to employ additional image\nbackbones for high-resolution inputs, but it also introduce significant\ncomputation overhead. Therefore, we propose MROVSeg, a multi-resolution\ntraining framework for open-vocabulary image segmentation with a single\npretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by interacting with multi-resolution features. To achieve\naccurate segmentation, we introduce Multi-grained Masked Attention scheme to\naggregate multi-grained semantics from multi-resolution CLIP features to object\nqueries. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary image segmentation benchmarks,\nestablishing new standards for open-vocabulary image segmentation.\n", "link": "http://arxiv.org/abs/2408.14776v2", "date": "2024-11-27", "relevancy": 2.9413, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MROVSeg%3A%20Breaking%20the%20Resolution%20Curse%20of%20Vision-Language%20Models%20in%0A%20%20Open-Vocabulary%20Image%20Segmentation&body=Title%3A%20MROVSeg%3A%20Breaking%20the%20Resolution%20Curse%20of%20Vision-Language%20Models%20in%0A%20%20Open-Vocabulary%20Image%20Segmentation%0AAuthor%3A%20Yuanbing%20Zhu%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Yunfang%20Niu%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20%5Ceg%20CLIP%2C%20are%20increasingly%20used%20to%0Abridge%20the%20gap%20between%20open-%20and%20close-vocabulary%20recognition%20in%0Aopen-vocabulary%20image%20segmentation.%20As%20VLMs%20are%20generally%20pretrained%20with%0Alow-resolution%20images%20%28e.g.%20%24224%5Ctimes224%24%29%2C%20most%20previous%20methods%20operate%20only%0Aon%20downscaled%20images.%20We%20question%20this%20design%20as%20low%20resolution%20features%20often%0Afail%20to%20preserve%20fine%20details.%20A%20typical%20solution%20is%20to%20employ%20additional%20image%0Abackbones%20for%20high-resolution%20inputs%2C%20but%20it%20also%20introduce%20significant%0Acomputation%20overhead.%20Therefore%2C%20we%20propose%20MROVSeg%2C%20a%20multi-resolution%0Atraining%20framework%20for%20open-vocabulary%20image%20segmentation%20with%20a%20single%0Apretrained%20CLIP%20backbone%2C%20that%20uses%20sliding%20windows%20to%20slice%20the%0Ahigh-resolution%20input%20into%20uniform%20patches%2C%20each%20matching%20the%20input%20size%20of%20the%0Awell-trained%20image%20encoder.%20Its%20key%20components%20include%20a%20Multi-Res%20Adapter%2C%0Awhich%20restores%20the%20spatial%20geometry%20and%20grasps%20local-global%20correspondences%0Aacross%20patches%20by%20interacting%20with%20multi-resolution%20features.%20To%20achieve%0Aaccurate%20segmentation%2C%20we%20introduce%20Multi-grained%20Masked%20Attention%20scheme%20to%0Aaggregate%20multi-grained%20semantics%20from%20multi-resolution%20CLIP%20features%20to%20object%0Aqueries.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20the%20superiority%20of%0AMROVSeg%20on%20well-established%20open-vocabulary%20image%20segmentation%20benchmarks%2C%0Aestablishing%20new%20standards%20for%20open-vocabulary%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMROVSeg%253A%2520Breaking%2520the%2520Resolution%2520Curse%2520of%2520Vision-Language%2520Models%2520in%250A%2520%2520Open-Vocabulary%2520Image%2520Segmentation%26entry.906535625%3DYuanbing%2520Zhu%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Yunfang%2520Niu%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520%255Ceg%2520CLIP%252C%2520are%2520increasingly%2520used%2520to%250Abridge%2520the%2520gap%2520between%2520open-%2520and%2520close-vocabulary%2520recognition%2520in%250Aopen-vocabulary%2520image%2520segmentation.%2520As%2520VLMs%2520are%2520generally%2520pretrained%2520with%250Alow-resolution%2520images%2520%2528e.g.%2520%2524224%255Ctimes224%2524%2529%252C%2520most%2520previous%2520methods%2520operate%2520only%250Aon%2520downscaled%2520images.%2520We%2520question%2520this%2520design%2520as%2520low%2520resolution%2520features%2520often%250Afail%2520to%2520preserve%2520fine%2520details.%2520A%2520typical%2520solution%2520is%2520to%2520employ%2520additional%2520image%250Abackbones%2520for%2520high-resolution%2520inputs%252C%2520but%2520it%2520also%2520introduce%2520significant%250Acomputation%2520overhead.%2520Therefore%252C%2520we%2520propose%2520MROVSeg%252C%2520a%2520multi-resolution%250Atraining%2520framework%2520for%2520open-vocabulary%2520image%2520segmentation%2520with%2520a%2520single%250Apretrained%2520CLIP%2520backbone%252C%2520that%2520uses%2520sliding%2520windows%2520to%2520slice%2520the%250Ahigh-resolution%2520input%2520into%2520uniform%2520patches%252C%2520each%2520matching%2520the%2520input%2520size%2520of%2520the%250Awell-trained%2520image%2520encoder.%2520Its%2520key%2520components%2520include%2520a%2520Multi-Res%2520Adapter%252C%250Awhich%2520restores%2520the%2520spatial%2520geometry%2520and%2520grasps%2520local-global%2520correspondences%250Aacross%2520patches%2520by%2520interacting%2520with%2520multi-resolution%2520features.%2520To%2520achieve%250Aaccurate%2520segmentation%252C%2520we%2520introduce%2520Multi-grained%2520Masked%2520Attention%2520scheme%2520to%250Aaggregate%2520multi-grained%2520semantics%2520from%2520multi-resolution%2520CLIP%2520features%2520to%2520object%250Aqueries.%2520Through%2520comprehensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520superiority%2520of%250AMROVSeg%2520on%2520well-established%2520open-vocabulary%2520image%2520segmentation%2520benchmarks%252C%250Aestablishing%2520new%2520standards%2520for%2520open-vocabulary%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MROVSeg%3A%20Breaking%20the%20Resolution%20Curse%20of%20Vision-Language%20Models%20in%0A%20%20Open-Vocabulary%20Image%20Segmentation&entry.906535625=Yuanbing%20Zhu%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Yunfang%20Niu%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20%5Ceg%20CLIP%2C%20are%20increasingly%20used%20to%0Abridge%20the%20gap%20between%20open-%20and%20close-vocabulary%20recognition%20in%0Aopen-vocabulary%20image%20segmentation.%20As%20VLMs%20are%20generally%20pretrained%20with%0Alow-resolution%20images%20%28e.g.%20%24224%5Ctimes224%24%29%2C%20most%20previous%20methods%20operate%20only%0Aon%20downscaled%20images.%20We%20question%20this%20design%20as%20low%20resolution%20features%20often%0Afail%20to%20preserve%20fine%20details.%20A%20typical%20solution%20is%20to%20employ%20additional%20image%0Abackbones%20for%20high-resolution%20inputs%2C%20but%20it%20also%20introduce%20significant%0Acomputation%20overhead.%20Therefore%2C%20we%20propose%20MROVSeg%2C%20a%20multi-resolution%0Atraining%20framework%20for%20open-vocabulary%20image%20segmentation%20with%20a%20single%0Apretrained%20CLIP%20backbone%2C%20that%20uses%20sliding%20windows%20to%20slice%20the%0Ahigh-resolution%20input%20into%20uniform%20patches%2C%20each%20matching%20the%20input%20size%20of%20the%0Awell-trained%20image%20encoder.%20Its%20key%20components%20include%20a%20Multi-Res%20Adapter%2C%0Awhich%20restores%20the%20spatial%20geometry%20and%20grasps%20local-global%20correspondences%0Aacross%20patches%20by%20interacting%20with%20multi-resolution%20features.%20To%20achieve%0Aaccurate%20segmentation%2C%20we%20introduce%20Multi-grained%20Masked%20Attention%20scheme%20to%0Aaggregate%20multi-grained%20semantics%20from%20multi-resolution%20CLIP%20features%20to%20object%0Aqueries.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20the%20superiority%20of%0AMROVSeg%20on%20well-established%20open-vocabulary%20image%20segmentation%20benchmarks%2C%0Aestablishing%20new%20standards%20for%20open-vocabulary%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14776v2&entry.124074799=Read"},
{"title": "ViTOC: Vision Transformer and Object-aware Captioner", "author": "Feiyang Huang", "abstract": "  This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a\nnovel vision-language model for image captioning that addresses the challenges\nof accuracy and diversity in generated descriptions. Unlike conventional\napproaches, ViTOC employs a dual-path architecture based on Vision Transformer\nand object detector, effectively fusing global visual features and local object\ninformation through learnable vectors. The model introduces an innovative\nobject-aware prompting strategy that significantly enhances its capability in\nhandling long-tail data. Experiments on the standard COCO dataset demonstrate\nthat ViTOC outperforms baseline models across all evaluation metrics.\nAdditionally, we propose a reference-free evaluation method based on CLIP to\nfurther validate the model's effectiveness. By utilizing pretrained visual\nmodel parameters, ViTOC achieves efficient end-to-end training.\n", "link": "http://arxiv.org/abs/2411.07265v3", "date": "2024-11-27", "relevancy": 2.8934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner&body=Title%3A%20ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner%0AAuthor%3A%20Feiyang%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20ViTOC%20%28Vision%20Transformer%20and%20Object-aware%20Captioner%29%2C%20a%0Anovel%20vision-language%20model%20for%20image%20captioning%20that%20addresses%20the%20challenges%0Aof%20accuracy%20and%20diversity%20in%20generated%20descriptions.%20Unlike%20conventional%0Aapproaches%2C%20ViTOC%20employs%20a%20dual-path%20architecture%20based%20on%20Vision%20Transformer%0Aand%20object%20detector%2C%20effectively%20fusing%20global%20visual%20features%20and%20local%20object%0Ainformation%20through%20learnable%20vectors.%20The%20model%20introduces%20an%20innovative%0Aobject-aware%20prompting%20strategy%20that%20significantly%20enhances%20its%20capability%20in%0Ahandling%20long-tail%20data.%20Experiments%20on%20the%20standard%20COCO%20dataset%20demonstrate%0Athat%20ViTOC%20outperforms%20baseline%20models%20across%20all%20evaluation%20metrics.%0AAdditionally%2C%20we%20propose%20a%20reference-free%20evaluation%20method%20based%20on%20CLIP%20to%0Afurther%20validate%20the%20model%27s%20effectiveness.%20By%20utilizing%20pretrained%20visual%0Amodel%20parameters%2C%20ViTOC%20achieves%20efficient%20end-to-end%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07265v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTOC%253A%2520Vision%2520Transformer%2520and%2520Object-aware%2520Captioner%26entry.906535625%3DFeiyang%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ViTOC%2520%2528Vision%2520Transformer%2520and%2520Object-aware%2520Captioner%2529%252C%2520a%250Anovel%2520vision-language%2520model%2520for%2520image%2520captioning%2520that%2520addresses%2520the%2520challenges%250Aof%2520accuracy%2520and%2520diversity%2520in%2520generated%2520descriptions.%2520Unlike%2520conventional%250Aapproaches%252C%2520ViTOC%2520employs%2520a%2520dual-path%2520architecture%2520based%2520on%2520Vision%2520Transformer%250Aand%2520object%2520detector%252C%2520effectively%2520fusing%2520global%2520visual%2520features%2520and%2520local%2520object%250Ainformation%2520through%2520learnable%2520vectors.%2520The%2520model%2520introduces%2520an%2520innovative%250Aobject-aware%2520prompting%2520strategy%2520that%2520significantly%2520enhances%2520its%2520capability%2520in%250Ahandling%2520long-tail%2520data.%2520Experiments%2520on%2520the%2520standard%2520COCO%2520dataset%2520demonstrate%250Athat%2520ViTOC%2520outperforms%2520baseline%2520models%2520across%2520all%2520evaluation%2520metrics.%250AAdditionally%252C%2520we%2520propose%2520a%2520reference-free%2520evaluation%2520method%2520based%2520on%2520CLIP%2520to%250Afurther%2520validate%2520the%2520model%2527s%2520effectiveness.%2520By%2520utilizing%2520pretrained%2520visual%250Amodel%2520parameters%252C%2520ViTOC%2520achieves%2520efficient%2520end-to-end%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07265v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTOC%3A%20Vision%20Transformer%20and%20Object-aware%20Captioner&entry.906535625=Feiyang%20Huang&entry.1292438233=%20%20This%20paper%20presents%20ViTOC%20%28Vision%20Transformer%20and%20Object-aware%20Captioner%29%2C%20a%0Anovel%20vision-language%20model%20for%20image%20captioning%20that%20addresses%20the%20challenges%0Aof%20accuracy%20and%20diversity%20in%20generated%20descriptions.%20Unlike%20conventional%0Aapproaches%2C%20ViTOC%20employs%20a%20dual-path%20architecture%20based%20on%20Vision%20Transformer%0Aand%20object%20detector%2C%20effectively%20fusing%20global%20visual%20features%20and%20local%20object%0Ainformation%20through%20learnable%20vectors.%20The%20model%20introduces%20an%20innovative%0Aobject-aware%20prompting%20strategy%20that%20significantly%20enhances%20its%20capability%20in%0Ahandling%20long-tail%20data.%20Experiments%20on%20the%20standard%20COCO%20dataset%20demonstrate%0Athat%20ViTOC%20outperforms%20baseline%20models%20across%20all%20evaluation%20metrics.%0AAdditionally%2C%20we%20propose%20a%20reference-free%20evaluation%20method%20based%20on%20CLIP%20to%0Afurther%20validate%20the%20model%27s%20effectiveness.%20By%20utilizing%20pretrained%20visual%0Amodel%20parameters%2C%20ViTOC%20achieves%20efficient%20end-to-end%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07265v3&entry.124074799=Read"},
{"title": "Cross-modal Information Flow in Multimodal Large Language Models", "author": "Zhi Zhang and Srishti Yadav and Fengze Han and Ekaterina Shutova", "abstract": "  The recent advancements in auto-regressive multimodal large language models\n(MLLMs) have demonstrated promising progress for vision-language tasks. While\nthere exists a variety of studies investigating the processing of linguistic\ninformation within large language models, little is currently known about the\ninner working mechanism of MLLMs and how linguistic and visual information\ninteract within these models. In this study, we aim to fill this gap by\nexamining the information flow between different modalities -- language and\nvision -- in MLLMs, focusing on visual question answering. Specifically, given\nan image-question pair as input, we investigate where in the model and how the\nvisual and linguistic information are combined to generate the final\nprediction. Conducting experiments with a series of models from the LLaVA\nseries, we find that there are two distinct stages in the process of\nintegration of the two modalities. In the lower layers, the model first\ntransfers the more general visual features of the whole image into the\nrepresentations of (linguistic) question tokens. In the middle layers, it once\nagain transfers visual information about specific objects relevant to the\nquestion to the respective token positions of the question. Finally, in the\nhigher layers, the resulting multimodal representation is propagated to the\nlast position of the input sequence for the final prediction. Overall, our\nfindings provide a new and comprehensive perspective on the spatial and\nfunctional aspects of image and language processing in the MLLMs, thereby\nfacilitating future research into multimodal information localization and\nediting.\n", "link": "http://arxiv.org/abs/2411.18620v1", "date": "2024-11-27", "relevancy": 2.8818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Information%20Flow%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Cross-modal%20Information%20Flow%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Zhi%20Zhang%20and%20Srishti%20Yadav%20and%20Fengze%20Han%20and%20Ekaterina%20Shutova%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20auto-regressive%20multimodal%20large%20language%20models%0A%28MLLMs%29%20have%20demonstrated%20promising%20progress%20for%20vision-language%20tasks.%20While%0Athere%20exists%20a%20variety%20of%20studies%20investigating%20the%20processing%20of%20linguistic%0Ainformation%20within%20large%20language%20models%2C%20little%20is%20currently%20known%20about%20the%0Ainner%20working%20mechanism%20of%20MLLMs%20and%20how%20linguistic%20and%20visual%20information%0Ainteract%20within%20these%20models.%20In%20this%20study%2C%20we%20aim%20to%20fill%20this%20gap%20by%0Aexamining%20the%20information%20flow%20between%20different%20modalities%20--%20language%20and%0Avision%20--%20in%20MLLMs%2C%20focusing%20on%20visual%20question%20answering.%20Specifically%2C%20given%0Aan%20image-question%20pair%20as%20input%2C%20we%20investigate%20where%20in%20the%20model%20and%20how%20the%0Avisual%20and%20linguistic%20information%20are%20combined%20to%20generate%20the%20final%0Aprediction.%20Conducting%20experiments%20with%20a%20series%20of%20models%20from%20the%20LLaVA%0Aseries%2C%20we%20find%20that%20there%20are%20two%20distinct%20stages%20in%20the%20process%20of%0Aintegration%20of%20the%20two%20modalities.%20In%20the%20lower%20layers%2C%20the%20model%20first%0Atransfers%20the%20more%20general%20visual%20features%20of%20the%20whole%20image%20into%20the%0Arepresentations%20of%20%28linguistic%29%20question%20tokens.%20In%20the%20middle%20layers%2C%20it%20once%0Aagain%20transfers%20visual%20information%20about%20specific%20objects%20relevant%20to%20the%0Aquestion%20to%20the%20respective%20token%20positions%20of%20the%20question.%20Finally%2C%20in%20the%0Ahigher%20layers%2C%20the%20resulting%20multimodal%20representation%20is%20propagated%20to%20the%0Alast%20position%20of%20the%20input%20sequence%20for%20the%20final%20prediction.%20Overall%2C%20our%0Afindings%20provide%20a%20new%20and%20comprehensive%20perspective%20on%20the%20spatial%20and%0Afunctional%20aspects%20of%20image%20and%20language%20processing%20in%20the%20MLLMs%2C%20thereby%0Afacilitating%20future%20research%20into%20multimodal%20information%20localization%20and%0Aediting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520Information%2520Flow%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DZhi%2520Zhang%2520and%2520Srishti%2520Yadav%2520and%2520Fengze%2520Han%2520and%2520Ekaterina%2520Shutova%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520auto-regressive%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520have%2520demonstrated%2520promising%2520progress%2520for%2520vision-language%2520tasks.%2520While%250Athere%2520exists%2520a%2520variety%2520of%2520studies%2520investigating%2520the%2520processing%2520of%2520linguistic%250Ainformation%2520within%2520large%2520language%2520models%252C%2520little%2520is%2520currently%2520known%2520about%2520the%250Ainner%2520working%2520mechanism%2520of%2520MLLMs%2520and%2520how%2520linguistic%2520and%2520visual%2520information%250Ainteract%2520within%2520these%2520models.%2520In%2520this%2520study%252C%2520we%2520aim%2520to%2520fill%2520this%2520gap%2520by%250Aexamining%2520the%2520information%2520flow%2520between%2520different%2520modalities%2520--%2520language%2520and%250Avision%2520--%2520in%2520MLLMs%252C%2520focusing%2520on%2520visual%2520question%2520answering.%2520Specifically%252C%2520given%250Aan%2520image-question%2520pair%2520as%2520input%252C%2520we%2520investigate%2520where%2520in%2520the%2520model%2520and%2520how%2520the%250Avisual%2520and%2520linguistic%2520information%2520are%2520combined%2520to%2520generate%2520the%2520final%250Aprediction.%2520Conducting%2520experiments%2520with%2520a%2520series%2520of%2520models%2520from%2520the%2520LLaVA%250Aseries%252C%2520we%2520find%2520that%2520there%2520are%2520two%2520distinct%2520stages%2520in%2520the%2520process%2520of%250Aintegration%2520of%2520the%2520two%2520modalities.%2520In%2520the%2520lower%2520layers%252C%2520the%2520model%2520first%250Atransfers%2520the%2520more%2520general%2520visual%2520features%2520of%2520the%2520whole%2520image%2520into%2520the%250Arepresentations%2520of%2520%2528linguistic%2529%2520question%2520tokens.%2520In%2520the%2520middle%2520layers%252C%2520it%2520once%250Aagain%2520transfers%2520visual%2520information%2520about%2520specific%2520objects%2520relevant%2520to%2520the%250Aquestion%2520to%2520the%2520respective%2520token%2520positions%2520of%2520the%2520question.%2520Finally%252C%2520in%2520the%250Ahigher%2520layers%252C%2520the%2520resulting%2520multimodal%2520representation%2520is%2520propagated%2520to%2520the%250Alast%2520position%2520of%2520the%2520input%2520sequence%2520for%2520the%2520final%2520prediction.%2520Overall%252C%2520our%250Afindings%2520provide%2520a%2520new%2520and%2520comprehensive%2520perspective%2520on%2520the%2520spatial%2520and%250Afunctional%2520aspects%2520of%2520image%2520and%2520language%2520processing%2520in%2520the%2520MLLMs%252C%2520thereby%250Afacilitating%2520future%2520research%2520into%2520multimodal%2520information%2520localization%2520and%250Aediting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Information%20Flow%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Zhi%20Zhang%20and%20Srishti%20Yadav%20and%20Fengze%20Han%20and%20Ekaterina%20Shutova&entry.1292438233=%20%20The%20recent%20advancements%20in%20auto-regressive%20multimodal%20large%20language%20models%0A%28MLLMs%29%20have%20demonstrated%20promising%20progress%20for%20vision-language%20tasks.%20While%0Athere%20exists%20a%20variety%20of%20studies%20investigating%20the%20processing%20of%20linguistic%0Ainformation%20within%20large%20language%20models%2C%20little%20is%20currently%20known%20about%20the%0Ainner%20working%20mechanism%20of%20MLLMs%20and%20how%20linguistic%20and%20visual%20information%0Ainteract%20within%20these%20models.%20In%20this%20study%2C%20we%20aim%20to%20fill%20this%20gap%20by%0Aexamining%20the%20information%20flow%20between%20different%20modalities%20--%20language%20and%0Avision%20--%20in%20MLLMs%2C%20focusing%20on%20visual%20question%20answering.%20Specifically%2C%20given%0Aan%20image-question%20pair%20as%20input%2C%20we%20investigate%20where%20in%20the%20model%20and%20how%20the%0Avisual%20and%20linguistic%20information%20are%20combined%20to%20generate%20the%20final%0Aprediction.%20Conducting%20experiments%20with%20a%20series%20of%20models%20from%20the%20LLaVA%0Aseries%2C%20we%20find%20that%20there%20are%20two%20distinct%20stages%20in%20the%20process%20of%0Aintegration%20of%20the%20two%20modalities.%20In%20the%20lower%20layers%2C%20the%20model%20first%0Atransfers%20the%20more%20general%20visual%20features%20of%20the%20whole%20image%20into%20the%0Arepresentations%20of%20%28linguistic%29%20question%20tokens.%20In%20the%20middle%20layers%2C%20it%20once%0Aagain%20transfers%20visual%20information%20about%20specific%20objects%20relevant%20to%20the%0Aquestion%20to%20the%20respective%20token%20positions%20of%20the%20question.%20Finally%2C%20in%20the%0Ahigher%20layers%2C%20the%20resulting%20multimodal%20representation%20is%20propagated%20to%20the%0Alast%20position%20of%20the%20input%20sequence%20for%20the%20final%20prediction.%20Overall%2C%20our%0Afindings%20provide%20a%20new%20and%20comprehensive%20perspective%20on%20the%20spatial%20and%0Afunctional%20aspects%20of%20image%20and%20language%20processing%20in%20the%20MLLMs%2C%20thereby%0Afacilitating%20future%20research%20into%20multimodal%20information%20localization%20and%0Aediting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18620v1&entry.124074799=Read"},
{"title": "DataVisT5: A Pre-trained Language Model for Jointly Understanding Text\n  and Data Visualization", "author": "Zhuoyue Wan and Yuanfeng Song and Shuaimin Li and Chen Jason Zhang and Raymond Chi-Wing Wong", "abstract": "  Data visualization (DV) is the fundamental and premise tool to improve the\nefficiency in conveying the insights behind the big data, which has been widely\naccepted in existing data-driven world. Task automation in DV, such as\nconverting natural language queries to visualizations (i.e., text-to-vis),\ngenerating explanations from visualizations (i.e., vis-to-text), answering\nDV-related questions in free form (i.e. FeVisQA), and explicating tabular data\n(i.e., table-to-text), is vital for advancing the field. Despite their\npotential, the application of pre-trained language models (PLMs) like T5 and\nBERT in DV has been limited by high costs and challenges in handling\ncross-modal information, leading to few studies on PLMs for DV. We introduce\nDataVisT5, a novel PLM tailored for DV that enhances the T5 architecture\nthrough a hybrid objective pre-training and multi-task fine-tuning strategy,\nintegrating text and DV datasets to effectively interpret cross-modal\nsemantics. Extensive evaluations on public datasets show that DataVisT5\nconsistently outperforms current state-of-the-art models on various DV-related\ntasks. We anticipate that DataVisT5 will not only inspire further research on\nvertical PLMs but also expand the range of applications for PLMs.\n", "link": "http://arxiv.org/abs/2408.07401v2", "date": "2024-11-27", "relevancy": 2.8478, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataVisT5%3A%20A%20Pre-trained%20Language%20Model%20for%20Jointly%20Understanding%20Text%0A%20%20and%20Data%20Visualization&body=Title%3A%20DataVisT5%3A%20A%20Pre-trained%20Language%20Model%20for%20Jointly%20Understanding%20Text%0A%20%20and%20Data%20Visualization%0AAuthor%3A%20Zhuoyue%20Wan%20and%20Yuanfeng%20Song%20and%20Shuaimin%20Li%20and%20Chen%20Jason%20Zhang%20and%20Raymond%20Chi-Wing%20Wong%0AAbstract%3A%20%20%20Data%20visualization%20%28DV%29%20is%20the%20fundamental%20and%20premise%20tool%20to%20improve%20the%0Aefficiency%20in%20conveying%20the%20insights%20behind%20the%20big%20data%2C%20which%20has%20been%20widely%0Aaccepted%20in%20existing%20data-driven%20world.%20Task%20automation%20in%20DV%2C%20such%20as%0Aconverting%20natural%20language%20queries%20to%20visualizations%20%28i.e.%2C%20text-to-vis%29%2C%0Agenerating%20explanations%20from%20visualizations%20%28i.e.%2C%20vis-to-text%29%2C%20answering%0ADV-related%20questions%20in%20free%20form%20%28i.e.%20FeVisQA%29%2C%20and%20explicating%20tabular%20data%0A%28i.e.%2C%20table-to-text%29%2C%20is%20vital%20for%20advancing%20the%20field.%20Despite%20their%0Apotential%2C%20the%20application%20of%20pre-trained%20language%20models%20%28PLMs%29%20like%20T5%20and%0ABERT%20in%20DV%20has%20been%20limited%20by%20high%20costs%20and%20challenges%20in%20handling%0Across-modal%20information%2C%20leading%20to%20few%20studies%20on%20PLMs%20for%20DV.%20We%20introduce%0ADataVisT5%2C%20a%20novel%20PLM%20tailored%20for%20DV%20that%20enhances%20the%20T5%20architecture%0Athrough%20a%20hybrid%20objective%20pre-training%20and%20multi-task%20fine-tuning%20strategy%2C%0Aintegrating%20text%20and%20DV%20datasets%20to%20effectively%20interpret%20cross-modal%0Asemantics.%20Extensive%20evaluations%20on%20public%20datasets%20show%20that%20DataVisT5%0Aconsistently%20outperforms%20current%20state-of-the-art%20models%20on%20various%20DV-related%0Atasks.%20We%20anticipate%20that%20DataVisT5%20will%20not%20only%20inspire%20further%20research%20on%0Avertical%20PLMs%20but%20also%20expand%20the%20range%20of%20applications%20for%20PLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataVisT5%253A%2520A%2520Pre-trained%2520Language%2520Model%2520for%2520Jointly%2520Understanding%2520Text%250A%2520%2520and%2520Data%2520Visualization%26entry.906535625%3DZhuoyue%2520Wan%2520and%2520Yuanfeng%2520Song%2520and%2520Shuaimin%2520Li%2520and%2520Chen%2520Jason%2520Zhang%2520and%2520Raymond%2520Chi-Wing%2520Wong%26entry.1292438233%3D%2520%2520Data%2520visualization%2520%2528DV%2529%2520is%2520the%2520fundamental%2520and%2520premise%2520tool%2520to%2520improve%2520the%250Aefficiency%2520in%2520conveying%2520the%2520insights%2520behind%2520the%2520big%2520data%252C%2520which%2520has%2520been%2520widely%250Aaccepted%2520in%2520existing%2520data-driven%2520world.%2520Task%2520automation%2520in%2520DV%252C%2520such%2520as%250Aconverting%2520natural%2520language%2520queries%2520to%2520visualizations%2520%2528i.e.%252C%2520text-to-vis%2529%252C%250Agenerating%2520explanations%2520from%2520visualizations%2520%2528i.e.%252C%2520vis-to-text%2529%252C%2520answering%250ADV-related%2520questions%2520in%2520free%2520form%2520%2528i.e.%2520FeVisQA%2529%252C%2520and%2520explicating%2520tabular%2520data%250A%2528i.e.%252C%2520table-to-text%2529%252C%2520is%2520vital%2520for%2520advancing%2520the%2520field.%2520Despite%2520their%250Apotential%252C%2520the%2520application%2520of%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520like%2520T5%2520and%250ABERT%2520in%2520DV%2520has%2520been%2520limited%2520by%2520high%2520costs%2520and%2520challenges%2520in%2520handling%250Across-modal%2520information%252C%2520leading%2520to%2520few%2520studies%2520on%2520PLMs%2520for%2520DV.%2520We%2520introduce%250ADataVisT5%252C%2520a%2520novel%2520PLM%2520tailored%2520for%2520DV%2520that%2520enhances%2520the%2520T5%2520architecture%250Athrough%2520a%2520hybrid%2520objective%2520pre-training%2520and%2520multi-task%2520fine-tuning%2520strategy%252C%250Aintegrating%2520text%2520and%2520DV%2520datasets%2520to%2520effectively%2520interpret%2520cross-modal%250Asemantics.%2520Extensive%2520evaluations%2520on%2520public%2520datasets%2520show%2520that%2520DataVisT5%250Aconsistently%2520outperforms%2520current%2520state-of-the-art%2520models%2520on%2520various%2520DV-related%250Atasks.%2520We%2520anticipate%2520that%2520DataVisT5%2520will%2520not%2520only%2520inspire%2520further%2520research%2520on%250Avertical%2520PLMs%2520but%2520also%2520expand%2520the%2520range%2520of%2520applications%2520for%2520PLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataVisT5%3A%20A%20Pre-trained%20Language%20Model%20for%20Jointly%20Understanding%20Text%0A%20%20and%20Data%20Visualization&entry.906535625=Zhuoyue%20Wan%20and%20Yuanfeng%20Song%20and%20Shuaimin%20Li%20and%20Chen%20Jason%20Zhang%20and%20Raymond%20Chi-Wing%20Wong&entry.1292438233=%20%20Data%20visualization%20%28DV%29%20is%20the%20fundamental%20and%20premise%20tool%20to%20improve%20the%0Aefficiency%20in%20conveying%20the%20insights%20behind%20the%20big%20data%2C%20which%20has%20been%20widely%0Aaccepted%20in%20existing%20data-driven%20world.%20Task%20automation%20in%20DV%2C%20such%20as%0Aconverting%20natural%20language%20queries%20to%20visualizations%20%28i.e.%2C%20text-to-vis%29%2C%0Agenerating%20explanations%20from%20visualizations%20%28i.e.%2C%20vis-to-text%29%2C%20answering%0ADV-related%20questions%20in%20free%20form%20%28i.e.%20FeVisQA%29%2C%20and%20explicating%20tabular%20data%0A%28i.e.%2C%20table-to-text%29%2C%20is%20vital%20for%20advancing%20the%20field.%20Despite%20their%0Apotential%2C%20the%20application%20of%20pre-trained%20language%20models%20%28PLMs%29%20like%20T5%20and%0ABERT%20in%20DV%20has%20been%20limited%20by%20high%20costs%20and%20challenges%20in%20handling%0Across-modal%20information%2C%20leading%20to%20few%20studies%20on%20PLMs%20for%20DV.%20We%20introduce%0ADataVisT5%2C%20a%20novel%20PLM%20tailored%20for%20DV%20that%20enhances%20the%20T5%20architecture%0Athrough%20a%20hybrid%20objective%20pre-training%20and%20multi-task%20fine-tuning%20strategy%2C%0Aintegrating%20text%20and%20DV%20datasets%20to%20effectively%20interpret%20cross-modal%0Asemantics.%20Extensive%20evaluations%20on%20public%20datasets%20show%20that%20DataVisT5%0Aconsistently%20outperforms%20current%20state-of-the-art%20models%20on%20various%20DV-related%0Atasks.%20We%20anticipate%20that%20DataVisT5%20will%20not%20only%20inspire%20further%20research%20on%0Avertical%20PLMs%20but%20also%20expand%20the%20range%20of%20applications%20for%20PLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07401v2&entry.124074799=Read"},
{"title": "Weakly Supervised Framework Considering Multi-temporal Information for\n  Large-scale Cropland Mapping with Satellite Imagery", "author": "Yuze Wang and Aoran Hu and Ji Qi and Yang Liu and Chao Tao", "abstract": "  Accurately mapping large-scale cropland is crucial for agricultural\nproduction management and planning. Currently, the combination of remote\nsensing data and deep learning techniques has shown outstanding performance in\ncropland mapping. However, those approaches require massive precise labels,\nwhich are labor-intensive. To reduce the label cost, this study presented a\nweakly supervised framework considering multi-temporal information for\nlarge-scale cropland mapping. Specifically, we extract high-quality labels\naccording to their consistency among global land cover (GLC) products to\nconstruct the supervised learning signal. On the one hand, to alleviate the\noverfitting problem caused by the model's over-trust of remaining errors in\nhigh-quality labels, we encode the similarity/aggregation of cropland in the\nvisual/spatial domain to construct the unsupervised learning signal, and take\nit as the regularization term to constrain the supervised part. On the other\nhand, to sufficiently leverage the plentiful information in the samples without\nhigh-quality labels, we also incorporate the unsupervised learning signal in\nthese samples, enriching the diversity of the feature space. After that, to\ncapture the phenological features of croplands, we introduce dense satellite\nimage time series (SITS) to extend the proposed framework in the temporal\ndimension. We also visualized the high dimensional phenological features to\nuncover how multi-temporal information benefits cropland extraction, and\nassessed the method's robustness under conditions of data scarcity. The\nproposed framework has been experimentally validated for strong adaptability\nacross three study areas (Hunan Province, Southeast France, and Kansas) in\nlarge-scale cropland mapping, and the internal mechanism and temporal\ngeneralizability are also investigated.\n", "link": "http://arxiv.org/abs/2411.18475v1", "date": "2024-11-27", "relevancy": 2.8443, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5897}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Framework%20Considering%20Multi-temporal%20Information%20for%0A%20%20Large-scale%20Cropland%20Mapping%20with%20Satellite%20Imagery&body=Title%3A%20Weakly%20Supervised%20Framework%20Considering%20Multi-temporal%20Information%20for%0A%20%20Large-scale%20Cropland%20Mapping%20with%20Satellite%20Imagery%0AAuthor%3A%20Yuze%20Wang%20and%20Aoran%20Hu%20and%20Ji%20Qi%20and%20Yang%20Liu%20and%20Chao%20Tao%0AAbstract%3A%20%20%20Accurately%20mapping%20large-scale%20cropland%20is%20crucial%20for%20agricultural%0Aproduction%20management%20and%20planning.%20Currently%2C%20the%20combination%20of%20remote%0Asensing%20data%20and%20deep%20learning%20techniques%20has%20shown%20outstanding%20performance%20in%0Acropland%20mapping.%20However%2C%20those%20approaches%20require%20massive%20precise%20labels%2C%0Awhich%20are%20labor-intensive.%20To%20reduce%20the%20label%20cost%2C%20this%20study%20presented%20a%0Aweakly%20supervised%20framework%20considering%20multi-temporal%20information%20for%0Alarge-scale%20cropland%20mapping.%20Specifically%2C%20we%20extract%20high-quality%20labels%0Aaccording%20to%20their%20consistency%20among%20global%20land%20cover%20%28GLC%29%20products%20to%0Aconstruct%20the%20supervised%20learning%20signal.%20On%20the%20one%20hand%2C%20to%20alleviate%20the%0Aoverfitting%20problem%20caused%20by%20the%20model%27s%20over-trust%20of%20remaining%20errors%20in%0Ahigh-quality%20labels%2C%20we%20encode%20the%20similarity/aggregation%20of%20cropland%20in%20the%0Avisual/spatial%20domain%20to%20construct%20the%20unsupervised%20learning%20signal%2C%20and%20take%0Ait%20as%20the%20regularization%20term%20to%20constrain%20the%20supervised%20part.%20On%20the%20other%0Ahand%2C%20to%20sufficiently%20leverage%20the%20plentiful%20information%20in%20the%20samples%20without%0Ahigh-quality%20labels%2C%20we%20also%20incorporate%20the%20unsupervised%20learning%20signal%20in%0Athese%20samples%2C%20enriching%20the%20diversity%20of%20the%20feature%20space.%20After%20that%2C%20to%0Acapture%20the%20phenological%20features%20of%20croplands%2C%20we%20introduce%20dense%20satellite%0Aimage%20time%20series%20%28SITS%29%20to%20extend%20the%20proposed%20framework%20in%20the%20temporal%0Adimension.%20We%20also%20visualized%20the%20high%20dimensional%20phenological%20features%20to%0Auncover%20how%20multi-temporal%20information%20benefits%20cropland%20extraction%2C%20and%0Aassessed%20the%20method%27s%20robustness%20under%20conditions%20of%20data%20scarcity.%20The%0Aproposed%20framework%20has%20been%20experimentally%20validated%20for%20strong%20adaptability%0Aacross%20three%20study%20areas%20%28Hunan%20Province%2C%20Southeast%20France%2C%20and%20Kansas%29%20in%0Alarge-scale%20cropland%20mapping%2C%20and%20the%20internal%20mechanism%20and%20temporal%0Ageneralizability%20are%20also%20investigated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Framework%2520Considering%2520Multi-temporal%2520Information%2520for%250A%2520%2520Large-scale%2520Cropland%2520Mapping%2520with%2520Satellite%2520Imagery%26entry.906535625%3DYuze%2520Wang%2520and%2520Aoran%2520Hu%2520and%2520Ji%2520Qi%2520and%2520Yang%2520Liu%2520and%2520Chao%2520Tao%26entry.1292438233%3D%2520%2520Accurately%2520mapping%2520large-scale%2520cropland%2520is%2520crucial%2520for%2520agricultural%250Aproduction%2520management%2520and%2520planning.%2520Currently%252C%2520the%2520combination%2520of%2520remote%250Asensing%2520data%2520and%2520deep%2520learning%2520techniques%2520has%2520shown%2520outstanding%2520performance%2520in%250Acropland%2520mapping.%2520However%252C%2520those%2520approaches%2520require%2520massive%2520precise%2520labels%252C%250Awhich%2520are%2520labor-intensive.%2520To%2520reduce%2520the%2520label%2520cost%252C%2520this%2520study%2520presented%2520a%250Aweakly%2520supervised%2520framework%2520considering%2520multi-temporal%2520information%2520for%250Alarge-scale%2520cropland%2520mapping.%2520Specifically%252C%2520we%2520extract%2520high-quality%2520labels%250Aaccording%2520to%2520their%2520consistency%2520among%2520global%2520land%2520cover%2520%2528GLC%2529%2520products%2520to%250Aconstruct%2520the%2520supervised%2520learning%2520signal.%2520On%2520the%2520one%2520hand%252C%2520to%2520alleviate%2520the%250Aoverfitting%2520problem%2520caused%2520by%2520the%2520model%2527s%2520over-trust%2520of%2520remaining%2520errors%2520in%250Ahigh-quality%2520labels%252C%2520we%2520encode%2520the%2520similarity/aggregation%2520of%2520cropland%2520in%2520the%250Avisual/spatial%2520domain%2520to%2520construct%2520the%2520unsupervised%2520learning%2520signal%252C%2520and%2520take%250Ait%2520as%2520the%2520regularization%2520term%2520to%2520constrain%2520the%2520supervised%2520part.%2520On%2520the%2520other%250Ahand%252C%2520to%2520sufficiently%2520leverage%2520the%2520plentiful%2520information%2520in%2520the%2520samples%2520without%250Ahigh-quality%2520labels%252C%2520we%2520also%2520incorporate%2520the%2520unsupervised%2520learning%2520signal%2520in%250Athese%2520samples%252C%2520enriching%2520the%2520diversity%2520of%2520the%2520feature%2520space.%2520After%2520that%252C%2520to%250Acapture%2520the%2520phenological%2520features%2520of%2520croplands%252C%2520we%2520introduce%2520dense%2520satellite%250Aimage%2520time%2520series%2520%2528SITS%2529%2520to%2520extend%2520the%2520proposed%2520framework%2520in%2520the%2520temporal%250Adimension.%2520We%2520also%2520visualized%2520the%2520high%2520dimensional%2520phenological%2520features%2520to%250Auncover%2520how%2520multi-temporal%2520information%2520benefits%2520cropland%2520extraction%252C%2520and%250Aassessed%2520the%2520method%2527s%2520robustness%2520under%2520conditions%2520of%2520data%2520scarcity.%2520The%250Aproposed%2520framework%2520has%2520been%2520experimentally%2520validated%2520for%2520strong%2520adaptability%250Aacross%2520three%2520study%2520areas%2520%2528Hunan%2520Province%252C%2520Southeast%2520France%252C%2520and%2520Kansas%2529%2520in%250Alarge-scale%2520cropland%2520mapping%252C%2520and%2520the%2520internal%2520mechanism%2520and%2520temporal%250Ageneralizability%2520are%2520also%2520investigated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Framework%20Considering%20Multi-temporal%20Information%20for%0A%20%20Large-scale%20Cropland%20Mapping%20with%20Satellite%20Imagery&entry.906535625=Yuze%20Wang%20and%20Aoran%20Hu%20and%20Ji%20Qi%20and%20Yang%20Liu%20and%20Chao%20Tao&entry.1292438233=%20%20Accurately%20mapping%20large-scale%20cropland%20is%20crucial%20for%20agricultural%0Aproduction%20management%20and%20planning.%20Currently%2C%20the%20combination%20of%20remote%0Asensing%20data%20and%20deep%20learning%20techniques%20has%20shown%20outstanding%20performance%20in%0Acropland%20mapping.%20However%2C%20those%20approaches%20require%20massive%20precise%20labels%2C%0Awhich%20are%20labor-intensive.%20To%20reduce%20the%20label%20cost%2C%20this%20study%20presented%20a%0Aweakly%20supervised%20framework%20considering%20multi-temporal%20information%20for%0Alarge-scale%20cropland%20mapping.%20Specifically%2C%20we%20extract%20high-quality%20labels%0Aaccording%20to%20their%20consistency%20among%20global%20land%20cover%20%28GLC%29%20products%20to%0Aconstruct%20the%20supervised%20learning%20signal.%20On%20the%20one%20hand%2C%20to%20alleviate%20the%0Aoverfitting%20problem%20caused%20by%20the%20model%27s%20over-trust%20of%20remaining%20errors%20in%0Ahigh-quality%20labels%2C%20we%20encode%20the%20similarity/aggregation%20of%20cropland%20in%20the%0Avisual/spatial%20domain%20to%20construct%20the%20unsupervised%20learning%20signal%2C%20and%20take%0Ait%20as%20the%20regularization%20term%20to%20constrain%20the%20supervised%20part.%20On%20the%20other%0Ahand%2C%20to%20sufficiently%20leverage%20the%20plentiful%20information%20in%20the%20samples%20without%0Ahigh-quality%20labels%2C%20we%20also%20incorporate%20the%20unsupervised%20learning%20signal%20in%0Athese%20samples%2C%20enriching%20the%20diversity%20of%20the%20feature%20space.%20After%20that%2C%20to%0Acapture%20the%20phenological%20features%20of%20croplands%2C%20we%20introduce%20dense%20satellite%0Aimage%20time%20series%20%28SITS%29%20to%20extend%20the%20proposed%20framework%20in%20the%20temporal%0Adimension.%20We%20also%20visualized%20the%20high%20dimensional%20phenological%20features%20to%0Auncover%20how%20multi-temporal%20information%20benefits%20cropland%20extraction%2C%20and%0Aassessed%20the%20method%27s%20robustness%20under%20conditions%20of%20data%20scarcity.%20The%0Aproposed%20framework%20has%20been%20experimentally%20validated%20for%20strong%20adaptability%0Aacross%20three%20study%20areas%20%28Hunan%20Province%2C%20Southeast%20France%2C%20and%20Kansas%29%20in%0Alarge-scale%20cropland%20mapping%2C%20and%20the%20internal%20mechanism%20and%20temporal%0Ageneralizability%20are%20also%20investigated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18475v1&entry.124074799=Read"},
{"title": "Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise\n  Attention", "author": "Peng Li and Yuan Liu and Xiaoxiao Long and Feihu Zhang and Cheng Lin and Mengfei Li and Xingqun Qi and Shanghang Zhang and Wenhan Luo and Ping Tan and Wenping Wang and Qifeng Liu and Yike Guo", "abstract": "  In this paper, we introduce Era3D, a novel multiview diffusion method that\ngenerates high-resolution multiview images from a single-view image. Despite\nsignificant advancements in multiview generation, existing methods still suffer\nfrom camera prior mismatch, inefficacy, and low resolution, resulting in\npoor-quality multiview images. Specifically, these methods assume that the\ninput images should comply with a predefined camera type, e.g. a perspective\ncamera with a fixed focal length, leading to distorted shapes when the\nassumption fails. Moreover, the full-image or dense multiview attention they\nemploy leads to an exponential explosion of computational complexity as image\nresolution increases, resulting in prohibitively expensive training costs. To\nbridge the gap between assumption and reality, Era3D first proposes a\ndiffusion-based camera prediction module to estimate the focal length and\nelevation of the input image, which allows our method to generate images\nwithout shape distortions. Furthermore, a simple but efficient attention layer,\nnamed row-wise attention, is used to enforce epipolar priors in the multiview\ndiffusion, facilitating efficient cross-view information fusion. Consequently,\ncompared with state-of-the-art methods, Era3D generates high-quality multiview\nimages with up to a 512*512 resolution while reducing computation complexity by\n12x times. Comprehensive experiments demonstrate that Era3D can reconstruct\nhigh-quality and detailed 3D meshes from diverse single-view input images,\nsignificantly outperforming baseline multiview diffusion methods. Project page:\nhttps://penghtyx.github.io/Era3D/.\n", "link": "http://arxiv.org/abs/2405.11616v3", "date": "2024-11-27", "relevancy": 2.8258, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.729}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention&body=Title%3A%20Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention%0AAuthor%3A%20Peng%20Li%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Feihu%20Zhang%20and%20Cheng%20Lin%20and%20Mengfei%20Li%20and%20Xingqun%20Qi%20and%20Shanghang%20Zhang%20and%20Wenhan%20Luo%20and%20Ping%20Tan%20and%20Wenping%20Wang%20and%20Qifeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Era3D%2C%20a%20novel%20multiview%20diffusion%20method%20that%0Agenerates%20high-resolution%20multiview%20images%20from%20a%20single-view%20image.%20Despite%0Asignificant%20advancements%20in%20multiview%20generation%2C%20existing%20methods%20still%20suffer%0Afrom%20camera%20prior%20mismatch%2C%20inefficacy%2C%20and%20low%20resolution%2C%20resulting%20in%0Apoor-quality%20multiview%20images.%20Specifically%2C%20these%20methods%20assume%20that%20the%0Ainput%20images%20should%20comply%20with%20a%20predefined%20camera%20type%2C%20e.g.%20a%20perspective%0Acamera%20with%20a%20fixed%20focal%20length%2C%20leading%20to%20distorted%20shapes%20when%20the%0Aassumption%20fails.%20Moreover%2C%20the%20full-image%20or%20dense%20multiview%20attention%20they%0Aemploy%20leads%20to%20an%20exponential%20explosion%20of%20computational%20complexity%20as%20image%0Aresolution%20increases%2C%20resulting%20in%20prohibitively%20expensive%20training%20costs.%20To%0Abridge%20the%20gap%20between%20assumption%20and%20reality%2C%20Era3D%20first%20proposes%20a%0Adiffusion-based%20camera%20prediction%20module%20to%20estimate%20the%20focal%20length%20and%0Aelevation%20of%20the%20input%20image%2C%20which%20allows%20our%20method%20to%20generate%20images%0Awithout%20shape%20distortions.%20Furthermore%2C%20a%20simple%20but%20efficient%20attention%20layer%2C%0Anamed%20row-wise%20attention%2C%20is%20used%20to%20enforce%20epipolar%20priors%20in%20the%20multiview%0Adiffusion%2C%20facilitating%20efficient%20cross-view%20information%20fusion.%20Consequently%2C%0Acompared%20with%20state-of-the-art%20methods%2C%20Era3D%20generates%20high-quality%20multiview%0Aimages%20with%20up%20to%20a%20512%2A512%20resolution%20while%20reducing%20computation%20complexity%20by%0A12x%20times.%20Comprehensive%20experiments%20demonstrate%20that%20Era3D%20can%20reconstruct%0Ahigh-quality%20and%20detailed%203D%20meshes%20from%20diverse%20single-view%20input%20images%2C%0Asignificantly%20outperforming%20baseline%20multiview%20diffusion%20methods.%20Project%20page%3A%0Ahttps%3A//penghtyx.github.io/Era3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11616v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEra3D%253A%2520High-Resolution%2520Multiview%2520Diffusion%2520using%2520Efficient%2520Row-wise%250A%2520%2520Attention%26entry.906535625%3DPeng%2520Li%2520and%2520Yuan%2520Liu%2520and%2520Xiaoxiao%2520Long%2520and%2520Feihu%2520Zhang%2520and%2520Cheng%2520Lin%2520and%2520Mengfei%2520Li%2520and%2520Xingqun%2520Qi%2520and%2520Shanghang%2520Zhang%2520and%2520Wenhan%2520Luo%2520and%2520Ping%2520Tan%2520and%2520Wenping%2520Wang%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Era3D%252C%2520a%2520novel%2520multiview%2520diffusion%2520method%2520that%250Agenerates%2520high-resolution%2520multiview%2520images%2520from%2520a%2520single-view%2520image.%2520Despite%250Asignificant%2520advancements%2520in%2520multiview%2520generation%252C%2520existing%2520methods%2520still%2520suffer%250Afrom%2520camera%2520prior%2520mismatch%252C%2520inefficacy%252C%2520and%2520low%2520resolution%252C%2520resulting%2520in%250Apoor-quality%2520multiview%2520images.%2520Specifically%252C%2520these%2520methods%2520assume%2520that%2520the%250Ainput%2520images%2520should%2520comply%2520with%2520a%2520predefined%2520camera%2520type%252C%2520e.g.%2520a%2520perspective%250Acamera%2520with%2520a%2520fixed%2520focal%2520length%252C%2520leading%2520to%2520distorted%2520shapes%2520when%2520the%250Aassumption%2520fails.%2520Moreover%252C%2520the%2520full-image%2520or%2520dense%2520multiview%2520attention%2520they%250Aemploy%2520leads%2520to%2520an%2520exponential%2520explosion%2520of%2520computational%2520complexity%2520as%2520image%250Aresolution%2520increases%252C%2520resulting%2520in%2520prohibitively%2520expensive%2520training%2520costs.%2520To%250Abridge%2520the%2520gap%2520between%2520assumption%2520and%2520reality%252C%2520Era3D%2520first%2520proposes%2520a%250Adiffusion-based%2520camera%2520prediction%2520module%2520to%2520estimate%2520the%2520focal%2520length%2520and%250Aelevation%2520of%2520the%2520input%2520image%252C%2520which%2520allows%2520our%2520method%2520to%2520generate%2520images%250Awithout%2520shape%2520distortions.%2520Furthermore%252C%2520a%2520simple%2520but%2520efficient%2520attention%2520layer%252C%250Anamed%2520row-wise%2520attention%252C%2520is%2520used%2520to%2520enforce%2520epipolar%2520priors%2520in%2520the%2520multiview%250Adiffusion%252C%2520facilitating%2520efficient%2520cross-view%2520information%2520fusion.%2520Consequently%252C%250Acompared%2520with%2520state-of-the-art%2520methods%252C%2520Era3D%2520generates%2520high-quality%2520multiview%250Aimages%2520with%2520up%2520to%2520a%2520512%252A512%2520resolution%2520while%2520reducing%2520computation%2520complexity%2520by%250A12x%2520times.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520Era3D%2520can%2520reconstruct%250Ahigh-quality%2520and%2520detailed%25203D%2520meshes%2520from%2520diverse%2520single-view%2520input%2520images%252C%250Asignificantly%2520outperforming%2520baseline%2520multiview%2520diffusion%2520methods.%2520Project%2520page%253A%250Ahttps%253A//penghtyx.github.io/Era3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11616v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Era3D%3A%20High-Resolution%20Multiview%20Diffusion%20using%20Efficient%20Row-wise%0A%20%20Attention&entry.906535625=Peng%20Li%20and%20Yuan%20Liu%20and%20Xiaoxiao%20Long%20and%20Feihu%20Zhang%20and%20Cheng%20Lin%20and%20Mengfei%20Li%20and%20Xingqun%20Qi%20and%20Shanghang%20Zhang%20and%20Wenhan%20Luo%20and%20Ping%20Tan%20and%20Wenping%20Wang%20and%20Qifeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Era3D%2C%20a%20novel%20multiview%20diffusion%20method%20that%0Agenerates%20high-resolution%20multiview%20images%20from%20a%20single-view%20image.%20Despite%0Asignificant%20advancements%20in%20multiview%20generation%2C%20existing%20methods%20still%20suffer%0Afrom%20camera%20prior%20mismatch%2C%20inefficacy%2C%20and%20low%20resolution%2C%20resulting%20in%0Apoor-quality%20multiview%20images.%20Specifically%2C%20these%20methods%20assume%20that%20the%0Ainput%20images%20should%20comply%20with%20a%20predefined%20camera%20type%2C%20e.g.%20a%20perspective%0Acamera%20with%20a%20fixed%20focal%20length%2C%20leading%20to%20distorted%20shapes%20when%20the%0Aassumption%20fails.%20Moreover%2C%20the%20full-image%20or%20dense%20multiview%20attention%20they%0Aemploy%20leads%20to%20an%20exponential%20explosion%20of%20computational%20complexity%20as%20image%0Aresolution%20increases%2C%20resulting%20in%20prohibitively%20expensive%20training%20costs.%20To%0Abridge%20the%20gap%20between%20assumption%20and%20reality%2C%20Era3D%20first%20proposes%20a%0Adiffusion-based%20camera%20prediction%20module%20to%20estimate%20the%20focal%20length%20and%0Aelevation%20of%20the%20input%20image%2C%20which%20allows%20our%20method%20to%20generate%20images%0Awithout%20shape%20distortions.%20Furthermore%2C%20a%20simple%20but%20efficient%20attention%20layer%2C%0Anamed%20row-wise%20attention%2C%20is%20used%20to%20enforce%20epipolar%20priors%20in%20the%20multiview%0Adiffusion%2C%20facilitating%20efficient%20cross-view%20information%20fusion.%20Consequently%2C%0Acompared%20with%20state-of-the-art%20methods%2C%20Era3D%20generates%20high-quality%20multiview%0Aimages%20with%20up%20to%20a%20512%2A512%20resolution%20while%20reducing%20computation%20complexity%20by%0A12x%20times.%20Comprehensive%20experiments%20demonstrate%20that%20Era3D%20can%20reconstruct%0Ahigh-quality%20and%20detailed%203D%20meshes%20from%20diverse%20single-view%20input%20images%2C%0Asignificantly%20outperforming%20baseline%20multiview%20diffusion%20methods.%20Project%20page%3A%0Ahttps%3A//penghtyx.github.io/Era3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11616v3&entry.124074799=Read"},
{"title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning", "author": "Di Zhang and Jingdi Lei and Junxian Li and Xunzhi Wang and Yujie Liu and Zonglin Yang and Jiatong Li and Weida Wang and Suorong Yang and Jianbo Wu and Peng Ye and Wanli Ouyang and Dongzhan Zhou", "abstract": "  Vision-language models~(VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n", "link": "http://arxiv.org/abs/2411.18203v1", "date": "2024-11-27", "relevancy": 2.8196, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning&body=Title%3A%20Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning%0AAuthor%3A%20Di%20Zhang%20and%20Jingdi%20Lei%20and%20Junxian%20Li%20and%20Xunzhi%20Wang%20and%20Yujie%20Liu%20and%20Zonglin%20Yang%20and%20Jiatong%20Li%20and%20Weida%20Wang%20and%20Suorong%20Yang%20and%20Jianbo%20Wu%20and%20Peng%20Ye%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Vision-language%20models~%28VLMs%29%20have%20shown%20remarkable%20advancements%20in%0Amultimodal%20reasoning%20tasks.%20However%2C%20they%20still%20often%20generate%20inaccurate%20or%0Airrelevant%20responses%20due%20to%20issues%20like%20hallucinated%20image%20understandings%20or%0Aunrefined%20reasoning%20paths.%20To%20address%20these%20challenges%2C%20we%20introduce%20Critic-V%2C%0Aa%20novel%20framework%20inspired%20by%20the%20Actor-Critic%20paradigm%20to%20boost%20the%20reasoning%0Acapability%20of%20VLMs.%20This%20framework%20decouples%20the%20reasoning%20process%20and%20critic%0Aprocess%20by%20integrating%20two%20independent%20components%3A%20the%20Reasoner%2C%20which%0Agenerates%20reasoning%20paths%20based%20on%20visual%20and%20textual%20inputs%2C%20and%20the%20Critic%2C%0Awhich%20provides%20constructive%20critique%20to%20refine%20these%20paths.%20In%20this%20approach%2C%0Athe%20Reasoner%20generates%20reasoning%20responses%20according%20to%20text%20prompts%2C%20which%20can%0Aevolve%20iteratively%20as%20a%20policy%20based%20on%20feedback%20from%20the%20Critic.%20This%0Ainteraction%20process%20was%20theoretically%20driven%20by%20a%20reinforcement%20learning%0Aframework%20where%20the%20Critic%20offers%20natural%20language%20critiques%20instead%20of%20scalar%0Arewards%2C%20enabling%20more%20nuanced%20feedback%20to%20boost%20the%20Reasoner%27s%20capability%20on%0Acomplex%20reasoning%20tasks.%20The%20Critic%20model%20is%20trained%20using%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20leveraging%20a%20preference%20dataset%20of%20critiques%20ranked%20by%0ARule-based%20Reward%28RBR%29%20to%20enhance%20its%20critic%20capabilities.%20Evaluation%20results%0Ashow%20that%20the%20Critic-V%20framework%20significantly%20outperforms%20existing%20methods%2C%0Aincluding%20GPT-4V%2C%20on%205%20out%20of%208%20benchmarks%2C%20especially%20regarding%20reasoning%0Aaccuracy%20and%20efficiency.%20Combining%20a%20dynamic%20text-based%20policy%20for%20the%20Reasoner%0Aand%20constructive%20feedback%20from%20the%20preference-optimized%20Critic%20enables%20a%20more%0Areliable%20and%20context-sensitive%20multimodal%20reasoning%20process.%20Our%20approach%0Aprovides%20a%20promising%20solution%20to%20enhance%20the%20reliability%20of%20VLMs%2C%20improving%0Atheir%20performance%20in%20real-world%20reasoning-heavy%20multimodal%20applications%20such%20as%0Aautonomous%20driving%20and%20embodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritic-V%253A%2520VLM%2520Critics%2520Help%2520Catch%2520VLM%2520Errors%2520in%2520Multimodal%2520Reasoning%26entry.906535625%3DDi%2520Zhang%2520and%2520Jingdi%2520Lei%2520and%2520Junxian%2520Li%2520and%2520Xunzhi%2520Wang%2520and%2520Yujie%2520Liu%2520and%2520Zonglin%2520Yang%2520and%2520Jiatong%2520Li%2520and%2520Weida%2520Wang%2520and%2520Suorong%2520Yang%2520and%2520Jianbo%2520Wu%2520and%2520Peng%2520Ye%2520and%2520Wanli%2520Ouyang%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Vision-language%2520models~%2528VLMs%2529%2520have%2520shown%2520remarkable%2520advancements%2520in%250Amultimodal%2520reasoning%2520tasks.%2520However%252C%2520they%2520still%2520often%2520generate%2520inaccurate%2520or%250Airrelevant%2520responses%2520due%2520to%2520issues%2520like%2520hallucinated%2520image%2520understandings%2520or%250Aunrefined%2520reasoning%2520paths.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Critic-V%252C%250Aa%2520novel%2520framework%2520inspired%2520by%2520the%2520Actor-Critic%2520paradigm%2520to%2520boost%2520the%2520reasoning%250Acapability%2520of%2520VLMs.%2520This%2520framework%2520decouples%2520the%2520reasoning%2520process%2520and%2520critic%250Aprocess%2520by%2520integrating%2520two%2520independent%2520components%253A%2520the%2520Reasoner%252C%2520which%250Agenerates%2520reasoning%2520paths%2520based%2520on%2520visual%2520and%2520textual%2520inputs%252C%2520and%2520the%2520Critic%252C%250Awhich%2520provides%2520constructive%2520critique%2520to%2520refine%2520these%2520paths.%2520In%2520this%2520approach%252C%250Athe%2520Reasoner%2520generates%2520reasoning%2520responses%2520according%2520to%2520text%2520prompts%252C%2520which%2520can%250Aevolve%2520iteratively%2520as%2520a%2520policy%2520based%2520on%2520feedback%2520from%2520the%2520Critic.%2520This%250Ainteraction%2520process%2520was%2520theoretically%2520driven%2520by%2520a%2520reinforcement%2520learning%250Aframework%2520where%2520the%2520Critic%2520offers%2520natural%2520language%2520critiques%2520instead%2520of%2520scalar%250Arewards%252C%2520enabling%2520more%2520nuanced%2520feedback%2520to%2520boost%2520the%2520Reasoner%2527s%2520capability%2520on%250Acomplex%2520reasoning%2520tasks.%2520The%2520Critic%2520model%2520is%2520trained%2520using%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520leveraging%2520a%2520preference%2520dataset%2520of%2520critiques%2520ranked%2520by%250ARule-based%2520Reward%2528RBR%2529%2520to%2520enhance%2520its%2520critic%2520capabilities.%2520Evaluation%2520results%250Ashow%2520that%2520the%2520Critic-V%2520framework%2520significantly%2520outperforms%2520existing%2520methods%252C%250Aincluding%2520GPT-4V%252C%2520on%25205%2520out%2520of%25208%2520benchmarks%252C%2520especially%2520regarding%2520reasoning%250Aaccuracy%2520and%2520efficiency.%2520Combining%2520a%2520dynamic%2520text-based%2520policy%2520for%2520the%2520Reasoner%250Aand%2520constructive%2520feedback%2520from%2520the%2520preference-optimized%2520Critic%2520enables%2520a%2520more%250Areliable%2520and%2520context-sensitive%2520multimodal%2520reasoning%2520process.%2520Our%2520approach%250Aprovides%2520a%2520promising%2520solution%2520to%2520enhance%2520the%2520reliability%2520of%2520VLMs%252C%2520improving%250Atheir%2520performance%2520in%2520real-world%2520reasoning-heavy%2520multimodal%2520applications%2520such%2520as%250Aautonomous%2520driving%2520and%2520embodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning&entry.906535625=Di%20Zhang%20and%20Jingdi%20Lei%20and%20Junxian%20Li%20and%20Xunzhi%20Wang%20and%20Yujie%20Liu%20and%20Zonglin%20Yang%20and%20Jiatong%20Li%20and%20Weida%20Wang%20and%20Suorong%20Yang%20and%20Jianbo%20Wu%20and%20Peng%20Ye%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Vision-language%20models~%28VLMs%29%20have%20shown%20remarkable%20advancements%20in%0Amultimodal%20reasoning%20tasks.%20However%2C%20they%20still%20often%20generate%20inaccurate%20or%0Airrelevant%20responses%20due%20to%20issues%20like%20hallucinated%20image%20understandings%20or%0Aunrefined%20reasoning%20paths.%20To%20address%20these%20challenges%2C%20we%20introduce%20Critic-V%2C%0Aa%20novel%20framework%20inspired%20by%20the%20Actor-Critic%20paradigm%20to%20boost%20the%20reasoning%0Acapability%20of%20VLMs.%20This%20framework%20decouples%20the%20reasoning%20process%20and%20critic%0Aprocess%20by%20integrating%20two%20independent%20components%3A%20the%20Reasoner%2C%20which%0Agenerates%20reasoning%20paths%20based%20on%20visual%20and%20textual%20inputs%2C%20and%20the%20Critic%2C%0Awhich%20provides%20constructive%20critique%20to%20refine%20these%20paths.%20In%20this%20approach%2C%0Athe%20Reasoner%20generates%20reasoning%20responses%20according%20to%20text%20prompts%2C%20which%20can%0Aevolve%20iteratively%20as%20a%20policy%20based%20on%20feedback%20from%20the%20Critic.%20This%0Ainteraction%20process%20was%20theoretically%20driven%20by%20a%20reinforcement%20learning%0Aframework%20where%20the%20Critic%20offers%20natural%20language%20critiques%20instead%20of%20scalar%0Arewards%2C%20enabling%20more%20nuanced%20feedback%20to%20boost%20the%20Reasoner%27s%20capability%20on%0Acomplex%20reasoning%20tasks.%20The%20Critic%20model%20is%20trained%20using%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20leveraging%20a%20preference%20dataset%20of%20critiques%20ranked%20by%0ARule-based%20Reward%28RBR%29%20to%20enhance%20its%20critic%20capabilities.%20Evaluation%20results%0Ashow%20that%20the%20Critic-V%20framework%20significantly%20outperforms%20existing%20methods%2C%0Aincluding%20GPT-4V%2C%20on%205%20out%20of%208%20benchmarks%2C%20especially%20regarding%20reasoning%0Aaccuracy%20and%20efficiency.%20Combining%20a%20dynamic%20text-based%20policy%20for%20the%20Reasoner%0Aand%20constructive%20feedback%20from%20the%20preference-optimized%20Critic%20enables%20a%20more%0Areliable%20and%20context-sensitive%20multimodal%20reasoning%20process.%20Our%20approach%0Aprovides%20a%20promising%20solution%20to%20enhance%20the%20reliability%20of%20VLMs%2C%20improving%0Atheir%20performance%20in%20real-world%20reasoning-heavy%20multimodal%20applications%20such%20as%0Aautonomous%20driving%20and%20embodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18203v1&entry.124074799=Read"},
{"title": "MotionCharacter: Identity-Preserving and Motion Controllable Human Video\n  Generation", "author": "Haopeng Fang and Di Qiu and Binjie Mao and Pengfei Yan and He Tang", "abstract": "  Recent advancements in personalized Text-to-Video (T2V) generation highlight\nthe importance of integrating character-specific identities and actions.\nHowever, previous T2V models struggle with identity consistency and\ncontrollable motion dynamics, mainly due to limited fine-grained facial and\naction-based textual prompts, and datasets that overlook key human attributes\nand actions. To address these challenges, we propose MotionCharacter, an\nefficient and high-fidelity human video generation framework designed for\nidentity preservation and fine-grained motion control. We introduce an\nID-preserving module to maintain identity fidelity while allowing flexible\nattribute modifications, and further integrate ID-consistency and region-aware\nloss mechanisms, significantly enhancing identity consistency and detail\nfidelity. Additionally, our approach incorporates a motion control module that\nprioritizes action-related text while maintaining subject consistency, along\nwith a dataset, Human-Motion, which utilizes large language models to generate\ndetailed motion descriptions. For simplify user control during inference, we\nparameterize motion intensity through a single coefficient, allowing for easy\nadjustments. Extensive experiments highlight the effectiveness of\nMotionCharacter, demonstrating significant improvements in ID-preserving,\nhigh-quality video generation.\n", "link": "http://arxiv.org/abs/2411.18281v1", "date": "2024-11-27", "relevancy": 2.8116, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7356}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6996}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionCharacter%3A%20Identity-Preserving%20and%20Motion%20Controllable%20Human%20Video%0A%20%20Generation&body=Title%3A%20MotionCharacter%3A%20Identity-Preserving%20and%20Motion%20Controllable%20Human%20Video%0A%20%20Generation%0AAuthor%3A%20Haopeng%20Fang%20and%20Di%20Qiu%20and%20Binjie%20Mao%20and%20Pengfei%20Yan%20and%20He%20Tang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20personalized%20Text-to-Video%20%28T2V%29%20generation%20highlight%0Athe%20importance%20of%20integrating%20character-specific%20identities%20and%20actions.%0AHowever%2C%20previous%20T2V%20models%20struggle%20with%20identity%20consistency%20and%0Acontrollable%20motion%20dynamics%2C%20mainly%20due%20to%20limited%20fine-grained%20facial%20and%0Aaction-based%20textual%20prompts%2C%20and%20datasets%20that%20overlook%20key%20human%20attributes%0Aand%20actions.%20To%20address%20these%20challenges%2C%20we%20propose%20MotionCharacter%2C%20an%0Aefficient%20and%20high-fidelity%20human%20video%20generation%20framework%20designed%20for%0Aidentity%20preservation%20and%20fine-grained%20motion%20control.%20We%20introduce%20an%0AID-preserving%20module%20to%20maintain%20identity%20fidelity%20while%20allowing%20flexible%0Aattribute%20modifications%2C%20and%20further%20integrate%20ID-consistency%20and%20region-aware%0Aloss%20mechanisms%2C%20significantly%20enhancing%20identity%20consistency%20and%20detail%0Afidelity.%20Additionally%2C%20our%20approach%20incorporates%20a%20motion%20control%20module%20that%0Aprioritizes%20action-related%20text%20while%20maintaining%20subject%20consistency%2C%20along%0Awith%20a%20dataset%2C%20Human-Motion%2C%20which%20utilizes%20large%20language%20models%20to%20generate%0Adetailed%20motion%20descriptions.%20For%20simplify%20user%20control%20during%20inference%2C%20we%0Aparameterize%20motion%20intensity%20through%20a%20single%20coefficient%2C%20allowing%20for%20easy%0Aadjustments.%20Extensive%20experiments%20highlight%20the%20effectiveness%20of%0AMotionCharacter%2C%20demonstrating%20significant%20improvements%20in%20ID-preserving%2C%0Ahigh-quality%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionCharacter%253A%2520Identity-Preserving%2520and%2520Motion%2520Controllable%2520Human%2520Video%250A%2520%2520Generation%26entry.906535625%3DHaopeng%2520Fang%2520and%2520Di%2520Qiu%2520and%2520Binjie%2520Mao%2520and%2520Pengfei%2520Yan%2520and%2520He%2520Tang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520personalized%2520Text-to-Video%2520%2528T2V%2529%2520generation%2520highlight%250Athe%2520importance%2520of%2520integrating%2520character-specific%2520identities%2520and%2520actions.%250AHowever%252C%2520previous%2520T2V%2520models%2520struggle%2520with%2520identity%2520consistency%2520and%250Acontrollable%2520motion%2520dynamics%252C%2520mainly%2520due%2520to%2520limited%2520fine-grained%2520facial%2520and%250Aaction-based%2520textual%2520prompts%252C%2520and%2520datasets%2520that%2520overlook%2520key%2520human%2520attributes%250Aand%2520actions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MotionCharacter%252C%2520an%250Aefficient%2520and%2520high-fidelity%2520human%2520video%2520generation%2520framework%2520designed%2520for%250Aidentity%2520preservation%2520and%2520fine-grained%2520motion%2520control.%2520We%2520introduce%2520an%250AID-preserving%2520module%2520to%2520maintain%2520identity%2520fidelity%2520while%2520allowing%2520flexible%250Aattribute%2520modifications%252C%2520and%2520further%2520integrate%2520ID-consistency%2520and%2520region-aware%250Aloss%2520mechanisms%252C%2520significantly%2520enhancing%2520identity%2520consistency%2520and%2520detail%250Afidelity.%2520Additionally%252C%2520our%2520approach%2520incorporates%2520a%2520motion%2520control%2520module%2520that%250Aprioritizes%2520action-related%2520text%2520while%2520maintaining%2520subject%2520consistency%252C%2520along%250Awith%2520a%2520dataset%252C%2520Human-Motion%252C%2520which%2520utilizes%2520large%2520language%2520models%2520to%2520generate%250Adetailed%2520motion%2520descriptions.%2520For%2520simplify%2520user%2520control%2520during%2520inference%252C%2520we%250Aparameterize%2520motion%2520intensity%2520through%2520a%2520single%2520coefficient%252C%2520allowing%2520for%2520easy%250Aadjustments.%2520Extensive%2520experiments%2520highlight%2520the%2520effectiveness%2520of%250AMotionCharacter%252C%2520demonstrating%2520significant%2520improvements%2520in%2520ID-preserving%252C%250Ahigh-quality%2520video%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionCharacter%3A%20Identity-Preserving%20and%20Motion%20Controllable%20Human%20Video%0A%20%20Generation&entry.906535625=Haopeng%20Fang%20and%20Di%20Qiu%20and%20Binjie%20Mao%20and%20Pengfei%20Yan%20and%20He%20Tang&entry.1292438233=%20%20Recent%20advancements%20in%20personalized%20Text-to-Video%20%28T2V%29%20generation%20highlight%0Athe%20importance%20of%20integrating%20character-specific%20identities%20and%20actions.%0AHowever%2C%20previous%20T2V%20models%20struggle%20with%20identity%20consistency%20and%0Acontrollable%20motion%20dynamics%2C%20mainly%20due%20to%20limited%20fine-grained%20facial%20and%0Aaction-based%20textual%20prompts%2C%20and%20datasets%20that%20overlook%20key%20human%20attributes%0Aand%20actions.%20To%20address%20these%20challenges%2C%20we%20propose%20MotionCharacter%2C%20an%0Aefficient%20and%20high-fidelity%20human%20video%20generation%20framework%20designed%20for%0Aidentity%20preservation%20and%20fine-grained%20motion%20control.%20We%20introduce%20an%0AID-preserving%20module%20to%20maintain%20identity%20fidelity%20while%20allowing%20flexible%0Aattribute%20modifications%2C%20and%20further%20integrate%20ID-consistency%20and%20region-aware%0Aloss%20mechanisms%2C%20significantly%20enhancing%20identity%20consistency%20and%20detail%0Afidelity.%20Additionally%2C%20our%20approach%20incorporates%20a%20motion%20control%20module%20that%0Aprioritizes%20action-related%20text%20while%20maintaining%20subject%20consistency%2C%20along%0Awith%20a%20dataset%2C%20Human-Motion%2C%20which%20utilizes%20large%20language%20models%20to%20generate%0Adetailed%20motion%20descriptions.%20For%20simplify%20user%20control%20during%20inference%2C%20we%0Aparameterize%20motion%20intensity%20through%20a%20single%20coefficient%2C%20allowing%20for%20easy%0Aadjustments.%20Extensive%20experiments%20highlight%20the%20effectiveness%20of%0AMotionCharacter%2C%20demonstrating%20significant%20improvements%20in%20ID-preserving%2C%0Ahigh-quality%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18281v1&entry.124074799=Read"},
{"title": "DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object\n  Detection", "author": "Felix Fent and Andras Palffy and Holger Caesar", "abstract": "  The perception of autonomous vehicles has to be efficient, robust, and\ncost-effective. However, cameras are not robust against severe weather\nconditions, lidar sensors are expensive, and the performance of radar-based\nperception is still inferior to the others. Camera-radar fusion methods have\nbeen proposed to address this issue, but these are constrained by the typical\nsparsity of radar point clouds and often designed for radars without elevation\ninformation. We propose a novel camera-radar fusion approach called Dual\nPerspective Fusion Transformer (DPFT), designed to overcome these limitations.\nOur method leverages lower-level radar data (the radar cube) instead of the\nprocessed point clouds to preserve as much information as possible and employs\nprojections in both the camera and ground planes to effectively use radars with\nelevation information and simplify the fusion with camera data. As a result,\nDPFT has demonstrated state-of-the-art performance on the K-Radar dataset while\nshowing remarkable robustness against adverse weather conditions and\nmaintaining a low inference time. The code is made available as open-source\nsoftware under https://github.com/TUMFTM/DPFT.\n", "link": "http://arxiv.org/abs/2404.03015v2", "date": "2024-11-27", "relevancy": 2.7932, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5843}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5463}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPFT%3A%20Dual%20Perspective%20Fusion%20Transformer%20for%20Camera-Radar-based%20Object%0A%20%20Detection&body=Title%3A%20DPFT%3A%20Dual%20Perspective%20Fusion%20Transformer%20for%20Camera-Radar-based%20Object%0A%20%20Detection%0AAuthor%3A%20Felix%20Fent%20and%20Andras%20Palffy%20and%20Holger%20Caesar%0AAbstract%3A%20%20%20The%20perception%20of%20autonomous%20vehicles%20has%20to%20be%20efficient%2C%20robust%2C%20and%0Acost-effective.%20However%2C%20cameras%20are%20not%20robust%20against%20severe%20weather%0Aconditions%2C%20lidar%20sensors%20are%20expensive%2C%20and%20the%20performance%20of%20radar-based%0Aperception%20is%20still%20inferior%20to%20the%20others.%20Camera-radar%20fusion%20methods%20have%0Abeen%20proposed%20to%20address%20this%20issue%2C%20but%20these%20are%20constrained%20by%20the%20typical%0Asparsity%20of%20radar%20point%20clouds%20and%20often%20designed%20for%20radars%20without%20elevation%0Ainformation.%20We%20propose%20a%20novel%20camera-radar%20fusion%20approach%20called%20Dual%0APerspective%20Fusion%20Transformer%20%28DPFT%29%2C%20designed%20to%20overcome%20these%20limitations.%0AOur%20method%20leverages%20lower-level%20radar%20data%20%28the%20radar%20cube%29%20instead%20of%20the%0Aprocessed%20point%20clouds%20to%20preserve%20as%20much%20information%20as%20possible%20and%20employs%0Aprojections%20in%20both%20the%20camera%20and%20ground%20planes%20to%20effectively%20use%20radars%20with%0Aelevation%20information%20and%20simplify%20the%20fusion%20with%20camera%20data.%20As%20a%20result%2C%0ADPFT%20has%20demonstrated%20state-of-the-art%20performance%20on%20the%20K-Radar%20dataset%20while%0Ashowing%20remarkable%20robustness%20against%20adverse%20weather%20conditions%20and%0Amaintaining%20a%20low%20inference%20time.%20The%20code%20is%20made%20available%20as%20open-source%0Asoftware%20under%20https%3A//github.com/TUMFTM/DPFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPFT%253A%2520Dual%2520Perspective%2520Fusion%2520Transformer%2520for%2520Camera-Radar-based%2520Object%250A%2520%2520Detection%26entry.906535625%3DFelix%2520Fent%2520and%2520Andras%2520Palffy%2520and%2520Holger%2520Caesar%26entry.1292438233%3D%2520%2520The%2520perception%2520of%2520autonomous%2520vehicles%2520has%2520to%2520be%2520efficient%252C%2520robust%252C%2520and%250Acost-effective.%2520However%252C%2520cameras%2520are%2520not%2520robust%2520against%2520severe%2520weather%250Aconditions%252C%2520lidar%2520sensors%2520are%2520expensive%252C%2520and%2520the%2520performance%2520of%2520radar-based%250Aperception%2520is%2520still%2520inferior%2520to%2520the%2520others.%2520Camera-radar%2520fusion%2520methods%2520have%250Abeen%2520proposed%2520to%2520address%2520this%2520issue%252C%2520but%2520these%2520are%2520constrained%2520by%2520the%2520typical%250Asparsity%2520of%2520radar%2520point%2520clouds%2520and%2520often%2520designed%2520for%2520radars%2520without%2520elevation%250Ainformation.%2520We%2520propose%2520a%2520novel%2520camera-radar%2520fusion%2520approach%2520called%2520Dual%250APerspective%2520Fusion%2520Transformer%2520%2528DPFT%2529%252C%2520designed%2520to%2520overcome%2520these%2520limitations.%250AOur%2520method%2520leverages%2520lower-level%2520radar%2520data%2520%2528the%2520radar%2520cube%2529%2520instead%2520of%2520the%250Aprocessed%2520point%2520clouds%2520to%2520preserve%2520as%2520much%2520information%2520as%2520possible%2520and%2520employs%250Aprojections%2520in%2520both%2520the%2520camera%2520and%2520ground%2520planes%2520to%2520effectively%2520use%2520radars%2520with%250Aelevation%2520information%2520and%2520simplify%2520the%2520fusion%2520with%2520camera%2520data.%2520As%2520a%2520result%252C%250ADPFT%2520has%2520demonstrated%2520state-of-the-art%2520performance%2520on%2520the%2520K-Radar%2520dataset%2520while%250Ashowing%2520remarkable%2520robustness%2520against%2520adverse%2520weather%2520conditions%2520and%250Amaintaining%2520a%2520low%2520inference%2520time.%2520The%2520code%2520is%2520made%2520available%2520as%2520open-source%250Asoftware%2520under%2520https%253A//github.com/TUMFTM/DPFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPFT%3A%20Dual%20Perspective%20Fusion%20Transformer%20for%20Camera-Radar-based%20Object%0A%20%20Detection&entry.906535625=Felix%20Fent%20and%20Andras%20Palffy%20and%20Holger%20Caesar&entry.1292438233=%20%20The%20perception%20of%20autonomous%20vehicles%20has%20to%20be%20efficient%2C%20robust%2C%20and%0Acost-effective.%20However%2C%20cameras%20are%20not%20robust%20against%20severe%20weather%0Aconditions%2C%20lidar%20sensors%20are%20expensive%2C%20and%20the%20performance%20of%20radar-based%0Aperception%20is%20still%20inferior%20to%20the%20others.%20Camera-radar%20fusion%20methods%20have%0Abeen%20proposed%20to%20address%20this%20issue%2C%20but%20these%20are%20constrained%20by%20the%20typical%0Asparsity%20of%20radar%20point%20clouds%20and%20often%20designed%20for%20radars%20without%20elevation%0Ainformation.%20We%20propose%20a%20novel%20camera-radar%20fusion%20approach%20called%20Dual%0APerspective%20Fusion%20Transformer%20%28DPFT%29%2C%20designed%20to%20overcome%20these%20limitations.%0AOur%20method%20leverages%20lower-level%20radar%20data%20%28the%20radar%20cube%29%20instead%20of%20the%0Aprocessed%20point%20clouds%20to%20preserve%20as%20much%20information%20as%20possible%20and%20employs%0Aprojections%20in%20both%20the%20camera%20and%20ground%20planes%20to%20effectively%20use%20radars%20with%0Aelevation%20information%20and%20simplify%20the%20fusion%20with%20camera%20data.%20As%20a%20result%2C%0ADPFT%20has%20demonstrated%20state-of-the-art%20performance%20on%20the%20K-Radar%20dataset%20while%0Ashowing%20remarkable%20robustness%20against%20adverse%20weather%20conditions%20and%0Amaintaining%20a%20low%20inference%20time.%20The%20code%20is%20made%20available%20as%20open-source%0Asoftware%20under%20https%3A//github.com/TUMFTM/DPFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03015v2&entry.124074799=Read"},
{"title": "3D-free meets 3D priors: Novel View Synthesis from a Single Image with\n  Pretrained Diffusion Guidance", "author": "Taewon Kang and Divya Kothandaraman and Dinesh Manocha and Ming C. Lin", "abstract": "  Recent 3D novel view synthesis (NVS) methods often require extensive 3D data\nfor training, and also typically lack generalization beyond the training\ndistribution. Moreover, they tend to be object centric and struggle with\ncomplex and intricate scenes. Conversely, 3D-free methods can generate\ntext-controlled views of complex, in-the-wild scenes using a pretrained stable\ndiffusion model without the need for a large amount of 3D-based training data,\nbut lack camera control. In this paper, we introduce a method capable of\ngenerating camera-controlled viewpoints from a single input image, by combining\nthe benefits of 3D-free and 3D-based approaches. Our method excels in handling\ncomplex and diverse scenes without extensive training or additional 3D and\nmultiview data. It leverages widely available pretrained NVS models for weak\nguidance, integrating this knowledge into a 3D-free view synthesis style\napproach, along with enriching the CLIP vision-language space with 3D camera\nangle information, to achieve the desired results. Experimental results\ndemonstrate that our method outperforms existing models in both qualitative and\nquantitative evaluations, achieving high-fidelity, consistent novel view\nsynthesis at desired camera angles across a wide variety of scenes while\nmaintaining accurate, natural detail representation and image clarity across\nvarious viewpoints. We also support our method with a comprehensive analysis of\n2D image generation models and the 3D space, providing a solid foundation and\nrationale for our solution.\n", "link": "http://arxiv.org/abs/2408.06157v4", "date": "2024-11-27", "relevancy": 2.7759, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7096}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7096}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance&body=Title%3A%203D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance%0AAuthor%3A%20Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin%0AAbstract%3A%20%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20often%20require%20extensive%203D%20data%0Afor%20training%2C%20and%20also%20typically%20lack%20generalization%20beyond%20the%20training%0Adistribution.%20Moreover%2C%20they%20tend%20to%20be%20object%20centric%20and%20struggle%20with%0Acomplex%20and%20intricate%20scenes.%20Conversely%2C%203D-free%20methods%20can%20generate%0Atext-controlled%20views%20of%20complex%2C%20in-the-wild%20scenes%20using%20a%20pretrained%20stable%0Adiffusion%20model%20without%20the%20need%20for%20a%20large%20amount%20of%203D-based%20training%20data%2C%0Abut%20lack%20camera%20control.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20capable%20of%0Agenerating%20camera-controlled%20viewpoints%20from%20a%20single%20input%20image%2C%20by%20combining%0Athe%20benefits%20of%203D-free%20and%203D-based%20approaches.%20Our%20method%20excels%20in%20handling%0Acomplex%20and%20diverse%20scenes%20without%20extensive%20training%20or%20additional%203D%20and%0Amultiview%20data.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%20weak%0Aguidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%20style%0Aapproach%2C%20along%20with%20enriching%20the%20CLIP%20vision-language%20space%20with%203D%20camera%0Aangle%20information%2C%20to%20achieve%20the%20desired%20results.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20models%20in%20both%20qualitative%20and%0Aquantitative%20evaluations%2C%20achieving%20high-fidelity%2C%20consistent%20novel%20view%0Asynthesis%20at%20desired%20camera%20angles%20across%20a%20wide%20variety%20of%20scenes%20while%0Amaintaining%20accurate%2C%20natural%20detail%20representation%20and%20image%20clarity%20across%0Avarious%20viewpoints.%20We%20also%20support%20our%20method%20with%20a%20comprehensive%20analysis%20of%0A2D%20image%20generation%20models%20and%20the%203D%20space%2C%20providing%20a%20solid%20foundation%20and%0Arationale%20for%20our%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06157v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-free%2520meets%25203D%2520priors%253A%2520Novel%2520View%2520Synthesis%2520from%2520a%2520Single%2520Image%2520with%250A%2520%2520Pretrained%2520Diffusion%2520Guidance%26entry.906535625%3DTaewon%2520Kang%2520and%2520Divya%2520Kothandaraman%2520and%2520Dinesh%2520Manocha%2520and%2520Ming%2520C.%2520Lin%26entry.1292438233%3D%2520%2520Recent%25203D%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520often%2520require%2520extensive%25203D%2520data%250Afor%2520training%252C%2520and%2520also%2520typically%2520lack%2520generalization%2520beyond%2520the%2520training%250Adistribution.%2520Moreover%252C%2520they%2520tend%2520to%2520be%2520object%2520centric%2520and%2520struggle%2520with%250Acomplex%2520and%2520intricate%2520scenes.%2520Conversely%252C%25203D-free%2520methods%2520can%2520generate%250Atext-controlled%2520views%2520of%2520complex%252C%2520in-the-wild%2520scenes%2520using%2520a%2520pretrained%2520stable%250Adiffusion%2520model%2520without%2520the%2520need%2520for%2520a%2520large%2520amount%2520of%25203D-based%2520training%2520data%252C%250Abut%2520lack%2520camera%2520control.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520capable%2520of%250Agenerating%2520camera-controlled%2520viewpoints%2520from%2520a%2520single%2520input%2520image%252C%2520by%2520combining%250Athe%2520benefits%2520of%25203D-free%2520and%25203D-based%2520approaches.%2520Our%2520method%2520excels%2520in%2520handling%250Acomplex%2520and%2520diverse%2520scenes%2520without%2520extensive%2520training%2520or%2520additional%25203D%2520and%250Amultiview%2520data.%2520It%2520leverages%2520widely%2520available%2520pretrained%2520NVS%2520models%2520for%2520weak%250Aguidance%252C%2520integrating%2520this%2520knowledge%2520into%2520a%25203D-free%2520view%2520synthesis%2520style%250Aapproach%252C%2520along%2520with%2520enriching%2520the%2520CLIP%2520vision-language%2520space%2520with%25203D%2520camera%250Aangle%2520information%252C%2520to%2520achieve%2520the%2520desired%2520results.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520models%2520in%2520both%2520qualitative%2520and%250Aquantitative%2520evaluations%252C%2520achieving%2520high-fidelity%252C%2520consistent%2520novel%2520view%250Asynthesis%2520at%2520desired%2520camera%2520angles%2520across%2520a%2520wide%2520variety%2520of%2520scenes%2520while%250Amaintaining%2520accurate%252C%2520natural%2520detail%2520representation%2520and%2520image%2520clarity%2520across%250Avarious%2520viewpoints.%2520We%2520also%2520support%2520our%2520method%2520with%2520a%2520comprehensive%2520analysis%2520of%250A2D%2520image%2520generation%2520models%2520and%2520the%25203D%2520space%252C%2520providing%2520a%2520solid%2520foundation%2520and%250Arationale%2520for%2520our%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06157v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance&entry.906535625=Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin&entry.1292438233=%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20often%20require%20extensive%203D%20data%0Afor%20training%2C%20and%20also%20typically%20lack%20generalization%20beyond%20the%20training%0Adistribution.%20Moreover%2C%20they%20tend%20to%20be%20object%20centric%20and%20struggle%20with%0Acomplex%20and%20intricate%20scenes.%20Conversely%2C%203D-free%20methods%20can%20generate%0Atext-controlled%20views%20of%20complex%2C%20in-the-wild%20scenes%20using%20a%20pretrained%20stable%0Adiffusion%20model%20without%20the%20need%20for%20a%20large%20amount%20of%203D-based%20training%20data%2C%0Abut%20lack%20camera%20control.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20capable%20of%0Agenerating%20camera-controlled%20viewpoints%20from%20a%20single%20input%20image%2C%20by%20combining%0Athe%20benefits%20of%203D-free%20and%203D-based%20approaches.%20Our%20method%20excels%20in%20handling%0Acomplex%20and%20diverse%20scenes%20without%20extensive%20training%20or%20additional%203D%20and%0Amultiview%20data.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%20weak%0Aguidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%20style%0Aapproach%2C%20along%20with%20enriching%20the%20CLIP%20vision-language%20space%20with%203D%20camera%0Aangle%20information%2C%20to%20achieve%20the%20desired%20results.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20models%20in%20both%20qualitative%20and%0Aquantitative%20evaluations%2C%20achieving%20high-fidelity%2C%20consistent%20novel%20view%0Asynthesis%20at%20desired%20camera%20angles%20across%20a%20wide%20variety%20of%20scenes%20while%0Amaintaining%20accurate%2C%20natural%20detail%20representation%20and%20image%20clarity%20across%0Avarious%20viewpoints.%20We%20also%20support%20our%20method%20with%20a%20comprehensive%20analysis%20of%0A2D%20image%20generation%20models%20and%20the%203D%20space%2C%20providing%20a%20solid%20foundation%20and%0Arationale%20for%20our%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06157v4&entry.124074799=Read"},
{"title": "CanFields: Consolidating 4D Dynamic Shapes from Raw Scans", "author": "Miaowei Wang and Changjian Li and Amir Vaxman", "abstract": "  We introduce Canonical Consolidation Fields (CanFields), a new method for\nreconstructing a time series of independently captured 3D scans into a single,\ncoherent deforming shape. This 4D representation enables continuous refinement\nacross both space and time. Unlike prior methods that often over-smooth the\ngeometry or produce topological and geometric artifacts, CanFields effectively\nlearns geometry and deformation in an unsupervised way by incorporating two\ngeometric priors. First, we introduce a dynamic consolidator module that\nadjusts the input and assigns confidence scores, balancing the learning of the\ncanonical shape and its deformations. Second, we use low-frequency velocity\nfields to guide deformation while preserving fine details in canonical shapes\nthrough high-frequency bias. We validate the robustness and accuracy of\nCanFields on diverse raw scans, demonstrating its superior performance even\nwith missing regions, sparse frames, and noise. Code is available in the\nsupplementary materials and will be released publicly upon acceptance.\n", "link": "http://arxiv.org/abs/2406.18582v2", "date": "2024-11-27", "relevancy": 2.764, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5582}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CanFields%3A%20Consolidating%204D%20Dynamic%20Shapes%20from%20Raw%20Scans&body=Title%3A%20CanFields%3A%20Consolidating%204D%20Dynamic%20Shapes%20from%20Raw%20Scans%0AAuthor%3A%20Miaowei%20Wang%20and%20Changjian%20Li%20and%20Amir%20Vaxman%0AAbstract%3A%20%20%20We%20introduce%20Canonical%20Consolidation%20Fields%20%28CanFields%29%2C%20a%20new%20method%20for%0Areconstructing%20a%20time%20series%20of%20independently%20captured%203D%20scans%20into%20a%20single%2C%0Acoherent%20deforming%20shape.%20This%204D%20representation%20enables%20continuous%20refinement%0Aacross%20both%20space%20and%20time.%20Unlike%20prior%20methods%20that%20often%20over-smooth%20the%0Ageometry%20or%20produce%20topological%20and%20geometric%20artifacts%2C%20CanFields%20effectively%0Alearns%20geometry%20and%20deformation%20in%20an%20unsupervised%20way%20by%20incorporating%20two%0Ageometric%20priors.%20First%2C%20we%20introduce%20a%20dynamic%20consolidator%20module%20that%0Aadjusts%20the%20input%20and%20assigns%20confidence%20scores%2C%20balancing%20the%20learning%20of%20the%0Acanonical%20shape%20and%20its%20deformations.%20Second%2C%20we%20use%20low-frequency%20velocity%0Afields%20to%20guide%20deformation%20while%20preserving%20fine%20details%20in%20canonical%20shapes%0Athrough%20high-frequency%20bias.%20We%20validate%20the%20robustness%20and%20accuracy%20of%0ACanFields%20on%20diverse%20raw%20scans%2C%20demonstrating%20its%20superior%20performance%20even%0Awith%20missing%20regions%2C%20sparse%20frames%2C%20and%20noise.%20Code%20is%20available%20in%20the%0Asupplementary%20materials%20and%20will%20be%20released%20publicly%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanFields%253A%2520Consolidating%25204D%2520Dynamic%2520Shapes%2520from%2520Raw%2520Scans%26entry.906535625%3DMiaowei%2520Wang%2520and%2520Changjian%2520Li%2520and%2520Amir%2520Vaxman%26entry.1292438233%3D%2520%2520We%2520introduce%2520Canonical%2520Consolidation%2520Fields%2520%2528CanFields%2529%252C%2520a%2520new%2520method%2520for%250Areconstructing%2520a%2520time%2520series%2520of%2520independently%2520captured%25203D%2520scans%2520into%2520a%2520single%252C%250Acoherent%2520deforming%2520shape.%2520This%25204D%2520representation%2520enables%2520continuous%2520refinement%250Aacross%2520both%2520space%2520and%2520time.%2520Unlike%2520prior%2520methods%2520that%2520often%2520over-smooth%2520the%250Ageometry%2520or%2520produce%2520topological%2520and%2520geometric%2520artifacts%252C%2520CanFields%2520effectively%250Alearns%2520geometry%2520and%2520deformation%2520in%2520an%2520unsupervised%2520way%2520by%2520incorporating%2520two%250Ageometric%2520priors.%2520First%252C%2520we%2520introduce%2520a%2520dynamic%2520consolidator%2520module%2520that%250Aadjusts%2520the%2520input%2520and%2520assigns%2520confidence%2520scores%252C%2520balancing%2520the%2520learning%2520of%2520the%250Acanonical%2520shape%2520and%2520its%2520deformations.%2520Second%252C%2520we%2520use%2520low-frequency%2520velocity%250Afields%2520to%2520guide%2520deformation%2520while%2520preserving%2520fine%2520details%2520in%2520canonical%2520shapes%250Athrough%2520high-frequency%2520bias.%2520We%2520validate%2520the%2520robustness%2520and%2520accuracy%2520of%250ACanFields%2520on%2520diverse%2520raw%2520scans%252C%2520demonstrating%2520its%2520superior%2520performance%2520even%250Awith%2520missing%2520regions%252C%2520sparse%2520frames%252C%2520and%2520noise.%2520Code%2520is%2520available%2520in%2520the%250Asupplementary%2520materials%2520and%2520will%2520be%2520released%2520publicly%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CanFields%3A%20Consolidating%204D%20Dynamic%20Shapes%20from%20Raw%20Scans&entry.906535625=Miaowei%20Wang%20and%20Changjian%20Li%20and%20Amir%20Vaxman&entry.1292438233=%20%20We%20introduce%20Canonical%20Consolidation%20Fields%20%28CanFields%29%2C%20a%20new%20method%20for%0Areconstructing%20a%20time%20series%20of%20independently%20captured%203D%20scans%20into%20a%20single%2C%0Acoherent%20deforming%20shape.%20This%204D%20representation%20enables%20continuous%20refinement%0Aacross%20both%20space%20and%20time.%20Unlike%20prior%20methods%20that%20often%20over-smooth%20the%0Ageometry%20or%20produce%20topological%20and%20geometric%20artifacts%2C%20CanFields%20effectively%0Alearns%20geometry%20and%20deformation%20in%20an%20unsupervised%20way%20by%20incorporating%20two%0Ageometric%20priors.%20First%2C%20we%20introduce%20a%20dynamic%20consolidator%20module%20that%0Aadjusts%20the%20input%20and%20assigns%20confidence%20scores%2C%20balancing%20the%20learning%20of%20the%0Acanonical%20shape%20and%20its%20deformations.%20Second%2C%20we%20use%20low-frequency%20velocity%0Afields%20to%20guide%20deformation%20while%20preserving%20fine%20details%20in%20canonical%20shapes%0Athrough%20high-frequency%20bias.%20We%20validate%20the%20robustness%20and%20accuracy%20of%0ACanFields%20on%20diverse%20raw%20scans%2C%20demonstrating%20its%20superior%20performance%20even%0Awith%20missing%20regions%2C%20sparse%20frames%2C%20and%20noise.%20Code%20is%20available%20in%20the%0Asupplementary%20materials%20and%20will%20be%20released%20publicly%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18582v2&entry.124074799=Read"},
{"title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model", "author": "Zhen Ye and Peiwen Sun and Jiahe Lei and Hongzhan Lin and Xu Tan and Zheqi Dai and Qiuqiang Kong and Jianyi Chen and Jiahao Pan and Qifeng Liu and Yike Guo and Wei Xue", "abstract": "  Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)\n", "link": "http://arxiv.org/abs/2408.17175v3", "date": "2024-11-27", "relevancy": 2.7605, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model&body=Title%3A%20Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model%0AAuthor%3A%20Zhen%20Ye%20and%20Peiwen%20Sun%20and%20Jiahe%20Lei%20and%20Hongzhan%20Lin%20and%20Xu%20Tan%20and%20Zheqi%20Dai%20and%20Qiuqiang%20Kong%20and%20Jianyi%20Chen%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Wei%20Xue%0AAbstract%3A%20%20%20Recent%20advancements%20in%20audio%20generation%20have%20been%20significantly%20propelled%20by%0Athe%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20existing%20research%20on%0Aaudio%20LLM%20has%20primarily%20focused%20on%20enhancing%20the%20architecture%20and%20scale%20of%0Aaudio%20language%20models%2C%20as%20well%20as%20leveraging%20larger%20datasets%2C%20and%20generally%2C%0Aacoustic%20codecs%2C%20such%20as%20EnCodec%2C%20are%20used%20for%20audio%20tokenization.%20However%2C%0Athese%20codecs%20were%20originally%20designed%20for%20audio%20compression%2C%20which%20may%20lead%20to%0Asuboptimal%20performance%20in%20the%20context%20of%20audio%20LLM.%20Our%20research%20aims%20to%0Aaddress%20the%20shortcomings%20of%20current%20audio%20LLM%20codecs%2C%20particularly%20their%0Achallenges%20in%20maintaining%20semantic%20integrity%20in%20generated%20audio.%20For%20instance%2C%0Aexisting%20methods%20like%20VALL-E%2C%20which%20condition%20acoustic%20token%20generation%20on%20text%0Atranscriptions%2C%20often%20suffer%20from%20content%20inaccuracies%20and%20elevated%20word%20error%0Arates%20%28WER%29%20due%20to%20semantic%20misinterpretations%20of%20acoustic%20tokens%2C%20resulting%20in%0Aword%20skipping%20and%20errors.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Astraightforward%20yet%20effective%20approach%20called%20X-Codec.%20X-Codec%20incorporates%0Asemantic%20features%20from%20a%20pre-trained%20semantic%20encoder%20before%20the%20Residual%0AVector%20Quantization%20%28RVQ%29%20stage%20and%20introduces%20a%20semantic%20reconstruction%20loss%0Aafter%20RVQ.%20By%20enhancing%20the%20semantic%20ability%20of%20the%20codec%2C%20X-Codec%0Asignificantly%20reduces%20WER%20in%20speech%20synthesis%20tasks%20and%20extends%20these%20benefits%0Ato%20non-speech%20applications%2C%20including%20music%20and%20sound%20generation.%20Our%0Aexperiments%20in%20text-to-speech%2C%20music%20continuation%2C%20and%20text-to-sound%20tasks%0Ademonstrate%20that%20integrating%20semantic%20information%20substantially%20improves%20the%0Aoverall%20performance%20of%20language%20models%20in%20audio%20generation.%20Our%20code%20and%20demo%0Aare%20available%20%28Demo%3A%20https%3A//x-codec-audio.github.io%20Code%3A%0Ahttps%3A//github.com/zhenye234/xcodec%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodec%2520Does%2520Matter%253A%2520Exploring%2520the%2520Semantic%2520Shortcoming%2520of%2520Codec%2520for%2520Audio%250A%2520%2520Language%2520Model%26entry.906535625%3DZhen%2520Ye%2520and%2520Peiwen%2520Sun%2520and%2520Jiahe%2520Lei%2520and%2520Hongzhan%2520Lin%2520and%2520Xu%2520Tan%2520and%2520Zheqi%2520Dai%2520and%2520Qiuqiang%2520Kong%2520and%2520Jianyi%2520Chen%2520and%2520Jiahao%2520Pan%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%2520and%2520Wei%2520Xue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520audio%2520generation%2520have%2520been%2520significantly%2520propelled%2520by%250Athe%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520existing%2520research%2520on%250Aaudio%2520LLM%2520has%2520primarily%2520focused%2520on%2520enhancing%2520the%2520architecture%2520and%2520scale%2520of%250Aaudio%2520language%2520models%252C%2520as%2520well%2520as%2520leveraging%2520larger%2520datasets%252C%2520and%2520generally%252C%250Aacoustic%2520codecs%252C%2520such%2520as%2520EnCodec%252C%2520are%2520used%2520for%2520audio%2520tokenization.%2520However%252C%250Athese%2520codecs%2520were%2520originally%2520designed%2520for%2520audio%2520compression%252C%2520which%2520may%2520lead%2520to%250Asuboptimal%2520performance%2520in%2520the%2520context%2520of%2520audio%2520LLM.%2520Our%2520research%2520aims%2520to%250Aaddress%2520the%2520shortcomings%2520of%2520current%2520audio%2520LLM%2520codecs%252C%2520particularly%2520their%250Achallenges%2520in%2520maintaining%2520semantic%2520integrity%2520in%2520generated%2520audio.%2520For%2520instance%252C%250Aexisting%2520methods%2520like%2520VALL-E%252C%2520which%2520condition%2520acoustic%2520token%2520generation%2520on%2520text%250Atranscriptions%252C%2520often%2520suffer%2520from%2520content%2520inaccuracies%2520and%2520elevated%2520word%2520error%250Arates%2520%2528WER%2529%2520due%2520to%2520semantic%2520misinterpretations%2520of%2520acoustic%2520tokens%252C%2520resulting%2520in%250Aword%2520skipping%2520and%2520errors.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%250Astraightforward%2520yet%2520effective%2520approach%2520called%2520X-Codec.%2520X-Codec%2520incorporates%250Asemantic%2520features%2520from%2520a%2520pre-trained%2520semantic%2520encoder%2520before%2520the%2520Residual%250AVector%2520Quantization%2520%2528RVQ%2529%2520stage%2520and%2520introduces%2520a%2520semantic%2520reconstruction%2520loss%250Aafter%2520RVQ.%2520By%2520enhancing%2520the%2520semantic%2520ability%2520of%2520the%2520codec%252C%2520X-Codec%250Asignificantly%2520reduces%2520WER%2520in%2520speech%2520synthesis%2520tasks%2520and%2520extends%2520these%2520benefits%250Ato%2520non-speech%2520applications%252C%2520including%2520music%2520and%2520sound%2520generation.%2520Our%250Aexperiments%2520in%2520text-to-speech%252C%2520music%2520continuation%252C%2520and%2520text-to-sound%2520tasks%250Ademonstrate%2520that%2520integrating%2520semantic%2520information%2520substantially%2520improves%2520the%250Aoverall%2520performance%2520of%2520language%2520models%2520in%2520audio%2520generation.%2520Our%2520code%2520and%2520demo%250Aare%2520available%2520%2528Demo%253A%2520https%253A//x-codec-audio.github.io%2520Code%253A%250Ahttps%253A//github.com/zhenye234/xcodec%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model&entry.906535625=Zhen%20Ye%20and%20Peiwen%20Sun%20and%20Jiahe%20Lei%20and%20Hongzhan%20Lin%20and%20Xu%20Tan%20and%20Zheqi%20Dai%20and%20Qiuqiang%20Kong%20and%20Jianyi%20Chen%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Wei%20Xue&entry.1292438233=%20%20Recent%20advancements%20in%20audio%20generation%20have%20been%20significantly%20propelled%20by%0Athe%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20existing%20research%20on%0Aaudio%20LLM%20has%20primarily%20focused%20on%20enhancing%20the%20architecture%20and%20scale%20of%0Aaudio%20language%20models%2C%20as%20well%20as%20leveraging%20larger%20datasets%2C%20and%20generally%2C%0Aacoustic%20codecs%2C%20such%20as%20EnCodec%2C%20are%20used%20for%20audio%20tokenization.%20However%2C%0Athese%20codecs%20were%20originally%20designed%20for%20audio%20compression%2C%20which%20may%20lead%20to%0Asuboptimal%20performance%20in%20the%20context%20of%20audio%20LLM.%20Our%20research%20aims%20to%0Aaddress%20the%20shortcomings%20of%20current%20audio%20LLM%20codecs%2C%20particularly%20their%0Achallenges%20in%20maintaining%20semantic%20integrity%20in%20generated%20audio.%20For%20instance%2C%0Aexisting%20methods%20like%20VALL-E%2C%20which%20condition%20acoustic%20token%20generation%20on%20text%0Atranscriptions%2C%20often%20suffer%20from%20content%20inaccuracies%20and%20elevated%20word%20error%0Arates%20%28WER%29%20due%20to%20semantic%20misinterpretations%20of%20acoustic%20tokens%2C%20resulting%20in%0Aword%20skipping%20and%20errors.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Astraightforward%20yet%20effective%20approach%20called%20X-Codec.%20X-Codec%20incorporates%0Asemantic%20features%20from%20a%20pre-trained%20semantic%20encoder%20before%20the%20Residual%0AVector%20Quantization%20%28RVQ%29%20stage%20and%20introduces%20a%20semantic%20reconstruction%20loss%0Aafter%20RVQ.%20By%20enhancing%20the%20semantic%20ability%20of%20the%20codec%2C%20X-Codec%0Asignificantly%20reduces%20WER%20in%20speech%20synthesis%20tasks%20and%20extends%20these%20benefits%0Ato%20non-speech%20applications%2C%20including%20music%20and%20sound%20generation.%20Our%0Aexperiments%20in%20text-to-speech%2C%20music%20continuation%2C%20and%20text-to-sound%20tasks%0Ademonstrate%20that%20integrating%20semantic%20information%20substantially%20improves%20the%0Aoverall%20performance%20of%20language%20models%20in%20audio%20generation.%20Our%20code%20and%20demo%0Aare%20available%20%28Demo%3A%20https%3A//x-codec-audio.github.io%20Code%3A%0Ahttps%3A//github.com/zhenye234/xcodec%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17175v3&entry.124074799=Read"},
{"title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates", "author": "Yijia Hong and Yuan-Chen Guo and Ran Yi and Yulong Chen and Yan-Pei Cao and Lizhuang Ma", "abstract": "  Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds. The project page is at\nhttps://hyj542682306.github.io/SuperMat/.\n", "link": "http://arxiv.org/abs/2411.17515v2", "date": "2024-11-27", "relevancy": 2.7429, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5689}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5392}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates&body=Title%3A%20SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates%0AAuthor%3A%20Yijia%20Hong%20and%20Yuan-Chen%20Guo%20and%20Ran%20Yi%20and%20Yulong%20Chen%20and%20Yan-Pei%20Cao%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Decomposing%20physically-based%20materials%20from%20images%20into%20their%20constituent%0Aproperties%20remains%20challenging%2C%20particularly%20when%20maintaining%20both%0Acomputational%20efficiency%20and%20physical%20consistency.%20While%20recent%20diffusion-based%0Aapproaches%20have%20shown%20promise%2C%20they%20face%20substantial%20computational%20overhead%20due%0Ato%20multiple%20denoising%20steps%20and%20separate%20models%20for%20different%20material%0Aproperties.%20We%20present%20SuperMat%2C%20a%20single-step%20framework%20that%20achieves%0Ahigh-quality%20material%20decomposition%20with%20one-step%20inference.%20This%20enables%0Aend-to-end%20training%20with%20perceptual%20and%20re-render%20losses%20while%20decomposing%0Aalbedo%2C%20metallic%2C%20and%20roughness%20maps%20at%20millisecond-scale%20speeds.%20We%20further%0Aextend%20our%20framework%20to%203D%20objects%20through%20a%20UV%20refinement%20network%2C%20enabling%0Aconsistent%20material%20estimation%20across%20viewpoints%20while%20maintaining%20efficiency.%0AExperiments%20demonstrate%20that%20SuperMat%20achieves%20state-of-the-art%20PBR%20material%0Adecomposition%20quality%20while%20reducing%20inference%20time%20from%20seconds%20to%0Amilliseconds%20per%20image%2C%20and%20completes%20PBR%20material%20estimation%20for%203D%20objects%20in%0Aapproximately%203%20seconds.%20The%20project%20page%20is%20at%0Ahttps%3A//hyj542682306.github.io/SuperMat/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperMat%253A%2520Physically%2520Consistent%2520PBR%2520Material%2520Estimation%2520at%2520Interactive%250A%2520%2520Rates%26entry.906535625%3DYijia%2520Hong%2520and%2520Yuan-Chen%2520Guo%2520and%2520Ran%2520Yi%2520and%2520Yulong%2520Chen%2520and%2520Yan-Pei%2520Cao%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Decomposing%2520physically-based%2520materials%2520from%2520images%2520into%2520their%2520constituent%250Aproperties%2520remains%2520challenging%252C%2520particularly%2520when%2520maintaining%2520both%250Acomputational%2520efficiency%2520and%2520physical%2520consistency.%2520While%2520recent%2520diffusion-based%250Aapproaches%2520have%2520shown%2520promise%252C%2520they%2520face%2520substantial%2520computational%2520overhead%2520due%250Ato%2520multiple%2520denoising%2520steps%2520and%2520separate%2520models%2520for%2520different%2520material%250Aproperties.%2520We%2520present%2520SuperMat%252C%2520a%2520single-step%2520framework%2520that%2520achieves%250Ahigh-quality%2520material%2520decomposition%2520with%2520one-step%2520inference.%2520This%2520enables%250Aend-to-end%2520training%2520with%2520perceptual%2520and%2520re-render%2520losses%2520while%2520decomposing%250Aalbedo%252C%2520metallic%252C%2520and%2520roughness%2520maps%2520at%2520millisecond-scale%2520speeds.%2520We%2520further%250Aextend%2520our%2520framework%2520to%25203D%2520objects%2520through%2520a%2520UV%2520refinement%2520network%252C%2520enabling%250Aconsistent%2520material%2520estimation%2520across%2520viewpoints%2520while%2520maintaining%2520efficiency.%250AExperiments%2520demonstrate%2520that%2520SuperMat%2520achieves%2520state-of-the-art%2520PBR%2520material%250Adecomposition%2520quality%2520while%2520reducing%2520inference%2520time%2520from%2520seconds%2520to%250Amilliseconds%2520per%2520image%252C%2520and%2520completes%2520PBR%2520material%2520estimation%2520for%25203D%2520objects%2520in%250Aapproximately%25203%2520seconds.%2520The%2520project%2520page%2520is%2520at%250Ahttps%253A//hyj542682306.github.io/SuperMat/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates&entry.906535625=Yijia%20Hong%20and%20Yuan-Chen%20Guo%20and%20Ran%20Yi%20and%20Yulong%20Chen%20and%20Yan-Pei%20Cao%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Decomposing%20physically-based%20materials%20from%20images%20into%20their%20constituent%0Aproperties%20remains%20challenging%2C%20particularly%20when%20maintaining%20both%0Acomputational%20efficiency%20and%20physical%20consistency.%20While%20recent%20diffusion-based%0Aapproaches%20have%20shown%20promise%2C%20they%20face%20substantial%20computational%20overhead%20due%0Ato%20multiple%20denoising%20steps%20and%20separate%20models%20for%20different%20material%0Aproperties.%20We%20present%20SuperMat%2C%20a%20single-step%20framework%20that%20achieves%0Ahigh-quality%20material%20decomposition%20with%20one-step%20inference.%20This%20enables%0Aend-to-end%20training%20with%20perceptual%20and%20re-render%20losses%20while%20decomposing%0Aalbedo%2C%20metallic%2C%20and%20roughness%20maps%20at%20millisecond-scale%20speeds.%20We%20further%0Aextend%20our%20framework%20to%203D%20objects%20through%20a%20UV%20refinement%20network%2C%20enabling%0Aconsistent%20material%20estimation%20across%20viewpoints%20while%20maintaining%20efficiency.%0AExperiments%20demonstrate%20that%20SuperMat%20achieves%20state-of-the-art%20PBR%20material%0Adecomposition%20quality%20while%20reducing%20inference%20time%20from%20seconds%20to%0Amilliseconds%20per%20image%2C%20and%20completes%20PBR%20material%20estimation%20for%203D%20objects%20in%0Aapproximately%203%20seconds.%20The%20project%20page%20is%20at%0Ahttps%3A//hyj542682306.github.io/SuperMat/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17515v2&entry.124074799=Read"},
{"title": "Local Map Construction with SDMap: A Comprehensive Survey", "author": "Jiaqi Li and Pingfan Jia and Jiaxing Chen and Jiaxi Liu and Lei He and Keqiang Li", "abstract": "  Local map construction is a vital component of intelligent driving\nperception, offering necessary reference for vehicle positioning and planning.\nStandard Definition map (SDMap), known for its low cost, accessibility, and\nversatility, has significant potential as prior information for local map\nperception. This paper mainly reviews the local map construction methods with\nSDMap, including definitions, general processing flow, and datasets. Besides,\nthis paper analyzes multimodal data representation and fusion methods in\nSDMap-based local map construction. This paper also discusses key challenges\nand future directions, such as optimizing SDMap processing, enhancing spatial\nalignment with real-time data, and incorporating richer environmental\ninformation. At last, the review looks forward to future research focusing on\nenhancing road topology inference and multimodal data fusion to improve the\nrobustness and scalability of local map perception.\n", "link": "http://arxiv.org/abs/2409.02415v2", "date": "2024-11-27", "relevancy": 2.7363, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6197}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Map%20Construction%20with%20SDMap%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Local%20Map%20Construction%20with%20SDMap%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Jiaqi%20Li%20and%20Pingfan%20Jia%20and%20Jiaxing%20Chen%20and%20Jiaxi%20Liu%20and%20Lei%20He%20and%20Keqiang%20Li%0AAbstract%3A%20%20%20Local%20map%20construction%20is%20a%20vital%20component%20of%20intelligent%20driving%0Aperception%2C%20offering%20necessary%20reference%20for%20vehicle%20positioning%20and%20planning.%0AStandard%20Definition%20map%20%28SDMap%29%2C%20known%20for%20its%20low%20cost%2C%20accessibility%2C%20and%0Aversatility%2C%20has%20significant%20potential%20as%20prior%20information%20for%20local%20map%0Aperception.%20This%20paper%20mainly%20reviews%20the%20local%20map%20construction%20methods%20with%0ASDMap%2C%20including%20definitions%2C%20general%20processing%20flow%2C%20and%20datasets.%20Besides%2C%0Athis%20paper%20analyzes%20multimodal%20data%20representation%20and%20fusion%20methods%20in%0ASDMap-based%20local%20map%20construction.%20This%20paper%20also%20discusses%20key%20challenges%0Aand%20future%20directions%2C%20such%20as%20optimizing%20SDMap%20processing%2C%20enhancing%20spatial%0Aalignment%20with%20real-time%20data%2C%20and%20incorporating%20richer%20environmental%0Ainformation.%20At%20last%2C%20the%20review%20looks%20forward%20to%20future%20research%20focusing%20on%0Aenhancing%20road%20topology%20inference%20and%20multimodal%20data%20fusion%20to%20improve%20the%0Arobustness%20and%20scalability%20of%20local%20map%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Map%2520Construction%2520with%2520SDMap%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DJiaqi%2520Li%2520and%2520Pingfan%2520Jia%2520and%2520Jiaxing%2520Chen%2520and%2520Jiaxi%2520Liu%2520and%2520Lei%2520He%2520and%2520Keqiang%2520Li%26entry.1292438233%3D%2520%2520Local%2520map%2520construction%2520is%2520a%2520vital%2520component%2520of%2520intelligent%2520driving%250Aperception%252C%2520offering%2520necessary%2520reference%2520for%2520vehicle%2520positioning%2520and%2520planning.%250AStandard%2520Definition%2520map%2520%2528SDMap%2529%252C%2520known%2520for%2520its%2520low%2520cost%252C%2520accessibility%252C%2520and%250Aversatility%252C%2520has%2520significant%2520potential%2520as%2520prior%2520information%2520for%2520local%2520map%250Aperception.%2520This%2520paper%2520mainly%2520reviews%2520the%2520local%2520map%2520construction%2520methods%2520with%250ASDMap%252C%2520including%2520definitions%252C%2520general%2520processing%2520flow%252C%2520and%2520datasets.%2520Besides%252C%250Athis%2520paper%2520analyzes%2520multimodal%2520data%2520representation%2520and%2520fusion%2520methods%2520in%250ASDMap-based%2520local%2520map%2520construction.%2520This%2520paper%2520also%2520discusses%2520key%2520challenges%250Aand%2520future%2520directions%252C%2520such%2520as%2520optimizing%2520SDMap%2520processing%252C%2520enhancing%2520spatial%250Aalignment%2520with%2520real-time%2520data%252C%2520and%2520incorporating%2520richer%2520environmental%250Ainformation.%2520At%2520last%252C%2520the%2520review%2520looks%2520forward%2520to%2520future%2520research%2520focusing%2520on%250Aenhancing%2520road%2520topology%2520inference%2520and%2520multimodal%2520data%2520fusion%2520to%2520improve%2520the%250Arobustness%2520and%2520scalability%2520of%2520local%2520map%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Map%20Construction%20with%20SDMap%3A%20A%20Comprehensive%20Survey&entry.906535625=Jiaqi%20Li%20and%20Pingfan%20Jia%20and%20Jiaxing%20Chen%20and%20Jiaxi%20Liu%20and%20Lei%20He%20and%20Keqiang%20Li&entry.1292438233=%20%20Local%20map%20construction%20is%20a%20vital%20component%20of%20intelligent%20driving%0Aperception%2C%20offering%20necessary%20reference%20for%20vehicle%20positioning%20and%20planning.%0AStandard%20Definition%20map%20%28SDMap%29%2C%20known%20for%20its%20low%20cost%2C%20accessibility%2C%20and%0Aversatility%2C%20has%20significant%20potential%20as%20prior%20information%20for%20local%20map%0Aperception.%20This%20paper%20mainly%20reviews%20the%20local%20map%20construction%20methods%20with%0ASDMap%2C%20including%20definitions%2C%20general%20processing%20flow%2C%20and%20datasets.%20Besides%2C%0Athis%20paper%20analyzes%20multimodal%20data%20representation%20and%20fusion%20methods%20in%0ASDMap-based%20local%20map%20construction.%20This%20paper%20also%20discusses%20key%20challenges%0Aand%20future%20directions%2C%20such%20as%20optimizing%20SDMap%20processing%2C%20enhancing%20spatial%0Aalignment%20with%20real-time%20data%2C%20and%20incorporating%20richer%20environmental%0Ainformation.%20At%20last%2C%20the%20review%20looks%20forward%20to%20future%20research%20focusing%20on%0Aenhancing%20road%20topology%20inference%20and%20multimodal%20data%20fusion%20to%20improve%20the%0Arobustness%20and%20scalability%20of%20local%20map%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02415v2&entry.124074799=Read"},
{"title": "Neural Image Unfolding: Flattening Sparse Anatomical Structures using\n  Neural Fields", "author": "Leonhard Rist and Pluvio Stephan and Noah Maul and Linda Vorberg and Hendrik Ditt and Michael S\u00fchling and Andreas Maier and Bernhard Egger and Oliver Taubmann", "abstract": "  Tomographic imaging reveals internal structures of 3D objects and is crucial\nfor medical diagnoses. Visualizing the morphology and appearance of non-planar\nsparse anatomical structures that extend over multiple 2D slices in tomographic\nvolumes is inherently difficult but valuable for decision-making and reporting.\nHence, various organ-specific unfolding techniques exist to map their densely\nsampled 3D surfaces to a distortion-minimized 2D representation. However, there\nis no versatile framework to flatten complex sparse structures including\nvascular, duct or bone systems. We deploy a neural field to fit the\ntransformation of the anatomy of interest to a 2D overview image. We further\npropose distortion regularization strategies and combine geometric with\nintensity-based loss formulations to also display non-annotated and auxiliary\ntargets. In addition to improved versatility, our unfolding technique\noutperforms mesh-based baselines for sparse structures w.r.t. peak distortion\nand our regularization scheme yields smoother transformations compared to\nJacobian formulations from neural field-based image registration.\n", "link": "http://arxiv.org/abs/2411.18415v1", "date": "2024-11-27", "relevancy": 2.7347, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5595}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Image%20Unfolding%3A%20Flattening%20Sparse%20Anatomical%20Structures%20using%0A%20%20Neural%20Fields&body=Title%3A%20Neural%20Image%20Unfolding%3A%20Flattening%20Sparse%20Anatomical%20Structures%20using%0A%20%20Neural%20Fields%0AAuthor%3A%20Leonhard%20Rist%20and%20Pluvio%20Stephan%20and%20Noah%20Maul%20and%20Linda%20Vorberg%20and%20Hendrik%20Ditt%20and%20Michael%20S%C3%BChling%20and%20Andreas%20Maier%20and%20Bernhard%20Egger%20and%20Oliver%20Taubmann%0AAbstract%3A%20%20%20Tomographic%20imaging%20reveals%20internal%20structures%20of%203D%20objects%20and%20is%20crucial%0Afor%20medical%20diagnoses.%20Visualizing%20the%20morphology%20and%20appearance%20of%20non-planar%0Asparse%20anatomical%20structures%20that%20extend%20over%20multiple%202D%20slices%20in%20tomographic%0Avolumes%20is%20inherently%20difficult%20but%20valuable%20for%20decision-making%20and%20reporting.%0AHence%2C%20various%20organ-specific%20unfolding%20techniques%20exist%20to%20map%20their%20densely%0Asampled%203D%20surfaces%20to%20a%20distortion-minimized%202D%20representation.%20However%2C%20there%0Ais%20no%20versatile%20framework%20to%20flatten%20complex%20sparse%20structures%20including%0Avascular%2C%20duct%20or%20bone%20systems.%20We%20deploy%20a%20neural%20field%20to%20fit%20the%0Atransformation%20of%20the%20anatomy%20of%20interest%20to%20a%202D%20overview%20image.%20We%20further%0Apropose%20distortion%20regularization%20strategies%20and%20combine%20geometric%20with%0Aintensity-based%20loss%20formulations%20to%20also%20display%20non-annotated%20and%20auxiliary%0Atargets.%20In%20addition%20to%20improved%20versatility%2C%20our%20unfolding%20technique%0Aoutperforms%20mesh-based%20baselines%20for%20sparse%20structures%20w.r.t.%20peak%20distortion%0Aand%20our%20regularization%20scheme%20yields%20smoother%20transformations%20compared%20to%0AJacobian%20formulations%20from%20neural%20field-based%20image%20registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Image%2520Unfolding%253A%2520Flattening%2520Sparse%2520Anatomical%2520Structures%2520using%250A%2520%2520Neural%2520Fields%26entry.906535625%3DLeonhard%2520Rist%2520and%2520Pluvio%2520Stephan%2520and%2520Noah%2520Maul%2520and%2520Linda%2520Vorberg%2520and%2520Hendrik%2520Ditt%2520and%2520Michael%2520S%25C3%25BChling%2520and%2520Andreas%2520Maier%2520and%2520Bernhard%2520Egger%2520and%2520Oliver%2520Taubmann%26entry.1292438233%3D%2520%2520Tomographic%2520imaging%2520reveals%2520internal%2520structures%2520of%25203D%2520objects%2520and%2520is%2520crucial%250Afor%2520medical%2520diagnoses.%2520Visualizing%2520the%2520morphology%2520and%2520appearance%2520of%2520non-planar%250Asparse%2520anatomical%2520structures%2520that%2520extend%2520over%2520multiple%25202D%2520slices%2520in%2520tomographic%250Avolumes%2520is%2520inherently%2520difficult%2520but%2520valuable%2520for%2520decision-making%2520and%2520reporting.%250AHence%252C%2520various%2520organ-specific%2520unfolding%2520techniques%2520exist%2520to%2520map%2520their%2520densely%250Asampled%25203D%2520surfaces%2520to%2520a%2520distortion-minimized%25202D%2520representation.%2520However%252C%2520there%250Ais%2520no%2520versatile%2520framework%2520to%2520flatten%2520complex%2520sparse%2520structures%2520including%250Avascular%252C%2520duct%2520or%2520bone%2520systems.%2520We%2520deploy%2520a%2520neural%2520field%2520to%2520fit%2520the%250Atransformation%2520of%2520the%2520anatomy%2520of%2520interest%2520to%2520a%25202D%2520overview%2520image.%2520We%2520further%250Apropose%2520distortion%2520regularization%2520strategies%2520and%2520combine%2520geometric%2520with%250Aintensity-based%2520loss%2520formulations%2520to%2520also%2520display%2520non-annotated%2520and%2520auxiliary%250Atargets.%2520In%2520addition%2520to%2520improved%2520versatility%252C%2520our%2520unfolding%2520technique%250Aoutperforms%2520mesh-based%2520baselines%2520for%2520sparse%2520structures%2520w.r.t.%2520peak%2520distortion%250Aand%2520our%2520regularization%2520scheme%2520yields%2520smoother%2520transformations%2520compared%2520to%250AJacobian%2520formulations%2520from%2520neural%2520field-based%2520image%2520registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Image%20Unfolding%3A%20Flattening%20Sparse%20Anatomical%20Structures%20using%0A%20%20Neural%20Fields&entry.906535625=Leonhard%20Rist%20and%20Pluvio%20Stephan%20and%20Noah%20Maul%20and%20Linda%20Vorberg%20and%20Hendrik%20Ditt%20and%20Michael%20S%C3%BChling%20and%20Andreas%20Maier%20and%20Bernhard%20Egger%20and%20Oliver%20Taubmann&entry.1292438233=%20%20Tomographic%20imaging%20reveals%20internal%20structures%20of%203D%20objects%20and%20is%20crucial%0Afor%20medical%20diagnoses.%20Visualizing%20the%20morphology%20and%20appearance%20of%20non-planar%0Asparse%20anatomical%20structures%20that%20extend%20over%20multiple%202D%20slices%20in%20tomographic%0Avolumes%20is%20inherently%20difficult%20but%20valuable%20for%20decision-making%20and%20reporting.%0AHence%2C%20various%20organ-specific%20unfolding%20techniques%20exist%20to%20map%20their%20densely%0Asampled%203D%20surfaces%20to%20a%20distortion-minimized%202D%20representation.%20However%2C%20there%0Ais%20no%20versatile%20framework%20to%20flatten%20complex%20sparse%20structures%20including%0Avascular%2C%20duct%20or%20bone%20systems.%20We%20deploy%20a%20neural%20field%20to%20fit%20the%0Atransformation%20of%20the%20anatomy%20of%20interest%20to%20a%202D%20overview%20image.%20We%20further%0Apropose%20distortion%20regularization%20strategies%20and%20combine%20geometric%20with%0Aintensity-based%20loss%20formulations%20to%20also%20display%20non-annotated%20and%20auxiliary%0Atargets.%20In%20addition%20to%20improved%20versatility%2C%20our%20unfolding%20technique%0Aoutperforms%20mesh-based%20baselines%20for%20sparse%20structures%20w.r.t.%20peak%20distortion%0Aand%20our%20regularization%20scheme%20yields%20smoother%20transformations%20compared%20to%0AJacobian%20formulations%20from%20neural%20field-based%20image%20registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18415v1&entry.124074799=Read"},
{"title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction\n  using Diffusion Models", "author": "Riza Velioglu and Petra Bevandic and Robin Chan and Barbara Hammer", "abstract": "  This paper introduces Virtual Try-Off (VTOFF), a novel task focused on\ngenerating standardized garment images from single photos of clothed\nindividuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses\nmodels, VTOFF aims to extract a canonical garment image, posing unique\nchallenges in capturing garment shape, texture, and intricate patterns. This\nwell-defined target makes VTOFF particularly effective for evaluating\nreconstruction fidelity in generative models. We present TryOffDiff, a model\nthat adapts Stable Diffusion with SigLIP-based visual conditioning to ensure\nhigh fidelity and detail retention. Experiments on a modified VITON-HD dataset\nshow that our approach outperforms baseline methods based on pose transfer and\nvirtual try-on with fewer pre- and post-processing steps. Our analysis reveals\nthat traditional image generation metrics inadequately assess reconstruction\nquality, prompting us to rely on DISTS for more accurate evaluation. Our\nresults highlight the potential of VTOFF to enhance product imagery in\ne-commerce applications, advance generative model evaluation, and inspire\nfuture work on high-fidelity reconstruction. Demo, code, and models are\navailable at: https://rizavelioglu.github.io/tryoffdiff/\n", "link": "http://arxiv.org/abs/2411.18350v1", "date": "2024-11-27", "relevancy": 2.7347, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7106}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6761}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TryOffDiff%3A%20Virtual-Try-Off%20via%20High-Fidelity%20Garment%20Reconstruction%0A%20%20using%20Diffusion%20Models&body=Title%3A%20TryOffDiff%3A%20Virtual-Try-Off%20via%20High-Fidelity%20Garment%20Reconstruction%0A%20%20using%20Diffusion%20Models%0AAuthor%3A%20Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20This%20paper%20introduces%20Virtual%20Try-Off%20%28VTOFF%29%2C%20a%20novel%20task%20focused%20on%0Agenerating%20standardized%20garment%20images%20from%20single%20photos%20of%20clothed%0Aindividuals.%20Unlike%20traditional%20Virtual%20Try-On%20%28VTON%29%2C%20which%20digitally%20dresses%0Amodels%2C%20VTOFF%20aims%20to%20extract%20a%20canonical%20garment%20image%2C%20posing%20unique%0Achallenges%20in%20capturing%20garment%20shape%2C%20texture%2C%20and%20intricate%20patterns.%20This%0Awell-defined%20target%20makes%20VTOFF%20particularly%20effective%20for%20evaluating%0Areconstruction%20fidelity%20in%20generative%20models.%20We%20present%20TryOffDiff%2C%20a%20model%0Athat%20adapts%20Stable%20Diffusion%20with%20SigLIP-based%20visual%20conditioning%20to%20ensure%0Ahigh%20fidelity%20and%20detail%20retention.%20Experiments%20on%20a%20modified%20VITON-HD%20dataset%0Ashow%20that%20our%20approach%20outperforms%20baseline%20methods%20based%20on%20pose%20transfer%20and%0Avirtual%20try-on%20with%20fewer%20pre-%20and%20post-processing%20steps.%20Our%20analysis%20reveals%0Athat%20traditional%20image%20generation%20metrics%20inadequately%20assess%20reconstruction%0Aquality%2C%20prompting%20us%20to%20rely%20on%20DISTS%20for%20more%20accurate%20evaluation.%20Our%0Aresults%20highlight%20the%20potential%20of%20VTOFF%20to%20enhance%20product%20imagery%20in%0Ae-commerce%20applications%2C%20advance%20generative%20model%20evaluation%2C%20and%20inspire%0Afuture%20work%20on%20high-fidelity%20reconstruction.%20Demo%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTryOffDiff%253A%2520Virtual-Try-Off%2520via%2520High-Fidelity%2520Garment%2520Reconstruction%250A%2520%2520using%2520Diffusion%2520Models%26entry.906535625%3DRiza%2520Velioglu%2520and%2520Petra%2520Bevandic%2520and%2520Robin%2520Chan%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Virtual%2520Try-Off%2520%2528VTOFF%2529%252C%2520a%2520novel%2520task%2520focused%2520on%250Agenerating%2520standardized%2520garment%2520images%2520from%2520single%2520photos%2520of%2520clothed%250Aindividuals.%2520Unlike%2520traditional%2520Virtual%2520Try-On%2520%2528VTON%2529%252C%2520which%2520digitally%2520dresses%250Amodels%252C%2520VTOFF%2520aims%2520to%2520extract%2520a%2520canonical%2520garment%2520image%252C%2520posing%2520unique%250Achallenges%2520in%2520capturing%2520garment%2520shape%252C%2520texture%252C%2520and%2520intricate%2520patterns.%2520This%250Awell-defined%2520target%2520makes%2520VTOFF%2520particularly%2520effective%2520for%2520evaluating%250Areconstruction%2520fidelity%2520in%2520generative%2520models.%2520We%2520present%2520TryOffDiff%252C%2520a%2520model%250Athat%2520adapts%2520Stable%2520Diffusion%2520with%2520SigLIP-based%2520visual%2520conditioning%2520to%2520ensure%250Ahigh%2520fidelity%2520and%2520detail%2520retention.%2520Experiments%2520on%2520a%2520modified%2520VITON-HD%2520dataset%250Ashow%2520that%2520our%2520approach%2520outperforms%2520baseline%2520methods%2520based%2520on%2520pose%2520transfer%2520and%250Avirtual%2520try-on%2520with%2520fewer%2520pre-%2520and%2520post-processing%2520steps.%2520Our%2520analysis%2520reveals%250Athat%2520traditional%2520image%2520generation%2520metrics%2520inadequately%2520assess%2520reconstruction%250Aquality%252C%2520prompting%2520us%2520to%2520rely%2520on%2520DISTS%2520for%2520more%2520accurate%2520evaluation.%2520Our%250Aresults%2520highlight%2520the%2520potential%2520of%2520VTOFF%2520to%2520enhance%2520product%2520imagery%2520in%250Ae-commerce%2520applications%252C%2520advance%2520generative%2520model%2520evaluation%252C%2520and%2520inspire%250Afuture%2520work%2520on%2520high-fidelity%2520reconstruction.%2520Demo%252C%2520code%252C%2520and%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//rizavelioglu.github.io/tryoffdiff/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TryOffDiff%3A%20Virtual-Try-Off%20via%20High-Fidelity%20Garment%20Reconstruction%0A%20%20using%20Diffusion%20Models&entry.906535625=Riza%20Velioglu%20and%20Petra%20Bevandic%20and%20Robin%20Chan%20and%20Barbara%20Hammer&entry.1292438233=%20%20This%20paper%20introduces%20Virtual%20Try-Off%20%28VTOFF%29%2C%20a%20novel%20task%20focused%20on%0Agenerating%20standardized%20garment%20images%20from%20single%20photos%20of%20clothed%0Aindividuals.%20Unlike%20traditional%20Virtual%20Try-On%20%28VTON%29%2C%20which%20digitally%20dresses%0Amodels%2C%20VTOFF%20aims%20to%20extract%20a%20canonical%20garment%20image%2C%20posing%20unique%0Achallenges%20in%20capturing%20garment%20shape%2C%20texture%2C%20and%20intricate%20patterns.%20This%0Awell-defined%20target%20makes%20VTOFF%20particularly%20effective%20for%20evaluating%0Areconstruction%20fidelity%20in%20generative%20models.%20We%20present%20TryOffDiff%2C%20a%20model%0Athat%20adapts%20Stable%20Diffusion%20with%20SigLIP-based%20visual%20conditioning%20to%20ensure%0Ahigh%20fidelity%20and%20detail%20retention.%20Experiments%20on%20a%20modified%20VITON-HD%20dataset%0Ashow%20that%20our%20approach%20outperforms%20baseline%20methods%20based%20on%20pose%20transfer%20and%0Avirtual%20try-on%20with%20fewer%20pre-%20and%20post-processing%20steps.%20Our%20analysis%20reveals%0Athat%20traditional%20image%20generation%20metrics%20inadequately%20assess%20reconstruction%0Aquality%2C%20prompting%20us%20to%20rely%20on%20DISTS%20for%20more%20accurate%20evaluation.%20Our%0Aresults%20highlight%20the%20potential%20of%20VTOFF%20to%20enhance%20product%20imagery%20in%0Ae-commerce%20applications%2C%20advance%20generative%20model%20evaluation%2C%20and%20inspire%0Afuture%20work%20on%20high-fidelity%20reconstruction.%20Demo%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//rizavelioglu.github.io/tryoffdiff/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18350v1&entry.124074799=Read"},
{"title": "Individual Content and Motion Dynamics Preserved Pruning for Video\n  Diffusion Models", "author": "Yiming Wu and Huan Wang and Zhenghao Chen and Dong Xu", "abstract": "  The high computational cost and slow inference time are major obstacles to\ndeploying the video diffusion model (VDM) in practical applications. To\novercome this, we introduce a new Video Diffusion Model Compression approach\nusing individual content and motion dynamics preserved pruning and consistency\nloss. First, we empirically observe that deeper VDM layers are crucial for\nmaintaining the quality of \\textbf{motion dynamics} e.g., coherence of the\nentire video, while shallower layers are more focused on \\textbf{individual\ncontent} e.g., individual frames. Therefore, we prune redundant blocks from the\nshallower layers while preserving more of the deeper layers, resulting in a\nlightweight VDM variant called VDMini. Additionally, we propose an\n\\textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain\ncomparable generation performance as larger VDM, i.e., the teacher to VDMini\ni.e., the student. Particularly, we first use the Individual Content\nDistillation (ICD) Loss to ensure consistency in the features of each generated\nframe between the teacher and student models. Next, we introduce a Multi-frame\nContent Adversarial (MCA) Loss to enhance the motion dynamics across the\ngenerated video as a whole. This method significantly accelerates inference\ntime while maintaining high-quality video generation. Extensive experiments\ndemonstrate the effectiveness of our VDMini on two important video generation\ntasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively\nachieve an average 2.5 $\\times$ and 1.4 $\\times$ speed up for the I2V method\nSF-V and the T2V method T2V-Turbo-v2, while maintaining the quality of the\ngenerated videos on two benchmarks, i.e., UCF101 and VBench.\n", "link": "http://arxiv.org/abs/2411.18375v1", "date": "2024-11-27", "relevancy": 2.7343, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7077}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7061}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Individual%20Content%20and%20Motion%20Dynamics%20Preserved%20Pruning%20for%20Video%0A%20%20Diffusion%20Models&body=Title%3A%20Individual%20Content%20and%20Motion%20Dynamics%20Preserved%20Pruning%20for%20Video%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yiming%20Wu%20and%20Huan%20Wang%20and%20Zhenghao%20Chen%20and%20Dong%20Xu%0AAbstract%3A%20%20%20The%20high%20computational%20cost%20and%20slow%20inference%20time%20are%20major%20obstacles%20to%0Adeploying%20the%20video%20diffusion%20model%20%28VDM%29%20in%20practical%20applications.%20To%0Aovercome%20this%2C%20we%20introduce%20a%20new%20Video%20Diffusion%20Model%20Compression%20approach%0Ausing%20individual%20content%20and%20motion%20dynamics%20preserved%20pruning%20and%20consistency%0Aloss.%20First%2C%20we%20empirically%20observe%20that%20deeper%20VDM%20layers%20are%20crucial%20for%0Amaintaining%20the%20quality%20of%20%5Ctextbf%7Bmotion%20dynamics%7D%20e.g.%2C%20coherence%20of%20the%0Aentire%20video%2C%20while%20shallower%20layers%20are%20more%20focused%20on%20%5Ctextbf%7Bindividual%0Acontent%7D%20e.g.%2C%20individual%20frames.%20Therefore%2C%20we%20prune%20redundant%20blocks%20from%20the%0Ashallower%20layers%20while%20preserving%20more%20of%20the%20deeper%20layers%2C%20resulting%20in%20a%0Alightweight%20VDM%20variant%20called%20VDMini.%20Additionally%2C%20we%20propose%20an%0A%5Ctextbf%7BIndividual%20Content%20and%20Motion%20Dynamics%20%28ICMD%29%7D%20Consistency%20Loss%20to%20gain%0Acomparable%20generation%20performance%20as%20larger%20VDM%2C%20i.e.%2C%20the%20teacher%20to%20VDMini%0Ai.e.%2C%20the%20student.%20Particularly%2C%20we%20first%20use%20the%20Individual%20Content%0ADistillation%20%28ICD%29%20Loss%20to%20ensure%20consistency%20in%20the%20features%20of%20each%20generated%0Aframe%20between%20the%20teacher%20and%20student%20models.%20Next%2C%20we%20introduce%20a%20Multi-frame%0AContent%20Adversarial%20%28MCA%29%20Loss%20to%20enhance%20the%20motion%20dynamics%20across%20the%0Agenerated%20video%20as%20a%20whole.%20This%20method%20significantly%20accelerates%20inference%0Atime%20while%20maintaining%20high-quality%20video%20generation.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20VDMini%20on%20two%20important%20video%20generation%0Atasks%2C%20Text-to-Video%20%28T2V%29%20and%20Image-to-Video%20%28I2V%29%2C%20where%20we%20respectively%0Aachieve%20an%20average%202.5%20%24%5Ctimes%24%20and%201.4%20%24%5Ctimes%24%20speed%20up%20for%20the%20I2V%20method%0ASF-V%20and%20the%20T2V%20method%20T2V-Turbo-v2%2C%20while%20maintaining%20the%20quality%20of%20the%0Agenerated%20videos%20on%20two%20benchmarks%2C%20i.e.%2C%20UCF101%20and%20VBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndividual%2520Content%2520and%2520Motion%2520Dynamics%2520Preserved%2520Pruning%2520for%2520Video%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYiming%2520Wu%2520and%2520Huan%2520Wang%2520and%2520Zhenghao%2520Chen%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520The%2520high%2520computational%2520cost%2520and%2520slow%2520inference%2520time%2520are%2520major%2520obstacles%2520to%250Adeploying%2520the%2520video%2520diffusion%2520model%2520%2528VDM%2529%2520in%2520practical%2520applications.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520a%2520new%2520Video%2520Diffusion%2520Model%2520Compression%2520approach%250Ausing%2520individual%2520content%2520and%2520motion%2520dynamics%2520preserved%2520pruning%2520and%2520consistency%250Aloss.%2520First%252C%2520we%2520empirically%2520observe%2520that%2520deeper%2520VDM%2520layers%2520are%2520crucial%2520for%250Amaintaining%2520the%2520quality%2520of%2520%255Ctextbf%257Bmotion%2520dynamics%257D%2520e.g.%252C%2520coherence%2520of%2520the%250Aentire%2520video%252C%2520while%2520shallower%2520layers%2520are%2520more%2520focused%2520on%2520%255Ctextbf%257Bindividual%250Acontent%257D%2520e.g.%252C%2520individual%2520frames.%2520Therefore%252C%2520we%2520prune%2520redundant%2520blocks%2520from%2520the%250Ashallower%2520layers%2520while%2520preserving%2520more%2520of%2520the%2520deeper%2520layers%252C%2520resulting%2520in%2520a%250Alightweight%2520VDM%2520variant%2520called%2520VDMini.%2520Additionally%252C%2520we%2520propose%2520an%250A%255Ctextbf%257BIndividual%2520Content%2520and%2520Motion%2520Dynamics%2520%2528ICMD%2529%257D%2520Consistency%2520Loss%2520to%2520gain%250Acomparable%2520generation%2520performance%2520as%2520larger%2520VDM%252C%2520i.e.%252C%2520the%2520teacher%2520to%2520VDMini%250Ai.e.%252C%2520the%2520student.%2520Particularly%252C%2520we%2520first%2520use%2520the%2520Individual%2520Content%250ADistillation%2520%2528ICD%2529%2520Loss%2520to%2520ensure%2520consistency%2520in%2520the%2520features%2520of%2520each%2520generated%250Aframe%2520between%2520the%2520teacher%2520and%2520student%2520models.%2520Next%252C%2520we%2520introduce%2520a%2520Multi-frame%250AContent%2520Adversarial%2520%2528MCA%2529%2520Loss%2520to%2520enhance%2520the%2520motion%2520dynamics%2520across%2520the%250Agenerated%2520video%2520as%2520a%2520whole.%2520This%2520method%2520significantly%2520accelerates%2520inference%250Atime%2520while%2520maintaining%2520high-quality%2520video%2520generation.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520VDMini%2520on%2520two%2520important%2520video%2520generation%250Atasks%252C%2520Text-to-Video%2520%2528T2V%2529%2520and%2520Image-to-Video%2520%2528I2V%2529%252C%2520where%2520we%2520respectively%250Aachieve%2520an%2520average%25202.5%2520%2524%255Ctimes%2524%2520and%25201.4%2520%2524%255Ctimes%2524%2520speed%2520up%2520for%2520the%2520I2V%2520method%250ASF-V%2520and%2520the%2520T2V%2520method%2520T2V-Turbo-v2%252C%2520while%2520maintaining%2520the%2520quality%2520of%2520the%250Agenerated%2520videos%2520on%2520two%2520benchmarks%252C%2520i.e.%252C%2520UCF101%2520and%2520VBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Individual%20Content%20and%20Motion%20Dynamics%20Preserved%20Pruning%20for%20Video%0A%20%20Diffusion%20Models&entry.906535625=Yiming%20Wu%20and%20Huan%20Wang%20and%20Zhenghao%20Chen%20and%20Dong%20Xu&entry.1292438233=%20%20The%20high%20computational%20cost%20and%20slow%20inference%20time%20are%20major%20obstacles%20to%0Adeploying%20the%20video%20diffusion%20model%20%28VDM%29%20in%20practical%20applications.%20To%0Aovercome%20this%2C%20we%20introduce%20a%20new%20Video%20Diffusion%20Model%20Compression%20approach%0Ausing%20individual%20content%20and%20motion%20dynamics%20preserved%20pruning%20and%20consistency%0Aloss.%20First%2C%20we%20empirically%20observe%20that%20deeper%20VDM%20layers%20are%20crucial%20for%0Amaintaining%20the%20quality%20of%20%5Ctextbf%7Bmotion%20dynamics%7D%20e.g.%2C%20coherence%20of%20the%0Aentire%20video%2C%20while%20shallower%20layers%20are%20more%20focused%20on%20%5Ctextbf%7Bindividual%0Acontent%7D%20e.g.%2C%20individual%20frames.%20Therefore%2C%20we%20prune%20redundant%20blocks%20from%20the%0Ashallower%20layers%20while%20preserving%20more%20of%20the%20deeper%20layers%2C%20resulting%20in%20a%0Alightweight%20VDM%20variant%20called%20VDMini.%20Additionally%2C%20we%20propose%20an%0A%5Ctextbf%7BIndividual%20Content%20and%20Motion%20Dynamics%20%28ICMD%29%7D%20Consistency%20Loss%20to%20gain%0Acomparable%20generation%20performance%20as%20larger%20VDM%2C%20i.e.%2C%20the%20teacher%20to%20VDMini%0Ai.e.%2C%20the%20student.%20Particularly%2C%20we%20first%20use%20the%20Individual%20Content%0ADistillation%20%28ICD%29%20Loss%20to%20ensure%20consistency%20in%20the%20features%20of%20each%20generated%0Aframe%20between%20the%20teacher%20and%20student%20models.%20Next%2C%20we%20introduce%20a%20Multi-frame%0AContent%20Adversarial%20%28MCA%29%20Loss%20to%20enhance%20the%20motion%20dynamics%20across%20the%0Agenerated%20video%20as%20a%20whole.%20This%20method%20significantly%20accelerates%20inference%0Atime%20while%20maintaining%20high-quality%20video%20generation.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20VDMini%20on%20two%20important%20video%20generation%0Atasks%2C%20Text-to-Video%20%28T2V%29%20and%20Image-to-Video%20%28I2V%29%2C%20where%20we%20respectively%0Aachieve%20an%20average%202.5%20%24%5Ctimes%24%20and%201.4%20%24%5Ctimes%24%20speed%20up%20for%20the%20I2V%20method%0ASF-V%20and%20the%20T2V%20method%20T2V-Turbo-v2%2C%20while%20maintaining%20the%20quality%20of%20the%0Agenerated%20videos%20on%20two%20benchmarks%2C%20i.e.%2C%20UCF101%20and%20VBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18375v1&entry.124074799=Read"},
{"title": "MvKeTR: Chest CT Report Generation with Multi-View Perception and\n  Knowledge Enhancement", "author": "Xiwei Deng and Xianchun He and Yudan Zhou and Shuhui Cai and Congbo Cai and Zhong Chen", "abstract": "  CT report generation (CTRG) aims to automatically generate diagnostic reports\nfor 3D volumes, relieving clinicians' workload and improving patient care.\nDespite clinical value, existing works fail to effectively incorporate\ndiagnostic information from multiple anatomical views and lack related clinical\nexpertise essential for accurate and reliable diagnosis. To resolve these\nlimitations, we propose a novel Multi-view perception Knowledge-enhanced\nTansformer (MvKeTR) to mimic the diagnostic workflow of clinicians. Just as\nradiologists first examine CT scans from multiple planes, a Multi-View\nPerception Aggregator (MVPA) with view-aware attention effectively synthesizes\ndiagnostic information from multiple anatomical views. Then, inspired by how\nradiologists further refer to relevant clinical records to guide diagnostic\ndecision-making, a Cross-Modal Knowledge Enhancer (CMKE) retrieves the most\nsimilar reports based on the query volume to incorporate domain knowledge into\nthe diagnosis procedure. Furthermore, instead of traditional MLPs, we employ\nKolmogorov-Arnold Networks (KANs) with learnable nonlinear activation functions\nas the fundamental building blocks of both modules to better capture intricate\ndiagnostic patterns in CT interpretation. Extensive experiments on the public\nCTRG-Chest-548K dataset demonstrate that our method outpaces prior\nstate-of-the-art models across all metrics.\n", "link": "http://arxiv.org/abs/2411.18309v1", "date": "2024-11-27", "relevancy": 2.726, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5498}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement&body=Title%3A%20MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement%0AAuthor%3A%20Xiwei%20Deng%20and%20Xianchun%20He%20and%20Yudan%20Zhou%20and%20Shuhui%20Cai%20and%20Congbo%20Cai%20and%20Zhong%20Chen%0AAbstract%3A%20%20%20CT%20report%20generation%20%28CTRG%29%20aims%20to%20automatically%20generate%20diagnostic%20reports%0Afor%203D%20volumes%2C%20relieving%20clinicians%27%20workload%20and%20improving%20patient%20care.%0ADespite%20clinical%20value%2C%20existing%20works%20fail%20to%20effectively%20incorporate%0Adiagnostic%20information%20from%20multiple%20anatomical%20views%20and%20lack%20related%20clinical%0Aexpertise%20essential%20for%20accurate%20and%20reliable%20diagnosis.%20To%20resolve%20these%0Alimitations%2C%20we%20propose%20a%20novel%20Multi-view%20perception%20Knowledge-enhanced%0ATansformer%20%28MvKeTR%29%20to%20mimic%20the%20diagnostic%20workflow%20of%20clinicians.%20Just%20as%0Aradiologists%20first%20examine%20CT%20scans%20from%20multiple%20planes%2C%20a%20Multi-View%0APerception%20Aggregator%20%28MVPA%29%20with%20view-aware%20attention%20effectively%20synthesizes%0Adiagnostic%20information%20from%20multiple%20anatomical%20views.%20Then%2C%20inspired%20by%20how%0Aradiologists%20further%20refer%20to%20relevant%20clinical%20records%20to%20guide%20diagnostic%0Adecision-making%2C%20a%20Cross-Modal%20Knowledge%20Enhancer%20%28CMKE%29%20retrieves%20the%20most%0Asimilar%20reports%20based%20on%20the%20query%20volume%20to%20incorporate%20domain%20knowledge%20into%0Athe%20diagnosis%20procedure.%20Furthermore%2C%20instead%20of%20traditional%20MLPs%2C%20we%20employ%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20with%20learnable%20nonlinear%20activation%20functions%0Aas%20the%20fundamental%20building%20blocks%20of%20both%20modules%20to%20better%20capture%20intricate%0Adiagnostic%20patterns%20in%20CT%20interpretation.%20Extensive%20experiments%20on%20the%20public%0ACTRG-Chest-548K%20dataset%20demonstrate%20that%20our%20method%20outpaces%20prior%0Astate-of-the-art%20models%20across%20all%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMvKeTR%253A%2520Chest%2520CT%2520Report%2520Generation%2520with%2520Multi-View%2520Perception%2520and%250A%2520%2520Knowledge%2520Enhancement%26entry.906535625%3DXiwei%2520Deng%2520and%2520Xianchun%2520He%2520and%2520Yudan%2520Zhou%2520and%2520Shuhui%2520Cai%2520and%2520Congbo%2520Cai%2520and%2520Zhong%2520Chen%26entry.1292438233%3D%2520%2520CT%2520report%2520generation%2520%2528CTRG%2529%2520aims%2520to%2520automatically%2520generate%2520diagnostic%2520reports%250Afor%25203D%2520volumes%252C%2520relieving%2520clinicians%2527%2520workload%2520and%2520improving%2520patient%2520care.%250ADespite%2520clinical%2520value%252C%2520existing%2520works%2520fail%2520to%2520effectively%2520incorporate%250Adiagnostic%2520information%2520from%2520multiple%2520anatomical%2520views%2520and%2520lack%2520related%2520clinical%250Aexpertise%2520essential%2520for%2520accurate%2520and%2520reliable%2520diagnosis.%2520To%2520resolve%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520Multi-view%2520perception%2520Knowledge-enhanced%250ATansformer%2520%2528MvKeTR%2529%2520to%2520mimic%2520the%2520diagnostic%2520workflow%2520of%2520clinicians.%2520Just%2520as%250Aradiologists%2520first%2520examine%2520CT%2520scans%2520from%2520multiple%2520planes%252C%2520a%2520Multi-View%250APerception%2520Aggregator%2520%2528MVPA%2529%2520with%2520view-aware%2520attention%2520effectively%2520synthesizes%250Adiagnostic%2520information%2520from%2520multiple%2520anatomical%2520views.%2520Then%252C%2520inspired%2520by%2520how%250Aradiologists%2520further%2520refer%2520to%2520relevant%2520clinical%2520records%2520to%2520guide%2520diagnostic%250Adecision-making%252C%2520a%2520Cross-Modal%2520Knowledge%2520Enhancer%2520%2528CMKE%2529%2520retrieves%2520the%2520most%250Asimilar%2520reports%2520based%2520on%2520the%2520query%2520volume%2520to%2520incorporate%2520domain%2520knowledge%2520into%250Athe%2520diagnosis%2520procedure.%2520Furthermore%252C%2520instead%2520of%2520traditional%2520MLPs%252C%2520we%2520employ%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520with%2520learnable%2520nonlinear%2520activation%2520functions%250Aas%2520the%2520fundamental%2520building%2520blocks%2520of%2520both%2520modules%2520to%2520better%2520capture%2520intricate%250Adiagnostic%2520patterns%2520in%2520CT%2520interpretation.%2520Extensive%2520experiments%2520on%2520the%2520public%250ACTRG-Chest-548K%2520dataset%2520demonstrate%2520that%2520our%2520method%2520outpaces%2520prior%250Astate-of-the-art%2520models%2520across%2520all%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement&entry.906535625=Xiwei%20Deng%20and%20Xianchun%20He%20and%20Yudan%20Zhou%20and%20Shuhui%20Cai%20and%20Congbo%20Cai%20and%20Zhong%20Chen&entry.1292438233=%20%20CT%20report%20generation%20%28CTRG%29%20aims%20to%20automatically%20generate%20diagnostic%20reports%0Afor%203D%20volumes%2C%20relieving%20clinicians%27%20workload%20and%20improving%20patient%20care.%0ADespite%20clinical%20value%2C%20existing%20works%20fail%20to%20effectively%20incorporate%0Adiagnostic%20information%20from%20multiple%20anatomical%20views%20and%20lack%20related%20clinical%0Aexpertise%20essential%20for%20accurate%20and%20reliable%20diagnosis.%20To%20resolve%20these%0Alimitations%2C%20we%20propose%20a%20novel%20Multi-view%20perception%20Knowledge-enhanced%0ATansformer%20%28MvKeTR%29%20to%20mimic%20the%20diagnostic%20workflow%20of%20clinicians.%20Just%20as%0Aradiologists%20first%20examine%20CT%20scans%20from%20multiple%20planes%2C%20a%20Multi-View%0APerception%20Aggregator%20%28MVPA%29%20with%20view-aware%20attention%20effectively%20synthesizes%0Adiagnostic%20information%20from%20multiple%20anatomical%20views.%20Then%2C%20inspired%20by%20how%0Aradiologists%20further%20refer%20to%20relevant%20clinical%20records%20to%20guide%20diagnostic%0Adecision-making%2C%20a%20Cross-Modal%20Knowledge%20Enhancer%20%28CMKE%29%20retrieves%20the%20most%0Asimilar%20reports%20based%20on%20the%20query%20volume%20to%20incorporate%20domain%20knowledge%20into%0Athe%20diagnosis%20procedure.%20Furthermore%2C%20instead%20of%20traditional%20MLPs%2C%20we%20employ%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20with%20learnable%20nonlinear%20activation%20functions%0Aas%20the%20fundamental%20building%20blocks%20of%20both%20modules%20to%20better%20capture%20intricate%0Adiagnostic%20patterns%20in%20CT%20interpretation.%20Extensive%20experiments%20on%20the%20public%0ACTRG-Chest-548K%20dataset%20demonstrate%20that%20our%20method%20outpaces%20prior%0Astate-of-the-art%20models%20across%20all%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18309v1&entry.124074799=Read"},
{"title": "A Comprehensive Study of Structural Pruning for Vision Models", "author": "Changhao Li and Haoling Li and Mengqi Xue and Gongfan Fang and Sheng Zhou and Zunlei Feng and Huiqiong Wang and Mingli Song and Jie Song", "abstract": "  Structural pruning has emerged as a promising approach for producing more\nefficient models. Nevertheless, the community suffers from a lack of\nstandardized benchmarks and metrics, leaving the progress in this area not\nfully comprehended.To fill this gap, we present the first comprehensive\nbenchmark, termed PruningBench, for structural pruning. PruningBench showcases\nthe following three characteristics: 1) PruningBench employs a unified and\nconsistent framework for evaluating the effectiveness of diverse structural\npruning techniques; 2) PruningBench systematically evaluates 16 existing\npruning methods, encompassing a wide array of models (e.g., CNNs and ViTs) and\ntasks (e.g., classification and detection); 3) PruningBench provides easily\nimplementable interfaces to facilitate the implementation of future pruning\nmethods, and enables the subsequent researchers to incorporate their work into\nour leaderboards. We provide an online pruning platform\nhttp://pruning.vipazoo.cn for customizing pruning tasks and reproducing all\nresults in this paper. Leaderboard results can be available on\nhttps://github.com/HollyLee2000/PruningBench.\n", "link": "http://arxiv.org/abs/2406.12315v4", "date": "2024-11-27", "relevancy": 2.6988, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Study%20of%20Structural%20Pruning%20for%20Vision%20Models&body=Title%3A%20A%20Comprehensive%20Study%20of%20Structural%20Pruning%20for%20Vision%20Models%0AAuthor%3A%20Changhao%20Li%20and%20Haoling%20Li%20and%20Mengqi%20Xue%20and%20Gongfan%20Fang%20and%20Sheng%20Zhou%20and%20Zunlei%20Feng%20and%20Huiqiong%20Wang%20and%20Mingli%20Song%20and%20Jie%20Song%0AAbstract%3A%20%20%20Structural%20pruning%20has%20emerged%20as%20a%20promising%20approach%20for%20producing%20more%0Aefficient%20models.%20Nevertheless%2C%20the%20community%20suffers%20from%20a%20lack%20of%0Astandardized%20benchmarks%20and%20metrics%2C%20leaving%20the%20progress%20in%20this%20area%20not%0Afully%20comprehended.To%20fill%20this%20gap%2C%20we%20present%20the%20first%20comprehensive%0Abenchmark%2C%20termed%20PruningBench%2C%20for%20structural%20pruning.%20PruningBench%20showcases%0Athe%20following%20three%20characteristics%3A%201%29%20PruningBench%20employs%20a%20unified%20and%0Aconsistent%20framework%20for%20evaluating%20the%20effectiveness%20of%20diverse%20structural%0Apruning%20techniques%3B%202%29%20PruningBench%20systematically%20evaluates%2016%20existing%0Apruning%20methods%2C%20encompassing%20a%20wide%20array%20of%20models%20%28e.g.%2C%20CNNs%20and%20ViTs%29%20and%0Atasks%20%28e.g.%2C%20classification%20and%20detection%29%3B%203%29%20PruningBench%20provides%20easily%0Aimplementable%20interfaces%20to%20facilitate%20the%20implementation%20of%20future%20pruning%0Amethods%2C%20and%20enables%20the%20subsequent%20researchers%20to%20incorporate%20their%20work%20into%0Aour%20leaderboards.%20We%20provide%20an%20online%20pruning%20platform%0Ahttp%3A//pruning.vipazoo.cn%20for%20customizing%20pruning%20tasks%20and%20reproducing%20all%0Aresults%20in%20this%20paper.%20Leaderboard%20results%20can%20be%20available%20on%0Ahttps%3A//github.com/HollyLee2000/PruningBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12315v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Study%2520of%2520Structural%2520Pruning%2520for%2520Vision%2520Models%26entry.906535625%3DChanghao%2520Li%2520and%2520Haoling%2520Li%2520and%2520Mengqi%2520Xue%2520and%2520Gongfan%2520Fang%2520and%2520Sheng%2520Zhou%2520and%2520Zunlei%2520Feng%2520and%2520Huiqiong%2520Wang%2520and%2520Mingli%2520Song%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Structural%2520pruning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520producing%2520more%250Aefficient%2520models.%2520Nevertheless%252C%2520the%2520community%2520suffers%2520from%2520a%2520lack%2520of%250Astandardized%2520benchmarks%2520and%2520metrics%252C%2520leaving%2520the%2520progress%2520in%2520this%2520area%2520not%250Afully%2520comprehended.To%2520fill%2520this%2520gap%252C%2520we%2520present%2520the%2520first%2520comprehensive%250Abenchmark%252C%2520termed%2520PruningBench%252C%2520for%2520structural%2520pruning.%2520PruningBench%2520showcases%250Athe%2520following%2520three%2520characteristics%253A%25201%2529%2520PruningBench%2520employs%2520a%2520unified%2520and%250Aconsistent%2520framework%2520for%2520evaluating%2520the%2520effectiveness%2520of%2520diverse%2520structural%250Apruning%2520techniques%253B%25202%2529%2520PruningBench%2520systematically%2520evaluates%252016%2520existing%250Apruning%2520methods%252C%2520encompassing%2520a%2520wide%2520array%2520of%2520models%2520%2528e.g.%252C%2520CNNs%2520and%2520ViTs%2529%2520and%250Atasks%2520%2528e.g.%252C%2520classification%2520and%2520detection%2529%253B%25203%2529%2520PruningBench%2520provides%2520easily%250Aimplementable%2520interfaces%2520to%2520facilitate%2520the%2520implementation%2520of%2520future%2520pruning%250Amethods%252C%2520and%2520enables%2520the%2520subsequent%2520researchers%2520to%2520incorporate%2520their%2520work%2520into%250Aour%2520leaderboards.%2520We%2520provide%2520an%2520online%2520pruning%2520platform%250Ahttp%253A//pruning.vipazoo.cn%2520for%2520customizing%2520pruning%2520tasks%2520and%2520reproducing%2520all%250Aresults%2520in%2520this%2520paper.%2520Leaderboard%2520results%2520can%2520be%2520available%2520on%250Ahttps%253A//github.com/HollyLee2000/PruningBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12315v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Study%20of%20Structural%20Pruning%20for%20Vision%20Models&entry.906535625=Changhao%20Li%20and%20Haoling%20Li%20and%20Mengqi%20Xue%20and%20Gongfan%20Fang%20and%20Sheng%20Zhou%20and%20Zunlei%20Feng%20and%20Huiqiong%20Wang%20and%20Mingli%20Song%20and%20Jie%20Song&entry.1292438233=%20%20Structural%20pruning%20has%20emerged%20as%20a%20promising%20approach%20for%20producing%20more%0Aefficient%20models.%20Nevertheless%2C%20the%20community%20suffers%20from%20a%20lack%20of%0Astandardized%20benchmarks%20and%20metrics%2C%20leaving%20the%20progress%20in%20this%20area%20not%0Afully%20comprehended.To%20fill%20this%20gap%2C%20we%20present%20the%20first%20comprehensive%0Abenchmark%2C%20termed%20PruningBench%2C%20for%20structural%20pruning.%20PruningBench%20showcases%0Athe%20following%20three%20characteristics%3A%201%29%20PruningBench%20employs%20a%20unified%20and%0Aconsistent%20framework%20for%20evaluating%20the%20effectiveness%20of%20diverse%20structural%0Apruning%20techniques%3B%202%29%20PruningBench%20systematically%20evaluates%2016%20existing%0Apruning%20methods%2C%20encompassing%20a%20wide%20array%20of%20models%20%28e.g.%2C%20CNNs%20and%20ViTs%29%20and%0Atasks%20%28e.g.%2C%20classification%20and%20detection%29%3B%203%29%20PruningBench%20provides%20easily%0Aimplementable%20interfaces%20to%20facilitate%20the%20implementation%20of%20future%20pruning%0Amethods%2C%20and%20enables%20the%20subsequent%20researchers%20to%20incorporate%20their%20work%20into%0Aour%20leaderboards.%20We%20provide%20an%20online%20pruning%20platform%0Ahttp%3A//pruning.vipazoo.cn%20for%20customizing%20pruning%20tasks%20and%20reproducing%20all%0Aresults%20in%20this%20paper.%20Leaderboard%20results%20can%20be%20available%20on%0Ahttps%3A//github.com/HollyLee2000/PruningBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12315v4&entry.124074799=Read"},
{"title": "MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview\n  and Text", "author": "Takayuki Hara and Tatsuya Harada", "abstract": "  The generation of 3D scenes from user-specified conditions offers a promising\navenue for alleviating the production burden in 3D applications. Previous\nstudies required significant effort to realize the desired scene, owing to\nlimited control conditions. We propose a method for controlling and generating\n3D scenes under multimodal conditions using partial images, layout information\nrepresented in the top view, and text prompts. Combining these conditions to\ngenerate a 3D scene involves the following significant difficulties: (1) the\ncreation of large datasets, (2) reflection on the interaction of multimodal\nconditions, and (3) domain dependence of the layout conditions. We decompose\nthe process of 3D scene generation into 2D image generation from the given\nconditions and 3D scene generation from 2D images. 2D image generation is\nachieved by fine-tuning a pretrained text-to-image model with a small\nartificial dataset of partial images and layouts, and 3D scene generation is\nachieved by layout-conditioned depth estimation and neural radiance fields\n(NeRF), thereby avoiding the creation of large datasets. The use of a common\nrepresentation of spatial information using 360-degree images allows for the\nconsideration of multimodal condition interactions and reduces the domain\ndependence of the layout control. The experimental results qualitatively and\nquantitatively demonstrated that the proposed method can generate 3D scenes in\ndiverse domains, from indoor to outdoor, according to multimodal conditions.\n", "link": "http://arxiv.org/abs/2404.00345v2", "date": "2024-11-27", "relevancy": 2.6811, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.676}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.676}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaGRITTe%3A%20Manipulative%20and%20Generative%203D%20Realization%20from%20Image%2C%20Topview%0A%20%20and%20Text&body=Title%3A%20MaGRITTe%3A%20Manipulative%20and%20Generative%203D%20Realization%20from%20Image%2C%20Topview%0A%20%20and%20Text%0AAuthor%3A%20Takayuki%20Hara%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20The%20generation%20of%203D%20scenes%20from%20user-specified%20conditions%20offers%20a%20promising%0Aavenue%20for%20alleviating%20the%20production%20burden%20in%203D%20applications.%20Previous%0Astudies%20required%20significant%20effort%20to%20realize%20the%20desired%20scene%2C%20owing%20to%0Alimited%20control%20conditions.%20We%20propose%20a%20method%20for%20controlling%20and%20generating%0A3D%20scenes%20under%20multimodal%20conditions%20using%20partial%20images%2C%20layout%20information%0Arepresented%20in%20the%20top%20view%2C%20and%20text%20prompts.%20Combining%20these%20conditions%20to%0Agenerate%20a%203D%20scene%20involves%20the%20following%20significant%20difficulties%3A%20%281%29%20the%0Acreation%20of%20large%20datasets%2C%20%282%29%20reflection%20on%20the%20interaction%20of%20multimodal%0Aconditions%2C%20and%20%283%29%20domain%20dependence%20of%20the%20layout%20conditions.%20We%20decompose%0Athe%20process%20of%203D%20scene%20generation%20into%202D%20image%20generation%20from%20the%20given%0Aconditions%20and%203D%20scene%20generation%20from%202D%20images.%202D%20image%20generation%20is%0Aachieved%20by%20fine-tuning%20a%20pretrained%20text-to-image%20model%20with%20a%20small%0Aartificial%20dataset%20of%20partial%20images%20and%20layouts%2C%20and%203D%20scene%20generation%20is%0Aachieved%20by%20layout-conditioned%20depth%20estimation%20and%20neural%20radiance%20fields%0A%28NeRF%29%2C%20thereby%20avoiding%20the%20creation%20of%20large%20datasets.%20The%20use%20of%20a%20common%0Arepresentation%20of%20spatial%20information%20using%20360-degree%20images%20allows%20for%20the%0Aconsideration%20of%20multimodal%20condition%20interactions%20and%20reduces%20the%20domain%0Adependence%20of%20the%20layout%20control.%20The%20experimental%20results%20qualitatively%20and%0Aquantitatively%20demonstrated%20that%20the%20proposed%20method%20can%20generate%203D%20scenes%20in%0Adiverse%20domains%2C%20from%20indoor%20to%20outdoor%2C%20according%20to%20multimodal%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaGRITTe%253A%2520Manipulative%2520and%2520Generative%25203D%2520Realization%2520from%2520Image%252C%2520Topview%250A%2520%2520and%2520Text%26entry.906535625%3DTakayuki%2520Hara%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520The%2520generation%2520of%25203D%2520scenes%2520from%2520user-specified%2520conditions%2520offers%2520a%2520promising%250Aavenue%2520for%2520alleviating%2520the%2520production%2520burden%2520in%25203D%2520applications.%2520Previous%250Astudies%2520required%2520significant%2520effort%2520to%2520realize%2520the%2520desired%2520scene%252C%2520owing%2520to%250Alimited%2520control%2520conditions.%2520We%2520propose%2520a%2520method%2520for%2520controlling%2520and%2520generating%250A3D%2520scenes%2520under%2520multimodal%2520conditions%2520using%2520partial%2520images%252C%2520layout%2520information%250Arepresented%2520in%2520the%2520top%2520view%252C%2520and%2520text%2520prompts.%2520Combining%2520these%2520conditions%2520to%250Agenerate%2520a%25203D%2520scene%2520involves%2520the%2520following%2520significant%2520difficulties%253A%2520%25281%2529%2520the%250Acreation%2520of%2520large%2520datasets%252C%2520%25282%2529%2520reflection%2520on%2520the%2520interaction%2520of%2520multimodal%250Aconditions%252C%2520and%2520%25283%2529%2520domain%2520dependence%2520of%2520the%2520layout%2520conditions.%2520We%2520decompose%250Athe%2520process%2520of%25203D%2520scene%2520generation%2520into%25202D%2520image%2520generation%2520from%2520the%2520given%250Aconditions%2520and%25203D%2520scene%2520generation%2520from%25202D%2520images.%25202D%2520image%2520generation%2520is%250Aachieved%2520by%2520fine-tuning%2520a%2520pretrained%2520text-to-image%2520model%2520with%2520a%2520small%250Aartificial%2520dataset%2520of%2520partial%2520images%2520and%2520layouts%252C%2520and%25203D%2520scene%2520generation%2520is%250Aachieved%2520by%2520layout-conditioned%2520depth%2520estimation%2520and%2520neural%2520radiance%2520fields%250A%2528NeRF%2529%252C%2520thereby%2520avoiding%2520the%2520creation%2520of%2520large%2520datasets.%2520The%2520use%2520of%2520a%2520common%250Arepresentation%2520of%2520spatial%2520information%2520using%2520360-degree%2520images%2520allows%2520for%2520the%250Aconsideration%2520of%2520multimodal%2520condition%2520interactions%2520and%2520reduces%2520the%2520domain%250Adependence%2520of%2520the%2520layout%2520control.%2520The%2520experimental%2520results%2520qualitatively%2520and%250Aquantitatively%2520demonstrated%2520that%2520the%2520proposed%2520method%2520can%2520generate%25203D%2520scenes%2520in%250Adiverse%2520domains%252C%2520from%2520indoor%2520to%2520outdoor%252C%2520according%2520to%2520multimodal%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaGRITTe%3A%20Manipulative%20and%20Generative%203D%20Realization%20from%20Image%2C%20Topview%0A%20%20and%20Text&entry.906535625=Takayuki%20Hara%20and%20Tatsuya%20Harada&entry.1292438233=%20%20The%20generation%20of%203D%20scenes%20from%20user-specified%20conditions%20offers%20a%20promising%0Aavenue%20for%20alleviating%20the%20production%20burden%20in%203D%20applications.%20Previous%0Astudies%20required%20significant%20effort%20to%20realize%20the%20desired%20scene%2C%20owing%20to%0Alimited%20control%20conditions.%20We%20propose%20a%20method%20for%20controlling%20and%20generating%0A3D%20scenes%20under%20multimodal%20conditions%20using%20partial%20images%2C%20layout%20information%0Arepresented%20in%20the%20top%20view%2C%20and%20text%20prompts.%20Combining%20these%20conditions%20to%0Agenerate%20a%203D%20scene%20involves%20the%20following%20significant%20difficulties%3A%20%281%29%20the%0Acreation%20of%20large%20datasets%2C%20%282%29%20reflection%20on%20the%20interaction%20of%20multimodal%0Aconditions%2C%20and%20%283%29%20domain%20dependence%20of%20the%20layout%20conditions.%20We%20decompose%0Athe%20process%20of%203D%20scene%20generation%20into%202D%20image%20generation%20from%20the%20given%0Aconditions%20and%203D%20scene%20generation%20from%202D%20images.%202D%20image%20generation%20is%0Aachieved%20by%20fine-tuning%20a%20pretrained%20text-to-image%20model%20with%20a%20small%0Aartificial%20dataset%20of%20partial%20images%20and%20layouts%2C%20and%203D%20scene%20generation%20is%0Aachieved%20by%20layout-conditioned%20depth%20estimation%20and%20neural%20radiance%20fields%0A%28NeRF%29%2C%20thereby%20avoiding%20the%20creation%20of%20large%20datasets.%20The%20use%20of%20a%20common%0Arepresentation%20of%20spatial%20information%20using%20360-degree%20images%20allows%20for%20the%0Aconsideration%20of%20multimodal%20condition%20interactions%20and%20reduces%20the%20domain%0Adependence%20of%20the%20layout%20control.%20The%20experimental%20results%20qualitatively%20and%0Aquantitatively%20demonstrated%20that%20the%20proposed%20method%20can%20generate%203D%20scenes%20in%0Adiverse%20domains%2C%20from%20indoor%20to%20outdoor%2C%20according%20to%20multimodal%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00345v2&entry.124074799=Read"},
{"title": "Visual Adversarial Attack on Vision-Language Models for Autonomous\n  Driving", "author": "Tianyuan Zhang and Lu Wang and Xinwei Zhang and Yitong Zhang and Boyi Jia and Siyuan Liang and Shengshan Hu and Qiang Fu and Aishan Liu and Xianglong Liu", "abstract": "  Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities. However, these models remain highly\nvulnerable to adversarial attacks. While existing research has primarily\nfocused on general VLM attacks, the development of attacks tailored to the\nsafety-critical AD context has been largely overlooked. In this paper, we take\nthe first step toward designing adversarial attacks specifically targeting VLMs\nin AD, exposing the substantial risks these attacks pose within this critical\ndomain. We identify two unique challenges for effective adversarial attacks on\nAD VLMs: the variability of textual instructions and the time-series nature of\nvisual scenarios. To this end, we propose ADvLM, the first visual adversarial\nattack framework specifically designed for VLMs in AD. Our framework introduces\nSemantic-Invariant Induction, which uses a large language model to create a\ndiverse prompt library of textual instructions with consistent semantic\ncontent, guided by semantic entropy. Building on this, we introduce\nScenario-Associated Enhancement, an approach where attention mechanisms select\nkey frames and perspectives within driving scenarios to optimize adversarial\nperturbations that generalize across the entire scenario. Extensive experiments\non several AD VLMs over multiple benchmarks show that ADvLM achieves\nstate-of-the-art attack effectiveness. Moreover, real-world attack studies\nfurther validate its applicability and potential in practice.\n", "link": "http://arxiv.org/abs/2411.18275v1", "date": "2024-11-27", "relevancy": 2.6799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Adversarial%20Attack%20on%20Vision-Language%20Models%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20Visual%20Adversarial%20Attack%20on%20Vision-Language%20Models%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Tianyuan%20Zhang%20and%20Lu%20Wang%20and%20Xinwei%20Zhang%20and%20Yitong%20Zhang%20and%20Boyi%20Jia%20and%20Siyuan%20Liang%20and%20Shengshan%20Hu%20and%20Qiang%20Fu%20and%20Aishan%20Liu%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20significantly%20advanced%20autonomous%20driving%0A%28AD%29%20by%20enhancing%20reasoning%20capabilities.%20However%2C%20these%20models%20remain%20highly%0Avulnerable%20to%20adversarial%20attacks.%20While%20existing%20research%20has%20primarily%0Afocused%20on%20general%20VLM%20attacks%2C%20the%20development%20of%20attacks%20tailored%20to%20the%0Asafety-critical%20AD%20context%20has%20been%20largely%20overlooked.%20In%20this%20paper%2C%20we%20take%0Athe%20first%20step%20toward%20designing%20adversarial%20attacks%20specifically%20targeting%20VLMs%0Ain%20AD%2C%20exposing%20the%20substantial%20risks%20these%20attacks%20pose%20within%20this%20critical%0Adomain.%20We%20identify%20two%20unique%20challenges%20for%20effective%20adversarial%20attacks%20on%0AAD%20VLMs%3A%20the%20variability%20of%20textual%20instructions%20and%20the%20time-series%20nature%20of%0Avisual%20scenarios.%20To%20this%20end%2C%20we%20propose%20ADvLM%2C%20the%20first%20visual%20adversarial%0Aattack%20framework%20specifically%20designed%20for%20VLMs%20in%20AD.%20Our%20framework%20introduces%0ASemantic-Invariant%20Induction%2C%20which%20uses%20a%20large%20language%20model%20to%20create%20a%0Adiverse%20prompt%20library%20of%20textual%20instructions%20with%20consistent%20semantic%0Acontent%2C%20guided%20by%20semantic%20entropy.%20Building%20on%20this%2C%20we%20introduce%0AScenario-Associated%20Enhancement%2C%20an%20approach%20where%20attention%20mechanisms%20select%0Akey%20frames%20and%20perspectives%20within%20driving%20scenarios%20to%20optimize%20adversarial%0Aperturbations%20that%20generalize%20across%20the%20entire%20scenario.%20Extensive%20experiments%0Aon%20several%20AD%20VLMs%20over%20multiple%20benchmarks%20show%20that%20ADvLM%20achieves%0Astate-of-the-art%20attack%20effectiveness.%20Moreover%2C%20real-world%20attack%20studies%0Afurther%20validate%20its%20applicability%20and%20potential%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Adversarial%2520Attack%2520on%2520Vision-Language%2520Models%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DTianyuan%2520Zhang%2520and%2520Lu%2520Wang%2520and%2520Xinwei%2520Zhang%2520and%2520Yitong%2520Zhang%2520and%2520Boyi%2520Jia%2520and%2520Siyuan%2520Liang%2520and%2520Shengshan%2520Hu%2520and%2520Qiang%2520Fu%2520and%2520Aishan%2520Liu%2520and%2520Xianglong%2520Liu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520significantly%2520advanced%2520autonomous%2520driving%250A%2528AD%2529%2520by%2520enhancing%2520reasoning%2520capabilities.%2520However%252C%2520these%2520models%2520remain%2520highly%250Avulnerable%2520to%2520adversarial%2520attacks.%2520While%2520existing%2520research%2520has%2520primarily%250Afocused%2520on%2520general%2520VLM%2520attacks%252C%2520the%2520development%2520of%2520attacks%2520tailored%2520to%2520the%250Asafety-critical%2520AD%2520context%2520has%2520been%2520largely%2520overlooked.%2520In%2520this%2520paper%252C%2520we%2520take%250Athe%2520first%2520step%2520toward%2520designing%2520adversarial%2520attacks%2520specifically%2520targeting%2520VLMs%250Ain%2520AD%252C%2520exposing%2520the%2520substantial%2520risks%2520these%2520attacks%2520pose%2520within%2520this%2520critical%250Adomain.%2520We%2520identify%2520two%2520unique%2520challenges%2520for%2520effective%2520adversarial%2520attacks%2520on%250AAD%2520VLMs%253A%2520the%2520variability%2520of%2520textual%2520instructions%2520and%2520the%2520time-series%2520nature%2520of%250Avisual%2520scenarios.%2520To%2520this%2520end%252C%2520we%2520propose%2520ADvLM%252C%2520the%2520first%2520visual%2520adversarial%250Aattack%2520framework%2520specifically%2520designed%2520for%2520VLMs%2520in%2520AD.%2520Our%2520framework%2520introduces%250ASemantic-Invariant%2520Induction%252C%2520which%2520uses%2520a%2520large%2520language%2520model%2520to%2520create%2520a%250Adiverse%2520prompt%2520library%2520of%2520textual%2520instructions%2520with%2520consistent%2520semantic%250Acontent%252C%2520guided%2520by%2520semantic%2520entropy.%2520Building%2520on%2520this%252C%2520we%2520introduce%250AScenario-Associated%2520Enhancement%252C%2520an%2520approach%2520where%2520attention%2520mechanisms%2520select%250Akey%2520frames%2520and%2520perspectives%2520within%2520driving%2520scenarios%2520to%2520optimize%2520adversarial%250Aperturbations%2520that%2520generalize%2520across%2520the%2520entire%2520scenario.%2520Extensive%2520experiments%250Aon%2520several%2520AD%2520VLMs%2520over%2520multiple%2520benchmarks%2520show%2520that%2520ADvLM%2520achieves%250Astate-of-the-art%2520attack%2520effectiveness.%2520Moreover%252C%2520real-world%2520attack%2520studies%250Afurther%2520validate%2520its%2520applicability%2520and%2520potential%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Adversarial%20Attack%20on%20Vision-Language%20Models%20for%20Autonomous%0A%20%20Driving&entry.906535625=Tianyuan%20Zhang%20and%20Lu%20Wang%20and%20Xinwei%20Zhang%20and%20Yitong%20Zhang%20and%20Boyi%20Jia%20and%20Siyuan%20Liang%20and%20Shengshan%20Hu%20and%20Qiang%20Fu%20and%20Aishan%20Liu%20and%20Xianglong%20Liu&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20significantly%20advanced%20autonomous%20driving%0A%28AD%29%20by%20enhancing%20reasoning%20capabilities.%20However%2C%20these%20models%20remain%20highly%0Avulnerable%20to%20adversarial%20attacks.%20While%20existing%20research%20has%20primarily%0Afocused%20on%20general%20VLM%20attacks%2C%20the%20development%20of%20attacks%20tailored%20to%20the%0Asafety-critical%20AD%20context%20has%20been%20largely%20overlooked.%20In%20this%20paper%2C%20we%20take%0Athe%20first%20step%20toward%20designing%20adversarial%20attacks%20specifically%20targeting%20VLMs%0Ain%20AD%2C%20exposing%20the%20substantial%20risks%20these%20attacks%20pose%20within%20this%20critical%0Adomain.%20We%20identify%20two%20unique%20challenges%20for%20effective%20adversarial%20attacks%20on%0AAD%20VLMs%3A%20the%20variability%20of%20textual%20instructions%20and%20the%20time-series%20nature%20of%0Avisual%20scenarios.%20To%20this%20end%2C%20we%20propose%20ADvLM%2C%20the%20first%20visual%20adversarial%0Aattack%20framework%20specifically%20designed%20for%20VLMs%20in%20AD.%20Our%20framework%20introduces%0ASemantic-Invariant%20Induction%2C%20which%20uses%20a%20large%20language%20model%20to%20create%20a%0Adiverse%20prompt%20library%20of%20textual%20instructions%20with%20consistent%20semantic%0Acontent%2C%20guided%20by%20semantic%20entropy.%20Building%20on%20this%2C%20we%20introduce%0AScenario-Associated%20Enhancement%2C%20an%20approach%20where%20attention%20mechanisms%20select%0Akey%20frames%20and%20perspectives%20within%20driving%20scenarios%20to%20optimize%20adversarial%0Aperturbations%20that%20generalize%20across%20the%20entire%20scenario.%20Extensive%20experiments%0Aon%20several%20AD%20VLMs%20over%20multiple%20benchmarks%20show%20that%20ADvLM%20achieves%0Astate-of-the-art%20attack%20effectiveness.%20Moreover%2C%20real-world%20attack%20studies%0Afurther%20validate%20its%20applicability%20and%20potential%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18275v1&entry.124074799=Read"},
{"title": "Promptable Anomaly Segmentation with SAM Through Self-Perception Tuning", "author": "Hui-Yue Yang and Hui Chen and Ao Wang and Kai Chen and Zijia Lin and Yongliang Tang and Pengcheng Gao and Yuming Quan and Jungong Han and Guiguang Ding", "abstract": "  Segment Anything Model (SAM) has made great progress in anomaly segmentation\ntasks due to its impressive generalization ability. However, existing methods\nthat directly apply SAM through prompting often overlook the domain shift\nissue, where SAM performs well on natural images but struggles in industrial\nscenarios. Parameter-Efficient Fine-Tuning (PEFT) offers a promising solution,\nbut it may yield suboptimal performance by not adequately addressing the\nperception challenges during adaptation to anomaly images. In this paper, we\npropose a novel Self-Perceptinon Tuning (SPT) method, aiming to enhance SAM's\nperception capability for anomaly segmentation. The SPT method incorporates a\nself-drafting tuning strategy, which generates an initial coarse draft of the\nanomaly mask, followed by a refinement process. Additionally, a\nvisual-relation-aware adapter is introduced to improve the perception of\ndiscriminative relational information for mask generation. Extensive\nexperimental results on several benchmark datasets demonstrate that our SPT\nmethod can significantly outperform baseline methods, validating its\neffectiveness. Models and codes will be available online.\n", "link": "http://arxiv.org/abs/2411.17217v2", "date": "2024-11-27", "relevancy": 2.6463, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5338}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5272}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Promptable%20Anomaly%20Segmentation%20with%20SAM%20Through%20Self-Perception%20Tuning&body=Title%3A%20Promptable%20Anomaly%20Segmentation%20with%20SAM%20Through%20Self-Perception%20Tuning%0AAuthor%3A%20Hui-Yue%20Yang%20and%20Hui%20Chen%20and%20Ao%20Wang%20and%20Kai%20Chen%20and%20Zijia%20Lin%20and%20Yongliang%20Tang%20and%20Pengcheng%20Gao%20and%20Yuming%20Quan%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20made%20great%20progress%20in%20anomaly%20segmentation%0Atasks%20due%20to%20its%20impressive%20generalization%20ability.%20However%2C%20existing%20methods%0Athat%20directly%20apply%20SAM%20through%20prompting%20often%20overlook%20the%20domain%20shift%0Aissue%2C%20where%20SAM%20performs%20well%20on%20natural%20images%20but%20struggles%20in%20industrial%0Ascenarios.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20offers%20a%20promising%20solution%2C%0Abut%20it%20may%20yield%20suboptimal%20performance%20by%20not%20adequately%20addressing%20the%0Aperception%20challenges%20during%20adaptation%20to%20anomaly%20images.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Self-Perceptinon%20Tuning%20%28SPT%29%20method%2C%20aiming%20to%20enhance%20SAM%27s%0Aperception%20capability%20for%20anomaly%20segmentation.%20The%20SPT%20method%20incorporates%20a%0Aself-drafting%20tuning%20strategy%2C%20which%20generates%20an%20initial%20coarse%20draft%20of%20the%0Aanomaly%20mask%2C%20followed%20by%20a%20refinement%20process.%20Additionally%2C%20a%0Avisual-relation-aware%20adapter%20is%20introduced%20to%20improve%20the%20perception%20of%0Adiscriminative%20relational%20information%20for%20mask%20generation.%20Extensive%0Aexperimental%20results%20on%20several%20benchmark%20datasets%20demonstrate%20that%20our%20SPT%0Amethod%20can%20significantly%20outperform%20baseline%20methods%2C%20validating%20its%0Aeffectiveness.%20Models%20and%20codes%20will%20be%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptable%2520Anomaly%2520Segmentation%2520with%2520SAM%2520Through%2520Self-Perception%2520Tuning%26entry.906535625%3DHui-Yue%2520Yang%2520and%2520Hui%2520Chen%2520and%2520Ao%2520Wang%2520and%2520Kai%2520Chen%2520and%2520Zijia%2520Lin%2520and%2520Yongliang%2520Tang%2520and%2520Pengcheng%2520Gao%2520and%2520Yuming%2520Quan%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520made%2520great%2520progress%2520in%2520anomaly%2520segmentation%250Atasks%2520due%2520to%2520its%2520impressive%2520generalization%2520ability.%2520However%252C%2520existing%2520methods%250Athat%2520directly%2520apply%2520SAM%2520through%2520prompting%2520often%2520overlook%2520the%2520domain%2520shift%250Aissue%252C%2520where%2520SAM%2520performs%2520well%2520on%2520natural%2520images%2520but%2520struggles%2520in%2520industrial%250Ascenarios.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520offers%2520a%2520promising%2520solution%252C%250Abut%2520it%2520may%2520yield%2520suboptimal%2520performance%2520by%2520not%2520adequately%2520addressing%2520the%250Aperception%2520challenges%2520during%2520adaptation%2520to%2520anomaly%2520images.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520Self-Perceptinon%2520Tuning%2520%2528SPT%2529%2520method%252C%2520aiming%2520to%2520enhance%2520SAM%2527s%250Aperception%2520capability%2520for%2520anomaly%2520segmentation.%2520The%2520SPT%2520method%2520incorporates%2520a%250Aself-drafting%2520tuning%2520strategy%252C%2520which%2520generates%2520an%2520initial%2520coarse%2520draft%2520of%2520the%250Aanomaly%2520mask%252C%2520followed%2520by%2520a%2520refinement%2520process.%2520Additionally%252C%2520a%250Avisual-relation-aware%2520adapter%2520is%2520introduced%2520to%2520improve%2520the%2520perception%2520of%250Adiscriminative%2520relational%2520information%2520for%2520mask%2520generation.%2520Extensive%250Aexperimental%2520results%2520on%2520several%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520SPT%250Amethod%2520can%2520significantly%2520outperform%2520baseline%2520methods%252C%2520validating%2520its%250Aeffectiveness.%2520Models%2520and%2520codes%2520will%2520be%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Promptable%20Anomaly%20Segmentation%20with%20SAM%20Through%20Self-Perception%20Tuning&entry.906535625=Hui-Yue%20Yang%20and%20Hui%20Chen%20and%20Ao%20Wang%20and%20Kai%20Chen%20and%20Zijia%20Lin%20and%20Yongliang%20Tang%20and%20Pengcheng%20Gao%20and%20Yuming%20Quan%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20made%20great%20progress%20in%20anomaly%20segmentation%0Atasks%20due%20to%20its%20impressive%20generalization%20ability.%20However%2C%20existing%20methods%0Athat%20directly%20apply%20SAM%20through%20prompting%20often%20overlook%20the%20domain%20shift%0Aissue%2C%20where%20SAM%20performs%20well%20on%20natural%20images%20but%20struggles%20in%20industrial%0Ascenarios.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20offers%20a%20promising%20solution%2C%0Abut%20it%20may%20yield%20suboptimal%20performance%20by%20not%20adequately%20addressing%20the%0Aperception%20challenges%20during%20adaptation%20to%20anomaly%20images.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Self-Perceptinon%20Tuning%20%28SPT%29%20method%2C%20aiming%20to%20enhance%20SAM%27s%0Aperception%20capability%20for%20anomaly%20segmentation.%20The%20SPT%20method%20incorporates%20a%0Aself-drafting%20tuning%20strategy%2C%20which%20generates%20an%20initial%20coarse%20draft%20of%20the%0Aanomaly%20mask%2C%20followed%20by%20a%20refinement%20process.%20Additionally%2C%20a%0Avisual-relation-aware%20adapter%20is%20introduced%20to%20improve%20the%20perception%20of%0Adiscriminative%20relational%20information%20for%20mask%20generation.%20Extensive%0Aexperimental%20results%20on%20several%20benchmark%20datasets%20demonstrate%20that%20our%20SPT%0Amethod%20can%20significantly%20outperform%20baseline%20methods%2C%20validating%20its%0Aeffectiveness.%20Models%20and%20codes%20will%20be%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17217v2&entry.124074799=Read"},
{"title": "Surveying the space of descriptions of a composite system with machine\n  learning", "author": "Kieran A. Murphy and Yujing Zhang and Dani S. Bassett", "abstract": "  Multivariate information theory provides a general and principled framework\nfor understanding how the components of a complex system are connected.\nExisting analyses are coarse in nature -- built up from characterizations of\ndiscrete subsystems -- and can be computationally prohibitive. In this work, we\npropose to study the continuous space of possible descriptions of a composite\nsystem as a window into its organizational structure. A description consists of\nspecific information conveyed about each of the components, and the space of\npossible descriptions is equivalent to the space of lossy compression schemes\nof the components. We introduce a machine learning framework to optimize\ndescriptions that extremize key information theoretic quantities used to\ncharacterize organization, such as total correlation and O-information. Through\ncase studies on spin systems, Sudoku boards, and letter sequences from natural\nlanguage, we identify extremal descriptions that reveal how system-wide\nvariation emerges from individual components. By integrating machine learning\ninto a fine-grained information theoretic analysis of composite random\nvariables, our framework opens a new avenues for probing the structure of\nreal-world complex systems.\n", "link": "http://arxiv.org/abs/2411.18579v1", "date": "2024-11-27", "relevancy": 2.6447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning&body=Title%3A%20Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning%0AAuthor%3A%20Kieran%20A.%20Murphy%20and%20Yujing%20Zhang%20and%20Dani%20S.%20Bassett%0AAbstract%3A%20%20%20Multivariate%20information%20theory%20provides%20a%20general%20and%20principled%20framework%0Afor%20understanding%20how%20the%20components%20of%20a%20complex%20system%20are%20connected.%0AExisting%20analyses%20are%20coarse%20in%20nature%20--%20built%20up%20from%20characterizations%20of%0Adiscrete%20subsystems%20--%20and%20can%20be%20computationally%20prohibitive.%20In%20this%20work%2C%20we%0Apropose%20to%20study%20the%20continuous%20space%20of%20possible%20descriptions%20of%20a%20composite%0Asystem%20as%20a%20window%20into%20its%20organizational%20structure.%20A%20description%20consists%20of%0Aspecific%20information%20conveyed%20about%20each%20of%20the%20components%2C%20and%20the%20space%20of%0Apossible%20descriptions%20is%20equivalent%20to%20the%20space%20of%20lossy%20compression%20schemes%0Aof%20the%20components.%20We%20introduce%20a%20machine%20learning%20framework%20to%20optimize%0Adescriptions%20that%20extremize%20key%20information%20theoretic%20quantities%20used%20to%0Acharacterize%20organization%2C%20such%20as%20total%20correlation%20and%20O-information.%20Through%0Acase%20studies%20on%20spin%20systems%2C%20Sudoku%20boards%2C%20and%20letter%20sequences%20from%20natural%0Alanguage%2C%20we%20identify%20extremal%20descriptions%20that%20reveal%20how%20system-wide%0Avariation%20emerges%20from%20individual%20components.%20By%20integrating%20machine%20learning%0Ainto%20a%20fine-grained%20information%20theoretic%20analysis%20of%20composite%20random%0Avariables%2C%20our%20framework%20opens%20a%20new%20avenues%20for%20probing%20the%20structure%20of%0Areal-world%20complex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurveying%2520the%2520space%2520of%2520descriptions%2520of%2520a%2520composite%2520system%2520with%2520machine%250A%2520%2520learning%26entry.906535625%3DKieran%2520A.%2520Murphy%2520and%2520Yujing%2520Zhang%2520and%2520Dani%2520S.%2520Bassett%26entry.1292438233%3D%2520%2520Multivariate%2520information%2520theory%2520provides%2520a%2520general%2520and%2520principled%2520framework%250Afor%2520understanding%2520how%2520the%2520components%2520of%2520a%2520complex%2520system%2520are%2520connected.%250AExisting%2520analyses%2520are%2520coarse%2520in%2520nature%2520--%2520built%2520up%2520from%2520characterizations%2520of%250Adiscrete%2520subsystems%2520--%2520and%2520can%2520be%2520computationally%2520prohibitive.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520study%2520the%2520continuous%2520space%2520of%2520possible%2520descriptions%2520of%2520a%2520composite%250Asystem%2520as%2520a%2520window%2520into%2520its%2520organizational%2520structure.%2520A%2520description%2520consists%2520of%250Aspecific%2520information%2520conveyed%2520about%2520each%2520of%2520the%2520components%252C%2520and%2520the%2520space%2520of%250Apossible%2520descriptions%2520is%2520equivalent%2520to%2520the%2520space%2520of%2520lossy%2520compression%2520schemes%250Aof%2520the%2520components.%2520We%2520introduce%2520a%2520machine%2520learning%2520framework%2520to%2520optimize%250Adescriptions%2520that%2520extremize%2520key%2520information%2520theoretic%2520quantities%2520used%2520to%250Acharacterize%2520organization%252C%2520such%2520as%2520total%2520correlation%2520and%2520O-information.%2520Through%250Acase%2520studies%2520on%2520spin%2520systems%252C%2520Sudoku%2520boards%252C%2520and%2520letter%2520sequences%2520from%2520natural%250Alanguage%252C%2520we%2520identify%2520extremal%2520descriptions%2520that%2520reveal%2520how%2520system-wide%250Avariation%2520emerges%2520from%2520individual%2520components.%2520By%2520integrating%2520machine%2520learning%250Ainto%2520a%2520fine-grained%2520information%2520theoretic%2520analysis%2520of%2520composite%2520random%250Avariables%252C%2520our%2520framework%2520opens%2520a%2520new%2520avenues%2520for%2520probing%2520the%2520structure%2520of%250Areal-world%2520complex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surveying%20the%20space%20of%20descriptions%20of%20a%20composite%20system%20with%20machine%0A%20%20learning&entry.906535625=Kieran%20A.%20Murphy%20and%20Yujing%20Zhang%20and%20Dani%20S.%20Bassett&entry.1292438233=%20%20Multivariate%20information%20theory%20provides%20a%20general%20and%20principled%20framework%0Afor%20understanding%20how%20the%20components%20of%20a%20complex%20system%20are%20connected.%0AExisting%20analyses%20are%20coarse%20in%20nature%20--%20built%20up%20from%20characterizations%20of%0Adiscrete%20subsystems%20--%20and%20can%20be%20computationally%20prohibitive.%20In%20this%20work%2C%20we%0Apropose%20to%20study%20the%20continuous%20space%20of%20possible%20descriptions%20of%20a%20composite%0Asystem%20as%20a%20window%20into%20its%20organizational%20structure.%20A%20description%20consists%20of%0Aspecific%20information%20conveyed%20about%20each%20of%20the%20components%2C%20and%20the%20space%20of%0Apossible%20descriptions%20is%20equivalent%20to%20the%20space%20of%20lossy%20compression%20schemes%0Aof%20the%20components.%20We%20introduce%20a%20machine%20learning%20framework%20to%20optimize%0Adescriptions%20that%20extremize%20key%20information%20theoretic%20quantities%20used%20to%0Acharacterize%20organization%2C%20such%20as%20total%20correlation%20and%20O-information.%20Through%0Acase%20studies%20on%20spin%20systems%2C%20Sudoku%20boards%2C%20and%20letter%20sequences%20from%20natural%0Alanguage%2C%20we%20identify%20extremal%20descriptions%20that%20reveal%20how%20system-wide%0Avariation%20emerges%20from%20individual%20components.%20By%20integrating%20machine%20learning%0Ainto%20a%20fine-grained%20information%20theoretic%20analysis%20of%20composite%20random%0Avariables%2C%20our%20framework%20opens%20a%20new%20avenues%20for%20probing%20the%20structure%20of%0Areal-world%20complex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18579v1&entry.124074799=Read"},
{"title": "Complexity Experts are Task-Discriminative Learners for Any Image\n  Restoration", "author": "Eduard Zamfir and Zongwei Wu and Nancy Mehta and Yuedong Tan and Danda Pani Paudel and Yulun Zhang and Radu Timofte", "abstract": "  Recent advancements in all-in-one image restoration models have\nrevolutionized the ability to address diverse degradations through a unified\nframework. However, parameters tied to specific tasks often remain inactive for\nother tasks, making mixture-of-experts (MoE) architectures a natural extension.\nDespite this, MoEs often show inconsistent behavior, with some experts\nunexpectedly generalizing across tasks while others struggle within their\nintended scope. This hinders leveraging MoEs' computational benefits by\nbypassing irrelevant experts during inference. We attribute this undesired\nbehavior to the uniform and rigid architecture of traditional MoEs. To address\nthis, we introduce ``complexity experts\" -- flexible expert blocks with varying\ncomputational complexity and receptive fields. A key challenge is assigning\ntasks to each expert, as degradation complexity is unknown in advance. Thus, we\nexecute tasks with a simple bias toward lower complexity. To our surprise, this\npreference effectively drives task-specific allocation, assigning tasks to\nexperts with the appropriate complexity. Extensive experiments validate our\napproach, demonstrating the ability to bypass irrelevant experts during\ninference while maintaining superior performance. The proposed MoCE-IR model\noutperforms state-of-the-art methods, affirming its efficiency and practical\napplicability. The source will be publicly made available at\n\\href{https://eduardzamfir.github.io/moceir/}{\\texttt{eduardzamfir.github.io/MoCE-IR/}}\n", "link": "http://arxiv.org/abs/2411.18466v1", "date": "2024-11-27", "relevancy": 2.6236, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complexity%20Experts%20are%20Task-Discriminative%20Learners%20for%20Any%20Image%0A%20%20Restoration&body=Title%3A%20Complexity%20Experts%20are%20Task-Discriminative%20Learners%20for%20Any%20Image%0A%20%20Restoration%0AAuthor%3A%20Eduard%20Zamfir%20and%20Zongwei%20Wu%20and%20Nancy%20Mehta%20and%20Yuedong%20Tan%20and%20Danda%20Pani%20Paudel%20and%20Yulun%20Zhang%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20Recent%20advancements%20in%20all-in-one%20image%20restoration%20models%20have%0Arevolutionized%20the%20ability%20to%20address%20diverse%20degradations%20through%20a%20unified%0Aframework.%20However%2C%20parameters%20tied%20to%20specific%20tasks%20often%20remain%20inactive%20for%0Aother%20tasks%2C%20making%20mixture-of-experts%20%28MoE%29%20architectures%20a%20natural%20extension.%0ADespite%20this%2C%20MoEs%20often%20show%20inconsistent%20behavior%2C%20with%20some%20experts%0Aunexpectedly%20generalizing%20across%20tasks%20while%20others%20struggle%20within%20their%0Aintended%20scope.%20This%20hinders%20leveraging%20MoEs%27%20computational%20benefits%20by%0Abypassing%20irrelevant%20experts%20during%20inference.%20We%20attribute%20this%20undesired%0Abehavior%20to%20the%20uniform%20and%20rigid%20architecture%20of%20traditional%20MoEs.%20To%20address%0Athis%2C%20we%20introduce%20%60%60complexity%20experts%22%20--%20flexible%20expert%20blocks%20with%20varying%0Acomputational%20complexity%20and%20receptive%20fields.%20A%20key%20challenge%20is%20assigning%0Atasks%20to%20each%20expert%2C%20as%20degradation%20complexity%20is%20unknown%20in%20advance.%20Thus%2C%20we%0Aexecute%20tasks%20with%20a%20simple%20bias%20toward%20lower%20complexity.%20To%20our%20surprise%2C%20this%0Apreference%20effectively%20drives%20task-specific%20allocation%2C%20assigning%20tasks%20to%0Aexperts%20with%20the%20appropriate%20complexity.%20Extensive%20experiments%20validate%20our%0Aapproach%2C%20demonstrating%20the%20ability%20to%20bypass%20irrelevant%20experts%20during%0Ainference%20while%20maintaining%20superior%20performance.%20The%20proposed%20MoCE-IR%20model%0Aoutperforms%20state-of-the-art%20methods%2C%20affirming%20its%20efficiency%20and%20practical%0Aapplicability.%20The%20source%20will%20be%20publicly%20made%20available%20at%0A%5Chref%7Bhttps%3A//eduardzamfir.github.io/moceir/%7D%7B%5Ctexttt%7Beduardzamfir.github.io/MoCE-IR/%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplexity%2520Experts%2520are%2520Task-Discriminative%2520Learners%2520for%2520Any%2520Image%250A%2520%2520Restoration%26entry.906535625%3DEduard%2520Zamfir%2520and%2520Zongwei%2520Wu%2520and%2520Nancy%2520Mehta%2520and%2520Yuedong%2520Tan%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Yulun%2520Zhang%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520all-in-one%2520image%2520restoration%2520models%2520have%250Arevolutionized%2520the%2520ability%2520to%2520address%2520diverse%2520degradations%2520through%2520a%2520unified%250Aframework.%2520However%252C%2520parameters%2520tied%2520to%2520specific%2520tasks%2520often%2520remain%2520inactive%2520for%250Aother%2520tasks%252C%2520making%2520mixture-of-experts%2520%2528MoE%2529%2520architectures%2520a%2520natural%2520extension.%250ADespite%2520this%252C%2520MoEs%2520often%2520show%2520inconsistent%2520behavior%252C%2520with%2520some%2520experts%250Aunexpectedly%2520generalizing%2520across%2520tasks%2520while%2520others%2520struggle%2520within%2520their%250Aintended%2520scope.%2520This%2520hinders%2520leveraging%2520MoEs%2527%2520computational%2520benefits%2520by%250Abypassing%2520irrelevant%2520experts%2520during%2520inference.%2520We%2520attribute%2520this%2520undesired%250Abehavior%2520to%2520the%2520uniform%2520and%2520rigid%2520architecture%2520of%2520traditional%2520MoEs.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520%2560%2560complexity%2520experts%2522%2520--%2520flexible%2520expert%2520blocks%2520with%2520varying%250Acomputational%2520complexity%2520and%2520receptive%2520fields.%2520A%2520key%2520challenge%2520is%2520assigning%250Atasks%2520to%2520each%2520expert%252C%2520as%2520degradation%2520complexity%2520is%2520unknown%2520in%2520advance.%2520Thus%252C%2520we%250Aexecute%2520tasks%2520with%2520a%2520simple%2520bias%2520toward%2520lower%2520complexity.%2520To%2520our%2520surprise%252C%2520this%250Apreference%2520effectively%2520drives%2520task-specific%2520allocation%252C%2520assigning%2520tasks%2520to%250Aexperts%2520with%2520the%2520appropriate%2520complexity.%2520Extensive%2520experiments%2520validate%2520our%250Aapproach%252C%2520demonstrating%2520the%2520ability%2520to%2520bypass%2520irrelevant%2520experts%2520during%250Ainference%2520while%2520maintaining%2520superior%2520performance.%2520The%2520proposed%2520MoCE-IR%2520model%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520affirming%2520its%2520efficiency%2520and%2520practical%250Aapplicability.%2520The%2520source%2520will%2520be%2520publicly%2520made%2520available%2520at%250A%255Chref%257Bhttps%253A//eduardzamfir.github.io/moceir/%257D%257B%255Ctexttt%257Beduardzamfir.github.io/MoCE-IR/%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complexity%20Experts%20are%20Task-Discriminative%20Learners%20for%20Any%20Image%0A%20%20Restoration&entry.906535625=Eduard%20Zamfir%20and%20Zongwei%20Wu%20and%20Nancy%20Mehta%20and%20Yuedong%20Tan%20and%20Danda%20Pani%20Paudel%20and%20Yulun%20Zhang%20and%20Radu%20Timofte&entry.1292438233=%20%20Recent%20advancements%20in%20all-in-one%20image%20restoration%20models%20have%0Arevolutionized%20the%20ability%20to%20address%20diverse%20degradations%20through%20a%20unified%0Aframework.%20However%2C%20parameters%20tied%20to%20specific%20tasks%20often%20remain%20inactive%20for%0Aother%20tasks%2C%20making%20mixture-of-experts%20%28MoE%29%20architectures%20a%20natural%20extension.%0ADespite%20this%2C%20MoEs%20often%20show%20inconsistent%20behavior%2C%20with%20some%20experts%0Aunexpectedly%20generalizing%20across%20tasks%20while%20others%20struggle%20within%20their%0Aintended%20scope.%20This%20hinders%20leveraging%20MoEs%27%20computational%20benefits%20by%0Abypassing%20irrelevant%20experts%20during%20inference.%20We%20attribute%20this%20undesired%0Abehavior%20to%20the%20uniform%20and%20rigid%20architecture%20of%20traditional%20MoEs.%20To%20address%0Athis%2C%20we%20introduce%20%60%60complexity%20experts%22%20--%20flexible%20expert%20blocks%20with%20varying%0Acomputational%20complexity%20and%20receptive%20fields.%20A%20key%20challenge%20is%20assigning%0Atasks%20to%20each%20expert%2C%20as%20degradation%20complexity%20is%20unknown%20in%20advance.%20Thus%2C%20we%0Aexecute%20tasks%20with%20a%20simple%20bias%20toward%20lower%20complexity.%20To%20our%20surprise%2C%20this%0Apreference%20effectively%20drives%20task-specific%20allocation%2C%20assigning%20tasks%20to%0Aexperts%20with%20the%20appropriate%20complexity.%20Extensive%20experiments%20validate%20our%0Aapproach%2C%20demonstrating%20the%20ability%20to%20bypass%20irrelevant%20experts%20during%0Ainference%20while%20maintaining%20superior%20performance.%20The%20proposed%20MoCE-IR%20model%0Aoutperforms%20state-of-the-art%20methods%2C%20affirming%20its%20efficiency%20and%20practical%0Aapplicability.%20The%20source%20will%20be%20publicly%20made%20available%20at%0A%5Chref%7Bhttps%3A//eduardzamfir.github.io/moceir/%7D%7B%5Ctexttt%7Beduardzamfir.github.io/MoCE-IR/%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18466v1&entry.124074799=Read"},
{"title": "kNN-Res: Residual Neural Network with kNN-Graph coherence for point\n  cloud registration", "author": "Muhammad S. Battikh and Artem Lensky and Dillon Hammill and Matthew Cook", "abstract": "  In this paper, we present a method based on a residual neural network for\npoint set registration that preserves the topological structure of the target\npoint set. Similar to coherent point drift (CPD), the registration (alignment)\nproblem is viewed as the movement of data points sampled from a target\ndistribution along a regularized displacement vector field. Although the\ncoherence constraint in CPD is stated in terms of local motion coherence, the\nproposed regularization relies on a global smoothness constraint as a proxy for\npreserving local topology. This makes CPD less flexible when the deformation is\nlocally rigid but globally non-rigid as in the case of multiple objects and\narticulate pose registration. A kNN-graph coherence cost and geometric-aware\nstatistical distances are proposed to mitigate these issues. To create an\nend-to-end trainable pipeline, a simple Jacobian-based cost is introduced as a\nproxy for the intrinsically discrete kNN-graph cost. We present a theoretical\njustification for our Jacobian-based cost showing that it is sufficient for the\npreservation of the kNN-graph of the transformed point set. Further, to tackle\nthe registration of high-dimensional point sets, a constant time stochastic\napproximation of the kNN-graph coherence cost is introduced. The proposed\nmethod is illustrated on several 2-dimensional examples and tested on\nhigh-dimensional flow cytometry datasets where the task is to align two\ndistributions of cells whilst preserving the kNN-graph in order to preserve the\nbiological signal of the transformed data. The implementation of the proposed\napproach is available at https://github.com/MuhammadSaeedBatikh/kNN-Res_Demo/\nunder the MIT license.\n", "link": "http://arxiv.org/abs/2304.00050v3", "date": "2024-11-27", "relevancy": 2.5876, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5682}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.497}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20kNN-Res%3A%20Residual%20Neural%20Network%20with%20kNN-Graph%20coherence%20for%20point%0A%20%20cloud%20registration&body=Title%3A%20kNN-Res%3A%20Residual%20Neural%20Network%20with%20kNN-Graph%20coherence%20for%20point%0A%20%20cloud%20registration%0AAuthor%3A%20Muhammad%20S.%20Battikh%20and%20Artem%20Lensky%20and%20Dillon%20Hammill%20and%20Matthew%20Cook%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20method%20based%20on%20a%20residual%20neural%20network%20for%0Apoint%20set%20registration%20that%20preserves%20the%20topological%20structure%20of%20the%20target%0Apoint%20set.%20Similar%20to%20coherent%20point%20drift%20%28CPD%29%2C%20the%20registration%20%28alignment%29%0Aproblem%20is%20viewed%20as%20the%20movement%20of%20data%20points%20sampled%20from%20a%20target%0Adistribution%20along%20a%20regularized%20displacement%20vector%20field.%20Although%20the%0Acoherence%20constraint%20in%20CPD%20is%20stated%20in%20terms%20of%20local%20motion%20coherence%2C%20the%0Aproposed%20regularization%20relies%20on%20a%20global%20smoothness%20constraint%20as%20a%20proxy%20for%0Apreserving%20local%20topology.%20This%20makes%20CPD%20less%20flexible%20when%20the%20deformation%20is%0Alocally%20rigid%20but%20globally%20non-rigid%20as%20in%20the%20case%20of%20multiple%20objects%20and%0Aarticulate%20pose%20registration.%20A%20kNN-graph%20coherence%20cost%20and%20geometric-aware%0Astatistical%20distances%20are%20proposed%20to%20mitigate%20these%20issues.%20To%20create%20an%0Aend-to-end%20trainable%20pipeline%2C%20a%20simple%20Jacobian-based%20cost%20is%20introduced%20as%20a%0Aproxy%20for%20the%20intrinsically%20discrete%20kNN-graph%20cost.%20We%20present%20a%20theoretical%0Ajustification%20for%20our%20Jacobian-based%20cost%20showing%20that%20it%20is%20sufficient%20for%20the%0Apreservation%20of%20the%20kNN-graph%20of%20the%20transformed%20point%20set.%20Further%2C%20to%20tackle%0Athe%20registration%20of%20high-dimensional%20point%20sets%2C%20a%20constant%20time%20stochastic%0Aapproximation%20of%20the%20kNN-graph%20coherence%20cost%20is%20introduced.%20The%20proposed%0Amethod%20is%20illustrated%20on%20several%202-dimensional%20examples%20and%20tested%20on%0Ahigh-dimensional%20flow%20cytometry%20datasets%20where%20the%20task%20is%20to%20align%20two%0Adistributions%20of%20cells%20whilst%20preserving%20the%20kNN-graph%20in%20order%20to%20preserve%20the%0Abiological%20signal%20of%20the%20transformed%20data.%20The%20implementation%20of%20the%20proposed%0Aapproach%20is%20available%20at%20https%3A//github.com/MuhammadSaeedBatikh/kNN-Res_Demo/%0Aunder%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00050v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DkNN-Res%253A%2520Residual%2520Neural%2520Network%2520with%2520kNN-Graph%2520coherence%2520for%2520point%250A%2520%2520cloud%2520registration%26entry.906535625%3DMuhammad%2520S.%2520Battikh%2520and%2520Artem%2520Lensky%2520and%2520Dillon%2520Hammill%2520and%2520Matthew%2520Cook%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520based%2520on%2520a%2520residual%2520neural%2520network%2520for%250Apoint%2520set%2520registration%2520that%2520preserves%2520the%2520topological%2520structure%2520of%2520the%2520target%250Apoint%2520set.%2520Similar%2520to%2520coherent%2520point%2520drift%2520%2528CPD%2529%252C%2520the%2520registration%2520%2528alignment%2529%250Aproblem%2520is%2520viewed%2520as%2520the%2520movement%2520of%2520data%2520points%2520sampled%2520from%2520a%2520target%250Adistribution%2520along%2520a%2520regularized%2520displacement%2520vector%2520field.%2520Although%2520the%250Acoherence%2520constraint%2520in%2520CPD%2520is%2520stated%2520in%2520terms%2520of%2520local%2520motion%2520coherence%252C%2520the%250Aproposed%2520regularization%2520relies%2520on%2520a%2520global%2520smoothness%2520constraint%2520as%2520a%2520proxy%2520for%250Apreserving%2520local%2520topology.%2520This%2520makes%2520CPD%2520less%2520flexible%2520when%2520the%2520deformation%2520is%250Alocally%2520rigid%2520but%2520globally%2520non-rigid%2520as%2520in%2520the%2520case%2520of%2520multiple%2520objects%2520and%250Aarticulate%2520pose%2520registration.%2520A%2520kNN-graph%2520coherence%2520cost%2520and%2520geometric-aware%250Astatistical%2520distances%2520are%2520proposed%2520to%2520mitigate%2520these%2520issues.%2520To%2520create%2520an%250Aend-to-end%2520trainable%2520pipeline%252C%2520a%2520simple%2520Jacobian-based%2520cost%2520is%2520introduced%2520as%2520a%250Aproxy%2520for%2520the%2520intrinsically%2520discrete%2520kNN-graph%2520cost.%2520We%2520present%2520a%2520theoretical%250Ajustification%2520for%2520our%2520Jacobian-based%2520cost%2520showing%2520that%2520it%2520is%2520sufficient%2520for%2520the%250Apreservation%2520of%2520the%2520kNN-graph%2520of%2520the%2520transformed%2520point%2520set.%2520Further%252C%2520to%2520tackle%250Athe%2520registration%2520of%2520high-dimensional%2520point%2520sets%252C%2520a%2520constant%2520time%2520stochastic%250Aapproximation%2520of%2520the%2520kNN-graph%2520coherence%2520cost%2520is%2520introduced.%2520The%2520proposed%250Amethod%2520is%2520illustrated%2520on%2520several%25202-dimensional%2520examples%2520and%2520tested%2520on%250Ahigh-dimensional%2520flow%2520cytometry%2520datasets%2520where%2520the%2520task%2520is%2520to%2520align%2520two%250Adistributions%2520of%2520cells%2520whilst%2520preserving%2520the%2520kNN-graph%2520in%2520order%2520to%2520preserve%2520the%250Abiological%2520signal%2520of%2520the%2520transformed%2520data.%2520The%2520implementation%2520of%2520the%2520proposed%250Aapproach%2520is%2520available%2520at%2520https%253A//github.com/MuhammadSaeedBatikh/kNN-Res_Demo/%250Aunder%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.00050v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=kNN-Res%3A%20Residual%20Neural%20Network%20with%20kNN-Graph%20coherence%20for%20point%0A%20%20cloud%20registration&entry.906535625=Muhammad%20S.%20Battikh%20and%20Artem%20Lensky%20and%20Dillon%20Hammill%20and%20Matthew%20Cook&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20method%20based%20on%20a%20residual%20neural%20network%20for%0Apoint%20set%20registration%20that%20preserves%20the%20topological%20structure%20of%20the%20target%0Apoint%20set.%20Similar%20to%20coherent%20point%20drift%20%28CPD%29%2C%20the%20registration%20%28alignment%29%0Aproblem%20is%20viewed%20as%20the%20movement%20of%20data%20points%20sampled%20from%20a%20target%0Adistribution%20along%20a%20regularized%20displacement%20vector%20field.%20Although%20the%0Acoherence%20constraint%20in%20CPD%20is%20stated%20in%20terms%20of%20local%20motion%20coherence%2C%20the%0Aproposed%20regularization%20relies%20on%20a%20global%20smoothness%20constraint%20as%20a%20proxy%20for%0Apreserving%20local%20topology.%20This%20makes%20CPD%20less%20flexible%20when%20the%20deformation%20is%0Alocally%20rigid%20but%20globally%20non-rigid%20as%20in%20the%20case%20of%20multiple%20objects%20and%0Aarticulate%20pose%20registration.%20A%20kNN-graph%20coherence%20cost%20and%20geometric-aware%0Astatistical%20distances%20are%20proposed%20to%20mitigate%20these%20issues.%20To%20create%20an%0Aend-to-end%20trainable%20pipeline%2C%20a%20simple%20Jacobian-based%20cost%20is%20introduced%20as%20a%0Aproxy%20for%20the%20intrinsically%20discrete%20kNN-graph%20cost.%20We%20present%20a%20theoretical%0Ajustification%20for%20our%20Jacobian-based%20cost%20showing%20that%20it%20is%20sufficient%20for%20the%0Apreservation%20of%20the%20kNN-graph%20of%20the%20transformed%20point%20set.%20Further%2C%20to%20tackle%0Athe%20registration%20of%20high-dimensional%20point%20sets%2C%20a%20constant%20time%20stochastic%0Aapproximation%20of%20the%20kNN-graph%20coherence%20cost%20is%20introduced.%20The%20proposed%0Amethod%20is%20illustrated%20on%20several%202-dimensional%20examples%20and%20tested%20on%0Ahigh-dimensional%20flow%20cytometry%20datasets%20where%20the%20task%20is%20to%20align%20two%0Adistributions%20of%20cells%20whilst%20preserving%20the%20kNN-graph%20in%20order%20to%20preserve%20the%0Abiological%20signal%20of%20the%20transformed%20data.%20The%20implementation%20of%20the%20proposed%0Aapproach%20is%20available%20at%20https%3A//github.com/MuhammadSaeedBatikh/kNN-Res_Demo/%0Aunder%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00050v3&entry.124074799=Read"},
{"title": "Agent Skill Acquisition for Large Language Models via CycleQD", "author": "So Kuroki and Taishi Nakamura and Takuya Akiba and Yujin Tang", "abstract": "  Training large language models to acquire specific skills remains a\nchallenging endeavor. Conventional training approaches often struggle with data\ndistribution imbalances and inadequacies in objective functions that do not\nalign well with task-specific performance. To address these challenges, we\nintroduce CycleQD, a novel approach that leverages the Quality Diversity\nframework through a cyclic adaptation of the algorithm, along with a model\nmerging based crossover and an SVD-based mutation. In CycleQD, each task's\nperformance metric is alternated as the quality measure while the others serve\nas the behavioral characteristics. This cyclic focus on individual tasks allows\nfor concentrated effort on one task at a time, eliminating the need for data\nratio tuning and simplifying the design of the objective function. Empirical\nresults from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT\nbased models not only enables them to surpass traditional fine-tuning methods\nin coding, operating systems, and database tasks, but also achieves performance\non par with GPT-3.5-TURBO, which potentially contains much more parameters,\nacross these domains. Crucially, this enhanced performance is achieved while\nretaining robust language capabilities, as evidenced by its performance on\nwidely adopted language benchmark tasks. We highlight the key design choices in\nCycleQD, detailing how these contribute to its effectiveness. Furthermore, our\nmethod is general and can be applied to image segmentation models, highlighting\nits applicability across different domains.\n", "link": "http://arxiv.org/abs/2410.14735v2", "date": "2024-11-27", "relevancy": 2.5846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Skill%20Acquisition%20for%20Large%20Language%20Models%20via%20CycleQD&body=Title%3A%20Agent%20Skill%20Acquisition%20for%20Large%20Language%20Models%20via%20CycleQD%0AAuthor%3A%20So%20Kuroki%20and%20Taishi%20Nakamura%20and%20Takuya%20Akiba%20and%20Yujin%20Tang%0AAbstract%3A%20%20%20Training%20large%20language%20models%20to%20acquire%20specific%20skills%20remains%20a%0Achallenging%20endeavor.%20Conventional%20training%20approaches%20often%20struggle%20with%20data%0Adistribution%20imbalances%20and%20inadequacies%20in%20objective%20functions%20that%20do%20not%0Aalign%20well%20with%20task-specific%20performance.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20CycleQD%2C%20a%20novel%20approach%20that%20leverages%20the%20Quality%20Diversity%0Aframework%20through%20a%20cyclic%20adaptation%20of%20the%20algorithm%2C%20along%20with%20a%20model%0Amerging%20based%20crossover%20and%20an%20SVD-based%20mutation.%20In%20CycleQD%2C%20each%20task%27s%0Aperformance%20metric%20is%20alternated%20as%20the%20quality%20measure%20while%20the%20others%20serve%0Aas%20the%20behavioral%20characteristics.%20This%20cyclic%20focus%20on%20individual%20tasks%20allows%0Afor%20concentrated%20effort%20on%20one%20task%20at%20a%20time%2C%20eliminating%20the%20need%20for%20data%0Aratio%20tuning%20and%20simplifying%20the%20design%20of%20the%20objective%20function.%20Empirical%0Aresults%20from%20AgentBench%20indicate%20that%20applying%20CycleQD%20to%20LLAMA3-8B-INSTRUCT%0Abased%20models%20not%20only%20enables%20them%20to%20surpass%20traditional%20fine-tuning%20methods%0Ain%20coding%2C%20operating%20systems%2C%20and%20database%20tasks%2C%20but%20also%20achieves%20performance%0Aon%20par%20with%20GPT-3.5-TURBO%2C%20which%20potentially%20contains%20much%20more%20parameters%2C%0Aacross%20these%20domains.%20Crucially%2C%20this%20enhanced%20performance%20is%20achieved%20while%0Aretaining%20robust%20language%20capabilities%2C%20as%20evidenced%20by%20its%20performance%20on%0Awidely%20adopted%20language%20benchmark%20tasks.%20We%20highlight%20the%20key%20design%20choices%20in%0ACycleQD%2C%20detailing%20how%20these%20contribute%20to%20its%20effectiveness.%20Furthermore%2C%20our%0Amethod%20is%20general%20and%20can%20be%20applied%20to%20image%20segmentation%20models%2C%20highlighting%0Aits%20applicability%20across%20different%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Skill%2520Acquisition%2520for%2520Large%2520Language%2520Models%2520via%2520CycleQD%26entry.906535625%3DSo%2520Kuroki%2520and%2520Taishi%2520Nakamura%2520and%2520Takuya%2520Akiba%2520and%2520Yujin%2520Tang%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520to%2520acquire%2520specific%2520skills%2520remains%2520a%250Achallenging%2520endeavor.%2520Conventional%2520training%2520approaches%2520often%2520struggle%2520with%2520data%250Adistribution%2520imbalances%2520and%2520inadequacies%2520in%2520objective%2520functions%2520that%2520do%2520not%250Aalign%2520well%2520with%2520task-specific%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520CycleQD%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520the%2520Quality%2520Diversity%250Aframework%2520through%2520a%2520cyclic%2520adaptation%2520of%2520the%2520algorithm%252C%2520along%2520with%2520a%2520model%250Amerging%2520based%2520crossover%2520and%2520an%2520SVD-based%2520mutation.%2520In%2520CycleQD%252C%2520each%2520task%2527s%250Aperformance%2520metric%2520is%2520alternated%2520as%2520the%2520quality%2520measure%2520while%2520the%2520others%2520serve%250Aas%2520the%2520behavioral%2520characteristics.%2520This%2520cyclic%2520focus%2520on%2520individual%2520tasks%2520allows%250Afor%2520concentrated%2520effort%2520on%2520one%2520task%2520at%2520a%2520time%252C%2520eliminating%2520the%2520need%2520for%2520data%250Aratio%2520tuning%2520and%2520simplifying%2520the%2520design%2520of%2520the%2520objective%2520function.%2520Empirical%250Aresults%2520from%2520AgentBench%2520indicate%2520that%2520applying%2520CycleQD%2520to%2520LLAMA3-8B-INSTRUCT%250Abased%2520models%2520not%2520only%2520enables%2520them%2520to%2520surpass%2520traditional%2520fine-tuning%2520methods%250Ain%2520coding%252C%2520operating%2520systems%252C%2520and%2520database%2520tasks%252C%2520but%2520also%2520achieves%2520performance%250Aon%2520par%2520with%2520GPT-3.5-TURBO%252C%2520which%2520potentially%2520contains%2520much%2520more%2520parameters%252C%250Aacross%2520these%2520domains.%2520Crucially%252C%2520this%2520enhanced%2520performance%2520is%2520achieved%2520while%250Aretaining%2520robust%2520language%2520capabilities%252C%2520as%2520evidenced%2520by%2520its%2520performance%2520on%250Awidely%2520adopted%2520language%2520benchmark%2520tasks.%2520We%2520highlight%2520the%2520key%2520design%2520choices%2520in%250ACycleQD%252C%2520detailing%2520how%2520these%2520contribute%2520to%2520its%2520effectiveness.%2520Furthermore%252C%2520our%250Amethod%2520is%2520general%2520and%2520can%2520be%2520applied%2520to%2520image%2520segmentation%2520models%252C%2520highlighting%250Aits%2520applicability%2520across%2520different%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Skill%20Acquisition%20for%20Large%20Language%20Models%20via%20CycleQD&entry.906535625=So%20Kuroki%20and%20Taishi%20Nakamura%20and%20Takuya%20Akiba%20and%20Yujin%20Tang&entry.1292438233=%20%20Training%20large%20language%20models%20to%20acquire%20specific%20skills%20remains%20a%0Achallenging%20endeavor.%20Conventional%20training%20approaches%20often%20struggle%20with%20data%0Adistribution%20imbalances%20and%20inadequacies%20in%20objective%20functions%20that%20do%20not%0Aalign%20well%20with%20task-specific%20performance.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20CycleQD%2C%20a%20novel%20approach%20that%20leverages%20the%20Quality%20Diversity%0Aframework%20through%20a%20cyclic%20adaptation%20of%20the%20algorithm%2C%20along%20with%20a%20model%0Amerging%20based%20crossover%20and%20an%20SVD-based%20mutation.%20In%20CycleQD%2C%20each%20task%27s%0Aperformance%20metric%20is%20alternated%20as%20the%20quality%20measure%20while%20the%20others%20serve%0Aas%20the%20behavioral%20characteristics.%20This%20cyclic%20focus%20on%20individual%20tasks%20allows%0Afor%20concentrated%20effort%20on%20one%20task%20at%20a%20time%2C%20eliminating%20the%20need%20for%20data%0Aratio%20tuning%20and%20simplifying%20the%20design%20of%20the%20objective%20function.%20Empirical%0Aresults%20from%20AgentBench%20indicate%20that%20applying%20CycleQD%20to%20LLAMA3-8B-INSTRUCT%0Abased%20models%20not%20only%20enables%20them%20to%20surpass%20traditional%20fine-tuning%20methods%0Ain%20coding%2C%20operating%20systems%2C%20and%20database%20tasks%2C%20but%20also%20achieves%20performance%0Aon%20par%20with%20GPT-3.5-TURBO%2C%20which%20potentially%20contains%20much%20more%20parameters%2C%0Aacross%20these%20domains.%20Crucially%2C%20this%20enhanced%20performance%20is%20achieved%20while%0Aretaining%20robust%20language%20capabilities%2C%20as%20evidenced%20by%20its%20performance%20on%0Awidely%20adopted%20language%20benchmark%20tasks.%20We%20highlight%20the%20key%20design%20choices%20in%0ACycleQD%2C%20detailing%20how%20these%20contribute%20to%20its%20effectiveness.%20Furthermore%2C%20our%0Amethod%20is%20general%20and%20can%20be%20applied%20to%20image%20segmentation%20models%2C%20highlighting%0Aits%20applicability%20across%20different%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14735v2&entry.124074799=Read"},
{"title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains", "author": "Yein Park and Chanwoong Yoon and Jungwoo Park and Donghyeon Lee and Minbyul Jeong and Jaewoo Kang", "abstract": "  Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply ourChroKnowPrompt,\nan in-depth prompting to elicit chronological knowledge by traversing\nstep-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.\n", "link": "http://arxiv.org/abs/2410.09870v2", "date": "2024-11-27", "relevancy": 2.5757, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChroKnowledge%3A%20Unveiling%20Chronological%20Knowledge%20of%20Language%20Models%20in%0A%20%20Multiple%20Domains&body=Title%3A%20ChroKnowledge%3A%20Unveiling%20Chronological%20Knowledge%20of%20Language%20Models%20in%0A%20%20Multiple%20Domains%0AAuthor%3A%20Yein%20Park%20and%20Chanwoong%20Yoon%20and%20Jungwoo%20Park%20and%20Donghyeon%20Lee%20and%20Minbyul%20Jeong%20and%20Jaewoo%20Kang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20significant%20changes%20to%20many%20aspects%0Aof%20our%20lives.%20However%2C%20assessing%20and%20ensuring%20their%20chronological%20knowledge%0Aremains%20challenging.%20Existing%20approaches%20fall%20short%20in%20addressing%20the%20temporal%0Aadaptability%20of%20knowledge%2C%20often%20relying%20on%20a%20fixed%20time-point%20view.%20To%0Aovercome%20this%2C%20we%20introduce%20ChroKnowBench%2C%20a%20benchmark%20dataset%20designed%20to%0Aevaluate%20chronologically%20accumulated%20knowledge%20across%20three%20key%20aspects%3A%0Amultiple%20domains%2C%20time%20dependency%2C%20temporal%20state.%20Our%20benchmark%20distinguishes%0Abetween%20knowledge%20that%20evolves%20%28e.g.%2C%20personal%20history%2C%20scientific%20discoveries%2C%0Aamended%20laws%29%20and%20knowledge%20that%20remain%20constant%20%28e.g.%2C%20mathematical%20truths%2C%0Acommonsense%20facts%29.%20Building%20on%20this%20benchmark%2C%20we%20present%20ChroKnowledge%0A%28Chronological%20Categorization%20of%20Knowledge%29%2C%20a%20novel%20sampling-based%20framework%0Afor%20evaluating%20LLMs%27%20non-parametric%20chronological%20knowledge.%20Our%20evaluation%20led%0Ato%20the%20following%20observations%3A%20%281%29%20The%20ability%20of%20eliciting%20temporal%20knowledge%0Avaries%20depending%20on%20the%20data%20format%20that%20model%20was%20trained%20on.%20%282%29%20LLMs%0Apartially%20recall%20knowledge%20or%20show%20a%20cut-off%20at%20temporal%20boundaries%20rather%20than%0Arecalling%20all%20aspects%20of%20knowledge%20correctly.%20Thus%2C%20we%20apply%20ourChroKnowPrompt%2C%0Aan%20in-depth%20prompting%20to%20elicit%20chronological%20knowledge%20by%20traversing%0Astep-by-step%20through%20the%20surrounding%20time%20spans.%20We%20observe%20that%20it%0Asuccessfully%20recalls%20objects%20across%20both%20open-source%20and%20proprietary%20LLMs%2C%0Ademonstrating%20versatility%2C%20though%20it%20faces%20challenges%20with%20dynamic%20datasets%20and%0Aunstructured%20formats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChroKnowledge%253A%2520Unveiling%2520Chronological%2520Knowledge%2520of%2520Language%2520Models%2520in%250A%2520%2520Multiple%2520Domains%26entry.906535625%3DYein%2520Park%2520and%2520Chanwoong%2520Yoon%2520and%2520Jungwoo%2520Park%2520and%2520Donghyeon%2520Lee%2520and%2520Minbyul%2520Jeong%2520and%2520Jaewoo%2520Kang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520brought%2520significant%2520changes%2520to%2520many%2520aspects%250Aof%2520our%2520lives.%2520However%252C%2520assessing%2520and%2520ensuring%2520their%2520chronological%2520knowledge%250Aremains%2520challenging.%2520Existing%2520approaches%2520fall%2520short%2520in%2520addressing%2520the%2520temporal%250Aadaptability%2520of%2520knowledge%252C%2520often%2520relying%2520on%2520a%2520fixed%2520time-point%2520view.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520ChroKnowBench%252C%2520a%2520benchmark%2520dataset%2520designed%2520to%250Aevaluate%2520chronologically%2520accumulated%2520knowledge%2520across%2520three%2520key%2520aspects%253A%250Amultiple%2520domains%252C%2520time%2520dependency%252C%2520temporal%2520state.%2520Our%2520benchmark%2520distinguishes%250Abetween%2520knowledge%2520that%2520evolves%2520%2528e.g.%252C%2520personal%2520history%252C%2520scientific%2520discoveries%252C%250Aamended%2520laws%2529%2520and%2520knowledge%2520that%2520remain%2520constant%2520%2528e.g.%252C%2520mathematical%2520truths%252C%250Acommonsense%2520facts%2529.%2520Building%2520on%2520this%2520benchmark%252C%2520we%2520present%2520ChroKnowledge%250A%2528Chronological%2520Categorization%2520of%2520Knowledge%2529%252C%2520a%2520novel%2520sampling-based%2520framework%250Afor%2520evaluating%2520LLMs%2527%2520non-parametric%2520chronological%2520knowledge.%2520Our%2520evaluation%2520led%250Ato%2520the%2520following%2520observations%253A%2520%25281%2529%2520The%2520ability%2520of%2520eliciting%2520temporal%2520knowledge%250Avaries%2520depending%2520on%2520the%2520data%2520format%2520that%2520model%2520was%2520trained%2520on.%2520%25282%2529%2520LLMs%250Apartially%2520recall%2520knowledge%2520or%2520show%2520a%2520cut-off%2520at%2520temporal%2520boundaries%2520rather%2520than%250Arecalling%2520all%2520aspects%2520of%2520knowledge%2520correctly.%2520Thus%252C%2520we%2520apply%2520ourChroKnowPrompt%252C%250Aan%2520in-depth%2520prompting%2520to%2520elicit%2520chronological%2520knowledge%2520by%2520traversing%250Astep-by-step%2520through%2520the%2520surrounding%2520time%2520spans.%2520We%2520observe%2520that%2520it%250Asuccessfully%2520recalls%2520objects%2520across%2520both%2520open-source%2520and%2520proprietary%2520LLMs%252C%250Ademonstrating%2520versatility%252C%2520though%2520it%2520faces%2520challenges%2520with%2520dynamic%2520datasets%2520and%250Aunstructured%2520formats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChroKnowledge%3A%20Unveiling%20Chronological%20Knowledge%20of%20Language%20Models%20in%0A%20%20Multiple%20Domains&entry.906535625=Yein%20Park%20and%20Chanwoong%20Yoon%20and%20Jungwoo%20Park%20and%20Donghyeon%20Lee%20and%20Minbyul%20Jeong%20and%20Jaewoo%20Kang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20brought%20significant%20changes%20to%20many%20aspects%0Aof%20our%20lives.%20However%2C%20assessing%20and%20ensuring%20their%20chronological%20knowledge%0Aremains%20challenging.%20Existing%20approaches%20fall%20short%20in%20addressing%20the%20temporal%0Aadaptability%20of%20knowledge%2C%20often%20relying%20on%20a%20fixed%20time-point%20view.%20To%0Aovercome%20this%2C%20we%20introduce%20ChroKnowBench%2C%20a%20benchmark%20dataset%20designed%20to%0Aevaluate%20chronologically%20accumulated%20knowledge%20across%20three%20key%20aspects%3A%0Amultiple%20domains%2C%20time%20dependency%2C%20temporal%20state.%20Our%20benchmark%20distinguishes%0Abetween%20knowledge%20that%20evolves%20%28e.g.%2C%20personal%20history%2C%20scientific%20discoveries%2C%0Aamended%20laws%29%20and%20knowledge%20that%20remain%20constant%20%28e.g.%2C%20mathematical%20truths%2C%0Acommonsense%20facts%29.%20Building%20on%20this%20benchmark%2C%20we%20present%20ChroKnowledge%0A%28Chronological%20Categorization%20of%20Knowledge%29%2C%20a%20novel%20sampling-based%20framework%0Afor%20evaluating%20LLMs%27%20non-parametric%20chronological%20knowledge.%20Our%20evaluation%20led%0Ato%20the%20following%20observations%3A%20%281%29%20The%20ability%20of%20eliciting%20temporal%20knowledge%0Avaries%20depending%20on%20the%20data%20format%20that%20model%20was%20trained%20on.%20%282%29%20LLMs%0Apartially%20recall%20knowledge%20or%20show%20a%20cut-off%20at%20temporal%20boundaries%20rather%20than%0Arecalling%20all%20aspects%20of%20knowledge%20correctly.%20Thus%2C%20we%20apply%20ourChroKnowPrompt%2C%0Aan%20in-depth%20prompting%20to%20elicit%20chronological%20knowledge%20by%20traversing%0Astep-by-step%20through%20the%20surrounding%20time%20spans.%20We%20observe%20that%20it%0Asuccessfully%20recalls%20objects%20across%20both%20open-source%20and%20proprietary%20LLMs%2C%0Ademonstrating%20versatility%2C%20though%20it%20faces%20challenges%20with%20dynamic%20datasets%20and%0Aunstructured%20formats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09870v2&entry.124074799=Read"},
{"title": "Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation", "author": "Xiaohan Yu and Li Zhang and Xin Zhao and Yue Wang", "abstract": "  The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.18262v1", "date": "2024-11-27", "relevancy": 2.5562, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Break%20the%20ID-Language%20Barrier%3A%20An%20Adaption%20Framework%20for%20Sequential%0A%20%20Recommendation&body=Title%3A%20Break%20the%20ID-Language%20Barrier%3A%20An%20Adaption%20Framework%20for%20Sequential%0A%20%20Recommendation%0AAuthor%3A%20Xiaohan%20Yu%20and%20Li%20Zhang%20and%20Xin%20Zhao%20and%20Yue%20Wang%0AAbstract%3A%20%20%20The%20recent%20breakthrough%20of%20large%20language%20models%20%28LLMs%29%20in%20natural%20language%0Aprocessing%20has%20sparked%20exploration%20in%20recommendation%20systems%2C%20however%2C%20their%0Alimited%20domain-specific%20knowledge%20remains%20a%20critical%20bottleneck.%20Specifically%2C%0ALLMs%20lack%20key%20pieces%20of%20information%20crucial%20for%20sequential%20recommendations%2C%0Asuch%20as%20user%20behavior%20patterns.%20To%20address%20this%20critical%20gap%2C%20we%20propose%0AIDLE-Adapter%2C%20a%20novel%20framework%20that%20integrates%20pre-trained%20ID%20embeddings%2C%20rich%0Ain%20domain-specific%20knowledge%2C%20into%20LLMs%20to%20improve%20recommendation%20accuracy.%0AIDLE-Adapter%20acts%20as%20a%20bridge%2C%20transforming%20sparse%20user-item%20interaction%20data%0Ainto%20dense%2C%20LLM-compatible%20representations%20through%20a%20Pre-trained%20ID%20Sequential%0AModel%2C%20Dimensionality%20Alignment%2C%20Layer-wise%20Embedding%20Refinement%2C%20and%0ALayer-wise%20Distribution%20Alignment.%20Furthermore%2C%20IDLE-Adapter%20demonstrates%0Aremarkable%20flexibility%20by%20seamlessly%20integrating%20ID%20embeddings%20from%20diverse%0AID-based%20sequential%20models%20and%20LLM%20architectures.%20Extensive%20experiments%20across%0Avarious%20datasets%20demonstrate%20the%20superiority%20of%20IDLE-Adapter%2C%20achieving%20over%0A10%5C%25%20and%2020%5C%25%20improvements%20in%20HitRate%405%20and%20NDCG%405%20metrics%2C%20respectively%2C%0Acompared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreak%2520the%2520ID-Language%2520Barrier%253A%2520An%2520Adaption%2520Framework%2520for%2520Sequential%250A%2520%2520Recommendation%26entry.906535625%3DXiaohan%2520Yu%2520and%2520Li%2520Zhang%2520and%2520Xin%2520Zhao%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520The%2520recent%2520breakthrough%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520natural%2520language%250Aprocessing%2520has%2520sparked%2520exploration%2520in%2520recommendation%2520systems%252C%2520however%252C%2520their%250Alimited%2520domain-specific%2520knowledge%2520remains%2520a%2520critical%2520bottleneck.%2520Specifically%252C%250ALLMs%2520lack%2520key%2520pieces%2520of%2520information%2520crucial%2520for%2520sequential%2520recommendations%252C%250Asuch%2520as%2520user%2520behavior%2520patterns.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520propose%250AIDLE-Adapter%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520pre-trained%2520ID%2520embeddings%252C%2520rich%250Ain%2520domain-specific%2520knowledge%252C%2520into%2520LLMs%2520to%2520improve%2520recommendation%2520accuracy.%250AIDLE-Adapter%2520acts%2520as%2520a%2520bridge%252C%2520transforming%2520sparse%2520user-item%2520interaction%2520data%250Ainto%2520dense%252C%2520LLM-compatible%2520representations%2520through%2520a%2520Pre-trained%2520ID%2520Sequential%250AModel%252C%2520Dimensionality%2520Alignment%252C%2520Layer-wise%2520Embedding%2520Refinement%252C%2520and%250ALayer-wise%2520Distribution%2520Alignment.%2520Furthermore%252C%2520IDLE-Adapter%2520demonstrates%250Aremarkable%2520flexibility%2520by%2520seamlessly%2520integrating%2520ID%2520embeddings%2520from%2520diverse%250AID-based%2520sequential%2520models%2520and%2520LLM%2520architectures.%2520Extensive%2520experiments%2520across%250Avarious%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520IDLE-Adapter%252C%2520achieving%2520over%250A10%255C%2525%2520and%252020%255C%2525%2520improvements%2520in%2520HitRate%25405%2520and%2520NDCG%25405%2520metrics%252C%2520respectively%252C%250Acompared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Break%20the%20ID-Language%20Barrier%3A%20An%20Adaption%20Framework%20for%20Sequential%0A%20%20Recommendation&entry.906535625=Xiaohan%20Yu%20and%20Li%20Zhang%20and%20Xin%20Zhao%20and%20Yue%20Wang&entry.1292438233=%20%20The%20recent%20breakthrough%20of%20large%20language%20models%20%28LLMs%29%20in%20natural%20language%0Aprocessing%20has%20sparked%20exploration%20in%20recommendation%20systems%2C%20however%2C%20their%0Alimited%20domain-specific%20knowledge%20remains%20a%20critical%20bottleneck.%20Specifically%2C%0ALLMs%20lack%20key%20pieces%20of%20information%20crucial%20for%20sequential%20recommendations%2C%0Asuch%20as%20user%20behavior%20patterns.%20To%20address%20this%20critical%20gap%2C%20we%20propose%0AIDLE-Adapter%2C%20a%20novel%20framework%20that%20integrates%20pre-trained%20ID%20embeddings%2C%20rich%0Ain%20domain-specific%20knowledge%2C%20into%20LLMs%20to%20improve%20recommendation%20accuracy.%0AIDLE-Adapter%20acts%20as%20a%20bridge%2C%20transforming%20sparse%20user-item%20interaction%20data%0Ainto%20dense%2C%20LLM-compatible%20representations%20through%20a%20Pre-trained%20ID%20Sequential%0AModel%2C%20Dimensionality%20Alignment%2C%20Layer-wise%20Embedding%20Refinement%2C%20and%0ALayer-wise%20Distribution%20Alignment.%20Furthermore%2C%20IDLE-Adapter%20demonstrates%0Aremarkable%20flexibility%20by%20seamlessly%20integrating%20ID%20embeddings%20from%20diverse%0AID-based%20sequential%20models%20and%20LLM%20architectures.%20Extensive%20experiments%20across%0Avarious%20datasets%20demonstrate%20the%20superiority%20of%20IDLE-Adapter%2C%20achieving%20over%0A10%5C%25%20and%2020%5C%25%20improvements%20in%20HitRate%405%20and%20NDCG%405%20metrics%2C%20respectively%2C%0Acompared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18262v1&entry.124074799=Read"},
{"title": "Utilizing the Mean Teacher with Supcontrast Loss for Wafer Pattern\n  Recognition", "author": "Qiyu Wei and Xun Xu and Zeng Zeng and Xulei Yang", "abstract": "  The patterns on wafer maps play a crucial role in helping engineers identify\nthe causes of production issues during semiconductor manufacturing. In order to\nreduce costs and improve accuracy, automation technology is essential, and\nrecent developments in deep learning have led to impressive results in wafer\nmap pattern recognition. In this context, inspired by the effectiveness of\nsemi-supervised learning and contrastive learning methods, we introduce an\ninnovative approach that integrates the Mean Teacher framework with the\nsupervised contrastive learning loss for enhanced wafer map pattern\nrecognition. Our methodology not only addresses the nuances of wafer patterns\nbut also tackles challenges arising from limited labeled data. To further\nrefine the process, we address data imbalance in the wafer dataset by employing\nSMOTE and under-sampling techniques. We conduct a comprehensive analysis of our\nproposed method and demonstrate its effectiveness through experiments using\nreal-world dataset WM811K obtained from semiconductor manufacturers. Compared\nto the baseline method, our method has achieved 5.46%, 6.68%, 5.42%, and 4.53%\nimprovements in Accuracy, Precision, Recall, and F1 score, respectively.\n", "link": "http://arxiv.org/abs/2411.18533v1", "date": "2024-11-27", "relevancy": 2.4958, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20the%20Mean%20Teacher%20with%20Supcontrast%20Loss%20for%20Wafer%20Pattern%0A%20%20Recognition&body=Title%3A%20Utilizing%20the%20Mean%20Teacher%20with%20Supcontrast%20Loss%20for%20Wafer%20Pattern%0A%20%20Recognition%0AAuthor%3A%20Qiyu%20Wei%20and%20Xun%20Xu%20and%20Zeng%20Zeng%20and%20Xulei%20Yang%0AAbstract%3A%20%20%20The%20patterns%20on%20wafer%20maps%20play%20a%20crucial%20role%20in%20helping%20engineers%20identify%0Athe%20causes%20of%20production%20issues%20during%20semiconductor%20manufacturing.%20In%20order%20to%0Areduce%20costs%20and%20improve%20accuracy%2C%20automation%20technology%20is%20essential%2C%20and%0Arecent%20developments%20in%20deep%20learning%20have%20led%20to%20impressive%20results%20in%20wafer%0Amap%20pattern%20recognition.%20In%20this%20context%2C%20inspired%20by%20the%20effectiveness%20of%0Asemi-supervised%20learning%20and%20contrastive%20learning%20methods%2C%20we%20introduce%20an%0Ainnovative%20approach%20that%20integrates%20the%20Mean%20Teacher%20framework%20with%20the%0Asupervised%20contrastive%20learning%20loss%20for%20enhanced%20wafer%20map%20pattern%0Arecognition.%20Our%20methodology%20not%20only%20addresses%20the%20nuances%20of%20wafer%20patterns%0Abut%20also%20tackles%20challenges%20arising%20from%20limited%20labeled%20data.%20To%20further%0Arefine%20the%20process%2C%20we%20address%20data%20imbalance%20in%20the%20wafer%20dataset%20by%20employing%0ASMOTE%20and%20under-sampling%20techniques.%20We%20conduct%20a%20comprehensive%20analysis%20of%20our%0Aproposed%20method%20and%20demonstrate%20its%20effectiveness%20through%20experiments%20using%0Areal-world%20dataset%20WM811K%20obtained%20from%20semiconductor%20manufacturers.%20Compared%0Ato%20the%20baseline%20method%2C%20our%20method%20has%20achieved%205.46%25%2C%206.68%25%2C%205.42%25%2C%20and%204.53%25%0Aimprovements%20in%20Accuracy%2C%20Precision%2C%20Recall%2C%20and%20F1%20score%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520the%2520Mean%2520Teacher%2520with%2520Supcontrast%2520Loss%2520for%2520Wafer%2520Pattern%250A%2520%2520Recognition%26entry.906535625%3DQiyu%2520Wei%2520and%2520Xun%2520Xu%2520and%2520Zeng%2520Zeng%2520and%2520Xulei%2520Yang%26entry.1292438233%3D%2520%2520The%2520patterns%2520on%2520wafer%2520maps%2520play%2520a%2520crucial%2520role%2520in%2520helping%2520engineers%2520identify%250Athe%2520causes%2520of%2520production%2520issues%2520during%2520semiconductor%2520manufacturing.%2520In%2520order%2520to%250Areduce%2520costs%2520and%2520improve%2520accuracy%252C%2520automation%2520technology%2520is%2520essential%252C%2520and%250Arecent%2520developments%2520in%2520deep%2520learning%2520have%2520led%2520to%2520impressive%2520results%2520in%2520wafer%250Amap%2520pattern%2520recognition.%2520In%2520this%2520context%252C%2520inspired%2520by%2520the%2520effectiveness%2520of%250Asemi-supervised%2520learning%2520and%2520contrastive%2520learning%2520methods%252C%2520we%2520introduce%2520an%250Ainnovative%2520approach%2520that%2520integrates%2520the%2520Mean%2520Teacher%2520framework%2520with%2520the%250Asupervised%2520contrastive%2520learning%2520loss%2520for%2520enhanced%2520wafer%2520map%2520pattern%250Arecognition.%2520Our%2520methodology%2520not%2520only%2520addresses%2520the%2520nuances%2520of%2520wafer%2520patterns%250Abut%2520also%2520tackles%2520challenges%2520arising%2520from%2520limited%2520labeled%2520data.%2520To%2520further%250Arefine%2520the%2520process%252C%2520we%2520address%2520data%2520imbalance%2520in%2520the%2520wafer%2520dataset%2520by%2520employing%250ASMOTE%2520and%2520under-sampling%2520techniques.%2520We%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520our%250Aproposed%2520method%2520and%2520demonstrate%2520its%2520effectiveness%2520through%2520experiments%2520using%250Areal-world%2520dataset%2520WM811K%2520obtained%2520from%2520semiconductor%2520manufacturers.%2520Compared%250Ato%2520the%2520baseline%2520method%252C%2520our%2520method%2520has%2520achieved%25205.46%2525%252C%25206.68%2525%252C%25205.42%2525%252C%2520and%25204.53%2525%250Aimprovements%2520in%2520Accuracy%252C%2520Precision%252C%2520Recall%252C%2520and%2520F1%2520score%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20the%20Mean%20Teacher%20with%20Supcontrast%20Loss%20for%20Wafer%20Pattern%0A%20%20Recognition&entry.906535625=Qiyu%20Wei%20and%20Xun%20Xu%20and%20Zeng%20Zeng%20and%20Xulei%20Yang&entry.1292438233=%20%20The%20patterns%20on%20wafer%20maps%20play%20a%20crucial%20role%20in%20helping%20engineers%20identify%0Athe%20causes%20of%20production%20issues%20during%20semiconductor%20manufacturing.%20In%20order%20to%0Areduce%20costs%20and%20improve%20accuracy%2C%20automation%20technology%20is%20essential%2C%20and%0Arecent%20developments%20in%20deep%20learning%20have%20led%20to%20impressive%20results%20in%20wafer%0Amap%20pattern%20recognition.%20In%20this%20context%2C%20inspired%20by%20the%20effectiveness%20of%0Asemi-supervised%20learning%20and%20contrastive%20learning%20methods%2C%20we%20introduce%20an%0Ainnovative%20approach%20that%20integrates%20the%20Mean%20Teacher%20framework%20with%20the%0Asupervised%20contrastive%20learning%20loss%20for%20enhanced%20wafer%20map%20pattern%0Arecognition.%20Our%20methodology%20not%20only%20addresses%20the%20nuances%20of%20wafer%20patterns%0Abut%20also%20tackles%20challenges%20arising%20from%20limited%20labeled%20data.%20To%20further%0Arefine%20the%20process%2C%20we%20address%20data%20imbalance%20in%20the%20wafer%20dataset%20by%20employing%0ASMOTE%20and%20under-sampling%20techniques.%20We%20conduct%20a%20comprehensive%20analysis%20of%20our%0Aproposed%20method%20and%20demonstrate%20its%20effectiveness%20through%20experiments%20using%0Areal-world%20dataset%20WM811K%20obtained%20from%20semiconductor%20manufacturers.%20Compared%0Ato%20the%20baseline%20method%2C%20our%20method%20has%20achieved%205.46%25%2C%206.68%25%2C%205.42%25%2C%20and%204.53%25%0Aimprovements%20in%20Accuracy%2C%20Precision%2C%20Recall%2C%20and%20F1%20score%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18533v1&entry.124074799=Read"},
{"title": "HiFiVFS: High Fidelity Video Face Swapping", "author": "Xu Chen and Keke He and Junwei Zhu and Yanhao Ge and Wei Li and Chengjie Wang", "abstract": "  Face swapping aims to generate results that combine the identity from the\nsource with attributes from the target. Existing methods primarily focus on\nimage-based face swapping. When processing videos, each frame is handled\nindependently, making it difficult to ensure temporal stability. From a model\nperspective, face swapping is gradually shifting from generative adversarial\nnetworks (GANs) to diffusion models (DMs), as DMs have been shown to possess\nstronger generative capabilities. Current diffusion-based approaches often\nemploy inpainting techniques, which struggle to preserve fine-grained\nattributes like lighting and makeup. To address these challenges, we propose a\nhigh fidelity video face swapping (HiFiVFS) framework, which leverages the\nstrong generative capability and temporal prior of Stable Video Diffusion\n(SVD). We build a fine-grained attribute module to extract\nidentity-disentangled and fine-grained attribute features through identity\ndesensitization and adversarial learning. Additionally, We introduce detailed\nidentity injection to further enhance identity similarity. Extensive\nexperiments demonstrate that our method achieves state-of-the-art (SOTA) in\nvideo face swapping, both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2411.18293v1", "date": "2024-11-27", "relevancy": 2.493, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6563}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6031}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFiVFS%3A%20High%20Fidelity%20Video%20Face%20Swapping&body=Title%3A%20HiFiVFS%3A%20High%20Fidelity%20Video%20Face%20Swapping%0AAuthor%3A%20Xu%20Chen%20and%20Keke%20He%20and%20Junwei%20Zhu%20and%20Yanhao%20Ge%20and%20Wei%20Li%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Face%20swapping%20aims%20to%20generate%20results%20that%20combine%20the%20identity%20from%20the%0Asource%20with%20attributes%20from%20the%20target.%20Existing%20methods%20primarily%20focus%20on%0Aimage-based%20face%20swapping.%20When%20processing%20videos%2C%20each%20frame%20is%20handled%0Aindependently%2C%20making%20it%20difficult%20to%20ensure%20temporal%20stability.%20From%20a%20model%0Aperspective%2C%20face%20swapping%20is%20gradually%20shifting%20from%20generative%20adversarial%0Anetworks%20%28GANs%29%20to%20diffusion%20models%20%28DMs%29%2C%20as%20DMs%20have%20been%20shown%20to%20possess%0Astronger%20generative%20capabilities.%20Current%20diffusion-based%20approaches%20often%0Aemploy%20inpainting%20techniques%2C%20which%20struggle%20to%20preserve%20fine-grained%0Aattributes%20like%20lighting%20and%20makeup.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Ahigh%20fidelity%20video%20face%20swapping%20%28HiFiVFS%29%20framework%2C%20which%20leverages%20the%0Astrong%20generative%20capability%20and%20temporal%20prior%20of%20Stable%20Video%20Diffusion%0A%28SVD%29.%20We%20build%20a%20fine-grained%20attribute%20module%20to%20extract%0Aidentity-disentangled%20and%20fine-grained%20attribute%20features%20through%20identity%0Adesensitization%20and%20adversarial%20learning.%20Additionally%2C%20We%20introduce%20detailed%0Aidentity%20injection%20to%20further%20enhance%20identity%20similarity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20in%0Avideo%20face%20swapping%2C%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFiVFS%253A%2520High%2520Fidelity%2520Video%2520Face%2520Swapping%26entry.906535625%3DXu%2520Chen%2520and%2520Keke%2520He%2520and%2520Junwei%2520Zhu%2520and%2520Yanhao%2520Ge%2520and%2520Wei%2520Li%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Face%2520swapping%2520aims%2520to%2520generate%2520results%2520that%2520combine%2520the%2520identity%2520from%2520the%250Asource%2520with%2520attributes%2520from%2520the%2520target.%2520Existing%2520methods%2520primarily%2520focus%2520on%250Aimage-based%2520face%2520swapping.%2520When%2520processing%2520videos%252C%2520each%2520frame%2520is%2520handled%250Aindependently%252C%2520making%2520it%2520difficult%2520to%2520ensure%2520temporal%2520stability.%2520From%2520a%2520model%250Aperspective%252C%2520face%2520swapping%2520is%2520gradually%2520shifting%2520from%2520generative%2520adversarial%250Anetworks%2520%2528GANs%2529%2520to%2520diffusion%2520models%2520%2528DMs%2529%252C%2520as%2520DMs%2520have%2520been%2520shown%2520to%2520possess%250Astronger%2520generative%2520capabilities.%2520Current%2520diffusion-based%2520approaches%2520often%250Aemploy%2520inpainting%2520techniques%252C%2520which%2520struggle%2520to%2520preserve%2520fine-grained%250Aattributes%2520like%2520lighting%2520and%2520makeup.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Ahigh%2520fidelity%2520video%2520face%2520swapping%2520%2528HiFiVFS%2529%2520framework%252C%2520which%2520leverages%2520the%250Astrong%2520generative%2520capability%2520and%2520temporal%2520prior%2520of%2520Stable%2520Video%2520Diffusion%250A%2528SVD%2529.%2520We%2520build%2520a%2520fine-grained%2520attribute%2520module%2520to%2520extract%250Aidentity-disentangled%2520and%2520fine-grained%2520attribute%2520features%2520through%2520identity%250Adesensitization%2520and%2520adversarial%2520learning.%2520Additionally%252C%2520We%2520introduce%2520detailed%250Aidentity%2520injection%2520to%2520further%2520enhance%2520identity%2520similarity.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520in%250Avideo%2520face%2520swapping%252C%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFiVFS%3A%20High%20Fidelity%20Video%20Face%20Swapping&entry.906535625=Xu%20Chen%20and%20Keke%20He%20and%20Junwei%20Zhu%20and%20Yanhao%20Ge%20and%20Wei%20Li%20and%20Chengjie%20Wang&entry.1292438233=%20%20Face%20swapping%20aims%20to%20generate%20results%20that%20combine%20the%20identity%20from%20the%0Asource%20with%20attributes%20from%20the%20target.%20Existing%20methods%20primarily%20focus%20on%0Aimage-based%20face%20swapping.%20When%20processing%20videos%2C%20each%20frame%20is%20handled%0Aindependently%2C%20making%20it%20difficult%20to%20ensure%20temporal%20stability.%20From%20a%20model%0Aperspective%2C%20face%20swapping%20is%20gradually%20shifting%20from%20generative%20adversarial%0Anetworks%20%28GANs%29%20to%20diffusion%20models%20%28DMs%29%2C%20as%20DMs%20have%20been%20shown%20to%20possess%0Astronger%20generative%20capabilities.%20Current%20diffusion-based%20approaches%20often%0Aemploy%20inpainting%20techniques%2C%20which%20struggle%20to%20preserve%20fine-grained%0Aattributes%20like%20lighting%20and%20makeup.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Ahigh%20fidelity%20video%20face%20swapping%20%28HiFiVFS%29%20framework%2C%20which%20leverages%20the%0Astrong%20generative%20capability%20and%20temporal%20prior%20of%20Stable%20Video%20Diffusion%0A%28SVD%29.%20We%20build%20a%20fine-grained%20attribute%20module%20to%20extract%0Aidentity-disentangled%20and%20fine-grained%20attribute%20features%20through%20identity%0Adesensitization%20and%20adversarial%20learning.%20Additionally%2C%20We%20introduce%20detailed%0Aidentity%20injection%20to%20further%20enhance%20identity%20similarity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20in%0Avideo%20face%20swapping%2C%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18293v1&entry.124074799=Read"},
{"title": "PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a\n  Single Image", "author": "Han Yan and Mingrui Zhang and Yang Li and Chao Ma and Pan Ji", "abstract": "  We present PhyCAGE, the first approach for physically plausible compositional\n3D asset generation from a single image. Given an input image, we first\ngenerate consistent multi-view images for components of the assets. These\nimages are then fitted with 3D Gaussian Splatting representations. To ensure\nthat the Gaussians representing objects are physically compatible with each\nother, we introduce a Physical Simulation-Enhanced Score Distillation Sampling\n(PSE-SDS) technique to further optimize the positions of the Gaussians. It is\nachieved by setting the gradient of the SDS loss as the initial velocity of the\nphysical simulation, allowing the simulator to act as a physics-guided\noptimizer that progressively corrects the Gaussians' positions to a physically\ncompatible state. Experimental results demonstrate that the proposed method can\ngenerate physically plausible compositional 3D assets given a single image.\n", "link": "http://arxiv.org/abs/2411.18548v1", "date": "2024-11-27", "relevancy": 2.4889, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6303}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6208}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyCAGE%3A%20Physically%20Plausible%20Compositional%203D%20Asset%20Generation%20from%20a%0A%20%20Single%20Image&body=Title%3A%20PhyCAGE%3A%20Physically%20Plausible%20Compositional%203D%20Asset%20Generation%20from%20a%0A%20%20Single%20Image%0AAuthor%3A%20Han%20Yan%20and%20Mingrui%20Zhang%20and%20Yang%20Li%20and%20Chao%20Ma%20and%20Pan%20Ji%0AAbstract%3A%20%20%20We%20present%20PhyCAGE%2C%20the%20first%20approach%20for%20physically%20plausible%20compositional%0A3D%20asset%20generation%20from%20a%20single%20image.%20Given%20an%20input%20image%2C%20we%20first%0Agenerate%20consistent%20multi-view%20images%20for%20components%20of%20the%20assets.%20These%0Aimages%20are%20then%20fitted%20with%203D%20Gaussian%20Splatting%20representations.%20To%20ensure%0Athat%20the%20Gaussians%20representing%20objects%20are%20physically%20compatible%20with%20each%0Aother%2C%20we%20introduce%20a%20Physical%20Simulation-Enhanced%20Score%20Distillation%20Sampling%0A%28PSE-SDS%29%20technique%20to%20further%20optimize%20the%20positions%20of%20the%20Gaussians.%20It%20is%0Aachieved%20by%20setting%20the%20gradient%20of%20the%20SDS%20loss%20as%20the%20initial%20velocity%20of%20the%0Aphysical%20simulation%2C%20allowing%20the%20simulator%20to%20act%20as%20a%20physics-guided%0Aoptimizer%20that%20progressively%20corrects%20the%20Gaussians%27%20positions%20to%20a%20physically%0Acompatible%20state.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20can%0Agenerate%20physically%20plausible%20compositional%203D%20assets%20given%20a%20single%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyCAGE%253A%2520Physically%2520Plausible%2520Compositional%25203D%2520Asset%2520Generation%2520from%2520a%250A%2520%2520Single%2520Image%26entry.906535625%3DHan%2520Yan%2520and%2520Mingrui%2520Zhang%2520and%2520Yang%2520Li%2520and%2520Chao%2520Ma%2520and%2520Pan%2520Ji%26entry.1292438233%3D%2520%2520We%2520present%2520PhyCAGE%252C%2520the%2520first%2520approach%2520for%2520physically%2520plausible%2520compositional%250A3D%2520asset%2520generation%2520from%2520a%2520single%2520image.%2520Given%2520an%2520input%2520image%252C%2520we%2520first%250Agenerate%2520consistent%2520multi-view%2520images%2520for%2520components%2520of%2520the%2520assets.%2520These%250Aimages%2520are%2520then%2520fitted%2520with%25203D%2520Gaussian%2520Splatting%2520representations.%2520To%2520ensure%250Athat%2520the%2520Gaussians%2520representing%2520objects%2520are%2520physically%2520compatible%2520with%2520each%250Aother%252C%2520we%2520introduce%2520a%2520Physical%2520Simulation-Enhanced%2520Score%2520Distillation%2520Sampling%250A%2528PSE-SDS%2529%2520technique%2520to%2520further%2520optimize%2520the%2520positions%2520of%2520the%2520Gaussians.%2520It%2520is%250Aachieved%2520by%2520setting%2520the%2520gradient%2520of%2520the%2520SDS%2520loss%2520as%2520the%2520initial%2520velocity%2520of%2520the%250Aphysical%2520simulation%252C%2520allowing%2520the%2520simulator%2520to%2520act%2520as%2520a%2520physics-guided%250Aoptimizer%2520that%2520progressively%2520corrects%2520the%2520Gaussians%2527%2520positions%2520to%2520a%2520physically%250Acompatible%2520state.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%250Agenerate%2520physically%2520plausible%2520compositional%25203D%2520assets%2520given%2520a%2520single%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyCAGE%3A%20Physically%20Plausible%20Compositional%203D%20Asset%20Generation%20from%20a%0A%20%20Single%20Image&entry.906535625=Han%20Yan%20and%20Mingrui%20Zhang%20and%20Yang%20Li%20and%20Chao%20Ma%20and%20Pan%20Ji&entry.1292438233=%20%20We%20present%20PhyCAGE%2C%20the%20first%20approach%20for%20physically%20plausible%20compositional%0A3D%20asset%20generation%20from%20a%20single%20image.%20Given%20an%20input%20image%2C%20we%20first%0Agenerate%20consistent%20multi-view%20images%20for%20components%20of%20the%20assets.%20These%0Aimages%20are%20then%20fitted%20with%203D%20Gaussian%20Splatting%20representations.%20To%20ensure%0Athat%20the%20Gaussians%20representing%20objects%20are%20physically%20compatible%20with%20each%0Aother%2C%20we%20introduce%20a%20Physical%20Simulation-Enhanced%20Score%20Distillation%20Sampling%0A%28PSE-SDS%29%20technique%20to%20further%20optimize%20the%20positions%20of%20the%20Gaussians.%20It%20is%0Aachieved%20by%20setting%20the%20gradient%20of%20the%20SDS%20loss%20as%20the%20initial%20velocity%20of%20the%0Aphysical%20simulation%2C%20allowing%20the%20simulator%20to%20act%20as%20a%20physics-guided%0Aoptimizer%20that%20progressively%20corrects%20the%20Gaussians%27%20positions%20to%20a%20physically%0Acompatible%20state.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20can%0Agenerate%20physically%20plausible%20compositional%203D%20assets%20given%20a%20single%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18548v1&entry.124074799=Read"},
{"title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data", "author": "Xinyi Wang and Antonis Antoniades and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang", "abstract": "  The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth. We also show the practical implications\nof our analysis through a novel prompt optimization algorithm.\n", "link": "http://arxiv.org/abs/2407.14985v4", "date": "2024-11-27", "relevancy": 2.4876, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20v.s.%20Memorization%3A%20Tracing%20Language%20Models%27%20Capabilities%0A%20%20Back%20to%20Pretraining%20Data&body=Title%3A%20Generalization%20v.s.%20Memorization%3A%20Tracing%20Language%20Models%27%20Capabilities%0A%20%20Back%20to%20Pretraining%20Data%0AAuthor%3A%20Xinyi%20Wang%20and%20Antonis%20Antoniades%20and%20Yanai%20Elazar%20and%20Alfonso%20Amayuelas%20and%20Alon%20Albalak%20and%20Kexun%20Zhang%20and%20William%20Yang%20Wang%0AAbstract%3A%20%20%20The%20impressive%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20sparked%0Adebate%20over%20whether%20these%20models%20genuinely%20generalize%20to%20unseen%20tasks%20or%0Apredominantly%20rely%20on%20memorizing%20vast%20amounts%20of%20pretraining%20data.%20To%20explore%0Athis%20issue%2C%20we%20introduce%20an%20extended%20concept%20of%20memorization%2C%20distributional%0Amemorization%2C%20which%20measures%20the%20correlation%20between%20the%20LLM%20output%0Aprobabilities%20and%20the%20pretraining%20data%20frequency.%20To%20effectively%20capture%0Atask-specific%20pretraining%20data%20frequency%2C%20we%20propose%20a%20novel%20task-gram%20language%0Amodel%2C%20which%20is%20built%20by%20counting%20the%20co-occurrence%20of%20semantically%20related%0A%24n%24-gram%20pairs%20from%20task%20inputs%20and%20outputs%20in%20the%20pretraining%20corpus.%20Using%0Athe%20Pythia%20models%20trained%20on%20the%20Pile%20dataset%2C%20we%20evaluate%20four%20distinct%20tasks%3A%0Amachine%20translation%2C%20factual%20question%20answering%2C%20world%20knowledge%20understanding%2C%0Aand%20math%20reasoning.%20Our%20findings%20reveal%20varying%20levels%20of%20memorization%2C%20with%0Athe%20strongest%20effect%20observed%20in%20factual%20question%20answering.%20Furthermore%2C%20while%0Amodel%20performance%20improves%20across%20all%20tasks%20as%20LLM%20size%20increases%2C%20only%20factual%0Aquestion%20answering%20shows%20an%20increase%20in%20memorization%2C%20whereas%20machine%0Atranslation%20and%20reasoning%20tasks%20exhibit%20greater%20generalization%2C%20producing%20more%0Anovel%20outputs.%20This%20study%20demonstrates%20that%20memorization%20plays%20a%20larger%20role%20in%0Asimpler%2C%20knowledge-intensive%20tasks%2C%20while%20generalization%20is%20the%20key%20for%20harder%2C%0Areasoning-based%20tasks%2C%20providing%20a%20scalable%20method%20for%20analyzing%20large%0Apretraining%20corpora%20in%20greater%20depth.%20We%20also%20show%20the%20practical%20implications%0Aof%20our%20analysis%20through%20a%20novel%20prompt%20optimization%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14985v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520v.s.%2520Memorization%253A%2520Tracing%2520Language%2520Models%2527%2520Capabilities%250A%2520%2520Back%2520to%2520Pretraining%2520Data%26entry.906535625%3DXinyi%2520Wang%2520and%2520Antonis%2520Antoniades%2520and%2520Yanai%2520Elazar%2520and%2520Alfonso%2520Amayuelas%2520and%2520Alon%2520Albalak%2520and%2520Kexun%2520Zhang%2520and%2520William%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520The%2520impressive%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520sparked%250Adebate%2520over%2520whether%2520these%2520models%2520genuinely%2520generalize%2520to%2520unseen%2520tasks%2520or%250Apredominantly%2520rely%2520on%2520memorizing%2520vast%2520amounts%2520of%2520pretraining%2520data.%2520To%2520explore%250Athis%2520issue%252C%2520we%2520introduce%2520an%2520extended%2520concept%2520of%2520memorization%252C%2520distributional%250Amemorization%252C%2520which%2520measures%2520the%2520correlation%2520between%2520the%2520LLM%2520output%250Aprobabilities%2520and%2520the%2520pretraining%2520data%2520frequency.%2520To%2520effectively%2520capture%250Atask-specific%2520pretraining%2520data%2520frequency%252C%2520we%2520propose%2520a%2520novel%2520task-gram%2520language%250Amodel%252C%2520which%2520is%2520built%2520by%2520counting%2520the%2520co-occurrence%2520of%2520semantically%2520related%250A%2524n%2524-gram%2520pairs%2520from%2520task%2520inputs%2520and%2520outputs%2520in%2520the%2520pretraining%2520corpus.%2520Using%250Athe%2520Pythia%2520models%2520trained%2520on%2520the%2520Pile%2520dataset%252C%2520we%2520evaluate%2520four%2520distinct%2520tasks%253A%250Amachine%2520translation%252C%2520factual%2520question%2520answering%252C%2520world%2520knowledge%2520understanding%252C%250Aand%2520math%2520reasoning.%2520Our%2520findings%2520reveal%2520varying%2520levels%2520of%2520memorization%252C%2520with%250Athe%2520strongest%2520effect%2520observed%2520in%2520factual%2520question%2520answering.%2520Furthermore%252C%2520while%250Amodel%2520performance%2520improves%2520across%2520all%2520tasks%2520as%2520LLM%2520size%2520increases%252C%2520only%2520factual%250Aquestion%2520answering%2520shows%2520an%2520increase%2520in%2520memorization%252C%2520whereas%2520machine%250Atranslation%2520and%2520reasoning%2520tasks%2520exhibit%2520greater%2520generalization%252C%2520producing%2520more%250Anovel%2520outputs.%2520This%2520study%2520demonstrates%2520that%2520memorization%2520plays%2520a%2520larger%2520role%2520in%250Asimpler%252C%2520knowledge-intensive%2520tasks%252C%2520while%2520generalization%2520is%2520the%2520key%2520for%2520harder%252C%250Areasoning-based%2520tasks%252C%2520providing%2520a%2520scalable%2520method%2520for%2520analyzing%2520large%250Apretraining%2520corpora%2520in%2520greater%2520depth.%2520We%2520also%2520show%2520the%2520practical%2520implications%250Aof%2520our%2520analysis%2520through%2520a%2520novel%2520prompt%2520optimization%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14985v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20v.s.%20Memorization%3A%20Tracing%20Language%20Models%27%20Capabilities%0A%20%20Back%20to%20Pretraining%20Data&entry.906535625=Xinyi%20Wang%20and%20Antonis%20Antoniades%20and%20Yanai%20Elazar%20and%20Alfonso%20Amayuelas%20and%20Alon%20Albalak%20and%20Kexun%20Zhang%20and%20William%20Yang%20Wang&entry.1292438233=%20%20The%20impressive%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%20sparked%0Adebate%20over%20whether%20these%20models%20genuinely%20generalize%20to%20unseen%20tasks%20or%0Apredominantly%20rely%20on%20memorizing%20vast%20amounts%20of%20pretraining%20data.%20To%20explore%0Athis%20issue%2C%20we%20introduce%20an%20extended%20concept%20of%20memorization%2C%20distributional%0Amemorization%2C%20which%20measures%20the%20correlation%20between%20the%20LLM%20output%0Aprobabilities%20and%20the%20pretraining%20data%20frequency.%20To%20effectively%20capture%0Atask-specific%20pretraining%20data%20frequency%2C%20we%20propose%20a%20novel%20task-gram%20language%0Amodel%2C%20which%20is%20built%20by%20counting%20the%20co-occurrence%20of%20semantically%20related%0A%24n%24-gram%20pairs%20from%20task%20inputs%20and%20outputs%20in%20the%20pretraining%20corpus.%20Using%0Athe%20Pythia%20models%20trained%20on%20the%20Pile%20dataset%2C%20we%20evaluate%20four%20distinct%20tasks%3A%0Amachine%20translation%2C%20factual%20question%20answering%2C%20world%20knowledge%20understanding%2C%0Aand%20math%20reasoning.%20Our%20findings%20reveal%20varying%20levels%20of%20memorization%2C%20with%0Athe%20strongest%20effect%20observed%20in%20factual%20question%20answering.%20Furthermore%2C%20while%0Amodel%20performance%20improves%20across%20all%20tasks%20as%20LLM%20size%20increases%2C%20only%20factual%0Aquestion%20answering%20shows%20an%20increase%20in%20memorization%2C%20whereas%20machine%0Atranslation%20and%20reasoning%20tasks%20exhibit%20greater%20generalization%2C%20producing%20more%0Anovel%20outputs.%20This%20study%20demonstrates%20that%20memorization%20plays%20a%20larger%20role%20in%0Asimpler%2C%20knowledge-intensive%20tasks%2C%20while%20generalization%20is%20the%20key%20for%20harder%2C%0Areasoning-based%20tasks%2C%20providing%20a%20scalable%20method%20for%20analyzing%20large%0Apretraining%20corpora%20in%20greater%20depth.%20We%20also%20show%20the%20practical%20implications%0Aof%20our%20analysis%20through%20a%20novel%20prompt%20optimization%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14985v4&entry.124074799=Read"},
{"title": "SWIM: Short-Window CNN Integrated with Mamba for EEG-Based Auditory\n  Spatial Attention Decoding", "author": "Ziyang Zhang and Andrew Thwaites and Alexandra Woolgar and Brian Moore and Chao Zhang", "abstract": "  In complex auditory environments, the human auditory system possesses the\nremarkable ability to focus on a specific speaker while disregarding others. In\nthis study, a new model named SWIM, a short-window convolution neural network\n(CNN) integrated with Mamba, is proposed for identifying the locus of auditory\nattention (left or right) from electroencephalography (EEG) signals without\nrelying on speech envelopes. SWIM consists of two parts. The first is a\nshort-window CNN (SW$_\\text{CNN}$), which acts as a short-term EEG feature\nextractor and achieves a final accuracy of 84.9% in the leave-one-speaker-out\nsetup on the widely used KUL dataset. This improvement is due to the use of an\nimproved CNN structure, data augmentation, multitask training, and model\ncombination. The second part, Mamba, is a sequence model first applied to\nauditory spatial attention decoding to leverage the long-term dependency from\nprevious SW$_\\text{CNN}$ time steps. By joint training SW$_\\text{CNN}$ and\nMamba, the proposed SWIM structure uses both short-term and long-term\ninformation and achieves an accuracy of 86.2%, which reduces the classification\nerrors by a relative 31.0% compared to the previous state-of-the-art result.\nThe source code is available at https://github.com/windowso/SWIM-ASAD.\n", "link": "http://arxiv.org/abs/2409.19884v2", "date": "2024-11-27", "relevancy": 2.4785, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4937}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWIM%3A%20Short-Window%20CNN%20Integrated%20with%20Mamba%20for%20EEG-Based%20Auditory%0A%20%20Spatial%20Attention%20Decoding&body=Title%3A%20SWIM%3A%20Short-Window%20CNN%20Integrated%20with%20Mamba%20for%20EEG-Based%20Auditory%0A%20%20Spatial%20Attention%20Decoding%0AAuthor%3A%20Ziyang%20Zhang%20and%20Andrew%20Thwaites%20and%20Alexandra%20Woolgar%20and%20Brian%20Moore%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20In%20complex%20auditory%20environments%2C%20the%20human%20auditory%20system%20possesses%20the%0Aremarkable%20ability%20to%20focus%20on%20a%20specific%20speaker%20while%20disregarding%20others.%20In%0Athis%20study%2C%20a%20new%20model%20named%20SWIM%2C%20a%20short-window%20convolution%20neural%20network%0A%28CNN%29%20integrated%20with%20Mamba%2C%20is%20proposed%20for%20identifying%20the%20locus%20of%20auditory%0Aattention%20%28left%20or%20right%29%20from%20electroencephalography%20%28EEG%29%20signals%20without%0Arelying%20on%20speech%20envelopes.%20SWIM%20consists%20of%20two%20parts.%20The%20first%20is%20a%0Ashort-window%20CNN%20%28SW%24_%5Ctext%7BCNN%7D%24%29%2C%20which%20acts%20as%20a%20short-term%20EEG%20feature%0Aextractor%20and%20achieves%20a%20final%20accuracy%20of%2084.9%25%20in%20the%20leave-one-speaker-out%0Asetup%20on%20the%20widely%20used%20KUL%20dataset.%20This%20improvement%20is%20due%20to%20the%20use%20of%20an%0Aimproved%20CNN%20structure%2C%20data%20augmentation%2C%20multitask%20training%2C%20and%20model%0Acombination.%20The%20second%20part%2C%20Mamba%2C%20is%20a%20sequence%20model%20first%20applied%20to%0Aauditory%20spatial%20attention%20decoding%20to%20leverage%20the%20long-term%20dependency%20from%0Aprevious%20SW%24_%5Ctext%7BCNN%7D%24%20time%20steps.%20By%20joint%20training%20SW%24_%5Ctext%7BCNN%7D%24%20and%0AMamba%2C%20the%20proposed%20SWIM%20structure%20uses%20both%20short-term%20and%20long-term%0Ainformation%20and%20achieves%20an%20accuracy%20of%2086.2%25%2C%20which%20reduces%20the%20classification%0Aerrors%20by%20a%20relative%2031.0%25%20compared%20to%20the%20previous%20state-of-the-art%20result.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/windowso/SWIM-ASAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWIM%253A%2520Short-Window%2520CNN%2520Integrated%2520with%2520Mamba%2520for%2520EEG-Based%2520Auditory%250A%2520%2520Spatial%2520Attention%2520Decoding%26entry.906535625%3DZiyang%2520Zhang%2520and%2520Andrew%2520Thwaites%2520and%2520Alexandra%2520Woolgar%2520and%2520Brian%2520Moore%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520complex%2520auditory%2520environments%252C%2520the%2520human%2520auditory%2520system%2520possesses%2520the%250Aremarkable%2520ability%2520to%2520focus%2520on%2520a%2520specific%2520speaker%2520while%2520disregarding%2520others.%2520In%250Athis%2520study%252C%2520a%2520new%2520model%2520named%2520SWIM%252C%2520a%2520short-window%2520convolution%2520neural%2520network%250A%2528CNN%2529%2520integrated%2520with%2520Mamba%252C%2520is%2520proposed%2520for%2520identifying%2520the%2520locus%2520of%2520auditory%250Aattention%2520%2528left%2520or%2520right%2529%2520from%2520electroencephalography%2520%2528EEG%2529%2520signals%2520without%250Arelying%2520on%2520speech%2520envelopes.%2520SWIM%2520consists%2520of%2520two%2520parts.%2520The%2520first%2520is%2520a%250Ashort-window%2520CNN%2520%2528SW%2524_%255Ctext%257BCNN%257D%2524%2529%252C%2520which%2520acts%2520as%2520a%2520short-term%2520EEG%2520feature%250Aextractor%2520and%2520achieves%2520a%2520final%2520accuracy%2520of%252084.9%2525%2520in%2520the%2520leave-one-speaker-out%250Asetup%2520on%2520the%2520widely%2520used%2520KUL%2520dataset.%2520This%2520improvement%2520is%2520due%2520to%2520the%2520use%2520of%2520an%250Aimproved%2520CNN%2520structure%252C%2520data%2520augmentation%252C%2520multitask%2520training%252C%2520and%2520model%250Acombination.%2520The%2520second%2520part%252C%2520Mamba%252C%2520is%2520a%2520sequence%2520model%2520first%2520applied%2520to%250Aauditory%2520spatial%2520attention%2520decoding%2520to%2520leverage%2520the%2520long-term%2520dependency%2520from%250Aprevious%2520SW%2524_%255Ctext%257BCNN%257D%2524%2520time%2520steps.%2520By%2520joint%2520training%2520SW%2524_%255Ctext%257BCNN%257D%2524%2520and%250AMamba%252C%2520the%2520proposed%2520SWIM%2520structure%2520uses%2520both%2520short-term%2520and%2520long-term%250Ainformation%2520and%2520achieves%2520an%2520accuracy%2520of%252086.2%2525%252C%2520which%2520reduces%2520the%2520classification%250Aerrors%2520by%2520a%2520relative%252031.0%2525%2520compared%2520to%2520the%2520previous%2520state-of-the-art%2520result.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/windowso/SWIM-ASAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWIM%3A%20Short-Window%20CNN%20Integrated%20with%20Mamba%20for%20EEG-Based%20Auditory%0A%20%20Spatial%20Attention%20Decoding&entry.906535625=Ziyang%20Zhang%20and%20Andrew%20Thwaites%20and%20Alexandra%20Woolgar%20and%20Brian%20Moore%20and%20Chao%20Zhang&entry.1292438233=%20%20In%20complex%20auditory%20environments%2C%20the%20human%20auditory%20system%20possesses%20the%0Aremarkable%20ability%20to%20focus%20on%20a%20specific%20speaker%20while%20disregarding%20others.%20In%0Athis%20study%2C%20a%20new%20model%20named%20SWIM%2C%20a%20short-window%20convolution%20neural%20network%0A%28CNN%29%20integrated%20with%20Mamba%2C%20is%20proposed%20for%20identifying%20the%20locus%20of%20auditory%0Aattention%20%28left%20or%20right%29%20from%20electroencephalography%20%28EEG%29%20signals%20without%0Arelying%20on%20speech%20envelopes.%20SWIM%20consists%20of%20two%20parts.%20The%20first%20is%20a%0Ashort-window%20CNN%20%28SW%24_%5Ctext%7BCNN%7D%24%29%2C%20which%20acts%20as%20a%20short-term%20EEG%20feature%0Aextractor%20and%20achieves%20a%20final%20accuracy%20of%2084.9%25%20in%20the%20leave-one-speaker-out%0Asetup%20on%20the%20widely%20used%20KUL%20dataset.%20This%20improvement%20is%20due%20to%20the%20use%20of%20an%0Aimproved%20CNN%20structure%2C%20data%20augmentation%2C%20multitask%20training%2C%20and%20model%0Acombination.%20The%20second%20part%2C%20Mamba%2C%20is%20a%20sequence%20model%20first%20applied%20to%0Aauditory%20spatial%20attention%20decoding%20to%20leverage%20the%20long-term%20dependency%20from%0Aprevious%20SW%24_%5Ctext%7BCNN%7D%24%20time%20steps.%20By%20joint%20training%20SW%24_%5Ctext%7BCNN%7D%24%20and%0AMamba%2C%20the%20proposed%20SWIM%20structure%20uses%20both%20short-term%20and%20long-term%0Ainformation%20and%20achieves%20an%20accuracy%20of%2086.2%25%2C%20which%20reduces%20the%20classification%0Aerrors%20by%20a%20relative%2031.0%25%20compared%20to%20the%20previous%20state-of-the-art%20result.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/windowso/SWIM-ASAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19884v2&entry.124074799=Read"},
{"title": "Leveraging Semantic Asymmetry for Precise Gross Tumor Volume\n  Segmentation of Nasopharyngeal Carcinoma in Planning CT", "author": "Zi Li and Ying Chen and Zeli Chen and Yanzhou Su and Tai Ma and Tony C. W. Mok and Yan-Jie Zhou and Yunhai Bai and Zhinlin Zheng and Le Lu and Yirui Wang and Jia Ge and Xianghua Ye and Senxiang Yan and Dakai Jin", "abstract": "  In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians\ntypically delineate the gross tumor volume (GTV) using non-contrast planning\ncomputed tomography to ensure accurate radiation dose delivery. However, the\nlow contrast between tumors and adjacent normal tissues necessitates that\nradiation oncologists manually delineate the tumors, often relying on\ndiagnostic MRI for guidance. % In this study, we propose a novel approach to\ndirectly segment NPC gross tumors on non-contrast planning CT images,\ncircumventing potential registration errors when aligning MRI or MRI-derived\ntumor masks to planning CT. To address the low contrast issues between tumors\nand adjacent normal structures in planning CT, we introduce a 3D Semantic\nAsymmetry Tumor segmentation (SATs) method. Specifically, we posit that a\nhealthy nasopharyngeal region is characteristically bilaterally symmetric,\nwhereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then,\nwe propose a Siamese contrastive learning segmentation framework that minimizes\nthe voxel-wise distance between original and flipped areas without tumor and\nencourages a larger distance between original and flipped areas with tumor.\nThus, our approach enhances the sensitivity of features to semantic\nasymmetries. % Extensive experiments demonstrate that the proposed SATs\nachieves the leading NPC GTV segmentation performance in both internal and\nexternal testing, \\emph{e.g.}, with at least 2\\% absolute Dice score\nimprovement and 12\\% average distance error reduction when compared to other\nstate-of-the-art methods in the external testing.\n", "link": "http://arxiv.org/abs/2411.18290v1", "date": "2024-11-27", "relevancy": 2.4783, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5039}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5037}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Semantic%20Asymmetry%20for%20Precise%20Gross%20Tumor%20Volume%0A%20%20Segmentation%20of%20Nasopharyngeal%20Carcinoma%20in%20Planning%20CT&body=Title%3A%20Leveraging%20Semantic%20Asymmetry%20for%20Precise%20Gross%20Tumor%20Volume%0A%20%20Segmentation%20of%20Nasopharyngeal%20Carcinoma%20in%20Planning%20CT%0AAuthor%3A%20Zi%20Li%20and%20Ying%20Chen%20and%20Zeli%20Chen%20and%20Yanzhou%20Su%20and%20Tai%20Ma%20and%20Tony%20C.%20W.%20Mok%20and%20Yan-Jie%20Zhou%20and%20Yunhai%20Bai%20and%20Zhinlin%20Zheng%20and%20Le%20Lu%20and%20Yirui%20Wang%20and%20Jia%20Ge%20and%20Xianghua%20Ye%20and%20Senxiang%20Yan%20and%20Dakai%20Jin%0AAbstract%3A%20%20%20In%20the%20radiation%20therapy%20of%20nasopharyngeal%20carcinoma%20%28NPC%29%2C%20clinicians%0Atypically%20delineate%20the%20gross%20tumor%20volume%20%28GTV%29%20using%20non-contrast%20planning%0Acomputed%20tomography%20to%20ensure%20accurate%20radiation%20dose%20delivery.%20However%2C%20the%0Alow%20contrast%20between%20tumors%20and%20adjacent%20normal%20tissues%20necessitates%20that%0Aradiation%20oncologists%20manually%20delineate%20the%20tumors%2C%20often%20relying%20on%0Adiagnostic%20MRI%20for%20guidance.%20%25%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%0Adirectly%20segment%20NPC%20gross%20tumors%20on%20non-contrast%20planning%20CT%20images%2C%0Acircumventing%20potential%20registration%20errors%20when%20aligning%20MRI%20or%20MRI-derived%0Atumor%20masks%20to%20planning%20CT.%20To%20address%20the%20low%20contrast%20issues%20between%20tumors%0Aand%20adjacent%20normal%20structures%20in%20planning%20CT%2C%20we%20introduce%20a%203D%20Semantic%0AAsymmetry%20Tumor%20segmentation%20%28SATs%29%20method.%20Specifically%2C%20we%20posit%20that%20a%0Ahealthy%20nasopharyngeal%20region%20is%20characteristically%20bilaterally%20symmetric%2C%0Awhereas%20the%20emergence%20of%20nasopharyngeal%20carcinoma%20disrupts%20this%20symmetry.%20Then%2C%0Awe%20propose%20a%20Siamese%20contrastive%20learning%20segmentation%20framework%20that%20minimizes%0Athe%20voxel-wise%20distance%20between%20original%20and%20flipped%20areas%20without%20tumor%20and%0Aencourages%20a%20larger%20distance%20between%20original%20and%20flipped%20areas%20with%20tumor.%0AThus%2C%20our%20approach%20enhances%20the%20sensitivity%20of%20features%20to%20semantic%0Aasymmetries.%20%25%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20SATs%0Aachieves%20the%20leading%20NPC%20GTV%20segmentation%20performance%20in%20both%20internal%20and%0Aexternal%20testing%2C%20%5Cemph%7Be.g.%7D%2C%20with%20at%20least%202%5C%25%20absolute%20Dice%20score%0Aimprovement%20and%2012%5C%25%20average%20distance%20error%20reduction%20when%20compared%20to%20other%0Astate-of-the-art%20methods%20in%20the%20external%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Semantic%2520Asymmetry%2520for%2520Precise%2520Gross%2520Tumor%2520Volume%250A%2520%2520Segmentation%2520of%2520Nasopharyngeal%2520Carcinoma%2520in%2520Planning%2520CT%26entry.906535625%3DZi%2520Li%2520and%2520Ying%2520Chen%2520and%2520Zeli%2520Chen%2520and%2520Yanzhou%2520Su%2520and%2520Tai%2520Ma%2520and%2520Tony%2520C.%2520W.%2520Mok%2520and%2520Yan-Jie%2520Zhou%2520and%2520Yunhai%2520Bai%2520and%2520Zhinlin%2520Zheng%2520and%2520Le%2520Lu%2520and%2520Yirui%2520Wang%2520and%2520Jia%2520Ge%2520and%2520Xianghua%2520Ye%2520and%2520Senxiang%2520Yan%2520and%2520Dakai%2520Jin%26entry.1292438233%3D%2520%2520In%2520the%2520radiation%2520therapy%2520of%2520nasopharyngeal%2520carcinoma%2520%2528NPC%2529%252C%2520clinicians%250Atypically%2520delineate%2520the%2520gross%2520tumor%2520volume%2520%2528GTV%2529%2520using%2520non-contrast%2520planning%250Acomputed%2520tomography%2520to%2520ensure%2520accurate%2520radiation%2520dose%2520delivery.%2520However%252C%2520the%250Alow%2520contrast%2520between%2520tumors%2520and%2520adjacent%2520normal%2520tissues%2520necessitates%2520that%250Aradiation%2520oncologists%2520manually%2520delineate%2520the%2520tumors%252C%2520often%2520relying%2520on%250Adiagnostic%2520MRI%2520for%2520guidance.%2520%2525%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%250Adirectly%2520segment%2520NPC%2520gross%2520tumors%2520on%2520non-contrast%2520planning%2520CT%2520images%252C%250Acircumventing%2520potential%2520registration%2520errors%2520when%2520aligning%2520MRI%2520or%2520MRI-derived%250Atumor%2520masks%2520to%2520planning%2520CT.%2520To%2520address%2520the%2520low%2520contrast%2520issues%2520between%2520tumors%250Aand%2520adjacent%2520normal%2520structures%2520in%2520planning%2520CT%252C%2520we%2520introduce%2520a%25203D%2520Semantic%250AAsymmetry%2520Tumor%2520segmentation%2520%2528SATs%2529%2520method.%2520Specifically%252C%2520we%2520posit%2520that%2520a%250Ahealthy%2520nasopharyngeal%2520region%2520is%2520characteristically%2520bilaterally%2520symmetric%252C%250Awhereas%2520the%2520emergence%2520of%2520nasopharyngeal%2520carcinoma%2520disrupts%2520this%2520symmetry.%2520Then%252C%250Awe%2520propose%2520a%2520Siamese%2520contrastive%2520learning%2520segmentation%2520framework%2520that%2520minimizes%250Athe%2520voxel-wise%2520distance%2520between%2520original%2520and%2520flipped%2520areas%2520without%2520tumor%2520and%250Aencourages%2520a%2520larger%2520distance%2520between%2520original%2520and%2520flipped%2520areas%2520with%2520tumor.%250AThus%252C%2520our%2520approach%2520enhances%2520the%2520sensitivity%2520of%2520features%2520to%2520semantic%250Aasymmetries.%2520%2525%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520SATs%250Aachieves%2520the%2520leading%2520NPC%2520GTV%2520segmentation%2520performance%2520in%2520both%2520internal%2520and%250Aexternal%2520testing%252C%2520%255Cemph%257Be.g.%257D%252C%2520with%2520at%2520least%25202%255C%2525%2520absolute%2520Dice%2520score%250Aimprovement%2520and%252012%255C%2525%2520average%2520distance%2520error%2520reduction%2520when%2520compared%2520to%2520other%250Astate-of-the-art%2520methods%2520in%2520the%2520external%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Semantic%20Asymmetry%20for%20Precise%20Gross%20Tumor%20Volume%0A%20%20Segmentation%20of%20Nasopharyngeal%20Carcinoma%20in%20Planning%20CT&entry.906535625=Zi%20Li%20and%20Ying%20Chen%20and%20Zeli%20Chen%20and%20Yanzhou%20Su%20and%20Tai%20Ma%20and%20Tony%20C.%20W.%20Mok%20and%20Yan-Jie%20Zhou%20and%20Yunhai%20Bai%20and%20Zhinlin%20Zheng%20and%20Le%20Lu%20and%20Yirui%20Wang%20and%20Jia%20Ge%20and%20Xianghua%20Ye%20and%20Senxiang%20Yan%20and%20Dakai%20Jin&entry.1292438233=%20%20In%20the%20radiation%20therapy%20of%20nasopharyngeal%20carcinoma%20%28NPC%29%2C%20clinicians%0Atypically%20delineate%20the%20gross%20tumor%20volume%20%28GTV%29%20using%20non-contrast%20planning%0Acomputed%20tomography%20to%20ensure%20accurate%20radiation%20dose%20delivery.%20However%2C%20the%0Alow%20contrast%20between%20tumors%20and%20adjacent%20normal%20tissues%20necessitates%20that%0Aradiation%20oncologists%20manually%20delineate%20the%20tumors%2C%20often%20relying%20on%0Adiagnostic%20MRI%20for%20guidance.%20%25%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%0Adirectly%20segment%20NPC%20gross%20tumors%20on%20non-contrast%20planning%20CT%20images%2C%0Acircumventing%20potential%20registration%20errors%20when%20aligning%20MRI%20or%20MRI-derived%0Atumor%20masks%20to%20planning%20CT.%20To%20address%20the%20low%20contrast%20issues%20between%20tumors%0Aand%20adjacent%20normal%20structures%20in%20planning%20CT%2C%20we%20introduce%20a%203D%20Semantic%0AAsymmetry%20Tumor%20segmentation%20%28SATs%29%20method.%20Specifically%2C%20we%20posit%20that%20a%0Ahealthy%20nasopharyngeal%20region%20is%20characteristically%20bilaterally%20symmetric%2C%0Awhereas%20the%20emergence%20of%20nasopharyngeal%20carcinoma%20disrupts%20this%20symmetry.%20Then%2C%0Awe%20propose%20a%20Siamese%20contrastive%20learning%20segmentation%20framework%20that%20minimizes%0Athe%20voxel-wise%20distance%20between%20original%20and%20flipped%20areas%20without%20tumor%20and%0Aencourages%20a%20larger%20distance%20between%20original%20and%20flipped%20areas%20with%20tumor.%0AThus%2C%20our%20approach%20enhances%20the%20sensitivity%20of%20features%20to%20semantic%0Aasymmetries.%20%25%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20SATs%0Aachieves%20the%20leading%20NPC%20GTV%20segmentation%20performance%20in%20both%20internal%20and%0Aexternal%20testing%2C%20%5Cemph%7Be.g.%7D%2C%20with%20at%20least%202%5C%25%20absolute%20Dice%20score%0Aimprovement%20and%2012%5C%25%20average%20distance%20error%20reduction%20when%20compared%20to%20other%0Astate-of-the-art%20methods%20in%20the%20external%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18290v1&entry.124074799=Read"},
{"title": "Structured light with a million light planes per second", "author": "Dhawal Sirikonda and Praneeth Chakravarthula and Ioannis Gkioulekas and Adithya Pediredla", "abstract": "  We introduce a structured light system that captures full-frame depth at\nrates of a thousand frames per second, four times faster than the previous\nstate of the art. Our key innovation to this end is the design of an\nacousto-optic light scanning device that can scan light planes at rates up to\ntwo million planes per second. We combine this device with an event camera for\nstructured light, using the sparse events triggered on the camera as we sweep a\nlight plane on the scene for depth triangulation. In contrast to prior work,\nwhere light scanning is the bottleneck towards faster structured light\noperation, our light scanning device is three orders of magnitude faster than\nthe event camera's full-frame bandwidth, thus allowing us to take full\nadvantage of the event camera's fast operation. To surpass this bandwidth, we\nadditionally demonstrate adaptive scanning of only regions of interest, at\nspeeds an order of magnitude faster than the theoretical full-frame limit for\nevent cameras.\n", "link": "http://arxiv.org/abs/2411.18597v1", "date": "2024-11-27", "relevancy": 2.4771, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5319}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20light%20with%20a%20million%20light%20planes%20per%20second&body=Title%3A%20Structured%20light%20with%20a%20million%20light%20planes%20per%20second%0AAuthor%3A%20Dhawal%20Sirikonda%20and%20Praneeth%20Chakravarthula%20and%20Ioannis%20Gkioulekas%20and%20Adithya%20Pediredla%0AAbstract%3A%20%20%20We%20introduce%20a%20structured%20light%20system%20that%20captures%20full-frame%20depth%20at%0Arates%20of%20a%20thousand%20frames%20per%20second%2C%20four%20times%20faster%20than%20the%20previous%0Astate%20of%20the%20art.%20Our%20key%20innovation%20to%20this%20end%20is%20the%20design%20of%20an%0Aacousto-optic%20light%20scanning%20device%20that%20can%20scan%20light%20planes%20at%20rates%20up%20to%0Atwo%20million%20planes%20per%20second.%20We%20combine%20this%20device%20with%20an%20event%20camera%20for%0Astructured%20light%2C%20using%20the%20sparse%20events%20triggered%20on%20the%20camera%20as%20we%20sweep%20a%0Alight%20plane%20on%20the%20scene%20for%20depth%20triangulation.%20In%20contrast%20to%20prior%20work%2C%0Awhere%20light%20scanning%20is%20the%20bottleneck%20towards%20faster%20structured%20light%0Aoperation%2C%20our%20light%20scanning%20device%20is%20three%20orders%20of%20magnitude%20faster%20than%0Athe%20event%20camera%27s%20full-frame%20bandwidth%2C%20thus%20allowing%20us%20to%20take%20full%0Aadvantage%20of%20the%20event%20camera%27s%20fast%20operation.%20To%20surpass%20this%20bandwidth%2C%20we%0Aadditionally%20demonstrate%20adaptive%20scanning%20of%20only%20regions%20of%20interest%2C%20at%0Aspeeds%20an%20order%20of%20magnitude%20faster%20than%20the%20theoretical%20full-frame%20limit%20for%0Aevent%20cameras.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520light%2520with%2520a%2520million%2520light%2520planes%2520per%2520second%26entry.906535625%3DDhawal%2520Sirikonda%2520and%2520Praneeth%2520Chakravarthula%2520and%2520Ioannis%2520Gkioulekas%2520and%2520Adithya%2520Pediredla%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520structured%2520light%2520system%2520that%2520captures%2520full-frame%2520depth%2520at%250Arates%2520of%2520a%2520thousand%2520frames%2520per%2520second%252C%2520four%2520times%2520faster%2520than%2520the%2520previous%250Astate%2520of%2520the%2520art.%2520Our%2520key%2520innovation%2520to%2520this%2520end%2520is%2520the%2520design%2520of%2520an%250Aacousto-optic%2520light%2520scanning%2520device%2520that%2520can%2520scan%2520light%2520planes%2520at%2520rates%2520up%2520to%250Atwo%2520million%2520planes%2520per%2520second.%2520We%2520combine%2520this%2520device%2520with%2520an%2520event%2520camera%2520for%250Astructured%2520light%252C%2520using%2520the%2520sparse%2520events%2520triggered%2520on%2520the%2520camera%2520as%2520we%2520sweep%2520a%250Alight%2520plane%2520on%2520the%2520scene%2520for%2520depth%2520triangulation.%2520In%2520contrast%2520to%2520prior%2520work%252C%250Awhere%2520light%2520scanning%2520is%2520the%2520bottleneck%2520towards%2520faster%2520structured%2520light%250Aoperation%252C%2520our%2520light%2520scanning%2520device%2520is%2520three%2520orders%2520of%2520magnitude%2520faster%2520than%250Athe%2520event%2520camera%2527s%2520full-frame%2520bandwidth%252C%2520thus%2520allowing%2520us%2520to%2520take%2520full%250Aadvantage%2520of%2520the%2520event%2520camera%2527s%2520fast%2520operation.%2520To%2520surpass%2520this%2520bandwidth%252C%2520we%250Aadditionally%2520demonstrate%2520adaptive%2520scanning%2520of%2520only%2520regions%2520of%2520interest%252C%2520at%250Aspeeds%2520an%2520order%2520of%2520magnitude%2520faster%2520than%2520the%2520theoretical%2520full-frame%2520limit%2520for%250Aevent%2520cameras.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20light%20with%20a%20million%20light%20planes%20per%20second&entry.906535625=Dhawal%20Sirikonda%20and%20Praneeth%20Chakravarthula%20and%20Ioannis%20Gkioulekas%20and%20Adithya%20Pediredla&entry.1292438233=%20%20We%20introduce%20a%20structured%20light%20system%20that%20captures%20full-frame%20depth%20at%0Arates%20of%20a%20thousand%20frames%20per%20second%2C%20four%20times%20faster%20than%20the%20previous%0Astate%20of%20the%20art.%20Our%20key%20innovation%20to%20this%20end%20is%20the%20design%20of%20an%0Aacousto-optic%20light%20scanning%20device%20that%20can%20scan%20light%20planes%20at%20rates%20up%20to%0Atwo%20million%20planes%20per%20second.%20We%20combine%20this%20device%20with%20an%20event%20camera%20for%0Astructured%20light%2C%20using%20the%20sparse%20events%20triggered%20on%20the%20camera%20as%20we%20sweep%20a%0Alight%20plane%20on%20the%20scene%20for%20depth%20triangulation.%20In%20contrast%20to%20prior%20work%2C%0Awhere%20light%20scanning%20is%20the%20bottleneck%20towards%20faster%20structured%20light%0Aoperation%2C%20our%20light%20scanning%20device%20is%20three%20orders%20of%20magnitude%20faster%20than%0Athe%20event%20camera%27s%20full-frame%20bandwidth%2C%20thus%20allowing%20us%20to%20take%20full%0Aadvantage%20of%20the%20event%20camera%27s%20fast%20operation.%20To%20surpass%20this%20bandwidth%2C%20we%0Aadditionally%20demonstrate%20adaptive%20scanning%20of%20only%20regions%20of%20interest%2C%20at%0Aspeeds%20an%20order%20of%20magnitude%20faster%20than%20the%20theoretical%20full-frame%20limit%20for%0Aevent%20cameras.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18597v1&entry.124074799=Read"},
{"title": "Mixture of Experts in Image Classification: What's the Sweet Spot?", "author": "Mathurin Videau and Alessandro Leite and Marc Schoenauer and Olivier Teytaud", "abstract": "  Mixture-of-Experts (MoE) models have shown promising potential for\nparameter-efficient scaling across various domains. However, the implementation\nin computer vision remains limited, and often requires large-scale datasets\ncomprising billions of samples. In this study, we investigate the integration\nof MoE within computer vision models and explore various MoE configurations on\nopen datasets. When introducing MoE layers in image classification, the best\nresults are obtained for models with a moderate number of activated parameters\nper sample. However, such improvements gradually vanish when the number of\nparameters per sample increases.\n", "link": "http://arxiv.org/abs/2411.18322v1", "date": "2024-11-27", "relevancy": 2.4705, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.519}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4863}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Experts%20in%20Image%20Classification%3A%20What%27s%20the%20Sweet%20Spot%3F&body=Title%3A%20Mixture%20of%20Experts%20in%20Image%20Classification%3A%20What%27s%20the%20Sweet%20Spot%3F%0AAuthor%3A%20Mathurin%20Videau%20and%20Alessandro%20Leite%20and%20Marc%20Schoenauer%20and%20Olivier%20Teytaud%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20shown%20promising%20potential%20for%0Aparameter-efficient%20scaling%20across%20various%20domains.%20However%2C%20the%20implementation%0Ain%20computer%20vision%20remains%20limited%2C%20and%20often%20requires%20large-scale%20datasets%0Acomprising%20billions%20of%20samples.%20In%20this%20study%2C%20we%20investigate%20the%20integration%0Aof%20MoE%20within%20computer%20vision%20models%20and%20explore%20various%20MoE%20configurations%20on%0Aopen%20datasets.%20When%20introducing%20MoE%20layers%20in%20image%20classification%2C%20the%20best%0Aresults%20are%20obtained%20for%20models%20with%20a%20moderate%20number%20of%20activated%20parameters%0Aper%20sample.%20However%2C%20such%20improvements%20gradually%20vanish%20when%20the%20number%20of%0Aparameters%20per%20sample%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Experts%2520in%2520Image%2520Classification%253A%2520What%2527s%2520the%2520Sweet%2520Spot%253F%26entry.906535625%3DMathurin%2520Videau%2520and%2520Alessandro%2520Leite%2520and%2520Marc%2520Schoenauer%2520and%2520Olivier%2520Teytaud%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%2520have%2520shown%2520promising%2520potential%2520for%250Aparameter-efficient%2520scaling%2520across%2520various%2520domains.%2520However%252C%2520the%2520implementation%250Ain%2520computer%2520vision%2520remains%2520limited%252C%2520and%2520often%2520requires%2520large-scale%2520datasets%250Acomprising%2520billions%2520of%2520samples.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520integration%250Aof%2520MoE%2520within%2520computer%2520vision%2520models%2520and%2520explore%2520various%2520MoE%2520configurations%2520on%250Aopen%2520datasets.%2520When%2520introducing%2520MoE%2520layers%2520in%2520image%2520classification%252C%2520the%2520best%250Aresults%2520are%2520obtained%2520for%2520models%2520with%2520a%2520moderate%2520number%2520of%2520activated%2520parameters%250Aper%2520sample.%2520However%252C%2520such%2520improvements%2520gradually%2520vanish%2520when%2520the%2520number%2520of%250Aparameters%2520per%2520sample%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Experts%20in%20Image%20Classification%3A%20What%27s%20the%20Sweet%20Spot%3F&entry.906535625=Mathurin%20Videau%20and%20Alessandro%20Leite%20and%20Marc%20Schoenauer%20and%20Olivier%20Teytaud&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20models%20have%20shown%20promising%20potential%20for%0Aparameter-efficient%20scaling%20across%20various%20domains.%20However%2C%20the%20implementation%0Ain%20computer%20vision%20remains%20limited%2C%20and%20often%20requires%20large-scale%20datasets%0Acomprising%20billions%20of%20samples.%20In%20this%20study%2C%20we%20investigate%20the%20integration%0Aof%20MoE%20within%20computer%20vision%20models%20and%20explore%20various%20MoE%20configurations%20on%0Aopen%20datasets.%20When%20introducing%20MoE%20layers%20in%20image%20classification%2C%20the%20best%0Aresults%20are%20obtained%20for%20models%20with%20a%20moderate%20number%20of%20activated%20parameters%0Aper%20sample.%20However%2C%20such%20improvements%20gradually%20vanish%20when%20the%20number%20of%0Aparameters%20per%20sample%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18322v1&entry.124074799=Read"},
{"title": "Perturbation Ontology based Graph Attention Networks", "author": "Yichen Wang and Jie Wang and Fulin Wang and Xiang Li and Hao Yin and Bhiksha Raj", "abstract": "  In recent years, graph representation learning has undergone a paradigm\nshift, driven by the emergence and proliferation of graph neural networks\n(GNNs) and their heterogeneous counterparts. Heterogeneous GNNs have shown\nremarkable success in extracting low-dimensional embeddings from complex graphs\nthat encompass diverse entity types and relationships. While meta-path-based\ntechniques have long been recognized for their ability to capture semantic\naffinities among nodes, their dependence on manual specification poses a\nsignificant limitation. In contrast, matrix-focused methods accelerate\nprocessing by utilizing structural cues but often overlook contextual richness.\nIn this paper, we challenge the current paradigm by introducing ontology as a\nfundamental semantic primitive within complex graphs. Our goal is to integrate\nthe strengths of both matrix-centric and meta-path-based approaches into a\nunified framework. We propose perturbation Ontology-based Graph Attention\nNetworks (POGAT), a novel methodology that combines ontology subgraphs with an\nadvanced self-supervised learning paradigm to achieve a deep contextual\nunderstanding. The core innovation of POGAT lies in our enhanced homogeneous\nperturbing scheme designed to generate rigorous negative samples, encouraging\nthe model to explore minimal contextual features more thoroughly. Through\nextensive empirical evaluations, we demonstrate that POGAT significantly\noutperforms state-of-the-art baselines, achieving a groundbreaking improvement\nof up to 10.78\\% in F1-score for the critical task of link prediction and\n12.01\\% in Micro-F1 for the critical task of node classification.\n", "link": "http://arxiv.org/abs/2411.18520v1", "date": "2024-11-27", "relevancy": 2.4655, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perturbation%20Ontology%20based%20Graph%20Attention%20Networks&body=Title%3A%20Perturbation%20Ontology%20based%20Graph%20Attention%20Networks%0AAuthor%3A%20Yichen%20Wang%20and%20Jie%20Wang%20and%20Fulin%20Wang%20and%20Xiang%20Li%20and%20Hao%20Yin%20and%20Bhiksha%20Raj%0AAbstract%3A%20%20%20In%20recent%20years%2C%20graph%20representation%20learning%20has%20undergone%20a%20paradigm%0Ashift%2C%20driven%20by%20the%20emergence%20and%20proliferation%20of%20graph%20neural%20networks%0A%28GNNs%29%20and%20their%20heterogeneous%20counterparts.%20Heterogeneous%20GNNs%20have%20shown%0Aremarkable%20success%20in%20extracting%20low-dimensional%20embeddings%20from%20complex%20graphs%0Athat%20encompass%20diverse%20entity%20types%20and%20relationships.%20While%20meta-path-based%0Atechniques%20have%20long%20been%20recognized%20for%20their%20ability%20to%20capture%20semantic%0Aaffinities%20among%20nodes%2C%20their%20dependence%20on%20manual%20specification%20poses%20a%0Asignificant%20limitation.%20In%20contrast%2C%20matrix-focused%20methods%20accelerate%0Aprocessing%20by%20utilizing%20structural%20cues%20but%20often%20overlook%20contextual%20richness.%0AIn%20this%20paper%2C%20we%20challenge%20the%20current%20paradigm%20by%20introducing%20ontology%20as%20a%0Afundamental%20semantic%20primitive%20within%20complex%20graphs.%20Our%20goal%20is%20to%20integrate%0Athe%20strengths%20of%20both%20matrix-centric%20and%20meta-path-based%20approaches%20into%20a%0Aunified%20framework.%20We%20propose%20perturbation%20Ontology-based%20Graph%20Attention%0ANetworks%20%28POGAT%29%2C%20a%20novel%20methodology%20that%20combines%20ontology%20subgraphs%20with%20an%0Aadvanced%20self-supervised%20learning%20paradigm%20to%20achieve%20a%20deep%20contextual%0Aunderstanding.%20The%20core%20innovation%20of%20POGAT%20lies%20in%20our%20enhanced%20homogeneous%0Aperturbing%20scheme%20designed%20to%20generate%20rigorous%20negative%20samples%2C%20encouraging%0Athe%20model%20to%20explore%20minimal%20contextual%20features%20more%20thoroughly.%20Through%0Aextensive%20empirical%20evaluations%2C%20we%20demonstrate%20that%20POGAT%20significantly%0Aoutperforms%20state-of-the-art%20baselines%2C%20achieving%20a%20groundbreaking%20improvement%0Aof%20up%20to%2010.78%5C%25%20in%20F1-score%20for%20the%20critical%20task%20of%20link%20prediction%20and%0A12.01%5C%25%20in%20Micro-F1%20for%20the%20critical%20task%20of%20node%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerturbation%2520Ontology%2520based%2520Graph%2520Attention%2520Networks%26entry.906535625%3DYichen%2520Wang%2520and%2520Jie%2520Wang%2520and%2520Fulin%2520Wang%2520and%2520Xiang%2520Li%2520and%2520Hao%2520Yin%2520and%2520Bhiksha%2520Raj%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520graph%2520representation%2520learning%2520has%2520undergone%2520a%2520paradigm%250Ashift%252C%2520driven%2520by%2520the%2520emergence%2520and%2520proliferation%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520and%2520their%2520heterogeneous%2520counterparts.%2520Heterogeneous%2520GNNs%2520have%2520shown%250Aremarkable%2520success%2520in%2520extracting%2520low-dimensional%2520embeddings%2520from%2520complex%2520graphs%250Athat%2520encompass%2520diverse%2520entity%2520types%2520and%2520relationships.%2520While%2520meta-path-based%250Atechniques%2520have%2520long%2520been%2520recognized%2520for%2520their%2520ability%2520to%2520capture%2520semantic%250Aaffinities%2520among%2520nodes%252C%2520their%2520dependence%2520on%2520manual%2520specification%2520poses%2520a%250Asignificant%2520limitation.%2520In%2520contrast%252C%2520matrix-focused%2520methods%2520accelerate%250Aprocessing%2520by%2520utilizing%2520structural%2520cues%2520but%2520often%2520overlook%2520contextual%2520richness.%250AIn%2520this%2520paper%252C%2520we%2520challenge%2520the%2520current%2520paradigm%2520by%2520introducing%2520ontology%2520as%2520a%250Afundamental%2520semantic%2520primitive%2520within%2520complex%2520graphs.%2520Our%2520goal%2520is%2520to%2520integrate%250Athe%2520strengths%2520of%2520both%2520matrix-centric%2520and%2520meta-path-based%2520approaches%2520into%2520a%250Aunified%2520framework.%2520We%2520propose%2520perturbation%2520Ontology-based%2520Graph%2520Attention%250ANetworks%2520%2528POGAT%2529%252C%2520a%2520novel%2520methodology%2520that%2520combines%2520ontology%2520subgraphs%2520with%2520an%250Aadvanced%2520self-supervised%2520learning%2520paradigm%2520to%2520achieve%2520a%2520deep%2520contextual%250Aunderstanding.%2520The%2520core%2520innovation%2520of%2520POGAT%2520lies%2520in%2520our%2520enhanced%2520homogeneous%250Aperturbing%2520scheme%2520designed%2520to%2520generate%2520rigorous%2520negative%2520samples%252C%2520encouraging%250Athe%2520model%2520to%2520explore%2520minimal%2520contextual%2520features%2520more%2520thoroughly.%2520Through%250Aextensive%2520empirical%2520evaluations%252C%2520we%2520demonstrate%2520that%2520POGAT%2520significantly%250Aoutperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520a%2520groundbreaking%2520improvement%250Aof%2520up%2520to%252010.78%255C%2525%2520in%2520F1-score%2520for%2520the%2520critical%2520task%2520of%2520link%2520prediction%2520and%250A12.01%255C%2525%2520in%2520Micro-F1%2520for%2520the%2520critical%2520task%2520of%2520node%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perturbation%20Ontology%20based%20Graph%20Attention%20Networks&entry.906535625=Yichen%20Wang%20and%20Jie%20Wang%20and%20Fulin%20Wang%20and%20Xiang%20Li%20and%20Hao%20Yin%20and%20Bhiksha%20Raj&entry.1292438233=%20%20In%20recent%20years%2C%20graph%20representation%20learning%20has%20undergone%20a%20paradigm%0Ashift%2C%20driven%20by%20the%20emergence%20and%20proliferation%20of%20graph%20neural%20networks%0A%28GNNs%29%20and%20their%20heterogeneous%20counterparts.%20Heterogeneous%20GNNs%20have%20shown%0Aremarkable%20success%20in%20extracting%20low-dimensional%20embeddings%20from%20complex%20graphs%0Athat%20encompass%20diverse%20entity%20types%20and%20relationships.%20While%20meta-path-based%0Atechniques%20have%20long%20been%20recognized%20for%20their%20ability%20to%20capture%20semantic%0Aaffinities%20among%20nodes%2C%20their%20dependence%20on%20manual%20specification%20poses%20a%0Asignificant%20limitation.%20In%20contrast%2C%20matrix-focused%20methods%20accelerate%0Aprocessing%20by%20utilizing%20structural%20cues%20but%20often%20overlook%20contextual%20richness.%0AIn%20this%20paper%2C%20we%20challenge%20the%20current%20paradigm%20by%20introducing%20ontology%20as%20a%0Afundamental%20semantic%20primitive%20within%20complex%20graphs.%20Our%20goal%20is%20to%20integrate%0Athe%20strengths%20of%20both%20matrix-centric%20and%20meta-path-based%20approaches%20into%20a%0Aunified%20framework.%20We%20propose%20perturbation%20Ontology-based%20Graph%20Attention%0ANetworks%20%28POGAT%29%2C%20a%20novel%20methodology%20that%20combines%20ontology%20subgraphs%20with%20an%0Aadvanced%20self-supervised%20learning%20paradigm%20to%20achieve%20a%20deep%20contextual%0Aunderstanding.%20The%20core%20innovation%20of%20POGAT%20lies%20in%20our%20enhanced%20homogeneous%0Aperturbing%20scheme%20designed%20to%20generate%20rigorous%20negative%20samples%2C%20encouraging%0Athe%20model%20to%20explore%20minimal%20contextual%20features%20more%20thoroughly.%20Through%0Aextensive%20empirical%20evaluations%2C%20we%20demonstrate%20that%20POGAT%20significantly%0Aoutperforms%20state-of-the-art%20baselines%2C%20achieving%20a%20groundbreaking%20improvement%0Aof%20up%20to%2010.78%5C%25%20in%20F1-score%20for%20the%20critical%20task%20of%20link%20prediction%20and%0A12.01%5C%25%20in%20Micro-F1%20for%20the%20critical%20task%20of%20node%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18520v1&entry.124074799=Read"},
{"title": "CASCRNet: An Atrous Spatial Pyramid Pooling and Shared Channel Residual\n  based Network for Capsule Endoscopy", "author": "K V Srinanda and M Manvith Prabhu and Shyam Lal", "abstract": "  This manuscript summarizes work on the Capsule Vision Challenge 2024 by\nMISAHUB. To address the multi-class disease classification task, which is\nchallenging due to the complexity and imbalance in the Capsule Vision challenge\ndataset, this paper proposes CASCRNet (Capsule endoscopy-Aspp-SCR-Network), a\nparameter-efficient and novel model that uses Shared Channel Residual (SCR)\nblocks and Atrous Spatial Pyramid Pooling (ASPP) blocks. Further, the\nperformance of the proposed model is compared with other well-known approaches.\nThe experimental results yield that proposed model provides better disease\nclassification results. The proposed model was successful in classifying\ndiseases with an F1 Score of 78.5% and a Mean AUC of 98.3%, which is promising\ngiven its compact architecture.\n", "link": "http://arxiv.org/abs/2410.17863v2", "date": "2024-11-27", "relevancy": 2.4498, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4948}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy&body=Title%3A%20CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy%0AAuthor%3A%20K%20V%20Srinanda%20and%20M%20Manvith%20Prabhu%20and%20Shyam%20Lal%0AAbstract%3A%20%20%20This%20manuscript%20summarizes%20work%20on%20the%20Capsule%20Vision%20Challenge%202024%20by%0AMISAHUB.%20To%20address%20the%20multi-class%20disease%20classification%20task%2C%20which%20is%0Achallenging%20due%20to%20the%20complexity%20and%20imbalance%20in%20the%20Capsule%20Vision%20challenge%0Adataset%2C%20this%20paper%20proposes%20CASCRNet%20%28Capsule%20endoscopy-Aspp-SCR-Network%29%2C%20a%0Aparameter-efficient%20and%20novel%20model%20that%20uses%20Shared%20Channel%20Residual%20%28SCR%29%0Ablocks%20and%20Atrous%20Spatial%20Pyramid%20Pooling%20%28ASPP%29%20blocks.%20Further%2C%20the%0Aperformance%20of%20the%20proposed%20model%20is%20compared%20with%20other%20well-known%20approaches.%0AThe%20experimental%20results%20yield%20that%20proposed%20model%20provides%20better%20disease%0Aclassification%20results.%20The%20proposed%20model%20was%20successful%20in%20classifying%0Adiseases%20with%20an%20F1%20Score%20of%2078.5%25%20and%20a%20Mean%20AUC%20of%2098.3%25%2C%20which%20is%20promising%0Agiven%20its%20compact%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCASCRNet%253A%2520An%2520Atrous%2520Spatial%2520Pyramid%2520Pooling%2520and%2520Shared%2520Channel%2520Residual%250A%2520%2520based%2520Network%2520for%2520Capsule%2520Endoscopy%26entry.906535625%3DK%2520V%2520Srinanda%2520and%2520M%2520Manvith%2520Prabhu%2520and%2520Shyam%2520Lal%26entry.1292438233%3D%2520%2520This%2520manuscript%2520summarizes%2520work%2520on%2520the%2520Capsule%2520Vision%2520Challenge%25202024%2520by%250AMISAHUB.%2520To%2520address%2520the%2520multi-class%2520disease%2520classification%2520task%252C%2520which%2520is%250Achallenging%2520due%2520to%2520the%2520complexity%2520and%2520imbalance%2520in%2520the%2520Capsule%2520Vision%2520challenge%250Adataset%252C%2520this%2520paper%2520proposes%2520CASCRNet%2520%2528Capsule%2520endoscopy-Aspp-SCR-Network%2529%252C%2520a%250Aparameter-efficient%2520and%2520novel%2520model%2520that%2520uses%2520Shared%2520Channel%2520Residual%2520%2528SCR%2529%250Ablocks%2520and%2520Atrous%2520Spatial%2520Pyramid%2520Pooling%2520%2528ASPP%2529%2520blocks.%2520Further%252C%2520the%250Aperformance%2520of%2520the%2520proposed%2520model%2520is%2520compared%2520with%2520other%2520well-known%2520approaches.%250AThe%2520experimental%2520results%2520yield%2520that%2520proposed%2520model%2520provides%2520better%2520disease%250Aclassification%2520results.%2520The%2520proposed%2520model%2520was%2520successful%2520in%2520classifying%250Adiseases%2520with%2520an%2520F1%2520Score%2520of%252078.5%2525%2520and%2520a%2520Mean%2520AUC%2520of%252098.3%2525%252C%2520which%2520is%2520promising%250Agiven%2520its%2520compact%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy&entry.906535625=K%20V%20Srinanda%20and%20M%20Manvith%20Prabhu%20and%20Shyam%20Lal&entry.1292438233=%20%20This%20manuscript%20summarizes%20work%20on%20the%20Capsule%20Vision%20Challenge%202024%20by%0AMISAHUB.%20To%20address%20the%20multi-class%20disease%20classification%20task%2C%20which%20is%0Achallenging%20due%20to%20the%20complexity%20and%20imbalance%20in%20the%20Capsule%20Vision%20challenge%0Adataset%2C%20this%20paper%20proposes%20CASCRNet%20%28Capsule%20endoscopy-Aspp-SCR-Network%29%2C%20a%0Aparameter-efficient%20and%20novel%20model%20that%20uses%20Shared%20Channel%20Residual%20%28SCR%29%0Ablocks%20and%20Atrous%20Spatial%20Pyramid%20Pooling%20%28ASPP%29%20blocks.%20Further%2C%20the%0Aperformance%20of%20the%20proposed%20model%20is%20compared%20with%20other%20well-known%20approaches.%0AThe%20experimental%20results%20yield%20that%20proposed%20model%20provides%20better%20disease%0Aclassification%20results.%20The%20proposed%20model%20was%20successful%20in%20classifying%0Adiseases%20with%20an%20F1%20Score%20of%2078.5%25%20and%20a%20Mean%20AUC%20of%2098.3%25%2C%20which%20is%20promising%0Agiven%20its%20compact%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17863v2&entry.124074799=Read"},
{"title": "Aligning Pre-trained Models for Spoken Language Translation", "author": "\u0160imon Sedl\u00e1\u010dek and Santosh Kesiraju and Alexander Polok and Jan \u010cernock\u00fd", "abstract": "  This paper investigates a novel approach to end-to-end speech translation\n(ST) based on aligning frozen pre-trained automatic speech recognition (ASR)\nand machine translation (MT) models via a small connector module (Q-Former, our\nSubsampler-Transformer Encoder). This connector bridges the gap between the\nspeech and text modalities, transforming ASR encoder embeddings into the latent\nrepresentation space of the MT encoder while being the only part of the system\noptimized during training. Experiments are conducted on the How2\nEnglish-Portuguese dataset as we investigate the alignment approach in a\nsmall-scale scenario focusing on ST. While keeping the size of the connector\nmodule constant and small in comparison ( < 5% of the size of the larger\naligned models), increasing the size and capability of the foundation ASR and\nMT models universally improves translation results. We also find that the\nconnectors can serve as domain adapters for the foundation MT models,\nsignificantly improving translation performance in the aligned ST setting. We\nconclude that this approach represents a viable and scalable approach to\ntraining end-to-end ST systems.\n", "link": "http://arxiv.org/abs/2411.18294v1", "date": "2024-11-27", "relevancy": 2.4378, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Pre-trained%20Models%20for%20Spoken%20Language%20Translation&body=Title%3A%20Aligning%20Pre-trained%20Models%20for%20Spoken%20Language%20Translation%0AAuthor%3A%20%C5%A0imon%20Sedl%C3%A1%C4%8Dek%20and%20Santosh%20Kesiraju%20and%20Alexander%20Polok%20and%20Jan%20%C4%8Cernock%C3%BD%0AAbstract%3A%20%20%20This%20paper%20investigates%20a%20novel%20approach%20to%20end-to-end%20speech%20translation%0A%28ST%29%20based%20on%20aligning%20frozen%20pre-trained%20automatic%20speech%20recognition%20%28ASR%29%0Aand%20machine%20translation%20%28MT%29%20models%20via%20a%20small%20connector%20module%20%28Q-Former%2C%20our%0ASubsampler-Transformer%20Encoder%29.%20This%20connector%20bridges%20the%20gap%20between%20the%0Aspeech%20and%20text%20modalities%2C%20transforming%20ASR%20encoder%20embeddings%20into%20the%20latent%0Arepresentation%20space%20of%20the%20MT%20encoder%20while%20being%20the%20only%20part%20of%20the%20system%0Aoptimized%20during%20training.%20Experiments%20are%20conducted%20on%20the%20How2%0AEnglish-Portuguese%20dataset%20as%20we%20investigate%20the%20alignment%20approach%20in%20a%0Asmall-scale%20scenario%20focusing%20on%20ST.%20While%20keeping%20the%20size%20of%20the%20connector%0Amodule%20constant%20and%20small%20in%20comparison%20%28%20%3C%205%25%20of%20the%20size%20of%20the%20larger%0Aaligned%20models%29%2C%20increasing%20the%20size%20and%20capability%20of%20the%20foundation%20ASR%20and%0AMT%20models%20universally%20improves%20translation%20results.%20We%20also%20find%20that%20the%0Aconnectors%20can%20serve%20as%20domain%20adapters%20for%20the%20foundation%20MT%20models%2C%0Asignificantly%20improving%20translation%20performance%20in%20the%20aligned%20ST%20setting.%20We%0Aconclude%20that%20this%20approach%20represents%20a%20viable%20and%20scalable%20approach%20to%0Atraining%20end-to-end%20ST%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Pre-trained%2520Models%2520for%2520Spoken%2520Language%2520Translation%26entry.906535625%3D%25C5%25A0imon%2520Sedl%25C3%25A1%25C4%258Dek%2520and%2520Santosh%2520Kesiraju%2520and%2520Alexander%2520Polok%2520and%2520Jan%2520%25C4%258Cernock%25C3%25BD%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520a%2520novel%2520approach%2520to%2520end-to-end%2520speech%2520translation%250A%2528ST%2529%2520based%2520on%2520aligning%2520frozen%2520pre-trained%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%250Aand%2520machine%2520translation%2520%2528MT%2529%2520models%2520via%2520a%2520small%2520connector%2520module%2520%2528Q-Former%252C%2520our%250ASubsampler-Transformer%2520Encoder%2529.%2520This%2520connector%2520bridges%2520the%2520gap%2520between%2520the%250Aspeech%2520and%2520text%2520modalities%252C%2520transforming%2520ASR%2520encoder%2520embeddings%2520into%2520the%2520latent%250Arepresentation%2520space%2520of%2520the%2520MT%2520encoder%2520while%2520being%2520the%2520only%2520part%2520of%2520the%2520system%250Aoptimized%2520during%2520training.%2520Experiments%2520are%2520conducted%2520on%2520the%2520How2%250AEnglish-Portuguese%2520dataset%2520as%2520we%2520investigate%2520the%2520alignment%2520approach%2520in%2520a%250Asmall-scale%2520scenario%2520focusing%2520on%2520ST.%2520While%2520keeping%2520the%2520size%2520of%2520the%2520connector%250Amodule%2520constant%2520and%2520small%2520in%2520comparison%2520%2528%2520%253C%25205%2525%2520of%2520the%2520size%2520of%2520the%2520larger%250Aaligned%2520models%2529%252C%2520increasing%2520the%2520size%2520and%2520capability%2520of%2520the%2520foundation%2520ASR%2520and%250AMT%2520models%2520universally%2520improves%2520translation%2520results.%2520We%2520also%2520find%2520that%2520the%250Aconnectors%2520can%2520serve%2520as%2520domain%2520adapters%2520for%2520the%2520foundation%2520MT%2520models%252C%250Asignificantly%2520improving%2520translation%2520performance%2520in%2520the%2520aligned%2520ST%2520setting.%2520We%250Aconclude%2520that%2520this%2520approach%2520represents%2520a%2520viable%2520and%2520scalable%2520approach%2520to%250Atraining%2520end-to-end%2520ST%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Pre-trained%20Models%20for%20Spoken%20Language%20Translation&entry.906535625=%C5%A0imon%20Sedl%C3%A1%C4%8Dek%20and%20Santosh%20Kesiraju%20and%20Alexander%20Polok%20and%20Jan%20%C4%8Cernock%C3%BD&entry.1292438233=%20%20This%20paper%20investigates%20a%20novel%20approach%20to%20end-to-end%20speech%20translation%0A%28ST%29%20based%20on%20aligning%20frozen%20pre-trained%20automatic%20speech%20recognition%20%28ASR%29%0Aand%20machine%20translation%20%28MT%29%20models%20via%20a%20small%20connector%20module%20%28Q-Former%2C%20our%0ASubsampler-Transformer%20Encoder%29.%20This%20connector%20bridges%20the%20gap%20between%20the%0Aspeech%20and%20text%20modalities%2C%20transforming%20ASR%20encoder%20embeddings%20into%20the%20latent%0Arepresentation%20space%20of%20the%20MT%20encoder%20while%20being%20the%20only%20part%20of%20the%20system%0Aoptimized%20during%20training.%20Experiments%20are%20conducted%20on%20the%20How2%0AEnglish-Portuguese%20dataset%20as%20we%20investigate%20the%20alignment%20approach%20in%20a%0Asmall-scale%20scenario%20focusing%20on%20ST.%20While%20keeping%20the%20size%20of%20the%20connector%0Amodule%20constant%20and%20small%20in%20comparison%20%28%20%3C%205%25%20of%20the%20size%20of%20the%20larger%0Aaligned%20models%29%2C%20increasing%20the%20size%20and%20capability%20of%20the%20foundation%20ASR%20and%0AMT%20models%20universally%20improves%20translation%20results.%20We%20also%20find%20that%20the%0Aconnectors%20can%20serve%20as%20domain%20adapters%20for%20the%20foundation%20MT%20models%2C%0Asignificantly%20improving%20translation%20performance%20in%20the%20aligned%20ST%20setting.%20We%0Aconclude%20that%20this%20approach%20represents%20a%20viable%20and%20scalable%20approach%20to%0Atraining%20end-to-end%20ST%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18294v1&entry.124074799=Read"},
{"title": "Unveiling the optimization process of Physics Informed Neural Networks:\n  How accurate and competitive can PINNs be?", "author": "Jorge F. Urb\u00e1n and Petros Stefanou and Jos\u00e9 A. Pons", "abstract": "  This study investigates the potential accuracy boundaries of physics-informed\nneural networks, contrasting their approach with previous similar works and\ntraditional numerical methods. We find that selecting improved optimization\nalgorithms significantly enhances the accuracy of the results. Simple\nmodifications to the loss function may also improve precision, offering an\nadditional avenue for enhancement. Despite optimization algorithms having a\ngreater impact on convergence than adjustments to the loss function, practical\nconsiderations often favor tweaking the latter due to ease of implementation.\nOn a global scale, the integration of an enhanced optimizer and a marginally\nadjusted loss function enables a reduction in the loss function by several\norders of magnitude across diverse physical problems. Consequently, our results\nobtained using compact networks (typically comprising 2 or 3 layers of 20-30\nneurons) achieve accuracies comparable to finite difference schemes employing\nthousands of grid points. This study encourages the continued advancement of\nPINNs and associated optimization techniques for broader applications across\nvarious fields.\n", "link": "http://arxiv.org/abs/2405.04230v2", "date": "2024-11-27", "relevancy": 2.4185, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&body=Title%3A%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F%0AAuthor%3A%20Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04230v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520optimization%2520process%2520of%2520Physics%2520Informed%2520Neural%2520Networks%253A%250A%2520%2520How%2520accurate%2520and%2520competitive%2520can%2520PINNs%2520be%253F%26entry.906535625%3DJorge%2520F.%2520Urb%25C3%25A1n%2520and%2520Petros%2520Stefanou%2520and%2520Jos%25C3%25A9%2520A.%2520Pons%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520potential%2520accuracy%2520boundaries%2520of%2520physics-informed%250Aneural%2520networks%252C%2520contrasting%2520their%2520approach%2520with%2520previous%2520similar%2520works%2520and%250Atraditional%2520numerical%2520methods.%2520We%2520find%2520that%2520selecting%2520improved%2520optimization%250Aalgorithms%2520significantly%2520enhances%2520the%2520accuracy%2520of%2520the%2520results.%2520Simple%250Amodifications%2520to%2520the%2520loss%2520function%2520may%2520also%2520improve%2520precision%252C%2520offering%2520an%250Aadditional%2520avenue%2520for%2520enhancement.%2520Despite%2520optimization%2520algorithms%2520having%2520a%250Agreater%2520impact%2520on%2520convergence%2520than%2520adjustments%2520to%2520the%2520loss%2520function%252C%2520practical%250Aconsiderations%2520often%2520favor%2520tweaking%2520the%2520latter%2520due%2520to%2520ease%2520of%2520implementation.%250AOn%2520a%2520global%2520scale%252C%2520the%2520integration%2520of%2520an%2520enhanced%2520optimizer%2520and%2520a%2520marginally%250Aadjusted%2520loss%2520function%2520enables%2520a%2520reduction%2520in%2520the%2520loss%2520function%2520by%2520several%250Aorders%2520of%2520magnitude%2520across%2520diverse%2520physical%2520problems.%2520Consequently%252C%2520our%2520results%250Aobtained%2520using%2520compact%2520networks%2520%2528typically%2520comprising%25202%2520or%25203%2520layers%2520of%252020-30%250Aneurons%2529%2520achieve%2520accuracies%2520comparable%2520to%2520finite%2520difference%2520schemes%2520employing%250Athousands%2520of%2520grid%2520points.%2520This%2520study%2520encourages%2520the%2520continued%2520advancement%2520of%250APINNs%2520and%2520associated%2520optimization%2520techniques%2520for%2520broader%2520applications%2520across%250Avarious%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04230v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&entry.906535625=Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons&entry.1292438233=%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04230v2&entry.124074799=Read"},
{"title": "Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for\n  Robust 3D Robotic Manipulation", "author": "Yueru Jia and Jiaming Liu and Sixiang Chen and Chenyang Gu and Zhilue Wang and Longzan Luo and Lily Lee and Pengwei Wang and Zhongyuan Wang and Renrui Zhang and Shanghang Zhang", "abstract": "  3D geometric information is essential for manipulation tasks, as robots need\nto perceive the 3D environment, reason about spatial relationships, and\ninteract with intricate spatial configurations. Recent research has\nincreasingly focused on the explicit extraction of 3D features, while still\nfacing challenges such as the lack of large-scale robotic 3D data and the\npotential loss of spatial geometry. To address these limitations, we propose\nthe Lift3D framework, which progressively enhances 2D foundation models with\nimplicit and explicit 3D robotic representations to construct a robust 3D\nmanipulation policy. Specifically, we first design a task-aware masked\nautoencoder that masks task-relevant affordance patches and reconstructs depth\ninformation, enhancing the 2D foundation model's implicit 3D robotic\nrepresentation. After self-supervised fine-tuning, we introduce a 2D\nmodel-lifting strategy that establishes a positional mapping between the input\n3D points and the positional embeddings of the 2D model. Based on the mapping,\nLift3D utilizes the 2D foundation model to directly encode point cloud data,\nleveraging large-scale pretrained knowledge to construct explicit 3D robotic\nrepresentations while minimizing spatial information loss. In experiments,\nLift3D consistently outperforms previous state-of-the-art methods across\nseveral simulation benchmarks and real-world scenarios.\n", "link": "http://arxiv.org/abs/2411.18623v1", "date": "2024-11-27", "relevancy": 2.3917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lift3D%20Foundation%20Policy%3A%20Lifting%202D%20Large-Scale%20Pretrained%20Models%20for%0A%20%20Robust%203D%20Robotic%20Manipulation&body=Title%3A%20Lift3D%20Foundation%20Policy%3A%20Lifting%202D%20Large-Scale%20Pretrained%20Models%20for%0A%20%20Robust%203D%20Robotic%20Manipulation%0AAuthor%3A%20Yueru%20Jia%20and%20Jiaming%20Liu%20and%20Sixiang%20Chen%20and%20Chenyang%20Gu%20and%20Zhilue%20Wang%20and%20Longzan%20Luo%20and%20Lily%20Lee%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Renrui%20Zhang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%203D%20geometric%20information%20is%20essential%20for%20manipulation%20tasks%2C%20as%20robots%20need%0Ato%20perceive%20the%203D%20environment%2C%20reason%20about%20spatial%20relationships%2C%20and%0Ainteract%20with%20intricate%20spatial%20configurations.%20Recent%20research%20has%0Aincreasingly%20focused%20on%20the%20explicit%20extraction%20of%203D%20features%2C%20while%20still%0Afacing%20challenges%20such%20as%20the%20lack%20of%20large-scale%20robotic%203D%20data%20and%20the%0Apotential%20loss%20of%20spatial%20geometry.%20To%20address%20these%20limitations%2C%20we%20propose%0Athe%20Lift3D%20framework%2C%20which%20progressively%20enhances%202D%20foundation%20models%20with%0Aimplicit%20and%20explicit%203D%20robotic%20representations%20to%20construct%20a%20robust%203D%0Amanipulation%20policy.%20Specifically%2C%20we%20first%20design%20a%20task-aware%20masked%0Aautoencoder%20that%20masks%20task-relevant%20affordance%20patches%20and%20reconstructs%20depth%0Ainformation%2C%20enhancing%20the%202D%20foundation%20model%27s%20implicit%203D%20robotic%0Arepresentation.%20After%20self-supervised%20fine-tuning%2C%20we%20introduce%20a%202D%0Amodel-lifting%20strategy%20that%20establishes%20a%20positional%20mapping%20between%20the%20input%0A3D%20points%20and%20the%20positional%20embeddings%20of%20the%202D%20model.%20Based%20on%20the%20mapping%2C%0ALift3D%20utilizes%20the%202D%20foundation%20model%20to%20directly%20encode%20point%20cloud%20data%2C%0Aleveraging%20large-scale%20pretrained%20knowledge%20to%20construct%20explicit%203D%20robotic%0Arepresentations%20while%20minimizing%20spatial%20information%20loss.%20In%20experiments%2C%0ALift3D%20consistently%20outperforms%20previous%20state-of-the-art%20methods%20across%0Aseveral%20simulation%20benchmarks%20and%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLift3D%2520Foundation%2520Policy%253A%2520Lifting%25202D%2520Large-Scale%2520Pretrained%2520Models%2520for%250A%2520%2520Robust%25203D%2520Robotic%2520Manipulation%26entry.906535625%3DYueru%2520Jia%2520and%2520Jiaming%2520Liu%2520and%2520Sixiang%2520Chen%2520and%2520Chenyang%2520Gu%2520and%2520Zhilue%2520Wang%2520and%2520Longzan%2520Luo%2520and%2520Lily%2520Lee%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Renrui%2520Zhang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%25203D%2520geometric%2520information%2520is%2520essential%2520for%2520manipulation%2520tasks%252C%2520as%2520robots%2520need%250Ato%2520perceive%2520the%25203D%2520environment%252C%2520reason%2520about%2520spatial%2520relationships%252C%2520and%250Ainteract%2520with%2520intricate%2520spatial%2520configurations.%2520Recent%2520research%2520has%250Aincreasingly%2520focused%2520on%2520the%2520explicit%2520extraction%2520of%25203D%2520features%252C%2520while%2520still%250Afacing%2520challenges%2520such%2520as%2520the%2520lack%2520of%2520large-scale%2520robotic%25203D%2520data%2520and%2520the%250Apotential%2520loss%2520of%2520spatial%2520geometry.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250Athe%2520Lift3D%2520framework%252C%2520which%2520progressively%2520enhances%25202D%2520foundation%2520models%2520with%250Aimplicit%2520and%2520explicit%25203D%2520robotic%2520representations%2520to%2520construct%2520a%2520robust%25203D%250Amanipulation%2520policy.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520task-aware%2520masked%250Aautoencoder%2520that%2520masks%2520task-relevant%2520affordance%2520patches%2520and%2520reconstructs%2520depth%250Ainformation%252C%2520enhancing%2520the%25202D%2520foundation%2520model%2527s%2520implicit%25203D%2520robotic%250Arepresentation.%2520After%2520self-supervised%2520fine-tuning%252C%2520we%2520introduce%2520a%25202D%250Amodel-lifting%2520strategy%2520that%2520establishes%2520a%2520positional%2520mapping%2520between%2520the%2520input%250A3D%2520points%2520and%2520the%2520positional%2520embeddings%2520of%2520the%25202D%2520model.%2520Based%2520on%2520the%2520mapping%252C%250ALift3D%2520utilizes%2520the%25202D%2520foundation%2520model%2520to%2520directly%2520encode%2520point%2520cloud%2520data%252C%250Aleveraging%2520large-scale%2520pretrained%2520knowledge%2520to%2520construct%2520explicit%25203D%2520robotic%250Arepresentations%2520while%2520minimizing%2520spatial%2520information%2520loss.%2520In%2520experiments%252C%250ALift3D%2520consistently%2520outperforms%2520previous%2520state-of-the-art%2520methods%2520across%250Aseveral%2520simulation%2520benchmarks%2520and%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lift3D%20Foundation%20Policy%3A%20Lifting%202D%20Large-Scale%20Pretrained%20Models%20for%0A%20%20Robust%203D%20Robotic%20Manipulation&entry.906535625=Yueru%20Jia%20and%20Jiaming%20Liu%20and%20Sixiang%20Chen%20and%20Chenyang%20Gu%20and%20Zhilue%20Wang%20and%20Longzan%20Luo%20and%20Lily%20Lee%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Renrui%20Zhang%20and%20Shanghang%20Zhang&entry.1292438233=%20%203D%20geometric%20information%20is%20essential%20for%20manipulation%20tasks%2C%20as%20robots%20need%0Ato%20perceive%20the%203D%20environment%2C%20reason%20about%20spatial%20relationships%2C%20and%0Ainteract%20with%20intricate%20spatial%20configurations.%20Recent%20research%20has%0Aincreasingly%20focused%20on%20the%20explicit%20extraction%20of%203D%20features%2C%20while%20still%0Afacing%20challenges%20such%20as%20the%20lack%20of%20large-scale%20robotic%203D%20data%20and%20the%0Apotential%20loss%20of%20spatial%20geometry.%20To%20address%20these%20limitations%2C%20we%20propose%0Athe%20Lift3D%20framework%2C%20which%20progressively%20enhances%202D%20foundation%20models%20with%0Aimplicit%20and%20explicit%203D%20robotic%20representations%20to%20construct%20a%20robust%203D%0Amanipulation%20policy.%20Specifically%2C%20we%20first%20design%20a%20task-aware%20masked%0Aautoencoder%20that%20masks%20task-relevant%20affordance%20patches%20and%20reconstructs%20depth%0Ainformation%2C%20enhancing%20the%202D%20foundation%20model%27s%20implicit%203D%20robotic%0Arepresentation.%20After%20self-supervised%20fine-tuning%2C%20we%20introduce%20a%202D%0Amodel-lifting%20strategy%20that%20establishes%20a%20positional%20mapping%20between%20the%20input%0A3D%20points%20and%20the%20positional%20embeddings%20of%20the%202D%20model.%20Based%20on%20the%20mapping%2C%0ALift3D%20utilizes%20the%202D%20foundation%20model%20to%20directly%20encode%20point%20cloud%20data%2C%0Aleveraging%20large-scale%20pretrained%20knowledge%20to%20construct%20explicit%203D%20robotic%0Arepresentations%20while%20minimizing%20spatial%20information%20loss.%20In%20experiments%2C%0ALift3D%20consistently%20outperforms%20previous%20state-of-the-art%20methods%20across%0Aseveral%20simulation%20benchmarks%20and%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18623v1&entry.124074799=Read"},
{"title": "On Designing Effective RL Reward at Training Time for LLM Reasoning", "author": "Jiaxuan Gao and Shusheng Xu and Wenjie Ye and Weilin Liu and Chuyi He and Wei Fu and Zhiyu Mei and Guangju Wang and Yi Wu", "abstract": "  Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.\n", "link": "http://arxiv.org/abs/2410.15115v3", "date": "2024-11-27", "relevancy": 2.3906, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning&body=Title%3A%20On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning%0AAuthor%3A%20Jiaxuan%20Gao%20and%20Shusheng%20Xu%20and%20Wenjie%20Ye%20and%20Weilin%20Liu%20and%20Chuyi%20He%20and%20Wei%20Fu%20and%20Zhiyu%20Mei%20and%20Guangju%20Wang%20and%20Yi%20Wu%0AAbstract%3A%20%20%20Reward%20models%20have%20been%20increasingly%20critical%20for%20improving%20the%20reasoning%0Acapability%20of%20LLMs.%20Existing%20research%20has%20shown%20that%20a%20well-trained%20reward%0Amodel%20can%20substantially%20improve%20model%20performances%20at%20inference%20time%20via%0Asearch.%20However%2C%20the%20potential%20of%20reward%20models%20during%20RL%20training%20time%20still%0Aremains%20largely%20under-explored.%20It%20is%20currently%20unclear%20whether%20these%20reward%0Amodels%20can%20provide%20additional%20training%20signals%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20in%20RL%20training%20that%20uses%20sparse%20success%20rewards%2C%20which%0Averify%20the%20correctness%20of%20solutions.%20In%20this%20work%2C%20we%20evaluate%20popular%20reward%0Amodels%20for%20RL%20training%2C%20including%20the%20Outcome-supervised%20Reward%20Model%20%28ORM%29%20and%0Athe%20Process-supervised%20Reward%20Model%20%28PRM%29%2C%20and%20train%20a%20collection%20of%20LLMs%20for%0Amath%20problems%20using%20RL%20by%20combining%20these%20learned%20rewards%20with%20success%20rewards.%0ASurprisingly%2C%20even%20though%20these%20learned%20reward%20models%20have%20strong%0Ainference-time%20performances%2C%20they%20may%20NOT%20help%20or%20even%20hurt%20RL%20training%2C%0Aproducing%20worse%20performances%20than%20LLMs%20trained%20with%20the%20success%20reward%20only.%0AOur%20analysis%20reveals%20that%20an%20LLM%20can%20receive%20high%20rewards%20from%20some%20of%20these%0Areward%20models%20by%20repeating%20correct%20but%20unnecessary%20reasoning%20steps%2C%20leading%20to%0Aa%20severe%20reward%20hacking%20issue.%20Therefore%2C%20we%20introduce%20two%20novel%20reward%0Arefinement%20techniques%2C%20including%20Clipping%20and%20Delta.%20The%20key%20idea%20is%20to%20ensure%0Athe%20accumulative%20reward%20of%20any%20reasoning%20trajectory%20is%20upper-bounded%20to%20keep%20a%0Alearned%20reward%20model%20effective%20without%20being%20exploited.%20We%20evaluate%20our%0Atechniques%20with%20multiple%20reward%20models%20over%20a%20set%20of%201.5B%20and%207B%20LLMs%20on%20MATH%0Aand%20GSM8K%20benchmarks%20and%20demonstrate%20that%20with%20a%20carefully%20designed%20reward%0Afunction%2C%20RL%20training%20without%20any%20additional%20supervised%20tuning%20can%20improve%20all%0Athe%20evaluated%20LLMs%2C%20including%20the%20state-of-the-art%207B%20LLM%0AQwen2.5-Math-7B-Instruct%20on%20MATH%20and%20GSM8K%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15115v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Designing%2520Effective%2520RL%2520Reward%2520at%2520Training%2520Time%2520for%2520LLM%2520Reasoning%26entry.906535625%3DJiaxuan%2520Gao%2520and%2520Shusheng%2520Xu%2520and%2520Wenjie%2520Ye%2520and%2520Weilin%2520Liu%2520and%2520Chuyi%2520He%2520and%2520Wei%2520Fu%2520and%2520Zhiyu%2520Mei%2520and%2520Guangju%2520Wang%2520and%2520Yi%2520Wu%26entry.1292438233%3D%2520%2520Reward%2520models%2520have%2520been%2520increasingly%2520critical%2520for%2520improving%2520the%2520reasoning%250Acapability%2520of%2520LLMs.%2520Existing%2520research%2520has%2520shown%2520that%2520a%2520well-trained%2520reward%250Amodel%2520can%2520substantially%2520improve%2520model%2520performances%2520at%2520inference%2520time%2520via%250Asearch.%2520However%252C%2520the%2520potential%2520of%2520reward%2520models%2520during%2520RL%2520training%2520time%2520still%250Aremains%2520largely%2520under-explored.%2520It%2520is%2520currently%2520unclear%2520whether%2520these%2520reward%250Amodels%2520can%2520provide%2520additional%2520training%2520signals%2520to%2520enhance%2520the%2520reasoning%250Acapabilities%2520of%2520LLMs%2520in%2520RL%2520training%2520that%2520uses%2520sparse%2520success%2520rewards%252C%2520which%250Averify%2520the%2520correctness%2520of%2520solutions.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520popular%2520reward%250Amodels%2520for%2520RL%2520training%252C%2520including%2520the%2520Outcome-supervised%2520Reward%2520Model%2520%2528ORM%2529%2520and%250Athe%2520Process-supervised%2520Reward%2520Model%2520%2528PRM%2529%252C%2520and%2520train%2520a%2520collection%2520of%2520LLMs%2520for%250Amath%2520problems%2520using%2520RL%2520by%2520combining%2520these%2520learned%2520rewards%2520with%2520success%2520rewards.%250ASurprisingly%252C%2520even%2520though%2520these%2520learned%2520reward%2520models%2520have%2520strong%250Ainference-time%2520performances%252C%2520they%2520may%2520NOT%2520help%2520or%2520even%2520hurt%2520RL%2520training%252C%250Aproducing%2520worse%2520performances%2520than%2520LLMs%2520trained%2520with%2520the%2520success%2520reward%2520only.%250AOur%2520analysis%2520reveals%2520that%2520an%2520LLM%2520can%2520receive%2520high%2520rewards%2520from%2520some%2520of%2520these%250Areward%2520models%2520by%2520repeating%2520correct%2520but%2520unnecessary%2520reasoning%2520steps%252C%2520leading%2520to%250Aa%2520severe%2520reward%2520hacking%2520issue.%2520Therefore%252C%2520we%2520introduce%2520two%2520novel%2520reward%250Arefinement%2520techniques%252C%2520including%2520Clipping%2520and%2520Delta.%2520The%2520key%2520idea%2520is%2520to%2520ensure%250Athe%2520accumulative%2520reward%2520of%2520any%2520reasoning%2520trajectory%2520is%2520upper-bounded%2520to%2520keep%2520a%250Alearned%2520reward%2520model%2520effective%2520without%2520being%2520exploited.%2520We%2520evaluate%2520our%250Atechniques%2520with%2520multiple%2520reward%2520models%2520over%2520a%2520set%2520of%25201.5B%2520and%25207B%2520LLMs%2520on%2520MATH%250Aand%2520GSM8K%2520benchmarks%2520and%2520demonstrate%2520that%2520with%2520a%2520carefully%2520designed%2520reward%250Afunction%252C%2520RL%2520training%2520without%2520any%2520additional%2520supervised%2520tuning%2520can%2520improve%2520all%250Athe%2520evaluated%2520LLMs%252C%2520including%2520the%2520state-of-the-art%25207B%2520LLM%250AQwen2.5-Math-7B-Instruct%2520on%2520MATH%2520and%2520GSM8K%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15115v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Designing%20Effective%20RL%20Reward%20at%20Training%20Time%20for%20LLM%20Reasoning&entry.906535625=Jiaxuan%20Gao%20and%20Shusheng%20Xu%20and%20Wenjie%20Ye%20and%20Weilin%20Liu%20and%20Chuyi%20He%20and%20Wei%20Fu%20and%20Zhiyu%20Mei%20and%20Guangju%20Wang%20and%20Yi%20Wu&entry.1292438233=%20%20Reward%20models%20have%20been%20increasingly%20critical%20for%20improving%20the%20reasoning%0Acapability%20of%20LLMs.%20Existing%20research%20has%20shown%20that%20a%20well-trained%20reward%0Amodel%20can%20substantially%20improve%20model%20performances%20at%20inference%20time%20via%0Asearch.%20However%2C%20the%20potential%20of%20reward%20models%20during%20RL%20training%20time%20still%0Aremains%20largely%20under-explored.%20It%20is%20currently%20unclear%20whether%20these%20reward%0Amodels%20can%20provide%20additional%20training%20signals%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20in%20RL%20training%20that%20uses%20sparse%20success%20rewards%2C%20which%0Averify%20the%20correctness%20of%20solutions.%20In%20this%20work%2C%20we%20evaluate%20popular%20reward%0Amodels%20for%20RL%20training%2C%20including%20the%20Outcome-supervised%20Reward%20Model%20%28ORM%29%20and%0Athe%20Process-supervised%20Reward%20Model%20%28PRM%29%2C%20and%20train%20a%20collection%20of%20LLMs%20for%0Amath%20problems%20using%20RL%20by%20combining%20these%20learned%20rewards%20with%20success%20rewards.%0ASurprisingly%2C%20even%20though%20these%20learned%20reward%20models%20have%20strong%0Ainference-time%20performances%2C%20they%20may%20NOT%20help%20or%20even%20hurt%20RL%20training%2C%0Aproducing%20worse%20performances%20than%20LLMs%20trained%20with%20the%20success%20reward%20only.%0AOur%20analysis%20reveals%20that%20an%20LLM%20can%20receive%20high%20rewards%20from%20some%20of%20these%0Areward%20models%20by%20repeating%20correct%20but%20unnecessary%20reasoning%20steps%2C%20leading%20to%0Aa%20severe%20reward%20hacking%20issue.%20Therefore%2C%20we%20introduce%20two%20novel%20reward%0Arefinement%20techniques%2C%20including%20Clipping%20and%20Delta.%20The%20key%20idea%20is%20to%20ensure%0Athe%20accumulative%20reward%20of%20any%20reasoning%20trajectory%20is%20upper-bounded%20to%20keep%20a%0Alearned%20reward%20model%20effective%20without%20being%20exploited.%20We%20evaluate%20our%0Atechniques%20with%20multiple%20reward%20models%20over%20a%20set%20of%201.5B%20and%207B%20LLMs%20on%20MATH%0Aand%20GSM8K%20benchmarks%20and%20demonstrate%20that%20with%20a%20carefully%20designed%20reward%0Afunction%2C%20RL%20training%20without%20any%20additional%20supervised%20tuning%20can%20improve%20all%0Athe%20evaluated%20LLMs%2C%20including%20the%20state-of-the-art%207B%20LLM%0AQwen2.5-Math-7B-Instruct%20on%20MATH%20and%20GSM8K%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15115v3&entry.124074799=Read"},
{"title": "IKUN: Initialization to Keep snn training and generalization great with\n  sUrrogate-stable variaNce", "author": "Da Chang and Deliang Wang and Xiao Yang", "abstract": "  Weight initialization significantly impacts the convergence and performance\nof neural networks. While traditional methods like Xavier and Kaiming\ninitialization are widely used, they often fall short for spiking neural\nnetworks (SNNs), which have distinct requirements compared to artificial neural\nnetworks (ANNs).\n  To address this, we introduce \\textbf{IKUN}, a variance-stabilizing\ninitialization method integrated with surrogate gradient functions,\nspecifically designed for SNNs. \\textbf{IKUN} stabilizes signal propagation,\naccelerates convergence, and enhances generalization. Experiments show\n\\textbf{IKUN} improves training efficiency by up to \\textbf{50\\%}, achieving\n\\textbf{95\\%} training accuracy and \\textbf{91\\%} generalization accuracy.\n  Hessian analysis reveals that \\textbf{IKUN}-trained models converge to\nflatter minima, characterized by Hessian eigenvalues near zero on the positive\nside, promoting better generalization. The method is open-sourced for further\nexploration:\n\\href{https://github.com/MaeChd/SurrogateVarStabe}{https://github.com/MaeChd/SurrogateVarStabe}.\n", "link": "http://arxiv.org/abs/2411.18250v1", "date": "2024-11-27", "relevancy": 2.3775, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4869}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IKUN%3A%20Initialization%20to%20Keep%20snn%20training%20and%20generalization%20great%20with%0A%20%20sUrrogate-stable%20variaNce&body=Title%3A%20IKUN%3A%20Initialization%20to%20Keep%20snn%20training%20and%20generalization%20great%20with%0A%20%20sUrrogate-stable%20variaNce%0AAuthor%3A%20Da%20Chang%20and%20Deliang%20Wang%20and%20Xiao%20Yang%0AAbstract%3A%20%20%20Weight%20initialization%20significantly%20impacts%20the%20convergence%20and%20performance%0Aof%20neural%20networks.%20While%20traditional%20methods%20like%20Xavier%20and%20Kaiming%0Ainitialization%20are%20widely%20used%2C%20they%20often%20fall%20short%20for%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20which%20have%20distinct%20requirements%20compared%20to%20artificial%20neural%0Anetworks%20%28ANNs%29.%0A%20%20To%20address%20this%2C%20we%20introduce%20%5Ctextbf%7BIKUN%7D%2C%20a%20variance-stabilizing%0Ainitialization%20method%20integrated%20with%20surrogate%20gradient%20functions%2C%0Aspecifically%20designed%20for%20SNNs.%20%5Ctextbf%7BIKUN%7D%20stabilizes%20signal%20propagation%2C%0Aaccelerates%20convergence%2C%20and%20enhances%20generalization.%20Experiments%20show%0A%5Ctextbf%7BIKUN%7D%20improves%20training%20efficiency%20by%20up%20to%20%5Ctextbf%7B50%5C%25%7D%2C%20achieving%0A%5Ctextbf%7B95%5C%25%7D%20training%20accuracy%20and%20%5Ctextbf%7B91%5C%25%7D%20generalization%20accuracy.%0A%20%20Hessian%20analysis%20reveals%20that%20%5Ctextbf%7BIKUN%7D-trained%20models%20converge%20to%0Aflatter%20minima%2C%20characterized%20by%20Hessian%20eigenvalues%20near%20zero%20on%20the%20positive%0Aside%2C%20promoting%20better%20generalization.%20The%20method%20is%20open-sourced%20for%20further%0Aexploration%3A%0A%5Chref%7Bhttps%3A//github.com/MaeChd/SurrogateVarStabe%7D%7Bhttps%3A//github.com/MaeChd/SurrogateVarStabe%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIKUN%253A%2520Initialization%2520to%2520Keep%2520snn%2520training%2520and%2520generalization%2520great%2520with%250A%2520%2520sUrrogate-stable%2520variaNce%26entry.906535625%3DDa%2520Chang%2520and%2520Deliang%2520Wang%2520and%2520Xiao%2520Yang%26entry.1292438233%3D%2520%2520Weight%2520initialization%2520significantly%2520impacts%2520the%2520convergence%2520and%2520performance%250Aof%2520neural%2520networks.%2520While%2520traditional%2520methods%2520like%2520Xavier%2520and%2520Kaiming%250Ainitialization%2520are%2520widely%2520used%252C%2520they%2520often%2520fall%2520short%2520for%2520spiking%2520neural%250Anetworks%2520%2528SNNs%2529%252C%2520which%2520have%2520distinct%2520requirements%2520compared%2520to%2520artificial%2520neural%250Anetworks%2520%2528ANNs%2529.%250A%2520%2520To%2520address%2520this%252C%2520we%2520introduce%2520%255Ctextbf%257BIKUN%257D%252C%2520a%2520variance-stabilizing%250Ainitialization%2520method%2520integrated%2520with%2520surrogate%2520gradient%2520functions%252C%250Aspecifically%2520designed%2520for%2520SNNs.%2520%255Ctextbf%257BIKUN%257D%2520stabilizes%2520signal%2520propagation%252C%250Aaccelerates%2520convergence%252C%2520and%2520enhances%2520generalization.%2520Experiments%2520show%250A%255Ctextbf%257BIKUN%257D%2520improves%2520training%2520efficiency%2520by%2520up%2520to%2520%255Ctextbf%257B50%255C%2525%257D%252C%2520achieving%250A%255Ctextbf%257B95%255C%2525%257D%2520training%2520accuracy%2520and%2520%255Ctextbf%257B91%255C%2525%257D%2520generalization%2520accuracy.%250A%2520%2520Hessian%2520analysis%2520reveals%2520that%2520%255Ctextbf%257BIKUN%257D-trained%2520models%2520converge%2520to%250Aflatter%2520minima%252C%2520characterized%2520by%2520Hessian%2520eigenvalues%2520near%2520zero%2520on%2520the%2520positive%250Aside%252C%2520promoting%2520better%2520generalization.%2520The%2520method%2520is%2520open-sourced%2520for%2520further%250Aexploration%253A%250A%255Chref%257Bhttps%253A//github.com/MaeChd/SurrogateVarStabe%257D%257Bhttps%253A//github.com/MaeChd/SurrogateVarStabe%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IKUN%3A%20Initialization%20to%20Keep%20snn%20training%20and%20generalization%20great%20with%0A%20%20sUrrogate-stable%20variaNce&entry.906535625=Da%20Chang%20and%20Deliang%20Wang%20and%20Xiao%20Yang&entry.1292438233=%20%20Weight%20initialization%20significantly%20impacts%20the%20convergence%20and%20performance%0Aof%20neural%20networks.%20While%20traditional%20methods%20like%20Xavier%20and%20Kaiming%0Ainitialization%20are%20widely%20used%2C%20they%20often%20fall%20short%20for%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20which%20have%20distinct%20requirements%20compared%20to%20artificial%20neural%0Anetworks%20%28ANNs%29.%0A%20%20To%20address%20this%2C%20we%20introduce%20%5Ctextbf%7BIKUN%7D%2C%20a%20variance-stabilizing%0Ainitialization%20method%20integrated%20with%20surrogate%20gradient%20functions%2C%0Aspecifically%20designed%20for%20SNNs.%20%5Ctextbf%7BIKUN%7D%20stabilizes%20signal%20propagation%2C%0Aaccelerates%20convergence%2C%20and%20enhances%20generalization.%20Experiments%20show%0A%5Ctextbf%7BIKUN%7D%20improves%20training%20efficiency%20by%20up%20to%20%5Ctextbf%7B50%5C%25%7D%2C%20achieving%0A%5Ctextbf%7B95%5C%25%7D%20training%20accuracy%20and%20%5Ctextbf%7B91%5C%25%7D%20generalization%20accuracy.%0A%20%20Hessian%20analysis%20reveals%20that%20%5Ctextbf%7BIKUN%7D-trained%20models%20converge%20to%0Aflatter%20minima%2C%20characterized%20by%20Hessian%20eigenvalues%20near%20zero%20on%20the%20positive%0Aside%2C%20promoting%20better%20generalization.%20The%20method%20is%20open-sourced%20for%20further%0Aexploration%3A%0A%5Chref%7Bhttps%3A//github.com/MaeChd/SurrogateVarStabe%7D%7Bhttps%3A//github.com/MaeChd/SurrogateVarStabe%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18250v1&entry.124074799=Read"},
{"title": "Continual Learning in Machine Speech Chain Using Gradient Episodic\n  Memory", "author": "Geoffrey Tyndall and Kurniawati Azizah and Dipta Tanaya and Ayu Purwarianti and Dessi Puji Lestari and Sakriani Sakti", "abstract": "  Continual learning for automatic speech recognition (ASR) systems poses a\nchallenge, especially with the need to avoid catastrophic forgetting while\nmaintaining performance on previously learned tasks. This paper introduces a\nnovel approach leveraging the machine speech chain framework to enable\ncontinual learning in ASR using gradient episodic memory (GEM). By\nincorporating a text-to-speech (TTS) component within the machine speech chain,\nwe support the replay mechanism essential for GEM, allowing the ASR model to\nlearn new tasks sequentially without significant performance degradation on\nearlier tasks. Our experiments, conducted on the LJ Speech dataset, demonstrate\nthat our method outperforms traditional fine-tuning and multitask learning\napproaches, achieving a substantial error rate reduction while maintaining high\nperformance across varying noise conditions. We showed the potential of our\nsemi-supervised machine speech chain approach for effective and efficient\ncontinual learning in speech recognition.\n", "link": "http://arxiv.org/abs/2411.18320v1", "date": "2024-11-27", "relevancy": 2.3764, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4974}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20in%20Machine%20Speech%20Chain%20Using%20Gradient%20Episodic%0A%20%20Memory&body=Title%3A%20Continual%20Learning%20in%20Machine%20Speech%20Chain%20Using%20Gradient%20Episodic%0A%20%20Memory%0AAuthor%3A%20Geoffrey%20Tyndall%20and%20Kurniawati%20Azizah%20and%20Dipta%20Tanaya%20and%20Ayu%20Purwarianti%20and%20Dessi%20Puji%20Lestari%20and%20Sakriani%20Sakti%0AAbstract%3A%20%20%20Continual%20learning%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems%20poses%20a%0Achallenge%2C%20especially%20with%20the%20need%20to%20avoid%20catastrophic%20forgetting%20while%0Amaintaining%20performance%20on%20previously%20learned%20tasks.%20This%20paper%20introduces%20a%0Anovel%20approach%20leveraging%20the%20machine%20speech%20chain%20framework%20to%20enable%0Acontinual%20learning%20in%20ASR%20using%20gradient%20episodic%20memory%20%28GEM%29.%20By%0Aincorporating%20a%20text-to-speech%20%28TTS%29%20component%20within%20the%20machine%20speech%20chain%2C%0Awe%20support%20the%20replay%20mechanism%20essential%20for%20GEM%2C%20allowing%20the%20ASR%20model%20to%0Alearn%20new%20tasks%20sequentially%20without%20significant%20performance%20degradation%20on%0Aearlier%20tasks.%20Our%20experiments%2C%20conducted%20on%20the%20LJ%20Speech%20dataset%2C%20demonstrate%0Athat%20our%20method%20outperforms%20traditional%20fine-tuning%20and%20multitask%20learning%0Aapproaches%2C%20achieving%20a%20substantial%20error%20rate%20reduction%20while%20maintaining%20high%0Aperformance%20across%20varying%20noise%20conditions.%20We%20showed%20the%20potential%20of%20our%0Asemi-supervised%20machine%20speech%20chain%20approach%20for%20effective%20and%20efficient%0Acontinual%20learning%20in%20speech%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520in%2520Machine%2520Speech%2520Chain%2520Using%2520Gradient%2520Episodic%250A%2520%2520Memory%26entry.906535625%3DGeoffrey%2520Tyndall%2520and%2520Kurniawati%2520Azizah%2520and%2520Dipta%2520Tanaya%2520and%2520Ayu%2520Purwarianti%2520and%2520Dessi%2520Puji%2520Lestari%2520and%2520Sakriani%2520Sakti%26entry.1292438233%3D%2520%2520Continual%2520learning%2520for%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520poses%2520a%250Achallenge%252C%2520especially%2520with%2520the%2520need%2520to%2520avoid%2520catastrophic%2520forgetting%2520while%250Amaintaining%2520performance%2520on%2520previously%2520learned%2520tasks.%2520This%2520paper%2520introduces%2520a%250Anovel%2520approach%2520leveraging%2520the%2520machine%2520speech%2520chain%2520framework%2520to%2520enable%250Acontinual%2520learning%2520in%2520ASR%2520using%2520gradient%2520episodic%2520memory%2520%2528GEM%2529.%2520By%250Aincorporating%2520a%2520text-to-speech%2520%2528TTS%2529%2520component%2520within%2520the%2520machine%2520speech%2520chain%252C%250Awe%2520support%2520the%2520replay%2520mechanism%2520essential%2520for%2520GEM%252C%2520allowing%2520the%2520ASR%2520model%2520to%250Alearn%2520new%2520tasks%2520sequentially%2520without%2520significant%2520performance%2520degradation%2520on%250Aearlier%2520tasks.%2520Our%2520experiments%252C%2520conducted%2520on%2520the%2520LJ%2520Speech%2520dataset%252C%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520traditional%2520fine-tuning%2520and%2520multitask%2520learning%250Aapproaches%252C%2520achieving%2520a%2520substantial%2520error%2520rate%2520reduction%2520while%2520maintaining%2520high%250Aperformance%2520across%2520varying%2520noise%2520conditions.%2520We%2520showed%2520the%2520potential%2520of%2520our%250Asemi-supervised%2520machine%2520speech%2520chain%2520approach%2520for%2520effective%2520and%2520efficient%250Acontinual%2520learning%2520in%2520speech%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20in%20Machine%20Speech%20Chain%20Using%20Gradient%20Episodic%0A%20%20Memory&entry.906535625=Geoffrey%20Tyndall%20and%20Kurniawati%20Azizah%20and%20Dipta%20Tanaya%20and%20Ayu%20Purwarianti%20and%20Dessi%20Puji%20Lestari%20and%20Sakriani%20Sakti&entry.1292438233=%20%20Continual%20learning%20for%20automatic%20speech%20recognition%20%28ASR%29%20systems%20poses%20a%0Achallenge%2C%20especially%20with%20the%20need%20to%20avoid%20catastrophic%20forgetting%20while%0Amaintaining%20performance%20on%20previously%20learned%20tasks.%20This%20paper%20introduces%20a%0Anovel%20approach%20leveraging%20the%20machine%20speech%20chain%20framework%20to%20enable%0Acontinual%20learning%20in%20ASR%20using%20gradient%20episodic%20memory%20%28GEM%29.%20By%0Aincorporating%20a%20text-to-speech%20%28TTS%29%20component%20within%20the%20machine%20speech%20chain%2C%0Awe%20support%20the%20replay%20mechanism%20essential%20for%20GEM%2C%20allowing%20the%20ASR%20model%20to%0Alearn%20new%20tasks%20sequentially%20without%20significant%20performance%20degradation%20on%0Aearlier%20tasks.%20Our%20experiments%2C%20conducted%20on%20the%20LJ%20Speech%20dataset%2C%20demonstrate%0Athat%20our%20method%20outperforms%20traditional%20fine-tuning%20and%20multitask%20learning%0Aapproaches%2C%20achieving%20a%20substantial%20error%20rate%20reduction%20while%20maintaining%20high%0Aperformance%20across%20varying%20noise%20conditions.%20We%20showed%20the%20potential%20of%20our%0Asemi-supervised%20machine%20speech%20chain%20approach%20for%20effective%20and%20efficient%0Acontinual%20learning%20in%20speech%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18320v1&entry.124074799=Read"},
{"title": "Efficient Dynamic LiDAR Odometry for Mobile Robots with Structured Point\n  Clouds", "author": "Jonathan Lichtenfeld and Kevin Daun and Oskar von Stryk", "abstract": "  We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in\nUrban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object\ndetection often rely on pretrained learned networks or computationally\nexpensive volumetric maps. To enhance efficiency on computationally limited\nrobots, we reuse data between the odometry and detection module. Utilizing a\nrange image segmentation technique and a novel residual-based heuristic, our\nmethod distinguishes dynamic from static objects before integrating them into\nthe point cloud map. The approach demonstrates robust object tracking and\nimproved map accuracy in environments with numerous dynamic objects. Even\nhighly non-rigid objects, such as running humans, are accurately detected at\npoint level without prior downsampling of the point cloud and hence, without\nloss of information. Evaluation on simulated and real-world data validates its\ncomputational efficiency. Compared to a state-of-the-art volumetric method, our\napproach shows comparable detection performance at a fraction of the processing\ntime, adding only 14 ms to the odometry module for dynamic object detection and\ntracking. The implementation and a new real-world dataset are available as\nopen-source for further research.\n", "link": "http://arxiv.org/abs/2411.18443v1", "date": "2024-11-27", "relevancy": 2.3713, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6165}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Dynamic%20LiDAR%20Odometry%20for%20Mobile%20Robots%20with%20Structured%20Point%0A%20%20Clouds&body=Title%3A%20Efficient%20Dynamic%20LiDAR%20Odometry%20for%20Mobile%20Robots%20with%20Structured%20Point%0A%20%20Clouds%0AAuthor%3A%20Jonathan%20Lichtenfeld%20and%20Kevin%20Daun%20and%20Oskar%20von%20Stryk%0AAbstract%3A%20%20%20We%20propose%20a%20real-time%20dynamic%20LiDAR%20odometry%20pipeline%20for%20mobile%20robots%20in%0AUrban%20Search%20and%20Rescue%20%28USAR%29%20scenarios.%20Existing%20approaches%20to%20dynamic%20object%0Adetection%20often%20rely%20on%20pretrained%20learned%20networks%20or%20computationally%0Aexpensive%20volumetric%20maps.%20To%20enhance%20efficiency%20on%20computationally%20limited%0Arobots%2C%20we%20reuse%20data%20between%20the%20odometry%20and%20detection%20module.%20Utilizing%20a%0Arange%20image%20segmentation%20technique%20and%20a%20novel%20residual-based%20heuristic%2C%20our%0Amethod%20distinguishes%20dynamic%20from%20static%20objects%20before%20integrating%20them%20into%0Athe%20point%20cloud%20map.%20The%20approach%20demonstrates%20robust%20object%20tracking%20and%0Aimproved%20map%20accuracy%20in%20environments%20with%20numerous%20dynamic%20objects.%20Even%0Ahighly%20non-rigid%20objects%2C%20such%20as%20running%20humans%2C%20are%20accurately%20detected%20at%0Apoint%20level%20without%20prior%20downsampling%20of%20the%20point%20cloud%20and%20hence%2C%20without%0Aloss%20of%20information.%20Evaluation%20on%20simulated%20and%20real-world%20data%20validates%20its%0Acomputational%20efficiency.%20Compared%20to%20a%20state-of-the-art%20volumetric%20method%2C%20our%0Aapproach%20shows%20comparable%20detection%20performance%20at%20a%20fraction%20of%20the%20processing%0Atime%2C%20adding%20only%2014%20ms%20to%20the%20odometry%20module%20for%20dynamic%20object%20detection%20and%0Atracking.%20The%20implementation%20and%20a%20new%20real-world%20dataset%20are%20available%20as%0Aopen-source%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Dynamic%2520LiDAR%2520Odometry%2520for%2520Mobile%2520Robots%2520with%2520Structured%2520Point%250A%2520%2520Clouds%26entry.906535625%3DJonathan%2520Lichtenfeld%2520and%2520Kevin%2520Daun%2520and%2520Oskar%2520von%2520Stryk%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520real-time%2520dynamic%2520LiDAR%2520odometry%2520pipeline%2520for%2520mobile%2520robots%2520in%250AUrban%2520Search%2520and%2520Rescue%2520%2528USAR%2529%2520scenarios.%2520Existing%2520approaches%2520to%2520dynamic%2520object%250Adetection%2520often%2520rely%2520on%2520pretrained%2520learned%2520networks%2520or%2520computationally%250Aexpensive%2520volumetric%2520maps.%2520To%2520enhance%2520efficiency%2520on%2520computationally%2520limited%250Arobots%252C%2520we%2520reuse%2520data%2520between%2520the%2520odometry%2520and%2520detection%2520module.%2520Utilizing%2520a%250Arange%2520image%2520segmentation%2520technique%2520and%2520a%2520novel%2520residual-based%2520heuristic%252C%2520our%250Amethod%2520distinguishes%2520dynamic%2520from%2520static%2520objects%2520before%2520integrating%2520them%2520into%250Athe%2520point%2520cloud%2520map.%2520The%2520approach%2520demonstrates%2520robust%2520object%2520tracking%2520and%250Aimproved%2520map%2520accuracy%2520in%2520environments%2520with%2520numerous%2520dynamic%2520objects.%2520Even%250Ahighly%2520non-rigid%2520objects%252C%2520such%2520as%2520running%2520humans%252C%2520are%2520accurately%2520detected%2520at%250Apoint%2520level%2520without%2520prior%2520downsampling%2520of%2520the%2520point%2520cloud%2520and%2520hence%252C%2520without%250Aloss%2520of%2520information.%2520Evaluation%2520on%2520simulated%2520and%2520real-world%2520data%2520validates%2520its%250Acomputational%2520efficiency.%2520Compared%2520to%2520a%2520state-of-the-art%2520volumetric%2520method%252C%2520our%250Aapproach%2520shows%2520comparable%2520detection%2520performance%2520at%2520a%2520fraction%2520of%2520the%2520processing%250Atime%252C%2520adding%2520only%252014%2520ms%2520to%2520the%2520odometry%2520module%2520for%2520dynamic%2520object%2520detection%2520and%250Atracking.%2520The%2520implementation%2520and%2520a%2520new%2520real-world%2520dataset%2520are%2520available%2520as%250Aopen-source%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Dynamic%20LiDAR%20Odometry%20for%20Mobile%20Robots%20with%20Structured%20Point%0A%20%20Clouds&entry.906535625=Jonathan%20Lichtenfeld%20and%20Kevin%20Daun%20and%20Oskar%20von%20Stryk&entry.1292438233=%20%20We%20propose%20a%20real-time%20dynamic%20LiDAR%20odometry%20pipeline%20for%20mobile%20robots%20in%0AUrban%20Search%20and%20Rescue%20%28USAR%29%20scenarios.%20Existing%20approaches%20to%20dynamic%20object%0Adetection%20often%20rely%20on%20pretrained%20learned%20networks%20or%20computationally%0Aexpensive%20volumetric%20maps.%20To%20enhance%20efficiency%20on%20computationally%20limited%0Arobots%2C%20we%20reuse%20data%20between%20the%20odometry%20and%20detection%20module.%20Utilizing%20a%0Arange%20image%20segmentation%20technique%20and%20a%20novel%20residual-based%20heuristic%2C%20our%0Amethod%20distinguishes%20dynamic%20from%20static%20objects%20before%20integrating%20them%20into%0Athe%20point%20cloud%20map.%20The%20approach%20demonstrates%20robust%20object%20tracking%20and%0Aimproved%20map%20accuracy%20in%20environments%20with%20numerous%20dynamic%20objects.%20Even%0Ahighly%20non-rigid%20objects%2C%20such%20as%20running%20humans%2C%20are%20accurately%20detected%20at%0Apoint%20level%20without%20prior%20downsampling%20of%20the%20point%20cloud%20and%20hence%2C%20without%0Aloss%20of%20information.%20Evaluation%20on%20simulated%20and%20real-world%20data%20validates%20its%0Acomputational%20efficiency.%20Compared%20to%20a%20state-of-the-art%20volumetric%20method%2C%20our%0Aapproach%20shows%20comparable%20detection%20performance%20at%20a%20fraction%20of%20the%20processing%0Atime%2C%20adding%20only%2014%20ms%20to%20the%20odometry%20module%20for%20dynamic%20object%20detection%20and%0Atracking.%20The%20implementation%20and%20a%20new%20real-world%20dataset%20are%20available%20as%0Aopen-source%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18443v1&entry.124074799=Read"},
{"title": "Task Arithmetic Through The Lens Of One-Shot Federated Learning", "author": "Zhixu Tao and Ian Mason and Sanjeev Kulkarni and Xavier Boix", "abstract": "  Task Arithmetic is a model merging technique that enables the combination of\nmultiple models' capabilities into a single model through simple arithmetic in\nthe weight space, without the need for additional fine-tuning or access to the\noriginal training data. However, the factors that determine the success of Task\nArithmetic remain unclear. In this paper, we examine Task Arithmetic for\nmulti-task learning by framing it as a one-shot Federated Learning problem. We\ndemonstrate that Task Arithmetic is mathematically equivalent to the commonly\nused algorithm in Federated Learning, called Federated Averaging (FedAvg). By\nleveraging well-established theoretical results from FedAvg, we identify two\nkey factors that impact the performance of Task Arithmetic: data heterogeneity\nand training heterogeneity. To mitigate these challenges, we adapt several\nalgorithms from Federated Learning to improve the effectiveness of Task\nArithmetic. Our experiments demonstrate that applying these algorithms can\noften significantly boost performance of the merged model compared to the\noriginal Task Arithmetic approach. This work bridges Task Arithmetic and\nFederated Learning, offering new theoretical perspectives on Task Arithmetic\nand improved practical methodologies for model merging.\n", "link": "http://arxiv.org/abs/2411.18607v1", "date": "2024-11-27", "relevancy": 2.3614, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Arithmetic%20Through%20The%20Lens%20Of%20One-Shot%20Federated%20Learning&body=Title%3A%20Task%20Arithmetic%20Through%20The%20Lens%20Of%20One-Shot%20Federated%20Learning%0AAuthor%3A%20Zhixu%20Tao%20and%20Ian%20Mason%20and%20Sanjeev%20Kulkarni%20and%20Xavier%20Boix%0AAbstract%3A%20%20%20Task%20Arithmetic%20is%20a%20model%20merging%20technique%20that%20enables%20the%20combination%20of%0Amultiple%20models%27%20capabilities%20into%20a%20single%20model%20through%20simple%20arithmetic%20in%0Athe%20weight%20space%2C%20without%20the%20need%20for%20additional%20fine-tuning%20or%20access%20to%20the%0Aoriginal%20training%20data.%20However%2C%20the%20factors%20that%20determine%20the%20success%20of%20Task%0AArithmetic%20remain%20unclear.%20In%20this%20paper%2C%20we%20examine%20Task%20Arithmetic%20for%0Amulti-task%20learning%20by%20framing%20it%20as%20a%20one-shot%20Federated%20Learning%20problem.%20We%0Ademonstrate%20that%20Task%20Arithmetic%20is%20mathematically%20equivalent%20to%20the%20commonly%0Aused%20algorithm%20in%20Federated%20Learning%2C%20called%20Federated%20Averaging%20%28FedAvg%29.%20By%0Aleveraging%20well-established%20theoretical%20results%20from%20FedAvg%2C%20we%20identify%20two%0Akey%20factors%20that%20impact%20the%20performance%20of%20Task%20Arithmetic%3A%20data%20heterogeneity%0Aand%20training%20heterogeneity.%20To%20mitigate%20these%20challenges%2C%20we%20adapt%20several%0Aalgorithms%20from%20Federated%20Learning%20to%20improve%20the%20effectiveness%20of%20Task%0AArithmetic.%20Our%20experiments%20demonstrate%20that%20applying%20these%20algorithms%20can%0Aoften%20significantly%20boost%20performance%20of%20the%20merged%20model%20compared%20to%20the%0Aoriginal%20Task%20Arithmetic%20approach.%20This%20work%20bridges%20Task%20Arithmetic%20and%0AFederated%20Learning%2C%20offering%20new%20theoretical%20perspectives%20on%20Task%20Arithmetic%0Aand%20improved%20practical%20methodologies%20for%20model%20merging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Arithmetic%2520Through%2520The%2520Lens%2520Of%2520One-Shot%2520Federated%2520Learning%26entry.906535625%3DZhixu%2520Tao%2520and%2520Ian%2520Mason%2520and%2520Sanjeev%2520Kulkarni%2520and%2520Xavier%2520Boix%26entry.1292438233%3D%2520%2520Task%2520Arithmetic%2520is%2520a%2520model%2520merging%2520technique%2520that%2520enables%2520the%2520combination%2520of%250Amultiple%2520models%2527%2520capabilities%2520into%2520a%2520single%2520model%2520through%2520simple%2520arithmetic%2520in%250Athe%2520weight%2520space%252C%2520without%2520the%2520need%2520for%2520additional%2520fine-tuning%2520or%2520access%2520to%2520the%250Aoriginal%2520training%2520data.%2520However%252C%2520the%2520factors%2520that%2520determine%2520the%2520success%2520of%2520Task%250AArithmetic%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520examine%2520Task%2520Arithmetic%2520for%250Amulti-task%2520learning%2520by%2520framing%2520it%2520as%2520a%2520one-shot%2520Federated%2520Learning%2520problem.%2520We%250Ademonstrate%2520that%2520Task%2520Arithmetic%2520is%2520mathematically%2520equivalent%2520to%2520the%2520commonly%250Aused%2520algorithm%2520in%2520Federated%2520Learning%252C%2520called%2520Federated%2520Averaging%2520%2528FedAvg%2529.%2520By%250Aleveraging%2520well-established%2520theoretical%2520results%2520from%2520FedAvg%252C%2520we%2520identify%2520two%250Akey%2520factors%2520that%2520impact%2520the%2520performance%2520of%2520Task%2520Arithmetic%253A%2520data%2520heterogeneity%250Aand%2520training%2520heterogeneity.%2520To%2520mitigate%2520these%2520challenges%252C%2520we%2520adapt%2520several%250Aalgorithms%2520from%2520Federated%2520Learning%2520to%2520improve%2520the%2520effectiveness%2520of%2520Task%250AArithmetic.%2520Our%2520experiments%2520demonstrate%2520that%2520applying%2520these%2520algorithms%2520can%250Aoften%2520significantly%2520boost%2520performance%2520of%2520the%2520merged%2520model%2520compared%2520to%2520the%250Aoriginal%2520Task%2520Arithmetic%2520approach.%2520This%2520work%2520bridges%2520Task%2520Arithmetic%2520and%250AFederated%2520Learning%252C%2520offering%2520new%2520theoretical%2520perspectives%2520on%2520Task%2520Arithmetic%250Aand%2520improved%2520practical%2520methodologies%2520for%2520model%2520merging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Arithmetic%20Through%20The%20Lens%20Of%20One-Shot%20Federated%20Learning&entry.906535625=Zhixu%20Tao%20and%20Ian%20Mason%20and%20Sanjeev%20Kulkarni%20and%20Xavier%20Boix&entry.1292438233=%20%20Task%20Arithmetic%20is%20a%20model%20merging%20technique%20that%20enables%20the%20combination%20of%0Amultiple%20models%27%20capabilities%20into%20a%20single%20model%20through%20simple%20arithmetic%20in%0Athe%20weight%20space%2C%20without%20the%20need%20for%20additional%20fine-tuning%20or%20access%20to%20the%0Aoriginal%20training%20data.%20However%2C%20the%20factors%20that%20determine%20the%20success%20of%20Task%0AArithmetic%20remain%20unclear.%20In%20this%20paper%2C%20we%20examine%20Task%20Arithmetic%20for%0Amulti-task%20learning%20by%20framing%20it%20as%20a%20one-shot%20Federated%20Learning%20problem.%20We%0Ademonstrate%20that%20Task%20Arithmetic%20is%20mathematically%20equivalent%20to%20the%20commonly%0Aused%20algorithm%20in%20Federated%20Learning%2C%20called%20Federated%20Averaging%20%28FedAvg%29.%20By%0Aleveraging%20well-established%20theoretical%20results%20from%20FedAvg%2C%20we%20identify%20two%0Akey%20factors%20that%20impact%20the%20performance%20of%20Task%20Arithmetic%3A%20data%20heterogeneity%0Aand%20training%20heterogeneity.%20To%20mitigate%20these%20challenges%2C%20we%20adapt%20several%0Aalgorithms%20from%20Federated%20Learning%20to%20improve%20the%20effectiveness%20of%20Task%0AArithmetic.%20Our%20experiments%20demonstrate%20that%20applying%20these%20algorithms%20can%0Aoften%20significantly%20boost%20performance%20of%20the%20merged%20model%20compared%20to%20the%0Aoriginal%20Task%20Arithmetic%20approach.%20This%20work%20bridges%20Task%20Arithmetic%20and%0AFederated%20Learning%2C%20offering%20new%20theoretical%20perspectives%20on%20Task%20Arithmetic%0Aand%20improved%20practical%20methodologies%20for%20model%20merging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18607v1&entry.124074799=Read"},
{"title": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering", "author": "Daniel Scalena and Elisabetta Fersini and Malvina Nissim", "abstract": "  Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language.\n", "link": "http://arxiv.org/abs/2411.18247v1", "date": "2024-11-27", "relevancy": 2.349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20gentle%20push%20funziona%20benissimo%3A%20making%20instructed%20models%20in%20Italian%0A%20%20via%20contrastive%20activation%20steering&body=Title%3A%20A%20gentle%20push%20funziona%20benissimo%3A%20making%20instructed%20models%20in%20Italian%0A%20%20via%20contrastive%20activation%20steering%0AAuthor%3A%20Daniel%20Scalena%20and%20Elisabetta%20Fersini%20and%20Malvina%20Nissim%0AAbstract%3A%20%20%20Adapting%20models%20to%20a%20language%20that%20was%20only%20partially%20present%20in%20the%0Apre-training%20data%20requires%20fine-tuning%2C%20which%20is%20expensive%20in%20terms%20of%20both%0Adata%20and%20computational%20resources.%20As%20an%20alternative%20to%20fine-tuning%2C%20we%20explore%0Athe%20potential%20of%20activation%20steering-based%20techniques%20to%20enhance%20model%0Aperformance%20on%20Italian%20tasks.%20Through%20our%20experiments%20we%20show%20that%20Italian%0Asteering%20%28i%29%20can%20be%20successfully%20applied%20to%20different%20models%2C%20%28ii%29%20achieves%0Aperformances%20comparable%20to%2C%20or%20even%20better%20than%2C%20fine-tuned%20models%20for%20Italian%2C%0Aand%20%28iii%29%20yields%20higher%20quality%20and%20consistency%20in%20Italian%20generations.%20We%20also%0Adiscuss%20the%20utility%20of%20steering%20and%20fine-tuning%20in%20the%20contemporary%20LLM%0Alandscape%20where%20models%20are%20anyway%20getting%20high%20Italian%20performances%20even%20if%20not%0Aexplicitly%20trained%20in%20this%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520gentle%2520push%2520funziona%2520benissimo%253A%2520making%2520instructed%2520models%2520in%2520Italian%250A%2520%2520via%2520contrastive%2520activation%2520steering%26entry.906535625%3DDaniel%2520Scalena%2520and%2520Elisabetta%2520Fersini%2520and%2520Malvina%2520Nissim%26entry.1292438233%3D%2520%2520Adapting%2520models%2520to%2520a%2520language%2520that%2520was%2520only%2520partially%2520present%2520in%2520the%250Apre-training%2520data%2520requires%2520fine-tuning%252C%2520which%2520is%2520expensive%2520in%2520terms%2520of%2520both%250Adata%2520and%2520computational%2520resources.%2520As%2520an%2520alternative%2520to%2520fine-tuning%252C%2520we%2520explore%250Athe%2520potential%2520of%2520activation%2520steering-based%2520techniques%2520to%2520enhance%2520model%250Aperformance%2520on%2520Italian%2520tasks.%2520Through%2520our%2520experiments%2520we%2520show%2520that%2520Italian%250Asteering%2520%2528i%2529%2520can%2520be%2520successfully%2520applied%2520to%2520different%2520models%252C%2520%2528ii%2529%2520achieves%250Aperformances%2520comparable%2520to%252C%2520or%2520even%2520better%2520than%252C%2520fine-tuned%2520models%2520for%2520Italian%252C%250Aand%2520%2528iii%2529%2520yields%2520higher%2520quality%2520and%2520consistency%2520in%2520Italian%2520generations.%2520We%2520also%250Adiscuss%2520the%2520utility%2520of%2520steering%2520and%2520fine-tuning%2520in%2520the%2520contemporary%2520LLM%250Alandscape%2520where%2520models%2520are%2520anyway%2520getting%2520high%2520Italian%2520performances%2520even%2520if%2520not%250Aexplicitly%2520trained%2520in%2520this%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20gentle%20push%20funziona%20benissimo%3A%20making%20instructed%20models%20in%20Italian%0A%20%20via%20contrastive%20activation%20steering&entry.906535625=Daniel%20Scalena%20and%20Elisabetta%20Fersini%20and%20Malvina%20Nissim&entry.1292438233=%20%20Adapting%20models%20to%20a%20language%20that%20was%20only%20partially%20present%20in%20the%0Apre-training%20data%20requires%20fine-tuning%2C%20which%20is%20expensive%20in%20terms%20of%20both%0Adata%20and%20computational%20resources.%20As%20an%20alternative%20to%20fine-tuning%2C%20we%20explore%0Athe%20potential%20of%20activation%20steering-based%20techniques%20to%20enhance%20model%0Aperformance%20on%20Italian%20tasks.%20Through%20our%20experiments%20we%20show%20that%20Italian%0Asteering%20%28i%29%20can%20be%20successfully%20applied%20to%20different%20models%2C%20%28ii%29%20achieves%0Aperformances%20comparable%20to%2C%20or%20even%20better%20than%2C%20fine-tuned%20models%20for%20Italian%2C%0Aand%20%28iii%29%20yields%20higher%20quality%20and%20consistency%20in%20Italian%20generations.%20We%20also%0Adiscuss%20the%20utility%20of%20steering%20and%20fine-tuning%20in%20the%20contemporary%20LLM%0Alandscape%20where%20models%20are%20anyway%20getting%20high%20Italian%20performances%20even%20if%20not%0Aexplicitly%20trained%20in%20this%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18247v1&entry.124074799=Read"},
{"title": "MM-Path: Multi-modal, Multi-granularity Path Representation Learning --\n  Extended Version", "author": "Ronghui Xu and Hanyin Cheng and Chenjuan Guo and Hongfan Gao and Jilin Hu and Sean Bin Yang and Bin Yang", "abstract": "  Developing effective path representations has become increasingly essential\nacross various fields within intelligent transportation. Although pre-trained\npath representation learning models have shown improved performance, they\npredominantly focus on the topological structures from single modality data,\ni.e., road networks, overlooking the geometric and contextual features\nassociated with path-related images, e.g., remote sensing images. Similar to\nhuman understanding, integrating information from multiple modalities can\nprovide a more comprehensive view, enhancing both representation accuracy and\ngeneralization. However, variations in information granularity impede the\nsemantic alignment of road network-based paths (road paths) and image-based\npaths (image paths), while the heterogeneity of multi-modal data poses\nsubstantial challenges for effective fusion and utilization. In this paper, we\npropose a novel Multi-modal, Multi-granularity Path Representation Learning\nFramework (MM-Path), which can learn a generic path representation by\nintegrating modalities from both road paths and image paths. To enhance the\nalignment of multi-modal data, we develop a multi-granularity alignment\nstrategy that systematically associates nodes, road sub-paths, and road paths\nwith their corresponding image patches, ensuring the synchronization of both\ndetailed local information and broader global contexts. To address the\nheterogeneity of multi-modal data effectively, we introduce a graph-based\ncross-modal residual fusion component designed to comprehensively fuse\ninformation across different modalities and granularities. Finally, we conduct\nextensive experiments on two large-scale real-world datasets under two\ndownstream tasks, validating the effectiveness of the proposed MM-Path. This is\nan extended version of the paper accepted by KDD 2025.\n", "link": "http://arxiv.org/abs/2411.18428v1", "date": "2024-11-27", "relevancy": 2.3022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5838}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Path%3A%20Multi-modal%2C%20Multi-granularity%20Path%20Representation%20Learning%20--%0A%20%20Extended%20Version&body=Title%3A%20MM-Path%3A%20Multi-modal%2C%20Multi-granularity%20Path%20Representation%20Learning%20--%0A%20%20Extended%20Version%0AAuthor%3A%20Ronghui%20Xu%20and%20Hanyin%20Cheng%20and%20Chenjuan%20Guo%20and%20Hongfan%20Gao%20and%20Jilin%20Hu%20and%20Sean%20Bin%20Yang%20and%20Bin%20Yang%0AAbstract%3A%20%20%20Developing%20effective%20path%20representations%20has%20become%20increasingly%20essential%0Aacross%20various%20fields%20within%20intelligent%20transportation.%20Although%20pre-trained%0Apath%20representation%20learning%20models%20have%20shown%20improved%20performance%2C%20they%0Apredominantly%20focus%20on%20the%20topological%20structures%20from%20single%20modality%20data%2C%0Ai.e.%2C%20road%20networks%2C%20overlooking%20the%20geometric%20and%20contextual%20features%0Aassociated%20with%20path-related%20images%2C%20e.g.%2C%20remote%20sensing%20images.%20Similar%20to%0Ahuman%20understanding%2C%20integrating%20information%20from%20multiple%20modalities%20can%0Aprovide%20a%20more%20comprehensive%20view%2C%20enhancing%20both%20representation%20accuracy%20and%0Ageneralization.%20However%2C%20variations%20in%20information%20granularity%20impede%20the%0Asemantic%20alignment%20of%20road%20network-based%20paths%20%28road%20paths%29%20and%20image-based%0Apaths%20%28image%20paths%29%2C%20while%20the%20heterogeneity%20of%20multi-modal%20data%20poses%0Asubstantial%20challenges%20for%20effective%20fusion%20and%20utilization.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Multi-modal%2C%20Multi-granularity%20Path%20Representation%20Learning%0AFramework%20%28MM-Path%29%2C%20which%20can%20learn%20a%20generic%20path%20representation%20by%0Aintegrating%20modalities%20from%20both%20road%20paths%20and%20image%20paths.%20To%20enhance%20the%0Aalignment%20of%20multi-modal%20data%2C%20we%20develop%20a%20multi-granularity%20alignment%0Astrategy%20that%20systematically%20associates%20nodes%2C%20road%20sub-paths%2C%20and%20road%20paths%0Awith%20their%20corresponding%20image%20patches%2C%20ensuring%20the%20synchronization%20of%20both%0Adetailed%20local%20information%20and%20broader%20global%20contexts.%20To%20address%20the%0Aheterogeneity%20of%20multi-modal%20data%20effectively%2C%20we%20introduce%20a%20graph-based%0Across-modal%20residual%20fusion%20component%20designed%20to%20comprehensively%20fuse%0Ainformation%20across%20different%20modalities%20and%20granularities.%20Finally%2C%20we%20conduct%0Aextensive%20experiments%20on%20two%20large-scale%20real-world%20datasets%20under%20two%0Adownstream%20tasks%2C%20validating%20the%20effectiveness%20of%20the%20proposed%20MM-Path.%20This%20is%0Aan%20extended%20version%20of%20the%20paper%20accepted%20by%20KDD%202025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Path%253A%2520Multi-modal%252C%2520Multi-granularity%2520Path%2520Representation%2520Learning%2520--%250A%2520%2520Extended%2520Version%26entry.906535625%3DRonghui%2520Xu%2520and%2520Hanyin%2520Cheng%2520and%2520Chenjuan%2520Guo%2520and%2520Hongfan%2520Gao%2520and%2520Jilin%2520Hu%2520and%2520Sean%2520Bin%2520Yang%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520Developing%2520effective%2520path%2520representations%2520has%2520become%2520increasingly%2520essential%250Aacross%2520various%2520fields%2520within%2520intelligent%2520transportation.%2520Although%2520pre-trained%250Apath%2520representation%2520learning%2520models%2520have%2520shown%2520improved%2520performance%252C%2520they%250Apredominantly%2520focus%2520on%2520the%2520topological%2520structures%2520from%2520single%2520modality%2520data%252C%250Ai.e.%252C%2520road%2520networks%252C%2520overlooking%2520the%2520geometric%2520and%2520contextual%2520features%250Aassociated%2520with%2520path-related%2520images%252C%2520e.g.%252C%2520remote%2520sensing%2520images.%2520Similar%2520to%250Ahuman%2520understanding%252C%2520integrating%2520information%2520from%2520multiple%2520modalities%2520can%250Aprovide%2520a%2520more%2520comprehensive%2520view%252C%2520enhancing%2520both%2520representation%2520accuracy%2520and%250Ageneralization.%2520However%252C%2520variations%2520in%2520information%2520granularity%2520impede%2520the%250Asemantic%2520alignment%2520of%2520road%2520network-based%2520paths%2520%2528road%2520paths%2529%2520and%2520image-based%250Apaths%2520%2528image%2520paths%2529%252C%2520while%2520the%2520heterogeneity%2520of%2520multi-modal%2520data%2520poses%250Asubstantial%2520challenges%2520for%2520effective%2520fusion%2520and%2520utilization.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520Multi-modal%252C%2520Multi-granularity%2520Path%2520Representation%2520Learning%250AFramework%2520%2528MM-Path%2529%252C%2520which%2520can%2520learn%2520a%2520generic%2520path%2520representation%2520by%250Aintegrating%2520modalities%2520from%2520both%2520road%2520paths%2520and%2520image%2520paths.%2520To%2520enhance%2520the%250Aalignment%2520of%2520multi-modal%2520data%252C%2520we%2520develop%2520a%2520multi-granularity%2520alignment%250Astrategy%2520that%2520systematically%2520associates%2520nodes%252C%2520road%2520sub-paths%252C%2520and%2520road%2520paths%250Awith%2520their%2520corresponding%2520image%2520patches%252C%2520ensuring%2520the%2520synchronization%2520of%2520both%250Adetailed%2520local%2520information%2520and%2520broader%2520global%2520contexts.%2520To%2520address%2520the%250Aheterogeneity%2520of%2520multi-modal%2520data%2520effectively%252C%2520we%2520introduce%2520a%2520graph-based%250Across-modal%2520residual%2520fusion%2520component%2520designed%2520to%2520comprehensively%2520fuse%250Ainformation%2520across%2520different%2520modalities%2520and%2520granularities.%2520Finally%252C%2520we%2520conduct%250Aextensive%2520experiments%2520on%2520two%2520large-scale%2520real-world%2520datasets%2520under%2520two%250Adownstream%2520tasks%252C%2520validating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520MM-Path.%2520This%2520is%250Aan%2520extended%2520version%2520of%2520the%2520paper%2520accepted%2520by%2520KDD%25202025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Path%3A%20Multi-modal%2C%20Multi-granularity%20Path%20Representation%20Learning%20--%0A%20%20Extended%20Version&entry.906535625=Ronghui%20Xu%20and%20Hanyin%20Cheng%20and%20Chenjuan%20Guo%20and%20Hongfan%20Gao%20and%20Jilin%20Hu%20and%20Sean%20Bin%20Yang%20and%20Bin%20Yang&entry.1292438233=%20%20Developing%20effective%20path%20representations%20has%20become%20increasingly%20essential%0Aacross%20various%20fields%20within%20intelligent%20transportation.%20Although%20pre-trained%0Apath%20representation%20learning%20models%20have%20shown%20improved%20performance%2C%20they%0Apredominantly%20focus%20on%20the%20topological%20structures%20from%20single%20modality%20data%2C%0Ai.e.%2C%20road%20networks%2C%20overlooking%20the%20geometric%20and%20contextual%20features%0Aassociated%20with%20path-related%20images%2C%20e.g.%2C%20remote%20sensing%20images.%20Similar%20to%0Ahuman%20understanding%2C%20integrating%20information%20from%20multiple%20modalities%20can%0Aprovide%20a%20more%20comprehensive%20view%2C%20enhancing%20both%20representation%20accuracy%20and%0Ageneralization.%20However%2C%20variations%20in%20information%20granularity%20impede%20the%0Asemantic%20alignment%20of%20road%20network-based%20paths%20%28road%20paths%29%20and%20image-based%0Apaths%20%28image%20paths%29%2C%20while%20the%20heterogeneity%20of%20multi-modal%20data%20poses%0Asubstantial%20challenges%20for%20effective%20fusion%20and%20utilization.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Multi-modal%2C%20Multi-granularity%20Path%20Representation%20Learning%0AFramework%20%28MM-Path%29%2C%20which%20can%20learn%20a%20generic%20path%20representation%20by%0Aintegrating%20modalities%20from%20both%20road%20paths%20and%20image%20paths.%20To%20enhance%20the%0Aalignment%20of%20multi-modal%20data%2C%20we%20develop%20a%20multi-granularity%20alignment%0Astrategy%20that%20systematically%20associates%20nodes%2C%20road%20sub-paths%2C%20and%20road%20paths%0Awith%20their%20corresponding%20image%20patches%2C%20ensuring%20the%20synchronization%20of%20both%0Adetailed%20local%20information%20and%20broader%20global%20contexts.%20To%20address%20the%0Aheterogeneity%20of%20multi-modal%20data%20effectively%2C%20we%20introduce%20a%20graph-based%0Across-modal%20residual%20fusion%20component%20designed%20to%20comprehensively%20fuse%0Ainformation%20across%20different%20modalities%20and%20granularities.%20Finally%2C%20we%20conduct%0Aextensive%20experiments%20on%20two%20large-scale%20real-world%20datasets%20under%20two%0Adownstream%20tasks%2C%20validating%20the%20effectiveness%20of%20the%20proposed%20MM-Path.%20This%20is%0Aan%20extended%20version%20of%20the%20paper%20accepted%20by%20KDD%202025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18428v1&entry.124074799=Read"},
{"title": "ProteinWeaver: A Divide-and-Assembly Approach for Protein Backbone\n  Design", "author": "Yiming Ma and Fei Ye and Yi Zhou and Zaixiang Zheng and Dongyu Xue and Quanquan Gu", "abstract": "  Nature creates diverse proteins through a 'divide and assembly' strategy.\nInspired by this idea, we introduce ProteinWeaver, a two-stage framework for\nprotein backbone design. Our method first generates individual protein domains\nand then employs an SE(3) diffusion model to flexibly assemble these domains. A\nkey challenge lies in the assembling step, given the complex and rugged nature\nof the inter-domain interaction landscape. To address this challenge, we employ\npreference alignment to discern complex relationships between structure and\ninteraction landscapes through comparative analysis of generated samples.\nComprehensive experiments demonstrate that ProteinWeaver: (1) generates\nhigh-quality, novel protein backbones through versatile domain assembly; (2)\noutperforms RFdiffusion, the current state-of-the-art in backbone design, by\n13\\% and 39\\% for long-chain proteins; (3) shows the potential for cooperative\nfunction design through illustrative case studies. To sum up, by introducing a\n`divide-and-assembly' paradigm, ProteinWeaver advances protein engineering and\nopens new avenues for functional protein design.\n", "link": "http://arxiv.org/abs/2411.16686v2", "date": "2024-11-27", "relevancy": 2.285, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4583}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4583}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProteinWeaver%3A%20A%20Divide-and-Assembly%20Approach%20for%20Protein%20Backbone%0A%20%20Design&body=Title%3A%20ProteinWeaver%3A%20A%20Divide-and-Assembly%20Approach%20for%20Protein%20Backbone%0A%20%20Design%0AAuthor%3A%20Yiming%20Ma%20and%20Fei%20Ye%20and%20Yi%20Zhou%20and%20Zaixiang%20Zheng%20and%20Dongyu%20Xue%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Nature%20creates%20diverse%20proteins%20through%20a%20%27divide%20and%20assembly%27%20strategy.%0AInspired%20by%20this%20idea%2C%20we%20introduce%20ProteinWeaver%2C%20a%20two-stage%20framework%20for%0Aprotein%20backbone%20design.%20Our%20method%20first%20generates%20individual%20protein%20domains%0Aand%20then%20employs%20an%20SE%283%29%20diffusion%20model%20to%20flexibly%20assemble%20these%20domains.%20A%0Akey%20challenge%20lies%20in%20the%20assembling%20step%2C%20given%20the%20complex%20and%20rugged%20nature%0Aof%20the%20inter-domain%20interaction%20landscape.%20To%20address%20this%20challenge%2C%20we%20employ%0Apreference%20alignment%20to%20discern%20complex%20relationships%20between%20structure%20and%0Ainteraction%20landscapes%20through%20comparative%20analysis%20of%20generated%20samples.%0AComprehensive%20experiments%20demonstrate%20that%20ProteinWeaver%3A%20%281%29%20generates%0Ahigh-quality%2C%20novel%20protein%20backbones%20through%20versatile%20domain%20assembly%3B%20%282%29%0Aoutperforms%20RFdiffusion%2C%20the%20current%20state-of-the-art%20in%20backbone%20design%2C%20by%0A13%5C%25%20and%2039%5C%25%20for%20long-chain%20proteins%3B%20%283%29%20shows%20the%20potential%20for%20cooperative%0Afunction%20design%20through%20illustrative%20case%20studies.%20To%20sum%20up%2C%20by%20introducing%20a%0A%60divide-and-assembly%27%20paradigm%2C%20ProteinWeaver%20advances%20protein%20engineering%20and%0Aopens%20new%20avenues%20for%20functional%20protein%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProteinWeaver%253A%2520A%2520Divide-and-Assembly%2520Approach%2520for%2520Protein%2520Backbone%250A%2520%2520Design%26entry.906535625%3DYiming%2520Ma%2520and%2520Fei%2520Ye%2520and%2520Yi%2520Zhou%2520and%2520Zaixiang%2520Zheng%2520and%2520Dongyu%2520Xue%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520Nature%2520creates%2520diverse%2520proteins%2520through%2520a%2520%2527divide%2520and%2520assembly%2527%2520strategy.%250AInspired%2520by%2520this%2520idea%252C%2520we%2520introduce%2520ProteinWeaver%252C%2520a%2520two-stage%2520framework%2520for%250Aprotein%2520backbone%2520design.%2520Our%2520method%2520first%2520generates%2520individual%2520protein%2520domains%250Aand%2520then%2520employs%2520an%2520SE%25283%2529%2520diffusion%2520model%2520to%2520flexibly%2520assemble%2520these%2520domains.%2520A%250Akey%2520challenge%2520lies%2520in%2520the%2520assembling%2520step%252C%2520given%2520the%2520complex%2520and%2520rugged%2520nature%250Aof%2520the%2520inter-domain%2520interaction%2520landscape.%2520To%2520address%2520this%2520challenge%252C%2520we%2520employ%250Apreference%2520alignment%2520to%2520discern%2520complex%2520relationships%2520between%2520structure%2520and%250Ainteraction%2520landscapes%2520through%2520comparative%2520analysis%2520of%2520generated%2520samples.%250AComprehensive%2520experiments%2520demonstrate%2520that%2520ProteinWeaver%253A%2520%25281%2529%2520generates%250Ahigh-quality%252C%2520novel%2520protein%2520backbones%2520through%2520versatile%2520domain%2520assembly%253B%2520%25282%2529%250Aoutperforms%2520RFdiffusion%252C%2520the%2520current%2520state-of-the-art%2520in%2520backbone%2520design%252C%2520by%250A13%255C%2525%2520and%252039%255C%2525%2520for%2520long-chain%2520proteins%253B%2520%25283%2529%2520shows%2520the%2520potential%2520for%2520cooperative%250Afunction%2520design%2520through%2520illustrative%2520case%2520studies.%2520To%2520sum%2520up%252C%2520by%2520introducing%2520a%250A%2560divide-and-assembly%2527%2520paradigm%252C%2520ProteinWeaver%2520advances%2520protein%2520engineering%2520and%250Aopens%2520new%2520avenues%2520for%2520functional%2520protein%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProteinWeaver%3A%20A%20Divide-and-Assembly%20Approach%20for%20Protein%20Backbone%0A%20%20Design&entry.906535625=Yiming%20Ma%20and%20Fei%20Ye%20and%20Yi%20Zhou%20and%20Zaixiang%20Zheng%20and%20Dongyu%20Xue%20and%20Quanquan%20Gu&entry.1292438233=%20%20Nature%20creates%20diverse%20proteins%20through%20a%20%27divide%20and%20assembly%27%20strategy.%0AInspired%20by%20this%20idea%2C%20we%20introduce%20ProteinWeaver%2C%20a%20two-stage%20framework%20for%0Aprotein%20backbone%20design.%20Our%20method%20first%20generates%20individual%20protein%20domains%0Aand%20then%20employs%20an%20SE%283%29%20diffusion%20model%20to%20flexibly%20assemble%20these%20domains.%20A%0Akey%20challenge%20lies%20in%20the%20assembling%20step%2C%20given%20the%20complex%20and%20rugged%20nature%0Aof%20the%20inter-domain%20interaction%20landscape.%20To%20address%20this%20challenge%2C%20we%20employ%0Apreference%20alignment%20to%20discern%20complex%20relationships%20between%20structure%20and%0Ainteraction%20landscapes%20through%20comparative%20analysis%20of%20generated%20samples.%0AComprehensive%20experiments%20demonstrate%20that%20ProteinWeaver%3A%20%281%29%20generates%0Ahigh-quality%2C%20novel%20protein%20backbones%20through%20versatile%20domain%20assembly%3B%20%282%29%0Aoutperforms%20RFdiffusion%2C%20the%20current%20state-of-the-art%20in%20backbone%20design%2C%20by%0A13%5C%25%20and%2039%5C%25%20for%20long-chain%20proteins%3B%20%283%29%20shows%20the%20potential%20for%20cooperative%0Afunction%20design%20through%20illustrative%20case%20studies.%20To%20sum%20up%2C%20by%20introducing%20a%0A%60divide-and-assembly%27%20paradigm%2C%20ProteinWeaver%20advances%20protein%20engineering%20and%0Aopens%20new%20avenues%20for%20functional%20protein%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16686v2&entry.124074799=Read"},
{"title": "Image Segmentation in Foundation Model Era: A Survey", "author": "Tianfei Zhou and Wang Xia and Fei Zhang and Boyu Chang and Wenguan Wang and Ye Yuan and Ender Konukoglu and Daniel Cremers", "abstract": "  Image segmentation is a long-standing challenge in computer vision, studied\ncontinuously over several decades, as evidenced by seminal algorithms such as\nN-Cut, FCN, and MaskFormer. With the advent of foundation models (FMs),\ncontemporary segmentation methodologies have embarked on a new epoch by either\nadapting FMs (e.g., CLIP, Stable Diffusion, DINO) for image segmentation or\ndeveloping dedicated segmentation foundation models (e.g., SAM). These\napproaches not only deliver superior segmentation performance, but also herald\nnewfound segmentation capabilities previously unseen in deep learning context.\nHowever, current research in image segmentation lacks a detailed analysis of\ndistinct characteristics, challenges, and solutions associated with these\nadvancements. This survey seeks to fill this gap by providing a thorough review\nof cutting-edge research centered around FM-driven image segmentation. We\ninvestigate two basic lines of research -- generic image segmentation (i.e.,\nsemantic segmentation, instance segmentation, panoptic segmentation), and\npromptable image segmentation (i.e., interactive segmentation, referring\nsegmentation, few-shot segmentation) -- by delineating their respective task\nsettings, background concepts, and key challenges. Furthermore, we provide\ninsights into the emergence of segmentation knowledge from FMs like CLIP,\nStable Diffusion, and DINO. An exhaustive overview of over 300 segmentation\napproaches is provided to encapsulate the breadth of current research efforts.\nSubsequently, we engage in a discussion of open issues and potential avenues\nfor future research. We envisage that this fresh, comprehensive, and systematic\nsurvey catalyzes the evolution of advanced image segmentation systems. A public\nwebsite is created to continuously track developments in this fast advancing\nfield: \\url{https://github.com/stanley-313/ImageSegFM-Survey}.\n", "link": "http://arxiv.org/abs/2408.12957v3", "date": "2024-11-27", "relevancy": 2.2841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Segmentation%20in%20Foundation%20Model%20Era%3A%20A%20Survey&body=Title%3A%20Image%20Segmentation%20in%20Foundation%20Model%20Era%3A%20A%20Survey%0AAuthor%3A%20Tianfei%20Zhou%20and%20Wang%20Xia%20and%20Fei%20Zhang%20and%20Boyu%20Chang%20and%20Wenguan%20Wang%20and%20Ye%20Yuan%20and%20Ender%20Konukoglu%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Image%20segmentation%20is%20a%20long-standing%20challenge%20in%20computer%20vision%2C%20studied%0Acontinuously%20over%20several%20decades%2C%20as%20evidenced%20by%20seminal%20algorithms%20such%20as%0AN-Cut%2C%20FCN%2C%20and%20MaskFormer.%20With%20the%20advent%20of%20foundation%20models%20%28FMs%29%2C%0Acontemporary%20segmentation%20methodologies%20have%20embarked%20on%20a%20new%20epoch%20by%20either%0Aadapting%20FMs%20%28e.g.%2C%20CLIP%2C%20Stable%20Diffusion%2C%20DINO%29%20for%20image%20segmentation%20or%0Adeveloping%20dedicated%20segmentation%20foundation%20models%20%28e.g.%2C%20SAM%29.%20These%0Aapproaches%20not%20only%20deliver%20superior%20segmentation%20performance%2C%20but%20also%20herald%0Anewfound%20segmentation%20capabilities%20previously%20unseen%20in%20deep%20learning%20context.%0AHowever%2C%20current%20research%20in%20image%20segmentation%20lacks%20a%20detailed%20analysis%20of%0Adistinct%20characteristics%2C%20challenges%2C%20and%20solutions%20associated%20with%20these%0Aadvancements.%20This%20survey%20seeks%20to%20fill%20this%20gap%20by%20providing%20a%20thorough%20review%0Aof%20cutting-edge%20research%20centered%20around%20FM-driven%20image%20segmentation.%20We%0Ainvestigate%20two%20basic%20lines%20of%20research%20--%20generic%20image%20segmentation%20%28i.e.%2C%0Asemantic%20segmentation%2C%20instance%20segmentation%2C%20panoptic%20segmentation%29%2C%20and%0Apromptable%20image%20segmentation%20%28i.e.%2C%20interactive%20segmentation%2C%20referring%0Asegmentation%2C%20few-shot%20segmentation%29%20--%20by%20delineating%20their%20respective%20task%0Asettings%2C%20background%20concepts%2C%20and%20key%20challenges.%20Furthermore%2C%20we%20provide%0Ainsights%20into%20the%20emergence%20of%20segmentation%20knowledge%20from%20FMs%20like%20CLIP%2C%0AStable%20Diffusion%2C%20and%20DINO.%20An%20exhaustive%20overview%20of%20over%20300%20segmentation%0Aapproaches%20is%20provided%20to%20encapsulate%20the%20breadth%20of%20current%20research%20efforts.%0ASubsequently%2C%20we%20engage%20in%20a%20discussion%20of%20open%20issues%20and%20potential%20avenues%0Afor%20future%20research.%20We%20envisage%20that%20this%20fresh%2C%20comprehensive%2C%20and%20systematic%0Asurvey%20catalyzes%20the%20evolution%20of%20advanced%20image%20segmentation%20systems.%20A%20public%0Awebsite%20is%20created%20to%20continuously%20track%20developments%20in%20this%20fast%20advancing%0Afield%3A%20%5Curl%7Bhttps%3A//github.com/stanley-313/ImageSegFM-Survey%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Segmentation%2520in%2520Foundation%2520Model%2520Era%253A%2520A%2520Survey%26entry.906535625%3DTianfei%2520Zhou%2520and%2520Wang%2520Xia%2520and%2520Fei%2520Zhang%2520and%2520Boyu%2520Chang%2520and%2520Wenguan%2520Wang%2520and%2520Ye%2520Yuan%2520and%2520Ender%2520Konukoglu%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520is%2520a%2520long-standing%2520challenge%2520in%2520computer%2520vision%252C%2520studied%250Acontinuously%2520over%2520several%2520decades%252C%2520as%2520evidenced%2520by%2520seminal%2520algorithms%2520such%2520as%250AN-Cut%252C%2520FCN%252C%2520and%2520MaskFormer.%2520With%2520the%2520advent%2520of%2520foundation%2520models%2520%2528FMs%2529%252C%250Acontemporary%2520segmentation%2520methodologies%2520have%2520embarked%2520on%2520a%2520new%2520epoch%2520by%2520either%250Aadapting%2520FMs%2520%2528e.g.%252C%2520CLIP%252C%2520Stable%2520Diffusion%252C%2520DINO%2529%2520for%2520image%2520segmentation%2520or%250Adeveloping%2520dedicated%2520segmentation%2520foundation%2520models%2520%2528e.g.%252C%2520SAM%2529.%2520These%250Aapproaches%2520not%2520only%2520deliver%2520superior%2520segmentation%2520performance%252C%2520but%2520also%2520herald%250Anewfound%2520segmentation%2520capabilities%2520previously%2520unseen%2520in%2520deep%2520learning%2520context.%250AHowever%252C%2520current%2520research%2520in%2520image%2520segmentation%2520lacks%2520a%2520detailed%2520analysis%2520of%250Adistinct%2520characteristics%252C%2520challenges%252C%2520and%2520solutions%2520associated%2520with%2520these%250Aadvancements.%2520This%2520survey%2520seeks%2520to%2520fill%2520this%2520gap%2520by%2520providing%2520a%2520thorough%2520review%250Aof%2520cutting-edge%2520research%2520centered%2520around%2520FM-driven%2520image%2520segmentation.%2520We%250Ainvestigate%2520two%2520basic%2520lines%2520of%2520research%2520--%2520generic%2520image%2520segmentation%2520%2528i.e.%252C%250Asemantic%2520segmentation%252C%2520instance%2520segmentation%252C%2520panoptic%2520segmentation%2529%252C%2520and%250Apromptable%2520image%2520segmentation%2520%2528i.e.%252C%2520interactive%2520segmentation%252C%2520referring%250Asegmentation%252C%2520few-shot%2520segmentation%2529%2520--%2520by%2520delineating%2520their%2520respective%2520task%250Asettings%252C%2520background%2520concepts%252C%2520and%2520key%2520challenges.%2520Furthermore%252C%2520we%2520provide%250Ainsights%2520into%2520the%2520emergence%2520of%2520segmentation%2520knowledge%2520from%2520FMs%2520like%2520CLIP%252C%250AStable%2520Diffusion%252C%2520and%2520DINO.%2520An%2520exhaustive%2520overview%2520of%2520over%2520300%2520segmentation%250Aapproaches%2520is%2520provided%2520to%2520encapsulate%2520the%2520breadth%2520of%2520current%2520research%2520efforts.%250ASubsequently%252C%2520we%2520engage%2520in%2520a%2520discussion%2520of%2520open%2520issues%2520and%2520potential%2520avenues%250Afor%2520future%2520research.%2520We%2520envisage%2520that%2520this%2520fresh%252C%2520comprehensive%252C%2520and%2520systematic%250Asurvey%2520catalyzes%2520the%2520evolution%2520of%2520advanced%2520image%2520segmentation%2520systems.%2520A%2520public%250Awebsite%2520is%2520created%2520to%2520continuously%2520track%2520developments%2520in%2520this%2520fast%2520advancing%250Afield%253A%2520%255Curl%257Bhttps%253A//github.com/stanley-313/ImageSegFM-Survey%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Segmentation%20in%20Foundation%20Model%20Era%3A%20A%20Survey&entry.906535625=Tianfei%20Zhou%20and%20Wang%20Xia%20and%20Fei%20Zhang%20and%20Boyu%20Chang%20and%20Wenguan%20Wang%20and%20Ye%20Yuan%20and%20Ender%20Konukoglu%20and%20Daniel%20Cremers&entry.1292438233=%20%20Image%20segmentation%20is%20a%20long-standing%20challenge%20in%20computer%20vision%2C%20studied%0Acontinuously%20over%20several%20decades%2C%20as%20evidenced%20by%20seminal%20algorithms%20such%20as%0AN-Cut%2C%20FCN%2C%20and%20MaskFormer.%20With%20the%20advent%20of%20foundation%20models%20%28FMs%29%2C%0Acontemporary%20segmentation%20methodologies%20have%20embarked%20on%20a%20new%20epoch%20by%20either%0Aadapting%20FMs%20%28e.g.%2C%20CLIP%2C%20Stable%20Diffusion%2C%20DINO%29%20for%20image%20segmentation%20or%0Adeveloping%20dedicated%20segmentation%20foundation%20models%20%28e.g.%2C%20SAM%29.%20These%0Aapproaches%20not%20only%20deliver%20superior%20segmentation%20performance%2C%20but%20also%20herald%0Anewfound%20segmentation%20capabilities%20previously%20unseen%20in%20deep%20learning%20context.%0AHowever%2C%20current%20research%20in%20image%20segmentation%20lacks%20a%20detailed%20analysis%20of%0Adistinct%20characteristics%2C%20challenges%2C%20and%20solutions%20associated%20with%20these%0Aadvancements.%20This%20survey%20seeks%20to%20fill%20this%20gap%20by%20providing%20a%20thorough%20review%0Aof%20cutting-edge%20research%20centered%20around%20FM-driven%20image%20segmentation.%20We%0Ainvestigate%20two%20basic%20lines%20of%20research%20--%20generic%20image%20segmentation%20%28i.e.%2C%0Asemantic%20segmentation%2C%20instance%20segmentation%2C%20panoptic%20segmentation%29%2C%20and%0Apromptable%20image%20segmentation%20%28i.e.%2C%20interactive%20segmentation%2C%20referring%0Asegmentation%2C%20few-shot%20segmentation%29%20--%20by%20delineating%20their%20respective%20task%0Asettings%2C%20background%20concepts%2C%20and%20key%20challenges.%20Furthermore%2C%20we%20provide%0Ainsights%20into%20the%20emergence%20of%20segmentation%20knowledge%20from%20FMs%20like%20CLIP%2C%0AStable%20Diffusion%2C%20and%20DINO.%20An%20exhaustive%20overview%20of%20over%20300%20segmentation%0Aapproaches%20is%20provided%20to%20encapsulate%20the%20breadth%20of%20current%20research%20efforts.%0ASubsequently%2C%20we%20engage%20in%20a%20discussion%20of%20open%20issues%20and%20potential%20avenues%0Afor%20future%20research.%20We%20envisage%20that%20this%20fresh%2C%20comprehensive%2C%20and%20systematic%0Asurvey%20catalyzes%20the%20evolution%20of%20advanced%20image%20segmentation%20systems.%20A%20public%0Awebsite%20is%20created%20to%20continuously%20track%20developments%20in%20this%20fast%20advancing%0Afield%3A%20%5Curl%7Bhttps%3A//github.com/stanley-313/ImageSegFM-Survey%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12957v3&entry.124074799=Read"},
{"title": "Factorized Visual Tokenization and Generation", "author": "Zechen Bai and Jianxiong Gao and Ziteng Gao and Pichao Wang and Zheng Zhang and Tong He and Mike Zheng Shou", "abstract": "  Visual tokenizers are fundamental to image generation. They convert visual\ndata into discrete tokens, enabling transformer-based models to excel at image\ngeneration. Despite their success, VQ-based tokenizers like VQGAN face\nsignificant limitations due to constrained vocabulary sizes. Simply expanding\nthe codebook often leads to training instability and diminishing performance\ngains, making scalability a critical challenge. In this work, we introduce\nFactorized Quantization (FQ), a novel approach that revitalizes VQ-based\ntokenizers by decomposing a large codebook into multiple independent\nsub-codebooks. This factorization reduces the lookup complexity of large\ncodebooks, enabling more efficient and scalable visual tokenization. To ensure\neach sub-codebook captures distinct and complementary information, we propose a\ndisentanglement regularization that explicitly reduces redundancy, promoting\ndiversity across the sub-codebooks. Furthermore, we integrate representation\nlearning into the training process, leveraging pretrained vision models like\nCLIP and DINO to infuse semantic richness into the learned representations.\nThis design ensures our tokenizer captures diverse semantic levels, leading to\nmore expressive and disentangled representations. Experiments show that the\nproposed FQGAN model substantially improves the reconstruction quality of\nvisual tokenizers, achieving state-of-the-art performance. We further\ndemonstrate that this tokenizer can be effectively adapted into auto-regressive\nimage generation. https://showlab.github.io/FQGAN\n", "link": "http://arxiv.org/abs/2411.16681v2", "date": "2024-11-27", "relevancy": 2.2672, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6072}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5426}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factorized%20Visual%20Tokenization%20and%20Generation&body=Title%3A%20Factorized%20Visual%20Tokenization%20and%20Generation%0AAuthor%3A%20Zechen%20Bai%20and%20Jianxiong%20Gao%20and%20Ziteng%20Gao%20and%20Pichao%20Wang%20and%20Zheng%20Zhang%20and%20Tong%20He%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Visual%20tokenizers%20are%20fundamental%20to%20image%20generation.%20They%20convert%20visual%0Adata%20into%20discrete%20tokens%2C%20enabling%20transformer-based%20models%20to%20excel%20at%20image%0Ageneration.%20Despite%20their%20success%2C%20VQ-based%20tokenizers%20like%20VQGAN%20face%0Asignificant%20limitations%20due%20to%20constrained%20vocabulary%20sizes.%20Simply%20expanding%0Athe%20codebook%20often%20leads%20to%20training%20instability%20and%20diminishing%20performance%0Agains%2C%20making%20scalability%20a%20critical%20challenge.%20In%20this%20work%2C%20we%20introduce%0AFactorized%20Quantization%20%28FQ%29%2C%20a%20novel%20approach%20that%20revitalizes%20VQ-based%0Atokenizers%20by%20decomposing%20a%20large%20codebook%20into%20multiple%20independent%0Asub-codebooks.%20This%20factorization%20reduces%20the%20lookup%20complexity%20of%20large%0Acodebooks%2C%20enabling%20more%20efficient%20and%20scalable%20visual%20tokenization.%20To%20ensure%0Aeach%20sub-codebook%20captures%20distinct%20and%20complementary%20information%2C%20we%20propose%20a%0Adisentanglement%20regularization%20that%20explicitly%20reduces%20redundancy%2C%20promoting%0Adiversity%20across%20the%20sub-codebooks.%20Furthermore%2C%20we%20integrate%20representation%0Alearning%20into%20the%20training%20process%2C%20leveraging%20pretrained%20vision%20models%20like%0ACLIP%20and%20DINO%20to%20infuse%20semantic%20richness%20into%20the%20learned%20representations.%0AThis%20design%20ensures%20our%20tokenizer%20captures%20diverse%20semantic%20levels%2C%20leading%20to%0Amore%20expressive%20and%20disentangled%20representations.%20Experiments%20show%20that%20the%0Aproposed%20FQGAN%20model%20substantially%20improves%20the%20reconstruction%20quality%20of%0Avisual%20tokenizers%2C%20achieving%20state-of-the-art%20performance.%20We%20further%0Ademonstrate%20that%20this%20tokenizer%20can%20be%20effectively%20adapted%20into%20auto-regressive%0Aimage%20generation.%20https%3A//showlab.github.io/FQGAN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorized%2520Visual%2520Tokenization%2520and%2520Generation%26entry.906535625%3DZechen%2520Bai%2520and%2520Jianxiong%2520Gao%2520and%2520Ziteng%2520Gao%2520and%2520Pichao%2520Wang%2520and%2520Zheng%2520Zhang%2520and%2520Tong%2520He%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Visual%2520tokenizers%2520are%2520fundamental%2520to%2520image%2520generation.%2520They%2520convert%2520visual%250Adata%2520into%2520discrete%2520tokens%252C%2520enabling%2520transformer-based%2520models%2520to%2520excel%2520at%2520image%250Ageneration.%2520Despite%2520their%2520success%252C%2520VQ-based%2520tokenizers%2520like%2520VQGAN%2520face%250Asignificant%2520limitations%2520due%2520to%2520constrained%2520vocabulary%2520sizes.%2520Simply%2520expanding%250Athe%2520codebook%2520often%2520leads%2520to%2520training%2520instability%2520and%2520diminishing%2520performance%250Agains%252C%2520making%2520scalability%2520a%2520critical%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%250AFactorized%2520Quantization%2520%2528FQ%2529%252C%2520a%2520novel%2520approach%2520that%2520revitalizes%2520VQ-based%250Atokenizers%2520by%2520decomposing%2520a%2520large%2520codebook%2520into%2520multiple%2520independent%250Asub-codebooks.%2520This%2520factorization%2520reduces%2520the%2520lookup%2520complexity%2520of%2520large%250Acodebooks%252C%2520enabling%2520more%2520efficient%2520and%2520scalable%2520visual%2520tokenization.%2520To%2520ensure%250Aeach%2520sub-codebook%2520captures%2520distinct%2520and%2520complementary%2520information%252C%2520we%2520propose%2520a%250Adisentanglement%2520regularization%2520that%2520explicitly%2520reduces%2520redundancy%252C%2520promoting%250Adiversity%2520across%2520the%2520sub-codebooks.%2520Furthermore%252C%2520we%2520integrate%2520representation%250Alearning%2520into%2520the%2520training%2520process%252C%2520leveraging%2520pretrained%2520vision%2520models%2520like%250ACLIP%2520and%2520DINO%2520to%2520infuse%2520semantic%2520richness%2520into%2520the%2520learned%2520representations.%250AThis%2520design%2520ensures%2520our%2520tokenizer%2520captures%2520diverse%2520semantic%2520levels%252C%2520leading%2520to%250Amore%2520expressive%2520and%2520disentangled%2520representations.%2520Experiments%2520show%2520that%2520the%250Aproposed%2520FQGAN%2520model%2520substantially%2520improves%2520the%2520reconstruction%2520quality%2520of%250Avisual%2520tokenizers%252C%2520achieving%2520state-of-the-art%2520performance.%2520We%2520further%250Ademonstrate%2520that%2520this%2520tokenizer%2520can%2520be%2520effectively%2520adapted%2520into%2520auto-regressive%250Aimage%2520generation.%2520https%253A//showlab.github.io/FQGAN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorized%20Visual%20Tokenization%20and%20Generation&entry.906535625=Zechen%20Bai%20and%20Jianxiong%20Gao%20and%20Ziteng%20Gao%20and%20Pichao%20Wang%20and%20Zheng%20Zhang%20and%20Tong%20He%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Visual%20tokenizers%20are%20fundamental%20to%20image%20generation.%20They%20convert%20visual%0Adata%20into%20discrete%20tokens%2C%20enabling%20transformer-based%20models%20to%20excel%20at%20image%0Ageneration.%20Despite%20their%20success%2C%20VQ-based%20tokenizers%20like%20VQGAN%20face%0Asignificant%20limitations%20due%20to%20constrained%20vocabulary%20sizes.%20Simply%20expanding%0Athe%20codebook%20often%20leads%20to%20training%20instability%20and%20diminishing%20performance%0Agains%2C%20making%20scalability%20a%20critical%20challenge.%20In%20this%20work%2C%20we%20introduce%0AFactorized%20Quantization%20%28FQ%29%2C%20a%20novel%20approach%20that%20revitalizes%20VQ-based%0Atokenizers%20by%20decomposing%20a%20large%20codebook%20into%20multiple%20independent%0Asub-codebooks.%20This%20factorization%20reduces%20the%20lookup%20complexity%20of%20large%0Acodebooks%2C%20enabling%20more%20efficient%20and%20scalable%20visual%20tokenization.%20To%20ensure%0Aeach%20sub-codebook%20captures%20distinct%20and%20complementary%20information%2C%20we%20propose%20a%0Adisentanglement%20regularization%20that%20explicitly%20reduces%20redundancy%2C%20promoting%0Adiversity%20across%20the%20sub-codebooks.%20Furthermore%2C%20we%20integrate%20representation%0Alearning%20into%20the%20training%20process%2C%20leveraging%20pretrained%20vision%20models%20like%0ACLIP%20and%20DINO%20to%20infuse%20semantic%20richness%20into%20the%20learned%20representations.%0AThis%20design%20ensures%20our%20tokenizer%20captures%20diverse%20semantic%20levels%2C%20leading%20to%0Amore%20expressive%20and%20disentangled%20representations.%20Experiments%20show%20that%20the%0Aproposed%20FQGAN%20model%20substantially%20improves%20the%20reconstruction%20quality%20of%0Avisual%20tokenizers%2C%20achieving%20state-of-the-art%20performance.%20We%20further%0Ademonstrate%20that%20this%20tokenizer%20can%20be%20effectively%20adapted%20into%20auto-regressive%0Aimage%20generation.%20https%3A//showlab.github.io/FQGAN%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16681v2&entry.124074799=Read"},
{"title": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning\n  in Large Language Models", "author": "Rong Wang and Kun Sun and Jonas Kuhn", "abstract": "  Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks. However, LLMs often struggle with spatial reasoning which is one\nessential part of reasoning and inference and requires understanding complex\nrelationships between objects in space. This paper proposes a novel\nneural-symbolic framework that enhances LLMs' spatial reasoning abilities. We\nevaluate our approach on two benchmark datasets: StepGame and SparQA,\nimplementing three distinct strategies: (1) ASP (Answer Set Programming)-based\nsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical\nrules. Our experiments demonstrate significant improvements over the baseline\nprompting methods, with accuracy increases of 40-50% on StepGame} dataset and\n3-13% on the more complex SparQA dataset. The \"LLM + ASP\" pipeline achieves\nparticularly strong results on the tasks of Finding Relations (FR) and Finding\nBlock (FB) questions, though performance varies across different question\ntypes. The impressive results suggest that while neural-symbolic approaches\noffer promising directions for enhancing spatial reasoning in LLMs, their\neffectiveness depends heavily on the specific task characteristics and\nimplementation strategies. We propose an integrated, simple yet effective set\nof strategies using a neural-symbolic pipeline to boost spatial reasoning\nabilities in LLMs. This pipeline and its strategies demonstrate strong and\nbroader applicability to other reasoning domains in LLMs, such as temporal\nreasoning, deductive inference etc.\n", "link": "http://arxiv.org/abs/2411.18564v1", "date": "2024-11-27", "relevancy": 2.26, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pipeline%20of%20Neural-Symbolic%20Integration%20to%20Enhance%20Spatial%20Reasoning%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20A%20Pipeline%20of%20Neural-Symbolic%20Integration%20to%20Enhance%20Spatial%20Reasoning%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Rong%20Wang%20and%20Kun%20Sun%20and%20Jonas%20Kuhn%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Avarious%20tasks.%20However%2C%20LLMs%20often%20struggle%20with%20spatial%20reasoning%20which%20is%20one%0Aessential%20part%20of%20reasoning%20and%20inference%20and%20requires%20understanding%20complex%0Arelationships%20between%20objects%20in%20space.%20This%20paper%20proposes%20a%20novel%0Aneural-symbolic%20framework%20that%20enhances%20LLMs%27%20spatial%20reasoning%20abilities.%20We%0Aevaluate%20our%20approach%20on%20two%20benchmark%20datasets%3A%20StepGame%20and%20SparQA%2C%0Aimplementing%20three%20distinct%20strategies%3A%20%281%29%20ASP%20%28Answer%20Set%20Programming%29-based%0Asymbolic%20reasoning%2C%20%282%29%20LLM%20%2B%20ASP%20pipeline%20using%20DSPy%2C%20and%20%283%29%20Fact%20%2B%20Logical%0Arules.%20Our%20experiments%20demonstrate%20significant%20improvements%20over%20the%20baseline%0Aprompting%20methods%2C%20with%20accuracy%20increases%20of%2040-50%25%20on%20StepGame%7D%20dataset%20and%0A3-13%25%20on%20the%20more%20complex%20SparQA%20dataset.%20The%20%22LLM%20%2B%20ASP%22%20pipeline%20achieves%0Aparticularly%20strong%20results%20on%20the%20tasks%20of%20Finding%20Relations%20%28FR%29%20and%20Finding%0ABlock%20%28FB%29%20questions%2C%20though%20performance%20varies%20across%20different%20question%0Atypes.%20The%20impressive%20results%20suggest%20that%20while%20neural-symbolic%20approaches%0Aoffer%20promising%20directions%20for%20enhancing%20spatial%20reasoning%20in%20LLMs%2C%20their%0Aeffectiveness%20depends%20heavily%20on%20the%20specific%20task%20characteristics%20and%0Aimplementation%20strategies.%20We%20propose%20an%20integrated%2C%20simple%20yet%20effective%20set%0Aof%20strategies%20using%20a%20neural-symbolic%20pipeline%20to%20boost%20spatial%20reasoning%0Aabilities%20in%20LLMs.%20This%20pipeline%20and%20its%20strategies%20demonstrate%20strong%20and%0Abroader%20applicability%20to%20other%20reasoning%20domains%20in%20LLMs%2C%20such%20as%20temporal%0Areasoning%2C%20deductive%20inference%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pipeline%2520of%2520Neural-Symbolic%2520Integration%2520to%2520Enhance%2520Spatial%2520Reasoning%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DRong%2520Wang%2520and%2520Kun%2520Sun%2520and%2520Jonas%2520Kuhn%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%250Avarious%2520tasks.%2520However%252C%2520LLMs%2520often%2520struggle%2520with%2520spatial%2520reasoning%2520which%2520is%2520one%250Aessential%2520part%2520of%2520reasoning%2520and%2520inference%2520and%2520requires%2520understanding%2520complex%250Arelationships%2520between%2520objects%2520in%2520space.%2520This%2520paper%2520proposes%2520a%2520novel%250Aneural-symbolic%2520framework%2520that%2520enhances%2520LLMs%2527%2520spatial%2520reasoning%2520abilities.%2520We%250Aevaluate%2520our%2520approach%2520on%2520two%2520benchmark%2520datasets%253A%2520StepGame%2520and%2520SparQA%252C%250Aimplementing%2520three%2520distinct%2520strategies%253A%2520%25281%2529%2520ASP%2520%2528Answer%2520Set%2520Programming%2529-based%250Asymbolic%2520reasoning%252C%2520%25282%2529%2520LLM%2520%252B%2520ASP%2520pipeline%2520using%2520DSPy%252C%2520and%2520%25283%2529%2520Fact%2520%252B%2520Logical%250Arules.%2520Our%2520experiments%2520demonstrate%2520significant%2520improvements%2520over%2520the%2520baseline%250Aprompting%2520methods%252C%2520with%2520accuracy%2520increases%2520of%252040-50%2525%2520on%2520StepGame%257D%2520dataset%2520and%250A3-13%2525%2520on%2520the%2520more%2520complex%2520SparQA%2520dataset.%2520The%2520%2522LLM%2520%252B%2520ASP%2522%2520pipeline%2520achieves%250Aparticularly%2520strong%2520results%2520on%2520the%2520tasks%2520of%2520Finding%2520Relations%2520%2528FR%2529%2520and%2520Finding%250ABlock%2520%2528FB%2529%2520questions%252C%2520though%2520performance%2520varies%2520across%2520different%2520question%250Atypes.%2520The%2520impressive%2520results%2520suggest%2520that%2520while%2520neural-symbolic%2520approaches%250Aoffer%2520promising%2520directions%2520for%2520enhancing%2520spatial%2520reasoning%2520in%2520LLMs%252C%2520their%250Aeffectiveness%2520depends%2520heavily%2520on%2520the%2520specific%2520task%2520characteristics%2520and%250Aimplementation%2520strategies.%2520We%2520propose%2520an%2520integrated%252C%2520simple%2520yet%2520effective%2520set%250Aof%2520strategies%2520using%2520a%2520neural-symbolic%2520pipeline%2520to%2520boost%2520spatial%2520reasoning%250Aabilities%2520in%2520LLMs.%2520This%2520pipeline%2520and%2520its%2520strategies%2520demonstrate%2520strong%2520and%250Abroader%2520applicability%2520to%2520other%2520reasoning%2520domains%2520in%2520LLMs%252C%2520such%2520as%2520temporal%250Areasoning%252C%2520deductive%2520inference%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pipeline%20of%20Neural-Symbolic%20Integration%20to%20Enhance%20Spatial%20Reasoning%0A%20%20in%20Large%20Language%20Models&entry.906535625=Rong%20Wang%20and%20Kun%20Sun%20and%20Jonas%20Kuhn&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%0Avarious%20tasks.%20However%2C%20LLMs%20often%20struggle%20with%20spatial%20reasoning%20which%20is%20one%0Aessential%20part%20of%20reasoning%20and%20inference%20and%20requires%20understanding%20complex%0Arelationships%20between%20objects%20in%20space.%20This%20paper%20proposes%20a%20novel%0Aneural-symbolic%20framework%20that%20enhances%20LLMs%27%20spatial%20reasoning%20abilities.%20We%0Aevaluate%20our%20approach%20on%20two%20benchmark%20datasets%3A%20StepGame%20and%20SparQA%2C%0Aimplementing%20three%20distinct%20strategies%3A%20%281%29%20ASP%20%28Answer%20Set%20Programming%29-based%0Asymbolic%20reasoning%2C%20%282%29%20LLM%20%2B%20ASP%20pipeline%20using%20DSPy%2C%20and%20%283%29%20Fact%20%2B%20Logical%0Arules.%20Our%20experiments%20demonstrate%20significant%20improvements%20over%20the%20baseline%0Aprompting%20methods%2C%20with%20accuracy%20increases%20of%2040-50%25%20on%20StepGame%7D%20dataset%20and%0A3-13%25%20on%20the%20more%20complex%20SparQA%20dataset.%20The%20%22LLM%20%2B%20ASP%22%20pipeline%20achieves%0Aparticularly%20strong%20results%20on%20the%20tasks%20of%20Finding%20Relations%20%28FR%29%20and%20Finding%0ABlock%20%28FB%29%20questions%2C%20though%20performance%20varies%20across%20different%20question%0Atypes.%20The%20impressive%20results%20suggest%20that%20while%20neural-symbolic%20approaches%0Aoffer%20promising%20directions%20for%20enhancing%20spatial%20reasoning%20in%20LLMs%2C%20their%0Aeffectiveness%20depends%20heavily%20on%20the%20specific%20task%20characteristics%20and%0Aimplementation%20strategies.%20We%20propose%20an%20integrated%2C%20simple%20yet%20effective%20set%0Aof%20strategies%20using%20a%20neural-symbolic%20pipeline%20to%20boost%20spatial%20reasoning%0Aabilities%20in%20LLMs.%20This%20pipeline%20and%20its%20strategies%20demonstrate%20strong%20and%0Abroader%20applicability%20to%20other%20reasoning%20domains%20in%20LLMs%2C%20such%20as%20temporal%0Areasoning%2C%20deductive%20inference%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18564v1&entry.124074799=Read"},
{"title": "Deep End-to-end Adaptive k-Space Sampling, Reconstruction, and\n  Registration for Dynamic MRI", "author": "George Yiasemis and Jan-Jakob Sonke and Jonas Teuwen", "abstract": "  Dynamic MRI enables a range of clinical applications, including cardiac\nfunction assessment, organ motion tracking, and radiotherapy guidance. However,\nfully sampling the dynamic k-space data is often infeasible due to time\nconstraints and physiological motion such as respiratory and cardiac motion.\nThis necessitates undersampling, which degrades the quality of reconstructed\nimages. Poor image quality not only hinders visualization but also impairs the\nestimation of deformation fields, crucial for registering dynamic (moving)\nimages to a static reference image. This registration enables tasks such as\nmotion correction, treatment planning, and quantitative analysis in\napplications like cardiac imaging and MR-guided radiotherapy. To overcome the\nchallenges posed by undersampling and motion, we introduce an end-to-end deep\nlearning (DL) framework that integrates adaptive dynamic k-space sampling,\nreconstruction, and registration. Our approach begins with a DL-based adaptive\nsampling strategy, optimizing dynamic k-space acquisition to capture the most\nrelevant data for each specific case. This is followed by a DL-based\nreconstruction module that produces images optimized for accurate deformation\nfield estimation from the undersampled moving data. Finally, a registration\nmodule estimates the deformation fields aligning the reconstructed dynamic\nimages with a static reference. The proposed framework is independent of\nspecific reconstruction and registration modules allowing for plug-and-play\nintegration of these components. The entire framework is jointly trained using\na combination of supervised and unsupervised loss functions, enabling\nend-to-end optimization for improved performance across all components. Through\ncontrolled experiments and ablation studies, we validate each component,\ndemonstrating that each choice contributes to robust motion estimation from\nundersampled dynamic data.\n", "link": "http://arxiv.org/abs/2411.18249v1", "date": "2024-11-27", "relevancy": 2.2465, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5831}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20End-to-end%20Adaptive%20k-Space%20Sampling%2C%20Reconstruction%2C%20and%0A%20%20Registration%20for%20Dynamic%20MRI&body=Title%3A%20Deep%20End-to-end%20Adaptive%20k-Space%20Sampling%2C%20Reconstruction%2C%20and%0A%20%20Registration%20for%20Dynamic%20MRI%0AAuthor%3A%20George%20Yiasemis%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Dynamic%20MRI%20enables%20a%20range%20of%20clinical%20applications%2C%20including%20cardiac%0Afunction%20assessment%2C%20organ%20motion%20tracking%2C%20and%20radiotherapy%20guidance.%20However%2C%0Afully%20sampling%20the%20dynamic%20k-space%20data%20is%20often%20infeasible%20due%20to%20time%0Aconstraints%20and%20physiological%20motion%20such%20as%20respiratory%20and%20cardiac%20motion.%0AThis%20necessitates%20undersampling%2C%20which%20degrades%20the%20quality%20of%20reconstructed%0Aimages.%20Poor%20image%20quality%20not%20only%20hinders%20visualization%20but%20also%20impairs%20the%0Aestimation%20of%20deformation%20fields%2C%20crucial%20for%20registering%20dynamic%20%28moving%29%0Aimages%20to%20a%20static%20reference%20image.%20This%20registration%20enables%20tasks%20such%20as%0Amotion%20correction%2C%20treatment%20planning%2C%20and%20quantitative%20analysis%20in%0Aapplications%20like%20cardiac%20imaging%20and%20MR-guided%20radiotherapy.%20To%20overcome%20the%0Achallenges%20posed%20by%20undersampling%20and%20motion%2C%20we%20introduce%20an%20end-to-end%20deep%0Alearning%20%28DL%29%20framework%20that%20integrates%20adaptive%20dynamic%20k-space%20sampling%2C%0Areconstruction%2C%20and%20registration.%20Our%20approach%20begins%20with%20a%20DL-based%20adaptive%0Asampling%20strategy%2C%20optimizing%20dynamic%20k-space%20acquisition%20to%20capture%20the%20most%0Arelevant%20data%20for%20each%20specific%20case.%20This%20is%20followed%20by%20a%20DL-based%0Areconstruction%20module%20that%20produces%20images%20optimized%20for%20accurate%20deformation%0Afield%20estimation%20from%20the%20undersampled%20moving%20data.%20Finally%2C%20a%20registration%0Amodule%20estimates%20the%20deformation%20fields%20aligning%20the%20reconstructed%20dynamic%0Aimages%20with%20a%20static%20reference.%20The%20proposed%20framework%20is%20independent%20of%0Aspecific%20reconstruction%20and%20registration%20modules%20allowing%20for%20plug-and-play%0Aintegration%20of%20these%20components.%20The%20entire%20framework%20is%20jointly%20trained%20using%0Aa%20combination%20of%20supervised%20and%20unsupervised%20loss%20functions%2C%20enabling%0Aend-to-end%20optimization%20for%20improved%20performance%20across%20all%20components.%20Through%0Acontrolled%20experiments%20and%20ablation%20studies%2C%20we%20validate%20each%20component%2C%0Ademonstrating%20that%20each%20choice%20contributes%20to%20robust%20motion%20estimation%20from%0Aundersampled%20dynamic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520End-to-end%2520Adaptive%2520k-Space%2520Sampling%252C%2520Reconstruction%252C%2520and%250A%2520%2520Registration%2520for%2520Dynamic%2520MRI%26entry.906535625%3DGeorge%2520Yiasemis%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Dynamic%2520MRI%2520enables%2520a%2520range%2520of%2520clinical%2520applications%252C%2520including%2520cardiac%250Afunction%2520assessment%252C%2520organ%2520motion%2520tracking%252C%2520and%2520radiotherapy%2520guidance.%2520However%252C%250Afully%2520sampling%2520the%2520dynamic%2520k-space%2520data%2520is%2520often%2520infeasible%2520due%2520to%2520time%250Aconstraints%2520and%2520physiological%2520motion%2520such%2520as%2520respiratory%2520and%2520cardiac%2520motion.%250AThis%2520necessitates%2520undersampling%252C%2520which%2520degrades%2520the%2520quality%2520of%2520reconstructed%250Aimages.%2520Poor%2520image%2520quality%2520not%2520only%2520hinders%2520visualization%2520but%2520also%2520impairs%2520the%250Aestimation%2520of%2520deformation%2520fields%252C%2520crucial%2520for%2520registering%2520dynamic%2520%2528moving%2529%250Aimages%2520to%2520a%2520static%2520reference%2520image.%2520This%2520registration%2520enables%2520tasks%2520such%2520as%250Amotion%2520correction%252C%2520treatment%2520planning%252C%2520and%2520quantitative%2520analysis%2520in%250Aapplications%2520like%2520cardiac%2520imaging%2520and%2520MR-guided%2520radiotherapy.%2520To%2520overcome%2520the%250Achallenges%2520posed%2520by%2520undersampling%2520and%2520motion%252C%2520we%2520introduce%2520an%2520end-to-end%2520deep%250Alearning%2520%2528DL%2529%2520framework%2520that%2520integrates%2520adaptive%2520dynamic%2520k-space%2520sampling%252C%250Areconstruction%252C%2520and%2520registration.%2520Our%2520approach%2520begins%2520with%2520a%2520DL-based%2520adaptive%250Asampling%2520strategy%252C%2520optimizing%2520dynamic%2520k-space%2520acquisition%2520to%2520capture%2520the%2520most%250Arelevant%2520data%2520for%2520each%2520specific%2520case.%2520This%2520is%2520followed%2520by%2520a%2520DL-based%250Areconstruction%2520module%2520that%2520produces%2520images%2520optimized%2520for%2520accurate%2520deformation%250Afield%2520estimation%2520from%2520the%2520undersampled%2520moving%2520data.%2520Finally%252C%2520a%2520registration%250Amodule%2520estimates%2520the%2520deformation%2520fields%2520aligning%2520the%2520reconstructed%2520dynamic%250Aimages%2520with%2520a%2520static%2520reference.%2520The%2520proposed%2520framework%2520is%2520independent%2520of%250Aspecific%2520reconstruction%2520and%2520registration%2520modules%2520allowing%2520for%2520plug-and-play%250Aintegration%2520of%2520these%2520components.%2520The%2520entire%2520framework%2520is%2520jointly%2520trained%2520using%250Aa%2520combination%2520of%2520supervised%2520and%2520unsupervised%2520loss%2520functions%252C%2520enabling%250Aend-to-end%2520optimization%2520for%2520improved%2520performance%2520across%2520all%2520components.%2520Through%250Acontrolled%2520experiments%2520and%2520ablation%2520studies%252C%2520we%2520validate%2520each%2520component%252C%250Ademonstrating%2520that%2520each%2520choice%2520contributes%2520to%2520robust%2520motion%2520estimation%2520from%250Aundersampled%2520dynamic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20End-to-end%20Adaptive%20k-Space%20Sampling%2C%20Reconstruction%2C%20and%0A%20%20Registration%20for%20Dynamic%20MRI&entry.906535625=George%20Yiasemis%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Dynamic%20MRI%20enables%20a%20range%20of%20clinical%20applications%2C%20including%20cardiac%0Afunction%20assessment%2C%20organ%20motion%20tracking%2C%20and%20radiotherapy%20guidance.%20However%2C%0Afully%20sampling%20the%20dynamic%20k-space%20data%20is%20often%20infeasible%20due%20to%20time%0Aconstraints%20and%20physiological%20motion%20such%20as%20respiratory%20and%20cardiac%20motion.%0AThis%20necessitates%20undersampling%2C%20which%20degrades%20the%20quality%20of%20reconstructed%0Aimages.%20Poor%20image%20quality%20not%20only%20hinders%20visualization%20but%20also%20impairs%20the%0Aestimation%20of%20deformation%20fields%2C%20crucial%20for%20registering%20dynamic%20%28moving%29%0Aimages%20to%20a%20static%20reference%20image.%20This%20registration%20enables%20tasks%20such%20as%0Amotion%20correction%2C%20treatment%20planning%2C%20and%20quantitative%20analysis%20in%0Aapplications%20like%20cardiac%20imaging%20and%20MR-guided%20radiotherapy.%20To%20overcome%20the%0Achallenges%20posed%20by%20undersampling%20and%20motion%2C%20we%20introduce%20an%20end-to-end%20deep%0Alearning%20%28DL%29%20framework%20that%20integrates%20adaptive%20dynamic%20k-space%20sampling%2C%0Areconstruction%2C%20and%20registration.%20Our%20approach%20begins%20with%20a%20DL-based%20adaptive%0Asampling%20strategy%2C%20optimizing%20dynamic%20k-space%20acquisition%20to%20capture%20the%20most%0Arelevant%20data%20for%20each%20specific%20case.%20This%20is%20followed%20by%20a%20DL-based%0Areconstruction%20module%20that%20produces%20images%20optimized%20for%20accurate%20deformation%0Afield%20estimation%20from%20the%20undersampled%20moving%20data.%20Finally%2C%20a%20registration%0Amodule%20estimates%20the%20deformation%20fields%20aligning%20the%20reconstructed%20dynamic%0Aimages%20with%20a%20static%20reference.%20The%20proposed%20framework%20is%20independent%20of%0Aspecific%20reconstruction%20and%20registration%20modules%20allowing%20for%20plug-and-play%0Aintegration%20of%20these%20components.%20The%20entire%20framework%20is%20jointly%20trained%20using%0Aa%20combination%20of%20supervised%20and%20unsupervised%20loss%20functions%2C%20enabling%0Aend-to-end%20optimization%20for%20improved%20performance%20across%20all%20components.%20Through%0Acontrolled%20experiments%20and%20ablation%20studies%2C%20we%20validate%20each%20component%2C%0Ademonstrating%20that%20each%20choice%20contributes%20to%20robust%20motion%20estimation%20from%0Aundersampled%20dynamic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18249v1&entry.124074799=Read"},
{"title": "Hierarchical Information Flow for Generalized Efficient Image\n  Restoration", "author": "Yawei Li and Bin Ren and Jingyun Liang and Rakesh Ranjan and Mengyuan Liu and Nicu Sebe and Ming-Hsuan Yang and Luca Benini", "abstract": "  While vision transformers show promise in numerous image restoration (IR)\ntasks, the challenge remains in efficiently generalizing and scaling up a model\nfor multiple IR tasks. To strike a balance between efficiency and model\ncapacity for a generalized transformer-based IR method, we propose a\nhierarchical information flow mechanism for image restoration, dubbed Hi-IR,\nwhich progressively propagates information among pixels in a bottom-up manner.\nHi-IR constructs a hierarchical information tree representing the degraded\nimage across three levels. Each level encapsulates different types of\ninformation, with higher levels encompassing broader objects and concepts and\nlower levels focusing on local details. Moreover, the hierarchical tree\narchitecture removes long-range self-attention, improves the computational\nefficiency and memory utilization, thus preparing it for effective model\nscaling. Based on that, we explore model scaling to improve our method's\ncapabilities, which is expected to positively impact IR in large-scale training\nsettings. Extensive experimental results show that Hi-IR achieves\nstate-of-the-art performance in seven common image restoration tasks, affirming\nits effectiveness and generalizability.\n", "link": "http://arxiv.org/abs/2411.18588v1", "date": "2024-11-27", "relevancy": 2.2442, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.55}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Information%20Flow%20for%20Generalized%20Efficient%20Image%0A%20%20Restoration&body=Title%3A%20Hierarchical%20Information%20Flow%20for%20Generalized%20Efficient%20Image%0A%20%20Restoration%0AAuthor%3A%20Yawei%20Li%20and%20Bin%20Ren%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Nicu%20Sebe%20and%20Ming-Hsuan%20Yang%20and%20Luca%20Benini%0AAbstract%3A%20%20%20While%20vision%20transformers%20show%20promise%20in%20numerous%20image%20restoration%20%28IR%29%0Atasks%2C%20the%20challenge%20remains%20in%20efficiently%20generalizing%20and%20scaling%20up%20a%20model%0Afor%20multiple%20IR%20tasks.%20To%20strike%20a%20balance%20between%20efficiency%20and%20model%0Acapacity%20for%20a%20generalized%20transformer-based%20IR%20method%2C%20we%20propose%20a%0Ahierarchical%20information%20flow%20mechanism%20for%20image%20restoration%2C%20dubbed%20Hi-IR%2C%0Awhich%20progressively%20propagates%20information%20among%20pixels%20in%20a%20bottom-up%20manner.%0AHi-IR%20constructs%20a%20hierarchical%20information%20tree%20representing%20the%20degraded%0Aimage%20across%20three%20levels.%20Each%20level%20encapsulates%20different%20types%20of%0Ainformation%2C%20with%20higher%20levels%20encompassing%20broader%20objects%20and%20concepts%20and%0Alower%20levels%20focusing%20on%20local%20details.%20Moreover%2C%20the%20hierarchical%20tree%0Aarchitecture%20removes%20long-range%20self-attention%2C%20improves%20the%20computational%0Aefficiency%20and%20memory%20utilization%2C%20thus%20preparing%20it%20for%20effective%20model%0Ascaling.%20Based%20on%20that%2C%20we%20explore%20model%20scaling%20to%20improve%20our%20method%27s%0Acapabilities%2C%20which%20is%20expected%20to%20positively%20impact%20IR%20in%20large-scale%20training%0Asettings.%20Extensive%20experimental%20results%20show%20that%20Hi-IR%20achieves%0Astate-of-the-art%20performance%20in%20seven%20common%20image%20restoration%20tasks%2C%20affirming%0Aits%20effectiveness%20and%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Information%2520Flow%2520for%2520Generalized%2520Efficient%2520Image%250A%2520%2520Restoration%26entry.906535625%3DYawei%2520Li%2520and%2520Bin%2520Ren%2520and%2520Jingyun%2520Liang%2520and%2520Rakesh%2520Ranjan%2520and%2520Mengyuan%2520Liu%2520and%2520Nicu%2520Sebe%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Luca%2520Benini%26entry.1292438233%3D%2520%2520While%2520vision%2520transformers%2520show%2520promise%2520in%2520numerous%2520image%2520restoration%2520%2528IR%2529%250Atasks%252C%2520the%2520challenge%2520remains%2520in%2520efficiently%2520generalizing%2520and%2520scaling%2520up%2520a%2520model%250Afor%2520multiple%2520IR%2520tasks.%2520To%2520strike%2520a%2520balance%2520between%2520efficiency%2520and%2520model%250Acapacity%2520for%2520a%2520generalized%2520transformer-based%2520IR%2520method%252C%2520we%2520propose%2520a%250Ahierarchical%2520information%2520flow%2520mechanism%2520for%2520image%2520restoration%252C%2520dubbed%2520Hi-IR%252C%250Awhich%2520progressively%2520propagates%2520information%2520among%2520pixels%2520in%2520a%2520bottom-up%2520manner.%250AHi-IR%2520constructs%2520a%2520hierarchical%2520information%2520tree%2520representing%2520the%2520degraded%250Aimage%2520across%2520three%2520levels.%2520Each%2520level%2520encapsulates%2520different%2520types%2520of%250Ainformation%252C%2520with%2520higher%2520levels%2520encompassing%2520broader%2520objects%2520and%2520concepts%2520and%250Alower%2520levels%2520focusing%2520on%2520local%2520details.%2520Moreover%252C%2520the%2520hierarchical%2520tree%250Aarchitecture%2520removes%2520long-range%2520self-attention%252C%2520improves%2520the%2520computational%250Aefficiency%2520and%2520memory%2520utilization%252C%2520thus%2520preparing%2520it%2520for%2520effective%2520model%250Ascaling.%2520Based%2520on%2520that%252C%2520we%2520explore%2520model%2520scaling%2520to%2520improve%2520our%2520method%2527s%250Acapabilities%252C%2520which%2520is%2520expected%2520to%2520positively%2520impact%2520IR%2520in%2520large-scale%2520training%250Asettings.%2520Extensive%2520experimental%2520results%2520show%2520that%2520Hi-IR%2520achieves%250Astate-of-the-art%2520performance%2520in%2520seven%2520common%2520image%2520restoration%2520tasks%252C%2520affirming%250Aits%2520effectiveness%2520and%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Information%20Flow%20for%20Generalized%20Efficient%20Image%0A%20%20Restoration&entry.906535625=Yawei%20Li%20and%20Bin%20Ren%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Nicu%20Sebe%20and%20Ming-Hsuan%20Yang%20and%20Luca%20Benini&entry.1292438233=%20%20While%20vision%20transformers%20show%20promise%20in%20numerous%20image%20restoration%20%28IR%29%0Atasks%2C%20the%20challenge%20remains%20in%20efficiently%20generalizing%20and%20scaling%20up%20a%20model%0Afor%20multiple%20IR%20tasks.%20To%20strike%20a%20balance%20between%20efficiency%20and%20model%0Acapacity%20for%20a%20generalized%20transformer-based%20IR%20method%2C%20we%20propose%20a%0Ahierarchical%20information%20flow%20mechanism%20for%20image%20restoration%2C%20dubbed%20Hi-IR%2C%0Awhich%20progressively%20propagates%20information%20among%20pixels%20in%20a%20bottom-up%20manner.%0AHi-IR%20constructs%20a%20hierarchical%20information%20tree%20representing%20the%20degraded%0Aimage%20across%20three%20levels.%20Each%20level%20encapsulates%20different%20types%20of%0Ainformation%2C%20with%20higher%20levels%20encompassing%20broader%20objects%20and%20concepts%20and%0Alower%20levels%20focusing%20on%20local%20details.%20Moreover%2C%20the%20hierarchical%20tree%0Aarchitecture%20removes%20long-range%20self-attention%2C%20improves%20the%20computational%0Aefficiency%20and%20memory%20utilization%2C%20thus%20preparing%20it%20for%20effective%20model%0Ascaling.%20Based%20on%20that%2C%20we%20explore%20model%20scaling%20to%20improve%20our%20method%27s%0Acapabilities%2C%20which%20is%20expected%20to%20positively%20impact%20IR%20in%20large-scale%20training%0Asettings.%20Extensive%20experimental%20results%20show%20that%20Hi-IR%20achieves%0Astate-of-the-art%20performance%20in%20seven%20common%20image%20restoration%20tasks%2C%20affirming%0Aits%20effectiveness%20and%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18588v1&entry.124074799=Read"},
{"title": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage\n  Synergistic Training of Deep Spiking Neural Networks", "author": "Haoran Gao and Xichuan Zhou and Yingcheng Lin and Min Tian and Liyuan Liu and Cong Shi", "abstract": "  The prevailing of artificial intelligence-of-things calls for higher\nenergy-efficient edge computing paradigms, such as neuromorphic agents\nleveraging brain-inspired spiking neural network (SNN) models based on\nspatiotemporally sparse binary spikes. However, the lack of efficient and\nhigh-accuracy deep SNN learning algorithms prevents them from practical edge\ndeployments at a strictly bounded cost. In this paper, we propose the\nspatiotemporal orthogonal propagation (STOP) algorithm to tackle this\nchallenge. Our algorithm enables fully synergistic learning of synaptic weights\nas well as firing thresholds and leakage factors in spiking neurons to improve\nSNN accuracy, in a unified temporally-forward trace-based framework to mitigate\nthe huge memory requirement for storing neural states across all time-steps in\nthe forward pass. Characteristically, the spatially-backward neuronal errors\nand temporally-forward traces propagate orthogonally to and independently of\neach other, substantially reducing computational complexity. Our STOP algorithm\nobtained high recognition accuracies of 94.84%, 74.92%, 98.26% and 77.10% on\nthe CIFAR-10, CIFAR-100, DVS-Gesture and DVS-CIFAR10 datasets with adequate\ndeep convolutional SNNs of VGG-11 or ResNet-18 structures. Compared with other\ndeep SNN training algorithms, our method is more plausible for edge intelligent\nscenarios where resources are limited but high-accuracy in-situ learning is\ndesired.\n", "link": "http://arxiv.org/abs/2411.11082v2", "date": "2024-11-27", "relevancy": 2.2305, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5665}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STOP%3A%20Spatiotemporal%20Orthogonal%20Propagation%20for%20Weight-Threshold-Leakage%0A%20%20Synergistic%20Training%20of%20Deep%20Spiking%20Neural%20Networks&body=Title%3A%20STOP%3A%20Spatiotemporal%20Orthogonal%20Propagation%20for%20Weight-Threshold-Leakage%0A%20%20Synergistic%20Training%20of%20Deep%20Spiking%20Neural%20Networks%0AAuthor%3A%20Haoran%20Gao%20and%20Xichuan%20Zhou%20and%20Yingcheng%20Lin%20and%20Min%20Tian%20and%20Liyuan%20Liu%20and%20Cong%20Shi%0AAbstract%3A%20%20%20The%20prevailing%20of%20artificial%20intelligence-of-things%20calls%20for%20higher%0Aenergy-efficient%20edge%20computing%20paradigms%2C%20such%20as%20neuromorphic%20agents%0Aleveraging%20brain-inspired%20spiking%20neural%20network%20%28SNN%29%20models%20based%20on%0Aspatiotemporally%20sparse%20binary%20spikes.%20However%2C%20the%20lack%20of%20efficient%20and%0Ahigh-accuracy%20deep%20SNN%20learning%20algorithms%20prevents%20them%20from%20practical%20edge%0Adeployments%20at%20a%20strictly%20bounded%20cost.%20In%20this%20paper%2C%20we%20propose%20the%0Aspatiotemporal%20orthogonal%20propagation%20%28STOP%29%20algorithm%20to%20tackle%20this%0Achallenge.%20Our%20algorithm%20enables%20fully%20synergistic%20learning%20of%20synaptic%20weights%0Aas%20well%20as%20firing%20thresholds%20and%20leakage%20factors%20in%20spiking%20neurons%20to%20improve%0ASNN%20accuracy%2C%20in%20a%20unified%20temporally-forward%20trace-based%20framework%20to%20mitigate%0Athe%20huge%20memory%20requirement%20for%20storing%20neural%20states%20across%20all%20time-steps%20in%0Athe%20forward%20pass.%20Characteristically%2C%20the%20spatially-backward%20neuronal%20errors%0Aand%20temporally-forward%20traces%20propagate%20orthogonally%20to%20and%20independently%20of%0Aeach%20other%2C%20substantially%20reducing%20computational%20complexity.%20Our%20STOP%20algorithm%0Aobtained%20high%20recognition%20accuracies%20of%2094.84%25%2C%2074.92%25%2C%2098.26%25%20and%2077.10%25%20on%0Athe%20CIFAR-10%2C%20CIFAR-100%2C%20DVS-Gesture%20and%20DVS-CIFAR10%20datasets%20with%20adequate%0Adeep%20convolutional%20SNNs%20of%20VGG-11%20or%20ResNet-18%20structures.%20Compared%20with%20other%0Adeep%20SNN%20training%20algorithms%2C%20our%20method%20is%20more%20plausible%20for%20edge%20intelligent%0Ascenarios%20where%20resources%20are%20limited%20but%20high-accuracy%20in-situ%20learning%20is%0Adesired.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTOP%253A%2520Spatiotemporal%2520Orthogonal%2520Propagation%2520for%2520Weight-Threshold-Leakage%250A%2520%2520Synergistic%2520Training%2520of%2520Deep%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DHaoran%2520Gao%2520and%2520Xichuan%2520Zhou%2520and%2520Yingcheng%2520Lin%2520and%2520Min%2520Tian%2520and%2520Liyuan%2520Liu%2520and%2520Cong%2520Shi%26entry.1292438233%3D%2520%2520The%2520prevailing%2520of%2520artificial%2520intelligence-of-things%2520calls%2520for%2520higher%250Aenergy-efficient%2520edge%2520computing%2520paradigms%252C%2520such%2520as%2520neuromorphic%2520agents%250Aleveraging%2520brain-inspired%2520spiking%2520neural%2520network%2520%2528SNN%2529%2520models%2520based%2520on%250Aspatiotemporally%2520sparse%2520binary%2520spikes.%2520However%252C%2520the%2520lack%2520of%2520efficient%2520and%250Ahigh-accuracy%2520deep%2520SNN%2520learning%2520algorithms%2520prevents%2520them%2520from%2520practical%2520edge%250Adeployments%2520at%2520a%2520strictly%2520bounded%2520cost.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250Aspatiotemporal%2520orthogonal%2520propagation%2520%2528STOP%2529%2520algorithm%2520to%2520tackle%2520this%250Achallenge.%2520Our%2520algorithm%2520enables%2520fully%2520synergistic%2520learning%2520of%2520synaptic%2520weights%250Aas%2520well%2520as%2520firing%2520thresholds%2520and%2520leakage%2520factors%2520in%2520spiking%2520neurons%2520to%2520improve%250ASNN%2520accuracy%252C%2520in%2520a%2520unified%2520temporally-forward%2520trace-based%2520framework%2520to%2520mitigate%250Athe%2520huge%2520memory%2520requirement%2520for%2520storing%2520neural%2520states%2520across%2520all%2520time-steps%2520in%250Athe%2520forward%2520pass.%2520Characteristically%252C%2520the%2520spatially-backward%2520neuronal%2520errors%250Aand%2520temporally-forward%2520traces%2520propagate%2520orthogonally%2520to%2520and%2520independently%2520of%250Aeach%2520other%252C%2520substantially%2520reducing%2520computational%2520complexity.%2520Our%2520STOP%2520algorithm%250Aobtained%2520high%2520recognition%2520accuracies%2520of%252094.84%2525%252C%252074.92%2525%252C%252098.26%2525%2520and%252077.10%2525%2520on%250Athe%2520CIFAR-10%252C%2520CIFAR-100%252C%2520DVS-Gesture%2520and%2520DVS-CIFAR10%2520datasets%2520with%2520adequate%250Adeep%2520convolutional%2520SNNs%2520of%2520VGG-11%2520or%2520ResNet-18%2520structures.%2520Compared%2520with%2520other%250Adeep%2520SNN%2520training%2520algorithms%252C%2520our%2520method%2520is%2520more%2520plausible%2520for%2520edge%2520intelligent%250Ascenarios%2520where%2520resources%2520are%2520limited%2520but%2520high-accuracy%2520in-situ%2520learning%2520is%250Adesired.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STOP%3A%20Spatiotemporal%20Orthogonal%20Propagation%20for%20Weight-Threshold-Leakage%0A%20%20Synergistic%20Training%20of%20Deep%20Spiking%20Neural%20Networks&entry.906535625=Haoran%20Gao%20and%20Xichuan%20Zhou%20and%20Yingcheng%20Lin%20and%20Min%20Tian%20and%20Liyuan%20Liu%20and%20Cong%20Shi&entry.1292438233=%20%20The%20prevailing%20of%20artificial%20intelligence-of-things%20calls%20for%20higher%0Aenergy-efficient%20edge%20computing%20paradigms%2C%20such%20as%20neuromorphic%20agents%0Aleveraging%20brain-inspired%20spiking%20neural%20network%20%28SNN%29%20models%20based%20on%0Aspatiotemporally%20sparse%20binary%20spikes.%20However%2C%20the%20lack%20of%20efficient%20and%0Ahigh-accuracy%20deep%20SNN%20learning%20algorithms%20prevents%20them%20from%20practical%20edge%0Adeployments%20at%20a%20strictly%20bounded%20cost.%20In%20this%20paper%2C%20we%20propose%20the%0Aspatiotemporal%20orthogonal%20propagation%20%28STOP%29%20algorithm%20to%20tackle%20this%0Achallenge.%20Our%20algorithm%20enables%20fully%20synergistic%20learning%20of%20synaptic%20weights%0Aas%20well%20as%20firing%20thresholds%20and%20leakage%20factors%20in%20spiking%20neurons%20to%20improve%0ASNN%20accuracy%2C%20in%20a%20unified%20temporally-forward%20trace-based%20framework%20to%20mitigate%0Athe%20huge%20memory%20requirement%20for%20storing%20neural%20states%20across%20all%20time-steps%20in%0Athe%20forward%20pass.%20Characteristically%2C%20the%20spatially-backward%20neuronal%20errors%0Aand%20temporally-forward%20traces%20propagate%20orthogonally%20to%20and%20independently%20of%0Aeach%20other%2C%20substantially%20reducing%20computational%20complexity.%20Our%20STOP%20algorithm%0Aobtained%20high%20recognition%20accuracies%20of%2094.84%25%2C%2074.92%25%2C%2098.26%25%20and%2077.10%25%20on%0Athe%20CIFAR-10%2C%20CIFAR-100%2C%20DVS-Gesture%20and%20DVS-CIFAR10%20datasets%20with%20adequate%0Adeep%20convolutional%20SNNs%20of%20VGG-11%20or%20ResNet-18%20structures.%20Compared%20with%20other%0Adeep%20SNN%20training%20algorithms%2C%20our%20method%20is%20more%20plausible%20for%20edge%20intelligent%0Ascenarios%20where%20resources%20are%20limited%20but%20high-accuracy%20in-situ%20learning%20is%0Adesired.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11082v2&entry.124074799=Read"},
{"title": "Deep Fourier-embedded Network for Bi-modal Salient Object Detection", "author": "Pengfei Lyu and Xiaosheng Yu and Chengdong Wu and Jagath C. Rajapakse", "abstract": "  The rapid development of deep learning provides a significant improvement of\nsalient object detection combining both RGB and thermal images. However,\nexisting deep learning-based models suffer from two major shortcomings. First,\nthe computation and memory demands of Transformer-based models with quadratic\ncomplexity are unbearable, especially in handling high-resolution bi-modal\nfeature fusion. Second, even if learning converges to an ideal solution, there\nremains a frequency gap between the prediction and ground truth. Therefore, we\npropose a purely fast Fourier transform-based model, namely deep\nFourier-embedded network (DFENet), for learning bi-modal information of RGB and\nthermal images. On one hand, fast Fourier transform efficiently fetches global\ndependencies with low complexity. Inspired by this, we design modal-coordinated\nperception attention to fuse the frequency gap between RGB and thermal\nmodalities with multi-dimensional representation enhancement. To obtain\nreliable detailed information during decoding, we design the\nfrequency-decomposed edge-aware module (FEM) to clarify object edges by deeply\ndecomposing low-level features. Moreover, we equip proposed Fourier residual\nchannel attention block in each decoder layer to prioritize high-frequency\ninformation while aligning channel global relationships. On the other hand, we\npropose co-focus frequency loss (CFL) to steer FEM towards minimizing the\nfrequency gap. CFL dynamically weights hard frequencies during edge frequency\nreconstruction by cross-referencing the bi-modal edge information in the\nFourier domain. This frequency-level refinement of edge features further\ncontributes to the quality of the final pixel-level prediction. Extensive\nexperiments on four bi-modal salient object detection benchmark datasets\ndemonstrate our proposed DFENet outperforms twelve existing state-of-the-art\nmodels.\n", "link": "http://arxiv.org/abs/2411.18409v1", "date": "2024-11-27", "relevancy": 2.2244, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5653}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Fourier-embedded%20Network%20for%20Bi-modal%20Salient%20Object%20Detection&body=Title%3A%20Deep%20Fourier-embedded%20Network%20for%20Bi-modal%20Salient%20Object%20Detection%0AAuthor%3A%20Pengfei%20Lyu%20and%20Xiaosheng%20Yu%20and%20Chengdong%20Wu%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20deep%20learning%20provides%20a%20significant%20improvement%20of%0Asalient%20object%20detection%20combining%20both%20RGB%20and%20thermal%20images.%20However%2C%0Aexisting%20deep%20learning-based%20models%20suffer%20from%20two%20major%20shortcomings.%20First%2C%0Athe%20computation%20and%20memory%20demands%20of%20Transformer-based%20models%20with%20quadratic%0Acomplexity%20are%20unbearable%2C%20especially%20in%20handling%20high-resolution%20bi-modal%0Afeature%20fusion.%20Second%2C%20even%20if%20learning%20converges%20to%20an%20ideal%20solution%2C%20there%0Aremains%20a%20frequency%20gap%20between%20the%20prediction%20and%20ground%20truth.%20Therefore%2C%20we%0Apropose%20a%20purely%20fast%20Fourier%20transform-based%20model%2C%20namely%20deep%0AFourier-embedded%20network%20%28DFENet%29%2C%20for%20learning%20bi-modal%20information%20of%20RGB%20and%0Athermal%20images.%20On%20one%20hand%2C%20fast%20Fourier%20transform%20efficiently%20fetches%20global%0Adependencies%20with%20low%20complexity.%20Inspired%20by%20this%2C%20we%20design%20modal-coordinated%0Aperception%20attention%20to%20fuse%20the%20frequency%20gap%20between%20RGB%20and%20thermal%0Amodalities%20with%20multi-dimensional%20representation%20enhancement.%20To%20obtain%0Areliable%20detailed%20information%20during%20decoding%2C%20we%20design%20the%0Afrequency-decomposed%20edge-aware%20module%20%28FEM%29%20to%20clarify%20object%20edges%20by%20deeply%0Adecomposing%20low-level%20features.%20Moreover%2C%20we%20equip%20proposed%20Fourier%20residual%0Achannel%20attention%20block%20in%20each%20decoder%20layer%20to%20prioritize%20high-frequency%0Ainformation%20while%20aligning%20channel%20global%20relationships.%20On%20the%20other%20hand%2C%20we%0Apropose%20co-focus%20frequency%20loss%20%28CFL%29%20to%20steer%20FEM%20towards%20minimizing%20the%0Afrequency%20gap.%20CFL%20dynamically%20weights%20hard%20frequencies%20during%20edge%20frequency%0Areconstruction%20by%20cross-referencing%20the%20bi-modal%20edge%20information%20in%20the%0AFourier%20domain.%20This%20frequency-level%20refinement%20of%20edge%20features%20further%0Acontributes%20to%20the%20quality%20of%20the%20final%20pixel-level%20prediction.%20Extensive%0Aexperiments%20on%20four%20bi-modal%20salient%20object%20detection%20benchmark%20datasets%0Ademonstrate%20our%20proposed%20DFENet%20outperforms%20twelve%20existing%20state-of-the-art%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Fourier-embedded%2520Network%2520for%2520Bi-modal%2520Salient%2520Object%2520Detection%26entry.906535625%3DPengfei%2520Lyu%2520and%2520Xiaosheng%2520Yu%2520and%2520Chengdong%2520Wu%2520and%2520Jagath%2520C.%2520Rajapakse%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520deep%2520learning%2520provides%2520a%2520significant%2520improvement%2520of%250Asalient%2520object%2520detection%2520combining%2520both%2520RGB%2520and%2520thermal%2520images.%2520However%252C%250Aexisting%2520deep%2520learning-based%2520models%2520suffer%2520from%2520two%2520major%2520shortcomings.%2520First%252C%250Athe%2520computation%2520and%2520memory%2520demands%2520of%2520Transformer-based%2520models%2520with%2520quadratic%250Acomplexity%2520are%2520unbearable%252C%2520especially%2520in%2520handling%2520high-resolution%2520bi-modal%250Afeature%2520fusion.%2520Second%252C%2520even%2520if%2520learning%2520converges%2520to%2520an%2520ideal%2520solution%252C%2520there%250Aremains%2520a%2520frequency%2520gap%2520between%2520the%2520prediction%2520and%2520ground%2520truth.%2520Therefore%252C%2520we%250Apropose%2520a%2520purely%2520fast%2520Fourier%2520transform-based%2520model%252C%2520namely%2520deep%250AFourier-embedded%2520network%2520%2528DFENet%2529%252C%2520for%2520learning%2520bi-modal%2520information%2520of%2520RGB%2520and%250Athermal%2520images.%2520On%2520one%2520hand%252C%2520fast%2520Fourier%2520transform%2520efficiently%2520fetches%2520global%250Adependencies%2520with%2520low%2520complexity.%2520Inspired%2520by%2520this%252C%2520we%2520design%2520modal-coordinated%250Aperception%2520attention%2520to%2520fuse%2520the%2520frequency%2520gap%2520between%2520RGB%2520and%2520thermal%250Amodalities%2520with%2520multi-dimensional%2520representation%2520enhancement.%2520To%2520obtain%250Areliable%2520detailed%2520information%2520during%2520decoding%252C%2520we%2520design%2520the%250Afrequency-decomposed%2520edge-aware%2520module%2520%2528FEM%2529%2520to%2520clarify%2520object%2520edges%2520by%2520deeply%250Adecomposing%2520low-level%2520features.%2520Moreover%252C%2520we%2520equip%2520proposed%2520Fourier%2520residual%250Achannel%2520attention%2520block%2520in%2520each%2520decoder%2520layer%2520to%2520prioritize%2520high-frequency%250Ainformation%2520while%2520aligning%2520channel%2520global%2520relationships.%2520On%2520the%2520other%2520hand%252C%2520we%250Apropose%2520co-focus%2520frequency%2520loss%2520%2528CFL%2529%2520to%2520steer%2520FEM%2520towards%2520minimizing%2520the%250Afrequency%2520gap.%2520CFL%2520dynamically%2520weights%2520hard%2520frequencies%2520during%2520edge%2520frequency%250Areconstruction%2520by%2520cross-referencing%2520the%2520bi-modal%2520edge%2520information%2520in%2520the%250AFourier%2520domain.%2520This%2520frequency-level%2520refinement%2520of%2520edge%2520features%2520further%250Acontributes%2520to%2520the%2520quality%2520of%2520the%2520final%2520pixel-level%2520prediction.%2520Extensive%250Aexperiments%2520on%2520four%2520bi-modal%2520salient%2520object%2520detection%2520benchmark%2520datasets%250Ademonstrate%2520our%2520proposed%2520DFENet%2520outperforms%2520twelve%2520existing%2520state-of-the-art%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Fourier-embedded%20Network%20for%20Bi-modal%20Salient%20Object%20Detection&entry.906535625=Pengfei%20Lyu%20and%20Xiaosheng%20Yu%20and%20Chengdong%20Wu%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=%20%20The%20rapid%20development%20of%20deep%20learning%20provides%20a%20significant%20improvement%20of%0Asalient%20object%20detection%20combining%20both%20RGB%20and%20thermal%20images.%20However%2C%0Aexisting%20deep%20learning-based%20models%20suffer%20from%20two%20major%20shortcomings.%20First%2C%0Athe%20computation%20and%20memory%20demands%20of%20Transformer-based%20models%20with%20quadratic%0Acomplexity%20are%20unbearable%2C%20especially%20in%20handling%20high-resolution%20bi-modal%0Afeature%20fusion.%20Second%2C%20even%20if%20learning%20converges%20to%20an%20ideal%20solution%2C%20there%0Aremains%20a%20frequency%20gap%20between%20the%20prediction%20and%20ground%20truth.%20Therefore%2C%20we%0Apropose%20a%20purely%20fast%20Fourier%20transform-based%20model%2C%20namely%20deep%0AFourier-embedded%20network%20%28DFENet%29%2C%20for%20learning%20bi-modal%20information%20of%20RGB%20and%0Athermal%20images.%20On%20one%20hand%2C%20fast%20Fourier%20transform%20efficiently%20fetches%20global%0Adependencies%20with%20low%20complexity.%20Inspired%20by%20this%2C%20we%20design%20modal-coordinated%0Aperception%20attention%20to%20fuse%20the%20frequency%20gap%20between%20RGB%20and%20thermal%0Amodalities%20with%20multi-dimensional%20representation%20enhancement.%20To%20obtain%0Areliable%20detailed%20information%20during%20decoding%2C%20we%20design%20the%0Afrequency-decomposed%20edge-aware%20module%20%28FEM%29%20to%20clarify%20object%20edges%20by%20deeply%0Adecomposing%20low-level%20features.%20Moreover%2C%20we%20equip%20proposed%20Fourier%20residual%0Achannel%20attention%20block%20in%20each%20decoder%20layer%20to%20prioritize%20high-frequency%0Ainformation%20while%20aligning%20channel%20global%20relationships.%20On%20the%20other%20hand%2C%20we%0Apropose%20co-focus%20frequency%20loss%20%28CFL%29%20to%20steer%20FEM%20towards%20minimizing%20the%0Afrequency%20gap.%20CFL%20dynamically%20weights%20hard%20frequencies%20during%20edge%20frequency%0Areconstruction%20by%20cross-referencing%20the%20bi-modal%20edge%20information%20in%20the%0AFourier%20domain.%20This%20frequency-level%20refinement%20of%20edge%20features%20further%0Acontributes%20to%20the%20quality%20of%20the%20final%20pixel-level%20prediction.%20Extensive%0Aexperiments%20on%20four%20bi-modal%20salient%20object%20detection%20benchmark%20datasets%0Ademonstrate%20our%20proposed%20DFENet%20outperforms%20twelve%20existing%20state-of-the-art%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18409v1&entry.124074799=Read"},
{"title": "Referential communication in heterogeneous communities of pre-trained\n  visual deep networks", "author": "Mat\u00e9o Mahaut and Francesca Franzon and Roberto Dess\u00ec and Marco Baroni", "abstract": "  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of referential communication\nin a community of heterogeneous state-of-the-art pre-trained visual networks,\nshowing that they can develop, in a self-supervised way, a shared protocol to\nrefer to a target object among a set of candidates. This shared protocol can\nalso be used, to some extent, to communicate about previously unseen object\ncategories of different granularity. Moreover, a visual network that was not\ninitially part of an existing community can learn the community's protocol with\nremarkable ease. Finally, we study, both qualitatively and quantitatively, the\nproperties of the emergent protocol, providing some evidence that it is\ncapturing high-level semantic features of objects.\n", "link": "http://arxiv.org/abs/2302.08913v5", "date": "2024-11-27", "relevancy": 2.2214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&body=Title%3A%20Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks%0AAuthor%3A%20Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni%0AAbstract%3A%20%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20referential%20communication%0Ain%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%20visual%20networks%2C%0Ashowing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%20shared%20protocol%20to%0Arefer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%20shared%20protocol%20can%0Aalso%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%20previously%20unseen%20object%0Acategories%20of%20different%20granularity.%20Moreover%2C%20a%20visual%20network%20that%20was%20not%0Ainitially%20part%20of%20an%20existing%20community%20can%20learn%20the%20community%27s%20protocol%20with%0Aremarkable%20ease.%20Finally%2C%20we%20study%2C%20both%20qualitatively%20and%20quantitatively%2C%20the%0Aproperties%20of%20the%20emergent%20protocol%2C%20providing%20some%20evidence%20that%20it%20is%0Acapturing%20high-level%20semantic%20features%20of%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08913v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferential%2520communication%2520in%2520heterogeneous%2520communities%2520of%2520pre-trained%250A%2520%2520visual%2520deep%2520networks%26entry.906535625%3DMat%25C3%25A9o%2520Mahaut%2520and%2520Francesca%2520Franzon%2520and%2520Roberto%2520Dess%25C3%25AC%2520and%2520Marco%2520Baroni%26entry.1292438233%3D%2520%2520As%2520large%2520pre-trained%2520image-processing%2520neural%2520networks%2520are%2520being%2520embedded%2520in%250Aautonomous%2520agents%2520such%2520as%2520self-driving%2520cars%2520or%2520robots%252C%2520the%2520question%2520arises%2520of%250Ahow%2520such%2520systems%2520can%2520communicate%2520with%2520each%2520other%2520about%2520the%2520surrounding%2520world%252C%250Adespite%2520their%2520different%2520architectures%2520and%2520training%2520regimes.%2520As%2520a%2520first%2520step%2520in%250Athis%2520direction%252C%2520we%2520systematically%2520explore%2520the%2520task%2520of%2520referential%2520communication%250Ain%2520a%2520community%2520of%2520heterogeneous%2520state-of-the-art%2520pre-trained%2520visual%2520networks%252C%250Ashowing%2520that%2520they%2520can%2520develop%252C%2520in%2520a%2520self-supervised%2520way%252C%2520a%2520shared%2520protocol%2520to%250Arefer%2520to%2520a%2520target%2520object%2520among%2520a%2520set%2520of%2520candidates.%2520This%2520shared%2520protocol%2520can%250Aalso%2520be%2520used%252C%2520to%2520some%2520extent%252C%2520to%2520communicate%2520about%2520previously%2520unseen%2520object%250Acategories%2520of%2520different%2520granularity.%2520Moreover%252C%2520a%2520visual%2520network%2520that%2520was%2520not%250Ainitially%2520part%2520of%2520an%2520existing%2520community%2520can%2520learn%2520the%2520community%2527s%2520protocol%2520with%250Aremarkable%2520ease.%2520Finally%252C%2520we%2520study%252C%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520the%250Aproperties%2520of%2520the%2520emergent%2520protocol%252C%2520providing%2520some%2520evidence%2520that%2520it%2520is%250Acapturing%2520high-level%2520semantic%2520features%2520of%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08913v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Referential%20communication%20in%20heterogeneous%20communities%20of%20pre-trained%0A%20%20visual%20deep%20networks&entry.906535625=Mat%C3%A9o%20Mahaut%20and%20Francesca%20Franzon%20and%20Roberto%20Dess%C3%AC%20and%20Marco%20Baroni&entry.1292438233=%20%20As%20large%20pre-trained%20image-processing%20neural%20networks%20are%20being%20embedded%20in%0Aautonomous%20agents%20such%20as%20self-driving%20cars%20or%20robots%2C%20the%20question%20arises%20of%0Ahow%20such%20systems%20can%20communicate%20with%20each%20other%20about%20the%20surrounding%20world%2C%0Adespite%20their%20different%20architectures%20and%20training%20regimes.%20As%20a%20first%20step%20in%0Athis%20direction%2C%20we%20systematically%20explore%20the%20task%20of%20referential%20communication%0Ain%20a%20community%20of%20heterogeneous%20state-of-the-art%20pre-trained%20visual%20networks%2C%0Ashowing%20that%20they%20can%20develop%2C%20in%20a%20self-supervised%20way%2C%20a%20shared%20protocol%20to%0Arefer%20to%20a%20target%20object%20among%20a%20set%20of%20candidates.%20This%20shared%20protocol%20can%0Aalso%20be%20used%2C%20to%20some%20extent%2C%20to%20communicate%20about%20previously%20unseen%20object%0Acategories%20of%20different%20granularity.%20Moreover%2C%20a%20visual%20network%20that%20was%20not%0Ainitially%20part%20of%20an%20existing%20community%20can%20learn%20the%20community%27s%20protocol%20with%0Aremarkable%20ease.%20Finally%2C%20we%20study%2C%20both%20qualitatively%20and%20quantitatively%2C%20the%0Aproperties%20of%20the%20emergent%20protocol%2C%20providing%20some%20evidence%20that%20it%20is%0Acapturing%20high-level%20semantic%20features%20of%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08913v5&entry.124074799=Read"},
{"title": "Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth\n  Estimation", "author": "Mehdi Zayene and Jannik Endres and Albias Havolli and Charles Corbi\u00e8re and Salim Cherkaoui and Alexandre Kontouli and Alexandre Alahi", "abstract": "  Despite considerable progress in stereo depth estimation, omnidirectional\nimaging remains underexplored, mainly due to the lack of appropriate data. We\nintroduce Helvipad, a real-world dataset for omnidirectional stereo depth\nestimation, consisting of 40K frames from video sequences across diverse\nenvironments, including crowded indoor and outdoor scenes with diverse lighting\nconditions. Collected using two 360{\\deg} cameras in a top-bottom setup and a\nLiDAR sensor, the dataset includes accurate depth and disparity labels by\nprojecting 3D point clouds onto equirectangular images. Additionally, we\nprovide an augmented training set with a significantly increased label density\nby using depth completion. We benchmark leading stereo depth estimation models\nfor both standard and omnidirectional images. The results show that while\nrecent stereo methods perform decently, a significant challenge persists in\naccurately estimating depth in omnidirectional imaging. To address this, we\nintroduce necessary adaptations to stereo models, achieving improved\nperformance.\n", "link": "http://arxiv.org/abs/2411.18335v1", "date": "2024-11-27", "relevancy": 2.2123, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helvipad%3A%20A%20Real-World%20Dataset%20for%20Omnidirectional%20Stereo%20Depth%0A%20%20Estimation&body=Title%3A%20Helvipad%3A%20A%20Real-World%20Dataset%20for%20Omnidirectional%20Stereo%20Depth%0A%20%20Estimation%0AAuthor%3A%20Mehdi%20Zayene%20and%20Jannik%20Endres%20and%20Albias%20Havolli%20and%20Charles%20Corbi%C3%A8re%20and%20Salim%20Cherkaoui%20and%20Alexandre%20Kontouli%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Despite%20considerable%20progress%20in%20stereo%20depth%20estimation%2C%20omnidirectional%0Aimaging%20remains%20underexplored%2C%20mainly%20due%20to%20the%20lack%20of%20appropriate%20data.%20We%0Aintroduce%20Helvipad%2C%20a%20real-world%20dataset%20for%20omnidirectional%20stereo%20depth%0Aestimation%2C%20consisting%20of%2040K%20frames%20from%20video%20sequences%20across%20diverse%0Aenvironments%2C%20including%20crowded%20indoor%20and%20outdoor%20scenes%20with%20diverse%20lighting%0Aconditions.%20Collected%20using%20two%20360%7B%5Cdeg%7D%20cameras%20in%20a%20top-bottom%20setup%20and%20a%0ALiDAR%20sensor%2C%20the%20dataset%20includes%20accurate%20depth%20and%20disparity%20labels%20by%0Aprojecting%203D%20point%20clouds%20onto%20equirectangular%20images.%20Additionally%2C%20we%0Aprovide%20an%20augmented%20training%20set%20with%20a%20significantly%20increased%20label%20density%0Aby%20using%20depth%20completion.%20We%20benchmark%20leading%20stereo%20depth%20estimation%20models%0Afor%20both%20standard%20and%20omnidirectional%20images.%20The%20results%20show%20that%20while%0Arecent%20stereo%20methods%20perform%20decently%2C%20a%20significant%20challenge%20persists%20in%0Aaccurately%20estimating%20depth%20in%20omnidirectional%20imaging.%20To%20address%20this%2C%20we%0Aintroduce%20necessary%20adaptations%20to%20stereo%20models%2C%20achieving%20improved%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelvipad%253A%2520A%2520Real-World%2520Dataset%2520for%2520Omnidirectional%2520Stereo%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DMehdi%2520Zayene%2520and%2520Jannik%2520Endres%2520and%2520Albias%2520Havolli%2520and%2520Charles%2520Corbi%25C3%25A8re%2520and%2520Salim%2520Cherkaoui%2520and%2520Alexandre%2520Kontouli%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Despite%2520considerable%2520progress%2520in%2520stereo%2520depth%2520estimation%252C%2520omnidirectional%250Aimaging%2520remains%2520underexplored%252C%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520appropriate%2520data.%2520We%250Aintroduce%2520Helvipad%252C%2520a%2520real-world%2520dataset%2520for%2520omnidirectional%2520stereo%2520depth%250Aestimation%252C%2520consisting%2520of%252040K%2520frames%2520from%2520video%2520sequences%2520across%2520diverse%250Aenvironments%252C%2520including%2520crowded%2520indoor%2520and%2520outdoor%2520scenes%2520with%2520diverse%2520lighting%250Aconditions.%2520Collected%2520using%2520two%2520360%257B%255Cdeg%257D%2520cameras%2520in%2520a%2520top-bottom%2520setup%2520and%2520a%250ALiDAR%2520sensor%252C%2520the%2520dataset%2520includes%2520accurate%2520depth%2520and%2520disparity%2520labels%2520by%250Aprojecting%25203D%2520point%2520clouds%2520onto%2520equirectangular%2520images.%2520Additionally%252C%2520we%250Aprovide%2520an%2520augmented%2520training%2520set%2520with%2520a%2520significantly%2520increased%2520label%2520density%250Aby%2520using%2520depth%2520completion.%2520We%2520benchmark%2520leading%2520stereo%2520depth%2520estimation%2520models%250Afor%2520both%2520standard%2520and%2520omnidirectional%2520images.%2520The%2520results%2520show%2520that%2520while%250Arecent%2520stereo%2520methods%2520perform%2520decently%252C%2520a%2520significant%2520challenge%2520persists%2520in%250Aaccurately%2520estimating%2520depth%2520in%2520omnidirectional%2520imaging.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520necessary%2520adaptations%2520to%2520stereo%2520models%252C%2520achieving%2520improved%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helvipad%3A%20A%20Real-World%20Dataset%20for%20Omnidirectional%20Stereo%20Depth%0A%20%20Estimation&entry.906535625=Mehdi%20Zayene%20and%20Jannik%20Endres%20and%20Albias%20Havolli%20and%20Charles%20Corbi%C3%A8re%20and%20Salim%20Cherkaoui%20and%20Alexandre%20Kontouli%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Despite%20considerable%20progress%20in%20stereo%20depth%20estimation%2C%20omnidirectional%0Aimaging%20remains%20underexplored%2C%20mainly%20due%20to%20the%20lack%20of%20appropriate%20data.%20We%0Aintroduce%20Helvipad%2C%20a%20real-world%20dataset%20for%20omnidirectional%20stereo%20depth%0Aestimation%2C%20consisting%20of%2040K%20frames%20from%20video%20sequences%20across%20diverse%0Aenvironments%2C%20including%20crowded%20indoor%20and%20outdoor%20scenes%20with%20diverse%20lighting%0Aconditions.%20Collected%20using%20two%20360%7B%5Cdeg%7D%20cameras%20in%20a%20top-bottom%20setup%20and%20a%0ALiDAR%20sensor%2C%20the%20dataset%20includes%20accurate%20depth%20and%20disparity%20labels%20by%0Aprojecting%203D%20point%20clouds%20onto%20equirectangular%20images.%20Additionally%2C%20we%0Aprovide%20an%20augmented%20training%20set%20with%20a%20significantly%20increased%20label%20density%0Aby%20using%20depth%20completion.%20We%20benchmark%20leading%20stereo%20depth%20estimation%20models%0Afor%20both%20standard%20and%20omnidirectional%20images.%20The%20results%20show%20that%20while%0Arecent%20stereo%20methods%20perform%20decently%2C%20a%20significant%20challenge%20persists%20in%0Aaccurately%20estimating%20depth%20in%20omnidirectional%20imaging.%20To%20address%20this%2C%20we%0Aintroduce%20necessary%20adaptations%20to%20stereo%20models%2C%20achieving%20improved%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18335v1&entry.124074799=Read"},
{"title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended\n  Interleaved Image-Text Generation", "author": "Pengfei Zhou and Xiaopeng Peng and Jiajun Song and Chuanhao Li and Zhaopan Xu and Yue Yang and Ziyao Guo and Hao Zhang and Yuqi Lin and Yefei He and Lirui Zhao and Shuo Liu and Tianhua Li and Yuxuan Xie and Xiaojun Chang and Yu Qiao and Wenqi Shao and Kaipeng Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) have made significant strides in\nvisual understanding and generation tasks. However, generating interleaved\nimage-text content remains a challenge, which requires integrated multimodal\nunderstanding and generation abilities. While the progress in unified models\noffers new solutions, existing benchmarks are insufficient for evaluating these\nmethods due to data size and diversity limitations. To bridge this gap, we\nintroduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400\nhigh-quality human-annotated instances across 56 real-world tasks. OpenING\ncovers diverse daily scenarios such as travel guide, design, and brainstorming,\noffering a robust platform for challenging interleaved generation methods. In\naddition, we present IntJudge, a judge model for evaluating open-ended\nmultimodal generation methods. Trained with a novel data pipeline, our IntJudge\nachieves an agreement rate of 82. 42% with human judgments, outperforming\nGPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that\ncurrent interleaved generation methods still have substantial room for\nimprovement. Key findings on interleaved image-text generation are further\npresented to guide the development of next-generation models. The OpenING is\nopen-sourced at https://opening.github.io.\n", "link": "http://arxiv.org/abs/2411.18499v1", "date": "2024-11-27", "relevancy": 2.211, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5682}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATE%20OpenING%3A%20A%20Comprehensive%20Benchmark%20for%20Judging%20Open-ended%0A%20%20Interleaved%20Image-Text%20Generation&body=Title%3A%20GATE%20OpenING%3A%20A%20Comprehensive%20Benchmark%20for%20Judging%20Open-ended%0A%20%20Interleaved%20Image-Text%20Generation%0AAuthor%3A%20Pengfei%20Zhou%20and%20Xiaopeng%20Peng%20and%20Jiajun%20Song%20and%20Chuanhao%20Li%20and%20Zhaopan%20Xu%20and%20Yue%20Yang%20and%20Ziyao%20Guo%20and%20Hao%20Zhang%20and%20Yuqi%20Lin%20and%20Yefei%20He%20and%20Lirui%20Zhao%20and%20Shuo%20Liu%20and%20Tianhua%20Li%20and%20Yuxuan%20Xie%20and%20Xiaojun%20Chang%20and%20Yu%20Qiao%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%20strides%20in%0Avisual%20understanding%20and%20generation%20tasks.%20However%2C%20generating%20interleaved%0Aimage-text%20content%20remains%20a%20challenge%2C%20which%20requires%20integrated%20multimodal%0Aunderstanding%20and%20generation%20abilities.%20While%20the%20progress%20in%20unified%20models%0Aoffers%20new%20solutions%2C%20existing%20benchmarks%20are%20insufficient%20for%20evaluating%20these%0Amethods%20due%20to%20data%20size%20and%20diversity%20limitations.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20GATE%20OpenING%20%28OpenING%29%2C%20a%20comprehensive%20benchmark%20comprising%205%2C400%0Ahigh-quality%20human-annotated%20instances%20across%2056%20real-world%20tasks.%20OpenING%0Acovers%20diverse%20daily%20scenarios%20such%20as%20travel%20guide%2C%20design%2C%20and%20brainstorming%2C%0Aoffering%20a%20robust%20platform%20for%20challenging%20interleaved%20generation%20methods.%20In%0Aaddition%2C%20we%20present%20IntJudge%2C%20a%20judge%20model%20for%20evaluating%20open-ended%0Amultimodal%20generation%20methods.%20Trained%20with%20a%20novel%20data%20pipeline%2C%20our%20IntJudge%0Aachieves%20an%20agreement%20rate%20of%2082.%2042%25%20with%20human%20judgments%2C%20outperforming%0AGPT-based%20evaluators%20by%2011.34%25.%20Extensive%20experiments%20on%20OpenING%20reveal%20that%0Acurrent%20interleaved%20generation%20methods%20still%20have%20substantial%20room%20for%0Aimprovement.%20Key%20findings%20on%20interleaved%20image-text%20generation%20are%20further%0Apresented%20to%20guide%20the%20development%20of%20next-generation%20models.%20The%20OpenING%20is%0Aopen-sourced%20at%20https%3A//opening.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATE%2520OpenING%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Judging%2520Open-ended%250A%2520%2520Interleaved%2520Image-Text%2520Generation%26entry.906535625%3DPengfei%2520Zhou%2520and%2520Xiaopeng%2520Peng%2520and%2520Jiajun%2520Song%2520and%2520Chuanhao%2520Li%2520and%2520Zhaopan%2520Xu%2520and%2520Yue%2520Yang%2520and%2520Ziyao%2520Guo%2520and%2520Hao%2520Zhang%2520and%2520Yuqi%2520Lin%2520and%2520Yefei%2520He%2520and%2520Lirui%2520Zhao%2520and%2520Shuo%2520Liu%2520and%2520Tianhua%2520Li%2520and%2520Yuxuan%2520Xie%2520and%2520Xiaojun%2520Chang%2520and%2520Yu%2520Qiao%2520and%2520Wenqi%2520Shao%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520strides%2520in%250Avisual%2520understanding%2520and%2520generation%2520tasks.%2520However%252C%2520generating%2520interleaved%250Aimage-text%2520content%2520remains%2520a%2520challenge%252C%2520which%2520requires%2520integrated%2520multimodal%250Aunderstanding%2520and%2520generation%2520abilities.%2520While%2520the%2520progress%2520in%2520unified%2520models%250Aoffers%2520new%2520solutions%252C%2520existing%2520benchmarks%2520are%2520insufficient%2520for%2520evaluating%2520these%250Amethods%2520due%2520to%2520data%2520size%2520and%2520diversity%2520limitations.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520GATE%2520OpenING%2520%2528OpenING%2529%252C%2520a%2520comprehensive%2520benchmark%2520comprising%25205%252C400%250Ahigh-quality%2520human-annotated%2520instances%2520across%252056%2520real-world%2520tasks.%2520OpenING%250Acovers%2520diverse%2520daily%2520scenarios%2520such%2520as%2520travel%2520guide%252C%2520design%252C%2520and%2520brainstorming%252C%250Aoffering%2520a%2520robust%2520platform%2520for%2520challenging%2520interleaved%2520generation%2520methods.%2520In%250Aaddition%252C%2520we%2520present%2520IntJudge%252C%2520a%2520judge%2520model%2520for%2520evaluating%2520open-ended%250Amultimodal%2520generation%2520methods.%2520Trained%2520with%2520a%2520novel%2520data%2520pipeline%252C%2520our%2520IntJudge%250Aachieves%2520an%2520agreement%2520rate%2520of%252082.%252042%2525%2520with%2520human%2520judgments%252C%2520outperforming%250AGPT-based%2520evaluators%2520by%252011.34%2525.%2520Extensive%2520experiments%2520on%2520OpenING%2520reveal%2520that%250Acurrent%2520interleaved%2520generation%2520methods%2520still%2520have%2520substantial%2520room%2520for%250Aimprovement.%2520Key%2520findings%2520on%2520interleaved%2520image-text%2520generation%2520are%2520further%250Apresented%2520to%2520guide%2520the%2520development%2520of%2520next-generation%2520models.%2520The%2520OpenING%2520is%250Aopen-sourced%2520at%2520https%253A//opening.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATE%20OpenING%3A%20A%20Comprehensive%20Benchmark%20for%20Judging%20Open-ended%0A%20%20Interleaved%20Image-Text%20Generation&entry.906535625=Pengfei%20Zhou%20and%20Xiaopeng%20Peng%20and%20Jiajun%20Song%20and%20Chuanhao%20Li%20and%20Zhaopan%20Xu%20and%20Yue%20Yang%20and%20Ziyao%20Guo%20and%20Hao%20Zhang%20and%20Yuqi%20Lin%20and%20Yefei%20He%20and%20Lirui%20Zhao%20and%20Shuo%20Liu%20and%20Tianhua%20Li%20and%20Yuxuan%20Xie%20and%20Xiaojun%20Chang%20and%20Yu%20Qiao%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%20strides%20in%0Avisual%20understanding%20and%20generation%20tasks.%20However%2C%20generating%20interleaved%0Aimage-text%20content%20remains%20a%20challenge%2C%20which%20requires%20integrated%20multimodal%0Aunderstanding%20and%20generation%20abilities.%20While%20the%20progress%20in%20unified%20models%0Aoffers%20new%20solutions%2C%20existing%20benchmarks%20are%20insufficient%20for%20evaluating%20these%0Amethods%20due%20to%20data%20size%20and%20diversity%20limitations.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20GATE%20OpenING%20%28OpenING%29%2C%20a%20comprehensive%20benchmark%20comprising%205%2C400%0Ahigh-quality%20human-annotated%20instances%20across%2056%20real-world%20tasks.%20OpenING%0Acovers%20diverse%20daily%20scenarios%20such%20as%20travel%20guide%2C%20design%2C%20and%20brainstorming%2C%0Aoffering%20a%20robust%20platform%20for%20challenging%20interleaved%20generation%20methods.%20In%0Aaddition%2C%20we%20present%20IntJudge%2C%20a%20judge%20model%20for%20evaluating%20open-ended%0Amultimodal%20generation%20methods.%20Trained%20with%20a%20novel%20data%20pipeline%2C%20our%20IntJudge%0Aachieves%20an%20agreement%20rate%20of%2082.%2042%25%20with%20human%20judgments%2C%20outperforming%0AGPT-based%20evaluators%20by%2011.34%25.%20Extensive%20experiments%20on%20OpenING%20reveal%20that%0Acurrent%20interleaved%20generation%20methods%20still%20have%20substantial%20room%20for%0Aimprovement.%20Key%20findings%20on%20interleaved%20image-text%20generation%20are%20further%0Apresented%20to%20guide%20the%20development%20of%20next-generation%20models.%20The%20OpenING%20is%0Aopen-sourced%20at%20https%3A//opening.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18499v1&entry.124074799=Read"},
{"title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding\n  with Superior Temporal Localization Ability", "author": "Shimin Chen and Xiaohan Lan and Yitian Yuan and Zequn Jie and Lin Ma", "abstract": "  Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}.\n", "link": "http://arxiv.org/abs/2411.18211v1", "date": "2024-11-27", "relevancy": 2.2085, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeMarker%3A%20A%20Versatile%20Video-LLM%20for%20Long%20and%20Short%20Video%20Understanding%0A%20%20with%20Superior%20Temporal%20Localization%20Ability&body=Title%3A%20TimeMarker%3A%20A%20Versatile%20Video-LLM%20for%20Long%20and%20Short%20Video%20Understanding%0A%20%20with%20Superior%20Temporal%20Localization%20Ability%0AAuthor%3A%20Shimin%20Chen%20and%20Xiaohan%20Lan%20and%20Yitian%20Yuan%20and%20Zequn%20Jie%20and%20Lin%20Ma%0AAbstract%3A%20%20%20Rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%20advanced%0Amultimodal%20large%20language%20models%20%28LMMs%29%2C%20particularly%20in%20vision-language%20tasks.%0AHowever%2C%20existing%20video-language%20models%20often%20overlook%20precise%20temporal%0Alocalization%20and%20struggle%20with%20videos%20of%20varying%20lengths.%20We%20introduce%0ATimeMarker%2C%20a%20versatile%20Video-LLM%20designed%20for%20high-quality%20dialogue%20based%20on%0Avideo%20content%2C%20emphasizing%20temporal%20localization.%20TimeMarker%20integrates%0ATemporal%20Separator%20Tokens%20to%20enhance%20temporal%20awareness%2C%20accurately%20marking%0Aspecific%20moments%20within%20videos.%20It%20employs%20the%20AnyLength%20mechanism%20for%20dynamic%0Aframe%20sampling%20and%20adaptive%20token%20merging%2C%20enabling%20effective%20handling%20of%20both%0Ashort%20and%20long%20videos.%20Additionally%2C%20TimeMarker%20utilizes%20diverse%20datasets%2C%0Aincluding%20further%20transformed%20temporal-related%20video%20QA%20datasets%2C%20to%20bolster%0Aits%20temporal%20understanding%20capabilities.%20Image%20and%20interleaved%20data%20are%20also%0Aemployed%20to%20further%20enhance%20the%20model%27s%20semantic%20perception%20ability.%0AEvaluations%20demonstrate%20that%20TimeMarker%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20benchmarks%2C%20excelling%20in%20both%20short%20and%20long%20video%20categories.%0AOur%20project%20page%20is%20at%20%5Curl%7Bhttps%3A//github.com/TimeMarker-LLM/TimeMarker/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeMarker%253A%2520A%2520Versatile%2520Video-LLM%2520for%2520Long%2520and%2520Short%2520Video%2520Understanding%250A%2520%2520with%2520Superior%2520Temporal%2520Localization%2520Ability%26entry.906535625%3DShimin%2520Chen%2520and%2520Xiaohan%2520Lan%2520and%2520Yitian%2520Yuan%2520and%2520Zequn%2520Jie%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520Rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520significantly%2520advanced%250Amultimodal%2520large%2520language%2520models%2520%2528LMMs%2529%252C%2520particularly%2520in%2520vision-language%2520tasks.%250AHowever%252C%2520existing%2520video-language%2520models%2520often%2520overlook%2520precise%2520temporal%250Alocalization%2520and%2520struggle%2520with%2520videos%2520of%2520varying%2520lengths.%2520We%2520introduce%250ATimeMarker%252C%2520a%2520versatile%2520Video-LLM%2520designed%2520for%2520high-quality%2520dialogue%2520based%2520on%250Avideo%2520content%252C%2520emphasizing%2520temporal%2520localization.%2520TimeMarker%2520integrates%250ATemporal%2520Separator%2520Tokens%2520to%2520enhance%2520temporal%2520awareness%252C%2520accurately%2520marking%250Aspecific%2520moments%2520within%2520videos.%2520It%2520employs%2520the%2520AnyLength%2520mechanism%2520for%2520dynamic%250Aframe%2520sampling%2520and%2520adaptive%2520token%2520merging%252C%2520enabling%2520effective%2520handling%2520of%2520both%250Ashort%2520and%2520long%2520videos.%2520Additionally%252C%2520TimeMarker%2520utilizes%2520diverse%2520datasets%252C%250Aincluding%2520further%2520transformed%2520temporal-related%2520video%2520QA%2520datasets%252C%2520to%2520bolster%250Aits%2520temporal%2520understanding%2520capabilities.%2520Image%2520and%2520interleaved%2520data%2520are%2520also%250Aemployed%2520to%2520further%2520enhance%2520the%2520model%2527s%2520semantic%2520perception%2520ability.%250AEvaluations%2520demonstrate%2520that%2520TimeMarker%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520multiple%2520benchmarks%252C%2520excelling%2520in%2520both%2520short%2520and%2520long%2520video%2520categories.%250AOur%2520project%2520page%2520is%2520at%2520%255Curl%257Bhttps%253A//github.com/TimeMarker-LLM/TimeMarker/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeMarker%3A%20A%20Versatile%20Video-LLM%20for%20Long%20and%20Short%20Video%20Understanding%0A%20%20with%20Superior%20Temporal%20Localization%20Ability&entry.906535625=Shimin%20Chen%20and%20Xiaohan%20Lan%20and%20Yitian%20Yuan%20and%20Zequn%20Jie%20and%20Lin%20Ma&entry.1292438233=%20%20Rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%20advanced%0Amultimodal%20large%20language%20models%20%28LMMs%29%2C%20particularly%20in%20vision-language%20tasks.%0AHowever%2C%20existing%20video-language%20models%20often%20overlook%20precise%20temporal%0Alocalization%20and%20struggle%20with%20videos%20of%20varying%20lengths.%20We%20introduce%0ATimeMarker%2C%20a%20versatile%20Video-LLM%20designed%20for%20high-quality%20dialogue%20based%20on%0Avideo%20content%2C%20emphasizing%20temporal%20localization.%20TimeMarker%20integrates%0ATemporal%20Separator%20Tokens%20to%20enhance%20temporal%20awareness%2C%20accurately%20marking%0Aspecific%20moments%20within%20videos.%20It%20employs%20the%20AnyLength%20mechanism%20for%20dynamic%0Aframe%20sampling%20and%20adaptive%20token%20merging%2C%20enabling%20effective%20handling%20of%20both%0Ashort%20and%20long%20videos.%20Additionally%2C%20TimeMarker%20utilizes%20diverse%20datasets%2C%0Aincluding%20further%20transformed%20temporal-related%20video%20QA%20datasets%2C%20to%20bolster%0Aits%20temporal%20understanding%20capabilities.%20Image%20and%20interleaved%20data%20are%20also%0Aemployed%20to%20further%20enhance%20the%20model%27s%20semantic%20perception%20ability.%0AEvaluations%20demonstrate%20that%20TimeMarker%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20benchmarks%2C%20excelling%20in%20both%20short%20and%20long%20video%20categories.%0AOur%20project%20page%20is%20at%20%5Curl%7Bhttps%3A//github.com/TimeMarker-LLM/TimeMarker/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18211v1&entry.124074799=Read"},
{"title": "XR-MBT: Multi-modal Full Body Tracking for XR through Self-Supervision\n  with Learned Depth Point Cloud Registration", "author": "Denys Rozumnyi and Nadine Bertsch and Othman Sbai and Filippo Arcadu and Yuhua Chen and Artsiom Sanakoyeu and Manoj Kumar and Catherine Herold and Robin Kips", "abstract": "  Tracking the full body motions of users in XR (AR/VR) devices is a\nfundamental challenge to bring a sense of authentic social presence. Due to the\nabsence of dedicated leg sensors, currently available body tracking methods\nadopt a synthesis approach to generate plausible motions given a 3-point signal\nfrom the head and controller tracking. In order to enable mixed reality\nfeatures, modern XR devices are capable of estimating depth information of the\nheadset surroundings using available sensors combined with dedicated machine\nlearning models. Such egocentric depth sensing cannot drive the body directly,\nas it is not registered and is incomplete due to limited field-of-view and body\nself-occlusions. For the first time, we propose to leverage the available depth\nsensing signal combined with self-supervision to learn a multi-modal pose\nestimation model capable of tracking full body motions in real time on XR\ndevices. We demonstrate how current 3-point motion synthesis models can be\nextended to point cloud modalities using a semantic point cloud encoder network\ncombined with a residual network for multi-modal pose estimation. These modules\nare trained jointly in a self-supervised way, leveraging a combination of real\nunregistered point clouds and simulated data obtained from motion capture. We\ncompare our approach against several state-of-the-art systems for XR body\ntracking and show that our method accurately tracks a diverse range of body\nmotions. XR-MBT tracks legs in XR for the first time, whereas traditional\nsynthesis approaches based on partial body tracking are blind.\n", "link": "http://arxiv.org/abs/2411.18377v1", "date": "2024-11-27", "relevancy": 2.2054, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XR-MBT%3A%20Multi-modal%20Full%20Body%20Tracking%20for%20XR%20through%20Self-Supervision%0A%20%20with%20Learned%20Depth%20Point%20Cloud%20Registration&body=Title%3A%20XR-MBT%3A%20Multi-modal%20Full%20Body%20Tracking%20for%20XR%20through%20Self-Supervision%0A%20%20with%20Learned%20Depth%20Point%20Cloud%20Registration%0AAuthor%3A%20Denys%20Rozumnyi%20and%20Nadine%20Bertsch%20and%20Othman%20Sbai%20and%20Filippo%20Arcadu%20and%20Yuhua%20Chen%20and%20Artsiom%20Sanakoyeu%20and%20Manoj%20Kumar%20and%20Catherine%20Herold%20and%20Robin%20Kips%0AAbstract%3A%20%20%20Tracking%20the%20full%20body%20motions%20of%20users%20in%20XR%20%28AR/VR%29%20devices%20is%20a%0Afundamental%20challenge%20to%20bring%20a%20sense%20of%20authentic%20social%20presence.%20Due%20to%20the%0Aabsence%20of%20dedicated%20leg%20sensors%2C%20currently%20available%20body%20tracking%20methods%0Aadopt%20a%20synthesis%20approach%20to%20generate%20plausible%20motions%20given%20a%203-point%20signal%0Afrom%20the%20head%20and%20controller%20tracking.%20In%20order%20to%20enable%20mixed%20reality%0Afeatures%2C%20modern%20XR%20devices%20are%20capable%20of%20estimating%20depth%20information%20of%20the%0Aheadset%20surroundings%20using%20available%20sensors%20combined%20with%20dedicated%20machine%0Alearning%20models.%20Such%20egocentric%20depth%20sensing%20cannot%20drive%20the%20body%20directly%2C%0Aas%20it%20is%20not%20registered%20and%20is%20incomplete%20due%20to%20limited%20field-of-view%20and%20body%0Aself-occlusions.%20For%20the%20first%20time%2C%20we%20propose%20to%20leverage%20the%20available%20depth%0Asensing%20signal%20combined%20with%20self-supervision%20to%20learn%20a%20multi-modal%20pose%0Aestimation%20model%20capable%20of%20tracking%20full%20body%20motions%20in%20real%20time%20on%20XR%0Adevices.%20We%20demonstrate%20how%20current%203-point%20motion%20synthesis%20models%20can%20be%0Aextended%20to%20point%20cloud%20modalities%20using%20a%20semantic%20point%20cloud%20encoder%20network%0Acombined%20with%20a%20residual%20network%20for%20multi-modal%20pose%20estimation.%20These%20modules%0Aare%20trained%20jointly%20in%20a%20self-supervised%20way%2C%20leveraging%20a%20combination%20of%20real%0Aunregistered%20point%20clouds%20and%20simulated%20data%20obtained%20from%20motion%20capture.%20We%0Acompare%20our%20approach%20against%20several%20state-of-the-art%20systems%20for%20XR%20body%0Atracking%20and%20show%20that%20our%20method%20accurately%20tracks%20a%20diverse%20range%20of%20body%0Amotions.%20XR-MBT%20tracks%20legs%20in%20XR%20for%20the%20first%20time%2C%20whereas%20traditional%0Asynthesis%20approaches%20based%20on%20partial%20body%20tracking%20are%20blind.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXR-MBT%253A%2520Multi-modal%2520Full%2520Body%2520Tracking%2520for%2520XR%2520through%2520Self-Supervision%250A%2520%2520with%2520Learned%2520Depth%2520Point%2520Cloud%2520Registration%26entry.906535625%3DDenys%2520Rozumnyi%2520and%2520Nadine%2520Bertsch%2520and%2520Othman%2520Sbai%2520and%2520Filippo%2520Arcadu%2520and%2520Yuhua%2520Chen%2520and%2520Artsiom%2520Sanakoyeu%2520and%2520Manoj%2520Kumar%2520and%2520Catherine%2520Herold%2520and%2520Robin%2520Kips%26entry.1292438233%3D%2520%2520Tracking%2520the%2520full%2520body%2520motions%2520of%2520users%2520in%2520XR%2520%2528AR/VR%2529%2520devices%2520is%2520a%250Afundamental%2520challenge%2520to%2520bring%2520a%2520sense%2520of%2520authentic%2520social%2520presence.%2520Due%2520to%2520the%250Aabsence%2520of%2520dedicated%2520leg%2520sensors%252C%2520currently%2520available%2520body%2520tracking%2520methods%250Aadopt%2520a%2520synthesis%2520approach%2520to%2520generate%2520plausible%2520motions%2520given%2520a%25203-point%2520signal%250Afrom%2520the%2520head%2520and%2520controller%2520tracking.%2520In%2520order%2520to%2520enable%2520mixed%2520reality%250Afeatures%252C%2520modern%2520XR%2520devices%2520are%2520capable%2520of%2520estimating%2520depth%2520information%2520of%2520the%250Aheadset%2520surroundings%2520using%2520available%2520sensors%2520combined%2520with%2520dedicated%2520machine%250Alearning%2520models.%2520Such%2520egocentric%2520depth%2520sensing%2520cannot%2520drive%2520the%2520body%2520directly%252C%250Aas%2520it%2520is%2520not%2520registered%2520and%2520is%2520incomplete%2520due%2520to%2520limited%2520field-of-view%2520and%2520body%250Aself-occlusions.%2520For%2520the%2520first%2520time%252C%2520we%2520propose%2520to%2520leverage%2520the%2520available%2520depth%250Asensing%2520signal%2520combined%2520with%2520self-supervision%2520to%2520learn%2520a%2520multi-modal%2520pose%250Aestimation%2520model%2520capable%2520of%2520tracking%2520full%2520body%2520motions%2520in%2520real%2520time%2520on%2520XR%250Adevices.%2520We%2520demonstrate%2520how%2520current%25203-point%2520motion%2520synthesis%2520models%2520can%2520be%250Aextended%2520to%2520point%2520cloud%2520modalities%2520using%2520a%2520semantic%2520point%2520cloud%2520encoder%2520network%250Acombined%2520with%2520a%2520residual%2520network%2520for%2520multi-modal%2520pose%2520estimation.%2520These%2520modules%250Aare%2520trained%2520jointly%2520in%2520a%2520self-supervised%2520way%252C%2520leveraging%2520a%2520combination%2520of%2520real%250Aunregistered%2520point%2520clouds%2520and%2520simulated%2520data%2520obtained%2520from%2520motion%2520capture.%2520We%250Acompare%2520our%2520approach%2520against%2520several%2520state-of-the-art%2520systems%2520for%2520XR%2520body%250Atracking%2520and%2520show%2520that%2520our%2520method%2520accurately%2520tracks%2520a%2520diverse%2520range%2520of%2520body%250Amotions.%2520XR-MBT%2520tracks%2520legs%2520in%2520XR%2520for%2520the%2520first%2520time%252C%2520whereas%2520traditional%250Asynthesis%2520approaches%2520based%2520on%2520partial%2520body%2520tracking%2520are%2520blind.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XR-MBT%3A%20Multi-modal%20Full%20Body%20Tracking%20for%20XR%20through%20Self-Supervision%0A%20%20with%20Learned%20Depth%20Point%20Cloud%20Registration&entry.906535625=Denys%20Rozumnyi%20and%20Nadine%20Bertsch%20and%20Othman%20Sbai%20and%20Filippo%20Arcadu%20and%20Yuhua%20Chen%20and%20Artsiom%20Sanakoyeu%20and%20Manoj%20Kumar%20and%20Catherine%20Herold%20and%20Robin%20Kips&entry.1292438233=%20%20Tracking%20the%20full%20body%20motions%20of%20users%20in%20XR%20%28AR/VR%29%20devices%20is%20a%0Afundamental%20challenge%20to%20bring%20a%20sense%20of%20authentic%20social%20presence.%20Due%20to%20the%0Aabsence%20of%20dedicated%20leg%20sensors%2C%20currently%20available%20body%20tracking%20methods%0Aadopt%20a%20synthesis%20approach%20to%20generate%20plausible%20motions%20given%20a%203-point%20signal%0Afrom%20the%20head%20and%20controller%20tracking.%20In%20order%20to%20enable%20mixed%20reality%0Afeatures%2C%20modern%20XR%20devices%20are%20capable%20of%20estimating%20depth%20information%20of%20the%0Aheadset%20surroundings%20using%20available%20sensors%20combined%20with%20dedicated%20machine%0Alearning%20models.%20Such%20egocentric%20depth%20sensing%20cannot%20drive%20the%20body%20directly%2C%0Aas%20it%20is%20not%20registered%20and%20is%20incomplete%20due%20to%20limited%20field-of-view%20and%20body%0Aself-occlusions.%20For%20the%20first%20time%2C%20we%20propose%20to%20leverage%20the%20available%20depth%0Asensing%20signal%20combined%20with%20self-supervision%20to%20learn%20a%20multi-modal%20pose%0Aestimation%20model%20capable%20of%20tracking%20full%20body%20motions%20in%20real%20time%20on%20XR%0Adevices.%20We%20demonstrate%20how%20current%203-point%20motion%20synthesis%20models%20can%20be%0Aextended%20to%20point%20cloud%20modalities%20using%20a%20semantic%20point%20cloud%20encoder%20network%0Acombined%20with%20a%20residual%20network%20for%20multi-modal%20pose%20estimation.%20These%20modules%0Aare%20trained%20jointly%20in%20a%20self-supervised%20way%2C%20leveraging%20a%20combination%20of%20real%0Aunregistered%20point%20clouds%20and%20simulated%20data%20obtained%20from%20motion%20capture.%20We%0Acompare%20our%20approach%20against%20several%20state-of-the-art%20systems%20for%20XR%20body%0Atracking%20and%20show%20that%20our%20method%20accurately%20tracks%20a%20diverse%20range%20of%20body%0Amotions.%20XR-MBT%20tracks%20legs%20in%20XR%20for%20the%20first%20time%2C%20whereas%20traditional%0Asynthesis%20approaches%20based%20on%20partial%20body%20tracking%20are%20blind.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18377v1&entry.124074799=Read"},
{"title": "Feature-Factory: Automating Software Feature Integration Using\n  Generative AI", "author": "Ruslan Idelfonso Magana Vsevolodovna", "abstract": "  Integrating new features into existing software projects can be a complex and\ntime-consuming process. Feature-Factory leverages Generative AI with WatsonX.ai\nto automate the analysis, planning, and implementation of feature requests. By\ncombining advanced project parsing, dependency resolution, and AI-generated\ncode, the program ensures seamless integration of features into software\nsystems while maintaining structural integrity. This paper presents the\nmethodology, mathematical model, and results of the Feature-Factory framework.\n", "link": "http://arxiv.org/abs/2411.18226v1", "date": "2024-11-27", "relevancy": 2.1988, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4708}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4243}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Factory%3A%20Automating%20Software%20Feature%20Integration%20Using%0A%20%20Generative%20AI&body=Title%3A%20Feature-Factory%3A%20Automating%20Software%20Feature%20Integration%20Using%0A%20%20Generative%20AI%0AAuthor%3A%20Ruslan%20Idelfonso%20Magana%20Vsevolodovna%0AAbstract%3A%20%20%20Integrating%20new%20features%20into%20existing%20software%20projects%20can%20be%20a%20complex%20and%0Atime-consuming%20process.%20Feature-Factory%20leverages%20Generative%20AI%20with%20WatsonX.ai%0Ato%20automate%20the%20analysis%2C%20planning%2C%20and%20implementation%20of%20feature%20requests.%20By%0Acombining%20advanced%20project%20parsing%2C%20dependency%20resolution%2C%20and%20AI-generated%0Acode%2C%20the%20program%20ensures%20seamless%20integration%20of%20features%20into%20software%0Asystems%20while%20maintaining%20structural%20integrity.%20This%20paper%20presents%20the%0Amethodology%2C%20mathematical%20model%2C%20and%20results%20of%20the%20Feature-Factory%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Factory%253A%2520Automating%2520Software%2520Feature%2520Integration%2520Using%250A%2520%2520Generative%2520AI%26entry.906535625%3DRuslan%2520Idelfonso%2520Magana%2520Vsevolodovna%26entry.1292438233%3D%2520%2520Integrating%2520new%2520features%2520into%2520existing%2520software%2520projects%2520can%2520be%2520a%2520complex%2520and%250Atime-consuming%2520process.%2520Feature-Factory%2520leverages%2520Generative%2520AI%2520with%2520WatsonX.ai%250Ato%2520automate%2520the%2520analysis%252C%2520planning%252C%2520and%2520implementation%2520of%2520feature%2520requests.%2520By%250Acombining%2520advanced%2520project%2520parsing%252C%2520dependency%2520resolution%252C%2520and%2520AI-generated%250Acode%252C%2520the%2520program%2520ensures%2520seamless%2520integration%2520of%2520features%2520into%2520software%250Asystems%2520while%2520maintaining%2520structural%2520integrity.%2520This%2520paper%2520presents%2520the%250Amethodology%252C%2520mathematical%2520model%252C%2520and%2520results%2520of%2520the%2520Feature-Factory%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Factory%3A%20Automating%20Software%20Feature%20Integration%20Using%0A%20%20Generative%20AI&entry.906535625=Ruslan%20Idelfonso%20Magana%20Vsevolodovna&entry.1292438233=%20%20Integrating%20new%20features%20into%20existing%20software%20projects%20can%20be%20a%20complex%20and%0Atime-consuming%20process.%20Feature-Factory%20leverages%20Generative%20AI%20with%20WatsonX.ai%0Ato%20automate%20the%20analysis%2C%20planning%2C%20and%20implementation%20of%20feature%20requests.%20By%0Acombining%20advanced%20project%20parsing%2C%20dependency%20resolution%2C%20and%20AI-generated%0Acode%2C%20the%20program%20ensures%20seamless%20integration%20of%20features%20into%20software%0Asystems%20while%20maintaining%20structural%20integrity.%20This%20paper%20presents%20the%0Amethodology%2C%20mathematical%20model%2C%20and%20results%20of%20the%20Feature-Factory%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18226v1&entry.124074799=Read"},
{"title": "Geometry of the Visual Cortex with Applications to Image Inpainting and\n  Enhancement", "author": "Francesco Ballerin and Erlend Grong", "abstract": "  Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure\ninspired by the visual cortex V1, we propose algorithms for image inpainting\nand enhancement based on hypoelliptic diffusion. We innovate on previous\nimplementations of the methods by Citti, Sarti, and Boscain et al., by\nproposing an alternative that prevents fading and is capable of producing\nsharper results in a procedure that we call WaxOn-WaxOff. We also exploit the\nsub-Riemannian structure to define a completely new unsharp filter using\n$SE(2)$, analogous to the classical unsharp filter for 2D image processing. We\ndemonstrate our method on blood vessels enhancement in retinal scans.\n", "link": "http://arxiv.org/abs/2308.07652v4", "date": "2024-11-27", "relevancy": 2.1956, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5875}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5214}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement&body=Title%3A%20Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement%0AAuthor%3A%20Francesco%20Ballerin%20and%20Erlend%20Grong%0AAbstract%3A%20%20%20Equipping%20the%20rototranslation%20group%20%24SE%282%29%24%20with%20a%20sub-Riemannian%20structure%0Ainspired%20by%20the%20visual%20cortex%20V1%2C%20we%20propose%20algorithms%20for%20image%20inpainting%0Aand%20enhancement%20based%20on%20hypoelliptic%20diffusion.%20We%20innovate%20on%20previous%0Aimplementations%20of%20the%20methods%20by%20Citti%2C%20Sarti%2C%20and%20Boscain%20et%20al.%2C%20by%0Aproposing%20an%20alternative%20that%20prevents%20fading%20and%20is%20capable%20of%20producing%0Asharper%20results%20in%20a%20procedure%20that%20we%20call%20WaxOn-WaxOff.%20We%20also%20exploit%20the%0Asub-Riemannian%20structure%20to%20define%20a%20completely%20new%20unsharp%20filter%20using%0A%24SE%282%29%24%2C%20analogous%20to%20the%20classical%20unsharp%20filter%20for%202D%20image%20processing.%20We%0Ademonstrate%20our%20method%20on%20blood%20vessels%20enhancement%20in%20retinal%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07652v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520of%2520the%2520Visual%2520Cortex%2520with%2520Applications%2520to%2520Image%2520Inpainting%2520and%250A%2520%2520Enhancement%26entry.906535625%3DFrancesco%2520Ballerin%2520and%2520Erlend%2520Grong%26entry.1292438233%3D%2520%2520Equipping%2520the%2520rototranslation%2520group%2520%2524SE%25282%2529%2524%2520with%2520a%2520sub-Riemannian%2520structure%250Ainspired%2520by%2520the%2520visual%2520cortex%2520V1%252C%2520we%2520propose%2520algorithms%2520for%2520image%2520inpainting%250Aand%2520enhancement%2520based%2520on%2520hypoelliptic%2520diffusion.%2520We%2520innovate%2520on%2520previous%250Aimplementations%2520of%2520the%2520methods%2520by%2520Citti%252C%2520Sarti%252C%2520and%2520Boscain%2520et%2520al.%252C%2520by%250Aproposing%2520an%2520alternative%2520that%2520prevents%2520fading%2520and%2520is%2520capable%2520of%2520producing%250Asharper%2520results%2520in%2520a%2520procedure%2520that%2520we%2520call%2520WaxOn-WaxOff.%2520We%2520also%2520exploit%2520the%250Asub-Riemannian%2520structure%2520to%2520define%2520a%2520completely%2520new%2520unsharp%2520filter%2520using%250A%2524SE%25282%2529%2524%252C%2520analogous%2520to%2520the%2520classical%2520unsharp%2520filter%2520for%25202D%2520image%2520processing.%2520We%250Ademonstrate%2520our%2520method%2520on%2520blood%2520vessels%2520enhancement%2520in%2520retinal%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07652v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement&entry.906535625=Francesco%20Ballerin%20and%20Erlend%20Grong&entry.1292438233=%20%20Equipping%20the%20rototranslation%20group%20%24SE%282%29%24%20with%20a%20sub-Riemannian%20structure%0Ainspired%20by%20the%20visual%20cortex%20V1%2C%20we%20propose%20algorithms%20for%20image%20inpainting%0Aand%20enhancement%20based%20on%20hypoelliptic%20diffusion.%20We%20innovate%20on%20previous%0Aimplementations%20of%20the%20methods%20by%20Citti%2C%20Sarti%2C%20and%20Boscain%20et%20al.%2C%20by%0Aproposing%20an%20alternative%20that%20prevents%20fading%20and%20is%20capable%20of%20producing%0Asharper%20results%20in%20a%20procedure%20that%20we%20call%20WaxOn-WaxOff.%20We%20also%20exploit%20the%0Asub-Riemannian%20structure%20to%20define%20a%20completely%20new%20unsharp%20filter%20using%0A%24SE%282%29%24%2C%20analogous%20to%20the%20classical%20unsharp%20filter%20for%202D%20image%20processing.%20We%0Ademonstrate%20our%20method%20on%20blood%20vessels%20enhancement%20in%20retinal%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07652v4&entry.124074799=Read"},
{"title": "Isometry pursuit", "author": "Samson Koelle and Marina Meila", "abstract": "  Isometry pursuit is a convex algorithm for identifying orthonormal\ncolumn-submatrices of wide matrices. It consists of a novel normalization\nmethod followed by multitask basis pursuit. Applied to Jacobians of putative\ncoordinate functions, it helps identity isometric embeddings from within\ninterpretable dictionaries. We provide theoretical and experimental results\njustifying this method. For problems involving coordinate selection and\ndiversification, it offers a synergistic alternative to greedy and brute force\nsearch.\n", "link": "http://arxiv.org/abs/2411.18502v1", "date": "2024-11-27", "relevancy": 2.1933, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4438}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Isometry%20pursuit&body=Title%3A%20Isometry%20pursuit%0AAuthor%3A%20Samson%20Koelle%20and%20Marina%20Meila%0AAbstract%3A%20%20%20Isometry%20pursuit%20is%20a%20convex%20algorithm%20for%20identifying%20orthonormal%0Acolumn-submatrices%20of%20wide%20matrices.%20It%20consists%20of%20a%20novel%20normalization%0Amethod%20followed%20by%20multitask%20basis%20pursuit.%20Applied%20to%20Jacobians%20of%20putative%0Acoordinate%20functions%2C%20it%20helps%20identity%20isometric%20embeddings%20from%20within%0Ainterpretable%20dictionaries.%20We%20provide%20theoretical%20and%20experimental%20results%0Ajustifying%20this%20method.%20For%20problems%20involving%20coordinate%20selection%20and%0Adiversification%2C%20it%20offers%20a%20synergistic%20alternative%20to%20greedy%20and%20brute%20force%0Asearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIsometry%2520pursuit%26entry.906535625%3DSamson%2520Koelle%2520and%2520Marina%2520Meila%26entry.1292438233%3D%2520%2520Isometry%2520pursuit%2520is%2520a%2520convex%2520algorithm%2520for%2520identifying%2520orthonormal%250Acolumn-submatrices%2520of%2520wide%2520matrices.%2520It%2520consists%2520of%2520a%2520novel%2520normalization%250Amethod%2520followed%2520by%2520multitask%2520basis%2520pursuit.%2520Applied%2520to%2520Jacobians%2520of%2520putative%250Acoordinate%2520functions%252C%2520it%2520helps%2520identity%2520isometric%2520embeddings%2520from%2520within%250Ainterpretable%2520dictionaries.%2520We%2520provide%2520theoretical%2520and%2520experimental%2520results%250Ajustifying%2520this%2520method.%2520For%2520problems%2520involving%2520coordinate%2520selection%2520and%250Adiversification%252C%2520it%2520offers%2520a%2520synergistic%2520alternative%2520to%2520greedy%2520and%2520brute%2520force%250Asearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Isometry%20pursuit&entry.906535625=Samson%20Koelle%20and%20Marina%20Meila&entry.1292438233=%20%20Isometry%20pursuit%20is%20a%20convex%20algorithm%20for%20identifying%20orthonormal%0Acolumn-submatrices%20of%20wide%20matrices.%20It%20consists%20of%20a%20novel%20normalization%0Amethod%20followed%20by%20multitask%20basis%20pursuit.%20Applied%20to%20Jacobians%20of%20putative%0Acoordinate%20functions%2C%20it%20helps%20identity%20isometric%20embeddings%20from%20within%0Ainterpretable%20dictionaries.%20We%20provide%20theoretical%20and%20experimental%20results%0Ajustifying%20this%20method.%20For%20problems%20involving%20coordinate%20selection%20and%0Adiversification%2C%20it%20offers%20a%20synergistic%20alternative%20to%20greedy%20and%20brute%20force%0Asearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18502v1&entry.124074799=Read"},
{"title": "Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse\n  Training Perspective", "author": "Zhi Zhang and Jiayi Shen and Congfeng Cao and Gaole Dai and Shiji Zhou and Qizhe Zhang and Shanghang Zhang and Ekaterina Shutova", "abstract": "  Advancing towards generalist agents necessitates the concurrent processing of\nmultiple tasks using a unified model, thereby underscoring the growing\nsignificance of simultaneous model training on multiple downstream tasks. A\ncommon issue in multi-task learning is the occurrence of gradient conflict,\nwhich leads to potential competition among different tasks during joint\ntraining. This competition often results in improvements in one task at the\nexpense of deterioration in another. Although several optimization methods have\nbeen developed to address this issue by manipulating task gradients for better\ntask balancing, they cannot decrease the incidence of gradient conflict. In\nthis paper, we systematically investigate the occurrence of gradient conflict\nacross different methods and propose a strategy to reduce such conflicts\nthrough sparse training (ST), wherein only a portion of the model's parameters\nare updated during training while keeping the rest unchanged. Our extensive\nexperiments demonstrate that ST effectively mitigates conflicting gradients and\nleads to superior performance. Furthermore, ST can be easily integrated with\ngradient manipulation techniques, thus enhancing their effectiveness.\n", "link": "http://arxiv.org/abs/2411.18615v1", "date": "2024-11-27", "relevancy": 2.1781, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5534}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5463}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proactive%20Gradient%20Conflict%20Mitigation%20in%20Multi-Task%20Learning%3A%20A%20Sparse%0A%20%20Training%20Perspective&body=Title%3A%20Proactive%20Gradient%20Conflict%20Mitigation%20in%20Multi-Task%20Learning%3A%20A%20Sparse%0A%20%20Training%20Perspective%0AAuthor%3A%20Zhi%20Zhang%20and%20Jiayi%20Shen%20and%20Congfeng%20Cao%20and%20Gaole%20Dai%20and%20Shiji%20Zhou%20and%20Qizhe%20Zhang%20and%20Shanghang%20Zhang%20and%20Ekaterina%20Shutova%0AAbstract%3A%20%20%20Advancing%20towards%20generalist%20agents%20necessitates%20the%20concurrent%20processing%20of%0Amultiple%20tasks%20using%20a%20unified%20model%2C%20thereby%20underscoring%20the%20growing%0Asignificance%20of%20simultaneous%20model%20training%20on%20multiple%20downstream%20tasks.%20A%0Acommon%20issue%20in%20multi-task%20learning%20is%20the%20occurrence%20of%20gradient%20conflict%2C%0Awhich%20leads%20to%20potential%20competition%20among%20different%20tasks%20during%20joint%0Atraining.%20This%20competition%20often%20results%20in%20improvements%20in%20one%20task%20at%20the%0Aexpense%20of%20deterioration%20in%20another.%20Although%20several%20optimization%20methods%20have%0Abeen%20developed%20to%20address%20this%20issue%20by%20manipulating%20task%20gradients%20for%20better%0Atask%20balancing%2C%20they%20cannot%20decrease%20the%20incidence%20of%20gradient%20conflict.%20In%0Athis%20paper%2C%20we%20systematically%20investigate%20the%20occurrence%20of%20gradient%20conflict%0Aacross%20different%20methods%20and%20propose%20a%20strategy%20to%20reduce%20such%20conflicts%0Athrough%20sparse%20training%20%28ST%29%2C%20wherein%20only%20a%20portion%20of%20the%20model%27s%20parameters%0Aare%20updated%20during%20training%20while%20keeping%20the%20rest%20unchanged.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20ST%20effectively%20mitigates%20conflicting%20gradients%20and%0Aleads%20to%20superior%20performance.%20Furthermore%2C%20ST%20can%20be%20easily%20integrated%20with%0Agradient%20manipulation%20techniques%2C%20thus%20enhancing%20their%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactive%2520Gradient%2520Conflict%2520Mitigation%2520in%2520Multi-Task%2520Learning%253A%2520A%2520Sparse%250A%2520%2520Training%2520Perspective%26entry.906535625%3DZhi%2520Zhang%2520and%2520Jiayi%2520Shen%2520and%2520Congfeng%2520Cao%2520and%2520Gaole%2520Dai%2520and%2520Shiji%2520Zhou%2520and%2520Qizhe%2520Zhang%2520and%2520Shanghang%2520Zhang%2520and%2520Ekaterina%2520Shutova%26entry.1292438233%3D%2520%2520Advancing%2520towards%2520generalist%2520agents%2520necessitates%2520the%2520concurrent%2520processing%2520of%250Amultiple%2520tasks%2520using%2520a%2520unified%2520model%252C%2520thereby%2520underscoring%2520the%2520growing%250Asignificance%2520of%2520simultaneous%2520model%2520training%2520on%2520multiple%2520downstream%2520tasks.%2520A%250Acommon%2520issue%2520in%2520multi-task%2520learning%2520is%2520the%2520occurrence%2520of%2520gradient%2520conflict%252C%250Awhich%2520leads%2520to%2520potential%2520competition%2520among%2520different%2520tasks%2520during%2520joint%250Atraining.%2520This%2520competition%2520often%2520results%2520in%2520improvements%2520in%2520one%2520task%2520at%2520the%250Aexpense%2520of%2520deterioration%2520in%2520another.%2520Although%2520several%2520optimization%2520methods%2520have%250Abeen%2520developed%2520to%2520address%2520this%2520issue%2520by%2520manipulating%2520task%2520gradients%2520for%2520better%250Atask%2520balancing%252C%2520they%2520cannot%2520decrease%2520the%2520incidence%2520of%2520gradient%2520conflict.%2520In%250Athis%2520paper%252C%2520we%2520systematically%2520investigate%2520the%2520occurrence%2520of%2520gradient%2520conflict%250Aacross%2520different%2520methods%2520and%2520propose%2520a%2520strategy%2520to%2520reduce%2520such%2520conflicts%250Athrough%2520sparse%2520training%2520%2528ST%2529%252C%2520wherein%2520only%2520a%2520portion%2520of%2520the%2520model%2527s%2520parameters%250Aare%2520updated%2520during%2520training%2520while%2520keeping%2520the%2520rest%2520unchanged.%2520Our%2520extensive%250Aexperiments%2520demonstrate%2520that%2520ST%2520effectively%2520mitigates%2520conflicting%2520gradients%2520and%250Aleads%2520to%2520superior%2520performance.%2520Furthermore%252C%2520ST%2520can%2520be%2520easily%2520integrated%2520with%250Agradient%2520manipulation%2520techniques%252C%2520thus%2520enhancing%2520their%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proactive%20Gradient%20Conflict%20Mitigation%20in%20Multi-Task%20Learning%3A%20A%20Sparse%0A%20%20Training%20Perspective&entry.906535625=Zhi%20Zhang%20and%20Jiayi%20Shen%20and%20Congfeng%20Cao%20and%20Gaole%20Dai%20and%20Shiji%20Zhou%20and%20Qizhe%20Zhang%20and%20Shanghang%20Zhang%20and%20Ekaterina%20Shutova&entry.1292438233=%20%20Advancing%20towards%20generalist%20agents%20necessitates%20the%20concurrent%20processing%20of%0Amultiple%20tasks%20using%20a%20unified%20model%2C%20thereby%20underscoring%20the%20growing%0Asignificance%20of%20simultaneous%20model%20training%20on%20multiple%20downstream%20tasks.%20A%0Acommon%20issue%20in%20multi-task%20learning%20is%20the%20occurrence%20of%20gradient%20conflict%2C%0Awhich%20leads%20to%20potential%20competition%20among%20different%20tasks%20during%20joint%0Atraining.%20This%20competition%20often%20results%20in%20improvements%20in%20one%20task%20at%20the%0Aexpense%20of%20deterioration%20in%20another.%20Although%20several%20optimization%20methods%20have%0Abeen%20developed%20to%20address%20this%20issue%20by%20manipulating%20task%20gradients%20for%20better%0Atask%20balancing%2C%20they%20cannot%20decrease%20the%20incidence%20of%20gradient%20conflict.%20In%0Athis%20paper%2C%20we%20systematically%20investigate%20the%20occurrence%20of%20gradient%20conflict%0Aacross%20different%20methods%20and%20propose%20a%20strategy%20to%20reduce%20such%20conflicts%0Athrough%20sparse%20training%20%28ST%29%2C%20wherein%20only%20a%20portion%20of%20the%20model%27s%20parameters%0Aare%20updated%20during%20training%20while%20keeping%20the%20rest%20unchanged.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20ST%20effectively%20mitigates%20conflicting%20gradients%20and%0Aleads%20to%20superior%20performance.%20Furthermore%2C%20ST%20can%20be%20easily%20integrated%20with%0Agradient%20manipulation%20techniques%2C%20thus%20enhancing%20their%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18615v1&entry.124074799=Read"},
{"title": "Large Language Model-Brained GUI Agents: A Survey", "author": "Chaoyun Zhang and Shilin He and Jiaxu Qian and Bowen Li and Liqun Li and Si Qin and Yu Kang and Minghua Ma and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n", "link": "http://arxiv.org/abs/2411.18279v1", "date": "2024-11-27", "relevancy": 2.1753, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&body=Title%3A%20Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey%0AAuthor%3A%20Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Brained%2520GUI%2520Agents%253A%2520A%2520Survey%26entry.906535625%3DChaoyun%2520Zhang%2520and%2520Shilin%2520He%2520and%2520Jiaxu%2520Qian%2520and%2520Bowen%2520Li%2520and%2520Liqun%2520Li%2520and%2520Si%2520Qin%2520and%2520Yu%2520Kang%2520and%2520Minghua%2520Ma%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520GUIs%2520have%2520long%2520been%2520central%2520to%2520human-computer%2520interaction%252C%2520providing%2520an%250Aintuitive%2520and%2520visually-driven%2520way%2520to%2520access%2520and%2520interact%2520with%2520digital%2520systems.%250AThe%2520advent%2520of%2520LLMs%252C%2520particularly%2520multimodal%2520models%252C%2520has%2520ushered%2520in%2520a%2520new%2520era%2520of%250AGUI%2520automation.%2520They%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%2520natural%250Alanguage%2520understanding%252C%2520code%2520generation%252C%2520and%2520visual%2520processing.%2520This%2520has%2520paved%250Athe%2520way%2520for%2520a%2520new%2520generation%2520of%2520LLM-brained%2520GUI%2520agents%2520capable%2520of%2520interpreting%250Acomplex%2520GUI%2520elements%2520and%2520autonomously%2520executing%2520actions%2520based%2520on%2520natural%250Alanguage%2520instructions.%2520These%2520agents%2520represent%2520a%2520paradigm%2520shift%252C%2520enabling%2520users%250Ato%2520perform%2520intricate%252C%2520multi-step%2520tasks%2520through%2520simple%2520conversational%2520commands.%250ATheir%2520applications%2520span%2520across%2520web%2520navigation%252C%2520mobile%2520app%2520interactions%252C%2520and%250Adesktop%2520automation%252C%2520offering%2520a%2520transformative%2520user%2520experience%2520that%250Arevolutionizes%2520how%2520individuals%2520interact%2520with%2520software.%2520This%2520emerging%2520field%2520is%250Arapidly%2520advancing%252C%2520with%2520significant%2520progress%2520in%2520both%2520research%2520and%2520industry.%250A%2520%2520To%2520provide%2520a%2520structured%2520understanding%2520of%2520this%2520trend%252C%2520this%2520paper%2520presents%2520a%250Acomprehensive%2520survey%2520of%2520LLM-brained%2520GUI%2520agents%252C%2520exploring%2520their%2520historical%250Aevolution%252C%2520core%2520components%252C%2520and%2520advanced%2520techniques.%2520We%2520address%2520research%250Aquestions%2520such%2520as%2520existing%2520GUI%2520agent%2520frameworks%252C%2520the%2520collection%2520and%2520utilization%250Aof%2520data%2520for%2520training%2520specialized%2520GUI%2520agents%252C%2520the%2520development%2520of%2520large%2520action%250Amodels%2520tailored%2520for%2520GUI%2520tasks%252C%2520and%2520the%2520evaluation%2520metrics%2520and%2520benchmarks%250Anecessary%2520to%2520assess%2520their%2520effectiveness.%2520Additionally%252C%2520we%2520examine%2520emerging%250Aapplications%2520powered%2520by%2520these%2520agents.%2520Through%2520a%2520detailed%2520analysis%252C%2520this%2520survey%250Aidentifies%2520key%2520research%2520gaps%2520and%2520outlines%2520a%2520roadmap%2520for%2520future%2520advancements%2520in%250Athe%2520field.%2520By%2520consolidating%2520foundational%2520knowledge%2520and%2520state-of-the-art%250Adevelopments%252C%2520this%2520work%2520aims%2520to%2520guide%2520both%2520researchers%2520and%2520practitioners%2520in%250Aovercoming%2520challenges%2520and%2520unlocking%2520the%2520full%2520potential%2520of%2520LLM-brained%2520GUI%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Brained%20GUI%20Agents%3A%20A%20Survey&entry.906535625=Chaoyun%20Zhang%20and%20Shilin%20He%20and%20Jiaxu%20Qian%20and%20Bowen%20Li%20and%20Liqun%20Li%20and%20Si%20Qin%20and%20Yu%20Kang%20and%20Minghua%20Ma%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20GUIs%20have%20long%20been%20central%20to%20human-computer%20interaction%2C%20providing%20an%0Aintuitive%20and%20visually-driven%20way%20to%20access%20and%20interact%20with%20digital%20systems.%0AThe%20advent%20of%20LLMs%2C%20particularly%20multimodal%20models%2C%20has%20ushered%20in%20a%20new%20era%20of%0AGUI%20automation.%20They%20have%20demonstrated%20exceptional%20capabilities%20in%20natural%0Alanguage%20understanding%2C%20code%20generation%2C%20and%20visual%20processing.%20This%20has%20paved%0Athe%20way%20for%20a%20new%20generation%20of%20LLM-brained%20GUI%20agents%20capable%20of%20interpreting%0Acomplex%20GUI%20elements%20and%20autonomously%20executing%20actions%20based%20on%20natural%0Alanguage%20instructions.%20These%20agents%20represent%20a%20paradigm%20shift%2C%20enabling%20users%0Ato%20perform%20intricate%2C%20multi-step%20tasks%20through%20simple%20conversational%20commands.%0ATheir%20applications%20span%20across%20web%20navigation%2C%20mobile%20app%20interactions%2C%20and%0Adesktop%20automation%2C%20offering%20a%20transformative%20user%20experience%20that%0Arevolutionizes%20how%20individuals%20interact%20with%20software.%20This%20emerging%20field%20is%0Arapidly%20advancing%2C%20with%20significant%20progress%20in%20both%20research%20and%20industry.%0A%20%20To%20provide%20a%20structured%20understanding%20of%20this%20trend%2C%20this%20paper%20presents%20a%0Acomprehensive%20survey%20of%20LLM-brained%20GUI%20agents%2C%20exploring%20their%20historical%0Aevolution%2C%20core%20components%2C%20and%20advanced%20techniques.%20We%20address%20research%0Aquestions%20such%20as%20existing%20GUI%20agent%20frameworks%2C%20the%20collection%20and%20utilization%0Aof%20data%20for%20training%20specialized%20GUI%20agents%2C%20the%20development%20of%20large%20action%0Amodels%20tailored%20for%20GUI%20tasks%2C%20and%20the%20evaluation%20metrics%20and%20benchmarks%0Anecessary%20to%20assess%20their%20effectiveness.%20Additionally%2C%20we%20examine%20emerging%0Aapplications%20powered%20by%20these%20agents.%20Through%20a%20detailed%20analysis%2C%20this%20survey%0Aidentifies%20key%20research%20gaps%20and%20outlines%20a%20roadmap%20for%20future%20advancements%20in%0Athe%20field.%20By%20consolidating%20foundational%20knowledge%20and%20state-of-the-art%0Adevelopments%2C%20this%20work%20aims%20to%20guide%20both%20researchers%20and%20practitioners%20in%0Aovercoming%20challenges%20and%20unlocking%20the%20full%20potential%20of%20LLM-brained%20GUI%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18279v1&entry.124074799=Read"},
{"title": "GPT as ghostwriter at the White House", "author": "Jacques Savoy", "abstract": "  Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses.\n", "link": "http://arxiv.org/abs/2411.18365v1", "date": "2024-11-27", "relevancy": 2.1729, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4432}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20as%20ghostwriter%20at%20the%20White%20House&body=Title%3A%20GPT%20as%20ghostwriter%20at%20the%20White%20House%0AAuthor%3A%20Jacques%20Savoy%0AAbstract%3A%20%20%20Recently%20several%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%0Acapability%20to%20generate%20a%20message%20in%20response%20to%20a%20user%20request.%20Such%20scientific%0Abreakthroughs%20promote%20new%20perspectives%20but%20also%20some%20fears.%20The%20main%20focus%20of%0Athis%20study%20is%20to%20analyze%20the%20written%20style%20of%20one%20LLM%20called%20ChatGPT%203.5%20by%0Acomparing%20its%20generated%20messages%20with%20those%20of%20the%20recent%20US%20presidents.%20To%0Aachieve%20this%20objective%2C%20we%20compare%20the%20State%20of%20the%20Union%20addresses%20written%20by%0AReagan%20to%20Obama%20with%20those%20automatically%20produced%20by%20ChatGPT.%20We%20found%20that%0AChatGPT%20tends%20to%20overuse%20the%20lemma%20%22we%22%20as%20well%20as%20nouns%20and%20commas.%20On%20the%0Aother%20hand%2C%20the%20generated%20speeches%20employ%20less%20verbs%20and%20include%2C%20in%20mean%2C%0Alonger%20sentences.%20Even%20when%20imposing%20a%20given%20style%20to%20ChatGPT%2C%20the%20resulting%0Aspeech%20remains%20distinct%20from%20messages%20written%20by%20the%20target%20author.%20Moreover%2C%0AChatGPT%20opts%20for%20a%20neutral%20tone%20with%20mainly%20positive%20emotional%20expressions%20and%0Asymbolic%20terms%20%28e.g.%2C%20freedom%2C%20nation%29.%20Finally%2C%20we%20show%20that%20the%20GPT%27s%20style%0Aexposes%20distinct%20features%20compared%20to%20real%20presidential%20addresses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520as%2520ghostwriter%2520at%2520the%2520White%2520House%26entry.906535625%3DJacques%2520Savoy%26entry.1292438233%3D%2520%2520Recently%2520several%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%250Acapability%2520to%2520generate%2520a%2520message%2520in%2520response%2520to%2520a%2520user%2520request.%2520Such%2520scientific%250Abreakthroughs%2520promote%2520new%2520perspectives%2520but%2520also%2520some%2520fears.%2520The%2520main%2520focus%2520of%250Athis%2520study%2520is%2520to%2520analyze%2520the%2520written%2520style%2520of%2520one%2520LLM%2520called%2520ChatGPT%25203.5%2520by%250Acomparing%2520its%2520generated%2520messages%2520with%2520those%2520of%2520the%2520recent%2520US%2520presidents.%2520To%250Aachieve%2520this%2520objective%252C%2520we%2520compare%2520the%2520State%2520of%2520the%2520Union%2520addresses%2520written%2520by%250AReagan%2520to%2520Obama%2520with%2520those%2520automatically%2520produced%2520by%2520ChatGPT.%2520We%2520found%2520that%250AChatGPT%2520tends%2520to%2520overuse%2520the%2520lemma%2520%2522we%2522%2520as%2520well%2520as%2520nouns%2520and%2520commas.%2520On%2520the%250Aother%2520hand%252C%2520the%2520generated%2520speeches%2520employ%2520less%2520verbs%2520and%2520include%252C%2520in%2520mean%252C%250Alonger%2520sentences.%2520Even%2520when%2520imposing%2520a%2520given%2520style%2520to%2520ChatGPT%252C%2520the%2520resulting%250Aspeech%2520remains%2520distinct%2520from%2520messages%2520written%2520by%2520the%2520target%2520author.%2520Moreover%252C%250AChatGPT%2520opts%2520for%2520a%2520neutral%2520tone%2520with%2520mainly%2520positive%2520emotional%2520expressions%2520and%250Asymbolic%2520terms%2520%2528e.g.%252C%2520freedom%252C%2520nation%2529.%2520Finally%252C%2520we%2520show%2520that%2520the%2520GPT%2527s%2520style%250Aexposes%2520distinct%2520features%2520compared%2520to%2520real%2520presidential%2520addresses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20as%20ghostwriter%20at%20the%20White%20House&entry.906535625=Jacques%20Savoy&entry.1292438233=%20%20Recently%20several%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20their%0Acapability%20to%20generate%20a%20message%20in%20response%20to%20a%20user%20request.%20Such%20scientific%0Abreakthroughs%20promote%20new%20perspectives%20but%20also%20some%20fears.%20The%20main%20focus%20of%0Athis%20study%20is%20to%20analyze%20the%20written%20style%20of%20one%20LLM%20called%20ChatGPT%203.5%20by%0Acomparing%20its%20generated%20messages%20with%20those%20of%20the%20recent%20US%20presidents.%20To%0Aachieve%20this%20objective%2C%20we%20compare%20the%20State%20of%20the%20Union%20addresses%20written%20by%0AReagan%20to%20Obama%20with%20those%20automatically%20produced%20by%20ChatGPT.%20We%20found%20that%0AChatGPT%20tends%20to%20overuse%20the%20lemma%20%22we%22%20as%20well%20as%20nouns%20and%20commas.%20On%20the%0Aother%20hand%2C%20the%20generated%20speeches%20employ%20less%20verbs%20and%20include%2C%20in%20mean%2C%0Alonger%20sentences.%20Even%20when%20imposing%20a%20given%20style%20to%20ChatGPT%2C%20the%20resulting%0Aspeech%20remains%20distinct%20from%20messages%20written%20by%20the%20target%20author.%20Moreover%2C%0AChatGPT%20opts%20for%20a%20neutral%20tone%20with%20mainly%20positive%20emotional%20expressions%20and%0Asymbolic%20terms%20%28e.g.%2C%20freedom%2C%20nation%29.%20Finally%2C%20we%20show%20that%20the%20GPT%27s%20style%0Aexposes%20distinct%20features%20compared%20to%20real%20presidential%20addresses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18365v1&entry.124074799=Read"},
{"title": "A comparison of extended object tracking with multi-modal sensors in\n  indoor environment", "author": "Jiangtao Shuai and Martin Baerveldt and Manh Nguyen-Duc and Anh Le-Tuan and Manfred Hauswirth and Danh Le-Phuoc", "abstract": "  This paper presents a preliminary study of an efficient object tracking\napproach, comparing the performance of two different 3D point cloud sensory\nsources: LiDAR and stereo cameras, which have significant price differences. In\nthis preliminary work, we focus on single object tracking. We first developed a\nfast heuristic object detector that utilizes prior information about the\nenvironment and target. The resulting target points are subsequently fed into\nan extended object tracking framework, where the target shape is parameterized\nusing a star-convex hypersurface model. Experimental results show that our\nobject tracking method using a stereo camera achieves performance similar to\nthat of a LiDAR sensor, with a cost difference of more than tenfold.\n", "link": "http://arxiv.org/abs/2411.18476v1", "date": "2024-11-27", "relevancy": 2.1691, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5532}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comparison%20of%20extended%20object%20tracking%20with%20multi-modal%20sensors%20in%0A%20%20indoor%20environment&body=Title%3A%20A%20comparison%20of%20extended%20object%20tracking%20with%20multi-modal%20sensors%20in%0A%20%20indoor%20environment%0AAuthor%3A%20Jiangtao%20Shuai%20and%20Martin%20Baerveldt%20and%20Manh%20Nguyen-Duc%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20preliminary%20study%20of%20an%20efficient%20object%20tracking%0Aapproach%2C%20comparing%20the%20performance%20of%20two%20different%203D%20point%20cloud%20sensory%0Asources%3A%20LiDAR%20and%20stereo%20cameras%2C%20which%20have%20significant%20price%20differences.%20In%0Athis%20preliminary%20work%2C%20we%20focus%20on%20single%20object%20tracking.%20We%20first%20developed%20a%0Afast%20heuristic%20object%20detector%20that%20utilizes%20prior%20information%20about%20the%0Aenvironment%20and%20target.%20The%20resulting%20target%20points%20are%20subsequently%20fed%20into%0Aan%20extended%20object%20tracking%20framework%2C%20where%20the%20target%20shape%20is%20parameterized%0Ausing%20a%20star-convex%20hypersurface%20model.%20Experimental%20results%20show%20that%20our%0Aobject%20tracking%20method%20using%20a%20stereo%20camera%20achieves%20performance%20similar%20to%0Athat%20of%20a%20LiDAR%20sensor%2C%20with%20a%20cost%20difference%20of%20more%20than%20tenfold.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comparison%2520of%2520extended%2520object%2520tracking%2520with%2520multi-modal%2520sensors%2520in%250A%2520%2520indoor%2520environment%26entry.906535625%3DJiangtao%2520Shuai%2520and%2520Martin%2520Baerveldt%2520and%2520Manh%2520Nguyen-Duc%2520and%2520Anh%2520Le-Tuan%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le-Phuoc%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520preliminary%2520study%2520of%2520an%2520efficient%2520object%2520tracking%250Aapproach%252C%2520comparing%2520the%2520performance%2520of%2520two%2520different%25203D%2520point%2520cloud%2520sensory%250Asources%253A%2520LiDAR%2520and%2520stereo%2520cameras%252C%2520which%2520have%2520significant%2520price%2520differences.%2520In%250Athis%2520preliminary%2520work%252C%2520we%2520focus%2520on%2520single%2520object%2520tracking.%2520We%2520first%2520developed%2520a%250Afast%2520heuristic%2520object%2520detector%2520that%2520utilizes%2520prior%2520information%2520about%2520the%250Aenvironment%2520and%2520target.%2520The%2520resulting%2520target%2520points%2520are%2520subsequently%2520fed%2520into%250Aan%2520extended%2520object%2520tracking%2520framework%252C%2520where%2520the%2520target%2520shape%2520is%2520parameterized%250Ausing%2520a%2520star-convex%2520hypersurface%2520model.%2520Experimental%2520results%2520show%2520that%2520our%250Aobject%2520tracking%2520method%2520using%2520a%2520stereo%2520camera%2520achieves%2520performance%2520similar%2520to%250Athat%2520of%2520a%2520LiDAR%2520sensor%252C%2520with%2520a%2520cost%2520difference%2520of%2520more%2520than%2520tenfold.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comparison%20of%20extended%20object%20tracking%20with%20multi-modal%20sensors%20in%0A%20%20indoor%20environment&entry.906535625=Jiangtao%20Shuai%20and%20Martin%20Baerveldt%20and%20Manh%20Nguyen-Duc%20and%20Anh%20Le-Tuan%20and%20Manfred%20Hauswirth%20and%20Danh%20Le-Phuoc&entry.1292438233=%20%20This%20paper%20presents%20a%20preliminary%20study%20of%20an%20efficient%20object%20tracking%0Aapproach%2C%20comparing%20the%20performance%20of%20two%20different%203D%20point%20cloud%20sensory%0Asources%3A%20LiDAR%20and%20stereo%20cameras%2C%20which%20have%20significant%20price%20differences.%20In%0Athis%20preliminary%20work%2C%20we%20focus%20on%20single%20object%20tracking.%20We%20first%20developed%20a%0Afast%20heuristic%20object%20detector%20that%20utilizes%20prior%20information%20about%20the%0Aenvironment%20and%20target.%20The%20resulting%20target%20points%20are%20subsequently%20fed%20into%0Aan%20extended%20object%20tracking%20framework%2C%20where%20the%20target%20shape%20is%20parameterized%0Ausing%20a%20star-convex%20hypersurface%20model.%20Experimental%20results%20show%20that%20our%0Aobject%20tracking%20method%20using%20a%20stereo%20camera%20achieves%20performance%20similar%20to%0Athat%20of%20a%20LiDAR%20sensor%2C%20with%20a%20cost%20difference%20of%20more%20than%20tenfold.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18476v1&entry.124074799=Read"},
{"title": "Towards Motion Compensation in Autonomous Robotic Subretinal Injections", "author": "Demir Arikan and Peiyao Zhang and Michael Sommersperger and Shervin Dehghani and Mojtaba Esfandiari and Russel H. Taylor and M. Ali Nasseri and Peter Gehlbach and Nassir Navab and Iulian Iordachita", "abstract": "  Exudative (wet) age-related macular degeneration (AMD) is a leading cause of\nvision loss in older adults, typically treated with intravitreal injections.\nEmerging therapies, such as subretinal injections of stem cells, gene therapy,\nsmall molecules or RPE cells require precise delivery to avoid damaging\ndelicate retinal structures. Autonomous robotic systems can potentially offer\nthe necessary precision for these procedures. This paper presents a novel\napproach for motion compensation in robotic subretinal injections, utilizing\nreal-time Optical Coherence Tomography (OCT). The proposed method leverages\nB$^{5}$-scans, a rapid acquisition of small-volume OCT data, for dynamic\ntracking of retinal motion along the Z-axis, compensating for physiological\nmovements such as breathing and heartbeat. Validation experiments on \\textit{ex\nvivo} porcine eyes revealed challenges in maintaining a consistent\ntool-to-retina distance, with deviations of up to 200 $\\mu m$ for 100 $\\mu m$\namplitude motions and over 80 $\\mu m$ for 25 $\\mu m$ amplitude motions over one\nminute. Subretinal injections faced additional difficulties, with horizontal\nshifts causing the needle to move off-target and inject into the vitreous.\nThese results highlight the need for improved motion prediction and horizontal\nstability to enhance the accuracy and safety of robotic subretinal procedures.\n", "link": "http://arxiv.org/abs/2411.18521v1", "date": "2024-11-27", "relevancy": 2.1685, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5448}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Motion%20Compensation%20in%20Autonomous%20Robotic%20Subretinal%20Injections&body=Title%3A%20Towards%20Motion%20Compensation%20in%20Autonomous%20Robotic%20Subretinal%20Injections%0AAuthor%3A%20Demir%20Arikan%20and%20Peiyao%20Zhang%20and%20Michael%20Sommersperger%20and%20Shervin%20Dehghani%20and%20Mojtaba%20Esfandiari%20and%20Russel%20H.%20Taylor%20and%20M.%20Ali%20Nasseri%20and%20Peter%20Gehlbach%20and%20Nassir%20Navab%20and%20Iulian%20Iordachita%0AAbstract%3A%20%20%20Exudative%20%28wet%29%20age-related%20macular%20degeneration%20%28AMD%29%20is%20a%20leading%20cause%20of%0Avision%20loss%20in%20older%20adults%2C%20typically%20treated%20with%20intravitreal%20injections.%0AEmerging%20therapies%2C%20such%20as%20subretinal%20injections%20of%20stem%20cells%2C%20gene%20therapy%2C%0Asmall%20molecules%20or%20RPE%20cells%20require%20precise%20delivery%20to%20avoid%20damaging%0Adelicate%20retinal%20structures.%20Autonomous%20robotic%20systems%20can%20potentially%20offer%0Athe%20necessary%20precision%20for%20these%20procedures.%20This%20paper%20presents%20a%20novel%0Aapproach%20for%20motion%20compensation%20in%20robotic%20subretinal%20injections%2C%20utilizing%0Areal-time%20Optical%20Coherence%20Tomography%20%28OCT%29.%20The%20proposed%20method%20leverages%0AB%24%5E%7B5%7D%24-scans%2C%20a%20rapid%20acquisition%20of%20small-volume%20OCT%20data%2C%20for%20dynamic%0Atracking%20of%20retinal%20motion%20along%20the%20Z-axis%2C%20compensating%20for%20physiological%0Amovements%20such%20as%20breathing%20and%20heartbeat.%20Validation%20experiments%20on%20%5Ctextit%7Bex%0Avivo%7D%20porcine%20eyes%20revealed%20challenges%20in%20maintaining%20a%20consistent%0Atool-to-retina%20distance%2C%20with%20deviations%20of%20up%20to%20200%20%24%5Cmu%20m%24%20for%20100%20%24%5Cmu%20m%24%0Aamplitude%20motions%20and%20over%2080%20%24%5Cmu%20m%24%20for%2025%20%24%5Cmu%20m%24%20amplitude%20motions%20over%20one%0Aminute.%20Subretinal%20injections%20faced%20additional%20difficulties%2C%20with%20horizontal%0Ashifts%20causing%20the%20needle%20to%20move%20off-target%20and%20inject%20into%20the%20vitreous.%0AThese%20results%20highlight%20the%20need%20for%20improved%20motion%20prediction%20and%20horizontal%0Astability%20to%20enhance%20the%20accuracy%20and%20safety%20of%20robotic%20subretinal%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Motion%2520Compensation%2520in%2520Autonomous%2520Robotic%2520Subretinal%2520Injections%26entry.906535625%3DDemir%2520Arikan%2520and%2520Peiyao%2520Zhang%2520and%2520Michael%2520Sommersperger%2520and%2520Shervin%2520Dehghani%2520and%2520Mojtaba%2520Esfandiari%2520and%2520Russel%2520H.%2520Taylor%2520and%2520M.%2520Ali%2520Nasseri%2520and%2520Peter%2520Gehlbach%2520and%2520Nassir%2520Navab%2520and%2520Iulian%2520Iordachita%26entry.1292438233%3D%2520%2520Exudative%2520%2528wet%2529%2520age-related%2520macular%2520degeneration%2520%2528AMD%2529%2520is%2520a%2520leading%2520cause%2520of%250Avision%2520loss%2520in%2520older%2520adults%252C%2520typically%2520treated%2520with%2520intravitreal%2520injections.%250AEmerging%2520therapies%252C%2520such%2520as%2520subretinal%2520injections%2520of%2520stem%2520cells%252C%2520gene%2520therapy%252C%250Asmall%2520molecules%2520or%2520RPE%2520cells%2520require%2520precise%2520delivery%2520to%2520avoid%2520damaging%250Adelicate%2520retinal%2520structures.%2520Autonomous%2520robotic%2520systems%2520can%2520potentially%2520offer%250Athe%2520necessary%2520precision%2520for%2520these%2520procedures.%2520This%2520paper%2520presents%2520a%2520novel%250Aapproach%2520for%2520motion%2520compensation%2520in%2520robotic%2520subretinal%2520injections%252C%2520utilizing%250Areal-time%2520Optical%2520Coherence%2520Tomography%2520%2528OCT%2529.%2520The%2520proposed%2520method%2520leverages%250AB%2524%255E%257B5%257D%2524-scans%252C%2520a%2520rapid%2520acquisition%2520of%2520small-volume%2520OCT%2520data%252C%2520for%2520dynamic%250Atracking%2520of%2520retinal%2520motion%2520along%2520the%2520Z-axis%252C%2520compensating%2520for%2520physiological%250Amovements%2520such%2520as%2520breathing%2520and%2520heartbeat.%2520Validation%2520experiments%2520on%2520%255Ctextit%257Bex%250Avivo%257D%2520porcine%2520eyes%2520revealed%2520challenges%2520in%2520maintaining%2520a%2520consistent%250Atool-to-retina%2520distance%252C%2520with%2520deviations%2520of%2520up%2520to%2520200%2520%2524%255Cmu%2520m%2524%2520for%2520100%2520%2524%255Cmu%2520m%2524%250Aamplitude%2520motions%2520and%2520over%252080%2520%2524%255Cmu%2520m%2524%2520for%252025%2520%2524%255Cmu%2520m%2524%2520amplitude%2520motions%2520over%2520one%250Aminute.%2520Subretinal%2520injections%2520faced%2520additional%2520difficulties%252C%2520with%2520horizontal%250Ashifts%2520causing%2520the%2520needle%2520to%2520move%2520off-target%2520and%2520inject%2520into%2520the%2520vitreous.%250AThese%2520results%2520highlight%2520the%2520need%2520for%2520improved%2520motion%2520prediction%2520and%2520horizontal%250Astability%2520to%2520enhance%2520the%2520accuracy%2520and%2520safety%2520of%2520robotic%2520subretinal%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Motion%20Compensation%20in%20Autonomous%20Robotic%20Subretinal%20Injections&entry.906535625=Demir%20Arikan%20and%20Peiyao%20Zhang%20and%20Michael%20Sommersperger%20and%20Shervin%20Dehghani%20and%20Mojtaba%20Esfandiari%20and%20Russel%20H.%20Taylor%20and%20M.%20Ali%20Nasseri%20and%20Peter%20Gehlbach%20and%20Nassir%20Navab%20and%20Iulian%20Iordachita&entry.1292438233=%20%20Exudative%20%28wet%29%20age-related%20macular%20degeneration%20%28AMD%29%20is%20a%20leading%20cause%20of%0Avision%20loss%20in%20older%20adults%2C%20typically%20treated%20with%20intravitreal%20injections.%0AEmerging%20therapies%2C%20such%20as%20subretinal%20injections%20of%20stem%20cells%2C%20gene%20therapy%2C%0Asmall%20molecules%20or%20RPE%20cells%20require%20precise%20delivery%20to%20avoid%20damaging%0Adelicate%20retinal%20structures.%20Autonomous%20robotic%20systems%20can%20potentially%20offer%0Athe%20necessary%20precision%20for%20these%20procedures.%20This%20paper%20presents%20a%20novel%0Aapproach%20for%20motion%20compensation%20in%20robotic%20subretinal%20injections%2C%20utilizing%0Areal-time%20Optical%20Coherence%20Tomography%20%28OCT%29.%20The%20proposed%20method%20leverages%0AB%24%5E%7B5%7D%24-scans%2C%20a%20rapid%20acquisition%20of%20small-volume%20OCT%20data%2C%20for%20dynamic%0Atracking%20of%20retinal%20motion%20along%20the%20Z-axis%2C%20compensating%20for%20physiological%0Amovements%20such%20as%20breathing%20and%20heartbeat.%20Validation%20experiments%20on%20%5Ctextit%7Bex%0Avivo%7D%20porcine%20eyes%20revealed%20challenges%20in%20maintaining%20a%20consistent%0Atool-to-retina%20distance%2C%20with%20deviations%20of%20up%20to%20200%20%24%5Cmu%20m%24%20for%20100%20%24%5Cmu%20m%24%0Aamplitude%20motions%20and%20over%2080%20%24%5Cmu%20m%24%20for%2025%20%24%5Cmu%20m%24%20amplitude%20motions%20over%20one%0Aminute.%20Subretinal%20injections%20faced%20additional%20difficulties%2C%20with%20horizontal%0Ashifts%20causing%20the%20needle%20to%20move%20off-target%20and%20inject%20into%20the%20vitreous.%0AThese%20results%20highlight%20the%20need%20for%20improved%20motion%20prediction%20and%20horizontal%0Astability%20to%20enhance%20the%20accuracy%20and%20safety%20of%20robotic%20subretinal%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18521v1&entry.124074799=Read"},
{"title": "Robust Dynamic Gesture Recognition at Ultra-Long Distances", "author": "Eran Bamani Beeri and Eden Nissinman and Avishai Sintov", "abstract": "  Dynamic hand gestures play a crucial role in conveying nonverbal information\nfor Human-Robot Interaction (HRI), eliminating the need for complex interfaces.\nCurrent models for dynamic gesture recognition suffer from limitations in\neffective recognition range, restricting their application to close proximity\nscenarios. In this letter, we present a novel approach to recognizing dynamic\ngestures in an ultra-range distance of up to 28 meters, enabling natural,\ndirective communication for guiding robots in both indoor and outdoor\nenvironments. Our proposed SlowFast-Transformer (SFT) model effectively\nintegrates the SlowFast architecture with Transformer layers to efficiently\nprocess and classify gesture sequences captured at ultra-range distances,\novercoming challenges of low resolution and environmental noise. We further\nintroduce a distance-weighted loss function shown to enhance learning and\nimprove model robustness at varying distances. Our model demonstrates\nsignificant performance improvement over state-of-the-art gesture recognition\nframeworks, achieving a recognition accuracy of 95.1% on a diverse dataset with\nchallenging ultra-range gestures. This enables robots to react appropriately to\nhuman commands from a far distance, providing an essential enhancement in HRI,\nespecially in scenarios requiring seamless and natural interaction.\n", "link": "http://arxiv.org/abs/2411.18413v1", "date": "2024-11-27", "relevancy": 2.1659, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5272}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Dynamic%20Gesture%20Recognition%20at%20Ultra-Long%20Distances&body=Title%3A%20Robust%20Dynamic%20Gesture%20Recognition%20at%20Ultra-Long%20Distances%0AAuthor%3A%20Eran%20Bamani%20Beeri%20and%20Eden%20Nissinman%20and%20Avishai%20Sintov%0AAbstract%3A%20%20%20Dynamic%20hand%20gestures%20play%20a%20crucial%20role%20in%20conveying%20nonverbal%20information%0Afor%20Human-Robot%20Interaction%20%28HRI%29%2C%20eliminating%20the%20need%20for%20complex%20interfaces.%0ACurrent%20models%20for%20dynamic%20gesture%20recognition%20suffer%20from%20limitations%20in%0Aeffective%20recognition%20range%2C%20restricting%20their%20application%20to%20close%20proximity%0Ascenarios.%20In%20this%20letter%2C%20we%20present%20a%20novel%20approach%20to%20recognizing%20dynamic%0Agestures%20in%20an%20ultra-range%20distance%20of%20up%20to%2028%20meters%2C%20enabling%20natural%2C%0Adirective%20communication%20for%20guiding%20robots%20in%20both%20indoor%20and%20outdoor%0Aenvironments.%20Our%20proposed%20SlowFast-Transformer%20%28SFT%29%20model%20effectively%0Aintegrates%20the%20SlowFast%20architecture%20with%20Transformer%20layers%20to%20efficiently%0Aprocess%20and%20classify%20gesture%20sequences%20captured%20at%20ultra-range%20distances%2C%0Aovercoming%20challenges%20of%20low%20resolution%20and%20environmental%20noise.%20We%20further%0Aintroduce%20a%20distance-weighted%20loss%20function%20shown%20to%20enhance%20learning%20and%0Aimprove%20model%20robustness%20at%20varying%20distances.%20Our%20model%20demonstrates%0Asignificant%20performance%20improvement%20over%20state-of-the-art%20gesture%20recognition%0Aframeworks%2C%20achieving%20a%20recognition%20accuracy%20of%2095.1%25%20on%20a%20diverse%20dataset%20with%0Achallenging%20ultra-range%20gestures.%20This%20enables%20robots%20to%20react%20appropriately%20to%0Ahuman%20commands%20from%20a%20far%20distance%2C%20providing%20an%20essential%20enhancement%20in%20HRI%2C%0Aespecially%20in%20scenarios%20requiring%20seamless%20and%20natural%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Dynamic%2520Gesture%2520Recognition%2520at%2520Ultra-Long%2520Distances%26entry.906535625%3DEran%2520Bamani%2520Beeri%2520and%2520Eden%2520Nissinman%2520and%2520Avishai%2520Sintov%26entry.1292438233%3D%2520%2520Dynamic%2520hand%2520gestures%2520play%2520a%2520crucial%2520role%2520in%2520conveying%2520nonverbal%2520information%250Afor%2520Human-Robot%2520Interaction%2520%2528HRI%2529%252C%2520eliminating%2520the%2520need%2520for%2520complex%2520interfaces.%250ACurrent%2520models%2520for%2520dynamic%2520gesture%2520recognition%2520suffer%2520from%2520limitations%2520in%250Aeffective%2520recognition%2520range%252C%2520restricting%2520their%2520application%2520to%2520close%2520proximity%250Ascenarios.%2520In%2520this%2520letter%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%2520recognizing%2520dynamic%250Agestures%2520in%2520an%2520ultra-range%2520distance%2520of%2520up%2520to%252028%2520meters%252C%2520enabling%2520natural%252C%250Adirective%2520communication%2520for%2520guiding%2520robots%2520in%2520both%2520indoor%2520and%2520outdoor%250Aenvironments.%2520Our%2520proposed%2520SlowFast-Transformer%2520%2528SFT%2529%2520model%2520effectively%250Aintegrates%2520the%2520SlowFast%2520architecture%2520with%2520Transformer%2520layers%2520to%2520efficiently%250Aprocess%2520and%2520classify%2520gesture%2520sequences%2520captured%2520at%2520ultra-range%2520distances%252C%250Aovercoming%2520challenges%2520of%2520low%2520resolution%2520and%2520environmental%2520noise.%2520We%2520further%250Aintroduce%2520a%2520distance-weighted%2520loss%2520function%2520shown%2520to%2520enhance%2520learning%2520and%250Aimprove%2520model%2520robustness%2520at%2520varying%2520distances.%2520Our%2520model%2520demonstrates%250Asignificant%2520performance%2520improvement%2520over%2520state-of-the-art%2520gesture%2520recognition%250Aframeworks%252C%2520achieving%2520a%2520recognition%2520accuracy%2520of%252095.1%2525%2520on%2520a%2520diverse%2520dataset%2520with%250Achallenging%2520ultra-range%2520gestures.%2520This%2520enables%2520robots%2520to%2520react%2520appropriately%2520to%250Ahuman%2520commands%2520from%2520a%2520far%2520distance%252C%2520providing%2520an%2520essential%2520enhancement%2520in%2520HRI%252C%250Aespecially%2520in%2520scenarios%2520requiring%2520seamless%2520and%2520natural%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Dynamic%20Gesture%20Recognition%20at%20Ultra-Long%20Distances&entry.906535625=Eran%20Bamani%20Beeri%20and%20Eden%20Nissinman%20and%20Avishai%20Sintov&entry.1292438233=%20%20Dynamic%20hand%20gestures%20play%20a%20crucial%20role%20in%20conveying%20nonverbal%20information%0Afor%20Human-Robot%20Interaction%20%28HRI%29%2C%20eliminating%20the%20need%20for%20complex%20interfaces.%0ACurrent%20models%20for%20dynamic%20gesture%20recognition%20suffer%20from%20limitations%20in%0Aeffective%20recognition%20range%2C%20restricting%20their%20application%20to%20close%20proximity%0Ascenarios.%20In%20this%20letter%2C%20we%20present%20a%20novel%20approach%20to%20recognizing%20dynamic%0Agestures%20in%20an%20ultra-range%20distance%20of%20up%20to%2028%20meters%2C%20enabling%20natural%2C%0Adirective%20communication%20for%20guiding%20robots%20in%20both%20indoor%20and%20outdoor%0Aenvironments.%20Our%20proposed%20SlowFast-Transformer%20%28SFT%29%20model%20effectively%0Aintegrates%20the%20SlowFast%20architecture%20with%20Transformer%20layers%20to%20efficiently%0Aprocess%20and%20classify%20gesture%20sequences%20captured%20at%20ultra-range%20distances%2C%0Aovercoming%20challenges%20of%20low%20resolution%20and%20environmental%20noise.%20We%20further%0Aintroduce%20a%20distance-weighted%20loss%20function%20shown%20to%20enhance%20learning%20and%0Aimprove%20model%20robustness%20at%20varying%20distances.%20Our%20model%20demonstrates%0Asignificant%20performance%20improvement%20over%20state-of-the-art%20gesture%20recognition%0Aframeworks%2C%20achieving%20a%20recognition%20accuracy%20of%2095.1%25%20on%20a%20diverse%20dataset%20with%0Achallenging%20ultra-range%20gestures.%20This%20enables%20robots%20to%20react%20appropriately%20to%0Ahuman%20commands%20from%20a%20far%20distance%2C%20providing%20an%20essential%20enhancement%20in%20HRI%2C%0Aespecially%20in%20scenarios%20requiring%20seamless%20and%20natural%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18413v1&entry.124074799=Read"},
{"title": "DistinctAD: Distinctive Audio Description Generation in Contexts", "author": "Bo Fang and Wenhao Wu and Qiangqiang Wu and Yuxin Song and Antoni B. Chan", "abstract": "  Audio Descriptions (ADs) aim to provide a narration of a movie in text form,\ndescribing non-dialogue-related narratives, such as characters, actions, or\nscene establishment. Automatic generation of ADs remains challenging due to: i)\nthe domain gap between movie-AD data and existing data used to train\nvision-language models, and ii) the issue of contextual redundancy arising from\nhighly similar neighboring visual clips in a long movie. In this work, we\npropose DistinctAD, a novel two-stage framework for generating ADs that\nemphasize distinctiveness to produce better narratives. To address the domain\ngap, we introduce a CLIP-AD adaptation strategy that does not require\nadditional AD corpora, enabling more effective alignment between movie and AD\nmodalities at both global and fine-grained levels. In Stage-II, DistinctAD\nincorporates two key innovations: (i) a Contextual Expectation-Maximization\nAttention (EMA) module that reduces redundancy by extracting common bases from\nconsecutive video clips, and (ii) an explicit distinctive word prediction loss\nthat filters out repeated words in the context, ensuring the prediction of\nunique terms specific to the current AD. Comprehensive evaluations on MAD-Eval,\nCMD-AD, and TV-AD benchmarks demonstrate the superiority of DistinctAD, with\nthe model consistently outperforming baselines, particularly in Recall@k/N,\nhighlighting its effectiveness in producing high-quality, distinctive ADs.\n", "link": "http://arxiv.org/abs/2411.18180v1", "date": "2024-11-27", "relevancy": 2.1622, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.551}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistinctAD%3A%20Distinctive%20Audio%20Description%20Generation%20in%20Contexts&body=Title%3A%20DistinctAD%3A%20Distinctive%20Audio%20Description%20Generation%20in%20Contexts%0AAuthor%3A%20Bo%20Fang%20and%20Wenhao%20Wu%20and%20Qiangqiang%20Wu%20and%20Yuxin%20Song%20and%20Antoni%20B.%20Chan%0AAbstract%3A%20%20%20Audio%20Descriptions%20%28ADs%29%20aim%20to%20provide%20a%20narration%20of%20a%20movie%20in%20text%20form%2C%0Adescribing%20non-dialogue-related%20narratives%2C%20such%20as%20characters%2C%20actions%2C%20or%0Ascene%20establishment.%20Automatic%20generation%20of%20ADs%20remains%20challenging%20due%20to%3A%20i%29%0Athe%20domain%20gap%20between%20movie-AD%20data%20and%20existing%20data%20used%20to%20train%0Avision-language%20models%2C%20and%20ii%29%20the%20issue%20of%20contextual%20redundancy%20arising%20from%0Ahighly%20similar%20neighboring%20visual%20clips%20in%20a%20long%20movie.%20In%20this%20work%2C%20we%0Apropose%20DistinctAD%2C%20a%20novel%20two-stage%20framework%20for%20generating%20ADs%20that%0Aemphasize%20distinctiveness%20to%20produce%20better%20narratives.%20To%20address%20the%20domain%0Agap%2C%20we%20introduce%20a%20CLIP-AD%20adaptation%20strategy%20that%20does%20not%20require%0Aadditional%20AD%20corpora%2C%20enabling%20more%20effective%20alignment%20between%20movie%20and%20AD%0Amodalities%20at%20both%20global%20and%20fine-grained%20levels.%20In%20Stage-II%2C%20DistinctAD%0Aincorporates%20two%20key%20innovations%3A%20%28i%29%20a%20Contextual%20Expectation-Maximization%0AAttention%20%28EMA%29%20module%20that%20reduces%20redundancy%20by%20extracting%20common%20bases%20from%0Aconsecutive%20video%20clips%2C%20and%20%28ii%29%20an%20explicit%20distinctive%20word%20prediction%20loss%0Athat%20filters%20out%20repeated%20words%20in%20the%20context%2C%20ensuring%20the%20prediction%20of%0Aunique%20terms%20specific%20to%20the%20current%20AD.%20Comprehensive%20evaluations%20on%20MAD-Eval%2C%0ACMD-AD%2C%20and%20TV-AD%20benchmarks%20demonstrate%20the%20superiority%20of%20DistinctAD%2C%20with%0Athe%20model%20consistently%20outperforming%20baselines%2C%20particularly%20in%20Recall%40k/N%2C%0Ahighlighting%20its%20effectiveness%20in%20producing%20high-quality%2C%20distinctive%20ADs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistinctAD%253A%2520Distinctive%2520Audio%2520Description%2520Generation%2520in%2520Contexts%26entry.906535625%3DBo%2520Fang%2520and%2520Wenhao%2520Wu%2520and%2520Qiangqiang%2520Wu%2520and%2520Yuxin%2520Song%2520and%2520Antoni%2520B.%2520Chan%26entry.1292438233%3D%2520%2520Audio%2520Descriptions%2520%2528ADs%2529%2520aim%2520to%2520provide%2520a%2520narration%2520of%2520a%2520movie%2520in%2520text%2520form%252C%250Adescribing%2520non-dialogue-related%2520narratives%252C%2520such%2520as%2520characters%252C%2520actions%252C%2520or%250Ascene%2520establishment.%2520Automatic%2520generation%2520of%2520ADs%2520remains%2520challenging%2520due%2520to%253A%2520i%2529%250Athe%2520domain%2520gap%2520between%2520movie-AD%2520data%2520and%2520existing%2520data%2520used%2520to%2520train%250Avision-language%2520models%252C%2520and%2520ii%2529%2520the%2520issue%2520of%2520contextual%2520redundancy%2520arising%2520from%250Ahighly%2520similar%2520neighboring%2520visual%2520clips%2520in%2520a%2520long%2520movie.%2520In%2520this%2520work%252C%2520we%250Apropose%2520DistinctAD%252C%2520a%2520novel%2520two-stage%2520framework%2520for%2520generating%2520ADs%2520that%250Aemphasize%2520distinctiveness%2520to%2520produce%2520better%2520narratives.%2520To%2520address%2520the%2520domain%250Agap%252C%2520we%2520introduce%2520a%2520CLIP-AD%2520adaptation%2520strategy%2520that%2520does%2520not%2520require%250Aadditional%2520AD%2520corpora%252C%2520enabling%2520more%2520effective%2520alignment%2520between%2520movie%2520and%2520AD%250Amodalities%2520at%2520both%2520global%2520and%2520fine-grained%2520levels.%2520In%2520Stage-II%252C%2520DistinctAD%250Aincorporates%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520a%2520Contextual%2520Expectation-Maximization%250AAttention%2520%2528EMA%2529%2520module%2520that%2520reduces%2520redundancy%2520by%2520extracting%2520common%2520bases%2520from%250Aconsecutive%2520video%2520clips%252C%2520and%2520%2528ii%2529%2520an%2520explicit%2520distinctive%2520word%2520prediction%2520loss%250Athat%2520filters%2520out%2520repeated%2520words%2520in%2520the%2520context%252C%2520ensuring%2520the%2520prediction%2520of%250Aunique%2520terms%2520specific%2520to%2520the%2520current%2520AD.%2520Comprehensive%2520evaluations%2520on%2520MAD-Eval%252C%250ACMD-AD%252C%2520and%2520TV-AD%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520DistinctAD%252C%2520with%250Athe%2520model%2520consistently%2520outperforming%2520baselines%252C%2520particularly%2520in%2520Recall%2540k/N%252C%250Ahighlighting%2520its%2520effectiveness%2520in%2520producing%2520high-quality%252C%2520distinctive%2520ADs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistinctAD%3A%20Distinctive%20Audio%20Description%20Generation%20in%20Contexts&entry.906535625=Bo%20Fang%20and%20Wenhao%20Wu%20and%20Qiangqiang%20Wu%20and%20Yuxin%20Song%20and%20Antoni%20B.%20Chan&entry.1292438233=%20%20Audio%20Descriptions%20%28ADs%29%20aim%20to%20provide%20a%20narration%20of%20a%20movie%20in%20text%20form%2C%0Adescribing%20non-dialogue-related%20narratives%2C%20such%20as%20characters%2C%20actions%2C%20or%0Ascene%20establishment.%20Automatic%20generation%20of%20ADs%20remains%20challenging%20due%20to%3A%20i%29%0Athe%20domain%20gap%20between%20movie-AD%20data%20and%20existing%20data%20used%20to%20train%0Avision-language%20models%2C%20and%20ii%29%20the%20issue%20of%20contextual%20redundancy%20arising%20from%0Ahighly%20similar%20neighboring%20visual%20clips%20in%20a%20long%20movie.%20In%20this%20work%2C%20we%0Apropose%20DistinctAD%2C%20a%20novel%20two-stage%20framework%20for%20generating%20ADs%20that%0Aemphasize%20distinctiveness%20to%20produce%20better%20narratives.%20To%20address%20the%20domain%0Agap%2C%20we%20introduce%20a%20CLIP-AD%20adaptation%20strategy%20that%20does%20not%20require%0Aadditional%20AD%20corpora%2C%20enabling%20more%20effective%20alignment%20between%20movie%20and%20AD%0Amodalities%20at%20both%20global%20and%20fine-grained%20levels.%20In%20Stage-II%2C%20DistinctAD%0Aincorporates%20two%20key%20innovations%3A%20%28i%29%20a%20Contextual%20Expectation-Maximization%0AAttention%20%28EMA%29%20module%20that%20reduces%20redundancy%20by%20extracting%20common%20bases%20from%0Aconsecutive%20video%20clips%2C%20and%20%28ii%29%20an%20explicit%20distinctive%20word%20prediction%20loss%0Athat%20filters%20out%20repeated%20words%20in%20the%20context%2C%20ensuring%20the%20prediction%20of%0Aunique%20terms%20specific%20to%20the%20current%20AD.%20Comprehensive%20evaluations%20on%20MAD-Eval%2C%0ACMD-AD%2C%20and%20TV-AD%20benchmarks%20demonstrate%20the%20superiority%20of%20DistinctAD%2C%20with%0Athe%20model%20consistently%20outperforming%20baselines%2C%20particularly%20in%20Recall%40k/N%2C%0Ahighlighting%20its%20effectiveness%20in%20producing%20high-quality%2C%20distinctive%20ADs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18180v1&entry.124074799=Read"},
{"title": "Enhancing weed detection performance by means of GenAI-based image\n  augmentation", "author": "Sourav Modak and Anthony Stein", "abstract": "  Precise weed management is essential for sustaining crop productivity and\necological balance. Traditional herbicide applications face economic and\nenvironmental challenges, emphasizing the need for intelligent weed control\nsystems powered by deep learning. These systems require vast amounts of\nhigh-quality training data. The reality of scarcity of well-annotated training\ndata, however, is often addressed through generating more data using data\naugmentation. Nevertheless, conventional augmentation techniques such as random\nflipping, color changes, and blurring lack sufficient fidelity and diversity.\nThis paper investigates a generative AI-based augmentation technique that uses\nthe Stable Diffusion model to produce diverse synthetic images that improve the\nquantity and quality of training datasets for weed detection models. Moreover,\nthis paper explores the impact of these synthetic images on the performance of\nreal-time detection systems, thus focusing on compact CNN-based models such as\nYOLO nano for edge devices. The experimental results show substantial\nimprovements in mean Average Precision (mAP50 and mAP50-95) scores for YOLO\nmodels trained with generative AI-augmented datasets, demonstrating the\npromising potential of synthetic data to enhance model robustness and accuracy.\n", "link": "http://arxiv.org/abs/2411.18513v1", "date": "2024-11-27", "relevancy": 2.1602, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5609}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5386}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20weed%20detection%20performance%20by%20means%20of%20GenAI-based%20image%0A%20%20augmentation&body=Title%3A%20Enhancing%20weed%20detection%20performance%20by%20means%20of%20GenAI-based%20image%0A%20%20augmentation%0AAuthor%3A%20Sourav%20Modak%20and%20Anthony%20Stein%0AAbstract%3A%20%20%20Precise%20weed%20management%20is%20essential%20for%20sustaining%20crop%20productivity%20and%0Aecological%20balance.%20Traditional%20herbicide%20applications%20face%20economic%20and%0Aenvironmental%20challenges%2C%20emphasizing%20the%20need%20for%20intelligent%20weed%20control%0Asystems%20powered%20by%20deep%20learning.%20These%20systems%20require%20vast%20amounts%20of%0Ahigh-quality%20training%20data.%20The%20reality%20of%20scarcity%20of%20well-annotated%20training%0Adata%2C%20however%2C%20is%20often%20addressed%20through%20generating%20more%20data%20using%20data%0Aaugmentation.%20Nevertheless%2C%20conventional%20augmentation%20techniques%20such%20as%20random%0Aflipping%2C%20color%20changes%2C%20and%20blurring%20lack%20sufficient%20fidelity%20and%20diversity.%0AThis%20paper%20investigates%20a%20generative%20AI-based%20augmentation%20technique%20that%20uses%0Athe%20Stable%20Diffusion%20model%20to%20produce%20diverse%20synthetic%20images%20that%20improve%20the%0Aquantity%20and%20quality%20of%20training%20datasets%20for%20weed%20detection%20models.%20Moreover%2C%0Athis%20paper%20explores%20the%20impact%20of%20these%20synthetic%20images%20on%20the%20performance%20of%0Areal-time%20detection%20systems%2C%20thus%20focusing%20on%20compact%20CNN-based%20models%20such%20as%0AYOLO%20nano%20for%20edge%20devices.%20The%20experimental%20results%20show%20substantial%0Aimprovements%20in%20mean%20Average%20Precision%20%28mAP50%20and%20mAP50-95%29%20scores%20for%20YOLO%0Amodels%20trained%20with%20generative%20AI-augmented%20datasets%2C%20demonstrating%20the%0Apromising%20potential%20of%20synthetic%20data%20to%20enhance%20model%20robustness%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520weed%2520detection%2520performance%2520by%2520means%2520of%2520GenAI-based%2520image%250A%2520%2520augmentation%26entry.906535625%3DSourav%2520Modak%2520and%2520Anthony%2520Stein%26entry.1292438233%3D%2520%2520Precise%2520weed%2520management%2520is%2520essential%2520for%2520sustaining%2520crop%2520productivity%2520and%250Aecological%2520balance.%2520Traditional%2520herbicide%2520applications%2520face%2520economic%2520and%250Aenvironmental%2520challenges%252C%2520emphasizing%2520the%2520need%2520for%2520intelligent%2520weed%2520control%250Asystems%2520powered%2520by%2520deep%2520learning.%2520These%2520systems%2520require%2520vast%2520amounts%2520of%250Ahigh-quality%2520training%2520data.%2520The%2520reality%2520of%2520scarcity%2520of%2520well-annotated%2520training%250Adata%252C%2520however%252C%2520is%2520often%2520addressed%2520through%2520generating%2520more%2520data%2520using%2520data%250Aaugmentation.%2520Nevertheless%252C%2520conventional%2520augmentation%2520techniques%2520such%2520as%2520random%250Aflipping%252C%2520color%2520changes%252C%2520and%2520blurring%2520lack%2520sufficient%2520fidelity%2520and%2520diversity.%250AThis%2520paper%2520investigates%2520a%2520generative%2520AI-based%2520augmentation%2520technique%2520that%2520uses%250Athe%2520Stable%2520Diffusion%2520model%2520to%2520produce%2520diverse%2520synthetic%2520images%2520that%2520improve%2520the%250Aquantity%2520and%2520quality%2520of%2520training%2520datasets%2520for%2520weed%2520detection%2520models.%2520Moreover%252C%250Athis%2520paper%2520explores%2520the%2520impact%2520of%2520these%2520synthetic%2520images%2520on%2520the%2520performance%2520of%250Areal-time%2520detection%2520systems%252C%2520thus%2520focusing%2520on%2520compact%2520CNN-based%2520models%2520such%2520as%250AYOLO%2520nano%2520for%2520edge%2520devices.%2520The%2520experimental%2520results%2520show%2520substantial%250Aimprovements%2520in%2520mean%2520Average%2520Precision%2520%2528mAP50%2520and%2520mAP50-95%2529%2520scores%2520for%2520YOLO%250Amodels%2520trained%2520with%2520generative%2520AI-augmented%2520datasets%252C%2520demonstrating%2520the%250Apromising%2520potential%2520of%2520synthetic%2520data%2520to%2520enhance%2520model%2520robustness%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20weed%20detection%20performance%20by%20means%20of%20GenAI-based%20image%0A%20%20augmentation&entry.906535625=Sourav%20Modak%20and%20Anthony%20Stein&entry.1292438233=%20%20Precise%20weed%20management%20is%20essential%20for%20sustaining%20crop%20productivity%20and%0Aecological%20balance.%20Traditional%20herbicide%20applications%20face%20economic%20and%0Aenvironmental%20challenges%2C%20emphasizing%20the%20need%20for%20intelligent%20weed%20control%0Asystems%20powered%20by%20deep%20learning.%20These%20systems%20require%20vast%20amounts%20of%0Ahigh-quality%20training%20data.%20The%20reality%20of%20scarcity%20of%20well-annotated%20training%0Adata%2C%20however%2C%20is%20often%20addressed%20through%20generating%20more%20data%20using%20data%0Aaugmentation.%20Nevertheless%2C%20conventional%20augmentation%20techniques%20such%20as%20random%0Aflipping%2C%20color%20changes%2C%20and%20blurring%20lack%20sufficient%20fidelity%20and%20diversity.%0AThis%20paper%20investigates%20a%20generative%20AI-based%20augmentation%20technique%20that%20uses%0Athe%20Stable%20Diffusion%20model%20to%20produce%20diverse%20synthetic%20images%20that%20improve%20the%0Aquantity%20and%20quality%20of%20training%20datasets%20for%20weed%20detection%20models.%20Moreover%2C%0Athis%20paper%20explores%20the%20impact%20of%20these%20synthetic%20images%20on%20the%20performance%20of%0Areal-time%20detection%20systems%2C%20thus%20focusing%20on%20compact%20CNN-based%20models%20such%20as%0AYOLO%20nano%20for%20edge%20devices.%20The%20experimental%20results%20show%20substantial%0Aimprovements%20in%20mean%20Average%20Precision%20%28mAP50%20and%20mAP50-95%29%20scores%20for%20YOLO%0Amodels%20trained%20with%20generative%20AI-augmented%20datasets%2C%20demonstrating%20the%0Apromising%20potential%20of%20synthetic%20data%20to%20enhance%20model%20robustness%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18513v1&entry.124074799=Read"},
{"title": "ChatGPT as speechwriter for the French presidents", "author": "Dominique Labb\u00e9 and Cyril Labb\u00e9 and Jacques Savoy", "abstract": "  Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches.\n", "link": "http://arxiv.org/abs/2411.18382v1", "date": "2024-11-27", "relevancy": 2.1586, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4657}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4377}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20as%20speechwriter%20for%20the%20French%20presidents&body=Title%3A%20ChatGPT%20as%20speechwriter%20for%20the%20French%20presidents%0AAuthor%3A%20Dominique%20Labb%C3%A9%20and%20Cyril%20Labb%C3%A9%20and%20Jacques%20Savoy%0AAbstract%3A%20%20%20Generative%20AI%20proposes%20several%20large%20language%20models%20%28LLMs%29%20to%20automatically%0Agenerate%20a%20message%20in%20response%20to%20users%27%20requests.%20Such%20scientific%0Abreakthroughs%20promote%20new%20writing%20assistants%20but%20with%20some%20fears.%20The%20main%0Afocus%20of%20this%20study%20is%20to%20analyze%20the%20written%20style%20of%20one%20LLM%20called%20ChatGPT%0Aby%20comparing%20its%20generated%20messages%20with%20those%20of%20the%20recent%20French%20presidents.%0ATo%20achieve%20this%2C%20we%20compare%20end-of-the-year%20addresses%20written%20by%20Chirac%2C%0ASarkozy%2C%20Hollande%2C%20and%20Macron%20with%20those%20automatically%20produced%20by%20ChatGPT.%20We%0Afound%20that%20ChatGPT%20tends%20to%20overuse%20nouns%2C%20possessive%20determiners%2C%20and%20numbers.%0AOn%20the%20other%20hand%2C%20the%20generated%20speeches%20employ%20less%20verbs%2C%20pronouns%2C%20and%0Aadverbs%20and%20include%2C%20in%20mean%2C%20too%20standardized%20sentences.%20Considering%20some%0Awords%2C%20one%20can%20observe%20that%20ChatGPT%20tends%20to%20overuse%20%22to%20must%22%20%28devoir%29%2C%20%22to%0Acontinue%22%20or%20the%20lemma%20%22we%22%20%28nous%29.%20Moreover%2C%20GPT%20underuses%20the%20auxiliary%20verb%0A%22to%20be%22%20%28%5Eetre%29%2C%20or%20the%20modal%20verbs%20%22to%20will%22%20%28vouloir%29%20or%20%22to%20have%20to%22%0A%28falloir%29.%20In%20addition%2C%20when%20a%20short%20text%20is%20provided%20as%20example%20to%20ChatGPT%2C%0Athe%20machine%20can%20generate%20a%20short%20message%20with%20a%20style%20closed%20to%20the%20original%0Awording.%20Finally%2C%20we%20reveal%20that%20ChatGPT%20style%20exposes%20distinct%20features%0Acompared%20to%20real%20presidential%20speeches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGPT%2520as%2520speechwriter%2520for%2520the%2520French%2520presidents%26entry.906535625%3DDominique%2520Labb%25C3%25A9%2520and%2520Cyril%2520Labb%25C3%25A9%2520and%2520Jacques%2520Savoy%26entry.1292438233%3D%2520%2520Generative%2520AI%2520proposes%2520several%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520automatically%250Agenerate%2520a%2520message%2520in%2520response%2520to%2520users%2527%2520requests.%2520Such%2520scientific%250Abreakthroughs%2520promote%2520new%2520writing%2520assistants%2520but%2520with%2520some%2520fears.%2520The%2520main%250Afocus%2520of%2520this%2520study%2520is%2520to%2520analyze%2520the%2520written%2520style%2520of%2520one%2520LLM%2520called%2520ChatGPT%250Aby%2520comparing%2520its%2520generated%2520messages%2520with%2520those%2520of%2520the%2520recent%2520French%2520presidents.%250ATo%2520achieve%2520this%252C%2520we%2520compare%2520end-of-the-year%2520addresses%2520written%2520by%2520Chirac%252C%250ASarkozy%252C%2520Hollande%252C%2520and%2520Macron%2520with%2520those%2520automatically%2520produced%2520by%2520ChatGPT.%2520We%250Afound%2520that%2520ChatGPT%2520tends%2520to%2520overuse%2520nouns%252C%2520possessive%2520determiners%252C%2520and%2520numbers.%250AOn%2520the%2520other%2520hand%252C%2520the%2520generated%2520speeches%2520employ%2520less%2520verbs%252C%2520pronouns%252C%2520and%250Aadverbs%2520and%2520include%252C%2520in%2520mean%252C%2520too%2520standardized%2520sentences.%2520Considering%2520some%250Awords%252C%2520one%2520can%2520observe%2520that%2520ChatGPT%2520tends%2520to%2520overuse%2520%2522to%2520must%2522%2520%2528devoir%2529%252C%2520%2522to%250Acontinue%2522%2520or%2520the%2520lemma%2520%2522we%2522%2520%2528nous%2529.%2520Moreover%252C%2520GPT%2520underuses%2520the%2520auxiliary%2520verb%250A%2522to%2520be%2522%2520%2528%255Eetre%2529%252C%2520or%2520the%2520modal%2520verbs%2520%2522to%2520will%2522%2520%2528vouloir%2529%2520or%2520%2522to%2520have%2520to%2522%250A%2528falloir%2529.%2520In%2520addition%252C%2520when%2520a%2520short%2520text%2520is%2520provided%2520as%2520example%2520to%2520ChatGPT%252C%250Athe%2520machine%2520can%2520generate%2520a%2520short%2520message%2520with%2520a%2520style%2520closed%2520to%2520the%2520original%250Awording.%2520Finally%252C%2520we%2520reveal%2520that%2520ChatGPT%2520style%2520exposes%2520distinct%2520features%250Acompared%2520to%2520real%2520presidential%2520speeches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20as%20speechwriter%20for%20the%20French%20presidents&entry.906535625=Dominique%20Labb%C3%A9%20and%20Cyril%20Labb%C3%A9%20and%20Jacques%20Savoy&entry.1292438233=%20%20Generative%20AI%20proposes%20several%20large%20language%20models%20%28LLMs%29%20to%20automatically%0Agenerate%20a%20message%20in%20response%20to%20users%27%20requests.%20Such%20scientific%0Abreakthroughs%20promote%20new%20writing%20assistants%20but%20with%20some%20fears.%20The%20main%0Afocus%20of%20this%20study%20is%20to%20analyze%20the%20written%20style%20of%20one%20LLM%20called%20ChatGPT%0Aby%20comparing%20its%20generated%20messages%20with%20those%20of%20the%20recent%20French%20presidents.%0ATo%20achieve%20this%2C%20we%20compare%20end-of-the-year%20addresses%20written%20by%20Chirac%2C%0ASarkozy%2C%20Hollande%2C%20and%20Macron%20with%20those%20automatically%20produced%20by%20ChatGPT.%20We%0Afound%20that%20ChatGPT%20tends%20to%20overuse%20nouns%2C%20possessive%20determiners%2C%20and%20numbers.%0AOn%20the%20other%20hand%2C%20the%20generated%20speeches%20employ%20less%20verbs%2C%20pronouns%2C%20and%0Aadverbs%20and%20include%2C%20in%20mean%2C%20too%20standardized%20sentences.%20Considering%20some%0Awords%2C%20one%20can%20observe%20that%20ChatGPT%20tends%20to%20overuse%20%22to%20must%22%20%28devoir%29%2C%20%22to%0Acontinue%22%20or%20the%20lemma%20%22we%22%20%28nous%29.%20Moreover%2C%20GPT%20underuses%20the%20auxiliary%20verb%0A%22to%20be%22%20%28%5Eetre%29%2C%20or%20the%20modal%20verbs%20%22to%20will%22%20%28vouloir%29%20or%20%22to%20have%20to%22%0A%28falloir%29.%20In%20addition%2C%20when%20a%20short%20text%20is%20provided%20as%20example%20to%20ChatGPT%2C%0Athe%20machine%20can%20generate%20a%20short%20message%20with%20a%20style%20closed%20to%20the%20original%0Awording.%20Finally%2C%20we%20reveal%20that%20ChatGPT%20style%20exposes%20distinct%20features%0Acompared%20to%20real%20presidential%20speeches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18382v1&entry.124074799=Read"},
{"title": "An End-to-End Smart Predict-then-Optimize Framework for Vehicle\n  Relocation Problems in Large-Scale Vehicle Crowd Sensing", "author": "Xinyu Wang and Yiyang Peng and Wei Ma", "abstract": "  Ubiquitous mobile devices have catalyzed the development of vehicle crowd\nsensing (VCS). In particular, vehicle sensing systems show great potential in\nthe flexible acquisition of spatio-temporal urban data through built-in sensors\nunder diverse sensing scenarios. However, vehicle systems often exhibit biased\ncoverage due to the heterogeneous nature of trip requests and routes. To\nachieve a high sensing coverage, a critical challenge lies in optimally\nrelocating vehicles to minimize the divergence between vehicle distributions\nand target sensing distributions. Conventional approaches typically employ a\ntwo-stage predict-then-optimize (PTO) process: first predicting real-time\nvehicle distributions and subsequently generating an optimal relocation\nstrategy based on the predictions. However, this approach can lead to\nsuboptimal decision-making due to the propagation of errors from upstream\nprediction. To this end, we develop an end-to-end Smart Predict-then-Optimize\n(SPO) framework by integrating optimization into prediction within the deep\nlearning architecture, and the entire framework is trained by minimizing the\ntask-specific matching divergence rather than the upstream prediction error.\nMethodologically, we formulate the vehicle relocation problem by quadratic\nprogramming (QP) and incorporate a novel unrolling approach based on the\nAlternating Direction Method of Multipliers (ADMM) within the SPO framework to\ncompute gradients of the QP layer, facilitating backpropagation and\ngradient-based optimization for end-to-end learning. The effectiveness of the\nproposed framework is validated by real-world taxi datasets in Hong Kong.\nUtilizing the alternating differentiation method, the general SPO framework\npresents a novel concept of addressing decision-making problems with\nuncertainty, demonstrating significant potential for advancing applications in\nintelligent transportation systems.\n", "link": "http://arxiv.org/abs/2411.18432v1", "date": "2024-11-27", "relevancy": 2.1553, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20End-to-End%20Smart%20Predict-then-Optimize%20Framework%20for%20Vehicle%0A%20%20Relocation%20Problems%20in%20Large-Scale%20Vehicle%20Crowd%20Sensing&body=Title%3A%20An%20End-to-End%20Smart%20Predict-then-Optimize%20Framework%20for%20Vehicle%0A%20%20Relocation%20Problems%20in%20Large-Scale%20Vehicle%20Crowd%20Sensing%0AAuthor%3A%20Xinyu%20Wang%20and%20Yiyang%20Peng%20and%20Wei%20Ma%0AAbstract%3A%20%20%20Ubiquitous%20mobile%20devices%20have%20catalyzed%20the%20development%20of%20vehicle%20crowd%0Asensing%20%28VCS%29.%20In%20particular%2C%20vehicle%20sensing%20systems%20show%20great%20potential%20in%0Athe%20flexible%20acquisition%20of%20spatio-temporal%20urban%20data%20through%20built-in%20sensors%0Aunder%20diverse%20sensing%20scenarios.%20However%2C%20vehicle%20systems%20often%20exhibit%20biased%0Acoverage%20due%20to%20the%20heterogeneous%20nature%20of%20trip%20requests%20and%20routes.%20To%0Aachieve%20a%20high%20sensing%20coverage%2C%20a%20critical%20challenge%20lies%20in%20optimally%0Arelocating%20vehicles%20to%20minimize%20the%20divergence%20between%20vehicle%20distributions%0Aand%20target%20sensing%20distributions.%20Conventional%20approaches%20typically%20employ%20a%0Atwo-stage%20predict-then-optimize%20%28PTO%29%20process%3A%20first%20predicting%20real-time%0Avehicle%20distributions%20and%20subsequently%20generating%20an%20optimal%20relocation%0Astrategy%20based%20on%20the%20predictions.%20However%2C%20this%20approach%20can%20lead%20to%0Asuboptimal%20decision-making%20due%20to%20the%20propagation%20of%20errors%20from%20upstream%0Aprediction.%20To%20this%20end%2C%20we%20develop%20an%20end-to-end%20Smart%20Predict-then-Optimize%0A%28SPO%29%20framework%20by%20integrating%20optimization%20into%20prediction%20within%20the%20deep%0Alearning%20architecture%2C%20and%20the%20entire%20framework%20is%20trained%20by%20minimizing%20the%0Atask-specific%20matching%20divergence%20rather%20than%20the%20upstream%20prediction%20error.%0AMethodologically%2C%20we%20formulate%20the%20vehicle%20relocation%20problem%20by%20quadratic%0Aprogramming%20%28QP%29%20and%20incorporate%20a%20novel%20unrolling%20approach%20based%20on%20the%0AAlternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20within%20the%20SPO%20framework%20to%0Acompute%20gradients%20of%20the%20QP%20layer%2C%20facilitating%20backpropagation%20and%0Agradient-based%20optimization%20for%20end-to-end%20learning.%20The%20effectiveness%20of%20the%0Aproposed%20framework%20is%20validated%20by%20real-world%20taxi%20datasets%20in%20Hong%20Kong.%0AUtilizing%20the%20alternating%20differentiation%20method%2C%20the%20general%20SPO%20framework%0Apresents%20a%20novel%20concept%20of%20addressing%20decision-making%20problems%20with%0Auncertainty%2C%20demonstrating%20significant%20potential%20for%20advancing%20applications%20in%0Aintelligent%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520End-to-End%2520Smart%2520Predict-then-Optimize%2520Framework%2520for%2520Vehicle%250A%2520%2520Relocation%2520Problems%2520in%2520Large-Scale%2520Vehicle%2520Crowd%2520Sensing%26entry.906535625%3DXinyu%2520Wang%2520and%2520Yiyang%2520Peng%2520and%2520Wei%2520Ma%26entry.1292438233%3D%2520%2520Ubiquitous%2520mobile%2520devices%2520have%2520catalyzed%2520the%2520development%2520of%2520vehicle%2520crowd%250Asensing%2520%2528VCS%2529.%2520In%2520particular%252C%2520vehicle%2520sensing%2520systems%2520show%2520great%2520potential%2520in%250Athe%2520flexible%2520acquisition%2520of%2520spatio-temporal%2520urban%2520data%2520through%2520built-in%2520sensors%250Aunder%2520diverse%2520sensing%2520scenarios.%2520However%252C%2520vehicle%2520systems%2520often%2520exhibit%2520biased%250Acoverage%2520due%2520to%2520the%2520heterogeneous%2520nature%2520of%2520trip%2520requests%2520and%2520routes.%2520To%250Aachieve%2520a%2520high%2520sensing%2520coverage%252C%2520a%2520critical%2520challenge%2520lies%2520in%2520optimally%250Arelocating%2520vehicles%2520to%2520minimize%2520the%2520divergence%2520between%2520vehicle%2520distributions%250Aand%2520target%2520sensing%2520distributions.%2520Conventional%2520approaches%2520typically%2520employ%2520a%250Atwo-stage%2520predict-then-optimize%2520%2528PTO%2529%2520process%253A%2520first%2520predicting%2520real-time%250Avehicle%2520distributions%2520and%2520subsequently%2520generating%2520an%2520optimal%2520relocation%250Astrategy%2520based%2520on%2520the%2520predictions.%2520However%252C%2520this%2520approach%2520can%2520lead%2520to%250Asuboptimal%2520decision-making%2520due%2520to%2520the%2520propagation%2520of%2520errors%2520from%2520upstream%250Aprediction.%2520To%2520this%2520end%252C%2520we%2520develop%2520an%2520end-to-end%2520Smart%2520Predict-then-Optimize%250A%2528SPO%2529%2520framework%2520by%2520integrating%2520optimization%2520into%2520prediction%2520within%2520the%2520deep%250Alearning%2520architecture%252C%2520and%2520the%2520entire%2520framework%2520is%2520trained%2520by%2520minimizing%2520the%250Atask-specific%2520matching%2520divergence%2520rather%2520than%2520the%2520upstream%2520prediction%2520error.%250AMethodologically%252C%2520we%2520formulate%2520the%2520vehicle%2520relocation%2520problem%2520by%2520quadratic%250Aprogramming%2520%2528QP%2529%2520and%2520incorporate%2520a%2520novel%2520unrolling%2520approach%2520based%2520on%2520the%250AAlternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528ADMM%2529%2520within%2520the%2520SPO%2520framework%2520to%250Acompute%2520gradients%2520of%2520the%2520QP%2520layer%252C%2520facilitating%2520backpropagation%2520and%250Agradient-based%2520optimization%2520for%2520end-to-end%2520learning.%2520The%2520effectiveness%2520of%2520the%250Aproposed%2520framework%2520is%2520validated%2520by%2520real-world%2520taxi%2520datasets%2520in%2520Hong%2520Kong.%250AUtilizing%2520the%2520alternating%2520differentiation%2520method%252C%2520the%2520general%2520SPO%2520framework%250Apresents%2520a%2520novel%2520concept%2520of%2520addressing%2520decision-making%2520problems%2520with%250Auncertainty%252C%2520demonstrating%2520significant%2520potential%2520for%2520advancing%2520applications%2520in%250Aintelligent%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20End-to-End%20Smart%20Predict-then-Optimize%20Framework%20for%20Vehicle%0A%20%20Relocation%20Problems%20in%20Large-Scale%20Vehicle%20Crowd%20Sensing&entry.906535625=Xinyu%20Wang%20and%20Yiyang%20Peng%20and%20Wei%20Ma&entry.1292438233=%20%20Ubiquitous%20mobile%20devices%20have%20catalyzed%20the%20development%20of%20vehicle%20crowd%0Asensing%20%28VCS%29.%20In%20particular%2C%20vehicle%20sensing%20systems%20show%20great%20potential%20in%0Athe%20flexible%20acquisition%20of%20spatio-temporal%20urban%20data%20through%20built-in%20sensors%0Aunder%20diverse%20sensing%20scenarios.%20However%2C%20vehicle%20systems%20often%20exhibit%20biased%0Acoverage%20due%20to%20the%20heterogeneous%20nature%20of%20trip%20requests%20and%20routes.%20To%0Aachieve%20a%20high%20sensing%20coverage%2C%20a%20critical%20challenge%20lies%20in%20optimally%0Arelocating%20vehicles%20to%20minimize%20the%20divergence%20between%20vehicle%20distributions%0Aand%20target%20sensing%20distributions.%20Conventional%20approaches%20typically%20employ%20a%0Atwo-stage%20predict-then-optimize%20%28PTO%29%20process%3A%20first%20predicting%20real-time%0Avehicle%20distributions%20and%20subsequently%20generating%20an%20optimal%20relocation%0Astrategy%20based%20on%20the%20predictions.%20However%2C%20this%20approach%20can%20lead%20to%0Asuboptimal%20decision-making%20due%20to%20the%20propagation%20of%20errors%20from%20upstream%0Aprediction.%20To%20this%20end%2C%20we%20develop%20an%20end-to-end%20Smart%20Predict-then-Optimize%0A%28SPO%29%20framework%20by%20integrating%20optimization%20into%20prediction%20within%20the%20deep%0Alearning%20architecture%2C%20and%20the%20entire%20framework%20is%20trained%20by%20minimizing%20the%0Atask-specific%20matching%20divergence%20rather%20than%20the%20upstream%20prediction%20error.%0AMethodologically%2C%20we%20formulate%20the%20vehicle%20relocation%20problem%20by%20quadratic%0Aprogramming%20%28QP%29%20and%20incorporate%20a%20novel%20unrolling%20approach%20based%20on%20the%0AAlternating%20Direction%20Method%20of%20Multipliers%20%28ADMM%29%20within%20the%20SPO%20framework%20to%0Acompute%20gradients%20of%20the%20QP%20layer%2C%20facilitating%20backpropagation%20and%0Agradient-based%20optimization%20for%20end-to-end%20learning.%20The%20effectiveness%20of%20the%0Aproposed%20framework%20is%20validated%20by%20real-world%20taxi%20datasets%20in%20Hong%20Kong.%0AUtilizing%20the%20alternating%20differentiation%20method%2C%20the%20general%20SPO%20framework%0Apresents%20a%20novel%20concept%20of%20addressing%20decision-making%20problems%20with%0Auncertainty%2C%20demonstrating%20significant%20potential%20for%20advancing%20applications%20in%0Aintelligent%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18432v1&entry.124074799=Read"},
{"title": "Incomplete Multi-view Multi-label Classification via a Dual-level\n  Contrastive Learning Framework", "author": "Bingyan Nie and Wulin Xie and Jiang Long and Xiaohuan Lu", "abstract": "  Recently, multi-view and multi-label classification have become significant\ndomains for comprehensive data analysis and exploration. However,\nincompleteness both in views and labels is still a real-world scenario for\nmulti-view multi-label classification. In this paper, we seek to focus on\ndouble missing multi-view multi-label classification tasks and propose our\ndual-level contrastive learning framework to solve this issue. Different from\nthe existing works, which couple consistent information and view-specific\ninformation in the same feature space, we decouple the two heterogeneous\nproperties into different spaces and employ contrastive learning theory to\nfully disentangle the two properties. Specifically, our method first introduces\na two-channel decoupling module that contains a shared representation and a\nview-proprietary representation to effectively extract consistency and\ncomplementarity information across all views. Second, to efficiently filter out\nhigh-quality consistent information from multi-view representations, two\nconsistency objectives based on contrastive learning are conducted on the\nhigh-level features and the semantic labels, respectively. Extensive\nexperiments on several widely used benchmark datasets demonstrate that the\nproposed method has more stable and superior classification performance.\n", "link": "http://arxiv.org/abs/2411.18267v1", "date": "2024-11-27", "relevancy": 2.1535, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incomplete%20Multi-view%20Multi-label%20Classification%20via%20a%20Dual-level%0A%20%20Contrastive%20Learning%20Framework&body=Title%3A%20Incomplete%20Multi-view%20Multi-label%20Classification%20via%20a%20Dual-level%0A%20%20Contrastive%20Learning%20Framework%0AAuthor%3A%20Bingyan%20Nie%20and%20Wulin%20Xie%20and%20Jiang%20Long%20and%20Xiaohuan%20Lu%0AAbstract%3A%20%20%20Recently%2C%20multi-view%20and%20multi-label%20classification%20have%20become%20significant%0Adomains%20for%20comprehensive%20data%20analysis%20and%20exploration.%20However%2C%0Aincompleteness%20both%20in%20views%20and%20labels%20is%20still%20a%20real-world%20scenario%20for%0Amulti-view%20multi-label%20classification.%20In%20this%20paper%2C%20we%20seek%20to%20focus%20on%0Adouble%20missing%20multi-view%20multi-label%20classification%20tasks%20and%20propose%20our%0Adual-level%20contrastive%20learning%20framework%20to%20solve%20this%20issue.%20Different%20from%0Athe%20existing%20works%2C%20which%20couple%20consistent%20information%20and%20view-specific%0Ainformation%20in%20the%20same%20feature%20space%2C%20we%20decouple%20the%20two%20heterogeneous%0Aproperties%20into%20different%20spaces%20and%20employ%20contrastive%20learning%20theory%20to%0Afully%20disentangle%20the%20two%20properties.%20Specifically%2C%20our%20method%20first%20introduces%0Aa%20two-channel%20decoupling%20module%20that%20contains%20a%20shared%20representation%20and%20a%0Aview-proprietary%20representation%20to%20effectively%20extract%20consistency%20and%0Acomplementarity%20information%20across%20all%20views.%20Second%2C%20to%20efficiently%20filter%20out%0Ahigh-quality%20consistent%20information%20from%20multi-view%20representations%2C%20two%0Aconsistency%20objectives%20based%20on%20contrastive%20learning%20are%20conducted%20on%20the%0Ahigh-level%20features%20and%20the%20semantic%20labels%2C%20respectively.%20Extensive%0Aexperiments%20on%20several%20widely%20used%20benchmark%20datasets%20demonstrate%20that%20the%0Aproposed%20method%20has%20more%20stable%20and%20superior%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncomplete%2520Multi-view%2520Multi-label%2520Classification%2520via%2520a%2520Dual-level%250A%2520%2520Contrastive%2520Learning%2520Framework%26entry.906535625%3DBingyan%2520Nie%2520and%2520Wulin%2520Xie%2520and%2520Jiang%2520Long%2520and%2520Xiaohuan%2520Lu%26entry.1292438233%3D%2520%2520Recently%252C%2520multi-view%2520and%2520multi-label%2520classification%2520have%2520become%2520significant%250Adomains%2520for%2520comprehensive%2520data%2520analysis%2520and%2520exploration.%2520However%252C%250Aincompleteness%2520both%2520in%2520views%2520and%2520labels%2520is%2520still%2520a%2520real-world%2520scenario%2520for%250Amulti-view%2520multi-label%2520classification.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520focus%2520on%250Adouble%2520missing%2520multi-view%2520multi-label%2520classification%2520tasks%2520and%2520propose%2520our%250Adual-level%2520contrastive%2520learning%2520framework%2520to%2520solve%2520this%2520issue.%2520Different%2520from%250Athe%2520existing%2520works%252C%2520which%2520couple%2520consistent%2520information%2520and%2520view-specific%250Ainformation%2520in%2520the%2520same%2520feature%2520space%252C%2520we%2520decouple%2520the%2520two%2520heterogeneous%250Aproperties%2520into%2520different%2520spaces%2520and%2520employ%2520contrastive%2520learning%2520theory%2520to%250Afully%2520disentangle%2520the%2520two%2520properties.%2520Specifically%252C%2520our%2520method%2520first%2520introduces%250Aa%2520two-channel%2520decoupling%2520module%2520that%2520contains%2520a%2520shared%2520representation%2520and%2520a%250Aview-proprietary%2520representation%2520to%2520effectively%2520extract%2520consistency%2520and%250Acomplementarity%2520information%2520across%2520all%2520views.%2520Second%252C%2520to%2520efficiently%2520filter%2520out%250Ahigh-quality%2520consistent%2520information%2520from%2520multi-view%2520representations%252C%2520two%250Aconsistency%2520objectives%2520based%2520on%2520contrastive%2520learning%2520are%2520conducted%2520on%2520the%250Ahigh-level%2520features%2520and%2520the%2520semantic%2520labels%252C%2520respectively.%2520Extensive%250Aexperiments%2520on%2520several%2520widely%2520used%2520benchmark%2520datasets%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520has%2520more%2520stable%2520and%2520superior%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incomplete%20Multi-view%20Multi-label%20Classification%20via%20a%20Dual-level%0A%20%20Contrastive%20Learning%20Framework&entry.906535625=Bingyan%20Nie%20and%20Wulin%20Xie%20and%20Jiang%20Long%20and%20Xiaohuan%20Lu&entry.1292438233=%20%20Recently%2C%20multi-view%20and%20multi-label%20classification%20have%20become%20significant%0Adomains%20for%20comprehensive%20data%20analysis%20and%20exploration.%20However%2C%0Aincompleteness%20both%20in%20views%20and%20labels%20is%20still%20a%20real-world%20scenario%20for%0Amulti-view%20multi-label%20classification.%20In%20this%20paper%2C%20we%20seek%20to%20focus%20on%0Adouble%20missing%20multi-view%20multi-label%20classification%20tasks%20and%20propose%20our%0Adual-level%20contrastive%20learning%20framework%20to%20solve%20this%20issue.%20Different%20from%0Athe%20existing%20works%2C%20which%20couple%20consistent%20information%20and%20view-specific%0Ainformation%20in%20the%20same%20feature%20space%2C%20we%20decouple%20the%20two%20heterogeneous%0Aproperties%20into%20different%20spaces%20and%20employ%20contrastive%20learning%20theory%20to%0Afully%20disentangle%20the%20two%20properties.%20Specifically%2C%20our%20method%20first%20introduces%0Aa%20two-channel%20decoupling%20module%20that%20contains%20a%20shared%20representation%20and%20a%0Aview-proprietary%20representation%20to%20effectively%20extract%20consistency%20and%0Acomplementarity%20information%20across%20all%20views.%20Second%2C%20to%20efficiently%20filter%20out%0Ahigh-quality%20consistent%20information%20from%20multi-view%20representations%2C%20two%0Aconsistency%20objectives%20based%20on%20contrastive%20learning%20are%20conducted%20on%20the%0Ahigh-level%20features%20and%20the%20semantic%20labels%2C%20respectively.%20Extensive%0Aexperiments%20on%20several%20widely%20used%20benchmark%20datasets%20demonstrate%20that%20the%0Aproposed%20method%20has%20more%20stable%20and%20superior%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18267v1&entry.124074799=Read"},
{"title": "Differentiable Weightless Neural Networks", "author": "Alan T. L. Bacellar and Zachary Susskind and Mauricio Breternitz Jr. and Eugene John and Lizy K. John and Priscila M. V. Lima and Felipe M. G. Fran\u00e7a", "abstract": "  We introduce the Differentiable Weightless Neural Network (DWN), a model\nbased on interconnected lookup tables. Training of DWNs is enabled by a novel\nExtended Finite Difference technique for approximate differentiation of binary\nvalues. We propose Learnable Mapping, Learnable Reduction, and Spectral\nRegularization to further improve the accuracy and efficiency of these models.\nWe evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware\naccelerator, where they demonstrate superior latency, throughput, energy\nefficiency, and model area compared to state-of-the-art solutions, (2) a\nlow-power microcontroller, where they achieve preferable accuracy to XGBoost\nwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,\nwhere they consistently outperform small models in both accuracy and projected\nhardware area. DWNs also compare favorably against leading approaches for\ntabular datasets, with higher average rank. Overall, our work positions DWNs as\na pioneering solution for edge-compatible high-throughput neural networks.\n", "link": "http://arxiv.org/abs/2410.11112v3", "date": "2024-11-27", "relevancy": 2.1381, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5604}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Weightless%20Neural%20Networks&body=Title%3A%20Differentiable%20Weightless%20Neural%20Networks%0AAuthor%3A%20Alan%20T.%20L.%20Bacellar%20and%20Zachary%20Susskind%20and%20Mauricio%20Breternitz%20Jr.%20and%20Eugene%20John%20and%20Lizy%20K.%20John%20and%20Priscila%20M.%20V.%20Lima%20and%20Felipe%20M.%20G.%20Fran%C3%A7a%0AAbstract%3A%20%20%20We%20introduce%20the%20Differentiable%20Weightless%20Neural%20Network%20%28DWN%29%2C%20a%20model%0Abased%20on%20interconnected%20lookup%20tables.%20Training%20of%20DWNs%20is%20enabled%20by%20a%20novel%0AExtended%20Finite%20Difference%20technique%20for%20approximate%20differentiation%20of%20binary%0Avalues.%20We%20propose%20Learnable%20Mapping%2C%20Learnable%20Reduction%2C%20and%20Spectral%0ARegularization%20to%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20these%20models.%0AWe%20evaluate%20DWNs%20in%20three%20edge%20computing%20contexts%3A%20%281%29%20an%20FPGA-based%20hardware%0Aaccelerator%2C%20where%20they%20demonstrate%20superior%20latency%2C%20throughput%2C%20energy%0Aefficiency%2C%20and%20model%20area%20compared%20to%20state-of-the-art%20solutions%2C%20%282%29%20a%0Alow-power%20microcontroller%2C%20where%20they%20achieve%20preferable%20accuracy%20to%20XGBoost%0Awhile%20subject%20to%20stringent%20memory%20constraints%2C%20and%20%283%29%20ultra-low-cost%20chips%2C%0Awhere%20they%20consistently%20outperform%20small%20models%20in%20both%20accuracy%20and%20projected%0Ahardware%20area.%20DWNs%20also%20compare%20favorably%20against%20leading%20approaches%20for%0Atabular%20datasets%2C%20with%20higher%20average%20rank.%20Overall%2C%20our%20work%20positions%20DWNs%20as%0Aa%20pioneering%20solution%20for%20edge-compatible%20high-throughput%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Weightless%2520Neural%2520Networks%26entry.906535625%3DAlan%2520T.%2520L.%2520Bacellar%2520and%2520Zachary%2520Susskind%2520and%2520Mauricio%2520Breternitz%2520Jr.%2520and%2520Eugene%2520John%2520and%2520Lizy%2520K.%2520John%2520and%2520Priscila%2520M.%2520V.%2520Lima%2520and%2520Felipe%2520M.%2520G.%2520Fran%25C3%25A7a%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Differentiable%2520Weightless%2520Neural%2520Network%2520%2528DWN%2529%252C%2520a%2520model%250Abased%2520on%2520interconnected%2520lookup%2520tables.%2520Training%2520of%2520DWNs%2520is%2520enabled%2520by%2520a%2520novel%250AExtended%2520Finite%2520Difference%2520technique%2520for%2520approximate%2520differentiation%2520of%2520binary%250Avalues.%2520We%2520propose%2520Learnable%2520Mapping%252C%2520Learnable%2520Reduction%252C%2520and%2520Spectral%250ARegularization%2520to%2520further%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520these%2520models.%250AWe%2520evaluate%2520DWNs%2520in%2520three%2520edge%2520computing%2520contexts%253A%2520%25281%2529%2520an%2520FPGA-based%2520hardware%250Aaccelerator%252C%2520where%2520they%2520demonstrate%2520superior%2520latency%252C%2520throughput%252C%2520energy%250Aefficiency%252C%2520and%2520model%2520area%2520compared%2520to%2520state-of-the-art%2520solutions%252C%2520%25282%2529%2520a%250Alow-power%2520microcontroller%252C%2520where%2520they%2520achieve%2520preferable%2520accuracy%2520to%2520XGBoost%250Awhile%2520subject%2520to%2520stringent%2520memory%2520constraints%252C%2520and%2520%25283%2529%2520ultra-low-cost%2520chips%252C%250Awhere%2520they%2520consistently%2520outperform%2520small%2520models%2520in%2520both%2520accuracy%2520and%2520projected%250Ahardware%2520area.%2520DWNs%2520also%2520compare%2520favorably%2520against%2520leading%2520approaches%2520for%250Atabular%2520datasets%252C%2520with%2520higher%2520average%2520rank.%2520Overall%252C%2520our%2520work%2520positions%2520DWNs%2520as%250Aa%2520pioneering%2520solution%2520for%2520edge-compatible%2520high-throughput%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Weightless%20Neural%20Networks&entry.906535625=Alan%20T.%20L.%20Bacellar%20and%20Zachary%20Susskind%20and%20Mauricio%20Breternitz%20Jr.%20and%20Eugene%20John%20and%20Lizy%20K.%20John%20and%20Priscila%20M.%20V.%20Lima%20and%20Felipe%20M.%20G.%20Fran%C3%A7a&entry.1292438233=%20%20We%20introduce%20the%20Differentiable%20Weightless%20Neural%20Network%20%28DWN%29%2C%20a%20model%0Abased%20on%20interconnected%20lookup%20tables.%20Training%20of%20DWNs%20is%20enabled%20by%20a%20novel%0AExtended%20Finite%20Difference%20technique%20for%20approximate%20differentiation%20of%20binary%0Avalues.%20We%20propose%20Learnable%20Mapping%2C%20Learnable%20Reduction%2C%20and%20Spectral%0ARegularization%20to%20further%20improve%20the%20accuracy%20and%20efficiency%20of%20these%20models.%0AWe%20evaluate%20DWNs%20in%20three%20edge%20computing%20contexts%3A%20%281%29%20an%20FPGA-based%20hardware%0Aaccelerator%2C%20where%20they%20demonstrate%20superior%20latency%2C%20throughput%2C%20energy%0Aefficiency%2C%20and%20model%20area%20compared%20to%20state-of-the-art%20solutions%2C%20%282%29%20a%0Alow-power%20microcontroller%2C%20where%20they%20achieve%20preferable%20accuracy%20to%20XGBoost%0Awhile%20subject%20to%20stringent%20memory%20constraints%2C%20and%20%283%29%20ultra-low-cost%20chips%2C%0Awhere%20they%20consistently%20outperform%20small%20models%20in%20both%20accuracy%20and%20projected%0Ahardware%20area.%20DWNs%20also%20compare%20favorably%20against%20leading%20approaches%20for%0Atabular%20datasets%2C%20with%20higher%20average%20rank.%20Overall%2C%20our%20work%20positions%20DWNs%20as%0Aa%20pioneering%20solution%20for%20edge-compatible%20high-throughput%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11112v3&entry.124074799=Read"},
{"title": "Leveraging Semi-Supervised Learning to Enhance Data Mining for Image\n  Classification under Limited Labeled Data", "author": "Aoran Shen and Minghao Dai and Jiacheng Hu and Yingbin Liang and Shiru Wang and Junliang Du", "abstract": "  In the 21st-century information age, with the development of big data\ntechnology, effectively extracting valuable information from massive data has\nbecome a key issue. Traditional data mining methods are inadequate when faced\nwith large-scale, high-dimensional and complex data. Especially when labeled\ndata is scarce, their performance is greatly limited. This study optimizes data\nmining algorithms by introducing semi-supervised learning methods, aiming to\nimprove the algorithm's ability to utilize unlabeled data, thereby achieving\nmore accurate data analysis and pattern recognition under limited labeled data\nconditions. Specifically, we adopt a self-training method and combine it with a\nconvolutional neural network (CNN) for image feature extraction and\nclassification, and continuously improve the model prediction performance\nthrough an iterative process. The experimental results demonstrate that the\nproposed method significantly outperforms traditional machine learning\ntechniques such as Support Vector Machine (SVM), XGBoost, and Multi-Layer\nPerceptron (MLP) on the CIFAR-10 image classification dataset. Notable\nimprovements were observed in key performance metrics, including accuracy,\nrecall, and F1 score. Furthermore, the robustness and noise-resistance\ncapabilities of the semi-supervised CNN model were validated through\nexperiments under varying noise levels, confirming its practical applicability\nin real-world scenarios.\n", "link": "http://arxiv.org/abs/2411.18622v1", "date": "2024-11-27", "relevancy": 2.1276, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5855}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4977}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Semi-Supervised%20Learning%20to%20Enhance%20Data%20Mining%20for%20Image%0A%20%20Classification%20under%20Limited%20Labeled%20Data&body=Title%3A%20Leveraging%20Semi-Supervised%20Learning%20to%20Enhance%20Data%20Mining%20for%20Image%0A%20%20Classification%20under%20Limited%20Labeled%20Data%0AAuthor%3A%20Aoran%20Shen%20and%20Minghao%20Dai%20and%20Jiacheng%20Hu%20and%20Yingbin%20Liang%20and%20Shiru%20Wang%20and%20Junliang%20Du%0AAbstract%3A%20%20%20In%20the%2021st-century%20information%20age%2C%20with%20the%20development%20of%20big%20data%0Atechnology%2C%20effectively%20extracting%20valuable%20information%20from%20massive%20data%20has%0Abecome%20a%20key%20issue.%20Traditional%20data%20mining%20methods%20are%20inadequate%20when%20faced%0Awith%20large-scale%2C%20high-dimensional%20and%20complex%20data.%20Especially%20when%20labeled%0Adata%20is%20scarce%2C%20their%20performance%20is%20greatly%20limited.%20This%20study%20optimizes%20data%0Amining%20algorithms%20by%20introducing%20semi-supervised%20learning%20methods%2C%20aiming%20to%0Aimprove%20the%20algorithm%27s%20ability%20to%20utilize%20unlabeled%20data%2C%20thereby%20achieving%0Amore%20accurate%20data%20analysis%20and%20pattern%20recognition%20under%20limited%20labeled%20data%0Aconditions.%20Specifically%2C%20we%20adopt%20a%20self-training%20method%20and%20combine%20it%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20for%20image%20feature%20extraction%20and%0Aclassification%2C%20and%20continuously%20improve%20the%20model%20prediction%20performance%0Athrough%20an%20iterative%20process.%20The%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20significantly%20outperforms%20traditional%20machine%20learning%0Atechniques%20such%20as%20Support%20Vector%20Machine%20%28SVM%29%2C%20XGBoost%2C%20and%20Multi-Layer%0APerceptron%20%28MLP%29%20on%20the%20CIFAR-10%20image%20classification%20dataset.%20Notable%0Aimprovements%20were%20observed%20in%20key%20performance%20metrics%2C%20including%20accuracy%2C%0Arecall%2C%20and%20F1%20score.%20Furthermore%2C%20the%20robustness%20and%20noise-resistance%0Acapabilities%20of%20the%20semi-supervised%20CNN%20model%20were%20validated%20through%0Aexperiments%20under%20varying%20noise%20levels%2C%20confirming%20its%20practical%20applicability%0Ain%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Semi-Supervised%2520Learning%2520to%2520Enhance%2520Data%2520Mining%2520for%2520Image%250A%2520%2520Classification%2520under%2520Limited%2520Labeled%2520Data%26entry.906535625%3DAoran%2520Shen%2520and%2520Minghao%2520Dai%2520and%2520Jiacheng%2520Hu%2520and%2520Yingbin%2520Liang%2520and%2520Shiru%2520Wang%2520and%2520Junliang%2520Du%26entry.1292438233%3D%2520%2520In%2520the%252021st-century%2520information%2520age%252C%2520with%2520the%2520development%2520of%2520big%2520data%250Atechnology%252C%2520effectively%2520extracting%2520valuable%2520information%2520from%2520massive%2520data%2520has%250Abecome%2520a%2520key%2520issue.%2520Traditional%2520data%2520mining%2520methods%2520are%2520inadequate%2520when%2520faced%250Awith%2520large-scale%252C%2520high-dimensional%2520and%2520complex%2520data.%2520Especially%2520when%2520labeled%250Adata%2520is%2520scarce%252C%2520their%2520performance%2520is%2520greatly%2520limited.%2520This%2520study%2520optimizes%2520data%250Amining%2520algorithms%2520by%2520introducing%2520semi-supervised%2520learning%2520methods%252C%2520aiming%2520to%250Aimprove%2520the%2520algorithm%2527s%2520ability%2520to%2520utilize%2520unlabeled%2520data%252C%2520thereby%2520achieving%250Amore%2520accurate%2520data%2520analysis%2520and%2520pattern%2520recognition%2520under%2520limited%2520labeled%2520data%250Aconditions.%2520Specifically%252C%2520we%2520adopt%2520a%2520self-training%2520method%2520and%2520combine%2520it%2520with%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520for%2520image%2520feature%2520extraction%2520and%250Aclassification%252C%2520and%2520continuously%2520improve%2520the%2520model%2520prediction%2520performance%250Athrough%2520an%2520iterative%2520process.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520significantly%2520outperforms%2520traditional%2520machine%2520learning%250Atechniques%2520such%2520as%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520XGBoost%252C%2520and%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%2520on%2520the%2520CIFAR-10%2520image%2520classification%2520dataset.%2520Notable%250Aimprovements%2520were%2520observed%2520in%2520key%2520performance%2520metrics%252C%2520including%2520accuracy%252C%250Arecall%252C%2520and%2520F1%2520score.%2520Furthermore%252C%2520the%2520robustness%2520and%2520noise-resistance%250Acapabilities%2520of%2520the%2520semi-supervised%2520CNN%2520model%2520were%2520validated%2520through%250Aexperiments%2520under%2520varying%2520noise%2520levels%252C%2520confirming%2520its%2520practical%2520applicability%250Ain%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Semi-Supervised%20Learning%20to%20Enhance%20Data%20Mining%20for%20Image%0A%20%20Classification%20under%20Limited%20Labeled%20Data&entry.906535625=Aoran%20Shen%20and%20Minghao%20Dai%20and%20Jiacheng%20Hu%20and%20Yingbin%20Liang%20and%20Shiru%20Wang%20and%20Junliang%20Du&entry.1292438233=%20%20In%20the%2021st-century%20information%20age%2C%20with%20the%20development%20of%20big%20data%0Atechnology%2C%20effectively%20extracting%20valuable%20information%20from%20massive%20data%20has%0Abecome%20a%20key%20issue.%20Traditional%20data%20mining%20methods%20are%20inadequate%20when%20faced%0Awith%20large-scale%2C%20high-dimensional%20and%20complex%20data.%20Especially%20when%20labeled%0Adata%20is%20scarce%2C%20their%20performance%20is%20greatly%20limited.%20This%20study%20optimizes%20data%0Amining%20algorithms%20by%20introducing%20semi-supervised%20learning%20methods%2C%20aiming%20to%0Aimprove%20the%20algorithm%27s%20ability%20to%20utilize%20unlabeled%20data%2C%20thereby%20achieving%0Amore%20accurate%20data%20analysis%20and%20pattern%20recognition%20under%20limited%20labeled%20data%0Aconditions.%20Specifically%2C%20we%20adopt%20a%20self-training%20method%20and%20combine%20it%20with%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20for%20image%20feature%20extraction%20and%0Aclassification%2C%20and%20continuously%20improve%20the%20model%20prediction%20performance%0Athrough%20an%20iterative%20process.%20The%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20significantly%20outperforms%20traditional%20machine%20learning%0Atechniques%20such%20as%20Support%20Vector%20Machine%20%28SVM%29%2C%20XGBoost%2C%20and%20Multi-Layer%0APerceptron%20%28MLP%29%20on%20the%20CIFAR-10%20image%20classification%20dataset.%20Notable%0Aimprovements%20were%20observed%20in%20key%20performance%20metrics%2C%20including%20accuracy%2C%0Arecall%2C%20and%20F1%20score.%20Furthermore%2C%20the%20robustness%20and%20noise-resistance%0Acapabilities%20of%20the%20semi-supervised%20CNN%20model%20were%20validated%20through%0Aexperiments%20under%20varying%20noise%20levels%2C%20confirming%20its%20practical%20applicability%0Ain%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18622v1&entry.124074799=Read"},
{"title": "HUPE: Heuristic Underwater Perceptual Enhancement with Semantic\n  Collaborative Learning", "author": "Zengxi Zhang and Zhiying Jiang and Long Ma and Jinyuan Liu and Xin Fan and Risheng Liu", "abstract": "  Underwater images are often affected by light refraction and absorption,\nreducing visibility and interfering with subsequent applications. Existing\nunderwater image enhancement methods primarily focus on improving visual\nquality while overlooking practical implications. To strike a balance between\nvisual quality and application, we propose a heuristic invertible network for\nunderwater perception enhancement, dubbed HUPE, which enhances visual quality\nand demonstrates flexibility in handling other downstream tasks. Specifically,\nwe introduced an information-preserving reversible transformation with embedded\nFourier transform to establish a bidirectional mapping between underwater\nimages and their clear images. Additionally, a heuristic prior is incorporated\ninto the enhancement process to better capture scene information. To further\nbridge the feature gap between vision-based enhancement images and\napplication-oriented images, a semantic collaborative learning module is\napplied in the joint optimization process of the visual enhancement task and\nthe downstream task, which guides the proposed enhancement model to extract\nmore task-oriented semantic features while obtaining visually pleasing images.\nExtensive experiments, both quantitative and qualitative, demonstrate the\nsuperiority of our HUPE over state-of-the-art methods. The source code is\navailable at https://github.com/ZengxiZhang/HUPE.\n", "link": "http://arxiv.org/abs/2411.18296v1", "date": "2024-11-27", "relevancy": 2.1229, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5543}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning&body=Title%3A%20HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning%0AAuthor%3A%20Zengxi%20Zhang%20and%20Zhiying%20Jiang%20and%20Long%20Ma%20and%20Jinyuan%20Liu%20and%20Xin%20Fan%20and%20Risheng%20Liu%0AAbstract%3A%20%20%20Underwater%20images%20are%20often%20affected%20by%20light%20refraction%20and%20absorption%2C%0Areducing%20visibility%20and%20interfering%20with%20subsequent%20applications.%20Existing%0Aunderwater%20image%20enhancement%20methods%20primarily%20focus%20on%20improving%20visual%0Aquality%20while%20overlooking%20practical%20implications.%20To%20strike%20a%20balance%20between%0Avisual%20quality%20and%20application%2C%20we%20propose%20a%20heuristic%20invertible%20network%20for%0Aunderwater%20perception%20enhancement%2C%20dubbed%20HUPE%2C%20which%20enhances%20visual%20quality%0Aand%20demonstrates%20flexibility%20in%20handling%20other%20downstream%20tasks.%20Specifically%2C%0Awe%20introduced%20an%20information-preserving%20reversible%20transformation%20with%20embedded%0AFourier%20transform%20to%20establish%20a%20bidirectional%20mapping%20between%20underwater%0Aimages%20and%20their%20clear%20images.%20Additionally%2C%20a%20heuristic%20prior%20is%20incorporated%0Ainto%20the%20enhancement%20process%20to%20better%20capture%20scene%20information.%20To%20further%0Abridge%20the%20feature%20gap%20between%20vision-based%20enhancement%20images%20and%0Aapplication-oriented%20images%2C%20a%20semantic%20collaborative%20learning%20module%20is%0Aapplied%20in%20the%20joint%20optimization%20process%20of%20the%20visual%20enhancement%20task%20and%0Athe%20downstream%20task%2C%20which%20guides%20the%20proposed%20enhancement%20model%20to%20extract%0Amore%20task-oriented%20semantic%20features%20while%20obtaining%20visually%20pleasing%20images.%0AExtensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20demonstrate%20the%0Asuperiority%20of%20our%20HUPE%20over%20state-of-the-art%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/ZengxiZhang/HUPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHUPE%253A%2520Heuristic%2520Underwater%2520Perceptual%2520Enhancement%2520with%2520Semantic%250A%2520%2520Collaborative%2520Learning%26entry.906535625%3DZengxi%2520Zhang%2520and%2520Zhiying%2520Jiang%2520and%2520Long%2520Ma%2520and%2520Jinyuan%2520Liu%2520and%2520Xin%2520Fan%2520and%2520Risheng%2520Liu%26entry.1292438233%3D%2520%2520Underwater%2520images%2520are%2520often%2520affected%2520by%2520light%2520refraction%2520and%2520absorption%252C%250Areducing%2520visibility%2520and%2520interfering%2520with%2520subsequent%2520applications.%2520Existing%250Aunderwater%2520image%2520enhancement%2520methods%2520primarily%2520focus%2520on%2520improving%2520visual%250Aquality%2520while%2520overlooking%2520practical%2520implications.%2520To%2520strike%2520a%2520balance%2520between%250Avisual%2520quality%2520and%2520application%252C%2520we%2520propose%2520a%2520heuristic%2520invertible%2520network%2520for%250Aunderwater%2520perception%2520enhancement%252C%2520dubbed%2520HUPE%252C%2520which%2520enhances%2520visual%2520quality%250Aand%2520demonstrates%2520flexibility%2520in%2520handling%2520other%2520downstream%2520tasks.%2520Specifically%252C%250Awe%2520introduced%2520an%2520information-preserving%2520reversible%2520transformation%2520with%2520embedded%250AFourier%2520transform%2520to%2520establish%2520a%2520bidirectional%2520mapping%2520between%2520underwater%250Aimages%2520and%2520their%2520clear%2520images.%2520Additionally%252C%2520a%2520heuristic%2520prior%2520is%2520incorporated%250Ainto%2520the%2520enhancement%2520process%2520to%2520better%2520capture%2520scene%2520information.%2520To%2520further%250Abridge%2520the%2520feature%2520gap%2520between%2520vision-based%2520enhancement%2520images%2520and%250Aapplication-oriented%2520images%252C%2520a%2520semantic%2520collaborative%2520learning%2520module%2520is%250Aapplied%2520in%2520the%2520joint%2520optimization%2520process%2520of%2520the%2520visual%2520enhancement%2520task%2520and%250Athe%2520downstream%2520task%252C%2520which%2520guides%2520the%2520proposed%2520enhancement%2520model%2520to%2520extract%250Amore%2520task-oriented%2520semantic%2520features%2520while%2520obtaining%2520visually%2520pleasing%2520images.%250AExtensive%2520experiments%252C%2520both%2520quantitative%2520and%2520qualitative%252C%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520HUPE%2520over%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/ZengxiZhang/HUPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning&entry.906535625=Zengxi%20Zhang%20and%20Zhiying%20Jiang%20and%20Long%20Ma%20and%20Jinyuan%20Liu%20and%20Xin%20Fan%20and%20Risheng%20Liu&entry.1292438233=%20%20Underwater%20images%20are%20often%20affected%20by%20light%20refraction%20and%20absorption%2C%0Areducing%20visibility%20and%20interfering%20with%20subsequent%20applications.%20Existing%0Aunderwater%20image%20enhancement%20methods%20primarily%20focus%20on%20improving%20visual%0Aquality%20while%20overlooking%20practical%20implications.%20To%20strike%20a%20balance%20between%0Avisual%20quality%20and%20application%2C%20we%20propose%20a%20heuristic%20invertible%20network%20for%0Aunderwater%20perception%20enhancement%2C%20dubbed%20HUPE%2C%20which%20enhances%20visual%20quality%0Aand%20demonstrates%20flexibility%20in%20handling%20other%20downstream%20tasks.%20Specifically%2C%0Awe%20introduced%20an%20information-preserving%20reversible%20transformation%20with%20embedded%0AFourier%20transform%20to%20establish%20a%20bidirectional%20mapping%20between%20underwater%0Aimages%20and%20their%20clear%20images.%20Additionally%2C%20a%20heuristic%20prior%20is%20incorporated%0Ainto%20the%20enhancement%20process%20to%20better%20capture%20scene%20information.%20To%20further%0Abridge%20the%20feature%20gap%20between%20vision-based%20enhancement%20images%20and%0Aapplication-oriented%20images%2C%20a%20semantic%20collaborative%20learning%20module%20is%0Aapplied%20in%20the%20joint%20optimization%20process%20of%20the%20visual%20enhancement%20task%20and%0Athe%20downstream%20task%2C%20which%20guides%20the%20proposed%20enhancement%20model%20to%20extract%0Amore%20task-oriented%20semantic%20features%20while%20obtaining%20visually%20pleasing%20images.%0AExtensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20demonstrate%20the%0Asuperiority%20of%20our%20HUPE%20over%20state-of-the-art%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/ZengxiZhang/HUPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18296v1&entry.124074799=Read"},
{"title": "SCoTT: Wireless-Aware Path Planning with Vision Language Models and\n  Strategic Chains-of-Thought", "author": "Aladin Djuhera and Vlad C. Andrei and Amin Seffo and Holger Boche and Walid Saad", "abstract": "  Path planning is a complex problem for many practical applications,\nparticularly in robotics. Existing algorithms, however, are exhaustive in\nnature and become increasingly complex when additional side constraints are\nincorporated alongside distance minimization. In this paper, a novel approach\nusing vision language models (VLMs) is proposed for enabling path planning in\ncomplex wireless-aware environments. To this end, insights from a digital twin\n(DT) with real-world wireless ray tracing data are explored in order to\nguarantee an average path gain threshold while minimizing the trajectory\nlength. First, traditional approaches such as A* are compared to several\nwireless-aware extensions, and an optimal iterative dynamic programming\napproach (DP-WA*) is derived, which fully takes into account all path gains and\ndistance metrics within the DT. On the basis of these baselines, the role of\nVLMs as an alternative assistant for path planning is investigated, and a\nstrategic chain-of-thought tasking (SCoTT) approach is proposed. SCoTT divides\nthe complex planning task into several subproblems and solves each with\nadvanced CoT prompting. Results show that SCoTT achieves very close average\npath gains compared to DP-WA* while at the same time yielding consistently\nshorter path lengths. The results also show that VLMs can be used to accelerate\nDP-WA* by efficiently reducing the algorithm's search space and thus saving up\nto 62\\% in execution time. This work underscores the potential of VLMs in\nfuture digital systems as capable assistants for solving complex tasks, while\nenhancing user interaction and accelerating rapid prototyping under diverse\nwireless constraints.\n", "link": "http://arxiv.org/abs/2411.18212v1", "date": "2024-11-27", "relevancy": 2.1149, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCoTT%3A%20Wireless-Aware%20Path%20Planning%20with%20Vision%20Language%20Models%20and%0A%20%20Strategic%20Chains-of-Thought&body=Title%3A%20SCoTT%3A%20Wireless-Aware%20Path%20Planning%20with%20Vision%20Language%20Models%20and%0A%20%20Strategic%20Chains-of-Thought%0AAuthor%3A%20Aladin%20Djuhera%20and%20Vlad%20C.%20Andrei%20and%20Amin%20Seffo%20and%20Holger%20Boche%20and%20Walid%20Saad%0AAbstract%3A%20%20%20Path%20planning%20is%20a%20complex%20problem%20for%20many%20practical%20applications%2C%0Aparticularly%20in%20robotics.%20Existing%20algorithms%2C%20however%2C%20are%20exhaustive%20in%0Anature%20and%20become%20increasingly%20complex%20when%20additional%20side%20constraints%20are%0Aincorporated%20alongside%20distance%20minimization.%20In%20this%20paper%2C%20a%20novel%20approach%0Ausing%20vision%20language%20models%20%28VLMs%29%20is%20proposed%20for%20enabling%20path%20planning%20in%0Acomplex%20wireless-aware%20environments.%20To%20this%20end%2C%20insights%20from%20a%20digital%20twin%0A%28DT%29%20with%20real-world%20wireless%20ray%20tracing%20data%20are%20explored%20in%20order%20to%0Aguarantee%20an%20average%20path%20gain%20threshold%20while%20minimizing%20the%20trajectory%0Alength.%20First%2C%20traditional%20approaches%20such%20as%20A%2A%20are%20compared%20to%20several%0Awireless-aware%20extensions%2C%20and%20an%20optimal%20iterative%20dynamic%20programming%0Aapproach%20%28DP-WA%2A%29%20is%20derived%2C%20which%20fully%20takes%20into%20account%20all%20path%20gains%20and%0Adistance%20metrics%20within%20the%20DT.%20On%20the%20basis%20of%20these%20baselines%2C%20the%20role%20of%0AVLMs%20as%20an%20alternative%20assistant%20for%20path%20planning%20is%20investigated%2C%20and%20a%0Astrategic%20chain-of-thought%20tasking%20%28SCoTT%29%20approach%20is%20proposed.%20SCoTT%20divides%0Athe%20complex%20planning%20task%20into%20several%20subproblems%20and%20solves%20each%20with%0Aadvanced%20CoT%20prompting.%20Results%20show%20that%20SCoTT%20achieves%20very%20close%20average%0Apath%20gains%20compared%20to%20DP-WA%2A%20while%20at%20the%20same%20time%20yielding%20consistently%0Ashorter%20path%20lengths.%20The%20results%20also%20show%20that%20VLMs%20can%20be%20used%20to%20accelerate%0ADP-WA%2A%20by%20efficiently%20reducing%20the%20algorithm%27s%20search%20space%20and%20thus%20saving%20up%0Ato%2062%5C%25%20in%20execution%20time.%20This%20work%20underscores%20the%20potential%20of%20VLMs%20in%0Afuture%20digital%20systems%20as%20capable%20assistants%20for%20solving%20complex%20tasks%2C%20while%0Aenhancing%20user%20interaction%20and%20accelerating%20rapid%20prototyping%20under%20diverse%0Awireless%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCoTT%253A%2520Wireless-Aware%2520Path%2520Planning%2520with%2520Vision%2520Language%2520Models%2520and%250A%2520%2520Strategic%2520Chains-of-Thought%26entry.906535625%3DAladin%2520Djuhera%2520and%2520Vlad%2520C.%2520Andrei%2520and%2520Amin%2520Seffo%2520and%2520Holger%2520Boche%2520and%2520Walid%2520Saad%26entry.1292438233%3D%2520%2520Path%2520planning%2520is%2520a%2520complex%2520problem%2520for%2520many%2520practical%2520applications%252C%250Aparticularly%2520in%2520robotics.%2520Existing%2520algorithms%252C%2520however%252C%2520are%2520exhaustive%2520in%250Anature%2520and%2520become%2520increasingly%2520complex%2520when%2520additional%2520side%2520constraints%2520are%250Aincorporated%2520alongside%2520distance%2520minimization.%2520In%2520this%2520paper%252C%2520a%2520novel%2520approach%250Ausing%2520vision%2520language%2520models%2520%2528VLMs%2529%2520is%2520proposed%2520for%2520enabling%2520path%2520planning%2520in%250Acomplex%2520wireless-aware%2520environments.%2520To%2520this%2520end%252C%2520insights%2520from%2520a%2520digital%2520twin%250A%2528DT%2529%2520with%2520real-world%2520wireless%2520ray%2520tracing%2520data%2520are%2520explored%2520in%2520order%2520to%250Aguarantee%2520an%2520average%2520path%2520gain%2520threshold%2520while%2520minimizing%2520the%2520trajectory%250Alength.%2520First%252C%2520traditional%2520approaches%2520such%2520as%2520A%252A%2520are%2520compared%2520to%2520several%250Awireless-aware%2520extensions%252C%2520and%2520an%2520optimal%2520iterative%2520dynamic%2520programming%250Aapproach%2520%2528DP-WA%252A%2529%2520is%2520derived%252C%2520which%2520fully%2520takes%2520into%2520account%2520all%2520path%2520gains%2520and%250Adistance%2520metrics%2520within%2520the%2520DT.%2520On%2520the%2520basis%2520of%2520these%2520baselines%252C%2520the%2520role%2520of%250AVLMs%2520as%2520an%2520alternative%2520assistant%2520for%2520path%2520planning%2520is%2520investigated%252C%2520and%2520a%250Astrategic%2520chain-of-thought%2520tasking%2520%2528SCoTT%2529%2520approach%2520is%2520proposed.%2520SCoTT%2520divides%250Athe%2520complex%2520planning%2520task%2520into%2520several%2520subproblems%2520and%2520solves%2520each%2520with%250Aadvanced%2520CoT%2520prompting.%2520Results%2520show%2520that%2520SCoTT%2520achieves%2520very%2520close%2520average%250Apath%2520gains%2520compared%2520to%2520DP-WA%252A%2520while%2520at%2520the%2520same%2520time%2520yielding%2520consistently%250Ashorter%2520path%2520lengths.%2520The%2520results%2520also%2520show%2520that%2520VLMs%2520can%2520be%2520used%2520to%2520accelerate%250ADP-WA%252A%2520by%2520efficiently%2520reducing%2520the%2520algorithm%2527s%2520search%2520space%2520and%2520thus%2520saving%2520up%250Ato%252062%255C%2525%2520in%2520execution%2520time.%2520This%2520work%2520underscores%2520the%2520potential%2520of%2520VLMs%2520in%250Afuture%2520digital%2520systems%2520as%2520capable%2520assistants%2520for%2520solving%2520complex%2520tasks%252C%2520while%250Aenhancing%2520user%2520interaction%2520and%2520accelerating%2520rapid%2520prototyping%2520under%2520diverse%250Awireless%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCoTT%3A%20Wireless-Aware%20Path%20Planning%20with%20Vision%20Language%20Models%20and%0A%20%20Strategic%20Chains-of-Thought&entry.906535625=Aladin%20Djuhera%20and%20Vlad%20C.%20Andrei%20and%20Amin%20Seffo%20and%20Holger%20Boche%20and%20Walid%20Saad&entry.1292438233=%20%20Path%20planning%20is%20a%20complex%20problem%20for%20many%20practical%20applications%2C%0Aparticularly%20in%20robotics.%20Existing%20algorithms%2C%20however%2C%20are%20exhaustive%20in%0Anature%20and%20become%20increasingly%20complex%20when%20additional%20side%20constraints%20are%0Aincorporated%20alongside%20distance%20minimization.%20In%20this%20paper%2C%20a%20novel%20approach%0Ausing%20vision%20language%20models%20%28VLMs%29%20is%20proposed%20for%20enabling%20path%20planning%20in%0Acomplex%20wireless-aware%20environments.%20To%20this%20end%2C%20insights%20from%20a%20digital%20twin%0A%28DT%29%20with%20real-world%20wireless%20ray%20tracing%20data%20are%20explored%20in%20order%20to%0Aguarantee%20an%20average%20path%20gain%20threshold%20while%20minimizing%20the%20trajectory%0Alength.%20First%2C%20traditional%20approaches%20such%20as%20A%2A%20are%20compared%20to%20several%0Awireless-aware%20extensions%2C%20and%20an%20optimal%20iterative%20dynamic%20programming%0Aapproach%20%28DP-WA%2A%29%20is%20derived%2C%20which%20fully%20takes%20into%20account%20all%20path%20gains%20and%0Adistance%20metrics%20within%20the%20DT.%20On%20the%20basis%20of%20these%20baselines%2C%20the%20role%20of%0AVLMs%20as%20an%20alternative%20assistant%20for%20path%20planning%20is%20investigated%2C%20and%20a%0Astrategic%20chain-of-thought%20tasking%20%28SCoTT%29%20approach%20is%20proposed.%20SCoTT%20divides%0Athe%20complex%20planning%20task%20into%20several%20subproblems%20and%20solves%20each%20with%0Aadvanced%20CoT%20prompting.%20Results%20show%20that%20SCoTT%20achieves%20very%20close%20average%0Apath%20gains%20compared%20to%20DP-WA%2A%20while%20at%20the%20same%20time%20yielding%20consistently%0Ashorter%20path%20lengths.%20The%20results%20also%20show%20that%20VLMs%20can%20be%20used%20to%20accelerate%0ADP-WA%2A%20by%20efficiently%20reducing%20the%20algorithm%27s%20search%20space%20and%20thus%20saving%20up%0Ato%2062%5C%25%20in%20execution%20time.%20This%20work%20underscores%20the%20potential%20of%20VLMs%20in%0Afuture%20digital%20systems%20as%20capable%20assistants%20for%20solving%20complex%20tasks%2C%20while%0Aenhancing%20user%20interaction%20and%20accelerating%20rapid%20prototyping%20under%20diverse%0Awireless%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18212v1&entry.124074799=Read"},
{"title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance", "author": "Yaxi Lu and Shenzhi Yang and Cheng Qian and Guirong Chen and Qinyu Luo and Yesai Wu and Huadong Wang and Xin Cong and Zhong Zhang and Yankai Lin and Weiwen Liu and Yasheng Wang and Zhiyuan Liu and Fangming Liu and Maosong Sun", "abstract": "  Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.\n", "link": "http://arxiv.org/abs/2410.12361v2", "date": "2024-11-27", "relevancy": 1.655, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5608}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proactive%20Agent%3A%20Shifting%20LLM%20Agents%20from%20Reactive%20Responses%20to%20Active%0A%20%20Assistance&body=Title%3A%20Proactive%20Agent%3A%20Shifting%20LLM%20Agents%20from%20Reactive%20Responses%20to%20Active%0A%20%20Assistance%0AAuthor%3A%20Yaxi%20Lu%20and%20Shenzhi%20Yang%20and%20Cheng%20Qian%20and%20Guirong%20Chen%20and%20Qinyu%20Luo%20and%20Yesai%20Wu%20and%20Huadong%20Wang%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yankai%20Lin%20and%20Weiwen%20Liu%20and%20Yasheng%20Wang%20and%20Zhiyuan%20Liu%20and%20Fangming%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Agents%20powered%20by%20large%20language%20models%20have%20shown%20remarkable%20abilities%20in%0Asolving%20complex%20tasks.%20However%2C%20most%20agent%20systems%20remain%20reactive%2C%20limiting%0Atheir%20effectiveness%20in%20scenarios%20requiring%20foresight%20and%20autonomous%0Adecision-making.%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20developing%20proactive%0Aagents%20capable%20of%20anticipating%20and%20initiating%20tasks%20without%20explicit%20human%0Ainstructions.%20We%20propose%20a%20novel%20data-driven%20approach%20for%20this%20problem.%0AFirstly%2C%20we%20collect%20real-world%20human%20activities%20to%20generate%20proactive%20task%0Apredictions.%20These%20predictions%20are%20then%20labeled%20by%20human%20annotators%20as%20either%0Aaccepted%20or%20rejected.%20The%20labeled%20data%20is%20used%20to%20train%20a%20reward%20model%20that%0Asimulates%20human%20judgment%20and%20serves%20as%20an%20automatic%20evaluator%20of%20the%0Aproactiveness%20of%20LLM%20agents.%20Building%20on%20this%2C%20we%20develop%20a%20comprehensive%20data%0Ageneration%20pipeline%20to%20create%20a%20diverse%20dataset%2C%20ProactiveBench%2C%20containing%0A6%2C790%20events.%20Finally%2C%20we%20demonstrate%20that%20fine-tuning%20models%20with%20the%20proposed%0AProactiveBench%20can%20significantly%20elicit%20the%20proactiveness%20of%20LLM%20agents.%0AExperimental%20results%20show%20that%20our%20fine-tuned%20model%20achieves%20an%20F1-Score%20of%0A66.47%25%20in%20proactively%20offering%20assistance%2C%20outperforming%20all%20open-source%20and%0Aclose-source%20models.%20These%20results%20highlight%20the%20potential%20of%20our%20method%20in%0Acreating%20more%20proactive%20and%20effective%20agent%20systems%2C%20paving%20the%20way%20for%20future%0Aadvancements%20in%20human-agent%20collaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactive%2520Agent%253A%2520Shifting%2520LLM%2520Agents%2520from%2520Reactive%2520Responses%2520to%2520Active%250A%2520%2520Assistance%26entry.906535625%3DYaxi%2520Lu%2520and%2520Shenzhi%2520Yang%2520and%2520Cheng%2520Qian%2520and%2520Guirong%2520Chen%2520and%2520Qinyu%2520Luo%2520and%2520Yesai%2520Wu%2520and%2520Huadong%2520Wang%2520and%2520Xin%2520Cong%2520and%2520Zhong%2520Zhang%2520and%2520Yankai%2520Lin%2520and%2520Weiwen%2520Liu%2520and%2520Yasheng%2520Wang%2520and%2520Zhiyuan%2520Liu%2520and%2520Fangming%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Agents%2520powered%2520by%2520large%2520language%2520models%2520have%2520shown%2520remarkable%2520abilities%2520in%250Asolving%2520complex%2520tasks.%2520However%252C%2520most%2520agent%2520systems%2520remain%2520reactive%252C%2520limiting%250Atheir%2520effectiveness%2520in%2520scenarios%2520requiring%2520foresight%2520and%2520autonomous%250Adecision-making.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520challenge%2520of%2520developing%2520proactive%250Aagents%2520capable%2520of%2520anticipating%2520and%2520initiating%2520tasks%2520without%2520explicit%2520human%250Ainstructions.%2520We%2520propose%2520a%2520novel%2520data-driven%2520approach%2520for%2520this%2520problem.%250AFirstly%252C%2520we%2520collect%2520real-world%2520human%2520activities%2520to%2520generate%2520proactive%2520task%250Apredictions.%2520These%2520predictions%2520are%2520then%2520labeled%2520by%2520human%2520annotators%2520as%2520either%250Aaccepted%2520or%2520rejected.%2520The%2520labeled%2520data%2520is%2520used%2520to%2520train%2520a%2520reward%2520model%2520that%250Asimulates%2520human%2520judgment%2520and%2520serves%2520as%2520an%2520automatic%2520evaluator%2520of%2520the%250Aproactiveness%2520of%2520LLM%2520agents.%2520Building%2520on%2520this%252C%2520we%2520develop%2520a%2520comprehensive%2520data%250Ageneration%2520pipeline%2520to%2520create%2520a%2520diverse%2520dataset%252C%2520ProactiveBench%252C%2520containing%250A6%252C790%2520events.%2520Finally%252C%2520we%2520demonstrate%2520that%2520fine-tuning%2520models%2520with%2520the%2520proposed%250AProactiveBench%2520can%2520significantly%2520elicit%2520the%2520proactiveness%2520of%2520LLM%2520agents.%250AExperimental%2520results%2520show%2520that%2520our%2520fine-tuned%2520model%2520achieves%2520an%2520F1-Score%2520of%250A66.47%2525%2520in%2520proactively%2520offering%2520assistance%252C%2520outperforming%2520all%2520open-source%2520and%250Aclose-source%2520models.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520our%2520method%2520in%250Acreating%2520more%2520proactive%2520and%2520effective%2520agent%2520systems%252C%2520paving%2520the%2520way%2520for%2520future%250Aadvancements%2520in%2520human-agent%2520collaboration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proactive%20Agent%3A%20Shifting%20LLM%20Agents%20from%20Reactive%20Responses%20to%20Active%0A%20%20Assistance&entry.906535625=Yaxi%20Lu%20and%20Shenzhi%20Yang%20and%20Cheng%20Qian%20and%20Guirong%20Chen%20and%20Qinyu%20Luo%20and%20Yesai%20Wu%20and%20Huadong%20Wang%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yankai%20Lin%20and%20Weiwen%20Liu%20and%20Yasheng%20Wang%20and%20Zhiyuan%20Liu%20and%20Fangming%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Agents%20powered%20by%20large%20language%20models%20have%20shown%20remarkable%20abilities%20in%0Asolving%20complex%20tasks.%20However%2C%20most%20agent%20systems%20remain%20reactive%2C%20limiting%0Atheir%20effectiveness%20in%20scenarios%20requiring%20foresight%20and%20autonomous%0Adecision-making.%20In%20this%20paper%2C%20we%20tackle%20the%20challenge%20of%20developing%20proactive%0Aagents%20capable%20of%20anticipating%20and%20initiating%20tasks%20without%20explicit%20human%0Ainstructions.%20We%20propose%20a%20novel%20data-driven%20approach%20for%20this%20problem.%0AFirstly%2C%20we%20collect%20real-world%20human%20activities%20to%20generate%20proactive%20task%0Apredictions.%20These%20predictions%20are%20then%20labeled%20by%20human%20annotators%20as%20either%0Aaccepted%20or%20rejected.%20The%20labeled%20data%20is%20used%20to%20train%20a%20reward%20model%20that%0Asimulates%20human%20judgment%20and%20serves%20as%20an%20automatic%20evaluator%20of%20the%0Aproactiveness%20of%20LLM%20agents.%20Building%20on%20this%2C%20we%20develop%20a%20comprehensive%20data%0Ageneration%20pipeline%20to%20create%20a%20diverse%20dataset%2C%20ProactiveBench%2C%20containing%0A6%2C790%20events.%20Finally%2C%20we%20demonstrate%20that%20fine-tuning%20models%20with%20the%20proposed%0AProactiveBench%20can%20significantly%20elicit%20the%20proactiveness%20of%20LLM%20agents.%0AExperimental%20results%20show%20that%20our%20fine-tuned%20model%20achieves%20an%20F1-Score%20of%0A66.47%25%20in%20proactively%20offering%20assistance%2C%20outperforming%20all%20open-source%20and%0Aclose-source%20models.%20These%20results%20highlight%20the%20potential%20of%20our%20method%20in%0Acreating%20more%20proactive%20and%20effective%20agent%20systems%2C%20paving%20the%20way%20for%20future%0Aadvancements%20in%20human-agent%20collaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12361v2&entry.124074799=Read"},
{"title": "LLM-ABBA: Understand time series via symbolic approximation", "author": "Erin Carson and Xinye Chen and Cheng Kang", "abstract": "  The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.\n", "link": "http://arxiv.org/abs/2411.18506v1", "date": "2024-11-27", "relevancy": 2.0082, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4992}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-ABBA%3A%20Understand%20time%20series%20via%20symbolic%20approximation&body=Title%3A%20LLM-ABBA%3A%20Understand%20time%20series%20via%20symbolic%20approximation%0AAuthor%3A%20Erin%20Carson%20and%20Xinye%20Chen%20and%20Cheng%20Kang%0AAbstract%3A%20%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20for%20time%20series%20has%20been%0Ademonstrated%20in%20previous%20work.%20Utilizing%20a%20symbolic%20time%20series%20representation%2C%0Aone%20can%20efficiently%20bridge%20the%20gap%20between%20LLMs%20and%20time%20series.%20However%2C%20the%0Aremaining%20challenge%20is%20to%20exploit%20the%20semantic%20information%20hidden%20in%20time%0Aseries%20by%20using%20symbols%20or%20existing%20tokens%20of%20LLMs%2C%20while%20aligning%20the%0Aembedding%20space%20of%20LLMs%20according%20to%20the%20hidden%20information%20of%20time%20series.%20The%0Asymbolic%20time%20series%20approximation%20%28STSA%29%20method%20called%20adaptive%20Brownian%0Abridge-based%20symbolic%20aggregation%20%28ABBA%29%20shows%20outstanding%20efficacy%20in%0Apreserving%20salient%20time%20series%20features%20by%20modeling%20time%20series%20patterns%20in%0Aterms%20of%20amplitude%20and%20period%20while%20using%20existing%20tokens%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%2C%20called%20LLM-ABBA%2C%20that%20integrates%20ABBA%0Ainto%20large%20language%20models%20for%20various%20downstream%20time%20series%20tasks.%20By%0Asymbolizing%20time%20series%2C%20LLM-ABBA%20compares%20favorably%20to%20the%20recent%0Astate-of-the-art%20%28SOTA%29%20in%20UCR%20and%20three%20medical%20time%20series%20classification%0Atasks.%20Meanwhile%2C%20a%20fixed-polygonal%20chain%20trick%20in%20ABBA%20is%20introduced%20to%0A%5Ckc%7Bavoid%20obvious%20drifting%7D%20during%20prediction%20tasks%20by%20significantly%20mitigating%0Athe%20effects%20of%20cumulative%20error%20arising%20from%20misused%20symbols%20during%20the%0Atransition%20from%20symbols%20to%20numerical%20values.%20In%20time%20series%20regression%20tasks%2C%0ALLM-ABBA%20achieves%20the%20new%20SOTA%20on%20Time%20Series%20Extrinsic%20Regression%20%28TSER%29%0Abenchmarks.%20LLM-ABBA%20also%20shows%20competitive%20prediction%20capability%20compared%20to%0Arecent%20SOTA%20time%20series%20prediction%20results.%20We%20believe%20this%20framework%20can%20also%0Aseamlessly%20extend%20to%20other%20time%20series%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-ABBA%253A%2520Understand%2520time%2520series%2520via%2520symbolic%2520approximation%26entry.906535625%3DErin%2520Carson%2520and%2520Xinye%2520Chen%2520and%2520Cheng%2520Kang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520time%2520series%2520has%2520been%250Ademonstrated%2520in%2520previous%2520work.%2520Utilizing%2520a%2520symbolic%2520time%2520series%2520representation%252C%250Aone%2520can%2520efficiently%2520bridge%2520the%2520gap%2520between%2520LLMs%2520and%2520time%2520series.%2520However%252C%2520the%250Aremaining%2520challenge%2520is%2520to%2520exploit%2520the%2520semantic%2520information%2520hidden%2520in%2520time%250Aseries%2520by%2520using%2520symbols%2520or%2520existing%2520tokens%2520of%2520LLMs%252C%2520while%2520aligning%2520the%250Aembedding%2520space%2520of%2520LLMs%2520according%2520to%2520the%2520hidden%2520information%2520of%2520time%2520series.%2520The%250Asymbolic%2520time%2520series%2520approximation%2520%2528STSA%2529%2520method%2520called%2520adaptive%2520Brownian%250Abridge-based%2520symbolic%2520aggregation%2520%2528ABBA%2529%2520shows%2520outstanding%2520efficacy%2520in%250Apreserving%2520salient%2520time%2520series%2520features%2520by%2520modeling%2520time%2520series%2520patterns%2520in%250Aterms%2520of%2520amplitude%2520and%2520period%2520while%2520using%2520existing%2520tokens%2520of%2520LLMs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%252C%2520called%2520LLM-ABBA%252C%2520that%2520integrates%2520ABBA%250Ainto%2520large%2520language%2520models%2520for%2520various%2520downstream%2520time%2520series%2520tasks.%2520By%250Asymbolizing%2520time%2520series%252C%2520LLM-ABBA%2520compares%2520favorably%2520to%2520the%2520recent%250Astate-of-the-art%2520%2528SOTA%2529%2520in%2520UCR%2520and%2520three%2520medical%2520time%2520series%2520classification%250Atasks.%2520Meanwhile%252C%2520a%2520fixed-polygonal%2520chain%2520trick%2520in%2520ABBA%2520is%2520introduced%2520to%250A%255Ckc%257Bavoid%2520obvious%2520drifting%257D%2520during%2520prediction%2520tasks%2520by%2520significantly%2520mitigating%250Athe%2520effects%2520of%2520cumulative%2520error%2520arising%2520from%2520misused%2520symbols%2520during%2520the%250Atransition%2520from%2520symbols%2520to%2520numerical%2520values.%2520In%2520time%2520series%2520regression%2520tasks%252C%250ALLM-ABBA%2520achieves%2520the%2520new%2520SOTA%2520on%2520Time%2520Series%2520Extrinsic%2520Regression%2520%2528TSER%2529%250Abenchmarks.%2520LLM-ABBA%2520also%2520shows%2520competitive%2520prediction%2520capability%2520compared%2520to%250Arecent%2520SOTA%2520time%2520series%2520prediction%2520results.%2520We%2520believe%2520this%2520framework%2520can%2520also%250Aseamlessly%2520extend%2520to%2520other%2520time%2520series%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-ABBA%3A%20Understand%20time%20series%20via%20symbolic%20approximation&entry.906535625=Erin%20Carson%20and%20Xinye%20Chen%20and%20Cheng%20Kang&entry.1292438233=%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20for%20time%20series%20has%20been%0Ademonstrated%20in%20previous%20work.%20Utilizing%20a%20symbolic%20time%20series%20representation%2C%0Aone%20can%20efficiently%20bridge%20the%20gap%20between%20LLMs%20and%20time%20series.%20However%2C%20the%0Aremaining%20challenge%20is%20to%20exploit%20the%20semantic%20information%20hidden%20in%20time%0Aseries%20by%20using%20symbols%20or%20existing%20tokens%20of%20LLMs%2C%20while%20aligning%20the%0Aembedding%20space%20of%20LLMs%20according%20to%20the%20hidden%20information%20of%20time%20series.%20The%0Asymbolic%20time%20series%20approximation%20%28STSA%29%20method%20called%20adaptive%20Brownian%0Abridge-based%20symbolic%20aggregation%20%28ABBA%29%20shows%20outstanding%20efficacy%20in%0Apreserving%20salient%20time%20series%20features%20by%20modeling%20time%20series%20patterns%20in%0Aterms%20of%20amplitude%20and%20period%20while%20using%20existing%20tokens%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%2C%20called%20LLM-ABBA%2C%20that%20integrates%20ABBA%0Ainto%20large%20language%20models%20for%20various%20downstream%20time%20series%20tasks.%20By%0Asymbolizing%20time%20series%2C%20LLM-ABBA%20compares%20favorably%20to%20the%20recent%0Astate-of-the-art%20%28SOTA%29%20in%20UCR%20and%20three%20medical%20time%20series%20classification%0Atasks.%20Meanwhile%2C%20a%20fixed-polygonal%20chain%20trick%20in%20ABBA%20is%20introduced%20to%0A%5Ckc%7Bavoid%20obvious%20drifting%7D%20during%20prediction%20tasks%20by%20significantly%20mitigating%0Athe%20effects%20of%20cumulative%20error%20arising%20from%20misused%20symbols%20during%20the%0Atransition%20from%20symbols%20to%20numerical%20values.%20In%20time%20series%20regression%20tasks%2C%0ALLM-ABBA%20achieves%20the%20new%20SOTA%20on%20Time%20Series%20Extrinsic%20Regression%20%28TSER%29%0Abenchmarks.%20LLM-ABBA%20also%20shows%20competitive%20prediction%20capability%20compared%20to%0Arecent%20SOTA%20time%20series%20prediction%20results.%20We%20believe%20this%20framework%20can%20also%0Aseamlessly%20extend%20to%20other%20time%20series%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18506v1&entry.124074799=Read"},
{"title": "Certified Training with Branch-and-Bound: A Case Study on\n  Lyapunov-stable Neural Control", "author": "Zhouxing Shi and Cho-Jui Hsieh and Huan Zhang", "abstract": "  We study the problem of learning Lyapunov-stable neural controllers which\nprovably satisfy the Lyapunov asymptotic stability condition within a\nregion-of-attraction. Compared to previous works which commonly used\ncounterexample guided training on this task, we develop a new and generally\nformulated certified training framework named CT-BaB, and we optimize for\ndifferentiable verified bounds, to produce verification-friendly models. In\norder to handle the relatively large region-of-interest, we propose a novel\nframework of training-time branch-and-bound to dynamically maintain a training\ndataset of subregions throughout training, such that the hardest subregions are\niteratively split into smaller ones whose verified bounds can be computed more\ntightly to ease the training. We demonstrate that our new training framework\ncan produce models which can be more efficiently verified at test time. On the\nlargest 2D quadrotor dynamical system, verification for our model is more than\n5X faster compared to the baseline, while our size of region-of-attraction is\n16X larger than the baseline.\n", "link": "http://arxiv.org/abs/2411.18235v1", "date": "2024-11-27", "relevancy": 2.1144, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Training%20with%20Branch-and-Bound%3A%20A%20Case%20Study%20on%0A%20%20Lyapunov-stable%20Neural%20Control&body=Title%3A%20Certified%20Training%20with%20Branch-and-Bound%3A%20A%20Case%20Study%20on%0A%20%20Lyapunov-stable%20Neural%20Control%0AAuthor%3A%20Zhouxing%20Shi%20and%20Cho-Jui%20Hsieh%20and%20Huan%20Zhang%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20Lyapunov-stable%20neural%20controllers%20which%0Aprovably%20satisfy%20the%20Lyapunov%20asymptotic%20stability%20condition%20within%20a%0Aregion-of-attraction.%20Compared%20to%20previous%20works%20which%20commonly%20used%0Acounterexample%20guided%20training%20on%20this%20task%2C%20we%20develop%20a%20new%20and%20generally%0Aformulated%20certified%20training%20framework%20named%20CT-BaB%2C%20and%20we%20optimize%20for%0Adifferentiable%20verified%20bounds%2C%20to%20produce%20verification-friendly%20models.%20In%0Aorder%20to%20handle%20the%20relatively%20large%20region-of-interest%2C%20we%20propose%20a%20novel%0Aframework%20of%20training-time%20branch-and-bound%20to%20dynamically%20maintain%20a%20training%0Adataset%20of%20subregions%20throughout%20training%2C%20such%20that%20the%20hardest%20subregions%20are%0Aiteratively%20split%20into%20smaller%20ones%20whose%20verified%20bounds%20can%20be%20computed%20more%0Atightly%20to%20ease%20the%20training.%20We%20demonstrate%20that%20our%20new%20training%20framework%0Acan%20produce%20models%20which%20can%20be%20more%20efficiently%20verified%20at%20test%20time.%20On%20the%0Alargest%202D%20quadrotor%20dynamical%20system%2C%20verification%20for%20our%20model%20is%20more%20than%0A5X%20faster%20compared%20to%20the%20baseline%2C%20while%20our%20size%20of%20region-of-attraction%20is%0A16X%20larger%20than%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Training%2520with%2520Branch-and-Bound%253A%2520A%2520Case%2520Study%2520on%250A%2520%2520Lyapunov-stable%2520Neural%2520Control%26entry.906535625%3DZhouxing%2520Shi%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Huan%2520Zhang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520Lyapunov-stable%2520neural%2520controllers%2520which%250Aprovably%2520satisfy%2520the%2520Lyapunov%2520asymptotic%2520stability%2520condition%2520within%2520a%250Aregion-of-attraction.%2520Compared%2520to%2520previous%2520works%2520which%2520commonly%2520used%250Acounterexample%2520guided%2520training%2520on%2520this%2520task%252C%2520we%2520develop%2520a%2520new%2520and%2520generally%250Aformulated%2520certified%2520training%2520framework%2520named%2520CT-BaB%252C%2520and%2520we%2520optimize%2520for%250Adifferentiable%2520verified%2520bounds%252C%2520to%2520produce%2520verification-friendly%2520models.%2520In%250Aorder%2520to%2520handle%2520the%2520relatively%2520large%2520region-of-interest%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520of%2520training-time%2520branch-and-bound%2520to%2520dynamically%2520maintain%2520a%2520training%250Adataset%2520of%2520subregions%2520throughout%2520training%252C%2520such%2520that%2520the%2520hardest%2520subregions%2520are%250Aiteratively%2520split%2520into%2520smaller%2520ones%2520whose%2520verified%2520bounds%2520can%2520be%2520computed%2520more%250Atightly%2520to%2520ease%2520the%2520training.%2520We%2520demonstrate%2520that%2520our%2520new%2520training%2520framework%250Acan%2520produce%2520models%2520which%2520can%2520be%2520more%2520efficiently%2520verified%2520at%2520test%2520time.%2520On%2520the%250Alargest%25202D%2520quadrotor%2520dynamical%2520system%252C%2520verification%2520for%2520our%2520model%2520is%2520more%2520than%250A5X%2520faster%2520compared%2520to%2520the%2520baseline%252C%2520while%2520our%2520size%2520of%2520region-of-attraction%2520is%250A16X%2520larger%2520than%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Training%20with%20Branch-and-Bound%3A%20A%20Case%20Study%20on%0A%20%20Lyapunov-stable%20Neural%20Control&entry.906535625=Zhouxing%20Shi%20and%20Cho-Jui%20Hsieh%20and%20Huan%20Zhang&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20Lyapunov-stable%20neural%20controllers%20which%0Aprovably%20satisfy%20the%20Lyapunov%20asymptotic%20stability%20condition%20within%20a%0Aregion-of-attraction.%20Compared%20to%20previous%20works%20which%20commonly%20used%0Acounterexample%20guided%20training%20on%20this%20task%2C%20we%20develop%20a%20new%20and%20generally%0Aformulated%20certified%20training%20framework%20named%20CT-BaB%2C%20and%20we%20optimize%20for%0Adifferentiable%20verified%20bounds%2C%20to%20produce%20verification-friendly%20models.%20In%0Aorder%20to%20handle%20the%20relatively%20large%20region-of-interest%2C%20we%20propose%20a%20novel%0Aframework%20of%20training-time%20branch-and-bound%20to%20dynamically%20maintain%20a%20training%0Adataset%20of%20subregions%20throughout%20training%2C%20such%20that%20the%20hardest%20subregions%20are%0Aiteratively%20split%20into%20smaller%20ones%20whose%20verified%20bounds%20can%20be%20computed%20more%0Atightly%20to%20ease%20the%20training.%20We%20demonstrate%20that%20our%20new%20training%20framework%0Acan%20produce%20models%20which%20can%20be%20more%20efficiently%20verified%20at%20test%20time.%20On%20the%0Alargest%202D%20quadrotor%20dynamical%20system%2C%20verification%20for%20our%20model%20is%20more%20than%0A5X%20faster%20compared%20to%20the%20baseline%2C%20while%20our%20size%20of%20region-of-attraction%20is%0A16X%20larger%20than%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18235v1&entry.124074799=Read"},
{"title": "EventCrab: Harnessing Frame and Point Synergy for Event-based Action\n  Recognition and Beyond", "author": "Meiqi Cao and Xiangbo Shu and Jiachao Zhang and Rui Yan and Zechao Li and Jinhui Tang", "abstract": "  Event-based Action Recognition (EAR) possesses the advantages of\nhigh-temporal resolution capturing and privacy preservation compared with\ntraditional action recognition. Current leading EAR solutions typically follow\ntwo regimes: project unconstructed event streams into dense constructed event\nframes and adopt powerful frame-specific networks, or employ lightweight\npoint-specific networks to handle sparse unconstructed event points directly.\nHowever, such two regimes are blind to a fundamental issue: failing to\naccommodate the unique dense temporal and sparse spatial properties of\nasynchronous event data. In this article, we present a synergy-aware framework,\ni.e., EventCrab, that adeptly integrates the \"lighter\" frame-specific networks\nfor dense event frames with the \"heavier\" point-specific networks for sparse\nevent points, balancing accuracy and efficiency. Furthermore, we establish a\njoint frame-text-point representation space to bridge distinct event frames and\npoints. In specific, to better exploit the unique spatiotemporal relationships\ninherent in asynchronous event points, we devise two strategies for the\n\"heavier\" point-specific embedding: i) a Spiking-like Context Learner (SCL)\nthat extracts contextualized event points from raw event streams. ii) an Event\nPoint Encoder (EPE) that further explores event-point long spatiotemporal\nfeatures in a Hilbert-scan way. Experiments on four datasets demonstrate the\nsignificant performance of our proposed EventCrab, particularly gaining\nimprovements of 5.17% on SeAct and 7.01% on HARDVS.\n", "link": "http://arxiv.org/abs/2411.18328v1", "date": "2024-11-27", "relevancy": 1.9301, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4844}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventCrab%3A%20Harnessing%20Frame%20and%20Point%20Synergy%20for%20Event-based%20Action%0A%20%20Recognition%20and%20Beyond&body=Title%3A%20EventCrab%3A%20Harnessing%20Frame%20and%20Point%20Synergy%20for%20Event-based%20Action%0A%20%20Recognition%20and%20Beyond%0AAuthor%3A%20Meiqi%20Cao%20and%20Xiangbo%20Shu%20and%20Jiachao%20Zhang%20and%20Rui%20Yan%20and%20Zechao%20Li%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Event-based%20Action%20Recognition%20%28EAR%29%20possesses%20the%20advantages%20of%0Ahigh-temporal%20resolution%20capturing%20and%20privacy%20preservation%20compared%20with%0Atraditional%20action%20recognition.%20Current%20leading%20EAR%20solutions%20typically%20follow%0Atwo%20regimes%3A%20project%20unconstructed%20event%20streams%20into%20dense%20constructed%20event%0Aframes%20and%20adopt%20powerful%20frame-specific%20networks%2C%20or%20employ%20lightweight%0Apoint-specific%20networks%20to%20handle%20sparse%20unconstructed%20event%20points%20directly.%0AHowever%2C%20such%20two%20regimes%20are%20blind%20to%20a%20fundamental%20issue%3A%20failing%20to%0Aaccommodate%20the%20unique%20dense%20temporal%20and%20sparse%20spatial%20properties%20of%0Aasynchronous%20event%20data.%20In%20this%20article%2C%20we%20present%20a%20synergy-aware%20framework%2C%0Ai.e.%2C%20EventCrab%2C%20that%20adeptly%20integrates%20the%20%22lighter%22%20frame-specific%20networks%0Afor%20dense%20event%20frames%20with%20the%20%22heavier%22%20point-specific%20networks%20for%20sparse%0Aevent%20points%2C%20balancing%20accuracy%20and%20efficiency.%20Furthermore%2C%20we%20establish%20a%0Ajoint%20frame-text-point%20representation%20space%20to%20bridge%20distinct%20event%20frames%20and%0Apoints.%20In%20specific%2C%20to%20better%20exploit%20the%20unique%20spatiotemporal%20relationships%0Ainherent%20in%20asynchronous%20event%20points%2C%20we%20devise%20two%20strategies%20for%20the%0A%22heavier%22%20point-specific%20embedding%3A%20i%29%20a%20Spiking-like%20Context%20Learner%20%28SCL%29%0Athat%20extracts%20contextualized%20event%20points%20from%20raw%20event%20streams.%20ii%29%20an%20Event%0APoint%20Encoder%20%28EPE%29%20that%20further%20explores%20event-point%20long%20spatiotemporal%0Afeatures%20in%20a%20Hilbert-scan%20way.%20Experiments%20on%20four%20datasets%20demonstrate%20the%0Asignificant%20performance%20of%20our%20proposed%20EventCrab%2C%20particularly%20gaining%0Aimprovements%20of%205.17%25%20on%20SeAct%20and%207.01%25%20on%20HARDVS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventCrab%253A%2520Harnessing%2520Frame%2520and%2520Point%2520Synergy%2520for%2520Event-based%2520Action%250A%2520%2520Recognition%2520and%2520Beyond%26entry.906535625%3DMeiqi%2520Cao%2520and%2520Xiangbo%2520Shu%2520and%2520Jiachao%2520Zhang%2520and%2520Rui%2520Yan%2520and%2520Zechao%2520Li%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Event-based%2520Action%2520Recognition%2520%2528EAR%2529%2520possesses%2520the%2520advantages%2520of%250Ahigh-temporal%2520resolution%2520capturing%2520and%2520privacy%2520preservation%2520compared%2520with%250Atraditional%2520action%2520recognition.%2520Current%2520leading%2520EAR%2520solutions%2520typically%2520follow%250Atwo%2520regimes%253A%2520project%2520unconstructed%2520event%2520streams%2520into%2520dense%2520constructed%2520event%250Aframes%2520and%2520adopt%2520powerful%2520frame-specific%2520networks%252C%2520or%2520employ%2520lightweight%250Apoint-specific%2520networks%2520to%2520handle%2520sparse%2520unconstructed%2520event%2520points%2520directly.%250AHowever%252C%2520such%2520two%2520regimes%2520are%2520blind%2520to%2520a%2520fundamental%2520issue%253A%2520failing%2520to%250Aaccommodate%2520the%2520unique%2520dense%2520temporal%2520and%2520sparse%2520spatial%2520properties%2520of%250Aasynchronous%2520event%2520data.%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520synergy-aware%2520framework%252C%250Ai.e.%252C%2520EventCrab%252C%2520that%2520adeptly%2520integrates%2520the%2520%2522lighter%2522%2520frame-specific%2520networks%250Afor%2520dense%2520event%2520frames%2520with%2520the%2520%2522heavier%2522%2520point-specific%2520networks%2520for%2520sparse%250Aevent%2520points%252C%2520balancing%2520accuracy%2520and%2520efficiency.%2520Furthermore%252C%2520we%2520establish%2520a%250Ajoint%2520frame-text-point%2520representation%2520space%2520to%2520bridge%2520distinct%2520event%2520frames%2520and%250Apoints.%2520In%2520specific%252C%2520to%2520better%2520exploit%2520the%2520unique%2520spatiotemporal%2520relationships%250Ainherent%2520in%2520asynchronous%2520event%2520points%252C%2520we%2520devise%2520two%2520strategies%2520for%2520the%250A%2522heavier%2522%2520point-specific%2520embedding%253A%2520i%2529%2520a%2520Spiking-like%2520Context%2520Learner%2520%2528SCL%2529%250Athat%2520extracts%2520contextualized%2520event%2520points%2520from%2520raw%2520event%2520streams.%2520ii%2529%2520an%2520Event%250APoint%2520Encoder%2520%2528EPE%2529%2520that%2520further%2520explores%2520event-point%2520long%2520spatiotemporal%250Afeatures%2520in%2520a%2520Hilbert-scan%2520way.%2520Experiments%2520on%2520four%2520datasets%2520demonstrate%2520the%250Asignificant%2520performance%2520of%2520our%2520proposed%2520EventCrab%252C%2520particularly%2520gaining%250Aimprovements%2520of%25205.17%2525%2520on%2520SeAct%2520and%25207.01%2525%2520on%2520HARDVS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventCrab%3A%20Harnessing%20Frame%20and%20Point%20Synergy%20for%20Event-based%20Action%0A%20%20Recognition%20and%20Beyond&entry.906535625=Meiqi%20Cao%20and%20Xiangbo%20Shu%20and%20Jiachao%20Zhang%20and%20Rui%20Yan%20and%20Zechao%20Li%20and%20Jinhui%20Tang&entry.1292438233=%20%20Event-based%20Action%20Recognition%20%28EAR%29%20possesses%20the%20advantages%20of%0Ahigh-temporal%20resolution%20capturing%20and%20privacy%20preservation%20compared%20with%0Atraditional%20action%20recognition.%20Current%20leading%20EAR%20solutions%20typically%20follow%0Atwo%20regimes%3A%20project%20unconstructed%20event%20streams%20into%20dense%20constructed%20event%0Aframes%20and%20adopt%20powerful%20frame-specific%20networks%2C%20or%20employ%20lightweight%0Apoint-specific%20networks%20to%20handle%20sparse%20unconstructed%20event%20points%20directly.%0AHowever%2C%20such%20two%20regimes%20are%20blind%20to%20a%20fundamental%20issue%3A%20failing%20to%0Aaccommodate%20the%20unique%20dense%20temporal%20and%20sparse%20spatial%20properties%20of%0Aasynchronous%20event%20data.%20In%20this%20article%2C%20we%20present%20a%20synergy-aware%20framework%2C%0Ai.e.%2C%20EventCrab%2C%20that%20adeptly%20integrates%20the%20%22lighter%22%20frame-specific%20networks%0Afor%20dense%20event%20frames%20with%20the%20%22heavier%22%20point-specific%20networks%20for%20sparse%0Aevent%20points%2C%20balancing%20accuracy%20and%20efficiency.%20Furthermore%2C%20we%20establish%20a%0Ajoint%20frame-text-point%20representation%20space%20to%20bridge%20distinct%20event%20frames%20and%0Apoints.%20In%20specific%2C%20to%20better%20exploit%20the%20unique%20spatiotemporal%20relationships%0Ainherent%20in%20asynchronous%20event%20points%2C%20we%20devise%20two%20strategies%20for%20the%0A%22heavier%22%20point-specific%20embedding%3A%20i%29%20a%20Spiking-like%20Context%20Learner%20%28SCL%29%0Athat%20extracts%20contextualized%20event%20points%20from%20raw%20event%20streams.%20ii%29%20an%20Event%0APoint%20Encoder%20%28EPE%29%20that%20further%20explores%20event-point%20long%20spatiotemporal%0Afeatures%20in%20a%20Hilbert-scan%20way.%20Experiments%20on%20four%20datasets%20demonstrate%20the%0Asignificant%20performance%20of%20our%20proposed%20EventCrab%2C%20particularly%20gaining%0Aimprovements%20of%205.17%25%20on%20SeAct%20and%207.01%25%20on%20HARDVS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18328v1&entry.124074799=Read"},
{"title": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems", "author": "Danial Ebrat and Eli Paradalis and Luis Rueda", "abstract": "  Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.\n", "link": "http://arxiv.org/abs/2405.13362v2", "date": "2024-11-27", "relevancy": 1.3971, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4718}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4673}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lusifer%3A%20LLM-based%20User%20SImulated%20Feedback%20Environment%20for%20online%0A%20%20Recommender%20systems&body=Title%3A%20Lusifer%3A%20LLM-based%20User%20SImulated%20Feedback%20Environment%20for%20online%0A%20%20Recommender%20systems%0AAuthor%3A%20Danial%20Ebrat%20and%20Eli%20Paradalis%20and%20Luis%20Rueda%0AAbstract%3A%20%20%20Training%20reinforcement%20learning-based%20recommender%20systems%20is%20often%20hindered%0Aby%20the%20lack%20of%20dynamic%20and%20realistic%20user%20interactions.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20Lusifer%2C%20a%20novel%20environment%20leveraging%20Large%20Language%0AModels%20%28LLMs%29%20to%20generate%20simulated%20user%20feedback.%20Lusifer%20synthesizes%20user%0Aprofiles%20and%20interaction%20histories%20to%20simulate%20responses%20and%20behaviors%20toward%0Arecommended%20items%2C%20with%20profiles%20updated%20after%20each%20rating%20to%20reflect%20evolving%0Auser%20characteristics.%20Utilizing%20the%20MovieLens%20dataset%20as%20a%20proof%20of%20concept%2C%20we%0Alimited%20our%20implementation%20to%20the%20last%2040%20interactions%20for%20each%20user%2C%0Arepresenting%20approximately%2039%25%20and%2022%25%20of%20the%20training%20sets%2C%20to%20focus%20on%20recent%0Auser%20behavior.%20For%20consistency%20and%20to%20gain%20insights%20into%20the%20performance%20of%0Atraditional%20methods%20with%20limited%20data%2C%20we%20implemented%20baseline%20approaches%20using%0Athe%20same%20data%20subset.%20Our%20results%20demonstrate%20that%20Lusifer%20accurately%20emulates%0Auser%20behavior%20and%20preferences%2C%20even%20with%20reduced%20training%20data%20having%20an%20RMSE%0Aof%201.3%20across%20various%20test%20sets.%20This%20paper%20presents%20Lusifer%27s%20operational%0Apipeline%2C%20including%20prompt%20generation%20and%20iterative%20user%20profile%20updates%2C%20and%0Acompares%20its%20performance%20against%20baseline%20methods.%20The%20findings%20validate%0ALusifer%27s%20ability%20to%20produce%20realistic%20dynamic%20feedback%20and%20suggest%20that%20it%0Aoffers%20a%20scalable%20and%20adjustable%20framework%20for%20user%20simulation%20in%20online%0Areinforcement%20learning%20recommender%20systems%20for%20future%20studies%2C%20particularly%0Awhen%20training%20data%20is%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLusifer%253A%2520LLM-based%2520User%2520SImulated%2520Feedback%2520Environment%2520for%2520online%250A%2520%2520Recommender%2520systems%26entry.906535625%3DDanial%2520Ebrat%2520and%2520Eli%2520Paradalis%2520and%2520Luis%2520Rueda%26entry.1292438233%3D%2520%2520Training%2520reinforcement%2520learning-based%2520recommender%2520systems%2520is%2520often%2520hindered%250Aby%2520the%2520lack%2520of%2520dynamic%2520and%2520realistic%2520user%2520interactions.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520Lusifer%252C%2520a%2520novel%2520environment%2520leveraging%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520generate%2520simulated%2520user%2520feedback.%2520Lusifer%2520synthesizes%2520user%250Aprofiles%2520and%2520interaction%2520histories%2520to%2520simulate%2520responses%2520and%2520behaviors%2520toward%250Arecommended%2520items%252C%2520with%2520profiles%2520updated%2520after%2520each%2520rating%2520to%2520reflect%2520evolving%250Auser%2520characteristics.%2520Utilizing%2520the%2520MovieLens%2520dataset%2520as%2520a%2520proof%2520of%2520concept%252C%2520we%250Alimited%2520our%2520implementation%2520to%2520the%2520last%252040%2520interactions%2520for%2520each%2520user%252C%250Arepresenting%2520approximately%252039%2525%2520and%252022%2525%2520of%2520the%2520training%2520sets%252C%2520to%2520focus%2520on%2520recent%250Auser%2520behavior.%2520For%2520consistency%2520and%2520to%2520gain%2520insights%2520into%2520the%2520performance%2520of%250Atraditional%2520methods%2520with%2520limited%2520data%252C%2520we%2520implemented%2520baseline%2520approaches%2520using%250Athe%2520same%2520data%2520subset.%2520Our%2520results%2520demonstrate%2520that%2520Lusifer%2520accurately%2520emulates%250Auser%2520behavior%2520and%2520preferences%252C%2520even%2520with%2520reduced%2520training%2520data%2520having%2520an%2520RMSE%250Aof%25201.3%2520across%2520various%2520test%2520sets.%2520This%2520paper%2520presents%2520Lusifer%2527s%2520operational%250Apipeline%252C%2520including%2520prompt%2520generation%2520and%2520iterative%2520user%2520profile%2520updates%252C%2520and%250Acompares%2520its%2520performance%2520against%2520baseline%2520methods.%2520The%2520findings%2520validate%250ALusifer%2527s%2520ability%2520to%2520produce%2520realistic%2520dynamic%2520feedback%2520and%2520suggest%2520that%2520it%250Aoffers%2520a%2520scalable%2520and%2520adjustable%2520framework%2520for%2520user%2520simulation%2520in%2520online%250Areinforcement%2520learning%2520recommender%2520systems%2520for%2520future%2520studies%252C%2520particularly%250Awhen%2520training%2520data%2520is%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lusifer%3A%20LLM-based%20User%20SImulated%20Feedback%20Environment%20for%20online%0A%20%20Recommender%20systems&entry.906535625=Danial%20Ebrat%20and%20Eli%20Paradalis%20and%20Luis%20Rueda&entry.1292438233=%20%20Training%20reinforcement%20learning-based%20recommender%20systems%20is%20often%20hindered%0Aby%20the%20lack%20of%20dynamic%20and%20realistic%20user%20interactions.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20Lusifer%2C%20a%20novel%20environment%20leveraging%20Large%20Language%0AModels%20%28LLMs%29%20to%20generate%20simulated%20user%20feedback.%20Lusifer%20synthesizes%20user%0Aprofiles%20and%20interaction%20histories%20to%20simulate%20responses%20and%20behaviors%20toward%0Arecommended%20items%2C%20with%20profiles%20updated%20after%20each%20rating%20to%20reflect%20evolving%0Auser%20characteristics.%20Utilizing%20the%20MovieLens%20dataset%20as%20a%20proof%20of%20concept%2C%20we%0Alimited%20our%20implementation%20to%20the%20last%2040%20interactions%20for%20each%20user%2C%0Arepresenting%20approximately%2039%25%20and%2022%25%20of%20the%20training%20sets%2C%20to%20focus%20on%20recent%0Auser%20behavior.%20For%20consistency%20and%20to%20gain%20insights%20into%20the%20performance%20of%0Atraditional%20methods%20with%20limited%20data%2C%20we%20implemented%20baseline%20approaches%20using%0Athe%20same%20data%20subset.%20Our%20results%20demonstrate%20that%20Lusifer%20accurately%20emulates%0Auser%20behavior%20and%20preferences%2C%20even%20with%20reduced%20training%20data%20having%20an%20RMSE%0Aof%201.3%20across%20various%20test%20sets.%20This%20paper%20presents%20Lusifer%27s%20operational%0Apipeline%2C%20including%20prompt%20generation%20and%20iterative%20user%20profile%20updates%2C%20and%0Acompares%20its%20performance%20against%20baseline%20methods.%20The%20findings%20validate%0ALusifer%27s%20ability%20to%20produce%20realistic%20dynamic%20feedback%20and%20suggest%20that%20it%0Aoffers%20a%20scalable%20and%20adjustable%20framework%20for%20user%20simulation%20in%20online%0Areinforcement%20learning%20recommender%20systems%20for%20future%20studies%2C%20particularly%0Awhen%20training%20data%20is%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13362v2&entry.124074799=Read"},
{"title": "Dynamic Throwing with Robotic Material Handling Machines", "author": "Lennart Werner and Fang Nan and Pol Eyschen and Filippo A. Spinelli and Hongyi Yang and Marco Hutter", "abstract": "  Automation of hydraulic material handling machinery is currently limited to\nsemi-static pick-and-place cycles. Dynamic throwing motions which utilize the\npassive joints, can greatly improve time efficiency as well as increase the\ndumping workspace. In this work, we use Reinforcement Learning (RL) to design\ndynamic controllers for material handlers with underactuated arms as commonly\nused in logistics. The controllers are tested both in simulation and in\nreal-world experiments on a 12-ton test platform. The method is able to exploit\nthe passive joints of the gripper to perform dynamic throwing motions. With the\nproposed controllers, the machine is able to throw individual objects to\ntargets outside the static reachability zone with good accuracy for its\npractical applications. The work demonstrates the possibility of using RL to\nperform highly dynamic tasks with heavy machinery, suggesting a potential for\nimproving the efficiency and precision of autonomous material handling tasks.\n", "link": "http://arxiv.org/abs/2405.19001v3", "date": "2024-11-27", "relevancy": 1.5581, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5509}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines&body=Title%3A%20Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines%0AAuthor%3A%20Lennart%20Werner%20and%20Fang%20Nan%20and%20Pol%20Eyschen%20and%20Filippo%20A.%20Spinelli%20and%20Hongyi%20Yang%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Automation%20of%20hydraulic%20material%20handling%20machinery%20is%20currently%20limited%20to%0Asemi-static%20pick-and-place%20cycles.%20Dynamic%20throwing%20motions%20which%20utilize%20the%0Apassive%20joints%2C%20can%20greatly%20improve%20time%20efficiency%20as%20well%20as%20increase%20the%0Adumping%20workspace.%20In%20this%20work%2C%20we%20use%20Reinforcement%20Learning%20%28RL%29%20to%20design%0Adynamic%20controllers%20for%20material%20handlers%20with%20underactuated%20arms%20as%20commonly%0Aused%20in%20logistics.%20The%20controllers%20are%20tested%20both%20in%20simulation%20and%20in%0Areal-world%20experiments%20on%20a%2012-ton%20test%20platform.%20The%20method%20is%20able%20to%20exploit%0Athe%20passive%20joints%20of%20the%20gripper%20to%20perform%20dynamic%20throwing%20motions.%20With%20the%0Aproposed%20controllers%2C%20the%20machine%20is%20able%20to%20throw%20individual%20objects%20to%0Atargets%20outside%20the%20static%20reachability%20zone%20with%20good%20accuracy%20for%20its%0Apractical%20applications.%20The%20work%20demonstrates%20the%20possibility%20of%20using%20RL%20to%0Aperform%20highly%20dynamic%20tasks%20with%20heavy%20machinery%2C%20suggesting%20a%20potential%20for%0Aimproving%20the%20efficiency%20and%20precision%20of%20autonomous%20material%20handling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Throwing%2520with%2520Robotic%2520Material%2520Handling%2520Machines%26entry.906535625%3DLennart%2520Werner%2520and%2520Fang%2520Nan%2520and%2520Pol%2520Eyschen%2520and%2520Filippo%2520A.%2520Spinelli%2520and%2520Hongyi%2520Yang%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Automation%2520of%2520hydraulic%2520material%2520handling%2520machinery%2520is%2520currently%2520limited%2520to%250Asemi-static%2520pick-and-place%2520cycles.%2520Dynamic%2520throwing%2520motions%2520which%2520utilize%2520the%250Apassive%2520joints%252C%2520can%2520greatly%2520improve%2520time%2520efficiency%2520as%2520well%2520as%2520increase%2520the%250Adumping%2520workspace.%2520In%2520this%2520work%252C%2520we%2520use%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520design%250Adynamic%2520controllers%2520for%2520material%2520handlers%2520with%2520underactuated%2520arms%2520as%2520commonly%250Aused%2520in%2520logistics.%2520The%2520controllers%2520are%2520tested%2520both%2520in%2520simulation%2520and%2520in%250Areal-world%2520experiments%2520on%2520a%252012-ton%2520test%2520platform.%2520The%2520method%2520is%2520able%2520to%2520exploit%250Athe%2520passive%2520joints%2520of%2520the%2520gripper%2520to%2520perform%2520dynamic%2520throwing%2520motions.%2520With%2520the%250Aproposed%2520controllers%252C%2520the%2520machine%2520is%2520able%2520to%2520throw%2520individual%2520objects%2520to%250Atargets%2520outside%2520the%2520static%2520reachability%2520zone%2520with%2520good%2520accuracy%2520for%2520its%250Apractical%2520applications.%2520The%2520work%2520demonstrates%2520the%2520possibility%2520of%2520using%2520RL%2520to%250Aperform%2520highly%2520dynamic%2520tasks%2520with%2520heavy%2520machinery%252C%2520suggesting%2520a%2520potential%2520for%250Aimproving%2520the%2520efficiency%2520and%2520precision%2520of%2520autonomous%2520material%2520handling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Throwing%20with%20Robotic%20Material%20Handling%20Machines&entry.906535625=Lennart%20Werner%20and%20Fang%20Nan%20and%20Pol%20Eyschen%20and%20Filippo%20A.%20Spinelli%20and%20Hongyi%20Yang%20and%20Marco%20Hutter&entry.1292438233=%20%20Automation%20of%20hydraulic%20material%20handling%20machinery%20is%20currently%20limited%20to%0Asemi-static%20pick-and-place%20cycles.%20Dynamic%20throwing%20motions%20which%20utilize%20the%0Apassive%20joints%2C%20can%20greatly%20improve%20time%20efficiency%20as%20well%20as%20increase%20the%0Adumping%20workspace.%20In%20this%20work%2C%20we%20use%20Reinforcement%20Learning%20%28RL%29%20to%20design%0Adynamic%20controllers%20for%20material%20handlers%20with%20underactuated%20arms%20as%20commonly%0Aused%20in%20logistics.%20The%20controllers%20are%20tested%20both%20in%20simulation%20and%20in%0Areal-world%20experiments%20on%20a%2012-ton%20test%20platform.%20The%20method%20is%20able%20to%20exploit%0Athe%20passive%20joints%20of%20the%20gripper%20to%20perform%20dynamic%20throwing%20motions.%20With%20the%0Aproposed%20controllers%2C%20the%20machine%20is%20able%20to%20throw%20individual%20objects%20to%0Atargets%20outside%20the%20static%20reachability%20zone%20with%20good%20accuracy%20for%20its%0Apractical%20applications.%20The%20work%20demonstrates%20the%20possibility%20of%20using%20RL%20to%0Aperform%20highly%20dynamic%20tasks%20with%20heavy%20machinery%2C%20suggesting%20a%20potential%20for%0Aimproving%20the%20efficiency%20and%20precision%20of%20autonomous%20material%20handling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19001v3&entry.124074799=Read"},
{"title": "ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with\n  Deep Learning and Aerial Imagery", "author": "Gyri Reiersen and David Dao and Bj\u00f6rn L\u00fctjens and Konstantin Klemmer and Kenza Amara and Attila Steinegger and Ce Zhang and Xiaoxiang Zhu", "abstract": "  Forest biomass is a key influence for future climate, and the world urgently\nneeds highly scalable financing schemes, such as carbon offsetting\ncertifications, to protect and restore forests. Current manual forest carbon\nstock inventory methods of measuring single trees by hand are time, labour, and\ncost-intensive and have been shown to be subjective. They can lead to\nsubstantial overestimation of the carbon stock and ultimately distrust in\nforest financing. The potential for impact and scale of leveraging advancements\nin machine learning and remote sensing technologies is promising but needs to\nbe of high quality in order to replace the current forest stock protocols for\ncertifications.\n  In this paper, we present ReforesTree, a benchmark dataset of forest carbon\nstock in six agro-forestry carbon offsetting sites in Ecuador. Furthermore, we\nshow that a deep learning-based end-to-end model using individual tree\ndetection from low cost RGB-only drone imagery is accurately estimating forest\ncarbon stock within official carbon offsetting certification standards.\nAdditionally, our baseline CNN model outperforms state-of-the-art\nsatellite-based forest biomass and carbon stock estimates for this type of\nsmall-scale, tropical agro-forestry sites. We present this dataset to encourage\nmachine learning research in this area to increase accountability and\ntransparency of monitoring, verification and reporting (MVR) in carbon\noffsetting projects, as well as scaling global reforestation financing through\naccurate remote sensing.\n", "link": "http://arxiv.org/abs/2201.11192v2", "date": "2024-11-27", "relevancy": 1.3288, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4446}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReforesTree%3A%20A%20Dataset%20for%20Estimating%20Tropical%20Forest%20Carbon%20Stock%20with%0A%20%20Deep%20Learning%20and%20Aerial%20Imagery&body=Title%3A%20ReforesTree%3A%20A%20Dataset%20for%20Estimating%20Tropical%20Forest%20Carbon%20Stock%20with%0A%20%20Deep%20Learning%20and%20Aerial%20Imagery%0AAuthor%3A%20Gyri%20Reiersen%20and%20David%20Dao%20and%20Bj%C3%B6rn%20L%C3%BCtjens%20and%20Konstantin%20Klemmer%20and%20Kenza%20Amara%20and%20Attila%20Steinegger%20and%20Ce%20Zhang%20and%20Xiaoxiang%20Zhu%0AAbstract%3A%20%20%20Forest%20biomass%20is%20a%20key%20influence%20for%20future%20climate%2C%20and%20the%20world%20urgently%0Aneeds%20highly%20scalable%20financing%20schemes%2C%20such%20as%20carbon%20offsetting%0Acertifications%2C%20to%20protect%20and%20restore%20forests.%20Current%20manual%20forest%20carbon%0Astock%20inventory%20methods%20of%20measuring%20single%20trees%20by%20hand%20are%20time%2C%20labour%2C%20and%0Acost-intensive%20and%20have%20been%20shown%20to%20be%20subjective.%20They%20can%20lead%20to%0Asubstantial%20overestimation%20of%20the%20carbon%20stock%20and%20ultimately%20distrust%20in%0Aforest%20financing.%20The%20potential%20for%20impact%20and%20scale%20of%20leveraging%20advancements%0Ain%20machine%20learning%20and%20remote%20sensing%20technologies%20is%20promising%20but%20needs%20to%0Abe%20of%20high%20quality%20in%20order%20to%20replace%20the%20current%20forest%20stock%20protocols%20for%0Acertifications.%0A%20%20In%20this%20paper%2C%20we%20present%20ReforesTree%2C%20a%20benchmark%20dataset%20of%20forest%20carbon%0Astock%20in%20six%20agro-forestry%20carbon%20offsetting%20sites%20in%20Ecuador.%20Furthermore%2C%20we%0Ashow%20that%20a%20deep%20learning-based%20end-to-end%20model%20using%20individual%20tree%0Adetection%20from%20low%20cost%20RGB-only%20drone%20imagery%20is%20accurately%20estimating%20forest%0Acarbon%20stock%20within%20official%20carbon%20offsetting%20certification%20standards.%0AAdditionally%2C%20our%20baseline%20CNN%20model%20outperforms%20state-of-the-art%0Asatellite-based%20forest%20biomass%20and%20carbon%20stock%20estimates%20for%20this%20type%20of%0Asmall-scale%2C%20tropical%20agro-forestry%20sites.%20We%20present%20this%20dataset%20to%20encourage%0Amachine%20learning%20research%20in%20this%20area%20to%20increase%20accountability%20and%0Atransparency%20of%20monitoring%2C%20verification%20and%20reporting%20%28MVR%29%20in%20carbon%0Aoffsetting%20projects%2C%20as%20well%20as%20scaling%20global%20reforestation%20financing%20through%0Aaccurate%20remote%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.11192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReforesTree%253A%2520A%2520Dataset%2520for%2520Estimating%2520Tropical%2520Forest%2520Carbon%2520Stock%2520with%250A%2520%2520Deep%2520Learning%2520and%2520Aerial%2520Imagery%26entry.906535625%3DGyri%2520Reiersen%2520and%2520David%2520Dao%2520and%2520Bj%25C3%25B6rn%2520L%25C3%25BCtjens%2520and%2520Konstantin%2520Klemmer%2520and%2520Kenza%2520Amara%2520and%2520Attila%2520Steinegger%2520and%2520Ce%2520Zhang%2520and%2520Xiaoxiang%2520Zhu%26entry.1292438233%3D%2520%2520Forest%2520biomass%2520is%2520a%2520key%2520influence%2520for%2520future%2520climate%252C%2520and%2520the%2520world%2520urgently%250Aneeds%2520highly%2520scalable%2520financing%2520schemes%252C%2520such%2520as%2520carbon%2520offsetting%250Acertifications%252C%2520to%2520protect%2520and%2520restore%2520forests.%2520Current%2520manual%2520forest%2520carbon%250Astock%2520inventory%2520methods%2520of%2520measuring%2520single%2520trees%2520by%2520hand%2520are%2520time%252C%2520labour%252C%2520and%250Acost-intensive%2520and%2520have%2520been%2520shown%2520to%2520be%2520subjective.%2520They%2520can%2520lead%2520to%250Asubstantial%2520overestimation%2520of%2520the%2520carbon%2520stock%2520and%2520ultimately%2520distrust%2520in%250Aforest%2520financing.%2520The%2520potential%2520for%2520impact%2520and%2520scale%2520of%2520leveraging%2520advancements%250Ain%2520machine%2520learning%2520and%2520remote%2520sensing%2520technologies%2520is%2520promising%2520but%2520needs%2520to%250Abe%2520of%2520high%2520quality%2520in%2520order%2520to%2520replace%2520the%2520current%2520forest%2520stock%2520protocols%2520for%250Acertifications.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520ReforesTree%252C%2520a%2520benchmark%2520dataset%2520of%2520forest%2520carbon%250Astock%2520in%2520six%2520agro-forestry%2520carbon%2520offsetting%2520sites%2520in%2520Ecuador.%2520Furthermore%252C%2520we%250Ashow%2520that%2520a%2520deep%2520learning-based%2520end-to-end%2520model%2520using%2520individual%2520tree%250Adetection%2520from%2520low%2520cost%2520RGB-only%2520drone%2520imagery%2520is%2520accurately%2520estimating%2520forest%250Acarbon%2520stock%2520within%2520official%2520carbon%2520offsetting%2520certification%2520standards.%250AAdditionally%252C%2520our%2520baseline%2520CNN%2520model%2520outperforms%2520state-of-the-art%250Asatellite-based%2520forest%2520biomass%2520and%2520carbon%2520stock%2520estimates%2520for%2520this%2520type%2520of%250Asmall-scale%252C%2520tropical%2520agro-forestry%2520sites.%2520We%2520present%2520this%2520dataset%2520to%2520encourage%250Amachine%2520learning%2520research%2520in%2520this%2520area%2520to%2520increase%2520accountability%2520and%250Atransparency%2520of%2520monitoring%252C%2520verification%2520and%2520reporting%2520%2528MVR%2529%2520in%2520carbon%250Aoffsetting%2520projects%252C%2520as%2520well%2520as%2520scaling%2520global%2520reforestation%2520financing%2520through%250Aaccurate%2520remote%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.11192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReforesTree%3A%20A%20Dataset%20for%20Estimating%20Tropical%20Forest%20Carbon%20Stock%20with%0A%20%20Deep%20Learning%20and%20Aerial%20Imagery&entry.906535625=Gyri%20Reiersen%20and%20David%20Dao%20and%20Bj%C3%B6rn%20L%C3%BCtjens%20and%20Konstantin%20Klemmer%20and%20Kenza%20Amara%20and%20Attila%20Steinegger%20and%20Ce%20Zhang%20and%20Xiaoxiang%20Zhu&entry.1292438233=%20%20Forest%20biomass%20is%20a%20key%20influence%20for%20future%20climate%2C%20and%20the%20world%20urgently%0Aneeds%20highly%20scalable%20financing%20schemes%2C%20such%20as%20carbon%20offsetting%0Acertifications%2C%20to%20protect%20and%20restore%20forests.%20Current%20manual%20forest%20carbon%0Astock%20inventory%20methods%20of%20measuring%20single%20trees%20by%20hand%20are%20time%2C%20labour%2C%20and%0Acost-intensive%20and%20have%20been%20shown%20to%20be%20subjective.%20They%20can%20lead%20to%0Asubstantial%20overestimation%20of%20the%20carbon%20stock%20and%20ultimately%20distrust%20in%0Aforest%20financing.%20The%20potential%20for%20impact%20and%20scale%20of%20leveraging%20advancements%0Ain%20machine%20learning%20and%20remote%20sensing%20technologies%20is%20promising%20but%20needs%20to%0Abe%20of%20high%20quality%20in%20order%20to%20replace%20the%20current%20forest%20stock%20protocols%20for%0Acertifications.%0A%20%20In%20this%20paper%2C%20we%20present%20ReforesTree%2C%20a%20benchmark%20dataset%20of%20forest%20carbon%0Astock%20in%20six%20agro-forestry%20carbon%20offsetting%20sites%20in%20Ecuador.%20Furthermore%2C%20we%0Ashow%20that%20a%20deep%20learning-based%20end-to-end%20model%20using%20individual%20tree%0Adetection%20from%20low%20cost%20RGB-only%20drone%20imagery%20is%20accurately%20estimating%20forest%0Acarbon%20stock%20within%20official%20carbon%20offsetting%20certification%20standards.%0AAdditionally%2C%20our%20baseline%20CNN%20model%20outperforms%20state-of-the-art%0Asatellite-based%20forest%20biomass%20and%20carbon%20stock%20estimates%20for%20this%20type%20of%0Asmall-scale%2C%20tropical%20agro-forestry%20sites.%20We%20present%20this%20dataset%20to%20encourage%0Amachine%20learning%20research%20in%20this%20area%20to%20increase%20accountability%20and%0Atransparency%20of%20monitoring%2C%20verification%20and%20reporting%20%28MVR%29%20in%20carbon%0Aoffsetting%20projects%2C%20as%20well%20as%20scaling%20global%20reforestation%20financing%20through%0Aaccurate%20remote%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.11192v2&entry.124074799=Read"},
{"title": "Active partitioning: inverting the paradigm of active learning", "author": "Marius Tacke and Matthias Busch and Kevin Linka and Christian J. Cyron and Roland C. Aydin", "abstract": "  Datasets often incorporate various functional patterns related to different\naspects or regimes, which are typically not equally present throughout the\ndataset. We propose a novel, general-purpose partitioning algorithm that\nutilizes competition between models to detect and separate these functional\npatterns. This competition is induced by multiple models iteratively submitting\ntheir predictions for the dataset, with the best prediction for each data point\nbeing rewarded with training on that data point. This reward mechanism\namplifies each model's strengths and encourages specialization in different\npatterns. The specializations can then be translated into a partitioning\nscheme. The amplification of each model's strengths inverts the active learning\nparadigm: while active learning typically focuses the training of models on\ntheir weaknesses to minimize the number of required training data points, our\nconcept reinforces the strengths of each model, thus specializing them. We\nvalidate our concept -- called active partitioning -- with various datasets\nwith clearly distinct functional patterns, such as mechanical stress and strain\ndata in a porous structure. The active partitioning algorithm produces valuable\ninsights into the datasets' structure, which can serve various further\napplications. As a demonstration of one exemplary usage, we set up modular\nmodels consisting of multiple expert models, each learning a single partition,\nand compare their performance on more than twenty popular regression problems\nwith single models learning all partitions simultaneously. Our results show\nsignificant improvements, with up to 54% loss reduction, confirming our\npartitioning algorithm's utility.\n", "link": "http://arxiv.org/abs/2411.18254v1", "date": "2024-11-27", "relevancy": 2.0003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20partitioning%3A%20inverting%20the%20paradigm%20of%20active%20learning&body=Title%3A%20Active%20partitioning%3A%20inverting%20the%20paradigm%20of%20active%20learning%0AAuthor%3A%20Marius%20Tacke%20and%20Matthias%20Busch%20and%20Kevin%20Linka%20and%20Christian%20J.%20Cyron%20and%20Roland%20C.%20Aydin%0AAbstract%3A%20%20%20Datasets%20often%20incorporate%20various%20functional%20patterns%20related%20to%20different%0Aaspects%20or%20regimes%2C%20which%20are%20typically%20not%20equally%20present%20throughout%20the%0Adataset.%20We%20propose%20a%20novel%2C%20general-purpose%20partitioning%20algorithm%20that%0Autilizes%20competition%20between%20models%20to%20detect%20and%20separate%20these%20functional%0Apatterns.%20This%20competition%20is%20induced%20by%20multiple%20models%20iteratively%20submitting%0Atheir%20predictions%20for%20the%20dataset%2C%20with%20the%20best%20prediction%20for%20each%20data%20point%0Abeing%20rewarded%20with%20training%20on%20that%20data%20point.%20This%20reward%20mechanism%0Aamplifies%20each%20model%27s%20strengths%20and%20encourages%20specialization%20in%20different%0Apatterns.%20The%20specializations%20can%20then%20be%20translated%20into%20a%20partitioning%0Ascheme.%20The%20amplification%20of%20each%20model%27s%20strengths%20inverts%20the%20active%20learning%0Aparadigm%3A%20while%20active%20learning%20typically%20focuses%20the%20training%20of%20models%20on%0Atheir%20weaknesses%20to%20minimize%20the%20number%20of%20required%20training%20data%20points%2C%20our%0Aconcept%20reinforces%20the%20strengths%20of%20each%20model%2C%20thus%20specializing%20them.%20We%0Avalidate%20our%20concept%20--%20called%20active%20partitioning%20--%20with%20various%20datasets%0Awith%20clearly%20distinct%20functional%20patterns%2C%20such%20as%20mechanical%20stress%20and%20strain%0Adata%20in%20a%20porous%20structure.%20The%20active%20partitioning%20algorithm%20produces%20valuable%0Ainsights%20into%20the%20datasets%27%20structure%2C%20which%20can%20serve%20various%20further%0Aapplications.%20As%20a%20demonstration%20of%20one%20exemplary%20usage%2C%20we%20set%20up%20modular%0Amodels%20consisting%20of%20multiple%20expert%20models%2C%20each%20learning%20a%20single%20partition%2C%0Aand%20compare%20their%20performance%20on%20more%20than%20twenty%20popular%20regression%20problems%0Awith%20single%20models%20learning%20all%20partitions%20simultaneously.%20Our%20results%20show%0Asignificant%20improvements%2C%20with%20up%20to%2054%25%20loss%20reduction%2C%20confirming%20our%0Apartitioning%20algorithm%27s%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520partitioning%253A%2520inverting%2520the%2520paradigm%2520of%2520active%2520learning%26entry.906535625%3DMarius%2520Tacke%2520and%2520Matthias%2520Busch%2520and%2520Kevin%2520Linka%2520and%2520Christian%2520J.%2520Cyron%2520and%2520Roland%2520C.%2520Aydin%26entry.1292438233%3D%2520%2520Datasets%2520often%2520incorporate%2520various%2520functional%2520patterns%2520related%2520to%2520different%250Aaspects%2520or%2520regimes%252C%2520which%2520are%2520typically%2520not%2520equally%2520present%2520throughout%2520the%250Adataset.%2520We%2520propose%2520a%2520novel%252C%2520general-purpose%2520partitioning%2520algorithm%2520that%250Autilizes%2520competition%2520between%2520models%2520to%2520detect%2520and%2520separate%2520these%2520functional%250Apatterns.%2520This%2520competition%2520is%2520induced%2520by%2520multiple%2520models%2520iteratively%2520submitting%250Atheir%2520predictions%2520for%2520the%2520dataset%252C%2520with%2520the%2520best%2520prediction%2520for%2520each%2520data%2520point%250Abeing%2520rewarded%2520with%2520training%2520on%2520that%2520data%2520point.%2520This%2520reward%2520mechanism%250Aamplifies%2520each%2520model%2527s%2520strengths%2520and%2520encourages%2520specialization%2520in%2520different%250Apatterns.%2520The%2520specializations%2520can%2520then%2520be%2520translated%2520into%2520a%2520partitioning%250Ascheme.%2520The%2520amplification%2520of%2520each%2520model%2527s%2520strengths%2520inverts%2520the%2520active%2520learning%250Aparadigm%253A%2520while%2520active%2520learning%2520typically%2520focuses%2520the%2520training%2520of%2520models%2520on%250Atheir%2520weaknesses%2520to%2520minimize%2520the%2520number%2520of%2520required%2520training%2520data%2520points%252C%2520our%250Aconcept%2520reinforces%2520the%2520strengths%2520of%2520each%2520model%252C%2520thus%2520specializing%2520them.%2520We%250Avalidate%2520our%2520concept%2520--%2520called%2520active%2520partitioning%2520--%2520with%2520various%2520datasets%250Awith%2520clearly%2520distinct%2520functional%2520patterns%252C%2520such%2520as%2520mechanical%2520stress%2520and%2520strain%250Adata%2520in%2520a%2520porous%2520structure.%2520The%2520active%2520partitioning%2520algorithm%2520produces%2520valuable%250Ainsights%2520into%2520the%2520datasets%2527%2520structure%252C%2520which%2520can%2520serve%2520various%2520further%250Aapplications.%2520As%2520a%2520demonstration%2520of%2520one%2520exemplary%2520usage%252C%2520we%2520set%2520up%2520modular%250Amodels%2520consisting%2520of%2520multiple%2520expert%2520models%252C%2520each%2520learning%2520a%2520single%2520partition%252C%250Aand%2520compare%2520their%2520performance%2520on%2520more%2520than%2520twenty%2520popular%2520regression%2520problems%250Awith%2520single%2520models%2520learning%2520all%2520partitions%2520simultaneously.%2520Our%2520results%2520show%250Asignificant%2520improvements%252C%2520with%2520up%2520to%252054%2525%2520loss%2520reduction%252C%2520confirming%2520our%250Apartitioning%2520algorithm%2527s%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20partitioning%3A%20inverting%20the%20paradigm%20of%20active%20learning&entry.906535625=Marius%20Tacke%20and%20Matthias%20Busch%20and%20Kevin%20Linka%20and%20Christian%20J.%20Cyron%20and%20Roland%20C.%20Aydin&entry.1292438233=%20%20Datasets%20often%20incorporate%20various%20functional%20patterns%20related%20to%20different%0Aaspects%20or%20regimes%2C%20which%20are%20typically%20not%20equally%20present%20throughout%20the%0Adataset.%20We%20propose%20a%20novel%2C%20general-purpose%20partitioning%20algorithm%20that%0Autilizes%20competition%20between%20models%20to%20detect%20and%20separate%20these%20functional%0Apatterns.%20This%20competition%20is%20induced%20by%20multiple%20models%20iteratively%20submitting%0Atheir%20predictions%20for%20the%20dataset%2C%20with%20the%20best%20prediction%20for%20each%20data%20point%0Abeing%20rewarded%20with%20training%20on%20that%20data%20point.%20This%20reward%20mechanism%0Aamplifies%20each%20model%27s%20strengths%20and%20encourages%20specialization%20in%20different%0Apatterns.%20The%20specializations%20can%20then%20be%20translated%20into%20a%20partitioning%0Ascheme.%20The%20amplification%20of%20each%20model%27s%20strengths%20inverts%20the%20active%20learning%0Aparadigm%3A%20while%20active%20learning%20typically%20focuses%20the%20training%20of%20models%20on%0Atheir%20weaknesses%20to%20minimize%20the%20number%20of%20required%20training%20data%20points%2C%20our%0Aconcept%20reinforces%20the%20strengths%20of%20each%20model%2C%20thus%20specializing%20them.%20We%0Avalidate%20our%20concept%20--%20called%20active%20partitioning%20--%20with%20various%20datasets%0Awith%20clearly%20distinct%20functional%20patterns%2C%20such%20as%20mechanical%20stress%20and%20strain%0Adata%20in%20a%20porous%20structure.%20The%20active%20partitioning%20algorithm%20produces%20valuable%0Ainsights%20into%20the%20datasets%27%20structure%2C%20which%20can%20serve%20various%20further%0Aapplications.%20As%20a%20demonstration%20of%20one%20exemplary%20usage%2C%20we%20set%20up%20modular%0Amodels%20consisting%20of%20multiple%20expert%20models%2C%20each%20learning%20a%20single%20partition%2C%0Aand%20compare%20their%20performance%20on%20more%20than%20twenty%20popular%20regression%20problems%0Awith%20single%20models%20learning%20all%20partitions%20simultaneously.%20Our%20results%20show%0Asignificant%20improvements%2C%20with%20up%20to%2054%25%20loss%20reduction%2C%20confirming%20our%0Apartitioning%20algorithm%27s%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18254v1&entry.124074799=Read"},
{"title": "Don't Let Your Robot be Harmful: Responsible Robotic Manipulation", "author": "Minheng Ni and Lei Zhang and Zihan Chen and Lei Zhang and Wangmeng Zuo", "abstract": "  Unthinking execution of human instructions in robotic manipulation can lead\nto severe safety risks, such as poisonings, fires, and even explosions. In this\npaper, we present responsible robotic manipulation, which requires robots to\nconsider potential hazards in the real-world environment while completing\ninstructions and performing complex operations safely and efficiently. However,\nsuch scenarios in real world are variable and risky for training. To address\nthis challenge, we propose Safety-as-policy, which includes (i) a world model\nto automatically generate scenarios containing safety risks and conduct virtual\ninteractions, and (ii) a mental model to infer consequences with reflections\nand gradually develop the cognition of safety, allowing robots to accomplish\ntasks while avoiding dangers. Additionally, we create the SafeBox synthetic\ndataset, which includes one hundred responsible robotic manipulation tasks with\ndifferent safety risk scenarios and instructions, effectively reducing the\nrisks associated with real-world experiments. Experiments demonstrate that\nSafety-as-policy can avoid risks and efficiently complete tasks in both\nsynthetic dataset and real-world experiments, significantly outperforming\nbaseline methods. Our SafeBox dataset shows consistent evaluation results with\nreal-world scenarios, serving as a safe and effective benchmark for future\nresearch.\n", "link": "http://arxiv.org/abs/2411.18289v1", "date": "2024-11-27", "relevancy": 1.6727, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6135}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Let%20Your%20Robot%20be%20Harmful%3A%20Responsible%20Robotic%20Manipulation&body=Title%3A%20Don%27t%20Let%20Your%20Robot%20be%20Harmful%3A%20Responsible%20Robotic%20Manipulation%0AAuthor%3A%20Minheng%20Ni%20and%20Lei%20Zhang%20and%20Zihan%20Chen%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Unthinking%20execution%20of%20human%20instructions%20in%20robotic%20manipulation%20can%20lead%0Ato%20severe%20safety%20risks%2C%20such%20as%20poisonings%2C%20fires%2C%20and%20even%20explosions.%20In%20this%0Apaper%2C%20we%20present%20responsible%20robotic%20manipulation%2C%20which%20requires%20robots%20to%0Aconsider%20potential%20hazards%20in%20the%20real-world%20environment%20while%20completing%0Ainstructions%20and%20performing%20complex%20operations%20safely%20and%20efficiently.%20However%2C%0Asuch%20scenarios%20in%20real%20world%20are%20variable%20and%20risky%20for%20training.%20To%20address%0Athis%20challenge%2C%20we%20propose%20Safety-as-policy%2C%20which%20includes%20%28i%29%20a%20world%20model%0Ato%20automatically%20generate%20scenarios%20containing%20safety%20risks%20and%20conduct%20virtual%0Ainteractions%2C%20and%20%28ii%29%20a%20mental%20model%20to%20infer%20consequences%20with%20reflections%0Aand%20gradually%20develop%20the%20cognition%20of%20safety%2C%20allowing%20robots%20to%20accomplish%0Atasks%20while%20avoiding%20dangers.%20Additionally%2C%20we%20create%20the%20SafeBox%20synthetic%0Adataset%2C%20which%20includes%20one%20hundred%20responsible%20robotic%20manipulation%20tasks%20with%0Adifferent%20safety%20risk%20scenarios%20and%20instructions%2C%20effectively%20reducing%20the%0Arisks%20associated%20with%20real-world%20experiments.%20Experiments%20demonstrate%20that%0ASafety-as-policy%20can%20avoid%20risks%20and%20efficiently%20complete%20tasks%20in%20both%0Asynthetic%20dataset%20and%20real-world%20experiments%2C%20significantly%20outperforming%0Abaseline%20methods.%20Our%20SafeBox%20dataset%20shows%20consistent%20evaluation%20results%20with%0Areal-world%20scenarios%2C%20serving%20as%20a%20safe%20and%20effective%20benchmark%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Let%2520Your%2520Robot%2520be%2520Harmful%253A%2520Responsible%2520Robotic%2520Manipulation%26entry.906535625%3DMinheng%2520Ni%2520and%2520Lei%2520Zhang%2520and%2520Zihan%2520Chen%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Unthinking%2520execution%2520of%2520human%2520instructions%2520in%2520robotic%2520manipulation%2520can%2520lead%250Ato%2520severe%2520safety%2520risks%252C%2520such%2520as%2520poisonings%252C%2520fires%252C%2520and%2520even%2520explosions.%2520In%2520this%250Apaper%252C%2520we%2520present%2520responsible%2520robotic%2520manipulation%252C%2520which%2520requires%2520robots%2520to%250Aconsider%2520potential%2520hazards%2520in%2520the%2520real-world%2520environment%2520while%2520completing%250Ainstructions%2520and%2520performing%2520complex%2520operations%2520safely%2520and%2520efficiently.%2520However%252C%250Asuch%2520scenarios%2520in%2520real%2520world%2520are%2520variable%2520and%2520risky%2520for%2520training.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520Safety-as-policy%252C%2520which%2520includes%2520%2528i%2529%2520a%2520world%2520model%250Ato%2520automatically%2520generate%2520scenarios%2520containing%2520safety%2520risks%2520and%2520conduct%2520virtual%250Ainteractions%252C%2520and%2520%2528ii%2529%2520a%2520mental%2520model%2520to%2520infer%2520consequences%2520with%2520reflections%250Aand%2520gradually%2520develop%2520the%2520cognition%2520of%2520safety%252C%2520allowing%2520robots%2520to%2520accomplish%250Atasks%2520while%2520avoiding%2520dangers.%2520Additionally%252C%2520we%2520create%2520the%2520SafeBox%2520synthetic%250Adataset%252C%2520which%2520includes%2520one%2520hundred%2520responsible%2520robotic%2520manipulation%2520tasks%2520with%250Adifferent%2520safety%2520risk%2520scenarios%2520and%2520instructions%252C%2520effectively%2520reducing%2520the%250Arisks%2520associated%2520with%2520real-world%2520experiments.%2520Experiments%2520demonstrate%2520that%250ASafety-as-policy%2520can%2520avoid%2520risks%2520and%2520efficiently%2520complete%2520tasks%2520in%2520both%250Asynthetic%2520dataset%2520and%2520real-world%2520experiments%252C%2520significantly%2520outperforming%250Abaseline%2520methods.%2520Our%2520SafeBox%2520dataset%2520shows%2520consistent%2520evaluation%2520results%2520with%250Areal-world%2520scenarios%252C%2520serving%2520as%2520a%2520safe%2520and%2520effective%2520benchmark%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Let%20Your%20Robot%20be%20Harmful%3A%20Responsible%20Robotic%20Manipulation&entry.906535625=Minheng%20Ni%20and%20Lei%20Zhang%20and%20Zihan%20Chen%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Unthinking%20execution%20of%20human%20instructions%20in%20robotic%20manipulation%20can%20lead%0Ato%20severe%20safety%20risks%2C%20such%20as%20poisonings%2C%20fires%2C%20and%20even%20explosions.%20In%20this%0Apaper%2C%20we%20present%20responsible%20robotic%20manipulation%2C%20which%20requires%20robots%20to%0Aconsider%20potential%20hazards%20in%20the%20real-world%20environment%20while%20completing%0Ainstructions%20and%20performing%20complex%20operations%20safely%20and%20efficiently.%20However%2C%0Asuch%20scenarios%20in%20real%20world%20are%20variable%20and%20risky%20for%20training.%20To%20address%0Athis%20challenge%2C%20we%20propose%20Safety-as-policy%2C%20which%20includes%20%28i%29%20a%20world%20model%0Ato%20automatically%20generate%20scenarios%20containing%20safety%20risks%20and%20conduct%20virtual%0Ainteractions%2C%20and%20%28ii%29%20a%20mental%20model%20to%20infer%20consequences%20with%20reflections%0Aand%20gradually%20develop%20the%20cognition%20of%20safety%2C%20allowing%20robots%20to%20accomplish%0Atasks%20while%20avoiding%20dangers.%20Additionally%2C%20we%20create%20the%20SafeBox%20synthetic%0Adataset%2C%20which%20includes%20one%20hundred%20responsible%20robotic%20manipulation%20tasks%20with%0Adifferent%20safety%20risk%20scenarios%20and%20instructions%2C%20effectively%20reducing%20the%0Arisks%20associated%20with%20real-world%20experiments.%20Experiments%20demonstrate%20that%0ASafety-as-policy%20can%20avoid%20risks%20and%20efficiently%20complete%20tasks%20in%20both%0Asynthetic%20dataset%20and%20real-world%20experiments%2C%20significantly%20outperforming%0Abaseline%20methods.%20Our%20SafeBox%20dataset%20shows%20consistent%20evaluation%20results%20with%0Areal-world%20scenarios%2C%20serving%20as%20a%20safe%20and%20effective%20benchmark%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18289v1&entry.124074799=Read"},
{"title": "Citywide Electric Vehicle Charging Demand Prediction Approach\n  Considering Urban Region and Dynamic Influences", "author": "Haoxuan Kuang and Kunxiang Deng and Linlin You and Jun Li", "abstract": "  Electric vehicle charging demand prediction is important for vacant charging\npile recommendation and charging infrastructure planning, thus facilitating\nvehicle electrification and green energy development. The performance of\nprevious spatio-temporal studies is still far from satisfactory nowadays\nbecause urban region attributes and multivariate temporal influences are not\nadequately taken into account. To tackle these issues, we propose a learning\napproach for citywide electric vehicle charging demand prediction, named\nCityEVCP. To learn non-pairwise relationships in urban areas, we cluster\nservice areas by the types and numbers of points of interest in the areas and\ndevelop attentive hypergraph networks accordingly. Graph attention mechanisms\nare employed for information propagation between neighboring areas.\nAdditionally, we propose a variable selection network to adaptively learn\ndynamic auxiliary information and improve the Transformer encoder utilizing\ngated mechanisms for fluctuating charging time-series data. Experiments on a\ncitywide electric vehicle charging dataset demonstrate the performances of our\nproposed approach compared with a broad range of competing baselines.\nFurthermore, we demonstrate the impact of dynamic influences on prediction\nresults in different areas of the city and the effectiveness of our area\nclustering method.\n", "link": "http://arxiv.org/abs/2410.18766v2", "date": "2024-11-27", "relevancy": 1.8348, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4645}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4549}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%20Approach%0A%20%20Considering%20Urban%20Region%20and%20Dynamic%20Influences&body=Title%3A%20Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%20Approach%0A%20%20Considering%20Urban%20Region%20and%20Dynamic%20Influences%0AAuthor%3A%20Haoxuan%20Kuang%20and%20Kunxiang%20Deng%20and%20Linlin%20You%20and%20Jun%20Li%0AAbstract%3A%20%20%20Electric%20vehicle%20charging%20demand%20prediction%20is%20important%20for%20vacant%20charging%0Apile%20recommendation%20and%20charging%20infrastructure%20planning%2C%20thus%20facilitating%0Avehicle%20electrification%20and%20green%20energy%20development.%20The%20performance%20of%0Aprevious%20spatio-temporal%20studies%20is%20still%20far%20from%20satisfactory%20nowadays%0Abecause%20urban%20region%20attributes%20and%20multivariate%20temporal%20influences%20are%20not%0Aadequately%20taken%20into%20account.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20learning%0Aapproach%20for%20citywide%20electric%20vehicle%20charging%20demand%20prediction%2C%20named%0ACityEVCP.%20To%20learn%20non-pairwise%20relationships%20in%20urban%20areas%2C%20we%20cluster%0Aservice%20areas%20by%20the%20types%20and%20numbers%20of%20points%20of%20interest%20in%20the%20areas%20and%0Adevelop%20attentive%20hypergraph%20networks%20accordingly.%20Graph%20attention%20mechanisms%0Aare%20employed%20for%20information%20propagation%20between%20neighboring%20areas.%0AAdditionally%2C%20we%20propose%20a%20variable%20selection%20network%20to%20adaptively%20learn%0Adynamic%20auxiliary%20information%20and%20improve%20the%20Transformer%20encoder%20utilizing%0Agated%20mechanisms%20for%20fluctuating%20charging%20time-series%20data.%20Experiments%20on%20a%0Acitywide%20electric%20vehicle%20charging%20dataset%20demonstrate%20the%20performances%20of%20our%0Aproposed%20approach%20compared%20with%20a%20broad%20range%20of%20competing%20baselines.%0AFurthermore%2C%20we%20demonstrate%20the%20impact%20of%20dynamic%20influences%20on%20prediction%0Aresults%20in%20different%20areas%20of%20the%20city%20and%20the%20effectiveness%20of%20our%20area%0Aclustering%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCitywide%2520Electric%2520Vehicle%2520Charging%2520Demand%2520Prediction%2520Approach%250A%2520%2520Considering%2520Urban%2520Region%2520and%2520Dynamic%2520Influences%26entry.906535625%3DHaoxuan%2520Kuang%2520and%2520Kunxiang%2520Deng%2520and%2520Linlin%2520You%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Electric%2520vehicle%2520charging%2520demand%2520prediction%2520is%2520important%2520for%2520vacant%2520charging%250Apile%2520recommendation%2520and%2520charging%2520infrastructure%2520planning%252C%2520thus%2520facilitating%250Avehicle%2520electrification%2520and%2520green%2520energy%2520development.%2520The%2520performance%2520of%250Aprevious%2520spatio-temporal%2520studies%2520is%2520still%2520far%2520from%2520satisfactory%2520nowadays%250Abecause%2520urban%2520region%2520attributes%2520and%2520multivariate%2520temporal%2520influences%2520are%2520not%250Aadequately%2520taken%2520into%2520account.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520learning%250Aapproach%2520for%2520citywide%2520electric%2520vehicle%2520charging%2520demand%2520prediction%252C%2520named%250ACityEVCP.%2520To%2520learn%2520non-pairwise%2520relationships%2520in%2520urban%2520areas%252C%2520we%2520cluster%250Aservice%2520areas%2520by%2520the%2520types%2520and%2520numbers%2520of%2520points%2520of%2520interest%2520in%2520the%2520areas%2520and%250Adevelop%2520attentive%2520hypergraph%2520networks%2520accordingly.%2520Graph%2520attention%2520mechanisms%250Aare%2520employed%2520for%2520information%2520propagation%2520between%2520neighboring%2520areas.%250AAdditionally%252C%2520we%2520propose%2520a%2520variable%2520selection%2520network%2520to%2520adaptively%2520learn%250Adynamic%2520auxiliary%2520information%2520and%2520improve%2520the%2520Transformer%2520encoder%2520utilizing%250Agated%2520mechanisms%2520for%2520fluctuating%2520charging%2520time-series%2520data.%2520Experiments%2520on%2520a%250Acitywide%2520electric%2520vehicle%2520charging%2520dataset%2520demonstrate%2520the%2520performances%2520of%2520our%250Aproposed%2520approach%2520compared%2520with%2520a%2520broad%2520range%2520of%2520competing%2520baselines.%250AFurthermore%252C%2520we%2520demonstrate%2520the%2520impact%2520of%2520dynamic%2520influences%2520on%2520prediction%250Aresults%2520in%2520different%2520areas%2520of%2520the%2520city%2520and%2520the%2520effectiveness%2520of%2520our%2520area%250Aclustering%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%20Approach%0A%20%20Considering%20Urban%20Region%20and%20Dynamic%20Influences&entry.906535625=Haoxuan%20Kuang%20and%20Kunxiang%20Deng%20and%20Linlin%20You%20and%20Jun%20Li&entry.1292438233=%20%20Electric%20vehicle%20charging%20demand%20prediction%20is%20important%20for%20vacant%20charging%0Apile%20recommendation%20and%20charging%20infrastructure%20planning%2C%20thus%20facilitating%0Avehicle%20electrification%20and%20green%20energy%20development.%20The%20performance%20of%0Aprevious%20spatio-temporal%20studies%20is%20still%20far%20from%20satisfactory%20nowadays%0Abecause%20urban%20region%20attributes%20and%20multivariate%20temporal%20influences%20are%20not%0Aadequately%20taken%20into%20account.%20To%20tackle%20these%20issues%2C%20we%20propose%20a%20learning%0Aapproach%20for%20citywide%20electric%20vehicle%20charging%20demand%20prediction%2C%20named%0ACityEVCP.%20To%20learn%20non-pairwise%20relationships%20in%20urban%20areas%2C%20we%20cluster%0Aservice%20areas%20by%20the%20types%20and%20numbers%20of%20points%20of%20interest%20in%20the%20areas%20and%0Adevelop%20attentive%20hypergraph%20networks%20accordingly.%20Graph%20attention%20mechanisms%0Aare%20employed%20for%20information%20propagation%20between%20neighboring%20areas.%0AAdditionally%2C%20we%20propose%20a%20variable%20selection%20network%20to%20adaptively%20learn%0Adynamic%20auxiliary%20information%20and%20improve%20the%20Transformer%20encoder%20utilizing%0Agated%20mechanisms%20for%20fluctuating%20charging%20time-series%20data.%20Experiments%20on%20a%0Acitywide%20electric%20vehicle%20charging%20dataset%20demonstrate%20the%20performances%20of%20our%0Aproposed%20approach%20compared%20with%20a%20broad%20range%20of%20competing%20baselines.%0AFurthermore%2C%20we%20demonstrate%20the%20impact%20of%20dynamic%20influences%20on%20prediction%0Aresults%20in%20different%20areas%20of%20the%20city%20and%20the%20effectiveness%20of%20our%20area%0Aclustering%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18766v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


